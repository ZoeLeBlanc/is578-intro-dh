work_id,conference_label,conference_short_title,conference_theme_title,conference_year,conference_organizers,conference_series,conference_hosting_institutions,conference_city,conference_state,conference_country,conference_url,work_title,work_url,work_authors,work_type,full_text,full_text_type,full_text_license,parent_work_id,keywords,languages,topics,cleaned_conference_year,lower_full_text,count_Voyant,lower_count_Voyant,toolname_Voyant
1580,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,4Humanities: Designing Digital Advocacy,,Lindsay Thomas;Alan Liu;Geoffrey Rockwell;Stéfan Sinclair;Melissa Terras;Jared Bielby;Victoria Smith;Mark Turcato;Christine Henseler,"paper, specified ""long paper""","Introduction: Cuts, Crisis and Criticisms of the Humanities
The Great Recession beginning in 2008 has resulted in a series of budgetary cuts to many universities, education programs, and cultural institutions that reflect indifference about, and even hostility toward, the humanities and arts in favor of scientific, engineering, business, and other applied fields. Even scientists are now concerned about the perceived legitimacy of their ""basic research"" when it lacks evident short-term application. However, the sciences have an established tradition of public advocacy and media communication that is quite effective in putting their discoveries before the public. The humanities have no such consistent, planned tradition of advocacy, and in many ways are starting from scratch.

This paper argues for and demonstrates planned humanities advocacy using the special affordances of the digital humanities. In particular, it discusses how the 4Humanities initiative is leveraging DH for next-generation advocacy. In this paper, we will:

Show how 4Humanities uses DH to help analyze public discourse, both pro and con, about the humanities (including text analysis of such discourse as well as crowd-sourced generation of arguments for the humanities).
Show how statistics and other evidence about the contribution of the humanities to society can be analyzed and visualized in support of effective arguments.
Discuss the role of 4Humanities and, more generally, how DH can provide special tools for humanities advocacy, while humanities advocacy can in return incentivize the creation of next-generation DH research and teaching tools with a built-in public engagement dimension.
Analysis of Arguments Against the Humanities
One way to bolster advocacy for the humanities is first to look closely at the arguments made in public against them. We have compiled a small corpus of recent articles (especially from news sources accessed by a broad public) representative of criticisms of the humanities (Auslin, 2012; Bauerlein, 2011; Cohan, 2012a; Cohan, 2012b; Ellouk, 2011; Fendrich, 2009; Fish, 2007; Fish 2008; Fund, 2012; Knapp, 2011; Murdoch, 2011; Pidgeon, 2007; Riley, 2012; Sini, 2011; Stephens, 2012; Wente, 2012; Wood, 2012).


Fig 1:
Voyant Collocate Cluster Visualization (Sinclair and Rockwell)

We find that arguments critical of the humanities cluster around certain ideas. Principally, detractors accuse the humanities of lacking cultural and/or economic relevance. The most nuanced critiques mix or shade the charges of cultural and economic irrelevance. But other arguments refute the social usefulness of the humanities entirely, simply taking for granted their complete economic and social irrelevance. (Interestingly, however, some of these articles also defend irrelevance as a virtue, as in the case of arguments from friends of the humanities who feel that irrelevance is the basic nature of the humanities and is perfectly acceptable.)

A related critique of the professoriate in the humanities is that our work is no longer accessible to a larger educated public. The argument is that we are our own worst enemies because we have descended into theoretical turf battles that no one cares about. For these commentators such cultural irrelevance goes hand in hand with the supposed lack of respect that academics show for the public’s values, as epitomized in Marxism, feminism, post-colonialism, and other approaches that attack iconic ideas. The antidote sometimes proposed is a return to a nebulous concept of the traditional humanities — e.g., to the venerable search for the “beautiful.”

This paper will summarize our reading of the articles as well as present results from some text analysis of other rhetoric about the humanities.

Analysis of Arguments For the Humanities
As important as it is to know the arguments critical of the humanities, it is also important to gather good arguments for the humanities. 4Humanities has taken two approaches to this. The first is to blog good arguments as we come across them with summaries for those who are looking for essays to help them in their advocacy. We have also summarized these in a digestible form for people to review (Bielby, 2012). Finally we ran an All Our Ideas vote on the value of the humanities (http://allourideas.org/4humanities). At the time of writing this proposal there were over 1600 votes and 31 user submitted possible answers (as opposed to 12 seeds that we provided.) The top choices at the time of writing included:


There is obviously overlap in the top choices, but these indicate what the digital humanities community considers important. In the paper we will provide a fuller analysis of the data along with our list of the best arguments.

Looking Closely at the Statistics
“Liberal arts graduates frequently catch or surpass graduates with career-oriented majors in both job quality and compensation.” (Koc, 2011)

In addition to defining and communicating the cultural value of the humanities, the 4Humanities initiative is also committed to gathering data on the economic value of the humanities. One of the most pernicious arguments against the humanities has been the poor job prospects of graduating humanists. For this reason, we review the statistical arguments carefully, especially to highlight the fact that the data on compensation of humanists at mid-career paints a different story. A humanist may find it harder to get a first job with a degree, but she/he will probably rise faster than many with professional certification.

When making arguments for the humanities’ contribution to society, of course, it can be difficult to produce statistics, given that there has been no comprehensive study that gathers together available facts and figures in a usable manner. As part of the 4Humanities project we have been compiling and listing all statistics we can find in the published literature about the benefit of the humanities to society. This was done by collating the literature – including newspaper articles, reports, websites, and op-ed pieces (listed on the 4Humanities website) – and locating numeric references to the humanities. We have compiled such quantitative evidence, and at DH2013 we will present an infographic that sums up the statistical argument that the humanities are relevant to economic and intellectual development.

In addition, we have recently begun our “Infographics Friday” series (http://humanistica.ualberta.ca/category/for-the-public/humanities-infographics/), highlighting a particular statistic or graphical representation on the 4Humanities website once a week, to demonstrate the range of evidence. A core remit of 4Humanities is to gather, analyse, and disseminate this disparate information to provide a knowledge base upon which others can build their opinions and bolster their understanding of the humanities.

Conclusion: 4Humanities and a Digital Humanities response to the Cuts, Crisis, and Criticism
We argue that the mission of public engagement and advocacy in the humanities as embodied by the 4Humanities initiative provides a unique way to consolidate leading technological and methodological directions in DH with outreach to society. The humanities today have an advantage that was not available earlier: the analytical and communication methods of the digital humanities. Not only do the digital humanities provide a strong argument for the relevance of humanities learning in a digital age; they also provide unique, fresh ways of studying the contributions of the humanities to society and then getting the message out. DH research and humanities advocacy can be one, where DH helps advance advocacy, and, reciprocally, the advocacy mission helps drive research in DH. The hunt is now on to develop and extend new generations of digital humanities platforms and tools that can integrate the core research and teaching work of humanists with public visibility and engagement. Such platforms and tools (for publishing, editing, research, pedagogy, etc.) can be designed from the ground up, both to serve the needs of academics and to engage with today's networked public. This paper is a step in that direction.

References
Auslin, M. (2012). Knowledge is Good. National Review Online. 15 March. http://www.aei.org/article/education/higher-education/knowledge-is-good/ (accessed 13 March 2013).
Bauerlein, M. (2011). Oh, the Humanities! The Weekly Standard. 16 May. http://www.weeklystandard.com/articles/oh-humanities_559340.html (accessed 13 March 2013).
Bielby, J. (2012). Arguments for the Humanities. CIRCA Wiki, October. http://circa.cs.ualberta.ca/index.php/CIRCA:Arguments_FOR_the_Humanities (accessed 13 March 2013).
Cohan, P. (2012a). To Boost Post-College Prospects, Cut Humanities Departments. Forbes, 29 May. http://www.forbes.com/sites/petercohan/2012/05/29/to-boost-post-college-prospects-cut-humanities-departments/ (accessed 13 March 2013).
Cohan, P. (2012b). The 13 Most Useless Majors, From Philosophy to Journalism. The Daily Beast, 23 April. http://www.thedailybeast.com/galleries/2012/04/23/the-13-most-useless-majors-from-philosophy-to-journalism.html (accessed 13 March 2013).
Davidson, C. N. (2011). Strangers on a Train. Academe. 97 (5). http://www.aaup.org/AAUP/pubsres/academe/2011/SO/Feat/davi.htm (accessed 13 March 2013).
Davidson, C. N. (2012). Humanities 2.0: Promise, Perils, Predictions. In Gold, M. (ed), Debates in the Digital Humanities. Minneapolis: University of Minnesota Press. 476-489.
Davidson, C. N., and D. T. Goldberg (2004). A Manifesto for the Humanities in a Technological Age. The Chronicle of Higher Education: The Chronicle Review. 13 Feb.
Delblanco, A. (2011). College: What It Was, Is, and Should Be. Princeton, NJ: Princeton University Press.
Ellouk, B. (2011). Do We Still Need the Humanities? The Daily of the University of Washington. 26 July. http://dailyuw.com/news/2011/jul/26/do-we-still-need-humanities/ (accessed 13 March 2013).
Fendrich, L. (2009). The Humanities Have No Purpose. The Chronicle of Higher Education, 20 March. http://chronicle.com/blogs/brainstorm/the-humanities-have-no-purpose/6738 (accessed 13 March 2013).
Fish, S. (2007). Bound For Academic Glory? The New York Times, 23 December. http://opinionator.blogs.nytimes.com/2007/12/23/bound-for-academic-glory/ (accessed 13 March 2013).
Fish, S. (2008). Will the Humanities Save Us? The New York Times, 6 January. http://opinionator.blogs.nytimes.com/2008/01/06/will-the-humanities-save-us/ (accessed 13 March 2013).
Fish, S. (2010). The Crisis of the Humanities Officially Arrives. The New York Times, 11 October. http://opinionator.blogs.nytimes.com/2010/10/11/the-crisis-of-the-humanities-officially-arrives/ (accessed 13 March 2013).
Fund, J. (2012). Censoring Naomi Riley. The National Review Online, 12 May. http://www.nationalreview.com/articles/299765/censoring-naomi-riley-john-fund (accessed 13 May 2013).
Kirschenbaum, M. (2012). Digital Humanities As/Is a Tactical Term. In Gold, M. (ed), Debates in the Digital Humanities. Minneapolis: University of Minnesota Press, 415-428.
Knapp, S. (2011). The Enduring Dilemma of the Humanities. The Phi Beta Kappa Society, 29 March. http://www.pbk.org/home/FocusNews.aspx?id=741 (accessed 13 March 2013).
Koc, E. W. (2011). Just Wait 10 Years. New York Times, 21 March. http://www.nytimes.com/roomfordebate/2011/03/20/career-counselor-bill-gates-or-steve-jobs/your-college-major-matter-less-over-time. (accessed 13 March 2013).
Lakoff, G. (2004). Don’t Think of an Elephant!: Know Your Values and Frame the Debate. White River, VT: Chelsea Green Publishing Company.
Liu, A. (2012). Where Is Cultural Criticism in the Digital Humanities? In Gold, M. (ed), Debates in the Digital Humanities. Minneapolis, MN: University of Minnesota Press, 490-509.
Murdoch, R. (2011). The Steve Jobs Model for Education Reform. The Wall Street Journal, 15 October. http://online.wsj.com/article/SB10001424052970203914304576631100415237430.html (accessed 13 March 2013).
Pidgeon, S. (2007). Commented on Fish, S. Bound For Academic Glory? The New York Times, 23 December. http://opinionator.blogs.nytimes.com/2007/12/23/bound-for-academic-glory/#comment-100883 (accessed 13 March 2013).
Riley, N. S. (2012). The Academic Mob Rules. The Wall Street Journal, 8 May. http://online.wsj.com/article/SB10001424052702304363104577391842133259230.html (accessed 13 May 2013).
Sinclair, S., and G. Rockwell Collocates Cluster, from Voyant Tools. http://docs.voyant-tools.org/tools/links/ (accessed 13 March 2013).
Sini, M. (2011). Oh the Humanities! (OR: A Critique of Crisis). OverLand, 22 February. http://overland.org.au/blogs/loudspeaker/2011/02/%E2%80%98oh-the-humanities%E2%80%99-or-a-critique-of-crisis/ (accessed 13 March 2013).
Stephens, B. (2012). To the Class of 2012. The Wall Street Journal, 9 May. http://online.wsj.com/article/SB10001424052702304451104577389750993890854.html (accessed 13 March 2013).
Wente, M. (2012). Quebec’s University Students are in for a Shock. The Globe and Mail, 1 May. http://www.theglobeandmail.com/commentary/quebecs-university-students-are-in-for-a-shock/article4104304/ (accessed 13 March 2013).
Wood, P. (2012). Rick Santorum is Right. The Chronicle of Higher Education, 29 February. http://chronicle.com/blogs/innovations/rick-santorum-is-right/31769 (accessed 13 March 2013).",txt,This text is republished here with permission from the original rights holder.,,4humanities;digital humanities;humanities advocacy,English,crowdsourcing;digital humanities - institutional support;text analysis,2013-01-01,"introduction: cuts, crisis and criticisms of the humanities
the great recession beginning in 2008 has resulted in a series of budgetary cuts to many universities, education programs, and cultural institutions that reflect indifference about, and even hostility toward, the humanities and arts in favor of scientific, engineering, business, and other applied fields. even scientists are now concerned about the perceived legitimacy of their ""basic research"" when it lacks evident short-term application. however, the sciences have an established tradition of public advocacy and media communication that is quite effective in putting their discoveries before the public. the humanities have no such consistent, planned tradition of advocacy, and in many ways are starting from scratch.

this paper argues for and demonstrates planned humanities advocacy using the special affordances of the digital humanities. in particular, it discusses how the 4humanities initiative is leveraging dh for next-generation advocacy. in this paper, we will:

show how 4humanities uses dh to help analyze public discourse, both pro and con, about the humanities (including text analysis of such discourse as well as crowd-sourced generation of arguments for the humanities).
show how statistics and other evidence about the contribution of the humanities to society can be analyzed and visualized in support of effective arguments.
discuss the role of 4humanities and, more generally, how dh can provide special tools for humanities advocacy, while humanities advocacy can in return incentivize the creation of next-generation dh research and teaching tools with a built-in public engagement dimension.
analysis of arguments against the humanities
one way to bolster advocacy for the humanities is first to look closely at the arguments made in public against them. we have compiled a small corpus of recent articles (especially from news sources accessed by a broad public) representative of criticisms of the humanities (auslin, 2012; bauerlein, 2011; cohan, 2012a; cohan, 2012b; ellouk, 2011; fendrich, 2009; fish, 2007; fish 2008; fund, 2012; knapp, 2011; murdoch, 2011; pidgeon, 2007; riley, 2012; sini, 2011; stephens, 2012; wente, 2012; wood, 2012).


fig 1:
voyant collocate cluster visualization (sinclair and rockwell)

we find that arguments critical of the humanities cluster around certain ideas. principally, detractors accuse the humanities of lacking cultural and/or economic relevance. the most nuanced critiques mix or shade the charges of cultural and economic irrelevance. but other arguments refute the social usefulness of the humanities entirely, simply taking for granted their complete economic and social irrelevance. (interestingly, however, some of these articles also defend irrelevance as a virtue, as in the case of arguments from friends of the humanities who feel that irrelevance is the basic nature of the humanities and is perfectly acceptable.)

a related critique of the professoriate in the humanities is that our work is no longer accessible to a larger educated public. the argument is that we are our own worst enemies because we have descended into theoretical turf battles that no one cares about. for these commentators such cultural irrelevance goes hand in hand with the supposed lack of respect that academics show for the public’s values, as epitomized in marxism, feminism, post-colonialism, and other approaches that attack iconic ideas. the antidote sometimes proposed is a return to a nebulous concept of the traditional humanities — e.g., to the venerable search for the “beautiful.”

this paper will summarize our reading of the articles as well as present results from some text analysis of other rhetoric about the humanities.

analysis of arguments for the humanities
as important as it is to know the arguments critical of the humanities, it is also important to gather good arguments for the humanities. 4humanities has taken two approaches to this. the first is to blog good arguments as we come across them with summaries for those who are looking for essays to help them in their advocacy. we have also summarized these in a digestible form for people to review (bielby, 2012). finally we ran an all our ideas vote on the value of the humanities (http://allourideas.org/4humanities). at the time of writing this proposal there were over 1600 votes and 31 user submitted possible answers (as opposed to 12 seeds that we provided.) the top choices at the time of writing included:


there is obviously overlap in the top choices, but these indicate what the digital humanities community considers important. in the paper we will provide a fuller analysis of the data along with our list of the best arguments.

looking closely at the statistics
“liberal arts graduates frequently catch or surpass graduates with career-oriented majors in both job quality and compensation.” (koc, 2011)

in addition to defining and communicating the cultural value of the humanities, the 4humanities initiative is also committed to gathering data on the economic value of the humanities. one of the most pernicious arguments against the humanities has been the poor job prospects of graduating humanists. for this reason, we review the statistical arguments carefully, especially to highlight the fact that the data on compensation of humanists at mid-career paints a different story. a humanist may find it harder to get a first job with a degree, but she/he will probably rise faster than many with professional certification.

when making arguments for the humanities’ contribution to society, of course, it can be difficult to produce statistics, given that there has been no comprehensive study that gathers together available facts and figures in a usable manner. as part of the 4humanities project we have been compiling and listing all statistics we can find in the published literature about the benefit of the humanities to society. this was done by collating the literature – including newspaper articles, reports, websites, and op-ed pieces (listed on the 4humanities website) – and locating numeric references to the humanities. we have compiled such quantitative evidence, and at dh2013 we will present an infographic that sums up the statistical argument that the humanities are relevant to economic and intellectual development.

in addition, we have recently begun our “infographics friday” series (http://humanistica.ualberta.ca/category/for-the-public/humanities-infographics/), highlighting a particular statistic or graphical representation on the 4humanities website once a week, to demonstrate the range of evidence. a core remit of 4humanities is to gather, analyse, and disseminate this disparate information to provide a knowledge base upon which others can build their opinions and bolster their understanding of the humanities.

conclusion: 4humanities and a digital humanities response to the cuts, crisis, and criticism
we argue that the mission of public engagement and advocacy in the humanities as embodied by the 4humanities initiative provides a unique way to consolidate leading technological and methodological directions in dh with outreach to society. the humanities today have an advantage that was not available earlier: the analytical and communication methods of the digital humanities. not only do the digital humanities provide a strong argument for the relevance of humanities learning in a digital age; they also provide unique, fresh ways of studying the contributions of the humanities to society and then getting the message out. dh research and humanities advocacy can be one, where dh helps advance advocacy, and, reciprocally, the advocacy mission helps drive research in dh. the hunt is now on to develop and extend new generations of digital humanities platforms and tools that can integrate the core research and teaching work of humanists with public visibility and engagement. such platforms and tools (for publishing, editing, research, pedagogy, etc.) can be designed from the ground up, both to serve the needs of academics and to engage with today's networked public. this paper is a step in that direction.

references
auslin, m. (2012). knowledge is good. national review online. 15 march. http://www.aei.org/article/education/higher-education/knowledge-is-good/ (accessed 13 march 2013).
bauerlein, m. (2011). oh, the humanities! the weekly standard. 16 may. http://www.weeklystandard.com/articles/oh-humanities_559340.html (accessed 13 march 2013).
bielby, j. (2012). arguments for the humanities. circa wiki, october. http://circa.cs.ualberta.ca/index.php/circa:arguments_for_the_humanities (accessed 13 march 2013).
cohan, p. (2012a). to boost post-college prospects, cut humanities departments. forbes, 29 may. http://www.forbes.com/sites/petercohan/2012/05/29/to-boost-post-college-prospects-cut-humanities-departments/ (accessed 13 march 2013).
cohan, p. (2012b). the 13 most useless majors, from philosophy to journalism. the daily beast, 23 april. http://www.thedailybeast.com/galleries/2012/04/23/the-13-most-useless-majors-from-philosophy-to-journalism.html (accessed 13 march 2013).
davidson, c. n. (2011). strangers on a train. academe. 97 (5). http://www.aaup.org/aaup/pubsres/academe/2011/so/feat/davi.htm (accessed 13 march 2013).
davidson, c. n. (2012). humanities 2.0: promise, perils, predictions. in gold, m. (ed), debates in the digital humanities. minneapolis: university of minnesota press. 476-489.
davidson, c. n., and d. t. goldberg (2004). a manifesto for the humanities in a technological age. the chronicle of higher education: the chronicle review. 13 feb.
delblanco, a. (2011). college: what it was, is, and should be. princeton, nj: princeton university press.
ellouk, b. (2011). do we still need the humanities? the daily of the university of washington. 26 july. http://dailyuw.com/news/2011/jul/26/do-we-still-need-humanities/ (accessed 13 march 2013).
fendrich, l. (2009). the humanities have no purpose. the chronicle of higher education, 20 march. http://chronicle.com/blogs/brainstorm/the-humanities-have-no-purpose/6738 (accessed 13 march 2013).
fish, s. (2007). bound for academic glory? the new york times, 23 december. http://opinionator.blogs.nytimes.com/2007/12/23/bound-for-academic-glory/ (accessed 13 march 2013).
fish, s. (2008). will the humanities save us? the new york times, 6 january. http://opinionator.blogs.nytimes.com/2008/01/06/will-the-humanities-save-us/ (accessed 13 march 2013).
fish, s. (2010). the crisis of the humanities officially arrives. the new york times, 11 october. http://opinionator.blogs.nytimes.com/2010/10/11/the-crisis-of-the-humanities-officially-arrives/ (accessed 13 march 2013).
fund, j. (2012). censoring naomi riley. the national review online, 12 may. http://www.nationalreview.com/articles/299765/censoring-naomi-riley-john-fund (accessed 13 may 2013).
kirschenbaum, m. (2012). digital humanities as/is a tactical term. in gold, m. (ed), debates in the digital humanities. minneapolis: university of minnesota press, 415-428.
knapp, s. (2011). the enduring dilemma of the humanities. the phi beta kappa society, 29 march. http://www.pbk.org/home/focusnews.aspx?id=741 (accessed 13 march 2013).
koc, e. w. (2011). just wait 10 years. new york times, 21 march. http://www.nytimes.com/roomfordebate/2011/03/20/career-counselor-bill-gates-or-steve-jobs/your-college-major-matter-less-over-time. (accessed 13 march 2013).
lakoff, g. (2004). don’t think of an elephant!: know your values and frame the debate. white river, vt: chelsea green publishing company.
liu, a. (2012). where is cultural criticism in the digital humanities? in gold, m. (ed), debates in the digital humanities. minneapolis, mn: university of minnesota press, 490-509.
murdoch, r. (2011). the steve jobs model for education reform. the wall street journal, 15 october. http://online.wsj.com/article/sb10001424052970203914304576631100415237430.html (accessed 13 march 2013).
pidgeon, s. (2007). commented on fish, s. bound for academic glory? the new york times, 23 december. http://opinionator.blogs.nytimes.com/2007/12/23/bound-for-academic-glory/#comment-100883 (accessed 13 march 2013).
riley, n. s. (2012). the academic mob rules. the wall street journal, 8 may. http://online.wsj.com/article/sb10001424052702304363104577391842133259230.html (accessed 13 may 2013).
sinclair, s., and g. rockwell collocates cluster, from voyant tools. http://docs.voyant-tools.org/tools/links/ (accessed 13 march 2013).
sini, m. (2011). oh the humanities! (or: a critique of crisis). overland, 22 february. http://overland.org.au/blogs/loudspeaker/2011/02/%e2%80%98oh-the-humanities%e2%80%99-or-a-critique-of-crisis/ (accessed 13 march 2013).
stephens, b. (2012). to the class of 2012. the wall street journal, 9 may. http://online.wsj.com/article/sb10001424052702304451104577389750993890854.html (accessed 13 march 2013).
wente, m. (2012). quebec’s university students are in for a shock. the globe and mail, 1 may. http://www.theglobeandmail.com/commentary/quebecs-university-students-are-in-for-a-shock/article4104304/ (accessed 13 march 2013).
wood, p. (2012). rick santorum is right. the chronicle of higher education, 29 february. http://chronicle.com/blogs/innovations/rick-santorum-is-right/31769 (accessed 13 march 2013).",2.0,3.0,Voyant
1582,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,Voyant Notebooks: Literate Programming and Programming Literacy,,Stéfan Sinclair;Geoffrey Rockwell,poster / demo / art installation,"Some thirty years ago Donald Knuth, a computer scientist, proposed literate programming as a better way of organizing narrative and code (1984). Knuth argued that more emphasis should be placed on explaining to humans what computers are meant to do, rather than simply instructing computers what to do. Knuth was especially interested in weaving together macrostyle code snippets with prose that provided a larger narrative context, not merely functional comments of specific lines of code that are the distilled remnants of an intellectual process.
Literate programming has been more influential in theory than in practice (Nørmark), despite several utilities and environments including Mathematica, Knuth's (C)WEB, Sweave for R, and Marginalia for Clojure. Perhaps the exigencies of programming in the real world correspond poorly with the vision of Knuth of the programmer as author: ""the practitioner of literate programming can be regarded as an essayist, whose main concern is with exposition and excellence of style"" (1992, 1). However, that balance of essayist and coder strikes us as perfectly appropriate for the digital humanities, a natural blend of the expression of intellectual process with the exposition of technical methodologies. The prose can gloss the code, or viceversa, in a symbiotic relationship that serves to strengthen an argument and demonstrate its own workings.
One of the most significant potential benefits of the literate programming paradigm is pedagogical: these works can both explain an interpretive insight and present the methodology for reproducing the data or results that were part of the process. Many widely-read digital humanities blogs already present these characteristics of exploration, explanation, interpretation and step-by-step instructions (see for example blogs by Ted Underwood, Benjamin Schmidt, Lisa Rhody and Scott Weingart). Literate programming can be more self-contained and more useful for those learning new methodologies and new programming techniques. This is about the principles of literate programming, but also about the potential for increasing programming literacy.
This poster will introduce Voyant Notebooks, a web-based literate programming environment designed for the digital humanities (see Appendix A). There is already a working prototype and we anticipate having a more feature-rich version available by July 2013. Voyant Notebooks inherits many of the characteristics of the Voyant Tools environment, including a concern for usability and flexibility (researchers and students should be able to use it with minimal or no training and with their own texts of interest). Voyant Notebooks also addresses one of the main weaknesses of Voyant Tools: the fact that most tools are constrained by assumptions about how they would be most commonly used. For instance, the Wordle-like (word cloud) Cirrus tool is designed to show the top frequency terms from a corpus or document; but what if the user instead wants to visualize the top frequency nouns, or people, or repeating phrases? All of that functionality could be built into the tool, but possibly at the cost of usability (endless menus and options), and it could still never address all of the possible use cases. Voyant Notebooks, by contrast, empowers the user to customize some of the functionality by leveraging the analytic capabilities of the Voyant back-end and the visualization interfaces in the front-end (like Cirrus). Our poster will have two parts, a) a usable demonstration on one or more laptops and b) a poster that illustrates how Voyant Notebooks implements Knuth’s concept of literate programming. In addition to these conceptual aspects, the poster will outline technical details about the Voyant Notebooks prototype for those interested, including the technologies used for both client-side (browser) and server-side components. Some of the technical challenges that will be described include:
•        managing the flow of code execution in an asynchronous architecture,
•        using web workers to avoid browser freezes during longer executions,
•        mitigating the security risks of user-defined and persistent Javascript code,
•        code variable scoping across editor instances and window components,
•        embedding of Voyant tool panels (visualizations) and other services,
•        developing a flexible API for different programming levels and styles,
•        developing an API that includes both client-side and server-side operations, and
•        ensuring efficiency of repeated code snippets during writing and viewing.
And of course, visitors to the poster session will be warmly encouraged to play with Voyant Notebooks.
Appendix A: Mockup of Voyant Notebooks (previously called Voyeur Notebooks).
 
Figure 1:
Mockup of Voyant Notebooks
References
Knuth, D. (1984). Literate Programming. The Computer Journal 27(2): 97-111, 1.
Knuth, D. (1992). Literate Programming. Stanford University Center for the Study of Language and Information.
Nørmark, K. (1998). Literate Programming: Issues and Problems. http://www.cs.aau.dk/~normark/litpro/issues-and-problems.html.
Sinclair, S. and G. Rockwell (2012). Teaching Computer-Assisted Text Analysis: Approaches to Learning New Methodologies. in Digital Humanities Pedagogy. Open Book Publishers.",txt,This text is republished here with permission from the original rights holder.,,literate computing;text analysis;voyant,English,text analysis,2013-01-01,"some thirty years ago donald knuth, a computer scientist, proposed literate programming as a better way of organizing narrative and code (1984). knuth argued that more emphasis should be placed on explaining to humans what computers are meant to do, rather than simply instructing computers what to do. knuth was especially interested in weaving together macrostyle code snippets with prose that provided a larger narrative context, not merely functional comments of specific lines of code that are the distilled remnants of an intellectual process.
literate programming has been more influential in theory than in practice (nørmark), despite several utilities and environments including mathematica, knuth's (c)web, sweave for r, and marginalia for clojure. perhaps the exigencies of programming in the real world correspond poorly with the vision of knuth of the programmer as author: ""the practitioner of literate programming can be regarded as an essayist, whose main concern is with exposition and excellence of style"" (1992, 1). however, that balance of essayist and coder strikes us as perfectly appropriate for the digital humanities, a natural blend of the expression of intellectual process with the exposition of technical methodologies. the prose can gloss the code, or viceversa, in a symbiotic relationship that serves to strengthen an argument and demonstrate its own workings.
one of the most significant potential benefits of the literate programming paradigm is pedagogical: these works can both explain an interpretive insight and present the methodology for reproducing the data or results that were part of the process. many widely-read digital humanities blogs already present these characteristics of exploration, explanation, interpretation and step-by-step instructions (see for example blogs by ted underwood, benjamin schmidt, lisa rhody and scott weingart). literate programming can be more self-contained and more useful for those learning new methodologies and new programming techniques. this is about the principles of literate programming, but also about the potential for increasing programming literacy.
this poster will introduce voyant notebooks, a web-based literate programming environment designed for the digital humanities (see appendix a). there is already a working prototype and we anticipate having a more feature-rich version available by july 2013. voyant notebooks inherits many of the characteristics of the voyant tools environment, including a concern for usability and flexibility (researchers and students should be able to use it with minimal or no training and with their own texts of interest). voyant notebooks also addresses one of the main weaknesses of voyant tools: the fact that most tools are constrained by assumptions about how they would be most commonly used. for instance, the wordle-like (word cloud) cirrus tool is designed to show the top frequency terms from a corpus or document; but what if the user instead wants to visualize the top frequency nouns, or people, or repeating phrases? all of that functionality could be built into the tool, but possibly at the cost of usability (endless menus and options), and it could still never address all of the possible use cases. voyant notebooks, by contrast, empowers the user to customize some of the functionality by leveraging the analytic capabilities of the voyant back-end and the visualization interfaces in the front-end (like cirrus). our poster will have two parts, a) a usable demonstration on one or more laptops and b) a poster that illustrates how voyant notebooks implements knuth’s concept of literate programming. in addition to these conceptual aspects, the poster will outline technical details about the voyant notebooks prototype for those interested, including the technologies used for both client-side (browser) and server-side components. some of the technical challenges that will be described include:
•        managing the flow of code execution in an asynchronous architecture,
•        using web workers to avoid browser freezes during longer executions,
•        mitigating the security risks of user-defined and persistent javascript code,
•        code variable scoping across editor instances and window components,
•        embedding of voyant tool panels (visualizations) and other services,
•        developing a flexible api for different programming levels and styles,
•        developing an api that includes both client-side and server-side operations, and
•        ensuring efficiency of repeated code snippets during writing and viewing.
and of course, visitors to the poster session will be warmly encouraged to play with voyant notebooks.
appendix a: mockup of voyant notebooks (previously called voyeur notebooks).
 
figure 1:
mockup of voyant notebooks
references
knuth, d. (1984). literate programming. the computer journal 27(2): 97-111, 1.
knuth, d. (1992). literate programming. stanford university center for the study of language and information.
nørmark, k. (1998). literate programming: issues and problems. http://www.cs.aau.dk/~normark/litpro/issues-and-problems.html.
sinclair, s. and g. rockwell (2012). teaching computer-assisted text analysis: approaches to learning new methodologies. in digital humanities pedagogy. open book publishers.",13.0,13.0,Voyant
1657,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,The Design of New Knowledge Environments,,Ann Blandford;Susan Brown;Teresa Dobson;Sarah Faisal;Carlos Fiorentino;Luciano Frizzera;Alejandro Giacometti;Brooke Heller;Geoff Roeder;Ernesto Peña;Mihaela Ilovan;Piotr Michura;Brent Nelson;Atefeh Mohseni;Milena Radzikowska;Geoffrey Rockwell;Stan Ruecker;Stéfan Sinclair;Daniel Sondheim;Sarah Vela;Jennifer Windsor;Tian Yi;Elena Dergacheva,panel / roundtable,"1. An Introduction to the Design of New Knowledge Environments
Ruecker, Stan|sruecker@id.iit.edu|IIT Institute of Design
Rockwell, Geoffrey|grockwel@ualberta.ca|University of Alberta
INKE Research Group||
In this panel, we report on our year 4 work in the Interface Design (ID) research team of the Implementing New Knowledge Environments (INKE) project. INKE is a 7-year major collaborative research initiative (MCRI) project funded in Canada by the Social Sciences and Humanities Research Council (SSHRC). INKE is led by Ray Siemens at the University of Victoria, and includes dozens of researchers worldwide. Each year, we have had a different focus for our work. The schedule is as follows:

Year 1: interdisciplinary citations
Year 2: corpora
Years 3 and 4: the scholarly edition
Year 5: the monograph and journal
Year 6: born-digital literature
Year 7: wrap-up and dissemination
For the first three years of the project, our focus has been on a wide range of experimental prototypes. Beginning in year 4, we attempted to aggregate these prototypes into a smaller set of “new knowledge environments”, where the goal is no longer to have a single piece of functionality that we can experiment with, but instead to begin imagining how the various pieces of functionality, as represented by our prototypes, can work in concert to produce an environment for people working with electronic books.

What makes these environments “new” is that they are comprised of a set of experiments into working with digital text. More often than not, these experiments involve the design and programming of a prototype, which may exist at any one of a range of levels of fidelity. Ideally, the lowest level of fidelity is developed that is necessary in order to come to grips with the central idea. To develop further is sometimes required in order to achieve an adequate user experience study, but in any case it is important to keep in mind that the end goal is the extension of our understanding rather than the production of a piece of stable software.

So the process is to conceive of a concept and produce a prototype to help us better understand the concept. Typically a prototype will generate some new knowledge itself, and that knowledge can contribute to the next iteration of the idea. In some cases, the prototype teaches us enough that there is no need for a further prototype. If what we’ve learned is of potential use, then we can consider a more robust implementation of the idea. If what we’ve learned is that the line of thought we’ve been pursuing may not be fruitful after all, then we can at that point abandon the trajectory and move to another idea.

In the case of the year 4 projects in INKE, we have selected some prototypes that we think deserve to be aggregated into an environment. We are at the same time trying to learn what we can about the theoretical and technical issues involved in this kind of redesign and reprogramming for the purposes of aggregation. These are environments rather than simpler tools in the sense that they afford more than a single task or set of tasks.

In the papers that follow, we describe each of the environments, examine the state of the art in scholarly editions for tablets, and discuss the user experience study of two of the prototypes that have gone on to inform one of our new knowledge environments.

2. Reading Skins: Voyant and Tool Aggregation
Rockwell, Geoffrey|grockwel@ualberta.ca|University of Alberta
Sinclair, Stéfan|stefan.sinclair@mcgill.ca|McMaster University
INKE Research Group||
Tools are not just ways to ask questions - they are also reading skins. Interface designers have known this for some time. They design interfaces to present affordances and views that facilitate different types of reading. A dictionary is designed for consultation reading (Blair 2010), a manual for training. Likewise an e-book minimizes distractions, presenting “just the text”, while text analysis environments embed the texts in alternative ways of reading, from visualizations to interactive controls. These design choices are based on a model of the reader and the tasks they are engaged in. In this paper we will look at the types of interfaces or “skins” presented by text analysis environments and end by discussing the decisions taken in the design of Voyant. We will do that in the following ways:

1 First, we will look back at one of the first computing humanists to consider visualization and the interface to text analysis tools, John Smith.
2 Second, we will survey different types of text analysis interfaces.
3 Third, we will close by discussing how the architecture of Voyant is designed to allow for different reading skins that aggregate different tools to suit different uses.
1. First Thoughts on the Interface
One of the first computing humanists to think about the interface to text analysis tools was John Smith. John Smith in “Computer Criticism” and other articles proposed a way that analytical tools like his ARRAS could fit into interpretative research practices. He saw the computer as a tool to help identify and then trace structures through a text and gave an example of how this can help rereading a text in his article “Image and Imagery in Joyce's Portrait.” In “A New Environment For Literary Analysis” he explicitly discussed how the analytical tool ARRAS was not meant to replace the inquirer but to amplify them, how it could provide what we today call visualizations, and how it should be thought of not as a program, but as an environment for work where one could switch from text analysis to editing the text or sending a message. In the presentation we will go into more detail about how Smith articulated the relationship between tool design and interpretative practices.

2. Survey of Type of Interface
In the second part of the paper we will survey various interface paradigms for text analysis tools including:

ARRAS: We will start with the interface John Smith developed for ARRAS, and discuss his ideas for a humanities accessible command line language.
TACT and TACTweb: We will then discuss TACT, which was released in 1989 and designed by John Bradley and others (Lancashire 1996). TACT, while running in MS DOS, was influenced by the then new idea of a Graphical User Interface (GUI). It had a primitive windowing model that let you split the screen to see multiple displays and use one to drive the other(s). TACTweb, which came later, brought TACT functionality to the web, and illustrated for the first time how the web separated interface from text database so that multiple interfaces could be built.
HyperPo: While HyperPo (hyperpo.org) wasn’t the first text analysis environment on the web, it was one of the first to fully exploit the web. It let you upload a text and it provided a number of innovative features including making displays themselves affordances for further interaction. It was also explicitly designed as a reading environment.
TextArc: One of the most beautiful interfaces to text analysis is the TextArc (textarc.org) visual concordance designed by W. Bradford Paley. This work pushes the idea of interface in interesting directions as it can be considered a work of art or design meant to be appreciated in and of itself rather than as a window onto something else.
Other interfaces could be mentioned like the visual programming interface idea of Eye-ConTACT that is also available in SEASR or the library interface of the MONK project, but these are more management interfaces than reading ones.

3. Voyant and Skins
In the last part of the presentation we will discuss the layered architecture of Voyant that allows one to create different combinations of tools into “skins” that aggregate different tools. A Skin Builder tool for creating your own skins will be demonstrated and different examples of skins for different reading purposes will be shown. Different uses call for different combinations of tools.

All of these text analysis interfaces are presented from the perspective that they are views on a static object that is studied, but what if the object of study is changing? What if we think of text analysis tools performing the text rather than skin it? The presentation will end with an alternative prototype interface designed not for reading, but for animating the text as if it were a performance.


Figure 1 -
Voyant Skin Builder

References
Blair, A. (2010). Too Much to Know: Managing Scholarly Information Before the Modern Age. New Haven: Yale University Press.
Galey, A., and S. Ruecker (2010). How a Prototype Argues. Literary and Linguistic Computing 25(4): 405–424.
Lancashire, I., et al. (1996). Using TACT with Electronic Texts: a Guide to Text-analysis Computing Tools, Version 2.1 for MS-DOS and PC DOS. Modern Language Association of America.
Parunak, H. V. D. (1981). Prolegomena to Pictorial Concordances. Computers and the Humanities 15(1): 15–36.
Rockwell, G., S. Sinclair, et al. (2010). Ubiquitous Text Analysis. The Journal of the Initiative for Digital Humanities, Media, and Culture 2(1).
Sinclair, S. (2003). Computer-Assisted Reading: Reconceiving Text Analysis. Literary and Linguistic Computing 18(2): 175–184.
Sinclair, S., and G. Rockwell (2009). Between Language and Literature: Digital Text Exploration. Teaching Literature and Language Online. In Lancashire, I. (ed. and introd.). vii, 460 pp. New York, NY: Modern Language Association of America. 104–117. Options for Teaching (OfT): 26.
Smith, J. (1984). A New Environment for Literary Analysis. Perspectives in Computing: Applications in the Academic and Scientific Community 4(2): 20–31.
Smith, J. (1989). Computer Criticism. Literary Computing and Literary Criticism: Theoretical and Practical Essays on Theme and Rhetoric. University of Pennsylvania Press. 326–356.
Smith, J. (1973). Image and Imagery in Joyce’s Portrait: A Computer-Assisted Analysis. Directions in Literary Criticism: Contemporary Approaches to Literature. Festschrift for Henry W. Sams, (ed.) Stanley Weintraub and Philip Young (University Park and London: Pennsylvania State University Press): 220–227.
Smith, J. (1985). Arras User's Manual: TR85-036. Chapel Hill, NC, The University of North Carolina at Chapel Hill.
3. Designing the interface within the interface: legibility and readability in the Dynamic Table of Contexts
Windsor, Jennifer|jwindsor@ualberta.ca|University of Alberta
Brown, Susan|sbrown@uoguelph.ca|University of Guelph
Nelson, Brent|brent.nelson@usask.ca|University of Saskatchewan
Radzikowska, Milena|mradzikowska@gmail.com|Mount Royal University
Sinclair, Stéfan|stefan.sinclair@mcgill.ca|McMaster University
INKE Research Group||
“Typography is to literature as musical performance is to composition: an essential act of interpretation, full of endless opportunities for insight or obtuseness.”

— Robert Bringhurst, The Elements of Typographic Style
As humanities scholars transition from reading traditional print texts to reading on computer screens, ebooks and tablets, we are discovering that the visible word has taken a step backwards in quality. Typographic considerations are often the most challenging aspects of user experience development of digital reading environments. Ereaders are still in their infancy, and thus far little attention has been paid to textual design beyond very basic choices of typeface and font size.

We have developed the Dynamic Table of Contexts, a text analysis environment that combines the traditional concepts of the table of contents and index to create new methods for Humanities scholars to read and interact with digital text. The main interface consists of four interactive panes that perform dynamically with each other: the table of contents, the index, the xml tag list and a large pane to read the text itself (see Figure 1). In addition, there is a pane for reader notes. The Dynamic Table of Contexts interface is predominantly textual in nature, but the text at the centre of it is itself an interface – the point of interaction between the reader and that which is read. This study examines the appearance and arrangement of the textual interface within the larger interface. We asked: how can we typographically optimize the ereading experience?

The guiding fundamentals of typography are legibility and readability. Legibility concerns the ease with which a letterform can be recognized and readability refers to the ease with which text can be understood (Lupton 2004). Many problems with legibility have arisen in the transition from print to screen. For example, rather than black ink on a paper page, black text on a screen is an absence in the glowing pixels that surround it, and the result is perceived as blurry regardless of screen resolution. Anti-aliasing is used to compensate for low resolution on most computer monitors but can't be applied consistently from character to character because of where the letter may land on the physical pixel grid. In an effort to offset this problem font size is often increased, which in itself can become a reading irritant and navigationally awkward as larger type creates shorter lines of text which in turn requires more left-right eye movements from the end of one line to the beginning of the next. Fewer lines of text also necessitate more scrolling which is visually uncomfortable and requires additional time and visual energy for the reader to relocate himself in the text after each movement. While we wait for technology to catch up in these areas, this study outlines the methods we used to minimize legibility issues and discusses which existing typefaces (most of which, it must be remembered, were never intended for the screen) best counteract blurring and movement. We identify optimal type size and line length combinations for comfortable extended on-screen reading in the Dynamic Table of Contexts.

Compared to legibility, issues of readability are less often addressed in discussions of digital reading environments. Readability is comprised not only of the arrangement of type on a page or screen, but also of attention to the entire visual entity and all the complex relationships between levels of type, symbols and images (Berryman 1984). Typography imparts semantic meaning as it interprets content and influences meaning by creating hierarchies, assigning values, and manipulating emphasis (Bachfischer, Robertson and Zmijewska 2007). Our investigations into optimal readability design for the Dynamic Table of Contexts raised interesting questions about the representation of text and the process of reading on-screen. In the remediation from print to screen, do we still read the same way? Do the rules that apply to the readability of print typography apply to on-screen typography? Do conventions that govern readability of websites also apply to extended reading of scholarly materials?

In this study we have also examined other components of the textual interface that affect readability but that are not necessarily typographic in nature. Often on-screen paging representations (such as a simulated left and right sides of a double page spread or an animated page turning) give the illusion of a traditional print book. We examine theories of whether these provide valuable perceptual cues to the reader or are simply a vestigial device that has lost its significance in a changing environment.

This research represents a move from an adequate to an optimized reading environment.


Figure 2 -
Dynamic Table of Contexts

References
Bachfischer, G., T. Robertson, and A. Zmijewska (2007). “Understanding Influences of the Typographic Quality of Text.” Journal of Internet Commerce 6(2): 97–122.
Berryman, G. (1984). Notes on Graphic Design and Visual Communication. Los Altos, California: W. Kaufmann.
Bringhurst, R. (2004). The Elements of Typographic Style. 3rd ed. Hartley and Marks Publishers.
Larson, K. (2004). “The Science of Word Recognition: Or How I Learned to Stop Worrying and Love the Bouma.” July. Microsoft Typography site. Retrieved August 20, 2006. http://www.microsoft.com/typography/ctfonts/WordRecognition.aspx
Lupton, E. (2010). Thinking with Type: A Critical Guide for Designers, Writers, Editors, & Students. New York. Princeton Architectural Press.
Mangen, A. (2008). “Hypertext Fiction Reading: Haptics and Immersion.” Journal of Research in Reading 31(4): 404–419.
Weinzettelova, S. (2012). “Traditional Type in the Digital Era.” Bulletin - Prague College Centre for Research and Interdisciplinary Studies 2012(2): 5–24.
4. The Tablet as a New Medium for Scholarly Editions
Mohseni, Atefeh|amohseni@ualberta.ca|University of Alberta
Sondheim, Daniel|sondheim@uvic.ca|University of Victoria
Frizzera, Luciano|dosreisf@ualberta.ca|University of Alberta
Rockwell, Geoffrey|grockwel@ualberta.ca|University of Alberta
Ruecker, Stan|sruecker@id.iit.edu|IIT Institute of Design
INKE Research Group||
How have scholarly editions been implemented on tablets? In the past few years, a significant number of scholarly editions have seen deployment on the Web, and the differences between printed editions and Web-based ones has become a matter of attention for many scholars. In this paper, we will investigate scholarly editions as they appear in tablets and other mobile devices.

Web-based scholarly editions offer many new features that do not and cannot exist in printed ones. Such features include opportunities for collaboration with other users or with the editors of the work, innovative methods to search and browse, specialized tools and visualizations for text analysis, means to customize the layout of the interface and the content, options to change the language of the interface or to see translations of the text, high resolution zoomable images, multimedia elements such as video and audio, and a wealth of material that would be too costly or cumbersome to include in a printed edition.

Despite these advantages, some users still prefer their scholarly editions to be in paper form. Reasons may include a lack of physical distance between reader and book, and the ability to interact directly with the material. Tablets have solved these problems to some extent, emulating the physicality of paper books, and allowing users an opportunity to touch and feel them. As Elena Pierazzo notes, “Usability studies have demonstrated that reading on tablets is more enjoyable than reading on the screen of computers and, in some cases, more than reading print” (Pierazzo 2011). Additionally, the fact that an app is an independent program housed on a particular machine results in increased speed and stability over Web-based editions, which also serves to reproduce some of the positive features of paper-based editions (McDayter 2012).

Tablets represent a return to printed editions in some more negative respects as well. For instance, users cannot create shared annotations or collaborate with each other in other ways; the app on the tablet is isolated from references and related material available on the Web; tools for text analysis are absent; and no options are provided to show the interface in different languages. Part of the reason for these omissions in tablets may be due to the fact that editions that are currently available on tablets have generally been produced more for a popular audience than a scholarly one. This is reflected not only in terms of functionalities of the interface, but with respect to the content as well. Bibliographies, glossaries, and textual apparatuses are typically missing, variants tend not to be included, and notes and annotations have been recycled from older print-based editions, rather than being the result of new scholarship. Tablet-based editions are often more focused on making an edition attractive and amusing than on making it scholarly, and may include games or quizzes, methods of sharing passages or images via social media sites, and other innovative but potentially distracting features likely to be of interest to keen fans of the material rather than to scholars.

So, tablets are in a way mediating between print and the Web, sharing some of the advantages and disadvantages of both. But what effects will these advantages and disadvantages have on scholarly editions? Do scholarly editions have a future in tablets? These are questions that we will discuss in this paper.

At this point, very few tablet-based scholarly editions have been released, and as discussed above, those that have are decidedly unscholarly in some respects. Touch Press has produced some notable examples, including editions of T.S Eliot’s The Waste Land, Leonardo da Vinci's notebooks, and Shakespeare’s sonnets. The Waste Land is the first edition produced by Touch Press, and continues to stand up as a nice example of the genre. This edition allows the poem to be viewed as plain text, and includes annotations, audio recordings, facsimile images of the original typescript, filmed performance of the poem, interviews about the poem and some image galleries.

Many of the features provided in The Waste Land would make using scholarly editions easier and more pleasurable. As is the case with most works of philosophy, art, and literature, The Waste Land is multi-layered, and multimedia is of great help in exploring, finding and analyzing the different existing layers. Study is also eased and enhanced by being able to quickly and easily reveal or hide information, switch between viewing multiple encodings of the text, and simply by having the ability to manually scroll through the text, rather than having to use a device.

With what has been discussed, some questions require further exploration. Will the advantages of tablet-based editions win out over their disadvantages? Will the tablet become accepted as an appropriate medium in which to produce and study scholarly editions? In this paper, we will consider whether scholarly editions have a future on tablets and how such a future might look.

References
Eliot, T. S. (2011). The Waste Land for iPad, ed. Justin Badger and Charles Chabot, Touch Press LLP, and Faber and Faber. http://touchpress.com/titles/thewasteland/
Pierazzo, E. (2011). Tablets Apps, or the future of the Scholarly Editions? Random Thoughts of a Digital Humanist with a Passion for Cookery, 27 November, 2011. http://epierazzo.blogspot.ca/2011/11/tablets-apps-or-future-of-scholarly.html
McDayter, M. (2012). Are We There Yet? Touch Press’s The Waste Land for iPad. February 25, 2012. http://clickherefordigitalhumanities.wordpress.com/2012/02/25/are-we-there-yet-touch-presss-the-waste-land-for-ipad/
5. Designing for Multi-Touch Surfaces as Social Reading Environments
Frizzera, Luciano|dosreisf@ualberta.ca|University of Alberta
Vela, Sarah|svela@ualberta.ca|University of Alberta
Ilovan, Mihaela|ilovan@ualberta.ca|University of Alberta
Michura, Piotr|pmichura@asp.krakow.pl|Academy of Fine Arts in Krakow
Sondheim, Daniel|sondheim@uvic.ca|University of Victoria
Rockwell, Geoffrey|grockwel@ualberta.ca|University of Alberta
Ruecker, Stan|sruecker@id.iit.edu|IIT Institute of Design
INKE Research Group||
The INKE Interface Design group has been exploring the application of multi-touch table technology for improving the comprehension, manipulation, and analysis of variorum editions beyond what has been accomplished in previous digital variorums. These volumes consist of three general components: the text of the work itself as selected by an editor; a list of variations between this ‘base-text’ and other manuscript or printed versions; and a comprehensive anthology of previous scholarly annotations on a particular line, passage, or the entire work. For many texts the existing notes are so extensive that it is difficult in a print medium to present “…simultaneous access to text and relevant commentary”, leading to an effort since the mid-nineteen-nineties to produce digital variorum editions (Werstine 2011).

Scholarly editions, especially variorums, raise interesting questions about the representation of elements and the act of reading. The richness of this type of edition creates dilemmas related to the organization of different pieces of information, as well as interacting with the text. A print version presents physical space constraints, such as the two-page spread and the necessity of linear presentation. This constraint become less of an issue in a digital environment, where space, time, and dimensionality in general are more fluid.

Although current iterations of digital variora (for example, the Online Chopin Variorum Edition, and the Electronic Variorum Edition of Don Quixote) attempt to take advantage of this flexibility of the medium, they present themselves as flat webpages, losing physical engagement with the materiality of the book. In order to bring back fuller physicality, we have used multi-touch surface technologies to simulate the “real space”, and to return to full gestures as opposed to clicks. A few projects focusing on eBooks and tablets have begun to emerge with the same idea (for example, The Wasteland, Shakespeare's Sonnets and Kerouac's On the Road), but this technology has not yet been applied to variorum editions.

Our project, The Comedy of Errors Tangible Variorum, involves creating a tangible user interface representing the Modern Language Association’s variorum edition of Shakespeare’s Comedy of Errors in order to explore the affordances of this technology to increase collaborative scholarly research and interpretation of the material.

Existing digital variorums have attempted to encourage collaboration and interaction by incorporating features into their web page interfaces. In the Electronic Variorum Edition of Don Quixote (EVE-DQ) from Texas A&M’s Cervantes project, for example, users can add their own commentary and annotations which can then be accessed by future users. Similarly the Online Chopin Variorum Edition (OCVE) in progress at King’s College London allows the attachment of notes to the base text with an option to share them publicly. For both projects, however, adding comments is an individual activity, with a single user at a workstation inputting notes with a mouse and keyboard. There is little room for group interaction, and any that exists must be sequential rather than communal. Most previous electronic New Variorum Shakespeare (eNVS) projects, particularly the recent Winter’s Tale version, have no digital annotation function at all, limiting them to use as a reference tool, rather than an interactive platform.

Furthermore, it can be exceptionally difficult to allow simultaneous visual comparison of the numerous features of a variorum on these web page formats given the small size of most screens. A study by Wästlund, Norlander, and Archer on the effect of page layout on mental workload shows that “manipulating an onscreen text document via scrolling necessitates a shift of focus from the text to the action of controlling the page movement,” (Wästlund, Norlander, and Archer, 1243) leading to decreased performance on reading comprehension tasks. Vandendorpe (2009) writes that “navigation by means of a mouse tends to give rise to chaotic, extremely rapid movement that is not very favorable to reading” (133).

By using a multi-touch table as a display device this project attempts to solve the problems of poor collaboration and broken comprehension. There has been significant research supporting the use of touch screens in improving reader focus, for example a study by Eva Siegenthaler et al. (2012) found that subjects performed better at various tasks involved in reading when using devices with tangible inputs, concluding that “a touch screen allows for an easier and more intuitive interaction” than a non-touch screen (Eva Siegenthaler et al. 2012, 94). The sizes of both the screen and the table perimeter of a multi-touch device, meanwhile, are conducive to multiple users working together. We thus believe that the application of multi-touch technology will have two effects. First, it represents another stage in the remediation of variorums, one that will better allow us to implement Unsworth’s (2000) scholarly primitives: to sample, compare, discover, represent, annotate and reference different versions of the base text. Simultaneously, it encourages these tasks to be performed socially, deepening understanding by incorporating multiple viewpoints.

The collaborative uses of tangible devices in research situations is one of the main goals of this research. In driving towards this end, however, our group has faced issues in two camps: the ability of users to adapt to multiple people concurrently using touch controls; and, less expectedly, the ability of a designer to structure elements on an unconventional screen.

From the perspective of users, much of the challenge is about breaking habits. Since the introduction of personal computers, users have learned how to interact with the machine in individual work spaces. The adoption of this concept is so intense that one is liable to think that group work is less effective than work alone with the computer. However, it is noticeable that the technology has determined this situation: people cannot work together because the machine allows just one input at a time. The questions that we raise are: what happens when more than one person can interact with the machine? What kind of operations and collaborations can a group perform when all of them are engaged with the same machine in the same environment?

For designers, learning to think beyond the confines of a screen and mouse proved to be a major obstacle. The original design conceived of the mutli-touch screen as being suspended on the wall, and the elements were placed accordingly (Figure 1). As we built and tested the interface it became clear that the layout would be effective for a single user, but was not ideal for the collaboration we were trying to achieve. While flipping the screen to a table was a logical choice to allow more users to work simultaneously, reassessing the design from that perspective resulted in a number of questions: where do you place touchstone elements when there is no clear top or bottom? How can items being used by different people overlap without disrupting anyone’s work?

This paper explores the problems faced when building a social reading environment, both for users and for designers. The technology to allow such interaction exists: a multi-touch table has a size that allows for the display of multiple documents side-by-side, and its status as a touch screen enables easy and intuitive operation, lessening the mental workload required to operate it and permitting users to focus on the content rather than the interface. The system accommodates familiar gestures such as touch, pinch and flick to let the user move, select, grab and scroll through information on the screen, and since more than one point of interaction is possible, multiple people are able to work at the same time in the examination of the material, improving collaborative work. Designing for these features, however, is a mental challenge that requires an upset of standards in the minds of all those involved, users and builders, and facing these problems is a necessary step in moving towards the future of collaboration.


Figure 1 -
Initial view showing chosen copy text in a full view and a reading panel. All variants and commentaries on the chosen page are marked. More to the right there are views of all textual notes and commentaries connected to the chosen part of the copy text. At the top there is a representation of all editions in chronological order, in which the dark bars shows the overall difference of a particular edition to the chosen copy text.

References
Bradley, J., and P. Vetch (2006). Supporting Annotation as a Scholarly Tool–Experiences From the Online Chopin Variorum Edition. Literary and Linguistic Computing 22(2): 225–241.
Siegenthaler, E., et al. (2012). The Effects of Touch Screen Technology on the Usability of E-Reading Devices. Journal of Usability Studies 7(3): 94–104.
Furuta, R., et al. (2001). Towards an Electronic Variorum Dition of Don Quixote. Proceedings of the 1st ACM/IEEE-CS Joint Conference on Digital Libraries. 444–445.
Galey, A. (2005). ‘Alms for Oblivion’: Bringing an Electronic New Variorum Shakespeare to the Screen. Shakespeare Association of America, Bermuda http://pear.hcmc.uvic.ca:8080/ach/site/xhtml.xq?id=98
Hinckley, K., et al. (1997). Cooperative Bimanual Action. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 27–34.
Unsworth, J. (2000). Scholarly Primitives: What Method",txt,This text is republished here with permission from the original rights holder.,,interface design;scholarly edition;tangible user interface;text analysis;user studies;visualization,English,content analysis;data mining / text mining;interdisciplinary collaboration;interface and user experience design;knowledge representation;mobile applications and mobile design;programming;scholarly editing;software design and development;text analysis;user studies / user needs;visualization;xml,2013-01-01,"1. an introduction to the design of new knowledge environments
ruecker, stan|sruecker@id.iit.edu|iit institute of design
rockwell, geoffrey|grockwel@ualberta.ca|university of alberta
inke research group||
in this panel, we report on our year 4 work in the interface design (id) research team of the implementing new knowledge environments (inke) project. inke is a 7-year major collaborative research initiative (mcri) project funded in canada by the social sciences and humanities research council (sshrc). inke is led by ray siemens at the university of victoria, and includes dozens of researchers worldwide. each year, we have had a different focus for our work. the schedule is as follows:

year 1: interdisciplinary citations
year 2: corpora
years 3 and 4: the scholarly edition
year 5: the monograph and journal
year 6: born-digital literature
year 7: wrap-up and dissemination
for the first three years of the project, our focus has been on a wide range of experimental prototypes. beginning in year 4, we attempted to aggregate these prototypes into a smaller set of “new knowledge environments”, where the goal is no longer to have a single piece of functionality that we can experiment with, but instead to begin imagining how the various pieces of functionality, as represented by our prototypes, can work in concert to produce an environment for people working with electronic books.

what makes these environments “new” is that they are comprised of a set of experiments into working with digital text. more often than not, these experiments involve the design and programming of a prototype, which may exist at any one of a range of levels of fidelity. ideally, the lowest level of fidelity is developed that is necessary in order to come to grips with the central idea. to develop further is sometimes required in order to achieve an adequate user experience study, but in any case it is important to keep in mind that the end goal is the extension of our understanding rather than the production of a piece of stable software.

so the process is to conceive of a concept and produce a prototype to help us better understand the concept. typically a prototype will generate some new knowledge itself, and that knowledge can contribute to the next iteration of the idea. in some cases, the prototype teaches us enough that there is no need for a further prototype. if what we’ve learned is of potential use, then we can consider a more robust implementation of the idea. if what we’ve learned is that the line of thought we’ve been pursuing may not be fruitful after all, then we can at that point abandon the trajectory and move to another idea.

in the case of the year 4 projects in inke, we have selected some prototypes that we think deserve to be aggregated into an environment. we are at the same time trying to learn what we can about the theoretical and technical issues involved in this kind of redesign and reprogramming for the purposes of aggregation. these are environments rather than simpler tools in the sense that they afford more than a single task or set of tasks.

in the papers that follow, we describe each of the environments, examine the state of the art in scholarly editions for tablets, and discuss the user experience study of two of the prototypes that have gone on to inform one of our new knowledge environments.

2. reading skins: voyant and tool aggregation
rockwell, geoffrey|grockwel@ualberta.ca|university of alberta
sinclair, stéfan|stefan.sinclair@mcgill.ca|mcmaster university
inke research group||
tools are not just ways to ask questions - they are also reading skins. interface designers have known this for some time. they design interfaces to present affordances and views that facilitate different types of reading. a dictionary is designed for consultation reading (blair 2010), a manual for training. likewise an e-book minimizes distractions, presenting “just the text”, while text analysis environments embed the texts in alternative ways of reading, from visualizations to interactive controls. these design choices are based on a model of the reader and the tasks they are engaged in. in this paper we will look at the types of interfaces or “skins” presented by text analysis environments and end by discussing the decisions taken in the design of voyant. we will do that in the following ways:

1 first, we will look back at one of the first computing humanists to consider visualization and the interface to text analysis tools, john smith.
2 second, we will survey different types of text analysis interfaces.
3 third, we will close by discussing how the architecture of voyant is designed to allow for different reading skins that aggregate different tools to suit different uses.
1. first thoughts on the interface
one of the first computing humanists to think about the interface to text analysis tools was john smith. john smith in “computer criticism” and other articles proposed a way that analytical tools like his arras could fit into interpretative research practices. he saw the computer as a tool to help identify and then trace structures through a text and gave an example of how this can help rereading a text in his article “image and imagery in joyce's portrait.” in “a new environment for literary analysis” he explicitly discussed how the analytical tool arras was not meant to replace the inquirer but to amplify them, how it could provide what we today call visualizations, and how it should be thought of not as a program, but as an environment for work where one could switch from text analysis to editing the text or sending a message. in the presentation we will go into more detail about how smith articulated the relationship between tool design and interpretative practices.

2. survey of type of interface
in the second part of the paper we will survey various interface paradigms for text analysis tools including:

arras: we will start with the interface john smith developed for arras, and discuss his ideas for a humanities accessible command line language.
tact and tactweb: we will then discuss tact, which was released in 1989 and designed by john bradley and others (lancashire 1996). tact, while running in ms dos, was influenced by the then new idea of a graphical user interface (gui). it had a primitive windowing model that let you split the screen to see multiple displays and use one to drive the other(s). tactweb, which came later, brought tact functionality to the web, and illustrated for the first time how the web separated interface from text database so that multiple interfaces could be built.
hyperpo: while hyperpo (hyperpo.org) wasn’t the first text analysis environment on the web, it was one of the first to fully exploit the web. it let you upload a text and it provided a number of innovative features including making displays themselves affordances for further interaction. it was also explicitly designed as a reading environment.
textarc: one of the most beautiful interfaces to text analysis is the textarc (textarc.org) visual concordance designed by w. bradford paley. this work pushes the idea of interface in interesting directions as it can be considered a work of art or design meant to be appreciated in and of itself rather than as a window onto something else.
other interfaces could be mentioned like the visual programming interface idea of eye-contact that is also available in seasr or the library interface of the monk project, but these are more management interfaces than reading ones.

3. voyant and skins
in the last part of the presentation we will discuss the layered architecture of voyant that allows one to create different combinations of tools into “skins” that aggregate different tools. a skin builder tool for creating your own skins will be demonstrated and different examples of skins for different reading purposes will be shown. different uses call for different combinations of tools.

all of these text analysis interfaces are presented from the perspective that they are views on a static object that is studied, but what if the object of study is changing? what if we think of text analysis tools performing the text rather than skin it? the presentation will end with an alternative prototype interface designed not for reading, but for animating the text as if it were a performance.


figure 1 -
voyant skin builder

references
blair, a. (2010). too much to know: managing scholarly information before the modern age. new haven: yale university press.
galey, a., and s. ruecker (2010). how a prototype argues. literary and linguistic computing 25(4): 405–424.
lancashire, i., et al. (1996). using tact with electronic texts: a guide to text-analysis computing tools, version 2.1 for ms-dos and pc dos. modern language association of america.
parunak, h. v. d. (1981). prolegomena to pictorial concordances. computers and the humanities 15(1): 15–36.
rockwell, g., s. sinclair, et al. (2010). ubiquitous text analysis. the journal of the initiative for digital humanities, media, and culture 2(1).
sinclair, s. (2003). computer-assisted reading: reconceiving text analysis. literary and linguistic computing 18(2): 175–184.
sinclair, s., and g. rockwell (2009). between language and literature: digital text exploration. teaching literature and language online. in lancashire, i. (ed. and introd.). vii, 460 pp. new york, ny: modern language association of america. 104–117. options for teaching (oft): 26.
smith, j. (1984). a new environment for literary analysis. perspectives in computing: applications in the academic and scientific community 4(2): 20–31.
smith, j. (1989). computer criticism. literary computing and literary criticism: theoretical and practical essays on theme and rhetoric. university of pennsylvania press. 326–356.
smith, j. (1973). image and imagery in joyce’s portrait: a computer-assisted analysis. directions in literary criticism: contemporary approaches to literature. festschrift for henry w. sams, (ed.) stanley weintraub and philip young (university park and london: pennsylvania state university press): 220–227.
smith, j. (1985). arras user's manual: tr85-036. chapel hill, nc, the university of north carolina at chapel hill.
3. designing the interface within the interface: legibility and readability in the dynamic table of contexts
windsor, jennifer|jwindsor@ualberta.ca|university of alberta
brown, susan|sbrown@uoguelph.ca|university of guelph
nelson, brent|brent.nelson@usask.ca|university of saskatchewan
radzikowska, milena|mradzikowska@gmail.com|mount royal university
sinclair, stéfan|stefan.sinclair@mcgill.ca|mcmaster university
inke research group||
“typography is to literature as musical performance is to composition: an essential act of interpretation, full of endless opportunities for insight or obtuseness.”

— robert bringhurst, the elements of typographic style
as humanities scholars transition from reading traditional print texts to reading on computer screens, ebooks and tablets, we are discovering that the visible word has taken a step backwards in quality. typographic considerations are often the most challenging aspects of user experience development of digital reading environments. ereaders are still in their infancy, and thus far little attention has been paid to textual design beyond very basic choices of typeface and font size.

we have developed the dynamic table of contexts, a text analysis environment that combines the traditional concepts of the table of contents and index to create new methods for humanities scholars to read and interact with digital text. the main interface consists of four interactive panes that perform dynamically with each other: the table of contents, the index, the xml tag list and a large pane to read the text itself (see figure 1). in addition, there is a pane for reader notes. the dynamic table of contexts interface is predominantly textual in nature, but the text at the centre of it is itself an interface – the point of interaction between the reader and that which is read. this study examines the appearance and arrangement of the textual interface within the larger interface. we asked: how can we typographically optimize the ereading experience?

the guiding fundamentals of typography are legibility and readability. legibility concerns the ease with which a letterform can be recognized and readability refers to the ease with which text can be understood (lupton 2004). many problems with legibility have arisen in the transition from print to screen. for example, rather than black ink on a paper page, black text on a screen is an absence in the glowing pixels that surround it, and the result is perceived as blurry regardless of screen resolution. anti-aliasing is used to compensate for low resolution on most computer monitors but can't be applied consistently from character to character because of where the letter may land on the physical pixel grid. in an effort to offset this problem font size is often increased, which in itself can become a reading irritant and navigationally awkward as larger type creates shorter lines of text which in turn requires more left-right eye movements from the end of one line to the beginning of the next. fewer lines of text also necessitate more scrolling which is visually uncomfortable and requires additional time and visual energy for the reader to relocate himself in the text after each movement. while we wait for technology to catch up in these areas, this study outlines the methods we used to minimize legibility issues and discusses which existing typefaces (most of which, it must be remembered, were never intended for the screen) best counteract blurring and movement. we identify optimal type size and line length combinations for comfortable extended on-screen reading in the dynamic table of contexts.

compared to legibility, issues of readability are less often addressed in discussions of digital reading environments. readability is comprised not only of the arrangement of type on a page or screen, but also of attention to the entire visual entity and all the complex relationships between levels of type, symbols and images (berryman 1984). typography imparts semantic meaning as it interprets content and influences meaning by creating hierarchies, assigning values, and manipulating emphasis (bachfischer, robertson and zmijewska 2007). our investigations into optimal readability design for the dynamic table of contexts raised interesting questions about the representation of text and the process of reading on-screen. in the remediation from print to screen, do we still read the same way? do the rules that apply to the readability of print typography apply to on-screen typography? do conventions that govern readability of websites also apply to extended reading of scholarly materials?

in this study we have also examined other components of the textual interface that affect readability but that are not necessarily typographic in nature. often on-screen paging representations (such as a simulated left and right sides of a double page spread or an animated page turning) give the illusion of a traditional print book. we examine theories of whether these provide valuable perceptual cues to the reader or are simply a vestigial device that has lost its significance in a changing environment.

this research represents a move from an adequate to an optimized reading environment.


figure 2 -
dynamic table of contexts

references
bachfischer, g., t. robertson, and a. zmijewska (2007). “understanding influences of the typographic quality of text.” journal of internet commerce 6(2): 97–122.
berryman, g. (1984). notes on graphic design and visual communication. los altos, california: w. kaufmann.
bringhurst, r. (2004). the elements of typographic style. 3rd ed. hartley and marks publishers.
larson, k. (2004). “the science of word recognition: or how i learned to stop worrying and love the bouma.” july. microsoft typography site. retrieved august 20, 2006. http://www.microsoft.com/typography/ctfonts/wordrecognition.aspx
lupton, e. (2010). thinking with type: a critical guide for designers, writers, editors, & students. new york. princeton architectural press.
mangen, a. (2008). “hypertext fiction reading: haptics and immersion.” journal of research in reading 31(4): 404–419.
weinzettelova, s. (2012). “traditional type in the digital era.” bulletin - prague college centre for research and interdisciplinary studies 2012(2): 5–24.
4. the tablet as a new medium for scholarly editions
mohseni, atefeh|amohseni@ualberta.ca|university of alberta
sondheim, daniel|sondheim@uvic.ca|university of victoria
frizzera, luciano|dosreisf@ualberta.ca|university of alberta
rockwell, geoffrey|grockwel@ualberta.ca|university of alberta
ruecker, stan|sruecker@id.iit.edu|iit institute of design
inke research group||
how have scholarly editions been implemented on tablets? in the past few years, a significant number of scholarly editions have seen deployment on the web, and the differences between printed editions and web-based ones has become a matter of attention for many scholars. in this paper, we will investigate scholarly editions as they appear in tablets and other mobile devices.

web-based scholarly editions offer many new features that do not and cannot exist in printed ones. such features include opportunities for collaboration with other users or with the editors of the work, innovative methods to search and browse, specialized tools and visualizations for text analysis, means to customize the layout of the interface and the content, options to change the language of the interface or to see translations of the text, high resolution zoomable images, multimedia elements such as video and audio, and a wealth of material that would be too costly or cumbersome to include in a printed edition.

despite these advantages, some users still prefer their scholarly editions to be in paper form. reasons may include a lack of physical distance between reader and book, and the ability to interact directly with the material. tablets have solved these problems to some extent, emulating the physicality of paper books, and allowing users an opportunity to touch and feel them. as elena pierazzo notes, “usability studies have demonstrated that reading on tablets is more enjoyable than reading on the screen of computers and, in some cases, more than reading print” (pierazzo 2011). additionally, the fact that an app is an independent program housed on a particular machine results in increased speed and stability over web-based editions, which also serves to reproduce some of the positive features of paper-based editions (mcdayter 2012).

tablets represent a return to printed editions in some more negative respects as well. for instance, users cannot create shared annotations or collaborate with each other in other ways; the app on the tablet is isolated from references and related material available on the web; tools for text analysis are absent; and no options are provided to show the interface in different languages. part of the reason for these omissions in tablets may be due to the fact that editions that are currently available on tablets have generally been produced more for a popular audience than a scholarly one. this is reflected not only in terms of functionalities of the interface, but with respect to the content as well. bibliographies, glossaries, and textual apparatuses are typically missing, variants tend not to be included, and notes and annotations have been recycled from older print-based editions, rather than being the result of new scholarship. tablet-based editions are often more focused on making an edition attractive and amusing than on making it scholarly, and may include games or quizzes, methods of sharing passages or images via social media sites, and other innovative but potentially distracting features likely to be of interest to keen fans of the material rather than to scholars.

so, tablets are in a way mediating between print and the web, sharing some of the advantages and disadvantages of both. but what effects will these advantages and disadvantages have on scholarly editions? do scholarly editions have a future in tablets? these are questions that we will discuss in this paper.

at this point, very few tablet-based scholarly editions have been released, and as discussed above, those that have are decidedly unscholarly in some respects. touch press has produced some notable examples, including editions of t.s eliot’s the waste land, leonardo da vinci's notebooks, and shakespeare’s sonnets. the waste land is the first edition produced by touch press, and continues to stand up as a nice example of the genre. this edition allows the poem to be viewed as plain text, and includes annotations, audio recordings, facsimile images of the original typescript, filmed performance of the poem, interviews about the poem and some image galleries.

many of the features provided in the waste land would make using scholarly editions easier and more pleasurable. as is the case with most works of philosophy, art, and literature, the waste land is multi-layered, and multimedia is of great help in exploring, finding and analyzing the different existing layers. study is also eased and enhanced by being able to quickly and easily reveal or hide information, switch between viewing multiple encodings of the text, and simply by having the ability to manually scroll through the text, rather than having to use a device.

with what has been discussed, some questions require further exploration. will the advantages of tablet-based editions win out over their disadvantages? will the tablet become accepted as an appropriate medium in which to produce and study scholarly editions? in this paper, we will consider whether scholarly editions have a future on tablets and how such a future might look.

references
eliot, t. s. (2011). the waste land for ipad, ed. justin badger and charles chabot, touch press llp, and faber and faber. http://touchpress.com/titles/thewasteland/
pierazzo, e. (2011). tablets apps, or the future of the scholarly editions? random thoughts of a digital humanist with a passion for cookery, 27 november, 2011. http://epierazzo.blogspot.ca/2011/11/tablets-apps-or-future-of-scholarly.html
mcdayter, m. (2012). are we there yet? touch press’s the waste land for ipad. february 25, 2012. http://clickherefordigitalhumanities.wordpress.com/2012/02/25/are-we-there-yet-touch-presss-the-waste-land-for-ipad/
5. designing for multi-touch surfaces as social reading environments
frizzera, luciano|dosreisf@ualberta.ca|university of alberta
vela, sarah|svela@ualberta.ca|university of alberta
ilovan, mihaela|ilovan@ualberta.ca|university of alberta
michura, piotr|pmichura@asp.krakow.pl|academy of fine arts in krakow
sondheim, daniel|sondheim@uvic.ca|university of victoria
rockwell, geoffrey|grockwel@ualberta.ca|university of alberta
ruecker, stan|sruecker@id.iit.edu|iit institute of design
inke research group||
the inke interface design group has been exploring the application of multi-touch table technology for improving the comprehension, manipulation, and analysis of variorum editions beyond what has been accomplished in previous digital variorums. these volumes consist of three general components: the text of the work itself as selected by an editor; a list of variations between this ‘base-text’ and other manuscript or printed versions; and a comprehensive anthology of previous scholarly annotations on a particular line, passage, or the entire work. for many texts the existing notes are so extensive that it is difficult in a print medium to present “…simultaneous access to text and relevant commentary”, leading to an effort since the mid-nineteen-nineties to produce digital variorum editions (werstine 2011).

scholarly editions, especially variorums, raise interesting questions about the representation of elements and the act of reading. the richness of this type of edition creates dilemmas related to the organization of different pieces of information, as well as interacting with the text. a print version presents physical space constraints, such as the two-page spread and the necessity of linear presentation. this constraint become less of an issue in a digital environment, where space, time, and dimensionality in general are more fluid.

although current iterations of digital variora (for example, the online chopin variorum edition, and the electronic variorum edition of don quixote) attempt to take advantage of this flexibility of the medium, they present themselves as flat webpages, losing physical engagement with the materiality of the book. in order to bring back fuller physicality, we have used multi-touch surface technologies to simulate the “real space”, and to return to full gestures as opposed to clicks. a few projects focusing on ebooks and tablets have begun to emerge with the same idea (for example, the wasteland, shakespeare's sonnets and kerouac's on the road), but this technology has not yet been applied to variorum editions.

our project, the comedy of errors tangible variorum, involves creating a tangible user interface representing the modern language association’s variorum edition of shakespeare’s comedy of errors in order to explore the affordances of this technology to increase collaborative scholarly research and interpretation of the material.

existing digital variorums have attempted to encourage collaboration and interaction by incorporating features into their web page interfaces. in the electronic variorum edition of don quixote (eve-dq) from texas a&m’s cervantes project, for example, users can add their own commentary and annotations which can then be accessed by future users. similarly the online chopin variorum edition (ocve) in progress at king’s college london allows the attachment of notes to the base text with an option to share them publicly. for both projects, however, adding comments is an individual activity, with a single user at a workstation inputting notes with a mouse and keyboard. there is little room for group interaction, and any that exists must be sequential rather than communal. most previous electronic new variorum shakespeare (envs) projects, particularly the recent winter’s tale version, have no digital annotation function at all, limiting them to use as a reference tool, rather than an interactive platform.

furthermore, it can be exceptionally difficult to allow simultaneous visual comparison of the numerous features of a variorum on these web page formats given the small size of most screens. a study by wästlund, norlander, and archer on the effect of page layout on mental workload shows that “manipulating an onscreen text document via scrolling necessitates a shift of focus from the text to the action of controlling the page movement,” (wästlund, norlander, and archer, 1243) leading to decreased performance on reading comprehension tasks. vandendorpe (2009) writes that “navigation by means of a mouse tends to give rise to chaotic, extremely rapid movement that is not very favorable to reading” (133).

by using a multi-touch table as a display device this project attempts to solve the problems of poor collaboration and broken comprehension. there has been significant research supporting the use of touch screens in improving reader focus, for example a study by eva siegenthaler et al. (2012) found that subjects performed better at various tasks involved in reading when using devices with tangible inputs, concluding that “a touch screen allows for an easier and more intuitive interaction” than a non-touch screen (eva siegenthaler et al. 2012, 94). the sizes of both the screen and the table perimeter of a multi-touch device, meanwhile, are conducive to multiple users working together. we thus believe that the application of multi-touch technology will have two effects. first, it represents another stage in the remediation of variorums, one that will better allow us to implement unsworth’s (2000) scholarly primitives: to sample, compare, discover, represent, annotate and reference different versions of the base text. simultaneously, it encourages these tasks to be performed socially, deepening understanding by incorporating multiple viewpoints.

the collaborative uses of tangible devices in research situations is one of the main goals of this research. in driving towards this end, however, our group has faced issues in two camps: the ability of users to adapt to multiple people concurrently using touch controls; and, less expectedly, the ability of a designer to structure elements on an unconventional screen.

from the perspective of users, much of the challenge is about breaking habits. since the introduction of personal computers, users have learned how to interact with the machine in individual work spaces. the adoption of this concept is so intense that one is liable to think that group work is less effective than work alone with the computer. however, it is noticeable that the technology has determined this situation: people cannot work together because the machine allows just one input at a time. the questions that we raise are: what happens when more than one person can interact with the machine? what kind of operations and collaborations can a group perform when all of them are engaged with the same machine in the same environment?

for designers, learning to think beyond the confines of a screen and mouse proved to be a major obstacle. the original design conceived of the mutli-touch screen as being suspended on the wall, and the elements were placed accordingly (figure 1). as we built and tested the interface it became clear that the layout would be effective for a single user, but was not ideal for the collaboration we were trying to achieve. while flipping the screen to a table was a logical choice to allow more users to work simultaneously, reassessing the design from that perspective resulted in a number of questions: where do you place touchstone elements when there is no clear top or bottom? how can items being used by different people overlap without disrupting anyone’s work?

this paper explores the problems faced when building a social reading environment, both for users and for designers. the technology to allow such interaction exists: a multi-touch table has a size that allows for the display of multiple documents side-by-side, and its status as a touch screen enables easy and intuitive operation, lessening the mental workload required to operate it and permitting users to focus on the content rather than the interface. the system accommodates familiar gestures such as touch, pinch and flick to let the user move, select, grab and scroll through information on the screen, and since more than one point of interaction is possible, multiple people are able to work at the same time in the examination of the material, improving collaborative work. designing for these features, however, is a mental challenge that requires an upset of standards in the minds of all those involved, users and builders, and facing these problems is a necessary step in moving towards the future of collaboration.


figure 1 -
initial view showing chosen copy text in a full view and a reading panel. all variants and commentaries on the chosen page are marked. more to the right there are views of all textual notes and commentaries connected to the chosen part of the copy text. at the top there is a representation of all editions in chronological order, in which the dark bars shows the overall difference of a particular edition to the chosen copy text.

references
bradley, j., and p. vetch (2006). supporting annotation as a scholarly tool–experiences from the online chopin variorum edition. literary and linguistic computing 22(2): 225–241.
siegenthaler, e., et al. (2012). the effects of touch screen technology on the usability of e-reading devices. journal of usability studies 7(3): 94–104.
furuta, r., et al. (2001). towards an electronic variorum dition of don quixote. proceedings of the 1st acm/ieee-cs joint conference on digital libraries. 444–445.
galey, a. (2005). ‘alms for oblivion’: bringing an electronic new variorum shakespeare to the screen. shakespeare association of america, bermuda http://pear.hcmc.uvic.ca:8080/ach/site/xhtml.xq?id=98
hinckley, k., et al. (1997). cooperative bimanual action. proceedings of the sigchi conference on human factors in computing systems. 27–34.
unsworth, j. (2000). scholarly primitives: what method",6.0,6.0,Voyant
1663,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,Designing a graduate DH course with DH tools and methods,,Kevin Bradley Kee;Spencer Roberts,"paper, specified ""short paper""","This paper describes an experimental approach to designing and teaching an introductory digital humanities course for graduate students. In 2011 Kevin Kee was asked to create and teach a class as part of a new interdisciplinary Humanities Ph.D. program. The graduate students taking the course would be largely unfamiliar with the digital humanities.

Kee began his preparation for the course by asking several questions. The first was ""what should an introductory digital humanities course attempt to accomplish""? As he searched for answers, another important question emerged: ""How could digital methods be used to design and deliver the course?"" In this paper, Kee and Spencer Roberts, a Research Assistant who worked with Kee, describe first their method for researching and designing the course. They then sketch the structure and content of the course that resulted from their research. Finally, they provide examples of student responses to the material and methods covered in the course (including Roberts's perspective as a graduate student). The collected responses and their reflections on the process suggest particular ways in which future courses of this kind might be designed, implemented, and improved. Most importantly, they found that an effective way to design and teach an introductory digital humanities course is to think about the discipline through discussions about its topics and to think with the discipline by using digital tools and methods in the classroom.

Overviews of digital humanities course offerings have been conducted throughout the past fifteen years. In 1999, Willard McCarty and Matthew Kirschenbaum identified only fourteen institutions that offered courses in humanities computing. In 2006, Melissa Terras conducted another survey of digital humanities curriculum, and in 2011, Lisa Spiro undertook to collect and analyze syllabi from digital humanities courses. Of these previous surveys, Spiro’s was the most comprehensive; she collected over 134 syllabi from various levels of study in the digital humanities. Although Spiro’s work parallels and was helpful to that of Kee and Roberts, the latter were unaware of her project when they began, and had no way to replicate her research method. As a result, Kee and Roberts drew on the results of their own analyses while designing the course.

For Kee and Roberts' research, Roberts designed a method by which syllabi were converted into sets of data representing reading lists, assignments, assessment methods, and digital tools used. Commonly occurring items from within those sets were highlighted and identified as items deemed important by the statistical consensus of instructors represented in their sample. For example, their results showed that seven authors of digital humanities-related articles and books appeared on reading lists at a significantly higher frequency than others; the data also showed that other instructors found these authors most useful. Kee drew on these results when deciding on readings for his course list.

Topics covered in the course included text encoding and markup, distant reading, building, mapping, modelling and simulating, playing and gaming, teaching, and collaborating. Each of these was paired with a practical application, usually drawn from a modified version of William Turkel’s ""Method"". For example, students learned the theory of text markup and were asked to create pages on the course wiki using the basic wiki standard markup. Franco Moretti’s theory of distant reading made more sense for students once they experimented with text mining and analysis tools such as Voyant. Kee’s assessment strategy required students to use blogs and Twitter to comment on the theories and tools they encountered; Kee also encouraged them to participate in scholarly discourse that occurs on the Web. Although most of the students studied history, Kee aimed to create an environment in which the digital humanities were understood as both theory and practice that could be incorporated into any humanities discipline.

Because Kee and Roberts hoped to learn from the experiences of graduate students new to the digital humanities, Kee built feedback mechanisms into the course assignments, and asked students to reflect on the course before and after completion. Nearly all of the students were challenged by the dual responsibility of learning theory and skills simultaneously. Although some students were relieved to finish experimenting, others were pleased with their progress and the opportunities for future research. One student commented, “Not only do I now have some new tools to use while I’m doing research… I’m also more open-minded towards using them in the first place and really trying to engage with them, rather than brushing them off.” While students readily adopted some of the tools, such as Zotero and Evernote, they found more complex tools such as DevonThink or Voyant required a level of commitment and time they did not want to make. In short, these students were not willing to commit to a new, digital research method at a time when they were simultaneously taking graduate courses rooted in conventional research methods. For some students, however, patience led to late or accidental discoveries that improved their methods; in at least one case, a student who was skeptical throughout the course became an enthusiastic supporter of digital methods and now avidly attends DH conferences and events. At the conclusion of the course, most students were open to the various theories and approaches used in the digital humanities, and were enthusiastic about trying new tools and experimenting with new methods that might improve their research and scholarship.

From the outset of the project, Kee and Roberts understood that they were asking questions for which there were several feasible answers. Some graduate level digital humanities courses focus on topics within the digital humanities; others primarily train students to develop digital skills using computational tools. Kee's approach was to combine these two approaches into one course that provided opportunity for theoretical discussion while also showcasing practical applications, so that students could see the potential beneDits of digital humanities methods without having to master sophisticated tools. The research method used to build the course syllabus employed the same theories and tools that were later discussed in the course, creating an iterative loop through which student feedback and developments in the discipline can be incorporated into future versions. Already there are new tools to improve the collection and analysis of digital humanities syllabi, and new methods being explored by instructors. Through the experimental approach described in this paper, Kee and Roberts have found that thinking about and thinking with the discipline, a method that many digital scholars employ in their research, is also an effective way to design a course, and appeals to students who are new to the discipline, fostering enthusiasm for its use in their own often conventional humanities scholarship. The authors hope that this approach contributes to the growing conversation about teaching digital humanities, while also reflecting and adapting to the dynamic topics within the field.",txt,This text is republished here with permission from the original rights holder.,,graduate course;method;pedagogy,English,"audio, video, multimedia;content analysis;digital humanities - pedagogy and curriculum;teaching and pedagogy",2013-01-01,"this paper describes an experimental approach to designing and teaching an introductory digital humanities course for graduate students. in 2011 kevin kee was asked to create and teach a class as part of a new interdisciplinary humanities ph.d. program. the graduate students taking the course would be largely unfamiliar with the digital humanities.

kee began his preparation for the course by asking several questions. the first was ""what should an introductory digital humanities course attempt to accomplish""? as he searched for answers, another important question emerged: ""how could digital methods be used to design and deliver the course?"" in this paper, kee and spencer roberts, a research assistant who worked with kee, describe first their method for researching and designing the course. they then sketch the structure and content of the course that resulted from their research. finally, they provide examples of student responses to the material and methods covered in the course (including roberts's perspective as a graduate student). the collected responses and their reflections on the process suggest particular ways in which future courses of this kind might be designed, implemented, and improved. most importantly, they found that an effective way to design and teach an introductory digital humanities course is to think about the discipline through discussions about its topics and to think with the discipline by using digital tools and methods in the classroom.

overviews of digital humanities course offerings have been conducted throughout the past fifteen years. in 1999, willard mccarty and matthew kirschenbaum identified only fourteen institutions that offered courses in humanities computing. in 2006, melissa terras conducted another survey of digital humanities curriculum, and in 2011, lisa spiro undertook to collect and analyze syllabi from digital humanities courses. of these previous surveys, spiro’s was the most comprehensive; she collected over 134 syllabi from various levels of study in the digital humanities. although spiro’s work parallels and was helpful to that of kee and roberts, the latter were unaware of her project when they began, and had no way to replicate her research method. as a result, kee and roberts drew on the results of their own analyses while designing the course.

for kee and roberts' research, roberts designed a method by which syllabi were converted into sets of data representing reading lists, assignments, assessment methods, and digital tools used. commonly occurring items from within those sets were highlighted and identified as items deemed important by the statistical consensus of instructors represented in their sample. for example, their results showed that seven authors of digital humanities-related articles and books appeared on reading lists at a significantly higher frequency than others; the data also showed that other instructors found these authors most useful. kee drew on these results when deciding on readings for his course list.

topics covered in the course included text encoding and markup, distant reading, building, mapping, modelling and simulating, playing and gaming, teaching, and collaborating. each of these was paired with a practical application, usually drawn from a modified version of william turkel’s ""method"". for example, students learned the theory of text markup and were asked to create pages on the course wiki using the basic wiki standard markup. franco moretti’s theory of distant reading made more sense for students once they experimented with text mining and analysis tools such as voyant. kee’s assessment strategy required students to use blogs and twitter to comment on the theories and tools they encountered; kee also encouraged them to participate in scholarly discourse that occurs on the web. although most of the students studied history, kee aimed to create an environment in which the digital humanities were understood as both theory and practice that could be incorporated into any humanities discipline.

because kee and roberts hoped to learn from the experiences of graduate students new to the digital humanities, kee built feedback mechanisms into the course assignments, and asked students to reflect on the course before and after completion. nearly all of the students were challenged by the dual responsibility of learning theory and skills simultaneously. although some students were relieved to finish experimenting, others were pleased with their progress and the opportunities for future research. one student commented, “not only do i now have some new tools to use while i’m doing research… i’m also more open-minded towards using them in the first place and really trying to engage with them, rather than brushing them off.” while students readily adopted some of the tools, such as zotero and evernote, they found more complex tools such as devonthink or voyant required a level of commitment and time they did not want to make. in short, these students were not willing to commit to a new, digital research method at a time when they were simultaneously taking graduate courses rooted in conventional research methods. for some students, however, patience led to late or accidental discoveries that improved their methods; in at least one case, a student who was skeptical throughout the course became an enthusiastic supporter of digital methods and now avidly attends dh conferences and events. at the conclusion of the course, most students were open to the various theories and approaches used in the digital humanities, and were enthusiastic about trying new tools and experimenting with new methods that might improve their research and scholarship.

from the outset of the project, kee and roberts understood that they were asking questions for which there were several feasible answers. some graduate level digital humanities courses focus on topics within the digital humanities; others primarily train students to develop digital skills using computational tools. kee's approach was to combine these two approaches into one course that provided opportunity for theoretical discussion while also showcasing practical applications, so that students could see the potential benedits of digital humanities methods without having to master sophisticated tools. the research method used to build the course syllabus employed the same theories and tools that were later discussed in the course, creating an iterative loop through which student feedback and developments in the discipline can be incorporated into future versions. already there are new tools to improve the collection and analysis of digital humanities syllabi, and new methods being explored by instructors. through the experimental approach described in this paper, kee and roberts have found that thinking about and thinking with the discipline, a method that many digital scholars employ in their research, is also an effective way to design a course, and appeals to students who are new to the discipline, fostering enthusiasm for its use in their own often conventional humanities scholarship. the authors hope that this approach contributes to the growing conversation about teaching digital humanities, while also reflecting and adapting to the dynamic topics within the field.",2.0,2.0,Voyant
1723,2013 - Nebraska,Nebraska,Freedom to Explore,2013,ADHO,ADHO,University of Nebraska–Lincoln,Lincoln,Nebraska,United States,http://dh2013.unl.edu/,Lexomics: Integrating the research and teaching spaces,,Mark D. LeBlanc;Michael Drout;Michael Kahn;Alicia Herbert;Richard Neal,"paper, specified ""short paper""","Integrating research and teaching is exciting, time intensive, and a prescription for energizing faculty and students. We present outcomes of a six-year effort in multidisciplinary collaboration centered on the digital humanities as experienced in our teaching and research. Rooted in a set of “connected” courses between English and Computer Science (LeBlanc, et al. 2010) and three summers of NEH-funded research, our Lexomics Research Group has developed a modest set of web-based applications for scholars of digitized texts. We report here on the iterative development of the open-source toolset, how scholars both in and outside our group have used these tools to make significant discoveries, and perhaps most important how our research and teaching collaborations introduce a spirit of experimentation to the digital humanities.

Our current website is both a repository for our tool set as well as an evangelistic platform and teaching resource: http://lexomics.wheatoncollege.edu. We continue to develop online tools for three independent, but logically connected functions that lead scholars through the steps needed for performing hierarchical cluster analyses of texts and/or sections of texts. At this point, our cluster analysis tools are more narrowly focused than other toolsets, c.f. Voyant Tools and the data-intensive flow execution environment of Meandre. Our scrubber tool (PHP, CSS) accepts texts in multiple formats (.txt, .html, .docx) and handles preprocessing steps including stripping tags, removing stop words, and applying lemma lists. A second tool, diviText (ExtJS, PHP), accepts the output from scrubber, cuts texts into “chunks” in one of three ways (fixed size chunks, a specific number of chunks, and/or by manually selecting locations between words for chunk breaks), computes word counts within each chunk, and allows users to merge chunks. The latter functionality has proved valuable for generating “virtual manuscripts”, that is, joining sections from different manuscripts. A third tool, treeView (PHP, R) accepts output from diviText, performs a number of variants of hierarchical cluster analysis, and returns a dendrogram plot in .pdf or phyloXML format.

Based on feedback from scholars who are using our tools, the website now provides video and written tutorials to help new users get started. These tutorials have been especially valuable for introducing these tools to our undergraduates. In the spirit of evangelizing, our website offers a series of “best practices” videos, discussions and step-by-step diagrams that shred insight to the process of how textual analysis at this level of detail can lead to rich new questions. The instructional videos include “The Story of Daniel”, a discussion of one of our initial successes when using the tools where we showed that lexomic methods can accurately characterize the structure and relationships of texts that are already known, for example, identifying Genesis B within the Old English Genesis and the section of Daniel that is paralleled in Azarias (Drout, et al. 2011). Other videos include: “How to Read a Dendrogram”, “How to Create a Dendrogram”, “How to Read a Ribbon Diagram”, “Lexomics for Comparison”, and “Lexomics for Source Detection”. A much longer video, “Editions and Manuscripts,” addresses the challenges of choosing between different kinds of editions that may exist for a text that is found in multiple forms.

We have made what we think are significant discoveries in a number of spaces, including Beowulf, the poems of Cynewulf, Anglo-Saxon prose, a few Old Norse sagas, and Modern English texts including the Harlem Renaissance play Mule Bone (by Zora Neale Hurston and Langston Hughes). Lexomics is both an excellent first step to augment traditional scholarship as well as a rich source of deep analysis.

For example, previous lexomic analysis of several Old English poems suggests that there is a connection between dendrograms with an isolated, single leaf and poems that have an external source for one subsection of the poem different from the source or sources of the main body of the poem. We ﬁnd in the dendrogram of Daniel a single-leaf clade corresponding to lines 299–455 of the poem. This section includes parts of Daniel that have external Latin sources that are different from the source of the rest of the poem (the Latin Bible). Similarly, in the Anglo-Saxon poem Christ III, a single-leaf clade that represents lines 1350–1510 has its source in Sermon 57 of Cæsarius of Arles (lines 1379–1498), and a single-leaf clade in Genesis A (lines 1079–1256) is associated with the genealogical lists from Adam to Noah that give the lineages of both Cain and Seth (lines 1055–1252), material that, for at least some of its content, must have a source different from the biblical text. These relationships were already known to scholars, but our investigation of the Old English poem Guthlac A resolved a century-long critical controversy by demonstrating that a key section of this poem (when demons drag Guthlac to the mouth of hell) has a different proximate source than the rest of the poem and that Guthlac A therefore must have been composed after a separately circulating text similar in content to Vercelli Homily 23 (Downey et al., 2012).

The toolset, instructional materials, and publications are obvious deliverables from our efforts. Yet, we submit that our collaborative experiences with faculty and undergraduate students are even more exciting and provide a significant use-case of how scholarship in the humanities is evolving from the stereotypical solitary scholar to a paradigm of community, collaboration, and experimentation (cf. Unsworth, 1997). In our recent NEH- and locally-funded summer experience, humanities faculty in particular were pleasantly surprised with the intellectual environment that emerged. We got a glimpse of what it must have been like to work at a place like Bell Labs when they were making daily discoveries. This kind of collaborative, fast-moving research is unfortunately largely unknown in the humanities.

So how to continue our own momentum as well as replicate a spirit of experimentation for others? Earhart (2010) rightly notes that “digital projects remain rare, often the product of tenacious participants rather than a supportive academic environment”(emphasis added). We submit that faculty (not administrators nor technologists in the library) are the prime drivers and change must begin with our syllabi. Robust working relationships in the lab are strongest after students have already applied new modes of thinking in the classroom; for example, the importance of exposing undergraduate humanities students to computational thinking: problem decomposition, algorithmic thinking, and the success and failures of experimentation. And we need not overplay the lab metaphor. Our image of the digital humanities lab need not include beakers and soapstone benches, rather, the “new lab” is a room filled with scholars from multiple disciplines and a whiteboard.

Even if we had discovered nothing during our past summers in the lab, the intellectual thrill of the research group would have been a major accomplishment that these students (and we faculty) will never forget. But in fact we made discoveries, so many that there were days when participating faculty got none of their own work done because we were so busy bouncing from student to student seeing what they had found. Most critically, the experience continues to shape the way we share our disciplines with new cohorts of students. The solitary scholar still has a role to be sure, but that is no longer sufficient for the multidisciplinary demands and rewards to be gained from collaborations in the digital humanities: in our teaching, to our research, and back again.

lexomics — The term was originally coined by Betsey Dexter Dyer and ﬁrst appeared in Genome Technology (2002). Since then ‘‘lexomics’’ has appeared on the internet and in some publications without attribution. Some of these appearances could be independent inventions of the term.

References
Downey, S., M. Drout, M. Kahn, and M. D. LeBlanc (2012). 'Books Tell Us': Lexomic and Traditional Evidence for the Sources of Guthlac A. Modern Philology 110: 1-29.
Drout, M., M. Kahn, M. D. LeBlanc, and C. Nelson (2011). Of Dendrogrammatology: Lexomic Methods for Analyzing Relationships among Old English Poems, Journal of English and Germanic Philology 110: 301–36.
Drout, M., M. D. LeBlanc, and M. Kahn (2011-2013). “Lexomic Tools and Methods for Textual Analysis: Providing Deep Access to Digitized Texts.” National Endowment for the Humanities–NEH PR-50112011.
Earhart, A. (2010). Challenging Gaps: Redesigning Collaboration in the Digital Humanities. In Earhart and Jewell (eds),The American Literature Scholar in the Digital Age Ann Arbor: University of Michigan Press. http://hdl.handle.net/2027/spo.9362034.0001.001.
Genome Technology (2002). ""In the News."" in Genome Technology 1(27), November 1, 2002.
LeBlanc, M. D., M. Gousie, and T. Armstrong (2010). Connecting Across Campus. 'Proceedings of the 41st SIGCSE Technical Symposium on Computer Science Education' held in Milwaukee, WI.
LeBlanc, M. D., M. Drout, and M. Kahn (2008-2010). “Pattern Recognition through Computational Stylistics: Old English and Beyond.” National Endowment for the Humanities–NEH HD-50300-08.
Lexomics Research Group. http://lexomics.wheatoncollege.edu
Meandre http://seasr.org/meandre/
Unsworth, J. (1997). Documenting the Reinvention of Text: The Importance of Failure. Journal of Electronic Publishing, 3:2. http://dx.doi.org/10.3998/3336451.0003.201.
Voyant Tools. http://voyant-tools.org/",txt,This text is republished here with permission from the original rights holder.,,clustering;dendrograms;interdisciplinary;lexomics;teaching,English,data mining / text mining;digital humanities - pedagogy and curriculum;interdisciplinary collaboration;medieval studies;teaching and pedagogy,2013-01-01,"integrating research and teaching is exciting, time intensive, and a prescription for energizing faculty and students. we present outcomes of a six-year effort in multidisciplinary collaboration centered on the digital humanities as experienced in our teaching and research. rooted in a set of “connected” courses between english and computer science (leblanc, et al. 2010) and three summers of neh-funded research, our lexomics research group has developed a modest set of web-based applications for scholars of digitized texts. we report here on the iterative development of the open-source toolset, how scholars both in and outside our group have used these tools to make significant discoveries, and perhaps most important how our research and teaching collaborations introduce a spirit of experimentation to the digital humanities.

our current website is both a repository for our tool set as well as an evangelistic platform and teaching resource: http://lexomics.wheatoncollege.edu. we continue to develop online tools for three independent, but logically connected functions that lead scholars through the steps needed for performing hierarchical cluster analyses of texts and/or sections of texts. at this point, our cluster analysis tools are more narrowly focused than other toolsets, c.f. voyant tools and the data-intensive flow execution environment of meandre. our scrubber tool (php, css) accepts texts in multiple formats (.txt, .html, .docx) and handles preprocessing steps including stripping tags, removing stop words, and applying lemma lists. a second tool, divitext (extjs, php), accepts the output from scrubber, cuts texts into “chunks” in one of three ways (fixed size chunks, a specific number of chunks, and/or by manually selecting locations between words for chunk breaks), computes word counts within each chunk, and allows users to merge chunks. the latter functionality has proved valuable for generating “virtual manuscripts”, that is, joining sections from different manuscripts. a third tool, treeview (php, r) accepts output from divitext, performs a number of variants of hierarchical cluster analysis, and returns a dendrogram plot in .pdf or phyloxml format.

based on feedback from scholars who are using our tools, the website now provides video and written tutorials to help new users get started. these tutorials have been especially valuable for introducing these tools to our undergraduates. in the spirit of evangelizing, our website offers a series of “best practices” videos, discussions and step-by-step diagrams that shred insight to the process of how textual analysis at this level of detail can lead to rich new questions. the instructional videos include “the story of daniel”, a discussion of one of our initial successes when using the tools where we showed that lexomic methods can accurately characterize the structure and relationships of texts that are already known, for example, identifying genesis b within the old english genesis and the section of daniel that is paralleled in azarias (drout, et al. 2011). other videos include: “how to read a dendrogram”, “how to create a dendrogram”, “how to read a ribbon diagram”, “lexomics for comparison”, and “lexomics for source detection”. a much longer video, “editions and manuscripts,” addresses the challenges of choosing between different kinds of editions that may exist for a text that is found in multiple forms.

we have made what we think are significant discoveries in a number of spaces, including beowulf, the poems of cynewulf, anglo-saxon prose, a few old norse sagas, and modern english texts including the harlem renaissance play mule bone (by zora neale hurston and langston hughes). lexomics is both an excellent first step to augment traditional scholarship as well as a rich source of deep analysis.

for example, previous lexomic analysis of several old english poems suggests that there is a connection between dendrograms with an isolated, single leaf and poems that have an external source for one subsection of the poem different from the source or sources of the main body of the poem. we ﬁnd in the dendrogram of daniel a single-leaf clade corresponding to lines 299–455 of the poem. this section includes parts of daniel that have external latin sources that are different from the source of the rest of the poem (the latin bible). similarly, in the anglo-saxon poem christ iii, a single-leaf clade that represents lines 1350–1510 has its source in sermon 57 of cæsarius of arles (lines 1379–1498), and a single-leaf clade in genesis a (lines 1079–1256) is associated with the genealogical lists from adam to noah that give the lineages of both cain and seth (lines 1055–1252), material that, for at least some of its content, must have a source different from the biblical text. these relationships were already known to scholars, but our investigation of the old english poem guthlac a resolved a century-long critical controversy by demonstrating that a key section of this poem (when demons drag guthlac to the mouth of hell) has a different proximate source than the rest of the poem and that guthlac a therefore must have been composed after a separately circulating text similar in content to vercelli homily 23 (downey et al., 2012).

the toolset, instructional materials, and publications are obvious deliverables from our efforts. yet, we submit that our collaborative experiences with faculty and undergraduate students are even more exciting and provide a significant use-case of how scholarship in the humanities is evolving from the stereotypical solitary scholar to a paradigm of community, collaboration, and experimentation (cf. unsworth, 1997). in our recent neh- and locally-funded summer experience, humanities faculty in particular were pleasantly surprised with the intellectual environment that emerged. we got a glimpse of what it must have been like to work at a place like bell labs when they were making daily discoveries. this kind of collaborative, fast-moving research is unfortunately largely unknown in the humanities.

so how to continue our own momentum as well as replicate a spirit of experimentation for others? earhart (2010) rightly notes that “digital projects remain rare, often the product of tenacious participants rather than a supportive academic environment”(emphasis added). we submit that faculty (not administrators nor technologists in the library) are the prime drivers and change must begin with our syllabi. robust working relationships in the lab are strongest after students have already applied new modes of thinking in the classroom; for example, the importance of exposing undergraduate humanities students to computational thinking: problem decomposition, algorithmic thinking, and the success and failures of experimentation. and we need not overplay the lab metaphor. our image of the digital humanities lab need not include beakers and soapstone benches, rather, the “new lab” is a room filled with scholars from multiple disciplines and a whiteboard.

even if we had discovered nothing during our past summers in the lab, the intellectual thrill of the research group would have been a major accomplishment that these students (and we faculty) will never forget. but in fact we made discoveries, so many that there were days when participating faculty got none of their own work done because we were so busy bouncing from student to student seeing what they had found. most critically, the experience continues to shape the way we share our disciplines with new cohorts of students. the solitary scholar still has a role to be sure, but that is no longer sufficient for the multidisciplinary demands and rewards to be gained from collaborations in the digital humanities: in our teaching, to our research, and back again.

lexomics — the term was originally coined by betsey dexter dyer and ﬁrst appeared in genome technology (2002). since then ‘‘lexomics’’ has appeared on the internet and in some publications without attribution. some of these appearances could be independent inventions of the term.

references
downey, s., m. drout, m. kahn, and m. d. leblanc (2012). 'books tell us': lexomic and traditional evidence for the sources of guthlac a. modern philology 110: 1-29.
drout, m., m. kahn, m. d. leblanc, and c. nelson (2011). of dendrogrammatology: lexomic methods for analyzing relationships among old english poems, journal of english and germanic philology 110: 301–36.
drout, m., m. d. leblanc, and m. kahn (2011-2013). “lexomic tools and methods for textual analysis: providing deep access to digitized texts.” national endowment for the humanities–neh pr-50112011.
earhart, a. (2010). challenging gaps: redesigning collaboration in the digital humanities. in earhart and jewell (eds),the american literature scholar in the digital age ann arbor: university of michigan press. http://hdl.handle.net/2027/spo.9362034.0001.001.
genome technology (2002). ""in the news."" in genome technology 1(27), november 1, 2002.
leblanc, m. d., m. gousie, and t. armstrong (2010). connecting across campus. 'proceedings of the 41st sigcse technical symposium on computer science education' held in milwaukee, wi.
leblanc, m. d., m. drout, and m. kahn (2008-2010). “pattern recognition through computational stylistics: old english and beyond.” national endowment for the humanities–neh hd-50300-08.
lexomics research group. http://lexomics.wheatoncollege.edu
meandre http://seasr.org/meandre/
unsworth, j. (1997). documenting the reinvention of text: the importance of failure. journal of electronic publishing, 3:2. http://dx.doi.org/10.3998/3336451.0003.201.
voyant tools. http://voyant-tools.org/",2.0,3.0,Voyant
1870,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Cirilo Client: An application for data curation and content preservation,,Elisabeth Steiner,poster / demo / art installation,"GAMS: A Fedora Commons instance

Since 2003 the Centre for Information Modeling - Austrian Centre for Digital Humanities at the University of Graz (Austria) provides an infrastructure for a variety of DH projects. After years of building insular solutions, the Centre introduced a powerful yet flexible new infrastructure, called GAMS (Geisteswissenschaftliches Asset Management System, AMS for the Humanities). It is based on the Fedora Commons architecture. Thus, the infrastructure inherits all features already provided by Fedora: full OAIS-compliance, strict separation of data and metadata, and predefined interfaces like OAI-PMH. A central advantage of the Fedora architecture is its object model: An asset consists of a primary source, some metadata and virtual representations derived from the primary source. The object is completely self-descriptive: It knows about all changes that have been made to it, its version history, datastreams and assigned context objects. Finally, it also knows about all possible representation forms. Each object contains all the necessary information to store, preserve, retrieve and view it.
Cirilo Client: Mass operations in Fedora made easy

Although Fedora is a powerful tool, front-end object management is not always easy, especially with regard to mass operations. The Centre has developed a tool for this use case, complementing Fedora’s built-in Admin Client. Cirilo is a java application developed for data curation and content preservation in Fedora-based repository systems. Content preservation and data curation in our sense include object management and creation, versioning, normalization and standards, and choice of data formats.
Cirilo makes use of Fedora’s management-API (API-M). It offers applications which are particularly prone to being used as tools for mass operations on Fedora repository objects, such as ingest or replacement processes: With Cirilo ingest processes can be performed from the file system, from an eXist database or an Excel spreadsheet. During the ingest metadata is automatically extracted from the source document and written to the newly created object (for instance in DC format).
  The client operates on a collection of predefined content models which can be used without further adjustments for standard workflow scenarios like the management of collections of TEI objects. The content models, which are based on the Fedora object model, are class definitions: On the one hand they define the (MIME-)type of the contained data streams, on the other hand they designate dissemination methods operating on these data streams. Every object in the repository is an instance of one of these class definitions. The advantage of this concept lies in the fact that very complex data sources and workflows can be handled easily.
Currently, the client offers various content models for specific purposes, special emphasis lies on the TEI model. The TEI ingest processes can be flexibly costumized: during ingest policies for the extraction of semantic information can be applied, referenced images can be uploaded simultaneously and ontology concepts can be resolved. A new content model currently in development creates the appropriate ontology objects, especially SKOS objects. A designated query object makes it possible to pose queries with parameters to the Mulgara triplestore. With the help of these ontology and query objects dynamic indices can be created. There is a container object for the creation of collections available, which makes it easy to organize your resources. Finally, there are some models optimized for specified primary sources like METS/MODS, HTML, PDF, BibTeX or external resources accessible via an URL. A content model for linguistic resources is in development (in cooperation with ICLTT, Vienna). Currently, we are testing how controlled vocabularies and thesauri (for instance geonames.org), can be sensibly integrated in the system.
The user can assign numerous virtual representations via the client. The METS/MODS object is designed to be viewed in the DFG-Viewer. TEI objects can be directly used as the input for the Voyant Tools or the Versioning Machine. The members of a context object can be projected on a map using Google Maps. Basically any web-based service can be integrated into the infrastructure. Of course, user- and project-specific stylesheets are often employed.
The Cirilo Client will be made available as an open source software project, including documentation, as a contribution of the Centre for Information Modeling - Austrian Centre for Digital Humanities to DARIAH-AT in 2014.
References

DARIAH-EU: www.dariah.eu [2013-10-28]
DFG-Viewer: dfg-viewer.de/ueber-das-projekt [2013-10-28]
Fedora Commons: www.fedora-commons.org [2013-10-28]
Google Maps: maps.google.at [2013-10-28]
Geisteswissenschaftliches Asset Management System, AMS for the Humanities: gams.uni-graz.at [2013-10-28]
Carl Lagoze, Sandy Payette, Edwin Shin, Chris Wilper, Fedora (2005). An Architecture for Complex Objects and their Relationships. arxiv.org/ftp/cs/papers/0501/0501012.pdf [2013-10-28]
Versioning Machine: v-machine.org [2013-10-28]
Voyant Tools: voyant-tools.org [2013-10-28]
",txt,This text is republished here with permission from the original rights holder.,,infrastructure;preservation,English,"archives, repositories, sustainability and preservation;cultural infrastructure;software design and development",2014-01-01,"gams: a fedora commons instance

since 2003 the centre for information modeling - austrian centre for digital humanities at the university of graz (austria) provides an infrastructure for a variety of dh projects. after years of building insular solutions, the centre introduced a powerful yet flexible new infrastructure, called gams (geisteswissenschaftliches asset management system, ams for the humanities). it is based on the fedora commons architecture. thus, the infrastructure inherits all features already provided by fedora: full oais-compliance, strict separation of data and metadata, and predefined interfaces like oai-pmh. a central advantage of the fedora architecture is its object model: an asset consists of a primary source, some metadata and virtual representations derived from the primary source. the object is completely self-descriptive: it knows about all changes that have been made to it, its version history, datastreams and assigned context objects. finally, it also knows about all possible representation forms. each object contains all the necessary information to store, preserve, retrieve and view it.
cirilo client: mass operations in fedora made easy

although fedora is a powerful tool, front-end object management is not always easy, especially with regard to mass operations. the centre has developed a tool for this use case, complementing fedora’s built-in admin client. cirilo is a java application developed for data curation and content preservation in fedora-based repository systems. content preservation and data curation in our sense include object management and creation, versioning, normalization and standards, and choice of data formats.
cirilo makes use of fedora’s management-api (api-m). it offers applications which are particularly prone to being used as tools for mass operations on fedora repository objects, such as ingest or replacement processes: with cirilo ingest processes can be performed from the file system, from an exist database or an excel spreadsheet. during the ingest metadata is automatically extracted from the source document and written to the newly created object (for instance in dc format).
  the client operates on a collection of predefined content models which can be used without further adjustments for standard workflow scenarios like the management of collections of tei objects. the content models, which are based on the fedora object model, are class definitions: on the one hand they define the (mime-)type of the contained data streams, on the other hand they designate dissemination methods operating on these data streams. every object in the repository is an instance of one of these class definitions. the advantage of this concept lies in the fact that very complex data sources and workflows can be handled easily.
currently, the client offers various content models for specific purposes, special emphasis lies on the tei model. the tei ingest processes can be flexibly costumized: during ingest policies for the extraction of semantic information can be applied, referenced images can be uploaded simultaneously and ontology concepts can be resolved. a new content model currently in development creates the appropriate ontology objects, especially skos objects. a designated query object makes it possible to pose queries with parameters to the mulgara triplestore. with the help of these ontology and query objects dynamic indices can be created. there is a container object for the creation of collections available, which makes it easy to organize your resources. finally, there are some models optimized for specified primary sources like mets/mods, html, pdf, bibtex or external resources accessible via an url. a content model for linguistic resources is in development (in cooperation with icltt, vienna). currently, we are testing how controlled vocabularies and thesauri (for instance geonames.org), can be sensibly integrated in the system.
the user can assign numerous virtual representations via the client. the mets/mods object is designed to be viewed in the dfg-viewer. tei objects can be directly used as the input for the voyant tools or the versioning machine. the members of a context object can be projected on a map using google maps. basically any web-based service can be integrated into the infrastructure. of course, user- and project-specific stylesheets are often employed.
the cirilo client will be made available as an open source software project, including documentation, as a contribution of the centre for information modeling - austrian centre for digital humanities to dariah-at in 2014.
references

dariah-eu: www.dariah.eu [2013-10-28]
dfg-viewer: dfg-viewer.de/ueber-das-projekt [2013-10-28]
fedora commons: www.fedora-commons.org [2013-10-28]
google maps: maps.google.at [2013-10-28]
geisteswissenschaftliches asset management system, ams for the humanities: gams.uni-graz.at [2013-10-28]
carl lagoze, sandy payette, edwin shin, chris wilper, fedora (2005). an architecture for complex objects and their relationships. arxiv.org/ftp/cs/papers/0501/0501012.pdf [2013-10-28]
versioning machine: v-machine.org [2013-10-28]
voyant tools: voyant-tools.org [2013-10-28]
",2.0,3.0,Voyant
1877,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Combining Macro- and Microanalysis for Exploring the Construal of Scientific Disciplinarity,,Peter Fankhauser;Hannah Kermes;Elke Teich,poster / demo / art installation,"1. Introduction

The English Scientific Text Corpus (SciTex) consists of about 5000 scientific papers with about 34 Mio tokens in two time slots, 1970/80s and 2000s 1, 2. It has been compiled to investigate the construal of scientific disciplinarity, in particular, how interdisciplinary contact disciplines emerge from their seed disciplines. Both time slots consist of nine disciplines: Computer Science (A) as one seed discipline, Linguistics (C1), Biology (C2), Mechanical Engineering (C3), Electrical Engineering (C4) as the other seed disciplines, and Computational Linguistics (B1), Bioinformatics (B2), Digital Construction (B3), and Microelectronics (B4) as the corresponding contact disciplines between A and C1-C4. The individual articles are subdivided into Abstract, Introduction, Main, and Conclusion.
The orthogonal dimensions time, discipline, and logical structure provide for many, potentially interesting setups of variational analysis: We can explore the diachronic evolution of contact disciplines in comparison to their seed disciplines, variation between contact disciplines and their seed disciplines, and genre variation between abstracts and text bodies in individual disciplines. In this paper we present an approach that combines a macroanalytic perspective 3 with the more traditional microanalytic perspective served by concordance search to explore variation along these dimensions.
2. Approach

2.1. Macroanalysis

For supporting explorative macroanalysis, we use well understood visualization techniques – heatmaps and wordclouds – and combine them with intuitive exploration paradigms – drill down and side by side comparison (see Figure 1). The heatmaps and wordclouds are interactive, allowing for a closer inspection at various levels. The leftmost heatmap visualizes the highest level contrast between abstracts and text bodies in the two time slots (1970s/80s and 2000s). The middle and right heatmaps serve for inspecting a chosen contrast at a lower level at the level of individual disciplines. A particular contrast can be chosen by clicking on the respective square, numbers indicating which contrast is displayed in the middle (Selection 1) and right heatmap (Selection 2). In this example, the middle heatmap visualizes the distances between abstracts and text bodies, and the right heatmap visualizes the distances between text bodies and abstracts.
The wordclouds underneath the heatmaps display the most typical words for a chosen contrast. In Figure 1 the wordcloud to the left visualizes the most typical words for abstracts as opposed to text bodies in the 2000s. Unlike in the common use of wordclouds, the size of words is proportional to their contribution to the distance (as defined in Section 2.2), whereas relative frequency is visualized by color, ranging from purple to red.

Fig. 1: Contrast between Abstracts and Text Bodies
Having a closer look at Figure 1, we can observe that the distance between abstracts is generally larger than the distance between text bodies, and that it has increased in the 30 years period. This general trend is mirrored in the individual disciplines (not shown here). Looking at the middle and right heatmaps, we can see that - not surprisingly - the distance between particular disciplines are at a minimum (squares forming the main diagonal), and the distances among the seed disciplines (A and C corpora), are generally larger than the distances among contract disciplines.
The corresponding wordclouds visualize the most typical words for abstracts (middle heatmap) and for text bodies (right heatmap) in the discipline B1 (Computational Linguistics). In this particular contrast, words typical for abstracts are clearly centered around constructions of exposition (we propose, describe, investigate), main topics of B1 (natural, language (generation), machine translation), words describing the methodology (method, statistical, computational, system) and function words (and, of, on). Words typical for text bodies are markedly different: they comprise B1's main entities of topic elaboration (tokens, nouns, object, vp, john, probability), references (see figure, table, section), conjunctions (when, since, because, if), auxiliary and modal verbs (be, is, was, were, do, will, would, may), and prominently, the determiner the. In summary, abstracts exhibit characteristics of an informationally dense text (e.g., omission of determiners) with topic oriented content. In contrast, text bodies are less dense (determiners, references, modality) and more elaborated.
Other contrastive pairs, such as the synchronic comparison between disciplines or the diachronic comparison of the two time slots, corroborate the results derived by means of computationally much more demanding techniques from machine learning [1], [2].
2.2. Corpus Representation and Distance Measures

The individual corpora are tokenized, and tokens are transformed to lower case. Stopwords are deliberately not excluded to inspect all levels of variation: style, lexico-grammar, and theme. On this basis, corpora are represented by means of unigram language models smoothed with Jelinek-Mercer smoothing, which is a linear interpolation between the relative frequency of a word in a subcorpus and its relative frequency in the entire corpus 4. The distance between two corpora P and Q is measured by relative entropy D, also known as Kullback-Leibler Divergence:
D(P||Q)>=Sum_w p(w)*log_2(p(w)/q(w))
Here p(w) is the probability of a word w in P, and q(w) is its probability in Q. Relative entropy thus measures the average amount of additional bits per word needed to encode words distributed according to P by using an encoding optimized for Q. Note that this measure is asymmetric, i.e., D(P||Q) != D(Q||P), and has its minimum at 0 for P = Q5.
The individual word weights are calculated by the pointwise Kullback-Leibler Divergence 6:
D_w(P||Q) = p(w)*log_2(p(w)/q(w))
For all words the statistical significance of a difference is calculated based on an unpaired Welch t-test on the observed word probabilities in the individual documents of a corpus. This is used for discarding words below a given level of significance (p-value). A more detailed comparison with other measures for comparing corpora 7 is beyond the scope of this paper and will appear in another venue.
2.3. Microanalysis

Wordclouds serve as a bridge between the big distance picture of macroanalysis and microanalyis. To this end, they are seamlessly integrated with the IMS Open Corpus Workbench (CQPWeb: http://cwb.sourceforge.net/index.php), which provides for an expressive corpus query language and several summarization tools, such as collocations and comparative word frequency lists. A click on a word sends a query to CQPWeb, which returns the word in the chosen context. For example, clicking on “do” in the right heatmap (B1 (Txt00) vs. B1 (Abs00)) generates the following query shown in Figure 2.

Fig. 2: Concordance for “do” in B1, text bodies, 2000s
This query returns a concordance for “do” in the 2000s slot of SciTex constrained to subcorpus B1 and to the divisions Introduction, Main, and Conclusion. Based on this list one can inspect the larger context of individual hits and get a ranked list of collocations to distinguish the uses of “do” as an auxiliary vs. main verb.
3. Related Work

The need for combining macroanalysis with microanalysis is well recognized in the DH community 8, 9, and there does exist a variety of frameworks with similar goals. Due to space restrictions, we can only give an exemplary selection; for a comprehensive overview see TAPoR 2.0 (http://tapor.ca/). The MONK workbench 10 allows to compare pairs of corpora using Dunning's log-likelihood ratio 11 for word weighting. Apart from the different distance measure, the main difference of our approach is that we combine the macro perspective of overall distance with the micro perspective of individual word weights to allow for an explorative analysis of variation. The Voyant Tools 12 provide a plethora of text visualizations, including word clouds, cooccurrences, and word trends based on frequencies. The focus of these tools, however, lies on summarizing and visualizing one text or corpus, rather than on exploring variation among corpora. Finally, the TXM platform 13 integrates the IMS Corpus Workbench with some macroanalysis R packages such as factorial correspondence analysis, contrastive word specificity, and cooccurrence analysis. While this integration certainly provides a broader set of analysis techniques, it is arguably more complicated to use than the system presented in this paper.
4. Summary and Future Work

We have presented a system that combines macroanalysis with microanalysis to explore language variation, and briefly illustrated its use for analyzing differences along the dimensions time, discipline, and genre in a corpus of scientific text. Future work will be devoted both to technical as well as methodological enhancements. A useful technical extension is the facility to interactively group subcorpora to larger units, maybe with the help of hierarchical clustering based on the distance matrix to form meaningful groups. More generally, the support for importing external corpora and exporting distance matrices and word weights for analysis with other tools is desirable – the presented system has been evaluated based on a number of corpora, but the underlying processing pipeline certainly needs to be generalized and improved. On the methodological side the main challenge lies in supporting a broader variety of feature sets beyond simple unigram language models. This includes latent language models such as topic models 14 and hidden markov models 15, but also enriched representations such as part-of-speech tagging, and other extensions of unigram models. Such richer feature sets allow to focus analysis by means of feature selection, but also bear new challenges in measuring and visualizing the contribution of features to a contrast at hand, and translating features into meaningful queries against the underlying corpus.
References

1. Elke Teich and Peter Fankhauser (2010). Exploring a Corpus of Scientific Texts using Data Mining. In S. Gries, S. Wulff, and M. Davies, editors, Corpus-linguistic applications: Current studies, new directions, pp. 233–247. Rodopi, Amsterdam and New York.
2. Stefania Degaetano-Ortlieb, Hannah Kermes, Ekaterina Lapshinova-Koltunski, and Elke Teich (2013). SciTex: A diachronic corpus for analyzing the development of scientific registers. In Paul Bennett, Martin Durrell, Silke Scheible, and Richard J. Whitt, editors, New Methods in Historical Corpus Linguistics, Corpus Linguistics and Interdisciplinary Perspectives on Language (CLIP), Volume 3, Narr, Tübingen.
3. Matthew L. Jockers (2013). Macroanalysis: Digital Methods & Literary History. University of Illinois Press, Urbana, Chicago, and Springfield.
4. Chengxiang Zhai and John Lafferty (2004). A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems (TOIS), 22(2):179–214.
5. David J. C. MacKay (2002). Information Theory, Inference & Learning Algorithms. Cambridge University Press, New York, NY, USA.
6. Takashi Tomokiyo and Matthew Hurst (2003). A language model approach to keyphrase extraction. Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment (MWE '03), Vol. 18, Association for Computational Linguistics, Stroudsburg, PA, USA, pp. 33–40. DOI=10.3115/1119282.1119287 dx.doi.org/10.3115/1119282.1119287
7. Adam Kilgarriff (2001). Comparing Corpora. International Journal of Corpus Linguistics, 6(1):97–133.
8. Michael Correll and Michael Gleicher (2012). What Shakespeare Taught Us About Text Visualization. IEEE Visualization Workshop Proceedings, 2nd Workshop on Interactive Visual Text Analytics: Task-Driven Analysis of Social Media Content, Seattle, Washington, USA, Oct 2012.
9. Matthew L. Jockers and Julia Flanders (2013). A Matter of Scale. Staged debate at the Boston Area Days of Digital Humanities Conference at Northeastern University, March 18, 2013. digitalcommons.unl.edu/englishfacpubs/106/
10. John Unsworth and Martin Mueller (2009). The MONK Project Final Report. Sep 2009. www.monkproject.org/MONKProjectFinalReport.pdf
11. Ted Dunning (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics 19(1):61–74.
12. Stéfan Sinclair, Geoffrey Rockwell and the Voyant Tools Team (2012). Voyant Tools (web application). http://voyant-tools.org/
13. Serge Heiden (2010). The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme. Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation, Institute for Digital Enhancement of Cognitive Development, Waseda University, Japan, Nov 2010, pp. 389-398.
14. David. M. Blei, Andrew Y. Ng, and Michael I. Jordan (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.
15. Sharon Goldwater and Tom Griffiths (2007). A fully Bayesian approach to unsupervised part-of-speech tagging. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL'07). Association for Computational Linguistics, Prague, Czech Republic, June 2007, pp. 744–751. www.aclweb.org/anthology/P07-1094
",txt,This text is republished here with permission from the original rights holder.,,corpus comparison;linguistic variation;macroanalysis;microanalysis,English,bibliographic methods / textual studies;concording and indexing;corpora and corpus activities;data mining / text mining;english studies;interface and user experience design;linguistics;text analysis;visualization,2014-01-01,"1. introduction

the english scientific text corpus (scitex) consists of about 5000 scientific papers with about 34 mio tokens in two time slots, 1970/80s and 2000s 1, 2. it has been compiled to investigate the construal of scientific disciplinarity, in particular, how interdisciplinary contact disciplines emerge from their seed disciplines. both time slots consist of nine disciplines: computer science (a) as one seed discipline, linguistics (c1), biology (c2), mechanical engineering (c3), electrical engineering (c4) as the other seed disciplines, and computational linguistics (b1), bioinformatics (b2), digital construction (b3), and microelectronics (b4) as the corresponding contact disciplines between a and c1-c4. the individual articles are subdivided into abstract, introduction, main, and conclusion.
the orthogonal dimensions time, discipline, and logical structure provide for many, potentially interesting setups of variational analysis: we can explore the diachronic evolution of contact disciplines in comparison to their seed disciplines, variation between contact disciplines and their seed disciplines, and genre variation between abstracts and text bodies in individual disciplines. in this paper we present an approach that combines a macroanalytic perspective 3 with the more traditional microanalytic perspective served by concordance search to explore variation along these dimensions.
2. approach

2.1. macroanalysis

for supporting explorative macroanalysis, we use well understood visualization techniques – heatmaps and wordclouds – and combine them with intuitive exploration paradigms – drill down and side by side comparison (see figure 1). the heatmaps and wordclouds are interactive, allowing for a closer inspection at various levels. the leftmost heatmap visualizes the highest level contrast between abstracts and text bodies in the two time slots (1970s/80s and 2000s). the middle and right heatmaps serve for inspecting a chosen contrast at a lower level at the level of individual disciplines. a particular contrast can be chosen by clicking on the respective square, numbers indicating which contrast is displayed in the middle (selection 1) and right heatmap (selection 2). in this example, the middle heatmap visualizes the distances between abstracts and text bodies, and the right heatmap visualizes the distances between text bodies and abstracts.
the wordclouds underneath the heatmaps display the most typical words for a chosen contrast. in figure 1 the wordcloud to the left visualizes the most typical words for abstracts as opposed to text bodies in the 2000s. unlike in the common use of wordclouds, the size of words is proportional to their contribution to the distance (as defined in section 2.2), whereas relative frequency is visualized by color, ranging from purple to red.

fig. 1: contrast between abstracts and text bodies
having a closer look at figure 1, we can observe that the distance between abstracts is generally larger than the distance between text bodies, and that it has increased in the 30 years period. this general trend is mirrored in the individual disciplines (not shown here). looking at the middle and right heatmaps, we can see that - not surprisingly - the distance between particular disciplines are at a minimum (squares forming the main diagonal), and the distances among the seed disciplines (a and c corpora), are generally larger than the distances among contract disciplines.
the corresponding wordclouds visualize the most typical words for abstracts (middle heatmap) and for text bodies (right heatmap) in the discipline b1 (computational linguistics). in this particular contrast, words typical for abstracts are clearly centered around constructions of exposition (we propose, describe, investigate), main topics of b1 (natural, language (generation), machine translation), words describing the methodology (method, statistical, computational, system) and function words (and, of, on). words typical for text bodies are markedly different: they comprise b1's main entities of topic elaboration (tokens, nouns, object, vp, john, probability), references (see figure, table, section), conjunctions (when, since, because, if), auxiliary and modal verbs (be, is, was, were, do, will, would, may), and prominently, the determiner the. in summary, abstracts exhibit characteristics of an informationally dense text (e.g., omission of determiners) with topic oriented content. in contrast, text bodies are less dense (determiners, references, modality) and more elaborated.
other contrastive pairs, such as the synchronic comparison between disciplines or the diachronic comparison of the two time slots, corroborate the results derived by means of computationally much more demanding techniques from machine learning [1], [2].
2.2. corpus representation and distance measures

the individual corpora are tokenized, and tokens are transformed to lower case. stopwords are deliberately not excluded to inspect all levels of variation: style, lexico-grammar, and theme. on this basis, corpora are represented by means of unigram language models smoothed with jelinek-mercer smoothing, which is a linear interpolation between the relative frequency of a word in a subcorpus and its relative frequency in the entire corpus 4. the distance between two corpora p and q is measured by relative entropy d, also known as kullback-leibler divergence:
d(p||q)>=sum_w p(w)*log_2(p(w)/q(w))
here p(w) is the probability of a word w in p, and q(w) is its probability in q. relative entropy thus measures the average amount of additional bits per word needed to encode words distributed according to p by using an encoding optimized for q. note that this measure is asymmetric, i.e., d(p||q) != d(q||p), and has its minimum at 0 for p = q5.
the individual word weights are calculated by the pointwise kullback-leibler divergence 6:
d_w(p||q) = p(w)*log_2(p(w)/q(w))
for all words the statistical significance of a difference is calculated based on an unpaired welch t-test on the observed word probabilities in the individual documents of a corpus. this is used for discarding words below a given level of significance (p-value). a more detailed comparison with other measures for comparing corpora 7 is beyond the scope of this paper and will appear in another venue.
2.3. microanalysis

wordclouds serve as a bridge between the big distance picture of macroanalysis and microanalyis. to this end, they are seamlessly integrated with the ims open corpus workbench (cqpweb: http://cwb.sourceforge.net/index.php), which provides for an expressive corpus query language and several summarization tools, such as collocations and comparative word frequency lists. a click on a word sends a query to cqpweb, which returns the word in the chosen context. for example, clicking on “do” in the right heatmap (b1 (txt00) vs. b1 (abs00)) generates the following query shown in figure 2.

fig. 2: concordance for “do” in b1, text bodies, 2000s
this query returns a concordance for “do” in the 2000s slot of scitex constrained to subcorpus b1 and to the divisions introduction, main, and conclusion. based on this list one can inspect the larger context of individual hits and get a ranked list of collocations to distinguish the uses of “do” as an auxiliary vs. main verb.
3. related work

the need for combining macroanalysis with microanalysis is well recognized in the dh community 8, 9, and there does exist a variety of frameworks with similar goals. due to space restrictions, we can only give an exemplary selection; for a comprehensive overview see tapor 2.0 (http://tapor.ca/). the monk workbench 10 allows to compare pairs of corpora using dunning's log-likelihood ratio 11 for word weighting. apart from the different distance measure, the main difference of our approach is that we combine the macro perspective of overall distance with the micro perspective of individual word weights to allow for an explorative analysis of variation. the voyant tools 12 provide a plethora of text visualizations, including word clouds, cooccurrences, and word trends based on frequencies. the focus of these tools, however, lies on summarizing and visualizing one text or corpus, rather than on exploring variation among corpora. finally, the txm platform 13 integrates the ims corpus workbench with some macroanalysis r packages such as factorial correspondence analysis, contrastive word specificity, and cooccurrence analysis. while this integration certainly provides a broader set of analysis techniques, it is arguably more complicated to use than the system presented in this paper.
4. summary and future work

we have presented a system that combines macroanalysis with microanalysis to explore language variation, and briefly illustrated its use for analyzing differences along the dimensions time, discipline, and genre in a corpus of scientific text. future work will be devoted both to technical as well as methodological enhancements. a useful technical extension is the facility to interactively group subcorpora to larger units, maybe with the help of hierarchical clustering based on the distance matrix to form meaningful groups. more generally, the support for importing external corpora and exporting distance matrices and word weights for analysis with other tools is desirable – the presented system has been evaluated based on a number of corpora, but the underlying processing pipeline certainly needs to be generalized and improved. on the methodological side the main challenge lies in supporting a broader variety of feature sets beyond simple unigram language models. this includes latent language models such as topic models 14 and hidden markov models 15, but also enriched representations such as part-of-speech tagging, and other extensions of unigram models. such richer feature sets allow to focus analysis by means of feature selection, but also bear new challenges in measuring and visualizing the contribution of features to a contrast at hand, and translating features into meaningful queries against the underlying corpus.
references

1. elke teich and peter fankhauser (2010). exploring a corpus of scientific texts using data mining. in s. gries, s. wulff, and m. davies, editors, corpus-linguistic applications: current studies, new directions, pp. 233–247. rodopi, amsterdam and new york.
2. stefania degaetano-ortlieb, hannah kermes, ekaterina lapshinova-koltunski, and elke teich (2013). scitex: a diachronic corpus for analyzing the development of scientific registers. in paul bennett, martin durrell, silke scheible, and richard j. whitt, editors, new methods in historical corpus linguistics, corpus linguistics and interdisciplinary perspectives on language (clip), volume 3, narr, tübingen.
3. matthew l. jockers (2013). macroanalysis: digital methods & literary history. university of illinois press, urbana, chicago, and springfield.
4. chengxiang zhai and john lafferty (2004). a study of smoothing methods for language models applied to information retrieval. acm transactions on information systems (tois), 22(2):179–214.
5. david j. c. mackay (2002). information theory, inference & learning algorithms. cambridge university press, new york, ny, usa.
6. takashi tomokiyo and matthew hurst (2003). a language model approach to keyphrase extraction. proceedings of the acl 2003 workshop on multiword expressions: analysis, acquisition and treatment (mwe '03), vol. 18, association for computational linguistics, stroudsburg, pa, usa, pp. 33–40. doi=10.3115/1119282.1119287 dx.doi.org/10.3115/1119282.1119287
7. adam kilgarriff (2001). comparing corpora. international journal of corpus linguistics, 6(1):97–133.
8. michael correll and michael gleicher (2012). what shakespeare taught us about text visualization. ieee visualization workshop proceedings, 2nd workshop on interactive visual text analytics: task-driven analysis of social media content, seattle, washington, usa, oct 2012.
9. matthew l. jockers and julia flanders (2013). a matter of scale. staged debate at the boston area days of digital humanities conference at northeastern university, march 18, 2013. digitalcommons.unl.edu/englishfacpubs/106/
10. john unsworth and martin mueller (2009). the monk project final report. sep 2009. www.monkproject.org/monkprojectfinalreport.pdf
11. ted dunning (1993). accurate methods for the statistics of surprise and coincidence. computational linguistics 19(1):61–74.
12. stéfan sinclair, geoffrey rockwell and the voyant tools team (2012). voyant tools (web application). http://voyant-tools.org/
13. serge heiden (2010). the txm platform: building open-source textual analysis software compatible with the tei encoding scheme. proceedings of the 24th pacific asia conference on language, information and computation, institute for digital enhancement of cognitive development, waseda university, japan, nov 2010, pp. 389-398.
14. david. m. blei, andrew y. ng, and michael i. jordan (2003). latent dirichlet allocation. journal of machine learning research, 3:993–1022.
15. sharon goldwater and tom griffiths (2007). a fully bayesian approach to unsupervised part-of-speech tagging. proceedings of the 45th annual meeting of the association of computational linguistics (acl'07). association for computational linguistics, prague, czech republic, june 2007, pp. 744–751. www.aclweb.org/anthology/p07-1094
",3.0,4.0,Voyant
1929,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Empowering Student Digital Scholarship: CLASS Program as a model for digital humanities scholarship in the Liberal Arts,,Janet Thomas Simons;Angel David Nieves;Kerri Grimaldi,poster / demo / art installation,"Proposal
Culture, Liberal Arts, and Society Scholars (CLASS) is an undergraduate research and fellowship program in the digital humanities awarded to student scholars at Hamilton College’s Digital Humanities Initiative (DHi). Basic literacies for the digital age are critical skills sets for students entering the professional world in the twenty-first century. The Digital Humanities Initiative provides new opportunities for students in the humanities to become fully engaged citizens in this ongoing digital revolution.

CLASS is based on three-broad areas of scholarly inquiry and their intersection with new and emerging digital technologies: 1) Culture, 2) Liberal Arts, and 3) Society. CLASS provides a unique partnership between departments, programs, and units across the liberal arts and humanities at Hamilton in partnership with the College’s Career Center. It begins with course connections in our Cinema and New Media Studies (CNMS) program but then removes the confines of the semester to promote deep understanding of digital humanities research within a specific field of interest. In these experiences, students and their faculty advisor become part of a collaborative working team of experts in DHi.

CLASS provides students with skills training in digital literacies through intensive research and scholarship coupled with two unique internship experiences. In the summer between sophomore and junior years CLASS offers undergraduate students an intensive professional development experience and provides a comprehensive overview of work in their respective field of interest. In the summer immediately after their junior year students enter their second internship off campus leading to employment and/or graduate study as a result of the eighteen-month program. Assistance with job placement, in a professional field, based on their CLASS internship placement, and/or graduate studies occurs in their final year at Hamilton. 


Fig. 1: Fig. 1. CLASS Program 18-month Structure

The coursework for the program begins in the fall of their sophomore year with CNMS 120 or 125 (Fig 1). In the spring semester students can enroll in courses offered in the CNMS minor. Students enroll in either CNMS 200W/Introduction to Digital Humanities or CNMS300/Interdisciplinary Research Methods that provide experiences writing grant proposals in the digital humanities.

The goals for CLASS include:

Collaboration with potential faculty and/or staff mentors to define and develop an interdisciplinary project
Writing a research proposal for their projects.
DHi committee reviews the proposals and recommend possible award opportunities.
Students begin work over a 10-week period in the summer, mid-June to late August.
A two-week intensive training program takes place in June of the first summer. Students survey mature digital humanities projects, participate in discussions of digital humanities readings, interact with invited speakers brought into the program, and explore technologies related to their research project goals. During the academic year following the first summer, students work with their mentor between 4-6 hours a week on their collaborative research project. In the summer between junior and senior years, CLASS offers undergraduate students an intensive professional development experience and provides a comprehensive overview of work in new digital technologies.

CLASS will:

Develop understanding of digital humanities methods
Develop technological expertise for careers and digital scholarship
Through their participation in an undergraduate research project, students will be able to:

Develop a research question, problem, or design;
Apply basic principles and knowledge found in the literature related to the research question;
Develop a research proposal to address or resolve a specific research question or problem;
Apply and evaluate interdisciplinary methodologies throughout the project;
Collect, interpret, and critique data in order to resolve a research question or evaluate a design;
Utilize digital skills (TEI, digital collection development, media object creation, geospatial visualization, etc.) necessary for robust digital scholarship in the humanities
Communicate research findings through oral presentations and digital publications.
Students in research collaborations with Faculty and members of the DHi, develop deep understanding of a specific long-term research agenda. They are expected to conduct collaborative investigation of a specific aspect of the research that is of great interest to them and integrate digital humanities research methods in their process. Deliverables include public presentation and/or publication at milestones in this process.

Accomplishments:
Cohort 2011

Sarah Bither and Melissa Yang worked with Professor Kyoko Omori to develop an understanding of Benshi performance art for contemporary audiences. The outcome of this work is a website, http://courses.hamilton.edu/dhi-class-1/sarah with components that will ultimately be incorporated into Professor Omori’s Japanese Comparative Literature Archive. Bither continued her study of Japanese culture and language by going abroad to Japan in the spring and summer of 2012. Randall Telfer worked with Professor Thomas Wilson to explore Confucian rituals and connections to contemporary religious practices in China. The outcomes of this work were additional edits to two of Professor Wilson’s websites http://academics.hamilton.edu/asian_studies/home/asc_test/index.html and The Cult of Confucius website: http://academics.hamilton.edu/asian_studies/home/coc_test/index.html. Brynna Tomassone worked with Professor Angel David Nieves to explore cultural connections between South Africa and the United States during the time period leading up to the events in 1976 Soweto. The outcome of this work is a series of book chapters currently in forthcoming publications including The Heritage of Iconic Planned Communities: The Challenges of Change (University of Pennsylvania Press, 2014). Tomassone is now a Ph.D. student in Hispanic/Spanish Studies at Syracuse University. 

 

Cohort 2012

Maxwell Lopez (’14) mentored by Professor Nathan Goodale. Continuing aspects of research on the history and culture of the Sinixt Nation in British Columbia, Lopez proposed to work for the 2012 -2013 year creating, “an accurate three dimensional digital representation of the British Columbia site along with some models of artifacts excavated” using the Unity game engine. He believes the project will create a “new way for people to experience and interact with history” and have great capacity for connecting with research with the public. By the end of the two- week CLASS session in June 2012, Lopez had already made several models in Blender (a 3D modeling software) and site maps in Unity (a virtual world platform for models to reside). The following is a screenshot of his initial construction of a pit house in Blender. Please see the folder on CLASS 2012 for screenshots of his work to date and his complete proposal. 


Fig. 2: Figure 2. Sinixt Ritual Pit House model (created in Blender).

Lopez continued work with Professor Nathan Goodale in GIS mapping and archeological data collection for continued development of Sinixt Ritual Pit (Fig. 2) houses at an archeology Field School in British Columbia summer 2013. Lopez has presented aspects of his work with Professor Goodale at several forums on campus in Fall 2012 and also at the 2013 Re:Humanities symposium April, 5, 2013.

Continuing aspects of O’Neill’s development of Beloved Witness: Agha Shahid Ali Archive, Ujjwal Pradham (’14) will explore the use of text analysis tools and TEI in developing aspects of the Agha Shahid Ali Archive. Pradham is also interested in establishing a connection between the archive and current communities of interest in Kashmir. 

Pradham explored the uses of social software to make connections between contemporary Kashmir communities and the developing Beloved Witness archive. The following is a screen shot of the Voyant Tools (Fig. 3) text analysis Ujjwal did to compare theme words (home, waiting, never, Spring, Kashmir) in multiple manuscripts over the development of a poem written by Ali. 


Fig. 3: Figure 3. Voyant Tools.

Cohort 2013

Working with Religious Studies Professor Abhishek Amar on aspects of his Sacred Centers in India Project, students Kenneth Ratliff (’16) and Alex Gioia (’14), embarked on a study of Indian sacred centers -- Buddhist Bodhgaya and Hindu Gaya. The students expanded their understanding of the Indian sacred cities of Gaya and Bodhgaya. They assisted Professor Amar in organizing his research data for these two cities (images, videos, and GPS coordinates) into the metadata schema for his digital research archive. This work of organizing and processing the over 418 data objects from Gaya into survey forms conducive to further analysis is necessary for long term sustainability of the digital archive. It is also the first step in the creation of interactive models of important artifacts and their locations within these religious sites. Appendix “A” is an example of one of the individual data survey forms used in this project and is based on those used by the K.P. Jayaswal Research Institute, with whom Professor Amar collaborates. Ratliff and Gioia have already begun creating an interactive two-dimensional line map of the Vishnupada complex (Fig 4). 


Fig. 4: Figure 4. Interactive line map of Vishnupada Complex with Mahadeva site highlighted (created in Blender).

This interactive map will link to images of the sites (Fig. 5) and 3D models of the artifacts in situ (Fig. 6). They hope that these virtual 3D and geographically correct models will foster greater interest in these religious sites due to the accessibility and interactivity of the maps, photographs, models, and videos.

Ultimately, the plans are to place the models that they produce into an online viewing space, developed from a game engine (Unity), that will make the models easily viewable and web accessible to the public. 


Fig. 5: Figure 5. Image of Mahadeva site and Tablet artifact on far wall.


Fig. 6: Figure 6. Image of Mahadeva Tablet being modeled in Blender (Free Download at Blender.org).

Working with Patricia O'Neill on aspects of her Beloved Witness archive, Kerri Grimaldi (’16) examined the significance of Emily Dickinson’s poetry to Agha Shahid Ali, a poet from Kashmir, whose work is the focus of the archive. Grimaldi’s project traces the depth of Emily Dickinson’s influence in Shahid’s poem, “A Nostalgist’s Map of America,” by placing Shahid’s poem side-by-side with Dickinson’s “A Route of Evanescence” in four stages of analysis, each increasing in level of explication. By analyzing Shahid’s poem, it is possible to read Dickinson’s in a completely different light, while also witnessing the resonating power of her poetry. Grimaldi has started to create a website to present her analysis of the relationship between the work of Shahid and Dickinson. Ultimately, this website will show not only that Dickinson influenced Shahid’s work, but that his work responded to and interpreted hers, such that their works are in conversation with each other. Please review the current status of this project, including the descriptive first layer of the website, the storyboards exploring intertextuality in the second layer and third layers and a draft of Grimaldi’s own creative video interpretation of the two poems in conversation. This project was submitted in September 2013 to the Dickinson Electronic Archives 2.0: CALL FOR PROPOSALS for volume 3 -- Emily Dickinson’s Reading Culture to be published in 2014. Part One of Kerri's website can be found at http://dhinitiative.org/demos/grimaldi/ 

Challenges
Initial Challenges for DHi in developing CLASS included answering, how do we publish in the digital Humanities? Much of collaborative research includes the use of copyrighted material and/or faculty research that is still in early development. These characteristics of the work in CLASS required that we reconsider the amount and type of information (public or not) conveyed to illustrate the progress of the students. We achieved our goal of facilitating discussion with other scholars in the field by including experts from across disciplines in the two-week intensive training program and through the natural association of collaborators on the faculty research projects. CLASS scholars biographies and research descriptions are announced on the DHi website. CLASS scholars give project prospectus presentations and/or example research projects at the end of their two-week training program in the first summer. These presentations and examples are given to an invited audience for feedback on the projects. Ultimately, each student presents or publishes their work off-campus. Several students have presented at Re:Humanities. 

CLASS Program Summary and Future Plans
Students in research collaborations with Faculty and members of the DHi, have been successful in developing deep understanding of a specific long-term research agenda. DHi provides the immersive experiences and ongoing continuity with faculty research necessary to support this engagement. Most of our CLASS scholars have conducted collaborative investigation of a specific aspect of a long-term research agenda, determined their specific interests and contributions to that agenda, and publicly presented their scholarship “in progress” at Hamilton events and professional conferences.

Our future plans are to continue the CLASS program and to collaborate on its development with other liberal arts schools. Several schools have asked us about building similar models at their schools. This would require thinking more about scaling-up the program. One option we are considering is a form of “Summer Institute” for undergraduates in which we bring them together with their mentors and a larger DH community for a two-week program. 

Appendix A: Proof of Concept for a Sustainable Digital Humanities Faculty Collection Infrastructure in the Liberal Arts. 


DHi’s technology infrastructure and research support model is designed to be sustainable. That is, our approach will reduce the need for regular revamping of static faculty research web pages by creating infrastructure and processes that maintain research outcomes as “living” web presences accessible for faculty and student collaborative scholarship over time. To this end we researched best practices in digital collection development and preservation in collaboration with members of our library and decided to develop an institutional warehouse (repository) for digital collections (Fedora Commons). Fedora was chosen for its scalability and ability to be extremely flexible in the way objects can be accessed. Fedora has built-in flexibility to allow creation and maintenance of relationships among objects and across digital collections over time.

After researching open source collaborative tools to interface with collections in Fedora Commons we decided to make use of Islandora. Islandora can be used to create customized themes for faculty collections and projects. Our DHi Collection Development Team is working with the Islandora and Fedora Commons consultants (at Discovery Garden to create our digital scholarship infrastructure. By using experts to help with development we are making efficient use of the Mellon Grant to move this complex project forward. 

2013 Appendix B: KPJR Form Vishnupada Complex Mahadeva Temple 
DOCUMENTATION SHEET OF BUILD HERITAGE/SITE N.M.M.A., ARCHAEOLOGICAL SURVERY OF INDIA

COMPILED AT K.P. JAYASWAL RESEARCH INSTITUTE, PATNA

Sl.No.	 Documentation Parameters 	 
 	 State/Dist./Block 	Gaya
 1	 Name of the monument/built heritage/site	 Mahdava Temple
 2	 Date/Period	 Early Medieval/Medieval?
 3	 Location 	To east and north of 16 Vedis/Padas in Vishnupada Complex 
 	 Geo-coordinate	 
 4	 Approach	East of the Vishnupada Temple, in the Vishnupada Complex 
 	 Airport	Gaya 
 	 Railway Station	Gaya 
 	 Bus Stand	Gaya 
 5	 Topographical features	Slope of the Mundaprishta hill on the western bank of the Phalgu 
 6	 Brief History	 
Temple seems to have origins in early medieval or medieval period. The exact date of the construction of the temple is difficult to determine because of lack of historical sources. It has images and inscriptions, but they may have been moved. 

 7	Local tradition associated with building/structure/site	Gaya Shraddha, place of Pinda-dana as well as Darshana 
 8	 Architectural style	 Inner sanctum, which has a Linga, and there are 20 pillars, which constitute the Mandapa
 9	 Description of the building/structure/site	 
Mahadeva temple: Inner sanctum with a Shiva Linga and a twenty pillared Mandapa. Narasimha Temple (east of Mahadeva): Small rock temple Sarasvati Temple (east of Narasimha): Small rock temple Facing Narasimha are two single chamber shrines. All five are treated as one unit in the Vishnupada complex. 

 10	 Building/Structural material and other	Stone and brick; pillars are stone 
11 	Usage(s) 	Active worship, Darshana 
12 	Ownership 	Same as Vishnupada Main Temple 
13 	Protection status 	Good 
14 	Present condition 	Maintained 
15 	Conservation assessment 	Alright 
16 	Photographs 	See attached 
17 	Plan/elevation, if available 	 
18 	Published references 	 
19 	General Remarks 	There is an inscription 
20 	Name and address of compiler with date elements used 	Abhishek Singh Amar Matthew Sayers 
Images: 

Mahadeva Temple (129):

By opening

1. Vishnu, 16'' (122)

2. Camunda, 16'' (120, 121)

There's no logic to the temple - they have plastered images all over the place; normally, you would not see Bishnu and Camunda next to each other, but in this case we do, in a disorganized fashion. For instance, they are in the same niche, but Camunda is platered higher than Vishnu, which speaks to the disorganization of the collection process. 
South wall

3. Ganesha, 16'' (122)

4. Inscription (123-125)

Appears to be painted more recently. 
5. Eroded Uma-Maheshvaga, 14'' (126)

Inner sanctum (127)

6. Huge Shiva Linga (127)

North wall

7. Uma-Maheshvara, 16'' (128)

By opening, on the north, west-facing niche; left

8. Durga, 36'' (130)

9. New inscription (130)

Outside, left

10. Dasavatara (131)",txt,This text is republished here with permission from the original rights holder.,,collaboration;curriculum;fellowship;methods;student scholarship,English,"digital humanities - nature and significance;digital humanities - pedagogy and curriculum;interdisciplinary collaboration;project design, organization, management;publishing and delivery systems",2014-01-01,"proposal
culture, liberal arts, and society scholars (class) is an undergraduate research and fellowship program in the digital humanities awarded to student scholars at hamilton college’s digital humanities initiative (dhi). basic literacies for the digital age are critical skills sets for students entering the professional world in the twenty-first century. the digital humanities initiative provides new opportunities for students in the humanities to become fully engaged citizens in this ongoing digital revolution.

class is based on three-broad areas of scholarly inquiry and their intersection with new and emerging digital technologies: 1) culture, 2) liberal arts, and 3) society. class provides a unique partnership between departments, programs, and units across the liberal arts and humanities at hamilton in partnership with the college’s career center. it begins with course connections in our cinema and new media studies (cnms) program but then removes the confines of the semester to promote deep understanding of digital humanities research within a specific field of interest. in these experiences, students and their faculty advisor become part of a collaborative working team of experts in dhi.

class provides students with skills training in digital literacies through intensive research and scholarship coupled with two unique internship experiences. in the summer between sophomore and junior years class offers undergraduate students an intensive professional development experience and provides a comprehensive overview of work in their respective field of interest. in the summer immediately after their junior year students enter their second internship off campus leading to employment and/or graduate study as a result of the eighteen-month program. assistance with job placement, in a professional field, based on their class internship placement, and/or graduate studies occurs in their final year at hamilton. 


fig. 1: fig. 1. class program 18-month structure

the coursework for the program begins in the fall of their sophomore year with cnms 120 or 125 (fig 1). in the spring semester students can enroll in courses offered in the cnms minor. students enroll in either cnms 200w/introduction to digital humanities or cnms300/interdisciplinary research methods that provide experiences writing grant proposals in the digital humanities.

the goals for class include:

collaboration with potential faculty and/or staff mentors to define and develop an interdisciplinary project
writing a research proposal for their projects.
dhi committee reviews the proposals and recommend possible award opportunities.
students begin work over a 10-week period in the summer, mid-june to late august.
a two-week intensive training program takes place in june of the first summer. students survey mature digital humanities projects, participate in discussions of digital humanities readings, interact with invited speakers brought into the program, and explore technologies related to their research project goals. during the academic year following the first summer, students work with their mentor between 4-6 hours a week on their collaborative research project. in the summer between junior and senior years, class offers undergraduate students an intensive professional development experience and provides a comprehensive overview of work in new digital technologies.

class will:

develop understanding of digital humanities methods
develop technological expertise for careers and digital scholarship
through their participation in an undergraduate research project, students will be able to:

develop a research question, problem, or design;
apply basic principles and knowledge found in the literature related to the research question;
develop a research proposal to address or resolve a specific research question or problem;
apply and evaluate interdisciplinary methodologies throughout the project;
collect, interpret, and critique data in order to resolve a research question or evaluate a design;
utilize digital skills (tei, digital collection development, media object creation, geospatial visualization, etc.) necessary for robust digital scholarship in the humanities
communicate research findings through oral presentations and digital publications.
students in research collaborations with faculty and members of the dhi, develop deep understanding of a specific long-term research agenda. they are expected to conduct collaborative investigation of a specific aspect of the research that is of great interest to them and integrate digital humanities research methods in their process. deliverables include public presentation and/or publication at milestones in this process.

accomplishments:
cohort 2011

sarah bither and melissa yang worked with professor kyoko omori to develop an understanding of benshi performance art for contemporary audiences. the outcome of this work is a website, http://courses.hamilton.edu/dhi-class-1/sarah with components that will ultimately be incorporated into professor omori’s japanese comparative literature archive. bither continued her study of japanese culture and language by going abroad to japan in the spring and summer of 2012. randall telfer worked with professor thomas wilson to explore confucian rituals and connections to contemporary religious practices in china. the outcomes of this work were additional edits to two of professor wilson’s websites http://academics.hamilton.edu/asian_studies/home/asc_test/index.html and the cult of confucius website: http://academics.hamilton.edu/asian_studies/home/coc_test/index.html. brynna tomassone worked with professor angel david nieves to explore cultural connections between south africa and the united states during the time period leading up to the events in 1976 soweto. the outcome of this work is a series of book chapters currently in forthcoming publications including the heritage of iconic planned communities: the challenges of change (university of pennsylvania press, 2014). tomassone is now a ph.d. student in hispanic/spanish studies at syracuse university. 

 

cohort 2012

maxwell lopez (’14) mentored by professor nathan goodale. continuing aspects of research on the history and culture of the sinixt nation in british columbia, lopez proposed to work for the 2012 -2013 year creating, “an accurate three dimensional digital representation of the british columbia site along with some models of artifacts excavated” using the unity game engine. he believes the project will create a “new way for people to experience and interact with history” and have great capacity for connecting with research with the public. by the end of the two- week class session in june 2012, lopez had already made several models in blender (a 3d modeling software) and site maps in unity (a virtual world platform for models to reside). the following is a screenshot of his initial construction of a pit house in blender. please see the folder on class 2012 for screenshots of his work to date and his complete proposal. 


fig. 2: figure 2. sinixt ritual pit house model (created in blender).

lopez continued work with professor nathan goodale in gis mapping and archeological data collection for continued development of sinixt ritual pit (fig. 2) houses at an archeology field school in british columbia summer 2013. lopez has presented aspects of his work with professor goodale at several forums on campus in fall 2012 and also at the 2013 re:humanities symposium april, 5, 2013.

continuing aspects of o’neill’s development of beloved witness: agha shahid ali archive, ujjwal pradham (’14) will explore the use of text analysis tools and tei in developing aspects of the agha shahid ali archive. pradham is also interested in establishing a connection between the archive and current communities of interest in kashmir. 

pradham explored the uses of social software to make connections between contemporary kashmir communities and the developing beloved witness archive. the following is a screen shot of the voyant tools (fig. 3) text analysis ujjwal did to compare theme words (home, waiting, never, spring, kashmir) in multiple manuscripts over the development of a poem written by ali. 


fig. 3: figure 3. voyant tools.

cohort 2013

working with religious studies professor abhishek amar on aspects of his sacred centers in india project, students kenneth ratliff (’16) and alex gioia (’14), embarked on a study of indian sacred centers -- buddhist bodhgaya and hindu gaya. the students expanded their understanding of the indian sacred cities of gaya and bodhgaya. they assisted professor amar in organizing his research data for these two cities (images, videos, and gps coordinates) into the metadata schema for his digital research archive. this work of organizing and processing the over 418 data objects from gaya into survey forms conducive to further analysis is necessary for long term sustainability of the digital archive. it is also the first step in the creation of interactive models of important artifacts and their locations within these religious sites. appendix “a” is an example of one of the individual data survey forms used in this project and is based on those used by the k.p. jayaswal research institute, with whom professor amar collaborates. ratliff and gioia have already begun creating an interactive two-dimensional line map of the vishnupada complex (fig 4). 


fig. 4: figure 4. interactive line map of vishnupada complex with mahadeva site highlighted (created in blender).

this interactive map will link to images of the sites (fig. 5) and 3d models of the artifacts in situ (fig. 6). they hope that these virtual 3d and geographically correct models will foster greater interest in these religious sites due to the accessibility and interactivity of the maps, photographs, models, and videos.

ultimately, the plans are to place the models that they produce into an online viewing space, developed from a game engine (unity), that will make the models easily viewable and web accessible to the public. 


fig. 5: figure 5. image of mahadeva site and tablet artifact on far wall.


fig. 6: figure 6. image of mahadeva tablet being modeled in blender (free download at blender.org).

working with patricia o'neill on aspects of her beloved witness archive, kerri grimaldi (’16) examined the significance of emily dickinson’s poetry to agha shahid ali, a poet from kashmir, whose work is the focus of the archive. grimaldi’s project traces the depth of emily dickinson’s influence in shahid’s poem, “a nostalgist’s map of america,” by placing shahid’s poem side-by-side with dickinson’s “a route of evanescence” in four stages of analysis, each increasing in level of explication. by analyzing shahid’s poem, it is possible to read dickinson’s in a completely different light, while also witnessing the resonating power of her poetry. grimaldi has started to create a website to present her analysis of the relationship between the work of shahid and dickinson. ultimately, this website will show not only that dickinson influenced shahid’s work, but that his work responded to and interpreted hers, such that their works are in conversation with each other. please review the current status of this project, including the descriptive first layer of the website, the storyboards exploring intertextuality in the second layer and third layers and a draft of grimaldi’s own creative video interpretation of the two poems in conversation. this project was submitted in september 2013 to the dickinson electronic archives 2.0: call for proposals for volume 3 -- emily dickinson’s reading culture to be published in 2014. part one of kerri's website can be found at http://dhinitiative.org/demos/grimaldi/ 

challenges
initial challenges for dhi in developing class included answering, how do we publish in the digital humanities? much of collaborative research includes the use of copyrighted material and/or faculty research that is still in early development. these characteristics of the work in class required that we reconsider the amount and type of information (public or not) conveyed to illustrate the progress of the students. we achieved our goal of facilitating discussion with other scholars in the field by including experts from across disciplines in the two-week intensive training program and through the natural association of collaborators on the faculty research projects. class scholars biographies and research descriptions are announced on the dhi website. class scholars give project prospectus presentations and/or example research projects at the end of their two-week training program in the first summer. these presentations and examples are given to an invited audience for feedback on the projects. ultimately, each student presents or publishes their work off-campus. several students have presented at re:humanities. 

class program summary and future plans
students in research collaborations with faculty and members of the dhi, have been successful in developing deep understanding of a specific long-term research agenda. dhi provides the immersive experiences and ongoing continuity with faculty research necessary to support this engagement. most of our class scholars have conducted collaborative investigation of a specific aspect of a long-term research agenda, determined their specific interests and contributions to that agenda, and publicly presented their scholarship “in progress” at hamilton events and professional conferences.

our future plans are to continue the class program and to collaborate on its development with other liberal arts schools. several schools have asked us about building similar models at their schools. this would require thinking more about scaling-up the program. one option we are considering is a form of “summer institute” for undergraduates in which we bring them together with their mentors and a larger dh community for a two-week program. 

appendix a: proof of concept for a sustainable digital humanities faculty collection infrastructure in the liberal arts. 


dhi’s technology infrastructure and research support model is designed to be sustainable. that is, our approach will reduce the need for regular revamping of static faculty research web pages by creating infrastructure and processes that maintain research outcomes as “living” web presences accessible for faculty and student collaborative scholarship over time. to this end we researched best practices in digital collection development and preservation in collaboration with members of our library and decided to develop an institutional warehouse (repository) for digital collections (fedora commons). fedora was chosen for its scalability and ability to be extremely flexible in the way objects can be accessed. fedora has built-in flexibility to allow creation and maintenance of relationships among objects and across digital collections over time.

after researching open source collaborative tools to interface with collections in fedora commons we decided to make use of islandora. islandora can be used to create customized themes for faculty collections and projects. our dhi collection development team is working with the islandora and fedora commons consultants (at discovery garden to create our digital scholarship infrastructure. by using experts to help with development we are making efficient use of the mellon grant to move this complex project forward. 

2013 appendix b: kpjr form vishnupada complex mahadeva temple 
documentation sheet of build heritage/site n.m.m.a., archaeological survery of india

compiled at k.p. jayaswal research institute, patna

sl.no.	 documentation parameters 	 
 	 state/dist./block 	gaya
 1	 name of the monument/built heritage/site	 mahdava temple
 2	 date/period	 early medieval/medieval?
 3	 location 	to east and north of 16 vedis/padas in vishnupada complex 
 	 geo-coordinate	 
 4	 approach	east of the vishnupada temple, in the vishnupada complex 
 	 airport	gaya 
 	 railway station	gaya 
 	 bus stand	gaya 
 5	 topographical features	slope of the mundaprishta hill on the western bank of the phalgu 
 6	 brief history	 
temple seems to have origins in early medieval or medieval period. the exact date of the construction of the temple is difficult to determine because of lack of historical sources. it has images and inscriptions, but they may have been moved. 

 7	local tradition associated with building/structure/site	gaya shraddha, place of pinda-dana as well as darshana 
 8	 architectural style	 inner sanctum, which has a linga, and there are 20 pillars, which constitute the mandapa
 9	 description of the building/structure/site	 
mahadeva temple: inner sanctum with a shiva linga and a twenty pillared mandapa. narasimha temple (east of mahadeva): small rock temple sarasvati temple (east of narasimha): small rock temple facing narasimha are two single chamber shrines. all five are treated as one unit in the vishnupada complex. 

 10	 building/structural material and other	stone and brick; pillars are stone 
11 	usage(s) 	active worship, darshana 
12 	ownership 	same as vishnupada main temple 
13 	protection status 	good 
14 	present condition 	maintained 
15 	conservation assessment 	alright 
16 	photographs 	see attached 
17 	plan/elevation, if available 	 
18 	published references 	 
19 	general remarks 	there is an inscription 
20 	name and address of compiler with date elements used 	abhishek singh amar matthew sayers 
images: 

mahadeva temple (129):

by opening

1. vishnu, 16'' (122)

2. camunda, 16'' (120, 121)

there's no logic to the temple - they have plastered images all over the place; normally, you would not see bishnu and camunda next to each other, but in this case we do, in a disorganized fashion. for instance, they are in the same niche, but camunda is platered higher than vishnu, which speaks to the disorganization of the collection process. 
south wall

3. ganesha, 16'' (122)

4. inscription (123-125)

appears to be painted more recently. 
5. eroded uma-maheshvaga, 14'' (126)

inner sanctum (127)

6. huge shiva linga (127)

north wall

7. uma-maheshvara, 16'' (128)

by opening, on the north, west-facing niche; left

8. durga, 36'' (130)

9. new inscription (130)

outside, left

10. dasavatara (131)",2.0,2.0,Voyant
1984,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Leaves of Grass: Data Animation and XML Technologies,,Brett Barney;Brian Pytlik-Zillig,"paper, specified ""long paper""","Walt Whitman’s Leaves of Grass is one of the most famous and often-studied works of American literature. In the century since Van Wyck Brooks declared Whitman the originator of “the sense of something organic in American life”—the first to combine high art and rude experience—Whitman’s masterwork has been thoroughly digested into a series of critical truisms that gives even new readers of the poems a sense of familiarity. Whether we have his poems committed to memory or have never actually read one of them, we “all know” that Whitman eschewed traditionally poetic diction, that his is a poetry of inclusiveness, that the first edition of his text in 1855 is more daring, lively, and experimental than later editions, etc. 

Such axioms are comforting in the face of what is on many levels a difficult text (actually, a set of texts) to assimilate. Because Whitman applied the title “Leaves of Grass” to more than ten distinctly different volumes over the course of three and a half decades—not only adding poems but also retitling, cancelling, drastically revising, combining, and re-grouping existing ones—the goal of accurately tracing the book’s evolution has consistently frustrated scholars. Recognizing that “for the reader to understand how Leaves of Grass grew from edition to edition, some sense had to be made of these often bewildering textual permutations,” a group of late-twentieth century scholars labored for over a decade to produce a variorum edition, a tremendous accomplishment that has, unfortunately, done little to alleviate the bewilderment of permutations. 

The hope that digital technologies might offer a way, at last, to lucidly represent the various stages in the evolution of Leaves of Grass was one of the early motivations for the creators of the Whitman Archive in the late 1990s. We have often revisited the question of how to convey visually the information represented in the arcane coding of the 3-volume print variorum and inherent in the separate digitized editions. Nearly two decades later, however, we haven’t made much progress. 

Though they cannot provide the kind of detailed, objective understanding that might be conveyed by the schematic, interactive interfaces that we’ve sometimes (very hazily) imagined—ones that somehow collate whole texts, poems, lines, and phrases—we have begun to experiment with distant reading strategies that provide a different sort of view. So while collation tools do not cope well with the scope of transformation involved in Whitman’s reworking the first edition’s 10,000-word prose preface into the 4,000-word poem “By Blue Ontario’s Shore,” text analysis tools such as Voyant offer a number of potentially enlightening prospects on the two works and their relationship. 

Likewise, such tools can begin to offer ways to assay and quantify some of the critical commonplaces that have grown up around Leaves of Grass: Is Whitman’s diction, in fact, innovative and what makes it so? How do Whitman’s early poems compare to his later poems? What basis might be found for claims that Whitman is the great poet of America, women, the body, male homoeroticism, or democracy?

At the University of Nebraska–Lincoln's Center for Digital Research in the Humanities we have been experimenting with a new way of visualizing phenomena in TEI corpora and have created Indigo, an experimental XSLT-based tool that queries TEI files and generates animated videos of the results. Using XPath and XQuery techniques, this tool makes it possible to ask specific or general questions of a corpus. The data are then output as scalable vector graphic (SVG) files that are converted to raster images and rendered in high definition H.264 video at 30 frames per second. At its core, Indigo is a program for performing scripted stop-motion animation, arranged in one or more scenes. What each scene contains is up to the user: it might include letters, numbers, shapes, colors, gradients, patterns, lines, paths, or imported raster images, each moving or not moving. The only requirement is that a scene must be modeled in XSLT, with SVG structures as the initial output. For the user wishing to visualize aspects of TEI text corpora, the news is good, for that format shares membership with XSLT and SVG in the XML ecosystem. Indigo provides a method for presenting, in fresh and unexpected ways, quantitative data relevant to scholarly questions in a way that is open-ended, making the user a co-creator with Whitman in the “meaning” of his texts. 

Our experiment involves such activities as creating quantitive analyses of some of the linguistic characteristics of Whitman's poetic corpus, comparing them to those of some of his popular contemporaries, and then ""presenting"" the results as a video sequence. Such a procedure is admittedly outside the mainstream of critical methodology in the humanities, but it is entirely in keeping with Whitman’s own theories of the proper relationships among authors, readers, and texts. “The process of reading,” he said, “is not a half-sleep, but, in highest sense, an exercise, a gymnast’s struggle; . . . the reader is to do something for himself, . . . must himself or herself construct indeed the poem, argument, history, metaphysical essay—the text furnishing the hints, the clue, the start or frame-work.”1

As Tanya Clement has recently observed, ""sometimes the view facilitated by digital tools generates the same data human beings . . . could generate by hand, but more quickly,"" and sometimes ""these vantage points are remarkably different . . . and provide us with a new perspective on texts.""2 And as Dana Solomon has written, ""due in large part to its often powerful and aesthetically pleasing visual impact, relatively quick learning curve … and overall 'cool,' the practice of visualizing textual data has been widely adopted by the digital humanities.""3

In representing the literary work as an absorbing performance, one that comprises both ""data"" and ""art,"" the method we are presenting is calculated to provoke responses in both informational and aesthetic registers. It is, in the terms of Jerome McGann and Lisa Samuels, an act of “interpretive deformance,” whereby “we are brought to a critical position in which we can imagine things about the text that we didn’t and perhaps couldn’t otherwise know.”4

References
1. Whitman, Walt (1892). Democratic Vistas in Complete Prose Works, (Philadelphia: David McKay), p. 257.

2. Clement, T (2013). Text Analysis, Data Mining, and Visualizations in Literary Scholarship in Literary Studies in the Digital Age: An Evolving Anthology (eds., Kenneth M. Price, Ray Siemens). Modern Language Association.

3. Solomon, D. (2013). Building the Infrastructural Layer: Reading Data Visualization in the Digital Humanities. MLA 2013 Conference Presentation. url: danaryansolomon.wordpress.com/2013/01/08/mla-2013-conference-presentation-from-sunday-162013/

4. McGann, Jerome and Lisa Samuels.Deformance and Interpretation url: www2.iath.virginia.edu/jjm2f/old/deform.html",txt,This text is republished here with permission from the original rights holder.,,animation;deformance;leaves of grass;walt whitman,English,"audio, video, multimedia;content analysis;corpora and corpus activities;literary studies;text analysis;visualization",2014-01-01,"walt whitman’s leaves of grass is one of the most famous and often-studied works of american literature. in the century since van wyck brooks declared whitman the originator of “the sense of something organic in american life”—the first to combine high art and rude experience—whitman’s masterwork has been thoroughly digested into a series of critical truisms that gives even new readers of the poems a sense of familiarity. whether we have his poems committed to memory or have never actually read one of them, we “all know” that whitman eschewed traditionally poetic diction, that his is a poetry of inclusiveness, that the first edition of his text in 1855 is more daring, lively, and experimental than later editions, etc. 

such axioms are comforting in the face of what is on many levels a difficult text (actually, a set of texts) to assimilate. because whitman applied the title “leaves of grass” to more than ten distinctly different volumes over the course of three and a half decades—not only adding poems but also retitling, cancelling, drastically revising, combining, and re-grouping existing ones—the goal of accurately tracing the book’s evolution has consistently frustrated scholars. recognizing that “for the reader to understand how leaves of grass grew from edition to edition, some sense had to be made of these often bewildering textual permutations,” a group of late-twentieth century scholars labored for over a decade to produce a variorum edition, a tremendous accomplishment that has, unfortunately, done little to alleviate the bewilderment of permutations. 

the hope that digital technologies might offer a way, at last, to lucidly represent the various stages in the evolution of leaves of grass was one of the early motivations for the creators of the whitman archive in the late 1990s. we have often revisited the question of how to convey visually the information represented in the arcane coding of the 3-volume print variorum and inherent in the separate digitized editions. nearly two decades later, however, we haven’t made much progress. 

though they cannot provide the kind of detailed, objective understanding that might be conveyed by the schematic, interactive interfaces that we’ve sometimes (very hazily) imagined—ones that somehow collate whole texts, poems, lines, and phrases—we have begun to experiment with distant reading strategies that provide a different sort of view. so while collation tools do not cope well with the scope of transformation involved in whitman’s reworking the first edition’s 10,000-word prose preface into the 4,000-word poem “by blue ontario’s shore,” text analysis tools such as voyant offer a number of potentially enlightening prospects on the two works and their relationship. 

likewise, such tools can begin to offer ways to assay and quantify some of the critical commonplaces that have grown up around leaves of grass: is whitman’s diction, in fact, innovative and what makes it so? how do whitman’s early poems compare to his later poems? what basis might be found for claims that whitman is the great poet of america, women, the body, male homoeroticism, or democracy?

at the university of nebraska–lincoln's center for digital research in the humanities we have been experimenting with a new way of visualizing phenomena in tei corpora and have created indigo, an experimental xslt-based tool that queries tei files and generates animated videos of the results. using xpath and xquery techniques, this tool makes it possible to ask specific or general questions of a corpus. the data are then output as scalable vector graphic (svg) files that are converted to raster images and rendered in high definition h.264 video at 30 frames per second. at its core, indigo is a program for performing scripted stop-motion animation, arranged in one or more scenes. what each scene contains is up to the user: it might include letters, numbers, shapes, colors, gradients, patterns, lines, paths, or imported raster images, each moving or not moving. the only requirement is that a scene must be modeled in xslt, with svg structures as the initial output. for the user wishing to visualize aspects of tei text corpora, the news is good, for that format shares membership with xslt and svg in the xml ecosystem. indigo provides a method for presenting, in fresh and unexpected ways, quantitative data relevant to scholarly questions in a way that is open-ended, making the user a co-creator with whitman in the “meaning” of his texts. 

our experiment involves such activities as creating quantitive analyses of some of the linguistic characteristics of whitman's poetic corpus, comparing them to those of some of his popular contemporaries, and then ""presenting"" the results as a video sequence. such a procedure is admittedly outside the mainstream of critical methodology in the humanities, but it is entirely in keeping with whitman’s own theories of the proper relationships among authors, readers, and texts. “the process of reading,” he said, “is not a half-sleep, but, in highest sense, an exercise, a gymnast’s struggle; . . . the reader is to do something for himself, . . . must himself or herself construct indeed the poem, argument, history, metaphysical essay—the text furnishing the hints, the clue, the start or frame-work.”1

as tanya clement has recently observed, ""sometimes the view facilitated by digital tools generates the same data human beings . . . could generate by hand, but more quickly,"" and sometimes ""these vantage points are remarkably different . . . and provide us with a new perspective on texts.""2 and as dana solomon has written, ""due in large part to its often powerful and aesthetically pleasing visual impact, relatively quick learning curve … and overall 'cool,' the practice of visualizing textual data has been widely adopted by the digital humanities.""3

in representing the literary work as an absorbing performance, one that comprises both ""data"" and ""art,"" the method we are presenting is calculated to provoke responses in both informational and aesthetic registers. it is, in the terms of jerome mcgann and lisa samuels, an act of “interpretive deformance,” whereby “we are brought to a critical position in which we can imagine things about the text that we didn’t and perhaps couldn’t otherwise know.”4

references
1. whitman, walt (1892). democratic vistas in complete prose works, (philadelphia: david mckay), p. 257.

2. clement, t (2013). text analysis, data mining, and visualizations in literary scholarship in literary studies in the digital age: an evolving anthology (eds., kenneth m. price, ray siemens). modern language association.

3. solomon, d. (2013). building the infrastructural layer: reading data visualization in the digital humanities. mla 2013 conference presentation. url: danaryansolomon.wordpress.com/2013/01/08/mla-2013-conference-presentation-from-sunday-162013/

4. mcgann, jerome and lisa samuels.deformance and interpretation url: www2.iath.virginia.edu/jjm2f/old/deform.html",1.0,1.0,Voyant
2031,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Probing Digital Scholarly Curation through the Dynamic Table of Contexts,,Susan Brown;Nadine Adelaar;Teresa Dobson;Ruth Knechtel;Andrew MacDonald;Brent Nelson;Ernesto Peña;Milena Radzikowska;Geoff Roeder;Stan Ruecker;Stéfan Sinclair;Jennifer Windsor,poster / demo / art installation,"In this paper, we theorize about the role of the curator – or perhaps it would be more accurate to say “custodian” or even “collection designer” – in preparing electronic texts for use in an online digital environment. This scholarly role is becoming increasingly important to new forms of knowledge dissemination as a result of the growth in aggregating or mashing up existing digital content. The goal of these aggregations is to add value for particular purposes, as seen in initiatives such as the Journal of Digital Humanities, which aims to collect already published materials into quarterly thematic collections, and the related website Digital Humanities Now, which curates weekly the feeds of other digital humanities websites (Digital Humanities Now).

By analogy with the definition of a curator as “The officer in charge of a museum, gallery of art, library, or the like; a keeper, custodian” (“curator,” def. n. 6), the digital scholarly curator performs a similar role with respect to the circulation of digital content, though with a number of significant differences. Gallery or museum curators, for instance, have their own scholarly and professional training and preparation, often with respect to proper handling and display of fine art and other valuable physical objects. There are those who feel that at least some curators should be considered artists themselves (e.g. Ventzislavov). There is also widespread acknowledgment that curators who work with digital materials require a different set of competencies. Melody Madrid, for example, reports a study that resulted in a list of 20, divided into the categories operational and managerial. Digital scholarly curation also harnesses social media technologies (e.g. crowdsourcing, folksonomies) to encourage a new level of user participation in the management and preservation of digital content (Poole). Not quite an editor, but acting as a mediator between the producers of these contents and its audience through such activities as selecting and reframing them, the digital scholarly curator is in a rather unique position. Our discussion considers this role in relation to the Dynamic Table of Contexts (DToC) interface, a generalized tool for the dissemination of digital text that enables the designer of the collection to mediate specifically between the XML encoding of the text and the affordances that such encoding provides in the reading interface (Brown et al.; Dobson et al.).

The Dynamic Table of Contexts (Fig 1) is a joint initiative between the Interface Design research team of the Implementing New Knowledge Environments (INKE) project, the Canadian Writing Research Collaboratory (CWRC), the University of Alberta Press, and the Voyant Tools project. The DToC currently leverages four principal components of a digital text: the actual text of the document, a table of contents, an index, and XML encoding of the document. The goal of the prototype is to provide an online reading environment where the table of contents provides a conventional overview of a book while at the same time incorporating the index terms and XML tags and the text to which they point (Ruecker et al.). The terms and tags can be selected and deselected, providing interactivity between these three means for accessing and navigating the content of the text or collection.


Fig. 1: Dynamic Table of Contexts Interface

The ability to leverage XML markup as part of the navigational interface is a distinguishing feature of the DToC. Customization of that affordance is enabled by what we call the DToC’s “curator mode,” which is distinct from the reading mode in that it allows technically adept superusers to create customized tag lists to serve as navigational aids alongside the index terms, as well as to determine the organization of the table of contents (login is not required, the curated view is expressed through a unique URL). In a print edition, and in particular for anthologies, it is necessary for the editor to decide which of the various alternatives will be used to organize a particular table of contents. For example, a collection of essays might be organized

alphabetically by title
alphabetically by author’s last name
chronologically
by theme
or by some other principle, in order to ensure a certain kind of development or coherence from beginning to end.
A collection of poems, for example, might add organization alphabetically by the first line of the poem, for cases where the poems do not have a title, and other arrangements are also possible, for instance by geographic location, language, or genre. In the case of an instructor preparing a course pack, the arrangement would naturally correspond to the sequence in which the materials will be used in the class. In the history of print, the possibilities for multiple representation of contents were limited.

In addition to the selection and organization of the contents themselves, curators of DToC collections need to make choices with respect to how the encoding works in the interface. Given the more generic and multi-purpose nature of XML encoding, particularly its use to structure a text, curators need to select which tags the DToC interface will display to the reader, and what user-friendly labels to use for those tags. For many purposes (although not all), the structural encoding can be set aside in favor of semantic encoding (when present). For choices among these tags, there is probably some golden mean that’s most appropriate for generic use; the primary benefit of the Dynamic Table of Contexts is to allow variants for more specific uses. Tags that have been rarely used may already be covered by the index, or may be too insignificant to take room on the list. On the other hand, in cases where the tags have been used heavily enough, it may not be useful to include them since it would result in too great a density of hits.

There are also large differences between what kind of curation of tags is required – depending on whether a schema or tagset has instead been applied throughout a collection, such as in the Brown Women Writers Project’s use of the Text Encoding Initiative – as opposed to highly customized versions of the TEI adapted to the needs of a particular text. Finally, the curator is also enabled to label how tags will appear in the interface, so that readers are not asked to decipher cryptic forms such as <biblStruct>, but are instead presented with labels for such tags that are meaningful in the context for which the curated text or collection is being prepared. This functionality is useful in cases where there is a nuanced difference between two similar tags that needs to be conveyed to the readers (Fig 2).


Fig. 2: Dynamic Table of Contents Curator Interface

What this means in terms of the training and qualifications of the DToC curator is that the person needs to hypothesize how the target readers will be dealing with the material, and to use the curatorial functions to customize the view of the text in the DToC interface to meet the anticipated use case(s). In the case of class instructors, to a certain extent the job will be simplified since the course pack has been chosen purposively for the class. In other situations, there may be multiple and possibly conflicting anticipated use cases. The curator also needs to be comfortable enough with XML not only to choose appropriate tags and rename them, but also, if needed, to specify XPath queries to locations in the document that cannot be identified through tag names alone. There will be considerable variation in XML expertise amongst curators, and a previous user study of ours on an earlier version of DToC indicated that a closer relationship to the encoding correlated with more positive experiences of the DToC interface (Dobson et al.).

The paper will frame our understanding of digital scholarly curation in relation to more traditional, historical understandings of curation and representation of contents and will demonstrate the curator mode in the DToC interface. Our previous user studies of the interface found both considerable confusion on the part of users with respect to the role of the encoding or tagging in the DToC (Brown et al.), and, among those who understood it, considerable emphasis on the importance of the XML markup and its ability to shape the reader experience in the interface. As one user said in mousing over the XML Markup pane: “Well, this seems to me the most relevant section, so whoever puts that together is pretty much the wizard in this Oz” (Dobson et al.). Our discussion will incorporate results of the next user study we are conducting on the DToC, with a stress on the curator mode, that will probe these findings which go to the heart of the DToC’s affordances. Our aim will be to gain a fuller understanding of the ways in which users with a range of technical knowledge understand the role of markup in the DToC interface, and their understanding as both readers and curators of the curatorial role. This study will inform our understanding not only of the ways in which reading environments such as the DToC can effectively leverage XML encoding, but more generally of the ways in which the idea of curation is rapidly evolving within the online scholarly environment.

References
Brown, Susan, Brent Nelson, Stan Ruecker, Stéfan Sinclair, Nadine Adelaar, Ruth Knechtel, Jennifer Windsor and the INKE Research Group. “Text Encoding, the Index, and the Dynamic Table of Contexts.” Paper presented at the annual Digital Humanities conference (DH2013), Lincoln, Nebraska. July 16-19, 2013

“Curator.” Def. n. 5. The Oxford English Dictionary. 2013. OED Online. Web. 1 Nov. 2013.

Digital Humanities Now. http://digitalhumanitiesnow.org/

Dobson, Teresa, Brooke Heller, Stan Ruecker, Milena Radzikowska, Mark Bieber, Susan Brown, and the INKE Research Group. “The Dynamic Table of Contexts: User Experience and Future Directions.” Paper presented in the panel “Designing Interactive Reading Environments for the Online Scholarly Edition” at the DH2012 conference, Hamburg, Germany. July 16-20, 2012.

Kholeif, O. The Curator’s New Medium. Art Monthly. February 2013, (363):9-12. Ipswich, MA.

Little, G. Thinking Like Curators. Journal of Academic Librarianship, 2013, 39(2), 123-125. doi:10.1016/j.acalib.2013.01.003

Madrid, Melody M. A study of digital curator competences: A survey of experts. The International Information & Library Review, Volume 45, Issues 3–4, December 2013, pp 149-156.

Poole, Alex H. “Now is the Future? The Urgency of Digital Curation in the Digital Humanities.” Digital Humanities Quarterly 7:2 (2013).

Ruecker, Stan and the INKE Research Group. “Introducing the Dynamic Table of Contexts for Scholarly Editions.” Paper presented at the Modern Language Association (MLA) Conference. Los Angeles, CA. Jan 6-8, 2011.

Ruecker, Stan, Susan Brown, Milena Radzikowska, Stéfan Sinclair, Thomas M. Nelson, Patricia Clements, Isobel Grundy, Sharon Balasz, and Jeff Antoniuk. “The Table of Contexts: A Dynamic Browsing Tool for Digitally Encoded Texts.” In The Charm of a List: From the Sumerians to Computerised Data Processing. Ed. Lucie Dolezalova. Cambridge: Cambridge Scholars Publishing, 2009. pp. 177-187.

Ventzislavov, R. (2014), Idle Arts: Reconsidering the Curator. The Journal of Aesthetics and Art Criticism, 72: 83–93. doi: 10.1111/jaac.12058",txt,This text is republished here with permission from the original rights holder.,,digital curation;digital scholarly edition;indexing;semantic tagging;xml,English,"concording and indexing;digitisation, resource creation, and discovery;encoding - theory and practice;information retrieval;interface and user experience design;metadata;scholarly editing;xml",2014-01-01,"in this paper, we theorize about the role of the curator – or perhaps it would be more accurate to say “custodian” or even “collection designer” – in preparing electronic texts for use in an online digital environment. this scholarly role is becoming increasingly important to new forms of knowledge dissemination as a result of the growth in aggregating or mashing up existing digital content. the goal of these aggregations is to add value for particular purposes, as seen in initiatives such as the journal of digital humanities, which aims to collect already published materials into quarterly thematic collections, and the related website digital humanities now, which curates weekly the feeds of other digital humanities websites (digital humanities now).

by analogy with the definition of a curator as “the officer in charge of a museum, gallery of art, library, or the like; a keeper, custodian” (“curator,” def. n. 6), the digital scholarly curator performs a similar role with respect to the circulation of digital content, though with a number of significant differences. gallery or museum curators, for instance, have their own scholarly and professional training and preparation, often with respect to proper handling and display of fine art and other valuable physical objects. there are those who feel that at least some curators should be considered artists themselves (e.g. ventzislavov). there is also widespread acknowledgment that curators who work with digital materials require a different set of competencies. melody madrid, for example, reports a study that resulted in a list of 20, divided into the categories operational and managerial. digital scholarly curation also harnesses social media technologies (e.g. crowdsourcing, folksonomies) to encourage a new level of user participation in the management and preservation of digital content (poole). not quite an editor, but acting as a mediator between the producers of these contents and its audience through such activities as selecting and reframing them, the digital scholarly curator is in a rather unique position. our discussion considers this role in relation to the dynamic table of contexts (dtoc) interface, a generalized tool for the dissemination of digital text that enables the designer of the collection to mediate specifically between the xml encoding of the text and the affordances that such encoding provides in the reading interface (brown et al.; dobson et al.).

the dynamic table of contexts (fig 1) is a joint initiative between the interface design research team of the implementing new knowledge environments (inke) project, the canadian writing research collaboratory (cwrc), the university of alberta press, and the voyant tools project. the dtoc currently leverages four principal components of a digital text: the actual text of the document, a table of contents, an index, and xml encoding of the document. the goal of the prototype is to provide an online reading environment where the table of contents provides a conventional overview of a book while at the same time incorporating the index terms and xml tags and the text to which they point (ruecker et al.). the terms and tags can be selected and deselected, providing interactivity between these three means for accessing and navigating the content of the text or collection.


fig. 1: dynamic table of contexts interface

the ability to leverage xml markup as part of the navigational interface is a distinguishing feature of the dtoc. customization of that affordance is enabled by what we call the dtoc’s “curator mode,” which is distinct from the reading mode in that it allows technically adept superusers to create customized tag lists to serve as navigational aids alongside the index terms, as well as to determine the organization of the table of contents (login is not required, the curated view is expressed through a unique url). in a print edition, and in particular for anthologies, it is necessary for the editor to decide which of the various alternatives will be used to organize a particular table of contents. for example, a collection of essays might be organized

alphabetically by title
alphabetically by author’s last name
chronologically
by theme
or by some other principle, in order to ensure a certain kind of development or coherence from beginning to end.
a collection of poems, for example, might add organization alphabetically by the first line of the poem, for cases where the poems do not have a title, and other arrangements are also possible, for instance by geographic location, language, or genre. in the case of an instructor preparing a course pack, the arrangement would naturally correspond to the sequence in which the materials will be used in the class. in the history of print, the possibilities for multiple representation of contents were limited.

in addition to the selection and organization of the contents themselves, curators of dtoc collections need to make choices with respect to how the encoding works in the interface. given the more generic and multi-purpose nature of xml encoding, particularly its use to structure a text, curators need to select which tags the dtoc interface will display to the reader, and what user-friendly labels to use for those tags. for many purposes (although not all), the structural encoding can be set aside in favor of semantic encoding (when present). for choices among these tags, there is probably some golden mean that’s most appropriate for generic use; the primary benefit of the dynamic table of contexts is to allow variants for more specific uses. tags that have been rarely used may already be covered by the index, or may be too insignificant to take room on the list. on the other hand, in cases where the tags have been used heavily enough, it may not be useful to include them since it would result in too great a density of hits.

there are also large differences between what kind of curation of tags is required – depending on whether a schema or tagset has instead been applied throughout a collection, such as in the brown women writers project’s use of the text encoding initiative – as opposed to highly customized versions of the tei adapted to the needs of a particular text. finally, the curator is also enabled to label how tags will appear in the interface, so that readers are not asked to decipher cryptic forms such as <biblstruct>, but are instead presented with labels for such tags that are meaningful in the context for which the curated text or collection is being prepared. this functionality is useful in cases where there is a nuanced difference between two similar tags that needs to be conveyed to the readers (fig 2).


fig. 2: dynamic table of contents curator interface

what this means in terms of the training and qualifications of the dtoc curator is that the person needs to hypothesize how the target readers will be dealing with the material, and to use the curatorial functions to customize the view of the text in the dtoc interface to meet the anticipated use case(s). in the case of class instructors, to a certain extent the job will be simplified since the course pack has been chosen purposively for the class. in other situations, there may be multiple and possibly conflicting anticipated use cases. the curator also needs to be comfortable enough with xml not only to choose appropriate tags and rename them, but also, if needed, to specify xpath queries to locations in the document that cannot be identified through tag names alone. there will be considerable variation in xml expertise amongst curators, and a previous user study of ours on an earlier version of dtoc indicated that a closer relationship to the encoding correlated with more positive experiences of the dtoc interface (dobson et al.).

the paper will frame our understanding of digital scholarly curation in relation to more traditional, historical understandings of curation and representation of contents and will demonstrate the curator mode in the dtoc interface. our previous user studies of the interface found both considerable confusion on the part of users with respect to the role of the encoding or tagging in the dtoc (brown et al.), and, among those who understood it, considerable emphasis on the importance of the xml markup and its ability to shape the reader experience in the interface. as one user said in mousing over the xml markup pane: “well, this seems to me the most relevant section, so whoever puts that together is pretty much the wizard in this oz” (dobson et al.). our discussion will incorporate results of the next user study we are conducting on the dtoc, with a stress on the curator mode, that will probe these findings which go to the heart of the dtoc’s affordances. our aim will be to gain a fuller understanding of the ways in which users with a range of technical knowledge understand the role of markup in the dtoc interface, and their understanding as both readers and curators of the curatorial role. this study will inform our understanding not only of the ways in which reading environments such as the dtoc can effectively leverage xml encoding, but more generally of the ways in which the idea of curation is rapidly evolving within the online scholarly environment.

references
brown, susan, brent nelson, stan ruecker, stéfan sinclair, nadine adelaar, ruth knechtel, jennifer windsor and the inke research group. “text encoding, the index, and the dynamic table of contexts.” paper presented at the annual digital humanities conference (dh2013), lincoln, nebraska. july 16-19, 2013

“curator.” def. n. 5. the oxford english dictionary. 2013. oed online. web. 1 nov. 2013.

digital humanities now. http://digitalhumanitiesnow.org/

dobson, teresa, brooke heller, stan ruecker, milena radzikowska, mark bieber, susan brown, and the inke research group. “the dynamic table of contexts: user experience and future directions.” paper presented in the panel “designing interactive reading environments for the online scholarly edition” at the dh2012 conference, hamburg, germany. july 16-20, 2012.

kholeif, o. the curator’s new medium. art monthly. february 2013, (363):9-12. ipswich, ma.

little, g. thinking like curators. journal of academic librarianship, 2013, 39(2), 123-125. doi:10.1016/j.acalib.2013.01.003

madrid, melody m. a study of digital curator competences: a survey of experts. the international information & library review, volume 45, issues 3–4, december 2013, pp 149-156.

poole, alex h. “now is the future? the urgency of digital curation in the digital humanities.” digital humanities quarterly 7:2 (2013).

ruecker, stan and the inke research group. “introducing the dynamic table of contexts for scholarly editions.” paper presented at the modern language association (mla) conference. los angeles, ca. jan 6-8, 2011.

ruecker, stan, susan brown, milena radzikowska, stéfan sinclair, thomas m. nelson, patricia clements, isobel grundy, sharon balasz, and jeff antoniuk. “the table of contexts: a dynamic browsing tool for digitally encoded texts.” in the charm of a list: from the sumerians to computerised data processing. ed. lucie dolezalova. cambridge: cambridge scholars publishing, 2009. pp. 177-187.

ventzislavov, r. (2014), idle arts: reconsidering the curator. the journal of aesthetics and art criticism, 72: 83–93. doi: 10.1111/jaac.12058",1.0,1.0,Voyant
2055,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Seeing Dialogue: Network Visualization of Dramatic Texts,,Daniel James Powell,poster / demo / art installation,">Contexts for Humanities Visualization
Literary scholars are increasingly turning to graphical display of humanistic information as a way to encounter texts in new and engaging ways. Digitally facilitated humanities visualization presents literary critics with opportunities for new insight1, while also opening practitioners to charges of wholesale importation of simplistic scientific methodologies.2 This poster session outlines the rationale, method, and significance of a focused humanities visualization project in order to demonstrate how new techniques of visualization may be undertaken with literary texts to produce new and speculative “aesthetic provocations” in literary studies.3

Materials and Scope
In order to model and theorize how such a project might develop, I am producing a network visualization of the sixteenth-century play Ralph Roister Doister by Nicholas Udall. Using the open-source ""interactive visualization and exploration"" platform Gephi, I map the relationships between characters in the play based upon dialogue. In other words, this project structurally maps dialogue between characters, producing a network visualization that productively reconfigures the play to provoke new analytical responses. Following the example of Franco Moretti’s work with Hamlet, these models prompt new insight into the “deep structures” of literary works. For Moretti, however, “the most important thing of all” about these reconfigurations is that they can be manipulated: “one can intervene on a model; make experiments.”4 [116, italics in original]. With a baseline visualization of dialogue established, I, following Jerome McGann, selectively intervene and “deform” the re-modeled text to continually reconfigure it. Such processes bring us “to a critical position in which we can imagine things about the texts that we didn’t and perhaps couldn’t otherwise know.”5 [116] Motivated by Drucker and Nowviskie’s call to “engage computing to produce new aesthetic provocations,” I use Roister Doister to understand how humanities visualization may reconfigure our approach to literary inquiry.6 

Methods of Production
In order to map the dialogic relationships between these characters in Gephi, I have created each character as a node in the network; these nodes are connected by directional dialogue originating at a particular node and terminating at another. Thus, the main characters Roister Doister and Merygreeke are nodes 1.0 and 2.0, for example. Within Gephi’s data manipulation environment, a line of dialogue from the former to the latter would be mapped visually as a line [or an “edge”] from node 1.0 to node 2.0. Each exchange is recorded as tabled data in Gephi, which is then used to produce visualizations. Visualizations can be produced for any discrete unit of the text, including scene, act, or the entirety of the play. This project produces visualizations of each of these divisions for comparative purposes. Following Moretti and McGann, I have undertaken selective deformations by, for example, removing various characters at different points, thereby revealing their network centrality. Criticism of Roister Doister has, for the most part, focused heavily on the play’s dramaturgical debt to the classical comedies of Terence and Plautus, the extent to which those formal structures were successfully integrated with “native” elements of English drama, and the play’s debts to the miles gloriosus [“braggart-soldier”] tradition of Classical comedy.78 This project is in part an attempt to revitalize an ossified critical conversation as an example of how new techniques can vigorously re-engage old texts. 

Significance of Anticipated Visualizations
Experimentation in visualization of textual works is a timely one. As can be seen from the growing use of the Voyant suite9 of analysis and visualization tools as well as the popular Mapping the Republic of Lettersproject,10 humanities visualization is a growing area of scholarly concern. Elijah Meeks has argued that “the shift from creating, annotating and analyzing archives to modeling systems can have a profound impact beyond the [admittedly high value of] usability of scholarly material developed during a digital humanities project.”11 [italics mine]. When humanities scholars have reached a certain point of visual literacy, we will begin to engage with such models in profoundly important ways. Indeed, these models may “provide a much more nuanced form of knowledge transmission than the raw datasets or interactive and dynamic applications typically presented as the future of digital scholarly media.”12 This project is an effort to explore how these new forms of knowledge transmission and analysis might impact literary inquiry. 

References
1. Manovich, Lev. (2010). What Is Visualization? Poetess Archive Journal 2(1), n. pag.

2. Drucker, Johanna. (2011). Humanities Approaches to Graphical Display. Digital Humanities Quarterly 5(1), n. page.

3. Drucker, Johanna, and Bethany Nowviskie. (2004). Speculative Computing: Aesthetic Provocations in Humanities Computing. Companion to Digital Humanities. In Schreibman, Susan, Siemens, Ray, & Unsworth, John (eds). Oxford, Blackwell. Accessed 03 April 2012. Available at [journals.tdl.org/paj/index.php/paj/article/view/19/58]

4,5. Moretti, Franco. (2011). Network Theory, Plot Analysis. Stanford, Stanford Literary Lab.

6. Drucker, Johanna, and Bethany Nowviskie. (2004). Speculative Computing: Aesthetic Provocations in Humanities Computing. Companion to Digital Humanities. In Schreibman, Susan, Siemens, Ray, & Unsworth, John (eds). Oxford, Blackwell. Accessed 03 April 2012. Available at [journals.tdl.org/paj/index.php/paj/article/view/19/58]

7. Boas, Frederick S. (1933). An Introduction to Tudor Drama. Oxford, Clarendon.

8. Brooke, C. F. Tucker. (1911). The Tudor Drama: A History of English National Drama to the Retirement of Shakespeare. Boston, Riverside.

9. Voyant Tools. (2012). [voyant-tools.org]. Accessed October 2012.

10. Mapping the Republic of Letters: Navigating Big Data from the Early Modern Period. (2012). Available at [republicofletters.stanford.edu].

11. Meeks, Elijah. (2011). More Networks in the Humanities or Did Books Have DNA? Digital Humanities Specialist. Published 6 December 2011. Accessed 17 April 2012.

12. Meeks, Elijah. (2011). More Networks in the Humanities or Did Books Have DNA? Digital Humanities Specialist. Published 6 December 2011. Accessed 17 April 2012.",txt,This text is republished here with permission from the original rights holder.,,drama;early modern studies;network analysis;visualization,English,"english studies;networks, relationships, graphs;renaissance studies;text analysis;visualization",2014-01-01,">contexts for humanities visualization
literary scholars are increasingly turning to graphical display of humanistic information as a way to encounter texts in new and engaging ways. digitally facilitated humanities visualization presents literary critics with opportunities for new insight1, while also opening practitioners to charges of wholesale importation of simplistic scientific methodologies.2 this poster session outlines the rationale, method, and significance of a focused humanities visualization project in order to demonstrate how new techniques of visualization may be undertaken with literary texts to produce new and speculative “aesthetic provocations” in literary studies.3

materials and scope
in order to model and theorize how such a project might develop, i am producing a network visualization of the sixteenth-century play ralph roister doister by nicholas udall. using the open-source ""interactive visualization and exploration"" platform gephi, i map the relationships between characters in the play based upon dialogue. in other words, this project structurally maps dialogue between characters, producing a network visualization that productively reconfigures the play to provoke new analytical responses. following the example of franco moretti’s work with hamlet, these models prompt new insight into the “deep structures” of literary works. for moretti, however, “the most important thing of all” about these reconfigurations is that they can be manipulated: “one can intervene on a model; make experiments.”4 [116, italics in original]. with a baseline visualization of dialogue established, i, following jerome mcgann, selectively intervene and “deform” the re-modeled text to continually reconfigure it. such processes bring us “to a critical position in which we can imagine things about the texts that we didn’t and perhaps couldn’t otherwise know.”5 [116] motivated by drucker and nowviskie’s call to “engage computing to produce new aesthetic provocations,” i use roister doister to understand how humanities visualization may reconfigure our approach to literary inquiry.6 

methods of production
in order to map the dialogic relationships between these characters in gephi, i have created each character as a node in the network; these nodes are connected by directional dialogue originating at a particular node and terminating at another. thus, the main characters roister doister and merygreeke are nodes 1.0 and 2.0, for example. within gephi’s data manipulation environment, a line of dialogue from the former to the latter would be mapped visually as a line [or an “edge”] from node 1.0 to node 2.0. each exchange is recorded as tabled data in gephi, which is then used to produce visualizations. visualizations can be produced for any discrete unit of the text, including scene, act, or the entirety of the play. this project produces visualizations of each of these divisions for comparative purposes. following moretti and mcgann, i have undertaken selective deformations by, for example, removing various characters at different points, thereby revealing their network centrality. criticism of roister doister has, for the most part, focused heavily on the play’s dramaturgical debt to the classical comedies of terence and plautus, the extent to which those formal structures were successfully integrated with “native” elements of english drama, and the play’s debts to the miles gloriosus [“braggart-soldier”] tradition of classical comedy.78 this project is in part an attempt to revitalize an ossified critical conversation as an example of how new techniques can vigorously re-engage old texts. 

significance of anticipated visualizations
experimentation in visualization of textual works is a timely one. as can be seen from the growing use of the voyant suite9 of analysis and visualization tools as well as the popular mapping the republic of lettersproject,10 humanities visualization is a growing area of scholarly concern. elijah meeks has argued that “the shift from creating, annotating and analyzing archives to modeling systems can have a profound impact beyond the [admittedly high value of] usability of scholarly material developed during a digital humanities project.”11 [italics mine]. when humanities scholars have reached a certain point of visual literacy, we will begin to engage with such models in profoundly important ways. indeed, these models may “provide a much more nuanced form of knowledge transmission than the raw datasets or interactive and dynamic applications typically presented as the future of digital scholarly media.”12 this project is an effort to explore how these new forms of knowledge transmission and analysis might impact literary inquiry. 

references
1. manovich, lev. (2010). what is visualization? poetess archive journal 2(1), n. pag.

2. drucker, johanna. (2011). humanities approaches to graphical display. digital humanities quarterly 5(1), n. page.

3. drucker, johanna, and bethany nowviskie. (2004). speculative computing: aesthetic provocations in humanities computing. companion to digital humanities. in schreibman, susan, siemens, ray, & unsworth, john (eds). oxford, blackwell. accessed 03 april 2012. available at [journals.tdl.org/paj/index.php/paj/article/view/19/58]

4,5. moretti, franco. (2011). network theory, plot analysis. stanford, stanford literary lab.

6. drucker, johanna, and bethany nowviskie. (2004). speculative computing: aesthetic provocations in humanities computing. companion to digital humanities. in schreibman, susan, siemens, ray, & unsworth, john (eds). oxford, blackwell. accessed 03 april 2012. available at [journals.tdl.org/paj/index.php/paj/article/view/19/58]

7. boas, frederick s. (1933). an introduction to tudor drama. oxford, clarendon.

8. brooke, c. f. tucker. (1911). the tudor drama: a history of english national drama to the retirement of shakespeare. boston, riverside.

9. voyant tools. (2012). [voyant-tools.org]. accessed october 2012.

10. mapping the republic of letters: navigating big data from the early modern period. (2012). available at [republicofletters.stanford.edu].

11. meeks, elijah. (2011). more networks in the humanities or did books have dna? digital humanities specialist. published 6 december 2011. accessed 17 april 2012.

12. meeks, elijah. (2011). more networks in the humanities or did books have dna? digital humanities specialist. published 6 december 2011. accessed 17 april 2012.",2.0,3.0,Voyant
2056,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Seeing the Trees & Understanding the Forest,,John Joseph Montague;Geoffrey Rockwell;Stan Ruecker;Stéfan Sinclair;Susan Brown;Ryan Chartier;Luciano Frizzera;John Edward Simpson,"paper, specified ""long paper""","“Humanists have always been explorers. They sail not on seas of water but on seas of color, sound, and, most especially, words.” — John B. Smith, 1984

Why is big data such a big deal? Modern digital communications are producing and recording data at a feverish rate, with 90% of the world’s recorded data, most of it unstructured, having been produced in just the last two years (Dragland, 2013). In addition, more traditional texts are being digitized all the time, and Crane (2006) tells us that while the largest academic digital libraries now hold tens of thousands of books, a completed Google Library will likely have more than ten million. To further Smith’s analogy, we are adrift in a sea of data, and we are at risk of floundering.

Front page news events like Edward Snowden’s May 2013 revelations disclosing the secret NSA (America’s National Security Agency) collection and analysis of massive amounts of ostensibly private data both domestically and internationally, are making big data analytics part of the public lexicon.  Major corporations, led by IBM with $1.3 billion in big data revenue in 2012, are scrambling to adopt practices that will allow them to capitalize on the volume of information now being generated.  Global big data related revenues in 2012 were $11.6 billion, and are projected to break $18 billion in 2013 (Kelly et al.).

While one might shudder to think what the NSA will do with all that information, outside of the world of politics and espionage, what can researchers and academics do with the volume of information now, or at least soon to be at our disposal? Matthew Jockers asks, “How do we mine them [texts] to find something we don’t already know?” (University of Nebraska-Lincoln) To utilize such massive corpora and avoid succumbing to the dilemma of what McCormick et al (1987) refer to as “information without interpretation” we need to find the most effective ways for end-users to explore, visualize and interact with big data such that it is both meaningful and understandable, if possible by both the trained and untrained eye. 

Big data analytics and computer-supported visualization offer ways to read collections as our cognitive abilities are stretched to their limit with the sheer volume of data available (Araya, 2003). However, as Franco Moretti has pointed out, what we are reading when we use text mining methods and visualizations are really models of the collections. (Moretti 2013, p. 157) It is important therefore to survey the visual models emerging and question if they can better be designed to suit humanities exploration. In this paper we therefore propose to look at visualization for text mining in the following ways:

Survey text mining visualization in the humanities. Who is using text mining and why? What kinds of visualizations do they find compelling and why?
Identify and Combine commonly presented visualizations for modeling. What visual models could be used for the exploration of large corpora? How could they be combined?  
Model interactive prototypes of different combinations of visualizations for exploration. 
Surveying: In a poster at DH 2013 we presented a framework of text mining tools that are useful in the humanities. Now we will survey the variety of visualizations used to present mining results. We will begin with early discussions of visualization like Smith’s “Computer Criticism” (Style 1978, p. 326), where he notes with agreement Paul de Man’s observation that as late as 1973 there had been no evolution beyond the close reading “techniques of description and interpretation” being used by literary critics since the 1930s or 40s, and suggests “pictorial representation” as one of the potential uses of computer aided text analysis. Brunet in a 1989 article talks about exploiting large corpora and provides a number of examples of visualizations. Our survey will examine advances in practice and understanding in the intervening thirty plus years, leading to contemporary works like Franco Moretti’s Graphs, Maps, and Trees, which, as the title suggests, introduces visual models for literary exploration.

Our survey pays particular attention to recent text mining projects and tools including “The Proceedings of the Old Bailey”, David L. Hoover’s work with cluster analyses at NYU using “MiniTab”, Matt Jockers’ topic modeling work in “Macroanalysis”, the University of Waikato’s “WEKA”, the open-source visualization tool “Gephi”, the UMass machine learning tool “MALLET”, the research tools for textual study reviewed on “TAPoR”as well as some of our own INKE related projects such as “Dynamic Table of Contents”, “CiteLens”, “TextTiles” and “dialR”. 

Identifying and Combining: While we recognize that output will assume a variety of formats including for instance heat maps, topographic plots or scatter plots, our survey suggests that text-mining projects commonly use five principal types of visualization:

1. Dendrograms, showing data clustering within sets
2. Histograms, showing change over time
3. Networkdiagrams, showing how entities are connected within a network
4. Wordclouds, representing topics of words
5. Scatterplots, showing words or parts in an abstract space
Visualizations that are presented in print are typically Spartan, focusing the attention on the results through careful design. All affordances are removed. The same types of visualizations automatically generated from large data sets, however, tend to be too dense to be useful and have to include affordances if meant to be interactive. Simple representative visualizations, like histograms for instance, are insufficient to display complex interrelationships. Dendrograms, especially if you are working with massive data sets, quickly become an illegible mass of inter-connectivity. Word clouds are good at showing the relative frequency of words in a text or topic, but not at comparing one text or topic to another. Network diagrams produce some beautiful results, but suffer from the same difficulties as dendrograms; large data sets quickly lead to illegibility.

“By visualizing information, we turn it into a landscape that you can explore with your eyes, a sort of information map. And when you’re lost in information, an information map is kind of useful.” — David McCandless, 2010

Modeling: Our research now is looking at ways to increase the exploratory power of visualization of large data sets. Used in combination, visualizations can provide otherwise elusive insight and clarity. They can also provide affordances for each other – a histogram can be used to explore a dendrogram. We have developed interactive prototypes using combinations of visualizations, and focusing on not simply allowing, but even encouraging the user to truly explore the (big) data, “function[ing] almost instinctively”, as McCullough (1996) stated, “to serve the process of development”. The more we can encourage users to explore and play with the data, the more likely they are to develop useful insights.


Fig. 1: Combination of a modified dendrogram showing clustering, and a diachronic timeline of 500 philosophy papers.

Figure 1 shows a prototype of a combination of dendrogram and histogram that we developed to visualize the clustering of 500 papers published between 1966 and 2004. The prototype is a combination of a modified dendrogram showing the clusters, and a diachronic visualization displaying the subjects over time. The displayed data is intuitively explorable, and the modified dendrogram is designed to encourage exploration. We developed the R code to prepare the data for interactivity. In Voyant we have developed skins that combine scatter plots and histograms, word clouds and histograms, and network diagrams with other tools. Again, the tools are open for others to recombine.

To conclude, surveying commonly used graphical representations allowed us to identify commonly used visualizations that humanists find useful. To scale these so that they can be used interactively to explore large data sets we have prototyped combinations, where one visualization can be used to explore another and vice versa. The goal is visualizations that help researchers make sense of big data; visualizations that let us explore the forest, not just the trees so that we can draw accurate and appropriate inferences from the data.

References
The Proceedings of the Old Bailey - http://www.oldbaileyonline.org/

Cluster Analysis, Principal Components Analysis (PCA), and T-testing in Minitab - https://files.nyu.edu/dh3/public/ClusterAnalysis-PCA-T-testingInMinitab.html

Jockers, M. Macroanalysis.

Weka 3: Data Mining Software in Java - http://www.cs.waikato.ac.nz/ml/weka/

Gephi - The Open Graph Viz Platform - https://gephi.org/

Mallet - MAchine Learning for LanguagE Toolkit - http://mallet.cs.umass.edu/

TAPoR - Research Tools for Textual Study - http://tapor.ca/

Dynamic Table of Contents - http://www.ualbertaprojects.info/dyntoc/dyntoc_v3_5/Main.html

CiteLens - http://labs.fluxo.art.br/CiteLens/

TextTiles - http://dev.giacometti.me/textTiles/trunk/

dialR - http://research.artsrn.ualberta.ca/~dialr/drMain.html

Allison, S., Heuser, R., Jockers, M., Moretti, F. and Witmore, M. (2012 ). Quantitative Formalism: An Experiment.Pamphlet 1, Stanford Literary Lab. Print.

Araya, A. A. (2003). The Hidden Side of Visualization. Techné: Research in Philosophy and Technology; Vol 7, No 2, Print.

Brunet, É. (1989). L'Exploitation des Grands Corpus: Le Bestiare de la Littérature Française. Literary and Linguistic Computing, Vol. 4, No. 2, p. 121-134. Print.

Crane, G. (2006). What Do You Do with a Million Books? D-Lib MagazineVol. 12 No. 3, Print.

Dragland, Å. (2013). Big Data, for better or worse. SINTEF.no. 22 May 2013. Web. 27 Oct. 2013.

Jockers, M. (2013). Macroanalysis : digital methods and literary history - University of Illinois Press, Urbana, Chicago & Springfield.

Kelly, J., Floyer, D., Vellante, D. and Miniman, S. (2013). Big Data Vendor Revenue and Market Forecast 2012-2017. Wikibon.org. 19 Feb. 2013. Web. 28 Oct. 2013.

McCandless, D. (2010). David McCandless: The beauty of data visualization. TED Talks. Web Video.  25 Oct.  2013. 

McCormick, Bruce H., DeFanti, Thomas A., and Brown, Maxine D. (1987). Visualization in Scientific Computing. Computer Graphics 21, 6 (November). New York: Association for Computing Machinery, SIGGRAPH, Print.

McCullough, M. (1996).  Abstracting Craft: The Practiced Digital Hand; Cambridge, MIT Press, Print.

Moretti, F. (2013). The End of the Beginning: A Reply to Christopher Prendergast.Distant Reading. London: Verso, p. 137-158. Print.

Risen, J. and Poitras, L. (2013). NSA Gathers Data on Social Connections of U.S. Citizens, New York Times, 28 Sep. 2013. Web, 27 Oct, 2013.

Simpson J.; Rockwell, G.; Sinclair, S.; Uszkalo, K.; Brown, S.; Dyrbye, A.; Chartier, R.. (2013). Framework for Testing Text Analysis and Mining Tools.Poster presented at the Digital Humanities 2013 conference at the University of Nebraska-Lincoln. Lincoln, Nebraska, USA.

Smith, J. (1978). Computer Criticism.Style. Vol XII, No 4. Print.

Smith, J. (1984). A New Environment For Literary Analysis.Perspectives in Computing 4. 2/3, (1984): 20-31. Print.

Tufte, Edward. (1983). The Visual Display of Quantitative Information; Cheshire, CT: Graphics Press, Print.

University of Nebraska-Lincoln. (2012). By text-mining the classics, UNL prof unearths new literary insights. UNL News Blog. 23 Aug. 2012. Web. 27 Oct. 2013.",txt,This text is republished here with permission from the original rights holder.,,data-exploration;design;interface;text-analysis;visualization,English,"content analysis;corpora and corpus activities;data mining / text mining;interface and user experience design;networks, relationships, graphs;text analysis;visualization",2014-01-01,"“humanists have always been explorers. they sail not on seas of water but on seas of color, sound, and, most especially, words.” — john b. smith, 1984

why is big data such a big deal? modern digital communications are producing and recording data at a feverish rate, with 90% of the world’s recorded data, most of it unstructured, having been produced in just the last two years (dragland, 2013). in addition, more traditional texts are being digitized all the time, and crane (2006) tells us that while the largest academic digital libraries now hold tens of thousands of books, a completed google library will likely have more than ten million. to further smith’s analogy, we are adrift in a sea of data, and we are at risk of floundering.

front page news events like edward snowden’s may 2013 revelations disclosing the secret nsa (america’s national security agency) collection and analysis of massive amounts of ostensibly private data both domestically and internationally, are making big data analytics part of the public lexicon.  major corporations, led by ibm with $1.3 billion in big data revenue in 2012, are scrambling to adopt practices that will allow them to capitalize on the volume of information now being generated.  global big data related revenues in 2012 were $11.6 billion, and are projected to break $18 billion in 2013 (kelly et al.).

while one might shudder to think what the nsa will do with all that information, outside of the world of politics and espionage, what can researchers and academics do with the volume of information now, or at least soon to be at our disposal? matthew jockers asks, “how do we mine them [texts] to find something we don’t already know?” (university of nebraska-lincoln) to utilize such massive corpora and avoid succumbing to the dilemma of what mccormick et al (1987) refer to as “information without interpretation” we need to find the most effective ways for end-users to explore, visualize and interact with big data such that it is both meaningful and understandable, if possible by both the trained and untrained eye. 

big data analytics and computer-supported visualization offer ways to read collections as our cognitive abilities are stretched to their limit with the sheer volume of data available (araya, 2003). however, as franco moretti has pointed out, what we are reading when we use text mining methods and visualizations are really models of the collections. (moretti 2013, p. 157) it is important therefore to survey the visual models emerging and question if they can better be designed to suit humanities exploration. in this paper we therefore propose to look at visualization for text mining in the following ways:

survey text mining visualization in the humanities. who is using text mining and why? what kinds of visualizations do they find compelling and why?
identify and combine commonly presented visualizations for modeling. what visual models could be used for the exploration of large corpora? how could they be combined?  
model interactive prototypes of different combinations of visualizations for exploration. 
surveying: in a poster at dh 2013 we presented a framework of text mining tools that are useful in the humanities. now we will survey the variety of visualizations used to present mining results. we will begin with early discussions of visualization like smith’s “computer criticism” (style 1978, p. 326), where he notes with agreement paul de man’s observation that as late as 1973 there had been no evolution beyond the close reading “techniques of description and interpretation” being used by literary critics since the 1930s or 40s, and suggests “pictorial representation” as one of the potential uses of computer aided text analysis. brunet in a 1989 article talks about exploiting large corpora and provides a number of examples of visualizations. our survey will examine advances in practice and understanding in the intervening thirty plus years, leading to contemporary works like franco moretti’s graphs, maps, and trees, which, as the title suggests, introduces visual models for literary exploration.

our survey pays particular attention to recent text mining projects and tools including “the proceedings of the old bailey”, david l. hoover’s work with cluster analyses at nyu using “minitab”, matt jockers’ topic modeling work in “macroanalysis”, the university of waikato’s “weka”, the open-source visualization tool “gephi”, the umass machine learning tool “mallet”, the research tools for textual study reviewed on “tapor”as well as some of our own inke related projects such as “dynamic table of contents”, “citelens”, “texttiles” and “dialr”. 

identifying and combining: while we recognize that output will assume a variety of formats including for instance heat maps, topographic plots or scatter plots, our survey suggests that text-mining projects commonly use five principal types of visualization:

1. dendrograms, showing data clustering within sets
2. histograms, showing change over time
3. networkdiagrams, showing how entities are connected within a network
4. wordclouds, representing topics of words
5. scatterplots, showing words or parts in an abstract space
visualizations that are presented in print are typically spartan, focusing the attention on the results through careful design. all affordances are removed. the same types of visualizations automatically generated from large data sets, however, tend to be too dense to be useful and have to include affordances if meant to be interactive. simple representative visualizations, like histograms for instance, are insufficient to display complex interrelationships. dendrograms, especially if you are working with massive data sets, quickly become an illegible mass of inter-connectivity. word clouds are good at showing the relative frequency of words in a text or topic, but not at comparing one text or topic to another. network diagrams produce some beautiful results, but suffer from the same difficulties as dendrograms; large data sets quickly lead to illegibility.

“by visualizing information, we turn it into a landscape that you can explore with your eyes, a sort of information map. and when you’re lost in information, an information map is kind of useful.” — david mccandless, 2010

modeling: our research now is looking at ways to increase the exploratory power of visualization of large data sets. used in combination, visualizations can provide otherwise elusive insight and clarity. they can also provide affordances for each other – a histogram can be used to explore a dendrogram. we have developed interactive prototypes using combinations of visualizations, and focusing on not simply allowing, but even encouraging the user to truly explore the (big) data, “function[ing] almost instinctively”, as mccullough (1996) stated, “to serve the process of development”. the more we can encourage users to explore and play with the data, the more likely they are to develop useful insights.


fig. 1: combination of a modified dendrogram showing clustering, and a diachronic timeline of 500 philosophy papers.

figure 1 shows a prototype of a combination of dendrogram and histogram that we developed to visualize the clustering of 500 papers published between 1966 and 2004. the prototype is a combination of a modified dendrogram showing the clusters, and a diachronic visualization displaying the subjects over time. the displayed data is intuitively explorable, and the modified dendrogram is designed to encourage exploration. we developed the r code to prepare the data for interactivity. in voyant we have developed skins that combine scatter plots and histograms, word clouds and histograms, and network diagrams with other tools. again, the tools are open for others to recombine.

to conclude, surveying commonly used graphical representations allowed us to identify commonly used visualizations that humanists find useful. to scale these so that they can be used interactively to explore large data sets we have prototyped combinations, where one visualization can be used to explore another and vice versa. the goal is visualizations that help researchers make sense of big data; visualizations that let us explore the forest, not just the trees so that we can draw accurate and appropriate inferences from the data.

references
the proceedings of the old bailey - http://www.oldbaileyonline.org/

cluster analysis, principal components analysis (pca), and t-testing in minitab - https://files.nyu.edu/dh3/public/clusteranalysis-pca-t-testinginminitab.html

jockers, m. macroanalysis.

weka 3: data mining software in java - http://www.cs.waikato.ac.nz/ml/weka/

gephi - the open graph viz platform - https://gephi.org/

mallet - machine learning for language toolkit - http://mallet.cs.umass.edu/

tapor - research tools for textual study - http://tapor.ca/

dynamic table of contents - http://www.ualbertaprojects.info/dyntoc/dyntoc_v3_5/main.html

citelens - http://labs.fluxo.art.br/citelens/

texttiles - http://dev.giacometti.me/texttiles/trunk/

dialr - http://research.artsrn.ualberta.ca/~dialr/drmain.html

allison, s., heuser, r., jockers, m., moretti, f. and witmore, m. (2012 ). quantitative formalism: an experiment.pamphlet 1, stanford literary lab. print.

araya, a. a. (2003). the hidden side of visualization. techné: research in philosophy and technology; vol 7, no 2, print.

brunet, é. (1989). l'exploitation des grands corpus: le bestiare de la littérature française. literary and linguistic computing, vol. 4, no. 2, p. 121-134. print.

crane, g. (2006). what do you do with a million books? d-lib magazinevol. 12 no. 3, print.

dragland, å. (2013). big data, for better or worse. sintef.no. 22 may 2013. web. 27 oct. 2013.

jockers, m. (2013). macroanalysis : digital methods and literary history - university of illinois press, urbana, chicago & springfield.

kelly, j., floyer, d., vellante, d. and miniman, s. (2013). big data vendor revenue and market forecast 2012-2017. wikibon.org. 19 feb. 2013. web. 28 oct. 2013.

mccandless, d. (2010). david mccandless: the beauty of data visualization. ted talks. web video.  25 oct.  2013. 

mccormick, bruce h., defanti, thomas a., and brown, maxine d. (1987). visualization in scientific computing. computer graphics 21, 6 (november). new york: association for computing machinery, siggraph, print.

mccullough, m. (1996).  abstracting craft: the practiced digital hand; cambridge, mit press, print.

moretti, f. (2013). the end of the beginning: a reply to christopher prendergast.distant reading. london: verso, p. 137-158. print.

risen, j. and poitras, l. (2013). nsa gathers data on social connections of u.s. citizens, new york times, 28 sep. 2013. web, 27 oct, 2013.

simpson j.; rockwell, g.; sinclair, s.; uszkalo, k.; brown, s.; dyrbye, a.; chartier, r.. (2013). framework for testing text analysis and mining tools.poster presented at the digital humanities 2013 conference at the university of nebraska-lincoln. lincoln, nebraska, usa.

smith, j. (1978). computer criticism.style. vol xii, no 4. print.

smith, j. (1984). a new environment for literary analysis.perspectives in computing 4. 2/3, (1984): 20-31. print.

tufte, edward. (1983). the visual display of quantitative information; cheshire, ct: graphics press, print.

university of nebraska-lincoln. (2012). by text-mining the classics, unl prof unearths new literary insights. unl news blog. 23 aug. 2012. web. 27 oct. 2013.",1.0,1.0,Voyant
2122,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,Towards an Archaeology of Text Analysis Tools,,Stéfan Sinclair;Geoffrey Rockwell,"paper, specified ""long paper""","How have text analysis tools in the humanities been imagined in the past? What did humanities computing developers think they were addressing with now dated technologies like punch cards, printed concordances and verbose command languages? Whether the analytic functionality is at the surface, as with Voyant Tools, or embedded at deeper levels, as with the Lucene-powered searching and browsing capabilities of the Old Bailey, the web-based text analysis tools that we use today are very different from the first tentative technologies developed by computing humanists. Following Siegfried Zieliniski's exploration of forgotten media technologies, this paper will look at three forgotten text analysis technologies and how they were introduced by their developers at the time. Specifically we will:

Discuss why is it important to recover forgotten tools and the discourse around these instruments,
Look at how punch cards were used in Roberto Busa’s Index Thomisticus project as a way of understanding data entry,
Look at Glickman’s ideas about custom card output from PRORA, as a way of recovering the importance of output,
Discuss the command language developed by John Smith for interacting with ARRAS, and
Conclude with a more general call for digital humanities archaeology. 
Zieliniski and Media Archaeology
Siegfried Zielinski, in Deep Time of the Media, argues that technology does not evolve smoothly and that we therefore need to look at periods of intense development and then look at the dead ends that get overlooked to understand the history of media technology. In particular he shows how important it is to look at technologies that are not in canonical histories as precursors to “successful” technologies, because they provide insight into the thinking at the time. A study of forgotten technologies can help us understand opportunities and challenges as they were perceived at the time and on their own terms rather than imposing our prejudices. From the 1950s until the early 1990s there was just such a period of technology development around mainframe and personal computer text analysis tools. The tools developed, the challenges they addressed, and the debates around these technologies have largely been forgotten in an age of web-mediated digital humanities. For this reason we recover three important mainframe projects that can help us understand how differently data entry, output and interaction were thought through before born- digital content, output to wall-sized screens, and interaction on a touchscreen. 

Busa and Tasman on Literary Data Processing 
The first case study we will present is about the methods that Father Busa and his collaborator Paul Tasman developed for the Index Thomisticus (Busa could hardly be considered a forgotten figure, but he's often referred to metonymically as a founder of the field, with relatively little attention paid to the specifics of his work and his collaborations). Busa, when reflecting back on the project justified his technical approach as supporting a philological method of research aimed at recapturing the way a past author used words, much as we want to recapture past development. He argued in 1980 that, “The reader should not simply attach to the words he reads the significance they have in his mind, but should try to find out what significance they had in the writer’s mind.” (Busa 1980, p. 83) Concordances could help redirect readers towards the “verbal system of an author” or how the author used words in their time and away from the temptation to interpret the text at hand using contemporary conceptual categories. Concording creates a new text that shows the verbal system, not the doctrine.

Busa’s collaborator Paul Tasman, however, presents a much more prosaic picture of their methodology that focuses on data entry using punch cards so you can actually get concordances of words. He published a paper in 1957 on “Literary Data Processing” in the IBM Journal of Research and Development that focuses on how they prepared their texts accounting for human error and other problems. Tasman writes, “It is evident, of course, that the transcription of the documents in these other fields necessitates special sets of ground rules and codes in order to provide for information retrieval, and the results will depend entirely upon the degree and refinement of coding and the variety of cross referencing desired.” (p. 256) This case study takes us back to a forgotten set of problems (representing text using punch cards) which led to more mature issues in text encoding. In the full presentation we will look closely at the data entry challenges faced by Busa’s team and how they were resolved with the card technology of the time. 

Glickman and Stallman on Printed Interfaces
The second case study we will look at is the development of the PRORA programs at the University of Toronto in the 1960s. PRORA was reviewed in the first issue of CHUM and with the publication of the Manual for the Printing of Literary Texts and Concordances by Computer by the University of Toronto Press in 1966 is one of the first academic analytical tools to be formally published in some fashion. What is particularly interesting, for our purposes, is the discussion in the Manual of how concordances might be printed. Glickman had idiosyncratic ideas about how concordances could be printed as cards for 2-ring binders so that they could be taken out and arranged on a table by users. He was combining binder technology with computing to reimagine the concordance text. Today we no longer think about output to paper as important to tools, and yet that is what the early tools were designed to do as they were not interactive. We will use this case study to recover what at the time was one of the most important features of a concording tool – how it could output something that could be published for others to use. 


Fig. 1: Example of PRORA output from the Manual

Smith and Interaction
One of the first text analysis tools designed to support interactive research was John Smith’s ARRAS. In ARRAS Smith developed a number of ideas about analysis that we now take for granted. ARRAS was interactive in the sense that it was not a batch program that you ran for output. It could generate visualizations and it was explicitly designed to be part of a multi-tasking research environment where you might be switching back and forth between analysis and word processing. Many of these ideas influenced the interactive PC concordancing tools that followed like TACT. In this paper, however, we are not going to focus on all the prescient features of ARRAS, but look at the now rather dated command language which Smith was so proud of. Almost no one uses a command language for text analysis any more; we expect our tools to have graphical user interfaces that provide affordances for direct manipulation. If you need to do something more than what Voyant, Tableau, Lucene, Gephi or Weka let you do, then you learn to program in a language like R or Python. John Smith by contrast, spent a lot of time trying to design a natural command language for ARRAS that humanists would find easy to use and this comes through in his publications on the tool (1984 & 1985). Command languages were, for a while, the way you interacted with such systems and attention to their design could make a difference. Smith tried to develop a command language that was conversational so humanists could learn to use it to explore “vast continents of literature or history or other realms of information, much as our ancestors explored new lands.” (Smith 1984, p. 31) Close commanding for distant reading. 

Conclusions
In the 2013 Busa Award lecture Willard McCarty called us to look to our history and specifically to look at the “incunabular” years before the web when humanists and artists were imagining what could be done. One challenge we face in reanimating this history is that so much of the story is in tools, standards and web sites – instruments difficult to interrogate the way we do texts. This paper looks back at one major thread of development - text analysis tools – not for the entertainment of outdated technology, but recover a way of thinking about technology. We will conclude by discussing other ways back including the need for better documentation about past tools, along the lines of what TAPoR 2.0 is supporting, and the need to preserve tools or at least a record of their usage. 

References
Busa, R. (1980). ""The Annals of Humanities Computing: The Index Thomisticus."" Computers and the Humanities. 14(2): 83-90.

Glickman, Robert Jay, and Gerrit Joseph Staalman. Manual for the Printing of Literary Texts and Concordances by Computer. Toronto: University of Toronto Press, 1966.

Liu, Alan. (2012) “Where is Cultural Criticism in the Digital Humanities.” In Debates in the Digital Humanities. Ed. Matthew K. Gold. University of Minnesota Press. Liu’s essay is online at <http://dhdebates.gc.cuny.edu/debates/part/11>.

Smith, J. B. (1978). ""Computer Criticism."" STYLE XII(4): 326-356.

Smith, J. B. (1984). ""A New Environment For Literary Analysis."" Perspectives in Computing 4(2/3): 20-31.

Smith, J. B. (1985). Arras User's Manual: TR85-036. Chapel Hill, NC, The University of North Carolina at Chapel Hill.

Tasman, P. (1957). ""Literary Data Processing."" IBM Journal of Research and Development 1(3): 249-256.

Zieliniski, Siegfried. (2008) Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. Cambridge, Massachusetts: The MIT Press.",txt,This text is republished here with permission from the original rights holder.,,media archaeology;text analysis,English,history of humanities computing/digital humanities;text analysis,2014-01-01,"how have text analysis tools in the humanities been imagined in the past? what did humanities computing developers think they were addressing with now dated technologies like punch cards, printed concordances and verbose command languages? whether the analytic functionality is at the surface, as with voyant tools, or embedded at deeper levels, as with the lucene-powered searching and browsing capabilities of the old bailey, the web-based text analysis tools that we use today are very different from the first tentative technologies developed by computing humanists. following siegfried zieliniski's exploration of forgotten media technologies, this paper will look at three forgotten text analysis technologies and how they were introduced by their developers at the time. specifically we will:

discuss why is it important to recover forgotten tools and the discourse around these instruments,
look at how punch cards were used in roberto busa’s index thomisticus project as a way of understanding data entry,
look at glickman’s ideas about custom card output from prora, as a way of recovering the importance of output,
discuss the command language developed by john smith for interacting with arras, and
conclude with a more general call for digital humanities archaeology. 
zieliniski and media archaeology
siegfried zielinski, in deep time of the media, argues that technology does not evolve smoothly and that we therefore need to look at periods of intense development and then look at the dead ends that get overlooked to understand the history of media technology. in particular he shows how important it is to look at technologies that are not in canonical histories as precursors to “successful” technologies, because they provide insight into the thinking at the time. a study of forgotten technologies can help us understand opportunities and challenges as they were perceived at the time and on their own terms rather than imposing our prejudices. from the 1950s until the early 1990s there was just such a period of technology development around mainframe and personal computer text analysis tools. the tools developed, the challenges they addressed, and the debates around these technologies have largely been forgotten in an age of web-mediated digital humanities. for this reason we recover three important mainframe projects that can help us understand how differently data entry, output and interaction were thought through before born- digital content, output to wall-sized screens, and interaction on a touchscreen. 

busa and tasman on literary data processing 
the first case study we will present is about the methods that father busa and his collaborator paul tasman developed for the index thomisticus (busa could hardly be considered a forgotten figure, but he's often referred to metonymically as a founder of the field, with relatively little attention paid to the specifics of his work and his collaborations). busa, when reflecting back on the project justified his technical approach as supporting a philological method of research aimed at recapturing the way a past author used words, much as we want to recapture past development. he argued in 1980 that, “the reader should not simply attach to the words he reads the significance they have in his mind, but should try to find out what significance they had in the writer’s mind.” (busa 1980, p. 83) concordances could help redirect readers towards the “verbal system of an author” or how the author used words in their time and away from the temptation to interpret the text at hand using contemporary conceptual categories. concording creates a new text that shows the verbal system, not the doctrine.

busa’s collaborator paul tasman, however, presents a much more prosaic picture of their methodology that focuses on data entry using punch cards so you can actually get concordances of words. he published a paper in 1957 on “literary data processing” in the ibm journal of research and development that focuses on how they prepared their texts accounting for human error and other problems. tasman writes, “it is evident, of course, that the transcription of the documents in these other fields necessitates special sets of ground rules and codes in order to provide for information retrieval, and the results will depend entirely upon the degree and refinement of coding and the variety of cross referencing desired.” (p. 256) this case study takes us back to a forgotten set of problems (representing text using punch cards) which led to more mature issues in text encoding. in the full presentation we will look closely at the data entry challenges faced by busa’s team and how they were resolved with the card technology of the time. 

glickman and stallman on printed interfaces
the second case study we will look at is the development of the prora programs at the university of toronto in the 1960s. prora was reviewed in the first issue of chum and with the publication of the manual for the printing of literary texts and concordances by computer by the university of toronto press in 1966 is one of the first academic analytical tools to be formally published in some fashion. what is particularly interesting, for our purposes, is the discussion in the manual of how concordances might be printed. glickman had idiosyncratic ideas about how concordances could be printed as cards for 2-ring binders so that they could be taken out and arranged on a table by users. he was combining binder technology with computing to reimagine the concordance text. today we no longer think about output to paper as important to tools, and yet that is what the early tools were designed to do as they were not interactive. we will use this case study to recover what at the time was one of the most important features of a concording tool – how it could output something that could be published for others to use. 


fig. 1: example of prora output from the manual

smith and interaction
one of the first text analysis tools designed to support interactive research was john smith’s arras. in arras smith developed a number of ideas about analysis that we now take for granted. arras was interactive in the sense that it was not a batch program that you ran for output. it could generate visualizations and it was explicitly designed to be part of a multi-tasking research environment where you might be switching back and forth between analysis and word processing. many of these ideas influenced the interactive pc concordancing tools that followed like tact. in this paper, however, we are not going to focus on all the prescient features of arras, but look at the now rather dated command language which smith was so proud of. almost no one uses a command language for text analysis any more; we expect our tools to have graphical user interfaces that provide affordances for direct manipulation. if you need to do something more than what voyant, tableau, lucene, gephi or weka let you do, then you learn to program in a language like r or python. john smith by contrast, spent a lot of time trying to design a natural command language for arras that humanists would find easy to use and this comes through in his publications on the tool (1984 & 1985). command languages were, for a while, the way you interacted with such systems and attention to their design could make a difference. smith tried to develop a command language that was conversational so humanists could learn to use it to explore “vast continents of literature or history or other realms of information, much as our ancestors explored new lands.” (smith 1984, p. 31) close commanding for distant reading. 

conclusions
in the 2013 busa award lecture willard mccarty called us to look to our history and specifically to look at the “incunabular” years before the web when humanists and artists were imagining what could be done. one challenge we face in reanimating this history is that so much of the story is in tools, standards and web sites – instruments difficult to interrogate the way we do texts. this paper looks back at one major thread of development - text analysis tools – not for the entertainment of outdated technology, but recover a way of thinking about technology. we will conclude by discussing other ways back including the need for better documentation about past tools, along the lines of what tapor 2.0 is supporting, and the need to preserve tools or at least a record of their usage. 

references
busa, r. (1980). ""the annals of humanities computing: the index thomisticus."" computers and the humanities. 14(2): 83-90.

glickman, robert jay, and gerrit joseph staalman. manual for the printing of literary texts and concordances by computer. toronto: university of toronto press, 1966.

liu, alan. (2012) “where is cultural criticism in the digital humanities.” in debates in the digital humanities. ed. matthew k. gold. university of minnesota press. liu’s essay is online at <http://dhdebates.gc.cuny.edu/debates/part/11>.

smith, j. b. (1978). ""computer criticism."" style xii(4): 326-356.

smith, j. b. (1984). ""a new environment for literary analysis."" perspectives in computing 4(2/3): 20-31.

smith, j. b. (1985). arras user's manual: tr85-036. chapel hill, nc, the university of north carolina at chapel hill.

tasman, p. (1957). ""literary data processing."" ibm journal of research and development 1(3): 249-256.

zieliniski, siegfried. (2008) deep time of the media: toward an archaeology of hearing and seeing by technical means. cambridge, massachusetts: the mit press.",2.0,2.0,Voyant
2147,2014 - Lausanne,Lausanne,Digital Cultural Empowerment,2014,ADHO,ADHO,École Polytechnique Fédérale de Lausanne (EPFL);Université de Lausanne,Lausanne,,Switzerland,https://web.archive.org/web/20161227182033/https://dh2014.org/program/,What is Modeling and What is Not?,,Joris Job Van Zundert;Fotis Jannidis;Johanna Drucker;Ted Underwood;Mike Kestemont;Tara Lee Andrews;Geoffrey Rockwell,panel / roundtable,"In A Companion to Digital Humanities1, Willard McCarty cites Nelson Goodman in saying that the term 'model' can be used to denote ""almost anything from a naked blonde to a quadratic equation"". Indeed the terms 'model' and 'modeling' seem almost painfully polysemous. Nevertheless within Digital Humanities we cannot ignore the terms or the concepts behind them—the notions are inextricably linked to what is one of the core objectives of humanities computing2, namely to render humanities data computationally tractable3 and processable45 to enhance our abilities for analysis. 
In light of the renewed debate on modeling in Digital Humanities6 this panel proposes to investigate how humanists currently understand the role and meaning of modeling, and how we may arrive at an understanding of the term appropriate for humanities research and pedagogy.
McCarty stated a decade ago that the humanities lack a disciplined way of talking about modeling7 which makes it extremely difficult to define the properties and uses of appropriate models for humanities research. Modeling is a commonplace implicit activity in digital humanities, yet our modeling activities are almost never explicitly discussed as such, and it is rarely pointed out that many of our results are in fact models: charts, probabilistic methods, interfaces to the information we structure in databases. This implicitness is attested by our language use. We do not speak of ""modeling an analogy"" or of ""modeling a chart"". We ""make"" or ""create"" them as concrete representations of an implicit and abstract model.
Yet, given the concrete applications and results that can already be seen within the humanities, modeling needs to be a humanities praxis to the same extent as it already is in other scientific fields such as biology and physics. As the social sciences –more specifically the ethnography practices in Science & Technology Studies for instance– show us, praxis by definition can be studied and interrogated for its properties by observing and following its practitioners8. This panel provides a first step in such observant interrogation. 
In the computational domain modeling can be delineated in a narrow mathematical sense where model theory9 defines Turing complete languages as models or instantiations of logic constructed from formulas (i.e. syntax or rules) and signatures (i.e. vocabulary or objects). Thus, computer languages are themselves mathematical models of logic. They provide a layer of expressive logic that in turn allows us to compositionally model data, objects and their relations10. Analogous to the statement made by Peter Robinson about interfaces11, we can argue that such a composition or model expresses an intellectual argument about the real world entities and relations they mimic, capture, or simulate—an intellectual argument that is made on several levels through the computational model and that eventually is communicated to an observer (or user) by way of its interface.
In recent years we find most notably the application of modeling in order to create maps, graphs, trees1213, analogies, diagrams, charts, simulations14, and stylometric analyses15, as well as in discourse analysis, topic modeling, and narrative modeling16. If the successful computational analytical models are quantitatively and statistically founded, does that mean that humanities modeling must necessarily be anchored in the somewhat narrowly defined models that are generally associated with quantification and computer science
More generally, must the concepts of ‘model’ and ‘modeling’ appropriate for Digital Humanities be bound solely by parameters of the mathematical foundations of binary logic? Modeling as activity and concept applies more widely to the humanities than merely in its computational applications. Is it possible to turn around the dynamic of the computational 'stack', so that rather than having mathematics drive humanities computability, the properties of humanistic problems and the data behind them might drive models of computation? We can argue that the goal of any computational approach within the humanities is to render computable the complexity, the abstraction, the ambiguity, the subjectivity, and multiplicity of perspective of the humanities17. Similarly: how do we encompass aspects of modeling present in simulation and (serious) gaming18 of which the humanistic aspects seem to transcend the narrow mathematical connotation of ‘model’? And how does modeling relate to the continuing history of developing and redeveloping digital humanities tools that–rather than merely representing infrastructure–creates a record of intellectual theorizing humanistic computational models?19 How do we break out of the mathematical sandbox defined by first-order logic to do justice to the modalities of humanities? Does this require completely new models for data, logic, and representation? Does it require a general theory of modeling? Even a new symbolic language inspired by the humanities?
This panel brings together some of the most visible practitioners of computational methods within the humanities who have captured analytic models in software code, as well as some of the most influential figures of what might be called 'tacit modeling theory in digital humanities'. We invite them to consider the characteristics of humanities modeling and how those contrast with computational modeling and mathematical modeling, so as to determine what idiosyncrasies modeling might have in a humanities domain. Do these idiosyncrasies allow us to delineate a computationally tractable vocabulary at all? To investigate these questions the panel will discuss and reflect on matters such as…
How do we address the role of modeling and models in the humanities?
How do we ensure that existing mathematical logic does not confine our ability to represent and manipulate humanistic evidence?
What benefits does a definition of modeling appropriated for the humanities hold?
What would a symbolic language for the humanities look like?
What are the standards of evaluation in modeling and do we need specific ones in the Humanities?
What is a useful vocabulary to talk about modeling in a humanities sense?
Panelists

Joris van Zundert (Chair) is in charge of methodological research at the Huygens Institute for the History of the Netherlands. Next to his research in computational humanities he is interested in exchanges between digital humanities and science and technology studies (STS).
Tara L. Andrews has implemented a digital workbench for the fully computational stemmatic analysis of text traditions (http://www.digitalbyzantinist.org/2012/09/announcing-stemmaweb.html). As an assistant professor of digital humanities she is currently developing and teaching a curriculum that emphasizes modeling and algorithmic approaches to humanistic analysis
Johanna Drucker vehemently called attention to the properties of humanities data that are normally neglected by mathematical and conventional computational models and analyses. She has argued that all data are in fact capta and that naïve approaches to statistics are at risk of defining all data as intrinsically quantitative
Fotis Jannidis is developing a white paper on modeling in digital humanities, a version of which will be included in the new edition of the Companion to Digital Humanities. He is a  member of the TEI consortium–most notably as the Chair of the Genetic Edition Encoding Special Interest Group. TEI can be designated the only de facto standard for text structure modeling and encoding
Mike Kestemont specializes in stylometry and together with the Computational Stylistics Group (https://sites.google.com/site/computationalstylistics/home) has developed ""Stylo"", a software package in the R statistical programming language. He is an expert of statistical models expressed through computer algorithms and applied to literature stud
Geoffrey Rockwell conceptualized a number of highly visible tools for text analyses (e.g. Voyant: http://voyant-tools.org). He is finalizing a book demonstrating amongst others how the hermeneutic and theoretical aspects of text analysis models in the form of tool development transcends mere IT mathematics and infrastructure.
Michael Sperberg-McQueen is a markup specialist by profession and was co-editor of the Extensible Markup Language (XML) specification, chair of the XML Schema working group, as well as heavily involved with the Text Encoding Initiative (TEI)
Ted Underwood works at the interface of literary history and machine learning and is particularly interested in using Bayesian statistics to develop models that reason about uncertainty in a principled way. He maintains an influential blog on his experiences in computational humanities (http://tedunderwood.com/).  
Organization of the panel

The primary selection criterion for the panelists is their expertise, but care has been taken to balance the panel as much as possible for age, gender, field, and region. The panel session will be organized as follows
The Chair will introduce the panel’s topic, discussion questions, and the panelists (10 minutes);
Each of the panelists will give a definition of modeling as a 1 minute provocative pitch (10 minutes);
An open forum between the panelists and the audience follows (60 minutes);
A circular setting of seats with panelists distributed among the attendees will be used to enhance audience participation in the discussion;
The panel discussion will be audio recorded, concise conclusions will be published to the web.
Further Reading

Checkland, P. & Holwell, S., 1998. Information, Systems, and Information Systems: Making Sense of the Field. Chichester: John Wiley & Sons, Ltd.
Davis, M., 2012. The Universal Computer: The Road From Leibniz to Turing. New York: CRC Press.
Mahoney, M.S., 2011. Histories of Computing. T. Haigh (ed.),Cambridge: Harvard University Press.
Hayles, K.N., 2012. How We Think: Digital Media and Contemporary Technogenesis. Chicago: University of Chicago Press.
Ramsay, Stephen, 2011. Reading Machines: Toward an Algorithmic Criticism (Topics in the Digital Humanities). Chicago: University of Illinois Press.
References

1. Schreibman, Susan, Raymond George Siemens, and John M. Unsworth (2004). A Companion to Digital Humanities. Wiley-Blackwell.
2. Unsworth, J., (2002). What Is Humanities Computing And What Is Not? G. Braungart, P. Gendolla, & F. Jannidis, eds. Jahrbuch für Computerphilologie, 4. Available at: computerphilologie.digital-humanities.de/jg02/unsworth.html (Accessed July 8, 2013).
3. Mccarty, W. (2005). Humanities Computing, New York: Palgrave MacMillan.
4. Unsworth, J., (2002). What Is Humanities Computing And What Is Not? G. Braungart, P. Gendolla, & F. Jannidis, eds. Jahrbuch für Computerphilologie, 4. Available at: computerphilologie.digital-humanities.de/jg02/unsworth.html (Accessed July 8, 2013).
5. Orlandi, T., The Scholarly Environment of Humanities Computing, A Reaction to Willard McCarty’s talk on The computational transformation of the humanities. Available at: rmcisadu.let.uniroma1.it/~orlandi/mccarty1.html (Accessed May 7, 2012).
6. Flanders, J. & Jannidis, F., (2012). Panel Discussion: Data Models in Humanities Theory and Practice, Providence (US). Available at: youtu.be/lHJmPT-VjPE (Accessed November 1, 2013).
7. McCarty, W., (2004). Modeling: A Study in Words and Meanings. In S. Schreibman, R. Siemens, & J. Unsworth, eds. A Companion to Digital Humanities. Oxford: Blackwell. Available at: www.digitalhumanities.org/companion/.
8. Kaptelinin, V. & Nardi, B.A., (2006). Acting with technology: activity theory and interaction design, Cambridge, MA, USA/London UK: MIT Press.
9. Rautenberg, W., (2009). A Concise Introduction to Mathematical Logic 3rd ed., Available at: page.mi.fu-berlin.de/raut/logic3/announce.pdf.
10. Forbus, K.D., (2008). Qualitative Modeling. In F. van Harmelen, V. Lifschitz, & B. Porter, eds. Handbook of Knowledge Representation. Foundations of Artificial Intelligence. Amsterdam, Boston, Heidelberg etc.: Elsevier, pp. 361–394.
11. Robinson, P., (2013). Five desiderata for scholarly editions in digital form. In Digital Humanities Conference 2013. Lincoln (NB, USA). Available at: dh2013.unl.edu/abstracts/ab-314.html.
12. Moretti, F. (2007). Maps, Graphs, and Trees: Abstract Models for Literary History. London: Verso.
13. Jockers, M. (2013). Macroanalysis: Digital Methods and Literary History. University of Illinois Press.
14. Mccarty, W. (2005). Humanities Computing, New York: Palgrave MacMillan.
15. Hoover, D.L., (2012). The Excel Text-Analysis Page: A Collection of Microsoft Excel © spreadsheets with macros, in the service of text-analysis. Available at: files.nyu.edu/dh3/public/The%20Excel%20Text-Analysis%20Pages.html (Accessed October 14, 2013).
16. Meister, J.C. & Gertz, M., (2013). heureCLÉA, collaborative literature exploration & annotation. heureCLÉA | Tools. Available at: heureclea.de/tools/.
17. Drucker, J., (2011). Humanities Approaches to Graphical Display. Digital Humanities Quarterly, 5(1). Available at: digitalhumanities.org/dhq/vol/5/1/000091/000091.html (Accessed August 24, 2012).
18. Bogdanovych, A., Cohen, A. & Roper, M., (2009). The City of Uruk: Virtual Instituions in Cultural Heritage. In Proceedings of the HCSNet 2009 Workshop on Interacting with Intelligent Virtual Characters. HCSNet 2009 Workshop on Interacting with Intelligent Virtual Characters. Sydney. Available at: www-staff.it.uts.edu.au/~anton/Publications/HCSNet09.pdf.
19. Ramsay, S. & Rockwell, G., (2012). Developing Things: Notes toward an Epistemology of Building in the Digital Humanities. In Debates in Digital Humanities. University of Minnesota Press. Available at: dhdebates.gc.cuny.edu/debates/text/11.",txt,This text is republished here with permission from the original rights holder.,,computational;humanistic;model;modeling;theory,English,content analysis;data modeling and architecture including hypothesis-driven modeling;digital humanities - nature and significance;information architecture;knowledge representation,2014-01-01,"in a companion to digital humanities1, willard mccarty cites nelson goodman in saying that the term 'model' can be used to denote ""almost anything from a naked blonde to a quadratic equation"". indeed the terms 'model' and 'modeling' seem almost painfully polysemous. nevertheless within digital humanities we cannot ignore the terms or the concepts behind them—the notions are inextricably linked to what is one of the core objectives of humanities computing2, namely to render humanities data computationally tractable3 and processable45 to enhance our abilities for analysis. 
in light of the renewed debate on modeling in digital humanities6 this panel proposes to investigate how humanists currently understand the role and meaning of modeling, and how we may arrive at an understanding of the term appropriate for humanities research and pedagogy.
mccarty stated a decade ago that the humanities lack a disciplined way of talking about modeling7 which makes it extremely difficult to define the properties and uses of appropriate models for humanities research. modeling is a commonplace implicit activity in digital humanities, yet our modeling activities are almost never explicitly discussed as such, and it is rarely pointed out that many of our results are in fact models: charts, probabilistic methods, interfaces to the information we structure in databases. this implicitness is attested by our language use. we do not speak of ""modeling an analogy"" or of ""modeling a chart"". we ""make"" or ""create"" them as concrete representations of an implicit and abstract model.
yet, given the concrete applications and results that can already be seen within the humanities, modeling needs to be a humanities praxis to the same extent as it already is in other scientific fields such as biology and physics. as the social sciences –more specifically the ethnography practices in science & technology studies for instance– show us, praxis by definition can be studied and interrogated for its properties by observing and following its practitioners8. this panel provides a first step in such observant interrogation. 
in the computational domain modeling can be delineated in a narrow mathematical sense where model theory9 defines turing complete languages as models or instantiations of logic constructed from formulas (i.e. syntax or rules) and signatures (i.e. vocabulary or objects). thus, computer languages are themselves mathematical models of logic. they provide a layer of expressive logic that in turn allows us to compositionally model data, objects and their relations10. analogous to the statement made by peter robinson about interfaces11, we can argue that such a composition or model expresses an intellectual argument about the real world entities and relations they mimic, capture, or simulate—an intellectual argument that is made on several levels through the computational model and that eventually is communicated to an observer (or user) by way of its interface.
in recent years we find most notably the application of modeling in order to create maps, graphs, trees1213, analogies, diagrams, charts, simulations14, and stylometric analyses15, as well as in discourse analysis, topic modeling, and narrative modeling16. if the successful computational analytical models are quantitatively and statistically founded, does that mean that humanities modeling must necessarily be anchored in the somewhat narrowly defined models that are generally associated with quantification and computer science
more generally, must the concepts of ‘model’ and ‘modeling’ appropriate for digital humanities be bound solely by parameters of the mathematical foundations of binary logic? modeling as activity and concept applies more widely to the humanities than merely in its computational applications. is it possible to turn around the dynamic of the computational 'stack', so that rather than having mathematics drive humanities computability, the properties of humanistic problems and the data behind them might drive models of computation? we can argue that the goal of any computational approach within the humanities is to render computable the complexity, the abstraction, the ambiguity, the subjectivity, and multiplicity of perspective of the humanities17. similarly: how do we encompass aspects of modeling present in simulation and (serious) gaming18 of which the humanistic aspects seem to transcend the narrow mathematical connotation of ‘model’? and how does modeling relate to the continuing history of developing and redeveloping digital humanities tools that–rather than merely representing infrastructure–creates a record of intellectual theorizing humanistic computational models?19 how do we break out of the mathematical sandbox defined by first-order logic to do justice to the modalities of humanities? does this require completely new models for data, logic, and representation? does it require a general theory of modeling? even a new symbolic language inspired by the humanities?
this panel brings together some of the most visible practitioners of computational methods within the humanities who have captured analytic models in software code, as well as some of the most influential figures of what might be called 'tacit modeling theory in digital humanities'. we invite them to consider the characteristics of humanities modeling and how those contrast with computational modeling and mathematical modeling, so as to determine what idiosyncrasies modeling might have in a humanities domain. do these idiosyncrasies allow us to delineate a computationally tractable vocabulary at all? to investigate these questions the panel will discuss and reflect on matters such as…
how do we address the role of modeling and models in the humanities?
how do we ensure that existing mathematical logic does not confine our ability to represent and manipulate humanistic evidence?
what benefits does a definition of modeling appropriated for the humanities hold?
what would a symbolic language for the humanities look like?
what are the standards of evaluation in modeling and do we need specific ones in the humanities?
what is a useful vocabulary to talk about modeling in a humanities sense?
panelists

joris van zundert (chair) is in charge of methodological research at the huygens institute for the history of the netherlands. next to his research in computational humanities he is interested in exchanges between digital humanities and science and technology studies (sts).
tara l. andrews has implemented a digital workbench for the fully computational stemmatic analysis of text traditions (http://www.digitalbyzantinist.org/2012/09/announcing-stemmaweb.html). as an assistant professor of digital humanities she is currently developing and teaching a curriculum that emphasizes modeling and algorithmic approaches to humanistic analysis
johanna drucker vehemently called attention to the properties of humanities data that are normally neglected by mathematical and conventional computational models and analyses. she has argued that all data are in fact capta and that naïve approaches to statistics are at risk of defining all data as intrinsically quantitative
fotis jannidis is developing a white paper on modeling in digital humanities, a version of which will be included in the new edition of the companion to digital humanities. he is a  member of the tei consortium–most notably as the chair of the genetic edition encoding special interest group. tei can be designated the only de facto standard for text structure modeling and encoding
mike kestemont specializes in stylometry and together with the computational stylistics group (https://sites.google.com/site/computationalstylistics/home) has developed ""stylo"", a software package in the r statistical programming language. he is an expert of statistical models expressed through computer algorithms and applied to literature stud
geoffrey rockwell conceptualized a number of highly visible tools for text analyses (e.g. voyant: http://voyant-tools.org). he is finalizing a book demonstrating amongst others how the hermeneutic and theoretical aspects of text analysis models in the form of tool development transcends mere it mathematics and infrastructure.
michael sperberg-mcqueen is a markup specialist by profession and was co-editor of the extensible markup language (xml) specification, chair of the xml schema working group, as well as heavily involved with the text encoding initiative (tei)
ted underwood works at the interface of literary history and machine learning and is particularly interested in using bayesian statistics to develop models that reason about uncertainty in a principled way. he maintains an influential blog on his experiences in computational humanities (http://tedunderwood.com/).  
organization of the panel

the primary selection criterion for the panelists is their expertise, but care has been taken to balance the panel as much as possible for age, gender, field, and region. the panel session will be organized as follows
the chair will introduce the panel’s topic, discussion questions, and the panelists (10 minutes);
each of the panelists will give a definition of modeling as a 1 minute provocative pitch (10 minutes);
an open forum between the panelists and the audience follows (60 minutes);
a circular setting of seats with panelists distributed among the attendees will be used to enhance audience participation in the discussion;
the panel discussion will be audio recorded, concise conclusions will be published to the web.
further reading

checkland, p. & holwell, s., 1998. information, systems, and information systems: making sense of the field. chichester: john wiley & sons, ltd.
davis, m., 2012. the universal computer: the road from leibniz to turing. new york: crc press.
mahoney, m.s., 2011. histories of computing. t. haigh (ed.),cambridge: harvard university press.
hayles, k.n., 2012. how we think: digital media and contemporary technogenesis. chicago: university of chicago press.
ramsay, stephen, 2011. reading machines: toward an algorithmic criticism (topics in the digital humanities). chicago: university of illinois press.
references

1. schreibman, susan, raymond george siemens, and john m. unsworth (2004). a companion to digital humanities. wiley-blackwell.
2. unsworth, j., (2002). what is humanities computing and what is not? g. braungart, p. gendolla, & f. jannidis, eds. jahrbuch für computerphilologie, 4. available at: computerphilologie.digital-humanities.de/jg02/unsworth.html (accessed july 8, 2013).
3. mccarty, w. (2005). humanities computing, new york: palgrave macmillan.
4. unsworth, j., (2002). what is humanities computing and what is not? g. braungart, p. gendolla, & f. jannidis, eds. jahrbuch für computerphilologie, 4. available at: computerphilologie.digital-humanities.de/jg02/unsworth.html (accessed july 8, 2013).
5. orlandi, t., the scholarly environment of humanities computing, a reaction to willard mccarty’s talk on the computational transformation of the humanities. available at: rmcisadu.let.uniroma1.it/~orlandi/mccarty1.html (accessed may 7, 2012).
6. flanders, j. & jannidis, f., (2012). panel discussion: data models in humanities theory and practice, providence (us). available at: youtu.be/lhjmpt-vjpe (accessed november 1, 2013).
7. mccarty, w., (2004). modeling: a study in words and meanings. in s. schreibman, r. siemens, & j. unsworth, eds. a companion to digital humanities. oxford: blackwell. available at: www.digitalhumanities.org/companion/.
8. kaptelinin, v. & nardi, b.a., (2006). acting with technology: activity theory and interaction design, cambridge, ma, usa/london uk: mit press.
9. rautenberg, w., (2009). a concise introduction to mathematical logic 3rd ed., available at: page.mi.fu-berlin.de/raut/logic3/announce.pdf.
10. forbus, k.d., (2008). qualitative modeling. in f. van harmelen, v. lifschitz, & b. porter, eds. handbook of knowledge representation. foundations of artificial intelligence. amsterdam, boston, heidelberg etc.: elsevier, pp. 361–394.
11. robinson, p., (2013). five desiderata for scholarly editions in digital form. in digital humanities conference 2013. lincoln (nb, usa). available at: dh2013.unl.edu/abstracts/ab-314.html.
12. moretti, f. (2007). maps, graphs, and trees: abstract models for literary history. london: verso.
13. jockers, m. (2013). macroanalysis: digital methods and literary history. university of illinois press.
14. mccarty, w. (2005). humanities computing, new york: palgrave macmillan.
15. hoover, d.l., (2012). the excel text-analysis page: a collection of microsoft excel © spreadsheets with macros, in the service of text-analysis. available at: files.nyu.edu/dh3/public/the%20excel%20text-analysis%20pages.html (accessed october 14, 2013).
16. meister, j.c. & gertz, m., (2013). heurecléa, collaborative literature exploration & annotation. heurecléa | tools. available at: heureclea.de/tools/.
17. drucker, j., (2011). humanities approaches to graphical display. digital humanities quarterly, 5(1). available at: digitalhumanities.org/dhq/vol/5/1/000091/000091.html (accessed august 24, 2012).
18. bogdanovych, a., cohen, a. & roper, m., (2009). the city of uruk: virtual instituions in cultural heritage. in proceedings of the hcsnet 2009 workshop on interacting with intelligent virtual characters. hcsnet 2009 workshop on interacting with intelligent virtual characters. sydney. available at: www-staff.it.uts.edu.au/~anton/publications/hcsnet09.pdf.
19. ramsay, s. & rockwell, g., (2012). developing things: notes toward an epistemology of building in the digital humanities. in debates in digital humanities. university of minnesota press. available at: dhdebates.gc.cuny.edu/debates/text/11.",1.0,2.0,Voyant
2239,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,DREaM: Distant Reading Early Modernity,https://github.com/ADHO/dh2015/blob/master/xml/WITTEK_Stephen_DREaM__Distant_Reading_Early_Modernity.xml,Stephen Wittek;Stéfan Sinclair;Matthew Milner,"paper, specified ""short paper""","<?xml version=""1.0"" encoding=""UTF-8""?>
<TEI xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>DREaM: Distant Reading Early Modernity</title>
                <author>
                    <persName>
                        <surname>Wittek</surname>
                        <forename>Stephen</forename>
                    </persName>
                    <affiliation>McGill University, Canada</affiliation>
                    <email>stephen.wittek@mcgill.ca</email>
                </author>
                <author>
                    <persName>
                        <surname>Sinclair</surname>
                        <forename>Stéfan</forename>
                    </persName>
                    <affiliation>McGill University, Canada</affiliation>
                    <email>stefan.sinclair@mcgill.ca</email>
                </author>
                <author>
                    <persName>
                        <surname>Milner</surname>
                        <forename>Matthew</forename>
                    </persName>
                    <affiliation>McGill University, Canada</affiliation>
                    <email>matthew.milner@mcgill.ca</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident=""DHCONVALIDATOR"" version=""1.9"">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme=""ConfTool"" n=""category"">
                    <term>Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""subcategory"">
                    <term>Short Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""keywords"">
                    <term>early modern</term>
                    <term>eebo</term>
                    <term>voyant</term>
                    <term>topic modeling</term>
                    <term>distant reading</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""topics"">
                    <term>archives</term>
                    <term>repositories</term>
                    <term>sustainability and preservation</term>
                    <term>corpora and corpus activities</term>
                    <term>literary studies</term>
                    <term>content analysis</term>
                    <term>bibliographic methods / textual studies</term>
                    <term>interdisciplinary collaboration</term>
                    <term>digital humanities - pedagogy and curriculum</term>
                    <term>english studies</term>
                    <term>renaissance studies</term>
                    <term>media studies</term>
                    <term>data mining / text mining</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>Our proposed paper will provide an overview of the theory and methodology driving the creation of Distant Reading Early Modernity (DREaM), a digital humanities project that has made a massive corpus of early modern texts amenable for use with macro-scale analytical tools. Key focus areas include the technical challenges deriving from non-standardized spelling, the philosophy of our tutorial program, the argument for our approach to the early modern archive, and the potential benefit to early modern scholarship of distant reading techniques. </p>
            <p>From Microfilm Library, to EEBO, to EEBO-TCP, to DREaM </p>
            <p>The foundational work for DREaM began in 1934, when Eugene B. Power used parts of two movie and still cameras to create one of the world’s first microfilm bookcameras, a device he used to photograph thousands of texts in British libraries (Anderson and Power, 1990). In 1998 Power’s microfilm library became the basis for Early English Books Online (EEBO), a database that comprises the images for some 125,000 texts from 1475 to 1700, and has profoundly expanded the horizons of early modern research.
                <hi rend=""superscript"">1</hi> To date, approximately one-third of the documents on EEBO are available as transcribed, full-text editions. Researchers for the EEBO Text Creation Partnership (EEBO-TCP) are currently working to transcribe the remaining 85,000 documents, which are as yet only available as digitized microfilm images.
                <hi rend=""superscript"">2</hi>
            </p>
            <p>Although completion of the transcription work is still at least 10 years in the future, the prospect of a full-text library of all documents from the first 225 years of English print points to the need for some careful re-thinking about the relation between scholarship and archival sources. As it now stands, the EEBO-TCP corpus amounts to 8.02 gigabytes of XML-encoded text and contains nearly 45,000 documents, for a grand total of well over a billion words (1,155,264,343 by our count). Confronted by the sheer expanse of a corpus several magnitudes larger than anything one could hope to read in a lifetime, early modern scholarship must now work to incorporate digital methodologies that enable a bird’s-eye view of large corpora, an approach that Franco Moretti has dubbed ‘distant reading’ (Moretti, 2007). DREaM has begun the work of making such a view possible. </p>
            <p>Unlike EEBO, DREaM enables batch downloading of custom-defined subsets rather than obliging users to download individual texts on a one-by-one basis. In other words, it functions at the level of ‘sets of texts’ (sometimes called 
                <hi rend=""italic"">worksets</hi>) rather than ‘individual texts’. Examples of subsets one might potentially generate include ‘all texts by Ben Jonson’, ‘all texts published in 1623’, or ‘all texts printed by John Wolfe’. A user-friendly interface makes subsets available as either plain text or XML-encoded files, and gives users the option to automatically name individual files by date, author, title, or combinations thereof (this file naming flexibility can be useful when interoperating with other tool suites). 
            </p>
            <p>The ability to generate custom-defined subsets is important because it allows researchers to explore the early modern canon with distant reading techniques, and to capture otherwise intractable data with visualizations such as graphs, charts, or other forms of graphic representation. On this note, another key feature of DREaM is that it allows users to transfer specially tailored subsets directly to the analytic interfaces of Voyant Tools (voyant-tools.org), a suite of textual visualization tools that collectively constitute the leading platform for open-access digital humanities research.
                <hi rend=""superscript"">3</hi> In fact, DREaM is actually implemented within Voyant Tools (version 2.0, not yet released, which provides much better support for very large text collections). DREaM thus provides a compelling example of a bridge between massive full-text repositories (that typically provide faceted searching) and more specialized analytic and visualization environments. By enabling simple transference between the EEBO-TCP archive and Voyant, DREaM has significantly expanded the range and sophistication of technologies currently available to researchers who wish to gain a broad sense of printed matter in early modern England. 
            </p>
            <p>Notably, however, DREaM does not aim to replace EEBO, or to supplant conventional forms of research. Rather, our goal is to simply add a new item to the scholar’s toolbox, and to increase transferability between distant reading methodologies and more fine-grained forms of analysis. </p>
            <p>
                <hi rend=""bold"">Negotiating the Complexities of Non-Standardized Spelling</hi>
            </p>
            <p>Standardized spelling had yet to emerge in early modernity: writers had the freedom to spell however they pleased. To take a famous example, the name ‘Shakespeare’ has 80 different recorded spellings, including ‘Shaxpere’ and ‘Shaxberd’. As one might imagine, variance on this scale presents a serious challenge for large-scale textual analysis. How is it possible to track the incidence of a specific word, or group of words, if any given word could have an unknown multiplicity of iterations? </p>
            <p>To address this problem, we enlisted the assistance of VARD 2, a tool that helps to improve the accuracy of textual analysis by finding candidate modern form replacements for spelling variants in historical texts.
                <hi rend=""superscript"">4</hi> As with conventional spellcheckers, a user can choose to process texts manually (selecting a candidate replacement offered by the system), automatically (allowing the system to use the best candidate replacement found), or semi-automatically (training the tool on a sample of the corpora). 
            </p>
            <p>After some preliminary training, we ran the TCP-EEBO corpus through VARD using the default settings (auto normalization at a threshold of 50%). Rather than using the ‘batch’ mode—which proved unreliable for such a big job—we wrote a script that normalized the texts on a one-by-one basis from the command-line. This process took about three days on a commodity machine. VARD normalized 80,676 terms for a grand total of 44,909,676 changes overall. </p>
            <p>A careful check through the list resulted in 373 term normalizations that we found problematic in one way or another. The problematic normalizations amounted to 462,975 changes overall, or only 1.03% of the total number of changes. These results were satisfactory: our goal was not to make the corpus ‘perfectly normalized’ (an impossibility, not least because perfection is debatable in this context), but, more pragmatically, to make it generally normalized, which is the best one can reasonably expect from an automatic process. On this point, it is important to note that VARD encodes a record of all changes within the output XML file, so scholars will be able to see if the program has made an erroneous normalization. </p>
            <p>Some of the problematic VARD normalizations seem to have derived from a dictionary error. For example, ‘chan’ became ‘champion’ and ‘ged’ became ‘general’. In other instances, the problematic normalizations were ambiguous or borderline cases that we preferred to simply leave unchanged. Examples include ‘piece’ for ‘peece’, and ‘land’ for ‘iland’. There were also cases where the replacement term was not quite correct: ‘strawberie’ became ‘strawy’ rather than ‘strawberry’, and ‘hoouering’ became ‘hoovering’ rather than ‘hovering’. We fixed as many of these kinks as we could by making adjustments to the VARD training file and running the entire corpus through the normalization process a second time. </p>
            <p>Of course, it is not difficult to imagine scenarios wherein a researcher may prefer to work with original spellings rather than normalized texts. With such projects in mind, we have kept both normalized and non-normalized versions of the EEBO-TCP corpus. </p>
            <p>
                <hi rend=""bold"">The DREaM Tutorial Program</hi>
            </p>
            <p>As noted above, one of the central objectives of DREaM is to create an interface that will maximize user-friendliness, allowing scholars with a minimal level of technical expertise to quickly and efficiently create subsets tailored for whatever specific research question they wish to pursue. We are building DREaM for our own research, but we also have a much broader pedagogical perspective in mind. To meet this objective, we have launched a pilot tutorial program, currently under way, that will teach scholars how to use DREaM, but will also point to ways in which DREaM could more effectively serve the demands of scholarly investigation. </p>
            <p>In a series of tasks that build toward the production of a short case-study report, pilot users must articulate a detailed research question and provide a description of their argument. In addition to establishing a valuable feedback loop for the project, this assignment aims to nudge new users toward a more comprehensive, more practical understanding of how macro-scale textual analysis can complement scholarly practice. The key conceptual challenge, as we see it, hinges on new users’ ability to understand, and learn to negotiate, the gap between distant reading and more conventional means of engaging archival sources. </p>
            <p>Our pool of pilot users derives from the membership of our parent project, Early Modern Conversions, a five-year interdisciplinary research initiative that has brought together a team of more than 100 scholars, partners, and graduate student associates from universities in Canada, the United States, England, New Zealand, and Australia.
                <hi rend=""superscript"">5</hi> Early Modern Conversions provides a propitious testing ground for DREaM because it is at the vanguard of early modern research, and because it entails a rich diversity of disciplinary approaches. Our presentation for DH2015 will report on the results of the tutorial program and on the progress of the project overall. 
            </p>
            <p>Screenshots </p>
            <figure>
                <graphic n=""1001"" width=""16.002cm"" height=""12.230805555555555cm"" url=""Pictures/image1.png"" rend=""block""/>
            </figure>
            <p>Figure 1. The DREaM interface. Search fields in the middle of the screen enable users to define a subset of EEBO-TCP texts by keyword, year, author, and publisher. Below the search field, an ‘Export’ button opens a dialogue box that offers the option of sending the subset directly to Voyant-tools.org, or downloading it as a ZIP archive. Users may also choose to download subsets as either plain text or XML-encoded files. A drag-and-drop mechanism (bottom) enables automatic naming of files within a subset by date, author, title, or combinations thereof. </p>
            <p>
                <pb/>
            </p>
            <figure>
                <graphic n=""1002"" width=""16.002cm"" height=""8.651875cm"" url=""Pictures/image2.png"" rend=""block""/>
            </figure>
            <p>Figure 2. A sample subset transferred to Voyant Tools. Beginning in the top left corner, one sees a word cloud representing the frequency of keywords in terms of font size. At a glance, it shows that the highest frequency words in the subset are ‘good’ and ‘come’. Below the word cloud, there is a summary that lists statistics for basic categories such as word count, vocabulary density, word frequency, etc. In addition, the summary lists words that have a notably high frequency for each year: ‘Rome’ and ‘death’ appeared with particular frequency in 1594, while ‘virtue’ and ‘envy’ stood out in 1612. Moving to the bottom left corner, one sees an ordered list of frequencies for each word in the corpus accompanied by a thumbnail graph that tracks the frequency of words over the 40-year delimitation. At a glance, the tool shows a significant spike for the word ‘knight’ in 1624. In the middle of the screen, a ‘Corpus Reader’ tool enables users to drill down into the corpus to examine the context for particular terms. </p>
            <p>Notes</p>
            <p>1. See http://eebo.chadwyck.com.</p>
            <p>2. See http://eebo.odl.ox.ac.uk/e/eebo/.</p>
            <p>3. See http://voyant-tools.org.</p>
            <p>4. See http://ucrel.lancs.ac.uk/vard/about/.</p>
            <p>5. See http://earlymodernconversions.com.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Anderson, R. and Power, E. B.</hi> (1990). The Autobiography of Eugene B. Power, Founder of University Microfilms. UMI, Ann Arbor, MI.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Moretti, F.</hi> (2007). Graphs, Maps, Trees: Abstract Models for Literary History. Verso, London.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>",xml,Creative Commons Attribution 4.0 International,,distant reading;early modern;eebo;topic modeling;voyant,English,"archives, repositories, sustainability and preservation;bibliographic methods / textual studies;content analysis;corpora and corpus activities;data mining / text mining;digital humanities - pedagogy and curriculum;english;english studies;interdisciplinary collaboration;literary studies;media studies;renaissance studies",2015-01-01,"<?xml version=""1.0"" encoding=""utf-8""?>
<tei xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiheader>
        <filedesc>
            <titlestmt>
                <title>dream: distant reading early modernity</title>
                <author>
                    <persname>
                        <surname>wittek</surname>
                        <forename>stephen</forename>
                    </persname>
                    <affiliation>mcgill university, canada</affiliation>
                    <email>stephen.wittek@mcgill.ca</email>
                </author>
                <author>
                    <persname>
                        <surname>sinclair</surname>
                        <forename>stéfan</forename>
                    </persname>
                    <affiliation>mcgill university, canada</affiliation>
                    <email>stefan.sinclair@mcgill.ca</email>
                </author>
                <author>
                    <persname>
                        <surname>milner</surname>
                        <forename>matthew</forename>
                    </persname>
                    <affiliation>mcgill university, canada</affiliation>
                    <email>matthew.milner@mcgill.ca</email>
                </author>
            </titlestmt>
            <editionstmt>
                <edition>
                    <date>2014-12-19t13:50:00z</date>
                </edition>
            </editionstmt>
            <publicationstmt>
                <publisher>paul arthur, university of western sidney</publisher>
                <address>
                    <addrline>locked bag 1797</addrline>
                    <addrline>penrith nsw 2751</addrline>
                    <addrline>australia</addrline>
                    <addrline>paul arthur</addrline>
                </address>
            </publicationstmt>
            <sourcedesc>
                <p>converted from a word document </p>
            </sourcedesc>
        </filedesc>
        <encodingdesc>
            <appinfo>
                <application ident=""dhconvalidator"" version=""1.9"">
                    <label>dhconvalidator</label>
                </application>
            </appinfo>
        </encodingdesc>
        <profiledesc>
            <textclass>
                <keywords scheme=""conftool"" n=""category"">
                    <term>paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""subcategory"">
                    <term>short paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""keywords"">
                    <term>early modern</term>
                    <term>eebo</term>
                    <term>voyant</term>
                    <term>topic modeling</term>
                    <term>distant reading</term>
                </keywords>
                <keywords scheme=""conftool"" n=""topics"">
                    <term>archives</term>
                    <term>repositories</term>
                    <term>sustainability and preservation</term>
                    <term>corpora and corpus activities</term>
                    <term>literary studies</term>
                    <term>content analysis</term>
                    <term>bibliographic methods / textual studies</term>
                    <term>interdisciplinary collaboration</term>
                    <term>digital humanities - pedagogy and curriculum</term>
                    <term>english studies</term>
                    <term>renaissance studies</term>
                    <term>media studies</term>
                    <term>data mining / text mining</term>
                    <term>english</term>
                </keywords>
            </textclass>
        </profiledesc>
    </teiheader>
    <text>
        <body>
            <p>our proposed paper will provide an overview of the theory and methodology driving the creation of distant reading early modernity (dream), a digital humanities project that has made a massive corpus of early modern texts amenable for use with macro-scale analytical tools. key focus areas include the technical challenges deriving from non-standardized spelling, the philosophy of our tutorial program, the argument for our approach to the early modern archive, and the potential benefit to early modern scholarship of distant reading techniques. </p>
            <p>from microfilm library, to eebo, to eebo-tcp, to dream </p>
            <p>the foundational work for dream began in 1934, when eugene b. power used parts of two movie and still cameras to create one of the world’s first microfilm bookcameras, a device he used to photograph thousands of texts in british libraries (anderson and power, 1990). in 1998 power’s microfilm library became the basis for early english books online (eebo), a database that comprises the images for some 125,000 texts from 1475 to 1700, and has profoundly expanded the horizons of early modern research.
                <hi rend=""superscript"">1</hi> to date, approximately one-third of the documents on eebo are available as transcribed, full-text editions. researchers for the eebo text creation partnership (eebo-tcp) are currently working to transcribe the remaining 85,000 documents, which are as yet only available as digitized microfilm images.
                <hi rend=""superscript"">2</hi>
            </p>
            <p>although completion of the transcription work is still at least 10 years in the future, the prospect of a full-text library of all documents from the first 225 years of english print points to the need for some careful re-thinking about the relation between scholarship and archival sources. as it now stands, the eebo-tcp corpus amounts to 8.02 gigabytes of xml-encoded text and contains nearly 45,000 documents, for a grand total of well over a billion words (1,155,264,343 by our count). confronted by the sheer expanse of a corpus several magnitudes larger than anything one could hope to read in a lifetime, early modern scholarship must now work to incorporate digital methodologies that enable a bird’s-eye view of large corpora, an approach that franco moretti has dubbed ‘distant reading’ (moretti, 2007). dream has begun the work of making such a view possible. </p>
            <p>unlike eebo, dream enables batch downloading of custom-defined subsets rather than obliging users to download individual texts on a one-by-one basis. in other words, it functions at the level of ‘sets of texts’ (sometimes called 
                <hi rend=""italic"">worksets</hi>) rather than ‘individual texts’. examples of subsets one might potentially generate include ‘all texts by ben jonson’, ‘all texts published in 1623’, or ‘all texts printed by john wolfe’. a user-friendly interface makes subsets available as either plain text or xml-encoded files, and gives users the option to automatically name individual files by date, author, title, or combinations thereof (this file naming flexibility can be useful when interoperating with other tool suites). 
            </p>
            <p>the ability to generate custom-defined subsets is important because it allows researchers to explore the early modern canon with distant reading techniques, and to capture otherwise intractable data with visualizations such as graphs, charts, or other forms of graphic representation. on this note, another key feature of dream is that it allows users to transfer specially tailored subsets directly to the analytic interfaces of voyant tools (voyant-tools.org), a suite of textual visualization tools that collectively constitute the leading platform for open-access digital humanities research.
                <hi rend=""superscript"">3</hi> in fact, dream is actually implemented within voyant tools (version 2.0, not yet released, which provides much better support for very large text collections). dream thus provides a compelling example of a bridge between massive full-text repositories (that typically provide faceted searching) and more specialized analytic and visualization environments. by enabling simple transference between the eebo-tcp archive and voyant, dream has significantly expanded the range and sophistication of technologies currently available to researchers who wish to gain a broad sense of printed matter in early modern england. 
            </p>
            <p>notably, however, dream does not aim to replace eebo, or to supplant conventional forms of research. rather, our goal is to simply add a new item to the scholar’s toolbox, and to increase transferability between distant reading methodologies and more fine-grained forms of analysis. </p>
            <p>
                <hi rend=""bold"">negotiating the complexities of non-standardized spelling</hi>
            </p>
            <p>standardized spelling had yet to emerge in early modernity: writers had the freedom to spell however they pleased. to take a famous example, the name ‘shakespeare’ has 80 different recorded spellings, including ‘shaxpere’ and ‘shaxberd’. as one might imagine, variance on this scale presents a serious challenge for large-scale textual analysis. how is it possible to track the incidence of a specific word, or group of words, if any given word could have an unknown multiplicity of iterations? </p>
            <p>to address this problem, we enlisted the assistance of vard 2, a tool that helps to improve the accuracy of textual analysis by finding candidate modern form replacements for spelling variants in historical texts.
                <hi rend=""superscript"">4</hi> as with conventional spellcheckers, a user can choose to process texts manually (selecting a candidate replacement offered by the system), automatically (allowing the system to use the best candidate replacement found), or semi-automatically (training the tool on a sample of the corpora). 
            </p>
            <p>after some preliminary training, we ran the tcp-eebo corpus through vard using the default settings (auto normalization at a threshold of 50%). rather than using the ‘batch’ mode—which proved unreliable for such a big job—we wrote a script that normalized the texts on a one-by-one basis from the command-line. this process took about three days on a commodity machine. vard normalized 80,676 terms for a grand total of 44,909,676 changes overall. </p>
            <p>a careful check through the list resulted in 373 term normalizations that we found problematic in one way or another. the problematic normalizations amounted to 462,975 changes overall, or only 1.03% of the total number of changes. these results were satisfactory: our goal was not to make the corpus ‘perfectly normalized’ (an impossibility, not least because perfection is debatable in this context), but, more pragmatically, to make it generally normalized, which is the best one can reasonably expect from an automatic process. on this point, it is important to note that vard encodes a record of all changes within the output xml file, so scholars will be able to see if the program has made an erroneous normalization. </p>
            <p>some of the problematic vard normalizations seem to have derived from a dictionary error. for example, ‘chan’ became ‘champion’ and ‘ged’ became ‘general’. in other instances, the problematic normalizations were ambiguous or borderline cases that we preferred to simply leave unchanged. examples include ‘piece’ for ‘peece’, and ‘land’ for ‘iland’. there were also cases where the replacement term was not quite correct: ‘strawberie’ became ‘strawy’ rather than ‘strawberry’, and ‘hoouering’ became ‘hoovering’ rather than ‘hovering’. we fixed as many of these kinks as we could by making adjustments to the vard training file and running the entire corpus through the normalization process a second time. </p>
            <p>of course, it is not difficult to imagine scenarios wherein a researcher may prefer to work with original spellings rather than normalized texts. with such projects in mind, we have kept both normalized and non-normalized versions of the eebo-tcp corpus. </p>
            <p>
                <hi rend=""bold"">the dream tutorial program</hi>
            </p>
            <p>as noted above, one of the central objectives of dream is to create an interface that will maximize user-friendliness, allowing scholars with a minimal level of technical expertise to quickly and efficiently create subsets tailored for whatever specific research question they wish to pursue. we are building dream for our own research, but we also have a much broader pedagogical perspective in mind. to meet this objective, we have launched a pilot tutorial program, currently under way, that will teach scholars how to use dream, but will also point to ways in which dream could more effectively serve the demands of scholarly investigation. </p>
            <p>in a series of tasks that build toward the production of a short case-study report, pilot users must articulate a detailed research question and provide a description of their argument. in addition to establishing a valuable feedback loop for the project, this assignment aims to nudge new users toward a more comprehensive, more practical understanding of how macro-scale textual analysis can complement scholarly practice. the key conceptual challenge, as we see it, hinges on new users’ ability to understand, and learn to negotiate, the gap between distant reading and more conventional means of engaging archival sources. </p>
            <p>our pool of pilot users derives from the membership of our parent project, early modern conversions, a five-year interdisciplinary research initiative that has brought together a team of more than 100 scholars, partners, and graduate student associates from universities in canada, the united states, england, new zealand, and australia.
                <hi rend=""superscript"">5</hi> early modern conversions provides a propitious testing ground for dream because it is at the vanguard of early modern research, and because it entails a rich diversity of disciplinary approaches. our presentation for dh2015 will report on the results of the tutorial program and on the progress of the project overall. 
            </p>
            <p>screenshots </p>
            <figure>
                <graphic n=""1001"" width=""16.002cm"" height=""12.230805555555555cm"" url=""pictures/image1.png"" rend=""block""/>
            </figure>
            <p>figure 1. the dream interface. search fields in the middle of the screen enable users to define a subset of eebo-tcp texts by keyword, year, author, and publisher. below the search field, an ‘export’ button opens a dialogue box that offers the option of sending the subset directly to voyant-tools.org, or downloading it as a zip archive. users may also choose to download subsets as either plain text or xml-encoded files. a drag-and-drop mechanism (bottom) enables automatic naming of files within a subset by date, author, title, or combinations thereof. </p>
            <p>
                <pb/>
            </p>
            <figure>
                <graphic n=""1002"" width=""16.002cm"" height=""8.651875cm"" url=""pictures/image2.png"" rend=""block""/>
            </figure>
            <p>figure 2. a sample subset transferred to voyant tools. beginning in the top left corner, one sees a word cloud representing the frequency of keywords in terms of font size. at a glance, it shows that the highest frequency words in the subset are ‘good’ and ‘come’. below the word cloud, there is a summary that lists statistics for basic categories such as word count, vocabulary density, word frequency, etc. in addition, the summary lists words that have a notably high frequency for each year: ‘rome’ and ‘death’ appeared with particular frequency in 1594, while ‘virtue’ and ‘envy’ stood out in 1612. moving to the bottom left corner, one sees an ordered list of frequencies for each word in the corpus accompanied by a thumbnail graph that tracks the frequency of words over the 40-year delimitation. at a glance, the tool shows a significant spike for the word ‘knight’ in 1624. in the middle of the screen, a ‘corpus reader’ tool enables users to drill down into the corpus to examine the context for particular terms. </p>
            <p>notes</p>
            <p>1. see http://eebo.chadwyck.com.</p>
            <p>2. see http://eebo.odl.ox.ac.uk/e/eebo/.</p>
            <p>3. see http://voyant-tools.org.</p>
            <p>4. see http://ucrel.lancs.ac.uk/vard/about/.</p>
            <p>5. see http://earlymodernconversions.com.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl>
                        <hi rend=""bold"">anderson, r. and power, e. b.</hi> (1990). the autobiography of eugene b. power, founder of university microfilms. umi, ann arbor, mi.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">moretti, f.</hi> (2007). graphs, maps, trees: abstract models for literary history. verso, london.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
</tei>",5.0,8.0,Voyant
2257,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,From Mapping the Republic of Letters to Humanities +Design Research Lab: Creating Visualization Tools for Humanistic Inquiry,https://github.com/ADHO/dh2015/blob/master/xml/COLEMAN_Catherine_Nicole_From_Mapping_the_Republic_of_L.xml,Catherine Nicole Coleman;Giorgio Caviglia;Maria Comsa;Mark Braude;Dan Edelstein;Giovanna Ceserani,poster / demo / art installation,"<?xml version=""1.0"" encoding=""UTF-8""?>
<TEI xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>From Mapping the Republic of Letters to Humanities +Design Research Lab: Creating Visualization Tools for Humanistic Inquiry</title>
                <author>
                    <persName>
                        <surname>Coleman</surname>
                        <forename>Catherine Nicole</forename>
                    </persName>
                    <affiliation>Stanford University, United States of America</affiliation>
                    <email>cncoleman@stanford.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Caviglia</surname>
                        <forename>Giorgio</forename>
                    </persName>
                    <affiliation>Stanford University, United States of America</affiliation>
                    <email>giorgio.caviglia@gmail.com</email>
                </author>
                <author>
                    <persName>
                        <surname>Comsa</surname>
                        <forename>Maria</forename>
                    </persName>
                    <affiliation>Stanford University, United States of America</affiliation>
                    <email>mcomsa@stanford.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Braude</surname>
                        <forename>Mark</forename>
                    </persName>
                    <affiliation>Stanford University, United States of America</affiliation>
                    <email>braude@stanford.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Edelstein</surname>
                        <forename>Dan</forename>
                    </persName>
                    <affiliation>Stanford University, United States of America</affiliation>
                    <email>danedels@stanford.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Ceserani</surname>
                        <forename>Giovanna</forename>
                    </persName>
                    <affiliation>Stanford University, United States of America</affiliation>
                    <email>ceserani@stanford.edu</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident=""DHCONVALIDATOR"" version=""1.9"">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme=""ConfTool"" n=""category"">
                    <term>Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""subcategory"">
                    <term>Long Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""keywords"">
                    <term>knowledge design</term>
                    <term>interface design</term>
                    <term>open design</term>
                    <term>visualization</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""topics"">
                    <term>interface and user experience design</term>
                    <term>software design and development</term>
                    <term>knowledge representation</term>
                    <term>visualisation</term>
                    <term>networks</term>
                    <term>relationships</term>
                    <term>graphs</term>
                    <term>spatio-temporal modeling</term>
                    <term>analysis and visualisation</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>What does it mean to build visualization tools that support the research process in the humanities? In this paper we will trace the evolution of our thinking about data-driven tools beginning with case studies in early modern intellectual history and eventually including a wide range of projects from classics, social history, performance studies, and other fields. We will give concrete examples of how individual tools were designed and whether those tools ultimately failed or succeeded to provide scholars with a means to gain insights into historical data. Through these examples, this paper argues for the role of an open design process in the development of visualization tools for humanities research that brings designers, developers, and scholars into deep collaboration to build nuanced and rigorous tools for humanities research. </p>
            <p>Mapping the Republic of Letters</p>
            <p>Mapping the Republic of Letters was formed on the assumption that intellectual history is one of the fields that stands the most to gain from the influx of big data. By combining metadata from library catalogues and large-scale digitization projects, the project seeks to maximize the transformative effect of all this information. The cartographic, chronological, and network visualizations ultimately produced allow researchers to examine some of the big questions that intellectual historians have long struggled with: How do intellectual networks function? How interconnected are they? How independent are these networks from other social networks? </p>
            <p>The 2009 Digging Into Data Challenge grant award launched an active tool development phase in the Mapping the Republic of Letters (MRofL) project at Stanford. In partnership with DensityDesign Research Lab in Milan, the team began to engage in a tool design process in response to concrete research questions. The data—based on individuals and their correspondence, travel, and publications—were multidimensional and qualitatively rich. It became clear that historians who wish to bring data visualization tools to bear on the study of the past face a number of challenges. Many available tools had a steep learning curve and were ultimately of limited help for humanists. These tools rest on assumptions about the completeness and empirical value of data that often do not hold true for humanities research. Historical data can be incomplete and messy: statistical analysis can be a helpful to a limited extent, but interpretation at the most fundamental level is required to uncover meaning. Humanists also ask questions about the data that cannot be answered by numerical analysis. We needed tools that help us filter, contextualize, compare, and see the gaps in our data. </p>
            <p>Humanities + Design</p>
            <p>
                <hi rend=""color(262626)"">Humanities + Design, a research lab founded in 2012 by Dan Edelstein, Paula Findlen, and Nicole Coleman, emerged directly out of lessons learned and opportunities for humanities data analysis discovered through MRofL. The mission of the lab is to produce, through the lens of humanistic inquiry, new modes of thinking in design and computer science to serve data-driven research in the humanities. We believe that humanistic inquiry, grounded in interpretation, has much to contribute to the development of technologies if they are to help us reveal ambiguity and paradox, allowing human-scale exploration of complex systems. In the laboratory environment, theoretical and methodological discussions happen side by side with hands-on work with digital materials. Humanities scholars and students, designers, engineers, and computer scientists engage together in ongoing tool design as defined by the specific needs of participating humanities projects.</hi>
            </p>
            <p>Palladio Project</p>
            <p>The award of the 2012 NEH Implementation Grant for Networks in History allowed the lab to pursue the development of visualization techniques and rich interaction with data that supports ‘thinking through data’ rather than using prescribed algorithms for data analysis. Palladio is a web-based demonstration application that allows any researcher to upload, visualize, and explore complex and multidimensional data, directly in a web browser. 
                <hi rend=""color(262626)"">It has been </hi>designed for humanistic inquiry, with a special focus on historical research. 
                <hi rend=""color(333333)"">The Palladio visualization system combines a primary view (for example, Map, Network Graph, and Tabular views) with filters to make it easy to query a dataset. </hi>
                <hi rend=""color(262626)"">There is no need to create an account, nor do we store any data</hi>
                <hi rend=""bold color(262626)"">. </hi>
                <hi rend=""color(262626)"">Researchers can save and shared the work they have done in the browser as a Palladio Project. Palladio’s TimeLine and TimeSpan filters encourage filtering and sorting temporal data, and allows the filtering of two or more discontinuous time periods. A Facet filter is also particularly useful when exploring multidimensional datasets and drilling down to specific aspects of one’s data. Using case studies (examples listed later in this document) we will discuss how scholars have used Palladio, highlighting those instances when uses of the tool diverged from our expectations or led us toward new insights that we incorporated (or plan to incorporate) in future versions.</hi>
            </p>
            <p>Open Design</p>
            <p>
                <hi rend=""color(444444)"">The development of Palladio has been an iterative process. We have been eliciting and incorporating feedback from the academic community concerning Palladio’s current and potential features and uses. Most specifically, we have engaged in sustained discussion with a small and inter-disciplinary group of scholars, known as Open Design Contributors. Our paper will offer insight into this design process and the ways that it has directly influenced current and future iterations of Palladio, as well as other tools.</hi>
            </p>
            <p>Summary</p>
            <p>The core innovation of our project is the design of visualization techniques that emphasize the contextualization and interpretation of data in cases where we lack the metrics for useful quantitative analysis. The two other key innovations both involve the leveraging of novel technologies that are particularly important to the study of cultural heritage data: we use new flexible data models to let individual scholars create and apply their own data categorizations, and we use open linked data sources to reconcile datasets against established authority files, in order to link entities across datasets and thereby explore networks across collections.</p>
            <p>
                <hi rend=""bold"">Additional Case Studies to Be Discussed</hi>
            </p>
            <p>
                <hi rend=""italic"">Case Study: Toward More Complex Ways of Displaying Travel</hi>
            </p>
            <p>
                <hi rend=""italic"">Kate Elswit, Lecturer in Theatre and Performance Studies at the University of Bristol, ‘Ballet, Digital History, and the Cold War: Visualizing the Labor of Dance Touring’</hi>
            </p>
            <p>Dance scholar Kate Elswit has been using Palladio in her research on the labor of dance touring. She writes, ‘Such [visualization] techniques enable us to feel the passage of time differently.’ Following discussion with Elswit and other scholars interested in tracing travel routes, we have been thinking about how to display point-to-point travel in ways that go beyond simple flight-path-like visualizations. How to account for the differences in traveling at night rather than in the day? How to represent different levels of comfort, safety, and efficiency in travel?</p>
            <p>Case Study: Questions of Scale and Incomplete Data</p>
            <p>
                <hi rend=""italic"">Molly Taylor-Poleskey, PhD Candidate, Department of History, Stanford: Food Culture in Brandenburg-Prussia</hi>
            </p>
            <p>
                <hi rend=""color(222222)"">Taylor-Poleskey uses a large base of manuscript sources detailing the yearly consumption of one of the palaces of Prince-Elector Friedrich Wilhelm of Brandenburg-Prussia. She argues that the elector’s cultural agenda helped transform his territories over the course of his reign from dilapidated and war-torn to stable and powerful. To support her argument, she wants to see how tastes and consumption patterns changed over time, to consider how such changes might reveal the court’s aesthetic values and cultural ambitions. Creating visualizations in Palladio have helped her analyze what proportions of different foods or food groups were consumed. We have worked with Taylor-Poleskey toward creating visualizations that privilege the display of relative magnitude, and that are especially sensitive to working in different registers and scales. As some of the years in the sources she studies have incomplete or missing data, her use case has also aided us in thinking about how best to work with and represent incomplete data in ways that are not misleading or overly simplistic.</hi>
            </p>
            <p>Case Study: Toward New Palladio Data-Visualization Iterations</p>
            <p>
                <hi rend=""italic"">Office of the Historian, US State Department, Foreign Relations of the United States</hi>
            </p>
            <p>We will share results from our ongoing work with Thomas Faith at the Office of the Historian at the US Department of State, with whom we have been working toward the goal of producing an integrated version of Palladio that would function as a visual browser for extant online data concerning the foreign relations of the United States. The State Department project is one of many we are working on, as we look to help other researchers to implement customized versions of Palladio that can be used as search, analysis, and visualization exploratory tools within extant large-scale research projects.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold color(262626)"">Balsamo, A. </hi>
                        <hi rend=""color(262626)"">(2009). Design. </hi>
                        <hi rend=""italic color(262626)"">International Journal of Learning and Media,</hi>
                        <hi rend=""bold color(262626)"">1</hi>
                        <hi rend=""color(262626)"">(4): 1–10.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Berry, D. M. </hi>
                        <hi rend=""color(262626)"">(2012). </hi>
                        <hi rend=""italic color(262626)"">Understanding Digital Humanities.</hi>
                        <hi rend=""color(262626)""> Palgrave Macmillan, New York.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Buchanan, R. </hi>
                        <hi rend=""color(262626)"">(2001). Design Research and the New Learning. </hi>
                        <hi rend=""italic color(262626)"">Design Issues,</hi>
                        <hi rend=""bold color(262626)"">17</hi>
                        <hi rend=""color(262626)"">(4): 3–23.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Burdick, A. </hi>
                        <hi rend=""color(262626)"">(2009). Design Without Designers. </hi>
                        <hi rend=""italic color(262626)"">Conference on the Future of Art and Design Education in the 21st Century</hi>
                        <hi rend=""color(262626)"">, University of Brighton, England, 29 April 2009.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Burdick, A. and Willis, H. </hi>
                        <hi rend=""color(262626)"">(2011). Digital Learning, Digital Scholarship, and Design Thinking.</hi>
                        <hi rend=""italic color(262626)"">Design Studies,</hi>
                        <hi rend=""bold color(262626)"">32</hi>
                        <hi rend=""color(262626)"">(6): 546–56.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Drucker, J. </hi>
                        <hi rend=""color(262626)"">(2009). SpecLab. In </hi>
                        <hi rend=""italic color(262626)"">Digital Aesthetics and Projects in Speculative Computing.</hi>
                        <hi rend=""color(262626)""> University of Chicago Press, Chicago.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Drucker, J. </hi>
                        <hi rend=""color(262626)"">(2011). Humanities Approach to Interface Theory. </hi>
                        <hi rend=""italic color(262626)"">Culture Machine,</hi>
                        <hi rend=""bold color(262626)"">12</hi>
                        <hi rend=""color(262626)"">: 1–20.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Friedman, K. </hi>
                        <hi rend=""color(262626)"">(2003). Theory Construction in Design Research: Criteria, Approaches, and Methods. </hi>
                        <hi rend=""italic color(262626)"">Design Studies,</hi>
                        <hi rend=""bold color(262626)"">24</hi>
                        <hi rend=""color(262626)"">(6): 16.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Fuller, M.</hi>
                        <hi rend=""color(262626)"">(2008). Software Studies. MIT Press, Cambridge, MA.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Ivanhoe.</hi>
                        <hi rend=""color(262626)""> (n.d.). http://www2.iath.virginia.edu/jjm2f/old/IGamehtm.html. </hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Lunenfeld, P., Burdick, A., Drucker, J., Presner, T. and Schnapp, J. P.</hi>
                        <hi rend=""color(262626)"">(2012).</hi>
                        <hi rend=""italic color(262626)"">Digital_Humanities</hi>
                        <hi rend=""color(262626)"">. MIT Press, Cambridge, MA.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Mandala Browser. </hi>
                        <hi rend=""color(262626)"">(n.d.). </hi>http://mandala.humviz.org/.
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Masud, L., Valsecchi, F., Ciuccarelli, P., Ricci, D. and Caviglia, G.</hi>
                        <hi rend=""color(262626)"">(2010). From Data to Knowledge: Visualizations as Transformation Processes within the Data-Information-Knowledge Continuum. In Banissi, E., Bertschi, S., Burkhard, R., Counsell, J., Dastbaz, M., Eppler, M., Forsell, C., et al. (eds),</hi>
                        <hi rend=""italic color(262626)"">Information Visualisation IV, 2010 14th International Conference</hi>
                        <hi rend=""color(262626)"">, pp. 445–49.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">McCarty, W. </hi>
                        <hi rend=""color(262626)"">(2003). </hi>
                        <hi rend=""italic color(262626)"">Encyclopedia of Library and Information Science.</hi>
                        <hi rend=""color(262626)""> Vol. 2. 2nd ed. New York: Dekker, pp. 1224–35.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">McGann, J. and Samuels, L. </hi>
                        <hi rend=""color(262626)"">(2004). Deformance and Interpretation. In </hi>
                        <hi rend=""italic color(262626)"">Radiant Textuality.</hi>
                        <hi rend=""color(262626)"">New York: Palgrave Macmillan.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Moretti, F. (</hi>
                        <hi rend=""color(262626)"">2005). </hi>
                        <hi rend=""italic color(262626)"">Graphs, Maps, Trees.</hi>
                        <hi rend=""color(262626)""> Verso Books, New York.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Nowviskie, B.</hi>
                        <hi rend=""color(262626)"">(2004). </hi>
                        <hi rend=""italic color(262626)"">Speculative Computing: Instruments for Interpretative Scholarship.</hi>
                        <hi rend=""color(262626)""> Ph.D. thesis, University of Virginia.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Orbis.</hi>
                        <hi rend=""color(262626)"">(n.d.). http://orbis.stanford.edu/.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Pope, R. (</hi>
                        <hi rend=""color(262626)"">1995). </hi>
                        <hi rend=""italic color(262626)"">Textual Intervention: Critical and Creative Strategies for Literary Studies.</hi>
                        <hi rend=""color(262626)"">Routledge, London.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Ramsay, S. </hi>
                        <hi rend=""color(262626)"">(2011a). On Building, 11 January, http://stephenramsay.us/text/2011/01/11/onNbuilding.html.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Ramsay, S. </hi>
                        <hi rend=""color(262626)"">(2011b). Who’s In and Who’s Out, 8 January, http://stephenramsay.us/text/2011/01/08/whosNinNandNwhosNout.html.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Ruecker, S., Radzikowska, M. and Sinclair, S.</hi>
                        <hi rend=""color(262626)""> (2011). </hi>
                        <hi rend=""italic color(262626)"">Visual Interface Design for Digital Cultural Heritage.</hi>
                        <hi rend=""color(262626)""> Ashgate.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Schnapp, J. and Presner, T. </hi>
                        <hi rend=""color(262626)"">(2009). The Digital Humanities Manifesto 2.0, 17 June, www.humanitiesblast.com/manifesto/Manifesto_V2.pdf.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Schon, D. A. </hi>
                        <hi rend=""color(262626)"">(1983). </hi>
                        <hi rend=""italic color(262626)"">The Reflective Practitioner: How Professionals Think in Action.</hi>
                        <hi rend=""color(262626)"">1st ed. Basic Books, New York.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Temporal Modeling. </hi>
                        <hi rend=""color(262626)"">(n.d.). http://www2.iath.virginia.edu/time/time.html.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">Voyant.</hi>
                        <hi rend=""color(262626)""> (n.d.). http://voyant-tools.org.</hi>
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>",xml,Creative Commons Attribution 4.0 International,,interface design;knowledge design;open design;visualization,English,"english;interface and user experience design;knowledge representation;networks, relationships, graphs;software design and development;spatio-temporal modeling, analysis and visualisation;visualization",2015-01-01,"<?xml version=""1.0"" encoding=""utf-8""?>
<tei xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiheader>
        <filedesc>
            <titlestmt>
                <title>from mapping the republic of letters to humanities +design research lab: creating visualization tools for humanistic inquiry</title>
                <author>
                    <persname>
                        <surname>coleman</surname>
                        <forename>catherine nicole</forename>
                    </persname>
                    <affiliation>stanford university, united states of america</affiliation>
                    <email>cncoleman@stanford.edu</email>
                </author>
                <author>
                    <persname>
                        <surname>caviglia</surname>
                        <forename>giorgio</forename>
                    </persname>
                    <affiliation>stanford university, united states of america</affiliation>
                    <email>giorgio.caviglia@gmail.com</email>
                </author>
                <author>
                    <persname>
                        <surname>comsa</surname>
                        <forename>maria</forename>
                    </persname>
                    <affiliation>stanford university, united states of america</affiliation>
                    <email>mcomsa@stanford.edu</email>
                </author>
                <author>
                    <persname>
                        <surname>braude</surname>
                        <forename>mark</forename>
                    </persname>
                    <affiliation>stanford university, united states of america</affiliation>
                    <email>braude@stanford.edu</email>
                </author>
                <author>
                    <persname>
                        <surname>edelstein</surname>
                        <forename>dan</forename>
                    </persname>
                    <affiliation>stanford university, united states of america</affiliation>
                    <email>danedels@stanford.edu</email>
                </author>
                <author>
                    <persname>
                        <surname>ceserani</surname>
                        <forename>giovanna</forename>
                    </persname>
                    <affiliation>stanford university, united states of america</affiliation>
                    <email>ceserani@stanford.edu</email>
                </author>
            </titlestmt>
            <editionstmt>
                <edition>
                    <date>2014-12-19t13:50:00z</date>
                </edition>
            </editionstmt>
            <publicationstmt>
                <publisher>paul arthur, university of western sidney</publisher>
                <address>
                    <addrline>locked bag 1797</addrline>
                    <addrline>penrith nsw 2751</addrline>
                    <addrline>australia</addrline>
                    <addrline>paul arthur</addrline>
                </address>
            </publicationstmt>
            <sourcedesc>
                <p>converted from a word document </p>
            </sourcedesc>
        </filedesc>
        <encodingdesc>
            <appinfo>
                <application ident=""dhconvalidator"" version=""1.9"">
                    <label>dhconvalidator</label>
                </application>
            </appinfo>
        </encodingdesc>
        <profiledesc>
            <textclass>
                <keywords scheme=""conftool"" n=""category"">
                    <term>paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""subcategory"">
                    <term>long paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""keywords"">
                    <term>knowledge design</term>
                    <term>interface design</term>
                    <term>open design</term>
                    <term>visualization</term>
                </keywords>
                <keywords scheme=""conftool"" n=""topics"">
                    <term>interface and user experience design</term>
                    <term>software design and development</term>
                    <term>knowledge representation</term>
                    <term>visualisation</term>
                    <term>networks</term>
                    <term>relationships</term>
                    <term>graphs</term>
                    <term>spatio-temporal modeling</term>
                    <term>analysis and visualisation</term>
                    <term>english</term>
                </keywords>
            </textclass>
        </profiledesc>
    </teiheader>
    <text>
        <body>
            <p>what does it mean to build visualization tools that support the research process in the humanities? in this paper we will trace the evolution of our thinking about data-driven tools beginning with case studies in early modern intellectual history and eventually including a wide range of projects from classics, social history, performance studies, and other fields. we will give concrete examples of how individual tools were designed and whether those tools ultimately failed or succeeded to provide scholars with a means to gain insights into historical data. through these examples, this paper argues for the role of an open design process in the development of visualization tools for humanities research that brings designers, developers, and scholars into deep collaboration to build nuanced and rigorous tools for humanities research. </p>
            <p>mapping the republic of letters</p>
            <p>mapping the republic of letters was formed on the assumption that intellectual history is one of the fields that stands the most to gain from the influx of big data. by combining metadata from library catalogues and large-scale digitization projects, the project seeks to maximize the transformative effect of all this information. the cartographic, chronological, and network visualizations ultimately produced allow researchers to examine some of the big questions that intellectual historians have long struggled with: how do intellectual networks function? how interconnected are they? how independent are these networks from other social networks? </p>
            <p>the 2009 digging into data challenge grant award launched an active tool development phase in the mapping the republic of letters (mrofl) project at stanford. in partnership with densitydesign research lab in milan, the team began to engage in a tool design process in response to concrete research questions. the data—based on individuals and their correspondence, travel, and publications—were multidimensional and qualitatively rich. it became clear that historians who wish to bring data visualization tools to bear on the study of the past face a number of challenges. many available tools had a steep learning curve and were ultimately of limited help for humanists. these tools rest on assumptions about the completeness and empirical value of data that often do not hold true for humanities research. historical data can be incomplete and messy: statistical analysis can be a helpful to a limited extent, but interpretation at the most fundamental level is required to uncover meaning. humanists also ask questions about the data that cannot be answered by numerical analysis. we needed tools that help us filter, contextualize, compare, and see the gaps in our data. </p>
            <p>humanities + design</p>
            <p>
                <hi rend=""color(262626)"">humanities + design, a research lab founded in 2012 by dan edelstein, paula findlen, and nicole coleman, emerged directly out of lessons learned and opportunities for humanities data analysis discovered through mrofl. the mission of the lab is to produce, through the lens of humanistic inquiry, new modes of thinking in design and computer science to serve data-driven research in the humanities. we believe that humanistic inquiry, grounded in interpretation, has much to contribute to the development of technologies if they are to help us reveal ambiguity and paradox, allowing human-scale exploration of complex systems. in the laboratory environment, theoretical and methodological discussions happen side by side with hands-on work with digital materials. humanities scholars and students, designers, engineers, and computer scientists engage together in ongoing tool design as defined by the specific needs of participating humanities projects.</hi>
            </p>
            <p>palladio project</p>
            <p>the award of the 2012 neh implementation grant for networks in history allowed the lab to pursue the development of visualization techniques and rich interaction with data that supports ‘thinking through data’ rather than using prescribed algorithms for data analysis. palladio is a web-based demonstration application that allows any researcher to upload, visualize, and explore complex and multidimensional data, directly in a web browser. 
                <hi rend=""color(262626)"">it has been </hi>designed for humanistic inquiry, with a special focus on historical research. 
                <hi rend=""color(333333)"">the palladio visualization system combines a primary view (for example, map, network graph, and tabular views) with filters to make it easy to query a dataset. </hi>
                <hi rend=""color(262626)"">there is no need to create an account, nor do we store any data</hi>
                <hi rend=""bold color(262626)"">. </hi>
                <hi rend=""color(262626)"">researchers can save and shared the work they have done in the browser as a palladio project. palladio’s timeline and timespan filters encourage filtering and sorting temporal data, and allows the filtering of two or more discontinuous time periods. a facet filter is also particularly useful when exploring multidimensional datasets and drilling down to specific aspects of one’s data. using case studies (examples listed later in this document) we will discuss how scholars have used palladio, highlighting those instances when uses of the tool diverged from our expectations or led us toward new insights that we incorporated (or plan to incorporate) in future versions.</hi>
            </p>
            <p>open design</p>
            <p>
                <hi rend=""color(444444)"">the development of palladio has been an iterative process. we have been eliciting and incorporating feedback from the academic community concerning palladio’s current and potential features and uses. most specifically, we have engaged in sustained discussion with a small and inter-disciplinary group of scholars, known as open design contributors. our paper will offer insight into this design process and the ways that it has directly influenced current and future iterations of palladio, as well as other tools.</hi>
            </p>
            <p>summary</p>
            <p>the core innovation of our project is the design of visualization techniques that emphasize the contextualization and interpretation of data in cases where we lack the metrics for useful quantitative analysis. the two other key innovations both involve the leveraging of novel technologies that are particularly important to the study of cultural heritage data: we use new flexible data models to let individual scholars create and apply their own data categorizations, and we use open linked data sources to reconcile datasets against established authority files, in order to link entities across datasets and thereby explore networks across collections.</p>
            <p>
                <hi rend=""bold"">additional case studies to be discussed</hi>
            </p>
            <p>
                <hi rend=""italic"">case study: toward more complex ways of displaying travel</hi>
            </p>
            <p>
                <hi rend=""italic"">kate elswit, lecturer in theatre and performance studies at the university of bristol, ‘ballet, digital history, and the cold war: visualizing the labor of dance touring’</hi>
            </p>
            <p>dance scholar kate elswit has been using palladio in her research on the labor of dance touring. she writes, ‘such [visualization] techniques enable us to feel the passage of time differently.’ following discussion with elswit and other scholars interested in tracing travel routes, we have been thinking about how to display point-to-point travel in ways that go beyond simple flight-path-like visualizations. how to account for the differences in traveling at night rather than in the day? how to represent different levels of comfort, safety, and efficiency in travel?</p>
            <p>case study: questions of scale and incomplete data</p>
            <p>
                <hi rend=""italic"">molly taylor-poleskey, phd candidate, department of history, stanford: food culture in brandenburg-prussia</hi>
            </p>
            <p>
                <hi rend=""color(222222)"">taylor-poleskey uses a large base of manuscript sources detailing the yearly consumption of one of the palaces of prince-elector friedrich wilhelm of brandenburg-prussia. she argues that the elector’s cultural agenda helped transform his territories over the course of his reign from dilapidated and war-torn to stable and powerful. to support her argument, she wants to see how tastes and consumption patterns changed over time, to consider how such changes might reveal the court’s aesthetic values and cultural ambitions. creating visualizations in palladio have helped her analyze what proportions of different foods or food groups were consumed. we have worked with taylor-poleskey toward creating visualizations that privilege the display of relative magnitude, and that are especially sensitive to working in different registers and scales. as some of the years in the sources she studies have incomplete or missing data, her use case has also aided us in thinking about how best to work with and represent incomplete data in ways that are not misleading or overly simplistic.</hi>
            </p>
            <p>case study: toward new palladio data-visualization iterations</p>
            <p>
                <hi rend=""italic"">office of the historian, us state department, foreign relations of the united states</hi>
            </p>
            <p>we will share results from our ongoing work with thomas faith at the office of the historian at the us department of state, with whom we have been working toward the goal of producing an integrated version of palladio that would function as a visual browser for extant online data concerning the foreign relations of the united states. the state department project is one of many we are working on, as we look to help other researchers to implement customized versions of palladio that can be used as search, analysis, and visualization exploratory tools within extant large-scale research projects.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl>
                        <hi rend=""bold color(262626)"">balsamo, a. </hi>
                        <hi rend=""color(262626)"">(2009). design. </hi>
                        <hi rend=""italic color(262626)"">international journal of learning and media,</hi>
                        <hi rend=""bold color(262626)"">1</hi>
                        <hi rend=""color(262626)"">(4): 1–10.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">berry, d. m. </hi>
                        <hi rend=""color(262626)"">(2012). </hi>
                        <hi rend=""italic color(262626)"">understanding digital humanities.</hi>
                        <hi rend=""color(262626)""> palgrave macmillan, new york.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">buchanan, r. </hi>
                        <hi rend=""color(262626)"">(2001). design research and the new learning. </hi>
                        <hi rend=""italic color(262626)"">design issues,</hi>
                        <hi rend=""bold color(262626)"">17</hi>
                        <hi rend=""color(262626)"">(4): 3–23.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">burdick, a. </hi>
                        <hi rend=""color(262626)"">(2009). design without designers. </hi>
                        <hi rend=""italic color(262626)"">conference on the future of art and design education in the 21st century</hi>
                        <hi rend=""color(262626)"">, university of brighton, england, 29 april 2009.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">burdick, a. and willis, h. </hi>
                        <hi rend=""color(262626)"">(2011). digital learning, digital scholarship, and design thinking.</hi>
                        <hi rend=""italic color(262626)"">design studies,</hi>
                        <hi rend=""bold color(262626)"">32</hi>
                        <hi rend=""color(262626)"">(6): 546–56.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">drucker, j. </hi>
                        <hi rend=""color(262626)"">(2009). speclab. in </hi>
                        <hi rend=""italic color(262626)"">digital aesthetics and projects in speculative computing.</hi>
                        <hi rend=""color(262626)""> university of chicago press, chicago.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">drucker, j. </hi>
                        <hi rend=""color(262626)"">(2011). humanities approach to interface theory. </hi>
                        <hi rend=""italic color(262626)"">culture machine,</hi>
                        <hi rend=""bold color(262626)"">12</hi>
                        <hi rend=""color(262626)"">: 1–20.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">friedman, k. </hi>
                        <hi rend=""color(262626)"">(2003). theory construction in design research: criteria, approaches, and methods. </hi>
                        <hi rend=""italic color(262626)"">design studies,</hi>
                        <hi rend=""bold color(262626)"">24</hi>
                        <hi rend=""color(262626)"">(6): 16.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">fuller, m.</hi>
                        <hi rend=""color(262626)"">(2008). software studies. mit press, cambridge, ma.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">ivanhoe.</hi>
                        <hi rend=""color(262626)""> (n.d.). http://www2.iath.virginia.edu/jjm2f/old/igamehtm.html. </hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">lunenfeld, p., burdick, a., drucker, j., presner, t. and schnapp, j. p.</hi>
                        <hi rend=""color(262626)"">(2012).</hi>
                        <hi rend=""italic color(262626)"">digital_humanities</hi>
                        <hi rend=""color(262626)"">. mit press, cambridge, ma.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">mandala browser. </hi>
                        <hi rend=""color(262626)"">(n.d.). </hi>http://mandala.humviz.org/.
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">masud, l., valsecchi, f., ciuccarelli, p., ricci, d. and caviglia, g.</hi>
                        <hi rend=""color(262626)"">(2010). from data to knowledge: visualizations as transformation processes within the data-information-knowledge continuum. in banissi, e., bertschi, s., burkhard, r., counsell, j., dastbaz, m., eppler, m., forsell, c., et al. (eds),</hi>
                        <hi rend=""italic color(262626)"">information visualisation iv, 2010 14th international conference</hi>
                        <hi rend=""color(262626)"">, pp. 445–49.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">mccarty, w. </hi>
                        <hi rend=""color(262626)"">(2003). </hi>
                        <hi rend=""italic color(262626)"">encyclopedia of library and information science.</hi>
                        <hi rend=""color(262626)""> vol. 2. 2nd ed. new york: dekker, pp. 1224–35.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">mcgann, j. and samuels, l. </hi>
                        <hi rend=""color(262626)"">(2004). deformance and interpretation. in </hi>
                        <hi rend=""italic color(262626)"">radiant textuality.</hi>
                        <hi rend=""color(262626)"">new york: palgrave macmillan.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">moretti, f. (</hi>
                        <hi rend=""color(262626)"">2005). </hi>
                        <hi rend=""italic color(262626)"">graphs, maps, trees.</hi>
                        <hi rend=""color(262626)""> verso books, new york.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">nowviskie, b.</hi>
                        <hi rend=""color(262626)"">(2004). </hi>
                        <hi rend=""italic color(262626)"">speculative computing: instruments for interpretative scholarship.</hi>
                        <hi rend=""color(262626)""> ph.d. thesis, university of virginia.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">orbis.</hi>
                        <hi rend=""color(262626)"">(n.d.). http://orbis.stanford.edu/.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">pope, r. (</hi>
                        <hi rend=""color(262626)"">1995). </hi>
                        <hi rend=""italic color(262626)"">textual intervention: critical and creative strategies for literary studies.</hi>
                        <hi rend=""color(262626)"">routledge, london.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">ramsay, s. </hi>
                        <hi rend=""color(262626)"">(2011a). on building, 11 january, http://stephenramsay.us/text/2011/01/11/onnbuilding.html.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">ramsay, s. </hi>
                        <hi rend=""color(262626)"">(2011b). who’s in and who’s out, 8 january, http://stephenramsay.us/text/2011/01/08/whosninnandnwhosnout.html.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">ruecker, s., radzikowska, m. and sinclair, s.</hi>
                        <hi rend=""color(262626)""> (2011). </hi>
                        <hi rend=""italic color(262626)"">visual interface design for digital cultural heritage.</hi>
                        <hi rend=""color(262626)""> ashgate.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">schnapp, j. and presner, t. </hi>
                        <hi rend=""color(262626)"">(2009). the digital humanities manifesto 2.0, 17 june, www.humanitiesblast.com/manifesto/manifesto_v2.pdf.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">schon, d. a. </hi>
                        <hi rend=""color(262626)"">(1983). </hi>
                        <hi rend=""italic color(262626)"">the reflective practitioner: how professionals think in action.</hi>
                        <hi rend=""color(262626)"">1st ed. basic books, new york.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">temporal modeling. </hi>
                        <hi rend=""color(262626)"">(n.d.). http://www2.iath.virginia.edu/time/time.html.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(262626)"">voyant.</hi>
                        <hi rend=""color(262626)""> (n.d.). http://voyant-tools.org.</hi>
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
</tei>",1.0,2.0,Voyant
2318,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,Nocht: An Open Source Tool for Text Analysis,https://github.com/ADHO/dh2015/blob/master/xml/O_SULLIVAN_James_Christopher_Nocht__An_Open_Source_Tool.xml,James Christopher O'Sullivan;Patricia Hswe;Christopher P. Long,poster / demo / art installation,"<?xml version=""1.0"" encoding=""UTF-8""?>
<TEI xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Nocht: An Open Source Tool for Text Analysis</title>
                <author>
                    <persName>
                        <surname>O'Sullivan</surname>
                        <forename>James Christopher</forename>
                    </persName>
                    <affiliation>Pennsylvania State University</affiliation>
                    <email>josullivan@psu.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Hswe</surname>
                        <forename>Patricia</forename>
                    </persName>
                    <affiliation>Pennsylvania State University</affiliation>
                    <email>phswe@psu.edu</email>
                </author>
                <author>
                    <persName>
                        <surname>Long</surname>
                        <forename>Christopher P.</forename>
                    </persName>
                    <affiliation>Pennsylvania State University</affiliation>
                    <email>cplong@psu.edu</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident=""DHCONVALIDATOR"" version=""1.9"">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme=""ConfTool"" n=""category"">
                    <term>Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""subcategory"">
                    <term>Short Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""keywords"">
                    <term>Tools</term>
                    <term>Text Analysis</term>
                    <term>Python</term>
                    <term>Digital Literary Studies</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""topics"">
                    <term>software design and development</term>
                    <term>text analysis</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>Computational approaches to text analysis have revolutionised the ways in which scholarly research is being conducted. A number of tools exist that help scholars, from a variety of disciplines, analyse textual data, whether literary, historical, or otherwise, using scientific methodologies. However, many of these tools are either proprietary, present a steep learning curve, or are constructed without much transparency, often leaving users with results whose means of production they do not understand. This poster will outline the development of a tool that is intuitive and completely free and open-source, so that scholars in literary studies, and indeed the broader humanities, can leverage computational methods and big data analytics in their research.</p>
            <p>Nocht</p>
            <p>Nocht (trans.: to reveal, uncover), is developed, primarily, in Python, so that it is flexible, scalable, and cross-platform. It has been developed in accordance with the following principles:</p>
            <p> • It offers users a low-barrier means of using computational approaches to text analysis.</p>
            <p> • It is designed and developed in a humanities / arts / social sciences context.</p>
            <p> • It is completely open-source, removing the ‘black-box’, closed-code issues.</p>
            <p> • It brings together existing libraries and code-sets, acting as a ‘script portal’ of sorts.</p>
            <p>At present, Nocht supports the following methodologies, though with some limitations:</p>
            <p> • Wordcount and most frequent wordlists.</p>
            <p> • Word / wordlist frequency plotting.</p>
            <p> • Syntax and sentence analysis.</p>
            <p> • Sentiment analysis.</p>
            <p> • Topic modeling.</p>
            <p> • Zeta analysis.</p>
            <p>It is hoped that Nocht will further contribute to our field’s ongoing commitment to open and sustainable research tools, complementing highly regarded projects like Voyant
                <hi rend=""superscript"">1</hi> and Stylo (Eder et al., 2013). Its name is an obvious tribute to the former, which has for so long been one of our field’s fundamental tools. It is hoped that Nocht will add further to the DH toolkit, as well as complement the ongoing work of Voyant’s creators in leveraging the iPython architecture. 
            </p>
            <p>From a technical perspective, Nocht is scalable and robust, and satisfies the needs of a wide range of scholars, many of whom wish to conduct this form of research but lack the expertise or resources to do so. In this respect, it enables scholars, both emerging and established, to engage with cutting-edge analyses across a variety of disciplines. In many cases, it draws on a series of existing libraries and proven methodologies, such as NLTK
                <hi rend=""superscript"">2</hi> and matplotlib,
                <hi rend=""superscript"">3</hi> and so acts as a set of original scripts as well as a portal to existing tools. A complete technical overview of the project’s features, as well as the components utilised in its modular development, will be provided at the session. 
            </p>
            <p>Discussion</p>
            <p>This poster proposes to introduce Nocht to the field, discussing possible future development directions, as well as issues to date. Some of the disciplinary particularities identified by Gibbs and Owens (2012), such as our need to enhance the usability of our tools, will be addressed. The tension between having an intuitive interface and the need for scholarly tools to produce verifiable results is particularly clear in this project. While there is a long-established requirement that such tools be user-friendly (Krug 2005), one might argue that this must be balanced with a commitment to avoiding ‘black-box’ projects; usability does not necessarily equate to understanding. </p>
            <p>Measuring the value and success of development projects also remains problematic for scholars and practitioners working across the digital humanities. Schreibman and Hanlon’s survey (2010) finds that the majority of respondents were satisfied that their tools had been ‘successful’, enabling themselves and others to further their research. However, respondents also outlined that they had measured this success from a ‘controlled list’. As our methods continue to gain prominence beyond the core digital humanities community, we must find new metrics through which we can reliably measure the impact of our tools, not just in terms of user volumes, but in relation to the quality of research output. As a project that has sacrificed some aspects of usability and marketability in favour of broad functionality and a commitment to open principles, perhaps to its detriment, Nocht is an ideal catalyst for this debate. It is a small development with limited financial support, so it will be interesting to see if projects of this scale have a future in our discipline.</p>
            <p>Notes</p>
            <p>1. See Stéfan Sinclair and Geoffrey Rockwell, http://voyant-tools.org/.</p>
            <p>2. Natural Language Toolkit, http://www.nltk.org/.</p>
            <p>3. matplotlib, http://matplotlib.org/.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Eder, M., Kestemont, M. and Rybicki, J.</hi> (2013). Stylometry with R: A Suite of Tools. 
                        <hi rend=""italic"">Digital Humanities 2013: Conference Abstracts</hi>, University of Nebraska–Lincoln, pp. 487–89.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Gibbs, F. and Owens, T.</hi> Building Better Digital Humanities Tools: Toward Broader Audiences and User-Centered Designs. 
                        <hi rend=""italic"">Digital Humanities Quarterly,</hi>
                        <hi rend=""bold"">6</hi>(2).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Krug, S.</hi> (2005). 
                        <hi rend=""italic"">Don’t Make Me Think! A Common Sense Approach to Web Usability</hi>. 2nd ed. New Riders Press, New York.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Schreibman, S. and Hanlon, A. M.</hi> (2010). Determining Value for Digital Humanities Tools: Report on a Survey of Tool Developers. 
                        <hi rend=""italic"">Digital Humanities Quarterly,</hi>
                        <hi rend=""bold"">4</hi>(2).
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>",xml,Creative Commons Attribution 4.0 International,,digital literary studies;python;text analysis;tools,English,english;software design and development;text analysis,2015-01-01,"<?xml version=""1.0"" encoding=""utf-8""?>
<tei xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiheader>
        <filedesc>
            <titlestmt>
                <title>nocht: an open source tool for text analysis</title>
                <author>
                    <persname>
                        <surname>o'sullivan</surname>
                        <forename>james christopher</forename>
                    </persname>
                    <affiliation>pennsylvania state university</affiliation>
                    <email>josullivan@psu.edu</email>
                </author>
                <author>
                    <persname>
                        <surname>hswe</surname>
                        <forename>patricia</forename>
                    </persname>
                    <affiliation>pennsylvania state university</affiliation>
                    <email>phswe@psu.edu</email>
                </author>
                <author>
                    <persname>
                        <surname>long</surname>
                        <forename>christopher p.</forename>
                    </persname>
                    <affiliation>pennsylvania state university</affiliation>
                    <email>cplong@psu.edu</email>
                </author>
            </titlestmt>
            <editionstmt>
                <edition>
                    <date>2014-12-19t13:50:00z</date>
                </edition>
            </editionstmt>
            <publicationstmt>
                <publisher>paul arthur, university of western sidney</publisher>
                <address>
                    <addrline>locked bag 1797</addrline>
                    <addrline>penrith nsw 2751</addrline>
                    <addrline>australia</addrline>
                    <addrline>paul arthur</addrline>
                </address>
            </publicationstmt>
            <sourcedesc>
                <p>converted from a word document </p>
            </sourcedesc>
        </filedesc>
        <encodingdesc>
            <appinfo>
                <application ident=""dhconvalidator"" version=""1.9"">
                    <label>dhconvalidator</label>
                </application>
            </appinfo>
        </encodingdesc>
        <profiledesc>
            <textclass>
                <keywords scheme=""conftool"" n=""category"">
                    <term>paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""subcategory"">
                    <term>short paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""keywords"">
                    <term>tools</term>
                    <term>text analysis</term>
                    <term>python</term>
                    <term>digital literary studies</term>
                </keywords>
                <keywords scheme=""conftool"" n=""topics"">
                    <term>software design and development</term>
                    <term>text analysis</term>
                    <term>english</term>
                </keywords>
            </textclass>
        </profiledesc>
    </teiheader>
    <text>
        <body>
            <p>computational approaches to text analysis have revolutionised the ways in which scholarly research is being conducted. a number of tools exist that help scholars, from a variety of disciplines, analyse textual data, whether literary, historical, or otherwise, using scientific methodologies. however, many of these tools are either proprietary, present a steep learning curve, or are constructed without much transparency, often leaving users with results whose means of production they do not understand. this poster will outline the development of a tool that is intuitive and completely free and open-source, so that scholars in literary studies, and indeed the broader humanities, can leverage computational methods and big data analytics in their research.</p>
            <p>nocht</p>
            <p>nocht (trans.: to reveal, uncover), is developed, primarily, in python, so that it is flexible, scalable, and cross-platform. it has been developed in accordance with the following principles:</p>
            <p> • it offers users a low-barrier means of using computational approaches to text analysis.</p>
            <p> • it is designed and developed in a humanities / arts / social sciences context.</p>
            <p> • it is completely open-source, removing the ‘black-box’, closed-code issues.</p>
            <p> • it brings together existing libraries and code-sets, acting as a ‘script portal’ of sorts.</p>
            <p>at present, nocht supports the following methodologies, though with some limitations:</p>
            <p> • wordcount and most frequent wordlists.</p>
            <p> • word / wordlist frequency plotting.</p>
            <p> • syntax and sentence analysis.</p>
            <p> • sentiment analysis.</p>
            <p> • topic modeling.</p>
            <p> • zeta analysis.</p>
            <p>it is hoped that nocht will further contribute to our field’s ongoing commitment to open and sustainable research tools, complementing highly regarded projects like voyant
                <hi rend=""superscript"">1</hi> and stylo (eder et al., 2013). its name is an obvious tribute to the former, which has for so long been one of our field’s fundamental tools. it is hoped that nocht will add further to the dh toolkit, as well as complement the ongoing work of voyant’s creators in leveraging the ipython architecture. 
            </p>
            <p>from a technical perspective, nocht is scalable and robust, and satisfies the needs of a wide range of scholars, many of whom wish to conduct this form of research but lack the expertise or resources to do so. in this respect, it enables scholars, both emerging and established, to engage with cutting-edge analyses across a variety of disciplines. in many cases, it draws on a series of existing libraries and proven methodologies, such as nltk
                <hi rend=""superscript"">2</hi> and matplotlib,
                <hi rend=""superscript"">3</hi> and so acts as a set of original scripts as well as a portal to existing tools. a complete technical overview of the project’s features, as well as the components utilised in its modular development, will be provided at the session. 
            </p>
            <p>discussion</p>
            <p>this poster proposes to introduce nocht to the field, discussing possible future development directions, as well as issues to date. some of the disciplinary particularities identified by gibbs and owens (2012), such as our need to enhance the usability of our tools, will be addressed. the tension between having an intuitive interface and the need for scholarly tools to produce verifiable results is particularly clear in this project. while there is a long-established requirement that such tools be user-friendly (krug 2005), one might argue that this must be balanced with a commitment to avoiding ‘black-box’ projects; usability does not necessarily equate to understanding. </p>
            <p>measuring the value and success of development projects also remains problematic for scholars and practitioners working across the digital humanities. schreibman and hanlon’s survey (2010) finds that the majority of respondents were satisfied that their tools had been ‘successful’, enabling themselves and others to further their research. however, respondents also outlined that they had measured this success from a ‘controlled list’. as our methods continue to gain prominence beyond the core digital humanities community, we must find new metrics through which we can reliably measure the impact of our tools, not just in terms of user volumes, but in relation to the quality of research output. as a project that has sacrificed some aspects of usability and marketability in favour of broad functionality and a commitment to open principles, perhaps to its detriment, nocht is an ideal catalyst for this debate. it is a small development with limited financial support, so it will be interesting to see if projects of this scale have a future in our discipline.</p>
            <p>notes</p>
            <p>1. see stéfan sinclair and geoffrey rockwell, http://voyant-tools.org/.</p>
            <p>2. natural language toolkit, http://www.nltk.org/.</p>
            <p>3. matplotlib, http://matplotlib.org/.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl>
                        <hi rend=""bold"">eder, m., kestemont, m. and rybicki, j.</hi> (2013). stylometry with r: a suite of tools. 
                        <hi rend=""italic"">digital humanities 2013: conference abstracts</hi>, university of nebraska–lincoln, pp. 487–89.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">gibbs, f. and owens, t.</hi> building better digital humanities tools: toward broader audiences and user-centered designs. 
                        <hi rend=""italic"">digital humanities quarterly,</hi>
                        <hi rend=""bold"">6</hi>(2).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">krug, s.</hi> (2005). 
                        <hi rend=""italic"">don’t make me think! a common sense approach to web usability</hi>. 2nd ed. new riders press, new york.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">schreibman, s. and hanlon, a. m.</hi> (2010). determining value for digital humanities tools: report on a survey of tool developers. 
                        <hi rend=""italic"">digital humanities quarterly,</hi>
                        <hi rend=""bold"">4</hi>(2).
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
</tei>",2.0,3.0,Voyant
2339,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,"Queen Luise of Prussia, a Digital Hagiography",https://github.com/ADHO/dh2015/blob/master/xml/ASKEY_Jennifer_D_Queen_Luise_of_Prussia__a_Digital_Hagi.xml,Jennifer D Askey,"paper, specified ""short paper""","<?xml version=""1.0"" encoding=""UTF-8""?>
<TEI xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Queen Luise of Prussia, a Digital Hagiography</title>
                <author>
                    <persName>
                        <surname>Askey</surname>
                        <forename>Jennifer D</forename>
                    </persName>
                    <affiliation>McMaster University, Canada</affiliation>
                    <email>askeyj@mcmaster.ca</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident=""DHCONVALIDATOR"" version=""1.9"">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme=""ConfTool"" n=""category"">
                    <term>Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""subcategory"">
                    <term>Short Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""keywords"">
                    <term>German history</term>
                    <term>digitization</term>
                    <term>nineteenth century</term>
                    <term>fraktur</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""topics"">
                    <term>literary studies</term>
                    <term>digitisation</term>
                    <term>resource creation</term>
                    <term>and discovery</term>
                    <term>text analysis</term>
                    <term>authorship attribution / authority</term>
                    <term>german studies</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>Popular biographies of Queen Luise of Prussia (1776–1810, r. 1797–1810) written in the second half of the nineteenth century uniformly adhere to what Wulf Wülfing has called a ‘stations of the cross’ structure (Wülfing, 1984). Independent of whether the biography focuses on her domestic life, her personal character and temperament, or her influence at court, it relates and pays homage to canonical moments in Luise’s life story: her childhood encounter with Goethe’s mother, her presence at the coronation of the last Holy Roman Emperor, her meeting with her future husband, and several instances of informal, uncourtly conduct upon becoming princess. Biographies written for a variety of intended audiences (the German people, young Germans, young ladies, etc.) all adhere to the same narrative thread. While biographies on the same subject might be expected to follow a similar time line, popular treatments of Luise’s life and times tend to treat the same set of life anecdotes using the same language (Askey, 2013). While Luise’s function as a secular Prussian saint is one reason for the similarity of these biographies, another is their grounding in the same foundational texts. After Luise’s death in 1810, two biographies written by members of the Prussian court reached the public. King Friedrich Wilhelm III’s court pastor, Ruleman Friedrich Eylert, published 
                <hi rend=""italic"">Charakter-Züge und historische Fragmente aus dem Leben des Königs von Preussen Friedrich Wilhelm III </hi>(Characteristics and Historical Fragments form the Life of King Friedrich Wilhelm III of Prussia), a multivolume court history that contained over one volume of information on Queen Luise. And Luise’s lady in waiting, Countess Sophie Marie von Voss, published 
                <hi rend=""italic"">Neunundsechzig Jahre am preussischen Hofe; aus den Erinnerungen der Oberhofmeisterin Sophie Marie Gräfin von Voss </hi>(Sixty-Nine Years at the Prussian Court: The Memories of the Senior Lady in Waiting Countess Sophie Marie von Voss)
                <hi rend=""italic"">.</hi> This poster will introduce a digitization and textual analysis project that brings court and popular biographies into conversation with one another. 
            </p>
            <p> While the court biographies are available digitally, most of the popular biographies, especially those for children, are not. Libraries in the nineteenth century did not generally collect children’s literature, and so my personal collection of around a dozen Luise biographies for young readers will be digitized. Our team at the Sherman Centre for Digital Scholarship will OCR the biographies, experimenting with ABBYY, OmniPage, and Tesseract to achieve optimal results with the notoriously difficult German 
                <hi rend=""italic"">Fraktur</hi> typescript. Once the corpus of official court and popular biographies has been created, we will perform standard text mining on the body of texts for young people to confirm or refute personal close readings. Our text mining queries will focus on the frequency of selected key terms relating to Luise’s life story, as well as the frequency of certain gendered textual markers (such as the word 
                <hi rend=""italic"">Gemüt</hi>—temperament, or words associated with clothing or fashion). The information gleaned from text mining will contribute to a scholarly discussion not only on the queen but on exemplarity, female childhood, and the gender discourse of the public sphere in the long nineteenth century. 
            </p>
            <p> The final step involves running subsets of the popular biographies (for the general public, for young people) and the court biographies through a comparison engine such as Juxta to determine adherence or deviation from a supposed ur-text. In the context of my previous work on Queen Luise and her literary function as exemplar for young German women, my hopes for the comparison step of the process are quite high. I use Voyant and Juxta to focus on specific language tokens that provide insight into gender norms and expectations, references to Luise’s physicality, and the development of nationalist discourse (apparent where the queen’s use of French at the court is set against her domestic use of German). </p>
            <p> The poster will illustrate the resource creation process (digitization and OCR of Fraktur texts, accessibility of that corpus for other scholars) as well as the first stages of textual analysis using the corpus. </p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Askey, J.</hi> (2013). Good Girls, Good Germans: Girls’ Education and Emotional Nationalism in Wilhelminan Germany. Camden House, Rochester, NY. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Eylert, R.</hi> (1844). Charakter-Züge und historische Fragmente aus dem Leben des Königs von Preußen Friedrich Wilhelm III: Gesammelt nach eigenen Beobachtungen und selbstgemachten Erfarhungen; wohlfeile Ausgabe für das Volk. Heinrichshofenschen Buchhandlung, Magdeburg.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Voß, S.</hi> (1876). Neunundsechsig Jahre am Preußischem Hofe: Aus den Erinnerungen der Oberhofmeisterin Sophie Marie Gräfin von Voß. Dunker und Humbolt, Leipzig.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Wülfing, W.</hi> (1984). ‘Die Heilige Luise von Preußen: Zur Mythisierung einer figure der Geschichte in der deutschen Literatur des 19. Jahrhunderts’: Bewegung und Stillstand in Metaphern und Mythen: Fallstudien zum Verhältnis von elementarem Wissen und Literatur im 19. Jahrhundert. Stuttgart: Klett-Cotta. 
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>",xml,Creative Commons Attribution 4.0 International,,digitization;fraktur;german history;nineteenth century,English,"authorship attribution / authority;digitisation, resource creation, and discovery;english;german studies;literary studies;text analysis",2015-01-01,"<?xml version=""1.0"" encoding=""utf-8""?>
<tei xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiheader>
        <filedesc>
            <titlestmt>
                <title>queen luise of prussia, a digital hagiography</title>
                <author>
                    <persname>
                        <surname>askey</surname>
                        <forename>jennifer d</forename>
                    </persname>
                    <affiliation>mcmaster university, canada</affiliation>
                    <email>askeyj@mcmaster.ca</email>
                </author>
            </titlestmt>
            <editionstmt>
                <edition>
                    <date>2014-12-19t13:50:00z</date>
                </edition>
            </editionstmt>
            <publicationstmt>
                <publisher>paul arthur, university of western sidney</publisher>
                <address>
                    <addrline>locked bag 1797</addrline>
                    <addrline>penrith nsw 2751</addrline>
                    <addrline>australia</addrline>
                    <addrline>paul arthur</addrline>
                </address>
            </publicationstmt>
            <sourcedesc>
                <p>converted from a word document </p>
            </sourcedesc>
        </filedesc>
        <encodingdesc>
            <appinfo>
                <application ident=""dhconvalidator"" version=""1.9"">
                    <label>dhconvalidator</label>
                </application>
            </appinfo>
        </encodingdesc>
        <profiledesc>
            <textclass>
                <keywords scheme=""conftool"" n=""category"">
                    <term>paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""subcategory"">
                    <term>short paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""keywords"">
                    <term>german history</term>
                    <term>digitization</term>
                    <term>nineteenth century</term>
                    <term>fraktur</term>
                </keywords>
                <keywords scheme=""conftool"" n=""topics"">
                    <term>literary studies</term>
                    <term>digitisation</term>
                    <term>resource creation</term>
                    <term>and discovery</term>
                    <term>text analysis</term>
                    <term>authorship attribution / authority</term>
                    <term>german studies</term>
                    <term>english</term>
                </keywords>
            </textclass>
        </profiledesc>
    </teiheader>
    <text>
        <body>
            <p>popular biographies of queen luise of prussia (1776–1810, r. 1797–1810) written in the second half of the nineteenth century uniformly adhere to what wulf wülfing has called a ‘stations of the cross’ structure (wülfing, 1984). independent of whether the biography focuses on her domestic life, her personal character and temperament, or her influence at court, it relates and pays homage to canonical moments in luise’s life story: her childhood encounter with goethe’s mother, her presence at the coronation of the last holy roman emperor, her meeting with her future husband, and several instances of informal, uncourtly conduct upon becoming princess. biographies written for a variety of intended audiences (the german people, young germans, young ladies, etc.) all adhere to the same narrative thread. while biographies on the same subject might be expected to follow a similar time line, popular treatments of luise’s life and times tend to treat the same set of life anecdotes using the same language (askey, 2013). while luise’s function as a secular prussian saint is one reason for the similarity of these biographies, another is their grounding in the same foundational texts. after luise’s death in 1810, two biographies written by members of the prussian court reached the public. king friedrich wilhelm iii’s court pastor, ruleman friedrich eylert, published 
                <hi rend=""italic"">charakter-züge und historische fragmente aus dem leben des königs von preussen friedrich wilhelm iii </hi>(characteristics and historical fragments form the life of king friedrich wilhelm iii of prussia), a multivolume court history that contained over one volume of information on queen luise. and luise’s lady in waiting, countess sophie marie von voss, published 
                <hi rend=""italic"">neunundsechzig jahre am preussischen hofe; aus den erinnerungen der oberhofmeisterin sophie marie gräfin von voss </hi>(sixty-nine years at the prussian court: the memories of the senior lady in waiting countess sophie marie von voss)
                <hi rend=""italic"">.</hi> this poster will introduce a digitization and textual analysis project that brings court and popular biographies into conversation with one another. 
            </p>
            <p> while the court biographies are available digitally, most of the popular biographies, especially those for children, are not. libraries in the nineteenth century did not generally collect children’s literature, and so my personal collection of around a dozen luise biographies for young readers will be digitized. our team at the sherman centre for digital scholarship will ocr the biographies, experimenting with abbyy, omnipage, and tesseract to achieve optimal results with the notoriously difficult german 
                <hi rend=""italic"">fraktur</hi> typescript. once the corpus of official court and popular biographies has been created, we will perform standard text mining on the body of texts for young people to confirm or refute personal close readings. our text mining queries will focus on the frequency of selected key terms relating to luise’s life story, as well as the frequency of certain gendered textual markers (such as the word 
                <hi rend=""italic"">gemüt</hi>—temperament, or words associated with clothing or fashion). the information gleaned from text mining will contribute to a scholarly discussion not only on the queen but on exemplarity, female childhood, and the gender discourse of the public sphere in the long nineteenth century. 
            </p>
            <p> the final step involves running subsets of the popular biographies (for the general public, for young people) and the court biographies through a comparison engine such as juxta to determine adherence or deviation from a supposed ur-text. in the context of my previous work on queen luise and her literary function as exemplar for young german women, my hopes for the comparison step of the process are quite high. i use voyant and juxta to focus on specific language tokens that provide insight into gender norms and expectations, references to luise’s physicality, and the development of nationalist discourse (apparent where the queen’s use of french at the court is set against her domestic use of german). </p>
            <p> the poster will illustrate the resource creation process (digitization and ocr of fraktur texts, accessibility of that corpus for other scholars) as well as the first stages of textual analysis using the corpus. </p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl>
                        <hi rend=""bold"">askey, j.</hi> (2013). good girls, good germans: girls’ education and emotional nationalism in wilhelminan germany. camden house, rochester, ny. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">eylert, r.</hi> (1844). charakter-züge und historische fragmente aus dem leben des königs von preußen friedrich wilhelm iii: gesammelt nach eigenen beobachtungen und selbstgemachten erfarhungen; wohlfeile ausgabe für das volk. heinrichshofenschen buchhandlung, magdeburg.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">voß, s.</hi> (1876). neunundsechsig jahre am preußischem hofe: aus den erinnerungen der oberhofmeisterin sophie marie gräfin von voß. dunker und humbolt, leipzig.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">wülfing, w.</hi> (1984). ‘die heilige luise von preußen: zur mythisierung einer figure der geschichte in der deutschen literatur des 19. jahrhunderts’: bewegung und stillstand in metaphern und mythen: fallstudien zum verhältnis von elementarem wissen und literatur im 19. jahrhundert. stuttgart: klett-cotta. 
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
</tei>",1.0,1.0,Voyant
2366,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,Talking About Programming in the Digital Humanities,https://github.com/ADHO/dh2015/blob/master/xml/ROCKWELL_Geoffrey_Talking_About_Programming_in_the_DIgi.xml,Geoffrey Rockwell;Stéfan Sinclair,"paper, specified ""long paper""","<?xml version=""1.0"" encoding=""UTF-8""?>
<TEI xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Talking About Programming in the DIgital Humanities</title>
                <author>
                    <persName>
                        <surname>Rockwell</surname>
                        <forename>Geoffrey</forename>
                    </persName>
                    <affiliation>University of Alberta, Canada</affiliation>
                    <email>geoffrey.rockwell@ualberta.ca</email>
                </author>
                <author>
                    <persName>
                        <surname>Sinclair</surname>
                        <forename>Stéfan</forename>
                    </persName>
                    <affiliation>McGill University</affiliation>
                    <email>stefan.sinclair@mcgill.ca</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident=""DHCONVALIDATOR"" version=""1.9"">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme=""ConfTool"" n=""category"">
                    <term>Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""subcategory"">
                    <term>Long Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""keywords"">
                    <term>History of DH</term>
                    <term>Programming</term>
                    <term>DH Courses</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""topics"">
                    <term>programming</term>
                    <term>history of Humanities Computing/Digital Humanities</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>In the first months of the HUMANIST discussion list a discussion erupted around the value of programming languages in humanities computing courses. Do computing humanists need to know how to program, or is learning expert use of applications enough? What is the point of learning to program? (McCarthy, 1987). This discussion can be traced back at least as far as the 1986 ACH/Sloan Workshop on Teaching ‘Computers and the Humanities’ Courses, where participants discussed ‘most vigorously of all whether programming should be taught, or only package programs’ (Sperberg-McQueen 1986). The issue has really never gone away as it is about disciplinary formation or ‘what it is we want our students to learn, the nature of the world into which we are sending them, and the relationship both of technology and (more fundamentally) the algorithmic approach to problem-solving’ (McCarty, 1987, 1–2). Based on a reading and analysis of a substantial collection of historical documents (journal articles, association newsletters, books, Listservs, etc.) in this paper we will examine how the humanities computing community discussed programming and how that discussion reflects changing views of the field over time. Specifically we will</p>
            <p> • Outline a three-part history of programming in the digital humanities starting with a concording phase when programming was not important to using computers.</p>
            <p> • Discuss the turn toward teaching programming as a way of teaching computational thinking.</p>
            <p> • Look at how the emergence of the Web changed the discussion. We will argue that as the Web became important, scripting skills suitable for building websites replaced older programming languages and it became possible to imagine the digital humanities being defined by the ability to program. </p>
            <p> • Conclude with some reflections on how the case for programming has been made more recently. </p>
            <p>
                <hi rend=""bold"">Humanities Computing as Concording</hi>
            </p>
            <p>I’ve tried to stay free of programming. I’m perfectly innocent of any knowledge of any programming language and I feel I must remain that way if I am going to continue to function as a scholar in literature; my heartfelt advice to any of you younger people embarking on this whole venture is to do the same—to ask programmers to do for you what they know how to do and what would be costly and painful for you to learn—to learn to talk their language but not to get involved in programming research or machine research. I know the best of you will not take this advice and will, therefore, break new barriers and so on, but I persist in offering it. (Parrish, 1969, 24–25) </p>
            <p>Programming in the early concording years of computing in the humanities was something often left to specialists. Stephen Parrish, who was general editor of the Cornell Concordance Series, saw himself as a ‘scholar in English literature who drifted laterally into the making of concordances’ (1969, 16). He stayed away from programming as a distraction from scholarship. This meant that concording ‘required an organized, teamwork approach, characterized by a substantial budget and a university computing centre’ (McCarty, 1993, 52–53). Parrish is not the only one to worry about programming distracting scholars, and in the presentation we will discuss other examples as a way of teasing out how programming was not considered of scholarly value. </p>
            <p>
                <hi rend=""bold"">Programming and Reasoning</hi>
            </p>
            <p>By the 1980s, things had changed and programming had become important, as can be seen in the way programming was included in a majority of courses. Surveys like Joseph Rudman’s ‘Teaching Computers and the Humanities Courses: A Survey’ (1987) report 175 courses teaching a programming language while 131 were ‘Applications Only’. In that survey, the most popular languages taught were BASIC (in 75 courses reporting), Pascal (30 courses), Prolog (15 courses), Lisp (13 courses), and SNOBOL (10 courses). What is impressive is the number of responses that indicated they had some sort of course for humanities students. We often assume that computing in the humanities is a recent, post-advent-of-the-Web thing, but given the number of courses, the discussions on HUMANIST, and articles in journals like 
                <hi rend=""italic"">Computing and the Humanities</hi>, it is clear that programming was becoming an acceptable activity, especially for those in support staff positions. 
            </p>
            <p>Despite the popularity of programming languages like BASIC and Pascal for courses, the languages about which humanists wrote in the 1980s tended to be languages like SNOBOL and Icon—all languages that were good for string (text) manipulation, as can be seen in the attention they received in books for teaching programming to humanists like John Abercrombie’s 
                <hi rend=""italic"">Computer Programs for Literary Analysis</hi> (1984)
                <hi rend=""italic"">,</hi> Susan Hockey’s 
                <hi rend=""italic"">SNOBOL Programming for the Humanities</hi> (1985)
                <hi rend=""italic"">,</hi> and Alan Corré’s 
                <hi rend=""italic"">Icon Programming for Humanists</hi> (1990)
                <hi rend=""italic"">.</hi> Then there are the discussions on HUMANIST and reviews of languages like Mark Olsen’s self-explanatory ‘Beyond SNOBOL: The Icon Programming Language’. 
            </p>
            <p>As an aside, one could argue that SGML (Standard Generalized Markup Language) and later XML (Extensible Markup Language) are also forms of programming—meta-languages with which one can create descriptive languages with which to rigorously describe texts for scholarly electronic editions. These were popular in humanities computing, especially after the Text Encoding Initiative began to provide guidelines for the encoding of texts in the late 1980s. How many humanists were first introduced to computing in the humanities when asked to develop a DTD (Document Type Definition) and encode a text? </p>
            <p>It is interesting that many of the discussions about programming in the 1980s and early 1990s circle around the teaching of it and that this issue is reported as contentious. No one believed that the ability to program was essential for a computing humanist, in part because so many couldn’t, but proponents of programming argued for it to be taught as way of teaching computational reasoning or problem solving. They also argued that there were social and ethical issues that could be understood through learning computing. You learned to program so as to understand what the computer could do or to be able to talk to programmers. In the presentation we will look at summative discussions of the issue like McCarty’s (1987) and a lovely balanced essay by Nancy Ide, ‘Computers and the Humanities Courses: Philosophical Bases and Approach’ that came out of the 1986 ACH/Sloan Workshop. We will also speculate as to why there was this shift towards including programming and arguing for its importance. Whatever the reasons, the discourse had changed and computing humanists were beginning to take an interest in programming and teaching it. </p>
            <p>
                <hi rend=""bold"">Programming the Web </hi>
            </p>
            <p>The emergence of the Web changed the languages that humanists were likely to use for programming and, we will argue, made it possible to make programming a defining skill for digital humanists. Moreover, learning to program for the Web provided a more appealing and transferable skillset; web and application development became normalized and desirable. Because the Web provided such a convenient way to show and distribute digital research, it changed which programming languages received attention or were taught. We can see the shift in these histograms of word frequencies in the HUMANIST Archives. The string processing languages popular in the late 1980s taper off and are replaced with languages like PHP with which one can build dynamic websites. </p>
            <figure>
                <graphic n=""1001"" width=""16.002cm"" height=""7.254875cm"" url=""Pictures/image1.png"" rend=""inline""/>
            </figure>
            <figure>
                <graphic n=""1002"" width=""16.002cm"" height=""7.226652777777778cm"" url=""Pictures/image2.png"" rend=""inline""/>
            </figure>
            <p>Figure 1. Two graphs showing the frequency of programming languages in the annual archives of the HUMANIST Discussion Group Listserv, produced with Voyant Tools. </p>
            <p>Now programming (and other web skills) went from being about teaching reasoning to being a skill students and humanists could actually use to create humanities products, i.e., websites. Programming became creative and expressive. There was also a convenient on-ramp as students could start creating HTML pages and then learn to use scripting languages like PHP that enhanced the web site until you knew enough to code a scholarly web site. </p>
            <p>We will further argue that once web development wasn’t something that digital humanists would necessarily be amateurs at, then it could become a defining skill, and a source of some pride. Digital humanists finally had something that they could be good at and a set of competencies unique to the digital humanities. Digital humanists like Stephen Ramsay could provocatively say, ‘Do you have to know how to code? I’m a tenured professor of Digital Humanities and I say “yes”’ (Ramsay 2011). This led to the ‘hack vs. yack’ discussion that followed about programming and other forms of building (see Nowviskie [2014] for more general context). </p>
            <p>
                <hi rend=""bold"">Conclusions: Programming in the Humanities and Disciplinary Formation </hi>
            </p>
            <p>This paper traces the ways programming has been discussed in the digital humanities. We believe that the role of programming was important to the way the field of humanities computing (and later digital humanities) conceived of itself. This is not surprising given the importance of programming to computing in general, but it is interesting to follow the particular ways programming was discussed as the discipline emerged. </p>
            <p>We will end the paper by theorizing, or at least speculating, about where programming in digital humanities is going. One direction (already well under way) is towards software studies and the studying of programmed artifacts as works of human art and expression worthy of the humanities. Matthew Kirschenbaum in a fine essay for the 
                <hi rend=""italic"">Chronicle of Higher Education</hi> talks about ‘procedural literacy’. 
            </p>
            <p>All programming entails world-making, as the ritual act of writing and running Hello World reminds us. . . . </p>
            <p> Ultimately what’s at stake is not the kind of vocational computer literacy I was taught as an undergraduate, but what a growing number of practitioners in the digital humanities (and related disciplines, like digital art or game studies) have begun to call procedural rhetoric, or procedural literacy. (Kirschenbaum, 2009) </p>
            <p>Another direction is data science. The opportunities for new insights through large-scale text mining, or what Moretti (2007) calls distant reading, have made programming languages like R attractive. There is a return to text processing languages, but now languages that can analyze large corpora or visualize results.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Ide, N. M.</hi> (1987). Computers and the Humanities Courses: Philosophical Bases and Approach. 
                        <hi rend=""italic"">Computers and the Humanities,</hi>
                        <hi rend=""bold"">21</hi>(4): 209–15. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kirschenbaum, M.</hi> (2009). Hello Worlds. 
                        <hi rend=""italic"">Chronicle of Higher Education,</hi>
                        <hi rend=""bold"">55</hi>(20): B10, http://chronicle.com/article/Hello-Worlds/5476. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">McCarty, W.</hi> (1987). HUMANIST So Far: A Review of the First Two Months. 
                        <hi rend=""italic"">ACH Newsletter, </hi>
                        <hi rend=""bold"">9</hi>(3): 1–3. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">McCarty, W.</hi> (1993). Handmade, Computer-Assisted, and Electronic Concordances of Chaucer. 
                        <hi rend=""italic"">CCH Working Papers,</hi>
                        <hi rend=""bold"">3</hi>: 49–65. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Moretti, F.</hi> (2007). 
                        <hi rend=""italic"">Graphs, Maps, Trees: Abstract Models for Literary History</hi>. Verso, London. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Nowviskie, B.</hi> (2014). On the Origin of ‘Hack’ and ‘Yack’. http://bit.ly/1ttih76.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Olsen, M.</hi> (1987). Beyond SNOBOL: The Icon Programming Language. 
                        <hi rend=""italic"">Computers and the Humanities,</hi>
                        <hi rend=""bold"">21</hi>(1): 61–66. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Parrish, S. M.</hi> (1969). Concordance-Making by Computer: Its Past, Future, Techniques, and Applications. In Burelbach, F. M. (ed.), 
                        <hi rend=""italic"">Proceedings: Computer Applications to Problems in the Humanities; A Conversation in the Disciplines</hi>, Conference at State University College, Brockport, NY, 4–5 April 1969, pp. 16–33. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ramsay, S.</hi> (2011). On Building. http://stephenramsay.us/text/2011/01/11/on-building/. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Rudman, J.</hi> (1987). Teaching Computers and the Humanities Courses: A Survey. 
                        <hi rend=""italic"">Computer and the Humanities, </hi>
                        <hi rend=""bold"">21</hi>(4): 235–43. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sinclair, S. and G. Rockwell.</hi> (2014). 
                        <hi rend=""italic"">Voyant Tools</hi>. http://voyant-tools.org/. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(222222)"">Sperberg-McQueen, C. M.</hi>
                        <hi rend=""color(222222)""> (1986). Report on ACH/Sloan Foundation Workshop on Teaching ‘Computers and the Humanities’ Courses. </hi>
                        <hi rend=""italic color(222222)"">ACH Newsletter,</hi>
                        <hi rend=""bold color(222222)"">8</hi>
                        <hi rend=""color(222222)"">(4): 1–2.</hi>
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>",xml,Creative Commons Attribution 4.0 International,,dh courses;history of dh;programming,English,english;history of humanities computing/digital humanities;programming,2015-01-01,"<?xml version=""1.0"" encoding=""utf-8""?>
<tei xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiheader>
        <filedesc>
            <titlestmt>
                <title>talking about programming in the digital humanities</title>
                <author>
                    <persname>
                        <surname>rockwell</surname>
                        <forename>geoffrey</forename>
                    </persname>
                    <affiliation>university of alberta, canada</affiliation>
                    <email>geoffrey.rockwell@ualberta.ca</email>
                </author>
                <author>
                    <persname>
                        <surname>sinclair</surname>
                        <forename>stéfan</forename>
                    </persname>
                    <affiliation>mcgill university</affiliation>
                    <email>stefan.sinclair@mcgill.ca</email>
                </author>
            </titlestmt>
            <editionstmt>
                <edition>
                    <date>2014-12-19t13:50:00z</date>
                </edition>
            </editionstmt>
            <publicationstmt>
                <publisher>paul arthur, university of western sidney</publisher>
                <address>
                    <addrline>locked bag 1797</addrline>
                    <addrline>penrith nsw 2751</addrline>
                    <addrline>australia</addrline>
                    <addrline>paul arthur</addrline>
                </address>
            </publicationstmt>
            <sourcedesc>
                <p>converted from a word document </p>
            </sourcedesc>
        </filedesc>
        <encodingdesc>
            <appinfo>
                <application ident=""dhconvalidator"" version=""1.9"">
                    <label>dhconvalidator</label>
                </application>
            </appinfo>
        </encodingdesc>
        <profiledesc>
            <textclass>
                <keywords scheme=""conftool"" n=""category"">
                    <term>paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""subcategory"">
                    <term>long paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""keywords"">
                    <term>history of dh</term>
                    <term>programming</term>
                    <term>dh courses</term>
                </keywords>
                <keywords scheme=""conftool"" n=""topics"">
                    <term>programming</term>
                    <term>history of humanities computing/digital humanities</term>
                    <term>english</term>
                </keywords>
            </textclass>
        </profiledesc>
    </teiheader>
    <text>
        <body>
            <p>in the first months of the humanist discussion list a discussion erupted around the value of programming languages in humanities computing courses. do computing humanists need to know how to program, or is learning expert use of applications enough? what is the point of learning to program? (mccarthy, 1987). this discussion can be traced back at least as far as the 1986 ach/sloan workshop on teaching ‘computers and the humanities’ courses, where participants discussed ‘most vigorously of all whether programming should be taught, or only package programs’ (sperberg-mcqueen 1986). the issue has really never gone away as it is about disciplinary formation or ‘what it is we want our students to learn, the nature of the world into which we are sending them, and the relationship both of technology and (more fundamentally) the algorithmic approach to problem-solving’ (mccarty, 1987, 1–2). based on a reading and analysis of a substantial collection of historical documents (journal articles, association newsletters, books, listservs, etc.) in this paper we will examine how the humanities computing community discussed programming and how that discussion reflects changing views of the field over time. specifically we will</p>
            <p> • outline a three-part history of programming in the digital humanities starting with a concording phase when programming was not important to using computers.</p>
            <p> • discuss the turn toward teaching programming as a way of teaching computational thinking.</p>
            <p> • look at how the emergence of the web changed the discussion. we will argue that as the web became important, scripting skills suitable for building websites replaced older programming languages and it became possible to imagine the digital humanities being defined by the ability to program. </p>
            <p> • conclude with some reflections on how the case for programming has been made more recently. </p>
            <p>
                <hi rend=""bold"">humanities computing as concording</hi>
            </p>
            <p>i’ve tried to stay free of programming. i’m perfectly innocent of any knowledge of any programming language and i feel i must remain that way if i am going to continue to function as a scholar in literature; my heartfelt advice to any of you younger people embarking on this whole venture is to do the same—to ask programmers to do for you what they know how to do and what would be costly and painful for you to learn—to learn to talk their language but not to get involved in programming research or machine research. i know the best of you will not take this advice and will, therefore, break new barriers and so on, but i persist in offering it. (parrish, 1969, 24–25) </p>
            <p>programming in the early concording years of computing in the humanities was something often left to specialists. stephen parrish, who was general editor of the cornell concordance series, saw himself as a ‘scholar in english literature who drifted laterally into the making of concordances’ (1969, 16). he stayed away from programming as a distraction from scholarship. this meant that concording ‘required an organized, teamwork approach, characterized by a substantial budget and a university computing centre’ (mccarty, 1993, 52–53). parrish is not the only one to worry about programming distracting scholars, and in the presentation we will discuss other examples as a way of teasing out how programming was not considered of scholarly value. </p>
            <p>
                <hi rend=""bold"">programming and reasoning</hi>
            </p>
            <p>by the 1980s, things had changed and programming had become important, as can be seen in the way programming was included in a majority of courses. surveys like joseph rudman’s ‘teaching computers and the humanities courses: a survey’ (1987) report 175 courses teaching a programming language while 131 were ‘applications only’. in that survey, the most popular languages taught were basic (in 75 courses reporting), pascal (30 courses), prolog (15 courses), lisp (13 courses), and snobol (10 courses). what is impressive is the number of responses that indicated they had some sort of course for humanities students. we often assume that computing in the humanities is a recent, post-advent-of-the-web thing, but given the number of courses, the discussions on humanist, and articles in journals like 
                <hi rend=""italic"">computing and the humanities</hi>, it is clear that programming was becoming an acceptable activity, especially for those in support staff positions. 
            </p>
            <p>despite the popularity of programming languages like basic and pascal for courses, the languages about which humanists wrote in the 1980s tended to be languages like snobol and icon—all languages that were good for string (text) manipulation, as can be seen in the attention they received in books for teaching programming to humanists like john abercrombie’s 
                <hi rend=""italic"">computer programs for literary analysis</hi> (1984)
                <hi rend=""italic"">,</hi> susan hockey’s 
                <hi rend=""italic"">snobol programming for the humanities</hi> (1985)
                <hi rend=""italic"">,</hi> and alan corré’s 
                <hi rend=""italic"">icon programming for humanists</hi> (1990)
                <hi rend=""italic"">.</hi> then there are the discussions on humanist and reviews of languages like mark olsen’s self-explanatory ‘beyond snobol: the icon programming language’. 
            </p>
            <p>as an aside, one could argue that sgml (standard generalized markup language) and later xml (extensible markup language) are also forms of programming—meta-languages with which one can create descriptive languages with which to rigorously describe texts for scholarly electronic editions. these were popular in humanities computing, especially after the text encoding initiative began to provide guidelines for the encoding of texts in the late 1980s. how many humanists were first introduced to computing in the humanities when asked to develop a dtd (document type definition) and encode a text? </p>
            <p>it is interesting that many of the discussions about programming in the 1980s and early 1990s circle around the teaching of it and that this issue is reported as contentious. no one believed that the ability to program was essential for a computing humanist, in part because so many couldn’t, but proponents of programming argued for it to be taught as way of teaching computational reasoning or problem solving. they also argued that there were social and ethical issues that could be understood through learning computing. you learned to program so as to understand what the computer could do or to be able to talk to programmers. in the presentation we will look at summative discussions of the issue like mccarty’s (1987) and a lovely balanced essay by nancy ide, ‘computers and the humanities courses: philosophical bases and approach’ that came out of the 1986 ach/sloan workshop. we will also speculate as to why there was this shift towards including programming and arguing for its importance. whatever the reasons, the discourse had changed and computing humanists were beginning to take an interest in programming and teaching it. </p>
            <p>
                <hi rend=""bold"">programming the web </hi>
            </p>
            <p>the emergence of the web changed the languages that humanists were likely to use for programming and, we will argue, made it possible to make programming a defining skill for digital humanists. moreover, learning to program for the web provided a more appealing and transferable skillset; web and application development became normalized and desirable. because the web provided such a convenient way to show and distribute digital research, it changed which programming languages received attention or were taught. we can see the shift in these histograms of word frequencies in the humanist archives. the string processing languages popular in the late 1980s taper off and are replaced with languages like php with which one can build dynamic websites. </p>
            <figure>
                <graphic n=""1001"" width=""16.002cm"" height=""7.254875cm"" url=""pictures/image1.png"" rend=""inline""/>
            </figure>
            <figure>
                <graphic n=""1002"" width=""16.002cm"" height=""7.226652777777778cm"" url=""pictures/image2.png"" rend=""inline""/>
            </figure>
            <p>figure 1. two graphs showing the frequency of programming languages in the annual archives of the humanist discussion group listserv, produced with voyant tools. </p>
            <p>now programming (and other web skills) went from being about teaching reasoning to being a skill students and humanists could actually use to create humanities products, i.e., websites. programming became creative and expressive. there was also a convenient on-ramp as students could start creating html pages and then learn to use scripting languages like php that enhanced the web site until you knew enough to code a scholarly web site. </p>
            <p>we will further argue that once web development wasn’t something that digital humanists would necessarily be amateurs at, then it could become a defining skill, and a source of some pride. digital humanists finally had something that they could be good at and a set of competencies unique to the digital humanities. digital humanists like stephen ramsay could provocatively say, ‘do you have to know how to code? i’m a tenured professor of digital humanities and i say “yes”’ (ramsay 2011). this led to the ‘hack vs. yack’ discussion that followed about programming and other forms of building (see nowviskie [2014] for more general context). </p>
            <p>
                <hi rend=""bold"">conclusions: programming in the humanities and disciplinary formation </hi>
            </p>
            <p>this paper traces the ways programming has been discussed in the digital humanities. we believe that the role of programming was important to the way the field of humanities computing (and later digital humanities) conceived of itself. this is not surprising given the importance of programming to computing in general, but it is interesting to follow the particular ways programming was discussed as the discipline emerged. </p>
            <p>we will end the paper by theorizing, or at least speculating, about where programming in digital humanities is going. one direction (already well under way) is towards software studies and the studying of programmed artifacts as works of human art and expression worthy of the humanities. matthew kirschenbaum in a fine essay for the 
                <hi rend=""italic"">chronicle of higher education</hi> talks about ‘procedural literacy’. 
            </p>
            <p>all programming entails world-making, as the ritual act of writing and running hello world reminds us. . . . </p>
            <p> ultimately what’s at stake is not the kind of vocational computer literacy i was taught as an undergraduate, but what a growing number of practitioners in the digital humanities (and related disciplines, like digital art or game studies) have begun to call procedural rhetoric, or procedural literacy. (kirschenbaum, 2009) </p>
            <p>another direction is data science. the opportunities for new insights through large-scale text mining, or what moretti (2007) calls distant reading, have made programming languages like r attractive. there is a return to text processing languages, but now languages that can analyze large corpora or visualize results.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl>
                        <hi rend=""bold"">ide, n. m.</hi> (1987). computers and the humanities courses: philosophical bases and approach. 
                        <hi rend=""italic"">computers and the humanities,</hi>
                        <hi rend=""bold"">21</hi>(4): 209–15. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">kirschenbaum, m.</hi> (2009). hello worlds. 
                        <hi rend=""italic"">chronicle of higher education,</hi>
                        <hi rend=""bold"">55</hi>(20): b10, http://chronicle.com/article/hello-worlds/5476. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">mccarty, w.</hi> (1987). humanist so far: a review of the first two months. 
                        <hi rend=""italic"">ach newsletter, </hi>
                        <hi rend=""bold"">9</hi>(3): 1–3. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">mccarty, w.</hi> (1993). handmade, computer-assisted, and electronic concordances of chaucer. 
                        <hi rend=""italic"">cch working papers,</hi>
                        <hi rend=""bold"">3</hi>: 49–65. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">moretti, f.</hi> (2007). 
                        <hi rend=""italic"">graphs, maps, trees: abstract models for literary history</hi>. verso, london. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">nowviskie, b.</hi> (2014). on the origin of ‘hack’ and ‘yack’. http://bit.ly/1ttih76.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">olsen, m.</hi> (1987). beyond snobol: the icon programming language. 
                        <hi rend=""italic"">computers and the humanities,</hi>
                        <hi rend=""bold"">21</hi>(1): 61–66. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">parrish, s. m.</hi> (1969). concordance-making by computer: its past, future, techniques, and applications. in burelbach, f. m. (ed.), 
                        <hi rend=""italic"">proceedings: computer applications to problems in the humanities; a conversation in the disciplines</hi>, conference at state university college, brockport, ny, 4–5 april 1969, pp. 16–33. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">ramsay, s.</hi> (2011). on building. http://stephenramsay.us/text/2011/01/11/on-building/. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">rudman, j.</hi> (1987). teaching computers and the humanities courses: a survey. 
                        <hi rend=""italic"">computer and the humanities, </hi>
                        <hi rend=""bold"">21</hi>(4): 235–43. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">sinclair, s. and g. rockwell.</hi> (2014). 
                        <hi rend=""italic"">voyant tools</hi>. http://voyant-tools.org/. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(222222)"">sperberg-mcqueen, c. m.</hi>
                        <hi rend=""color(222222)""> (1986). report on ach/sloan foundation workshop on teaching ‘computers and the humanities’ courses. </hi>
                        <hi rend=""italic color(222222)"">ach newsletter,</hi>
                        <hi rend=""bold color(222222)"">8</hi>
                        <hi rend=""color(222222)"">(4): 1–2.</hi>
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
</tei>",2.0,3.0,Voyant
2388,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,The Trials of Tokenization,https://github.com/ADHO/dh2015/blob/master/xml/HOOVER_David_L__The_Trials_of_Tokenization.xml,David L. Hoover,"paper, specified ""long paper""","<?xml version=""1.0"" encoding=""UTF-8""?>
<TEI xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>The Trials of Tokenization</title>
                <author>
                    <persName>
                        <surname>Hoover</surname>
                        <forename>David L.</forename>
                    </persName>
                    <affiliation>New York University, United States of America</affiliation>
                    <email>david.hoover@nyu.edu</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident=""DHCONVALIDATOR"" version=""1.9"">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme=""ConfTool"" n=""category"">
                    <term>Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""subcategory"">
                    <term>Long Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""keywords"">
                    <term>Python</term>
                    <term>tokenization</term>
                    <term>word frequency lists</term>
                    <term>programming</term>
                    <term>punctuation</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""topics"">
                    <term>natural language processing</term>
                    <term>software design and development</term>
                    <term>text analysis</term>
                    <term>programming</term>
                    <term>standards and interoperability</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p>The process of tokenizing texts is typically out of sight and almost out of mind—often handled invisibly by the analyst’s program or R script, and rarely described, discussed, or even mentioned. For ‘big data’, even if questions did arise about the nature of the word list produced, testing is not feasible. Furthermore, tokenizer accuracy is so critically affected by the state and nature of the texts that probably no general measure of accuracy or appropriateness is possible. Finally, built-in programming functions and libraries are all too often used uncritically with little realization that their output does not conform to the assumptions or expectations of the analyst. I suggest that we should pay a little more attention to the theory and practice of tokenization.
                <hi rend=""superscript"">1</hi>
            </p>
            <p>Consider a hypothetical case. Let’s say I want to analyze 5,000 novels, have access to the texts at HathiTrust, download 5,000 novels in plain text, and tokenize them. Below is part of a page from Elizabeth Gaskell’s 
                <hi rend=""italic"">Cranford</hi>, from HathiTrust (Gaskell, 1910 [1851], 107):
            </p>
            <figure>
                <graphic n=""1001"" width=""9.710208333333334cm"" height=""10.4775cm"" url=""Pictures/image1.png"" rend=""block""/>
            </figure>
            <p>Figure 1. 
                <hi rend=""italic"">Cranford</hi>, Elizabeth Gaskell, from page 107.
            </p>
            <p>A human reader would have little trouble tokenizing this passage, and it is not extremely problematic, though minor OCR problems exist (mainly spacing issues around single quotation marks / apostrophes and dashes, and the line-end hyphen). I tokenized this passage with The Intelligent Archive (2012), KWIC (Tsukamoto, 2004) WordSmith Tools (Scott, 2012), Voyant (Sinclair et al., 2012), and Stylo (Eder et al., 2014).
                <hi rend=""superscript"">2</hi> Even on this short text, the five programs identify three different numbers of types and two different numbers of tokens, largely because of the handling of single quotation marks. KWIC and WordSmith produce identical lists, as do Voyant and Stylo, but neither of these match The Intelligent Archive.
            </p>
            <p>Now consider Charles Chesnutt’s 
                <hi rend=""italic"">The House Behind the Cedars</hi> (1900, 13), also from HathiTrust:
            </p>
            <figure>
                <graphic n=""1002"" width=""8.89cm"" height=""8.810625cm"" url=""Pictures/image2.png"" rend=""block""/>
            </figure>
            <p>Figure 2. 
                <hi rend=""italic"">The House Behind the Cedars</hi>, Charles Chesnutt, from page 13.
            </p>
            <p>The dialect in this passage is challenging even for human readers, and the OCR is more problematic. For example, the printed text (judging from the PDF) had spaced contractions, which explains ‘you 're’ in the fourth line from the bottom and the space in ‘lie 's’ in the first line, where the text reads “he 's.” This classic OCR problem occurs several times in this novel. And in the last line ‘you '11’ has both a space and an erroneous number 11 for the ‘ll’ (double el), another common OCR problem. Those analyzing big data usually rely on the insignificance of random error, but these and many other kinds of error are not random, and systematic error within one text, one author, one genre, or one collection could easily lead to thousands of inaccurate word frequency counts in this hypothetical study of 5,000 texts.</p>
            <p>The use of apostrophes in the Chesnutt passage to indicate dialect pronunciations can also severely affect tokenization. Although The Intelligent Archive, KWIC, and WordSmith Tools produce exactly the same lists for this brief passage, and Voyant has the same number of types and tokens, Voyant removes all initial (but not final) apostrophes, creating different words for eight of the 97 types. Stylo removes all numbers, all initial and final apostrophes, and many internal apostrophes, retaining them only in 
                <hi rend=""italic"">ain^t, gentleman^s</hi>, and 
                <hi rend=""italic"">spen^s</hi> (replaced with a caret). It produces six more tokens and four more types than the other programs, and many more differences in the word list. Unfortunately, in Chesnutt’s short novel, more than 650 words begin and/or end with apostrophes crucial to the identity of the word, so that the word lists produced by Voyant and Stylo are quite inaccurate. Furthermore, only KWIC and WordSmith Tools let the user choose whether apostrophes and hyphens are part of a word, and whether numbers can appear in the word list or not. Only WordSmith Tools allows the user to choose whether to allow apostrophes at the beginnings and/or ends of the word as well as internally.
            </p>
            <p>Obviously, the two texts examined above cause different problems, and different tokenizers are more accurate for one than for the other. Worse yet, these problems are found even in relatively carefully edited texts like those from Project Gutenberg. Although Gutenberg’s 
                <hi rend=""italic"">The House Behind the Cedars</hi> does not have spaced contractions, and correctly has 
                <hi rend=""italic"">he’s</hi> in the first line and 
                <hi rend=""italic"">you’ll</hi> in the final line, the 29 initial and final dialect apostrophes remain problematic. The Gutenberg text also represents dashes as two hyphens without spaces, creating more problems for tokenizers. The Intelligent Archive and Stylo treat these double-hyphen dashes as breaking characters, while retaining single hyphens within compound words, but KWIC, WordSmith Tools, and Voyant treat them like single hyphens, creating compounds with double hyphens where dashes are needed. The situation is still more complex if a double-hyphen is preceded or followed by a breaking character. If this sounds esoteric, consider that this short novel contains nearly 400 double-hyphen dashes (Dickens’ 
                <hi rend=""italic"">Dombey and Son</hi> has more than 2,200). And this problem, too, is highly systematic: words vary considerably in how frequently they are preceded or followed by a dash, and 1,000 dash errors per text would produce 5,000,000 errors in our hypothetical 5,000 novels. (For a practical example of the effects of error, see Matt Jockers’ discussion of topic modeling and several ‘topics’ that arose from OCR error and metadata (Jockers, 2013, 135).
            </p>
            <p>It might seem that we just need more sophisticated tokenizers, but the required level of sophistication to handle double-hyphen dashes correctly is quite high, and the problems caused by apostrophes and single quotation marks cannot be correctly solved computationally at all. In some cases, not even a human reader can tokenize with certainty; in others, a computer can solve problems a human cannot. </p>
            <p>Let’s consider a few further tokenization questions:</p>
            <p>He said, “That’s ’bout ‘nough, sho’.”</p>
            <p>“That’s ‘bout’, not ‘fight’; ’nough said,” Nough said.</p>
            <p>“John tried that ‘Nough told me to’ on me,” Bill whined.</p>
            <p>He remarked, “John said, ‘Bout starts at nine.’”</p>
            <p>He remarked, “John said, ‘It’s ’bout time.’”</p>
            <p>He remarked, “John said, ‘‘Bout time.’” Can these apostrophes/single quotes be handled correctly computationally? How about the two single quotes before ‘Bout’ in the last example?</p>
            <p>I visited the U.S.S.R. Four tokens? Seven? Is the final period part of the final token?</p>
            <p>I visited the U.S.S.R.! Four tokens? Seven? Is the final period part of the final token?</p>
            <p>Is that C------? Is ‘C------’ the token ‘C’ followed by a dash, or the token ‘C------’? What about ‘C—’? Or ‘C-’?</p>
            <p>C------ is here. Same questions.</p>
            <p>Oh d--n it! Is ‘d--n’ the tokens ‘d’ and ‘n’ separated by a dash, or the token ‘d--n’? How about ‘d---n’? or ‘d-n’? or ‘G-d’?</p>
            <p>I said--never mind. If ‘d--n’ is a token, can we prevent ‘said--never’ from being a token here?</p>
            <p>That’s what I--a mistake, sorry. How do we get ‘d--n’ correct without failing here?</p>
            <p>You’re a real %#@$! Three tokens? Four? Does the last include the final ‘!’? What if there were a period after the ‘!’?</p>
            <p>You’re a real %#@$!. How about now?</p>
            <p>I am working on a Python tokenizer that can handle most of these issues correctly, and some of these problems are fairly rare, but I despair of the possibility of creating a word frequency list that is ‘correct’ even in my own opinion. For many years I have ‘corrected’ the texts before tokenizing, but that is not a practical solution for 5,000 novels and presents its own problems.</p>
            <p>Perhaps in sufficiently big data, the error introduced by tokenizers will not significantly alter the results, and Maciej Eder (2013) has recently shown that some corpora are remarkably resistant to some kinds of intentionally introduced error. And improving the quality of the corpus had a relatively small effect on the attribution of the Federalist Papers (Levitan and Argamon, 2006). More study seems needed before we can be complacent, however, even in large-scale problems involving only authorship or classification. For smaller-scale stylistic studies, tokenization decisions can clearly have serious repercussions. Consider Ramsay’s (2011) analysis of 
                <hi rend=""italic"">The Waves</hi>, where decisions about tokenization significantly alter the lists of men-only and women-only words and words that characterize the six narrative voices (see Hoover [2014a] and Plasek and Hoover [2014], for discussion). Another example that replicates an experience I have had several times is that a Full Spectrum analysis (Hoover, 2014b), based on Craig’s version of Burrows’s Zeta (Burrows, 2007; Craig and Kinney, 2010) can give strange results if uncorrected texts are inadvertently included. For example, in a test of Charlotte Brontë versus Anne and Emily Brontë, 11 of the 100 most distinctive words were words with inappropriate initial “apostrophes” because the novels of Anne and Emily in the analysis both used single quotation marks for dialogue.
            </p>
            <p>Far from being an insignificant tool that can be taken for granted, a tokenizer expresses its author’s theory of text and can significantly affect the results of many kinds of text analysis.</p>
            <p>Notes</p>
            <p rend=""footnote text"">1. As a reviewer of this paper has pointed out, the problems of tokenization have been more widely recognized recently in the NLP community. For example, Dridan and Oepen (2012) and Chiarcos et al. (2012) address and suggest partial solutions for some of the problems discussed here. Even if the problems had all been solved within the NLP community (a fact not in evidence), however, this would not diminish the force of my argument for the DH community, where there has been much less attention paid to them.</p>
            <p rend=""footnote text"">2. These programs represent a variety of those used in DH work (in order): a mature Java program with a database function, a venerable corpus linguistics program with lots of functions and user-options, a highly customizable and powerful commercial program from OUP, a widely used online tool, and a recently developed set of tools written in the currently popular R.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Burrows, J. F.</hi> (2007). All the Way Through: Testing for Authorship in Different Frequency Strata. 
                        <hi rend=""italic"">LLC,</hi>
                        <hi rend=""bold"">22</hi>(1): 27–47.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Chesnutt, C. W.</hi> (1900). 
                        <hi rend=""italic"">The House Behind the Cedars</hi>. Houghton Mifflin, Boston, http://babel.hathitrust.org/cgi/pt?view=plaintext;size=100;id=nc01.ark%3A%2F13960%2Ft7cr7221k;page=root;seq=25;num=13.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Chiarcos, C., Ritz, J. and Stede, M.</hi> (2012). By All These Lovely Tokens . . . : Merging Conflicting Tokenizations. 
                        <hi rend=""italic"">Language Resources and Evaluation,</hi>
                        <hi rend=""bold"">46</hi>(1): 53–74. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Craig, H. and Kinney, A. F.</hi> (2010). 
                        <hi rend=""italic"">Shakespeare, Computers, and the Mystery of Authorship</hi>. Cambridge University Press, Cambridge. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Dridan, R., and Oepen, S.</hi> (2012). Tokenization: Returning to a Long Solved Problem: A Survey, Contrastive Experiment, Recommendations, and Toolkit. 
                        <hi rend=""italic"">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, </hi>pp. 378–82.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Eder, M.</hi> (2013). Mind Your Corpus: Systematic Errors in Authorship Attribution. 
                        <hi rend=""italic"">LLC,</hi>
                        <hi rend=""bold"">28</hi>(4): 603–14.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Eder, M., Rybicki, J. and Kestemont, M.</hi> (2014). Stylo.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Gaskell, E. </hi>(1910 [1851]). 
                        <hi rend=""italic"">Cranford</hi>. Houghton Mifflin, Boston, http://babel.hathitrust.org/cgi/pt?q1=twelve;id=hvd.32044097042071;view=plaintext;start=1;sz=10;page=root;size=100;seq=143;num=107.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Hoover, D. L. </hi>(2014a). Making Waves: Algorithmic Criticism Revisited. 
                        <hi rend=""italic"">DH2014</hi>, Lausanne, Switzerland: EPFL-UNIL, pp. 202–4.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Hoover, D. L.</hi> (2014b). The Full-Spectrum Text-Analysis Spreadsheet. 
                        <hi rend=""italic"">Digital Humanities 2013</hi>, Center for Digital Research in the Humanities, Lincoln, NE, University of Nebraska, pp. 226–29.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">The Intelligent Archive.</hi> (2012). Centre for Literary and Linguistic Computing, University of Newcastle, Australia.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Jockers, M. L.</hi> (2013). 
                        <hi rend=""italic"">Macroanalysis: Digital Methods and Literary History</hi>. University of Illinois Press, Urbana-Champaign.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Levitan, S. and Argamon, S.</hi> (2006). Fixing the Federalist: Correcting Results and Evaluating Editions for Automated Attribution. 
                        <hi rend=""italic"">Digital Humanities 2006</hi>. Paris: Centre de Recherche Cultures Anglophones et Technologies de l’Information, pp. 323–26.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Plasek, A. and Hoover, D. L. </hi>(2014). Starting the Conversation: Literary Studies, Algorithmic Opacity, and Computer-Assisted Literary Insight. 
                        <hi rend=""italic"">DH2014</hi>, Lausanne: EPFL-UNIL, pp. 305–6.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ramsay, S.</hi> (2011). 
                        <hi rend=""italic"">Reading Machines: Toward an Algorithmic Criticism</hi>. University of Illinois Press, Urbana.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Scott, M.</hi> (2012). WordSmith Tools version 6. Liverpool: Lexical Analysis Software.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sinclair, S., Rockwell, G. and the Voyant Tools Team</hi>. (2012). Voyant Tools (web application).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Tsukamoto, S. </hi>(2004). KWIC Concordance for Windows version 4.7.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
</TEI>",xml,Creative Commons Attribution 4.0 International,,programming;punctuation;python;tokenization;word frequency lists,English,english;natural language processing;programming;software design and development;standards and interoperability;text analysis,2015-01-01,"<?xml version=""1.0"" encoding=""utf-8""?>
<tei xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiheader>
        <filedesc>
            <titlestmt>
                <title>the trials of tokenization</title>
                <author>
                    <persname>
                        <surname>hoover</surname>
                        <forename>david l.</forename>
                    </persname>
                    <affiliation>new york university, united states of america</affiliation>
                    <email>david.hoover@nyu.edu</email>
                </author>
            </titlestmt>
            <editionstmt>
                <edition>
                    <date>2014-12-19t13:50:00z</date>
                </edition>
            </editionstmt>
            <publicationstmt>
                <publisher>paul arthur, university of western sidney</publisher>
                <address>
                    <addrline>locked bag 1797</addrline>
                    <addrline>penrith nsw 2751</addrline>
                    <addrline>australia</addrline>
                    <addrline>paul arthur</addrline>
                </address>
            </publicationstmt>
            <sourcedesc>
                <p>converted from a word document </p>
            </sourcedesc>
        </filedesc>
        <encodingdesc>
            <appinfo>
                <application ident=""dhconvalidator"" version=""1.9"">
                    <label>dhconvalidator</label>
                </application>
            </appinfo>
        </encodingdesc>
        <profiledesc>
            <textclass>
                <keywords scheme=""conftool"" n=""category"">
                    <term>paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""subcategory"">
                    <term>long paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""keywords"">
                    <term>python</term>
                    <term>tokenization</term>
                    <term>word frequency lists</term>
                    <term>programming</term>
                    <term>punctuation</term>
                </keywords>
                <keywords scheme=""conftool"" n=""topics"">
                    <term>natural language processing</term>
                    <term>software design and development</term>
                    <term>text analysis</term>
                    <term>programming</term>
                    <term>standards and interoperability</term>
                    <term>english</term>
                </keywords>
            </textclass>
        </profiledesc>
    </teiheader>
    <text>
        <body>
            <p>the process of tokenizing texts is typically out of sight and almost out of mind—often handled invisibly by the analyst’s program or r script, and rarely described, discussed, or even mentioned. for ‘big data’, even if questions did arise about the nature of the word list produced, testing is not feasible. furthermore, tokenizer accuracy is so critically affected by the state and nature of the texts that probably no general measure of accuracy or appropriateness is possible. finally, built-in programming functions and libraries are all too often used uncritically with little realization that their output does not conform to the assumptions or expectations of the analyst. i suggest that we should pay a little more attention to the theory and practice of tokenization.
                <hi rend=""superscript"">1</hi>
            </p>
            <p>consider a hypothetical case. let’s say i want to analyze 5,000 novels, have access to the texts at hathitrust, download 5,000 novels in plain text, and tokenize them. below is part of a page from elizabeth gaskell’s 
                <hi rend=""italic"">cranford</hi>, from hathitrust (gaskell, 1910 [1851], 107):
            </p>
            <figure>
                <graphic n=""1001"" width=""9.710208333333334cm"" height=""10.4775cm"" url=""pictures/image1.png"" rend=""block""/>
            </figure>
            <p>figure 1. 
                <hi rend=""italic"">cranford</hi>, elizabeth gaskell, from page 107.
            </p>
            <p>a human reader would have little trouble tokenizing this passage, and it is not extremely problematic, though minor ocr problems exist (mainly spacing issues around single quotation marks / apostrophes and dashes, and the line-end hyphen). i tokenized this passage with the intelligent archive (2012), kwic (tsukamoto, 2004) wordsmith tools (scott, 2012), voyant (sinclair et al., 2012), and stylo (eder et al., 2014).
                <hi rend=""superscript"">2</hi> even on this short text, the five programs identify three different numbers of types and two different numbers of tokens, largely because of the handling of single quotation marks. kwic and wordsmith produce identical lists, as do voyant and stylo, but neither of these match the intelligent archive.
            </p>
            <p>now consider charles chesnutt’s 
                <hi rend=""italic"">the house behind the cedars</hi> (1900, 13), also from hathitrust:
            </p>
            <figure>
                <graphic n=""1002"" width=""8.89cm"" height=""8.810625cm"" url=""pictures/image2.png"" rend=""block""/>
            </figure>
            <p>figure 2. 
                <hi rend=""italic"">the house behind the cedars</hi>, charles chesnutt, from page 13.
            </p>
            <p>the dialect in this passage is challenging even for human readers, and the ocr is more problematic. for example, the printed text (judging from the pdf) had spaced contractions, which explains ‘you 're’ in the fourth line from the bottom and the space in ‘lie 's’ in the first line, where the text reads “he 's.” this classic ocr problem occurs several times in this novel. and in the last line ‘you '11’ has both a space and an erroneous number 11 for the ‘ll’ (double el), another common ocr problem. those analyzing big data usually rely on the insignificance of random error, but these and many other kinds of error are not random, and systematic error within one text, one author, one genre, or one collection could easily lead to thousands of inaccurate word frequency counts in this hypothetical study of 5,000 texts.</p>
            <p>the use of apostrophes in the chesnutt passage to indicate dialect pronunciations can also severely affect tokenization. although the intelligent archive, kwic, and wordsmith tools produce exactly the same lists for this brief passage, and voyant has the same number of types and tokens, voyant removes all initial (but not final) apostrophes, creating different words for eight of the 97 types. stylo removes all numbers, all initial and final apostrophes, and many internal apostrophes, retaining them only in 
                <hi rend=""italic"">ain^t, gentleman^s</hi>, and 
                <hi rend=""italic"">spen^s</hi> (replaced with a caret). it produces six more tokens and four more types than the other programs, and many more differences in the word list. unfortunately, in chesnutt’s short novel, more than 650 words begin and/or end with apostrophes crucial to the identity of the word, so that the word lists produced by voyant and stylo are quite inaccurate. furthermore, only kwic and wordsmith tools let the user choose whether apostrophes and hyphens are part of a word, and whether numbers can appear in the word list or not. only wordsmith tools allows the user to choose whether to allow apostrophes at the beginnings and/or ends of the word as well as internally.
            </p>
            <p>obviously, the two texts examined above cause different problems, and different tokenizers are more accurate for one than for the other. worse yet, these problems are found even in relatively carefully edited texts like those from project gutenberg. although gutenberg’s 
                <hi rend=""italic"">the house behind the cedars</hi> does not have spaced contractions, and correctly has 
                <hi rend=""italic"">he’s</hi> in the first line and 
                <hi rend=""italic"">you’ll</hi> in the final line, the 29 initial and final dialect apostrophes remain problematic. the gutenberg text also represents dashes as two hyphens without spaces, creating more problems for tokenizers. the intelligent archive and stylo treat these double-hyphen dashes as breaking characters, while retaining single hyphens within compound words, but kwic, wordsmith tools, and voyant treat them like single hyphens, creating compounds with double hyphens where dashes are needed. the situation is still more complex if a double-hyphen is preceded or followed by a breaking character. if this sounds esoteric, consider that this short novel contains nearly 400 double-hyphen dashes (dickens’ 
                <hi rend=""italic"">dombey and son</hi> has more than 2,200). and this problem, too, is highly systematic: words vary considerably in how frequently they are preceded or followed by a dash, and 1,000 dash errors per text would produce 5,000,000 errors in our hypothetical 5,000 novels. (for a practical example of the effects of error, see matt jockers’ discussion of topic modeling and several ‘topics’ that arose from ocr error and metadata (jockers, 2013, 135).
            </p>
            <p>it might seem that we just need more sophisticated tokenizers, but the required level of sophistication to handle double-hyphen dashes correctly is quite high, and the problems caused by apostrophes and single quotation marks cannot be correctly solved computationally at all. in some cases, not even a human reader can tokenize with certainty; in others, a computer can solve problems a human cannot. </p>
            <p>let’s consider a few further tokenization questions:</p>
            <p>he said, “that’s ’bout ‘nough, sho’.”</p>
            <p>“that’s ‘bout’, not ‘fight’; ’nough said,” nough said.</p>
            <p>“john tried that ‘nough told me to’ on me,” bill whined.</p>
            <p>he remarked, “john said, ‘bout starts at nine.’”</p>
            <p>he remarked, “john said, ‘it’s ’bout time.’”</p>
            <p>he remarked, “john said, ‘‘bout time.’” can these apostrophes/single quotes be handled correctly computationally? how about the two single quotes before ‘bout’ in the last example?</p>
            <p>i visited the u.s.s.r. four tokens? seven? is the final period part of the final token?</p>
            <p>i visited the u.s.s.r.! four tokens? seven? is the final period part of the final token?</p>
            <p>is that c------? is ‘c------’ the token ‘c’ followed by a dash, or the token ‘c------’? what about ‘c—’? or ‘c-’?</p>
            <p>c------ is here. same questions.</p>
            <p>oh d--n it! is ‘d--n’ the tokens ‘d’ and ‘n’ separated by a dash, or the token ‘d--n’? how about ‘d---n’? or ‘d-n’? or ‘g-d’?</p>
            <p>i said--never mind. if ‘d--n’ is a token, can we prevent ‘said--never’ from being a token here?</p>
            <p>that’s what i--a mistake, sorry. how do we get ‘d--n’ correct without failing here?</p>
            <p>you’re a real %#@$! three tokens? four? does the last include the final ‘!’? what if there were a period after the ‘!’?</p>
            <p>you’re a real %#@$!. how about now?</p>
            <p>i am working on a python tokenizer that can handle most of these issues correctly, and some of these problems are fairly rare, but i despair of the possibility of creating a word frequency list that is ‘correct’ even in my own opinion. for many years i have ‘corrected’ the texts before tokenizing, but that is not a practical solution for 5,000 novels and presents its own problems.</p>
            <p>perhaps in sufficiently big data, the error introduced by tokenizers will not significantly alter the results, and maciej eder (2013) has recently shown that some corpora are remarkably resistant to some kinds of intentionally introduced error. and improving the quality of the corpus had a relatively small effect on the attribution of the federalist papers (levitan and argamon, 2006). more study seems needed before we can be complacent, however, even in large-scale problems involving only authorship or classification. for smaller-scale stylistic studies, tokenization decisions can clearly have serious repercussions. consider ramsay’s (2011) analysis of 
                <hi rend=""italic"">the waves</hi>, where decisions about tokenization significantly alter the lists of men-only and women-only words and words that characterize the six narrative voices (see hoover [2014a] and plasek and hoover [2014], for discussion). another example that replicates an experience i have had several times is that a full spectrum analysis (hoover, 2014b), based on craig’s version of burrows’s zeta (burrows, 2007; craig and kinney, 2010) can give strange results if uncorrected texts are inadvertently included. for example, in a test of charlotte brontë versus anne and emily brontë, 11 of the 100 most distinctive words were words with inappropriate initial “apostrophes” because the novels of anne and emily in the analysis both used single quotation marks for dialogue.
            </p>
            <p>far from being an insignificant tool that can be taken for granted, a tokenizer expresses its author’s theory of text and can significantly affect the results of many kinds of text analysis.</p>
            <p>notes</p>
            <p rend=""footnote text"">1. as a reviewer of this paper has pointed out, the problems of tokenization have been more widely recognized recently in the nlp community. for example, dridan and oepen (2012) and chiarcos et al. (2012) address and suggest partial solutions for some of the problems discussed here. even if the problems had all been solved within the nlp community (a fact not in evidence), however, this would not diminish the force of my argument for the dh community, where there has been much less attention paid to them.</p>
            <p rend=""footnote text"">2. these programs represent a variety of those used in dh work (in order): a mature java program with a database function, a venerable corpus linguistics program with lots of functions and user-options, a highly customizable and powerful commercial program from oup, a widely used online tool, and a recently developed set of tools written in the currently popular r.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl>
                        <hi rend=""bold"">burrows, j. f.</hi> (2007). all the way through: testing for authorship in different frequency strata. 
                        <hi rend=""italic"">llc,</hi>
                        <hi rend=""bold"">22</hi>(1): 27–47.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">chesnutt, c. w.</hi> (1900). 
                        <hi rend=""italic"">the house behind the cedars</hi>. houghton mifflin, boston, http://babel.hathitrust.org/cgi/pt?view=plaintext;size=100;id=nc01.ark%3a%2f13960%2ft7cr7221k;page=root;seq=25;num=13.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">chiarcos, c., ritz, j. and stede, m.</hi> (2012). by all these lovely tokens . . . : merging conflicting tokenizations. 
                        <hi rend=""italic"">language resources and evaluation,</hi>
                        <hi rend=""bold"">46</hi>(1): 53–74. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">craig, h. and kinney, a. f.</hi> (2010). 
                        <hi rend=""italic"">shakespeare, computers, and the mystery of authorship</hi>. cambridge university press, cambridge. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">dridan, r., and oepen, s.</hi> (2012). tokenization: returning to a long solved problem: a survey, contrastive experiment, recommendations, and toolkit. 
                        <hi rend=""italic"">proceedings of the 50th annual meeting of the association for computational linguistics, </hi>pp. 378–82.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">eder, m.</hi> (2013). mind your corpus: systematic errors in authorship attribution. 
                        <hi rend=""italic"">llc,</hi>
                        <hi rend=""bold"">28</hi>(4): 603–14.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">eder, m., rybicki, j. and kestemont, m.</hi> (2014). stylo.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">gaskell, e. </hi>(1910 [1851]). 
                        <hi rend=""italic"">cranford</hi>. houghton mifflin, boston, http://babel.hathitrust.org/cgi/pt?q1=twelve;id=hvd.32044097042071;view=plaintext;start=1;sz=10;page=root;size=100;seq=143;num=107.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">hoover, d. l. </hi>(2014a). making waves: algorithmic criticism revisited. 
                        <hi rend=""italic"">dh2014</hi>, lausanne, switzerland: epfl-unil, pp. 202–4.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">hoover, d. l.</hi> (2014b). the full-spectrum text-analysis spreadsheet. 
                        <hi rend=""italic"">digital humanities 2013</hi>, center for digital research in the humanities, lincoln, ne, university of nebraska, pp. 226–29.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">the intelligent archive.</hi> (2012). centre for literary and linguistic computing, university of newcastle, australia.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">jockers, m. l.</hi> (2013). 
                        <hi rend=""italic"">macroanalysis: digital methods and literary history</hi>. university of illinois press, urbana-champaign.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">levitan, s. and argamon, s.</hi> (2006). fixing the federalist: correcting results and evaluating editions for automated attribution. 
                        <hi rend=""italic"">digital humanities 2006</hi>. paris: centre de recherche cultures anglophones et technologies de l’information, pp. 323–26.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">plasek, a. and hoover, d. l. </hi>(2014). starting the conversation: literary studies, algorithmic opacity, and computer-assisted literary insight. 
                        <hi rend=""italic"">dh2014</hi>, lausanne: epfl-unil, pp. 305–6.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">ramsay, s.</hi> (2011). 
                        <hi rend=""italic"">reading machines: toward an algorithmic criticism</hi>. university of illinois press, urbana.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">scott, m.</hi> (2012). wordsmith tools version 6. liverpool: lexical analysis software.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">sinclair, s., rockwell, g. and the voyant tools team</hi>. (2012). voyant tools (web application).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">tsukamoto, s. </hi>(2004). kwic concordance for windows version 4.7.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
</tei>",8.0,8.0,Voyant
2481,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,Defining the Core Entities of an Environment for Textual Processing in Literary Computing,,Angelo Mario Del Grosso;Davide Albanesi;Emiliano Giovannetti;Simone Marchi,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Introduction</head>
                <p>The development of applications in the field of Digital Humanities (DH) does not adequately take into account domain modelling, software design principles and software engineering methodologies (Bozzi, 2013; D'Iorio, 2015; McCarty, 2008; Terras and Crane, 2010). In fact, many systems developed in the context of DH-related projects have not been conceived to be modular, extensible, and scalable: they only tend to solve specific problems such as data-driven and project-oriented tools (Boschetti and Del Grosso, 2015). In addition, most projects focus on the requirements of humanists (as end users), but leave out the needs of software developers.</p>
                <p>This research was motivated by a number of issues emerged from the projects we worked on 
                    <hi rend=""background-color(#ffff00)""> </hi>(Abrate et al., 2014a; Albanesi et al., 2015; Bellandi et al., 2014; Bozzi, 2015; Del Grosso, 2013; Ruimy et al., 2012) and it fits into an ongoing discussion about textual modelling and research infrastructures (Moulin et al., 2011; Pierazzo, 2015; Schmidt, 2014). In particular, this work aims at providing methodological guidelines for the definition of the core entities of a digital scholarly environment. We chose to adopt an object-oriented approach since it can bring benefits in the definition of efficient and effective digital tools (Boschetti et al., 2014; Del Grosso and Nahli, 2014). To give an analogy, the environment we propose can help developers and scholars as CMS (e.g. Wordpress) can help Web designers and publishers.
                </p>
                <p>The development of the environment follows three criteria: i) adopting an agile process (Ashmore and Runyan, 2014) to define the nature and behavior of the environment through both functional (e.g. user stories) and non-functional requirements (e.g. data model, system architecture) (Cohn, 2004; Collins-Cope et al., 2005); ii) providing well-defined Application Programming Interfaces (APIs) among components (Grill et al., 2012; Tulach, 2008); iii) applying analysis, architectural and design patterns for the sake of abstraction, generalization and flexibility (Ackerman and Gonzalez, 2011; Buschmann et al., 2007; Gamma et al., 1995).</p>
                <p>Following the agile methodology, we are developing a modular environment by starting from the design and implementation of a 
                    <hi rend=""bold"">microkernel</hi> (Buschmann et al., 1996) as the manager of the different components. In addition, the microkernel provides all the operations needed to manipulate the domain basic entities which are described in the section “Domain entities and design patterns”.
                </p>
                <div type=""div2"" rend=""DH-Heading2"">
                    <head>Related works</head>
                    <p>Digital humanists have access to several tools for literary studies. TextGrid, for example, provides integrated tools for analyzing texts and gives computer support for digital editing purposes (Hedges et al., 2013). The NINES project offers an environment to support scholars in the creation of long-term digital research materials. It includes three main tools: Collex (Nowviskie, 2007), Juxta, and Ivanhoe. Annotation Studio is a collaborative system to annotate texts and add links to multimedia objects (Paradis et al., 2013). The CULTURA project aims at developing a “corpus agnostic research environment” providing customizable services for a wide range of users (Steiner et al., 2014). The development of an online workspace which helps scholars in the production of critical editions is the main objective of the Workspace for Collaborative Editing framework (Houghton et al., 2014). It uses existing standards and open-source solutions to create an architecture of reusable components. Other platforms worth mentioning are TUSTEP/TXSTEP (Ott, 2000; Ott and Ott, 2014), WebLicht (Hinrichs et al., 2010), Perseids (Almas and Beaulieu, 2013), Muruca/Pundit (Grassi et al., 2013), Textual Communities (Bordalejo and Robinson, 2015), SAWS (Jordanous et al., 2012), Voyant Tools (Sinclair and Rockwell, 2012), Transcribe Bentham (Causer and Terras, 2014) and Alcide (Moretti et al., 2014). However, the aforementioned initiatives allow digital scholars to meet specific needs, but none of them seems to provide, simultaneously, all the following characteristics: i) reusability and extensibility, ii) ease of use and configuration, iii) continuous availability of the services and development over time, iv) a well-grounded software data model.</p>
                </div>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Domain entities and design patterns</head>
                <p>One of the main challenges of the DH community is to provide suitable software models and tools (Ciotti, 2014). To model the literary domain and the relative user requirements, we chose to follow the engineering principles of 
                    <hi rend=""bold"">object-oriented analysis and design</hi> (Ackerman and Gonzalez, 2011). The digital representation of a textual resource is a challenge as it involves several theoretical and epistemological issues in semiotics, paleography, philology, linguistics, engineering, and computer science (McCarty, 2005; Meister, 2012; Moretti, 2013; Robinson, 2013; Sahle, 2013).
                </p>
                <p>In this work, we define each textual element by means of four properties: i) the 
                    <hi rend=""bold"">version</hi> allows to select a specific textual element among those available in its history of changes; ii) the 
                    <hi rend=""bold"">granularity</hi> represents a level of a hierarchical structure (e.g. a page composed of lines); iii) the 
                    <hi rend=""bold"">position</hi> provides the location of a textual element within the hierarchical representation (e.g. the second page of a book); iv) the 
                    <hi rend=""bold"">layer</hi> indicates the set of homogeneous information the textual element belongs to (e.g. morphological layer). As pointed outby (Buzzetti, 2002; McGann, 2004; Pierazzo, 2015), the information conveyed by a textual resource is logically organized through multiple layers (also called 
                    <hi rend=""italic"">dimensions</hi>) of information.
                </p>
                <p>
                    <figure>
                        <graphic url=""24/100002010000070400000824354E1F7F0D4DC755.png""/>
                        <head>Fig. 1: Class diagram of the domain entities</head>
                    </figure>On these four properties we have designed and implemented a set of core entities as the fundamental data types shared among all the components of the environment (Fig. 1)
                    <note xml:id=""ftn1"" place=""foot"" n=""1""> The ongoing implementation of the environment is available at: https://github.com/literarycomputinglab</note>. The 
                    <hi rend=""bold"">Source</hi> class is in charge of managing the low-level data: it is composed of a 
                    <hi rend=""bold"">Payload</hi> representing the information conveyed by the textual resource and a
                    <hi rend=""bold"">SourceType</hi> which indicates the nature of the Source (e.g. text, image, audio, etc.). Payload objects (as used in networking) have the only purpose of encapsulating the information. The 
                    <hi rend=""bold"">Locus</hi> and the 
                    <hi rend=""bold"">P</hi>
                    <hi rend=""bold"">lace</hi>
                    <hi rend=""bold"">OfInterest</hi> (POI) classes identify, through a 
                    <hi rend=""italic"">composition pattern</hi>, specific data fragments of the source content, and they are used to establish the boundaries of an 
                    <hi rend=""bold"">Annotation</hi>. A chunk of text, for example, can be addressed to by a locus having a POI (of type Sequence of Interest) representing its start and end coordinates. Similarly, a region of an image can be identified by a locus having a POI (of type Region of Interest) composed of a sequence of coordinates. The Locus and POI provide a stand-off text annotation technique able to tackle, for example, the overlapping hierarchies problem, which cannot be handled easily with inline markup techniques (Schmidt, 2010). As a matter of fact, it is possible, simultaneously, to manipulate a resource on the basis of its documental and textual structure (Renear et al., 1996; Robinson, 2013) (see the example in the following section). However, since stand-off models are affected by the issue of the indexing updating, a dedicated component must be in charge of automatically maintaining the coherence of the annotations each time the underlying text is edited.
                </p>
                <p>An Annotation represents an information associated to a locus and is defined by an 
                    <hi rend=""bold"">AnnotationType</hi> (e.g. a token, a lemma, a named entity, etc.). Since the hierarchical structure of the source may evolve over time, the changes to the relative tree must be managed. For example, a tree structure having tokens as leaves could need to be updated with a finer-grained layer of characters (e.g. to assign annotations to specific letters). In this case, the tokens should become intermediate nodes and the characters would become the leaf nodes. Typically, this kind of editing is unpredictable and it often implies heavy adaptations if the software is not flexible enough to manage changes in the underlying text representation schema. Consequently, we decided to exploit the flexibility of the Object Oriented model by adopting the Role Design Pattern (Fowler, 1997) to switch between leaf and intermediate nodes dynamically. This pattern has been implemented by the 
                    <hi rend=""bold"">AnnotationRole</hi>, 
                    <hi rend=""bold"">AnnotationRoleElement</hi> and 
                    <hi rend=""bold"">AnnotationRoleStructure</hi> classes. Moreover, an annotation is a source in itself (see the inheritance relationship between the Annotation and the Source classes in Fig. 1) and, thus, it can be annotated recursively.
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>An Example</head>
                <p>We here introduce an example showing a representation of a snippet of text with annotations. The chosen text is an excerpt of a letter, written in Latin, belonging to the epistolary corpus of the Clavius on the Web project
                    <note xml:id=""ftn2"" place=""foot"" n=""2""> Clavius on the Web is a project funded by Registro.it and partecipated by the Institute of Informatics and Telematics (IIT-CNR), the Institute of Computational Linguistics “A. Zampolli” (ILC-CNR), and the Historical Archives of the Pontifical Gregorian University (APUG). Website: http://claviusontheweb.it/</note> (Abrate et al., 2014b). Fig. 2 shows a typical way of encoding sentences and lines with a markup language as TEI (Burnard, 2014): the resulting XML hierarchical structure has been broken by the addition of the line anchors (<lb />) mixing up the textual and documental structure of the text. Indeed, to preserve the integrity of the word “Dinostrati” (spanning across lines 4 and 5), it is necessary to encapsulate it with the element <w />.
                </p>
                <p>
                    <figure>
                        <graphic url=""24/10000201000006EC00000398E68D91980964673A.png""/>
                        <head>Fig. 2: A standard way of encoding a text with TEI-XML</head>
                    </figure>
                    <figure>
                        <graphic url=""24/1000020100000386000001F5970322C0327A03CA.png""/>
                        <head>Fig. 3: Multi-layered stand-off annotation of text</head>
                    </figure>The model we propose solves this problem with the stand-off annotations: as shown in Fig. 3 the document (made of lines) and the textual structure (made of sentences and words) are logically separated. Lines, sentences and words do not overlap and they are structured in separate hierarchies.
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading"">
                <head>Next Steps</head>
                <p>We plan, in future works, to release a first version of a web environment, called 
                    <hi rend=""italic"">Omega</hi>, built around the core entities that we here described. The environment will allow to load, index, annotate, and query a textual collection. Furthermore, we’ll carry on the development of modules for text analysis and textual scholarship with the related APIs.
                </p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Abrate, M., </hi>
                        <hi rend=""bold"">Del </hi>
                        <hi rend=""bold"">Grosso, A. M., Giovannetti, E., </hi>
                        <hi rend=""bold"">Lo </hi>
                        <hi rend=""bold"">Duca, A., Luzzi, D., Mancini, L., Marchetti, A., Pedretti, I. and Piccini, S.</hi> (2014a). Sharing Cultural Heritage: the Clavius on the Web Project. In Calzolari, N., Choukri, K., Declerck, T., Loftsson, H., Maegaard, B., Mariani, J., Moreno, A., Odijk, J. and Piperidis, S. (eds), 
                        <hi rend=""italic"">Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC), Reykjavik</hi>. European Language Resources Association (ELRA), pp. 627–34.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Abrate, M., </hi>
                        <hi rend=""bold"">Del </hi>
                        <hi rend=""bold"">Grosso,</hi>
                        <hi rend=""bold""> </hi>
                        <hi rend=""bold"">A. M., Giovannetti, E., </hi>
                        <hi rend=""bold"">Lo </hi>
                        <hi rend=""bold"">Duca, A., Marchetti, A., Mancini, L., Pedretti, I. and Piccini, S.</hi> (2014b). Il Progetto Clavius on the Web: tecnologie linguistico-semantiche al servizio del patrimonio documentale e degli archivi storici. In Rossi, F. and Tomasi, F. (eds), 
                        <hi rend=""italic"">Book of Abstracts of 3</hi>
                        <hi rend=""sup italic"">o</hi>
                        <hi rend=""italic""> AIUCD Conference, Bologna</hi>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ackerman, L. and Gonzalez, C.</hi> (2011). 
                        <hi rend=""italic"">Patterns-Based Engineering: Successfully Delivering Solutions Via Patterns</hi>. Addison-Wesley.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Albanesi, D., Bellandi, A., Benotto, G., </hi>
                        <hi rend=""bold"">Di </hi>
                        <hi rend=""bold"">Segni, G. and Giovannetti, E.</hi> (2015). When Translation Requires Interpretation: Collaborative Computer–Assisted Translation of Ancient Texts. 
                        <hi rend=""italic"">LaTeCH 2015</hi>: 84–88.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Almas, B. and Beaulieu, M.-C.</hi> (2013). Developing a New Integrated Editing Platform for Source Documents in Classics. 
                        <hi rend=""italic"">Literary and Linguistic Computing</hi>, 
                        <hi rend=""bold"">28</hi>(4): 493–503 doi:10.1093/llc/fqt046.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ashmore, S. and Runyan, K.</hi> (2014). 
                        <hi rend=""italic"">Introduction to Agile Methods</hi>. Upper Saddle River, NJ: Addison-Wesley Professional, Pearson Education.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Bellandi, A., Albanesi, D., Bellusci, A., Bozzi, A. and Giovannetti, E.</hi> (2014). The Talmud System: a Collaborative web Application for the Translation of the Babylonian Talmud Into Italian. 
                        <hi rend=""italic"">The First Italian Conference on Computational Linguistics CLiC-It 2014</hi>, pp. 53–57.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Bordalejo, B. and Robinson, P.</hi> (2015). A new system for collaborative online creation of Scholarly Editions in digital form. 
                        <hi rend=""italic"">1st Dixit Convension on Technology, Software, Standards for the Digital Scholarly Edition Workshop</hi>. The Hague.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Boschetti, F. and </hi>
                        <hi rend=""bold"">Del </hi>
                        <hi rend=""bold"">Grosso, A. M.</hi> (2015). TeiCoPhiLib: A Library of Components for the Domain of Collaborative Philology. 
                        <hi rend=""italic"">Journal of the Text Encoding Initiative</hi>(8). doi:10.4000/jtei.1285. http://jtei.revues.org/1285 (accessed 3 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Boschetti, F., </hi>
                        <hi rend=""bold"">Del </hi>
                        <hi rend=""bold"">Grosso, A. M., Khan, A. F., Lamé, M. and Nahli, O.</hi> (2014). A top-down approach to the design of components for the philological domain. 
                        <hi rend=""italic"">Book of Abstract of Digital Humanities Conference (DH), Lausanne, Switzerland</hi>. Alliance of Digital Humanities Organisations, pp. 109–11.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Bozzi, A.</hi> (2013). G2A: A Web application to study, annotate and scholarly edit ancient texts and their aligned translations. (Ed.) ERC Ideas 249431 
                        <hi rend=""italic"">Studia Graeco-Arabica</hi>, 
                        <hi rend=""bold"">3</hi>: 159–71.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Bozzi, A.</hi> (2015). Greek into Arabic, a research Infrascructure based on computational modules to annotate and query historical and philosophical digital texts. Part I: Methodological aspects. In Bozzi, A. (ed), 
                        <hi rend=""italic"">Digital Texts, Translations, Lexicons in a Multi-Modular Web Application: Methods and Samples</hi>. Firenze: Leo S. Olschki editore, pp. 27–42.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Burnard, L.</hi> (2014). TEI P5: Guidelines for Electronic Text Encoding and Interchange. Version 2.9.1. http://www.tei-c.org/Guidelines/P5/index.xml (accessed 3 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Buschmann, F., Henney, K. and Schmidt, D. C.</hi> (2007). 
                        <hi rend=""italic"">Pattern-Oriented Software Architecture, On Patterns and Pattern Languages</hi>. (Pattern-Oriented Software Architecture). Hoboken: John Wiley & Sons.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Buschmann, F., Meunier, R., Rohnert, H., Sommerlad, P. and Stal, M.</hi> (1996). Pattern-oriented Software Architecture - A System of Patterns. J. Wiley and Sons Ltd., pp. 171–92.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Buzzetti, D.</hi> (2002). Digital Representation and the Text Model. 
                        <hi rend=""italic"">New Literary History</hi>, 
                        <hi rend=""bold"">33</hi>(1): 61–88.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Causer, T. and Terras, M.</hi> (2014). “Many hands make light work. Many hands together make merry work”: Transcribe Bentham and crowdsourcing manuscript collections.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ciotti, F.</hi> (2014). Digital Literary and Cultural Studies: State of the Art and Perspectives. 
                        <hi rend=""italic"">Between</hi>, 
                        <hi rend=""bold"">4</hi>(8). doi:10.13125/2039-6597/1392. http://dx.doi.org/10.13125/2039-6597/1392 (accessed 3 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Cohn, M.</hi> (2004). 
                        <hi rend=""italic"">User Stories Applied: For Agile Software Development</hi>. Redwood City, CA, USA: Addison Wesley Longman Publishing Co., Inc.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Collins-Cope, M., Rosenberg, D. and Stephens, M.</hi> (2005). 
                        <hi rend=""italic"">Agile Development with ICONIX Process: People, Process, and Pragmatism</hi>. Berkely, CA, USA: Apress.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Fowler, M.</hi> (1997). Dealing with roles. 
                        <hi rend=""italic"">Proceedings of the International Conference on Pattern Languages of Programs</hi>, vol. 97, pp. 13–37.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Gamma, E., Helm, R., Johnson, R. and Vlissides, J.</hi> (1995). 
                        <hi rend=""italic"">Design Patterns: Elements of Reusable Object-Oriented Software</hi>. Boston, MA, USA: Addison-Wesley Longman Publishing Co., Inc.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Grassi, M., Morbidoni, C., Nucci, M., Fonda, S. and Piazza, F.</hi> (2013). Pundit: augmenting web contents with semantics. 
                        <hi rend=""italic"">Literary and Linguistic Computing</hi>, 
                        <hi rend=""bold"">28</hi>(4): 640–59.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Grill, T., Polacek, O. and Tscheligi, M.</hi> (October 29-312012). Methods Towards API Usability: A Structural Analysis of Usability Problem Categories. 
                        <hi rend=""italic"">Proceedings of the 4th International Conference on Human-Centered Software Engineering, Toulouse, France</hi>. Berlin, Heidelberg: Springer-Verlag, pp. 164–80. doi:10.1007/978-3-642-34347-6_10.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Del </hi>
                        <hi rend=""bold"">Grosso, A. M.</hi> (2013). Indexing techniques and variant readings management. (Ed.) D'Ancona, C. 
                        <hi rend=""italic"">Studia Graeco-Arabica</hi>, 
                        <hi rend=""bold"">3</hi>: 209–30.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Del </hi>
                        <hi rend=""bold"">Grosso, A. M. and Nahli, O.</hi> (2014). Towards a flexible open-source software library for multi-layered scholarly textual studies: An Arabic case study dealing with semi-automatic language processing. 
                        <hi rend=""italic"">Proceedings of 3rd IEEE International Colloquium, Information Science and Technology (CIST), Tetouan, Marocco</hi>. Washington, DC, USA: IEEE, pp. 285–90. doi:10.1109/CIST.2014.7016633.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Hedges, M., Neuroth, H., Smith, K. M., Blanke, T., Romary, L., Küster, M. and Illingworth, M.</hi> (2013). TextGrid, TEXTvre, and DARIAH: Sustainability of Infrastructures for Textual Scholarship. 
                        <hi rend=""italic"">Journal of the Text Encoding Initiative</hi>(5). doi:10.4000/jtei.774 (accessed 3 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Hinrichs, E., Hinrichs, M. and Zastrow, T.</hi> (2010). WebLicht: Web-based LRT services for German. 
                        <hi rend=""italic"">Proceedings of the ACL 2010 System Demonstrations</hi>. Association for Computational Linguistics, pp. 25–29.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Houghton, H., Sievers, M. and Smith, Catherine</hi> (2014). The Workspace for Collaborative Editing. 
                        <hi rend=""italic"">Digital Humanities 2014</hi>. Laussanne: Alliance of Digital Humanities Organisations, pp. 204–05.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">D'</hi>
                        <hi rend=""bold"">Iorio, P. </hi> (2015). On the scholarly use of the Internet, a conceptual model. In Bozzi, A. (ed), 
                        <hi rend=""italic"">Digital Texts, Translations, Lexicons in a Multi-Modular Web Application: Methods and Samples</hi>. Firenze: Leo S. Olschki editore, pp. 1–25.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Jordanous, A., Lawrence, K. F., Hedges, M. and Tupman, C.</hi> (June 13-152012). Exploring Manuscripts: Sharing Ancient Wisdoms Across the Semantic Web. 
                        <hi rend=""italic"">Proceedings of the 2nd International Conference on Web Intelligence, Mining and Semantics (WIMS), Craiova, Romania</hi>. New York, NY, USA: ACM, pp. 44:1–44:12. doi:10.1145/2254129.2254184.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">McCarty, W.</hi> (2005). 
                        <hi rend=""italic"">Humanities Computing</hi>. Palgrave Macmillan.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">McCarty, W.</hi> (2008). Signs of times present and future. 
                        <hi rend=""italic"">Human Discussion Group</hi>, 
                        <hi rend=""bold"">22</hi>(218).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">McGann, J.</hi> (2004). Marking Texts of Many Dimensions. In Schreibman, S., Siemens, R. and Unsworth, J. (eds), 
                        <hi rend=""italic"">A Companion to Digital Humanities</hi>. (Blackwell Companions to Literature and Culture). Blackwell Publishing Ltd, pp. 198–217.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Meister, J. C.</hi> (2012). DH is us or on the unbearable lightness of a shared methodology. 
                        <hi rend=""italic"">Historical Social Research</hi>, 
                        <hi rend=""bold"">37</hi>(3): 77–85.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Moretti, F.</hi> (2013). 
                        <hi rend=""italic"">Distant Reading</hi>. Verso Books.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Moretti, G., Tonelli, S., Menini, S. and Sprugnoli, R.</hi> (2014). ALCIDE: An online platform for the Analysis of Language and Content In a Digital Environment. 
                        <hi rend=""italic"">Proceedings of the First Italian Conference on Computational Linguistics (CLIC-2014)</hi>. Pisa, Italy.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Moulin, C., Nyhan, J., Ciula, A., Kelleher, M., Mittler, E., Tadić, M., Ågren, M., Bozzi, A. and Kuutma, K.</hi> (2011). 
                        <hi rend=""italic"">Research Infrastructures in the Digital Humanities</hi>. http://www.esf.org/hosting-experts/scientific-review-groups/humanities-hum/strategic-activities/research-infrastructures-in-the-humanities.html.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Nowviskie, B.</hi> (2007). Collex: Facets, Folksonomy, and Fashioning the Remixable web. 
                        <hi rend=""italic"">Book of Abstract of Digital Humanities Conference (DH), University of Illinois at Urbana-Champaign</hi>. Alliance of Digital Humanities Organisations.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ott, W.</hi> (2000). Strategies and tools for textual scholarship: the Tübingen system of text processing programs (TUSTEP). 
                        <hi rend=""italic"">Literary and Linguistic Computing</hi>, 
                        <hi rend=""bold"">15</hi>(1): 93–108. doi:10.1093/llc/15.1.93. http://llc.oxfordjournals.org/content/15/1/93.abstract.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ott, W. and Ott, T.</hi> (2014). Critical Editing with TXSTEP. In Terras, M. (ed), 
                        <hi rend=""italic"">Book of Abstracts of the Digital Humanities Conference, Lausanne, Switzerland</hi>. Alliance of Digital Humanities Organisations, pp. 509–13.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Paradis, J., Fendt, K., Kelley, W., Folsom, J., Pankow, J., Graham, E. and Subbaraj, L.</hi> (2013). Annotation Studio: Bringing a Time-Honored Learning Practice into the Digital Age. 
                        <hi rend=""italic"">Whitepaper</hi>. http://cmsw.mit.edu/annotation-studio-whitepaper/ (accessed 3 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Pierazzo, E.</hi> (2015). 
                        <hi rend=""italic"">Digital Scholarly Editing : Theories, Models and Methods</hi>. Farnham Surrey: Ashgate.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Renear, A. H., Mylonas, E. and Durand, D.</hi> (1996). Refining our notion of what text really is: The problem of overlapping hierarchies. (Ed.) Hockey, S. M. 
                        <hi rend=""italic"">Research in Humanities Computing</hi>, 
                        <hi rend=""bold"">4</hi>: 263–80.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Robinson, P.</hi> (2013). Towards a theory of digital editions. (Ed.) Mierlo, W. V. and Fachard, A. 
                        <hi rend=""italic"">Variants</hi>, (10): 105–31.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ruimy, N., Piccini, S. and Giovannetti, E.</hi> (2012). Defining and Structuring Saussure’s Terminology. In Fjeld, R. V. and Torjusen, J. M. (eds), 
                        <hi rend=""italic"">Proceedings of 15th EURALEX International Congress</hi>. Oslo, Norway, Department of Linguistics and Scandinavian Studies, University of Oslo, Reprosentralen: UiO press, pp. 828–33.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sahle, P.</hi> (2013). 
                        <hi rend=""italic"">Digitale Editionsformen: Teil 3: Textbegriffe Und Recodierung; Zum Umgang Mit Der Überlieferung Unter Den Bedingungen Des Medienwandels</hi>. Vol. 3. BoD–Books on Demand.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Schmidt, D.</hi> (2010). The inadequacy of embedded markup for cultural heritage texts. 
                        <hi rend=""italic"">Literary and Linguistic Computing</hi>, 
                        <hi rend=""bold"">25</hi>(3): 337–56. doi:10.1093/llc/fqq007.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Schmidt, D.</hi> (2014). Towards an Interoperable Digital Scholarly Edition. 
                        <hi rend=""italic"">Journal of the Text Encoding Initiative</hi>(7). doi:10.4000/jtei.979.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sinclair, S. and Rockwell, G.</hi> (2012). the Voyant Tools Team (web application) 
                        <hi rend=""italic"">Voyant Tools</hi>. http://voyant-tools.org (accessed 3 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Steiner, C., Agosti, M., Sweetnam, M., Hillemann, E.-C., Orio, N., Ponchia, C., Hampson, C., et al.</hi> (2014). Evaluating a digital humanities research environment: the CULTURA approach. 
                        <hi rend=""italic"">International Journal on Digital Libraries</hi>, 
                        <hi rend=""bold"">15</hi>(1): 53–70. doi:10.1007/s00799-014-0127-x.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Terras, M. and Crane, G. (eds).</hi> (2010). 
                        <hi rend=""italic"">Changing the Center of Gravity: Transforming Classical Studies through Cyberinfrastructure</hi>. Piscataway: Gorgias Press.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Tulach, J.</hi> (2008). 
                        <hi rend=""italic"">Practical API Design: Confessions of a Java Framework Architect</hi>. 1st ed. Berkely, CA, USA: Apress.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,analysis architectural and design patterns;literary computing;object-oriented analysis and design;textual processing;web infrastructure,English,"archives, repositories, sustainability and preservation;bibliographic methods / textual studies;classical studies;concording and indexing;content analysis;corpora and corpus activities;cultural infrastructure;cultural studies;databases & dbms;data mining / text mining;digital humanities - facilities;digital humanities - institutional support;digitisation, resource creation, and discovery;digitisation - theory and practice;encoding - theory and practice;GLAM: galleries, libraries, archives, museums;information architecture;information retrieval;lexicography;linguistics;linking and annotation;literary studies;metadata;natural language processing;philology;programming;project design, organization, management;scholarly editing;software design and development;standards and interoperability;text analysis;user studies / user needs;xml",2016-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <div type=""div1"" rend=""dh-heading1"">
                <head>introduction</head>
                <p>the development of applications in the field of digital humanities (dh) does not adequately take into account domain modelling, software design principles and software engineering methodologies (bozzi, 2013; d'iorio, 2015; mccarty, 2008; terras and crane, 2010). in fact, many systems developed in the context of dh-related projects have not been conceived to be modular, extensible, and scalable: they only tend to solve specific problems such as data-driven and project-oriented tools (boschetti and del grosso, 2015). in addition, most projects focus on the requirements of humanists (as end users), but leave out the needs of software developers.</p>
                <p>this research was motivated by a number of issues emerged from the projects we worked on 
                    <hi rend=""background-color(#ffff00)""> </hi>(abrate et al., 2014a; albanesi et al., 2015; bellandi et al., 2014; bozzi, 2015; del grosso, 2013; ruimy et al., 2012) and it fits into an ongoing discussion about textual modelling and research infrastructures (moulin et al., 2011; pierazzo, 2015; schmidt, 2014). in particular, this work aims at providing methodological guidelines for the definition of the core entities of a digital scholarly environment. we chose to adopt an object-oriented approach since it can bring benefits in the definition of efficient and effective digital tools (boschetti et al., 2014; del grosso and nahli, 2014). to give an analogy, the environment we propose can help developers and scholars as cms (e.g. wordpress) can help web designers and publishers.
                </p>
                <p>the development of the environment follows three criteria: i) adopting an agile process (ashmore and runyan, 2014) to define the nature and behavior of the environment through both functional (e.g. user stories) and non-functional requirements (e.g. data model, system architecture) (cohn, 2004; collins-cope et al., 2005); ii) providing well-defined application programming interfaces (apis) among components (grill et al., 2012; tulach, 2008); iii) applying analysis, architectural and design patterns for the sake of abstraction, generalization and flexibility (ackerman and gonzalez, 2011; buschmann et al., 2007; gamma et al., 1995).</p>
                <p>following the agile methodology, we are developing a modular environment by starting from the design and implementation of a 
                    <hi rend=""bold"">microkernel</hi> (buschmann et al., 1996) as the manager of the different components. in addition, the microkernel provides all the operations needed to manipulate the domain basic entities which are described in the section “domain entities and design patterns”.
                </p>
                <div type=""div2"" rend=""dh-heading2"">
                    <head>related works</head>
                    <p>digital humanists have access to several tools for literary studies. textgrid, for example, provides integrated tools for analyzing texts and gives computer support for digital editing purposes (hedges et al., 2013). the nines project offers an environment to support scholars in the creation of long-term digital research materials. it includes three main tools: collex (nowviskie, 2007), juxta, and ivanhoe. annotation studio is a collaborative system to annotate texts and add links to multimedia objects (paradis et al., 2013). the cultura project aims at developing a “corpus agnostic research environment” providing customizable services for a wide range of users (steiner et al., 2014). the development of an online workspace which helps scholars in the production of critical editions is the main objective of the workspace for collaborative editing framework (houghton et al., 2014). it uses existing standards and open-source solutions to create an architecture of reusable components. other platforms worth mentioning are tustep/txstep (ott, 2000; ott and ott, 2014), weblicht (hinrichs et al., 2010), perseids (almas and beaulieu, 2013), muruca/pundit (grassi et al., 2013), textual communities (bordalejo and robinson, 2015), saws (jordanous et al., 2012), voyant tools (sinclair and rockwell, 2012), transcribe bentham (causer and terras, 2014) and alcide (moretti et al., 2014). however, the aforementioned initiatives allow digital scholars to meet specific needs, but none of them seems to provide, simultaneously, all the following characteristics: i) reusability and extensibility, ii) ease of use and configuration, iii) continuous availability of the services and development over time, iv) a well-grounded software data model.</p>
                </div>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>domain entities and design patterns</head>
                <p>one of the main challenges of the dh community is to provide suitable software models and tools (ciotti, 2014). to model the literary domain and the relative user requirements, we chose to follow the engineering principles of 
                    <hi rend=""bold"">object-oriented analysis and design</hi> (ackerman and gonzalez, 2011). the digital representation of a textual resource is a challenge as it involves several theoretical and epistemological issues in semiotics, paleography, philology, linguistics, engineering, and computer science (mccarty, 2005; meister, 2012; moretti, 2013; robinson, 2013; sahle, 2013).
                </p>
                <p>in this work, we define each textual element by means of four properties: i) the 
                    <hi rend=""bold"">version</hi> allows to select a specific textual element among those available in its history of changes; ii) the 
                    <hi rend=""bold"">granularity</hi> represents a level of a hierarchical structure (e.g. a page composed of lines); iii) the 
                    <hi rend=""bold"">position</hi> provides the location of a textual element within the hierarchical representation (e.g. the second page of a book); iv) the 
                    <hi rend=""bold"">layer</hi> indicates the set of homogeneous information the textual element belongs to (e.g. morphological layer). as pointed outby (buzzetti, 2002; mcgann, 2004; pierazzo, 2015), the information conveyed by a textual resource is logically organized through multiple layers (also called 
                    <hi rend=""italic"">dimensions</hi>) of information.
                </p>
                <p>
                    <figure>
                        <graphic url=""24/100002010000070400000824354e1f7f0d4dc755.png""/>
                        <head>fig. 1: class diagram of the domain entities</head>
                    </figure>on these four properties we have designed and implemented a set of core entities as the fundamental data types shared among all the components of the environment (fig. 1)
                    <note xml:id=""ftn1"" place=""foot"" n=""1""> the ongoing implementation of the environment is available at: https://github.com/literarycomputinglab</note>. the 
                    <hi rend=""bold"">source</hi> class is in charge of managing the low-level data: it is composed of a 
                    <hi rend=""bold"">payload</hi> representing the information conveyed by the textual resource and a
                    <hi rend=""bold"">sourcetype</hi> which indicates the nature of the source (e.g. text, image, audio, etc.). payload objects (as used in networking) have the only purpose of encapsulating the information. the 
                    <hi rend=""bold"">locus</hi> and the 
                    <hi rend=""bold"">p</hi>
                    <hi rend=""bold"">lace</hi>
                    <hi rend=""bold"">ofinterest</hi> (poi) classes identify, through a 
                    <hi rend=""italic"">composition pattern</hi>, specific data fragments of the source content, and they are used to establish the boundaries of an 
                    <hi rend=""bold"">annotation</hi>. a chunk of text, for example, can be addressed to by a locus having a poi (of type sequence of interest) representing its start and end coordinates. similarly, a region of an image can be identified by a locus having a poi (of type region of interest) composed of a sequence of coordinates. the locus and poi provide a stand-off text annotation technique able to tackle, for example, the overlapping hierarchies problem, which cannot be handled easily with inline markup techniques (schmidt, 2010). as a matter of fact, it is possible, simultaneously, to manipulate a resource on the basis of its documental and textual structure (renear et al., 1996; robinson, 2013) (see the example in the following section). however, since stand-off models are affected by the issue of the indexing updating, a dedicated component must be in charge of automatically maintaining the coherence of the annotations each time the underlying text is edited.
                </p>
                <p>an annotation represents an information associated to a locus and is defined by an 
                    <hi rend=""bold"">annotationtype</hi> (e.g. a token, a lemma, a named entity, etc.). since the hierarchical structure of the source may evolve over time, the changes to the relative tree must be managed. for example, a tree structure having tokens as leaves could need to be updated with a finer-grained layer of characters (e.g. to assign annotations to specific letters). in this case, the tokens should become intermediate nodes and the characters would become the leaf nodes. typically, this kind of editing is unpredictable and it often implies heavy adaptations if the software is not flexible enough to manage changes in the underlying text representation schema. consequently, we decided to exploit the flexibility of the object oriented model by adopting the role design pattern (fowler, 1997) to switch between leaf and intermediate nodes dynamically. this pattern has been implemented by the 
                    <hi rend=""bold"">annotationrole</hi>, 
                    <hi rend=""bold"">annotationroleelement</hi> and 
                    <hi rend=""bold"">annotationrolestructure</hi> classes. moreover, an annotation is a source in itself (see the inheritance relationship between the annotation and the source classes in fig. 1) and, thus, it can be annotated recursively.
                </p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>an example</head>
                <p>we here introduce an example showing a representation of a snippet of text with annotations. the chosen text is an excerpt of a letter, written in latin, belonging to the epistolary corpus of the clavius on the web project
                    <note xml:id=""ftn2"" place=""foot"" n=""2""> clavius on the web is a project funded by registro.it and partecipated by the institute of informatics and telematics (iit-cnr), the institute of computational linguistics “a. zampolli” (ilc-cnr), and the historical archives of the pontifical gregorian university (apug). website: http://claviusontheweb.it/</note> (abrate et al., 2014b). fig. 2 shows a typical way of encoding sentences and lines with a markup language as tei (burnard, 2014): the resulting xml hierarchical structure has been broken by the addition of the line anchors (<lb />) mixing up the textual and documental structure of the text. indeed, to preserve the integrity of the word “dinostrati” (spanning across lines 4 and 5), it is necessary to encapsulate it with the element <w />.
                </p>
                <p>
                    <figure>
                        <graphic url=""24/10000201000006ec00000398e68d91980964673a.png""/>
                        <head>fig. 2: a standard way of encoding a text with tei-xml</head>
                    </figure>
                    <figure>
                        <graphic url=""24/1000020100000386000001f5970322c0327a03ca.png""/>
                        <head>fig. 3: multi-layered stand-off annotation of text</head>
                    </figure>the model we propose solves this problem with the stand-off annotations: as shown in fig. 3 the document (made of lines) and the textual structure (made of sentences and words) are logically separated. lines, sentences and words do not overlap and they are structured in separate hierarchies.
                </p>
            </div>
            <div type=""div1"" rend=""dh-heading"">
                <head>next steps</head>
                <p>we plan, in future works, to release a first version of a web environment, called 
                    <hi rend=""italic"">omega</hi>, built around the core entities that we here described. the environment will allow to load, index, annotate, and query a textual collection. furthermore, we’ll carry on the development of modules for text analysis and textual scholarship with the related apis.
                </p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl>
                        <hi rend=""bold"">abrate, m., </hi>
                        <hi rend=""bold"">del </hi>
                        <hi rend=""bold"">grosso, a. m., giovannetti, e., </hi>
                        <hi rend=""bold"">lo </hi>
                        <hi rend=""bold"">duca, a., luzzi, d., mancini, l., marchetti, a., pedretti, i. and piccini, s.</hi> (2014a). sharing cultural heritage: the clavius on the web project. in calzolari, n., choukri, k., declerck, t., loftsson, h., maegaard, b., mariani, j., moreno, a., odijk, j. and piperidis, s. (eds), 
                        <hi rend=""italic"">proceedings of the 9th international conference on language resources and evaluation (lrec), reykjavik</hi>. european language resources association (elra), pp. 627–34.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">abrate, m., </hi>
                        <hi rend=""bold"">del </hi>
                        <hi rend=""bold"">grosso,</hi>
                        <hi rend=""bold""> </hi>
                        <hi rend=""bold"">a. m., giovannetti, e., </hi>
                        <hi rend=""bold"">lo </hi>
                        <hi rend=""bold"">duca, a., marchetti, a., mancini, l., pedretti, i. and piccini, s.</hi> (2014b). il progetto clavius on the web: tecnologie linguistico-semantiche al servizio del patrimonio documentale e degli archivi storici. in rossi, f. and tomasi, f. (eds), 
                        <hi rend=""italic"">book of abstracts of 3</hi>
                        <hi rend=""sup italic"">o</hi>
                        <hi rend=""italic""> aiucd conference, bologna</hi>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">ackerman, l. and gonzalez, c.</hi> (2011). 
                        <hi rend=""italic"">patterns-based engineering: successfully delivering solutions via patterns</hi>. addison-wesley.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">albanesi, d., bellandi, a., benotto, g., </hi>
                        <hi rend=""bold"">di </hi>
                        <hi rend=""bold"">segni, g. and giovannetti, e.</hi> (2015). when translation requires interpretation: collaborative computer–assisted translation of ancient texts. 
                        <hi rend=""italic"">latech 2015</hi>: 84–88.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">almas, b. and beaulieu, m.-c.</hi> (2013). developing a new integrated editing platform for source documents in classics. 
                        <hi rend=""italic"">literary and linguistic computing</hi>, 
                        <hi rend=""bold"">28</hi>(4): 493–503 doi:10.1093/llc/fqt046.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">ashmore, s. and runyan, k.</hi> (2014). 
                        <hi rend=""italic"">introduction to agile methods</hi>. upper saddle river, nj: addison-wesley professional, pearson education.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">bellandi, a., albanesi, d., bellusci, a., bozzi, a. and giovannetti, e.</hi> (2014). the talmud system: a collaborative web application for the translation of the babylonian talmud into italian. 
                        <hi rend=""italic"">the first italian conference on computational linguistics clic-it 2014</hi>, pp. 53–57.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">bordalejo, b. and robinson, p.</hi> (2015). a new system for collaborative online creation of scholarly editions in digital form. 
                        <hi rend=""italic"">1st dixit convension on technology, software, standards for the digital scholarly edition workshop</hi>. the hague.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">boschetti, f. and </hi>
                        <hi rend=""bold"">del </hi>
                        <hi rend=""bold"">grosso, a. m.</hi> (2015). teicophilib: a library of components for the domain of collaborative philology. 
                        <hi rend=""italic"">journal of the text encoding initiative</hi>(8). doi:10.4000/jtei.1285. http://jtei.revues.org/1285 (accessed 3 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">boschetti, f., </hi>
                        <hi rend=""bold"">del </hi>
                        <hi rend=""bold"">grosso, a. m., khan, a. f., lamé, m. and nahli, o.</hi> (2014). a top-down approach to the design of components for the philological domain. 
                        <hi rend=""italic"">book of abstract of digital humanities conference (dh), lausanne, switzerland</hi>. alliance of digital humanities organisations, pp. 109–11.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">bozzi, a.</hi> (2013). g2a: a web application to study, annotate and scholarly edit ancient texts and their aligned translations. (ed.) erc ideas 249431 
                        <hi rend=""italic"">studia graeco-arabica</hi>, 
                        <hi rend=""bold"">3</hi>: 159–71.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">bozzi, a.</hi> (2015). greek into arabic, a research infrascructure based on computational modules to annotate and query historical and philosophical digital texts. part i: methodological aspects. in bozzi, a. (ed), 
                        <hi rend=""italic"">digital texts, translations, lexicons in a multi-modular web application: methods and samples</hi>. firenze: leo s. olschki editore, pp. 27–42.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">burnard, l.</hi> (2014). tei p5: guidelines for electronic text encoding and interchange. version 2.9.1. http://www.tei-c.org/guidelines/p5/index.xml (accessed 3 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">buschmann, f., henney, k. and schmidt, d. c.</hi> (2007). 
                        <hi rend=""italic"">pattern-oriented software architecture, on patterns and pattern languages</hi>. (pattern-oriented software architecture). hoboken: john wiley & sons.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">buschmann, f., meunier, r., rohnert, h., sommerlad, p. and stal, m.</hi> (1996). pattern-oriented software architecture - a system of patterns. j. wiley and sons ltd., pp. 171–92.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">buzzetti, d.</hi> (2002). digital representation and the text model. 
                        <hi rend=""italic"">new literary history</hi>, 
                        <hi rend=""bold"">33</hi>(1): 61–88.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">causer, t. and terras, m.</hi> (2014). “many hands make light work. many hands together make merry work”: transcribe bentham and crowdsourcing manuscript collections.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">ciotti, f.</hi> (2014). digital literary and cultural studies: state of the art and perspectives. 
                        <hi rend=""italic"">between</hi>, 
                        <hi rend=""bold"">4</hi>(8). doi:10.13125/2039-6597/1392. http://dx.doi.org/10.13125/2039-6597/1392 (accessed 3 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">cohn, m.</hi> (2004). 
                        <hi rend=""italic"">user stories applied: for agile software development</hi>. redwood city, ca, usa: addison wesley longman publishing co., inc.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">collins-cope, m., rosenberg, d. and stephens, m.</hi> (2005). 
                        <hi rend=""italic"">agile development with iconix process: people, process, and pragmatism</hi>. berkely, ca, usa: apress.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">fowler, m.</hi> (1997). dealing with roles. 
                        <hi rend=""italic"">proceedings of the international conference on pattern languages of programs</hi>, vol. 97, pp. 13–37.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">gamma, e., helm, r., johnson, r. and vlissides, j.</hi> (1995). 
                        <hi rend=""italic"">design patterns: elements of reusable object-oriented software</hi>. boston, ma, usa: addison-wesley longman publishing co., inc.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">grassi, m., morbidoni, c., nucci, m., fonda, s. and piazza, f.</hi> (2013). pundit: augmenting web contents with semantics. 
                        <hi rend=""italic"">literary and linguistic computing</hi>, 
                        <hi rend=""bold"">28</hi>(4): 640–59.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">grill, t., polacek, o. and tscheligi, m.</hi> (october 29-312012). methods towards api usability: a structural analysis of usability problem categories. 
                        <hi rend=""italic"">proceedings of the 4th international conference on human-centered software engineering, toulouse, france</hi>. berlin, heidelberg: springer-verlag, pp. 164–80. doi:10.1007/978-3-642-34347-6_10.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">del </hi>
                        <hi rend=""bold"">grosso, a. m.</hi> (2013). indexing techniques and variant readings management. (ed.) d'ancona, c. 
                        <hi rend=""italic"">studia graeco-arabica</hi>, 
                        <hi rend=""bold"">3</hi>: 209–30.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">del </hi>
                        <hi rend=""bold"">grosso, a. m. and nahli, o.</hi> (2014). towards a flexible open-source software library for multi-layered scholarly textual studies: an arabic case study dealing with semi-automatic language processing. 
                        <hi rend=""italic"">proceedings of 3rd ieee international colloquium, information science and technology (cist), tetouan, marocco</hi>. washington, dc, usa: ieee, pp. 285–90. doi:10.1109/cist.2014.7016633.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">hedges, m., neuroth, h., smith, k. m., blanke, t., romary, l., küster, m. and illingworth, m.</hi> (2013). textgrid, textvre, and dariah: sustainability of infrastructures for textual scholarship. 
                        <hi rend=""italic"">journal of the text encoding initiative</hi>(5). doi:10.4000/jtei.774 (accessed 3 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">hinrichs, e., hinrichs, m. and zastrow, t.</hi> (2010). weblicht: web-based lrt services for german. 
                        <hi rend=""italic"">proceedings of the acl 2010 system demonstrations</hi>. association for computational linguistics, pp. 25–29.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">houghton, h., sievers, m. and smith, catherine</hi> (2014). the workspace for collaborative editing. 
                        <hi rend=""italic"">digital humanities 2014</hi>. laussanne: alliance of digital humanities organisations, pp. 204–05.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">d'</hi>
                        <hi rend=""bold"">iorio, p. </hi> (2015). on the scholarly use of the internet, a conceptual model. in bozzi, a. (ed), 
                        <hi rend=""italic"">digital texts, translations, lexicons in a multi-modular web application: methods and samples</hi>. firenze: leo s. olschki editore, pp. 1–25.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">jordanous, a., lawrence, k. f., hedges, m. and tupman, c.</hi> (june 13-152012). exploring manuscripts: sharing ancient wisdoms across the semantic web. 
                        <hi rend=""italic"">proceedings of the 2nd international conference on web intelligence, mining and semantics (wims), craiova, romania</hi>. new york, ny, usa: acm, pp. 44:1–44:12. doi:10.1145/2254129.2254184.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">mccarty, w.</hi> (2005). 
                        <hi rend=""italic"">humanities computing</hi>. palgrave macmillan.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">mccarty, w.</hi> (2008). signs of times present and future. 
                        <hi rend=""italic"">human discussion group</hi>, 
                        <hi rend=""bold"">22</hi>(218).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">mcgann, j.</hi> (2004). marking texts of many dimensions. in schreibman, s., siemens, r. and unsworth, j. (eds), 
                        <hi rend=""italic"">a companion to digital humanities</hi>. (blackwell companions to literature and culture). blackwell publishing ltd, pp. 198–217.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">meister, j. c.</hi> (2012). dh is us or on the unbearable lightness of a shared methodology. 
                        <hi rend=""italic"">historical social research</hi>, 
                        <hi rend=""bold"">37</hi>(3): 77–85.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">moretti, f.</hi> (2013). 
                        <hi rend=""italic"">distant reading</hi>. verso books.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">moretti, g., tonelli, s., menini, s. and sprugnoli, r.</hi> (2014). alcide: an online platform for the analysis of language and content in a digital environment. 
                        <hi rend=""italic"">proceedings of the first italian conference on computational linguistics (clic-2014)</hi>. pisa, italy.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">moulin, c., nyhan, j., ciula, a., kelleher, m., mittler, e., tadić, m., ågren, m., bozzi, a. and kuutma, k.</hi> (2011). 
                        <hi rend=""italic"">research infrastructures in the digital humanities</hi>. http://www.esf.org/hosting-experts/scientific-review-groups/humanities-hum/strategic-activities/research-infrastructures-in-the-humanities.html.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">nowviskie, b.</hi> (2007). collex: facets, folksonomy, and fashioning the remixable web. 
                        <hi rend=""italic"">book of abstract of digital humanities conference (dh), university of illinois at urbana-champaign</hi>. alliance of digital humanities organisations.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">ott, w.</hi> (2000). strategies and tools for textual scholarship: the tübingen system of text processing programs (tustep). 
                        <hi rend=""italic"">literary and linguistic computing</hi>, 
                        <hi rend=""bold"">15</hi>(1): 93–108. doi:10.1093/llc/15.1.93. http://llc.oxfordjournals.org/content/15/1/93.abstract.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">ott, w. and ott, t.</hi> (2014). critical editing with txstep. in terras, m. (ed), 
                        <hi rend=""italic"">book of abstracts of the digital humanities conference, lausanne, switzerland</hi>. alliance of digital humanities organisations, pp. 509–13.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">paradis, j., fendt, k., kelley, w., folsom, j., pankow, j., graham, e. and subbaraj, l.</hi> (2013). annotation studio: bringing a time-honored learning practice into the digital age. 
                        <hi rend=""italic"">whitepaper</hi>. http://cmsw.mit.edu/annotation-studio-whitepaper/ (accessed 3 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">pierazzo, e.</hi> (2015). 
                        <hi rend=""italic"">digital scholarly editing : theories, models and methods</hi>. farnham surrey: ashgate.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">renear, a. h., mylonas, e. and durand, d.</hi> (1996). refining our notion of what text really is: the problem of overlapping hierarchies. (ed.) hockey, s. m. 
                        <hi rend=""italic"">research in humanities computing</hi>, 
                        <hi rend=""bold"">4</hi>: 263–80.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">robinson, p.</hi> (2013). towards a theory of digital editions. (ed.) mierlo, w. v. and fachard, a. 
                        <hi rend=""italic"">variants</hi>, (10): 105–31.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">ruimy, n., piccini, s. and giovannetti, e.</hi> (2012). defining and structuring saussure’s terminology. in fjeld, r. v. and torjusen, j. m. (eds), 
                        <hi rend=""italic"">proceedings of 15th euralex international congress</hi>. oslo, norway, department of linguistics and scandinavian studies, university of oslo, reprosentralen: uio press, pp. 828–33.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">sahle, p.</hi> (2013). 
                        <hi rend=""italic"">digitale editionsformen: teil 3: textbegriffe und recodierung; zum umgang mit der überlieferung unter den bedingungen des medienwandels</hi>. vol. 3. bod–books on demand.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">schmidt, d.</hi> (2010). the inadequacy of embedded markup for cultural heritage texts. 
                        <hi rend=""italic"">literary and linguistic computing</hi>, 
                        <hi rend=""bold"">25</hi>(3): 337–56. doi:10.1093/llc/fqq007.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">schmidt, d.</hi> (2014). towards an interoperable digital scholarly edition. 
                        <hi rend=""italic"">journal of the text encoding initiative</hi>(7). doi:10.4000/jtei.979.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">sinclair, s. and rockwell, g.</hi> (2012). the voyant tools team (web application) 
                        <hi rend=""italic"">voyant tools</hi>. http://voyant-tools.org (accessed 3 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">steiner, c., agosti, m., sweetnam, m., hillemann, e.-c., orio, n., ponchia, c., hampson, c., et al.</hi> (2014). evaluating a digital humanities research environment: the cultura approach. 
                        <hi rend=""italic"">international journal on digital libraries</hi>, 
                        <hi rend=""bold"">15</hi>(1): 53–70. doi:10.1007/s00799-014-0127-x.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">terras, m. and crane, g. (eds).</hi> (2010). 
                        <hi rend=""italic"">changing the center of gravity: transforming classical studies through cyberinfrastructure</hi>. piscataway: gorgias press.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">tulach, j.</hi> (2008). 
                        <hi rend=""italic"">practical api design: confessions of a java framework architect</hi>. 1st ed. berkely, ca, usa: apress.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
",3.0,4.0,Voyant
2492,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,Enhancing Close Reading,,Muhammad Faisal Cheema;Stefan Jänicke;Gerik Scheuermann,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Motivation</head>
                <p>In last years, the advancements in computer science brought a global change in the way information is stored, retrieved and analyzed. The digital humanities also benefit from these developments, and now, a vast amount of texts is available in digital form. This information explosion generates interesting research questions for humanities scholars who are capable of deriving new insights from this knowledge bank. In order to support humanities scholars, many visualization techniques – summarized in a survey (Jänicke et al., 2015b) – were developed to aid exploring large texts collections. Most of these techniques are interactive and belong to the category of distant reading (Moretti, 2005). The authors of the mentioned survey observe that less work has been done to improve the close reading capabilities of humanities scholars even though they are often focused on close reading text passages.</p>
                <p>Close reading is the careful interpretation of the text, where the scholar iteratively reads the text in order to explore its meaning, inherent topics and occurring relationships (Boyles, 2013). Traditionally, close reading is done on paper. Several ideas and thoughts are made persistent by annotations written at the margins alongside the text (see Figure 1). But as the margin space is limited, not all observations can be put around the text. So, annotations may become cluttered and confusing for the reader, especially, when obsolete ideas are struck through. Despite its disadvantages, annotating on paper is still quite popular as it benefits the scholars to record observations about the hypothesis and all these changes reappear in front of the scholar’s eyes as soon as he re-reads the text passage. We observed that the way of annotating in close reading resembles the idea of mind maps (Buzan et al., 1993) that are based on a central concept and thoughts are represented around it using lines and text. In the close reading scenario, the text can be considered as the central concept and annotations represent thoughts.</p>
                <p>An important task of computer science is to enhance the original workflows of researchers with computational methods. As most humanities scholars are well trained in close reading and nowadays often work with digital texts, it is necessary to enhance their capabilities for digital close reading. We propose an enhanced close reading design inspired by mind-maps that not only mimics the traditional way of annotating a text on paper, but also helps humanities scholars to perform live visual analyses. Furthermore, we use extendible margins to provide enough space for all thoughts of the scholar.</p>
                <p>
                    <anchor xml:id=""id_docs-internal-guid-cfc761b5-411d-b964-effb-76d244703cf0""/>
                    <hi rend=""color(#000000)"">
                        <figure>
                            <graphic url=""262/1000000000000640000004D5F3E06A1C.png""/>
                            <head>Figure 1: Traditional close reading on paper</head>
                        </figure>
                    </hi>
                    <hi rend=""color(#000000)"">
                        <note xml:id=""ftn1"" place=""foot"" n=""1"">
                            <anchor xml:id=""id_docs-internal-guid-cfc761b5-418b-4abf-34b3-191157a29b72""/>
                            <hi rend=""color(#000000)"">Image reproduced with permission from Kehoe (Kehoe et al., 2013)</hi>
                        </note>
                    </hi>
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Related Work</head>
                <p>Nancy Boyles (Boyles, 2013) defines close reading, which has become a fundamental method in literary criticism in the 20th century (Hawthorn, 2000), as follows: “Essentially, close reading means reading to uncover layers of meaning that lead to deep comprehension.” Annotating the text in close reading is a strong method for scholars to facilitate the understanding of a text passage. Figure 1 shows the result of a traditional close reading approach. In this example, various annotation methods were used by the scholar to annotate various features of a text passage in Charles Dickens' „David Copperfield“. </p>
                <p>The availability of digital texts has further awaken the interest of humanities scholars in collaboratively close reading the same texts. There are several annotation tools for such a purpose, such as eMargin (Kehoe et al., 2013), Hypothes.is (Bonn et al., 2014) and NB (Zyto et al., 2012). These tools are beneficial for collaborative research and classroom environments as they provide an excellent paradigm to share thoughts, as well as find collective answers. To avoid clutter, these tools work with popup windows that are only shown on demand. In Figure 2, the eMargin system is shown where colors are used to highlight different text features, and a popup window on demand, lists the comments of collaborating scholars.</p>
                <p>
                    <anchor xml:id=""id_docs-internal-guid-cfc761b5-412e-0f91-0c44-c948a3dc5484""/>
                    <hi rend=""color(#000000)"">
                        <figure>
                            <graphic url=""262/10000201000003CB000002F649BAF3AA.png""/>
                            <head>Figure 2: eMargin annotation tool</head>
                        </figure>
                    </hi>
                    <hi rend=""color(#000000)"">
                        <note xml:id=""ftn2"" place=""foot"" n=""2"">
                            <anchor xml:id=""id_docs-internal-guid-cfc761b5-418b-4abf-34b3-191157a29b722""/>
                            <hi rend=""color(#000000)"">Image reproduced with permission from Kehoe (Kehoe et al., 2013)</hi>
                        </note>
                    </hi>
                </p>
                <p>Digital Ink Annotations systems (Schilit, 1998, Bargeron et al., 2003, Agrawala et al., 2005, Yoon et al., 2013) also support annotating text, but their use is only limited to pen-based computing devices such as tablets. The systems are designed to work well on smaller screens, and the adaption to larger screens is not appropriately implemented. </p>
                <p>Close reading tasks can also be assisted via distant reading tools. For example, parallel coordinates, a heatmap and a dot plot are used to analyze the variance of a selected text passage from different German translations of Shakespeare’s Othello (Geng et al., 2013). Heat maps are appropriate visualizations to illustrate the distribution of specific phrases or annotations in a corpus (Muralidharan, 2011, Alex et al., 2015). Voyant Tools allow the user to perform basic text mining functions with selected word statistics shown in linked views (Sinclair et al., 2012). The Voyant Tools interface in Figure 3 shows statistics about Chapter 2 of Oscar Wilde's “David Copperfield”. Goffin's idea to enhance close reading is the integration of small visualizations (e.g., maps or bar charts) besides the words of a text (Goffin et al., 2014).</p>
                <p>
                    <figure>
                        <graphic url=""262/100002010000077C000003DA2D6B26FC.png""/>
                        <head>Figure 3: Screenshot of web-based Voyant Tools (Sinclair et al., 2012).</head>
                    </figure>
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Enhanced Close Reading Design</head>
                <p>In contrast to the tools mentioned above, we combine traditional annotation tasks with distant reading analyses to enhance the close reading capabilities of the scholar. We suggest a design inspired by mind mapping (an example mind map is shown in Figure 4a), a methodology that allows a researcher to work on a central concept, and thoughts and features about that concept are placed around it using figures, lines etc. In a mind map, the associations spread out from a central concept in a free-flowing, yet organized and coherent manner (Budd, 2004) - thus forming a mental map of the central concept. We observe that like in the case of mind maps, fixed annotations around the central text in a traditional close reading process facilitate forming a mental map of the thoughts about the text of interest, and help the scholar to draw conclusions when seeing the whole picture.</p>
                <table rend=""frame"" xml:id=""Table1"">
                    <row>
                        <cell>
                            <figure>
                                <graphic url=""262/10000200000002F4000001B428DDC0DA.png""/>
                                <head>Figure 4a: An example mind map</head>
                            </figure>
                            <note xml:id=""ftn3"" place=""foot"" n=""3""> Image reproduced with permission from Kanter (Kanter, 2015) (Figure under CC BY 2.0 license, see 
                                <ptr target=""https://creativecommons.org/licenses/by/2.0/""/> for details).
                            </note>
                        </cell>
                        <cell>
                            <figure>
                                <graphic url=""262/10000201000002F4000001B45878ECC5.png""/>
                                <head>Figure 4b: Mind-map inspired close reading</head>
                            </figure>
                        </cell>
                    </row>
                </table>
                <p>Figure 4b illustrates the idea of a mind map inspired interface with multiple types of annotations supporting the scholar in the close reading process. Textual annotations known from the traditional close reading are also necessary in the digital process. In addition, images, videos and charts can facilitate text interpretation and the generation of valuable hypotheses about the text. To support dynamic, multifarious views on a certain text passage or a term of interest, we designed our interface the way that the literary scholar can apply a multitude of visual analyses and generate distant reading visualizations that are placed as annotations alongside the text. This combines the traditional close reading paradigm with elaborated text visualization techniques valuable for exploration purposes. An important feature of our proposed interface design is to support the scholar to „stay in the flow“ (Bederson, 2004), so that the central focus remains on the text, which can be analyzed without interrupting the scholar. The major advantage of our design over existing tools that assist close reading tasks is interface versatility. For example, Voyant Tools (see Figure 3) provide a predefined set of visualizations based on text statistics. On the other hand, our design allows the scholar to choose an appropriate text visualization as an annotation alongside the text, which is based on a user-defined query on the text.. Therefore, the scholar can apply different text visualizations for different passages of the text to support a variety of close reading tasks. </p>
                <p>An example of the design discussed above is shown in Figure 5. The example from Figure 1 is annotated using different kinds of annotations. Like in other digital tools, certain topics of the text are annotated using colors. In addition, the character(s) Peggotty is marked and a panel shows thumbnail images based on a Google Images search. Also the relative word frequency chart of the term “Peggotty” in Chapter 2 is shown on the bottom left. Furthermore, on the left area, a TagPie (Jänicke et al., 2015a) showing the co-occurrences of both the terms memory and observation helps to investigate the hypothesis of the literary scholar about the similar meaning of both topics. The example depicts how the scholar can use different annotation tools as well as different distant reading tools to enrich the close reading experience.</p>
                <p>
                    <figure>
                        <graphic url=""262/100002010000075F00000440C3691AC9.png""/>
                        <head>Figure 5: Example of our design</head>
                    </figure>
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Future Work and Conclusions</head>
                <p>We held discussion with the collaborating humanities scholars about the design as well as the usability of the proposed interface. The scholars remarked that such an interface will help removing fears of using digital humanities tools and that they intend to use the tool as it mimics their existing workflows. They also mentioned that such a tool could help users getting a better big picture of the text, and that it enhances the close reading capabilities of the scholar. Another important point is the capability in supporting teaching activities. They mentioned that various types of annotations (text, pictures, charts) are also used in teaching material, but it is not easy to share these with students. Such a tool could support this process as it generates persistent annotations to be analyzed and discussed collaboratively in courses. </p>
                <p>We observe that the scholar’s initial reactions after seeing the prototype of the tool, which is still in development, are convincing and encouraging. We think that rigid modeling syntax is inappropriate for annotation. Our final interface will allow the scholar to make annotation styles versatile. At the digital humanities conference, we will demonstrate our prototype and discuss future prospects within the community. An additional user study will compare the viability of our proposed, mind map inspired annotation technique to existing approaches.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Acknowledgements</head>
                <p>We thank our colleagues from the humanities department, Judith Blumenstein in particular, who provided insights and expertise that greatly assisted this research.</p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Agrawala, M. and Shilman, M.</hi> (2005). DIZI: a digital ink zooming interface for document annotation. <hi rend=""italic"">Human-Computer Interaction-INTERACT 2005</hi>, Springer Berlin Heidelberg, pp. 69-79.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Alex, B., Grover, C., Zhou, K., Hinrichs and Palimpsest, U.</hi> (2015). Improving Assisted Curation of Loco-specific Literature. <hi rend=""italic"">Proceedings of the Digital Humanities 2015</hi>, pp. 5-7.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Bargeron, D. and Moscovich, T.</hi> (2003). Reflowing digital ink annotations. <hi rend=""italic"">Proceedings of the SIGCHI conference on Human factors in computing systems</hi>, ACM, pp. 385-93.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Bederson, B. B.</hi> (2004). Interfaces for staying in the flow. <hi rend=""italic"">Ubiquity</hi>, 1-1.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Bonn, M. and McGlone, J.</hi> (2014). New Feature: Article Annotation with Hypothesis. <hi rend=""italic"">Journal of Electronic Publishing</hi>, <hi rend=""bold"">17</hi>(2).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Boyles, N.</hi> (2013). Closing in on Close Reading. <hi rend=""italic"">Educational Leadership</hi>, <hi rend=""bold"">70</hi>(4): 36–41.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Budd, J. W.</hi> (2004). Mind Maps as Classroom Exercises. <hi rend=""italic"">The Journal of Economic Education</hi>, <hi rend=""bold"">35</hi>(1): 35–46.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Buzan, T. and Buzan, B.</hi> (1993). The Mind Map Book How to Use Radiant Thinking to Maximise Your Brain's Untapped Potential. New York: Plume.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Geng, Z., Cheesman, T., Laramee, R. S., Flanagan, K. and Thiel, S.</hi> (2013). ShakerVis: Visual analysis of segment variation of German translations of Shakespeare’s Othello. <hi rend=""italic"">Information Visualization</hi>, <hi rend=""bold"">15</hi>: 93-116.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Goffin, P., Willett, W., Fekete, J. D. and Isenberg, P.</hi> (2014). Exploring the placement and design of word-scale visualizations. Visualization and Computer Graphics, <hi rend=""italic"">IEEE Transactions</hi>, <hi rend=""bold"">20</hi>(12): 2291-300.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Hawthorn, J.</hi> (2000). <hi rend=""italic"">A glossary of contemporary literary theory</hi>. Oxford University Press.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Jänicke, S., Blumenstein, J., Rücker, M., Zeckzer, D. and Scheuermann, G.</hi> (2015a). Visualizing the Results of Search Queries on Ancient Text Corpora with Tag Pies. <hi rend=""italic"">Digital Humanities Quarterly</hi>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Jänicke, S., Franzini, G., Cheema, M. F. and Scheuermann, G.</hi> (2015b). On Close and Distant Reading in Digital Humanities: A Survey and Future Challenges. In Borgo, R., Ganovelli, F., and Viola, I. (eds.), <hi rend=""italic"">Eurographics Conference on Visualization (EuroVis) - STARs (2015)</hi>, The Eurographics Association.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kanter, B.</hi> (2015). Cambodia4kids.org, https://www.flickr.com/photos/cambodia4kidsorg/6195211411 (Retrieved 2015-11-25).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kehoe, A. and Gee, M.</hi> (2013). eMargin: A Collaborative Textual Annotation Tool. <hi rend=""italic"">Ariadne</hi>, <hi rend=""bold"">71</hi>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">McCabe, M. M.</hi> (2015). <hi rend=""italic"">Platonic Conversations</hi>. Oxford University Press.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Moretti, F.</hi> (2005). <hi rend=""italic"">Graphs, Maps, Trees: Abstract Models for a Literary History</hi>. New York: Verso.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Muralidharan, A.</hi> (2011). A Visual Interface for Exploring Language Use in Slave Narratives. <hi rend=""italic"">Proceedings of the Digital Humanities 2011</hi>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Schilit, B. N., Golovchinsky, G. and Price, M. N.</hi> (1998). Beyond paper: supporting active reading with free form digital ink annotations. <hi rend=""italic"">Proceedings of the SIGCHI conference on Human factors in computing systems</hi>, ACM Press/Addison-Wesley Publishing Co., pp. 249-56.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sinclair, S. and Rockwell, G.</hi> (2012). Voyant Tools. Online: http://voyant-tools.org (Retrieved 2015-11-25).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Yoon, D., Chen, N. and Guimbretière, F.</hi> (2013). TextTearing: Opening white space for digital ink annotation. <hi rend=""italic"">Proceedings of the 26th annual ACM symposium on User interface software and technology</hi>, ACM, pp. 107-12. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Zyto, S., Karger, D., Ackerman, M. and Mahajan, S. (2012).</hi> Successful classroom deployment of a social document annotation system. <hi rend=""italic"">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</hi>, ACM, pp. 1883-92.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,annotations;close reading;distant reading,English,interface and user experience design;knowledge representation;linking and annotation;literary studies;text analysis;visualization,2016-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <div type=""div1"" rend=""dh-heading1"">
                <head>motivation</head>
                <p>in last years, the advancements in computer science brought a global change in the way information is stored, retrieved and analyzed. the digital humanities also benefit from these developments, and now, a vast amount of texts is available in digital form. this information explosion generates interesting research questions for humanities scholars who are capable of deriving new insights from this knowledge bank. in order to support humanities scholars, many visualization techniques – summarized in a survey (jänicke et al., 2015b) – were developed to aid exploring large texts collections. most of these techniques are interactive and belong to the category of distant reading (moretti, 2005). the authors of the mentioned survey observe that less work has been done to improve the close reading capabilities of humanities scholars even though they are often focused on close reading text passages.</p>
                <p>close reading is the careful interpretation of the text, where the scholar iteratively reads the text in order to explore its meaning, inherent topics and occurring relationships (boyles, 2013). traditionally, close reading is done on paper. several ideas and thoughts are made persistent by annotations written at the margins alongside the text (see figure 1). but as the margin space is limited, not all observations can be put around the text. so, annotations may become cluttered and confusing for the reader, especially, when obsolete ideas are struck through. despite its disadvantages, annotating on paper is still quite popular as it benefits the scholars to record observations about the hypothesis and all these changes reappear in front of the scholar’s eyes as soon as he re-reads the text passage. we observed that the way of annotating in close reading resembles the idea of mind maps (buzan et al., 1993) that are based on a central concept and thoughts are represented around it using lines and text. in the close reading scenario, the text can be considered as the central concept and annotations represent thoughts.</p>
                <p>an important task of computer science is to enhance the original workflows of researchers with computational methods. as most humanities scholars are well trained in close reading and nowadays often work with digital texts, it is necessary to enhance their capabilities for digital close reading. we propose an enhanced close reading design inspired by mind-maps that not only mimics the traditional way of annotating a text on paper, but also helps humanities scholars to perform live visual analyses. furthermore, we use extendible margins to provide enough space for all thoughts of the scholar.</p>
                <p>
                    <anchor xml:id=""id_docs-internal-guid-cfc761b5-411d-b964-effb-76d244703cf0""/>
                    <hi rend=""color(#000000)"">
                        <figure>
                            <graphic url=""262/1000000000000640000004d5f3e06a1c.png""/>
                            <head>figure 1: traditional close reading on paper</head>
                        </figure>
                    </hi>
                    <hi rend=""color(#000000)"">
                        <note xml:id=""ftn1"" place=""foot"" n=""1"">
                            <anchor xml:id=""id_docs-internal-guid-cfc761b5-418b-4abf-34b3-191157a29b72""/>
                            <hi rend=""color(#000000)"">image reproduced with permission from kehoe (kehoe et al., 2013)</hi>
                        </note>
                    </hi>
                </p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>related work</head>
                <p>nancy boyles (boyles, 2013) defines close reading, which has become a fundamental method in literary criticism in the 20th century (hawthorn, 2000), as follows: “essentially, close reading means reading to uncover layers of meaning that lead to deep comprehension.” annotating the text in close reading is a strong method for scholars to facilitate the understanding of a text passage. figure 1 shows the result of a traditional close reading approach. in this example, various annotation methods were used by the scholar to annotate various features of a text passage in charles dickens' „david copperfield“. </p>
                <p>the availability of digital texts has further awaken the interest of humanities scholars in collaboratively close reading the same texts. there are several annotation tools for such a purpose, such as emargin (kehoe et al., 2013), hypothes.is (bonn et al., 2014) and nb (zyto et al., 2012). these tools are beneficial for collaborative research and classroom environments as they provide an excellent paradigm to share thoughts, as well as find collective answers. to avoid clutter, these tools work with popup windows that are only shown on demand. in figure 2, the emargin system is shown where colors are used to highlight different text features, and a popup window on demand, lists the comments of collaborating scholars.</p>
                <p>
                    <anchor xml:id=""id_docs-internal-guid-cfc761b5-412e-0f91-0c44-c948a3dc5484""/>
                    <hi rend=""color(#000000)"">
                        <figure>
                            <graphic url=""262/10000201000003cb000002f649baf3aa.png""/>
                            <head>figure 2: emargin annotation tool</head>
                        </figure>
                    </hi>
                    <hi rend=""color(#000000)"">
                        <note xml:id=""ftn2"" place=""foot"" n=""2"">
                            <anchor xml:id=""id_docs-internal-guid-cfc761b5-418b-4abf-34b3-191157a29b722""/>
                            <hi rend=""color(#000000)"">image reproduced with permission from kehoe (kehoe et al., 2013)</hi>
                        </note>
                    </hi>
                </p>
                <p>digital ink annotations systems (schilit, 1998, bargeron et al., 2003, agrawala et al., 2005, yoon et al., 2013) also support annotating text, but their use is only limited to pen-based computing devices such as tablets. the systems are designed to work well on smaller screens, and the adaption to larger screens is not appropriately implemented. </p>
                <p>close reading tasks can also be assisted via distant reading tools. for example, parallel coordinates, a heatmap and a dot plot are used to analyze the variance of a selected text passage from different german translations of shakespeare’s othello (geng et al., 2013). heat maps are appropriate visualizations to illustrate the distribution of specific phrases or annotations in a corpus (muralidharan, 2011, alex et al., 2015). voyant tools allow the user to perform basic text mining functions with selected word statistics shown in linked views (sinclair et al., 2012). the voyant tools interface in figure 3 shows statistics about chapter 2 of oscar wilde's “david copperfield”. goffin's idea to enhance close reading is the integration of small visualizations (e.g., maps or bar charts) besides the words of a text (goffin et al., 2014).</p>
                <p>
                    <figure>
                        <graphic url=""262/100002010000077c000003da2d6b26fc.png""/>
                        <head>figure 3: screenshot of web-based voyant tools (sinclair et al., 2012).</head>
                    </figure>
                </p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>enhanced close reading design</head>
                <p>in contrast to the tools mentioned above, we combine traditional annotation tasks with distant reading analyses to enhance the close reading capabilities of the scholar. we suggest a design inspired by mind mapping (an example mind map is shown in figure 4a), a methodology that allows a researcher to work on a central concept, and thoughts and features about that concept are placed around it using figures, lines etc. in a mind map, the associations spread out from a central concept in a free-flowing, yet organized and coherent manner (budd, 2004) - thus forming a mental map of the central concept. we observe that like in the case of mind maps, fixed annotations around the central text in a traditional close reading process facilitate forming a mental map of the thoughts about the text of interest, and help the scholar to draw conclusions when seeing the whole picture.</p>
                <table rend=""frame"" xml:id=""table1"">
                    <row>
                        <cell>
                            <figure>
                                <graphic url=""262/10000200000002f4000001b428ddc0da.png""/>
                                <head>figure 4a: an example mind map</head>
                            </figure>
                            <note xml:id=""ftn3"" place=""foot"" n=""3""> image reproduced with permission from kanter (kanter, 2015) (figure under cc by 2.0 license, see 
                                <ptr target=""https://creativecommons.org/licenses/by/2.0/""/> for details).
                            </note>
                        </cell>
                        <cell>
                            <figure>
                                <graphic url=""262/10000201000002f4000001b45878ecc5.png""/>
                                <head>figure 4b: mind-map inspired close reading</head>
                            </figure>
                        </cell>
                    </row>
                </table>
                <p>figure 4b illustrates the idea of a mind map inspired interface with multiple types of annotations supporting the scholar in the close reading process. textual annotations known from the traditional close reading are also necessary in the digital process. in addition, images, videos and charts can facilitate text interpretation and the generation of valuable hypotheses about the text. to support dynamic, multifarious views on a certain text passage or a term of interest, we designed our interface the way that the literary scholar can apply a multitude of visual analyses and generate distant reading visualizations that are placed as annotations alongside the text. this combines the traditional close reading paradigm with elaborated text visualization techniques valuable for exploration purposes. an important feature of our proposed interface design is to support the scholar to „stay in the flow“ (bederson, 2004), so that the central focus remains on the text, which can be analyzed without interrupting the scholar. the major advantage of our design over existing tools that assist close reading tasks is interface versatility. for example, voyant tools (see figure 3) provide a predefined set of visualizations based on text statistics. on the other hand, our design allows the scholar to choose an appropriate text visualization as an annotation alongside the text, which is based on a user-defined query on the text.. therefore, the scholar can apply different text visualizations for different passages of the text to support a variety of close reading tasks. </p>
                <p>an example of the design discussed above is shown in figure 5. the example from figure 1 is annotated using different kinds of annotations. like in other digital tools, certain topics of the text are annotated using colors. in addition, the character(s) peggotty is marked and a panel shows thumbnail images based on a google images search. also the relative word frequency chart of the term “peggotty” in chapter 2 is shown on the bottom left. furthermore, on the left area, a tagpie (jänicke et al., 2015a) showing the co-occurrences of both the terms memory and observation helps to investigate the hypothesis of the literary scholar about the similar meaning of both topics. the example depicts how the scholar can use different annotation tools as well as different distant reading tools to enrich the close reading experience.</p>
                <p>
                    <figure>
                        <graphic url=""262/100002010000075f00000440c3691ac9.png""/>
                        <head>figure 5: example of our design</head>
                    </figure>
                </p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>future work and conclusions</head>
                <p>we held discussion with the collaborating humanities scholars about the design as well as the usability of the proposed interface. the scholars remarked that such an interface will help removing fears of using digital humanities tools and that they intend to use the tool as it mimics their existing workflows. they also mentioned that such a tool could help users getting a better big picture of the text, and that it enhances the close reading capabilities of the scholar. another important point is the capability in supporting teaching activities. they mentioned that various types of annotations (text, pictures, charts) are also used in teaching material, but it is not easy to share these with students. such a tool could support this process as it generates persistent annotations to be analyzed and discussed collaboratively in courses. </p>
                <p>we observe that the scholar’s initial reactions after seeing the prototype of the tool, which is still in development, are convincing and encouraging. we think that rigid modeling syntax is inappropriate for annotation. our final interface will allow the scholar to make annotation styles versatile. at the digital humanities conference, we will demonstrate our prototype and discuss future prospects within the community. an additional user study will compare the viability of our proposed, mind map inspired annotation technique to existing approaches.</p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>acknowledgements</head>
                <p>we thank our colleagues from the humanities department, judith blumenstein in particular, who provided insights and expertise that greatly assisted this research.</p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl>
                        <hi rend=""bold"">agrawala, m. and shilman, m.</hi> (2005). dizi: a digital ink zooming interface for document annotation. <hi rend=""italic"">human-computer interaction-interact 2005</hi>, springer berlin heidelberg, pp. 69-79.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">alex, b., grover, c., zhou, k., hinrichs and palimpsest, u.</hi> (2015). improving assisted curation of loco-specific literature. <hi rend=""italic"">proceedings of the digital humanities 2015</hi>, pp. 5-7.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">bargeron, d. and moscovich, t.</hi> (2003). reflowing digital ink annotations. <hi rend=""italic"">proceedings of the sigchi conference on human factors in computing systems</hi>, acm, pp. 385-93.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">bederson, b. b.</hi> (2004). interfaces for staying in the flow. <hi rend=""italic"">ubiquity</hi>, 1-1.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">bonn, m. and mcglone, j.</hi> (2014). new feature: article annotation with hypothesis. <hi rend=""italic"">journal of electronic publishing</hi>, <hi rend=""bold"">17</hi>(2).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">boyles, n.</hi> (2013). closing in on close reading. <hi rend=""italic"">educational leadership</hi>, <hi rend=""bold"">70</hi>(4): 36–41.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">budd, j. w.</hi> (2004). mind maps as classroom exercises. <hi rend=""italic"">the journal of economic education</hi>, <hi rend=""bold"">35</hi>(1): 35–46.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">buzan, t. and buzan, b.</hi> (1993). the mind map book how to use radiant thinking to maximise your brain's untapped potential. new york: plume.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">geng, z., cheesman, t., laramee, r. s., flanagan, k. and thiel, s.</hi> (2013). shakervis: visual analysis of segment variation of german translations of shakespeare’s othello. <hi rend=""italic"">information visualization</hi>, <hi rend=""bold"">15</hi>: 93-116.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">goffin, p., willett, w., fekete, j. d. and isenberg, p.</hi> (2014). exploring the placement and design of word-scale visualizations. visualization and computer graphics, <hi rend=""italic"">ieee transactions</hi>, <hi rend=""bold"">20</hi>(12): 2291-300.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">hawthorn, j.</hi> (2000). <hi rend=""italic"">a glossary of contemporary literary theory</hi>. oxford university press.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">jänicke, s., blumenstein, j., rücker, m., zeckzer, d. and scheuermann, g.</hi> (2015a). visualizing the results of search queries on ancient text corpora with tag pies. <hi rend=""italic"">digital humanities quarterly</hi>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">jänicke, s., franzini, g., cheema, m. f. and scheuermann, g.</hi> (2015b). on close and distant reading in digital humanities: a survey and future challenges. in borgo, r., ganovelli, f., and viola, i. (eds.), <hi rend=""italic"">eurographics conference on visualization (eurovis) - stars (2015)</hi>, the eurographics association.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">kanter, b.</hi> (2015). cambodia4kids.org, https://www.flickr.com/photos/cambodia4kidsorg/6195211411 (retrieved 2015-11-25).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">kehoe, a. and gee, m.</hi> (2013). emargin: a collaborative textual annotation tool. <hi rend=""italic"">ariadne</hi>, <hi rend=""bold"">71</hi>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">mccabe, m. m.</hi> (2015). <hi rend=""italic"">platonic conversations</hi>. oxford university press.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">moretti, f.</hi> (2005). <hi rend=""italic"">graphs, maps, trees: abstract models for a literary history</hi>. new york: verso.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">muralidharan, a.</hi> (2011). a visual interface for exploring language use in slave narratives. <hi rend=""italic"">proceedings of the digital humanities 2011</hi>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">schilit, b. n., golovchinsky, g. and price, m. n.</hi> (1998). beyond paper: supporting active reading with free form digital ink annotations. <hi rend=""italic"">proceedings of the sigchi conference on human factors in computing systems</hi>, acm press/addison-wesley publishing co., pp. 249-56.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">sinclair, s. and rockwell, g.</hi> (2012). voyant tools. online: http://voyant-tools.org (retrieved 2015-11-25).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">yoon, d., chen, n. and guimbretière, f.</hi> (2013). texttearing: opening white space for digital ink annotation. <hi rend=""italic"">proceedings of the 26th annual acm symposium on user interface software and technology</hi>, acm, pp. 107-12. 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">zyto, s., karger, d., ackerman, m. and mahajan, s. (2012).</hi> successful classroom deployment of a social document annotation system. <hi rend=""italic"">proceedings of the sigchi conference on human factors in computing systems</hi>, acm, pp. 1883-92.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
",5.0,6.0,Voyant
2524,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,The Trace of Theory: Extracting Subsets from Large Collections,,Geoffrey Rockwell;Laura Mandell;Stéfan Sinclair;Matthew Wilkens;Boris Capitanu;Stephen Downie,panel / roundtable,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Introduction</head>
                <p>Can we find and track theory, especially literary theory, in very large collections of texts using computers? This panel discusses a pragmatic two-step approach to trying to track and then visually explore theory through its textual traces in large collections like those of the HathiTrust.</p>
                <list type=""ordered"">
                    <item>
                        <hi rend=""bold"">Subsetting: </hi>The first problem we will discuss is how to extract thematic subsets of texts from very large collections like those of the HathiTrust. We experimented with two methods for identifying “theoretical” subsets of texts from large collections, using keyword lists and machine learning. The first two panel presentations will look at developing two different types of theoretical keyword lists. The third presentation will discuss a machine learning approach to extracting the same sorts of subsets.
                    </item>
                    <item>
                        <hi rend=""bold"">Topic Modelling: </hi>The second problem we tackled was what to do with such subsets, especially since they are likely to still be too large for conventional text analysis tools like Voyant (voyant-tools.org) and users will want to explore the results to understand what they got. The fourth panel presentation will therefore discuss how the HathiTrust Research Center (HTRC) adapted Topic Modelling tools to work on large collections to help exploring subsets. The fifth panel talk will then show an adapted visualization tool, the Galaxy Viewer, that allows one to explore the results of Topic Modelling. 
                    </item>
                </list>
                <p>The panel brings together a team of researchers who are part of the “Text Mining the Novel” (TMN) project that is funded by the Social Sciences and Humanities Research Council of Canada (SSHRC) and led by Andrew Piper at McGill University. Text Mining the Novel (novel-tm.ca) is a multi-year and multi-university cross-cultural study looking at the use of quantitative methods in the study of literature, with the HathiTrust Research Center is a project partner.</p>
                <p>The issue of how to extract thematic subsets from very large corpora such as the HathiTrust is a problem common to many projects that want to use diachronic collections to study the history of ideas or other phenomena. To conclude the panel, a summary reflective presentation will discuss the support the HTRC offers to DH researchers and how the HTRC notion of “worksets” can help with the challenges posed by creating useful subsets. It will further show how the techniques developed in this project can be used by the HTRC to help other future scholarly investigations.</p>
            </div>
            <div xml:id=""h.t9dpcs25grxn"" type=""div1"" rend=""DH-Heading1"">
                <head>Using Word Lists to Subset</head>
                <p>Geoffrey Rockwell (Kevin Schenk, Zachary Palmer, Robert Budac and Boris Capitanu)</p>
                <p>How can one extract subsets from a corpus without appropriate metadata? Extracting subsets is a problem particular to very large corpora like those kept by the HathiTrust (www.hathitrust.org/). Such collections are too large to be manually curated and their metadata is of limited use in many cases. And yet, one needs ways to classify all the texts in a collection in order to extract subsets if one wants to study particular themes, genres or types of works. In our case we wanted to extract theoretical works which for the purpose of this project we defined as Philosophical works or Literary Critical works. In this first panel presentation we will discuss the use of keyword lists as a way of identifying a subset of “philosophical” texts.</p>
                <p>Why philosophical? We choose to experiment extracting philosophical texts first as philosophy is a discipline with a long history and a vocabulary that we hypothesized would lend itself to a keyword approach. Unlike more recent theoretical traditions, philosophical words might allow us to extract works from the HathiTrust going back thousands of years.</p>
                <p>
                    <hi rend=""bold"">Keywords. </hi>For the first part of this project we adapted a list of philosophical keywords from the Indiana Philosophy Ontology Project (inpho.cogs.indiana.edu/). Our adapted list has 4,437 words and names starting with ""abauzit"", ""abbagnano"", ""abdolkarim"", ""abduction"", ""abduh"", ""abel"", and so on. There are a number of ways of generating such lists of keywords or features, in our case we were able to start with a very large curated list. The second paper in this panel discusses generating a list of literary critical keywords.
                </p>
                <p>
                    <hi rend=""bold"">Process. </hi>We used this list with a process we wrote in Python that calculates the relative frequency of each word in a text and does this over a collection. The process also calculates the sum of the relative frequencies giving us a simple measurement of the use of philosophical keywords in a text. The word frequency process generates a CSV with the titles, author, frequency sum and individual keyword frequencies which can be checked and manipulated in Excel.
                </p>
                <p>
                    <hi rend=""bold"">Testing.</hi> We iteratively tested this keyword approach on larger and larger collections. First we gathered a collection of 20 philosophical and 20 non-philosophical texts from Project Gutenberg (www.gutenberg.org/). We found the summed frequency accurately distinguished the philosophical from the non-philosophical texts. The process was then run by the HTRC on a larger collection of some 9,000 volumes and the results returned to us. We used the results to refine our list of keywords so that a summed relative frequency of .09 gave us mostly philosophical works with a few false positives. We did this by sorting the false positives by which words contributed to their summed relative frequency and then eliminating those words from the larger list that seemed to be ambiguous.
                </p>
                <p>The process was then run on the HathiTust Open Open collection of 254,000 volumes. This generated some 3230 volumes that had a summed relative frequency over .1, which seemed a safe cut-off point given how .09 had worked with a smaller collection. To assess the accuracy of this method we manually went through these 3,230 and categorized them using the titles producing a CSV that could be used with other classification methods.</p>
                <figure>
                    <graphic url=""622/image1.png"" rend=""inline""/>
                </figure>
                <p>The table below summarizes the categories of volumes that we found, though it should be noted that the categorization was based on the titles, which can be misleading. “Unsure” was for works which we weren’t sure about. “Not-Philosophical” were those works that we were reasonably sure were not philosophical from the title. The categories like Science and Education were for works about science and philosophy or education and philosophy.</p>
                <table rend=""rules"">
                    <row>
                        <cell>Tag (Type)</cell>
                        <cell>Number of Volumes</cell>
                        <cell>Example</cell>
                    </row>
                    <row>
                        <cell>Unsure</cell>
                        <cell>349</cell>
                        <cell>The coming revolution (1918)</cell>
                    </row>
                    <row>
                        <cell>Education</cell>
                        <cell>473</cell>
                        <cell>Education and national character (1904)</cell>
                    </row>
                    <row>
                        <cell>Philosophy</cell>
                        <cell>813</cell>
                        <cell>Outlines of metaphysics (1911)</cell>
                    </row>
                    <row>
                        <cell>Science</cell>
                        <cell>189</cell>
                        <cell>Relativity; a new view of the universe (1922)</cell>
                    </row>
                    <row>
                        <cell>Social</cell>
                        <cell>526</cell>
                        <cell>The study of history and sociology (1890)</cell>
                    </row>
                    <row>
                        <cell>Religion</cell>
                        <cell>722</cell>
                        <cell>Prolegomena to theism (1910)</cell>
                    </row>
                    <row>
                        <cell>Not Philosophical</cell>
                        <cell>158</cell>
                        <cell>Pennsylvania archives (1874)</cell>
                    </row>
                </table>
                <p>One of the things that stands out is the overlap between religious titles and philosophical ones. This is not surprising given that the fields have been intertwined for centuries and often treat of the same issues. We also note how many educational works and works dealing with society can have a philosophical bent. It was gratifying to find only 4.9% of the volumes classified seemed clearly not philosophical. If one includes the Unsure category it is 15.7%, but the Unsure category is in many ways the most interesting as one reason for classifying by computer is to find unexpected texts that challenge assumptions about what is theory.</p>
                <p>
                    <hi rend=""bold"">Conclusions</hi>. Using large keyword lists to classify texts is a conceptually simple method that can be understood and used by humanists. We have lists of words and names at hand in specialized dictionaries and existing classification systems. Lists can be managed to suit different purposes. Our list from InPhO had the advantage that is was large and inclusive, but also the disadvantage that included words like “being” and “affairs” that have philosophical uses but are also used in everyday prose. The same is true of the names gathered like Croce that can refer to the philosopher or the cross (in Italian). Further trimming and then weighting of words/names could improve the classification of strictly philosophical texts. We also need to look deeper into the results to find not just the false positives, but also the true negatives. In sum, this method has the virtue of simplicity and accessibility and in the case of philosophical texts can be used to extract useful, though not complete, subsets. 
                </p>
            </div>
            <div xml:id=""h.hts4jtuwsy3j"" type=""div1"" rend=""DH-Heading1"">
                <head>The Problem with Literary Theory</head>
                <p>Laura Mandell (Boris Capitanu, Stefan Sinclair, and Susan Brown)</p>
                <p>In this short paper, I describe adapting the word list approach developed by Geoffrey Rockwell for extracting a subset of philosophical texts from a large, undifferentiated corpus, to the task of identifying works of literary theory. The degree to which running the list of terms did in fact pull out and gather together works of literary criticism and theory is very high, despite potential problems with such an enterprise, which we discuss in this talk in detail.</p>
                <list type=""ordered"">
                    <item>
                        <hi rend=""bold"">Developing the list of literary terms</hi>. Susan Brown and I decided to gather lists of literary terms. Susan initiated a discussion with the MLA about using terms from the 
                        <hi rend=""italic"">MLA Bibliography</hi> but upon consideration these were in fact not at all what we needed: they classified subjects of texts as opposed to listing terms that would appear in those texts. I had recently spent some time learning about JSTOR’s new initiative in which sets of terms are created by what they call “SMEs”--Subject Matter Experts--and then used to locate articles all participating in an interdisciplinary subject. Their first foray is available in Beta: it gathers together all articles in no matter what field on the topic of Environmental Sustainabilty (labs.jstor.org/sustainability/). The terms collected are terms that would appear 
                        <hi rend=""italic"">in</hi> the relevant texts, not in the metadata about them; the goal is to collect documents across multiple categories related to specialization, discipline, and field, since the desired result to gather together interdisciplinary texts concerning a common topic. 
                    </item>
                    <item>
                        <hi rend=""bold"">Anachronism. </hi>JSTOR had started a “literary terms” list, and I finished the list of terms relying on encyclopedias of literary theory. Could a list of terms significant in the late-twentieth-century theories of literature as expressed in articles gathered in JSTOR be used to extract a set of texts published much earlier that analyze literature? What about the historical inaccuracy of using twentieth-century terms to find eighteenth- and nineteenth-century literary criticism?
                    </item>
                </list>
                <p>Results:</p>
                <figure>
                    <graphic url=""622/image2.png"" rend=""inline""/>
                </figure>
                <p>In fact, results show solidly that this anachronistic list of terms developed by experts do work to gather materials that preceded and fed into, served to develop, the discipline of literary theory. One of two falsely identified texts among the top relevant documents has to do with water distribution systems which had, as part of its most frequent terms, “meter” and “collection,” two terms relevant to analyzing the medium and content of poetry. Other false positives are similarly explicable, and, most important, they are rare.</p>
                <p>In this paper, we report upon the effects of running these frequent words on very large datasets using both unsupervised to supervised learning.</p>
            </div>
            <div xml:id=""h.r6cofa424xj8"" type=""div1"" rend=""DH-Heading1"">
                <head>Machine Learning</head>
                <p>Stefan Sinclair (and Matthew Wilkens)</p>
                <p>The third panel presentation deals with machine learning techniques to extract subsets. Unsupervised learning techniques allow us to evaluate the relative coherence of theoretical clusters within large textual fields and to identify distinct theoretical subclasses in the absence of any firmly established anatomy of the discipline. For these reasons, we performed unsupervised classification on three corpora: (1) A large collection (c. 250,000 volumes) of mixed fiction and nonfiction published in the nineteenth and twentieth centuries. (2) A subset of that corpus identified by algorithmic and manual methods as highly philosophical. And (3) A subset similarly identified as literary-critical.</p>
                <p>In the case of the large corpus, the goal was to identify subsets containing high proportions of philosophy and criticism. For the smaller sets, we sought to produce coherent groupings of texts that would resemble subfields or concentrations within those areas. In each case, we extracted textual features including word frequency distributions, formal and stylistic measures, and basic metadata information, then performed both 
                    <hi rend=""italic"">k</hi>-means and DBSCAN clustering on the derived Euclidean distances between volumes.
                </p>
                <p>As in past work on literary texts (Wilkens, 105), we found that we were able to identify highly distinct groups of texts, often those dealing with specialized and comparatively codified subdomains, and that we could subdivide larger fields with reasonable but lower accuracy. The model that emerges from this work, however, is one emphasizing continuity over clear distinction. Subfields and areas of intensely shared textual focus do exist, but a systematic view of large corpora in the philosophical and literary critical domains suggests a more fluid conception of knowledge space in the nineteenth and twentieth centuries.</p>
                <p>In parallel with the unsupervised classification performed – an attempt to allow distinctive features to emerge without, or with less, bias – we also performed supervised classification, starting with the training set of 40 texts labelled as Philosophical and Other (mentioned in ""Using Word Lists to Subset"" above). We experimented with several machine learning algorithms and several parameters to determine which ones seemed most suitable for our dataset. Indeed, part of this work was to recognize and and normalize the situation of the budding digital humanist confronting a dizzying array of choices: stoplists, keywords, relative frequencies, TF-IDF values, number of terms to use, Naïve Bayes Multinomial, 
                    <hi rend=""background(white)"">Linear Support Vector Classification, penalty parameter, iterations, and so on ad infinitum. Some testing is desirable; some guesswork and some craftwork are essential. We reflect on these tensions more in the iPython notebook (Sinclair et al., 2016) and we will discuss them during the presentation as well.</hi>
                </p>
                <p>One of the surprises from these initial experiments in machine learning was that using an unbiased list of terms from the full corpus (with stopwords removed) was considerably more effective than attempting to classify using the constrained philosophical vocabulary. Again, this may be because the keywords list was overly greedy.</p>
                <p>
                    <hi rend=""background(white)"">Just as we experimented with ever-larger corpora for the ""Using Lists to Subset"" sub-project, the supervised learning subproject broadened its scope gradually in an attempt to identify theoretical texts unknown to us while examining the efficacy of the methodologies along the way. Indeed, the overarching purpose of adopting all three approaches (keyword-based, unsupervised classification, machine learning) was to compare and contrast different ways of studying theory in a large-scale corpus.</hi>
                </p>
            </div>
            <div xml:id=""h.yxbg2h3ogjn8"" type=""div1"" rend=""DH-Heading1"">
                <head>Working with HTRC datasets</head>
                <p>Boris Capitanu</p>
                <p>The fourth panel presentation focuses on working with the HathiTrust and the particular format of HathiTrust texts. Researchers may obtain datasets directly from HathiTrust [1] by making a special request, after having fulfilled appropriate security and licensing requirements. Datasets in HathiTrust and HTRC are available in two different ways:</p>
                <list type=""unordered"">
                    <item>via rsync in Pairtree format</item>
                    <item>via Data API</item>
                </list>
                <p>According to “Pairtrees for Object Storage (V0.1)” [2], the Pairtree is ""a filesystem hierarchy for holding objects that are located within that hierarchy by mapping identifier strings to object directory (or folder) paths, two characters at a time”. In the HathiTrust, the objects consist of the individual volume and associated metadata. Volumes are stored as ZIP files containing text files, one text file for each page, where the text file is named by the page number. A volume ZIP file may contain additional non-page text files, whose purpose can be identified from the file name. The metadata for the volume is encoded in METS XML [3] and lives in a file next to the volume ZIP file. For example, a volume with id “loc.ark:/13960/t8pc38p4b” is stored in Pairtree as:</p>
                <p>loc/pairtree_root/ar/k+/=1/39/60/=t/8p/c3/8p/4b/ark+=13960=t8pc38p4b/ark+=13960=t8pc38p4b.zip loc/pairtree_root/ar/k+/=1/39/60/=t/8p/c3/8p/4b/ark+=13960=t8pc38p4b/ark+=13960=t8pc38p4b.mets.xml</p>
                <p>where “loc” represents the 3-letter code of the library of origin (in this case Library of Congress). As mentioned, the volume ZIP files contain text files named for the page number. For example, here are the first few entries when listing the contents of the above ZIP file:</p>
                <p> ark+=13960=t8pc38p4b/</p>
                <p> ark+=13960=t8pc38p4b/00000001.txt</p>
                <p> ark+=13960=t8pc38p4b/00000002.txt</p>
                <p> ark+=13960=t8pc38p4b/00000003.txt</p>
                <p> …</p>
                <p>Note that the strings that encode the volume id and the ZIP filename are different. Before a volume id can be encoded as a file name, it goes through a “cleaning” process that converts any character that is not a valid character to be used in a filename into one that is (for example “:” was converted to “+” and “/” to “=“), also dropping the 3-letter library code. The specific conversion rules are obscure, but library code already exists [4][5] for multiple languages that is able to perform this conversion both ways.</p>
                <p>The pairtree is an efficient structure for storing a large number of files. However, working with this structure can pose certain challenges. One of the issues is that this deeply nested folder hierarchy is slow to traverse. Applications needing to recursively process the volumes in a particular dataset stored in pairtree will have to traverse a large number of folders to “discover” every volume. A second inconvenience stems from the use of ZIP to store the content of a volume. While efficient in terms of disk space usage, it’s inconvenient when applications need to process the text data of the volume as they would need to uncompress the ZIP file and read its contents, in the proper order, concatenating all pages, in order to obtain the entire volume text content. A further complication is due to the fact that the exact ordering and naming of the page text files in the ZIP file is only provided as part of the METS XML metadata file. So, if the goal is to create a large blob of text containing all the pages of a volume (and only the pages, in the proper order, without any additional non-page data), the most correct way of doing so is to first parse the METS XML to determine the page sequence and file names, and then uncompress the ZIP file concatenating the pages in the exact sequence specified. This, of course, has a large performance penalty if it needs to be done on a large dataset every time this dataset is used to address some research question.</p>
                <p>An alternative way to obtain a particular dataset is to use the Data API [6]. Currently, access to Data API is limited, and is allowed only from the Data Capsule [7] while in Secure Mode. Using the Data API a researcher can retrieve multiple volumes, pages of volumes, token counts, and METS metadata documents. Authentication via the OAuth protocol is required when making requests to the Data API. The advantage of using the Data API in place of the pairtree (other than disk storage savings) is that one can request already-concatenated text blobs for volumes, and make more granular requests for token counts or page ranges without having to traverse deeply-nested folder structures or parse METS metadata.</p>
                <p>In this panel presentation we will show how the tools developed for the Trace of Theory project were adapted to work with the Pairtree format. The goal is to help others be able to work with the HathiTrust data format.</p>
                <p>Notes:</p>
                <p>[1] https://www.hathitrust.org/datasets</p>
                <p>[2] http://tools.ietf.org/html/draft-kunze-pairtree-01</p>
                <p>[3] http://www.loc.gov/standards/mets/</p>
                <p>[4] https://confluence.ucop.edu/display/Curation/PairTree</p>
                <p>[5] https://github.com/htrc/HTRC-Tools-PairtreeHelper</p>
                <p>[6] https://wiki.htrc.illinois.edu/display/COM/HTRC+Data+API+Users+Guide</p>
                <p>[7] https://wiki.htrc.illinois.edu/display/COM/HTRC+Data+Capsule</p>
            </div>
            <div xml:id=""h.f2bgagc46fwp"" type=""div1"" rend=""DH-Heading1"">
                <head>Topic Modelling and Visualization for Exploration</head>
                <p>Susan Brown (Geoffrey Rockwell, Boris Capitanu, Ryan Chartier, and John Montague)</p>
                <p>When working with very large collections even subsets can be too large to manage with conventional text analysis tools. Further, one needs ways of exploring the results of extraction techniques to figure out if you got what you were expecting or something surprising in an interesting way. In the fifth panel presentation we will discuss the adaptation of a tool called the Galaxy Viewer for visualizing the results of Topic Modelling (Montague et al., 2015). Topic modeling is an automated text mining technique that has proven popular in the humanities that tries to identify groups of words with a tendency to occur together within the same documents in a corpus. Chaney and Blei explain that, “One of the main applications of topic models is for exploratory data analysis, that is, to help browse, understand, and summarize otherwise unstructured collections.” (Chaney et al., 2012)</p>
                <figure>
                    <graphic url=""622/image3.png"" rend=""inline""/>
                </figure>
                <p>The Galaxy Viewer prototype was developed to explore the results of topic modelling over large collections. It combines different views so that one can select topics, compare topics, explore the words in topics, follow topic tokens over time, and see the document titles associated with topics. In this presentation we will demonstrate the Galaxy Viewer and then discuss how it was scaled to handle much larger collections.</p>
                <p>The prototype Galaxy Viewer backend code uses Mallet (McCallum, 2002) to infer the set of topics, topic distributions per document, and word probabilities per topic. Unfortunately, Mallet is meant to be used on small- to medium-sized corpora as it requires that the entire dataset be loaded into RAM during training. An additional constraint with Mallet is the fact that although Mallet can fully utilize all the CPU cores on a single machine, it’s not designed to work in a distributed-computing fashion across a number of machines, to speed up execution. As such, processing very large datasets (if even possible) might take a very long time (as the algorithm makes multiple passes over the entire dataset). Many implementations of LDA exist, which primarily fall into one of two categories: Batch LDA, or Online LDA. The core difference between batch and online LDA stems from what happens during each iteration of the algorithm. In batch mode, as mentioned earlier, each iteration of the algorithm makes a full pass over all the documents in the dataset in order to re-estimate the parameters, checking each time for convergence. In contrast, online LDA only makes a single sweep over the dataset, analyzing a subset of the documents each iteration. The memory requirement for online LDA depends on the chosen batch size only, not on the size of the dataset - as is the case with batch LDA.</p>
                <p>We are currently in the process of researching/comparing the available implementations of LDA to establish which one would be best suited to use for the Galaxy Viewer. We are also considering the option of not fixing the LDA implementation, but instead make the backend flexible so that any LDA implementation can be used (as long as it provides the appropriate results that are needed). In the latter case we’d have to create specific result interpreters that can translate the output from the specific implementation of LDA to the appropriate format to be used to store in the database (to be served by the web service).</p>
                <p>Given that Topic Modeling results do not expose the textual content of the documents analyzed, and cannot be used to reconstruct the original text, they are safe to be publicly shared without fear of violating copyright law. This is great news for researchers working with collections like those of the HathiTrust as they should be able to gain insight into datasets which are still currently in-copyright and would, otherwise, not be available to be inspected freely.</p>
                <p>In the prototype Galaxy Viewer implementation, the output of the topic modeling step is processed through a set of R functions that reshape the data and augment it with additional calculated metrics that are used by the web frontend to construct the visualization. These post-processing results are saved to the filesystem as a set of five CSV files. One of these CSV files is quite large as it contains the topic modeling state data from Mallet (containing topic assignments for each document and word, and associated frequency count). The visual web frontend code loads this set of five files into memory when the interface is accessed the first time, which can take several minutes. For the prototype this approach was tolerated, but it has serious scalability and performance issue that needs to be addressed before the tool can be truly usable by other researchers.</p>
                <p>Scaling the Galaxy Viewer therefore consists of creating a web service backed with a (NoSQL) database which will service AJAX requests from the front-end for the data needed to construct the topic visualization and related graphs. We are developing the set of service calls that need to be implemented/exposed by the web service to fulfill the needs of the front-end web-app. The backend service will query the database to retrieve the necessary data to service the requests. The database will be created based on the output of the Topic Modeling process, after required post-processing of the results is completed (to calculate the topic trends, topic distances, and other metrics used in the display). Relevant metadata at the volume and dataset level will also be stored to be made available to the front-end upon request. This work will be completed by the end of December 2015 so that it can be demonstrated in the new year. The scaled Galaxy Viewer will then provide a non-consumptive way of allowing users of the HathiTrust to explore the copyrighted collections. Extraction of subsets and Topic Modelling can take place under the supervision of the HTRC and the results database can then be exposed to visualization tools like the Galaxy Viewer (and others) for exploration.</p>
            </div>
            <div xml:id=""h.izhdfmsd0qww"" type=""div1"" rend=""DH-Heading1"">
                <head>Closing reflections: How “Trace of Theory” will improve the HTRC
                    <hi rend=""italic"">. </hi>
                </head>
                <p>J. Stephen Downie</p>
                <p>The HathiTrust Research Center exists to give the Digital Humanities community analytic access to the HathiTrust’s 13.7 million volumes. The HT volumes comprise over 4.8 billion pages each in turn represented by a high-resolution image file and two OCR files yielding some 14.4 billion data files! Thus, as the earlier papers have highlighted, the sheer size of the collection, along with the idiosyncratic nature of the HT data, together create several hurdles that impede meaningful analytic research.The HTRC is engaged in two ongoing endeavours designed to assist DH researchers in overcoming these obstacles: The Advance Collaborative Support (ACS) program [1]; and, the Workset Creation for Scholarly Analysis (WCSA) project [2]. </p>
                <p>The ACS program at HTRC provides no-cost senior developer time, data wrangling assistance, computation time and analytic consultations to DH researchers who are prototyping new research ideas using the HT data resources. The ACS program is an integral part of the HTRC’s operation mission and was part of its value-added proposition when the HTRC launched its recent four-year operations plan (2014-2018). It is a fundamental component of the HTRC’s outreach activities and as such, has staff dedicated to its planning, management and day-to-delivery. The ACS team was responsible for creating, and then reviewing, the competitive ACS Request for Proposals (RFP) that ask interested DH researchers outline their intellectual goals, describe their data needs, and estimate their computational requirements. The ACS team is generally looking for new projects that could benefit from some kickstarting help from HTRC. HTRC welcomes proposals from researchers with a wide range of experience and skills. Projects run 6 to 12 months.</p>
                <p>Originally funded by the Andrew W. Mellon Foundation (2013-2015), the current WCSA program is building upon, extending and implementing the development made during the funding period. The HTRC project team, along with subaward collaborators at University of Oxford, University of Maryland, Texas Agriculture and Marine University and University of Waikato, developed a group of prototype techniques for empowering scholars who want to do computational analyses of the HT materials to more efficiently and effectively create user-specific analytic subsets (called “worksets”). A formal model has been designed to describe the items in a workset along with necessary bibliographic and provenance metadata that is now being incorporated into the HTRC infrastructure (Jett, 2015). </p>
                <p>The Trace of Theory project was selected from the first round of ACS proposals. This concluding panel presentation will discuss in what ways the Trace of Theory project has been both a representative and a unique exemplar of the ACS program. It will present some emergent themes that evolved from the HTRC-Trace of Theory interactions that we believe will have an important influence on the delivery of future ACS projects. In the same manner, it will reflect upon the problems the team of researchers had in subsetting the data to build their necessary worksets along with the solutions that the HRTC-Trace of Theory collaboration developed to surmount those difficulties. The panel will finish with a summary of how HTRC intends to incorporate the lessons learned into its day-to-day operations as well as future ACS projects. </p>
                <p>Notes:</p>
                <p>[1] The 2014 ACS RFP is available at: https://www.hathitrust.org/htrc/acs-rfp</p>
                <p>[2] https://www.lis.illinois.edu/research/projects/workset-creation-scholarly-analysis-prototyping-project</p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Chaney, A. J. and Blei, D. M.</hi> (2012). <hi rend=""italic"">Visualizing Topic Models</hi>, ICWSM. 
                        <ref target=""http://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/download/4645%26lt%3B/5021"">http://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/download/4645%26lt%3B/5021</ref> (accessed Dec 2015).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Jett, J.</hi> (2015).<hi rend=""italic""> Modeling worksets in the HathiTrust Research Center</hi>. CIRSS Technical Report WCSA0715. Champaign, IL: University of Illinois at Urbana-Champaign. Available via: http://hdl.handle.net/2142/78149 (accessed Dec 2015)
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">McCallum, A. K.</hi> (2002). <hi rend=""italic"">MALLET: A Machine Learning for Language Toolkit</hi>, http://mallet.cs.umass.edu (accessed Dec 2015).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Montague, J., Simpson, J., Brown, S., Rockwell, G. and Ruecker, S.</hi> (2015). Exploring Large Datasets with Topic Model Visualization. <hi rend=""italic"">Paper presented by Montague at DH 2015 at the University of Western Sydney</hi>, Australia.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sinclair, S., G. Rockwell and the Trace of Theory Team</hi>. (2016). <hi rend=""bold"">Classifying Philosophical Texts</hi>. Online at 
                        <ref target=""http://bit.ly/1kHBy56"">http://bit.ly/1kHBy56</ref> (accessed Dec 2015). 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Wilkens, M.</hi> (2016). Genre, Computation, and the Weird Canonicity of Recently Dead White Men. <hi rend=""italic"">NovelTM Working Paper</hi>.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,hathitrust;machine learning;very large collections,English,data mining / text mining;software design and development;text analysis,2016-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <div type=""div1"" rend=""dh-heading1"">
                <head>introduction</head>
                <p>can we find and track theory, especially literary theory, in very large collections of texts using computers? this panel discusses a pragmatic two-step approach to trying to track and then visually explore theory through its textual traces in large collections like those of the hathitrust.</p>
                <list type=""ordered"">
                    <item>
                        <hi rend=""bold"">subsetting: </hi>the first problem we will discuss is how to extract thematic subsets of texts from very large collections like those of the hathitrust. we experimented with two methods for identifying “theoretical” subsets of texts from large collections, using keyword lists and machine learning. the first two panel presentations will look at developing two different types of theoretical keyword lists. the third presentation will discuss a machine learning approach to extracting the same sorts of subsets.
                    </item>
                    <item>
                        <hi rend=""bold"">topic modelling: </hi>the second problem we tackled was what to do with such subsets, especially since they are likely to still be too large for conventional text analysis tools like voyant (voyant-tools.org) and users will want to explore the results to understand what they got. the fourth panel presentation will therefore discuss how the hathitrust research center (htrc) adapted topic modelling tools to work on large collections to help exploring subsets. the fifth panel talk will then show an adapted visualization tool, the galaxy viewer, that allows one to explore the results of topic modelling. 
                    </item>
                </list>
                <p>the panel brings together a team of researchers who are part of the “text mining the novel” (tmn) project that is funded by the social sciences and humanities research council of canada (sshrc) and led by andrew piper at mcgill university. text mining the novel (novel-tm.ca) is a multi-year and multi-university cross-cultural study looking at the use of quantitative methods in the study of literature, with the hathitrust research center is a project partner.</p>
                <p>the issue of how to extract thematic subsets from very large corpora such as the hathitrust is a problem common to many projects that want to use diachronic collections to study the history of ideas or other phenomena. to conclude the panel, a summary reflective presentation will discuss the support the htrc offers to dh researchers and how the htrc notion of “worksets” can help with the challenges posed by creating useful subsets. it will further show how the techniques developed in this project can be used by the htrc to help other future scholarly investigations.</p>
            </div>
            <div xml:id=""h.t9dpcs25grxn"" type=""div1"" rend=""dh-heading1"">
                <head>using word lists to subset</head>
                <p>geoffrey rockwell (kevin schenk, zachary palmer, robert budac and boris capitanu)</p>
                <p>how can one extract subsets from a corpus without appropriate metadata? extracting subsets is a problem particular to very large corpora like those kept by the hathitrust (www.hathitrust.org/). such collections are too large to be manually curated and their metadata is of limited use in many cases. and yet, one needs ways to classify all the texts in a collection in order to extract subsets if one wants to study particular themes, genres or types of works. in our case we wanted to extract theoretical works which for the purpose of this project we defined as philosophical works or literary critical works. in this first panel presentation we will discuss the use of keyword lists as a way of identifying a subset of “philosophical” texts.</p>
                <p>why philosophical? we choose to experiment extracting philosophical texts first as philosophy is a discipline with a long history and a vocabulary that we hypothesized would lend itself to a keyword approach. unlike more recent theoretical traditions, philosophical words might allow us to extract works from the hathitrust going back thousands of years.</p>
                <p>
                    <hi rend=""bold"">keywords. </hi>for the first part of this project we adapted a list of philosophical keywords from the indiana philosophy ontology project (inpho.cogs.indiana.edu/). our adapted list has 4,437 words and names starting with ""abauzit"", ""abbagnano"", ""abdolkarim"", ""abduction"", ""abduh"", ""abel"", and so on. there are a number of ways of generating such lists of keywords or features, in our case we were able to start with a very large curated list. the second paper in this panel discusses generating a list of literary critical keywords.
                </p>
                <p>
                    <hi rend=""bold"">process. </hi>we used this list with a process we wrote in python that calculates the relative frequency of each word in a text and does this over a collection. the process also calculates the sum of the relative frequencies giving us a simple measurement of the use of philosophical keywords in a text. the word frequency process generates a csv with the titles, author, frequency sum and individual keyword frequencies which can be checked and manipulated in excel.
                </p>
                <p>
                    <hi rend=""bold"">testing.</hi> we iteratively tested this keyword approach on larger and larger collections. first we gathered a collection of 20 philosophical and 20 non-philosophical texts from project gutenberg (www.gutenberg.org/). we found the summed frequency accurately distinguished the philosophical from the non-philosophical texts. the process was then run by the htrc on a larger collection of some 9,000 volumes and the results returned to us. we used the results to refine our list of keywords so that a summed relative frequency of .09 gave us mostly philosophical works with a few false positives. we did this by sorting the false positives by which words contributed to their summed relative frequency and then eliminating those words from the larger list that seemed to be ambiguous.
                </p>
                <p>the process was then run on the hathitust open open collection of 254,000 volumes. this generated some 3230 volumes that had a summed relative frequency over .1, which seemed a safe cut-off point given how .09 had worked with a smaller collection. to assess the accuracy of this method we manually went through these 3,230 and categorized them using the titles producing a csv that could be used with other classification methods.</p>
                <figure>
                    <graphic url=""622/image1.png"" rend=""inline""/>
                </figure>
                <p>the table below summarizes the categories of volumes that we found, though it should be noted that the categorization was based on the titles, which can be misleading. “unsure” was for works which we weren’t sure about. “not-philosophical” were those works that we were reasonably sure were not philosophical from the title. the categories like science and education were for works about science and philosophy or education and philosophy.</p>
                <table rend=""rules"">
                    <row>
                        <cell>tag (type)</cell>
                        <cell>number of volumes</cell>
                        <cell>example</cell>
                    </row>
                    <row>
                        <cell>unsure</cell>
                        <cell>349</cell>
                        <cell>the coming revolution (1918)</cell>
                    </row>
                    <row>
                        <cell>education</cell>
                        <cell>473</cell>
                        <cell>education and national character (1904)</cell>
                    </row>
                    <row>
                        <cell>philosophy</cell>
                        <cell>813</cell>
                        <cell>outlines of metaphysics (1911)</cell>
                    </row>
                    <row>
                        <cell>science</cell>
                        <cell>189</cell>
                        <cell>relativity; a new view of the universe (1922)</cell>
                    </row>
                    <row>
                        <cell>social</cell>
                        <cell>526</cell>
                        <cell>the study of history and sociology (1890)</cell>
                    </row>
                    <row>
                        <cell>religion</cell>
                        <cell>722</cell>
                        <cell>prolegomena to theism (1910)</cell>
                    </row>
                    <row>
                        <cell>not philosophical</cell>
                        <cell>158</cell>
                        <cell>pennsylvania archives (1874)</cell>
                    </row>
                </table>
                <p>one of the things that stands out is the overlap between religious titles and philosophical ones. this is not surprising given that the fields have been intertwined for centuries and often treat of the same issues. we also note how many educational works and works dealing with society can have a philosophical bent. it was gratifying to find only 4.9% of the volumes classified seemed clearly not philosophical. if one includes the unsure category it is 15.7%, but the unsure category is in many ways the most interesting as one reason for classifying by computer is to find unexpected texts that challenge assumptions about what is theory.</p>
                <p>
                    <hi rend=""bold"">conclusions</hi>. using large keyword lists to classify texts is a conceptually simple method that can be understood and used by humanists. we have lists of words and names at hand in specialized dictionaries and existing classification systems. lists can be managed to suit different purposes. our list from inpho had the advantage that is was large and inclusive, but also the disadvantage that included words like “being” and “affairs” that have philosophical uses but are also used in everyday prose. the same is true of the names gathered like croce that can refer to the philosopher or the cross (in italian). further trimming and then weighting of words/names could improve the classification of strictly philosophical texts. we also need to look deeper into the results to find not just the false positives, but also the true negatives. in sum, this method has the virtue of simplicity and accessibility and in the case of philosophical texts can be used to extract useful, though not complete, subsets. 
                </p>
            </div>
            <div xml:id=""h.hts4jtuwsy3j"" type=""div1"" rend=""dh-heading1"">
                <head>the problem with literary theory</head>
                <p>laura mandell (boris capitanu, stefan sinclair, and susan brown)</p>
                <p>in this short paper, i describe adapting the word list approach developed by geoffrey rockwell for extracting a subset of philosophical texts from a large, undifferentiated corpus, to the task of identifying works of literary theory. the degree to which running the list of terms did in fact pull out and gather together works of literary criticism and theory is very high, despite potential problems with such an enterprise, which we discuss in this talk in detail.</p>
                <list type=""ordered"">
                    <item>
                        <hi rend=""bold"">developing the list of literary terms</hi>. susan brown and i decided to gather lists of literary terms. susan initiated a discussion with the mla about using terms from the 
                        <hi rend=""italic"">mla bibliography</hi> but upon consideration these were in fact not at all what we needed: they classified subjects of texts as opposed to listing terms that would appear in those texts. i had recently spent some time learning about jstor’s new initiative in which sets of terms are created by what they call “smes”--subject matter experts--and then used to locate articles all participating in an interdisciplinary subject. their first foray is available in beta: it gathers together all articles in no matter what field on the topic of environmental sustainabilty (labs.jstor.org/sustainability/). the terms collected are terms that would appear 
                        <hi rend=""italic"">in</hi> the relevant texts, not in the metadata about them; the goal is to collect documents across multiple categories related to specialization, discipline, and field, since the desired result to gather together interdisciplinary texts concerning a common topic. 
                    </item>
                    <item>
                        <hi rend=""bold"">anachronism. </hi>jstor had started a “literary terms” list, and i finished the list of terms relying on encyclopedias of literary theory. could a list of terms significant in the late-twentieth-century theories of literature as expressed in articles gathered in jstor be used to extract a set of texts published much earlier that analyze literature? what about the historical inaccuracy of using twentieth-century terms to find eighteenth- and nineteenth-century literary criticism?
                    </item>
                </list>
                <p>results:</p>
                <figure>
                    <graphic url=""622/image2.png"" rend=""inline""/>
                </figure>
                <p>in fact, results show solidly that this anachronistic list of terms developed by experts do work to gather materials that preceded and fed into, served to develop, the discipline of literary theory. one of two falsely identified texts among the top relevant documents has to do with water distribution systems which had, as part of its most frequent terms, “meter” and “collection,” two terms relevant to analyzing the medium and content of poetry. other false positives are similarly explicable, and, most important, they are rare.</p>
                <p>in this paper, we report upon the effects of running these frequent words on very large datasets using both unsupervised to supervised learning.</p>
            </div>
            <div xml:id=""h.r6cofa424xj8"" type=""div1"" rend=""dh-heading1"">
                <head>machine learning</head>
                <p>stefan sinclair (and matthew wilkens)</p>
                <p>the third panel presentation deals with machine learning techniques to extract subsets. unsupervised learning techniques allow us to evaluate the relative coherence of theoretical clusters within large textual fields and to identify distinct theoretical subclasses in the absence of any firmly established anatomy of the discipline. for these reasons, we performed unsupervised classification on three corpora: (1) a large collection (c. 250,000 volumes) of mixed fiction and nonfiction published in the nineteenth and twentieth centuries. (2) a subset of that corpus identified by algorithmic and manual methods as highly philosophical. and (3) a subset similarly identified as literary-critical.</p>
                <p>in the case of the large corpus, the goal was to identify subsets containing high proportions of philosophy and criticism. for the smaller sets, we sought to produce coherent groupings of texts that would resemble subfields or concentrations within those areas. in each case, we extracted textual features including word frequency distributions, formal and stylistic measures, and basic metadata information, then performed both 
                    <hi rend=""italic"">k</hi>-means and dbscan clustering on the derived euclidean distances between volumes.
                </p>
                <p>as in past work on literary texts (wilkens, 105), we found that we were able to identify highly distinct groups of texts, often those dealing with specialized and comparatively codified subdomains, and that we could subdivide larger fields with reasonable but lower accuracy. the model that emerges from this work, however, is one emphasizing continuity over clear distinction. subfields and areas of intensely shared textual focus do exist, but a systematic view of large corpora in the philosophical and literary critical domains suggests a more fluid conception of knowledge space in the nineteenth and twentieth centuries.</p>
                <p>in parallel with the unsupervised classification performed – an attempt to allow distinctive features to emerge without, or with less, bias – we also performed supervised classification, starting with the training set of 40 texts labelled as philosophical and other (mentioned in ""using word lists to subset"" above). we experimented with several machine learning algorithms and several parameters to determine which ones seemed most suitable for our dataset. indeed, part of this work was to recognize and and normalize the situation of the budding digital humanist confronting a dizzying array of choices: stoplists, keywords, relative frequencies, tf-idf values, number of terms to use, naïve bayes multinomial, 
                    <hi rend=""background(white)"">linear support vector classification, penalty parameter, iterations, and so on ad infinitum. some testing is desirable; some guesswork and some craftwork are essential. we reflect on these tensions more in the ipython notebook (sinclair et al., 2016) and we will discuss them during the presentation as well.</hi>
                </p>
                <p>one of the surprises from these initial experiments in machine learning was that using an unbiased list of terms from the full corpus (with stopwords removed) was considerably more effective than attempting to classify using the constrained philosophical vocabulary. again, this may be because the keywords list was overly greedy.</p>
                <p>
                    <hi rend=""background(white)"">just as we experimented with ever-larger corpora for the ""using lists to subset"" sub-project, the supervised learning subproject broadened its scope gradually in an attempt to identify theoretical texts unknown to us while examining the efficacy of the methodologies along the way. indeed, the overarching purpose of adopting all three approaches (keyword-based, unsupervised classification, machine learning) was to compare and contrast different ways of studying theory in a large-scale corpus.</hi>
                </p>
            </div>
            <div xml:id=""h.yxbg2h3ogjn8"" type=""div1"" rend=""dh-heading1"">
                <head>working with htrc datasets</head>
                <p>boris capitanu</p>
                <p>the fourth panel presentation focuses on working with the hathitrust and the particular format of hathitrust texts. researchers may obtain datasets directly from hathitrust [1] by making a special request, after having fulfilled appropriate security and licensing requirements. datasets in hathitrust and htrc are available in two different ways:</p>
                <list type=""unordered"">
                    <item>via rsync in pairtree format</item>
                    <item>via data api</item>
                </list>
                <p>according to “pairtrees for object storage (v0.1)” [2], the pairtree is ""a filesystem hierarchy for holding objects that are located within that hierarchy by mapping identifier strings to object directory (or folder) paths, two characters at a time”. in the hathitrust, the objects consist of the individual volume and associated metadata. volumes are stored as zip files containing text files, one text file for each page, where the text file is named by the page number. a volume zip file may contain additional non-page text files, whose purpose can be identified from the file name. the metadata for the volume is encoded in mets xml [3] and lives in a file next to the volume zip file. for example, a volume with id “loc.ark:/13960/t8pc38p4b” is stored in pairtree as:</p>
                <p>loc/pairtree_root/ar/k+/=1/39/60/=t/8p/c3/8p/4b/ark+=13960=t8pc38p4b/ark+=13960=t8pc38p4b.zip loc/pairtree_root/ar/k+/=1/39/60/=t/8p/c3/8p/4b/ark+=13960=t8pc38p4b/ark+=13960=t8pc38p4b.mets.xml</p>
                <p>where “loc” represents the 3-letter code of the library of origin (in this case library of congress). as mentioned, the volume zip files contain text files named for the page number. for example, here are the first few entries when listing the contents of the above zip file:</p>
                <p> ark+=13960=t8pc38p4b/</p>
                <p> ark+=13960=t8pc38p4b/00000001.txt</p>
                <p> ark+=13960=t8pc38p4b/00000002.txt</p>
                <p> ark+=13960=t8pc38p4b/00000003.txt</p>
                <p> …</p>
                <p>note that the strings that encode the volume id and the zip filename are different. before a volume id can be encoded as a file name, it goes through a “cleaning” process that converts any character that is not a valid character to be used in a filename into one that is (for example “:” was converted to “+” and “/” to “=“), also dropping the 3-letter library code. the specific conversion rules are obscure, but library code already exists [4][5] for multiple languages that is able to perform this conversion both ways.</p>
                <p>the pairtree is an efficient structure for storing a large number of files. however, working with this structure can pose certain challenges. one of the issues is that this deeply nested folder hierarchy is slow to traverse. applications needing to recursively process the volumes in a particular dataset stored in pairtree will have to traverse a large number of folders to “discover” every volume. a second inconvenience stems from the use of zip to store the content of a volume. while efficient in terms of disk space usage, it’s inconvenient when applications need to process the text data of the volume as they would need to uncompress the zip file and read its contents, in the proper order, concatenating all pages, in order to obtain the entire volume text content. a further complication is due to the fact that the exact ordering and naming of the page text files in the zip file is only provided as part of the mets xml metadata file. so, if the goal is to create a large blob of text containing all the pages of a volume (and only the pages, in the proper order, without any additional non-page data), the most correct way of doing so is to first parse the mets xml to determine the page sequence and file names, and then uncompress the zip file concatenating the pages in the exact sequence specified. this, of course, has a large performance penalty if it needs to be done on a large dataset every time this dataset is used to address some research question.</p>
                <p>an alternative way to obtain a particular dataset is to use the data api [6]. currently, access to data api is limited, and is allowed only from the data capsule [7] while in secure mode. using the data api a researcher can retrieve multiple volumes, pages of volumes, token counts, and mets metadata documents. authentication via the oauth protocol is required when making requests to the data api. the advantage of using the data api in place of the pairtree (other than disk storage savings) is that one can request already-concatenated text blobs for volumes, and make more granular requests for token counts or page ranges without having to traverse deeply-nested folder structures or parse mets metadata.</p>
                <p>in this panel presentation we will show how the tools developed for the trace of theory project were adapted to work with the pairtree format. the goal is to help others be able to work with the hathitrust data format.</p>
                <p>notes:</p>
                <p>[1] https://www.hathitrust.org/datasets</p>
                <p>[2] http://tools.ietf.org/html/draft-kunze-pairtree-01</p>
                <p>[3] http://www.loc.gov/standards/mets/</p>
                <p>[4] https://confluence.ucop.edu/display/curation/pairtree</p>
                <p>[5] https://github.com/htrc/htrc-tools-pairtreehelper</p>
                <p>[6] https://wiki.htrc.illinois.edu/display/com/htrc+data+api+users+guide</p>
                <p>[7] https://wiki.htrc.illinois.edu/display/com/htrc+data+capsule</p>
            </div>
            <div xml:id=""h.f2bgagc46fwp"" type=""div1"" rend=""dh-heading1"">
                <head>topic modelling and visualization for exploration</head>
                <p>susan brown (geoffrey rockwell, boris capitanu, ryan chartier, and john montague)</p>
                <p>when working with very large collections even subsets can be too large to manage with conventional text analysis tools. further, one needs ways of exploring the results of extraction techniques to figure out if you got what you were expecting or something surprising in an interesting way. in the fifth panel presentation we will discuss the adaptation of a tool called the galaxy viewer for visualizing the results of topic modelling (montague et al., 2015). topic modeling is an automated text mining technique that has proven popular in the humanities that tries to identify groups of words with a tendency to occur together within the same documents in a corpus. chaney and blei explain that, “one of the main applications of topic models is for exploratory data analysis, that is, to help browse, understand, and summarize otherwise unstructured collections.” (chaney et al., 2012)</p>
                <figure>
                    <graphic url=""622/image3.png"" rend=""inline""/>
                </figure>
                <p>the galaxy viewer prototype was developed to explore the results of topic modelling over large collections. it combines different views so that one can select topics, compare topics, explore the words in topics, follow topic tokens over time, and see the document titles associated with topics. in this presentation we will demonstrate the galaxy viewer and then discuss how it was scaled to handle much larger collections.</p>
                <p>the prototype galaxy viewer backend code uses mallet (mccallum, 2002) to infer the set of topics, topic distributions per document, and word probabilities per topic. unfortunately, mallet is meant to be used on small- to medium-sized corpora as it requires that the entire dataset be loaded into ram during training. an additional constraint with mallet is the fact that although mallet can fully utilize all the cpu cores on a single machine, it’s not designed to work in a distributed-computing fashion across a number of machines, to speed up execution. as such, processing very large datasets (if even possible) might take a very long time (as the algorithm makes multiple passes over the entire dataset). many implementations of lda exist, which primarily fall into one of two categories: batch lda, or online lda. the core difference between batch and online lda stems from what happens during each iteration of the algorithm. in batch mode, as mentioned earlier, each iteration of the algorithm makes a full pass over all the documents in the dataset in order to re-estimate the parameters, checking each time for convergence. in contrast, online lda only makes a single sweep over the dataset, analyzing a subset of the documents each iteration. the memory requirement for online lda depends on the chosen batch size only, not on the size of the dataset - as is the case with batch lda.</p>
                <p>we are currently in the process of researching/comparing the available implementations of lda to establish which one would be best suited to use for the galaxy viewer. we are also considering the option of not fixing the lda implementation, but instead make the backend flexible so that any lda implementation can be used (as long as it provides the appropriate results that are needed). in the latter case we’d have to create specific result interpreters that can translate the output from the specific implementation of lda to the appropriate format to be used to store in the database (to be served by the web service).</p>
                <p>given that topic modeling results do not expose the textual content of the documents analyzed, and cannot be used to reconstruct the original text, they are safe to be publicly shared without fear of violating copyright law. this is great news for researchers working with collections like those of the hathitrust as they should be able to gain insight into datasets which are still currently in-copyright and would, otherwise, not be available to be inspected freely.</p>
                <p>in the prototype galaxy viewer implementation, the output of the topic modeling step is processed through a set of r functions that reshape the data and augment it with additional calculated metrics that are used by the web frontend to construct the visualization. these post-processing results are saved to the filesystem as a set of five csv files. one of these csv files is quite large as it contains the topic modeling state data from mallet (containing topic assignments for each document and word, and associated frequency count). the visual web frontend code loads this set of five files into memory when the interface is accessed the first time, which can take several minutes. for the prototype this approach was tolerated, but it has serious scalability and performance issue that needs to be addressed before the tool can be truly usable by other researchers.</p>
                <p>scaling the galaxy viewer therefore consists of creating a web service backed with a (nosql) database which will service ajax requests from the front-end for the data needed to construct the topic visualization and related graphs. we are developing the set of service calls that need to be implemented/exposed by the web service to fulfill the needs of the front-end web-app. the backend service will query the database to retrieve the necessary data to service the requests. the database will be created based on the output of the topic modeling process, after required post-processing of the results is completed (to calculate the topic trends, topic distances, and other metrics used in the display). relevant metadata at the volume and dataset level will also be stored to be made available to the front-end upon request. this work will be completed by the end of december 2015 so that it can be demonstrated in the new year. the scaled galaxy viewer will then provide a non-consumptive way of allowing users of the hathitrust to explore the copyrighted collections. extraction of subsets and topic modelling can take place under the supervision of the htrc and the results database can then be exposed to visualization tools like the galaxy viewer (and others) for exploration.</p>
            </div>
            <div xml:id=""h.izhdfmsd0qww"" type=""div1"" rend=""dh-heading1"">
                <head>closing reflections: how “trace of theory” will improve the htrc
                    <hi rend=""italic"">. </hi>
                </head>
                <p>j. stephen downie</p>
                <p>the hathitrust research center exists to give the digital humanities community analytic access to the hathitrust’s 13.7 million volumes. the ht volumes comprise over 4.8 billion pages each in turn represented by a high-resolution image file and two ocr files yielding some 14.4 billion data files! thus, as the earlier papers have highlighted, the sheer size of the collection, along with the idiosyncratic nature of the ht data, together create several hurdles that impede meaningful analytic research.the htrc is engaged in two ongoing endeavours designed to assist dh researchers in overcoming these obstacles: the advance collaborative support (acs) program [1]; and, the workset creation for scholarly analysis (wcsa) project [2]. </p>
                <p>the acs program at htrc provides no-cost senior developer time, data wrangling assistance, computation time and analytic consultations to dh researchers who are prototyping new research ideas using the ht data resources. the acs program is an integral part of the htrc’s operation mission and was part of its value-added proposition when the htrc launched its recent four-year operations plan (2014-2018). it is a fundamental component of the htrc’s outreach activities and as such, has staff dedicated to its planning, management and day-to-delivery. the acs team was responsible for creating, and then reviewing, the competitive acs request for proposals (rfp) that ask interested dh researchers outline their intellectual goals, describe their data needs, and estimate their computational requirements. the acs team is generally looking for new projects that could benefit from some kickstarting help from htrc. htrc welcomes proposals from researchers with a wide range of experience and skills. projects run 6 to 12 months.</p>
                <p>originally funded by the andrew w. mellon foundation (2013-2015), the current wcsa program is building upon, extending and implementing the development made during the funding period. the htrc project team, along with subaward collaborators at university of oxford, university of maryland, texas agriculture and marine university and university of waikato, developed a group of prototype techniques for empowering scholars who want to do computational analyses of the ht materials to more efficiently and effectively create user-specific analytic subsets (called “worksets”). a formal model has been designed to describe the items in a workset along with necessary bibliographic and provenance metadata that is now being incorporated into the htrc infrastructure (jett, 2015). </p>
                <p>the trace of theory project was selected from the first round of acs proposals. this concluding panel presentation will discuss in what ways the trace of theory project has been both a representative and a unique exemplar of the acs program. it will present some emergent themes that evolved from the htrc-trace of theory interactions that we believe will have an important influence on the delivery of future acs projects. in the same manner, it will reflect upon the problems the team of researchers had in subsetting the data to build their necessary worksets along with the solutions that the hrtc-trace of theory collaboration developed to surmount those difficulties. the panel will finish with a summary of how htrc intends to incorporate the lessons learned into its day-to-day operations as well as future acs projects. </p>
                <p>notes:</p>
                <p>[1] the 2014 acs rfp is available at: https://www.hathitrust.org/htrc/acs-rfp</p>
                <p>[2] https://www.lis.illinois.edu/research/projects/workset-creation-scholarly-analysis-prototyping-project</p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl>
                        <hi rend=""bold"">chaney, a. j. and blei, d. m.</hi> (2012). <hi rend=""italic"">visualizing topic models</hi>, icwsm. 
                        <ref target=""http://www.aaai.org/ocs/index.php/icwsm/icwsm12/paper/download/4645%26lt%3b/5021"">http://www.aaai.org/ocs/index.php/icwsm/icwsm12/paper/download/4645%26lt%3b/5021</ref> (accessed dec 2015).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">jett, j.</hi> (2015).<hi rend=""italic""> modeling worksets in the hathitrust research center</hi>. cirss technical report wcsa0715. champaign, il: university of illinois at urbana-champaign. available via: http://hdl.handle.net/2142/78149 (accessed dec 2015)
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">mccallum, a. k.</hi> (2002). <hi rend=""italic"">mallet: a machine learning for language toolkit</hi>, http://mallet.cs.umass.edu (accessed dec 2015).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">montague, j., simpson, j., brown, s., rockwell, g. and ruecker, s.</hi> (2015). exploring large datasets with topic model visualization. <hi rend=""italic"">paper presented by montague at dh 2015 at the university of western sydney</hi>, australia.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">sinclair, s., g. rockwell and the trace of theory team</hi>. (2016). <hi rend=""bold"">classifying philosophical texts</hi>. online at 
                        <ref target=""http://bit.ly/1khby56"">http://bit.ly/1khby56</ref> (accessed dec 2015). 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">wilkens, m.</hi> (2016). genre, computation, and the weird canonicity of recently dead white men. <hi rend=""italic"">noveltm working paper</hi>.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
",1.0,2.0,Voyant
2620,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,Reflecting On And Refracting User Needs Through Case Studies In The Light Of Europeana Research,,Agiatis Benardou;Alastair Dunning;Stefan Ekman;Vicky Garnett;Jordan Caspar;Ilze Lace;Eliza Papaki,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>Summary: This poster presents work on documenting user needs in the Humanities and Social Sciences as illustrated through Case Studies in the context of the Europeana Cloud “Unlocking Europe's Research via the Cloud” project. Conducted as part of a wider methodological effort including desk research, expert fora and a web survey, methodology and findings of actual use of innovative digital tools and services will be visually represented. This work will form the basis of the Europeana Research Case Studies which will seek to gather and process an evidence-based record of the information practices, needs and scholarly methods in the respective communities.</p>
            <p>This poster reports on collaborative, cross-European work conducted during 2013-2015 in the context of the Europeana Cloud “Unlocking Europe's Research via the Cloud” project, and touches upon planned activities in the context of Europeana Research in 2016. Europeana Research is an initiative which aims to create stronger links between the cultural heritage sector and academia. More particularly, it aims to ensure that open, high-quality data from the cultural sector is available for reuse by the digital humanities community.</p>
            <p>One of the main objectives of Europeana Cloud was the enhancement of the understanding of digital tools, research processes and scholarly content used in the Humanities and Social Sciences, thus informing the development of tools and aggregation of content in Europeana for research purposes. To this end, and in order to contribute towards the development of the new platform of Europeana Research, Case Studies were developed as part of a wider methodological effort which included desk research, expert fora and web survey for reaching user requirements.</p>
            <p>The purpose of this poster is to visually represent the methodology followed and results reached in documenting actual use of innovative digital tools and services in the Humanities and Social Sciences research communities illustrated in three main Case Studies in the disciplines of Education, Art History and Sociology, and further complemented by satellite cases. </p>
            <p>The Case Studies were initially selected based on the disciplines and tools that might best make use of current Europeana content. By defining “innovative” as “either performing functions that were previously unavailable, or performing already available functions in a qualitatively different way”, three tools were identified as best fitting this criteria (Transana, HyperImage, NodeXL) enriched by two “satellite” tools more frequently used in the respective research disciplines (NVivo, Voyant).</p>
            <p>These were further approached following a threefold methodology of semi-structured interviews, empirical observation of the tools and background research. The results were then discussed both from the perspective of the discipline area, and through the lens of the scholarly primitives (Unsworth 2000, Palmer et al 2009), to determine their use with Europeana content. The poster will also highlight the importance of accessibility of data for research infrastructures and research groups and need to focus on high quality metadata and content both for Europeana Research and the wider GLAM sector, and will illustrate how digital tools are not themselves a guarantee of good research, as researchers do not necessarily use the same digital tool throughout the research process; rather they use one tool per step (one tool = one research primitive).</p>
            <p>This poster will also present future work planned to be undertaken in the context of Europeana Research in 2016. Based on Europeana Cloud, a series of new Case Studies will be developed and expanded towards different research communities. The Europeana Research Case Studies will be undertaken in collaboration with existing European research initiatives, and will seek to gather and process an evidence-based record of the information practices, needs and scholarly methods of arts and humanities and social sciences researchers within the broad Europeana ecosystem and particularly in relation to Europeana content. Τhe Case Studies will employ a mixed methods approach combining various ways of gathering empirical evidence on the information needs and scholarly methods employed in digitally-enabled arts and humanities and social sciences research across Europe and beyond.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">Hughes, L.</hi> (2011). Using ICT methods and tools in arts and humanities research, L. Hughes (Ed.), 
                        <hi rend=""italic"">Evaluating and measuring the value, use and impact of digital collections</hi>. London: Facet Publishing.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Palmer, C. L., Teffeau, L. C. and Pirmann, C. M.</hi> (2009). 
                        <hi rend=""italic"">Scholarly information practices in the online environment. </hi>Report commissioned by OCLC Research. Published online at: 
                        <ref target=""http://www.oclc.org/programs/publications/reports/2009-02.pdf"">www.oclc.org/programs/publications/reports/2009-02.pdf</ref>. 
                    </bibl>
                    <bibl>University of Virginia (2005). 
                        <hi rend=""italic"">Summit on Digital Tools for the Humanities - Report on Summit Accomplishments.</hi> Retrieved from http://www.iath.virginia.edu/dtsummit/SummitText.pdf.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Unsworth, J.</hi> (2000). Scholarly primitives: What methods do humanities researchers have in common, and how might our tools reflect this. In J. Unsworth (Ed.), 
                        <hi rend=""italic"">Humanities Computing, Formal Methods, Experimental Practice Symposium, </hi>pp. 5-100.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Unsworth, J.</hi> (2003). Tool-Time, or Haven't We Been Here Already?. Presented at the 
                        <hi rend=""italic"">Transforming Disciplines: The Humanities and Computer Science</hi>. Washington, DC. Retrieved from http://people.lis.illinois.edu/~unsworth/carnegie-ninch.03.html.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,case studies;interdisciplinarity;research infrastructures;user needs,English,"archives, repositories, sustainability and preservation;audio, video, multimedia;cultural infrastructure;digital humanities - diversity;digital humanities - nature and significance;GLAM: galleries, libraries, archives, museums;interdisciplinary collaboration;interface and user experience design;metadata;project design, organization, management;software design and development;user studies / user needs",2016-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>summary: this poster presents work on documenting user needs in the humanities and social sciences as illustrated through case studies in the context of the europeana cloud “unlocking europe's research via the cloud” project. conducted as part of a wider methodological effort including desk research, expert fora and a web survey, methodology and findings of actual use of innovative digital tools and services will be visually represented. this work will form the basis of the europeana research case studies which will seek to gather and process an evidence-based record of the information practices, needs and scholarly methods in the respective communities.</p>
            <p>this poster reports on collaborative, cross-european work conducted during 2013-2015 in the context of the europeana cloud “unlocking europe's research via the cloud” project, and touches upon planned activities in the context of europeana research in 2016. europeana research is an initiative which aims to create stronger links between the cultural heritage sector and academia. more particularly, it aims to ensure that open, high-quality data from the cultural sector is available for reuse by the digital humanities community.</p>
            <p>one of the main objectives of europeana cloud was the enhancement of the understanding of digital tools, research processes and scholarly content used in the humanities and social sciences, thus informing the development of tools and aggregation of content in europeana for research purposes. to this end, and in order to contribute towards the development of the new platform of europeana research, case studies were developed as part of a wider methodological effort which included desk research, expert fora and web survey for reaching user requirements.</p>
            <p>the purpose of this poster is to visually represent the methodology followed and results reached in documenting actual use of innovative digital tools and services in the humanities and social sciences research communities illustrated in three main case studies in the disciplines of education, art history and sociology, and further complemented by satellite cases. </p>
            <p>the case studies were initially selected based on the disciplines and tools that might best make use of current europeana content. by defining “innovative” as “either performing functions that were previously unavailable, or performing already available functions in a qualitatively different way”, three tools were identified as best fitting this criteria (transana, hyperimage, nodexl) enriched by two “satellite” tools more frequently used in the respective research disciplines (nvivo, voyant).</p>
            <p>these were further approached following a threefold methodology of semi-structured interviews, empirical observation of the tools and background research. the results were then discussed both from the perspective of the discipline area, and through the lens of the scholarly primitives (unsworth 2000, palmer et al 2009), to determine their use with europeana content. the poster will also highlight the importance of accessibility of data for research infrastructures and research groups and need to focus on high quality metadata and content both for europeana research and the wider glam sector, and will illustrate how digital tools are not themselves a guarantee of good research, as researchers do not necessarily use the same digital tool throughout the research process; rather they use one tool per step (one tool = one research primitive).</p>
            <p>this poster will also present future work planned to be undertaken in the context of europeana research in 2016. based on europeana cloud, a series of new case studies will be developed and expanded towards different research communities. the europeana research case studies will be undertaken in collaboration with existing european research initiatives, and will seek to gather and process an evidence-based record of the information practices, needs and scholarly methods of arts and humanities and social sciences researchers within the broad europeana ecosystem and particularly in relation to europeana content. τhe case studies will employ a mixed methods approach combining various ways of gathering empirical evidence on the information needs and scholarly methods employed in digitally-enabled arts and humanities and social sciences research across europe and beyond.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl>
                        <hi rend=""bold"">hughes, l.</hi> (2011). using ict methods and tools in arts and humanities research, l. hughes (ed.), 
                        <hi rend=""italic"">evaluating and measuring the value, use and impact of digital collections</hi>. london: facet publishing.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">palmer, c. l., teffeau, l. c. and pirmann, c. m.</hi> (2009). 
                        <hi rend=""italic"">scholarly information practices in the online environment. </hi>report commissioned by oclc research. published online at: 
                        <ref target=""http://www.oclc.org/programs/publications/reports/2009-02.pdf"">www.oclc.org/programs/publications/reports/2009-02.pdf</ref>. 
                    </bibl>
                    <bibl>university of virginia (2005). 
                        <hi rend=""italic"">summit on digital tools for the humanities - report on summit accomplishments.</hi> retrieved from http://www.iath.virginia.edu/dtsummit/summittext.pdf.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">unsworth, j.</hi> (2000). scholarly primitives: what methods do humanities researchers have in common, and how might our tools reflect this. in j. unsworth (ed.), 
                        <hi rend=""italic"">humanities computing, formal methods, experimental practice symposium, </hi>pp. 5-100.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">unsworth, j.</hi> (2003). tool-time, or haven't we been here already?. presented at the 
                        <hi rend=""italic"">transforming disciplines: the humanities and computer science</hi>. washington, dc. retrieved from http://people.lis.illinois.edu/~unsworth/carnegie-ninch.03.html.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
",1.0,1.0,Voyant
2621,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,"Remapping Leigh Hunt's Circles"": Voyant Tools and Hunt's Dramatic Criticism",,Michael Eberle-Sinatra;Stéfan Sinclair;Emmanuel Chateau,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>“Remapping Leigh Hunt’s Circles” is an ambitious project that explores Leigh Hunt’s central position in the London literary and critical scene of the first half of the nineteenth century, through the lens of digital humanities tools. Hunt is today considered one of the key figures of the Romantic period in England, known for his work as editor, journalist, poet, and facilitator. Numerous articles, essay collections, biographies, and monographs published in the last fifteen years have made this clear. Hunt's contribution to Romantic and Victorian literature was as extensive as it has proven durable, in matters as various as prosodic experimentation and the modernization of the magazine essay. Yet little work (beyond some biographical notes) has been done on the second half of his life, a period that was as productive as the first, and during which Hunt was intimate with many of the finest writers of the time, and continued to contribute to London’s literary circles through the ongoing publication of critical essays in periodicals and anthologies. This project aims to redress this imbalance/oversight and reassert Hunt’s place in the Romantic and Victorian eras, as well as his continuing significance for understanding the London literary scene between 1805 (publication of his first critical essay) and 1859 (date of his death, with his last article published only a few weeks before). </p>
            <p>“Remapping Leigh Hunt’s Circles” makes a case for Hunt’s position as a key critical voice in London beyond his already established prominence during the 
                <hi rend=""italic"">Examiner</hi> years. It does so through a careful analysis of his critical reviews and essays (with a specific focus on his drama criticism to underscore Hunt’s ongoing engagement with the public sphere) published during his entire career, which spanned the first half of the nineteenth century. Data mining and textual analysis offer exciting opportunities to bring together different sets of data which, when prepared to the highest standard of text encoding, can yield new and innovative results that encourage reconsideration of preconceived notions regarding the transfer of ideas from one author to another, or one literary genre to another. The results of the research undertaken in “Remapping Leigh Hunt’s Circles” will be presented in a collaborative, visual context that reimagines the digital scholarly edition as a transparent workspace in which established primary objects from existing databases can be gathered, organized, correlated, annotated, and augmented by multiple users in a dynamic environment. 
            </p>
            <p>All the texts prepared for inclusion in our project are encoded to the Text-Encoding-Initiative (TEI) standards. The mark-up language and quality controls for improving metadata in all the resources provide more accurate search and discovery, allow for the presentation of well-supported content on multiple devices and develop tools for assembling, archiving and indexing research objects and artifacts. Ongoing work on this platform will enable researchers to undertake world-class research by providing the means to link data-sets to published content, encouraging data reanalysis, replication studies, and data re-purposing, all of which improve research quality and efficiency.</p>
            <p>Our poster will report on the first year of this project, and the implementation of the latest version of the 
                <hi rend=""italic"">Voyant Tools</hi> to examine the dramatic essays written by Hunt between 1805 and 1813 (when he was sentenced to two years in prison for libel against the Prince Regent). We will showcase in particular two aspects of the integration between the Hunt archives and Voyant Tools. First, the ability to identify and visualize named entity connections and their networks across multiple documents (this a refinement of the previous RezoViz tool in Voyant). The Hunt collection presents an ideal corpus for network exploration given the interconnectedness of the people, locations and events that animate the documents. Second, Voyant provides a generic and customizable way of presenting a web-based corpus catalogue with the same kinds of faceted browsing and advanced querying capabilities we have come to expect from library databases and online stores. A further benefit of this functionality is the ability to create dynamic subsets of a corpus to examine more closely (in other words, using a catalogue skin in Voyant to create worksets destined for Voyant’s more conventional analytic skin).
            </p>
            <p>The “Remapping Leigh Hunt’s Circles” is essentially a project of digital text editing and literary criticism whereas Voyant Tools is essentially a software platform for reading, analyzing and visualizing digital texts. These are separate traditions and separate concerns, but this poster will demonstrate the value of symbiotic development: both projects benefit from the collaboration.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl><hi rend=""bold"">McGann, J.</hi> (2014). 
                        <hi rend=""italic"">A New Republic of Letters</hi>. Cambridge, MA: Harvard UP. 
                    </bibl>
                    <bibl><hi rend=""bold"">Sinatra, M., and Sinclair, S.</hi> (2015). Special issue “Repenser le numérique au 21
                        <hi rend=""superscript"">ème</hi> siècle”. 
                        <hi rend=""italic"">Sens public</hi> (hiver 2015). 
                    </bibl>
                    <bibl><hi rend=""bold"">Sinatra, M.</hi> (2015). “Representing Leigh Hunt’s Autobiography”.
                        <hi rend=""italic"">Virtual Victorians: Networks, Connections, Technologies</hi>. Eds. Stauffer, A. and Alfano, V. R., Palgrave.
                    </bibl>
                    <bibl><hi rend=""bold"">Sinclair, S., Rucker, S. and Radzikowska, M.</hi> (2011). 
                        <hi rend=""italic"">Visual Interface Design for Digital Cultural Heritage</hi>. Ashgate.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,british romanticism;text analysis;textual criticism,English,english studies;text analysis,2016-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>“remapping leigh hunt’s circles” is an ambitious project that explores leigh hunt’s central position in the london literary and critical scene of the first half of the nineteenth century, through the lens of digital humanities tools. hunt is today considered one of the key figures of the romantic period in england, known for his work as editor, journalist, poet, and facilitator. numerous articles, essay collections, biographies, and monographs published in the last fifteen years have made this clear. hunt's contribution to romantic and victorian literature was as extensive as it has proven durable, in matters as various as prosodic experimentation and the modernization of the magazine essay. yet little work (beyond some biographical notes) has been done on the second half of his life, a period that was as productive as the first, and during which hunt was intimate with many of the finest writers of the time, and continued to contribute to london’s literary circles through the ongoing publication of critical essays in periodicals and anthologies. this project aims to redress this imbalance/oversight and reassert hunt’s place in the romantic and victorian eras, as well as his continuing significance for understanding the london literary scene between 1805 (publication of his first critical essay) and 1859 (date of his death, with his last article published only a few weeks before). </p>
            <p>“remapping leigh hunt’s circles” makes a case for hunt’s position as a key critical voice in london beyond his already established prominence during the 
                <hi rend=""italic"">examiner</hi> years. it does so through a careful analysis of his critical reviews and essays (with a specific focus on his drama criticism to underscore hunt’s ongoing engagement with the public sphere) published during his entire career, which spanned the first half of the nineteenth century. data mining and textual analysis offer exciting opportunities to bring together different sets of data which, when prepared to the highest standard of text encoding, can yield new and innovative results that encourage reconsideration of preconceived notions regarding the transfer of ideas from one author to another, or one literary genre to another. the results of the research undertaken in “remapping leigh hunt’s circles” will be presented in a collaborative, visual context that reimagines the digital scholarly edition as a transparent workspace in which established primary objects from existing databases can be gathered, organized, correlated, annotated, and augmented by multiple users in a dynamic environment. 
            </p>
            <p>all the texts prepared for inclusion in our project are encoded to the text-encoding-initiative (tei) standards. the mark-up language and quality controls for improving metadata in all the resources provide more accurate search and discovery, allow for the presentation of well-supported content on multiple devices and develop tools for assembling, archiving and indexing research objects and artifacts. ongoing work on this platform will enable researchers to undertake world-class research by providing the means to link data-sets to published content, encouraging data reanalysis, replication studies, and data re-purposing, all of which improve research quality and efficiency.</p>
            <p>our poster will report on the first year of this project, and the implementation of the latest version of the 
                <hi rend=""italic"">voyant tools</hi> to examine the dramatic essays written by hunt between 1805 and 1813 (when he was sentenced to two years in prison for libel against the prince regent). we will showcase in particular two aspects of the integration between the hunt archives and voyant tools. first, the ability to identify and visualize named entity connections and their networks across multiple documents (this a refinement of the previous rezoviz tool in voyant). the hunt collection presents an ideal corpus for network exploration given the interconnectedness of the people, locations and events that animate the documents. second, voyant provides a generic and customizable way of presenting a web-based corpus catalogue with the same kinds of faceted browsing and advanced querying capabilities we have come to expect from library databases and online stores. a further benefit of this functionality is the ability to create dynamic subsets of a corpus to examine more closely (in other words, using a catalogue skin in voyant to create worksets destined for voyant’s more conventional analytic skin).
            </p>
            <p>the “remapping leigh hunt’s circles” is essentially a project of digital text editing and literary criticism whereas voyant tools is essentially a software platform for reading, analyzing and visualizing digital texts. these are separate traditions and separate concerns, but this poster will demonstrate the value of symbiotic development: both projects benefit from the collaboration.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl><hi rend=""bold"">mcgann, j.</hi> (2014). 
                        <hi rend=""italic"">a new republic of letters</hi>. cambridge, ma: harvard up. 
                    </bibl>
                    <bibl><hi rend=""bold"">sinatra, m., and sinclair, s.</hi> (2015). special issue “repenser le numérique au 21
                        <hi rend=""superscript"">ème</hi> siècle”. 
                        <hi rend=""italic"">sens public</hi> (hiver 2015). 
                    </bibl>
                    <bibl><hi rend=""bold"">sinatra, m.</hi> (2015). “representing leigh hunt’s autobiography”.
                        <hi rend=""italic"">virtual victorians: networks, connections, technologies</hi>. eds. stauffer, a. and alfano, v. r., palgrave.
                    </bibl>
                    <bibl><hi rend=""bold"">sinclair, s., rucker, s. and radzikowska, m.</hi> (2011). 
                        <hi rend=""italic"">visual interface design for digital cultural heritage</hi>. ashgate.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
",7.0,7.0,Voyant
2627,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,Scholarly Requirements for Large Scale Text Analysis: A User Needs Assessment by the HathiTrust Research Center,,Harriett Elizabeth Green;Eleanor Frances Dickson;Sayan Bhattacharyya,"paper, specified ""short paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Introduction</head>
                <p>The HathiTrust Research Center (HTRC) aims to facilitate large-scale computational text analysis of the contents of the HathiTrust Digital Library (HTDL) through data services and analytical tools. We conducted a study of current and potential users of the HTRC to investigate how scholars integrate text analysis into their research. Our study aims to inform the development of HTRC services and also to generate deeper insights into scholarly research practices with large-scale digitized text corpora.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Background</head>
                <p>Studies on the use of digital content by humanities scholars, ranging from humanities cyberinfrastructure (ACLS, 2006) and patterns in scholarly practices (Brockman et al., 2001; Palmer and Neumann, 2002; Green and Courtney, 2015), to discipline-specific studies (Zorich, 2012; Babeu, 2011; Rutner and Schonfeld, 2011), reveal that scholars acquire and analyze digital content in multi-faceted ways. Several investigations particularly examine scholarly uses of digital tools (Frischer et al., 2006; Toms and O’Brien, 2008; Gibbs and Owens, 2012). Computational text analysis dates from the beginnings of humanities computing (Hindley, 2013), and the resources of the ARTFL Project (Argamon et al., 2009; Horton et al., 2009), MONK (Unsworth, 2011), Wordseer (Muralidharan and Hearst, 2013), Voyant and TaPOR (Rockwell et al., 2010), and Lexos (LeBlanc et al., 2013), among others, inform the current work of the HTRC to provide a secure computational and data environment for researchers to conduct analyses of content from the HathiTrust Digital Library.</p>
                <p>Our study builds on an earlier user needs assessment conducted for the HTRC and its Mellon Foundation-funded Workset Creation for Scholarly Analysis project. That earlier study analyzed interviews and focus groups in order to identify capabilities needed in large text corpora to facilitate scholarly research use (Fenlon et al., 2014). These desired capabilities included the ability to create and manipulate collections as reusable datasets and research products, the ability to work at different units of analysis, and access to highly enriched metadata (Green et al., 2014; Fenlon et al., 2014). </p>
                <p>Our present study especially builds upon that previous investigation by examining the text analysis research practices of current and potential users of the HTRC.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Research Design</head>
                <div type=""div2"" rend=""DH-Heading2"">
                    <head>Goals</head>
                    <p>Our study’s primary goals are:</p>
                    <list type=""unordered"">
                        <item>To analyze current scholarly research practices with textual corpora to identify user requirements for HTRC services;</item>
                        <item>to develop illustrative use cases of text analysis research for shaping training curricula; and</item>
                        <item>to obtain information for guiding the development of HTRC research services in the University of Illinois Library’s Scholarly Commons and similar digital scholarship centers.</item>
                    </list>
                    <p>While the findings of this study specifically will inform the development of services to meet the needs of HTRC users, it also contributes broader insights into how to develop similar digital resources and research services for computational text analysis.</p>
                </div>
                <div type=""div2"" rend=""DH-Heading2"">
                    <head>Methods</head>
                    <p>We conducted fifteen semi-structured interviews with students, faculty, researchers, administrators, and librarians who pursue work that includes text analysis, or have familiarity with text analysis methods. Some participants were recruited at professional conferences for digital humanities and libraries, while others were active in HTRC user group forums. Several of the interviewees had previously interacted with the HTRC, and most had experience with the HTDL. The participants were from various disciplines — including English, Anthropology, History, and Computer Science —and ranged from newcomers to digital humanities to long-time researchers. </p>
                    <p>We performed an initial analysis of the interview data through open coding and will continue detailed qualitative analysis using ATLAS.ti. Data was independently coded by the authors to ensure inter-coder reliability. While we are still actively analyzing interview data, we identified several preliminary themes discussed here. These themes include strategies for obtaining and managing data, research workflows and results, collaborations, and teaching. </p>
                </div>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Analysis and Discussion</head>
                <div type=""div2"" rend=""DH-Heading2"">
                    <head>Data Acquisition and Management</head>
                    <p>Several respondents characterized text analysis research as being time-intensive in spite of the speed of computational tools. One interviewee noted, ‘It’s funny, often people think, “Oh we have it digitized, now it’s useful.” Scholars realize that you have a lot more work to do after that. And that can often slow projects down terribly.’</p>
                    <p>The interviewees indicated that gathering, managing, and manipulating text data comprised a considerable portion of their work. An interviewee explained, ‘I think the biggest challenge is data, getting good data to work with. I think people underestimate the problems and difficulties in doing that.’</p>
                    <p>Interviewees also expressed a desire for improved ways to identify and extract the content they needed, especially when navigating large-scale collections to find the volumes, pages, or passages relevant to a research project. As one interviewee remarked, ‘Even if you had somehow structured your texts, I would be saying, “What was left out? How do I bring it back in?”’ </p>
                </div>
                <div type=""div2"" rend=""DH-Heading2"">
                    <head>Research Workflows and Results</head>
                    <p>Several interviewees described the potential of text analysis to challenge previously held understandings of text, as differences between human and computational readings emerged. One respondent noted, ‘There are many cases in which the computer is at least as good—if not better—a reader than humans are. That’s very difficult for people to accept... sometimes the computer gets it right and it bears looking at that difference. So we kind of want to get that new ground truth on this kind of work.’</p>
                    <p>Many researchers highlighted the importance of interpretive work in understanding how the tools interact with the text, and characterized the interactions as dynamic. One respondent observed, ‘I yearn for workflows where the scholar could actually set their own tokenization rules.... It would be a way that we could create less language-specific [rules] or control the language specificity of the algorithm. I think that is the real need.’ Several respondents highlighted the importance of tools that flexibly fit into various stages of the research process, and also are accessible to users of different skill levels. Interviewees also suggested enhancements specific to the HTRC, which included expanded visualization capabilities, improved generation of statistics about text corpora, and better ability to handle languages other than English. </p>
                </div>
                <div type=""div2"" rend=""DH-Heading2"">
                    <head>Research Collaborations</head>
                    <p>Interviewees repeatedly cited collaboration and research support, both virtual and in-person, as important. Many interviewees worked with digital humanities initiatives, and reported that their local resources ranged from limited technical support to well-resourced research centers. For some interviewees, online support communities— such as Digital Humanities Questions and Answers or Stack Overflow — also were significant.</p>
                    <p>Interdisciplinary collaborations between departments and across institutions emerged as the most prominent kind of partnership, but interviewees also noted the challenges that such collaborations pose. As one interviewee explained, ‘Collaborations between institutions: much more difficult. There’s money, there’s institutional blockages, and then anything over half a dozen people, it gets complicated very quickly. And so the people dynamics get very complicated.’ Some respondents noted that these collaborations affected their research practices and acquisition of research resources. </p>
                    <p> Interviewees reported that their collaborations with libraries ranged from non-existent to critical partnerships. Many saw the library as a key space because ‘the library is actually the one functioning interdisciplinary space on a university campus.’ Collaborations with the HTRC and digital repositories for working with data also were important to respondents.</p>
                </div>
                <div type=""div2"" rend=""DH-Heading2"">
                    <head>Teaching and Training</head>
                    <p>Interviewees mentioned their active efforts and intentions to incorporate computational text analysis into their teaching. Some remarked on institutional constraints that make it difficult to incorporate computational tools into curricula. As one respondent explained: ‘I once imagined teaching a class in which students learn to script and actually run analyses against data, but I was told, basically, that that class isn’t a humanities class anymore—that belongs in computer science.’</p>
                    <p>Some stated that the courses that they currently teach may not require or allow for the incorporation of computational analysis. Yet others noted that there is only a limited amount of technical or scientific skills that a humanities student could realistically master within a short period of time, with one interviewee noting that ‘you can only get people to learn so much about the math; as much as they can learn, they should — at the same time, it’s hard.’</p>
                    <p> Although the demand from students for learning about computational text analysis was, overall, reported to be increasing, some interviewees noted that they are constrained by not only limited resources, but also uncertainty as to how to carry out such activities. One interviewee reported prevailing sentiments that the digital humanities ‘doesn’t even fit anywhere,’ leading to the question of whether ‘there should be a whole separate department that’s digital humanities,’ or to offer training within existing curricula.</p>
                </div>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Conclusion</head>
                <p> The immediate aims of this study are to generate an updated framework of user requirements that will guide the development of the HTRC’s educational programming and research support services and also to inform forthcoming Mellon Foundation-funded development of the HTRC Data Capsule. But our preliminary findings also provide insights into scholars’ needs as they increasingly incorporate text analysis in research and teaching. These findings also reveal how digital scholarship centers, information professionals, and providers of digitized content can best support scholarship as digital humanities resources evolve.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Acknowledgements</head>
                <p>We thank Megan Senseney, Angela Courtney, Nicholae Cline, and Leanne Mobley for their collaboration in this study. </p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl>
                        <hi rend=""bold"">American Council of Learned Societies. </hi>(2006). 
                        <hi rend=""italic"">Our Cultural Commonwealth: The report of the ACLS Commission on Cyberinfrastructure for the Humanities and Social Sciences</hi>. New York: American Council of Learned Societies. 
                        <ref target=""http://www.acls.org/uploadedFiles/Publications/Programs/Our_Cultural_Commonwealth.pdf"">http://www.acls.org/uploadedFiles/Publications/Programs/Our_Cultural_Commonwealth.pdf</ref> (accessed 4 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Argamon, S., et al. </hi>(2009). Gender, Race, and Nationality in Black Drama, 1850-2000: Mining Differences in Language Use in Authors and their Characters. 
                        <hi rend=""italic"">Digital Humanities Quarterly</hi>
                        <hi rend=""bold"">3</hi>(2): http://www.digitalhumanities.org/dhq/vol/3/2/000043/000043.html 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Babeu, A.</hi> (2011). 
                        <hi rend=""italic"">Rome wasn't digitized in a day: Building a cyberinfrastructure for digital classics.</hi> CLIR Publication, <hi rend=""bold"">150</hi>, Washington, DC: Council of Library and Information Resources. 
                        <ref target=""http://www.clir.org/pubs/reports/pub150/reports/pub150/pub150.pdf"">http://www.clir.org/pubs/reports/pub150/reports/pub150/pub150.pdf</ref> (accessed 4 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Brockman, W. S., et al.</hi> (2001). 
                        <hi rend=""italic"">Scholarly work in the humanities and the evolving information environment</hi> CLIR Publication,<hi rend=""bold""> 104</hi>, Washington, D.C.: Digital Library Federation, Council on Library and Information Resources. 
                        <ref target=""http://www.clir.org/pubs/reports/pub104/pub104.pdf"">http://www.clir.org/pubs/reports/pub104/pub104.pdf</ref> (accessed 4 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Fenlon, K., et al. </hi>(2014). Scholar-built collections: A study of user requirements for research in large-scale digital libraries. 
                        <hi rend=""italic"">Proceedings of the American Society for Information Science and Technology</hi>,
                        <hi rend=""bold"">51</hi>(1): 1–10.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Frischer, B., et al.</hi> (2006). 
                        <hi rend=""italic"">Summit on digital tools for the humanities: Report on summit accomplishments</hi>. Institute for Advanced Technology in the Humanities, University of Virginia. 
                        <ref target=""http://www.iath.virginia.edu/dtsummit/SummitText.pdf"">http://www.iath.virginia.edu/dtsummit/SummitText.pdf</ref> (accessed 4 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Gibbs, F. and Owens, T.</hi> (2012). Building Better Digital Humanities Tools: Toward broader audiences and user-centered designs. 
                        <hi rend=""italic"">Digital Humanities Quarterly</hi>,
                        <hi rend=""bold"">6</hi>(2).
                        <ref target=""http://www.digitalhumanities.org/dhq/vol/6/2/000136/000136.html"">http://www.digitalhumanities.org/dhq/vol/6/2/000136/000136.html</ref> (accessed 4 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Green, H. and Courtney, A.</hi> (2015). Beyond the Scanned Image: A Needs Assessment of Scholarly Users of Digital Collections. 
                        <hi rend=""italic"">College and Research Libraries</hi>,
                        <hi rend=""bold"">76</hi>(5): 690-707.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Green, H. E., et al.</hi>, (2014). Using Collections and Worksets in Large-Scale Corpora: Preliminary Findings from the Workset Creation for Scholarly Analysis Prototyping Project. <hi rend=""italic"">Poster presented at iConference 2014</hi>, Berlin, Germany.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Hindley, M. </hi>(2013). The Rise of the Machines: NEH and the Digital Humanities: the early years. 
                        <hi rend=""italic"">Humanities</hi>,
                        <hi rend=""bold"">34</hi>(4).
                        <ref target=""http://www.neh.gov/humanities/2013/julyaugust/feature/the-rise-the-machines"">http://www.neh.gov/humanities/2013/julyaugust/feature/the-rise-the-machines</ref> (accessed 4 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Horton, R., et al.</hi> (2009). Mining Eighteenth Century Ontologies: Machine Learning and Knowledge Classification in the Encyclopédie. 
                        <hi rend=""italic"">Digital Humanities Quarterly</hi>,
                        <hi rend=""bold"">3</hi>(2): http://www.digitalhumanities.org/dhq/vol/3/2/000044/000044.html 
                        <lb/>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">LeBlanc, M. D., et al.</hi> (2013). Lexomics: Integrating the Research and Teaching Spaces. 
                        <hi rend=""italic"">Digital Humanities 2013 Conference Abstracts, University of Nebraska–Lincoln, 16-19 July 2013</hi>. Lincoln, NE: Association of Digital Humanities Organizations, pp. 274-76. http://dh2013.unl.edu/abstracts/ab-293.html (accessed 4 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Muralidharan, A. and Hearst, M. A. </hi>
                        <hi rend=""background(white)"">(2013). Supporting Exploratory Text Analysis in Literature Study.</hi>
                        <hi rend=""italic"">Literary and Linguistic Computing</hi>,
                        <hi rend=""bold"">28</hi>(2): 283-95. 10.1093/llc/fqs044.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Palmer, C. L. and Neumann, L. J.</hi> (2002). The Information Work of Interdisciplinary Humanities Scholars: Exploration and Translation.
                        <hi rend=""italic""> Library Quarterly</hi>
                        <hi rend=""bold"">7</hi>(1): 85-117.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Rockwell, G., et al.</hi> (2010). Ubiquitous Text Analysis. 
                        <hi rend=""italic"">Poetess Archive Journal</hi>, <hi rend=""bold"">1</hi>(2).
                        <ref target=""https://journals.tdl.org/paj/index.php/paj/article/view/13"">https://journals.tdl.org/paj/index.php/paj/article/view/13</ref> (accessed 4 March 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Rutner, J. and Schonfeld, R.</hi> (2012). 
                        <hi rend=""italic"">Supporting the Changing Research Practices of Historians.</hi> New York: Ithaka S+R. http://sr.ithaka.org/?p=22532
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sukovic, S. </hi>(2011). E-Texts in Research Projects in the Humanities, A. Woodsworth and W. D. Penniman (eds.), 
                        <hi rend=""italic"">Advances in Librarianship</hi>. Bingley, UK: Emerald Group Publishing, pp. 131-202.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Toms, E. G. and O’Brien, H.</hi> (2008). Understanding the Information and Communication Technology Needs of the E-Humanist. 
                        <hi rend=""italic"">Journal of Documentation</hi>,
                        <hi rend=""bold"">64</hi>(1): 102-30.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Unsworth, J.</hi> (2011). Computational Work with Very Large Text Collections: Interoperability, Sustainability, and the TEI. 
                        <hi rend=""italic"">Journal of the Text Encoding Initiative</hi>
                        <hi rend=""bold"">1</hi>, 10.4000/jtei.215 (accessed 4 March 2016). 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Zorich, D. </hi>(2012). 
                        <hi rend=""italic"">Transitioning to a Digital World: Art History, Its Research Centers and Digital Scholarship: A Report to the Samuel H. Kress Foundation and the Roy Rosenzweig Center for History and New Media</hi>. New York: Samuel H. Kress Foundation. http://www.kressfoundation.org/research/Default.aspx?id=35379 (accessed 4 March 2016).
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,digital scholarship centers;text analysis;user assessment,English,"digital humanities - institutional support;GLAM: galleries, libraries, archives, museums;interdisciplinary collaboration;text analysis;user studies / user needs",2016-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <div type=""div1"" rend=""dh-heading1"">
                <head>introduction</head>
                <p>the hathitrust research center (htrc) aims to facilitate large-scale computational text analysis of the contents of the hathitrust digital library (htdl) through data services and analytical tools. we conducted a study of current and potential users of the htrc to investigate how scholars integrate text analysis into their research. our study aims to inform the development of htrc services and also to generate deeper insights into scholarly research practices with large-scale digitized text corpora.</p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>background</head>
                <p>studies on the use of digital content by humanities scholars, ranging from humanities cyberinfrastructure (acls, 2006) and patterns in scholarly practices (brockman et al., 2001; palmer and neumann, 2002; green and courtney, 2015), to discipline-specific studies (zorich, 2012; babeu, 2011; rutner and schonfeld, 2011), reveal that scholars acquire and analyze digital content in multi-faceted ways. several investigations particularly examine scholarly uses of digital tools (frischer et al., 2006; toms and o’brien, 2008; gibbs and owens, 2012). computational text analysis dates from the beginnings of humanities computing (hindley, 2013), and the resources of the artfl project (argamon et al., 2009; horton et al., 2009), monk (unsworth, 2011), wordseer (muralidharan and hearst, 2013), voyant and tapor (rockwell et al., 2010), and lexos (leblanc et al., 2013), among others, inform the current work of the htrc to provide a secure computational and data environment for researchers to conduct analyses of content from the hathitrust digital library.</p>
                <p>our study builds on an earlier user needs assessment conducted for the htrc and its mellon foundation-funded workset creation for scholarly analysis project. that earlier study analyzed interviews and focus groups in order to identify capabilities needed in large text corpora to facilitate scholarly research use (fenlon et al., 2014). these desired capabilities included the ability to create and manipulate collections as reusable datasets and research products, the ability to work at different units of analysis, and access to highly enriched metadata (green et al., 2014; fenlon et al., 2014). </p>
                <p>our present study especially builds upon that previous investigation by examining the text analysis research practices of current and potential users of the htrc.</p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>research design</head>
                <div type=""div2"" rend=""dh-heading2"">
                    <head>goals</head>
                    <p>our study’s primary goals are:</p>
                    <list type=""unordered"">
                        <item>to analyze current scholarly research practices with textual corpora to identify user requirements for htrc services;</item>
                        <item>to develop illustrative use cases of text analysis research for shaping training curricula; and</item>
                        <item>to obtain information for guiding the development of htrc research services in the university of illinois library’s scholarly commons and similar digital scholarship centers.</item>
                    </list>
                    <p>while the findings of this study specifically will inform the development of services to meet the needs of htrc users, it also contributes broader insights into how to develop similar digital resources and research services for computational text analysis.</p>
                </div>
                <div type=""div2"" rend=""dh-heading2"">
                    <head>methods</head>
                    <p>we conducted fifteen semi-structured interviews with students, faculty, researchers, administrators, and librarians who pursue work that includes text analysis, or have familiarity with text analysis methods. some participants were recruited at professional conferences for digital humanities and libraries, while others were active in htrc user group forums. several of the interviewees had previously interacted with the htrc, and most had experience with the htdl. the participants were from various disciplines — including english, anthropology, history, and computer science —and ranged from newcomers to digital humanities to long-time researchers. </p>
                    <p>we performed an initial analysis of the interview data through open coding and will continue detailed qualitative analysis using atlas.ti. data was independently coded by the authors to ensure inter-coder reliability. while we are still actively analyzing interview data, we identified several preliminary themes discussed here. these themes include strategies for obtaining and managing data, research workflows and results, collaborations, and teaching. </p>
                </div>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>analysis and discussion</head>
                <div type=""div2"" rend=""dh-heading2"">
                    <head>data acquisition and management</head>
                    <p>several respondents characterized text analysis research as being time-intensive in spite of the speed of computational tools. one interviewee noted, ‘it’s funny, often people think, “oh we have it digitized, now it’s useful.” scholars realize that you have a lot more work to do after that. and that can often slow projects down terribly.’</p>
                    <p>the interviewees indicated that gathering, managing, and manipulating text data comprised a considerable portion of their work. an interviewee explained, ‘i think the biggest challenge is data, getting good data to work with. i think people underestimate the problems and difficulties in doing that.’</p>
                    <p>interviewees also expressed a desire for improved ways to identify and extract the content they needed, especially when navigating large-scale collections to find the volumes, pages, or passages relevant to a research project. as one interviewee remarked, ‘even if you had somehow structured your texts, i would be saying, “what was left out? how do i bring it back in?”’ </p>
                </div>
                <div type=""div2"" rend=""dh-heading2"">
                    <head>research workflows and results</head>
                    <p>several interviewees described the potential of text analysis to challenge previously held understandings of text, as differences between human and computational readings emerged. one respondent noted, ‘there are many cases in which the computer is at least as good—if not better—a reader than humans are. that’s very difficult for people to accept... sometimes the computer gets it right and it bears looking at that difference. so we kind of want to get that new ground truth on this kind of work.’</p>
                    <p>many researchers highlighted the importance of interpretive work in understanding how the tools interact with the text, and characterized the interactions as dynamic. one respondent observed, ‘i yearn for workflows where the scholar could actually set their own tokenization rules.... it would be a way that we could create less language-specific [rules] or control the language specificity of the algorithm. i think that is the real need.’ several respondents highlighted the importance of tools that flexibly fit into various stages of the research process, and also are accessible to users of different skill levels. interviewees also suggested enhancements specific to the htrc, which included expanded visualization capabilities, improved generation of statistics about text corpora, and better ability to handle languages other than english. </p>
                </div>
                <div type=""div2"" rend=""dh-heading2"">
                    <head>research collaborations</head>
                    <p>interviewees repeatedly cited collaboration and research support, both virtual and in-person, as important. many interviewees worked with digital humanities initiatives, and reported that their local resources ranged from limited technical support to well-resourced research centers. for some interviewees, online support communities— such as digital humanities questions and answers or stack overflow — also were significant.</p>
                    <p>interdisciplinary collaborations between departments and across institutions emerged as the most prominent kind of partnership, but interviewees also noted the challenges that such collaborations pose. as one interviewee explained, ‘collaborations between institutions: much more difficult. there’s money, there’s institutional blockages, and then anything over half a dozen people, it gets complicated very quickly. and so the people dynamics get very complicated.’ some respondents noted that these collaborations affected their research practices and acquisition of research resources. </p>
                    <p> interviewees reported that their collaborations with libraries ranged from non-existent to critical partnerships. many saw the library as a key space because ‘the library is actually the one functioning interdisciplinary space on a university campus.’ collaborations with the htrc and digital repositories for working with data also were important to respondents.</p>
                </div>
                <div type=""div2"" rend=""dh-heading2"">
                    <head>teaching and training</head>
                    <p>interviewees mentioned their active efforts and intentions to incorporate computational text analysis into their teaching. some remarked on institutional constraints that make it difficult to incorporate computational tools into curricula. as one respondent explained: ‘i once imagined teaching a class in which students learn to script and actually run analyses against data, but i was told, basically, that that class isn’t a humanities class anymore—that belongs in computer science.’</p>
                    <p>some stated that the courses that they currently teach may not require or allow for the incorporation of computational analysis. yet others noted that there is only a limited amount of technical or scientific skills that a humanities student could realistically master within a short period of time, with one interviewee noting that ‘you can only get people to learn so much about the math; as much as they can learn, they should — at the same time, it’s hard.’</p>
                    <p> although the demand from students for learning about computational text analysis was, overall, reported to be increasing, some interviewees noted that they are constrained by not only limited resources, but also uncertainty as to how to carry out such activities. one interviewee reported prevailing sentiments that the digital humanities ‘doesn’t even fit anywhere,’ leading to the question of whether ‘there should be a whole separate department that’s digital humanities,’ or to offer training within existing curricula.</p>
                </div>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>conclusion</head>
                <p> the immediate aims of this study are to generate an updated framework of user requirements that will guide the development of the htrc’s educational programming and research support services and also to inform forthcoming mellon foundation-funded development of the htrc data capsule. but our preliminary findings also provide insights into scholars’ needs as they increasingly incorporate text analysis in research and teaching. these findings also reveal how digital scholarship centers, information professionals, and providers of digitized content can best support scholarship as digital humanities resources evolve.</p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>acknowledgements</head>
                <p>we thank megan senseney, angela courtney, nicholae cline, and leanne mobley for their collaboration in this study. </p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl>
                        <hi rend=""bold"">american council of learned societies. </hi>(2006). 
                        <hi rend=""italic"">our cultural commonwealth: the report of the acls commission on cyberinfrastructure for the humanities and social sciences</hi>. new york: american council of learned societies. 
                        <ref target=""http://www.acls.org/uploadedfiles/publications/programs/our_cultural_commonwealth.pdf"">http://www.acls.org/uploadedfiles/publications/programs/our_cultural_commonwealth.pdf</ref> (accessed 4 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">argamon, s., et al. </hi>(2009). gender, race, and nationality in black drama, 1850-2000: mining differences in language use in authors and their characters. 
                        <hi rend=""italic"">digital humanities quarterly</hi>
                        <hi rend=""bold"">3</hi>(2): http://www.digitalhumanities.org/dhq/vol/3/2/000043/000043.html 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">babeu, a.</hi> (2011). 
                        <hi rend=""italic"">rome wasn't digitized in a day: building a cyberinfrastructure for digital classics.</hi> clir publication, <hi rend=""bold"">150</hi>, washington, dc: council of library and information resources. 
                        <ref target=""http://www.clir.org/pubs/reports/pub150/reports/pub150/pub150.pdf"">http://www.clir.org/pubs/reports/pub150/reports/pub150/pub150.pdf</ref> (accessed 4 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">brockman, w. s., et al.</hi> (2001). 
                        <hi rend=""italic"">scholarly work in the humanities and the evolving information environment</hi> clir publication,<hi rend=""bold""> 104</hi>, washington, d.c.: digital library federation, council on library and information resources. 
                        <ref target=""http://www.clir.org/pubs/reports/pub104/pub104.pdf"">http://www.clir.org/pubs/reports/pub104/pub104.pdf</ref> (accessed 4 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">fenlon, k., et al. </hi>(2014). scholar-built collections: a study of user requirements for research in large-scale digital libraries. 
                        <hi rend=""italic"">proceedings of the american society for information science and technology</hi>,
                        <hi rend=""bold"">51</hi>(1): 1–10.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">frischer, b., et al.</hi> (2006). 
                        <hi rend=""italic"">summit on digital tools for the humanities: report on summit accomplishments</hi>. institute for advanced technology in the humanities, university of virginia. 
                        <ref target=""http://www.iath.virginia.edu/dtsummit/summittext.pdf"">http://www.iath.virginia.edu/dtsummit/summittext.pdf</ref> (accessed 4 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">gibbs, f. and owens, t.</hi> (2012). building better digital humanities tools: toward broader audiences and user-centered designs. 
                        <hi rend=""italic"">digital humanities quarterly</hi>,
                        <hi rend=""bold"">6</hi>(2).
                        <ref target=""http://www.digitalhumanities.org/dhq/vol/6/2/000136/000136.html"">http://www.digitalhumanities.org/dhq/vol/6/2/000136/000136.html</ref> (accessed 4 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">green, h. and courtney, a.</hi> (2015). beyond the scanned image: a needs assessment of scholarly users of digital collections. 
                        <hi rend=""italic"">college and research libraries</hi>,
                        <hi rend=""bold"">76</hi>(5): 690-707.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">green, h. e., et al.</hi>, (2014). using collections and worksets in large-scale corpora: preliminary findings from the workset creation for scholarly analysis prototyping project. <hi rend=""italic"">poster presented at iconference 2014</hi>, berlin, germany.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">hindley, m. </hi>(2013). the rise of the machines: neh and the digital humanities: the early years. 
                        <hi rend=""italic"">humanities</hi>,
                        <hi rend=""bold"">34</hi>(4).
                        <ref target=""http://www.neh.gov/humanities/2013/julyaugust/feature/the-rise-the-machines"">http://www.neh.gov/humanities/2013/julyaugust/feature/the-rise-the-machines</ref> (accessed 4 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">horton, r., et al.</hi> (2009). mining eighteenth century ontologies: machine learning and knowledge classification in the encyclopédie. 
                        <hi rend=""italic"">digital humanities quarterly</hi>,
                        <hi rend=""bold"">3</hi>(2): http://www.digitalhumanities.org/dhq/vol/3/2/000044/000044.html 
                        <lb/>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">leblanc, m. d., et al.</hi> (2013). lexomics: integrating the research and teaching spaces. 
                        <hi rend=""italic"">digital humanities 2013 conference abstracts, university of nebraska–lincoln, 16-19 july 2013</hi>. lincoln, ne: association of digital humanities organizations, pp. 274-76. http://dh2013.unl.edu/abstracts/ab-293.html (accessed 4 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">muralidharan, a. and hearst, m. a. </hi>
                        <hi rend=""background(white)"">(2013). supporting exploratory text analysis in literature study.</hi>
                        <hi rend=""italic"">literary and linguistic computing</hi>,
                        <hi rend=""bold"">28</hi>(2): 283-95. 10.1093/llc/fqs044.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">palmer, c. l. and neumann, l. j.</hi> (2002). the information work of interdisciplinary humanities scholars: exploration and translation.
                        <hi rend=""italic""> library quarterly</hi>
                        <hi rend=""bold"">7</hi>(1): 85-117.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">rockwell, g., et al.</hi> (2010). ubiquitous text analysis. 
                        <hi rend=""italic"">poetess archive journal</hi>, <hi rend=""bold"">1</hi>(2).
                        <ref target=""https://journals.tdl.org/paj/index.php/paj/article/view/13"">https://journals.tdl.org/paj/index.php/paj/article/view/13</ref> (accessed 4 march 2016).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">rutner, j. and schonfeld, r.</hi> (2012). 
                        <hi rend=""italic"">supporting the changing research practices of historians.</hi> new york: ithaka s+r. http://sr.ithaka.org/?p=22532
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">sukovic, s. </hi>(2011). e-texts in research projects in the humanities, a. woodsworth and w. d. penniman (eds.), 
                        <hi rend=""italic"">advances in librarianship</hi>. bingley, uk: emerald group publishing, pp. 131-202.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">toms, e. g. and o’brien, h.</hi> (2008). understanding the information and communication technology needs of the e-humanist. 
                        <hi rend=""italic"">journal of documentation</hi>,
                        <hi rend=""bold"">64</hi>(1): 102-30.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">unsworth, j.</hi> (2011). computational work with very large text collections: interoperability, sustainability, and the tei. 
                        <hi rend=""italic"">journal of the text encoding initiative</hi>
                        <hi rend=""bold"">1</hi>, 10.4000/jtei.215 (accessed 4 march 2016). 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">zorich, d. </hi>(2012). 
                        <hi rend=""italic"">transitioning to a digital world: art history, its research centers and digital scholarship: a report to the samuel h. kress foundation and the roy rosenzweig center for history and new media</hi>. new york: samuel h. kress foundation. http://www.kressfoundation.org/research/default.aspx?id=35379 (accessed 4 march 2016).
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
",1.0,1.0,Voyant
2720,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,Early English Books in Context: Towards a History of the Technological Humanities,,Daniel James Powell,"paper, specified ""long paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>Writing in 
                <hi rend=""italic"">Literary and Linguistic Computing</hi>, Julianne Nyhan et al argue that “without a better understanding—a more appropriate term might be “body of interpretations”—of the near and distant history of computing in the humanities, we are condemned to repeat the revolutionary trope 
                <hi rend=""italic"">ad infinitum</hi>.” (Nyhan, Flinn, and Welsh, 2013). Willard McCarty, amplifying this, writes that “rather than hypnotizing ourselves with supposedly unprecedented marvels, we must learn to see computing in its historical and social contexts, and so be able to ground modelling in something larger than itself. For computing to be 
                <hi rend=""italic"">of</hi> the humanities as well as 
                <hi rend=""italic"">in</hi> them, we must get beyond catalogues, chronologies, and heroic firsts to a genuine history. There are none yet.” (McCarty, 2008). 
            </p>
            <p>Susan Hockey wrote in 
                <hi rend=""italic"">Companion to Digital Humanities</hi> that “humanities computing has a very well-known beginning,” by which she means the decades-long collaboration between Father Roberto Busa and IBM to create a concordance of the work of Thomas Aquinas (Hockey, 2004). This is the heroic first, which Busa, writing in the forward of that same volume, summed up with admirable brevity: “During the World War II, between 1941 and 1946, I began to look for machines for the automation of the linguistic analysis of written texts. I found them, in 1949, at IBM in New York City.”
                <hi rend=""superscript"">_</hi> This narrative is familiar to many of those working in digital humanities today, and has become openly accepted as the standard historical background for, first, humanities computing and, subsequently, the digital humanities writ large. 
            </p>
            <p>This presentation aims to upset this easy narrative by re-situating the history of one type of digital humanities project—Early English Books Online—as one chapter in an overall history of a technological humanities. That history—the history of a technological humanities—the story of how academics have deployed technology to better understand human creations, especially in textual form—or to understand and explore texts—did not begin in 1949. The creation of digital humanities, radiating outward from those early years, is surely part of the larger story of how technology and text have come together and drifted apart over many centuries. I claim that there is a great deal more continuity in the apparatuses, in the knowledge infrastructure of the humanities than we put forward in our “official” histories. In our search for a neat disciplinary history, we elide technology as a whole with the digital electronic computer. Busa’s project likely does represent the beginning of one type of computational textual processing. It bears remembering, however, that his goal was to create a concordance, a type of reference tool and interface in existence in Western Europe since at least the 13th century. What is the history of 
                <hi rend=""italic"">this</hi> type of textual processing in the intervening six hundred years? 
            </p>
            <p>Instead of a history of tools for textual work beginning with the rise of humanities computing and moving forward to the present day, I hope to juxtapose a different narrative, one that troubles the rhetoric of a textual digital humanities that arises from a clear break with what came before. I hope to, perhaps polemically, test the boundaries of histories of digital humanities by considering an equally technologically sophisticated pre-digital humanities. Such a reframing opens many avenues of inquiry, including a consideration of Linked Open Data in the context of cooperative cataloguing practices from the 19th & 20th centuries, or contemporary textual analysis tools such as Voyant alongside the imposing machinery of an electromechanical Hinman Collator. This presentation, however, will highlight particularly those technologies of textual reproduction developed prior to the oft-quoted originary moment of 1949. Drawing on the history of Early English Books Online (EEBO), I argue that while a 
                <hi rend=""italic"">computational </hi>humanities may indeed be limited to the last half-century, the 
                <hi rend=""italic"">technological </hi>humanities—in both materialist and cultural senses—have a much longer history. 
            </p>
            <p>ProQuest introduces the resource on their front page: </p>
            <quote>From the first book printed in English by William Caxton, through the age of Spenser and Shakespeare and the tumult of the English Civil War, Early English Books Online (EEBO) will contain over 125,000 titles listed in Pollard and Redgrave's Short-Title Catalogue (1475-1640), Wing's Short-Title Catalogue (1641-1700), the Thomason Tracts (1640-1661), and the Early English Tract Supplement - all in full digital facsimile from the Early English Books microfilm collection.
                <hi rend=""superscript"">_</hi>
                <note place=""foot"" xml:id=""ftn1"" n=""1"">
                    <p> This text was current as of summer 2015 and is available in cached form. Since that time, Proquest has altered the front page description of EEBO to the following: </p>
                    <quote>Early English Books Online (EEBO) contains digital facsimile page images of virtually every work printed in England, Ireland, Scotland, Wales and British North America and works in English printed elsewhere from 1473-1700 - from the first book printed in English by William Caxton, through the age of Spenser and Shakespeare and the tumult of the English Civil War. </quote>
                    <p>Strangely, this newer version eliminates reference to the Early English Books microfilm Collection, as well as collapsing a number of distinct early modern collections of content into what might be called the EEBO brand. See the current version of <http://eebo.chadwyck.com/home> and a cached version <https://web.archive.org/web/20150905141338/http://eebo.chadwyck.com/home> from September 2015.</p>
                </note>
            </quote>
            <p>In practice, this means that users are able to view the metadata for a given text; view page images of the original, early modern books in TIFF or PDF format; and, where available, view a full text transcription of the volume that are derived from the EEBO - Text Creation Partnership. Efforts to microfilm early English books began in 1931, intensified as World War II loomed, and continue today. Digital images of these microfilmed documents were made (and are still being made) available online first in 1998. The printed 
                <hi rend=""italic"">Short Title Catalogue </hi>(itself published in 1926) has determined what objects were photographed and, subsequently, scanned and put online
                <hi rend=""superscript"">_</hi>(EEBO).
            </p>
            <p>The history of EEBO crosses multiple media, was directly impacted by global war, involves private companies and public universities, and is both analog and digital. To bracket EEBO (or EEBO-TCP) as a only a digital project impoverishes our understanding of how digital technologies have impacted the reproduction, preservation, and use of texts in humanistic scholarship. To write the full history of the early English books project, Early English Books Online, the Early English Books Online Text Creation Partnership is to engage in an act of disciplinary archaeology, one that forces digital humanists to grapple with the pre-digital origins and ideologies that inflect contemporary digital resources undergirding scholarship. EEBO is a microcosm through which one body of interpretations of digital humanities might be seen. </p>
            <p>As much as it is a history, this presentation is also engaged in answering claims by Alan Liu and Tara McPherson, amongst others, that digital humanities has chosen disciplinarily to disengage from socio-critical questions.
                <hi rend=""superscript"">_</hi>
                <note place=""foot"" xml:id=""ftn2"" n=""2"">
                    <p> For work by Liu on this topic, see “Where is the Cultural Criticism in the Digital Humanites?,” published in 
                        <hi rend=""italic"">Debates in the Digital Humaniteies</hi>, ed Matthew K. Gold <http://dhdebates.gc.cuny.edu/debates/text/20>. For McPherson work on UNIX and ideologies of race, see “Why are the Digital Humanities So White? or Thinking the Histories of Race and Computation” in the same volume <http://dhdebates.gc.cuny.edu/debates/text/29>.
                    </p>
                </note> Thinking through the history of EEBO is one way to approach the digital humanities as a discipline tied to war-driven technological development; the uneasy relationships between private-sector providers and our shared cultural heritage; or the many varieties of labour that are imbricated within the knowledge infrastructures humanists use day in and day out.
                <hi rend=""superscript"">_</hi>
                <note place=""foot"" xml:id=""ftn3"" n=""3"">
                    <p> It is worth noting that one of the very few publications to deal with the EEBO set of projects in this way was published in 
                        <hi rend=""italic"">Literary and Linguistic Computing </hi>(now 
                        <hi rend=""italic"">Digital Scholarship in the Humanities</hi>). See Diana Kichuk, “Metamorphosis: Remediation in 
                        <hi rend=""italic"">Early English Books Online (EEBO)</hi>” (2007) 22 (3): 291-303. DOI: http://dx.doi.org/10.1093/llc/fqm018. Kichuk’s efforts have helped establish a historical framework for this discussion; this presentation seeks to contextualise the facts she has brought together and extend their relevance into discourses about DH as a whole.
                    </p>
                </note> Blending media analysis, historical perspectives, and in-depth knowledge of humanities research tools, I hope to question the boundaries of what we consider digital humanities to be, how we write our histories, and how we move forward.
            </p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl><hi rend=""bold"">About EEBO.</hi> 
                        <hi rend=""italic"">Early English Books Online</hi>. <http://eebo.chadwyck.com/marketing/about.htm>
                    </bibl>
                    <bibl><hi rend=""bold"">Hockey, S.</hi> (2004). The History of Humanities Computing. 
                        <hi rend=""italic"">A Companion to Digital Humanities</hi>. (Eds.) Susan Schreibman, Ray Siemens, and John Unsworth. Oxford: Blackwell. 
                    </bibl>
                    <bibl><hi rend=""bold"">McCarty, W.</hi> (2008). What’s going on? 
                        <hi rend=""italic"">Literary and Linguistic Computing</hi>, <hi rend=""bold"">23</hi>(3): 253-61, doi: 10.1093/llc/fqn014.
                    </bibl>
                    <bibl><hi rend=""bold"">Nyhan, J., Flinn, A. and Welsh, A.</hi> (2013). Oral History and the Hidden: Histories project: towards histories of computing in the humanities. 
                        <hi rend=""italic"">Literary and Linguistic Computing</hi>, doi: 10.1093/llc/fqt044. 
                    </bibl>
                    <bibl><hi rend=""bold"">Svensson, P.</hi> (2009). Humanities Computing as Digital Humanities. 
                        <hi rend=""italic"">Digital Humanities Quarterly</hi>, <hi rend=""bold"">3</hi>(3). http://digitalhumanities.org/dhq/vol/3/3/000065/000065.html.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,eebo;history;media archaeology;text,English,digitisation - theory and practice;history of humanities computing/digital humanities;media studies;renaissance studies,2016-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>writing in 
                <hi rend=""italic"">literary and linguistic computing</hi>, julianne nyhan et al argue that “without a better understanding—a more appropriate term might be “body of interpretations”—of the near and distant history of computing in the humanities, we are condemned to repeat the revolutionary trope 
                <hi rend=""italic"">ad infinitum</hi>.” (nyhan, flinn, and welsh, 2013). willard mccarty, amplifying this, writes that “rather than hypnotizing ourselves with supposedly unprecedented marvels, we must learn to see computing in its historical and social contexts, and so be able to ground modelling in something larger than itself. for computing to be 
                <hi rend=""italic"">of</hi> the humanities as well as 
                <hi rend=""italic"">in</hi> them, we must get beyond catalogues, chronologies, and heroic firsts to a genuine history. there are none yet.” (mccarty, 2008). 
            </p>
            <p>susan hockey wrote in 
                <hi rend=""italic"">companion to digital humanities</hi> that “humanities computing has a very well-known beginning,” by which she means the decades-long collaboration between father roberto busa and ibm to create a concordance of the work of thomas aquinas (hockey, 2004). this is the heroic first, which busa, writing in the forward of that same volume, summed up with admirable brevity: “during the world war ii, between 1941 and 1946, i began to look for machines for the automation of the linguistic analysis of written texts. i found them, in 1949, at ibm in new york city.”
                <hi rend=""superscript"">_</hi> this narrative is familiar to many of those working in digital humanities today, and has become openly accepted as the standard historical background for, first, humanities computing and, subsequently, the digital humanities writ large. 
            </p>
            <p>this presentation aims to upset this easy narrative by re-situating the history of one type of digital humanities project—early english books online—as one chapter in an overall history of a technological humanities. that history—the history of a technological humanities—the story of how academics have deployed technology to better understand human creations, especially in textual form—or to understand and explore texts—did not begin in 1949. the creation of digital humanities, radiating outward from those early years, is surely part of the larger story of how technology and text have come together and drifted apart over many centuries. i claim that there is a great deal more continuity in the apparatuses, in the knowledge infrastructure of the humanities than we put forward in our “official” histories. in our search for a neat disciplinary history, we elide technology as a whole with the digital electronic computer. busa’s project likely does represent the beginning of one type of computational textual processing. it bears remembering, however, that his goal was to create a concordance, a type of reference tool and interface in existence in western europe since at least the 13th century. what is the history of 
                <hi rend=""italic"">this</hi> type of textual processing in the intervening six hundred years? 
            </p>
            <p>instead of a history of tools for textual work beginning with the rise of humanities computing and moving forward to the present day, i hope to juxtapose a different narrative, one that troubles the rhetoric of a textual digital humanities that arises from a clear break with what came before. i hope to, perhaps polemically, test the boundaries of histories of digital humanities by considering an equally technologically sophisticated pre-digital humanities. such a reframing opens many avenues of inquiry, including a consideration of linked open data in the context of cooperative cataloguing practices from the 19th & 20th centuries, or contemporary textual analysis tools such as voyant alongside the imposing machinery of an electromechanical hinman collator. this presentation, however, will highlight particularly those technologies of textual reproduction developed prior to the oft-quoted originary moment of 1949. drawing on the history of early english books online (eebo), i argue that while a 
                <hi rend=""italic"">computational </hi>humanities may indeed be limited to the last half-century, the 
                <hi rend=""italic"">technological </hi>humanities—in both materialist and cultural senses—have a much longer history. 
            </p>
            <p>proquest introduces the resource on their front page: </p>
            <quote>from the first book printed in english by william caxton, through the age of spenser and shakespeare and the tumult of the english civil war, early english books online (eebo) will contain over 125,000 titles listed in pollard and redgrave's short-title catalogue (1475-1640), wing's short-title catalogue (1641-1700), the thomason tracts (1640-1661), and the early english tract supplement - all in full digital facsimile from the early english books microfilm collection.
                <hi rend=""superscript"">_</hi>
                <note place=""foot"" xml:id=""ftn1"" n=""1"">
                    <p> this text was current as of summer 2015 and is available in cached form. since that time, proquest has altered the front page description of eebo to the following: </p>
                    <quote>early english books online (eebo) contains digital facsimile page images of virtually every work printed in england, ireland, scotland, wales and british north america and works in english printed elsewhere from 1473-1700 - from the first book printed in english by william caxton, through the age of spenser and shakespeare and the tumult of the english civil war. </quote>
                    <p>strangely, this newer version eliminates reference to the early english books microfilm collection, as well as collapsing a number of distinct early modern collections of content into what might be called the eebo brand. see the current version of <http://eebo.chadwyck.com/home> and a cached version <https://web.archive.org/web/20150905141338/http://eebo.chadwyck.com/home> from september 2015.</p>
                </note>
            </quote>
            <p>in practice, this means that users are able to view the metadata for a given text; view page images of the original, early modern books in tiff or pdf format; and, where available, view a full text transcription of the volume that are derived from the eebo - text creation partnership. efforts to microfilm early english books began in 1931, intensified as world war ii loomed, and continue today. digital images of these microfilmed documents were made (and are still being made) available online first in 1998. the printed 
                <hi rend=""italic"">short title catalogue </hi>(itself published in 1926) has determined what objects were photographed and, subsequently, scanned and put online
                <hi rend=""superscript"">_</hi>(eebo).
            </p>
            <p>the history of eebo crosses multiple media, was directly impacted by global war, involves private companies and public universities, and is both analog and digital. to bracket eebo (or eebo-tcp) as a only a digital project impoverishes our understanding of how digital technologies have impacted the reproduction, preservation, and use of texts in humanistic scholarship. to write the full history of the early english books project, early english books online, the early english books online text creation partnership is to engage in an act of disciplinary archaeology, one that forces digital humanists to grapple with the pre-digital origins and ideologies that inflect contemporary digital resources undergirding scholarship. eebo is a microcosm through which one body of interpretations of digital humanities might be seen. </p>
            <p>as much as it is a history, this presentation is also engaged in answering claims by alan liu and tara mcpherson, amongst others, that digital humanities has chosen disciplinarily to disengage from socio-critical questions.
                <hi rend=""superscript"">_</hi>
                <note place=""foot"" xml:id=""ftn2"" n=""2"">
                    <p> for work by liu on this topic, see “where is the cultural criticism in the digital humanites?,” published in 
                        <hi rend=""italic"">debates in the digital humaniteies</hi>, ed matthew k. gold <http://dhdebates.gc.cuny.edu/debates/text/20>. for mcpherson work on unix and ideologies of race, see “why are the digital humanities so white? or thinking the histories of race and computation” in the same volume <http://dhdebates.gc.cuny.edu/debates/text/29>.
                    </p>
                </note> thinking through the history of eebo is one way to approach the digital humanities as a discipline tied to war-driven technological development; the uneasy relationships between private-sector providers and our shared cultural heritage; or the many varieties of labour that are imbricated within the knowledge infrastructures humanists use day in and day out.
                <hi rend=""superscript"">_</hi>
                <note place=""foot"" xml:id=""ftn3"" n=""3"">
                    <p> it is worth noting that one of the very few publications to deal with the eebo set of projects in this way was published in 
                        <hi rend=""italic"">literary and linguistic computing </hi>(now 
                        <hi rend=""italic"">digital scholarship in the humanities</hi>). see diana kichuk, “metamorphosis: remediation in 
                        <hi rend=""italic"">early english books online (eebo)</hi>” (2007) 22 (3): 291-303. doi: http://dx.doi.org/10.1093/llc/fqm018. kichuk’s efforts have helped establish a historical framework for this discussion; this presentation seeks to contextualise the facts she has brought together and extend their relevance into discourses about dh as a whole.
                    </p>
                </note> blending media analysis, historical perspectives, and in-depth knowledge of humanities research tools, i hope to question the boundaries of what we consider digital humanities to be, how we write our histories, and how we move forward.
            </p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl><hi rend=""bold"">about eebo.</hi> 
                        <hi rend=""italic"">early english books online</hi>. <http://eebo.chadwyck.com/marketing/about.htm>
                    </bibl>
                    <bibl><hi rend=""bold"">hockey, s.</hi> (2004). the history of humanities computing. 
                        <hi rend=""italic"">a companion to digital humanities</hi>. (eds.) susan schreibman, ray siemens, and john unsworth. oxford: blackwell. 
                    </bibl>
                    <bibl><hi rend=""bold"">mccarty, w.</hi> (2008). what’s going on? 
                        <hi rend=""italic"">literary and linguistic computing</hi>, <hi rend=""bold"">23</hi>(3): 253-61, doi: 10.1093/llc/fqn014.
                    </bibl>
                    <bibl><hi rend=""bold"">nyhan, j., flinn, a. and welsh, a.</hi> (2013). oral history and the hidden: histories project: towards histories of computing in the humanities. 
                        <hi rend=""italic"">literary and linguistic computing</hi>, doi: 10.1093/llc/fqt044. 
                    </bibl>
                    <bibl><hi rend=""bold"">svensson, p.</hi> (2009). humanities computing as digital humanities. 
                        <hi rend=""italic"">digital humanities quarterly</hi>, <hi rend=""bold"">3</hi>(3). http://digitalhumanities.org/dhq/vol/3/3/000065/000065.html.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>
",1.0,1.0,Voyant
2787,2016 - Kraków,Kraków,Digital Identities: the Past and the Future,2016,ADHO,ADHO,Jagiellonian University;Pedagogical University of Krakow,Kraków,,Poland,https://dh2016.adho.org/,The Digital Émigré: Russian Periodical Studies and DH in the Slavic Fields,,Natalia Ermolaev;Philip Gleissner,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>Digital Humanities has seen slow adoption in the Slavic language and literature fields in North American academia. This issue frames our project, the Digital Émigré, a digital resource for exploring Russian émigré periodical literature. Our project has a threefold aim. As periodical studies scholars, we want to enable access to Russian émigré journals for new audiences. As digital humanists, we believe that DH tools and methodologies can facilitate new forms of knowledge about twentieth-century Russian, and more broadly diaspora, literary and cultural history. Finally, as Slavists, we hope our project will be a hub for discussion about the applicability of DH theory and practice for scholars working with Russian-language material.</p>
            <p>At this pilot stage, Digital Émigré is a web-based searchable database of article-level metadata of Russian-language journals published outside of Russia in the twentieth century. Our pilot contains four titles (approximately 100 issues and 1,500 articles): 
                <hi rend=""italic"">Novoselye</hi> and 
                <hi rend=""italic"">Novyi zhurnal</hi> were published in the 1940s in New York, and 
                <hi rend=""italic"">Sintaksis</hi> and 
                <hi rend=""italic"">Kontinent</hi>, in the late 1970s and 1980s in Paris. Our pilot site provides insight into literary culture at both the beginning and end of the Cold War, bookending the twentieth-century Russian diaspora experience. Digital Émigré is intended to scale, and will eventually contain additional titles and new functionality. 
            </p>
            <p>We will highlight the main scholarly avenues that DH methods allow us investigate, such as mapping networks of co-publication, tracking evolving political, social and cultural concerns of émigrés over the course of the Cold War, demonstrating the increased opportunities for émigré women as editors and contributors, and highlighting the proportion of original vs. re-printed work in émigré publications. This way, our project encourages experimentation that will enrich the study of Slavic periodical culture: accessing journals through their data can challenge narratives that are often framed by retroactive canonization, close reading and focus on individual authors. Digital Émigré thereby bridges philological approaches and sociological questions about intellectual networks and communities of artistic production. </p>
            <p>The poster address the project’s core technical design: our strategy for data modeling and management and database design.  We will also present our plans for next steps, which is to provide full-text access and to federate our titles with other digital periodical collections. For this, we are designing a TEI schema modeled on major periodical studies digital collections -  specifically the Blue Mountain Project at Princeton University (
                <ref target=""http://bluemountain.princeton.edu"">http://bluemountain.princeton.edu</ref> and the Yellow 90’s Online at Ryerson University (
                <ref target=""http://www.1890s.ca"">http://www.1890s.ca</ref>)  
            </p>
            <p>We will also discuss the specific challenges of working with Russian language material and Cyrillic script, such as character encoding, transliteration, translation, and  tokenizing and stemming. These issues can be barriers to success when working with popular DH tools that are developed primarily for Western scripts and languages, and we will show our solutions for using some well-known tools for: data normalization (OpenRefine), text analysis (Voyant), network analysis (Gephi), visualization (Raw, Palladio), and topic modeling (MALLET).</p>
            <p>Digital Émigré is committed not only to the exploration of the intellectual experience of diaspora cultural life. As a digital humanities project, it is itself invested in building intellectual communities around the engagement with this material and its afterlife. It aims to foster contact between scholars working with Russian and other Slavic languages internationally, especially through the discussion of issues of interoperability and creating multilingual digital research environments.</p>
        </body>
    </text>
",xml,This text is republished here with permission from the original rights holder.,,literary studies;project design;russian;slavic studies,English,"archives, repositories, sustainability and preservation;digital humanities - multilinguality;encoding - theory and practice;literary studies;multilingual / multicultural approaches;philology;project design, organization, management",2016-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"">
        <body>
            <p>digital humanities has seen slow adoption in the slavic language and literature fields in north american academia. this issue frames our project, the digital émigré, a digital resource for exploring russian émigré periodical literature. our project has a threefold aim. as periodical studies scholars, we want to enable access to russian émigré journals for new audiences. as digital humanists, we believe that dh tools and methodologies can facilitate new forms of knowledge about twentieth-century russian, and more broadly diaspora, literary and cultural history. finally, as slavists, we hope our project will be a hub for discussion about the applicability of dh theory and practice for scholars working with russian-language material.</p>
            <p>at this pilot stage, digital émigré is a web-based searchable database of article-level metadata of russian-language journals published outside of russia in the twentieth century. our pilot contains four titles (approximately 100 issues and 1,500 articles): 
                <hi rend=""italic"">novoselye</hi> and 
                <hi rend=""italic"">novyi zhurnal</hi> were published in the 1940s in new york, and 
                <hi rend=""italic"">sintaksis</hi> and 
                <hi rend=""italic"">kontinent</hi>, in the late 1970s and 1980s in paris. our pilot site provides insight into literary culture at both the beginning and end of the cold war, bookending the twentieth-century russian diaspora experience. digital émigré is intended to scale, and will eventually contain additional titles and new functionality. 
            </p>
            <p>we will highlight the main scholarly avenues that dh methods allow us investigate, such as mapping networks of co-publication, tracking evolving political, social and cultural concerns of émigrés over the course of the cold war, demonstrating the increased opportunities for émigré women as editors and contributors, and highlighting the proportion of original vs. re-printed work in émigré publications. this way, our project encourages experimentation that will enrich the study of slavic periodical culture: accessing journals through their data can challenge narratives that are often framed by retroactive canonization, close reading and focus on individual authors. digital émigré thereby bridges philological approaches and sociological questions about intellectual networks and communities of artistic production. </p>
            <p>the poster address the project’s core technical design: our strategy for data modeling and management and database design.  we will also present our plans for next steps, which is to provide full-text access and to federate our titles with other digital periodical collections. for this, we are designing a tei schema modeled on major periodical studies digital collections -  specifically the blue mountain project at princeton university (
                <ref target=""http://bluemountain.princeton.edu"">http://bluemountain.princeton.edu</ref> and the yellow 90’s online at ryerson university (
                <ref target=""http://www.1890s.ca"">http://www.1890s.ca</ref>)  
            </p>
            <p>we will also discuss the specific challenges of working with russian language material and cyrillic script, such as character encoding, transliteration, translation, and  tokenizing and stemming. these issues can be barriers to success when working with popular dh tools that are developed primarily for western scripts and languages, and we will show our solutions for using some well-known tools for: data normalization (openrefine), text analysis (voyant), network analysis (gephi), visualization (raw, palladio), and topic modeling (mallet).</p>
            <p>digital émigré is committed not only to the exploration of the intellectual experience of diaspora cultural life. as a digital humanities project, it is itself invested in building intellectual communities around the engagement with this material and its afterlife. it aims to foster contact between scholars working with russian and other slavic languages internationally, especially through the discussion of issues of interoperability and creating multilingual digital research environments.</p>
        </body>
    </text>
",1.0,1.0,Voyant
3775,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Digital Religion – Digital Theology,https://dh2017.adho.org/abstracts/021/021.pdf,Claire Clivaz;Emily S. Clark;Paul Dilley;Katherine Mary Faull;Rachel McBride-Lindsey;Peter Phillips,panel / roundtable,"Introduction
Scholarly discourse evaluating the digital turn in biblical and religious studies is at an early stage in its development, as attested to by the creation of two new

book series in 2016: Introduction to Digital Humanities: Religion (IDH, de Gruyter), and Digital Biblical Studies (DBS, Brill). Previously, Heidi Campbell published an overview of the topic (Campbell 2013), developed in further publications (Campbell-Althenho-fen 2015, Campbell-Garner 2016). In a recent overview, Carrie Schroeder develops two central questions on the topic: “what does it mean for Biblical Studies to be marginal to the Digital Humanities when DH is a field positioning itself as transformative for the humanities? How can our expertise in Biblical Studies influence and shape Digital Humanities for the better?” (Schroeder 2016). Using her field, Coptic studies, as an example she shows that the particular skills and needs of a marginal field within a marginal field can be a strong driver in DH.

Consequently, and for the first time at a DH meeting, this ninety-minute panel session asks what is the impact of the digital turn on religious studies and theology, and to what extent these somewhat marginal fields can bring something specific to the big DH tent. They particularly focus on textuality and on the symbolic impact of the “book” as attested to in the expression, “religions of the book,” coined in a programmatic lecture given in 1870 by F. Max Müller (2010). The symbolic, Western impact of books and writing was amplified by this notion, born at the time when the legal status of printed texts and authorship was completely secured in Western culture (Clivaz 2012).

For centuries, “books were perceived as a ‘wide angle' from which it was possible for everything to be observed, related to, and perhaps even decided” (Carrière-Eco 2009). The panel will consequently consider the hypothesis that the DH have been deeply influenced by this fascination with textuality and books during the first decades of their development; while keeping “the discourse of written texts” as a central pillar to the discussion according to the words of Roberto Busa, a foundational DH figure (Busa 2004). Busa's relationship to Biblical and religious materials has played a role in his approach to the computing field, as Jones point out (Jones 2016). The double impact of the book and the notion of “religions of the book”, successful in Western culture since the 19th century, provides an opening to understanding why DH in religious fields is still so focused on textuality. Indeed, when we collect examples of DH studies in diverse religious fields, we are unsurprisingly faced with very textual DH (Clivaz et al. 2016e). This observation

strengthens the necessity for religions in DH to consider the multimodal and multicultural turn provoked by digital culture.

With these different questions in mind, five panelists will participate in the presentations (sixty minutes in total) and a thirty-minute panel discussion that will be moderated by Claire Clivaz representing the Swiss Institute of Bioinformatics, Vital-IT (Lausanne, CH). The following five speakers have agreed to participate and to discuss the general topic from the perspectives of their own research projects. In alphabetical order:

A Neophyte Proselytizes for Digital Humanities Pedagogy
Emily S. Clark
This presentation explores the ways in which Digital Humanities can enhance a Religious Studies classroom by focusing on two assignments that ask new questions of traditional course materials. The first is a project that was the culmination of a month's work collaboratively amongst a class of 25 students with a database platform (Omeka). This project entailed the digitization of archival photographs of a Native American community from 1916, along with the reading of Jesuit mission material (Clark et al. 2016). The second is an assignment that took two class periods and introduced students to data visualization (Voyant). This assignment introduced students to the differences between close reading and distant reading, along with

practicing both on excerpts from Jesuit mission documents (Mentrak - Bucko, 2016).

Topic Modeling the Bible
Paul Dilley
The talk will present the first full-scale topic model of the Bible and related literature in four different languages: Greek, Latin, Syriac and English. It will discuss both technical aspects of the process (e.g., the use or not of lemmatization; retention or removal of function words; optimal number of topics), as well as what we gain from comparing topic models of the same corpus translated into different languages. The presentation will focus on the interpretive gains and losses involved in topic modeling, one of the richest strategies of distant reading to the Bible which has been the subject of centuries of minute examination of the close reading tradition which Moretti has pointedly labeled a “theological exercise” (Moretti 2013).

Digital Lives: Reading Moravian Memoirs in the Age of the Internet
Katherine M. Faull
An international collaborative research project (USA, Sweden, Germany) is developing a digital platform for the investigation of the metadata and text of Moravian memoirs, composed since the mid-18th century by members of the Moravian Church to be read at their funeral (over 65,000 memoirs, housed in Germany and the US, Faull 1997). Less than 10% of the earliest manuscripts have been published. The developing digital interface (moravianlives.org) allows for geospatial and chronological visualization of author's birth and death place (Haskins 2007). This paper will investigate the intersection of the digital, the autobiographical, and the sacred in the age of the internet. How can the act of reading the lives of thousands of Moravians also be understood as an act of reconstituting the “invisible church” ? (van Dijk, 2007; Eakin 2014).

Material Religions in a Digital World
Rachel McBride-Lindsey
For much of the modern era, religion and theology have been intertwined in a decidedly material world. Over the last several decades, students of religion have begun to carve out intellectual headroom for an approach to material culture that recognizes objects and images as generative sources of theological inquiry and religious practice. Cultural institutions can be an effective tool for inviting researchers and the public into physical spaces and into contact with deeper dimensions of the material world. At the same time, these very contributions work against methodological gains in the study of material culture. Rachel McBride-Lindsay's presentation starts with this tension and draws from pedagogical attempts to incorporate digital platforms into projects anchored in the study of objects.

Exploring developmental patterns within Digital Theology Research within the Digital Humanities
Peter Phillips
Campbell and Altenhofen (2015) explore four waves in digital research development in theology and religion back into the late twentieth century. Their wave pattern picks up both historical and technological trends and patterns in research. However, a three wave theory dominates discussion within introductions to the Digital Humanities, discussed by David Berry (2011) and in the Digital Humanities Manifesto 2.0. It tends to reflect modes of research, or groups of methodologies used in research rather than time periods. Reflecting on CODEC's own experience of Digital Theology in association with a range of other scholars, this paper will assess whether too many waves are a problem in our methodological theorizing.

Bibliography
Berry, D (2011), “The computational turn: thinking about

the Digital Humanities”, Culture Machine 12, 1-22.

Busa, R. (2004), “Foreword: Perspectives on the Digital Humanities”, in S. Schreibman, R. Siemens, J. Unsworth (ed.), A Companion to Digital Humanities, Oxford: Blackwell, http://www.digitalhumanities.org/companion/ (Accessed 27 March 2017).

Campbell, H.A (2013) (ed.), Digital Religion. Understanding religious practice in new media worlds, London/ New

York : Routledge.

Campbell, H.A. and Altenhofen, B (2015), “Methodological Challenges, Innovations and Growing Pains in Digital Religion Research”, in Digital Methodologies in the Sociology of Religion, S. Cheruvallil-Contractor - S. Shakkour (eds.), Blumsburry Publishing, Kindle edition.

Campbell, H.A. and Garner, S. (2016), Networked theology.

Negotiating faith in digital culture, Grand Rapids, MA : Baker Academy.

Carrière, J-.C. and Eco, U. (2009), N'espérez pas vous débarrasser des livres, Paris, Seuil.

Clark, E.S. et al. (2016), Digital Jesuits and Ignatian Pedagogy, King Island Collection, Jesuit Oregon Province Archives, Gonzaga University,    http://as-dh.gon-

zaga.edu/omeka/ (Accessed 27 March 2017)

Clivaz, C. (2012a), “Homer and the New Testament as ‘Multitexts’ in the Digital Age ?”, SRC 3/3, 1-15 ; http://src-online.ca/index.php/src/article/view/97 (Accessed 27 March 2017).

Clivaz et al. (eds.) (2016), Digital Humanities in Jewish, Christian and Arabic traditions, special issue JRMDC 5 (2016/1),    https://www.jrmdc.com/journal/is-

sue/view/9 (Accessed 27 March 2017).

Von Dijk, J. (2007), Mediated Memories in the Digital Age. Stanford: Stanford UP.

Moretti, F. (2013), Distant Reading, Verso, London, New York.

Schroeder, C.T. (2016), “The Digital Humanities as Cultural Capital: Implications for Biblical and Religious Studies”, Journal of Religion, Media and Digital Culture 5(1), 21-49, <http://www.jrmdc.com/journal/issue/view/9>    (Ac

cessed 27 March 2017).

Eakin, P. J. (2014), “Autobiography as Cosmogram”, Story-worlds: A Journal of Narrative Studies 6/1: 21-43.

Faull, K. M. (1997), ed. and trans., Moravian Women's Memoirs: Their Related Lives, 1750-1820, Syracuse, NY: Syracuse University Press.

Haskins, E. (2007), “Between Archive and Participation: Public Memory in a Digital Age”, Rhetoric Society Quarterly 37/4, 401-422.

Jones, S. (2016), Roberto Busa, S. J., and the Emergence of Humanities Computing: The Priest and the Punched Cards, Routledge Press, London.

Mentrak, T. and Bucko, R.A. (2016), Jesuit Relations and Allied Documents 1610 to 1791, http://mo-ses.creighton.edu/kripke/jesuitrelations/ (Accessed 27 March 2017).

Müller, F.M. (2010), “Second Lecture Delivered at the Royal Institution, February 26, 1870”, in F. M. Müller, Introduction to the science of religion. Four lectures delivered at the Royal Institution, Seneca Falls, NY: Wilson Press, 5282 (Original lectures February & May 1870).",txt,Creative Commons Attribution 4.0 International,,epistemology;multimodality;religious studies,English,"classical studies;content analysis;corpora and corpus activities;cultural studies;digitisation - theory and practice;diversity;geospatial analysis, interfaces and technology;literary studies;spatio-temporal modeling, analysis and visualisation;text analysis;theology",2017-01-01,"introduction
scholarly discourse evaluating the digital turn in biblical and religious studies is at an early stage in its development, as attested to by the creation of two new

book series in 2016: introduction to digital humanities: religion (idh, de gruyter), and digital biblical studies (dbs, brill). previously, heidi campbell published an overview of the topic (campbell 2013), developed in further publications (campbell-althenho-fen 2015, campbell-garner 2016). in a recent overview, carrie schroeder develops two central questions on the topic: “what does it mean for biblical studies to be marginal to the digital humanities when dh is a field positioning itself as transformative for the humanities? how can our expertise in biblical studies influence and shape digital humanities for the better?” (schroeder 2016). using her field, coptic studies, as an example she shows that the particular skills and needs of a marginal field within a marginal field can be a strong driver in dh.

consequently, and for the first time at a dh meeting, this ninety-minute panel session asks what is the impact of the digital turn on religious studies and theology, and to what extent these somewhat marginal fields can bring something specific to the big dh tent. they particularly focus on textuality and on the symbolic impact of the “book” as attested to in the expression, “religions of the book,” coined in a programmatic lecture given in 1870 by f. max müller (2010). the symbolic, western impact of books and writing was amplified by this notion, born at the time when the legal status of printed texts and authorship was completely secured in western culture (clivaz 2012).

for centuries, “books were perceived as a ‘wide angle' from which it was possible for everything to be observed, related to, and perhaps even decided” (carrière-eco 2009). the panel will consequently consider the hypothesis that the dh have been deeply influenced by this fascination with textuality and books during the first decades of their development; while keeping “the discourse of written texts” as a central pillar to the discussion according to the words of roberto busa, a foundational dh figure (busa 2004). busa's relationship to biblical and religious materials has played a role in his approach to the computing field, as jones point out (jones 2016). the double impact of the book and the notion of “religions of the book”, successful in western culture since the 19th century, provides an opening to understanding why dh in religious fields is still so focused on textuality. indeed, when we collect examples of dh studies in diverse religious fields, we are unsurprisingly faced with very textual dh (clivaz et al. 2016e). this observation

strengthens the necessity for religions in dh to consider the multimodal and multicultural turn provoked by digital culture.

with these different questions in mind, five panelists will participate in the presentations (sixty minutes in total) and a thirty-minute panel discussion that will be moderated by claire clivaz representing the swiss institute of bioinformatics, vital-it (lausanne, ch). the following five speakers have agreed to participate and to discuss the general topic from the perspectives of their own research projects. in alphabetical order:

a neophyte proselytizes for digital humanities pedagogy
emily s. clark
this presentation explores the ways in which digital humanities can enhance a religious studies classroom by focusing on two assignments that ask new questions of traditional course materials. the first is a project that was the culmination of a month's work collaboratively amongst a class of 25 students with a database platform (omeka). this project entailed the digitization of archival photographs of a native american community from 1916, along with the reading of jesuit mission material (clark et al. 2016). the second is an assignment that took two class periods and introduced students to data visualization (voyant). this assignment introduced students to the differences between close reading and distant reading, along with

practicing both on excerpts from jesuit mission documents (mentrak - bucko, 2016).

topic modeling the bible
paul dilley
the talk will present the first full-scale topic model of the bible and related literature in four different languages: greek, latin, syriac and english. it will discuss both technical aspects of the process (e.g., the use or not of lemmatization; retention or removal of function words; optimal number of topics), as well as what we gain from comparing topic models of the same corpus translated into different languages. the presentation will focus on the interpretive gains and losses involved in topic modeling, one of the richest strategies of distant reading to the bible which has been the subject of centuries of minute examination of the close reading tradition which moretti has pointedly labeled a “theological exercise” (moretti 2013).

digital lives: reading moravian memoirs in the age of the internet
katherine m. faull
an international collaborative research project (usa, sweden, germany) is developing a digital platform for the investigation of the metadata and text of moravian memoirs, composed since the mid-18th century by members of the moravian church to be read at their funeral (over 65,000 memoirs, housed in germany and the us, faull 1997). less than 10% of the earliest manuscripts have been published. the developing digital interface (moravianlives.org) allows for geospatial and chronological visualization of author's birth and death place (haskins 2007). this paper will investigate the intersection of the digital, the autobiographical, and the sacred in the age of the internet. how can the act of reading the lives of thousands of moravians also be understood as an act of reconstituting the “invisible church” ? (van dijk, 2007; eakin 2014).

material religions in a digital world
rachel mcbride-lindsey
for much of the modern era, religion and theology have been intertwined in a decidedly material world. over the last several decades, students of religion have begun to carve out intellectual headroom for an approach to material culture that recognizes objects and images as generative sources of theological inquiry and religious practice. cultural institutions can be an effective tool for inviting researchers and the public into physical spaces and into contact with deeper dimensions of the material world. at the same time, these very contributions work against methodological gains in the study of material culture. rachel mcbride-lindsay's presentation starts with this tension and draws from pedagogical attempts to incorporate digital platforms into projects anchored in the study of objects.

exploring developmental patterns within digital theology research within the digital humanities
peter phillips
campbell and altenhofen (2015) explore four waves in digital research development in theology and religion back into the late twentieth century. their wave pattern picks up both historical and technological trends and patterns in research. however, a three wave theory dominates discussion within introductions to the digital humanities, discussed by david berry (2011) and in the digital humanities manifesto 2.0. it tends to reflect modes of research, or groups of methodologies used in research rather than time periods. reflecting on codec's own experience of digital theology in association with a range of other scholars, this paper will assess whether too many waves are a problem in our methodological theorizing.

bibliography
berry, d (2011), “the computational turn: thinking about

the digital humanities”, culture machine 12, 1-22.

busa, r. (2004), “foreword: perspectives on the digital humanities”, in s. schreibman, r. siemens, j. unsworth (ed.), a companion to digital humanities, oxford: blackwell, http://www.digitalhumanities.org/companion/ (accessed 27 march 2017).

campbell, h.a (2013) (ed.), digital religion. understanding religious practice in new media worlds, london/ new

york : routledge.

campbell, h.a. and altenhofen, b (2015), “methodological challenges, innovations and growing pains in digital religion research”, in digital methodologies in the sociology of religion, s. cheruvallil-contractor - s. shakkour (eds.), blumsburry publishing, kindle edition.

campbell, h.a. and garner, s. (2016), networked theology.

negotiating faith in digital culture, grand rapids, ma : baker academy.

carrière, j-.c. and eco, u. (2009), n'espérez pas vous débarrasser des livres, paris, seuil.

clark, e.s. et al. (2016), digital jesuits and ignatian pedagogy, king island collection, jesuit oregon province archives, gonzaga university,    http://as-dh.gon-

zaga.edu/omeka/ (accessed 27 march 2017)

clivaz, c. (2012a), “homer and the new testament as ‘multitexts’ in the digital age ?”, src 3/3, 1-15 ; http://src-online.ca/index.php/src/article/view/97 (accessed 27 march 2017).

clivaz et al. (eds.) (2016), digital humanities in jewish, christian and arabic traditions, special issue jrmdc 5 (2016/1),    https://www.jrmdc.com/journal/is-

sue/view/9 (accessed 27 march 2017).

von dijk, j. (2007), mediated memories in the digital age. stanford: stanford up.

moretti, f. (2013), distant reading, verso, london, new york.

schroeder, c.t. (2016), “the digital humanities as cultural capital: implications for biblical and religious studies”, journal of religion, media and digital culture 5(1), 21-49, <http://www.jrmdc.com/journal/issue/view/9>    (ac

cessed 27 march 2017).

eakin, p. j. (2014), “autobiography as cosmogram”, story-worlds: a journal of narrative studies 6/1: 21-43.

faull, k. m. (1997), ed. and trans., moravian women's memoirs: their related lives, 1750-1820, syracuse, ny: syracuse university press.

haskins, e. (2007), “between archive and participation: public memory in a digital age”, rhetoric society quarterly 37/4, 401-422.

jones, s. (2016), roberto busa, s. j., and the emergence of humanities computing: the priest and the punched cards, routledge press, london.

mentrak, t. and bucko, r.a. (2016), jesuit relations and allied documents 1610 to 1791, http://mo-ses.creighton.edu/kripke/jesuitrelations/ (accessed 27 march 2017).

müller, f.m. (2010), “second lecture delivered at the royal institution, february 26, 1870”, in f. m. müller, introduction to the science of religion. four lectures delivered at the royal institution, seneca falls, ny: wilson press, 5282 (original lectures february & may 1870).",1.0,1.0,Voyant
3777,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Toward Reproducibility in DH Experiments: A Case Study in Search of Edgar Allan Poe’s First Published Work,https://dh2017.adho.org/abstracts/027/027.pdf,Mark D. LeBlanc,"paper, specified ""short paper""","Summary
Reproducing experimental results is a hallmark of empirical investigation and serves both to verify and inspire. This paper is a call for more systematic documentation of computational stylistic experiments. Publishing only summaries of the methods and results of empirical work is an artifact of traditional print media. To facilitate experimental reproducibility and to help the growing community who wish to learn how to apply computational methods and subsequently teach the next generation of scholars, the publication of results must include (i) access to the digitized texts, (ii) a clear workflow and most essentially (iii) the source code that led to each and all of the experimental results. By way of example, we present the steps and process in a GitHub repository for computationally probing the unknown and contested authorship of an 1831 short story entitled “A Dream” as we seek evidence if this work is similar to other attributed works by Edgar Allan Poe. The entire framework is intended as a pedagogical jumpstart for others, especially those new to computational stylometry. If Poe did write the story, it would be his first published work.

Introduction
As the Digital Humanities gains access to a wide array of digitized corpora and matures to a discipline that creatively defines new methods for computationally close and distant readings, a growing gap has emerged between those who apply sophisticated programming, e.g., Stylo In R (Eder et al., 2016) and those who are new to the game and need an introduction to the field. Typical of the community spirit in DH, significant efforts are underway to bridge this gap, including web-based tools for entry-level exploration including

Voyant Tools (Sinclair and Rockwell, 2016) and Lexos (Kleinman et al., 2016) and domain-specific introductions to programming, including Jockers' text (2014) and the Programming Historian (Crymble et al., 2016). This paper attempts to narrow the gap by encouraging both sides to document their experimental methods more fully to embrace previous calls for the replication of experimental methods (Rudman, 2012 et al.) and thereby teach effective practices by “leaving a trail” of experimental methods that enable others to execute and extend.

A Good Mystery: Towards Reproducibility
A GitHub repository or “repo” offers a workflow

that explores whether an 1831 story published under

the attribution of only ‘P' might have been written by Edgar Allan Poe. If so, it would be Poe's first published work. In addition to sharing a set of analytical methods applied in this experiment, the broader methodological-pedagogical goals are two-fold: (i) the dissemination of data and code should be championed as a cornerstone of DH research, thereby facilitating the replication of results and (ii) to share a workflow so that others may apply similar analyses to their texts of interest.

The workflow is stored as a set of numbered folders containing the texts and scripts (code) needed to complete each step. The workflow includes: collecting texts, the preprocessing, tokenization, and culling decisions made, unsupervised cluster analyses (k-means, hierarchical-agglomerative, bootstrap consensus tree), and supervised classification methods using Stylo in R's Delta, SVM, and NSC models. Each step represents scaffolding for a “teachable moment” with materials provided so faculty can more easily use them with students.

Scrubbing, Tokenization, Cutting, and
Culling
Lexos, a web-based, open-source workflow of tools (Kleinman, et al., 2016) was used to upload texts and “scrub” them by applying the following options: (i) convert words to lowercase, (ii) all punctuation was removed, (iii) however, a single word-internal hyphen and word-internal apostrophes were kept, and (iv) all digits were removed. Each individual word is considered as its own token. Larger stories were segmented (“cut”) into pieces. We experimented with various culling options, e.g., keeping only the most frequent words that appear in each text at least once.

Cluster Analysis
As a set of initial probes, we compared the contested story “A Dream” to (i) other stories attributed to Poe and (ii) mixed in with stories by other contemporaries. In the repo, we share four variations using cluster analysis:

1. K-means clustering on only Poe's stories (using Lexos)

2. Hierarchical agglomerative clustering on only Poe's stories (uses a Python sklearn module and a script to convert the cluster to ETE and Newick formats)

3. K-means clustering when all stories by each author are concatenated together (Lexos)

4. Bootstrap Consensus Tree (using Stylo in R).

The result from the Bootstrap Consensus Tree is shown in Figure 1. Of interest is that each author's stories cluster consistently together (with the exception that Bird's initial section of “Sheppard Lee” and his “Calavar” are found in different clades, at six and eight o'clock). “A Dream” clusters with the smaller Poe texts. As you'll see, we couldn't resist tossing in the four stories sometimes attributed to Edgar's brother Henry (“Monte Video”, “A Fragment”, “The Pirate”, and “Recollections”). These four stories are found within the cluster of Poe's known works (c.f. Collins, 2013).

A series of cluster analyses often serves well as a preliminary exploration, especially for scholars who are new to this game. Some of the file sizes are very small (e.g., one-half of the Poe stories in this corpus have fewer than 2000 words) and when strict culling is enforced (top-N words that appear at least once in each segment), the available set of words is reduced to only 38 when dealing with “A Dream” and the other eighteen Poe stories. That noted, these exploratory investigations shed some light on why some scholars consider that Poe's “first published tale may have been ‘A Dream'” (Silverman, 1991, p87).


Figure 1. Using Stylo in R Bootstrap Consensus Tree (BCT) showing “A Dream” consistently clustering with other Poe stories. The BCT aggregates results over multiple cluster analyses and shows those texts that satisfy a consensus number of the individual trials. Using 12 different authors

and at least two texts by each author for a total of 46 stories, Stylo formed clusters of the texts for the following frequency bands when using the most-frequent words: 100 to 1000 MFW.

Classification
Three classification models differentiated authorial writing style as implemented in Stylo in R. We scripted in R alongside Stylo to test “A Dream” over N-trials (N=10, 100) using a random selection of files for training sets in each trial. At least one text from each author is also included in the test set for each trial. A followup Python script parses the collected results to build confusion matrices for each author to provide metrics on how well the models predict each author's works. The most-frequently occurring, top-40 words (MFW, 1-grams) that appear in all the texts at least once were used.

Confusion Matrix values for all Poe Stories

Model

Attributions of

“A Dream” to Poe

True+

True-

False+

False-

Delta

9

13

200

0

7

NSC

10

16

170

30

4

SVM

7

14

198

2

6

Table 1: Attributions of the contested story “A Dream” over ten (10) trials with “A Dream” and another randomly selected Poe story in the test set in every trial. Confusion matrix values for results of testing Poe texts over all trials provide overall measures of model effectiveness. In the three cases

where “A Dream” was attributed to a different author, Poe was ranked second.

Summary
We offer a start to an exploration to collect evidence

as to whether Poe may have written the 1831 story “A

Dream” (c.f., Schoberlein (2016) who used the most frequent character 3-grams and attributed the story to Poe using Delta, but not so when using NSC nor SVM models). Evidence and methods aside, a GitHub repo provides a framework to share experimental workflows in a spirit similar to Jupyter notebooks, as well as one that facilitates both reproducible results and opportunities for subsequent contributions.

Notes
Forming an appropriate corpus is hard: thanks to Sam Coale, Ryan Cordell, Cary Gouldin, David Hoover, Shirrel Rhoades, and Ted Underwood. Four undergraduates: Weiqi Feng, Alec Horwitz, Jingxian Liu, and Khaled Sharafaddin worked with us on this problem. Thanks to Maciej Eder for his help with Stylo in R.

Sinclair, S. and Rockwell, G., (2016). Voyant Tools. Web: http://voyant-tools.org/.

Bibliography
Crymble, A., Gibbs, F., Hegel, A., McDaniel, C., Milligan, I., Taparata, E., Visconti, A., and Wieringa, J.,

eds. (2016). The Programming Historian. 2nd ed.. Web: http://programminghistorian.org/.

Eder, M., Kestemont, M. and Rybicki, J. (2016). Stylometry with R: A package for computational text analysis. R Journal, 16(1): 107-121.

GitHub repository:    A Good    Mystery.    .

https://github.com/WheatonCS/aGoodMystery

Jockers, M. (2014). Text Analysis with R for Students of Literature. Springer, New York.

Kleinman, S., LeBlanc, M.D., Drout, M., and Zhang, C.

(2016). Lexos v3.0. Web: http://lexos.wheatoncol-lege.edu.

Rudman, J. (2012). The State of Non-Traditional Authorship Attribution Studies -- 2012: Some Problems and Solutions. English Studies, v93(3), 259-274.

Schoberlein, S. (2016). Poe or Not Poe? A Stylometric Analysis of Edgar Allan Poe's Disputed Writings. Digital Scholarship in the Humanities, July 24, 2016.

Silverman, K. (1991). Edgar A. Poe: Mournful and Never-Ending Remembrance. HarperCollins, New York.",txt,Creative Commons Attribution 4.0 International,,attribution;methods;poe;reproducibility;stylometry,English,"authorship attribution / authority;computer science;design;english studies;interdisciplinary collaboration;software design and development;stylistics and stylometry;teaching, pedagogy, and curriculum;text analysis",2017-01-01,"summary
reproducing experimental results is a hallmark of empirical investigation and serves both to verify and inspire. this paper is a call for more systematic documentation of computational stylistic experiments. publishing only summaries of the methods and results of empirical work is an artifact of traditional print media. to facilitate experimental reproducibility and to help the growing community who wish to learn how to apply computational methods and subsequently teach the next generation of scholars, the publication of results must include (i) access to the digitized texts, (ii) a clear workflow and most essentially (iii) the source code that led to each and all of the experimental results. by way of example, we present the steps and process in a github repository for computationally probing the unknown and contested authorship of an 1831 short story entitled “a dream” as we seek evidence if this work is similar to other attributed works by edgar allan poe. the entire framework is intended as a pedagogical jumpstart for others, especially those new to computational stylometry. if poe did write the story, it would be his first published work.

introduction
as the digital humanities gains access to a wide array of digitized corpora and matures to a discipline that creatively defines new methods for computationally close and distant readings, a growing gap has emerged between those who apply sophisticated programming, e.g., stylo in r (eder et al., 2016) and those who are new to the game and need an introduction to the field. typical of the community spirit in dh, significant efforts are underway to bridge this gap, including web-based tools for entry-level exploration including

voyant tools (sinclair and rockwell, 2016) and lexos (kleinman et al., 2016) and domain-specific introductions to programming, including jockers' text (2014) and the programming historian (crymble et al., 2016). this paper attempts to narrow the gap by encouraging both sides to document their experimental methods more fully to embrace previous calls for the replication of experimental methods (rudman, 2012 et al.) and thereby teach effective practices by “leaving a trail” of experimental methods that enable others to execute and extend.

a good mystery: towards reproducibility
a github repository or “repo” offers a workflow

that explores whether an 1831 story published under

the attribution of only ‘p' might have been written by edgar allan poe. if so, it would be poe's first published work. in addition to sharing a set of analytical methods applied in this experiment, the broader methodological-pedagogical goals are two-fold: (i) the dissemination of data and code should be championed as a cornerstone of dh research, thereby facilitating the replication of results and (ii) to share a workflow so that others may apply similar analyses to their texts of interest.

the workflow is stored as a set of numbered folders containing the texts and scripts (code) needed to complete each step. the workflow includes: collecting texts, the preprocessing, tokenization, and culling decisions made, unsupervised cluster analyses (k-means, hierarchical-agglomerative, bootstrap consensus tree), and supervised classification methods using stylo in r's delta, svm, and nsc models. each step represents scaffolding for a “teachable moment” with materials provided so faculty can more easily use them with students.

scrubbing, tokenization, cutting, and
culling
lexos, a web-based, open-source workflow of tools (kleinman, et al., 2016) was used to upload texts and “scrub” them by applying the following options: (i) convert words to lowercase, (ii) all punctuation was removed, (iii) however, a single word-internal hyphen and word-internal apostrophes were kept, and (iv) all digits were removed. each individual word is considered as its own token. larger stories were segmented (“cut”) into pieces. we experimented with various culling options, e.g., keeping only the most frequent words that appear in each text at least once.

cluster analysis
as a set of initial probes, we compared the contested story “a dream” to (i) other stories attributed to poe and (ii) mixed in with stories by other contemporaries. in the repo, we share four variations using cluster analysis:

1. k-means clustering on only poe's stories (using lexos)

2. hierarchical agglomerative clustering on only poe's stories (uses a python sklearn module and a script to convert the cluster to ete and newick formats)

3. k-means clustering when all stories by each author are concatenated together (lexos)

4. bootstrap consensus tree (using stylo in r).

the result from the bootstrap consensus tree is shown in figure 1. of interest is that each author's stories cluster consistently together (with the exception that bird's initial section of “sheppard lee” and his “calavar” are found in different clades, at six and eight o'clock). “a dream” clusters with the smaller poe texts. as you'll see, we couldn't resist tossing in the four stories sometimes attributed to edgar's brother henry (“monte video”, “a fragment”, “the pirate”, and “recollections”). these four stories are found within the cluster of poe's known works (c.f. collins, 2013).

a series of cluster analyses often serves well as a preliminary exploration, especially for scholars who are new to this game. some of the file sizes are very small (e.g., one-half of the poe stories in this corpus have fewer than 2000 words) and when strict culling is enforced (top-n words that appear at least once in each segment), the available set of words is reduced to only 38 when dealing with “a dream” and the other eighteen poe stories. that noted, these exploratory investigations shed some light on why some scholars consider that poe's “first published tale may have been ‘a dream'” (silverman, 1991, p87).


figure 1. using stylo in r bootstrap consensus tree (bct) showing “a dream” consistently clustering with other poe stories. the bct aggregates results over multiple cluster analyses and shows those texts that satisfy a consensus number of the individual trials. using 12 different authors

and at least two texts by each author for a total of 46 stories, stylo formed clusters of the texts for the following frequency bands when using the most-frequent words: 100 to 1000 mfw.

classification
three classification models differentiated authorial writing style as implemented in stylo in r. we scripted in r alongside stylo to test “a dream” over n-trials (n=10, 100) using a random selection of files for training sets in each trial. at least one text from each author is also included in the test set for each trial. a followup python script parses the collected results to build confusion matrices for each author to provide metrics on how well the models predict each author's works. the most-frequently occurring, top-40 words (mfw, 1-grams) that appear in all the texts at least once were used.

confusion matrix values for all poe stories

model

attributions of

“a dream” to poe

true+

true-

false+

false-

delta

9

13

200

0

7

nsc

10

16

170

30

4

svm

7

14

198

2

6

table 1: attributions of the contested story “a dream” over ten (10) trials with “a dream” and another randomly selected poe story in the test set in every trial. confusion matrix values for results of testing poe texts over all trials provide overall measures of model effectiveness. in the three cases

where “a dream” was attributed to a different author, poe was ranked second.

summary
we offer a start to an exploration to collect evidence

as to whether poe may have written the 1831 story “a

dream” (c.f., schoberlein (2016) who used the most frequent character 3-grams and attributed the story to poe using delta, but not so when using nsc nor svm models). evidence and methods aside, a github repo provides a framework to share experimental workflows in a spirit similar to jupyter notebooks, as well as one that facilitates both reproducible results and opportunities for subsequent contributions.

notes
forming an appropriate corpus is hard: thanks to sam coale, ryan cordell, cary gouldin, david hoover, shirrel rhoades, and ted underwood. four undergraduates: weiqi feng, alec horwitz, jingxian liu, and khaled sharafaddin worked with us on this problem. thanks to maciej eder for his help with stylo in r.

sinclair, s. and rockwell, g., (2016). voyant tools. web: http://voyant-tools.org/.

bibliography
crymble, a., gibbs, f., hegel, a., mcdaniel, c., milligan, i., taparata, e., visconti, a., and wieringa, j.,

eds. (2016). the programming historian. 2nd ed.. web: http://programminghistorian.org/.

eder, m., kestemont, m. and rybicki, j. (2016). stylometry with r: a package for computational text analysis. r journal, 16(1): 107-121.

github repository:    a good    mystery.    .

https://github.com/wheatoncs/agoodmystery

jockers, m. (2014). text analysis with r for students of literature. springer, new york.

kleinman, s., leblanc, m.d., drout, m., and zhang, c.

(2016). lexos v3.0. web: http://lexos.wheatoncol-lege.edu.

rudman, j. (2012). the state of non-traditional authorship attribution studies -- 2012: some problems and solutions. english studies, v93(3), 259-274.

schoberlein, s. (2016). poe or not poe? a stylometric analysis of edgar allan poe's disputed writings. digital scholarship in the humanities, july 24, 2016.

silverman, k. (1991). edgar a. poe: mournful and never-ending remembrance. harpercollins, new york.",2.0,3.0,Voyant
3794,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Lexos: An Integrated Lexomics Workflow,https://dh2017.adho.org/abstracts/054/054.pdf,Scott Kleinman;Mark D. LeBlanc,poster / demo / art installation,"Lexos is a browser-based suite of tools that helps lower barriers of entry to computational text analysis for humanities scholars and students. Situated within a clean and simple interface, Lexos consolidates the common pre-processing operations needed for subsequent analysis, either with Lexos or with external tools. It is especially useful for scholars who wish to engage in research involving computational text analysis and/or wish to teach their students how to do so but lack the time for a manual preparation of texts, the skill sets needed to prepare their texts analysis, or the intellectual contexts for situating computational methods within their work. Lexos is also targeted at researchers studying early texts and texts in non-Western languages, which may involve specialized processing rules. It is thus designed to facilitate advanced research in these fields even for users more familiar with computational techniques. Lexos is developed by the Lexomics research group led by Michael Drout (Wheaton College), Mark LeBlanc (Wheaton College), and Scott Kleinman (California State University, Northridge). It is built on Python 2.7-Flask microframework, with jQuery-Bootstrap UI, and visualizations in d3.js. The Lexomics research group provides access to an public installation of Lexos which does not retain data after a session has expired. Users may also install Lexos locally by cloning the GitHub repository.

Lexos guides users through a workflow of steps that reflects effective practices when working with digitized texts. The workflow includes: (i) uploading Unicode-encoded texts in plain text, HTML, or XML formats; (ii) “scrubbing” functions for consolidating preprocessing decisions such as the handling of punctuation, white-space, and stop words, the use of lemmati-zation rules, and the handling of embedded markup tags and special character entities; (iii) “cutting” texts into segments based on the number of characters, tokens, or lines, or by embedded milestones such as chapter breaks; (iv) tokenization into a Document Term Matrix of raw or proportional counts using character or word n-grams; (v) visualizations such as comparative word clouds per segment (including the ability to visualize topic models generated by MALLET); Rolling Window Analysis that plots the frequency of string, phrase, or regular expression patterns or pattern-pair ratios over the course of a document or collection; and (vi) analysis tools including statistical summaries, hierarchical and k-means clustering, cosine similarity rankings, and Z-tests to identify the relative prominence of terms in documents, document classes, and the collection as whole. At each stage in the workflow the user may download data, visualizations, or the results of the analytical tools, along with metadata about their preprocessing decisions or the parameters selected for their experiments. Lexos thus enables the export of data for use with other tools and facilitates experimental reproducibility.

Lexos {scrubber} An Integrated Lexomics Workflow

Scrubbing Options

Q Remove All Punctuation

B Keep Hyphens ©

Q Make Lowercase

B Keep Word-Internal Apostrophes©

Q Remove Digits

■ Remove Whitespace 0

B Scrub Tags 0

Additional Options

Stop Words/Keep Words O >


Previews of Documents


A1.3_Dan_T00030.txt




Gefr&ae;gn ic Hebreos eadge lifgean in Hierusalem goldhord d&ae;lan cyningdom hab ban swa him gecynde w&ae;s si&d;&d;an &t;urh metodes m&ae;gen on Moyses hand w

ealra gesceafta drihten and waldend se him dom forgeaf unscyndne bl&ae;d eor&d;an rices and &t;u lignest nu &t;&ae;t sie lifgende se ofer deoflum duge&t;um wealde&d;. A3.3_Az_T00130.txt


orn dryhten herede wis in weorcum ond &t;as Word acw&ae;&d;: Meotud allwihta &t;u eart meahtum swi&d; ni&t;as to nerganne. Is &t;in noma m&ae;re wütig ond wul h




Special Characters 0 v


Lemmas 0 v


Consolidations 0 v


Figure 1: The Lexos Scrubber Tool

Lexos addresses three significant challenges for our intended users. The first challenge involves the adoption of computational text analysis methods. Many approaches require proficiency with command line scripting or the use of complex user interfaces that require time to master. Lexos addresses this problem through a simple, browser-based interface that manages workflow through the three major steps of text analysis: pre-processing, generation of statistical data, and visualization. In this, Lexos resembles Voyant Tools (Sinclair and Rockwell, 2016), although Lexos places more emphasis on and providing more tools for preprocessing and segmenting texts. Lexos also shares with tools like Stylometry with R (Eder, et al., 2013; Eder, 2013) and emphasis on cluster analysis, providing both hierarchical and K-Means clustering with silhouette scores as limited form of statistical validation. While Lexos is not a topic modeling tool, it provides a useful “topic cloud” feature for MALLET data that will be useful for beginners since there are few accessible ways to visualize MALLET output that work well out of the box.


Figure 2: The Lexos Multicloud tool showing Chinese ""topic clouds""

The second challenge is the opacity of the procedures required to move between computational and traditional forms of text analysis. In order to reduce the “black boxiness” of algorithmic methods, Lexos contains an embedded component called “In the Margins” which provides non-technical explanations of the statistical methods used and effective practices for

handling situations typical of humanities data. “In the

Margins” is a Scalar “book” which can be read separately; however, its individual pages are embedded in Lexos using Scalar's API, making them easily accessible for users of the tool. Lexos shares with tools like Voyant an engagement with the hermeneutics of text analysis and attempts to embed “In the Margins” discussion of these issues in the user interface close to the user's workflow. We hope “In the Margins” will host advice and commentary from contributors with the Digital Humanities community.

A third challenge is the tension between quantitative and computational approaches and the traditions of theoretical and cultural criticism that dominate the humanities in the academy. As Alan Liu (2013) has recently argued, the challenge is to give a better theoretical grounding to the hybrid quantitative-qualitative method of the Digital Humanities by exploring the ways in which we negotiate the difficulties imposed by “the aporia between tabula rasa quantitative interpretation and humanly meaningful qualitative interpretation” (414). The design of Lexos and the discussions in “In the Margins” are intended to open a space for discussion of issues related to the opacity of algorithmic approaches and the limitations and epistemological challenges of computational stylistic analysis and visual representation of humanities data.

This poster presentation provides demonstrations of Lexos using some literature from Old, Middle, and Modern English, as well Chinese, which are in our current test suite. We also discuss use cases and best practices, how to install Lexos locally, and how scholars may contribute to the still growing content of “In the Margins”.

Bibliography

Drout, M., Kleinman, S., and LeBlanc, M. 2016-. “In the Margins.” http://scalar.usc.edu/works/lexos./

Eder, M. (2013). “Mind Your Corpus: Systematic Errors in Authorship Attribution.” Literary and Linguistic Computing 28 (4): 603-14.

Eder, M., Kestemont, M., and Rybiki, J. 2013. “Stylometry with R: A Suite of Tools (Abstract of Poster Session)”. Presented at Digital Humanities 2013, Lincoln, Nebraska. http://dh2013.unl.edu/abstracts/ab-136.html, https://sites.google.com/site/computationalstylistics/

Kleinman, S., LeBlanc, M.D., Drout, M. and Zhang, C. 2016. Lexos v3.0. https://github.com/WheatonCS/Lexos/.

Liu, A. (2013). “The Meaning of the Digital Humanities.” PMLA 128 (2): 409-23.

McCallum, A.K. (2002). MALLET: A Machine Learning for

Language Toolkit. http://mallet.cs.umass.edu.

Sinclair, S., and Rockwell, G. (2016). Voyant Tools. Web.

http://voyant-tools.org/.",txt,Creative Commons Attribution 4.0 International,,interdisciplinary;lexomics;stylometry,English,"authorship attribution / authority;computer science;interdisciplinary collaboration;literary studies;medieval studies;philology;stylistics and stylometry;teaching, pedagogy, and curriculum;text analysis",2017-01-01,"lexos is a browser-based suite of tools that helps lower barriers of entry to computational text analysis for humanities scholars and students. situated within a clean and simple interface, lexos consolidates the common pre-processing operations needed for subsequent analysis, either with lexos or with external tools. it is especially useful for scholars who wish to engage in research involving computational text analysis and/or wish to teach their students how to do so but lack the time for a manual preparation of texts, the skill sets needed to prepare their texts analysis, or the intellectual contexts for situating computational methods within their work. lexos is also targeted at researchers studying early texts and texts in non-western languages, which may involve specialized processing rules. it is thus designed to facilitate advanced research in these fields even for users more familiar with computational techniques. lexos is developed by the lexomics research group led by michael drout (wheaton college), mark leblanc (wheaton college), and scott kleinman (california state university, northridge). it is built on python 2.7-flask microframework, with jquery-bootstrap ui, and visualizations in d3.js. the lexomics research group provides access to an public installation of lexos which does not retain data after a session has expired. users may also install lexos locally by cloning the github repository.

lexos guides users through a workflow of steps that reflects effective practices when working with digitized texts. the workflow includes: (i) uploading unicode-encoded texts in plain text, html, or xml formats; (ii) “scrubbing” functions for consolidating preprocessing decisions such as the handling of punctuation, white-space, and stop words, the use of lemmati-zation rules, and the handling of embedded markup tags and special character entities; (iii) “cutting” texts into segments based on the number of characters, tokens, or lines, or by embedded milestones such as chapter breaks; (iv) tokenization into a document term matrix of raw or proportional counts using character or word n-grams; (v) visualizations such as comparative word clouds per segment (including the ability to visualize topic models generated by mallet); rolling window analysis that plots the frequency of string, phrase, or regular expression patterns or pattern-pair ratios over the course of a document or collection; and (vi) analysis tools including statistical summaries, hierarchical and k-means clustering, cosine similarity rankings, and z-tests to identify the relative prominence of terms in documents, document classes, and the collection as whole. at each stage in the workflow the user may download data, visualizations, or the results of the analytical tools, along with metadata about their preprocessing decisions or the parameters selected for their experiments. lexos thus enables the export of data for use with other tools and facilitates experimental reproducibility.

lexos {scrubber} an integrated lexomics workflow

scrubbing options

q remove all punctuation

b keep hyphens ©

q make lowercase

b keep word-internal apostrophes©

q remove digits

■ remove whitespace 0

b scrub tags 0

additional options

stop words/keep words o >


previews of documents


a1.3_dan_t00030.txt




gefr&ae;gn ic hebreos eadge lifgean in hierusalem goldhord d&ae;lan cyningdom hab ban swa him gecynde w&ae;s si&d;&d;an &t;urh metodes m&ae;gen on moyses hand w

ealra gesceafta drihten and waldend se him dom forgeaf unscyndne bl&ae;d eor&d;an rices and &t;u lignest nu &t;&ae;t sie lifgende se ofer deoflum duge&t;um wealde&d;. a3.3_az_t00130.txt


orn dryhten herede wis in weorcum ond &t;as word acw&ae;&d;: meotud allwihta &t;u eart meahtum swi&d; ni&t;as to nerganne. is &t;in noma m&ae;re wütig ond wul h




special characters 0 v


lemmas 0 v


consolidations 0 v


figure 1: the lexos scrubber tool

lexos addresses three significant challenges for our intended users. the first challenge involves the adoption of computational text analysis methods. many approaches require proficiency with command line scripting or the use of complex user interfaces that require time to master. lexos addresses this problem through a simple, browser-based interface that manages workflow through the three major steps of text analysis: pre-processing, generation of statistical data, and visualization. in this, lexos resembles voyant tools (sinclair and rockwell, 2016), although lexos places more emphasis on and providing more tools for preprocessing and segmenting texts. lexos also shares with tools like stylometry with r (eder, et al., 2013; eder, 2013) and emphasis on cluster analysis, providing both hierarchical and k-means clustering with silhouette scores as limited form of statistical validation. while lexos is not a topic modeling tool, it provides a useful “topic cloud” feature for mallet data that will be useful for beginners since there are few accessible ways to visualize mallet output that work well out of the box.


figure 2: the lexos multicloud tool showing chinese ""topic clouds""

the second challenge is the opacity of the procedures required to move between computational and traditional forms of text analysis. in order to reduce the “black boxiness” of algorithmic methods, lexos contains an embedded component called “in the margins” which provides non-technical explanations of the statistical methods used and effective practices for

handling situations typical of humanities data. “in the

margins” is a scalar “book” which can be read separately; however, its individual pages are embedded in lexos using scalar's api, making them easily accessible for users of the tool. lexos shares with tools like voyant an engagement with the hermeneutics of text analysis and attempts to embed “in the margins” discussion of these issues in the user interface close to the user's workflow. we hope “in the margins” will host advice and commentary from contributors with the digital humanities community.

a third challenge is the tension between quantitative and computational approaches and the traditions of theoretical and cultural criticism that dominate the humanities in the academy. as alan liu (2013) has recently argued, the challenge is to give a better theoretical grounding to the hybrid quantitative-qualitative method of the digital humanities by exploring the ways in which we negotiate the difficulties imposed by “the aporia between tabula rasa quantitative interpretation and humanly meaningful qualitative interpretation” (414). the design of lexos and the discussions in “in the margins” are intended to open a space for discussion of issues related to the opacity of algorithmic approaches and the limitations and epistemological challenges of computational stylistic analysis and visual representation of humanities data.

this poster presentation provides demonstrations of lexos using some literature from old, middle, and modern english, as well chinese, which are in our current test suite. we also discuss use cases and best practices, how to install lexos locally, and how scholars may contribute to the still growing content of “in the margins”.

bibliography

drout, m., kleinman, s., and leblanc, m. 2016-. “in the margins.” http://scalar.usc.edu/works/lexos./

eder, m. (2013). “mind your corpus: systematic errors in authorship attribution.” literary and linguistic computing 28 (4): 603-14.

eder, m., kestemont, m., and rybiki, j. 2013. “stylometry with r: a suite of tools (abstract of poster session)”. presented at digital humanities 2013, lincoln, nebraska. http://dh2013.unl.edu/abstracts/ab-136.html, https://sites.google.com/site/computationalstylistics/

kleinman, s., leblanc, m.d., drout, m. and zhang, c. 2016. lexos v3.0. https://github.com/wheatoncs/lexos/.

liu, a. (2013). “the meaning of the digital humanities.” pmla 128 (2): 409-23.

mccallum, a.k. (2002). mallet: a machine learning for

language toolkit. http://mallet.cs.umass.edu.

sinclair, s., and rockwell, g. (2016). voyant tools. web.

http://voyant-tools.org/.",3.0,4.0,Voyant
3797,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Modelling Interpretation in 3DH: New dimensions of visualization,https://dh2017.adho.org/abstracts/058/058.pdf,Jan Christoph Meister;Johanna Drucker;Geoffrey Rockwell,"paper, specified ""long paper""","Introduction
Visualization techniques developed in the sciences normally focus on the (re)presentation of empirical data. But how can we graphically express interpretations? This paper presents the intellectual framework

underpinning the 3DH project (Three-dimensional Visualizations for the Digital Humanities), a collaborative project conducted at the University of Hamburg from 2016 to 2019. The project foregrounds data interpretation and develops a visualization paradigm from the epistemological perspective of the humanities. The “third dimension” required in DH visualization techniques is therefore not merely that of an additional quantitative z-axis. Rather, it is an axis that can ‘unflatten' (Sousanis 2015) the objectivist notion of visualized data. In our presentation, we will do three things:

•    Digital and visual turn: Review existing visualization paradigms that emphasize the representational approach. We start with the epistemological issues raised by the digital and visual turn.

•    Visual modelling: Outline and discuss an interpretative modelling alternative through two case studies of existing tools, CATMA and Voyant, and Temporal Modelling, a platform for creating data through graphical means.

• “Hermeneuticizing” visualization: Discuss the design of a full visual framework. We will present possible conventions and prototypes that use them. These inform our case studies and the envisaged infrastructure.

Case studies in our presentation will be drawn from CATMA (a collaborative mark-up & text analysis environment), Voyant (a text analysis platform), and humanities research projects using base images (historical maps) and original models (for non-standard chronologies).

The digital and the visual turn: a hermeneutic ceterum censeo

For centuries, academic discourse in humanities disciplines has relied predominantly on text. In DH, however, visualizations increasingly claim the status of arguments and proofs that play a decisive role in the development and presentation of ideas, findings, and conclusions.

The visual and the digital turn have thus gone hand in hand - but the way in which this synergy manifests itself remains constrained in a symptomatic way. We can print a chart or render it on screen just as we can print or display a text in various media, but we normally cannot subject the chart to in-depth critique in the way we can question and respond to the text. Inadvertently, once generated and communicated as ‘output', visualizations seem to take on a quasi-dogmatic quality - they are hard to deconstruct, let alone reconfigure; they state their case but seem removed from critical reflection.

Most current DH visualizations are thus epistemological one-way avenues toward knowledge, from data via rendering algorithm to visual display. Charts, graphs, interactive maps, timelines, and similar representations are by and large imports from the natural and social sciences (Friendly 2008). Many of them emanate from domains of empirical research that conceptualize knowledge production as a function of empirical observation and objective measurement followed by analysis, inference, and conclusion. These approaches to visualization, however, hide two critical aspects, namely

(a) the underlying human modeling of the represented phenomena as data, which is already an interpretive and meaning-creating act that often oscillates repeatedly between observation and interpretation (Kitchin 2014), and

(b) the meaning-lessness of certain visual effects that are owed to contingent technological constraints (screen size, rendering, scaling, choice of color, etc.).

DH is in a unique position to investigate the domains of human experience and of its expression in symbolic practices and artefacts from two complementary methodological vantage points: the numeric, which models them as statistical phenomena, and the hermeneutic, which explores them as phenomena of meaning and thus by definition as a function of interpretation (Rockwell & Sinclair 2016). Where meaning comes into focus, our theories, object models, and practices

must therefore be conceptually aligned and ‘herme-

neuticized' - just as numeric approaches come with the pre-requisite of quantification. Against this backdrop, we propose to reintroduce the dimension of interpretation into visualization: Methodological principles of hermeneutic approaches, such as multi-per-spectivity, subjectivity, and context-boundedness present a challenge which representational visualization cannot and which interpretational visualization must meet.

Two questions arise: What are the defining principles of a genuinely humanistic and hermeneutically oriented approach to visualization? And how can we graphically express and support interpretation in DH visualizations - both as an activity and as a product of humanistic enquiry?

Visual modeling of interpretation vs. visualization of data

In the 3DH project, we address the former question by conceptual analysis and critique of existing approaches to visualization in DH, and then by systematically specifying and developing a visualization environment that can support higher level data interpretation rather than base-level data representation. In the presentation, we will share our survey of existing tools and their affordances but focus on two tools that we have developed, CATMA and Voyant.


Figure 1: Visualization of interpretive text annotation in CATMA

Our premise is that interpretation happens through the deliberate activity of an individual engaging with an image, text, display, or other artifact to create an argument about its meaning and a way it should be read.

For example, in CATMA (Figure 1) such an activity - in this instance the interpretive act of text annotation - is executed and represented by (a) highlighting a string on screen, (b) assigning it a tag, and (c) storing the annotation in a stand-off markup file. However, the annotation is at the same time (d) visually expressed as colored underlining. Moreover, via its visual representation on screen - the colored underlining - the markup data can also be (e) inspected, analyzed, manipulated directly, and even (f) enriched with meta-annotation. This is but one example of interpretative modeling.

Current representational ‘one-way' techniques like topic modeling (see Figure 2) are seen as a way to deal with scale, they process large amounts of data into summary abstractions called topics that can be displayed as lists or in other ways (Montague et. al 2015). In our second case study, we will therefore show how we are adapting scale tools to create a prototypical bidirectional 3DH visual modeling environment for big



Figure 2: Galaxy Viewer

data. We believe visual modeling can support not only interpretative close reading of primary data but also the reading of large collections like the collections of the Hathi Trust.

‘Hermeneuticizing' base-level visualization through activators: the 3DH framework of interpretive parameters and dimensions

A key goal of the 3DH project is to develop a set of generic graphic features that can be used to create interpretative attributes and/or inflections of visual representations of data, alter underlying data structures, and activate three-dimensional space in the service of interpretative activity. These features which aim to ‘hermeneuticize' visualizations are termed activators. In the presentation we will show the framework of the

activator set that was developed during a series of cha-

rettes (design workshops) in 2016.


Figure 3: Framework of Concept Modeling workspace: Shows features, activators, and dimensions from various pictorial conventions.

The visual activators in our feature set are not simply graphical marks or animations on a screen display: They perform data structuring functions and as such provide a conceptual framework for ‘hermeneu-ticizing' existing base-level data visualization techniques (see Fig.3). The individual features of this framework indicate and facilitate interpretative moves made by the user, such as a qualification of visualized data structures in terms of salience, irrelevance, uncertainty, degree of completeness, and other attributes or inflections. For example, uncertainty can be expressed by overlaying a standard graph with visual effects such as blur or shading, whereas the introduction of additional interpretative dimensions, such as point of view systems, parallax, relative scales, and other conventions from the visual arts, will support higher levels of interpretative critique and reflection, such as explicitly marking the historicity and context-dependency of underlying data.

Conclusion

As Pinker (1990) argues, the ease with which a particular graph can be understood is a function of the processing effort that goes into the exercise: The more we can rely on ‘hard-wired' encoding connections between the visual and the conceptual and the more we are guided by established graph and comprehension schemata (such as Gestalt phenomena), the less ‘intelligent' effort we have to put into reading a graph. Yet in a humanities perspective such conventionalized ‘ease of comprehension' is a double-edged sword: It may optimize the process of (re)cognition - but it also progressively obscures the constructedness of a visualization, turning it into an apparently self-evident object of perception. The 3DH project counters this anti-hermeneutic tendency toward reification by moving from a conceptualization of the principles of visualization as interpretative modeling to the development of a visual language framework, and finally the instantiation of the principles and language in two case studies. In terms of implementation, this approach is supported by drawing on Bertin's Semiology of Graphics and the high-level object-oriented Grammar of Graphics approach outlined by Wilkinson (2005), and features from game engines, three-dimensional modelling, and other pictorial conventions (Panofsky (1991) and Burgin (1991)).

To conclude, we will discuss next steps toward developing a 3DH environment that can act as a generic, project independent infrastructure for introducing user parameterized enunciative functionality into graphical displays. This infrastructure will make it possible to inscribe into visualizations the critical features of authorship, speaking/spoken subject, and an epistemological perspective grounded in situated and constructed approaches to knowledge. These interpretative principles are well mapped in, e.g., critical theory, narratology, visual studies, and cultural studies, but they have not been integrated into a graphical environment for hermeneutic practice yet: the methodological lacuna which the 3DH project tries to address.

Bibliography

Bertin, J. (1983). Semiology of Graphics: Diagrams, Networks, Maps. Madison, WI, University of Wisconsin Press.

Burgin, V. (1991). “Geometry and Abjection”. In: J. Donald

(ed.), Psychoanalysis and Cultural Theory: Thresholds.

London, Macmillan Education, pp. 11-26 .

Chandrasekaran, B. & Lele, O. (2010). “Mapping Descriptive Models of Graph Comprehension into Requirements for a Computational Architecture: Need for Supporting Imagery Operations”. In: A. K. Goel, M. Jamnik & N. H. Narayanan (eds.), Diagrammatic Representation and Inference. 6th International Conference, Diagrams 2010, Portland, OR, USA, August 9-11, 2010. Proceedings. Berlin & Heidelberg, Springer Verlag, pp. 235-242.

Drucker, J. (2011). “Humanities approaches to Graphical

Display”. In: DHQ, Digital Humanities Quarterly, 5 (1). http://digitalhumani-

ties.org/dhq/vol/5/1/000091/000091.html [March 17 2017].

(2014). Graphesis, Cambridge, Harvard University Press.

Friendly, M. (2008). “A Brief History of Data Visualization”. In: C.-H. Chen, W. K. Hardle & A. Unwin (eds.), Handbook of Data Visualization. Heidelberg, Springer-Verlag, pp. 134.

Kath, R., Schaal, G. S. & Dumm, S. (2016). „New Visual Hermeneutics“. In: Cybernetics and Human Knowing, 23 (2), pp. 51-75.

Kitchin, R. (2014). The Data Revolution: Big Data, Open Data, Data Infrastructures & Their Consequences. Los Angeles, SAGE.

Montague, J., Simpson, J., Brown, S., Rockwell, G. & Ruecker, S. (2015). “Exploring Large Datasets with Topic Model Visualization”. Conference paper at DH 2015, University of Western Sydney, Australia.

Panofsky, E. (1991). Perspective as Symbolic Form; C. Wood, trans.; New York, Zone Books.

Pinker, S. (1990). “A Theory of Graph Comprehension”. In: R. Feedle (ed.), Artificial Intelligence and the future of testing. Marwah, NJ, Erlbaum Hillsdale, pp. 73-126.

Rockwell, G. & Sinclair, S. (2016). Hermeneutica. Cambridge, MS & London, MIT Press.

Sousanis, N. (2015). Unflattening. Cambridge, MS & London, Harvard University Press.

Wilkinson, L. (2005). The Grammar of Graphics. 2nd ed.; New York, Springer.",txt,Creative Commons Attribution 4.0 International,,interpretation;modelling;visualization,English,"computer science;data modeling and architecture including hypothesis-driven modeling;geospatial analysis, interfaces and technology;interface & user experience design/publishing & delivery systems/user studies/user needs;knowledge representation;linguistics;literary studies;philosophy;spatio-temporal modeling, analysis and visualisation;visualization",2017-01-01,"introduction
visualization techniques developed in the sciences normally focus on the (re)presentation of empirical data. but how can we graphically express interpretations? this paper presents the intellectual framework

underpinning the 3dh project (three-dimensional visualizations for the digital humanities), a collaborative project conducted at the university of hamburg from 2016 to 2019. the project foregrounds data interpretation and develops a visualization paradigm from the epistemological perspective of the humanities. the “third dimension” required in dh visualization techniques is therefore not merely that of an additional quantitative z-axis. rather, it is an axis that can ‘unflatten' (sousanis 2015) the objectivist notion of visualized data. in our presentation, we will do three things:

•    digital and visual turn: review existing visualization paradigms that emphasize the representational approach. we start with the epistemological issues raised by the digital and visual turn.

•    visual modelling: outline and discuss an interpretative modelling alternative through two case studies of existing tools, catma and voyant, and temporal modelling, a platform for creating data through graphical means.

• “hermeneuticizing” visualization: discuss the design of a full visual framework. we will present possible conventions and prototypes that use them. these inform our case studies and the envisaged infrastructure.

case studies in our presentation will be drawn from catma (a collaborative mark-up & text analysis environment), voyant (a text analysis platform), and humanities research projects using base images (historical maps) and original models (for non-standard chronologies).

the digital and the visual turn: a hermeneutic ceterum censeo

for centuries, academic discourse in humanities disciplines has relied predominantly on text. in dh, however, visualizations increasingly claim the status of arguments and proofs that play a decisive role in the development and presentation of ideas, findings, and conclusions.

the visual and the digital turn have thus gone hand in hand - but the way in which this synergy manifests itself remains constrained in a symptomatic way. we can print a chart or render it on screen just as we can print or display a text in various media, but we normally cannot subject the chart to in-depth critique in the way we can question and respond to the text. inadvertently, once generated and communicated as ‘output', visualizations seem to take on a quasi-dogmatic quality - they are hard to deconstruct, let alone reconfigure; they state their case but seem removed from critical reflection.

most current dh visualizations are thus epistemological one-way avenues toward knowledge, from data via rendering algorithm to visual display. charts, graphs, interactive maps, timelines, and similar representations are by and large imports from the natural and social sciences (friendly 2008). many of them emanate from domains of empirical research that conceptualize knowledge production as a function of empirical observation and objective measurement followed by analysis, inference, and conclusion. these approaches to visualization, however, hide two critical aspects, namely

(a) the underlying human modeling of the represented phenomena as data, which is already an interpretive and meaning-creating act that often oscillates repeatedly between observation and interpretation (kitchin 2014), and

(b) the meaning-lessness of certain visual effects that are owed to contingent technological constraints (screen size, rendering, scaling, choice of color, etc.).

dh is in a unique position to investigate the domains of human experience and of its expression in symbolic practices and artefacts from two complementary methodological vantage points: the numeric, which models them as statistical phenomena, and the hermeneutic, which explores them as phenomena of meaning and thus by definition as a function of interpretation (rockwell & sinclair 2016). where meaning comes into focus, our theories, object models, and practices

must therefore be conceptually aligned and ‘herme-

neuticized' - just as numeric approaches come with the pre-requisite of quantification. against this backdrop, we propose to reintroduce the dimension of interpretation into visualization: methodological principles of hermeneutic approaches, such as multi-per-spectivity, subjectivity, and context-boundedness present a challenge which representational visualization cannot and which interpretational visualization must meet.

two questions arise: what are the defining principles of a genuinely humanistic and hermeneutically oriented approach to visualization? and how can we graphically express and support interpretation in dh visualizations - both as an activity and as a product of humanistic enquiry?

visual modeling of interpretation vs. visualization of data

in the 3dh project, we address the former question by conceptual analysis and critique of existing approaches to visualization in dh, and then by systematically specifying and developing a visualization environment that can support higher level data interpretation rather than base-level data representation. in the presentation, we will share our survey of existing tools and their affordances but focus on two tools that we have developed, catma and voyant.


figure 1: visualization of interpretive text annotation in catma

our premise is that interpretation happens through the deliberate activity of an individual engaging with an image, text, display, or other artifact to create an argument about its meaning and a way it should be read.

for example, in catma (figure 1) such an activity - in this instance the interpretive act of text annotation - is executed and represented by (a) highlighting a string on screen, (b) assigning it a tag, and (c) storing the annotation in a stand-off markup file. however, the annotation is at the same time (d) visually expressed as colored underlining. moreover, via its visual representation on screen - the colored underlining - the markup data can also be (e) inspected, analyzed, manipulated directly, and even (f) enriched with meta-annotation. this is but one example of interpretative modeling.

current representational ‘one-way' techniques like topic modeling (see figure 2) are seen as a way to deal with scale, they process large amounts of data into summary abstractions called topics that can be displayed as lists or in other ways (montague et. al 2015). in our second case study, we will therefore show how we are adapting scale tools to create a prototypical bidirectional 3dh visual modeling environment for big



figure 2: galaxy viewer

data. we believe visual modeling can support not only interpretative close reading of primary data but also the reading of large collections like the collections of the hathi trust.

‘hermeneuticizing' base-level visualization through activators: the 3dh framework of interpretive parameters and dimensions

a key goal of the 3dh project is to develop a set of generic graphic features that can be used to create interpretative attributes and/or inflections of visual representations of data, alter underlying data structures, and activate three-dimensional space in the service of interpretative activity. these features which aim to ‘hermeneuticize' visualizations are termed activators. in the presentation we will show the framework of the

activator set that was developed during a series of cha-

rettes (design workshops) in 2016.


figure 3: framework of concept modeling workspace: shows features, activators, and dimensions from various pictorial conventions.

the visual activators in our feature set are not simply graphical marks or animations on a screen display: they perform data structuring functions and as such provide a conceptual framework for ‘hermeneu-ticizing' existing base-level data visualization techniques (see fig.3). the individual features of this framework indicate and facilitate interpretative moves made by the user, such as a qualification of visualized data structures in terms of salience, irrelevance, uncertainty, degree of completeness, and other attributes or inflections. for example, uncertainty can be expressed by overlaying a standard graph with visual effects such as blur or shading, whereas the introduction of additional interpretative dimensions, such as point of view systems, parallax, relative scales, and other conventions from the visual arts, will support higher levels of interpretative critique and reflection, such as explicitly marking the historicity and context-dependency of underlying data.

conclusion

as pinker (1990) argues, the ease with which a particular graph can be understood is a function of the processing effort that goes into the exercise: the more we can rely on ‘hard-wired' encoding connections between the visual and the conceptual and the more we are guided by established graph and comprehension schemata (such as gestalt phenomena), the less ‘intelligent' effort we have to put into reading a graph. yet in a humanities perspective such conventionalized ‘ease of comprehension' is a double-edged sword: it may optimize the process of (re)cognition - but it also progressively obscures the constructedness of a visualization, turning it into an apparently self-evident object of perception. the 3dh project counters this anti-hermeneutic tendency toward reification by moving from a conceptualization of the principles of visualization as interpretative modeling to the development of a visual language framework, and finally the instantiation of the principles and language in two case studies. in terms of implementation, this approach is supported by drawing on bertin's semiology of graphics and the high-level object-oriented grammar of graphics approach outlined by wilkinson (2005), and features from game engines, three-dimensional modelling, and other pictorial conventions (panofsky (1991) and burgin (1991)).

to conclude, we will discuss next steps toward developing a 3dh environment that can act as a generic, project independent infrastructure for introducing user parameterized enunciative functionality into graphical displays. this infrastructure will make it possible to inscribe into visualizations the critical features of authorship, speaking/spoken subject, and an epistemological perspective grounded in situated and constructed approaches to knowledge. these interpretative principles are well mapped in, e.g., critical theory, narratology, visual studies, and cultural studies, but they have not been integrated into a graphical environment for hermeneutic practice yet: the methodological lacuna which the 3dh project tries to address.

bibliography

bertin, j. (1983). semiology of graphics: diagrams, networks, maps. madison, wi, university of wisconsin press.

burgin, v. (1991). “geometry and abjection”. in: j. donald

(ed.), psychoanalysis and cultural theory: thresholds.

london, macmillan education, pp. 11-26 .

chandrasekaran, b. & lele, o. (2010). “mapping descriptive models of graph comprehension into requirements for a computational architecture: need for supporting imagery operations”. in: a. k. goel, m. jamnik & n. h. narayanan (eds.), diagrammatic representation and inference. 6th international conference, diagrams 2010, portland, or, usa, august 9-11, 2010. proceedings. berlin & heidelberg, springer verlag, pp. 235-242.

drucker, j. (2011). “humanities approaches to graphical

display”. in: dhq, digital humanities quarterly, 5 (1). http://digitalhumani-

ties.org/dhq/vol/5/1/000091/000091.html [march 17 2017].

(2014). graphesis, cambridge, harvard university press.

friendly, m. (2008). “a brief history of data visualization”. in: c.-h. chen, w. k. hardle & a. unwin (eds.), handbook of data visualization. heidelberg, springer-verlag, pp. 134.

kath, r., schaal, g. s. & dumm, s. (2016). „new visual hermeneutics“. in: cybernetics and human knowing, 23 (2), pp. 51-75.

kitchin, r. (2014). the data revolution: big data, open data, data infrastructures & their consequences. los angeles, sage.

montague, j., simpson, j., brown, s., rockwell, g. & ruecker, s. (2015). “exploring large datasets with topic model visualization”. conference paper at dh 2015, university of western sydney, australia.

panofsky, e. (1991). perspective as symbolic form; c. wood, trans.; new york, zone books.

pinker, s. (1990). “a theory of graph comprehension”. in: r. feedle (ed.), artificial intelligence and the future of testing. marwah, nj, erlbaum hillsdale, pp. 73-126.

rockwell, g. & sinclair, s. (2016). hermeneutica. cambridge, ms & london, mit press.

sousanis, n. (2015). unflattening. cambridge, ms & london, harvard university press.

wilkinson, l. (2005). the grammar of graphics. 2nd ed.; new york, springer.",3.0,3.0,Voyant
3835,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Voces. An R-based Dashboard for Lexical Semantics,https://dh2017.adho.org/abstracts/114/114.pdf,Krzysztof Nowak,"paper, specified ""short paper""","Introduction
Voces (from Lat. vox 'voice', 'word') is an analysis

and visualisation dashboard for corpus-based research in lexical semantics. Currently developed as a Shiny application communicating with R session running in the background, Voces provides users with possibly exhaustive account of how selected Latin word is distributed across the corpus and what can be told about its meaning. The application is built around a corpus which currently consists of ca. 200M words from texts dating from the Classical era (1 BCE) to the Middle Ages (14th CE). Although Voces was originally conceived as a tool of historical semantics research, the application - due to its modular design - may be modified and the code basis can be re-used in new research contexts.

Lexical Semantics with R


Information computed on a basis of a CWB-indexed corpus is presented to a user through a single-page interface composed of separate widgets arranged in a clear grid layout. Each widget is responsible for displaying in textual or graphical form a clear-cut property of word's distribution or meaning. A heavy use of data visualisation techniques renders Voces a convenient tool for exploratory analysis of textual corpora, but the grid layout is also reflection of modular architecture of the application. Each widget is implemented as a separate function which can be extended and adopted by researchers with even limited R programming skills.

Use scenarios
A typical use scenario is triggered when the user specifies a lemma to be looked up. If the search fails, a list of lemmas to choose from is provided. In case of success, neatly separated sections of the dashboard are populated with widgets, each of which corresponds to one sense or distributional property of the word under scrutiny.


Fig. 2: Voces. User Interface: Frequency Spectrum Plot (Voces. User Interface (tempus 'time')

Word's frequency is summarised as a number of occurrences in the corpus (both raw and p.m.w. counts) and displayed as a highlighted point on a frequency spectrum plot (Baayen 2001). A barplot is provided for investigating change of frequency in subsequent corpus sections. Study of language variation is enabled through widgets presenting word's frequency as a function of such variables as author, work, genre, and - most importantly - time. Users are, therefore, provided with a list of authors who use the word most frequently or a word cloud summarising terms to be found in the titles of works with a particularly frequent use of the word under scrutiny. Genre variation is presented in form of a pie chart, while diachronic dimension - through a bar plot of frequency counts in partitions of the corpus. Diatopic variation study is still to be implemented.

A word's meaning potential can be investigated by means of a set of widgets presenting its contextual properties. The most frequent co-occurrences are enumerated on a simple count list which may be further analysed according to period and genre criteria. A Distributional Semantics Model (Baroni and Lenci 2010) is built from the corpus in order to enable simple meaning computation. Evert's (2014) wordspace package and a set of Alain Guerreau's scripts is employed in order to cluster co-occurrences. Similar terms of a looked up word are also computed and then presented in both textual and graphical form.

Users are supported in data and visualisation interpretation through hints which accompany every widget. Their role is to explain not only what the data can mean, but also how the figures were computed, how one can interpret the geometrical properties of a plot, and so on. This, along with the availability of data sets, code snippets, and reports generated on the fly, is what makes Voces a tool of reproductive research.

Architecture
Voces was built as a Shiny application (Chang et al. 2016). Its development was greatly facilitated by the availability of a decent documentation and community support (both particularly useful when dealing with framework's complex reactivity model). It turned out soon, however, that it may not be the best choice for web application which has to combine heterogeneous data and non-R code as well. Hence, other solutions are being tested at the moment, those in particular which would provide, for example, more flexible integration of external APIs. The most promising seems to be OpenCPU (Ooms 2014), an application which exposes R session through a RESTful API. This approach allows any application written in some of the less or more popular web development frameworks to easily communicate with an R server instance.

As for the architecture, Voces depends on a CQP server instance running in the background which requires corpora to be indexed with the CWB. Communication of the R server with the CWB is assured through the rcqp package (Desgraupes and Loiseau 2012) which offers a set of useful functions providing access to both positional (token-level) and structural (document-level) attributes. Unfortunately, development of this very helpful tool seems to be less active recently and thus Voces will soon accept also tabular data as input.

Previous research
Nowadays, corpus linguists may chose from a vast array of free, open source and stable corpus query systems (CQS) which not only allow for efficient indexing of large corpora, but also provide a user-friendly concordance interface and offer out-of-the-box a set of such essential functionalities as collocation lists, simple corpus statistics etc. Both web (CQPweb, NoSketchEngine etc.) and desktop applications (TXM etc.) are also usually equipped with a less or more intuitive corpus management interface. Voces, a dashboard for vocabulary research, is not yet another CQS and has no intention to supersede well-established tools which cannot be easily combated in terms of either robustness or speed. Quite the contrary, the application communicates with the CWB engine and adapts some of the design choices and features of the popular CQS, while hopefully does not inherit their drawbacks.

Unlike the case of the well-known CQS, more emphasis has been put on quick access to multifaceted information rather than on close analysis of occurrences. Voces does not attempt, then, to implement some of the features which are traditionally considered an important part of the corpus analytical toolbox, such as concordance sampling, sorting etc. Undoubtedly, the strength of popular CQS lies in their wide applicability: by default, they do not preclude any research scenario. Although agnostic of linguistic theory, Voces was originally built for more specific purposes and focuses on semantic properties of the word and its distribution.

What is believed to be one of the main advantages of the present application is that - thanks to its modular architecture - it can be easily extended or adopted by a researcher with even moderate programming skills. In that Voces attempts to fill the gap that exists between, on the one hand, fully-blown CQS, which are normally quite conservative when it comes to adding

new features, and, on the other hand, single-purpose

research workflows built ad hoc by researchers. What also distinguishes Voces from other CQS is its emphasis on helping users to interpret data. A system of visual and textual hints keeps a researcher informed about where does the data come from, how have they been computed etc.

The grid layout is well-known from analytical environment and is especially popular in finances or engineering (Few 2013); in humanities it was adopted, among others, in the Voyant Tools project. It offers a quick insight into otherwise dispersed data and a coherent account of word's properties.

Further research
Voces is currently in an early stage of development. The work focuses on adding new functionalities and plotting types which may sometimes affect application's efficiency. Future work will focus on: (1) optimising user experience; (2) implementing tools for (a)

comparative (ie. two-lemma) research and (b) tracking language change; (3) better processing user input (multi-word search).

Bibliography
Baayen, R. H. (2001). Word Frequency Distributions. Dordrecht: Kluwer.

Baroni, M., and Lenci, A. (2010). Distributional Memory: A

General Framework for Corpus-Based Semantics. Com-

putationalLinguistics 36 (4): 673-721.

Chang, W., Cheng, J., Allaire, J. J., Xie, Y., and McPherson,

J. (2016). Shiny: Web Application Framework for R.

https://CRAN.R-project.org/package=shiny.

Desgraupes, B., and Loiseau, S. (2012). Rcqp: Interface to the Corpus Query Protocol. http://CRAN.R-pro-ject.org/package=rcqp.

Evert, S. (2014). Distributional Semantics in R with the Wordspace Package. In Proceedings of COLING 2014, the

25th International Conference on Computational Linguistics: System Demonstrations, 110-114. Dublin, Ireland:

Dublin City University and Association for Computational Linguistics.

Few, S. (2013). Information Dashboard Design: Displaying Data for at-a-Glance Monitoring. Burlingame, CA: Analytics Press.

Nowak, K., and Bon, B. (2015). Medialatinitas.eu. Towards

Shallow Integration of Lexical, Textual and Encyclopaedic Resources for Latin. In Electronic Lexicography in the

21st Century: Linking Lexical Data in the Digital Age. Proceedings of the eLex 2015 Conference, edited by Iztok

Kosem, Milos Jakubi'cek, Jelena Kallas, and Simon Krek, 152-69. Ljubljana-Brighton: Trojina, Institute for Applied Slovene Studies - Lexical Computing Ltd.

Ooms, J. (2014). The OpenCPU System: Towards a Universal Interface for Scientific Computing through Separation of Concerns. ArXiv E-Prints, June.",txt,Creative Commons Attribution 4.0 International,,dashboard design;diachronic linguistics;latin;lexical statistics;r;semantics;shiny;visualisation,English,classical studies;corpora and corpus activities;lexicography;linguistics;medieval studies;natural language processing;philology;semantic analysis;software design and development;visualization,2017-01-01,"introduction
voces (from lat. vox 'voice', 'word') is an analysis

and visualisation dashboard for corpus-based research in lexical semantics. currently developed as a shiny application communicating with r session running in the background, voces provides users with possibly exhaustive account of how selected latin word is distributed across the corpus and what can be told about its meaning. the application is built around a corpus which currently consists of ca. 200m words from texts dating from the classical era (1 bce) to the middle ages (14th ce). although voces was originally conceived as a tool of historical semantics research, the application - due to its modular design - may be modified and the code basis can be re-used in new research contexts.

lexical semantics with r


information computed on a basis of a cwb-indexed corpus is presented to a user through a single-page interface composed of separate widgets arranged in a clear grid layout. each widget is responsible for displaying in textual or graphical form a clear-cut property of word's distribution or meaning. a heavy use of data visualisation techniques renders voces a convenient tool for exploratory analysis of textual corpora, but the grid layout is also reflection of modular architecture of the application. each widget is implemented as a separate function which can be extended and adopted by researchers with even limited r programming skills.

use scenarios
a typical use scenario is triggered when the user specifies a lemma to be looked up. if the search fails, a list of lemmas to choose from is provided. in case of success, neatly separated sections of the dashboard are populated with widgets, each of which corresponds to one sense or distributional property of the word under scrutiny.


fig. 2: voces. user interface: frequency spectrum plot (voces. user interface (tempus 'time')

word's frequency is summarised as a number of occurrences in the corpus (both raw and p.m.w. counts) and displayed as a highlighted point on a frequency spectrum plot (baayen 2001). a barplot is provided for investigating change of frequency in subsequent corpus sections. study of language variation is enabled through widgets presenting word's frequency as a function of such variables as author, work, genre, and - most importantly - time. users are, therefore, provided with a list of authors who use the word most frequently or a word cloud summarising terms to be found in the titles of works with a particularly frequent use of the word under scrutiny. genre variation is presented in form of a pie chart, while diachronic dimension - through a bar plot of frequency counts in partitions of the corpus. diatopic variation study is still to be implemented.

a word's meaning potential can be investigated by means of a set of widgets presenting its contextual properties. the most frequent co-occurrences are enumerated on a simple count list which may be further analysed according to period and genre criteria. a distributional semantics model (baroni and lenci 2010) is built from the corpus in order to enable simple meaning computation. evert's (2014) wordspace package and a set of alain guerreau's scripts is employed in order to cluster co-occurrences. similar terms of a looked up word are also computed and then presented in both textual and graphical form.

users are supported in data and visualisation interpretation through hints which accompany every widget. their role is to explain not only what the data can mean, but also how the figures were computed, how one can interpret the geometrical properties of a plot, and so on. this, along with the availability of data sets, code snippets, and reports generated on the fly, is what makes voces a tool of reproductive research.

architecture
voces was built as a shiny application (chang et al. 2016). its development was greatly facilitated by the availability of a decent documentation and community support (both particularly useful when dealing with framework's complex reactivity model). it turned out soon, however, that it may not be the best choice for web application which has to combine heterogeneous data and non-r code as well. hence, other solutions are being tested at the moment, those in particular which would provide, for example, more flexible integration of external apis. the most promising seems to be opencpu (ooms 2014), an application which exposes r session through a restful api. this approach allows any application written in some of the less or more popular web development frameworks to easily communicate with an r server instance.

as for the architecture, voces depends on a cqp server instance running in the background which requires corpora to be indexed with the cwb. communication of the r server with the cwb is assured through the rcqp package (desgraupes and loiseau 2012) which offers a set of useful functions providing access to both positional (token-level) and structural (document-level) attributes. unfortunately, development of this very helpful tool seems to be less active recently and thus voces will soon accept also tabular data as input.

previous research
nowadays, corpus linguists may chose from a vast array of free, open source and stable corpus query systems (cqs) which not only allow for efficient indexing of large corpora, but also provide a user-friendly concordance interface and offer out-of-the-box a set of such essential functionalities as collocation lists, simple corpus statistics etc. both web (cqpweb, nosketchengine etc.) and desktop applications (txm etc.) are also usually equipped with a less or more intuitive corpus management interface. voces, a dashboard for vocabulary research, is not yet another cqs and has no intention to supersede well-established tools which cannot be easily combated in terms of either robustness or speed. quite the contrary, the application communicates with the cwb engine and adapts some of the design choices and features of the popular cqs, while hopefully does not inherit their drawbacks.

unlike the case of the well-known cqs, more emphasis has been put on quick access to multifaceted information rather than on close analysis of occurrences. voces does not attempt, then, to implement some of the features which are traditionally considered an important part of the corpus analytical toolbox, such as concordance sampling, sorting etc. undoubtedly, the strength of popular cqs lies in their wide applicability: by default, they do not preclude any research scenario. although agnostic of linguistic theory, voces was originally built for more specific purposes and focuses on semantic properties of the word and its distribution.

what is believed to be one of the main advantages of the present application is that - thanks to its modular architecture - it can be easily extended or adopted by a researcher with even moderate programming skills. in that voces attempts to fill the gap that exists between, on the one hand, fully-blown cqs, which are normally quite conservative when it comes to adding

new features, and, on the other hand, single-purpose

research workflows built ad hoc by researchers. what also distinguishes voces from other cqs is its emphasis on helping users to interpret data. a system of visual and textual hints keeps a researcher informed about where does the data come from, how have they been computed etc.

the grid layout is well-known from analytical environment and is especially popular in finances or engineering (few 2013); in humanities it was adopted, among others, in the voyant tools project. it offers a quick insight into otherwise dispersed data and a coherent account of word's properties.

further research
voces is currently in an early stage of development. the work focuses on adding new functionalities and plotting types which may sometimes affect application's efficiency. future work will focus on: (1) optimising user experience; (2) implementing tools for (a)

comparative (ie. two-lemma) research and (b) tracking language change; (3) better processing user input (multi-word search).

bibliography
baayen, r. h. (2001). word frequency distributions. dordrecht: kluwer.

baroni, m., and lenci, a. (2010). distributional memory: a

general framework for corpus-based semantics. com-

putationallinguistics 36 (4): 673-721.

chang, w., cheng, j., allaire, j. j., xie, y., and mcpherson,

j. (2016). shiny: web application framework for r.

https://cran.r-project.org/package=shiny.

desgraupes, b., and loiseau, s. (2012). rcqp: interface to the corpus query protocol. http://cran.r-pro-ject.org/package=rcqp.

evert, s. (2014). distributional semantics in r with the wordspace package. in proceedings of coling 2014, the

25th international conference on computational linguistics: system demonstrations, 110-114. dublin, ireland:

dublin city university and association for computational linguistics.

few, s. (2013). information dashboard design: displaying data for at-a-glance monitoring. burlingame, ca: analytics press.

nowak, k., and bon, b. (2015). medialatinitas.eu. towards

shallow integration of lexical, textual and encyclopaedic resources for latin. in electronic lexicography in the

21st century: linking lexical data in the digital age. proceedings of the elex 2015 conference, edited by iztok

kosem, milos jakubi'cek, jelena kallas, and simon krek, 152-69. ljubljana-brighton: trojina, institute for applied slovene studies - lexical computing ltd.

ooms, j. (2014). the opencpu system: towards a universal interface for scientific computing through separation of concerns. arxiv e-prints, june.",1.0,1.0,Voyant
4008,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Embedding Digital Humanities in a Classics Master Programme,https://dh2017.adho.org/abstracts/385/385.pdf,Aurélien Berra,poster / demo / art installation,"Introduction

Pedagogy in the Digital Humanities is now leaving its “bracketed” state - a term used by HIRSCH 2012 to emphasise the fact that this dimension was not given the consideration its practical importance deserves. As programmes and courses are created on a larger scale and increasingly drive institutional strategies, also in Europe (see Sahle, 2013 and the DARIAH Digital Humanities Course Registry), it becomes essential to make comparisons and shared reflections possible.

Since 2014 all students of Greek and Latin languages and literatures at the Université Paris-Ouest Nanterre (France) have been enrolling in a Master programme entitled “Humanités classiques et humani-tés numériques.” Each semester features a fully fledged course of Digital Humanities: it is therefore an experiment in embedding Digital Humanities into an existing discipline, or rather into the array of disciplines which constitute the field of Classical studies around its philological backbone.

The aim of this poster is to share the approach I take in designing and teaching these courses, and to reflect on what this experience suggests about digital educational models, in Classics and beyond.

The poster will have three components, devoted to situating, describing and comparing the courses. Context and History

I will set out the conditions in which the curriculum was reformed (which involves both national and local contexts), the specific problems encountered (as the heterogeneous levels and motivations of the students, the relationships with the other courses, the available technical options, or the recent introduction of podcasting and distance learning), as well as the rationale and methods which shape the courses, including its main sources of inspiration in the Digital

Humanities community, whether online syllabi or publications like Jockers (2014) and Rockwell and Sinclair (2016).

Overview of the Courses

The courses alternately take the form of more traditional classes and collaborative or personal projects. Across the two years, their contents include theoretical and historical insights, while concentrating on hands-on experience: digital literacy elements are gradually integrated as students go from traditional scholarly editing recreated in Markdown and HTML to critical editing in TEI XML (the focus of year 1) and, beyond text and editing, discover computer-assisted analytical and visualisation methods with the Voyant Tools software environment and then work in a literate programming framework (For which the canonical reference is Knuth, 1984) implemented in R Markdown (the focus of year 2, see Figure 1).


Figure 1: Text analysis in RStudio

The principles of the courses will be expounded: favouring active participation, learning-by-doing and flipped classroom teaching; insisting on the critical, reflexive dimension of digital procedures; promoting free resources like TEI by Example (Van den Branden, Terras, and Vanhoutte) and The Programming Historian (Crymble et al), as well as data reuse; developing an open publication culture through the Classiques et numériques blog maintained by the students (see Figure 2) or a shared Zotero group library; creating an awareness of the surrounding Digital Humanities communities; fostering actual collaboration, both between the students and with other projects or programmes - to date, with another MA specialised in Web design on an online edition prototype, with the Pelagios Commons project on the annotation of place names and with the Sunoikisis

Digital Classics network in its effort to collectively define a core syllabus.


Figure 2: Classiques et numériques, the blog of the MA

Sahle, P. (2013). “DH Studieren! Auf Dem Weg Zu Einem Kern- Und Referenzcurriculum Der Digital Humanities.” DARIAH-DE Working Papers, 1. http://web-

doc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-1.pdf.

Van den Branden, R., Terras, M. and Vanhoutte, E. (n.d.)

“TEI by Example.” accessed 1 November 2016.

http://teibyexample.org/.

Comparing Models

Finally, drawing on this experience I will address several aspects of the current development of Di-gital Humanities pedagogy: as a separate entreprise or within established disciplines, with or without infrastructural, collegial or cross-departmental support, in various time formats, with different modes of external collaboration, etc. To sketch this broader typology, I will compare this French series of courses with other models, using in particular the data contributed to the aforementioned Digital Humanities Course Registry.

The poster will be in English, but I will naturally interact with the audience of the poster sessions both in English and French.

Bibliography

Crymble, A., Gibbs, F., Hegel, A., McDaniel, C., Milligan, I.,

Taparata, E., Visconti, A. and Wieringa, J. (eds.) (n.d.)

. The Programming Historian. http://programmin-ghistorian.org/.

Hirsch, B. (ed.) (2012). Digital Humanities Pedagogy: Practices, Principles and Politics. Open Book Publishers. http://www.openbookpublishers.com/reader/161.

Jockers, M. (2014). Text Analysis with R for Students of Literature. New York: Springer.

Knuth, D. (1984). “Literate Programming.” The Computer

Journal, 27(2): 97-111. http://comjnl.oxfordjour-

nals.org/content/27/2/97.short.

Rockwell, G. and Sinclair, S. (1984). Hermeneutica. Computer-Assisted Interpretation in the Humanities. Cambridge, Massachusetts: MIT Press.",txt,Creative Commons Attribution 4.0 International,,classics;digital literacy;disciplines;models;pedagogy,English,"classical studies;corpora and corpus activities;data mining / text mining;encoding - theory and practice;scholarly editing;teaching, pedagogy, and curriculum;text analysis",2017-01-01,"introduction

pedagogy in the digital humanities is now leaving its “bracketed” state - a term used by hirsch 2012 to emphasise the fact that this dimension was not given the consideration its practical importance deserves. as programmes and courses are created on a larger scale and increasingly drive institutional strategies, also in europe (see sahle, 2013 and the dariah digital humanities course registry), it becomes essential to make comparisons and shared reflections possible.

since 2014 all students of greek and latin languages and literatures at the université paris-ouest nanterre (france) have been enrolling in a master programme entitled “humanités classiques et humani-tés numériques.” each semester features a fully fledged course of digital humanities: it is therefore an experiment in embedding digital humanities into an existing discipline, or rather into the array of disciplines which constitute the field of classical studies around its philological backbone.

the aim of this poster is to share the approach i take in designing and teaching these courses, and to reflect on what this experience suggests about digital educational models, in classics and beyond.

the poster will have three components, devoted to situating, describing and comparing the courses. context and history

i will set out the conditions in which the curriculum was reformed (which involves both national and local contexts), the specific problems encountered (as the heterogeneous levels and motivations of the students, the relationships with the other courses, the available technical options, or the recent introduction of podcasting and distance learning), as well as the rationale and methods which shape the courses, including its main sources of inspiration in the digital

humanities community, whether online syllabi or publications like jockers (2014) and rockwell and sinclair (2016).

overview of the courses

the courses alternately take the form of more traditional classes and collaborative or personal projects. across the two years, their contents include theoretical and historical insights, while concentrating on hands-on experience: digital literacy elements are gradually integrated as students go from traditional scholarly editing recreated in markdown and html to critical editing in tei xml (the focus of year 1) and, beyond text and editing, discover computer-assisted analytical and visualisation methods with the voyant tools software environment and then work in a literate programming framework (for which the canonical reference is knuth, 1984) implemented in r markdown (the focus of year 2, see figure 1).


figure 1: text analysis in rstudio

the principles of the courses will be expounded: favouring active participation, learning-by-doing and flipped classroom teaching; insisting on the critical, reflexive dimension of digital procedures; promoting free resources like tei by example (van den branden, terras, and vanhoutte) and the programming historian (crymble et al), as well as data reuse; developing an open publication culture through the classiques et numériques blog maintained by the students (see figure 2) or a shared zotero group library; creating an awareness of the surrounding digital humanities communities; fostering actual collaboration, both between the students and with other projects or programmes - to date, with another ma specialised in web design on an online edition prototype, with the pelagios commons project on the annotation of place names and with the sunoikisis

digital classics network in its effort to collectively define a core syllabus.


figure 2: classiques et numériques, the blog of the ma

sahle, p. (2013). “dh studieren! auf dem weg zu einem kern- und referenzcurriculum der digital humanities.” dariah-de working papers, 1. http://web-

doc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-1.pdf.

van den branden, r., terras, m. and vanhoutte, e. (n.d.)

“tei by example.” accessed 1 november 2016.

http://teibyexample.org/.

comparing models

finally, drawing on this experience i will address several aspects of the current development of di-gital humanities pedagogy: as a separate entreprise or within established disciplines, with or without infrastructural, collegial or cross-departmental support, in various time formats, with different modes of external collaboration, etc. to sketch this broader typology, i will compare this french series of courses with other models, using in particular the data contributed to the aforementioned digital humanities course registry.

the poster will be in english, but i will naturally interact with the audience of the poster sessions both in english and french.

bibliography

crymble, a., gibbs, f., hegel, a., mcdaniel, c., milligan, i.,

taparata, e., visconti, a. and wieringa, j. (eds.) (n.d.)

. the programming historian. http://programmin-ghistorian.org/.

hirsch, b. (ed.) (2012). digital humanities pedagogy: practices, principles and politics. open book publishers. http://www.openbookpublishers.com/reader/161.

jockers, m. (2014). text analysis with r for students of literature. new york: springer.

knuth, d. (1984). “literate programming.” the computer

journal, 27(2): 97-111. http://comjnl.oxfordjour-

nals.org/content/27/2/97.short.

rockwell, g. and sinclair, s. (1984). hermeneutica. computer-assisted interpretation in the humanities. cambridge, massachusetts: mit press.",1.0,1.0,Voyant
4071,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Digital Humanities from Scratch: A Pedagogy-Driven Investigation of an In-Copyright Corpus,https://dh2017.adho.org/abstracts/477/477.pdf,Brian Croxall,"paper, specified ""long paper""","Following the publication of Franco Moretti’s GrapHs, Maps, Trees, scholars looking to apply digital humanities methods to literature have increasingly been drawn to “distant reading.” The influence of distant reading in digital humanities is apparent not only in the work it has inspired (see, among others, Cordell and Smith; Elson, Dames, and McKeown; Jockers; Long and So; Rhody; and Underwood) but also for its regular inclusion as a method in courses introducing DH. “Teaching digital humanities,” it turns out, often means “teaching distant reading.”

Teaching students the techniques of distant reading can be challenging as it depends on re-framing the familiar object of study. But another difficulty altogether is that this approach depends on a digitized corpus; and such a corpus, in turn, depends on someone, somewhere doing the difficult labor of digitization. One might ask, then: if “teaching digital humanities” means “teaching distant reading,” shouldn’t it also mean “teaching digitization”?

In this paper, I will discuss a collaborative, multiyear assignment that I conducted in two of my “Introduction to Digital Humanities” courses at Emory University: the digitization and analysis of the complete works of Ernest Hemingway (Croxall). With the goal of teaching my students not only how to do distant reading but also about the intense labor that goes into corpus preparation, we digitized the whole of Hemingway’s work in just two weeks. Working from newly purchased copies of the texts, the students and I rapidly scanned hundreds of pages, performed and corrected optical character recognition, and assembled a corpus—with each of us spending no more than 4 hours on the task. Our from-scratch corpus was composed expressly so we could draw important distinctions among Hemingway’s works: individual works vs the whole collection; fiction vs non-fiction; and works published before while Hemingway was alive vs those that appeared after his death in 1961. I will detail what we learned from rapid digitization and how those lessons affected the second iteration of the assignment.

After preparing the corpus, students worked in groups to analyze the many works of Hemingway that they had not had time to read. Making use of Voyant Tools, they identified themes in the corpus and charted patterns that could never have been observed through regular, close reading methods. For example, the class confirmed that while Hemingway insists on writing about “men,” the women to whom they are attached are inevitably just “girls.” In an attempt to chart the patterns of Hemingway’s diction, another group of students investigated the terms he uses to introduce dialogue. Unsurprisingly, the students discovered that “said” is by far the most frequent such term across the entire corpus. What was more surprising, however, was to observe that in late and posthumous writings, the frequency of “said” suddenly drops by 50%. In short, by building our own corpus from scratch, the students were able to conduct original research, something that is relatively rare for many undergraduates in humanities programs.

Building our collection of texts from scratch had two critical advantages. First, we were able to create a small, relatively clean corpus whose provenance we knew. This provided a sense of confidence in the data as we began to distant read. Furthermore, while our analysis of Hemingway’s works was “distant” compared to traditional close reading of a single novel or story, it was not nearly as distant as projects that deal with several thousand texts. We became engaged, in short, in close-distant reading. Second, digitizing the texts ourselves allowed us to skirt a problem that frequently plagues distant reading texts from the twentieth century: copyright. As an educational endeavor focused on teaching the students how to prepare their research materials, this guerilla digitization project fell under the regime of fair use in the United States.

To close, I will discuss how students at Brown University and I have taken further steps with the Hemingway corpus and with their digital humanities education as we have used it as a means to explore the methods and utility of topic modeling. Topic modeling is frequently deployed to come to terms with large and unwieldy corpora (see Jockers; Nelson; Nelson, Mimno, and Brown; Underwood and Goldstone). But working with a small, relatively clean corpus that is created from scratch allows students to better understand what takes place via unsupervised machine learning. At the same time, topic modeling allows us to ask in a new way some of the same questions that my former students had already uncovered: how does Hemingway’s dialog differ from his prose? how different are the topics in Hemingway’s fiction from those of his non-fiction? to what degree does his late—or even posthumous—work differ from what he wrote three decades earlier?

In the end, the process of modeling Hemingway becomes a means by which we can model all of digital humanities—both analysis and corpus creation—in a student-focused environment (see also Brier; Croxall and Singer; Harris; Hirsch; Jewell and Lorang; and Swafford). By doing digital humanities from scratch, students can be engaged in original research and see for themselves, from start to finish, how digital humanities gets done.

Bibliography

Brier, S. (2012). “Where’s the Pedagogy? The Role of

Teaching and Learning in the Digital Humanities.” In

Gold, M. K. (ed), Debates in the Digital Humanities.

Minnesota University Press, pp. 350-367.

Cordell, R. and Smith, D. A. (2017). Viral Texts: Mapping

Networks of Reprinting in 19th-Century Newspapers and

Magazines. http: //viraltexts.org/ (accessed 7 April

2017).

Croxall, B. (2015). “How to NOT Read Hemingway.” Intro to DH.

http://www.briancroxall.net/s15dh/assignments/how

-to-not-read-hemingway/ (accessed 7 April 2017).

Croxall, B. and Singer, K. (2013). “The Future of

Undergraduate Digital Humanities.” Digital Humanities

2013, Lincoln, NE, July 2013.

Elson, D. K., Dames, N. and McKeown, K. R. (2010).

“Extracting Social Networks from Literary Fiction.”

Proceedings of the 48th Annual Meeting of the

Association for Computational Linguisticsi, Uppsala,

Sweden.

http://www.cs.columbia.edu/~delson/pubs/ACL2010-

ElsonDamesMcKeown.pdf (accessed 7 April 2017).

Goldstone, A. and Underwood, T. (2014). “The Quiet

Transformations of Literary Studies: What Thirteen

Thousand Scholars Could Tell Us.” New Literary History

45.3: 359-384.

Harris, K. D. (2011). “Pedagogy & Play: Revising Learning through Digital Humanities.” Digital Humanities 2011, Stanford, CA, June 2011.

Hirsch, B. D. (2012). Digital Humanities Pedagogy:

Practices, Principles and Politics. Open Book Publishers.

Jewell, A. and Lorang, E. (2016). “Teaching Digital Humanities Through a Community-Engaged, Team-Based Pedagogy.” Digital Humanities 2016, Krakow, Poland, July 2016.

Jockers, M. L. (2013). Macroanalysis: Digital Methods and Literary History. Urbana Champaign, IL: University of Illinois Press.

Long, H. and So, R. J. (2016). “Literary Pattern

Recognition: Modernism between Close Reading and Machine Learning.” Critical Inquiry 42.2: 235-267.

Moretti, F. (2013). Distant Reading. London: Verso. Moretti, F. (2007). Graphs, Maps, Trees. London: Verso.

Nelson, R. K. (2011). Mining the Dispatch.

http://dsl.richmond.edu/dispatch/ (accessed 7 April 2017).

Nelson, R. K., Mimno, D. and Brown, T. (2012) “Topic Modeling the Past.” Digital Humanities 2012, Hamburg, Germany, July 2012.

Rhody, L. M. (2013). “Revising Ekphrasis: Methods and Models.” The Association for Computers and the Humanities. http://ach.org/2013/12/30/revising-ekphrasis-methods-and-models/ (accessed 7 April 2017).

Sinclair, S. and Rockwell, G. (2017). Voyant Tools. http://voyant-tools.org/ (accessed 7 April 2017).

Swafford, J. E. (2016). “Read, Play, Build: Teaching Sherlock Holmes through Digital Humanities.” Digital Humanities 2016, Krakow, Poland, July 2016.

Underwood, T. (2013). Why Literary Periods Mattered: Historical Contrast and the Prestige of English Studies. Stanford: Stanford University Press.",txt,Creative Commons Attribution 4.0 International,,ernest hemingway;teaching;topic modeling;undergraduate,English,"copyright, licensing, and open access;crowdsourcing;cultural studies;data mining / text mining;english studies;project design, organization, management;teaching, pedagogy, and curriculum;text analysis",2017-01-01,"following the publication of franco moretti’s graphs, maps, trees, scholars looking to apply digital humanities methods to literature have increasingly been drawn to “distant reading.” the influence of distant reading in digital humanities is apparent not only in the work it has inspired (see, among others, cordell and smith; elson, dames, and mckeown; jockers; long and so; rhody; and underwood) but also for its regular inclusion as a method in courses introducing dh. “teaching digital humanities,” it turns out, often means “teaching distant reading.”

teaching students the techniques of distant reading can be challenging as it depends on re-framing the familiar object of study. but another difficulty altogether is that this approach depends on a digitized corpus; and such a corpus, in turn, depends on someone, somewhere doing the difficult labor of digitization. one might ask, then: if “teaching digital humanities” means “teaching distant reading,” shouldn’t it also mean “teaching digitization”?

in this paper, i will discuss a collaborative, multiyear assignment that i conducted in two of my “introduction to digital humanities” courses at emory university: the digitization and analysis of the complete works of ernest hemingway (croxall). with the goal of teaching my students not only how to do distant reading but also about the intense labor that goes into corpus preparation, we digitized the whole of hemingway’s work in just two weeks. working from newly purchased copies of the texts, the students and i rapidly scanned hundreds of pages, performed and corrected optical character recognition, and assembled a corpus—with each of us spending no more than 4 hours on the task. our from-scratch corpus was composed expressly so we could draw important distinctions among hemingway’s works: individual works vs the whole collection; fiction vs non-fiction; and works published before while hemingway was alive vs those that appeared after his death in 1961. i will detail what we learned from rapid digitization and how those lessons affected the second iteration of the assignment.

after preparing the corpus, students worked in groups to analyze the many works of hemingway that they had not had time to read. making use of voyant tools, they identified themes in the corpus and charted patterns that could never have been observed through regular, close reading methods. for example, the class confirmed that while hemingway insists on writing about “men,” the women to whom they are attached are inevitably just “girls.” in an attempt to chart the patterns of hemingway’s diction, another group of students investigated the terms he uses to introduce dialogue. unsurprisingly, the students discovered that “said” is by far the most frequent such term across the entire corpus. what was more surprising, however, was to observe that in late and posthumous writings, the frequency of “said” suddenly drops by 50%. in short, by building our own corpus from scratch, the students were able to conduct original research, something that is relatively rare for many undergraduates in humanities programs.

building our collection of texts from scratch had two critical advantages. first, we were able to create a small, relatively clean corpus whose provenance we knew. this provided a sense of confidence in the data as we began to distant read. furthermore, while our analysis of hemingway’s works was “distant” compared to traditional close reading of a single novel or story, it was not nearly as distant as projects that deal with several thousand texts. we became engaged, in short, in close-distant reading. second, digitizing the texts ourselves allowed us to skirt a problem that frequently plagues distant reading texts from the twentieth century: copyright. as an educational endeavor focused on teaching the students how to prepare their research materials, this guerilla digitization project fell under the regime of fair use in the united states.

to close, i will discuss how students at brown university and i have taken further steps with the hemingway corpus and with their digital humanities education as we have used it as a means to explore the methods and utility of topic modeling. topic modeling is frequently deployed to come to terms with large and unwieldy corpora (see jockers; nelson; nelson, mimno, and brown; underwood and goldstone). but working with a small, relatively clean corpus that is created from scratch allows students to better understand what takes place via unsupervised machine learning. at the same time, topic modeling allows us to ask in a new way some of the same questions that my former students had already uncovered: how does hemingway’s dialog differ from his prose? how different are the topics in hemingway’s fiction from those of his non-fiction? to what degree does his late—or even posthumous—work differ from what he wrote three decades earlier?

in the end, the process of modeling hemingway becomes a means by which we can model all of digital humanities—both analysis and corpus creation—in a student-focused environment (see also brier; croxall and singer; harris; hirsch; jewell and lorang; and swafford). by doing digital humanities from scratch, students can be engaged in original research and see for themselves, from start to finish, how digital humanities gets done.

bibliography

brier, s. (2012). “where’s the pedagogy? the role of

teaching and learning in the digital humanities.” in

gold, m. k. (ed), debates in the digital humanities.

minnesota university press, pp. 350-367.

cordell, r. and smith, d. a. (2017). viral texts: mapping

networks of reprinting in 19th-century newspapers and

magazines. http: //viraltexts.org/ (accessed 7 april

2017).

croxall, b. (2015). “how to not read hemingway.” intro to dh.

http://www.briancroxall.net/s15dh/assignments/how

-to-not-read-hemingway/ (accessed 7 april 2017).

croxall, b. and singer, k. (2013). “the future of

undergraduate digital humanities.” digital humanities

2013, lincoln, ne, july 2013.

elson, d. k., dames, n. and mckeown, k. r. (2010).

“extracting social networks from literary fiction.”

proceedings of the 48th annual meeting of the

association for computational linguisticsi, uppsala,

sweden.

http://www.cs.columbia.edu/~delson/pubs/acl2010-

elsondamesmckeown.pdf (accessed 7 april 2017).

goldstone, a. and underwood, t. (2014). “the quiet

transformations of literary studies: what thirteen

thousand scholars could tell us.” new literary history

45.3: 359-384.

harris, k. d. (2011). “pedagogy & play: revising learning through digital humanities.” digital humanities 2011, stanford, ca, june 2011.

hirsch, b. d. (2012). digital humanities pedagogy:

practices, principles and politics. open book publishers.

jewell, a. and lorang, e. (2016). “teaching digital humanities through a community-engaged, team-based pedagogy.” digital humanities 2016, krakow, poland, july 2016.

jockers, m. l. (2013). macroanalysis: digital methods and literary history. urbana champaign, il: university of illinois press.

long, h. and so, r. j. (2016). “literary pattern

recognition: modernism between close reading and machine learning.” critical inquiry 42.2: 235-267.

moretti, f. (2013). distant reading. london: verso. moretti, f. (2007). graphs, maps, trees. london: verso.

nelson, r. k. (2011). mining the dispatch.

http://dsl.richmond.edu/dispatch/ (accessed 7 april 2017).

nelson, r. k., mimno, d. and brown, t. (2012) “topic modeling the past.” digital humanities 2012, hamburg, germany, july 2012.

rhody, l. m. (2013). “revising ekphrasis: methods and models.” the association for computers and the humanities. http://ach.org/2013/12/30/revising-ekphrasis-methods-and-models/ (accessed 7 april 2017).

sinclair, s. and rockwell, g. (2017). voyant tools. http://voyant-tools.org/ (accessed 7 april 2017).

swafford, j. e. (2016). “read, play, build: teaching sherlock holmes through digital humanities.” digital humanities 2016, krakow, poland, july 2016.

underwood, t. (2013). why literary periods mattered: historical contrast and the prestige of english studies. stanford: stanford university press.",2.0,3.0,Voyant
4118,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,"An Open, Reproducible Method for Teaching Text Analysis with R",https://dh2017.adho.org/abstracts/544/544.pdf,Tassie Gniady;Eric Wernert,poster / demo / art installation,"Over the past year and a half, the Cyberinfrastructure for Digital Humanities (CyberDH) Group at Indiana University has been developing an open instructional workflow for text analysis that aims to build algorithmic understanding and basic coding skills before scaling up analyses (Gniady et al., 2017). We have chosen to bootstrap in R, a high level and high productivity language, with methods that are open, repeatable, and sustainable. The aim is to provide code templates that can be adapted, remixed, and scaled to fit a wide range of text analysis tasks. This poster presents our approach to teaching computational text analysis and a representative hypothetical case study in which two different users are able to start with the same corpus and adapt code to achieve very different end results in a way not currently possible with black box tools.

This paradigm is fundamentally different from that currently practiced by many in the digital humanities. Black-boxed tools with GUIs that hide computation are very popular for introducing new practitioners of text analysis in the digital humanities to basic algorithms and outputs. In 2012, AntConc was downloaded 120,000 times by users in 80 different countries (Anthony, 2014). Voyant 1.0 had 113 sites linking to it actively in 2012 (Sinclair and Rockwell, 2013) and the week Voyant 2.0 was released the server went down multiple times from excess traffic (@VoyantTools, 2016). However, one of its default corpora is the Shakespearean dramas, with speaker names and stage directions. ((Sinclair and Rockwell, 2016). The inclusion of speaker names skews all algorithms related to frequency counts of characters (e.g. word clouds), which a new user may not even think to take into account. Using AntConc's concordance tool with a Shakespearean corpora including speaker names gives an idea of when a character speaks and when a character is mentioned, but this conflation might not jump out at a new user. If anything, we suggest learning about algorithms first and then moving up to black-box tools when one has the means to critique them.

Having looked at popular “plug-and-play” tools for corpora visualization, it becomes evident that even simple visualizations can lead to inaccurate results if the user is not thinking through how a corpus is being processed to produce a result. We believe that if the user understands how the algorithm is generating visualizations, they can contribute more meaningfully to critiques of sophisticated algorithms when partnered with programmers or even go on to bootstrap themselves with awareness of their domain's particular caveats. Thus, we advocate teaching humanists the basics of coding to create conversant programmers similar to the methodology behind Matthew Jockers' Text Analysis with R for Students of Literature, but with a slightly slower ramp up. To this end we have a three-step process of introducing R: web-deployed Shiny apps, highly marked up RNotebooks, and lightly commented RScripts, both in “regular” and higher performance versions. All are available for download on Github (with associated sample data from Shakespeare and Twitter) (CyberDH Team, 2017). We hope that this simpler bootstrapping method that mixes code and explanation, pedagogy and self-driven inquiry, will be of use to those looking to onramp new practitioners who may go on to partner with programmers if needed or to remix available code to look at their own knowledge domain.

Bibliography

Anthony, L. (2016). Antconc 3.4.4. Software.

http://www.laurenceanthony.net/soft-

ware/antconc/.

Gniady, T. Thomas, G. and Kloster, D. (2017). Text Analysis Github Repository. https://github.com/cyberdh/Text-Analysis.

Jockers, M. (2014). Text Analysis with R for Students of Literature. New York: Springer International Publishing.

Sinclair, S. and Rockwell, G. (2013). “Voyant Notebooks: Literate Programming, Programming Literacy.” Digital

Humanities 2014: Conference Abstracts. Nebraska-Lin-

coin: http://dh2013.unl.edu/abstracts/ab-295.html.

Sinclair, S. and Rockwell, G. (2016). Voyant Tools. http://voyant-tools.org/.

@VoyantTools. Twitter. 8 April 2016.",txt,Creative Commons Attribution 4.0 International,,r;repeatable workflows;text analysis;visualization,English,"content analysis;english studies;literary studies;teaching, pedagogy, and curriculum;text analysis;visualization",2017-01-01,"over the past year and a half, the cyberinfrastructure for digital humanities (cyberdh) group at indiana university has been developing an open instructional workflow for text analysis that aims to build algorithmic understanding and basic coding skills before scaling up analyses (gniady et al., 2017). we have chosen to bootstrap in r, a high level and high productivity language, with methods that are open, repeatable, and sustainable. the aim is to provide code templates that can be adapted, remixed, and scaled to fit a wide range of text analysis tasks. this poster presents our approach to teaching computational text analysis and a representative hypothetical case study in which two different users are able to start with the same corpus and adapt code to achieve very different end results in a way not currently possible with black box tools.

this paradigm is fundamentally different from that currently practiced by many in the digital humanities. black-boxed tools with guis that hide computation are very popular for introducing new practitioners of text analysis in the digital humanities to basic algorithms and outputs. in 2012, antconc was downloaded 120,000 times by users in 80 different countries (anthony, 2014). voyant 1.0 had 113 sites linking to it actively in 2012 (sinclair and rockwell, 2013) and the week voyant 2.0 was released the server went down multiple times from excess traffic (@voyanttools, 2016). however, one of its default corpora is the shakespearean dramas, with speaker names and stage directions. ((sinclair and rockwell, 2016). the inclusion of speaker names skews all algorithms related to frequency counts of characters (e.g. word clouds), which a new user may not even think to take into account. using antconc's concordance tool with a shakespearean corpora including speaker names gives an idea of when a character speaks and when a character is mentioned, but this conflation might not jump out at a new user. if anything, we suggest learning about algorithms first and then moving up to black-box tools when one has the means to critique them.

having looked at popular “plug-and-play” tools for corpora visualization, it becomes evident that even simple visualizations can lead to inaccurate results if the user is not thinking through how a corpus is being processed to produce a result. we believe that if the user understands how the algorithm is generating visualizations, they can contribute more meaningfully to critiques of sophisticated algorithms when partnered with programmers or even go on to bootstrap themselves with awareness of their domain's particular caveats. thus, we advocate teaching humanists the basics of coding to create conversant programmers similar to the methodology behind matthew jockers' text analysis with r for students of literature, but with a slightly slower ramp up. to this end we have a three-step process of introducing r: web-deployed shiny apps, highly marked up rnotebooks, and lightly commented rscripts, both in “regular” and higher performance versions. all are available for download on github (with associated sample data from shakespeare and twitter) (cyberdh team, 2017). we hope that this simpler bootstrapping method that mixes code and explanation, pedagogy and self-driven inquiry, will be of use to those looking to onramp new practitioners who may go on to partner with programmers if needed or to remix available code to look at their own knowledge domain.

bibliography

anthony, l. (2016). antconc 3.4.4. software.

http://www.laurenceanthony.net/soft-

ware/antconc/.

gniady, t. thomas, g. and kloster, d. (2017). text analysis github repository. https://github.com/cyberdh/text-analysis.

jockers, m. (2014). text analysis with r for students of literature. new york: springer international publishing.

sinclair, s. and rockwell, g. (2013). “voyant notebooks: literate programming, programming literacy.” digital

humanities 2014: conference abstracts. nebraska-lin-

coin: http://dh2013.unl.edu/abstracts/ab-295.html.

sinclair, s. and rockwell, g. (2016). voyant tools. http://voyant-tools.org/.

@voyanttools. twitter. 8 april 2016.",4.0,5.0,Voyant
4160,2017 - Montréal,Montréal,Access/Accès,2017,ADHO,ADHO,McGill University;Université de Montréal,Montréal,,Canada,https://dh2017.adho.org/,Scaffolded Hermeneutica for Literary Scholars with Novice Technical Skills,https://dh2017.adho.org/abstracts/601/601.pdf,Jeremy Browne,"paper, specified ""short paper""","Hermeneutica
In Hermeneutica, Geoffrey Rockwell and Stéfan Sinclair (2016) argue for an approach to the digital humanities that deemphasizes the tool and positivist notions of proof. Their proposed approach, also called Hermeneutica, champions tool accessibility over tool sophistication. Similarly, scholarly play is legitimated as a useful step in developing research questions and as a means to reconsider established notions within literary disciplines. The aim of Hermeneutica as a methodology seems to be the generation of interesting humanistic questions as much as the resolution of open questions.

Rockwell and Sinclair demonstrate the difference between Hermeneutica and typical DH approaches by quoting from Gary Wong's 2009 blog post:

[Typical DH] takes the worst part of the scientific papers (really really long sets of tabular data in the body of the text) and the worst part of papers from the humanities (really really complicated language where simple language would have done) and puts it in one. If this is what the cooperation of computational text analysis and traditional literary analysis yield, I

am scared.

Because Hermeneutica attempts to join the best parts of these fields, it has the potential to turn DH into a discipline that is more useful for the vast majority of non-DH humanists. It could be the means of accelerating the mainstreaming of DH methods and bringing us to the eventual point where all humanities are digital—a destination Claire Clivaz described succinctly (DARIAH, 2016).

Voyant
One feature that distinguishes Hermeneutica from many other DH approaches is its companion set of tools meant to demonstrate its application. Voyant Tools, now referred to simply as Voyant, is a web-based, modular suite of tools meant to be “worth thinking with” (Rockwell and Sinclair, 2016: 10, original emphasis). The goal is to accommodate playful exploration of text and sharing of corpora across the web. It is not designed as an industrial-grade text analysis tool, but as a “toy” that allows scholars to uncover new questions and gain new appreciation of texts.

Current limitations of Hermeneutica
A fundamental component of Hermeneutica is that the scholar views text through the lens of Voyant (or other computational text analysis tools), and then synthesizes that experience with their prior knowledge of the text and its milieu. A problem that Voyant addresses, but does not solve, is that many scholars who know the most about specific texts lack the technological skills that would be considered pre-novice in DH circles. Voyant allows everyone with a text and a browser to explore word frequencies, collocations, etc., but it presupposes that the text is available and clean enough for use. In order for Hermeneutica to appeal to non-DH humanities scholars, these issues of text availability and the lack of user skill must first be addressed.

On the issue of text availability, it is not often that scholars wish to analyze text that is rare or missing. More often they are interested in text that is protected by various copyright laws, which prohibit posting the text to public websites such as Voyant. Thankfully, in the Unites States at least, Google Books' recent court victory (Stohr, 2016) now permits scholars to publish online the analysis results derived from copyrighted texts, so long as the original text is not recoverable by the user. To this end Rockwell and Sinclair developed Voyant 2's “non-consumptive” mode which restricts access to tools that allow full-text views.

While such developments represent Rockwell and Sinclair's amenability to meet the ever-evolving needs of Hermeneuticans, accommodating users' lack of technology skill is beyond the scope of their involvement. For example, it is not reasonable to expect the Voyant developers to be concerned over issues of text acquisition or text preparation. Rather, those con-cerns—while critical to expanding the pool of potential Hermeneuticans—are issues of local implementation. Similarly, it makes sense that Voyant would offer the ability to link to a corpus after uploading the text, but uploading the text and keeping track of various versions of corpora is beyond the scope of Voyant. A local practice of adding some structure around the

Voyant suite ought to make Hermeneutica useful to a far greater audience than it is now.

Scaffolding
In the field of instructional design, such structure is called scaffolding. Specifically, scaffolding refers to the process of providing learners adequate introduction and examples before allowing them to attempt a task on their own (Bruner, 1978). For scaffolded Her-meneutica, DH-savvy professionals can work to acquire, clean, and upload text to Voyant (and other tools), and then provide public listings of the resulting corpora.

Examples of scaffolded Hermeneutica
We have implemented this scaffolded Hermeneu-tica approach in our Office of Digital Humanities beginning with the Cormac McCarthy Corpus Project (CMCP). The CMCP includes 13 Voyant corpora of McCarthy's 10 novels: one for the complete works, one for each novel, and two for novels (The Orchard Keeper and The Road) where the narration has been segregated from the dialogue. But the linchpin of scaffolded Hermeneutica is the CMCP's publicly-accessible website that organizes these Voyant corpora. The website is built on WordPress with the Pods content management plugin, and contains information on McCarthy's work, descriptions of Voyant (and other tools), and listings of links to the Voyant corpora. An essential feature of the website's structure is the ability to accommodate revisions to the current corpora as well as the addition of other tools in the future. Already, there is a non-Voyant sentence structure search tool attached as a beta-testing option.

A rough version of the Cormac McCarthy Corpus Project was presented at the 2015 conference of the Cormac McCarthy Society. The reaction to these tools being available for public use was strongly positive. One attendee referred to the website as “a game-changer.”

The same scaffolded Hermeneutica is being implemented on two other projects: Machado a longa distancia and The Modernist Short Fiction Project. Preliminary demonstrations of the approach have yielded similar reactions to what we observed with the CMCP. Non-DH scholars become excited rather than anxious when the digital analysis tools are scaffolded to provide them ready access. In fact, these demonstrations turn into play sessions where non-DH scholars repeatedly request for certain words to be added to the frequency charts and other Voyant panels.

Hermeneutica and Voyant represent the greatest potential for growth in DH not because they are the most technologically or theoretically advanced developments, but because they are the most accessible to non-DH scholars. Still, they don't quite reach the ground level of technology skills possessed by most researchers in the humanities. The scaffolded Herme-neutica approach proposed in this paper seems to span that gap to make Hermeneutica more accessible.

Bibliography
Bruner, J. S. (1978). “The role of dialogue in language acquisition.” In Sinclair, A., Jarvelle, R., J., and W. J.M. Levelt (eds), The Child's Concept of Language. New York: Springer-Verlag.

DARIAH (2016). My Digital Humanities - Part 1. YouTube.

https://www.youtube.com/watch?v=I8aRtHW3b6g

(accessed 1 November 2016).

Rockwell, G. and Sinclair, S. (2016). Hermeneutica. Cambridge: MIT Press.

Stohr, G. (2016). Google Book Project Can Proceed as Supreme Court Spurns Appeal. Bloomberg Politics. http://www.bloomberg.com/politics/articles/2016-04-18/google-book-project-can-proceed-as-top-u-s-court-spurns-appeal (accessed 1 November 2016).

Conclusion",txt,Creative Commons Attribution 4.0 International,,hermenutica;text analysis;voyant,English,corpora and corpus activities;literary studies;text analysis,2017-01-01,"hermeneutica
in hermeneutica, geoffrey rockwell and stéfan sinclair (2016) argue for an approach to the digital humanities that deemphasizes the tool and positivist notions of proof. their proposed approach, also called hermeneutica, champions tool accessibility over tool sophistication. similarly, scholarly play is legitimated as a useful step in developing research questions and as a means to reconsider established notions within literary disciplines. the aim of hermeneutica as a methodology seems to be the generation of interesting humanistic questions as much as the resolution of open questions.

rockwell and sinclair demonstrate the difference between hermeneutica and typical dh approaches by quoting from gary wong's 2009 blog post:

[typical dh] takes the worst part of the scientific papers (really really long sets of tabular data in the body of the text) and the worst part of papers from the humanities (really really complicated language where simple language would have done) and puts it in one. if this is what the cooperation of computational text analysis and traditional literary analysis yield, i

am scared.

because hermeneutica attempts to join the best parts of these fields, it has the potential to turn dh into a discipline that is more useful for the vast majority of non-dh humanists. it could be the means of accelerating the mainstreaming of dh methods and bringing us to the eventual point where all humanities are digital—a destination claire clivaz described succinctly (dariah, 2016).

voyant
one feature that distinguishes hermeneutica from many other dh approaches is its companion set of tools meant to demonstrate its application. voyant tools, now referred to simply as voyant, is a web-based, modular suite of tools meant to be “worth thinking with” (rockwell and sinclair, 2016: 10, original emphasis). the goal is to accommodate playful exploration of text and sharing of corpora across the web. it is not designed as an industrial-grade text analysis tool, but as a “toy” that allows scholars to uncover new questions and gain new appreciation of texts.

current limitations of hermeneutica
a fundamental component of hermeneutica is that the scholar views text through the lens of voyant (or other computational text analysis tools), and then synthesizes that experience with their prior knowledge of the text and its milieu. a problem that voyant addresses, but does not solve, is that many scholars who know the most about specific texts lack the technological skills that would be considered pre-novice in dh circles. voyant allows everyone with a text and a browser to explore word frequencies, collocations, etc., but it presupposes that the text is available and clean enough for use. in order for hermeneutica to appeal to non-dh humanities scholars, these issues of text availability and the lack of user skill must first be addressed.

on the issue of text availability, it is not often that scholars wish to analyze text that is rare or missing. more often they are interested in text that is protected by various copyright laws, which prohibit posting the text to public websites such as voyant. thankfully, in the unites states at least, google books' recent court victory (stohr, 2016) now permits scholars to publish online the analysis results derived from copyrighted texts, so long as the original text is not recoverable by the user. to this end rockwell and sinclair developed voyant 2's “non-consumptive” mode which restricts access to tools that allow full-text views.

while such developments represent rockwell and sinclair's amenability to meet the ever-evolving needs of hermeneuticans, accommodating users' lack of technology skill is beyond the scope of their involvement. for example, it is not reasonable to expect the voyant developers to be concerned over issues of text acquisition or text preparation. rather, those con-cerns—while critical to expanding the pool of potential hermeneuticans—are issues of local implementation. similarly, it makes sense that voyant would offer the ability to link to a corpus after uploading the text, but uploading the text and keeping track of various versions of corpora is beyond the scope of voyant. a local practice of adding some structure around the

voyant suite ought to make hermeneutica useful to a far greater audience than it is now.

scaffolding
in the field of instructional design, such structure is called scaffolding. specifically, scaffolding refers to the process of providing learners adequate introduction and examples before allowing them to attempt a task on their own (bruner, 1978). for scaffolded her-meneutica, dh-savvy professionals can work to acquire, clean, and upload text to voyant (and other tools), and then provide public listings of the resulting corpora.

examples of scaffolded hermeneutica
we have implemented this scaffolded hermeneu-tica approach in our office of digital humanities beginning with the cormac mccarthy corpus project (cmcp). the cmcp includes 13 voyant corpora of mccarthy's 10 novels: one for the complete works, one for each novel, and two for novels (the orchard keeper and the road) where the narration has been segregated from the dialogue. but the linchpin of scaffolded hermeneutica is the cmcp's publicly-accessible website that organizes these voyant corpora. the website is built on wordpress with the pods content management plugin, and contains information on mccarthy's work, descriptions of voyant (and other tools), and listings of links to the voyant corpora. an essential feature of the website's structure is the ability to accommodate revisions to the current corpora as well as the addition of other tools in the future. already, there is a non-voyant sentence structure search tool attached as a beta-testing option.

a rough version of the cormac mccarthy corpus project was presented at the 2015 conference of the cormac mccarthy society. the reaction to these tools being available for public use was strongly positive. one attendee referred to the website as “a game-changer.”

the same scaffolded hermeneutica is being implemented on two other projects: machado a longa distancia and the modernist short fiction project. preliminary demonstrations of the approach have yielded similar reactions to what we observed with the cmcp. non-dh scholars become excited rather than anxious when the digital analysis tools are scaffolded to provide them ready access. in fact, these demonstrations turn into play sessions where non-dh scholars repeatedly request for certain words to be added to the frequency charts and other voyant panels.

hermeneutica and voyant represent the greatest potential for growth in dh not because they are the most technologically or theoretically advanced developments, but because they are the most accessible to non-dh scholars. still, they don't quite reach the ground level of technology skills possessed by most researchers in the humanities. the scaffolded herme-neutica approach proposed in this paper seems to span that gap to make hermeneutica more accessible.

bibliography
bruner, j. s. (1978). “the role of dialogue in language acquisition.” in sinclair, a., jarvelle, r., j., and w. j.m. levelt (eds), the child's concept of language. new york: springer-verlag.

dariah (2016). my digital humanities - part 1. youtube.

https://www.youtube.com/watch?v=i8arthw3b6g

(accessed 1 november 2016).

rockwell, g. and sinclair, s. (2016). hermeneutica. cambridge: mit press.

stohr, g. (2016). google book project can proceed as supreme court spurns appeal. bloomberg politics. http://www.bloomberg.com/politics/articles/2016-04-18/google-book-project-can-proceed-as-top-u-s-court-spurns-appeal (accessed 1 november 2016).

conclusion",20.0,20.0,Voyant
6266,2018 - Mexico City,Mexico City,Puentes/Bridges,2018,ADHO,ADHO;EHD,El Colegio de México;Universidad Nacional Autónoma de México (UNAM) (National Autonomous University of Mexico),Mexico City,,Mexico,https://dh2018.adho.org/,Archiving Small Twitter Datasets for Text Analysis: A Workshop for Beginners,,Ernesto Priego,workshop / tutorial,"<text>
        
            <p>Abstract </p>
            <p>In this workshop for non - coders, participants will be guided through two tasks: the first task will guide participants in creating an application to tap into Twitter’s API, in our case to get Twitter data. The second task will guide participants in the use of a Google spreadsheet to capture streaming (live) data from Twitter in order to archive it, download it and perform text analysis, data visualization and other studies. This workshop will include a brief introduction contextualizing social media data collection good practices including user data privacy issues. </p>
            <p>
                <hi rend=""bold"" xml:space=""preserve"">Keywords: </hi>Archiving, Data Collection, Social Media, Twitter, Text Analysis 
            </p>
            <p>Rationale </p>
            <p>Twitter data can be very valuable for researchers of perhaps all disciplines, not just DH. Given the difficulties to properly collect and analyse Twitter data as viewable from most Twitter Web and mobile clients (as most people use Twitter) and the very limited short - span of search results, there is the danger of losing huge amounts of valuable historical material. </p>
            <p>Tweets are like butterflies – one can only really look at them for long if one pins them down out of their natural environment. The reason why we have access to Twitter in any form is because of Twitter’s API, which stands for Application Programming Interface. Free access to historic Twitter search results is limited to the last 7 days. This is due to several reasons, including the incredible amount of data that is requested from Twitter’s API, and – this is an educated guess – not disconnected from the fact that Twitter’s business model relies on its data being a commodity that can be resold for research. Twitter’s data is stored and managed by Twitter’s enterprise API platform. </p>
            <p>For the researcher interested in researching Twitter data, this means that harvesting needs to be do ne not only through automated means but in real time. It also puts scholars without the required coding and data mining skills at a disadvantage. As a researcher, this basically means that there is no way to do proper research of Twitter data without understanding how it works at API level, and this means understanding the limitations and possibilities this imposes on researchers. </p>
            <p>What’s a n individual researcher without access to pay corporate access to do? The whole butterfly colony cannot be captured with the nets most of us have available. At small scale, however, and collecting in a timely fashion, it is still possible to capture interesting and more - or - less complete specimens using fairly simply, non - coding required methods. (The Library of Congress h s now 12 years’ worth of text - only Tweets. However, as before, the Library of Congress Twitter collection will remain embargoed and there was no projected timetable for providing public access as of 26 December 2017). </p>
            <p>Most researchers out there are likely not to benefit from access to huge Twitter data dumps. For researchers without much resources that are trying to do the talk whilst doing the walk, and conduct research 
                <hi rend=""italic"" xml:space=""preserve"">on </hi>Twitter and 
                <hi rend=""italic"" xml:space=""preserve"">about </hi>Twitter, this workshop and tutorial will guide participants into creating a Twitter application in order to tap into the Twitter API, followed 
            </p>
            <p>by the setting up of a Twitter Google Archiving Spreadsheet. Once a trial archive or dataset has been collected, we will attempt text analysis and basic visualisations using Excel and Voyant Tools. This workshop will include a brief introduction contextualizing social media data collection good practices including user data privacy and research ethics issues. </p>
            <p>Workshop Requirements </p>
            <p>• Room with projector and screen </p>
            <p>• Wifi access </p>
            <p>• Power plugs for participants to charge devices if required </p>
            <p>Participants Requirements </p>
            <p>• Interest in collecting small Twitter datasets and basic Text Analysis </p>
            <p>• Wifi - enabled Laptop with Excel or similar spreadsheet software </p>
            <p>• Twitter account, and the login credentials to access it (username and password) </p>
            <p>
                <hi rend=""bold"" xml:space=""preserve"">Tools We’ll Use </hi>
            </p>
            <p>• TAGS </p>
            <p>https://tags.hawksey.info/ </p>
            <p>• Voyant Tools </p>
            <p>https://voyant - tools.org/ </p>
            <p>El taller se puede dar también en español o bilingüe inglés - español. </p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>For complete references please follow links in the referenced outputs below and in the body of the text above. </bibl>
                    <bibl>Priego, E. 2018. #rfringe17: Top 230 Terms in Tweetage. </bibl>
                    <bibl>
                        <ref target=""https://epriego.blog/2017/08/05/rfringe17-top%20230-terms-in-tweetage/"">https://epriego.blog/2017/08/05/rfringe17-top 230-terms-in-tweetage/</ref>
                    </bibl>
                    <bibl>[Accessed 30 January 2018] </bibl>
                    <bibl>Priego, E., 2016. Bar Chart: Number of #DH2016 Tweets in Archive per Conference Day (Sunday 10 to Friday 15 July 2016 GMT). Available from: </bibl>
                    <bibl>
                        <ref target=""https://figshare.com/articles/Bar_Chart_Number_of_DH2016_Tweets_in_Archive_per_Conf%20erence_Day_Sunday_10_to_Friday_15_July_2016_GMT_/3490001/1"">https://figshare.com/articles/Bar_Chart_Number_of_DH2016_Tweets_in_Archive_per_Conf erence_Day_Sunday_10_to_Friday_15_July_2016_GMT_/3490001/1</ref> [Accessed 31 Jan 2018]. 
                    </bibl>
                    <bibl>Priego, E. 2016. “Stronger In”: Looking Into a Sample Archive of 1,005 StrongerIn Tweets. </bibl>
                    <bibl>
                        <ref target=""https://epriego.blog/2016/06/21/stronger-in-looking-into-a-sample-archive-of-1005-%20strongerin-tweets/"">https://epriego.blog/2016/06/21/stronger-in-looking-into-a-sample-archive-of-1005- strongerin-tweets/</ref> [Accessed 30 January 2018] 
                    </bibl>
                    <bibl>Priego, E. and Zarate, C., 2014. #MLA14 Twitter Archive, 9 - 12 January 2014. Available from: </bibl>
                    <bibl>
                        <ref target=""https://figshare.com/aticles_MLA14_Twitter_Archive_9_12_January_2014/924801/1"">https://figshare.com/aticles_MLA14_Twitter_Archive_9_12_January_2014/924801/1</ref>
                    </bibl>
                    <bibl>[Accessed 31 Jan 2018]. </bibl>
                    <bibl>Priego, E. 2014. Some Thoughts on Why You Would Like to Archive and Share [Small] Twitter Data Sets. Available from:</bibl>
                    <bibl>
                        <ref target=""https://epriego.blog/2014/05/28/some-thoughts-why-you-would-like-to-archive-and-share-twitter-small-data%20/"">https://epriego.blog/2014/05/28/some-thoughts-why-you-would-like-to-archive-and-share-twitter-small-data /</ref> [Accessed 30 January 2018] 
                    </bibl>
                    <bibl>Priego, E. 2014. Publicly available data from Twitter is public evidence and does not necessarily constitute an “ethical dilemma”. London School of Economics Impact Blog. Available from:</bibl>
                    <bibl>
                        <ref target=""http://blogs.lse.ac.uk/impactofsoc%20ialsciences/2014/05/28/twitter-as-public-evidence/"">http://blogs.lse.ac.uk/impactofsoc ialsciences/2014/05/28/twitter-as-public-evidence/</ref> [Accessed 30 January 2018] 
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,Creative Commons Attribution 4.0 International,,archiving;social media;text analysis;twitter,English,"archives, repositories, sustainability and preservation;computer science;english;library & information science;public humanities and community engaged scholarship;sociology;text analysis;visualization",2018-01-01,"<text>
        
            <p>abstract </p>
            <p>in this workshop for non - coders, participants will be guided through two tasks: the first task will guide participants in creating an application to tap into twitter’s api, in our case to get twitter data. the second task will guide participants in the use of a google spreadsheet to capture streaming (live) data from twitter in order to archive it, download it and perform text analysis, data visualization and other studies. this workshop will include a brief introduction contextualizing social media data collection good practices including user data privacy issues. </p>
            <p>
                <hi rend=""bold"" xml:space=""preserve"">keywords: </hi>archiving, data collection, social media, twitter, text analysis 
            </p>
            <p>rationale </p>
            <p>twitter data can be very valuable for researchers of perhaps all disciplines, not just dh. given the difficulties to properly collect and analyse twitter data as viewable from most twitter web and mobile clients (as most people use twitter) and the very limited short - span of search results, there is the danger of losing huge amounts of valuable historical material. </p>
            <p>tweets are like butterflies – one can only really look at them for long if one pins them down out of their natural environment. the reason why we have access to twitter in any form is because of twitter’s api, which stands for application programming interface. free access to historic twitter search results is limited to the last 7 days. this is due to several reasons, including the incredible amount of data that is requested from twitter’s api, and – this is an educated guess – not disconnected from the fact that twitter’s business model relies on its data being a commodity that can be resold for research. twitter’s data is stored and managed by twitter’s enterprise api platform. </p>
            <p>for the researcher interested in researching twitter data, this means that harvesting needs to be do ne not only through automated means but in real time. it also puts scholars without the required coding and data mining skills at a disadvantage. as a researcher, this basically means that there is no way to do proper research of twitter data without understanding how it works at api level, and this means understanding the limitations and possibilities this imposes on researchers. </p>
            <p>what’s a n individual researcher without access to pay corporate access to do? the whole butterfly colony cannot be captured with the nets most of us have available. at small scale, however, and collecting in a timely fashion, it is still possible to capture interesting and more - or - less complete specimens using fairly simply, non - coding required methods. (the library of congress h s now 12 years’ worth of text - only tweets. however, as before, the library of congress twitter collection will remain embargoed and there was no projected timetable for providing public access as of 26 december 2017). </p>
            <p>most researchers out there are likely not to benefit from access to huge twitter data dumps. for researchers without much resources that are trying to do the talk whilst doing the walk, and conduct research 
                <hi rend=""italic"" xml:space=""preserve"">on </hi>twitter and 
                <hi rend=""italic"" xml:space=""preserve"">about </hi>twitter, this workshop and tutorial will guide participants into creating a twitter application in order to tap into the twitter api, followed 
            </p>
            <p>by the setting up of a twitter google archiving spreadsheet. once a trial archive or dataset has been collected, we will attempt text analysis and basic visualisations using excel and voyant tools. this workshop will include a brief introduction contextualizing social media data collection good practices including user data privacy and research ethics issues. </p>
            <p>workshop requirements </p>
            <p>• room with projector and screen </p>
            <p>• wifi access </p>
            <p>• power plugs for participants to charge devices if required </p>
            <p>participants requirements </p>
            <p>• interest in collecting small twitter datasets and basic text analysis </p>
            <p>• wifi - enabled laptop with excel or similar spreadsheet software </p>
            <p>• twitter account, and the login credentials to access it (username and password) </p>
            <p>
                <hi rend=""bold"" xml:space=""preserve"">tools we’ll use </hi>
            </p>
            <p>• tags </p>
            <p>https://tags.hawksey.info/ </p>
            <p>• voyant tools </p>
            <p>https://voyant - tools.org/ </p>
            <p>el taller se puede dar también en español o bilingüe inglés - español. </p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    bibliography
                    <bibl>for complete references please follow links in the referenced outputs below and in the body of the text above. </bibl>
                    <bibl>priego, e. 2018. #rfringe17: top 230 terms in tweetage. </bibl>
                    <bibl>
                        <ref target=""https://epriego.blog/2017/08/05/rfringe17-top%20230-terms-in-tweetage/"">https://epriego.blog/2017/08/05/rfringe17-top 230-terms-in-tweetage/</ref>
                    </bibl>
                    <bibl>[accessed 30 january 2018] </bibl>
                    <bibl>priego, e., 2016. bar chart: number of #dh2016 tweets in archive per conference day (sunday 10 to friday 15 july 2016 gmt). available from: </bibl>
                    <bibl>
                        <ref target=""https://figshare.com/articles/bar_chart_number_of_dh2016_tweets_in_archive_per_conf%20erence_day_sunday_10_to_friday_15_july_2016_gmt_/3490001/1"">https://figshare.com/articles/bar_chart_number_of_dh2016_tweets_in_archive_per_conf erence_day_sunday_10_to_friday_15_july_2016_gmt_/3490001/1</ref> [accessed 31 jan 2018]. 
                    </bibl>
                    <bibl>priego, e. 2016. “stronger in”: looking into a sample archive of 1,005 strongerin tweets. </bibl>
                    <bibl>
                        <ref target=""https://epriego.blog/2016/06/21/stronger-in-looking-into-a-sample-archive-of-1005-%20strongerin-tweets/"">https://epriego.blog/2016/06/21/stronger-in-looking-into-a-sample-archive-of-1005- strongerin-tweets/</ref> [accessed 30 january 2018] 
                    </bibl>
                    <bibl>priego, e. and zarate, c., 2014. #mla14 twitter archive, 9 - 12 january 2014. available from: </bibl>
                    <bibl>
                        <ref target=""https://figshare.com/aticles_mla14_twitter_archive_9_12_january_2014/924801/1"">https://figshare.com/aticles_mla14_twitter_archive_9_12_january_2014/924801/1</ref>
                    </bibl>
                    <bibl>[accessed 31 jan 2018]. </bibl>
                    <bibl>priego, e. 2014. some thoughts on why you would like to archive and share [small] twitter data sets. available from:</bibl>
                    <bibl>
                        <ref target=""https://epriego.blog/2014/05/28/some-thoughts-why-you-would-like-to-archive-and-share-twitter-small-data%20/"">https://epriego.blog/2014/05/28/some-thoughts-why-you-would-like-to-archive-and-share-twitter-small-data /</ref> [accessed 30 january 2018] 
                    </bibl>
                    <bibl>priego, e. 2014. publicly available data from twitter is public evidence and does not necessarily constitute an “ethical dilemma”. london school of economics impact blog. available from:</bibl>
                    <bibl>
                        <ref target=""http://blogs.lse.ac.uk/impactofsoc%20ialsciences/2014/05/28/twitter-as-public-evidence/"">http://blogs.lse.ac.uk/impactofsoc ialsciences/2014/05/28/twitter-as-public-evidence/</ref> [accessed 30 january 2018] 
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",2.0,3.0,Voyant
6326,2018 - Mexico City,Mexico City,Puentes/Bridges,2018,ADHO,ADHO;EHD,El Colegio de México;Universidad Nacional Autónoma de México (UNAM) (National Autonomous University of Mexico),Mexico City,,Mexico,https://dh2018.adho.org/,LitViz: Visualizing Literary Data by Means of text2voronoi,,Tolga Uslu;Alexander Mehler;Dirk Meyer,"paper, specified ""long paper""","<text>
        
            <p>Abstract</p>
            <p>We present LitViz, a webbased tool for visualizing literary data which utilizes the text2voronoi algorithm to map natural language texts onto voronoi diagrams. These diagrams can be used, for example, to visually differentiate between (groups of) authors. Text2voronoi utilizes the paradigm of text visualization to reconstruct text classification (e.g., authorship attribution) as a task of image classification. This means that, in contrast to conventional approaches to text classifiction, we do not directly use linguistic features, but explore visual features derived from the texts' visualizations to perform operations on texts. We illustrate LitViz by means of 18 authors, each of whom is represented by 5 literary works.</p>
            <p>Introduction</p>
            <p>In this paper we present a new tool, called LitViz, for the visual depiction of literary works. To this end, we utilize the text2voronoi algorithm (see Mehler et al. (2016b)) which maps natural language texts to image representations. The idea is to generate images of texts which can be used instead of these texts’ symbolic information to characterize them, for example, in terms of authorship, topic or genre. Text2voronoi is in line with the paradigm of text visualization to reconstruct text classification (e.g., authorship attribution) as a task of image classification. In contrast to conventional approaches to text classification, we therefore do not directly use linguistic features, but explore visual features derived from the texts’ visualizations in order to identify, for example, their authors. We exemplify LitViz by means of 18 authors each of whom is represent by 5 literary works. LitViz allows for interacting with the visualizations of these works in two modes: two- and three-dimensionally (see Figure 1 and 2).</p>
            <p>
                <graphic n=""1001"" width=""16.007291666666667cm"" height=""6.517569444444445cm"" url=""Pictures/be74701dca4e08230d49e542171e6a5d.png"" rend=""inline""></graphic>
                <lb></lb>Figure 1: Visual depiction of E.T.A. Hoffmann’s Das steinerne Herz
                <lb></lb>
            </p>
            <p>Related Work</p>
            <p>The idea of visualizing literature was inspired by Martin Wattenberg’s The Shape of Song1 (Wattenberg, 2001; Wattenberg, 2002). Wattenberg explores identical or otherwise repetitive passages of a composition to visually depict them. This is done by means of semicircles, which combine repeated and repetitive positions in such a way that the micro- and macro-structure of a composition becomes visible. Our idea is to transpose this idea to the visualization of literary data. </p>
            <p>Kucher and Kerren (2015) give an overview of state-of-theart techniques of text visualization and present a website that allows for differentiating between these techniques. Cao and Cui (2016) provide a systematic review of many advanced visualization techniques and discuss the fundamental notion of information visualization. </p>
            <p>Mehler et al. (2016a) present a web tool called Wikidition which allows for automatically generating large-scale editions of text corpora. This is done by using multiple text mining tools for automatically linking lexical, sentential and textual data. The output is stored and visualized using a MediaWiki. Thus, any Wikidition is extensible by its readers based on the wiki principle. </p>
            <p>Rockwell and Sinclair (2016) present a detailed web tool, called Voyant tools, for visualizing texts. Unlike Voyant, our focus is on non-standard techniques of visualizing textual data that go beyond histograms, scatterplots, line charts and related tools. </p>
            <p>Generally speaking, text visualization supports distant reading as introduced and exemplified by Moretti (2013), Rule et al. (2015) and Michel et al. (2011). These approaches show how visualizations that support distant reading may look like to get overviews of documents by just looking at the final visualizations. LitViz is a tool following this tradition: it utilizes text2voronoi to extend the set of techniques mapping textual data. In this way, it combines Wattenberg’s approach with distant reading techniques from the point of view of text visualization.
                <lb></lb>
            </p>
            <p>
                <graphic n=""1002"" width=""16.007291666666667cm"" height=""5.847291666666667cm"" url=""Pictures/771f44c5ee65b6f49f69e348028c7e11.png"" rend=""inline""></graphic>
                <lb></lb>Figure 2: 3D visualization of Franz Kafka’s Der Kübelreiter.
            </p>
            <p>Model</p>
            <p>Our goal is to generate images from literary works in a way that text classifiers can be fed by the features of these iconic representations in order to perform classification experiments, for which usually linguistic features are explored. This is the task of the text2voronoi algorithm, which calculates image representations of texts in four steps Mehler et al. (2016b): In the first step, the input text is analyzed by means of TextImager Hemati et al. (2016) to extract linguistic features in the usual way, that is, features, spanning a vector space of linguistic data. In the second step, the resulting vector space is used to compute embeddings for each of the extracted linguistic features. Embeddings are produced by means of word2vec (Mikolov et al., 2013). In the third step, a voronoi tessellation of the embedded features is computed. As a result, each lexical feature is mapped onto a separate voronoi cell whose neighborhood reflects the feature’s syntagmatic and paradigmatic associations with other features of the same space. The topology of the voronoi cells spans a voronoi diagram that visually represents the input text. Each of these cells is characterized by its filling level, transparency and height (third dimension) thereby reflecting its co-occurrence statistics within the input text, while the position and size of a cell is determined by the embedding of the corresponding feature – for the mathematical details of this algorithm see Mehler et al. (2016b). Finally, the text2voronoi algorithm extracts visual features from the voronoi diagrams to feed classifiers performing classifications of the input texts. </p>
            <p>LitViz utilizes the first three steps of this algorithm. Unlike the classical text2voronoi procedure, it does not address the final step of classification. Rather, it gives access to voronoi diagrams of input texts via a two-dimensional graphical interface, which can be transformed into a three-dimensional one by means of user interaction. These two- and threedimensional text representations can be used by the user of LitViz to interact with the underlying input texts in order to highlight single voronoi cells, to change her or his reading perspective or to visually compare voronoi diagrams of different texts. In this way, LitViz paves the way to a kind of a comparative distant reading by making accessible the visual depictions of different texts in an interactive manner.</p>
            <p>The LitViz Tool</p>
            <p>We have selected 18 authors of German literature each of whom is represent by 5 literary works. The works are taken from the Project Gutenberg (https://www.gutenberg.org/) and visualized by means of the text2voronoi algorithm. Any of these examples is made accessible by the front page of LitViz (see Figure 3). When hovering over a voronoi cell of the voronoi diagram of a sample work, information about the underlying linguistic feature represented by this cell is displayed. According to Mehler et al. (2016b), we call these images VoTes: Voronoi diagram of a Text. LitViz presents VoTes via a graphical user interface for two- and three-dimensional interactive graphics. In this way, we go beyond Wattenberg’s 2D depictions of musical pieces. 
                <lb></lb>
            </p>
            <p>
                <graphic n=""1003"" width=""15.959666666666667cm"" height=""4.817180555555556cm"" url=""Pictures/481488721fc6847b4f57e22770d25408.png"" rend=""inline""></graphic>
                <lb></lb>Figure 3: Front page of LitViz.
            </p>
            <p>
                <lb></lb>The second page (tab) of LitViz gives access to the comparison tool. Here the user first selects the number of VoTes to be compared. Then the user selects a subset of works of the authors to be compared. In the example in Figure 4, we compare four VoTes of two authors: two VoTes of two works of Heinrich Heine (top) and two VoTes of Heinrich Mann (bottom). It is easy to see that these VoTes fall into two classes, depending on the underlying authorship. Heinrich Mann’s two VoTes are organized around a center that is composed of many small cells, while there is a small subgroup of peripheral cells that are large. In contrast to this, the two VoTes of Heinrich Heine do not display such a center and are more evenly distributed in terms of their size. It is a main task of LitViz to allow for such comparisons. In this way, that is, by interacting with the texts’ image representations and by using the mouse-over technique, the user can study single features and how they are related to other features of the same representational space. 
                <lb></lb>
            </p>
            <p>
                <graphic n=""1004"" width=""15.982597222222223cm"" height=""5.224638888888889cm"" url=""Pictures/1abfe2d045a494252a0cb0da2de2d714.png"" rend=""inline""></graphic>
                <lb></lb>Figure 4: Comparison tool: Heinrich Heine (top) in comparison to Heinrich Mann (bottom).
            </p>
            <p>
                <lb></lb>Last but not least, LitViz provides a so-called custom tab. Here, the user can upload and visualize its own texts. It is then possible to set filter options using an option tool (see Figure 5) in order to further restrict the visualization.
            </p>
            <p>
                <graphic n=""1005"" width=""15.982597222222223cm"" height=""10.06475cm"" url=""Pictures/bfe110d0986ba62a2e99ff851e33d720.png"" rend=""inline""></graphic>
                <lb></lb>Figure 5: Custom VoTe with filter options.
            </p>
            <p>Conclusion</p>
            <p>We introduced a novel web tool, called LitViz, for visually depicting natural language texts based on the text2voronoi algorithm. LitViz enables the comparison of the visualizations of different texts. This allows, for example, for comparing the styles of the underlying authors visually. In this way, we extend the existing tool palette of distant reading. LitViz can be accessed via: 
                <ref target=""http://alba.hucompute.org/text2voronoi"">http://alba.hucompute.org/text2voronoi</ref>
            </p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>Cao, N. and Cui, W. (2016). Introduction to Text Visualization. Atlantis Briefs in Artificial Intelligence. Atlantis Press. </bibl>
                    <bibl>Hemati, W., Uslu, T., and Mehler, A. (2016). TextImager: a distributed uima-based system for NLP. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 59–63.</bibl>
                    <bibl>Kucher, K. and Kerren, A. (2015). Text visualization techniques: Taxonomy, visual survey, and community in sights. In Visualization Symposium (PacificVis), 2015 IEEE Pacific, pages 117–121. IEEE.</bibl>
                    <bibl>Mehler, A., Gleim, R., vor der Bruck, T., Hemati, W., Uslu, ¨ T., and Eger, S. (2016a). Wikidition: Automatic lexiconization and linkification of text corpora. Information Technology, pages 70–79. </bibl>
                    <bibl>Mehler, A., Uslu, T., and Hemati, W. (2016b). Text2Voronoi: An image-driven approach to differential diagnosis. In Proceedings of the 5th Workshop on Vision and Language (VL’16) hosted by the 54th Annual Meeting of the Association for Computational Linguistics (ACL), Berlin. </bibl>
                    <bibl>Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Pickett, J. P., Hoiberg, D., Clancy, D., Norvig, P., Orwant, J., et al. (2011). Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176–182. </bibl>
                    <bibl>Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. </bibl>
                    <bibl>Moretti, F. (2013). Distant reading. Verso Books. </bibl>
                    <bibl>Rockwell, G. and Sinclair, S. (2016). Hermeneutica: Computer-Assisted Interpretation in the Humanities. MIT Press.</bibl>
                    <bibl>Rule, A., Cointet, J.-P., and Bearman, P. S. (2015). Lexical shifts, substantive changes, and continuity in state of the union discourse, 1790–2014. Proceedings of the National Academy of Sciences, 112(35):10837–10844. </bibl>
                    <bibl>Wattenberg, M. (2001). The shape of song. Website 
                        <ref target=""http://www.turbulence.org/Works/song/mono.html"">http://www.turbulence.org/Works/song/mono.html</ref>.
                    </bibl>
                    <bibl>Wattenberg, M. (2002). Arc diagrams: Visualizing structure in strings. In Information Visualization, 2002. INFOVIS 2002. IEEE Symposium on, pages 110–116. IEEE.</bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,Creative Commons Attribution 4.0 International,,distant reading;stylometry;text imaging;text visualization,English,"computer science;creative and performing arts, including writing;english;internet / world wide web;linguistics;stylistics and stylometry;visualization",2018-01-01,"<text>
        
            <p>abstract</p>
            <p>we present litviz, a webbased tool for visualizing literary data which utilizes the text2voronoi algorithm to map natural language texts onto voronoi diagrams. these diagrams can be used, for example, to visually differentiate between (groups of) authors. text2voronoi utilizes the paradigm of text visualization to reconstruct text classification (e.g., authorship attribution) as a task of image classification. this means that, in contrast to conventional approaches to text classifiction, we do not directly use linguistic features, but explore visual features derived from the texts' visualizations to perform operations on texts. we illustrate litviz by means of 18 authors, each of whom is represented by 5 literary works.</p>
            <p>introduction</p>
            <p>in this paper we present a new tool, called litviz, for the visual depiction of literary works. to this end, we utilize the text2voronoi algorithm (see mehler et al. (2016b)) which maps natural language texts to image representations. the idea is to generate images of texts which can be used instead of these texts’ symbolic information to characterize them, for example, in terms of authorship, topic or genre. text2voronoi is in line with the paradigm of text visualization to reconstruct text classification (e.g., authorship attribution) as a task of image classification. in contrast to conventional approaches to text classification, we therefore do not directly use linguistic features, but explore visual features derived from the texts’ visualizations in order to identify, for example, their authors. we exemplify litviz by means of 18 authors each of whom is represent by 5 literary works. litviz allows for interacting with the visualizations of these works in two modes: two- and three-dimensionally (see figure 1 and 2).</p>
            <p>
                <graphic n=""1001"" width=""16.007291666666667cm"" height=""6.517569444444445cm"" url=""pictures/be74701dca4e08230d49e542171e6a5d.png"" rend=""inline""></graphic>
                <lb></lb>figure 1: visual depiction of e.t.a. hoffmann’s das steinerne herz
                <lb></lb>
            </p>
            <p>related work</p>
            <p>the idea of visualizing literature was inspired by martin wattenberg’s the shape of song1 (wattenberg, 2001; wattenberg, 2002). wattenberg explores identical or otherwise repetitive passages of a composition to visually depict them. this is done by means of semicircles, which combine repeated and repetitive positions in such a way that the micro- and macro-structure of a composition becomes visible. our idea is to transpose this idea to the visualization of literary data. </p>
            <p>kucher and kerren (2015) give an overview of state-of-theart techniques of text visualization and present a website that allows for differentiating between these techniques. cao and cui (2016) provide a systematic review of many advanced visualization techniques and discuss the fundamental notion of information visualization. </p>
            <p>mehler et al. (2016a) present a web tool called wikidition which allows for automatically generating large-scale editions of text corpora. this is done by using multiple text mining tools for automatically linking lexical, sentential and textual data. the output is stored and visualized using a mediawiki. thus, any wikidition is extensible by its readers based on the wiki principle. </p>
            <p>rockwell and sinclair (2016) present a detailed web tool, called voyant tools, for visualizing texts. unlike voyant, our focus is on non-standard techniques of visualizing textual data that go beyond histograms, scatterplots, line charts and related tools. </p>
            <p>generally speaking, text visualization supports distant reading as introduced and exemplified by moretti (2013), rule et al. (2015) and michel et al. (2011). these approaches show how visualizations that support distant reading may look like to get overviews of documents by just looking at the final visualizations. litviz is a tool following this tradition: it utilizes text2voronoi to extend the set of techniques mapping textual data. in this way, it combines wattenberg’s approach with distant reading techniques from the point of view of text visualization.
                <lb></lb>
            </p>
            <p>
                <graphic n=""1002"" width=""16.007291666666667cm"" height=""5.847291666666667cm"" url=""pictures/771f44c5ee65b6f49f69e348028c7e11.png"" rend=""inline""></graphic>
                <lb></lb>figure 2: 3d visualization of franz kafka’s der kübelreiter.
            </p>
            <p>model</p>
            <p>our goal is to generate images from literary works in a way that text classifiers can be fed by the features of these iconic representations in order to perform classification experiments, for which usually linguistic features are explored. this is the task of the text2voronoi algorithm, which calculates image representations of texts in four steps mehler et al. (2016b): in the first step, the input text is analyzed by means of textimager hemati et al. (2016) to extract linguistic features in the usual way, that is, features, spanning a vector space of linguistic data. in the second step, the resulting vector space is used to compute embeddings for each of the extracted linguistic features. embeddings are produced by means of word2vec (mikolov et al., 2013). in the third step, a voronoi tessellation of the embedded features is computed. as a result, each lexical feature is mapped onto a separate voronoi cell whose neighborhood reflects the feature’s syntagmatic and paradigmatic associations with other features of the same space. the topology of the voronoi cells spans a voronoi diagram that visually represents the input text. each of these cells is characterized by its filling level, transparency and height (third dimension) thereby reflecting its co-occurrence statistics within the input text, while the position and size of a cell is determined by the embedding of the corresponding feature – for the mathematical details of this algorithm see mehler et al. (2016b). finally, the text2voronoi algorithm extracts visual features from the voronoi diagrams to feed classifiers performing classifications of the input texts. </p>
            <p>litviz utilizes the first three steps of this algorithm. unlike the classical text2voronoi procedure, it does not address the final step of classification. rather, it gives access to voronoi diagrams of input texts via a two-dimensional graphical interface, which can be transformed into a three-dimensional one by means of user interaction. these two- and threedimensional text representations can be used by the user of litviz to interact with the underlying input texts in order to highlight single voronoi cells, to change her or his reading perspective or to visually compare voronoi diagrams of different texts. in this way, litviz paves the way to a kind of a comparative distant reading by making accessible the visual depictions of different texts in an interactive manner.</p>
            <p>the litviz tool</p>
            <p>we have selected 18 authors of german literature each of whom is represent by 5 literary works. the works are taken from the project gutenberg (https://www.gutenberg.org/) and visualized by means of the text2voronoi algorithm. any of these examples is made accessible by the front page of litviz (see figure 3). when hovering over a voronoi cell of the voronoi diagram of a sample work, information about the underlying linguistic feature represented by this cell is displayed. according to mehler et al. (2016b), we call these images votes: voronoi diagram of a text. litviz presents votes via a graphical user interface for two- and three-dimensional interactive graphics. in this way, we go beyond wattenberg’s 2d depictions of musical pieces. 
                <lb></lb>
            </p>
            <p>
                <graphic n=""1003"" width=""15.959666666666667cm"" height=""4.817180555555556cm"" url=""pictures/481488721fc6847b4f57e22770d25408.png"" rend=""inline""></graphic>
                <lb></lb>figure 3: front page of litviz.
            </p>
            <p>
                <lb></lb>the second page (tab) of litviz gives access to the comparison tool. here the user first selects the number of votes to be compared. then the user selects a subset of works of the authors to be compared. in the example in figure 4, we compare four votes of two authors: two votes of two works of heinrich heine (top) and two votes of heinrich mann (bottom). it is easy to see that these votes fall into two classes, depending on the underlying authorship. heinrich mann’s two votes are organized around a center that is composed of many small cells, while there is a small subgroup of peripheral cells that are large. in contrast to this, the two votes of heinrich heine do not display such a center and are more evenly distributed in terms of their size. it is a main task of litviz to allow for such comparisons. in this way, that is, by interacting with the texts’ image representations and by using the mouse-over technique, the user can study single features and how they are related to other features of the same representational space. 
                <lb></lb>
            </p>
            <p>
                <graphic n=""1004"" width=""15.982597222222223cm"" height=""5.224638888888889cm"" url=""pictures/1abfe2d045a494252a0cb0da2de2d714.png"" rend=""inline""></graphic>
                <lb></lb>figure 4: comparison tool: heinrich heine (top) in comparison to heinrich mann (bottom).
            </p>
            <p>
                <lb></lb>last but not least, litviz provides a so-called custom tab. here, the user can upload and visualize its own texts. it is then possible to set filter options using an option tool (see figure 5) in order to further restrict the visualization.
            </p>
            <p>
                <graphic n=""1005"" width=""15.982597222222223cm"" height=""10.06475cm"" url=""pictures/bfe110d0986ba62a2e99ff851e33d720.png"" rend=""inline""></graphic>
                <lb></lb>figure 5: custom vote with filter options.
            </p>
            <p>conclusion</p>
            <p>we introduced a novel web tool, called litviz, for visually depicting natural language texts based on the text2voronoi algorithm. litviz enables the comparison of the visualizations of different texts. this allows, for example, for comparing the styles of the underlying authors visually. in this way, we extend the existing tool palette of distant reading. litviz can be accessed via: 
                <ref target=""http://alba.hucompute.org/text2voronoi"">http://alba.hucompute.org/text2voronoi</ref>
            </p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    bibliography
                    <bibl>cao, n. and cui, w. (2016). introduction to text visualization. atlantis briefs in artificial intelligence. atlantis press. </bibl>
                    <bibl>hemati, w., uslu, t., and mehler, a. (2016). textimager: a distributed uima-based system for nlp. in proceedings of coling 2016, the 26th international conference on computational linguistics: system demonstrations, pages 59–63.</bibl>
                    <bibl>kucher, k. and kerren, a. (2015). text visualization techniques: taxonomy, visual survey, and community in sights. in visualization symposium (pacificvis), 2015 ieee pacific, pages 117–121. ieee.</bibl>
                    <bibl>mehler, a., gleim, r., vor der bruck, t., hemati, w., uslu, ¨ t., and eger, s. (2016a). wikidition: automatic lexiconization and linkification of text corpora. information technology, pages 70–79. </bibl>
                    <bibl>mehler, a., uslu, t., and hemati, w. (2016b). text2voronoi: an image-driven approach to differential diagnosis. in proceedings of the 5th workshop on vision and language (vl’16) hosted by the 54th annual meeting of the association for computational linguistics (acl), berlin. </bibl>
                    <bibl>michel, j.-b., shen, y. k., aiden, a. p., veres, a., gray, m. k., pickett, j. p., hoiberg, d., clancy, d., norvig, p., orwant, j., et al. (2011). quantitative analysis of culture using millions of digitized books. science, 331(6014):176–182. </bibl>
                    <bibl>mikolov, t., chen, k., corrado, g., and dean, j. (2013). efficient estimation of word representations in vector space. arxiv preprint arxiv:1301.3781. </bibl>
                    <bibl>moretti, f. (2013). distant reading. verso books. </bibl>
                    <bibl>rockwell, g. and sinclair, s. (2016). hermeneutica: computer-assisted interpretation in the humanities. mit press.</bibl>
                    <bibl>rule, a., cointet, j.-p., and bearman, p. s. (2015). lexical shifts, substantive changes, and continuity in state of the union discourse, 1790–2014. proceedings of the national academy of sciences, 112(35):10837–10844. </bibl>
                    <bibl>wattenberg, m. (2001). the shape of song. website 
                        <ref target=""http://www.turbulence.org/works/song/mono.html"">http://www.turbulence.org/works/song/mono.html</ref>.
                    </bibl>
                    <bibl>wattenberg, m. (2002). arc diagrams: visualizing structure in strings. in information visualization, 2002. infovis 2002. ieee symposium on, pages 110–116. ieee.</bibl>
                </listbibl>
            </div>
        </back>
    </text>",2.0,2.0,Voyant
6429,2018 - Mexico City,Mexico City,Puentes/Bridges,2018,ADHO,ADHO;EHD,El Colegio de México;Universidad Nacional Autónoma de México (UNAM) (National Autonomous University of Mexico),Mexico City,,Mexico,https://dh2018.adho.org/,A Tool to Visualize Data on Scientific Performance in the Czech Republic,,Radim Hladík,poster / demo / art installation,"<text>
        
            <p>The poster introduces a project to develop a visualization application for a unique data source on Czech sciences. Information Register of R&amp;D Results (RIV) is the Czech Republic’s inventory of the outputs of basic and applied research since 1992. Although it is potentially an important source of data for analyses of various aspects of the intellectual organization and publication culture in Czech sciences, this particular data source has earned itself a pejorative nickname – “a coffee grinder” – for its central role in purely mechanistic science evaluation in the country.</p>
            <p>By employing text-mining technique that are standard in the digital humanities and by getting inspiration from visualization platforms such as 
                <ref target=""https://voyant-tools.org/"">
                    <hi rend=""italic"">Voyant Tools</hi>
                </ref> (Sinclair and Rockwell 2012), the project aims to contribute to the shift in the Czech narrative of science evaluation from the exclusively bibliometric perspective to a more diverse one. For example, the hope is that the visual display of the plethora of topics that are discussed in the research outputs registered in RIV will implicitly criticize the myopic vision in which all disciplines are leveled to the singular measure of the number of publications. The latter system is not only intellectually dubious, but it has had documented adverse effects on the quality of research results. Crucially, it stimulates institutions as well as individuals to prioritize quantity over quality (Good et al. 2015; Grančay, Vveinhardt, and Šumilo 2017).
            </p>
            <p>The ill-fated usage of the RIV data to mold nationwide fiscal policies for scientific research reminds us that data analytics is not necessarily a neutral enterprise. A proper treatment of the data is a matter that confronts a data analyst with questions on the borderline of ethics. Although it is perfectly feasible in technical terms, we wish to discourage users from attempts to track individuals researchers; instead we offer features that display institutional or disciplinary dimensions of the data (see Figure 1). Furthermore, the web application will provide a module to visualize textual information from the register. Textual strings, such as abstracts and keywords, have been part and parcel of the recorded entries, but have only served thus far as mere search terms. Meanwhile, the utility of textual data has been demonstrated in studies that strive to map the intellectual organization and relationships within and between disciplines (Leydesdroff 1989; Moody 2004).</p>
            <p> </p>
            <p>
                <figure>
                    <graphic url=""Pictures/0a5ef826babeca514dd3bb0573c8c44a.jpg""></graphic>
                </figure> 
            </p>
            <p>Figure 1. Using RIVVIZ to visualize a trend in the publication frequency of research outputs in the “J” (journal) category of the Information Register of R&amp;D Results for the discipline “Philosophy and Religion” [note: the data are only a sample used in the development version] </p>
            <p> </p>
            <p>The target group of the application are the researchers themselves. Namely, the textual module is intended to serve their needs by providing an overview of the trending topics in research or to identify institutions working on similar problems. The specialist user sub-group is envisaged to come from the fields focusing on social and other studies of science. The accessibility of visualized data and the simplicity of the interface can also attract journalists or other members of the public. The prospective users are also likely to be recruit from among the stakeholders in scientific policy-making and management who may wish to gain quick insights into the quantitatively assessed rates of output per research institutions or funding bodies. </p>
            <p>The RIVVIZ application is developed in the R language and deployed on the R Server platform using the standard Shiny library. The data are imported from the publicly available repository of the Czech Research, Development and Innovation Information System. The internal setup is also fairly straightforward, relying predominately on the Tidyverse collection of packages, with ggplot2 library being the primary engine for visualization tasks. The underlying principles of the “grammar of graphics”(Wickham 2009) are particularly suitable for programming a user-oriented environment that allows for a control over a wide range of visualization parameters.</p>
            <p>Giving the users more choices should help to make them more engaged with the application, although there is a trade-off between user-friendliness and complexity. Reasonable defaults can partially alleviate this dilemma. The user engagement will be important for the future application development (Galey and Ruecker 2010). In the case of visualization schemes, locking users in a single – no matter how aesthetically pleasing – perspective is problematic. The apparent self-explanatory style and transparent communication of images may draw attention away from the complex and multifaceted nature of the data by making some of their aspects more easily accessible than others (Drucker 2011).</p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Drucker, J.</hi> (2011). Humanities Approaches to Graphical Display. 
                        <hi rend=""italic"">Digital Humanities Quarterly</hi> (DHQ), 5(1).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Galey A. and Ruecker, S.</hi> (2010). How a Prototype Argues. 
                        <hi rend=""italic"">Literary and Linguistic Computing</hi>, 25 (4): 405-424.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Good, B., Vermeulen, N., Tiefenthaler, B. and Arnold, E.</hi> (2015). Counting Quality? The Czech Performance-Based Research Funding System. 
                        <hi rend=""italic"">Research Evaluation </hi>24 (2): 91–105.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Grančay, M., Vveinhardt, J. and Šumilo, Ē</hi>. (2017). Publish or Perish: How Central and Eastern European Economists Have Dealt with the Ever-Increasing Academic Publishing Requirements 2000–2015. 
                        <hi rend=""italic"">Scientometrics </hi>111 (3): 1813– 37.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Leydesdroff, L.</hi> (1989). Words and Co-Words as Indicators of Intellectual Organization. 
                        <hi rend=""italic"">Research Policy </hi>18 (4): 209–223.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Moody, J.</hi> (2004). The Structure of a Social Science Collaboration Network: Disciplinary Cohesion from 1963 to 1999. 
                        <hi rend=""italic"">American Sociological Review </hi>69 (2): 213–238.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sinclair, S., Rockwell, G. and the Voyant Tools Team</hi>. (2012). 
                        <hi rend=""italic"">Voyant Tools</hi> (web application).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Wickham, H.</hi> (2009). 
                        <hi rend=""italic"">Ggplot2: Elegant Graphics for Data Analysis</hi>. Dordrecht: Springer.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,Creative Commons Attribution 4.0 International,,evaluation;keywords;science;scientometrics;visualisation,English,bibliographic methods / textual studies;english;interface & user experience design/publishing & delivery systems/user studies/user needs;library & information science;public humanities and community engaged scholarship;sociology;visualization,2018-01-01,"<text>
        
            <p>the poster introduces a project to develop a visualization application for a unique data source on czech sciences. information register of r&amp;d results (riv) is the czech republic’s inventory of the outputs of basic and applied research since 1992. although it is potentially an important source of data for analyses of various aspects of the intellectual organization and publication culture in czech sciences, this particular data source has earned itself a pejorative nickname – “a coffee grinder” – for its central role in purely mechanistic science evaluation in the country.</p>
            <p>by employing text-mining technique that are standard in the digital humanities and by getting inspiration from visualization platforms such as 
                <ref target=""https://voyant-tools.org/"">
                    <hi rend=""italic"">voyant tools</hi>
                </ref> (sinclair and rockwell 2012), the project aims to contribute to the shift in the czech narrative of science evaluation from the exclusively bibliometric perspective to a more diverse one. for example, the hope is that the visual display of the plethora of topics that are discussed in the research outputs registered in riv will implicitly criticize the myopic vision in which all disciplines are leveled to the singular measure of the number of publications. the latter system is not only intellectually dubious, but it has had documented adverse effects on the quality of research results. crucially, it stimulates institutions as well as individuals to prioritize quantity over quality (good et al. 2015; grančay, vveinhardt, and šumilo 2017).
            </p>
            <p>the ill-fated usage of the riv data to mold nationwide fiscal policies for scientific research reminds us that data analytics is not necessarily a neutral enterprise. a proper treatment of the data is a matter that confronts a data analyst with questions on the borderline of ethics. although it is perfectly feasible in technical terms, we wish to discourage users from attempts to track individuals researchers; instead we offer features that display institutional or disciplinary dimensions of the data (see figure 1). furthermore, the web application will provide a module to visualize textual information from the register. textual strings, such as abstracts and keywords, have been part and parcel of the recorded entries, but have only served thus far as mere search terms. meanwhile, the utility of textual data has been demonstrated in studies that strive to map the intellectual organization and relationships within and between disciplines (leydesdroff 1989; moody 2004).</p>
            <p> </p>
            <p>
                <figure>
                    <graphic url=""pictures/0a5ef826babeca514dd3bb0573c8c44a.jpg""></graphic>
                </figure> 
            </p>
            <p>figure 1. using rivviz to visualize a trend in the publication frequency of research outputs in the “j” (journal) category of the information register of r&amp;d results for the discipline “philosophy and religion” [note: the data are only a sample used in the development version] </p>
            <p> </p>
            <p>the target group of the application are the researchers themselves. namely, the textual module is intended to serve their needs by providing an overview of the trending topics in research or to identify institutions working on similar problems. the specialist user sub-group is envisaged to come from the fields focusing on social and other studies of science. the accessibility of visualized data and the simplicity of the interface can also attract journalists or other members of the public. the prospective users are also likely to be recruit from among the stakeholders in scientific policy-making and management who may wish to gain quick insights into the quantitatively assessed rates of output per research institutions or funding bodies. </p>
            <p>the rivviz application is developed in the r language and deployed on the r server platform using the standard shiny library. the data are imported from the publicly available repository of the czech research, development and innovation information system. the internal setup is also fairly straightforward, relying predominately on the tidyverse collection of packages, with ggplot2 library being the primary engine for visualization tasks. the underlying principles of the “grammar of graphics”(wickham 2009) are particularly suitable for programming a user-oriented environment that allows for a control over a wide range of visualization parameters.</p>
            <p>giving the users more choices should help to make them more engaged with the application, although there is a trade-off between user-friendliness and complexity. reasonable defaults can partially alleviate this dilemma. the user engagement will be important for the future application development (galey and ruecker 2010). in the case of visualization schemes, locking users in a single – no matter how aesthetically pleasing – perspective is problematic. the apparent self-explanatory style and transparent communication of images may draw attention away from the complex and multifaceted nature of the data by making some of their aspects more easily accessible than others (drucker 2011).</p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    bibliography
                    <bibl>
                        <hi rend=""bold"">drucker, j.</hi> (2011). humanities approaches to graphical display. 
                        <hi rend=""italic"">digital humanities quarterly</hi> (dhq), 5(1).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">galey a. and ruecker, s.</hi> (2010). how a prototype argues. 
                        <hi rend=""italic"">literary and linguistic computing</hi>, 25 (4): 405-424.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">good, b., vermeulen, n., tiefenthaler, b. and arnold, e.</hi> (2015). counting quality? the czech performance-based research funding system. 
                        <hi rend=""italic"">research evaluation </hi>24 (2): 91–105.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">grančay, m., vveinhardt, j. and šumilo, ē</hi>. (2017). publish or perish: how central and eastern european economists have dealt with the ever-increasing academic publishing requirements 2000–2015. 
                        <hi rend=""italic"">scientometrics </hi>111 (3): 1813– 37.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">leydesdroff, l.</hi> (1989). words and co-words as indicators of intellectual organization. 
                        <hi rend=""italic"">research policy </hi>18 (4): 209–223.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">moody, j.</hi> (2004). the structure of a social science collaboration network: disciplinary cohesion from 1963 to 1999. 
                        <hi rend=""italic"">american sociological review </hi>69 (2): 213–238.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">sinclair, s., rockwell, g. and the voyant tools team</hi>. (2012). 
                        <hi rend=""italic"">voyant tools</hi> (web application).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">wickham, h.</hi> (2009). 
                        <hi rend=""italic"">ggplot2: elegant graphics for data analysis</hi>. dordrecht: springer.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",3.0,4.0,Voyant
6465,2018 - Mexico City,Mexico City,Puentes/Bridges,2018,ADHO,ADHO;EHD,El Colegio de México;Universidad Nacional Autónoma de México (UNAM) (National Autonomous University of Mexico),Mexico City,,Mexico,https://dh2018.adho.org/,Unanticipated Afterlives: Resurrecting Dead Projects and Research Data for Pedagogical Use,,Megan Finn Senseney;Paige Morgan;Miriam Posner;Andrea Thomer;Helene Williams,panel / roundtable,"<text>
        
            <div type=""div1"" rend=""DH-Heading1"">
                Overview
                <p>Pedagogical exercises in the digital humanities rely on student access to humanities data. While strategies range from instructor-prepared datasets (Sinclair and Rockwell, 2012) to having students digitize texts directly from print materials (Croxall, 2017), data repositories and web-based DH projects are two of the most attractive sources for identifying, appraising, and accessing data for classroom use.</p>
                <p>Yet data for teaching is rarely cited as a prime motivation or rationale for sharing research data. In “The Conundrum of Sharing Research Data,” Christine Borgman examines four rationales for sharing research data: (1) to reproduce or to verify research, (2) to make results of publicly funded research available to the public, (3) to enable others to ask new questions of extant data, and (4) to advance the state of research and innovation (2012). Pedagogy may be included implicitly in the third rationale, but by foregrounding pedagogical intentions, we can more readily operationalize a process for how we enable others to ask new questions of our data, which, in turn, will inform our motivations for sharing as well as the manner in which we do so.</p>
                <p>Web-based DH projects are often conceived and developed for public consumption with short-term support through grant funding. While initiatives such as these have proliferated since the 1990s, they often languish as legacy projects on institutional servers without clear plans for sustainability or sunsetting (Rockwell et al., 2014). Rather than construe long dormant projects as an institutional burden, these artifacts may continue to function as object lessons and raw materials for use in the DH classroom. Evaluating early digital projects based on their fitness for use as pedagogical datasets distinguishes the project from its component parts and allows aspects of the project to live on in new contexts.</p>
                <p>This panel will include representatives from five public research universities across the United States. We will begin with a brief overview, followed by four case studies. Each panelist will speak for fifteen to twenty minutes, leaving time for questions from—and conversations with—the audience. Cases are drawn from the DH 101 course at UCLA, the DH Librarianship course at the University of Washington, the University of Miami Libraries’ Legacy Site Adoption Project, and the Humanities Data workshop at DHOxSS. Our goal is to explore the intersection of data sharing and digital pedagogy to interrogate how past projects (whether formally archived or otherwise) are adopted as data sets for teaching and training; propose evaluation criteria for selecting these data sets; discuss what these classroom efforts indicate about the sustainability of DH projects (and their data); and examine how our knowledge of these classroom cases might inform curatorial decisions in active DH projects.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Learning from our mistakes: Using old projects to create better library/faculty collaborations
                <p>The Legacy Sites Adoption Project (LSAP) developed in response to what the library administration saw as a significant problem: the library website hosted nearly 40 digital projects built 5-20 years earlier by a former library faculty member, now malingering in various states of brokenness, but still placed prominently on the website. Retiring and removing the sites would erase the memory of the library’s institutional history, but repairing them would create an impossible burden for the web &amp; application development team; and would reinforce the idea of the library playing a service-and-support role in DH, rather than an active partnership.</p>
                <p>The solution that we are currently implementing is to experiment with making the legacy sites “adoptable”: the content and metadata of each site are made available as a zip file containing CSVs of data and metadata and accompanying images/audio/visual files, along with a readme pointing both to the current site on the library servers and an archived (and often more functional) version of the site in the Internet Archive’s Wayback Machine. Faculty and students are able to use the zip files as base material for creating their own version(s) of the original sites, either carrying on the original concept as stated or taking it in a new direction. The original versions of the sites present opportunities for classes to think about developing DH projects with a direct focus on revision -- potentially reading and critiquing the original sites through the lenses of recent scholarly essays, or considering the choices made by the original creators in the light of how DH practices and tools have changed since the sites were built.</p>
                <p>LSAP engages with ongoing questions about what makes a good entry point into digital humanities work. Instead of building entry points around particular tools (Omeka, Voyant, etc.); or around a particular research question or collection of material that is not a project yet, adopting legacy sites centers and foregrounds the iterative nature and inevitable fragility of project webpages, while making explicit the relationship between the websites and the flat files of their content. </p>
                <p>With LSAP, we are also attempting a positive intervention into collaborative relationships between departmental faculty and librarians. Frequently, faculty come to librarians to ask for support for a particular idea for a digital project; or to incorporate digital methodologies into a classroom setting. In such instances, the faculty member may have little experience or knowledge with various key factors, including scoping and scaling project milestones, the availability of digitized objects, copyright/permissions restrictions, and the affordances of out-of-the-box tools. Our hope is that by offering projects that are ripe for revision, and focusing on areas that are frequently taught and studied at the university, we can provide an entry-point for collaboration that is more appropriately bounded, resulting in less uncertainty and less labor-intensive experiences for faculty, students, and librarians. </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Awakening sleeping data for the DH classroom
                <p>As anyone who teaches digital humanities knows, humanities-related datasets are as hard to find as they are desirable. Since the closure of the Arts and Humanities Data Service in 2008, no centralized repository for humanities data has emerged. The DH instructor is faced with the necessity of scouring the web for data to share with students so that they can practice data-cleaning, -manipulation, and -visualization. Sometimes this data comes from libraries, archives, and museums, but it comes just as often from scholars’ long-hibernating research projects. Indeed, scholars are often surprised to learn that their data has taken on a new life as the basis for student projects.</p>
                <p>The last several decades have seen explosive growth in flexible, accessible tools for working with data. These new platforms offer possibilities for visualization and analysis that would only have been possible with custom programming just 10 or 15 years ago. Because of this palette of tools, even relatively inexperienced students can breathe new life into data left mostly untouched for years.</p>
                <p>This presentation offers some case studies of student projects built on “dormant” data, explaining how students are trained to analyze, contextualize, visualize, and make sense of data they had no involvement in collecting. It discusses best practices for providing this data, as well as a scaffolded approach to helping students become conversant in techniques for understanding and working with data. It suggests a “toolkit” of off-the-shelf platforms that are affordable and easy for students to grasp and shows how one can build on the other until even novice students are able to create full-fledged, sophisticated digital humanities projects in the space of a semester.</p>
                <p>For those who have collected data they wish to share with students, this presentation offers some suggestions for documenting, packaging, and contextualizing research data so that it is not only technically sound, but in a format that students can understand. It also offers a set of best practices for collaborating with students on a data-based research project, including methods for sharing, documenting, citing, and reusing data.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Fit for use: Repurposing research data, reconstructing provenance, and refining “clean” data
                <p>When it comes to teaching materials, data curation education may have become a victim of its own success: finding “dirty” data for classroom use is persistently difficult, in part because most published datasets have already been cleaned and curated! However, there are teachable moments to be found even when working with relatively “clean” data. Published data can be mined, re-structured, re-formatted and otherwise curated for new uses. Additionally, the process of tracking down and contextualizing already published datasets can prove instructive in and of itself. The detective work needed to understand someone else’s project, and to reconstruct its provenance, can reveal unexpected idiosyncrasies about the dataset, and thereby reveal useful data wrangling skills to be taught.</p>
                <p>In this talk, we describe our work finding, curating and reconstructing the provenance of “The Pettigrew Papers,” a published (and relatively clean) dataset we have used over two years of teaching week-long workshops on digital humanities data curation at the Digital Humanities at Oxford Summer School (DHOxSS). Thomas J. Pettigrew (also known as Thomas “Mummy” Pettigrew) was a Victorian surgeon, antiquarian, and Egypotologist. Pettigrew wrote several early texts on Egyptian mummies and was the founding treasurer of the British Archaeological Association. Though his correspondence is archived at Yale University’s Beinecke Rare Book and Manuscript Library, it came to our attention via a “data paper” published in the Journal of Open Archaeology Data (Moshenska, 2012), containing transcriptions of select letters. </p>
                <p>In our first year teaching with the Pettigrew dataset, we wrote simple Python scripts to mine named entities from the letters, and to pull out header information about the letters as a spreadsheet for cleaning in OpenRefine. In hands-on sessions, we asked students to consider how they would clean and curate the dataset for new uses: what steps would need to be taken to create a network diagram of the entities named in his letters? To create a map of his correspondents? To create a timeline? </p>
                <p>In our second year teaching with this dataset, we spent more time reconstructing the original provenance of the Pettigrew letters themselves. In addition to the hands-on sessions from the first year, we asked students to consider how they might improve the metadata for the original data paper, and how they might resolve discrepancies between the data paper and the original finding aids created by the Beinecke (Ducharme, 2010). We additionally discussed how they might incorporate copies of Pettigrew’s publications available in the HathiTrust Digital Library in their work.</p>
                <p>Overall, we found that asking students to clean and re-curate this already published dataset was only the starting point in our teaching; as we found further connections in digital libraries and archives beyond the original data paper, we identified subtle and important issues in the digital humanities and digital curation that guided our workshop design. In addition to teaching hands-on data cleaning and manipulation skills, we found it important to teach students a nuanced understanding of provenance: both in the sense of the archival “chain of custody” that contextualizes and validates a fonds, and in the sense of the processes that led to a dataset’s current form.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                Training DH librarians: Using old DH projects to move forward
                <p>The DH Librarianship course at the University of Washington Information School investigates the multiple roles librarians play in DH scholarship and prepares students for a wide range of career options in libraries, DH centers, and academic departments. DH librarian roles range from fully-credited collaborator with faculty to last-minute data cleaner, and everything in between. DH librarians also need to be prepared to support projects and research across the spectrum of disciplines, so we examine varying research methods across the humanities. The final project for the course asks that students locate an abandoned, or complete but aging DH project, and insert themselves as a librarian; they provide an evaluation of the content as well as the technology of the project and suggest ways to improve or update both.</p>
                <p>The data sets in these projects varies and examples include: hand-collated quotations by a famous author on a fan site; census numbers provided in a project about London families in the 17th century; a list of shooting locations for a television show; metadata for photos of logging camps in the Pacific Northwest; multimedia elements in a documentary film; boxes of music programs from a summer camp; and quilting patterns. </p>
                <p>Some projects also include the more typical (and larger) type of data set, such as those from HathiTrust or Google-generated Ngrams, but they have proven to be the exception. Working with small data sets means that cleaning doesn’t occupy much time during a 10-week quarter, and they can be rearranged quickly to utilize multiple visualization or data processing options.</p>
                <p>Students evaluate the data sets early in the process; in nearly all cases, data sets are either incomplete or inaccurate, and for some, updated data or other content is available. This is where the multi-disciplinary expertise of librarians comes in, as MLIS students are trained in searching out valid information sources from multiple perspectives, whether that’s using vendor-supplied databases, open web search engines, or (gasp) sources in print or microform. This is also where students begin to see the striation of roles between true collaborators, project leaders, subject specialists, technical consultants, or data-wranglers.</p>
                <p>In reviewing aging or abandoned projects, students learn how easily the data, other content, and the functionality of the site/project can be lost. This gives them the added perspective they need to start thinking about curation and preservation, rather than tackling those issues as add-ons if they have time.</p>
                <p>Through these immersive projects, students have a chance to see DH through multiple lenses: those of a potential user, a collaborator, and a disciplinary specialist. They learn how to re-create and improve on a project. In doing so, they gain experience in evaluating and collecting data as well as in multiple platforms and software that are prominent in DH (some current, some defunct). Some students also reach out to the original site or project owner, and in a few cases have worked with that person to update the project, putting preservation or stabilizing features in place for future users.</p>
            </div>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Borgman, C. L.</hi> (2012). The conundrum of sharing research data, 
                        <hi rend=""italic"">Journal of the Association for Information Science and Technology</hi>, 63(6): 1059-78.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Croxall, B.</hi> (2017). Digital humanities from scratch: A pedagogy-driven investigation of an in-copyright corpus
                        <hi rend=""italic"">, Digital Humanities 2017: Conference Abstracts</hi>, Montreal: McGill University, pp. 206-7.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ducharme, D. J.</hi> (2010). 
                        <hi rend=""italic"">Guide to the Pettigrew Papers OSB MSS 113</hi>. New Haven: Beinecke Rare Book and Manuscript Library. 
                        <ref target=""http://hdl.handle.net/10079/fa/beinecke.pettis1"">
                            <hi rend=""underline color(1155CC)"">http://hdl.handle.net/10079/fa/beinecke.pettis1</hi>
                        </ref> (accessed 27 April 2018).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Moshenska, G.</hi> (2012). Selected correspondence from the papers of Thomas Pettigrew (1791-1865), surgeon and antiquary. 
                        <hi rend=""italic"">Journal of Open Archaeology Data</hi>, 1(0). 
                        <ref target=""https://doi.org/10.5334/4f913ca0cbb89"">
                            <hi rend=""underline color(1155CC)"">https://doi.org/10.5334/4f913ca0cbb89</hi>
                        </ref> (accessed 27 April 2018).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Rockwell, G., Day, S., Yu., J., and Engel, M.</hi> (2014). Burying dead projects: depositing the Globalization Compendium. 
                        <hi rend=""italic"">Digital Humanities Quarterly</hi>, 8(2). Retrieved from 
                        <ref target=""http://www.digitalhumanities.org/dhq/vol/8/2/000179/000179.html"">
                            <hi rend=""underline color(1155CC)"">http://www.digitalhumanities.org/dhq/vol/8/2/000179/000179.html</hi>
                        </ref> (accessed 27 April 2018).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sinclair, S. and Rockwell, G.</hi> (2012). Teaching computer-assisted text analysis. In Hirsch, B. (ed) 
                        <hi rend=""italic"">Digital Humanities Pedagogy: Practices, Principles, Politics</hi>. Open Book Publishers, pp. 241-54. Retrieved from 
                        <ref target=""https://www.openbookpublishers.com/product.php/161"">
                            <hi rend=""underline color(1155CC)"">https://www.openbookpublishers.com/product.php/161</hi>
                        </ref> (accessed 27 April 2018).
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,Creative Commons Attribution 4.0 International,,data sharing;dh curriculum;digital pedagogy;legacy projects;research data,English,"archives, repositories, sustainability and preservation;english;library & information science;teaching, pedagogy, and curriculum",2018-01-01,"<text>
        
            <div type=""div1"" rend=""dh-heading1"">
                overview
                <p>pedagogical exercises in the digital humanities rely on student access to humanities data. while strategies range from instructor-prepared datasets (sinclair and rockwell, 2012) to having students digitize texts directly from print materials (croxall, 2017), data repositories and web-based dh projects are two of the most attractive sources for identifying, appraising, and accessing data for classroom use.</p>
                <p>yet data for teaching is rarely cited as a prime motivation or rationale for sharing research data. in “the conundrum of sharing research data,” christine borgman examines four rationales for sharing research data: (1) to reproduce or to verify research, (2) to make results of publicly funded research available to the public, (3) to enable others to ask new questions of extant data, and (4) to advance the state of research and innovation (2012). pedagogy may be included implicitly in the third rationale, but by foregrounding pedagogical intentions, we can more readily operationalize a process for how we enable others to ask new questions of our data, which, in turn, will inform our motivations for sharing as well as the manner in which we do so.</p>
                <p>web-based dh projects are often conceived and developed for public consumption with short-term support through grant funding. while initiatives such as these have proliferated since the 1990s, they often languish as legacy projects on institutional servers without clear plans for sustainability or sunsetting (rockwell et al., 2014). rather than construe long dormant projects as an institutional burden, these artifacts may continue to function as object lessons and raw materials for use in the dh classroom. evaluating early digital projects based on their fitness for use as pedagogical datasets distinguishes the project from its component parts and allows aspects of the project to live on in new contexts.</p>
                <p>this panel will include representatives from five public research universities across the united states. we will begin with a brief overview, followed by four case studies. each panelist will speak for fifteen to twenty minutes, leaving time for questions from—and conversations with—the audience. cases are drawn from the dh 101 course at ucla, the dh librarianship course at the university of washington, the university of miami libraries’ legacy site adoption project, and the humanities data workshop at dhoxss. our goal is to explore the intersection of data sharing and digital pedagogy to interrogate how past projects (whether formally archived or otherwise) are adopted as data sets for teaching and training; propose evaluation criteria for selecting these data sets; discuss what these classroom efforts indicate about the sustainability of dh projects (and their data); and examine how our knowledge of these classroom cases might inform curatorial decisions in active dh projects.</p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                learning from our mistakes: using old projects to create better library/faculty collaborations
                <p>the legacy sites adoption project (lsap) developed in response to what the library administration saw as a significant problem: the library website hosted nearly 40 digital projects built 5-20 years earlier by a former library faculty member, now malingering in various states of brokenness, but still placed prominently on the website. retiring and removing the sites would erase the memory of the library’s institutional history, but repairing them would create an impossible burden for the web &amp; application development team; and would reinforce the idea of the library playing a service-and-support role in dh, rather than an active partnership.</p>
                <p>the solution that we are currently implementing is to experiment with making the legacy sites “adoptable”: the content and metadata of each site are made available as a zip file containing csvs of data and metadata and accompanying images/audio/visual files, along with a readme pointing both to the current site on the library servers and an archived (and often more functional) version of the site in the internet archive’s wayback machine. faculty and students are able to use the zip files as base material for creating their own version(s) of the original sites, either carrying on the original concept as stated or taking it in a new direction. the original versions of the sites present opportunities for classes to think about developing dh projects with a direct focus on revision -- potentially reading and critiquing the original sites through the lenses of recent scholarly essays, or considering the choices made by the original creators in the light of how dh practices and tools have changed since the sites were built.</p>
                <p>lsap engages with ongoing questions about what makes a good entry point into digital humanities work. instead of building entry points around particular tools (omeka, voyant, etc.); or around a particular research question or collection of material that is not a project yet, adopting legacy sites centers and foregrounds the iterative nature and inevitable fragility of project webpages, while making explicit the relationship between the websites and the flat files of their content. </p>
                <p>with lsap, we are also attempting a positive intervention into collaborative relationships between departmental faculty and librarians. frequently, faculty come to librarians to ask for support for a particular idea for a digital project; or to incorporate digital methodologies into a classroom setting. in such instances, the faculty member may have little experience or knowledge with various key factors, including scoping and scaling project milestones, the availability of digitized objects, copyright/permissions restrictions, and the affordances of out-of-the-box tools. our hope is that by offering projects that are ripe for revision, and focusing on areas that are frequently taught and studied at the university, we can provide an entry-point for collaboration that is more appropriately bounded, resulting in less uncertainty and less labor-intensive experiences for faculty, students, and librarians. </p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                awakening sleeping data for the dh classroom
                <p>as anyone who teaches digital humanities knows, humanities-related datasets are as hard to find as they are desirable. since the closure of the arts and humanities data service in 2008, no centralized repository for humanities data has emerged. the dh instructor is faced with the necessity of scouring the web for data to share with students so that they can practice data-cleaning, -manipulation, and -visualization. sometimes this data comes from libraries, archives, and museums, but it comes just as often from scholars’ long-hibernating research projects. indeed, scholars are often surprised to learn that their data has taken on a new life as the basis for student projects.</p>
                <p>the last several decades have seen explosive growth in flexible, accessible tools for working with data. these new platforms offer possibilities for visualization and analysis that would only have been possible with custom programming just 10 or 15 years ago. because of this palette of tools, even relatively inexperienced students can breathe new life into data left mostly untouched for years.</p>
                <p>this presentation offers some case studies of student projects built on “dormant” data, explaining how students are trained to analyze, contextualize, visualize, and make sense of data they had no involvement in collecting. it discusses best practices for providing this data, as well as a scaffolded approach to helping students become conversant in techniques for understanding and working with data. it suggests a “toolkit” of off-the-shelf platforms that are affordable and easy for students to grasp and shows how one can build on the other until even novice students are able to create full-fledged, sophisticated digital humanities projects in the space of a semester.</p>
                <p>for those who have collected data they wish to share with students, this presentation offers some suggestions for documenting, packaging, and contextualizing research data so that it is not only technically sound, but in a format that students can understand. it also offers a set of best practices for collaborating with students on a data-based research project, including methods for sharing, documenting, citing, and reusing data.</p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                fit for use: repurposing research data, reconstructing provenance, and refining “clean” data
                <p>when it comes to teaching materials, data curation education may have become a victim of its own success: finding “dirty” data for classroom use is persistently difficult, in part because most published datasets have already been cleaned and curated! however, there are teachable moments to be found even when working with relatively “clean” data. published data can be mined, re-structured, re-formatted and otherwise curated for new uses. additionally, the process of tracking down and contextualizing already published datasets can prove instructive in and of itself. the detective work needed to understand someone else’s project, and to reconstruct its provenance, can reveal unexpected idiosyncrasies about the dataset, and thereby reveal useful data wrangling skills to be taught.</p>
                <p>in this talk, we describe our work finding, curating and reconstructing the provenance of “the pettigrew papers,” a published (and relatively clean) dataset we have used over two years of teaching week-long workshops on digital humanities data curation at the digital humanities at oxford summer school (dhoxss). thomas j. pettigrew (also known as thomas “mummy” pettigrew) was a victorian surgeon, antiquarian, and egypotologist. pettigrew wrote several early texts on egyptian mummies and was the founding treasurer of the british archaeological association. though his correspondence is archived at yale university’s beinecke rare book and manuscript library, it came to our attention via a “data paper” published in the journal of open archaeology data (moshenska, 2012), containing transcriptions of select letters. </p>
                <p>in our first year teaching with the pettigrew dataset, we wrote simple python scripts to mine named entities from the letters, and to pull out header information about the letters as a spreadsheet for cleaning in openrefine. in hands-on sessions, we asked students to consider how they would clean and curate the dataset for new uses: what steps would need to be taken to create a network diagram of the entities named in his letters? to create a map of his correspondents? to create a timeline? </p>
                <p>in our second year teaching with this dataset, we spent more time reconstructing the original provenance of the pettigrew letters themselves. in addition to the hands-on sessions from the first year, we asked students to consider how they might improve the metadata for the original data paper, and how they might resolve discrepancies between the data paper and the original finding aids created by the beinecke (ducharme, 2010). we additionally discussed how they might incorporate copies of pettigrew’s publications available in the hathitrust digital library in their work.</p>
                <p>overall, we found that asking students to clean and re-curate this already published dataset was only the starting point in our teaching; as we found further connections in digital libraries and archives beyond the original data paper, we identified subtle and important issues in the digital humanities and digital curation that guided our workshop design. in addition to teaching hands-on data cleaning and manipulation skills, we found it important to teach students a nuanced understanding of provenance: both in the sense of the archival “chain of custody” that contextualizes and validates a fonds, and in the sense of the processes that led to a dataset’s current form.</p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                training dh librarians: using old dh projects to move forward
                <p>the dh librarianship course at the university of washington information school investigates the multiple roles librarians play in dh scholarship and prepares students for a wide range of career options in libraries, dh centers, and academic departments. dh librarian roles range from fully-credited collaborator with faculty to last-minute data cleaner, and everything in between. dh librarians also need to be prepared to support projects and research across the spectrum of disciplines, so we examine varying research methods across the humanities. the final project for the course asks that students locate an abandoned, or complete but aging dh project, and insert themselves as a librarian; they provide an evaluation of the content as well as the technology of the project and suggest ways to improve or update both.</p>
                <p>the data sets in these projects varies and examples include: hand-collated quotations by a famous author on a fan site; census numbers provided in a project about london families in the 17th century; a list of shooting locations for a television show; metadata for photos of logging camps in the pacific northwest; multimedia elements in a documentary film; boxes of music programs from a summer camp; and quilting patterns. </p>
                <p>some projects also include the more typical (and larger) type of data set, such as those from hathitrust or google-generated ngrams, but they have proven to be the exception. working with small data sets means that cleaning doesn’t occupy much time during a 10-week quarter, and they can be rearranged quickly to utilize multiple visualization or data processing options.</p>
                <p>students evaluate the data sets early in the process; in nearly all cases, data sets are either incomplete or inaccurate, and for some, updated data or other content is available. this is where the multi-disciplinary expertise of librarians comes in, as mlis students are trained in searching out valid information sources from multiple perspectives, whether that’s using vendor-supplied databases, open web search engines, or (gasp) sources in print or microform. this is also where students begin to see the striation of roles between true collaborators, project leaders, subject specialists, technical consultants, or data-wranglers.</p>
                <p>in reviewing aging or abandoned projects, students learn how easily the data, other content, and the functionality of the site/project can be lost. this gives them the added perspective they need to start thinking about curation and preservation, rather than tackling those issues as add-ons if they have time.</p>
                <p>through these immersive projects, students have a chance to see dh through multiple lenses: those of a potential user, a collaborator, and a disciplinary specialist. they learn how to re-create and improve on a project. in doing so, they gain experience in evaluating and collecting data as well as in multiple platforms and software that are prominent in dh (some current, some defunct). some students also reach out to the original site or project owner, and in a few cases have worked with that person to update the project, putting preservation or stabilizing features in place for future users.</p>
            </div>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    bibliography
                    <bibl>
                        <hi rend=""bold"">borgman, c. l.</hi> (2012). the conundrum of sharing research data, 
                        <hi rend=""italic"">journal of the association for information science and technology</hi>, 63(6): 1059-78.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">croxall, b.</hi> (2017). digital humanities from scratch: a pedagogy-driven investigation of an in-copyright corpus
                        <hi rend=""italic"">, digital humanities 2017: conference abstracts</hi>, montreal: mcgill university, pp. 206-7.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">ducharme, d. j.</hi> (2010). 
                        <hi rend=""italic"">guide to the pettigrew papers osb mss 113</hi>. new haven: beinecke rare book and manuscript library. 
                        <ref target=""http://hdl.handle.net/10079/fa/beinecke.pettis1"">
                            <hi rend=""underline color(1155cc)"">http://hdl.handle.net/10079/fa/beinecke.pettis1</hi>
                        </ref> (accessed 27 april 2018).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">moshenska, g.</hi> (2012). selected correspondence from the papers of thomas pettigrew (1791-1865), surgeon and antiquary. 
                        <hi rend=""italic"">journal of open archaeology data</hi>, 1(0). 
                        <ref target=""https://doi.org/10.5334/4f913ca0cbb89"">
                            <hi rend=""underline color(1155cc)"">https://doi.org/10.5334/4f913ca0cbb89</hi>
                        </ref> (accessed 27 april 2018).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">rockwell, g., day, s., yu., j., and engel, m.</hi> (2014). burying dead projects: depositing the globalization compendium. 
                        <hi rend=""italic"">digital humanities quarterly</hi>, 8(2). retrieved from 
                        <ref target=""http://www.digitalhumanities.org/dhq/vol/8/2/000179/000179.html"">
                            <hi rend=""underline color(1155cc)"">http://www.digitalhumanities.org/dhq/vol/8/2/000179/000179.html</hi>
                        </ref> (accessed 27 april 2018).
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">sinclair, s. and rockwell, g.</hi> (2012). teaching computer-assisted text analysis. in hirsch, b. (ed) 
                        <hi rend=""italic"">digital humanities pedagogy: practices, principles, politics</hi>. open book publishers, pp. 241-54. retrieved from 
                        <ref target=""https://www.openbookpublishers.com/product.php/161"">
                            <hi rend=""underline color(1155cc)"">https://www.openbookpublishers.com/product.php/161</hi>
                        </ref> (accessed 27 april 2018).
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",1.0,1.0,Voyant
6472,2018 - Mexico City,Mexico City,Puentes/Bridges,2018,ADHO,ADHO;EHD,El Colegio de México;Universidad Nacional Autónoma de México (UNAM) (National Autonomous University of Mexico),Mexico City,,Mexico,https://dh2018.adho.org/,Digital Humanities in Middle and High School: Case Studies and Pedagogical Approaches,,Alexander Gil;Roopika Risam;Stan Golanka;Nina Rosenblatt;David Thomas;Matt Applegate;James Cohen;Eric Rettberg;Schuyler Esprit,panel / roundtable,"<text>
        
            <p>Overview</p>
            <p>While scholarship on pedagogy in digital humanities has been growing, its focus has largely been on graduate and, to a lesser extent, undergraduate education. Yet, digital humanities pedagogy—namely its value for cultivating 21st century literacies tied to the production of knowledge and the ability to interpret digital media and computation—is as valuable, this panel argues, for middle- and high- school students as it is in higher education. Given that we are pursuing what Matthew Kirschenbaum describes as forms of ""scholarship and pedagogy that are bound up with infrastructure in ways that are deeper and more explicit than we are generally accustomed to"" (60), this panel examines the work of instructors who are beginning to plant the seeds of these new “customs” early on in humanities and social science training.</p>
            <p>Using digital humanities pedagogy in the middle- and high-school classroom, panelists argue, can redress gaps in these literacies. It enables students, as Mark Sample suggests, “[to think] through their engagement with seemingly incongruous materials, developing a critical thinking practice about process and product” (405). In this way, the approaches to digital humanities pedagogy in middle and high schools articulated by panelists are not an attempt to teach students particular technical skills, applications, or platforms. Rather, this pedagogical approach enables students to envision a relationship between themselves and knowledge production. </p>
            <p>The approaches to digital humanities voiced in this panel are rooted in digital humanities pedagogies in higher education, particularly project-based approaches to humanities knowledge that foster collaboration. As Tanya Clement has argued:</p>
            <p>
                <hi rend=""italic"" xml:space=""preserve"">Like pedagogy intended to teach students to read more critically, project-based learning in digital humanities demonstrates that when students learn how to study digital media, they are learning how to study knowledge production as it is represented in symbolic constructs that circulate within information systems that are themselves a form of knowledge production. </hi>(366)
            </p>
            <p>In the case studies and pedagogical approaches discussed by panelists, the project form complements more traditional forms of knowledge production and evaluation in the classroom. As Brett D. Hirsch argues, this “introduces a new mode of work that emphasizes collectivity and collaboration in the pursuit and creation of new knowledge” (16). While these new modes can be linked to participatory forms of culture, made possible by low barriers for civic engagement and creative expression online (Jenkins et al. 9), panelists make the case for greater attention to pedagogies that offer instruction to middle- and high-school students in collaborative production. </p>
            <p>However, as panelists argue, middle- and high-school pedagogies for digital humanities require attention to the unique needs of students in curricula, the developmental trajectories of the students, and the socio-economic dimensions of these students’ lives. In light of these concerns, what are the biggest challenges to doing digital humanities in middle and high schools? Which methods are most valuable and practically achievable? And how can we effectively prepare teachers to incorporate digital humanities into their teaching practices? In this panel we bring together an international team of researchers and faculty already engaged in answering these questions and implementing curricula in schools of education, digital humanities centers, and high schools. Our goal is both to present models and facilitate discussion with broader digital humanities communities about pedagogical infrastructures, methods, long-term goals, and the exciting possibility of cultivating digital humanities pipelines through intervention in middle and high schools.</p>
            <p>Panel moderator: Alex Gil, Columbia University Libraries</p>
            <p>Designing Digital Humanities Pedagogy Infrastructures for Teachers</p>
            <p>Roopika Risam, Salem State University</p>
            <p>While digital humanities pedagogy has increasingly received attention from practitioners who want to teach their own students more effectively, how do we prepare 
                <hi rend=""italic"">teachers</hi> for the challenging task of engaging with digital humanities in their own classrooms? This talk offers an answer to this question by examining the digital humanities pedagogy infrastructure for middle- and high-school teachers designed at Salem State University. I first discuss findings from a study undertaken with teachers in Massachusetts to identify their attitudes towards digital humanities. The results indicate lack of knowledge about digital humanities but significant interest in incorporating computational approaches to humanities into teaching. Teachers also raised concerns including the time needed to learn technologies and teach them to students, cost of software and hardware, uneven access to computers or the internet in classrooms and for students at home, fear of implementing unsuccessful lessons, and a lack of professional development opportunities for digital humanities.
            </p>
            <p>This talk then considers the interdisciplinary graduate certificate in digital studies that Salem State University designed in response to the study. The program provides professional development while addressing teachers’ perceived obstacles to including digital humanities in their teaching. I discuss the relationship between study results and program design, focusing on development of core courses, selection of elective courses, differentiation of course delivery methods, integration into existing master’s programs, and creation of a directed study for curriculum design. To illustrate the impact of the program, I describe my work advising a team of teachers and administrators in the graduate certificate program who were planning technology needs for a new school building under construction and designing technology-infused curricula in English and History. While core and elective courses gave the teachers and administrators a solid background in digital humanities, a group directed study assisted the team with developing a scaffolded curriculum across middle-school humanities courses, designing classroom technology, and creating a professional learning community to provide in-school pedagogical support for teachers. </p>
            <p>Finally, this talk discusses a follow-up study with graduates of the certificate programs that assessed program outcomes. These outcomes include assignments implemented by teachers in their classrooms, exemplar student work, and a marked difference in attitudes and perceptions of teachers who completed the certificate in comparison to those who participated in the initial study. Based on the outcomes and the success of the graduate certificate program, Salem State has begun integrating digital humanities pedagogy directly into its teacher training programs. Consequently, this talk argues, this digital humanities pedagogical infrastructure for teachers serves as an effective model for addressing the barriers to incorporating digital humanities into middle- and high-school curricula for teachers who are already in the classroom and those preparing for teaching careers in the humanities. </p>
            <p>Digital Inquiry: The History of Youth</p>
            <p>Nina Rosenblatt, Trevor Day School</p>
            <p>David Thomas, Trevor Day School</p>
            <p>Stan Golanka, Trevor Day School</p>
            <p>On September 12th, 2017 Trevor Day School, an Independent School on the Upper East Side of New York City, launched two sections of an advanced history course entitled 
                <hi rend=""italic"">Digital Inquiry: The History of Youth</hi>. This course was the culmination of seven years of curriculum development work that began with a November 16th, 2010 article in the 
                <hi rend=""italic"">New York Times</hi> about Humanities 2.0 and the Stanford Republic of Letters Project. After an initial round of research we came to understand that digital projects had a role to play in our High School History curriculum. This realization coincided with our adoption of inquiry-based learning pedagogies. In a fundamental way, we argue, the techniques and disciplines involved in digital humanities allow high school students to conduct their own independent research in digital archives and become producers of history in their own right.
            </p>
            <p>In order to motivate students to collaborate and learn unfamiliar working methods, we developed our course around a subject that would engage all students. We wanted a subject that would not require a textbook, was accessible to juniors and seniors in high school, and would lend itself to seminar style classes. In addition, we wanted to be able to supplement the subject matter with texts illuminating the nature of historical narrative, archives, and the use of digital techniques in academic research such as the paper by Lauren Klein, “The Image of Absence: Archival Silence, Data Visualization, and James Hemings” in 
                <hi rend=""italic"" xml:space=""preserve"">American Literature </hi>Volume 85, Number 4, December 2013
                <hi rend=""italic"">.</hi>
                <hi rend=""color(333333)"" xml:space=""preserve"">The resulting course </hi>delved into the history of youth, looking at how being young is experienced and imagined differently in different times and places, and what we can learn about a society from its expectations for and attitude towards its youth, while teaching them production and analysis techniques for them to create new representations of that history.
            </p>
            <p>The final consideration was to craft a series of lesson plans to embed a digital humanities knowledge-production laboratory in the class. The course lab was divided into three modules: Digital editions and markup (an introduction to the fundamentals of plain text and markup), digital collections/exhibits (an introduction to the fundamentals of metadata and databases), and cultural analytics (an introduction to the fundamentals of algorithmic thinking and data mining). Through these modules the students were immersed in the process of selection, digitization, mark-up, the creation of a database/archive, data extraction and cleanup and data analysis, all driven by the imperative to create and interpret history. 
                <hi rend=""color(333333)"" xml:space=""preserve"">Technologies taught included, but were not limited to, command line, git, GitHub, plain text editors, Markdown, YAML, Jekyll, Omeka, Python and Voyant Tools. These technologies were directly tied to the </hi>variety of ways in which historians collect “data” including using literary, psychological, sociological, statistical, and visual sources, working towards creating our own historical knowledge using the digital tools for collecting, visualizing, mapping, and analyzing the information.
            </p>
            <p>In this panel we will present the results of our two course prototypes, lessons learned, future improvements, and argue for a generalizable model of instruction for high schools in the United States based on our experiences.</p>
            <p>Digital Literary Studies in the High School Environment</p>
            <p>Eric Rettberg, Illinois Mathematics and Science Academy</p>
            <p>What are the challenges of adapting a course in Digital Humanities and Digital Culture from the pedagogical environment of the university to that of the high school classroom? What new challenges arise from asking minors to produce digital and public scholarship, and how can digitally inflected scholars and teachers foster innovative humanities work in school environments bound to pre-existing curricula? In this talk, I use my experience adapting a class in Digital Literary Studies to the high-school level to share unexpected challenges and opportunities and to suggest digital work as a strategy for promoting the humanities to administrators, peers, and students in STEM-oriented high school environments.</p>
            <p>In early 2016, I left higher education to teach in the English department of the Illinois Mathematics and Science Academy, a state-funded boarding school for students talented in math and science. Given the immediate appeal of classes combining humanities with computing for STEM-focused students, I naively expected that I might be able to simply bring a college elective for English majors to my high school students. The actual challenges of doing so, however have been instructive: administrators have been less familiar with the existence of the methods of the Digital Humanities, digital assignments have had to be reframed to accommodate shared practice among teachers in my department, my school’s technology environment has needed to be customized to accommodate the software installations that I took for granted before, oversight from administrators, colleagues, and parents has been more intensive, and without the support staff available to me at my higher education institutions, I’ve had to think creatively around constraints. By demonstrating small-scale digital humanities work in core classes, designing a week-long intersession class on a similar topic, and sharing my knowledge of University-level digital humanities, though, I’ve been able to design a class that has colleagues and students excited.</p>
            <p>Heeding Ryan Cordell’s call to embed digital humanities instruction in larger narratives beyond “recent scholarly revolution,” I treat digital humanities praxis as one of three major components of change in literary production and study in the digital era. In addition to digital humanities projects centered on historical texts of students’ choice, students read and discuss fictive works that represent cultures of technology in the digital era and computationally enabled works of electronic literature. Throughout the class, students sample digital humanities practice in lab sessions and build small-scale web resources and undertake digital-humanities experiments in group projects. By exploring electronic texts, they begin to more fully recognizes the affordances of digital technologies, and by reading print texts that represent digital culture, they think about their own roles as consumers of and creators of digital tools and cultures. While my school’s student population and focus are especially suited to the STEAM focus that a class like this one offers, my experiences suggest that students at a wide variety of high schools would be engaged by these materials and skills.</p>
            <p>Impact od Digital Humanities on High School History and Heritage Teaching and Learning in the Caribbean</p>
            <p>Schuyler K Esprit, Create Caribbean, Inc.</p>
            <p>The experience of Create Caribbean Research Institute, the first Digital Humanities center in the English Speaking Caribbean tells an interesting story of how digital humanities can covertly and explicitly reshape the curriculum in history and literature of the Caribbean without necessarily requiring a massive paradigm shift of the national and regional curriculum requirements.</p>
            <p>In Dominica (where Create Caribbean operates) and the Eastern Caribbean – among other islands – the secondary education curriculum responds to the mandates of the Caribbean Examinations Council (CXC) who sets the CSEC and CAPE syllabi for high school and post-secondary certification in the region. These examinations frame the education curriculum for the five to six years of high school in many islands and many educators in this system find themselves bound to deliver content in limiting and limited methods in order to ensure that students simply meet requirements to excel at subject exams at Caribbean History and English B (Literature), which has a heavy focus on Caribbean Literature.</p>
            <p>However, students leave with an abstract and formalized understand of Caribbean history and culture, without a nuanced understanding of its relevance to their own lived experiences and the implications for their future. Create Caribbean uses digital humanities projects to reframe the conversation and disrupt traditional methods for learning. One of these projects uniquely highlights the potential for technology to change the face of education in Dominica and to get students more invested in Dominica’s history and culture. This project, made by students for students, can be found at 
                <ref target=""www.dominicahistory.org"">www.dominicahistory.org</ref>. The college student change-makers of Create Caribbean’s internship program build digital humanities projects with a primary and secondary student audience in mind. The example of dominicahistory.org highlights one way that a collaboration with a national organization has allowed for a broader consideration of the methods of heritage and culture education for students while actually providing solid academic source material for their formal study requirements.
            </p>
            <p>This presentation will discuss the origin, process and impacts of the Dominica History and Imagined Homeland digital projects of Create Caribbean as examples of disruptive secondary education. The presentation will also address the ways in which the projects have attracted the attention of high school teachers and transformed their interests in using technology to revise classroom experiences when they face limitations in adjusting other curricular frameworks.</p>
            <p>Precarity and Practicality: DH, New Media, &amp; Secondary Education</p>
            <p>Matt Appegate, Molloy College</p>
            <p>Jamie Cohen, Molloy College</p>
            <p>In 2015, faculty at Molloy College in Long Island worked with faculty and administrators to found the Baldwin High School New Media Academy, a co-organized effort to bring the study of New Media and Digital Humanities to underserved high school populations in Baldwin, New York. Working collectively, faculty at both institutions have established a curriculum and internship path at Baldwin High School that exposes students to methods of DH praxis and principles of New Media in a variety of means and environments (high school, college, in-person, online).</p>
            <p>Our curriculum is based on five modules and two college-credit bearing courses. Our modules include Critical Making, Digital Storytelling, Multimodal Composition, Online Expression, and Social Media. Our college-credit bearing courses are Introduction to New Media and College Composition (the course is taught entirely on the methods of multimodal composition). Each module is integrated into existing high school courses, i.e., Social Studies, English, Wood Shop, etc., where students take college-credit bearing courses in their junior and senior years. Ultimately, the “academy” concept introduces students to DH methods and New Media in a gradated process--students choose their academy prior to entering their freshman year of high school and are enrolled in courses that employ our modules.</p>
            <p>Our curriculum is based on principles of social good; it emphasizes both civic engagement and social justice, and provides sample assignments with grading rubrics for each module (Ratto). The civic-minded focus of our curriculum was developed in consultation with Baldwin High School, and fleshed out over 18 months of training. Our curriculum attempts to account for the precarious position women and people of color already inhabited in online spaces and demonstrate how DH methods and New Media principles can be mobilized to empower students via digital tools and languages. </p>
            <p>The focus of this paper is to report on our work with underserved high school populations and relay the challenges of bringing this kind of material to a secondary education setting. We focus on the practicalities of bringing DH methods and New Media principles to high school (i.e., funding, time, expertise, bureaucracy), as well as the necessary training that takes places between high school and college faculty (PT days, on campus conferences, and student events). Finally, we discuss the opportunities that working with underserved high school populations provides both politically and pedagogically. In this context, DH operates on a minimal scale, but addresses communal needs. </p>
            <p>Bios</p>
            <p>Matt Applegate is an Assistant Professor of English and Digital Humanities at Molloy College. His work focuses on critical theory, digital humanities, digital literacy, and screen studies. His work as appeared in 
                <hi rend=""italic"">Amodern</hi>, 
                <hi rend=""italic"">Theory &amp; Event</hi>, 
                <hi rend=""italic"">Cultural Politics</hi>, 
                <hi rend=""italic"">Cultural Critique</hi>, 
                <hi rend=""italic"">Telos</hi>, and more.
            </p>
            <p>
                <hi rend=""color(222222) background(white)"" xml:space=""preserve"">Jamie Cohen is the director, co-founder and assistant professor of the New Media program at Molloy College in New York. Jamie is the author of </hi>
                <hi rend=""italic color(222222) background(white)"">Producing New and Digital Media: Your Guide to Savvy Use of the Web</hi>
                <hi rend=""color(222222) background(white)"" xml:space=""preserve""> (Routledge 2015) and his published and presented research focuses on memes, YouTubers, populism, VR/AR/MR, and digital media literacy. He is a fellow of the Salzburg Academy on Media and Global Change and the Academy of Television Arts and Sciences.</hi>
            </p>
            <p>Schuyler K Esprit is the Director of Create Caribbean Research Institute at Dominica State College, the first Digital Humanities center in the Caribbean. Dr. Esprit holds a PhD in English literature from University of Maryland – College Park. She is a scholar of Caribbean literature and cultural studies, and postcolonial theory. She is now completing her book entitled 
                <hi rend=""italic"">West Indian Readers: A Social History</hi> and its digital companion, both of which are historical explorations of reading culture in the Caribbean. She is currently Dean of Academic Affairs at Dominica State College. 
            </p>
            <p>Stan Golanka is Director of Academic Technology at Trevor Day School. He teaches computer programming and co-teaches Advanced History: Digital Inquiry. He holds a MA in Computing in Education from Teachers College at Columbia University. </p>
            <p>Eric Rettberg teaches English at the Illinois Mathematics and Science Academy. He remains an active scholar of modernism, experimental poetry, sound studies, and the digital humanities. His work has appeared in 
                <hi rend=""italic"">Comparative Literature Studies</hi> and 
                <hi rend=""italic"">Jacket 2</hi>. 
            </p>
            <p>Roopika Risam is an assistant professor of English and English education at Salem State University. Her research considers the intersections of postcolonial cultures, African diaspora studies, and digital humanities. She is the author of 
                <hi rend=""italic"">New Digital Worlds: Postcolonial Digital Humanities in Theory, Praxis, and Pedagogy</hi> (Northwestern UP 2018) and her work has recently appeared in 
                <hi rend=""italic"">Debates in the Digital Humanities 2016</hi>, 
                <hi rend=""italic"">Digital Scholarship in the Humanities</hi>, and 
                <hi rend=""italic"" xml:space=""preserve"">South Asian Review. </hi>
            </p>
            <p>Nina Rosenblatt teaches US History, Art History, and Advanced History: Digital Inquiry at Trevor Day School. She holds a PhD in Art History from Columbia University. </p>
            <p>David Thomas is Chair of the History Department at Trevor Day School, he teaches European History, Advanced European History, The History of China, and Advanced History: Digital Inquiry.</p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>Clement, Tanya. “Multiliteracies in the Undergraduate Digital Humanities Curriculum: Skills, Principles, and Habits of Mind,” in 
                        <hi rend=""italic"">Digital Humanities Pedagogy: Practices, Principles, and Politics</hi>, ed. Brett D. Hirsch (Cambridge: Open Book Publishers, 2012), 365-88.
                    </bibl>
                    <bibl>Cordell, Ryan. ""How Not to Teach Digital Humanities."" 
                        <hi rend=""italic"">Ryancordell.org</hi>, 1 Feb. 2015, 
                        <ref target=""http://ryancordell.org/teaching/how-not-to-teach-digital-humanities/"">http://ryancordell.org/teaching/how-not-to-teach-digital-humanities/</ref>.
                    </bibl>
                    <bibl>Hirsch, Brett D. “&lt;/Parentheses&gt;: Digital Humanities and the Place of Pedagogy.” 
                        <hi rend=""italic"" xml:space=""preserve"">Digital Humanities Pedagogy: Practices, Principles, and Politics, </hi>ed. Brett D. Hirsch (Cambridge: Open Book Publishers, 2012), 3-30.
                    </bibl>
                    <bibl>Jenkins, Henry and Ravi Purushotma, Margaret Weigel, Katie Clinton, Alice J. Robison. 
                        <hi rend=""italic"">Confronting the Challenges of Participatory Culture: Media Education for the 21</hi>
                        <hi rend=""italic superscript"">st</hi>
                        <hi rend=""italic"" xml:space=""preserve""> Century</hi> (Cambridge, MA: MIT Press, 2009).
                    </bibl>
                    <bibl>Kirschenbaum, Matthew. “What Is Digital Humanities and What’s It Doing in English Departments?” 
                        <hi rend=""italic"" xml:space=""preserve"">ADE Bulletin </hi>150 (2010): 55-61.
                    </bibl>
                    <bibl>Ratto, Matt. “OPEN DESIGN NOW.” 
                        <hi rend=""italic"">Open Design Now</hi>, Netherlands Institute for Design and Fashion and Waag Society, opendesignnow.org/index.html%3Fp=434.html.
                    </bibl>
                    <bibl>Sample, Mark. “What’s Wrong with Writing Essays?” 
                        <hi rend=""italic"">Debates in the Digital Humanities</hi>, ed. Matthew K. Gold (Minneapolis: University of Minnesota Press, 2012), 404-5.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,Creative Commons Attribution 4.0 International,,collaboration;curriculum;high school;pedagogy;projects,English,"computer science;cultural and/or institutional infrastructure;cultural studies;english;historical studies;knowledge representation;project design, organization, management",2018-01-01,"<text>
        
            <p>overview</p>
            <p>while scholarship on pedagogy in digital humanities has been growing, its focus has largely been on graduate and, to a lesser extent, undergraduate education. yet, digital humanities pedagogy—namely its value for cultivating 21st century literacies tied to the production of knowledge and the ability to interpret digital media and computation—is as valuable, this panel argues, for middle- and high- school students as it is in higher education. given that we are pursuing what matthew kirschenbaum describes as forms of ""scholarship and pedagogy that are bound up with infrastructure in ways that are deeper and more explicit than we are generally accustomed to"" (60), this panel examines the work of instructors who are beginning to plant the seeds of these new “customs” early on in humanities and social science training.</p>
            <p>using digital humanities pedagogy in the middle- and high-school classroom, panelists argue, can redress gaps in these literacies. it enables students, as mark sample suggests, “[to think] through their engagement with seemingly incongruous materials, developing a critical thinking practice about process and product” (405). in this way, the approaches to digital humanities pedagogy in middle and high schools articulated by panelists are not an attempt to teach students particular technical skills, applications, or platforms. rather, this pedagogical approach enables students to envision a relationship between themselves and knowledge production. </p>
            <p>the approaches to digital humanities voiced in this panel are rooted in digital humanities pedagogies in higher education, particularly project-based approaches to humanities knowledge that foster collaboration. as tanya clement has argued:</p>
            <p>
                <hi rend=""italic"" xml:space=""preserve"">like pedagogy intended to teach students to read more critically, project-based learning in digital humanities demonstrates that when students learn how to study digital media, they are learning how to study knowledge production as it is represented in symbolic constructs that circulate within information systems that are themselves a form of knowledge production. </hi>(366)
            </p>
            <p>in the case studies and pedagogical approaches discussed by panelists, the project form complements more traditional forms of knowledge production and evaluation in the classroom. as brett d. hirsch argues, this “introduces a new mode of work that emphasizes collectivity and collaboration in the pursuit and creation of new knowledge” (16). while these new modes can be linked to participatory forms of culture, made possible by low barriers for civic engagement and creative expression online (jenkins et al. 9), panelists make the case for greater attention to pedagogies that offer instruction to middle- and high-school students in collaborative production. </p>
            <p>however, as panelists argue, middle- and high-school pedagogies for digital humanities require attention to the unique needs of students in curricula, the developmental trajectories of the students, and the socio-economic dimensions of these students’ lives. in light of these concerns, what are the biggest challenges to doing digital humanities in middle and high schools? which methods are most valuable and practically achievable? and how can we effectively prepare teachers to incorporate digital humanities into their teaching practices? in this panel we bring together an international team of researchers and faculty already engaged in answering these questions and implementing curricula in schools of education, digital humanities centers, and high schools. our goal is both to present models and facilitate discussion with broader digital humanities communities about pedagogical infrastructures, methods, long-term goals, and the exciting possibility of cultivating digital humanities pipelines through intervention in middle and high schools.</p>
            <p>panel moderator: alex gil, columbia university libraries</p>
            <p>designing digital humanities pedagogy infrastructures for teachers</p>
            <p>roopika risam, salem state university</p>
            <p>while digital humanities pedagogy has increasingly received attention from practitioners who want to teach their own students more effectively, how do we prepare 
                <hi rend=""italic"">teachers</hi> for the challenging task of engaging with digital humanities in their own classrooms? this talk offers an answer to this question by examining the digital humanities pedagogy infrastructure for middle- and high-school teachers designed at salem state university. i first discuss findings from a study undertaken with teachers in massachusetts to identify their attitudes towards digital humanities. the results indicate lack of knowledge about digital humanities but significant interest in incorporating computational approaches to humanities into teaching. teachers also raised concerns including the time needed to learn technologies and teach them to students, cost of software and hardware, uneven access to computers or the internet in classrooms and for students at home, fear of implementing unsuccessful lessons, and a lack of professional development opportunities for digital humanities.
            </p>
            <p>this talk then considers the interdisciplinary graduate certificate in digital studies that salem state university designed in response to the study. the program provides professional development while addressing teachers’ perceived obstacles to including digital humanities in their teaching. i discuss the relationship between study results and program design, focusing on development of core courses, selection of elective courses, differentiation of course delivery methods, integration into existing master’s programs, and creation of a directed study for curriculum design. to illustrate the impact of the program, i describe my work advising a team of teachers and administrators in the graduate certificate program who were planning technology needs for a new school building under construction and designing technology-infused curricula in english and history. while core and elective courses gave the teachers and administrators a solid background in digital humanities, a group directed study assisted the team with developing a scaffolded curriculum across middle-school humanities courses, designing classroom technology, and creating a professional learning community to provide in-school pedagogical support for teachers. </p>
            <p>finally, this talk discusses a follow-up study with graduates of the certificate programs that assessed program outcomes. these outcomes include assignments implemented by teachers in their classrooms, exemplar student work, and a marked difference in attitudes and perceptions of teachers who completed the certificate in comparison to those who participated in the initial study. based on the outcomes and the success of the graduate certificate program, salem state has begun integrating digital humanities pedagogy directly into its teacher training programs. consequently, this talk argues, this digital humanities pedagogical infrastructure for teachers serves as an effective model for addressing the barriers to incorporating digital humanities into middle- and high-school curricula for teachers who are already in the classroom and those preparing for teaching careers in the humanities. </p>
            <p>digital inquiry: the history of youth</p>
            <p>nina rosenblatt, trevor day school</p>
            <p>david thomas, trevor day school</p>
            <p>stan golanka, trevor day school</p>
            <p>on september 12th, 2017 trevor day school, an independent school on the upper east side of new york city, launched two sections of an advanced history course entitled 
                <hi rend=""italic"">digital inquiry: the history of youth</hi>. this course was the culmination of seven years of curriculum development work that began with a november 16th, 2010 article in the 
                <hi rend=""italic"">new york times</hi> about humanities 2.0 and the stanford republic of letters project. after an initial round of research we came to understand that digital projects had a role to play in our high school history curriculum. this realization coincided with our adoption of inquiry-based learning pedagogies. in a fundamental way, we argue, the techniques and disciplines involved in digital humanities allow high school students to conduct their own independent research in digital archives and become producers of history in their own right.
            </p>
            <p>in order to motivate students to collaborate and learn unfamiliar working methods, we developed our course around a subject that would engage all students. we wanted a subject that would not require a textbook, was accessible to juniors and seniors in high school, and would lend itself to seminar style classes. in addition, we wanted to be able to supplement the subject matter with texts illuminating the nature of historical narrative, archives, and the use of digital techniques in academic research such as the paper by lauren klein, “the image of absence: archival silence, data visualization, and james hemings” in 
                <hi rend=""italic"" xml:space=""preserve"">american literature </hi>volume 85, number 4, december 2013
                <hi rend=""italic"">.</hi>
                <hi rend=""color(333333)"" xml:space=""preserve"">the resulting course </hi>delved into the history of youth, looking at how being young is experienced and imagined differently in different times and places, and what we can learn about a society from its expectations for and attitude towards its youth, while teaching them production and analysis techniques for them to create new representations of that history.
            </p>
            <p>the final consideration was to craft a series of lesson plans to embed a digital humanities knowledge-production laboratory in the class. the course lab was divided into three modules: digital editions and markup (an introduction to the fundamentals of plain text and markup), digital collections/exhibits (an introduction to the fundamentals of metadata and databases), and cultural analytics (an introduction to the fundamentals of algorithmic thinking and data mining). through these modules the students were immersed in the process of selection, digitization, mark-up, the creation of a database/archive, data extraction and cleanup and data analysis, all driven by the imperative to create and interpret history. 
                <hi rend=""color(333333)"" xml:space=""preserve"">technologies taught included, but were not limited to, command line, git, github, plain text editors, markdown, yaml, jekyll, omeka, python and voyant tools. these technologies were directly tied to the </hi>variety of ways in which historians collect “data” including using literary, psychological, sociological, statistical, and visual sources, working towards creating our own historical knowledge using the digital tools for collecting, visualizing, mapping, and analyzing the information.
            </p>
            <p>in this panel we will present the results of our two course prototypes, lessons learned, future improvements, and argue for a generalizable model of instruction for high schools in the united states based on our experiences.</p>
            <p>digital literary studies in the high school environment</p>
            <p>eric rettberg, illinois mathematics and science academy</p>
            <p>what are the challenges of adapting a course in digital humanities and digital culture from the pedagogical environment of the university to that of the high school classroom? what new challenges arise from asking minors to produce digital and public scholarship, and how can digitally inflected scholars and teachers foster innovative humanities work in school environments bound to pre-existing curricula? in this talk, i use my experience adapting a class in digital literary studies to the high-school level to share unexpected challenges and opportunities and to suggest digital work as a strategy for promoting the humanities to administrators, peers, and students in stem-oriented high school environments.</p>
            <p>in early 2016, i left higher education to teach in the english department of the illinois mathematics and science academy, a state-funded boarding school for students talented in math and science. given the immediate appeal of classes combining humanities with computing for stem-focused students, i naively expected that i might be able to simply bring a college elective for english majors to my high school students. the actual challenges of doing so, however have been instructive: administrators have been less familiar with the existence of the methods of the digital humanities, digital assignments have had to be reframed to accommodate shared practice among teachers in my department, my school’s technology environment has needed to be customized to accommodate the software installations that i took for granted before, oversight from administrators, colleagues, and parents has been more intensive, and without the support staff available to me at my higher education institutions, i’ve had to think creatively around constraints. by demonstrating small-scale digital humanities work in core classes, designing a week-long intersession class on a similar topic, and sharing my knowledge of university-level digital humanities, though, i’ve been able to design a class that has colleagues and students excited.</p>
            <p>heeding ryan cordell’s call to embed digital humanities instruction in larger narratives beyond “recent scholarly revolution,” i treat digital humanities praxis as one of three major components of change in literary production and study in the digital era. in addition to digital humanities projects centered on historical texts of students’ choice, students read and discuss fictive works that represent cultures of technology in the digital era and computationally enabled works of electronic literature. throughout the class, students sample digital humanities practice in lab sessions and build small-scale web resources and undertake digital-humanities experiments in group projects. by exploring electronic texts, they begin to more fully recognizes the affordances of digital technologies, and by reading print texts that represent digital culture, they think about their own roles as consumers of and creators of digital tools and cultures. while my school’s student population and focus are especially suited to the steam focus that a class like this one offers, my experiences suggest that students at a wide variety of high schools would be engaged by these materials and skills.</p>
            <p>impact od digital humanities on high school history and heritage teaching and learning in the caribbean</p>
            <p>schuyler k esprit, create caribbean, inc.</p>
            <p>the experience of create caribbean research institute, the first digital humanities center in the english speaking caribbean tells an interesting story of how digital humanities can covertly and explicitly reshape the curriculum in history and literature of the caribbean without necessarily requiring a massive paradigm shift of the national and regional curriculum requirements.</p>
            <p>in dominica (where create caribbean operates) and the eastern caribbean – among other islands – the secondary education curriculum responds to the mandates of the caribbean examinations council (cxc) who sets the csec and cape syllabi for high school and post-secondary certification in the region. these examinations frame the education curriculum for the five to six years of high school in many islands and many educators in this system find themselves bound to deliver content in limiting and limited methods in order to ensure that students simply meet requirements to excel at subject exams at caribbean history and english b (literature), which has a heavy focus on caribbean literature.</p>
            <p>however, students leave with an abstract and formalized understand of caribbean history and culture, without a nuanced understanding of its relevance to their own lived experiences and the implications for their future. create caribbean uses digital humanities projects to reframe the conversation and disrupt traditional methods for learning. one of these projects uniquely highlights the potential for technology to change the face of education in dominica and to get students more invested in dominica’s history and culture. this project, made by students for students, can be found at 
                <ref target=""www.dominicahistory.org"">www.dominicahistory.org</ref>. the college student change-makers of create caribbean’s internship program build digital humanities projects with a primary and secondary student audience in mind. the example of dominicahistory.org highlights one way that a collaboration with a national organization has allowed for a broader consideration of the methods of heritage and culture education for students while actually providing solid academic source material for their formal study requirements.
            </p>
            <p>this presentation will discuss the origin, process and impacts of the dominica history and imagined homeland digital projects of create caribbean as examples of disruptive secondary education. the presentation will also address the ways in which the projects have attracted the attention of high school teachers and transformed their interests in using technology to revise classroom experiences when they face limitations in adjusting other curricular frameworks.</p>
            <p>precarity and practicality: dh, new media, &amp; secondary education</p>
            <p>matt appegate, molloy college</p>
            <p>jamie cohen, molloy college</p>
            <p>in 2015, faculty at molloy college in long island worked with faculty and administrators to found the baldwin high school new media academy, a co-organized effort to bring the study of new media and digital humanities to underserved high school populations in baldwin, new york. working collectively, faculty at both institutions have established a curriculum and internship path at baldwin high school that exposes students to methods of dh praxis and principles of new media in a variety of means and environments (high school, college, in-person, online).</p>
            <p>our curriculum is based on five modules and two college-credit bearing courses. our modules include critical making, digital storytelling, multimodal composition, online expression, and social media. our college-credit bearing courses are introduction to new media and college composition (the course is taught entirely on the methods of multimodal composition). each module is integrated into existing high school courses, i.e., social studies, english, wood shop, etc., where students take college-credit bearing courses in their junior and senior years. ultimately, the “academy” concept introduces students to dh methods and new media in a gradated process--students choose their academy prior to entering their freshman year of high school and are enrolled in courses that employ our modules.</p>
            <p>our curriculum is based on principles of social good; it emphasizes both civic engagement and social justice, and provides sample assignments with grading rubrics for each module (ratto). the civic-minded focus of our curriculum was developed in consultation with baldwin high school, and fleshed out over 18 months of training. our curriculum attempts to account for the precarious position women and people of color already inhabited in online spaces and demonstrate how dh methods and new media principles can be mobilized to empower students via digital tools and languages. </p>
            <p>the focus of this paper is to report on our work with underserved high school populations and relay the challenges of bringing this kind of material to a secondary education setting. we focus on the practicalities of bringing dh methods and new media principles to high school (i.e., funding, time, expertise, bureaucracy), as well as the necessary training that takes places between high school and college faculty (pt days, on campus conferences, and student events). finally, we discuss the opportunities that working with underserved high school populations provides both politically and pedagogically. in this context, dh operates on a minimal scale, but addresses communal needs. </p>
            <p>bios</p>
            <p>matt applegate is an assistant professor of english and digital humanities at molloy college. his work focuses on critical theory, digital humanities, digital literacy, and screen studies. his work as appeared in 
                <hi rend=""italic"">amodern</hi>, 
                <hi rend=""italic"">theory &amp; event</hi>, 
                <hi rend=""italic"">cultural politics</hi>, 
                <hi rend=""italic"">cultural critique</hi>, 
                <hi rend=""italic"">telos</hi>, and more.
            </p>
            <p>
                <hi rend=""color(222222) background(white)"" xml:space=""preserve"">jamie cohen is the director, co-founder and assistant professor of the new media program at molloy college in new york. jamie is the author of </hi>
                <hi rend=""italic color(222222) background(white)"">producing new and digital media: your guide to savvy use of the web</hi>
                <hi rend=""color(222222) background(white)"" xml:space=""preserve""> (routledge 2015) and his published and presented research focuses on memes, youtubers, populism, vr/ar/mr, and digital media literacy. he is a fellow of the salzburg academy on media and global change and the academy of television arts and sciences.</hi>
            </p>
            <p>schuyler k esprit is the director of create caribbean research institute at dominica state college, the first digital humanities center in the caribbean. dr. esprit holds a phd in english literature from university of maryland – college park. she is a scholar of caribbean literature and cultural studies, and postcolonial theory. she is now completing her book entitled 
                <hi rend=""italic"">west indian readers: a social history</hi> and its digital companion, both of which are historical explorations of reading culture in the caribbean. she is currently dean of academic affairs at dominica state college. 
            </p>
            <p>stan golanka is director of academic technology at trevor day school. he teaches computer programming and co-teaches advanced history: digital inquiry. he holds a ma in computing in education from teachers college at columbia university. </p>
            <p>eric rettberg teaches english at the illinois mathematics and science academy. he remains an active scholar of modernism, experimental poetry, sound studies, and the digital humanities. his work has appeared in 
                <hi rend=""italic"">comparative literature studies</hi> and 
                <hi rend=""italic"">jacket 2</hi>. 
            </p>
            <p>roopika risam is an assistant professor of english and english education at salem state university. her research considers the intersections of postcolonial cultures, african diaspora studies, and digital humanities. she is the author of 
                <hi rend=""italic"">new digital worlds: postcolonial digital humanities in theory, praxis, and pedagogy</hi> (northwestern up 2018) and her work has recently appeared in 
                <hi rend=""italic"">debates in the digital humanities 2016</hi>, 
                <hi rend=""italic"">digital scholarship in the humanities</hi>, and 
                <hi rend=""italic"" xml:space=""preserve"">south asian review. </hi>
            </p>
            <p>nina rosenblatt teaches us history, art history, and advanced history: digital inquiry at trevor day school. she holds a phd in art history from columbia university. </p>
            <p>david thomas is chair of the history department at trevor day school, he teaches european history, advanced european history, the history of china, and advanced history: digital inquiry.</p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    bibliography
                    <bibl>clement, tanya. “multiliteracies in the undergraduate digital humanities curriculum: skills, principles, and habits of mind,” in 
                        <hi rend=""italic"">digital humanities pedagogy: practices, principles, and politics</hi>, ed. brett d. hirsch (cambridge: open book publishers, 2012), 365-88.
                    </bibl>
                    <bibl>cordell, ryan. ""how not to teach digital humanities."" 
                        <hi rend=""italic"">ryancordell.org</hi>, 1 feb. 2015, 
                        <ref target=""http://ryancordell.org/teaching/how-not-to-teach-digital-humanities/"">http://ryancordell.org/teaching/how-not-to-teach-digital-humanities/</ref>.
                    </bibl>
                    <bibl>hirsch, brett d. “&lt;/parentheses&gt;: digital humanities and the place of pedagogy.” 
                        <hi rend=""italic"" xml:space=""preserve"">digital humanities pedagogy: practices, principles, and politics, </hi>ed. brett d. hirsch (cambridge: open book publishers, 2012), 3-30.
                    </bibl>
                    <bibl>jenkins, henry and ravi purushotma, margaret weigel, katie clinton, alice j. robison. 
                        <hi rend=""italic"">confronting the challenges of participatory culture: media education for the 21</hi>
                        <hi rend=""italic superscript"">st</hi>
                        <hi rend=""italic"" xml:space=""preserve""> century</hi> (cambridge, ma: mit press, 2009).
                    </bibl>
                    <bibl>kirschenbaum, matthew. “what is digital humanities and what’s it doing in english departments?” 
                        <hi rend=""italic"" xml:space=""preserve"">ade bulletin </hi>150 (2010): 55-61.
                    </bibl>
                    <bibl>ratto, matt. “open design now.” 
                        <hi rend=""italic"">open design now</hi>, netherlands institute for design and fashion and waag society, opendesignnow.org/index.html%3fp=434.html.
                    </bibl>
                    <bibl>sample, mark. “what’s wrong with writing essays?” 
                        <hi rend=""italic"">debates in the digital humanities</hi>, ed. matthew k. gold (minneapolis: university of minnesota press, 2012), 404-5.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",1.0,1.0,Voyant
7862,2015 - Sydney,Sydney,Global Digital Humanities,2015,ADHO,ADHO,Western Sydney University,Sydney,,Australia,https://web.archive.org/web/20190121165412/http://dh2015.org/,"Voyant Tools 2.0: The New, The Neat & the Gnarly",https://github.com/ADHO/dh2015/blob/master/xml/SINCLAIR_St_fan_Voyant_Tools_2_0__The_New__The_Neat___t.xml,Stéfan Sinclair;Geoffrey Rockwell,workshop / tutorial,"<?xml version=""1.0"" encoding=""UTF-8""?>
<TEI xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiHeader>
        <fileDesc>
            <titleStmt>
                <title>Voyant Tools 2.0: The New, The Neat &amp; the Gnarly</title>
                <author>
                    <persName>
                        <surname>Sinclair</surname>
                        <forename>Stéfan</forename>
                    </persName>
                    <affiliation>McGill University, Canada</affiliation>
                    <email>sgsinclair@gmail.com</email>
                </author>
                <author>
                    <persName>
                        <surname>Rockwell</surname>
                        <forename>Geoffrey</forename>
                    </persName>
                    <affiliation>University of Alberta, Canada</affiliation>
                    <email>grockwel@ualberta.ca</email>
                </author>
            </titleStmt>
            <editionStmt>
                <edition>
                    <date>2014-12-19T13:50:00Z</date>
                </edition>
            </editionStmt>
            <publicationStmt>
                <publisher>Paul Arthur, University of Western Sidney</publisher>
                <address>
                    <addrLine>Locked Bag 1797</addrLine>
                    <addrLine>Penrith NSW 2751</addrLine>
                    <addrLine>Australia</addrLine>
                    <addrLine>Paul Arthur</addrLine>
                </address>
            </publicationStmt>
            <sourceDesc>
                <p>Converted from a Word document </p>
            </sourceDesc>
        </fileDesc>
        <encodingDesc>
            <appInfo>
                <application ident=""DHCONVALIDATOR"" version=""1.9"">
                    <label>DHConvalidator</label>
                </application>
            </appInfo>
        </encodingDesc>
        <profileDesc>
            <textClass>
                <keywords scheme=""ConfTool"" n=""category"">
                    <term>Paper</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""subcategory"">
                    <term>Pre-Conference Workshop and Tutorial (Round 2)</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""keywords"">
                    <term>text analysis</term>
                    <term>Voyant</term>
                </keywords>
                <keywords scheme=""ConfTool"" n=""topics"">
                    <term>text analysis</term>
                    <term>data mining / text mining</term>
                    <term>English</term>
                </keywords>
            </textClass>
        </profileDesc>
    </teiHeader>
    <text>
        <body>
            <p rend=""CM4"">Voyant Tools (voyant-tools.org) is a web-based reading and analysis environment for digital texts. Users can create their own corpus of texts to study by pointing to URLs or uploading files in a variety of formats (plain text, XML, HTML, PDF, MS Word, RTF, etc.). Voyant allows users to navigate between macro views of the corpus (e.g., a word cloud visualization of the entire corpus) and micro views (e.g., a reading individual occurrences of a specific term in context). The default interface provides access to a basic set of tools for reading texts and studying word frequency and distribution. There are also more tools available in various pre-defined or user-defined ‘skins’ (a layout of tools that are coupled). </p>
            <p rend=""CM4"">Voyant Tools is deliberately designed to be user-friendly and welcoming for text analysis. Voyant currently averages nearly 50,000 visits and about 750,000 tool invocations per month (not counting the downloadable instances of VoyantServer). This will be the seventh consecutive workshop of Voyant, with past sessions focusing on different aspects (pedagogy, multilingualism, customizability, standalone version, etc.). </p>
            <p rend=""CM4"">The 2015 workshop will focus on the second major release of Voyant Tools (2.0), which represents an entire rewrite of the codebase to address several of the major shortcomings and irritants of the currently available version 1.0. Version 2.0 is currently available in a beta version online with a major release due in early spring 2015. In addition to performance improvements throughout, the search and filtering functionality have been vastly enhanced, and Voyant now supports proximity and n-gram operations. Voyant 2.0 also has improved corpus handling. Documents can be reordered or added to corpora on the fly, and there is a lightweight access management layer that differentiates between full access, full-text access, and expressive/consumptive access. </p>
            <p rend=""CM4"">We have designed this workshop to be of interest both to new users of Voyant, who will get an introduction to the platform, and to existing users, who will discover all the new functionality 2.0 has to offer. As always, a crucial aspect of the workshop will be to get feedback from the community. </p>
            <p rend=""CM3"">Workshop Outline </p>
            <p rend=""CM2"">
                <hi rend=""italic"">1. Introduction to Text Analysis with Voyant (1 hour) </hi>
            </p>
            <p rend=""CM4"">We will begin with a general introduction to text analysis using Voyant aimed at those who haven’t used it before. We will provide a brief overview of Voyant’s user interface and discuss its strengths and weaknesses. We will provide initial text collections that users can use with Voyant, with a view to having participants experiment subsequently with their own text collections. </p>
            <p rend=""CM2"">
                <hi rend=""italic"">2. Voyant 2.0: What’s New? (1 hour) </hi>
            </p>
            <p rend=""CM2"">This second part of the workshop will focus on what’s new in Voyant 2.0. Examples of changes include more powerful proximity and fuzzy searching of terms, infinite scrolling instead of paginated scrolling for tabular data, in-place modifications of corpora (adding documents or re-ordering them), and new tools (collocate networks, n-gram wordtrees, etc.). This part of the workshop will be useful both for users familiar with the old Voyant (to understand the changes and enhancements) and also to newcomers who will get a better sense of the variety of available tools. </p>
            <p>
                <hi rend=""italic"">3. Voyant: Text Repository or Analytic Platform? (1 hour) </hi>
            </p>
            <p rend=""CM4"">One benefit of the enhanced scalability of Voyant Tools 2.0 is the ability to bridge the gap between existing text repositories (typically focused on searching for documents) and analytic platforms (for text mining). We are collaborating with several large-scale content providers (like TCP-EEBO and Érudit.org) to create custom Voyant skins that allow users to search and filter within very large text collections in order to create smaller worksets of relevant documents for analysis. Because everything is happening in Voyant, the jump from text repository to text analysis is smooth and efficient (very few text repositories allow mass downloading of worksets, but even when they do, additional steps are typically required for re-ingesting the workset into an analytic platform). This component of the workshop will demonstrate some of our existing collaborations and describe how other content providers and projects might be able to leverage this hybrid functionality. </p>
            <p rend=""CM3"">Workshop Leaders </p>
            <p rend=""CM4"">
                <hi rend=""italic"">Stéfan Sinclair</hi>, sgsinclair@gmail.com, is an associate professor in digital humanities at McGill University. His research focuses primarily on the design, development, and theorization of tools for the digital humanities, especially for text analysis and visualization. He has led or contributed significantly to projects such as Voyant Tools, the Text Analysis Portal for Research (TAPoR), and BonPatron. Other professional activities include serving as associate editor of 
                <hi rend=""italic"">Digital Humanities Quarterly</hi>, as well as serving on the executive boards of ACH, CSDH/SCHN, ADHO, and centerNET. 
            </p>
            <p rend=""CM2"">
                <hi rend=""italic"">Geoffrey Rockwell</hi>, grockwel@ualberta.ca, is a professor of philosophy and humanities computing at the University of Alberta, Canada. He has published and presented papers in the area of philosophical dialogue, textual visualization and analysis, humanities computing, instructional technology, computer games, and multimedia. He was the project leader for the CFI (Canada Foundation for Innovation)–funded project TAPoR, a Text Analysis Portal for Research (tapor.ca), which has developed a text tool portal for researchers who work with electronic texts. He is the author of 
                <hi rend=""italic"">Defining Dialogue: From Socrates to the Internet </hi>(Humanity Books).
            </p>
            <p rend=""CM4"">Target Audience </p>
            <p rend=""CM4"">A wide range of DH practitioners interested in text analysis, particularly for research, teaching, or technical support. Voyant Tools workshops are typically fully subscribed; we prefer to limit registration to about 25 people to allow us to help participants as needed. </p>
            <p rend=""CM2"">Format</p>
            <p rend=""CM2"">Half-day.</p>
        </body>
    </text>
</TEI>",xml,Creative Commons Attribution 4.0 International,,text analysis;voyant,English,data mining / text mining;english;text analysis,2015-01-01,"<?xml version=""1.0"" encoding=""utf-8""?>
<tei xmlns=""http://www.tei-c.org/ns/1.0"">
    <teiheader>
        <filedesc>
            <titlestmt>
                <title>voyant tools 2.0: the new, the neat &amp; the gnarly</title>
                <author>
                    <persname>
                        <surname>sinclair</surname>
                        <forename>stéfan</forename>
                    </persname>
                    <affiliation>mcgill university, canada</affiliation>
                    <email>sgsinclair@gmail.com</email>
                </author>
                <author>
                    <persname>
                        <surname>rockwell</surname>
                        <forename>geoffrey</forename>
                    </persname>
                    <affiliation>university of alberta, canada</affiliation>
                    <email>grockwel@ualberta.ca</email>
                </author>
            </titlestmt>
            <editionstmt>
                <edition>
                    <date>2014-12-19t13:50:00z</date>
                </edition>
            </editionstmt>
            <publicationstmt>
                <publisher>paul arthur, university of western sidney</publisher>
                <address>
                    <addrline>locked bag 1797</addrline>
                    <addrline>penrith nsw 2751</addrline>
                    <addrline>australia</addrline>
                    <addrline>paul arthur</addrline>
                </address>
            </publicationstmt>
            <sourcedesc>
                <p>converted from a word document </p>
            </sourcedesc>
        </filedesc>
        <encodingdesc>
            <appinfo>
                <application ident=""dhconvalidator"" version=""1.9"">
                    <label>dhconvalidator</label>
                </application>
            </appinfo>
        </encodingdesc>
        <profiledesc>
            <textclass>
                <keywords scheme=""conftool"" n=""category"">
                    <term>paper</term>
                </keywords>
                <keywords scheme=""conftool"" n=""subcategory"">
                    <term>pre-conference workshop and tutorial (round 2)</term>
                </keywords>
                <keywords scheme=""conftool"" n=""keywords"">
                    <term>text analysis</term>
                    <term>voyant</term>
                </keywords>
                <keywords scheme=""conftool"" n=""topics"">
                    <term>text analysis</term>
                    <term>data mining / text mining</term>
                    <term>english</term>
                </keywords>
            </textclass>
        </profiledesc>
    </teiheader>
    <text>
        <body>
            <p rend=""cm4"">voyant tools (voyant-tools.org) is a web-based reading and analysis environment for digital texts. users can create their own corpus of texts to study by pointing to urls or uploading files in a variety of formats (plain text, xml, html, pdf, ms word, rtf, etc.). voyant allows users to navigate between macro views of the corpus (e.g., a word cloud visualization of the entire corpus) and micro views (e.g., a reading individual occurrences of a specific term in context). the default interface provides access to a basic set of tools for reading texts and studying word frequency and distribution. there are also more tools available in various pre-defined or user-defined ‘skins’ (a layout of tools that are coupled). </p>
            <p rend=""cm4"">voyant tools is deliberately designed to be user-friendly and welcoming for text analysis. voyant currently averages nearly 50,000 visits and about 750,000 tool invocations per month (not counting the downloadable instances of voyantserver). this will be the seventh consecutive workshop of voyant, with past sessions focusing on different aspects (pedagogy, multilingualism, customizability, standalone version, etc.). </p>
            <p rend=""cm4"">the 2015 workshop will focus on the second major release of voyant tools (2.0), which represents an entire rewrite of the codebase to address several of the major shortcomings and irritants of the currently available version 1.0. version 2.0 is currently available in a beta version online with a major release due in early spring 2015. in addition to performance improvements throughout, the search and filtering functionality have been vastly enhanced, and voyant now supports proximity and n-gram operations. voyant 2.0 also has improved corpus handling. documents can be reordered or added to corpora on the fly, and there is a lightweight access management layer that differentiates between full access, full-text access, and expressive/consumptive access. </p>
            <p rend=""cm4"">we have designed this workshop to be of interest both to new users of voyant, who will get an introduction to the platform, and to existing users, who will discover all the new functionality 2.0 has to offer. as always, a crucial aspect of the workshop will be to get feedback from the community. </p>
            <p rend=""cm3"">workshop outline </p>
            <p rend=""cm2"">
                <hi rend=""italic"">1. introduction to text analysis with voyant (1 hour) </hi>
            </p>
            <p rend=""cm4"">we will begin with a general introduction to text analysis using voyant aimed at those who haven’t used it before. we will provide a brief overview of voyant’s user interface and discuss its strengths and weaknesses. we will provide initial text collections that users can use with voyant, with a view to having participants experiment subsequently with their own text collections. </p>
            <p rend=""cm2"">
                <hi rend=""italic"">2. voyant 2.0: what’s new? (1 hour) </hi>
            </p>
            <p rend=""cm2"">this second part of the workshop will focus on what’s new in voyant 2.0. examples of changes include more powerful proximity and fuzzy searching of terms, infinite scrolling instead of paginated scrolling for tabular data, in-place modifications of corpora (adding documents or re-ordering them), and new tools (collocate networks, n-gram wordtrees, etc.). this part of the workshop will be useful both for users familiar with the old voyant (to understand the changes and enhancements) and also to newcomers who will get a better sense of the variety of available tools. </p>
            <p>
                <hi rend=""italic"">3. voyant: text repository or analytic platform? (1 hour) </hi>
            </p>
            <p rend=""cm4"">one benefit of the enhanced scalability of voyant tools 2.0 is the ability to bridge the gap between existing text repositories (typically focused on searching for documents) and analytic platforms (for text mining). we are collaborating with several large-scale content providers (like tcp-eebo and érudit.org) to create custom voyant skins that allow users to search and filter within very large text collections in order to create smaller worksets of relevant documents for analysis. because everything is happening in voyant, the jump from text repository to text analysis is smooth and efficient (very few text repositories allow mass downloading of worksets, but even when they do, additional steps are typically required for re-ingesting the workset into an analytic platform). this component of the workshop will demonstrate some of our existing collaborations and describe how other content providers and projects might be able to leverage this hybrid functionality. </p>
            <p rend=""cm3"">workshop leaders </p>
            <p rend=""cm4"">
                <hi rend=""italic"">stéfan sinclair</hi>, sgsinclair@gmail.com, is an associate professor in digital humanities at mcgill university. his research focuses primarily on the design, development, and theorization of tools for the digital humanities, especially for text analysis and visualization. he has led or contributed significantly to projects such as voyant tools, the text analysis portal for research (tapor), and bonpatron. other professional activities include serving as associate editor of 
                <hi rend=""italic"">digital humanities quarterly</hi>, as well as serving on the executive boards of ach, csdh/schn, adho, and centernet. 
            </p>
            <p rend=""cm2"">
                <hi rend=""italic"">geoffrey rockwell</hi>, grockwel@ualberta.ca, is a professor of philosophy and humanities computing at the university of alberta, canada. he has published and presented papers in the area of philosophical dialogue, textual visualization and analysis, humanities computing, instructional technology, computer games, and multimedia. he was the project leader for the cfi (canada foundation for innovation)–funded project tapor, a text analysis portal for research (tapor.ca), which has developed a text tool portal for researchers who work with electronic texts. he is the author of 
                <hi rend=""italic"">defining dialogue: from socrates to the internet </hi>(humanity books).
            </p>
            <p rend=""cm4"">target audience </p>
            <p rend=""cm4"">a wide range of dh practitioners interested in text analysis, particularly for research, teaching, or technical support. voyant tools workshops are typically fully subscribed; we prefer to limit registration to about 25 people to allow us to help participants as needed. </p>
            <p rend=""cm2"">format</p>
            <p rend=""cm2"">half-day.</p>
        </body>
    </text>
</tei>",24.0,25.0,Voyant
9252,2020 - Ottawa,Ottawa,carrefours / intersections,2020,ADHO,ADHO,Carleton University;Université d'Ottawa (University of Ottawa),Ottawa,Ontario,Canada,https://dh2020.adho.org/,Human-Centered Computing for Humanists: Case Studies from the Computational Thinking and Learning Initiative at Vanderbilt University,,Clifford B. Anderson;Corey E. Brady;Brian Broll;Lynn T. Ramey,"paper, specified ""short paper""","The Computational Thinking and Learning Initiative (CTLI) at Vanderbilt University formed out of an awareness that, across society and academe, computation is changing the nature of knowledge. As the practices and methods for producing, sharing, and contesting knowledge change, the enterprise of the university—its disciplinary scholarship, liberal arts mission, and charge to prepare professionals for the world of work—is being reshaped by algorithmic norms. The rise of computational thinking as a transdisciplinary category holds both promise and peril for the humanities. How can humanists, especially digital humanists, take advantage of the push for computational thinking across the curriculum while avoiding the dangers of appropriation? Alternatively, how can those developing and refining computational methods tap into the critical perspectives of the humanities? This paper discusses how an interdisciplinary group of colleagues is drawing on the theory of human-centered computing to develop environments and curricula for students of the humanities to explore the basics of text mining while also providing them with space to critique and resist the imposition of algorithmic rationality.The CTLI foregrounds a particular image of computational thinking across the curriculum, working to identify, stabilize, and study new forms of human-computer partnership that are responsive to disciplinary ways of knowing. Our aim is to study new ways in which individuals or groups of humans, along with computers or groups of computational entities, can come together productively and critically as collective computational-thinking units and build on complementary strengths to investigate problems while rejecting facile technical solutions. The perspective of human-centered computing, that is, the “design of computing systems with a human focus from beginning to end,”1 functions as an Archimedean point of our collaboration.A trans-institutional group of researchers and scholars have assembled at the CTLI, including faculty from the schools of arts and science, engineering, and education as well as the library and the data sciences institute. Together, we are exploring how human-centered computing collaborations transform epistemologies, practices, and pedagogies across disciplines.Following the programmatic overview that Anderson and Ramey presented at DH2019,2 these collaborators selected textual analysis as one of two focus areas (the other being climate change) for the first year of the initiative. Using NetsBlox,3 a block-based programming environment developed at Vanderbilt University, we constructed components and curricula to teach students in the humanities the fundamentals of computational thinking (variables, looping, functions, recursion, etc.) by manipulating textual corpora rather than matrices of numbers. We wanted this environment to be ""low threshold"" enough for middle and high school students to use it as an entry point to disciplinary inquiry, while having a sufficiently ""high ceiling"" to allow scholars of literature and history to use it meaningfully as well.Given our commitment to discipline-specific visions of computational thinking, our pilot project attempted to understand how the professional vision4 of the humanist might resonate with the technological capacity of new computational tools and methods. We viewed human-computer collaboration as an integration of productively-different ways of interacting with the objects of analysis (e.g., texts). Computers and humans “read” differently, and the trick was to find a way to put these ways-of-reading into conversation. We used design-based research5 to investigate possibilities, supporting and studying scholars' creative efforts to engage with technological tools and achieve what they regarded as progress on humanistic projects. We then developed a learning environment that enabled younger students to take on similar relations to computational tools, but in simpler, playful settings. Working with the same computational approaches at different levels of complexity and sophistication, we aimed to gain new perspectives on their power and limitations. We explored a design space with scholars and students centered on the analysis of style and affect in poetry, iteratively identifying and testing functionality with the ultimate objective of creating activities for both secondary-school and undergraduate courses.In the course of our explorations, we narrowed the team’s focus to three different technologies for textual analysis: (i) fundamental natural language processing concepts such as named entity recognition; (ii) word embeddings, such as Word2Vec;6 and (iii) a “query runner” to TEI-encoded documents in BaseX,7 a native XML database. During our collaboration, the team applied each of these technologies, first playfully, to explore what insights they could yield with familiar and constructed texts; then more deliberately, in settings in which it was likely that the computer’s “readings” might be put in productive conversation with humans’ readings. Finally, the team proposed questions, on the one hand, that revealed new patterns in larger corpora; and activities, on the other, that engaged younger learners in reflecting on style as a feature of writing under the writer’s control.In this paper, we report on this first year’s effort and our progress in building a block-based computing environment and curriculum that supports textual analysis by both high school students and professors of literature. We discuss how these tools and perspectives scaffold activities as diverse as teaching secondary students about the linguistic differences between poetry and prose and detecting stylistic patterns among Victorian writers. We also discuss the limitations of taking a human-centered computing approach when conducting digital humanities research; in particular, we examine the drawbacks of block-based languages for text mining in comparison with tools like Lexos8 and Voyant.9",txt,This text is republished here with permission from the original rights holder.,,block-based programming;computational thinking;human-centered computing,English,computer science;contemporary;curricular and pedagogical development and analysis;education/ pedagogy;english;north america;text mining and analysis,2020-01-01,"the computational thinking and learning initiative (ctli) at vanderbilt university formed out of an awareness that, across society and academe, computation is changing the nature of knowledge. as the practices and methods for producing, sharing, and contesting knowledge change, the enterprise of the university—its disciplinary scholarship, liberal arts mission, and charge to prepare professionals for the world of work—is being reshaped by algorithmic norms. the rise of computational thinking as a transdisciplinary category holds both promise and peril for the humanities. how can humanists, especially digital humanists, take advantage of the push for computational thinking across the curriculum while avoiding the dangers of appropriation? alternatively, how can those developing and refining computational methods tap into the critical perspectives of the humanities? this paper discusses how an interdisciplinary group of colleagues is drawing on the theory of human-centered computing to develop environments and curricula for students of the humanities to explore the basics of text mining while also providing them with space to critique and resist the imposition of algorithmic rationality.the ctli foregrounds a particular image of computational thinking across the curriculum, working to identify, stabilize, and study new forms of human-computer partnership that are responsive to disciplinary ways of knowing. our aim is to study new ways in which individuals or groups of humans, along with computers or groups of computational entities, can come together productively and critically as collective computational-thinking units and build on complementary strengths to investigate problems while rejecting facile technical solutions. the perspective of human-centered computing, that is, the “design of computing systems with a human focus from beginning to end,”1 functions as an archimedean point of our collaboration.a trans-institutional group of researchers and scholars have assembled at the ctli, including faculty from the schools of arts and science, engineering, and education as well as the library and the data sciences institute. together, we are exploring how human-centered computing collaborations transform epistemologies, practices, and pedagogies across disciplines.following the programmatic overview that anderson and ramey presented at dh2019,2 these collaborators selected textual analysis as one of two focus areas (the other being climate change) for the first year of the initiative. using netsblox,3 a block-based programming environment developed at vanderbilt university, we constructed components and curricula to teach students in the humanities the fundamentals of computational thinking (variables, looping, functions, recursion, etc.) by manipulating textual corpora rather than matrices of numbers. we wanted this environment to be ""low threshold"" enough for middle and high school students to use it as an entry point to disciplinary inquiry, while having a sufficiently ""high ceiling"" to allow scholars of literature and history to use it meaningfully as well.given our commitment to discipline-specific visions of computational thinking, our pilot project attempted to understand how the professional vision4 of the humanist might resonate with the technological capacity of new computational tools and methods. we viewed human-computer collaboration as an integration of productively-different ways of interacting with the objects of analysis (e.g., texts). computers and humans “read” differently, and the trick was to find a way to put these ways-of-reading into conversation. we used design-based research5 to investigate possibilities, supporting and studying scholars' creative efforts to engage with technological tools and achieve what they regarded as progress on humanistic projects. we then developed a learning environment that enabled younger students to take on similar relations to computational tools, but in simpler, playful settings. working with the same computational approaches at different levels of complexity and sophistication, we aimed to gain new perspectives on their power and limitations. we explored a design space with scholars and students centered on the analysis of style and affect in poetry, iteratively identifying and testing functionality with the ultimate objective of creating activities for both secondary-school and undergraduate courses.in the course of our explorations, we narrowed the team’s focus to three different technologies for textual analysis: (i) fundamental natural language processing concepts such as named entity recognition; (ii) word embeddings, such as word2vec;6 and (iii) a “query runner” to tei-encoded documents in basex,7 a native xml database. during our collaboration, the team applied each of these technologies, first playfully, to explore what insights they could yield with familiar and constructed texts; then more deliberately, in settings in which it was likely that the computer’s “readings” might be put in productive conversation with humans’ readings. finally, the team proposed questions, on the one hand, that revealed new patterns in larger corpora; and activities, on the other, that engaged younger learners in reflecting on style as a feature of writing under the writer’s control.in this paper, we report on this first year’s effort and our progress in building a block-based computing environment and curriculum that supports textual analysis by both high school students and professors of literature. we discuss how these tools and perspectives scaffold activities as diverse as teaching secondary students about the linguistic differences between poetry and prose and detecting stylistic patterns among victorian writers. we also discuss the limitations of taking a human-centered computing approach when conducting digital humanities research; in particular, we examine the drawbacks of block-based languages for text mining in comparison with tools like lexos8 and voyant.9",1.0,1.0,Voyant
9318,2020 - Ottawa,Ottawa,carrefours / intersections,2020,ADHO,ADHO,Carleton University;Université d'Ottawa (University of Ottawa),Ottawa,Ontario,Canada,https://dh2020.adho.org/,Trans-Hispanic Networks of Feminist Solidarity: The Rise and Spread of #8M,,Vanessa Ceia;Rhian Lewis,"paper, specified ""long paper""","On March 8 2019, Spanish-speaking Twitter communities erupted with a polyvocal outcry against gender violence, while simultaneous offline protests took place in cities across the globe. This paper presents the results of a multi-modal spatial and thematic analysis of regional appropriations of the hashtag #8M (8 March) to illustrate the carrefours/intersections of trans-Hispanic networks of feminist solidarity online. Through spatial, network, and word frequency analyses of #8M, we examine the hashtag’s intersections with existing transnational and regionally-specific feminist Twitter dialogues, including the #NiUnaMenos movement launched to combat femicide in Argentina and the hashtag #Cuéntalo used to protest the lenient sentencing of a group of men who raped a woman in Pamplona, Spain. By combining quantitative and geospatial analyses using Twitter Archiving Google Sheets, Voyant-Tools, and Carto with targeted close readings of tweets containing #8M, this paper traces the regional variations in a large, multinational online Spanish-language conversation about gender violence.",txt,This text is republished here with permission from the original rights holder.,,#8M;Digital Activism;Feminism;Hispanic Feminisms;Twitter,English,comparative (2 or more geographical areas);contemporary;cultural studies;digital activism and advocacy;english;europe;feminist studies;social media analysis and methods;south america,2020-01-01,"on march 8 2019, spanish-speaking twitter communities erupted with a polyvocal outcry against gender violence, while simultaneous offline protests took place in cities across the globe. this paper presents the results of a multi-modal spatial and thematic analysis of regional appropriations of the hashtag #8m (8 march) to illustrate the carrefours/intersections of trans-hispanic networks of feminist solidarity online. through spatial, network, and word frequency analyses of #8m, we examine the hashtag’s intersections with existing transnational and regionally-specific feminist twitter dialogues, including the #niunamenos movement launched to combat femicide in argentina and the hashtag #cuéntalo used to protest the lenient sentencing of a group of men who raped a woman in pamplona, spain. by combining quantitative and geospatial analyses using twitter archiving google sheets, voyant-tools, and carto with targeted close readings of tweets containing #8m, this paper traces the regional variations in a large, multinational online spanish-language conversation about gender violence.",1.0,1.0,Voyant
9440,2020 - Ottawa,Ottawa,carrefours / intersections,2020,ADHO,ADHO,Carleton University;Université d'Ottawa (University of Ottawa),Ottawa,Ontario,Canada,https://dh2020.adho.org/,Teaching at the Intersection of Digital Humanities and Visualization,,Stefan Jänicke,"paper, specified ""short paper""","The increasing value of information visualization techniques to support investigating quantitative research questions in humanities applications is well-documented (Jänicke et al., 2017; Windhager et al., 2019). At the same time, it is important to make (digital) humanities scholars literate in dealing with visualizations to ensure that accurate conclusions can be drawn. In order to make younger generations ready for interdisciplinary work in a digital humanities context, I taught a module that attracted both computer science and humanities students. This article reflects on the most important aspects in teaching the course throughout three years.Cohort of studentsTeaching needs to be flexibly organized in dependency on the backgrounds of students joining a course. A course might be offered exclusively for either humanities or computer science students, or it can attract students with diverse study subjects. I faced different constellations, and emphasized different aspects according to the demands of the cohort. Whereas humanities students profit from more intense discussions on computational thinking and data modeling, it is especially helpful for computer science students to get to know typical research interests and traditional workflows of humanities scholars. The major focus should be to ensure that students, independent of their backgrounds, learn to “speak the same language” using the same terminology.Teaching visualization theoryTo serve students with an easy-to-digest overview of visualization design, I recommend Tamara Munzner’s book “Visualization Analysis and Design” (Munzner, 2014). It provides an introduction to data and task abstraction that are necessary to comprehend and to develop new or adapt existing visualizations. It further discusses how data features can be appropriately mapped to visual features, and how users can interact with visualizations. Also, emphasis should be devoted to Shneiderman’s Information Seeking Mantra “Overview first, zoom and filter, then details-on-demand” (Shneiderman, 1996) as it encapsulates the general idea of quantitative data analysis without losing the materials a visualization is composed of. This rather theoretical frame should be accomplished with discussing visualization techniques that are of particular importance for digital humanities research: geographical maps, timelines, tag clouds, heat maps and graphs.Ready-to-use-based vs. development-driven visualizationsNext to theoretical contents, the course should have a strong focus on practical work. It is known that different approaches to make use of visualization for knowledge discovery in digital humanities applications exist (Jänicke, 2016). The first approach is to apply ready-to-use tools like Voyant (Sinclair and Rockwell, 2020) for quantitative visual text analysis or Gephi (Bastian et al., 2009) for graph visualization. While the advantage of using such frameworks is generating arguable visual output in a short amount of time, research interests might deviate from what the tool can provide. Further, scholars need to learn how to use potentially complex tools and how to interpret upcoming results. In contrast to applying existing tools to generate visualizations, (digital) humanities and visualization scholars might also engage with each other aiming to generate a new visual vocabulary to expedite knowledge discovery, thereby facing the problems of interdisciplinary collaborations. While the first approach should be carried out simultaneously to the theoretical sessions, I recommend to conduct interdisciplinary student projects in the second part of the course.Supervisory roles during project workTraining to apply the learned visualization-related terminology should be the main focus of a project, while the visualization result itself plays a secondary role. Especially in a setting with mixed backgrounds, students should be engaged to think about potentially interesting project ideas. Each conducted project should include at least one participant with a (digital) humanities and one with a computer science background to expedite interdisciplinary exchange. This constraint generates different supervisory roles:The Mediator: Projects involve students having a computer science and a humanities background alike. The entire project is managed by the students, whereas the teacher might supervise in the form of a mediator during meetings. While students face typical pitfalls of interdisciplinary projects, such projects still brought forth considerable results, one of which is shown in Figure 1.The Real & the Fake Humanities Scholar: When the number of students with a humanities background is too low, project groups that only include computer science students need to be complemented with domain experts. For some of the projects, I was able to involve partners from the humanities, the real humanities scholars, with research interests targeted towards available data sets. This setting generated very good results (see Figure 2) and guaranteed the steepest learning curve for computer science students as they cooperated with domain experts experienced in digital humanities. However, for some projects I, educated in computer science, needed to act as a humanities scholar, being the least favorable setting as only fake interdisciplinary discussions are possible.The Helper: On the other hand, the number of computer science students joining a course might be limited. In that case, I could advise students in the role of a computer scientist, better suitable considering my own background. The focus in such projects was rather on data modeling and acquisition as well as applying existing tools and libraries than developing new solutions. An example is shown in Figure 3.Figure 1: The mediator project developed a method to semi-automatically extract biographical information about members of the German Bundestag in 2019, and the adapted stream visualization allows multifaceted exploration of biographical features.Figure 2: The real humanities scholar project focused on the development of an interactive tag cloud that supports composing engineering branches based on the study subjects of engineering professors. More information can be found in a related publication on the project (Meinecke and Jänicke, 2018).Figure 3: The helper project focused on the contents published at three German websites known for publishing fake news articles. For comparatively analyzing the results, the TagPies visualization (Jänicke et al., 2018) was adapted.I recommend making such courses accessible to all students as it prepares them best for potential future collaborations in a digital humanities context. More detailed information on theoretical contents, conducted student projects and course reflections can be found in my related IVAPP article (Jänicke, 2020).",txt,This text is republished here with permission from the original rights holder.,,teaching;visualization,English,"computer science;contemporary;curricular and pedagogical development and analysis;english;global;spatial & spatio-temporal analysis, modeling and visualization",2020-01-01,"the increasing value of information visualization techniques to support investigating quantitative research questions in humanities applications is well-documented (jänicke et al., 2017; windhager et al., 2019). at the same time, it is important to make (digital) humanities scholars literate in dealing with visualizations to ensure that accurate conclusions can be drawn. in order to make younger generations ready for interdisciplinary work in a digital humanities context, i taught a module that attracted both computer science and humanities students. this article reflects on the most important aspects in teaching the course throughout three years.cohort of studentsteaching needs to be flexibly organized in dependency on the backgrounds of students joining a course. a course might be offered exclusively for either humanities or computer science students, or it can attract students with diverse study subjects. i faced different constellations, and emphasized different aspects according to the demands of the cohort. whereas humanities students profit from more intense discussions on computational thinking and data modeling, it is especially helpful for computer science students to get to know typical research interests and traditional workflows of humanities scholars. the major focus should be to ensure that students, independent of their backgrounds, learn to “speak the same language” using the same terminology.teaching visualization theoryto serve students with an easy-to-digest overview of visualization design, i recommend tamara munzner’s book “visualization analysis and design” (munzner, 2014). it provides an introduction to data and task abstraction that are necessary to comprehend and to develop new or adapt existing visualizations. it further discusses how data features can be appropriately mapped to visual features, and how users can interact with visualizations. also, emphasis should be devoted to shneiderman’s information seeking mantra “overview first, zoom and filter, then details-on-demand” (shneiderman, 1996) as it encapsulates the general idea of quantitative data analysis without losing the materials a visualization is composed of. this rather theoretical frame should be accomplished with discussing visualization techniques that are of particular importance for digital humanities research: geographical maps, timelines, tag clouds, heat maps and graphs.ready-to-use-based vs. development-driven visualizationsnext to theoretical contents, the course should have a strong focus on practical work. it is known that different approaches to make use of visualization for knowledge discovery in digital humanities applications exist (jänicke, 2016). the first approach is to apply ready-to-use tools like voyant (sinclair and rockwell, 2020) for quantitative visual text analysis or gephi (bastian et al., 2009) for graph visualization. while the advantage of using such frameworks is generating arguable visual output in a short amount of time, research interests might deviate from what the tool can provide. further, scholars need to learn how to use potentially complex tools and how to interpret upcoming results. in contrast to applying existing tools to generate visualizations, (digital) humanities and visualization scholars might also engage with each other aiming to generate a new visual vocabulary to expedite knowledge discovery, thereby facing the problems of interdisciplinary collaborations. while the first approach should be carried out simultaneously to the theoretical sessions, i recommend to conduct interdisciplinary student projects in the second part of the course.supervisory roles during project worktraining to apply the learned visualization-related terminology should be the main focus of a project, while the visualization result itself plays a secondary role. especially in a setting with mixed backgrounds, students should be engaged to think about potentially interesting project ideas. each conducted project should include at least one participant with a (digital) humanities and one with a computer science background to expedite interdisciplinary exchange. this constraint generates different supervisory roles:the mediator: projects involve students having a computer science and a humanities background alike. the entire project is managed by the students, whereas the teacher might supervise in the form of a mediator during meetings. while students face typical pitfalls of interdisciplinary projects, such projects still brought forth considerable results, one of which is shown in figure 1.the real & the fake humanities scholar: when the number of students with a humanities background is too low, project groups that only include computer science students need to be complemented with domain experts. for some of the projects, i was able to involve partners from the humanities, the real humanities scholars, with research interests targeted towards available data sets. this setting generated very good results (see figure 2) and guaranteed the steepest learning curve for computer science students as they cooperated with domain experts experienced in digital humanities. however, for some projects i, educated in computer science, needed to act as a humanities scholar, being the least favorable setting as only fake interdisciplinary discussions are possible.the helper: on the other hand, the number of computer science students joining a course might be limited. in that case, i could advise students in the role of a computer scientist, better suitable considering my own background. the focus in such projects was rather on data modeling and acquisition as well as applying existing tools and libraries than developing new solutions. an example is shown in figure 3.figure 1: the mediator project developed a method to semi-automatically extract biographical information about members of the german bundestag in 2019, and the adapted stream visualization allows multifaceted exploration of biographical features.figure 2: the real humanities scholar project focused on the development of an interactive tag cloud that supports composing engineering branches based on the study subjects of engineering professors. more information can be found in a related publication on the project (meinecke and jänicke, 2018).figure 3: the helper project focused on the contents published at three german websites known for publishing fake news articles. for comparatively analyzing the results, the tagpies visualization (jänicke et al., 2018) was adapted.i recommend making such courses accessible to all students as it prepares them best for potential future collaborations in a digital humanities context. more detailed information on theoretical contents, conducted student projects and course reflections can be found in my related ivapp article (jänicke, 2020).",1.0,1.0,Voyant
9724,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Thinking Computationally in the Digital Humanities: Toward Block-Based Programming for Humanists,,Clifford B. Anderson;Lynn T. Ramey,"paper, specified ""long paper""","<text>
        
            <div type=""div1"" rend=""DH-Heading1"">
                What is computational thinking in the digital humanities?
                <p style=""text-align:left; "">The question whether digital humanists should learn to code has been highly-contested (Ramsay, 2011: 243–45). The ‘hack’ versus ‘yack’ debate has lost its edge as scholars concede that theory and programming praxis can be brought together productively through what Davidson terms “collaboration by difference” (Davidson, 2015: 134). Given that the majority of digital humanists will not need to program professionally, to what extent ought computation be taught in the digital humanities? Jeannette M. Wing coined the term “computational thinking” to address the teaching of digital literacy beyond computer science. She contended that “computational thinking is a fundamental skill for everyone, not just for computer scientists” (Wing, 2006: 33). In 
                    <hi rend=""italic"">Digital Humanities</hi>, Berry and Fagerjord comment at length on Wing’s definition, concluding that “a critical understanding of computing at its different levels is a prerequisite for a digital humanist...” (Berry and Fagerjord, 2017: 59). There is little agreement, however, about the best way to teach computational thinking to humanists. We review the potential of visual or “block-based” programming languages for teaching computational literacy in the digital humanities. We argue that digital humanists should learn from these tools’ emphasis on the ludic over the pragmatic. We also offer suggestions about how digital humanists might adapt and critically adopt block-based programming as they seek to expand their understanding of fundamental concepts of computer science.
                </p>
            </div>
            <div xml:id=""review-of-educational-programming-enviro"" type=""div1"" rend=""DH-Heading1"">
                Review of educational programming environments
                <p style=""text-align:left; "">The use of visual or “block-based” programming has become a mainstay for computer science education in the K-12 arena. Block-based programming involves the manipulation of graphical elements to create units of computation. The authors of 
                    <hi rend=""italic"">Learnable Programming: Blocks and Beyond</hi> argue that block-based programming makes it easier to learn to program for three primary reasons: emphasizing recognition over recall, “chunking code,” and constraining options (Bau et al., 2017: 72–80). These pedagogical advantages would also seem to apply in the digital humanities though, as a quick review of the evolution of these languages demonstrates, they were not created to teach computational thinking to adults.
                </p>
                <list type=""unordered"">
                    <item>
                        <hi rend=""bold"">Logo.</hi> The origins of block-based programming stretch back to Logo, a programming language and graphical environment for computer science education. While not a visual programming language, when paired with Turtle Graphics Logo provides students with the ability to visualize their computations (Papert, 1980: 16–20). Logo has gained renewed popularity among the elementary age set due to Gene Luen Yang and Mike Holmes’ 
                        <hi rend=""italic"">Secret Coders</hi>, a series of graphic novels that employs Logo to teach basic computational literacy (Yang and Holmes, 2015).
                        <lb></lb>
                    </item>
                    <item>
                        <hi rend=""bold"">Scratch.</hi> The designers of Scratch sought to create a computational environment for kids and teens to become active manipulators rather than passive consumers of digital media. The designers stripped away many of the complexities of software development (e.g., linking libraries and compiling binaries) to create what they term a “tinkerable” environment, pioneering the use of blocks for syntax (Maloney et al., 2010: 16:4).
                    </item>
                    <item>
                        <hi rend=""bold"">Snap!</hi> While Scratch succeeded in developing an extensive community of users in the K-12 arena, its emphasis on semantic simplicity inhibits its usefulness for teaching students computer science at the postsecondary level. The Snap! programming environment emerged from a collaboration between Brian Harvey at Berkeley and Jens Mönig, a software developer currently at SAP. Drawing on long experience teaching functional programming in Scheme, the authors created a semantics with lambda expressions, recursion, and high-order functions using a Scratch-like syntax (Harvey and Mönig, 2015: 35–38).
                    </item>
                    <item>
                        <hi rend=""bold"">NetsBlox</hi>. NetsBlox is an adaptation of Snap! that makes it straightforward for users to communicate with internet services and to communicate peer-to-peer (Broll et al., 2017: 81–86). Students can draw on these features of NetsBlox, for example, to place markers representing art museums in the vicinity on a Google Map or to create a shared digital whiteboard. By fostering the ability to communicate beyond the boundaries of the programmer’s laptop, NetsBlox paves the way for creating data-driven digital humanities projects. Data may also be persisted in the cloud, making it possible to preserve state. Currently, NetsBlox comes with the ability to call out to a select number of services. However, the developers envision “adding a lot of new services and data sources to NetsBlox...”(Broll et al., 2017: 86). This raises the question whether a version of NetsBlox could be developed specifically for digital humanists, integrating web-based application programming interfaces (or APIs) for platforms like the DPLA, the HathiTrust, and Europeana, among others.
                    </item>
                </list>
                <figure>
                    <graphic n=""1001"" width=""15.980833333333333cm"" height=""8.989216666666668cm"" url=""Pictures/4e4065d832a2206acf1bc0a4c40bd8c4.png"" rend=""inline""></graphic>
                </figure>
                <p>NetsBlox with prototype RPC block for Wikidata</p>
                <p style=""text-align:left; "">The digital humanities community also embraces visual programming models. Voyant Tools, for instance, provides a graphical interface for scholars seeking to study textual corpora (Sinclair et al., 2016). To date, there appears to be little to no scholarship about the pedagogical effectiveness of using visual programming environments in the digital humanities.</p>
            </div>
            <div xml:id=""programming-as-ludic-rather-than-pragmat"" type=""div1"" rend=""DH-Heading1"">
                Programming as ludic rather than pragmatic
                <p style=""text-align:left; "">Is learning block-based programming a means to an end or an end in itself? While computer science students will inevitably move from block-based to text-based programming (Kölling et al., 2015: 29–38), the designers of Scratch claim that many students will fruitfully remain within its environment (Resnick et al., 2009: 66f). At the secondary and post-secondary level, “The Beauty and Joy of Computing” curriculum likewise promotes the enjoyment of programming within the Snap! environment: “having fun is an explicit course goal” (Garcia et al., 2015: 71). Leading digital humanists also acknowledge the playful aspects of programing. In “On Building,” Stephen Ramsay remarks, “Learn to code because it’s fun and because it will change the way you look at the world” (Ramsay, 2011: 245). Nick Montfort argues that motivations for learning programming go beyond the “merely instrumental” (Montfort, 2016: 268), remarking “it is enjoyable to write computer programs and to use them to create and discover” (Montfort, 2016: 277). By customizing the visual representations and selecting domain-specific exercises, block-based programming could find wide application in the digital humanities, promoting the joy of learning computation for its own sake while providing humanists with a better conceptual grounding for the evaluation and application of algorithms and software in their digital research.</p>
            </div>
            <div xml:id=""prolegomena-to-any-future-visual-program"" type=""div1"" rend=""DH-Heading1"">
                Prolegomena to any future visual programming environment for the digital humanities
                <p style=""text-align:left; "">What would a digital humanities version of a block-based programing environment look like? By way of conclusion, we suggest how NetsBlox might evolve past its origins in Scratch to provide a shared platform for teaching computational thinking in the digital humanities. We propose three developments: 1. creating default sprites that represent the domains of digital humanities research (i.e. representing books rather than basketballs); 2. establishing libraries of blocks to call commonly-used web-based APIs in the digital humanities; 3. providing a curriculum focusing on major research areas in the digital humanities, including distant reading, educational gaming, geospatial analysis, and steganography, among other topics. By developing a block-based environment for the digital humanities, we hope not only to advance computational thinking in our field, but also to provide resources for introducing the digital humanities into secondary and postsecondary courses on computational thinking.</p>
            </div>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Bau, D., Gray, J., Kelleher, C., Sheldon, J. and Turbak, F.</hi> (2017). Learnable programming: Blocks and beyond. 
                        <hi rend=""italic"">Commun. ACM</hi>, 
                        <hi rend=""bold"">60</hi>(6). New York, NY, USA: ACM: 72–80.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Berry, D. M. and Fagerjord, A.</hi> (2017). 
                        <hi rend=""italic"">Digital Humanities: Knowledge and Critique in a Digital Age</hi>. Cambridge: Polity.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Broll, B., Lédeczi, A., Volgyesi, P., Sallai, J., Maroti, M., Carrillo, A., Weeden-Wright, S. L., Vanags, C., Swartz, J. D. and Lu, M.</hi> (2017). A visual programming environment for learning distributed programming. In, 
                        <hi rend=""italic"">Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education</hi>. ACM, pp. 81–86.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Davidson, C.</hi> (2015). Why yack needs hack (and vice versa): From digital humanities to digital literacy. 
                        <hi rend=""italic"">Svensson, P. , Goldberg, DT, Ed</hi>: 131–45.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Garcia, D., Harvey, B. and Barnes, T.</hi> (2015). The beauty and joy of computing. 
                        <hi rend=""italic"">ACM Inroads</hi>, 
                        <hi rend=""bold"">6</hi>(4). New York, NY, USA: ACM: 71–79.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Harvey, B. and Mönig, J.</hi> (2015). Lambda in blocks languages: Lessons learned. In, 
                        <hi rend=""italic"">2015 IEEE Blocks and Beyond Workshop (Blocks and Beyond)</hi>. pp. 35–38.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kölling, M., Brown, N. C. C. and Altadmri, A.</hi> (2015). Frame-based editing: Easing the transition from blocks to text-based programming. In, 
                        <hi rend=""italic"">Proceedings of the Workshop in Primary and Secondary Computing Education</hi>. (WiPSCE ’15). New York, NY, USA: ACM, pp. 29–38.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Maloney, J., Resnick, M., Rusk, N., Silverman, B. and Eastmond, E.</hi> (2010). The scratch programming language and environment. 
                        <hi rend=""italic"">Trans. Comput. Educ.</hi>, 
                        <hi rend=""bold"">10</hi>(4). New York, NY, USA: ACM: 16:1–16:15.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Montfort, N.</hi> (2016). 
                        <hi rend=""italic"">Exploratory Programming for the Arts and Humanities</hi>. Cambridge: MIT Press.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Papert, S.</hi> (1980). 
                        <hi rend=""italic"">Mindstorms: Children, Computers, and Powerful Ideas</hi>. New York, NY, USA: Basic Books, Inc.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ramsay, S.</hi> (2011). On building. In Melissa Terras, Julianne Nyhan, (ed), 
                        <hi rend=""italic"">Defining Digital Humanities: A Reader</hi>. London: Routledge, pp. 243–45.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Resnick, M., Maloney, J., Monroy-Hernández, A., Rusk, N., Eastmond, E., Brennan, K., Millner, A., et al.</hi> (2009). Scratch: Programming for all. 
                        <hi rend=""italic"">Commun. ACM</hi>, 
                        <hi rend=""bold"">52</hi>(11). New York, NY, USA: ACM: 60–67.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sinclair, S., Rockwell, G. and Others</hi> (2016). Voyant tools. 
                        <hi rend=""italic"">URL: Http://Voyant-Tools. Org/[September 5, 2016]</hi>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Wing, J. M.</hi> (2006). Computational thinking. 
                        <hi rend=""italic"">Commun. ACM</hi>, 
                        <hi rend=""bold"">49</hi>(3): 33–35.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Yang, G. L. and Holmes, M.</hi> (2015). 
                        <hi rend=""italic"">Secret Coders</hi>. New York: Macmillan.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,block-based computing;computational thinking;visual programming,English,"computer science and informatics;digital humanities (history, theory and methodology);english;teaching, pedagogy, and curriculum",2019-01-01,"<text>
        
            <div type=""div1"" rend=""dh-heading1"">
                what is computational thinking in the digital humanities?
                <p style=""text-align:left; "">the question whether digital humanists should learn to code has been highly-contested (ramsay, 2011: 243–45). the ‘hack’ versus ‘yack’ debate has lost its edge as scholars concede that theory and programming praxis can be brought together productively through what davidson terms “collaboration by difference” (davidson, 2015: 134). given that the majority of digital humanists will not need to program professionally, to what extent ought computation be taught in the digital humanities? jeannette m. wing coined the term “computational thinking” to address the teaching of digital literacy beyond computer science. she contended that “computational thinking is a fundamental skill for everyone, not just for computer scientists” (wing, 2006: 33). in 
                    <hi rend=""italic"">digital humanities</hi>, berry and fagerjord comment at length on wing’s definition, concluding that “a critical understanding of computing at its different levels is a prerequisite for a digital humanist...” (berry and fagerjord, 2017: 59). there is little agreement, however, about the best way to teach computational thinking to humanists. we review the potential of visual or “block-based” programming languages for teaching computational literacy in the digital humanities. we argue that digital humanists should learn from these tools’ emphasis on the ludic over the pragmatic. we also offer suggestions about how digital humanists might adapt and critically adopt block-based programming as they seek to expand their understanding of fundamental concepts of computer science.
                </p>
            </div>
            <div xml:id=""review-of-educational-programming-enviro"" type=""div1"" rend=""dh-heading1"">
                review of educational programming environments
                <p style=""text-align:left; "">the use of visual or “block-based” programming has become a mainstay for computer science education in the k-12 arena. block-based programming involves the manipulation of graphical elements to create units of computation. the authors of 
                    <hi rend=""italic"">learnable programming: blocks and beyond</hi> argue that block-based programming makes it easier to learn to program for three primary reasons: emphasizing recognition over recall, “chunking code,” and constraining options (bau et al., 2017: 72–80). these pedagogical advantages would also seem to apply in the digital humanities though, as a quick review of the evolution of these languages demonstrates, they were not created to teach computational thinking to adults.
                </p>
                <list type=""unordered"">
                    <item>
                        <hi rend=""bold"">logo.</hi> the origins of block-based programming stretch back to logo, a programming language and graphical environment for computer science education. while not a visual programming language, when paired with turtle graphics logo provides students with the ability to visualize their computations (papert, 1980: 16–20). logo has gained renewed popularity among the elementary age set due to gene luen yang and mike holmes’ 
                        <hi rend=""italic"">secret coders</hi>, a series of graphic novels that employs logo to teach basic computational literacy (yang and holmes, 2015).
                        <lb></lb>
                    </item>
                    <item>
                        <hi rend=""bold"">scratch.</hi> the designers of scratch sought to create a computational environment for kids and teens to become active manipulators rather than passive consumers of digital media. the designers stripped away many of the complexities of software development (e.g., linking libraries and compiling binaries) to create what they term a “tinkerable” environment, pioneering the use of blocks for syntax (maloney et al., 2010: 16:4).
                    </item>
                    <item>
                        <hi rend=""bold"">snap!</hi> while scratch succeeded in developing an extensive community of users in the k-12 arena, its emphasis on semantic simplicity inhibits its usefulness for teaching students computer science at the postsecondary level. the snap! programming environment emerged from a collaboration between brian harvey at berkeley and jens mönig, a software developer currently at sap. drawing on long experience teaching functional programming in scheme, the authors created a semantics with lambda expressions, recursion, and high-order functions using a scratch-like syntax (harvey and mönig, 2015: 35–38).
                    </item>
                    <item>
                        <hi rend=""bold"">netsblox</hi>. netsblox is an adaptation of snap! that makes it straightforward for users to communicate with internet services and to communicate peer-to-peer (broll et al., 2017: 81–86). students can draw on these features of netsblox, for example, to place markers representing art museums in the vicinity on a google map or to create a shared digital whiteboard. by fostering the ability to communicate beyond the boundaries of the programmer’s laptop, netsblox paves the way for creating data-driven digital humanities projects. data may also be persisted in the cloud, making it possible to preserve state. currently, netsblox comes with the ability to call out to a select number of services. however, the developers envision “adding a lot of new services and data sources to netsblox...”(broll et al., 2017: 86). this raises the question whether a version of netsblox could be developed specifically for digital humanists, integrating web-based application programming interfaces (or apis) for platforms like the dpla, the hathitrust, and europeana, among others.
                    </item>
                </list>
                <figure>
                    <graphic n=""1001"" width=""15.980833333333333cm"" height=""8.989216666666668cm"" url=""pictures/4e4065d832a2206acf1bc0a4c40bd8c4.png"" rend=""inline""></graphic>
                </figure>
                <p>netsblox with prototype rpc block for wikidata</p>
                <p style=""text-align:left; "">the digital humanities community also embraces visual programming models. voyant tools, for instance, provides a graphical interface for scholars seeking to study textual corpora (sinclair et al., 2016). to date, there appears to be little to no scholarship about the pedagogical effectiveness of using visual programming environments in the digital humanities.</p>
            </div>
            <div xml:id=""programming-as-ludic-rather-than-pragmat"" type=""div1"" rend=""dh-heading1"">
                programming as ludic rather than pragmatic
                <p style=""text-align:left; "">is learning block-based programming a means to an end or an end in itself? while computer science students will inevitably move from block-based to text-based programming (kölling et al., 2015: 29–38), the designers of scratch claim that many students will fruitfully remain within its environment (resnick et al., 2009: 66f). at the secondary and post-secondary level, “the beauty and joy of computing” curriculum likewise promotes the enjoyment of programming within the snap! environment: “having fun is an explicit course goal” (garcia et al., 2015: 71). leading digital humanists also acknowledge the playful aspects of programing. in “on building,” stephen ramsay remarks, “learn to code because it’s fun and because it will change the way you look at the world” (ramsay, 2011: 245). nick montfort argues that motivations for learning programming go beyond the “merely instrumental” (montfort, 2016: 268), remarking “it is enjoyable to write computer programs and to use them to create and discover” (montfort, 2016: 277). by customizing the visual representations and selecting domain-specific exercises, block-based programming could find wide application in the digital humanities, promoting the joy of learning computation for its own sake while providing humanists with a better conceptual grounding for the evaluation and application of algorithms and software in their digital research.</p>
            </div>
            <div xml:id=""prolegomena-to-any-future-visual-program"" type=""div1"" rend=""dh-heading1"">
                prolegomena to any future visual programming environment for the digital humanities
                <p style=""text-align:left; "">what would a digital humanities version of a block-based programing environment look like? by way of conclusion, we suggest how netsblox might evolve past its origins in scratch to provide a shared platform for teaching computational thinking in the digital humanities. we propose three developments: 1. creating default sprites that represent the domains of digital humanities research (i.e. representing books rather than basketballs); 2. establishing libraries of blocks to call commonly-used web-based apis in the digital humanities; 3. providing a curriculum focusing on major research areas in the digital humanities, including distant reading, educational gaming, geospatial analysis, and steganography, among other topics. by developing a block-based environment for the digital humanities, we hope not only to advance computational thinking in our field, but also to provide resources for introducing the digital humanities into secondary and postsecondary courses on computational thinking.</p>
            </div>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    bibliography
                    <bibl>
                        <hi rend=""bold"">bau, d., gray, j., kelleher, c., sheldon, j. and turbak, f.</hi> (2017). learnable programming: blocks and beyond. 
                        <hi rend=""italic"">commun. acm</hi>, 
                        <hi rend=""bold"">60</hi>(6). new york, ny, usa: acm: 72–80.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">berry, d. m. and fagerjord, a.</hi> (2017). 
                        <hi rend=""italic"">digital humanities: knowledge and critique in a digital age</hi>. cambridge: polity.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">broll, b., lédeczi, a., volgyesi, p., sallai, j., maroti, m., carrillo, a., weeden-wright, s. l., vanags, c., swartz, j. d. and lu, m.</hi> (2017). a visual programming environment for learning distributed programming. in, 
                        <hi rend=""italic"">proceedings of the 2017 acm sigcse technical symposium on computer science education</hi>. acm, pp. 81–86.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">davidson, c.</hi> (2015). why yack needs hack (and vice versa): from digital humanities to digital literacy. 
                        <hi rend=""italic"">svensson, p. , goldberg, dt, ed</hi>: 131–45.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">garcia, d., harvey, b. and barnes, t.</hi> (2015). the beauty and joy of computing. 
                        <hi rend=""italic"">acm inroads</hi>, 
                        <hi rend=""bold"">6</hi>(4). new york, ny, usa: acm: 71–79.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">harvey, b. and mönig, j.</hi> (2015). lambda in blocks languages: lessons learned. in, 
                        <hi rend=""italic"">2015 ieee blocks and beyond workshop (blocks and beyond)</hi>. pp. 35–38.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">kölling, m., brown, n. c. c. and altadmri, a.</hi> (2015). frame-based editing: easing the transition from blocks to text-based programming. in, 
                        <hi rend=""italic"">proceedings of the workshop in primary and secondary computing education</hi>. (wipsce ’15). new york, ny, usa: acm, pp. 29–38.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">maloney, j., resnick, m., rusk, n., silverman, b. and eastmond, e.</hi> (2010). the scratch programming language and environment. 
                        <hi rend=""italic"">trans. comput. educ.</hi>, 
                        <hi rend=""bold"">10</hi>(4). new york, ny, usa: acm: 16:1–16:15.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">montfort, n.</hi> (2016). 
                        <hi rend=""italic"">exploratory programming for the arts and humanities</hi>. cambridge: mit press.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">papert, s.</hi> (1980). 
                        <hi rend=""italic"">mindstorms: children, computers, and powerful ideas</hi>. new york, ny, usa: basic books, inc.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">ramsay, s.</hi> (2011). on building. in melissa terras, julianne nyhan, (ed), 
                        <hi rend=""italic"">defining digital humanities: a reader</hi>. london: routledge, pp. 243–45.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">resnick, m., maloney, j., monroy-hernández, a., rusk, n., eastmond, e., brennan, k., millner, a., et al.</hi> (2009). scratch: programming for all. 
                        <hi rend=""italic"">commun. acm</hi>, 
                        <hi rend=""bold"">52</hi>(11). new york, ny, usa: acm: 60–67.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">sinclair, s., rockwell, g. and others</hi> (2016). voyant tools. 
                        <hi rend=""italic"">url: http://voyant-tools. org/[september 5, 2016]</hi>.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">wing, j. m.</hi> (2006). computational thinking. 
                        <hi rend=""italic"">commun. acm</hi>, 
                        <hi rend=""bold"">49</hi>(3): 33–35.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">yang, g. l. and holmes, m.</hi> (2015). 
                        <hi rend=""italic"">secret coders</hi>. new york: macmillan.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",3.0,3.0,Voyant
9773,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,"The Complexities Of The Representation Of Xhosa Protagonists, Represented By Male And Female Authors In IsiXhosa Dramas Using Computational Methodologies",,Andiswa Bukula;Juan Steyn,"paper, specified ""short paper""","<text>
        
            <p>IsiXhosa is an Nguni language classified in the south-eastern geographical zone of South Africa (Guthrie, 1971:33). It is one of the official South African languages, and one of the most widely spoken (after isiZulu) with approximately eight million mother-tongue speakers. In terms of natural language processing, particularly computational morphology, the Nguni languages including isiXhosa belong to the lesser-studied languages of the world and can be classified as under-resourced languages. Nguni languages are characterised by a rich agglutinating morphological structure, based on two principles: the nominal classification system and the concordial agreement system (Bosch &amp; Pretorius, 2008:97).</p>
            <p rend=""List Paragraph"">The principal author’s masters study focused on the representation of women protagonists by male and female authors in isiXhosa dramas. The whole analysis process was done manually, mainly because digitised isiXhosa literature books were not available. This limited the study to only four books. The analysis focused on gender inequality in the way women are represented by male authors, as opposed to the way in which women authors represent women protagonists, and also on patriarchal traces found in isiXhosa dramas.</p>
            <p rend=""List Paragraph"">
                <hi style=""font-family:Calisto MT;font-size:12pt"">When examining the representation of female protagonists in isiXhosa dramas, similar works from other scholars are noteworthy. The first contribution that narrates the same viewpoint as the one investigated here is by Ngqase (2002), in which she examines the representations of women in four isiXhosa drama books. The study highlights the interplay between culture and women's social space. The second contribution by Peter (2010) expresses female character portrayal in various drama works written by males. He concludes that many male writers are unwilling to portray female characters in their totality and true complexity, which is evident in the way some writers have resorted to the use of stereotypes (Peter, 2010:15).</hi>
            </p>
            <p>As an isiXhosa language researcher at the South African Centre for Digital Language Resources (SADiLaR), the principal author has been introduced to computational methods which could afford new ways to approach the research topic described. </p>
            <p>Assessing and reporting on the usability of computational tools when analysing isiXhosa texts </p>
            <p>
                <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve"">This presentation reports on the same research topic, with the focus on computational methods to analyse the texts instead of manual approaches. The computational tools which were utilised include, Voyant Tools and regular expressions (regular expressions) as well as testing the feasibility of BookNLP when used conjunctively with written languages. </hi>
            </p>
            <p>
                <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve"">The creators of </hi>
                <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"">Voyant Tools</hi>
                <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> note that it supports analysis in any language since it mostly operates on character sequences; however, limited language-specific support is available (Sinclair &amp; Rockwell, 2019). Capitalisation in isiXhosa is of special importance in the proposed study as the language follows a pattern where the second letter of a word is capitalised instead of the first. For instance, the “Context” tool in Voyant Tools produces search terms only in lower case.</hi>
            </p>
            <p>
                <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve"">With </hi>
                <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"">BookNLP</hi>
                <hi style=""font-family:Calisto MT;font-size:12pt"">, which is specifically built for English texts, the authors will now focus on how successfully the sub-processes in its pipeline fare with a non-Western language and how it could be adapted and/or how a similar pipeline could be developed for isiXhosa using tools developed by SADiLaR. BookNLP was developed by Bamman, Underwood and Smith (2014). The study follows similar approaches to those utilised by Algee-Hewitt, Porter and Walser (2016).</hi>
            </p>
            <p>
                <hi style=""font-family:Calisto MT;font-size:12pt"">Finally,</hi>
                <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> regular expressions</hi>
                <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> will be used as well, as it allows to match patterns and search for very specific character sequences more effectively.</hi>
            </p>
            <p>Operationalisation of the research questions</p>
            <p>
                <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve"">The study has two parts. First, by reporting on the performance of the computational tools on the isiXhosa drama corpus versus an English equivalent. The specific steps for Voyant Tools and differences when using regular expressions will be provided and compared. Second, in terms of research questions focusing on the representation of Xhosa protagonists by male and female authors, regular expressions was used as the main investigation tool. </hi>
            </p>
            <p>
                <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"">The paper reports on:</hi>
            </p>
            <list type=""ordered"">
                <item>How can computational tools used to analyse Western languages be used for conjunctively written South African languages? </item>
                <item>Are authors of isiXhosa literature influenced or led by their gender when writing? </item>
                <item>
                    <hi rend=""color(212121)"" style=""font-family:Calisto MT;font-size:12pt"">Do authors conceptualise their work with the intention to uplift one gender while diminishing the other?</hi>
                </item>
                <item>How can the gap caused by inequality between sexes be bridged through written literature? </item>
            </list>
            <p>A practical example:</p>
            <p>
                <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve"">If data from an English corpus is analysed using Voyant Tools, the tool would automatically be able to provide word frequencies and links between words. However, with a conjunctive language like isiXhosa, this would only be possible by making use of special search options, because the generic frequency table will be skewed owing to the difference in semantic properties of the words. For example, gender association in isiXhosa depends solely on the prefix. Only through the prefix will one be able to confirm whether a noun is referring to a single person or a group of people. Furthermore, only through contextualisation will one know whether that person is male or female, as isiXhosa prefixes are also unisex. </hi>
            </p>
            <p>
                <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"">E.g.:</hi>
                <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> uPeter </hi>
                <hi rend=""color(943634)"" style=""font-family:Calisto MT;font-size:12pt"">u</hi>
                <hi style=""font-family:Calisto MT;font-size:12pt"">sela amanzi</hi>
            </p>
            <p>Peter (he) is drinking water</p>
            <p>
                <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve"">uSammy </hi>
                <hi rend=""color(943634)"" style=""font-family:Calisto MT;font-size:12pt"">u</hi>
                <hi style=""font-family:Calisto MT;font-size:12pt"">phunga iti.</hi>
            </p>
            <p>Sammy (she) is drinking tea.</p>
            <p>This research also aims to provide a point of departure for new scholars interested in analysing isiXhosa literary works using computational approaches.</p>
            <p>
                <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"">Key words</hi>
                <hi style=""font-family:Calisto MT;font-size:12pt"">: Conjunctive language, Nguni language, computational methodologies, voyant tools, regular expressions, BookNLB</hi>
            </p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"">Algee-Hewitt, M., Porter, J., Walser, H</hi>
                        <hi style=""font-family:Calisto MT;font-size:12pt"">. (2016). Representations Of Race: Mining Identity In American Fiction, 1789-1964. In Digital Humanities 2016: Conference Abstracts. Jagiellonian University &amp; Pedagogical University, Kraków, pp. 111-112.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"">Bamman, D., Underwood, T. and Smith, N. A.</hi>
                        <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> (2014). A Bayesian Mixed Effects Model of Literary Character. </hi>
                        <hi rend=""italic"" style=""font-family:Calisto MT;font-size:12pt"">Proceedings of the 52nd Annual Meeting of the Association for Computation Linguistics.</hi>
                        <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> Baltimore, Maryland, pp. 370-79.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(333333)"" style=""font-family:Calisto MT;font-size:12pt"">Bosch, S., Pretorius, L. and Fleisch, A.</hi>
                        <hi rend=""color(333333)"" style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> (2008). Experimental Bootstrapping of Morphological Analysers for Nguni Languages. </hi>
                        <hi rend=""italic color(333333)"" style=""font-family:Calisto MT;font-size:12pt"">Nordic Journal of African Studies</hi>
                        <hi rend=""color(333333)"" style=""font-family:Calisto MT;font-size:12pt""> 17(2):66-88.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"">Guthrie, M.,</hi>
                        <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> (1969). </hi>
                        <hi rend=""italic"" style=""font-family:Calisto MT;font-size:12pt"">Comparative Bantu, Farnborough</hi>
                        <hi style=""font-family:Calisto MT;font-size:12pt"">: Gregg, vol. 4.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"">Guthrie, M.,</hi>
                        <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> (1970). Contributions from Comparative Bantu studies to the prehistory of Africa.  </hi>
                        <hi rend=""italic"" style=""font-family:Calisto MT;font-size:12pt"">Language and history in Africa.</hi>
                        <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> (Dalby ed.), 1:1-27.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"">Ngqase, F.F.,</hi>
                        <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> (2002). </hi>
                        <hi rend=""italic"" style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve"">The way in which women are portrayed in isiXhosa dramas. </hi>
                        <hi style=""font-family:Calisto MT;font-size:12pt"">B.A Thesis. University of Stellenbosch. Available at: (Accessed: 24 April 2019)</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve"">Peter, Z.W., </hi>
                        <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve"">(2010). </hi>
                        <hi rend=""italic"" style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve"">The depiction of female characters by male writers in selected isiXhosa drama works. </hi>
                        <hi style=""font-family:Calisto MT;font-size:12pt"">BA Thesis. Nelson Mandela Metropolitan University. Available at:</hi>
                        <ref target=""http://hdl.handle.net/10948/1482"">
                            <hi style=""font-family:Calisto MT;font-size:12pt"">http://hdl.handle.net/10948/1482</hi>
                        </ref>
                        <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> (Accessed: 24 April 2019)</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-family:Calisto MT;font-size:12pt"">Sinclair, S. and Rockwell, G.</hi>
                        <hi style=""font-family:Calisto MT;font-size:12pt"" xml:space=""preserve""> (2019) Languages.-</hi>
                        <ref target=""https://voyant-tools.org/docs/%23!/guide/languages%20Date%20of%20access:%2024%20April%202019."">
                            <hi style=""font-family:Calisto MT;font-size:12pt"">https://voyant-tools.org/docs/#!/guide/languages Date of access: 24 April 2019.</hi>
                        </ref>
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,book nlb.;computational methodologies;conjunctive language;nguni language;regular expressions;voyant tools,English,african studies;content analysis;corpus and text analysis;english;gender studies,2019-01-01,"<text>
        
            <p>isixhosa is an nguni language classified in the south-eastern geographical zone of south africa (guthrie, 1971:33). it is one of the official south african languages, and one of the most widely spoken (after isizulu) with approximately eight million mother-tongue speakers. in terms of natural language processing, particularly computational morphology, the nguni languages including isixhosa belong to the lesser-studied languages of the world and can be classified as under-resourced languages. nguni languages are characterised by a rich agglutinating morphological structure, based on two principles: the nominal classification system and the concordial agreement system (bosch &amp; pretorius, 2008:97).</p>
            <p rend=""list paragraph"">the principal author’s masters study focused on the representation of women protagonists by male and female authors in isixhosa dramas. the whole analysis process was done manually, mainly because digitised isixhosa literature books were not available. this limited the study to only four books. the analysis focused on gender inequality in the way women are represented by male authors, as opposed to the way in which women authors represent women protagonists, and also on patriarchal traces found in isixhosa dramas.</p>
            <p rend=""list paragraph"">
                <hi style=""font-family:calisto mt;font-size:12pt"">when examining the representation of female protagonists in isixhosa dramas, similar works from other scholars are noteworthy. the first contribution that narrates the same viewpoint as the one investigated here is by ngqase (2002), in which she examines the representations of women in four isixhosa drama books. the study highlights the interplay between culture and women's social space. the second contribution by peter (2010) expresses female character portrayal in various drama works written by males. he concludes that many male writers are unwilling to portray female characters in their totality and true complexity, which is evident in the way some writers have resorted to the use of stereotypes (peter, 2010:15).</hi>
            </p>
            <p>as an isixhosa language researcher at the south african centre for digital language resources (sadilar), the principal author has been introduced to computational methods which could afford new ways to approach the research topic described. </p>
            <p>assessing and reporting on the usability of computational tools when analysing isixhosa texts </p>
            <p>
                <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve"">this presentation reports on the same research topic, with the focus on computational methods to analyse the texts instead of manual approaches. the computational tools which were utilised include, voyant tools and regular expressions (regular expressions) as well as testing the feasibility of booknlp when used conjunctively with written languages. </hi>
            </p>
            <p>
                <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve"">the creators of </hi>
                <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"">voyant tools</hi>
                <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> note that it supports analysis in any language since it mostly operates on character sequences; however, limited language-specific support is available (sinclair &amp; rockwell, 2019). capitalisation in isixhosa is of special importance in the proposed study as the language follows a pattern where the second letter of a word is capitalised instead of the first. for instance, the “context” tool in voyant tools produces search terms only in lower case.</hi>
            </p>
            <p>
                <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve"">with </hi>
                <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"">booknlp</hi>
                <hi style=""font-family:calisto mt;font-size:12pt"">, which is specifically built for english texts, the authors will now focus on how successfully the sub-processes in its pipeline fare with a non-western language and how it could be adapted and/or how a similar pipeline could be developed for isixhosa using tools developed by sadilar. booknlp was developed by bamman, underwood and smith (2014). the study follows similar approaches to those utilised by algee-hewitt, porter and walser (2016).</hi>
            </p>
            <p>
                <hi style=""font-family:calisto mt;font-size:12pt"">finally,</hi>
                <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> regular expressions</hi>
                <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> will be used as well, as it allows to match patterns and search for very specific character sequences more effectively.</hi>
            </p>
            <p>operationalisation of the research questions</p>
            <p>
                <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve"">the study has two parts. first, by reporting on the performance of the computational tools on the isixhosa drama corpus versus an english equivalent. the specific steps for voyant tools and differences when using regular expressions will be provided and compared. second, in terms of research questions focusing on the representation of xhosa protagonists by male and female authors, regular expressions was used as the main investigation tool. </hi>
            </p>
            <p>
                <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"">the paper reports on:</hi>
            </p>
            <list type=""ordered"">
                <item>how can computational tools used to analyse western languages be used for conjunctively written south african languages? </item>
                <item>are authors of isixhosa literature influenced or led by their gender when writing? </item>
                <item>
                    <hi rend=""color(212121)"" style=""font-family:calisto mt;font-size:12pt"">do authors conceptualise their work with the intention to uplift one gender while diminishing the other?</hi>
                </item>
                <item>how can the gap caused by inequality between sexes be bridged through written literature? </item>
            </list>
            <p>a practical example:</p>
            <p>
                <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve"">if data from an english corpus is analysed using voyant tools, the tool would automatically be able to provide word frequencies and links between words. however, with a conjunctive language like isixhosa, this would only be possible by making use of special search options, because the generic frequency table will be skewed owing to the difference in semantic properties of the words. for example, gender association in isixhosa depends solely on the prefix. only through the prefix will one be able to confirm whether a noun is referring to a single person or a group of people. furthermore, only through contextualisation will one know whether that person is male or female, as isixhosa prefixes are also unisex. </hi>
            </p>
            <p>
                <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"">e.g.:</hi>
                <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> upeter </hi>
                <hi rend=""color(943634)"" style=""font-family:calisto mt;font-size:12pt"">u</hi>
                <hi style=""font-family:calisto mt;font-size:12pt"">sela amanzi</hi>
            </p>
            <p>peter (he) is drinking water</p>
            <p>
                <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve"">usammy </hi>
                <hi rend=""color(943634)"" style=""font-family:calisto mt;font-size:12pt"">u</hi>
                <hi style=""font-family:calisto mt;font-size:12pt"">phunga iti.</hi>
            </p>
            <p>sammy (she) is drinking tea.</p>
            <p>this research also aims to provide a point of departure for new scholars interested in analysing isixhosa literary works using computational approaches.</p>
            <p>
                <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"">key words</hi>
                <hi style=""font-family:calisto mt;font-size:12pt"">: conjunctive language, nguni language, computational methodologies, voyant tools, regular expressions, booknlb</hi>
            </p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    bibliography
                    <bibl>
                        <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"">algee-hewitt, m., porter, j., walser, h</hi>
                        <hi style=""font-family:calisto mt;font-size:12pt"">. (2016). representations of race: mining identity in american fiction, 1789-1964. in digital humanities 2016: conference abstracts. jagiellonian university &amp; pedagogical university, kraków, pp. 111-112.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"">bamman, d., underwood, t. and smith, n. a.</hi>
                        <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> (2014). a bayesian mixed effects model of literary character. </hi>
                        <hi rend=""italic"" style=""font-family:calisto mt;font-size:12pt"">proceedings of the 52nd annual meeting of the association for computation linguistics.</hi>
                        <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> baltimore, maryland, pp. 370-79.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold color(333333)"" style=""font-family:calisto mt;font-size:12pt"">bosch, s., pretorius, l. and fleisch, a.</hi>
                        <hi rend=""color(333333)"" style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> (2008). experimental bootstrapping of morphological analysers for nguni languages. </hi>
                        <hi rend=""italic color(333333)"" style=""font-family:calisto mt;font-size:12pt"">nordic journal of african studies</hi>
                        <hi rend=""color(333333)"" style=""font-family:calisto mt;font-size:12pt""> 17(2):66-88.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"">guthrie, m.,</hi>
                        <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> (1969). </hi>
                        <hi rend=""italic"" style=""font-family:calisto mt;font-size:12pt"">comparative bantu, farnborough</hi>
                        <hi style=""font-family:calisto mt;font-size:12pt"">: gregg, vol. 4.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"">guthrie, m.,</hi>
                        <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> (1970). contributions from comparative bantu studies to the prehistory of africa.  </hi>
                        <hi rend=""italic"" style=""font-family:calisto mt;font-size:12pt"">language and history in africa.</hi>
                        <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> (dalby ed.), 1:1-27.</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"">ngqase, f.f.,</hi>
                        <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> (2002). </hi>
                        <hi rend=""italic"" style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve"">the way in which women are portrayed in isixhosa dramas. </hi>
                        <hi style=""font-family:calisto mt;font-size:12pt"">b.a thesis. university of stellenbosch. available at: (accessed: 24 april 2019)</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve"">peter, z.w., </hi>
                        <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve"">(2010). </hi>
                        <hi rend=""italic"" style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve"">the depiction of female characters by male writers in selected isixhosa drama works. </hi>
                        <hi style=""font-family:calisto mt;font-size:12pt"">ba thesis. nelson mandela metropolitan university. available at:</hi>
                        <ref target=""http://hdl.handle.net/10948/1482"">
                            <hi style=""font-family:calisto mt;font-size:12pt"">http://hdl.handle.net/10948/1482</hi>
                        </ref>
                        <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> (accessed: 24 april 2019)</hi>
                    </bibl>
                    <bibl>
                        <hi rend=""bold"" style=""font-family:calisto mt;font-size:12pt"">sinclair, s. and rockwell, g.</hi>
                        <hi style=""font-family:calisto mt;font-size:12pt"" xml:space=""preserve""> (2019) languages.-</hi>
                        <ref target=""https://voyant-tools.org/docs/%23!/guide/languages%20date%20of%20access:%2024%20april%202019."">
                            <hi style=""font-family:calisto mt;font-size:12pt"">https://voyant-tools.org/docs/#!/guide/languages date of access: 24 april 2019.</hi>
                        </ref>
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",5.0,8.0,Voyant
9817,2019 - Utrecht,Utrecht,Complexities,2019,ADHO,ADHO,Utrecht University,Utrecht,,Netherlands,http://staticweb.hum.uu.nl/dh2019/dh2019.adho.org/index.html,Digital Documentation of Abandoned Heritage. The Case of Château de Noisy,,Negin Eisazadeh;Barbara Bordalejo,"paper, specified ""short paper""","<text>
        
            <p>Urban exploration or urbex is the exploration of human-made spaces that are generally inaccessible and hidden away from the general public. Recording the visit of these ‘forgotten’ spaces through photography is a main component of this phenomenon which has resulted in a wealth of urban exploration photos and videos of abandoned sites. </p>
            <p>Urbex destinations are located worldwide and include a wide range of abandoned sites. Belgium has been a very popular destination for urban explorers and Château de Noisy, a neo-gothic castle in Belgium dating back to the 19
                <hi rend=""superscript"">th</hi> century was a very famous destination which was demolished in 2017. There is a rich collection of urbex materials on this building which urban explorers have shared through various online platforms, such as personal websites, Facebook, YouTube, and Flickr. The latter has become a significant repository of urban exploration photographs. 
            </p>
            <p>Regardless of the social and political complexities of this phenomenon, urban exploration is intertwined with abandoned historic sites and in recent years the potential of urban exploration for preservation of heritage has been brought to the attention of academia. Considering that urban exploration is becoming increasingly popular, the importance and possible contribution of this activity and its records for research on abandoned heritage sites cannot be neglected.</p>
            <p>This research focuses on the documentation and information management of abandoned heritage sites and looks into the potentials of the rich collection of existing digital urbex resources for their preservation by exploring their content and new means of representation and engagement. The unique value of such iconographic data can be attributed to the fact that normally these abandoned sites are inaccessible to the general public. Hence these photos and videos can shed light on these unknown places, and with the right utilization can not only document and digitally preserve some aspects of the valuable heritage but also can bring public attention to heritage sites that may still be saved from deterioration and revived. </p>
            <p>To explore the potentials of urbex produced materials for heritage preservation, concentrating on the rich collection of urbex data of numerous abandoned sites, this research aims to gain insights into the urbex scene and its evolution. Moreover, focusing on Château de Noisy, considering that the prevalent methods of documentation of a historic site which require physical access and presence are not applicable, it aims to explore the potential of ‘distant documentation’ by investigating the application of existing tools and software to create a new approach for the preservation of abandoned and even demolished heritage sites and their story. To reach these objectives, focusing on Flickr and using the Flickr API service, two Flickr Dataset are collected: One of general photos related to urban exploration on Flickr (from 2000 to 2017) and another which includes the specific photos of Château de Noisy on Flickr. To collect, prepare, visualize, analyse, create and present the data for this study multiple tools and methods are employed: Python Scripting Language (collection and preparation), Tableau Desktop (visualization and analysis), Voyant (textual analysis), ContextCapture (creation/reconstruction) and WebStorm (presentation via the creation of a website). </p>
            <p>Terminology of the urbex Flickr photo titles and visualizing the distribution of the urbex Flickr photos, led to interesting insights into the urbex scene. Furthermore, the collected and downloaded images of Château de Noisy from Flickr offer insights into this abandoned building carrying information on diverse aspects such as its function, materials (and pathology), structure and context over the course of many years. </p>
            <p>Château de Noisy was demolished without being given the chance for detailed documentation through advanced 
                <hi rend=""italic"">in situ</hi> techniques. Using the ContextCapture software and a selection of the images and videos of the castle that were identified through retrieving the images of the Château de Noisy Flickr Dataset, a 3D mesh model of the building and its immediate context is created. This scalable 3D reconstructed model can allow a flexible interactive experience of the site and can be used to curate and create an immersive experience of the exterior of the building and its immediate context while providing additional heritage information. A digital reconstruction of the building and subsequent narration that builds upon this vessel can create an engaging and immersive experience for the public and digitally preserve the ‘fairytale castle’ building that once stood in Celles. The experience of such distant documentation of Château de Noisy can also be implemented in other heritage sites which are demolished or inaccessible. For buildings that still exist, raising awareness of its current state and heritage values can lead to their potential preservation and revival.
            </p>
            <figure>
                <graphic n=""1001"" width=""16.002cm"" height=""10.242902777777777cm"" url=""Pictures/13603f79601b1b6337f1cf700801bf35.jpg"" rend=""inline""></graphic>
            </figure>
            <p style=""text-align:left; "">Figure 1. Image extract from the reconstructed 3D model of Château de Noisy</p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    Bibliography
                    <bibl>
                        <hi rend=""bold"">Arboleda, P.</hi> (2016). Heritage views through urban exploration: The case of ‘Abandoned Berlin’. 
                        <hi rend=""italic"">International Journal of Heritage Studies</hi>, 22(5): 368-381.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Bennett, L.</hi> (2013). Who goes there? Accounting for gender in the urge to explore abandoned military bunkers. 
                        <hi rend=""italic"">Gender, Place &amp; Culture</hi>, 20(5): 630-646.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">De Liedekerke Beaufort, C.</hi> (n.d.). 
                        <hi rend=""italic"">Le Château de Noisy.</hi> [Historic 16 page text on Château de Noisy available at Augustijns Historisch Instit. in Heverlee, Belgium]
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">DeSilvey, C.</hi> (2006). Observed decay: Telling stories with mutable things. 
                        <hi rend=""italic"">Journal of Material Culture</hi>, 11(3): 318-338.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Garrett, B.</hi> (2010). Urban explorers: Quests for myth, mystery, and meaning. 
                        <hi rend=""italic"">Geography Compass</hi>, 4(10): 1448-1461.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Garrett, B.</hi> (2013). 
                        <hi rend=""italic"">Explore Everything: Place-Hacking the City</hi>. London: Verso Books.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Garrett, B.</hi> (2014). Undertaking recreational trespass: urban exploration and infiltration. 
                        <hi rend=""italic"">Transactions of the Institute of British Geographers</hi>. 39(1): 1-13.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Kindynis, T.</hi> (2017). Urban exploration: From subterranea to spectacle. 
                        <hi rend=""italic"">British Journal of Criminology</hi>, 57(4): 982-1001.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Mott, C. and Roberts, S.M.</hi> (2014). Not everyone has (the) balls: Urban exploration and the persistence of masculinist geography. 
                        <hi rend=""italic"">Antipode</hi>, 46(1): 229-245.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Ninjalicious [Chapman, J.]</hi> (2005). 
                        <hi rend=""italic"">Access All Areas: A User’s Guide to the Art of Urban Exploration</hi>. Toronto: Infilpress.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Novel, C., Kervien, R., Graindorge, P. and Poux, F.</hi> (2016). Comparing aerial photogrammetry and 3d laser scanning methods for creating 3d models of complex objects [White paper], https://www.bentley.com/en/products/brands/contextcapture
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Paiva, T.</hi> (2008). 
                        <hi rend=""italic"">Night Vision: The Art of Urban Exploration</hi>. San Francisco: Chronicle Books.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Palombini, A.</hi> (2017). Storytelling and telling history. Towards a grammar of narratives for Cultural Heritage dissemination in the Digital Era. 
                        <hi rend=""italic"">Journal of Cultural Heritage</hi>, 24: 134-139.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Pinder, D.</hi> (2005). Arts of urban exploration. 
                        <hi rend=""italic"">Cultural Geographies</hi>, 12(4): 383-411.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Sansivero, B</hi>. (2015). Belgium's Abandoned Fairytale Castle. 
                        <hi rend=""italic"">Atlas Obscura,</hi> https://www.atlasobscura.com/articles/belgiums-abandoned-fairytale-castle 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Spyrou, E. and Mylonas, P.</hi> (2016). A survey on Flickr multimedia research challenges. 
                        <hi rend=""italic"">Engineering Applications of Artificial Intelligence</hi>, 51(C): 71-91.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">Stones, S.</hi> (2016). The value of heritage: Urban exploration and
                        <lb></lb>the historic environment. 
                        <hi rend=""italic"">The Historic Environment: Policy &amp; Practice</hi>, 7(4): 301-320.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",xml,This text is republished here with permission from the original rights holder.,,abandoned heritage;château de noisy;digital reconstruction;documentation;urbex,English,"3d/4d modeling;archives, repositories, sustainability and preservation;art history and design studies;corpus and text analysis;cultural artifacts digitisation - theory;cultural studies;english;methods and technologies;modeling;simulation",2019-01-01,"<text>
        
            <p>urban exploration or urbex is the exploration of human-made spaces that are generally inaccessible and hidden away from the general public. recording the visit of these ‘forgotten’ spaces through photography is a main component of this phenomenon which has resulted in a wealth of urban exploration photos and videos of abandoned sites. </p>
            <p>urbex destinations are located worldwide and include a wide range of abandoned sites. belgium has been a very popular destination for urban explorers and château de noisy, a neo-gothic castle in belgium dating back to the 19
                <hi rend=""superscript"">th</hi> century was a very famous destination which was demolished in 2017. there is a rich collection of urbex materials on this building which urban explorers have shared through various online platforms, such as personal websites, facebook, youtube, and flickr. the latter has become a significant repository of urban exploration photographs. 
            </p>
            <p>regardless of the social and political complexities of this phenomenon, urban exploration is intertwined with abandoned historic sites and in recent years the potential of urban exploration for preservation of heritage has been brought to the attention of academia. considering that urban exploration is becoming increasingly popular, the importance and possible contribution of this activity and its records for research on abandoned heritage sites cannot be neglected.</p>
            <p>this research focuses on the documentation and information management of abandoned heritage sites and looks into the potentials of the rich collection of existing digital urbex resources for their preservation by exploring their content and new means of representation and engagement. the unique value of such iconographic data can be attributed to the fact that normally these abandoned sites are inaccessible to the general public. hence these photos and videos can shed light on these unknown places, and with the right utilization can not only document and digitally preserve some aspects of the valuable heritage but also can bring public attention to heritage sites that may still be saved from deterioration and revived. </p>
            <p>to explore the potentials of urbex produced materials for heritage preservation, concentrating on the rich collection of urbex data of numerous abandoned sites, this research aims to gain insights into the urbex scene and its evolution. moreover, focusing on château de noisy, considering that the prevalent methods of documentation of a historic site which require physical access and presence are not applicable, it aims to explore the potential of ‘distant documentation’ by investigating the application of existing tools and software to create a new approach for the preservation of abandoned and even demolished heritage sites and their story. to reach these objectives, focusing on flickr and using the flickr api service, two flickr dataset are collected: one of general photos related to urban exploration on flickr (from 2000 to 2017) and another which includes the specific photos of château de noisy on flickr. to collect, prepare, visualize, analyse, create and present the data for this study multiple tools and methods are employed: python scripting language (collection and preparation), tableau desktop (visualization and analysis), voyant (textual analysis), contextcapture (creation/reconstruction) and webstorm (presentation via the creation of a website). </p>
            <p>terminology of the urbex flickr photo titles and visualizing the distribution of the urbex flickr photos, led to interesting insights into the urbex scene. furthermore, the collected and downloaded images of château de noisy from flickr offer insights into this abandoned building carrying information on diverse aspects such as its function, materials (and pathology), structure and context over the course of many years. </p>
            <p>château de noisy was demolished without being given the chance for detailed documentation through advanced 
                <hi rend=""italic"">in situ</hi> techniques. using the contextcapture software and a selection of the images and videos of the castle that were identified through retrieving the images of the château de noisy flickr dataset, a 3d mesh model of the building and its immediate context is created. this scalable 3d reconstructed model can allow a flexible interactive experience of the site and can be used to curate and create an immersive experience of the exterior of the building and its immediate context while providing additional heritage information. a digital reconstruction of the building and subsequent narration that builds upon this vessel can create an engaging and immersive experience for the public and digitally preserve the ‘fairytale castle’ building that once stood in celles. the experience of such distant documentation of château de noisy can also be implemented in other heritage sites which are demolished or inaccessible. for buildings that still exist, raising awareness of its current state and heritage values can lead to their potential preservation and revival.
            </p>
            <figure>
                <graphic n=""1001"" width=""16.002cm"" height=""10.242902777777777cm"" url=""pictures/13603f79601b1b6337f1cf700801bf35.jpg"" rend=""inline""></graphic>
            </figure>
            <p style=""text-align:left; "">figure 1. image extract from the reconstructed 3d model of château de noisy</p>
        
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    bibliography
                    <bibl>
                        <hi rend=""bold"">arboleda, p.</hi> (2016). heritage views through urban exploration: the case of ‘abandoned berlin’. 
                        <hi rend=""italic"">international journal of heritage studies</hi>, 22(5): 368-381.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">bennett, l.</hi> (2013). who goes there? accounting for gender in the urge to explore abandoned military bunkers. 
                        <hi rend=""italic"">gender, place &amp; culture</hi>, 20(5): 630-646.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">de liedekerke beaufort, c.</hi> (n.d.). 
                        <hi rend=""italic"">le château de noisy.</hi> [historic 16 page text on château de noisy available at augustijns historisch instit. in heverlee, belgium]
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">desilvey, c.</hi> (2006). observed decay: telling stories with mutable things. 
                        <hi rend=""italic"">journal of material culture</hi>, 11(3): 318-338.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">garrett, b.</hi> (2010). urban explorers: quests for myth, mystery, and meaning. 
                        <hi rend=""italic"">geography compass</hi>, 4(10): 1448-1461.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">garrett, b.</hi> (2013). 
                        <hi rend=""italic"">explore everything: place-hacking the city</hi>. london: verso books.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">garrett, b.</hi> (2014). undertaking recreational trespass: urban exploration and infiltration. 
                        <hi rend=""italic"">transactions of the institute of british geographers</hi>. 39(1): 1-13.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">kindynis, t.</hi> (2017). urban exploration: from subterranea to spectacle. 
                        <hi rend=""italic"">british journal of criminology</hi>, 57(4): 982-1001.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">mott, c. and roberts, s.m.</hi> (2014). not everyone has (the) balls: urban exploration and the persistence of masculinist geography. 
                        <hi rend=""italic"">antipode</hi>, 46(1): 229-245.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">ninjalicious [chapman, j.]</hi> (2005). 
                        <hi rend=""italic"">access all areas: a user’s guide to the art of urban exploration</hi>. toronto: infilpress.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">novel, c., kervien, r., graindorge, p. and poux, f.</hi> (2016). comparing aerial photogrammetry and 3d laser scanning methods for creating 3d models of complex objects [white paper], https://www.bentley.com/en/products/brands/contextcapture
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">paiva, t.</hi> (2008). 
                        <hi rend=""italic"">night vision: the art of urban exploration</hi>. san francisco: chronicle books.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">palombini, a.</hi> (2017). storytelling and telling history. towards a grammar of narratives for cultural heritage dissemination in the digital era. 
                        <hi rend=""italic"">journal of cultural heritage</hi>, 24: 134-139.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">pinder, d.</hi> (2005). arts of urban exploration. 
                        <hi rend=""italic"">cultural geographies</hi>, 12(4): 383-411.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">sansivero, b</hi>. (2015). belgium's abandoned fairytale castle. 
                        <hi rend=""italic"">atlas obscura,</hi> https://www.atlasobscura.com/articles/belgiums-abandoned-fairytale-castle 
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">spyrou, e. and mylonas, p.</hi> (2016). a survey on flickr multimedia research challenges. 
                        <hi rend=""italic"">engineering applications of artificial intelligence</hi>, 51(c): 71-91.
                    </bibl>
                    <bibl>
                        <hi rend=""bold"">stones, s.</hi> (2016). the value of heritage: urban exploration and
                        <lb></lb>the historic environment. 
                        <hi rend=""italic"">the historic environment: policy &amp; practice</hi>, 7(4): 301-320.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>",1.0,1.0,Voyant
10752,2018 - University of Cologne,University of Cologne,Kritik der digitalen vernunft,2018,DHd,DHd,,Cologne,,Germany,https://dhd2018.uni-koeln.de/,A Reporting Tool for Relational Visualization and Analysis of Character Mentions in Literature,,Florian Barth;Evgeny Kim;Sandra Murr;Roman Klinger,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Introduction and Motivation</head>
            <p>The emergence of computational methods of text processing has created new paradigms of research in literary studies in recent years (Jockers & Underwood, 2016), for instance 
                    <hi rend=""italic"">distant reading</hi> to find patterns and regularities (Moretti, 2005). Network analysis and extraction of information about relations between characters from literary texts is an example for distant reading methods. Such information can not only be helpful for better understanding of character interactions but can also facilitate the comparison of thereof in different texts.
                </p>
            <p>Existing tools of text analysis and network visualization such as Voyant
                    <ref target=""ftn1"" n=""1""/> or Gephi
                    <ref target=""ftn2"" n=""2""/> are either missing modules for character network analysis or require preliminary steps on data preprocessing from the user and therefore are not easy-to-use for some humanities scholars who lack programming skills. Interactive tools in addition often lack features to ensure reproducibility of results.
                </p>
            <p>We present our ongoing effort on closing this gap by developing a literary analysis reporting tool 
                    <hi rend=""italic"">rCAT</hi>
               <hi rend=""italic"">
                  <ref target=""ftn3"" n=""3""/>
               </hi>, whose primary purpose is to provide an easy-to-use, stable, and reusable solution for automatic extraction of relational information from text and to characterize these relationships automatically to provide the user with deeper qualitative insight. We opt for implementation as a web-based reporting tool instead of an interactive tool for two reasons: (1) automatically generated reports in PDF format can serve as a stable foundation for discussion and can be reused in publications and visualizations easily, and (2) the results are clearly connected to the chosen input parameters such that reproducibility of results is ensured.
                </p>
            <p>As a use-case study, we apply 
                    <hi rend=""italic"">rCAT</hi> to Johann Wolfgang von Goethe's epistolary novel 
                    <hi rend=""italic"">Die Leiden des jungen Werthers</hi>. On the basis of this epistolary novel, we show that not only the network can be generated, but also the characteristic triangular relationship of the protagonists is easily identified. The goal is to automatically determine this triad in the original text and in the adaptations that have been published since the publication of 
                    <hi rend=""italic"">Werther</hi> in 1774.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>
               <anchor id=""id__92ac5kpawoaz""/>Previous Work
                </head>
            <p>Previous research on social networks in literary fiction generally fall into one of the two categories: (1) works that explore methods for extracting and formalizing character networks (
                    <hi rend=""italic"">cf</hi>., Elson et al. (2010), Agarwal et al. (2012, 2013), Park et al. (2012)), and (2) works that primarily focus on qualitative implications of network analysis (
                    <hi rend=""italic"">cf</hi>., Rydberg-Cox (2011), Moretti (2011), Nalisnick & Baird (2013), Jayannavar et al. (2015)). It is common to address both tasks at the same time, as in Beveridge & Shan (2016), who introduce a number of formal measures for analyzing the centrality of the characters in 
                    <hi rend=""italic"">Game of Thrones</hi> books, which results in both expected and surprising findings. 
                </p>
            <p>Building on graph theory extensively elaborated in the past fifty years (e.g., Bondy and Murty, 1976 or West, 2001), our work is similar to Beveridge & Shan (2016), in particular, in terms of the weighted degree measure, and to Park et al. (2012), in terms of distance measure for detecting closely related characters in a text.</p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>
               <anchor id=""id__8b2r3yxu7toy""/>Methods
                </head>
            <p>In the following, we explain the different components in 
                    <hi rend=""italic"">rCAT</hi>, which are available for text analysis. After that, we discuss the results based on a use-case study.
                    <anchor id=""id__3h2icj93kpb3""/>
            </p>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Character lists and character identification</head>
               <p>To detect character mentions in the text we use a fundamental named-entity recognition approach based on dictionaries. This approach is suitable for scholars who analyze texts they already know. Consequently, we opt for a transparent and simple character recognition procedure: The user provides a list of character names to be included in the analysis specifying a canonical name form and all variations thereof she would like to take into account (
                         <hi rend=""italic"">e.g.</hi>, “Lotte” is the canonical name and “Lotten”, “Lottens”, “Lottgen”, “Lottchen”, “Charlotten S.”. are its variants).
                     </p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Relation detection and context words</head>
               <p>We define the closeness of relationship between two characters using a 
                    <hi rend=""italic"">distance measure</hi>
                  <hi rend=""italic"">dist</hi>
                  <hi rend=""italic"">X</hi>
                  <hi rend=""italic"">(p,q)</hi>, where 
                    <hi rend=""italic"">p</hi> and 
                    <hi rend=""italic"">q</hi> are the strings corresponding to these characters and 
                    <hi rend=""italic"">X</hi> is the number of tokens between them (Park et al., 2012). In addition, we introduce the 
                    <hi rend=""italic"">context measure</hi>
                  <hi rend=""italic"">cont</hi>
                  <hi rend=""italic"">Y</hi>
                  <hi rend=""italic"">(p,q)</hi>, where 
                    <hi rend=""italic"">p </hi>and 
                    <hi rend=""italic"">q</hi> are the strings corresponding to these characters and 
                    <hi rend=""italic"">Y</hi> is the number of tokens before the character 
                    <hi rend=""italic"">p</hi> and after the character 
                    <hi rend=""italic"">q</hi>. While the former measure allows for detecting those characters that are closely related to each other, the latter one enables a contextual analysis of their relationship.
                </p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Network analysis</head>
               <p>We visualize the network of characters with an undirected graph 
                    <hi rend=""italic"">G=(V,E)</hi>, where 
                    <hi rend=""italic"">V</hi> are the vertices, each vertex corresponding to one character, and each edge 
                    <hi rend=""italic"">E=(V</hi>
                  <hi rend=""italic"">i,</hi>
                  <hi rend=""italic"">,V</hi>
                  <hi rend=""italic"">j</hi>
                  <hi rend=""italic"">)</hi> corresponding to relations between pairs of characters. We output the following measures for each character node: 
                    <hi rend=""italic"">degree</hi>, 
                    <hi rend=""italic"">edge weight</hi>, 
                    <hi rend=""italic"">weighted degree </hi>and 
                    <hi rend=""italic"">density</hi>. The degree is the number of edges occurring with a given vertex. The edge weight, 
                    <hi rend=""italic"">w</hi>
                  <hi rend=""italic"">i,j</hi>
                  <hi rend=""math""> ≥ </hi>0, is defined as the number of interactions between the vertices V
                    <hi rend=""italic"">i</hi> and V
                    <hi rend=""italic"">j</hi>. The weighted degree is the sum of weights of the edges occurring with a vertex 
                    <hi rend=""italic"">i</hi>. Density is the ratio of occurring edges between two vertices and all possible vertex pairs.
                    </p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Word clouds</head>
               <p>Word clouds are an approach to visualize the vocabulary of a text. The size of one word corresponds to its frequency. We use two different kinds of word clouds: For each character in the character list, we show word clouds based on the context of a window size 
                        <hi rend=""italic"">n. </hi>For each pair of characters occurring in the network, we present a word cloud based on the words between them as well as on the words found in the context. Both types of word clouds can be filtered to the specific word fields (words from specific domains) which is helpful in gaining a focused insight into the characters relations.
                    </p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Word Field developments</head>
               <p>We plot the timeline of multiple predefined world fields (specified by word lists) in the text. This feature is helpful in representing how certain fields (
                      <hi rend=""italic"">e.g.</hi>, concepts, emotions) develop throughout the narrative (Kim et al., 2017).
                   </p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Implementation</head>
               <p>The tool was developed using Python v.3.6 and the Flask
                    <ref target=""ftn4"" n=""4""/> web development framework. The tool outputs a single PDF report. The resulting document contains information from the analysis modules described in the previous section. Network graphs included in the report are generated with 
                    <hi rend=""italic"">graphviz</hi>. Additionally, the tool can generate a CSV file that can be used as input to Gephi. 
                    </p>
            </div>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>
               <anchor id=""id__j13murs3z0ou""/>Use-case Demonstration
                </head>
            <p>For a use-case analysis, we apply 
                    <hi rend=""italic"">rCAT</hi> to 
                    <hi rend=""italic"">Die Leiden des jungen Werther</hi> by Johann Wolfgang Goethe with the following parameters: X=8, Y=5, stop words removed (previous work focused on this analysis without rCAT, 
                    <hi rend=""italic"">cf. </hi>Murr, 2017).
                </p>
            <p>In Goethe's epistolary novel, the protagonist Werther describes his unhappy love for Lotte, who is engaged to Albert. The characteristic triangular relationship in the novel arises from this constellation (protagonist - beloved woman - antagonist). With 
                    <hi rend=""italic"">rCAT</hi> we expect to identify and characterize this relationship. Figures 1 and 2 show a sample network analysis output (tables are shown only partly).
                </p>
            <p>The protagonist Werther shows a degree of 21, which is the number of characters with whom he interacts. The closest relationship measured by edge weight (Figure 2) is observed between Werther and Lotte (81 interactions). The antagonist Albert has a low degree of 3. However, his weighted degree is 36 (third highest after Werther and Lotte), which confirms his important role in the triangular relationship.</p>
            <p>
               <figure>
                  <graphic url=""KLINGER_Roman_A_Reporting_Tool_for_Relational_Visualization_-1000000000000311000001DB10CD1460576FED1F.png""/>
                  <head>Illustration 1: Degrees and weighted degrees for most important characters of Goethe’s Werther</head>
               </figure>
            </p>
            <p>
               <figure>
                  <graphic url=""KLINGER_Roman_A_Reporting_Tool_for_Relational_Visualization_-100000000000030D00000197FFF1DE34125AE4EF.png""/>
                  <head>Illustration 2: Edge weights</head>
               </figure>
            </p>
            <p>
               <figure>
                  <graphic url=""KLINGER_Roman_A_Reporting_Tool_for_Relational_Visualization_-10000201000002CE0000026C841F717B924E2366.png""/>
                  <head>Illustration 3: Complete network of Goethe’s Werther</head>
               </figure>
            </p>
            <p>Highlighted in red is the typical triangular relationship in Goethe’s novel, which corresponds to the three highest weighted degrees. In further steps, we will use 
                    <hi rend=""italic"">rCAT</hi> to analyze the adaptations of Goethe's novel with a focus on this triad.
                </p>
            <p>To better characterize the edges, the tool outputs top-
                    <hi rend=""italic"">n</hi> word clouds sorted by edge weight (
                    <hi rend=""italic"">n</hi> is specified by the user) for character pairs and by degree for single characters. Figure 4 and 5 show examples of the word clouds for character pairs filtered to the words from the emotion domain.
                </p>
            <p>
               <figure>
                  <graphic url=""KLINGER_Roman_A_Reporting_Tool_for_Relational_Visualization_-10000201000001AC000000D93E1514FF4941C591.png""/>
                  <head>Illustration 4: Word clouds for Werther-Lotte</head>
               </figure>
               <figure>
                  <graphic url=""KLINGER_Roman_A_Reporting_Tool_for_Relational_Visualization_-10000201000001AB000000CF288645A8166E0D92.png""/>
                  <head>Illustration 5: Werther-Albert</head>
               </figure>
            </p>
            <p>The word clouds enable first conclusions about the relationships of the characters. Werther and Lotte's word cloud characterizes their ambivalent relationship. The key words ""Leidenschaft"" and ""Freude"" reflect Werther's love, whereas the mentions of ""sterben"" and ""Verblendung"" are characteristic of the unrequited love, which leads Werther into his ""disease unto death"". As Werther and Albert’s word cloud reveals, their relationship is dominated by the ""Unruhe"" that Werther feels through his adversary. </p>
            <p>Additionally, the tool plots the development of the narrative (not bound to specific characters) based on the word fields, an example of which is shown on Figure 6. In this case we used words from the emotion domain (with emotion dictionaries by Klinger et al. (2016)).</p>
            <p>
               <figure>
                  <graphic url=""KLINGER_Roman_A_Reporting_Tool_for_Relational_Visualization_-10000000000002C600000315B691CA7597929A2D.png""/>
                  <head>Illustration 6: Word field development for Goethe’s Werther</head>
               </figure>
            </p>
            <p>The word field development can highlight the prevalence of individual emotion domains across the text. The accumulation of the negative emotion words (Wut,Trauer, Furcht) towards the end suggests, for example, that Goethe’s novel has no “happy ending”. The striking rash on “Freude”, however, captures the last happy hours Werther spends with Lotte in the second part of the narration before he kills himself.</p>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>
                  <anchor id=""id__6d0nxh3f2e6m""/>Future Work
                    </head>
               <p>The next version of the tool will include a character-oriented word field development calculated and plotted for the main characters of the stories. In addition, future releases will include more analysis features and bulk file processing.</p>
            </div>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note id=""ftn1"" n=""1"" place=""foot"">
               <ptr target=""https://voyant-tools.org/""/>
            </note>
            <note id=""ftn2"" n=""2"" place=""foot"">
               <ptr target=""https://gephi.org/""/>
            </note>
            <note id=""ftn3"" n=""3"" place=""foot"">
               <ref target=""http://www.ims.uni-stuttgart.de/data/rcat"">
                  <hi rend=""underline"">www.ims.uni-stuttgart.de/data/rcat</hi>
               </ref>
            </note>
            <note id=""ftn4"" n=""4"" place=""foot"">
               <ptr target=""http://flask.pocoo.org/""/>
            </note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Agarwal, A. / Corvalan, A. / Jensen, J. / Rambow, O. </hi>(2012): “Social Network Analysis of Alice in Wonderland”, in: CLfL@ NAACL-HLT 88-96.</bibl>
               <bibl>
                  <hi rend=""bold"">Agarwal, A. / Kotalwar, A. / Rambow, O. </hi>(2013): “Automatic Extraction of Social Networks from Literary Text. A Case Study on Alice in Wonderland”, in: IJCNLP 1202-1208.</bibl>
               <bibl>
                  <hi rend=""bold"">Beveridge, A. / Shan, J., </hi>(2016): “Network of thrones”, in: Math Horizons, 23(4): 18-22.</bibl>
               <bibl>
                  <hi rend=""bold"">Bondy, J.A. / Murty, U.S.R. </hi>(1976): Graph theory with applications (Vol. 290). London: Macmillan.</bibl>
               <bibl>
                  <hi rend=""bold"">Burrows, J.F. </hi>(1987): “Word-patterns and story-shapes: The statistical analysis of narrative style”, in: Literary & Linguistic Computing, 2(2): 61-70.</bibl>
               <bibl>
                  <hi rend=""bold"">Elson, D.K. / Dames, N. / McKeown, K.R. </hi>(2010): “Extracting social networks from literary fiction”, in: Proceedings of the 48th annual meeting of the association for computational linguistics 138-147. Association for Computational Linguistics.</bibl>
               <bibl>
                  <hi rend=""bold"">Heuser, R., F. Moretti / E. Steiner </hi>(2016): The Emotions of London. Technical report. Stanford University. Pamphlets of the Stanford Literary Lab.</bibl>
               <bibl>
                  <hi rend=""bold"">Jayannavar, P. / Agarwal, A. / Ju, M. and Rambow, O. </hi>(2015): “Validating Literary Theories Using Automatic Social Network Extraction”, in CLfL@ NAACL-HLT 32-41.</bibl>
               <bibl>
                  <hi rend=""bold"">Jockers, M.L. / Underwood, T. </hi>(2016): “Text‐Mining the Humanities”, in: Schreibman, Susan / Siemens, Ray / Unsworth, John (eds.): A New Companion to Digital Humanities 291-306.</bibl>
               <bibl>
                  <hi rend=""bold"">Kim, E. / Padó, S. / Klinger, R. </hi>(2017): “Investigating the Relationship between Literary Genres and Emotional Plot Development”, in: Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature 17-26.</bibl>
               <bibl>
                  <hi rend=""bold"">Klinger, R. / Sulliya S.S. / Reiter N. </hi>(2016): “Automatic Emotion Detection for Quantitative Literary Studies -- A Case Study on Kafka's ‘Das Schloss’ and ‘Amerika’”, in: Digital Humanities (DH), Conference Abstracts, Kraków, Poland, 2016.</bibl>
               <bibl>
                  <hi rend=""bold"">Michel, J.B. / Shen, Y.K. / Aiden, A.P. / Veres, A. / Gray, M.K. / Pickett, J.P. / Hoiberg, D. / Clancy, D. / Norvig, P. / Orwant, J. / Pinker, S. </hi>(2011): “Quantitative analysis of culture using millions of digitized books”, in: science, 331(6014) 176-182.</bibl>
               <bibl>
                  <hi rend=""bold"">Moretti, F. </hi>(2005): Graphs, maps, trees: abstract models for a literary history. Verso.</bibl>
               <bibl>
                  <hi rend=""bold"">Moretti, F. </hi>(2011). Network theory, plot analysis. Stanford Literary Lab Pamphlet Series 2. Available at: https://litlab.stanford.edu/LiteraryLabPamphlet2.pdf</bibl>
               <bibl>
                  <hi rend=""bold"">Murr, S. / Barth, F. </hi>(2017): Digital Analysis of the Literary Reception of J.W. v. Goethe’s ‘Die Leiden des jungen Werthers’, in: Digital Humanities (DH), Conference Abstracts, Montreal, Canada 2017.</bibl>
               <bibl>
                  <hi rend=""bold"">Nalisnick, E.T. / Baird, H.S. </hi>(2013): “Extracting sentiment networks from Shakespeare's plays”, in: Document Analysis and Recognition (ICDAR), 2013 12th International Conference on IEEE 758-762.</bibl>
               <bibl>
                  <hi rend=""bold"">Park, G.M. / Kim, S.H. / Cho, H.G. </hi>(2013): “Structural analysis on social network constructed from characters in literature texts”, in: Journal of Computers, 8(9): 2442-2447.</bibl>
               <bibl>
                  <hi rend=""bold"">Rydberg-Cox, J., </hi>(2011): “Social networks and the language of greek tragedy”, in: Journal of the Chicago Colloquium on Digital Humanities and Computer Science (Vol. 1, No. 3).</bibl>
               <bibl>
                  <hi rend=""bold"">West, D.B. </hi>(2001): Introduction to graph theory (Vol. 2). Upper Saddle River: Prentice Hall.</bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,network analysis;reporting tool;text mining;visualization,German,beziehungsanalyse;inhaltsanalyse;literatur;netzwerkanalyse;text;visualisierung,2018-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""dh-heading1"">
            <head>introduction and motivation</head>
            <p>the emergence of computational methods of text processing has created new paradigms of research in literary studies in recent years (jockers & underwood, 2016), for instance 
                    <hi rend=""italic"">distant reading</hi> to find patterns and regularities (moretti, 2005). network analysis and extraction of information about relations between characters from literary texts is an example for distant reading methods. such information can not only be helpful for better understanding of character interactions but can also facilitate the comparison of thereof in different texts.
                </p>
            <p>existing tools of text analysis and network visualization such as voyant
                    <ref target=""ftn1"" n=""1""/> or gephi
                    <ref target=""ftn2"" n=""2""/> are either missing modules for character network analysis or require preliminary steps on data preprocessing from the user and therefore are not easy-to-use for some humanities scholars who lack programming skills. interactive tools in addition often lack features to ensure reproducibility of results.
                </p>
            <p>we present our ongoing effort on closing this gap by developing a literary analysis reporting tool 
                    <hi rend=""italic"">rcat</hi>
               <hi rend=""italic"">
                  <ref target=""ftn3"" n=""3""/>
               </hi>, whose primary purpose is to provide an easy-to-use, stable, and reusable solution for automatic extraction of relational information from text and to characterize these relationships automatically to provide the user with deeper qualitative insight. we opt for implementation as a web-based reporting tool instead of an interactive tool for two reasons: (1) automatically generated reports in pdf format can serve as a stable foundation for discussion and can be reused in publications and visualizations easily, and (2) the results are clearly connected to the chosen input parameters such that reproducibility of results is ensured.
                </p>
            <p>as a use-case study, we apply 
                    <hi rend=""italic"">rcat</hi> to johann wolfgang von goethe's epistolary novel 
                    <hi rend=""italic"">die leiden des jungen werthers</hi>. on the basis of this epistolary novel, we show that not only the network can be generated, but also the characteristic triangular relationship of the protagonists is easily identified. the goal is to automatically determine this triad in the original text and in the adaptations that have been published since the publication of 
                    <hi rend=""italic"">werther</hi> in 1774.
                </p>
         </div>
         <div type=""div1"" rend=""dh-heading1"">
            <head>
               <anchor id=""id__92ac5kpawoaz""/>previous work
                </head>
            <p>previous research on social networks in literary fiction generally fall into one of the two categories: (1) works that explore methods for extracting and formalizing character networks (
                    <hi rend=""italic"">cf</hi>., elson et al. (2010), agarwal et al. (2012, 2013), park et al. (2012)), and (2) works that primarily focus on qualitative implications of network analysis (
                    <hi rend=""italic"">cf</hi>., rydberg-cox (2011), moretti (2011), nalisnick & baird (2013), jayannavar et al. (2015)). it is common to address both tasks at the same time, as in beveridge & shan (2016), who introduce a number of formal measures for analyzing the centrality of the characters in 
                    <hi rend=""italic"">game of thrones</hi> books, which results in both expected and surprising findings. 
                </p>
            <p>building on graph theory extensively elaborated in the past fifty years (e.g., bondy and murty, 1976 or west, 2001), our work is similar to beveridge & shan (2016), in particular, in terms of the weighted degree measure, and to park et al. (2012), in terms of distance measure for detecting closely related characters in a text.</p>
         </div>
         <div type=""div1"" rend=""dh-heading1"">
            <head>
               <anchor id=""id__8b2r3yxu7toy""/>methods
                </head>
            <p>in the following, we explain the different components in 
                    <hi rend=""italic"">rcat</hi>, which are available for text analysis. after that, we discuss the results based on a use-case study.
                    <anchor id=""id__3h2icj93kpb3""/>
            </p>
            <div type=""div2"" rend=""dh-heading2"">
               <head>character lists and character identification</head>
               <p>to detect character mentions in the text we use a fundamental named-entity recognition approach based on dictionaries. this approach is suitable for scholars who analyze texts they already know. consequently, we opt for a transparent and simple character recognition procedure: the user provides a list of character names to be included in the analysis specifying a canonical name form and all variations thereof she would like to take into account (
                         <hi rend=""italic"">e.g.</hi>, “lotte” is the canonical name and “lotten”, “lottens”, “lottgen”, “lottchen”, “charlotten s.”. are its variants).
                     </p>
            </div>
            <div type=""div2"" rend=""dh-heading2"">
               <head>relation detection and context words</head>
               <p>we define the closeness of relationship between two characters using a 
                    <hi rend=""italic"">distance measure</hi>
                  <hi rend=""italic"">dist</hi>
                  <hi rend=""italic"">x</hi>
                  <hi rend=""italic"">(p,q)</hi>, where 
                    <hi rend=""italic"">p</hi> and 
                    <hi rend=""italic"">q</hi> are the strings corresponding to these characters and 
                    <hi rend=""italic"">x</hi> is the number of tokens between them (park et al., 2012). in addition, we introduce the 
                    <hi rend=""italic"">context measure</hi>
                  <hi rend=""italic"">cont</hi>
                  <hi rend=""italic"">y</hi>
                  <hi rend=""italic"">(p,q)</hi>, where 
                    <hi rend=""italic"">p </hi>and 
                    <hi rend=""italic"">q</hi> are the strings corresponding to these characters and 
                    <hi rend=""italic"">y</hi> is the number of tokens before the character 
                    <hi rend=""italic"">p</hi> and after the character 
                    <hi rend=""italic"">q</hi>. while the former measure allows for detecting those characters that are closely related to each other, the latter one enables a contextual analysis of their relationship.
                </p>
            </div>
            <div type=""div2"" rend=""dh-heading2"">
               <head>network analysis</head>
               <p>we visualize the network of characters with an undirected graph 
                    <hi rend=""italic"">g=(v,e)</hi>, where 
                    <hi rend=""italic"">v</hi> are the vertices, each vertex corresponding to one character, and each edge 
                    <hi rend=""italic"">e=(v</hi>
                  <hi rend=""italic"">i,</hi>
                  <hi rend=""italic"">,v</hi>
                  <hi rend=""italic"">j</hi>
                  <hi rend=""italic"">)</hi> corresponding to relations between pairs of characters. we output the following measures for each character node: 
                    <hi rend=""italic"">degree</hi>, 
                    <hi rend=""italic"">edge weight</hi>, 
                    <hi rend=""italic"">weighted degree </hi>and 
                    <hi rend=""italic"">density</hi>. the degree is the number of edges occurring with a given vertex. the edge weight, 
                    <hi rend=""italic"">w</hi>
                  <hi rend=""italic"">i,j</hi>
                  <hi rend=""math""> ≥ </hi>0, is defined as the number of interactions between the vertices v
                    <hi rend=""italic"">i</hi> and v
                    <hi rend=""italic"">j</hi>. the weighted degree is the sum of weights of the edges occurring with a vertex 
                    <hi rend=""italic"">i</hi>. density is the ratio of occurring edges between two vertices and all possible vertex pairs.
                    </p>
            </div>
            <div type=""div2"" rend=""dh-heading2"">
               <head>word clouds</head>
               <p>word clouds are an approach to visualize the vocabulary of a text. the size of one word corresponds to its frequency. we use two different kinds of word clouds: for each character in the character list, we show word clouds based on the context of a window size 
                        <hi rend=""italic"">n. </hi>for each pair of characters occurring in the network, we present a word cloud based on the words between them as well as on the words found in the context. both types of word clouds can be filtered to the specific word fields (words from specific domains) which is helpful in gaining a focused insight into the characters relations.
                    </p>
            </div>
            <div type=""div2"" rend=""dh-heading2"">
               <head>word field developments</head>
               <p>we plot the timeline of multiple predefined world fields (specified by word lists) in the text. this feature is helpful in representing how certain fields (
                      <hi rend=""italic"">e.g.</hi>, concepts, emotions) develop throughout the narrative (kim et al., 2017).
                   </p>
            </div>
            <div type=""div2"" rend=""dh-heading2"">
               <head>implementation</head>
               <p>the tool was developed using python v.3.6 and the flask
                    <ref target=""ftn4"" n=""4""/> web development framework. the tool outputs a single pdf report. the resulting document contains information from the analysis modules described in the previous section. network graphs included in the report are generated with 
                    <hi rend=""italic"">graphviz</hi>. additionally, the tool can generate a csv file that can be used as input to gephi. 
                    </p>
            </div>
         </div>
         <div type=""div1"" rend=""dh-heading1"">
            <head>
               <anchor id=""id__j13murs3z0ou""/>use-case demonstration
                </head>
            <p>for a use-case analysis, we apply 
                    <hi rend=""italic"">rcat</hi> to 
                    <hi rend=""italic"">die leiden des jungen werther</hi> by johann wolfgang goethe with the following parameters: x=8, y=5, stop words removed (previous work focused on this analysis without rcat, 
                    <hi rend=""italic"">cf. </hi>murr, 2017).
                </p>
            <p>in goethe's epistolary novel, the protagonist werther describes his unhappy love for lotte, who is engaged to albert. the characteristic triangular relationship in the novel arises from this constellation (protagonist - beloved woman - antagonist). with 
                    <hi rend=""italic"">rcat</hi> we expect to identify and characterize this relationship. figures 1 and 2 show a sample network analysis output (tables are shown only partly).
                </p>
            <p>the protagonist werther shows a degree of 21, which is the number of characters with whom he interacts. the closest relationship measured by edge weight (figure 2) is observed between werther and lotte (81 interactions). the antagonist albert has a low degree of 3. however, his weighted degree is 36 (third highest after werther and lotte), which confirms his important role in the triangular relationship.</p>
            <p>
               <figure>
                  <graphic url=""klinger_roman_a_reporting_tool_for_relational_visualization_-1000000000000311000001db10cd1460576fed1f.png""/>
                  <head>illustration 1: degrees and weighted degrees for most important characters of goethe’s werther</head>
               </figure>
            </p>
            <p>
               <figure>
                  <graphic url=""klinger_roman_a_reporting_tool_for_relational_visualization_-100000000000030d00000197fff1de34125ae4ef.png""/>
                  <head>illustration 2: edge weights</head>
               </figure>
            </p>
            <p>
               <figure>
                  <graphic url=""klinger_roman_a_reporting_tool_for_relational_visualization_-10000201000002ce0000026c841f717b924e2366.png""/>
                  <head>illustration 3: complete network of goethe’s werther</head>
               </figure>
            </p>
            <p>highlighted in red is the typical triangular relationship in goethe’s novel, which corresponds to the three highest weighted degrees. in further steps, we will use 
                    <hi rend=""italic"">rcat</hi> to analyze the adaptations of goethe's novel with a focus on this triad.
                </p>
            <p>to better characterize the edges, the tool outputs top-
                    <hi rend=""italic"">n</hi> word clouds sorted by edge weight (
                    <hi rend=""italic"">n</hi> is specified by the user) for character pairs and by degree for single characters. figure 4 and 5 show examples of the word clouds for character pairs filtered to the words from the emotion domain.
                </p>
            <p>
               <figure>
                  <graphic url=""klinger_roman_a_reporting_tool_for_relational_visualization_-10000201000001ac000000d93e1514ff4941c591.png""/>
                  <head>illustration 4: word clouds for werther-lotte</head>
               </figure>
               <figure>
                  <graphic url=""klinger_roman_a_reporting_tool_for_relational_visualization_-10000201000001ab000000cf288645a8166e0d92.png""/>
                  <head>illustration 5: werther-albert</head>
               </figure>
            </p>
            <p>the word clouds enable first conclusions about the relationships of the characters. werther and lotte's word cloud characterizes their ambivalent relationship. the key words ""leidenschaft"" and ""freude"" reflect werther's love, whereas the mentions of ""sterben"" and ""verblendung"" are characteristic of the unrequited love, which leads werther into his ""disease unto death"". as werther and albert’s word cloud reveals, their relationship is dominated by the ""unruhe"" that werther feels through his adversary. </p>
            <p>additionally, the tool plots the development of the narrative (not bound to specific characters) based on the word fields, an example of which is shown on figure 6. in this case we used words from the emotion domain (with emotion dictionaries by klinger et al. (2016)).</p>
            <p>
               <figure>
                  <graphic url=""klinger_roman_a_reporting_tool_for_relational_visualization_-10000000000002c600000315b691ca7597929a2d.png""/>
                  <head>illustration 6: word field development for goethe’s werther</head>
               </figure>
            </p>
            <p>the word field development can highlight the prevalence of individual emotion domains across the text. the accumulation of the negative emotion words (wut,trauer, furcht) towards the end suggests, for example, that goethe’s novel has no “happy ending”. the striking rash on “freude”, however, captures the last happy hours werther spends with lotte in the second part of the narration before he kills himself.</p>
            <div type=""div2"" rend=""dh-heading2"">
               <head>
                  <anchor id=""id__6d0nxh3f2e6m""/>future work
                    </head>
               <p>the next version of the tool will include a character-oriented word field development calculated and plotted for the main characters of the stories. in addition, future releases will include more analysis features and bulk file processing.</p>
            </div>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note id=""ftn1"" n=""1"" place=""foot"">
               <ptr target=""https://voyant-tools.org/""/>
            </note>
            <note id=""ftn2"" n=""2"" place=""foot"">
               <ptr target=""https://gephi.org/""/>
            </note>
            <note id=""ftn3"" n=""3"" place=""foot"">
               <ref target=""http://www.ims.uni-stuttgart.de/data/rcat"">
                  <hi rend=""underline"">www.ims.uni-stuttgart.de/data/rcat</hi>
               </ref>
            </note>
            <note id=""ftn4"" n=""4"" place=""foot"">
               <ptr target=""http://flask.pocoo.org/""/>
            </note>
         </div>
         <div type=""bibliogr"">
            <listbibl>
               <head>bibliographie</head>
               <bibl>
                  <hi rend=""bold"">agarwal, a. / corvalan, a. / jensen, j. / rambow, o. </hi>(2012): “social network analysis of alice in wonderland”, in: clfl@ naacl-hlt 88-96.</bibl>
               <bibl>
                  <hi rend=""bold"">agarwal, a. / kotalwar, a. / rambow, o. </hi>(2013): “automatic extraction of social networks from literary text. a case study on alice in wonderland”, in: ijcnlp 1202-1208.</bibl>
               <bibl>
                  <hi rend=""bold"">beveridge, a. / shan, j., </hi>(2016): “network of thrones”, in: math horizons, 23(4): 18-22.</bibl>
               <bibl>
                  <hi rend=""bold"">bondy, j.a. / murty, u.s.r. </hi>(1976): graph theory with applications (vol. 290). london: macmillan.</bibl>
               <bibl>
                  <hi rend=""bold"">burrows, j.f. </hi>(1987): “word-patterns and story-shapes: the statistical analysis of narrative style”, in: literary & linguistic computing, 2(2): 61-70.</bibl>
               <bibl>
                  <hi rend=""bold"">elson, d.k. / dames, n. / mckeown, k.r. </hi>(2010): “extracting social networks from literary fiction”, in: proceedings of the 48th annual meeting of the association for computational linguistics 138-147. association for computational linguistics.</bibl>
               <bibl>
                  <hi rend=""bold"">heuser, r., f. moretti / e. steiner </hi>(2016): the emotions of london. technical report. stanford university. pamphlets of the stanford literary lab.</bibl>
               <bibl>
                  <hi rend=""bold"">jayannavar, p. / agarwal, a. / ju, m. and rambow, o. </hi>(2015): “validating literary theories using automatic social network extraction”, in clfl@ naacl-hlt 32-41.</bibl>
               <bibl>
                  <hi rend=""bold"">jockers, m.l. / underwood, t. </hi>(2016): “text‐mining the humanities”, in: schreibman, susan / siemens, ray / unsworth, john (eds.): a new companion to digital humanities 291-306.</bibl>
               <bibl>
                  <hi rend=""bold"">kim, e. / padó, s. / klinger, r. </hi>(2017): “investigating the relationship between literary genres and emotional plot development”, in: proceedings of the joint sighum workshop on computational linguistics for cultural heritage, social sciences, humanities and literature 17-26.</bibl>
               <bibl>
                  <hi rend=""bold"">klinger, r. / sulliya s.s. / reiter n. </hi>(2016): “automatic emotion detection for quantitative literary studies -- a case study on kafka's ‘das schloss’ and ‘amerika’”, in: digital humanities (dh), conference abstracts, kraków, poland, 2016.</bibl>
               <bibl>
                  <hi rend=""bold"">michel, j.b. / shen, y.k. / aiden, a.p. / veres, a. / gray, m.k. / pickett, j.p. / hoiberg, d. / clancy, d. / norvig, p. / orwant, j. / pinker, s. </hi>(2011): “quantitative analysis of culture using millions of digitized books”, in: science, 331(6014) 176-182.</bibl>
               <bibl>
                  <hi rend=""bold"">moretti, f. </hi>(2005): graphs, maps, trees: abstract models for a literary history. verso.</bibl>
               <bibl>
                  <hi rend=""bold"">moretti, f. </hi>(2011). network theory, plot analysis. stanford literary lab pamphlet series 2. available at: https://litlab.stanford.edu/literarylabpamphlet2.pdf</bibl>
               <bibl>
                  <hi rend=""bold"">murr, s. / barth, f. </hi>(2017): digital analysis of the literary reception of j.w. v. goethe’s ‘die leiden des jungen werthers’, in: digital humanities (dh), conference abstracts, montreal, canada 2017.</bibl>
               <bibl>
                  <hi rend=""bold"">nalisnick, e.t. / baird, h.s. </hi>(2013): “extracting sentiment networks from shakespeare's plays”, in: document analysis and recognition (icdar), 2013 12th international conference on ieee 758-762.</bibl>
               <bibl>
                  <hi rend=""bold"">park, g.m. / kim, s.h. / cho, h.g. </hi>(2013): “structural analysis on social network constructed from characters in literature texts”, in: journal of computers, 8(9): 2442-2447.</bibl>
               <bibl>
                  <hi rend=""bold"">rydberg-cox, j., </hi>(2011): “social networks and the language of greek tragedy”, in: journal of the chicago colloquium on digital humanities and computer science (vol. 1, no. 3).</bibl>
               <bibl>
                  <hi rend=""bold"">west, d.b. </hi>(2001): introduction to graph theory (vol. 2). upper saddle river: prentice hall.</bibl>
            </listbibl>
         </div>
      </back>
   </text>

",1.0,2.0,Voyant
10856,2019 - Johannes Gutenberg University;Goethe University,Johannes Gutenberg University;Goethe University,multimedial & multimodal,2019,DHd,DHd,Johannes Gutenberg-Universität Mainz (Johannes Gutenberg University of Mainz);Johann-Wolfgang-Goethe-Universität Frankfurt am Main (Goethe University of Frankfurt),Frankfurt & Mainz,,Germany,https://dhd2019.org/,Das Latin Text Archive (LTA) – Digitale Historische Semantik von der Projektentwicklung an der Universität zur Institutionalisierung an der Akademie,,Tim Geelhaar,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Einleitung</head>
            <p>Die Arbeit zur digitalen historischen Semantik in Frankfurt am Main erreicht mit der neuen webbasierten Plattform und Datenbank „Latin Text Archive“ 
                (LTA)<ref target=""ftn1"" n=""1""/> im Rahmen der Dateninfrastrukturen der Berlin-Brandenburgischen Akademie der Wissenschaften (BBAW) ein neues Level. Nach über zehn Jahren Entwicklungsarbeit an den Datenbanken „Historical Semantics Corpus Management 
                (HSCM)“<ref target=""ftn2"" n=""2""/>, „Frankfurt Latin Lexicon (FLL)“ sowie an der Webseite 
                „www.comphistsem.org“<ref target=""ftn3"" n=""3""/> ergaben sich drei grundlegende Herausforderungen: (1) Nachhaltigkeit und Verfügbarkeit der geleisteten Arbeit mussten gesichert, (2) Benutzerfreundlichkeit und Funktionalität verbessert sowie (3) Akzeptanz und Verankerung innerhalb der Fachwissenschaft gesteigert werden. Das LTA antwortet auf diese Herausforderungen und führt mit historischen Referenzkorpora neue Arbeitsinstrumente ein. Das LTA ist auch das Ergebnis einer neuen strategischen Partnerschaft, um die bisherige Arbeit aus der stets zeitlich begrenzten Projektentwicklung an der Universität in einen dauerhaften Betrieb an einer Akademie zu überführen. 
                </p>
            <p>Zur Standortbestimmung der Frankfurter Arbeit lässt sich Michael Piotrowskis Definition von 
                    <hi rend=""italic"">digital humanities</hi> (Piotrowski 2018: 2) anwenden: Danach ist sie den 
                    <hi rend=""italic"">applied digital humanities</hi> zuzuorten, da ein konkretes geschichtswissenschaftliches Forschungsziel mit digitalen Techniken verfolgt wird. Der Historiker Bernhard Jussen hat zur Umsetzung seines Postulates einer „kulturellen Semantik“ nach Wegen gesucht, wie computerlinguistische Methoden helfen können, Bedingungen, Mittel und Formen multimedialer Sinnproduktion in vergangenen Gesellschaften zu erforschen (Jussen 2000: 24ff.). Es geht um die kontrollierte Analyse von semantischem Wandel innerhalb der lateinischen Textproduktion in poströmischer Zeit. Ein Anwendungsgebiet ist die politische Geschichte. So lässt sich mittels der computergestützten Semantik danach fragen, welche impliziten politischen Ordnungsmodelle in Texten sichtbar werden, bevor mit der Wiederentdeckung der Politik des Aristoteles der Begriff des Politischen aufkommt? Außerdem kann die Begriffs- und Ideengeschichte Verwendungszusammenhänge von zentralen Vokabeln untersuchen (Geelhaar 2015; Schwandt 2018). Am Ende sollen aber nicht nur Forschungsergebnisse und -methoden, sondern auch Arbeitsinstrumente und Forschungsdaten einem breiten fachwissenschaftlichen Publikum ohne Programmierkenntnissen zur Weiterverwendung bereitstehen. Hierzu hat Bernhard Jussen den Computerlinguisten Alexander Mehler und dessen Text Technology 
                    Laboratory<ref target=""ftn4"" n=""4""/> für eine Kooperation gewonnen, in der die texttechnologische und die geschichtswissenschaftliche Seite ihre jeweiligen Agenden verfolgen und vom interdisziplinären Austausch profitieren. Aus dieser Konstellation ergibt sich, dass die Vorstellung des LTA aus geschichtswissenschaftlicher Perspektive den Fokus nicht auf technische bzw. technologische, sondern auf programmatische und anwenderbezogene Aspekte legt.
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Das Latin Text Archive: Teil des DTA-Markenstamms</head>
            <p>Das LTA ist eine frei zugängliche, webbasierte Plattform zu Korpusaufbau und Korpusanalyse sowie auch eine Datenbank. Technisch baut es auf den am Deutschen Textarchiv (DTA, DFG-gefördert zwischen 2007 und 2016, siehe Geyken et al. 2018) entwickelten Komponenten zur Textpräsentation auf und erweitert somit den Markenstamm des DTA um eine lateinische Textkomponente. Diese umfasst die Textproduktion im lateinischsprachigen Europa von (zunächst) 400 bis 1500. Die versammelten Texte basieren auf kritischen und somit für die Geschichtswissenschaft validen Editionen, soweit sie in Open Access verfügbar sind. Sie werden zum Zweck des Text Mining um den kritischen Apparat 
                gekürzt<ref target=""ftn5"" n=""5""/>, im TEI-P5-Format nach dem für HSCM entwickelten Datenmodell aufbereitet, vollständig lemmatisiert und mit einer aufwändigen Metadaten-Annotierung angereicht, die zugleich die Vernetzung zu anderen digitalen Ressourcen herstellt – nicht zuletzt zu den textgebenden Institutionen 
                selbst.<ref target=""ftn6"" n=""6""/> Hierbei handelt es sich u. a. um die 
                    <hi rend=""italic"">Monumenta Germaniae Historica</hi> (MGH). Dieses wichtige deutsche Editionsunternehmen für mittelalterliche Texte stellt seine Editionen im „openMGH“-Projekt unter Creative-Commons-Lizenz in TEI-konformen Versionen zur 
                    Verfügung.<ref target=""ftn7"" n=""7""/> Doch erst durch die Datenintegration ins LTA können auch reine Anwender vom openMGH-Projekt profitieren, da die Editionen nun erst wortstatistisch vergleichend analysiert werden können. Des Weiteren ist das LTA auf kontrollierte Datenerweiterung durch die gezielte Aufbereitung und Übernahme aus projektexternen Quellen ausgelegt, um in Zukunft ein repräsentatives Korpus historischer, lateinischer Textproduktion analysierbar zu machen. Hierzu können die Daten entweder als Gesamtkorpus und nach freier Auswahl durch den Anwender an Analysemodule weitergereicht werden, von denen die Voyant Tools bereits verfügbar 
                    sind.<ref target=""ftn8"" n=""8""/> Die in den Vorgängerprojekten entwickelten Analysetools werden als weiteres, externes Modul zugänglich gemacht. Dabei handelt es sich um Konkordanz- und Kookkurrenzanalysen sowie die Berechnung semantischer Netzwerke. 
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Zusammenhang zwischen LTA und HSCM</head>
            <p>Das LTA unterscheidet sich in mehrfacher Hinsicht von seinen Vorgängeranwendungen. Die Überführung des Datenbestandes aus HSCM in das LTA als Teil der von der BBAW betreuten Dateninfrastruktur dient dem Zweck der nachhaltigen Verfügbarkeit. Zudem wird das LTA als explizites Parallelangebot zum DTA vom Renommee der BBAW profitieren, die durch eigene Datenprojekte nicht nur eine ausgezeichnete Expertise in den DH vorweisen kann, sondern auch bereits hohe Anerkennung in den Geisteswissenschaften genießt. Zudem gewinnt das LTA durch die Anlehnung an das DTA, da es einen Wiedererkennungseffekt in der Benutzerführung gibt, der das Arbeiten mit dem LTA erleichtert. Die wesentlichen Neuerungen gegenüber HSCM bestehen aber nicht nur in der verbesserten Benutzerführung; wichtiger noch ist die Trennung der Primärdatenaufbereitung von der Datenverwaltung, indem die Daten in HSCM kuratiert und im LTA zur Verfügung gestellt werden. Das Preprocessing neuer Texte wird weiterhin über HSCM als Teil des eHumanities-
                Desktops<ref target=""ftn9"" n=""9""/> laufen und über den eigens vom Text Technology Lab entwickelten TT Lab Tagger (vor der Brück/Mehler 2016; Eger/Gleim/Mehler 2016) für die automatische Lemmatisierung lateinischer Texte, über das dafür nötige morphologische Lexikon („Frankfurt Latin Lexicon“) sowie über Editoren zur kontrollierenden, manuellen Nachlemmatisierung und zur nachträglichen Korrektur des TEI-Codes. Die vollständig bearbeiteten Texte werden anschließend in das LTA überführt, wo es nicht mehr möglich sein wird, in den jeweiligen Source-Code des Textes einzugreifen. Dies erlaubt eine feste Indexierung (auch der Lemmatisierungsinformationen), wodurch die Schnelligkeit bei der Verarbeitung von Suchanfragen bedeutend gesteigert wird. Darüber hinaus ist durch den Datentransfer die Analyse des Materials nicht mehr auf die in HSCM vorhandenen Tools beschränkt, sondern können im Grunde von allen denkbaren Toolkits wie eben den Voyant Tools oder 
                Diacollo<ref target=""ftn10"" n=""10""/> weiterverwendet werden. 
                </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Geschichtswissenschaftliche Referenzkorpora</head>
            <p>Die dritte wesentliche Neuerung sind die unter geschichtswissenschaftlichen Aspekten kontrollierten 
                Referenzkorpora<ref target=""ftn11"" n=""11""/>, um Veränderungen im Sprachgebrauch zeitlich wie genrespezifisch berechnen und visualisieren zu können. Wie die DTA-Referenzkorpora beinhalten diese Referenzkorpora ganze Werke und nicht nur Samples wie linguistische Korpora (z. B. das British National Corpus). Das Clustering von Texten wird nach Vierteljahrhunderten und nicht nach Zehn-Jahres-Schritten wie im DTA 
                geschehen<ref target=""ftn12"" n=""12""/>, weil eine präzisere zeitliche Zuordnung aufgrund fehlender Datierungen und mitunter komplizierten Überlieferungsgeschichten nicht möglich ist. Dieser Arbeitsschritt erforderte zudem eine erneute klassische Quellenkritik zur Chronologie einzelner Texte. Die Textmengen pro Zeiteinheit sollen quantitativ nicht zu sehr voneinander abweichen, was angesichts der teilweise eklatanten Disparität historischer Schriftproduktion eine große Herausforderung darstellt. Außerdem wird, soweit möglich, der Verbreitungsgrad handschriftlicher Überlieferung berücksichtigt, wenngleich das LTA ansonsten an der Idee des Textes als abstrakter Größe aus konzeptionellen Gründen festhalten 
                muss.<ref target=""ftn13"" n=""13""/> Das erste Referenzkorpus besteht aus narrativen Texten, die aus den Scriptores-Reihen der MGH stammen und historiographische wie hagiographische Texte beinhalten. Künftige Korpora werden Briefe bzw. Urkunden und vor allem auch theologische bzw. juristische Traktate umfassen, um somit Sprachgebrauch in verschiedenen Genres vergleichen zu können.
                </p>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note id=""ftn1"" n=""1"" rend=""footnote text"">
               <ref target=""http://lta.bbaw.de"">http://lta.bbaw.de</ref>
            </note>
            <note id=""ftn2"" n=""2"" rend=""footnote text"">
                    Jussen/Mehler/Ernst 2007; Cimino/Geelhaar/Schwandt 2015. HSCM wurde zwischen 2008 und 2014 aus den Mitteln des Gottfried Wilhelm Leibniz-Preises der DFG sowie aus den Mitteln des LOEWE-Schwerpunktes „Digitale Humanities“ finanziert, um im BMBF-Projekt „Computational Historical Semantics“ (2013-2016) weiterentwickelt zu werden.
                </note>
            <note id=""ftn3"" n=""3"" rned=""footnote text"">
                    Eine Präsentation der Frankfurter Projekte für das CCeH 2017 findet sich unter: 
                    <ref target=""http://www.geschichte.uni-frankfurt.de/43013259/geelhaart"">http://www.geschichte.uni-frankfurt.de/43013259/geelhaart</ref> (alle folgenden Links wurden eingesehen am 6.10.2018)
                </note>
            <note id=""ftn4"" n=""4"" rend=""footnote text"">
               <ref target=""https://www.texttechnologylab.org/"">https://www.texttechnologylab.org/</ref>
            </note>
            <note id=""ftn5"" n=""5"" rend=""footnote text"">
                    Kritisch hierzu Fischer 2017: S266.
                </note>
            <note id=""ftn6"" n=""6"" rend=""footnote text"">
                    Es gibt Verlinkungen, soweit möglich, zum Repertorium Fontium Mediae Aevi 
                    <ref target=""http://www.geschichtsquellen.de"">(www.geschichtsquellen.de)</ref>, zu VIAF (Personen und Werke) und zum Katalog der Staatsbibliothek zu Berlin für bibliographische Angaben.
                </note>
            <note id=""ftn7"" n=""7"" rend=""footnote text"">
               <ref target=""http://www.mgh.de/dmgh/openmgh/"">http://www.mgh.de/dmgh/openmgh/</ref>
            </note>
            <note id=""ftn8"" n=""8"" rend=""footnote text"">
               <ref target=""https://voyant-tools.org/"">https://voyant-tools.org/</ref> Zu dessen Anwendung in der Geschichtswissenschaft siehe Schwandt 2018: 125-133.
                </note>
            <note id=""ftn9"" n=""9"" rend=""footnote text"">
                    HSCM ist ein Modul der VRE „eHumanities Desktop 2.2“ 
                    <ref target=""http://www.hudesktop.hucompute.org"">(www.hudesktop.hucompute.org)</ref> des Text Technology Laboratory.
                </note>
            <note id=""ftn10"" n=""10"" rend=""footnote text"">
               <ref target=""https://clarin-d.de/de/kollokationsanalyse-in-diachroner-perspektive"">https://clarin-d.de/de/kollokationsanalyse-in-diachroner-perspektive</ref>
            </note>
            <note id=""ftn11"" n=""11"" rend=""footnote text"">
                    Zu den Schwierigkeiten mit historischen Korpora und von Historikern organisierten Korpora siehe Geelhaar 2015: 11f.
                </note>
            <note id=""ftn12"" n=""12"" rend=""footnote text"">
                    Unser Kooperationspartner IRHT/CNRS verfolgt im Corpus-Building-Project VELUM 
                    <ref target=""http://www.agence-nationale-recherche.fr/Project-ANR-17-CE27-0015"">(http://www.agence-nationale-recherche.fr/Project-ANR-17-CE27-0015)</ref> eine sehr viel gröbere Stratifikation.
                </note>
            <note id=""ftn13"" n=""13"" rend=""footnote text"">
                    Eine Korpusanreicherung mittels digital edierter Handschriften ist technisch in HSCM/LTA realisierbar, würde aber zu Inkonsistenzen im Materialbestand führen. Hierzu auch Fischer 2017: S280.
                </note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Vor der Brück, Tim / Mehler, Alexander (2016)</hi>: <hi rend=""italic"">„TLT-CRF: A Lexicon-supported Morphological Tagger for Latin Based on Conditional Random Fields”</hi>, in: “Proceedings of the 10th International Conference on Language Resources and Evaluation”.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Eger, Steffen/ Gleim, Rüdiger / Mehler, A. (2016)</hi>: <hi rend=""italic"">„Lemmatization and Morphological Tagging in German and Latin: A comparison and a survey of the state-of-the-art”</hi>, in: “Proceedings of the 10th International Conference on Language Resources and Evaluation”.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Cimino, Roberta / Geelhaar, Tim / Schwandt, Silke (2015)</hi>: <hi rend=""italic"">“Digital Approaches to Historical Semantics: new research directions at Frankfurt University”</hi>. In: Storicamente 11. http://storicamente.org/historical_semantics [letzter Zugriff 12.10.2018] 7. DOI: <ref target=""http://dx.doi.org/10.12977/stor594"" space=""preserve"">10.12977/stor594 </ref>
               </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Fischer, Franz (2017)</hi>: <hi rend=""italic"">„Digital Corpora and Scholarly Editions of Latin Texts: Features and Requirements of Textual Critism”</hi>, in: Speculum 92/S1: S266-S287. https://doi.org/10.1086/693823
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Geelhaar, Tim (2015)</hi>: <hi rend=""italic"">“Talking About christianitas at the Time of Innocent III (1198–1216): What Does Word Use Contribute to the History of Concepts?”</hi> in: Contributions to the history of concepts 10/2: 7–28. https://doi.org/10.3167/choc.2015.100202
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Geyken, Alexander / Boenig, Matthias / Haaf, Susanne / Jurish, Bryan / Thomas, Christian / Wiegand, Frank (2018)</hi>: <hi rend=""italic"">„Das Deutsche Textarchiv als Forschungsplattform für historische Daten in CLARIN“</hi>, in: <hi rend=""bold"">Lobin, Henning/ Schneider, Roman / Witt, Andreas (Hgg.)</hi>: <hi rend=""italic"">Digitale Infrastrukturen für die germanistische Forschung</hi> (= Germanistische Sprachwissenschaft um 2020, Bd. 6). Berlin/Boston: De Gruyter, 219–248. https://doi.org/10.1515/9783110538663-011
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Jussen, Bernhard, Mehler, Alexander / Ernst, Alexandra (2007)</hi>: <hi rend=""italic"">„A Corpus Management System for Historical Semantics. Sprache und Datenverarbeitung“</hi>, in: International Journal for Language Data Processing 31/2: 81-87. 
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Jussen, Bernhard (2000)</hi>: <hi rend=""italic"">Der Name der Witwe. Erkundungen zur Semantik der mittelalterlichen Bußkultur</hi>. (VMPIG, Bd. 158). Göttingen.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Piotrowski, Michael (2018)</hi>: <hi rend=""italic"">„Digital Humanities – An explication”</hi>, in: <hi rend=""bold"">Burghardt, Manuel, Müller-Birn, Christian (eds.)</hi>: <hi rend=""italic"">INF-DH 2018 – Workshopband</hi>, 25. Sept. 2018, Berlin <ref target=""https://doi.org/10.18420/infdh2018-07"">https://doi.org/10.18420/infdh2018-07</ref>
               </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Schwandt, Silke (2018)</hi>: <hi rend=""italic"">„Digitale Methoden für die Historische Semantik. Auf den Spuren von Begriffen in digitalen Korpora“</hi>, in: Geschichte und Gesellschaft 44: 107-134. <ref target=""https://doi.org/10.13109/gege.2018.44.1.107"">https://doi.org/10.13109/gege.2018.44.1.107</ref>
               </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Mehler, Alexander / vor der Brück, Tim  / Gleim, Rüdiger / Geelhaar, Tim (2015)</hi>: <hi rend=""italic"">„Towards a Network Model of the Coreness of Texts: An Experiment in Classifying Latin Texts using the TTLab Latin Tagger”</hi>, in Text Mining: From Ontology Learning to Automated text Processing Applications, C. Biemann and A. Mehler, Eds., Berlin/New York: Springer, 2015, pp. 87-112. 
					</bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Mehler, Alexander / Schwandt, Silke / Gleim, Rüdiger / Jussen, Bernhard</hi>: <hi rend=""italic"">„Der eHumanities Desktop als Werkzeug in der historischen Semantik: Funktionsspektrum und Einsatzszenarien”""</hi>, Journal for Language Technology and Computational Linguistics (JLCL), vol. 26, iss. 1, pp. 97-117, 2011.
					</bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,analyse;historische semantik;korpuslinguistik;latein;lemmatisierung,German,annotieren;forschung;organisation;sprache;strukturanalyse;text,2019-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""dh-heading1"">
            <head>einleitung</head>
            <p>die arbeit zur digitalen historischen semantik in frankfurt am main erreicht mit der neuen webbasierten plattform und datenbank „latin text archive“ 
                (lta)<ref target=""ftn1"" n=""1""/> im rahmen der dateninfrastrukturen der berlin-brandenburgischen akademie der wissenschaften (bbaw) ein neues level. nach über zehn jahren entwicklungsarbeit an den datenbanken „historical semantics corpus management 
                (hscm)“<ref target=""ftn2"" n=""2""/>, „frankfurt latin lexicon (fll)“ sowie an der webseite 
                „www.comphistsem.org“<ref target=""ftn3"" n=""3""/> ergaben sich drei grundlegende herausforderungen: (1) nachhaltigkeit und verfügbarkeit der geleisteten arbeit mussten gesichert, (2) benutzerfreundlichkeit und funktionalität verbessert sowie (3) akzeptanz und verankerung innerhalb der fachwissenschaft gesteigert werden. das lta antwortet auf diese herausforderungen und führt mit historischen referenzkorpora neue arbeitsinstrumente ein. das lta ist auch das ergebnis einer neuen strategischen partnerschaft, um die bisherige arbeit aus der stets zeitlich begrenzten projektentwicklung an der universität in einen dauerhaften betrieb an einer akademie zu überführen. 
                </p>
            <p>zur standortbestimmung der frankfurter arbeit lässt sich michael piotrowskis definition von 
                    <hi rend=""italic"">digital humanities</hi> (piotrowski 2018: 2) anwenden: danach ist sie den 
                    <hi rend=""italic"">applied digital humanities</hi> zuzuorten, da ein konkretes geschichtswissenschaftliches forschungsziel mit digitalen techniken verfolgt wird. der historiker bernhard jussen hat zur umsetzung seines postulates einer „kulturellen semantik“ nach wegen gesucht, wie computerlinguistische methoden helfen können, bedingungen, mittel und formen multimedialer sinnproduktion in vergangenen gesellschaften zu erforschen (jussen 2000: 24ff.). es geht um die kontrollierte analyse von semantischem wandel innerhalb der lateinischen textproduktion in poströmischer zeit. ein anwendungsgebiet ist die politische geschichte. so lässt sich mittels der computergestützten semantik danach fragen, welche impliziten politischen ordnungsmodelle in texten sichtbar werden, bevor mit der wiederentdeckung der politik des aristoteles der begriff des politischen aufkommt? außerdem kann die begriffs- und ideengeschichte verwendungszusammenhänge von zentralen vokabeln untersuchen (geelhaar 2015; schwandt 2018). am ende sollen aber nicht nur forschungsergebnisse und -methoden, sondern auch arbeitsinstrumente und forschungsdaten einem breiten fachwissenschaftlichen publikum ohne programmierkenntnissen zur weiterverwendung bereitstehen. hierzu hat bernhard jussen den computerlinguisten alexander mehler und dessen text technology 
                    laboratory<ref target=""ftn4"" n=""4""/> für eine kooperation gewonnen, in der die texttechnologische und die geschichtswissenschaftliche seite ihre jeweiligen agenden verfolgen und vom interdisziplinären austausch profitieren. aus dieser konstellation ergibt sich, dass die vorstellung des lta aus geschichtswissenschaftlicher perspektive den fokus nicht auf technische bzw. technologische, sondern auf programmatische und anwenderbezogene aspekte legt.
                </p>
         </div>
         <div type=""div1"" rend=""dh-heading1"">
            <head>das latin text archive: teil des dta-markenstamms</head>
            <p>das lta ist eine frei zugängliche, webbasierte plattform zu korpusaufbau und korpusanalyse sowie auch eine datenbank. technisch baut es auf den am deutschen textarchiv (dta, dfg-gefördert zwischen 2007 und 2016, siehe geyken et al. 2018) entwickelten komponenten zur textpräsentation auf und erweitert somit den markenstamm des dta um eine lateinische textkomponente. diese umfasst die textproduktion im lateinischsprachigen europa von (zunächst) 400 bis 1500. die versammelten texte basieren auf kritischen und somit für die geschichtswissenschaft validen editionen, soweit sie in open access verfügbar sind. sie werden zum zweck des text mining um den kritischen apparat 
                gekürzt<ref target=""ftn5"" n=""5""/>, im tei-p5-format nach dem für hscm entwickelten datenmodell aufbereitet, vollständig lemmatisiert und mit einer aufwändigen metadaten-annotierung angereicht, die zugleich die vernetzung zu anderen digitalen ressourcen herstellt – nicht zuletzt zu den textgebenden institutionen 
                selbst.<ref target=""ftn6"" n=""6""/> hierbei handelt es sich u. a. um die 
                    <hi rend=""italic"">monumenta germaniae historica</hi> (mgh). dieses wichtige deutsche editionsunternehmen für mittelalterliche texte stellt seine editionen im „openmgh“-projekt unter creative-commons-lizenz in tei-konformen versionen zur 
                    verfügung.<ref target=""ftn7"" n=""7""/> doch erst durch die datenintegration ins lta können auch reine anwender vom openmgh-projekt profitieren, da die editionen nun erst wortstatistisch vergleichend analysiert werden können. des weiteren ist das lta auf kontrollierte datenerweiterung durch die gezielte aufbereitung und übernahme aus projektexternen quellen ausgelegt, um in zukunft ein repräsentatives korpus historischer, lateinischer textproduktion analysierbar zu machen. hierzu können die daten entweder als gesamtkorpus und nach freier auswahl durch den anwender an analysemodule weitergereicht werden, von denen die voyant tools bereits verfügbar 
                    sind.<ref target=""ftn8"" n=""8""/> die in den vorgängerprojekten entwickelten analysetools werden als weiteres, externes modul zugänglich gemacht. dabei handelt es sich um konkordanz- und kookkurrenzanalysen sowie die berechnung semantischer netzwerke. 
                </p>
         </div>
         <div type=""div1"" rend=""dh-heading1"">
            <head>zusammenhang zwischen lta und hscm</head>
            <p>das lta unterscheidet sich in mehrfacher hinsicht von seinen vorgängeranwendungen. die überführung des datenbestandes aus hscm in das lta als teil der von der bbaw betreuten dateninfrastruktur dient dem zweck der nachhaltigen verfügbarkeit. zudem wird das lta als explizites parallelangebot zum dta vom renommee der bbaw profitieren, die durch eigene datenprojekte nicht nur eine ausgezeichnete expertise in den dh vorweisen kann, sondern auch bereits hohe anerkennung in den geisteswissenschaften genießt. zudem gewinnt das lta durch die anlehnung an das dta, da es einen wiedererkennungseffekt in der benutzerführung gibt, der das arbeiten mit dem lta erleichtert. die wesentlichen neuerungen gegenüber hscm bestehen aber nicht nur in der verbesserten benutzerführung; wichtiger noch ist die trennung der primärdatenaufbereitung von der datenverwaltung, indem die daten in hscm kuratiert und im lta zur verfügung gestellt werden. das preprocessing neuer texte wird weiterhin über hscm als teil des ehumanities-
                desktops<ref target=""ftn9"" n=""9""/> laufen und über den eigens vom text technology lab entwickelten tt lab tagger (vor der brück/mehler 2016; eger/gleim/mehler 2016) für die automatische lemmatisierung lateinischer texte, über das dafür nötige morphologische lexikon („frankfurt latin lexicon“) sowie über editoren zur kontrollierenden, manuellen nachlemmatisierung und zur nachträglichen korrektur des tei-codes. die vollständig bearbeiteten texte werden anschließend in das lta überführt, wo es nicht mehr möglich sein wird, in den jeweiligen source-code des textes einzugreifen. dies erlaubt eine feste indexierung (auch der lemmatisierungsinformationen), wodurch die schnelligkeit bei der verarbeitung von suchanfragen bedeutend gesteigert wird. darüber hinaus ist durch den datentransfer die analyse des materials nicht mehr auf die in hscm vorhandenen tools beschränkt, sondern können im grunde von allen denkbaren toolkits wie eben den voyant tools oder 
                diacollo<ref target=""ftn10"" n=""10""/> weiterverwendet werden. 
                </p>
         </div>
         <div type=""div1"" rend=""dh-heading1"">
            <head>geschichtswissenschaftliche referenzkorpora</head>
            <p>die dritte wesentliche neuerung sind die unter geschichtswissenschaftlichen aspekten kontrollierten 
                referenzkorpora<ref target=""ftn11"" n=""11""/>, um veränderungen im sprachgebrauch zeitlich wie genrespezifisch berechnen und visualisieren zu können. wie die dta-referenzkorpora beinhalten diese referenzkorpora ganze werke und nicht nur samples wie linguistische korpora (z. b. das british national corpus). das clustering von texten wird nach vierteljahrhunderten und nicht nach zehn-jahres-schritten wie im dta 
                geschehen<ref target=""ftn12"" n=""12""/>, weil eine präzisere zeitliche zuordnung aufgrund fehlender datierungen und mitunter komplizierten überlieferungsgeschichten nicht möglich ist. dieser arbeitsschritt erforderte zudem eine erneute klassische quellenkritik zur chronologie einzelner texte. die textmengen pro zeiteinheit sollen quantitativ nicht zu sehr voneinander abweichen, was angesichts der teilweise eklatanten disparität historischer schriftproduktion eine große herausforderung darstellt. außerdem wird, soweit möglich, der verbreitungsgrad handschriftlicher überlieferung berücksichtigt, wenngleich das lta ansonsten an der idee des textes als abstrakter größe aus konzeptionellen gründen festhalten 
                muss.<ref target=""ftn13"" n=""13""/> das erste referenzkorpus besteht aus narrativen texten, die aus den scriptores-reihen der mgh stammen und historiographische wie hagiographische texte beinhalten. künftige korpora werden briefe bzw. urkunden und vor allem auch theologische bzw. juristische traktate umfassen, um somit sprachgebrauch in verschiedenen genres vergleichen zu können.
                </p>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note id=""ftn1"" n=""1"" rend=""footnote text"">
               <ref target=""http://lta.bbaw.de"">http://lta.bbaw.de</ref>
            </note>
            <note id=""ftn2"" n=""2"" rend=""footnote text"">
                    jussen/mehler/ernst 2007; cimino/geelhaar/schwandt 2015. hscm wurde zwischen 2008 und 2014 aus den mitteln des gottfried wilhelm leibniz-preises der dfg sowie aus den mitteln des loewe-schwerpunktes „digitale humanities“ finanziert, um im bmbf-projekt „computational historical semantics“ (2013-2016) weiterentwickelt zu werden.
                </note>
            <note id=""ftn3"" n=""3"" rned=""footnote text"">
                    eine präsentation der frankfurter projekte für das cceh 2017 findet sich unter: 
                    <ref target=""http://www.geschichte.uni-frankfurt.de/43013259/geelhaart"">http://www.geschichte.uni-frankfurt.de/43013259/geelhaart</ref> (alle folgenden links wurden eingesehen am 6.10.2018)
                </note>
            <note id=""ftn4"" n=""4"" rend=""footnote text"">
               <ref target=""https://www.texttechnologylab.org/"">https://www.texttechnologylab.org/</ref>
            </note>
            <note id=""ftn5"" n=""5"" rend=""footnote text"">
                    kritisch hierzu fischer 2017: s266.
                </note>
            <note id=""ftn6"" n=""6"" rend=""footnote text"">
                    es gibt verlinkungen, soweit möglich, zum repertorium fontium mediae aevi 
                    <ref target=""http://www.geschichtsquellen.de"">(www.geschichtsquellen.de)</ref>, zu viaf (personen und werke) und zum katalog der staatsbibliothek zu berlin für bibliographische angaben.
                </note>
            <note id=""ftn7"" n=""7"" rend=""footnote text"">
               <ref target=""http://www.mgh.de/dmgh/openmgh/"">http://www.mgh.de/dmgh/openmgh/</ref>
            </note>
            <note id=""ftn8"" n=""8"" rend=""footnote text"">
               <ref target=""https://voyant-tools.org/"">https://voyant-tools.org/</ref> zu dessen anwendung in der geschichtswissenschaft siehe schwandt 2018: 125-133.
                </note>
            <note id=""ftn9"" n=""9"" rend=""footnote text"">
                    hscm ist ein modul der vre „ehumanities desktop 2.2“ 
                    <ref target=""http://www.hudesktop.hucompute.org"">(www.hudesktop.hucompute.org)</ref> des text technology laboratory.
                </note>
            <note id=""ftn10"" n=""10"" rend=""footnote text"">
               <ref target=""https://clarin-d.de/de/kollokationsanalyse-in-diachroner-perspektive"">https://clarin-d.de/de/kollokationsanalyse-in-diachroner-perspektive</ref>
            </note>
            <note id=""ftn11"" n=""11"" rend=""footnote text"">
                    zu den schwierigkeiten mit historischen korpora und von historikern organisierten korpora siehe geelhaar 2015: 11f.
                </note>
            <note id=""ftn12"" n=""12"" rend=""footnote text"">
                    unser kooperationspartner irht/cnrs verfolgt im corpus-building-project velum 
                    <ref target=""http://www.agence-nationale-recherche.fr/project-anr-17-ce27-0015"">(http://www.agence-nationale-recherche.fr/project-anr-17-ce27-0015)</ref> eine sehr viel gröbere stratifikation.
                </note>
            <note id=""ftn13"" n=""13"" rend=""footnote text"">
                    eine korpusanreicherung mittels digital edierter handschriften ist technisch in hscm/lta realisierbar, würde aber zu inkonsistenzen im materialbestand führen. hierzu auch fischer 2017: s280.
                </note>
         </div>
         <div type=""bibliogr"">
            <listbibl>
               <head>bibliographie</head>
               <bibl>
                  <hi rend=""bold"">vor der brück, tim / mehler, alexander (2016)</hi>: <hi rend=""italic"">„tlt-crf: a lexicon-supported morphological tagger for latin based on conditional random fields”</hi>, in: “proceedings of the 10th international conference on language resources and evaluation”.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">eger, steffen/ gleim, rüdiger / mehler, a. (2016)</hi>: <hi rend=""italic"">„lemmatization and morphological tagging in german and latin: a comparison and a survey of the state-of-the-art”</hi>, in: “proceedings of the 10th international conference on language resources and evaluation”.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">cimino, roberta / geelhaar, tim / schwandt, silke (2015)</hi>: <hi rend=""italic"">“digital approaches to historical semantics: new research directions at frankfurt university”</hi>. in: storicamente 11. http://storicamente.org/historical_semantics [letzter zugriff 12.10.2018] 7. doi: <ref target=""http://dx.doi.org/10.12977/stor594"" space=""preserve"">10.12977/stor594 </ref>
               </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">fischer, franz (2017)</hi>: <hi rend=""italic"">„digital corpora and scholarly editions of latin texts: features and requirements of textual critism”</hi>, in: speculum 92/s1: s266-s287. https://doi.org/10.1086/693823
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">geelhaar, tim (2015)</hi>: <hi rend=""italic"">“talking about christianitas at the time of innocent iii (1198–1216): what does word use contribute to the history of concepts?”</hi> in: contributions to the history of concepts 10/2: 7–28. https://doi.org/10.3167/choc.2015.100202
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">geyken, alexander / boenig, matthias / haaf, susanne / jurish, bryan / thomas, christian / wiegand, frank (2018)</hi>: <hi rend=""italic"">„das deutsche textarchiv als forschungsplattform für historische daten in clarin“</hi>, in: <hi rend=""bold"">lobin, henning/ schneider, roman / witt, andreas (hgg.)</hi>: <hi rend=""italic"">digitale infrastrukturen für die germanistische forschung</hi> (= germanistische sprachwissenschaft um 2020, bd. 6). berlin/boston: de gruyter, 219–248. https://doi.org/10.1515/9783110538663-011
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">jussen, bernhard, mehler, alexander / ernst, alexandra (2007)</hi>: <hi rend=""italic"">„a corpus management system for historical semantics. sprache und datenverarbeitung“</hi>, in: international journal for language data processing 31/2: 81-87. 
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">jussen, bernhard (2000)</hi>: <hi rend=""italic"">der name der witwe. erkundungen zur semantik der mittelalterlichen bußkultur</hi>. (vmpig, bd. 158). göttingen.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">piotrowski, michael (2018)</hi>: <hi rend=""italic"">„digital humanities – an explication”</hi>, in: <hi rend=""bold"">burghardt, manuel, müller-birn, christian (eds.)</hi>: <hi rend=""italic"">inf-dh 2018 – workshopband</hi>, 25. sept. 2018, berlin <ref target=""https://doi.org/10.18420/infdh2018-07"">https://doi.org/10.18420/infdh2018-07</ref>
               </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">schwandt, silke (2018)</hi>: <hi rend=""italic"">„digitale methoden für die historische semantik. auf den spuren von begriffen in digitalen korpora“</hi>, in: geschichte und gesellschaft 44: 107-134. <ref target=""https://doi.org/10.13109/gege.2018.44.1.107"">https://doi.org/10.13109/gege.2018.44.1.107</ref>
               </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">mehler, alexander / vor der brück, tim  / gleim, rüdiger / geelhaar, tim (2015)</hi>: <hi rend=""italic"">„towards a network model of the coreness of texts: an experiment in classifying latin texts using the ttlab latin tagger”</hi>, in text mining: from ontology learning to automated text processing applications, c. biemann and a. mehler, eds., berlin/new york: springer, 2015, pp. 87-112. 
					</bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">mehler, alexander / schwandt, silke / gleim, rüdiger / jussen, bernhard</hi>: <hi rend=""italic"">„der ehumanities desktop als werkzeug in der historischen semantik: funktionsspektrum und einsatzszenarien”""</hi>, journal for language technology and computational linguistics (jlcl), vol. 26, iss. 1, pp. 97-117, 2011.
					</bibl>
            </listbibl>
         </div>
      </back>
   </text>

",2.0,4.0,Voyant
10884,2019 - Johannes Gutenberg University;Goethe University,Johannes Gutenberg University;Goethe University,multimedial & multimodal,2019,DHd,DHd,Johannes Gutenberg-Universität Mainz (Johannes Gutenberg University of Mainz);Johann-Wolfgang-Goethe-Universität Frankfurt am Main (Goethe University of Frankfurt),Frankfurt & Mainz,,Germany,https://dhd2019.org/,DHd 2019 Book of Abstracts Hackathon,,Peter Andorfer;Fabian Cremer;Timo Steyer,workshop / tutorial,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Einleitung</head>
            <p>
               <hi style=""font-size:11pt"">”Auf der Konferenz [DHd 2018; d. Verf.] war zu jeder Zeit an jedem Ort genug Kompetenz versammelt, um in einer Kaffeepause eine digitale Publikation der Abstracts zu bauen, mit Inhaltsverzeichnis, Volltext- und Schlagwortsuche und anderen netten Features, die sonst zum Standard jeder Webpräsentation in den DH gehören.” So postuliert Fabian Cremer in seinem Blog-Post “Nun sag, wie hältst Du es mit dem Digitalen Publizieren, Digital Humanities?” (Cremer 2018). Nur, wie Cremer weiter ausführt, niemand habe es gemacht oder irrt der Autor in dieser Aussage, da die Aufgabe nicht so trivial ist, wie auf dem ersten Blick erscheint? Der hier vorgeschlagene Workshop “DHd 2019 Book of Abstracts Hackathon” soll der DHd-Community den Raum bieten, dieser Frage nachzugehen, eine gemeinsame digitale Publikation der Konferenz-Abstracts zu realisieren und so einen Diskussionsimpuls zur Zukunft des Digitalen Publizierens in den Digital Humanities zu geben.  </hi>
            </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Digitales Publizieren und Digital Humanities</head>
            <p>
               <hi style=""font-size:11pt"">Das digitale Publizieren hat sich im Rahmen des Kanonisierungsprozesses zu einem etablierten Bestandteil der Digital Humanities entwickelt. Dies umfasst sowohl methodische Überlegungen (Kohle 2017) wie auch die praktische Umsetzungen (DHd-AG 2016). Längst existieren etablierte digitale Publikationsorgane der Digital Humanities, welche die Vielfalt und Potentiale der digitalen Publikationsformate demonstrieren und als Vorbild der Publikationspraxis und Wissenschaftskommunikation 
                    gelten.</hi>
               <ref target=""ftn1"" n=""1""/>
               <hi style=""font-size:11pt"" space=""preserve""> Diesen Entwicklungen zum Trotz werden in der Breite und Spitze der Digital Humanities Forschung diese Potentiale nicht ausgeschöpft und traditionelle Publikationspraktiken weiterhin gepflegt (Stäcker 2012). Dies gilt für die Ebene der Technologie, so liegt das Book of Abstracts der DHd 2018 als PDF ohne Strukturdaten vor (Vogeler 2018). Aber gleichfalls auch für die offene Zugänglichkeit, so wurde das deutschsprachige Standardwerk zu den DH nicht als Open Access publiziert (Jannidis et al. 2017 vgl. dazu Stäcker 2017). Die Anforderungen an digitale Publikationen sind schon seit längerem formuliert. “Digital publishing is not simply repackaging a book or article as a computer file, although even a searchable pdf has advantages over paper”, bemerkt Borgman in ihrem “Call to Action for the Humanities” und adressiert hier auch dezidiert die Digital Humanities (Borgman 2010: #p16).</hi>
            </p>
            <p>
               <hi style=""font-size:11pt"">Neben den wissenschaftsökonomischen und wissenschaftspolitischen Vorteilen digitalen Publizierens eröffnen digitale Formate neue wissenschaftliche Methoden zur Weiterverarbeitung. Diese Potentiale sind sich insbesondere die mit digitalen Quellen und Daten arbeitenden  Geisteswissenschaftler*innen bewusst und Formulieren entsprechende Ansprüche an die Digitalisierung und Bereitstellung der Untersuchungsgegenstände, wie etwa Volltexte mit standardisierter Strukturierung und Interoperabilität sowie mit Entitäten und komplexen Strukturmerkmalen angereichert 
                    (Klaffki et al. 2018: 19-20).</hi>
               <ref target=""ftn2"" n=""2""/>
               <hi style=""font-size:11pt"" space=""preserve""> Diese Ansprüche müssten in den Geisteswissenschaften, in der die eigenen Texte in Form einer kritischen Rezeption und Iteration Teil der Informationsquellen sind, auch an die eigene Textproduktion gestellt werden. Folgerichtig lautet die Empfehlung der DHd-AG “Digitales Publizieren”, die semantischen Strukturen zu kodieren, die Dokumente maschinenlesbar und prozessierbar zu machen und PDF nicht als primäres Publikationsformat zu verwenden (DHd-AG 2016). Die TEI-basierten Veröffentlichungen der Digital Humanities Community demonstrieren das Potential der Publikationen als Untersuchungsgegenstand des Faches (Sahle/Henny-Krahmer 2018 und Hannesschläger/Andorfer 2018). Allein die Zusammenführung der strukturierten Datenbasis der Texte mit den vorhandenen Technologien und Methoden neuer Publikationsformate steht häufig noch aus. Diese Lücke adressiert das vorliegende Konzept.</hi>
            </p>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Konzeption des Workshops</head>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Ziele</head>
               <p>
                  <hi style=""font-size:11pt"">Der Hackathon liefert einerseits ein Proof-of-Concept für die Implementierung digitaler Technologien in einen Publikationsprozess (der digitalen Geisteswissenschaften) und möchte andererseits ein partizipatives Format zur Unterstützung der DHd-Tagung durch die DHd-Community in kollaborativer Arbeitsform (Hackathon) darstellen. Der Einsatz vorhandener Frameworks aus der Community demonstriert die Leistungsfähigkeit und das vorhandene Potential. Die erarbeiteten Transformationsskripte und Workflows können die Basis für eine Weiterentwicklung und Nachnutzung in institutionellen Publikationsprozessen darstellen. Eine Analyse des Workshops, der Ergebnisse und der damit verbundenen Rezeption liefert die Grundlage für die Formulierung von Empfehlungen zum möglichen zukünftigen Umgang mit den DHd-Abstracts und deren stetig steigende Relevanz als Publikationsformat. Der Hackathon kann dabei sowohl technologische wie methodische Impulse für das Digitale Publizieren in der DHd-Community liefern.</hi>
               </p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Vorbereitungen</head>
               <p>
                  <hi style=""font-size:11pt"">Die Datengrundlage für den Workshop bilden die zur DHd 2019 eingereichten Abstracts im dhc-Format und idealerweise in einer harmonisierten TEI-Version. Die Workshopleiter vertrauen hier auf die wertvolle Redaktionsarbeit der DHd-Konferenzorganisation, wie es die DHd 2018 vorbildlich umgesetzt 
                        hat.</hi>
                  <ref target=""ftn3"" n=""3""/>
                  <hi style=""font-size:11pt"" space=""preserve""> Um die hier angestrebten Verarbeitungsprozesse und Methoden des Digitalen Publizierens umsetzen zu können, müssen die Daten weiter vorbereitet und angereichert werden. Die vorbereitende Prozessierung soll umfassen: </hi>
               </p>
               <list type=""unordered"">
                  <item>Harmonisierung (Sichtung; Vereinheitlichung von Ambiguitäten, Setzen der Mindeststandards)</item>
                  <item>
                     <hi style=""font-size:11pt"">Strukturdatenauszeichnung (Überschriften, Absatznummerierung, Metadaten)</hi>
                  </item>
                  <item>Verknüpfung mit Normdaten (Erkennung und Auszeichnung von zentralen Entitäten wie Personen, Orte, Institutionen, Werke)</item>
                  <item>Rahmenwerke (Generierung von Listen und Registern für Schlagworte, Titel, Namen)</item>
               </list>
               <p>
                  <hi style=""font-size:11pt"">Die Aufbereitung und Anreicherung der Datenbasis wird von den Organisatoren in Zusammenarbeit mit dem Austrian Centre for Digital Humanities (ACDH) im Vorfeld der DHd 2019 vorgenommen. Die Workshopleitung wird dahingehend mit der Konferenzorganisation und dem Programmkomitee eine enge Kooperation anstreben.</hi>
               </p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Themen und Arbeitsgruppen</head>
               <p>
                  <hi style=""font-size:11pt"">Die Entwicklungsphasen des Hackathons finden als Gruppenarbeit statt. Die parallel arbeitenden Gruppen widmen sich verschiedenen Repräsentationen und Verarbeitungsprozessen der gemeinsamen Datenbasis. Die hier skizzierten Themen und Arbeitsgruppen sind als Vorschläge von Seiten der Organisatoren zu verstehen und werden abhängig von den Kompetenzen und Interessen der Teilnehmenden zu Beginn des Workshops adaptiert. Die beiden Arbeitsbereiche “Transformation” und “Präsentation” werden für die Ziele des Workshops als zwingend notwendig erachtet und sind daher gesetzt. Weitere Themen und Arbeitsgruppen können von den Teilnehmenden ausgestaltet werden. Die Organisatoren moderieren die Bildung der Arbeitsgruppen und begleiten beratend die Entwicklungsphasen.</hi>
               </p>
               <div type=""div3"" rend=""DH-Heading3"">
                  <head>AG “Style und Sheet”: Transformation</head>
                  <p>
                     <hi style=""font-size:11pt"">Die Transformation der Ausgangsdaten (XML/TEI) in verschiedene Zielformate bildet die Grundlage für die verschiedenen Nutzungs- und Rezeptionsformen. Als Zielformate der Stylesheets bieten sich u.a. an: HTML, LaTex, PDF, MS-Word, Markdown, JATS. Idealerweise können die Teilnehmenden eigene, bereits genutzte Transformationen in der Gruppe diskutieren, erweitern und optimieren.</hi>
                  </p>
               </div>
               <div type=""div3"" rend=""DH-Heading3"">
                  <head>AG “Web und App”: Präsentation</head>
                  <p>
                     <hi style=""font-size:11pt"">Der Zeitrahmen des Workshops erlaubt keine Neuentwicklung eines Frontends. Für die Realisierung verschiedener Präsentationschichten müssen die Basisdaten in vorhandene Publikationsframeworks integriert werden. Dabei sind generische Ansätze (z.B. eXist Webapp, GitHub pages, eLife Lens Viewer, Jupyter) ebenso möglich wie spezifische oder instiutionelle Lösungen der DH-Community.</hi>
                  </p>
               </div>
               <div type=""div3"" rend=""DH-Heading3"">
                  <head>AG “Maschine und Modell”: Schnittstellen</head>
                  <p>Die Bereitstellung der Daten über standardisierte maschinenlesbare Schnittstellen (API) ist eine grundlegende Repräsentationsform zukünftiger Publikationspraktiken. Idealerweise wird hier prototypisch ein Protokoll oder eine Spezifikation implementiert (z.B. OAI-PMH, FCS, DTS).</p>
               </div>
               <div type=""div3"" rend=""DH-Heading3"">
                  <head>AG “Wolke und Vektor”: Textanalyse</head>
                  <p>
                     <hi style=""font-size:11pt"">Durch Tokenisieren, Lemmatisieren, Vektorisieren werden Wortlisten, Wortfrequenzen, Wortwolken, Topicmodelling realisiert. Die Strukturierungs- und Visualisierungsformen sind die Grundlage für alternative Rezeptionsformen und textinterne Analysen bis zu (korpus)linguistischen Methoden.</hi>
                  </p>
               </div>
               <div type=""div3"" rend=""DH-Heading3"">
                  <head>AG “Beziehung und Geflecht”: Netzwerke</head>
                  <p>
                     <hi style=""font-size:11pt"">Die mit Normdaten angereicherte Datenbasis bietet die Möglichkeit, die Beziehungen zwischen den Entitäten (Personen, Orte, Institutionen, Werke) zu extrahieren, zu visualisieren und zu analysieren. Zu den Anwendungsfällen gehören Netzwerkanalysen und explorative Navigationsformen.</hi>
                  </p>
               </div>
               <div type=""div3"" rend=""DH-Heading3"">
                  <head>AG “Medial und Modular”: Multimedia</head>
                  <p>Die Verwaltung, Adressierung und Einbettung multimedialer Inhalte die sowie Erzeugung und Integration interaktiver Elemente, wie dynamische Visualisierungen, gehören zu den großen Herausforderungen des Digitalen Publizierens. Prototypische Umsetzungen können hierfür Impulse liefern oder Lücken in bestehenden Frameworks aufzeigen.</p>
               </div>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Agenda:</head>
               <p>
                  <hi style=""font-size:11pt"">15’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">Begrüßung, Organisatorisches, Vorstellungsrunde,</hi>
               </p>
               <p>
                  <hi style=""font-size:11pt"">15’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">Vorstellung der Daten und Aufteilung auf Arbeitsgruppen</hi>
               </p>
               <p>
                  <hi style=""font-size:11pt"">60’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">Entwicklungsphase I</hi>
               </p>
               <p>
                  <hi style=""font-size:11pt"">30’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">Pause</hi>
               </p>
               <p>
                  <hi style=""font-size:11pt"">45’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">Entwicklungsphase II</hi>
               </p>
               <p>
                  <hi style=""font-size:11pt"">30’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">Vorstellung der Ergebnisse</hi>
               </p>
               <p>
                  <hi style=""font-size:11pt"">15’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">Abschlussdiskussion</hi>
               </p>
            </div>
         </div>
         <div type=""div1"" rend=""DH-Heading1"">
            <head>Organisation</head>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>
                  <hi style=""font-family:Trebuchet MS;font-size:13pt"">Ergebnissicherung und Outreach</hi>
               </head>
               <p>
                  <hi style=""font-size:11pt"">Die ausgearbeiteten Ergebnisse der Arbeitsgruppen (Daten, Skripte, Anwendungen) werden offen lizensiert und frei zur Verfügung gestellt. Ein Workshopbericht fasst die Ergebnisse übersichtlich als Blogpost zusammen. Gemeinsam mit interessierten Teilnehmenden plant die Workshopleitung eine Ausarbeitung der Ergebnisse als Empfehlung für mögliche Umsetzungen der DHd-Abstracts. Die konkreten Implementierungen sollen während der Konferenz online verfügbar gehalten werden und werden von der Workshopleitung (und freiwillig Teilnehmenden) über soziale Medien bekannt gegeben und zur Diskussion gestellt. Weitere erwünschte Kommunikationskanäle (Website, Email) werden mit der Konferenzorganisation besprochen.</hi>
               </p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Datenmanagement und Infrastruktur</head>
               <p>
                  <hi style=""font-size:11pt"">Die Einrichtung einer dezidierten Organisation auf GitHub.com für den Workshop erlaubt das Management der verschiedenen Entwicklungen und kollaborative Arbeitsformen. Die Dokumentation und das Projektmanagement werden ebenfalls über git umgesetzt (GitHubWiki/  GitHub-Projects/Issues). Die Publikation der Ergebnisse erfolgt über GitHub-Repositories via Zenodo. Grundsätzlich stehen auch nichtkommerzielle Gitlab-Instanzen und das DARIAH-DE-Repository zur Sicherung zur Verfügung. Das ACDH kann für das kurz- und mittelfristige Hosting der im Rahmen des Workshops entwickelten Applikationen und Services die erforderliche Server-Infrastruktur zur Verfügung stellen. Weiter ist die Bereitstellung von mehreren Instanzen der Applikationen Voyant und eXistDB zur Nutzung im Workshop über DARIAH-DE vorgesehen.</hi>
               </p>
            </div>
            <div type=""div2"" rend=""DH-Heading2"">
               <head>Teilnehmer*innen</head>
               <p>
                  <hi style=""font-size:11pt"">Die praktischen Arbeitsphasen der Gruppenarbeit erlauben nur kleine Gruppengrößen. Die Zahl der möglichen Teilnehmer*innen beträgt daher maximal 25 Personen. Für die Workshopteilnahme werden zwar keine spezifischen technischen oder methodischen Grundlagen (jenseits des Umgangs mit den TEI/XML-Basisdaten) vorausgesetzt, jedoch erfordert ein Hackathon von den Teilnehmenden selbständiges Arbeiten und die Anwendung vorhandener Kompetenzen auf den Gegenstandsbereich. Der Workshop bietet den Rahmen und Raum für die gemeinsames Arbeiten und offenen Austausch.</hi>
               </p>
            </div>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note id=""ftn1"" n=""1"" rend=""footnote text"">
                    Dies zeigt sich am Beispiel der Zeitschrift für digitale Geisteswissenschaften (ZfdG), die als Vorbild für die Zeitschrift Medieval and Early Modern Material Culture Online (MEMO) fungierte.
                </note>
            <note id=""ftn2"" n=""2"" rend=""footnote text"">
                    Siehe die Digitalisierungsklassen IV und V für Texte.
                </note>
            <note id=""ftn3"" n=""3"" rend=""footnote text"">
                    Die vielgestaltigen TEI-Kodierungen aus den Einreichungen und Transformationen wurden einheitlich angeglichen, dokumentiert auf: 
                    <ref target=""https://github.com/GVogeler/DHd2018"">
                  <hi rend=""color(1155CC)"">https://github.com/GVogeler/DHd2018</hi>
               </ref>.
                </note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Borgman, Christine L. (2010)</hi>: <hi rend=""italic"">“The Digital Future is Now. A Call to Action for the Humanities”</hi>, in: Digital Humanities Quarterly 3 (4): #p16, <ref target=""http://www.digitalhumanities.org/dhq/vol/3/4/000077/000077.html"">http://www.digitalhumanities.org/dhq/vol/3/4/000077/000077.html</ref> [letzter Zugriff 26. September 2018].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Cremer, Fabian (2018)</hi>: <hi rend=""italic"">“Nun sag, wie hältst Du es mit dem Digitalen Publizieren, Digital Humanities?”</hi>, in: Digitale Redaktion (Blog), 21.03.2018, <ref target=""https://editorial.hypotheses.org/113"">https://editorial.hypotheses.org/113</ref> [letzter Zugriff 26. September 2018].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">DHd-AG ""Digitales Publizieren"" (2016)</hi>: <hi rend=""italic"">“Digitales Publizieren”</hi>, Working Paper, 01.03.2016, <ref target=""http://diglib.hab.de/ejournals/ed000008/startx.htm"">http://diglib.hab.de/ejournals/ed000008/startx.htm</ref> [letzter Zugriff 26. September 2018].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Hannesschläger, Vanessa / Andorfer, Peter (2018)</hi>: <hi rend=""italic"">Menschen gendern? Datenmodellierung zur Erhebung von Geschlechterverteilung am Beispiel der TEI2016 Abstracts App</hi>, DHd 2018, Köln,<ref target=""https://doi.org/10.5281/zenodo.1182576"">https://doi.org/10.5281/zenodo.1182576</ref>
               </bibl>
               <bibl>
                  <hi rend=""bold"">Jannidis, Fotis /  Kohle, Hubertus / Rehbein, Malte (2017)(eds.)</hi>: <hi rend=""italic"">Digital Humanities. Eine Einführung</hi>,  Stuttgart: J. B. Metzler.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Kohle, Hubertus (2017)</hi>: <hi rend=""italic"">“Digitales Publizieren”</hi>, in: <hi rend=""bold"">Jannidis, Fotis et al. (eds.)</hi>: <hi rend=""italic"">Digital Humanities. Eine Einführung</hi>,  Stuttgart: J. B. Metzler 199-205.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Klaffki, Lisa / Schmunk, Stefan / Stäcker Thomas (2018)</hi>: <hi rend=""italic"">“Stand der Kulturgutdigitalisierung in Deutschland. Eine Analyse und Handlungsvorschläge des DARIAH-DE Stakeholdergremiums 'Wissenschaftliche Sammlungen'”</hi>, DARIAH-DE Working Papers 26, Göttingen, URN: <ref target=""http://resolver.sub.uni-goettingen.de/purl/?dariah-2018-1"">urn:nbn:de:gbv:7-dariah-2018-1-3</ref>.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Stäcker, Thomas (2012)</hi>: <hi rend=""italic"">“Wie schreibt man digital humanities?”</hi>, in: DHd-Blog, 19.08.2012, <ref target=""https://dhd-blog.org/?p=673"">https://dhd-blog.org/?p=673</ref> [letzter Zugriff 26. September 2018].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Stäcker, Thomas (2017)</hi>: <hi rend=""italic"">“Digital Humanities : eine Einführung / Fotis Jannidis, Hubertus Kohle, Malte Rehbein (Hg.)”</hi>, in:  O-Bib. Das Offene Bibliotheksjournal 4(3): 142-148, <ref target=""https://doi.org/10.5282/o-bib/2017H3S142-148"">https://doi.org/10.5282/o-bib/2017H3S142-148</ref>
               </bibl>
               <bibl>
                  <hi rend=""bold"">Vogeler, Georg (2018)(ed.)</hi>: <hi rend=""italic"">DHd 2018. Kritik der digitalen Vernunft. Konferenzabstracts</hi>, Köln: Universität zu Köln 2018.
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,abstracts;digitales publizieren;publikationsprozess;tei;transformation,German,software;text;veröffentlichung;webentwicklung,2019-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div type=""div1"" rend=""dh-heading1"">
            <head>einleitung</head>
            <p>
               <hi style=""font-size:11pt"">”auf der konferenz [dhd 2018; d. verf.] war zu jeder zeit an jedem ort genug kompetenz versammelt, um in einer kaffeepause eine digitale publikation der abstracts zu bauen, mit inhaltsverzeichnis, volltext- und schlagwortsuche und anderen netten features, die sonst zum standard jeder webpräsentation in den dh gehören.” so postuliert fabian cremer in seinem blog-post “nun sag, wie hältst du es mit dem digitalen publizieren, digital humanities?” (cremer 2018). nur, wie cremer weiter ausführt, niemand habe es gemacht oder irrt der autor in dieser aussage, da die aufgabe nicht so trivial ist, wie auf dem ersten blick erscheint? der hier vorgeschlagene workshop “dhd 2019 book of abstracts hackathon” soll der dhd-community den raum bieten, dieser frage nachzugehen, eine gemeinsame digitale publikation der konferenz-abstracts zu realisieren und so einen diskussionsimpuls zur zukunft des digitalen publizierens in den digital humanities zu geben.  </hi>
            </p>
         </div>
         <div type=""div1"" rend=""dh-heading1"">
            <head>digitales publizieren und digital humanities</head>
            <p>
               <hi style=""font-size:11pt"">das digitale publizieren hat sich im rahmen des kanonisierungsprozesses zu einem etablierten bestandteil der digital humanities entwickelt. dies umfasst sowohl methodische überlegungen (kohle 2017) wie auch die praktische umsetzungen (dhd-ag 2016). längst existieren etablierte digitale publikationsorgane der digital humanities, welche die vielfalt und potentiale der digitalen publikationsformate demonstrieren und als vorbild der publikationspraxis und wissenschaftskommunikation 
                    gelten.</hi>
               <ref target=""ftn1"" n=""1""/>
               <hi style=""font-size:11pt"" space=""preserve""> diesen entwicklungen zum trotz werden in der breite und spitze der digital humanities forschung diese potentiale nicht ausgeschöpft und traditionelle publikationspraktiken weiterhin gepflegt (stäcker 2012). dies gilt für die ebene der technologie, so liegt das book of abstracts der dhd 2018 als pdf ohne strukturdaten vor (vogeler 2018). aber gleichfalls auch für die offene zugänglichkeit, so wurde das deutschsprachige standardwerk zu den dh nicht als open access publiziert (jannidis et al. 2017 vgl. dazu stäcker 2017). die anforderungen an digitale publikationen sind schon seit längerem formuliert. “digital publishing is not simply repackaging a book or article as a computer file, although even a searchable pdf has advantages over paper”, bemerkt borgman in ihrem “call to action for the humanities” und adressiert hier auch dezidiert die digital humanities (borgman 2010: #p16).</hi>
            </p>
            <p>
               <hi style=""font-size:11pt"">neben den wissenschaftsökonomischen und wissenschaftspolitischen vorteilen digitalen publizierens eröffnen digitale formate neue wissenschaftliche methoden zur weiterverarbeitung. diese potentiale sind sich insbesondere die mit digitalen quellen und daten arbeitenden  geisteswissenschaftler*innen bewusst und formulieren entsprechende ansprüche an die digitalisierung und bereitstellung der untersuchungsgegenstände, wie etwa volltexte mit standardisierter strukturierung und interoperabilität sowie mit entitäten und komplexen strukturmerkmalen angereichert 
                    (klaffki et al. 2018: 19-20).</hi>
               <ref target=""ftn2"" n=""2""/>
               <hi style=""font-size:11pt"" space=""preserve""> diese ansprüche müssten in den geisteswissenschaften, in der die eigenen texte in form einer kritischen rezeption und iteration teil der informationsquellen sind, auch an die eigene textproduktion gestellt werden. folgerichtig lautet die empfehlung der dhd-ag “digitales publizieren”, die semantischen strukturen zu kodieren, die dokumente maschinenlesbar und prozessierbar zu machen und pdf nicht als primäres publikationsformat zu verwenden (dhd-ag 2016). die tei-basierten veröffentlichungen der digital humanities community demonstrieren das potential der publikationen als untersuchungsgegenstand des faches (sahle/henny-krahmer 2018 und hannesschläger/andorfer 2018). allein die zusammenführung der strukturierten datenbasis der texte mit den vorhandenen technologien und methoden neuer publikationsformate steht häufig noch aus. diese lücke adressiert das vorliegende konzept.</hi>
            </p>
         </div>
         <div type=""div1"" rend=""dh-heading1"">
            <head>konzeption des workshops</head>
            <div type=""div2"" rend=""dh-heading2"">
               <head>ziele</head>
               <p>
                  <hi style=""font-size:11pt"">der hackathon liefert einerseits ein proof-of-concept für die implementierung digitaler technologien in einen publikationsprozess (der digitalen geisteswissenschaften) und möchte andererseits ein partizipatives format zur unterstützung der dhd-tagung durch die dhd-community in kollaborativer arbeitsform (hackathon) darstellen. der einsatz vorhandener frameworks aus der community demonstriert die leistungsfähigkeit und das vorhandene potential. die erarbeiteten transformationsskripte und workflows können die basis für eine weiterentwicklung und nachnutzung in institutionellen publikationsprozessen darstellen. eine analyse des workshops, der ergebnisse und der damit verbundenen rezeption liefert die grundlage für die formulierung von empfehlungen zum möglichen zukünftigen umgang mit den dhd-abstracts und deren stetig steigende relevanz als publikationsformat. der hackathon kann dabei sowohl technologische wie methodische impulse für das digitale publizieren in der dhd-community liefern.</hi>
               </p>
            </div>
            <div type=""div2"" rend=""dh-heading2"">
               <head>vorbereitungen</head>
               <p>
                  <hi style=""font-size:11pt"">die datengrundlage für den workshop bilden die zur dhd 2019 eingereichten abstracts im dhc-format und idealerweise in einer harmonisierten tei-version. die workshopleiter vertrauen hier auf die wertvolle redaktionsarbeit der dhd-konferenzorganisation, wie es die dhd 2018 vorbildlich umgesetzt 
                        hat.</hi>
                  <ref target=""ftn3"" n=""3""/>
                  <hi style=""font-size:11pt"" space=""preserve""> um die hier angestrebten verarbeitungsprozesse und methoden des digitalen publizierens umsetzen zu können, müssen die daten weiter vorbereitet und angereichert werden. die vorbereitende prozessierung soll umfassen: </hi>
               </p>
               <list type=""unordered"">
                  <item>harmonisierung (sichtung; vereinheitlichung von ambiguitäten, setzen der mindeststandards)</item>
                  <item>
                     <hi style=""font-size:11pt"">strukturdatenauszeichnung (überschriften, absatznummerierung, metadaten)</hi>
                  </item>
                  <item>verknüpfung mit normdaten (erkennung und auszeichnung von zentralen entitäten wie personen, orte, institutionen, werke)</item>
                  <item>rahmenwerke (generierung von listen und registern für schlagworte, titel, namen)</item>
               </list>
               <p>
                  <hi style=""font-size:11pt"">die aufbereitung und anreicherung der datenbasis wird von den organisatoren in zusammenarbeit mit dem austrian centre for digital humanities (acdh) im vorfeld der dhd 2019 vorgenommen. die workshopleitung wird dahingehend mit der konferenzorganisation und dem programmkomitee eine enge kooperation anstreben.</hi>
               </p>
            </div>
            <div type=""div2"" rend=""dh-heading2"">
               <head>themen und arbeitsgruppen</head>
               <p>
                  <hi style=""font-size:11pt"">die entwicklungsphasen des hackathons finden als gruppenarbeit statt. die parallel arbeitenden gruppen widmen sich verschiedenen repräsentationen und verarbeitungsprozessen der gemeinsamen datenbasis. die hier skizzierten themen und arbeitsgruppen sind als vorschläge von seiten der organisatoren zu verstehen und werden abhängig von den kompetenzen und interessen der teilnehmenden zu beginn des workshops adaptiert. die beiden arbeitsbereiche “transformation” und “präsentation” werden für die ziele des workshops als zwingend notwendig erachtet und sind daher gesetzt. weitere themen und arbeitsgruppen können von den teilnehmenden ausgestaltet werden. die organisatoren moderieren die bildung der arbeitsgruppen und begleiten beratend die entwicklungsphasen.</hi>
               </p>
               <div type=""div3"" rend=""dh-heading3"">
                  <head>ag “style und sheet”: transformation</head>
                  <p>
                     <hi style=""font-size:11pt"">die transformation der ausgangsdaten (xml/tei) in verschiedene zielformate bildet die grundlage für die verschiedenen nutzungs- und rezeptionsformen. als zielformate der stylesheets bieten sich u.a. an: html, latex, pdf, ms-word, markdown, jats. idealerweise können die teilnehmenden eigene, bereits genutzte transformationen in der gruppe diskutieren, erweitern und optimieren.</hi>
                  </p>
               </div>
               <div type=""div3"" rend=""dh-heading3"">
                  <head>ag “web und app”: präsentation</head>
                  <p>
                     <hi style=""font-size:11pt"">der zeitrahmen des workshops erlaubt keine neuentwicklung eines frontends. für die realisierung verschiedener präsentationschichten müssen die basisdaten in vorhandene publikationsframeworks integriert werden. dabei sind generische ansätze (z.b. exist webapp, github pages, elife lens viewer, jupyter) ebenso möglich wie spezifische oder instiutionelle lösungen der dh-community.</hi>
                  </p>
               </div>
               <div type=""div3"" rend=""dh-heading3"">
                  <head>ag “maschine und modell”: schnittstellen</head>
                  <p>die bereitstellung der daten über standardisierte maschinenlesbare schnittstellen (api) ist eine grundlegende repräsentationsform zukünftiger publikationspraktiken. idealerweise wird hier prototypisch ein protokoll oder eine spezifikation implementiert (z.b. oai-pmh, fcs, dts).</p>
               </div>
               <div type=""div3"" rend=""dh-heading3"">
                  <head>ag “wolke und vektor”: textanalyse</head>
                  <p>
                     <hi style=""font-size:11pt"">durch tokenisieren, lemmatisieren, vektorisieren werden wortlisten, wortfrequenzen, wortwolken, topicmodelling realisiert. die strukturierungs- und visualisierungsformen sind die grundlage für alternative rezeptionsformen und textinterne analysen bis zu (korpus)linguistischen methoden.</hi>
                  </p>
               </div>
               <div type=""div3"" rend=""dh-heading3"">
                  <head>ag “beziehung und geflecht”: netzwerke</head>
                  <p>
                     <hi style=""font-size:11pt"">die mit normdaten angereicherte datenbasis bietet die möglichkeit, die beziehungen zwischen den entitäten (personen, orte, institutionen, werke) zu extrahieren, zu visualisieren und zu analysieren. zu den anwendungsfällen gehören netzwerkanalysen und explorative navigationsformen.</hi>
                  </p>
               </div>
               <div type=""div3"" rend=""dh-heading3"">
                  <head>ag “medial und modular”: multimedia</head>
                  <p>die verwaltung, adressierung und einbettung multimedialer inhalte die sowie erzeugung und integration interaktiver elemente, wie dynamische visualisierungen, gehören zu den großen herausforderungen des digitalen publizierens. prototypische umsetzungen können hierfür impulse liefern oder lücken in bestehenden frameworks aufzeigen.</p>
               </div>
            </div>
            <div type=""div2"" rend=""dh-heading2"">
               <head>agenda:</head>
               <p>
                  <hi style=""font-size:11pt"">15’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">begrüßung, organisatorisches, vorstellungsrunde,</hi>
               </p>
               <p>
                  <hi style=""font-size:11pt"">15’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">vorstellung der daten und aufteilung auf arbeitsgruppen</hi>
               </p>
               <p>
                  <hi style=""font-size:11pt"">60’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">entwicklungsphase i</hi>
               </p>
               <p>
                  <hi style=""font-size:11pt"">30’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">pause</hi>
               </p>
               <p>
                  <hi style=""font-size:11pt"">45’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">entwicklungsphase ii</hi>
               </p>
               <p>
                  <hi style=""font-size:11pt"">30’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">vorstellung der ergebnisse</hi>
               </p>
               <p>
                  <hi style=""font-size:11pt"">15’</hi>
                  <hi rend=""apple-tab-span""> </hi>
                  <hi style=""font-size:11pt"">abschlussdiskussion</hi>
               </p>
            </div>
         </div>
         <div type=""div1"" rend=""dh-heading1"">
            <head>organisation</head>
            <div type=""div2"" rend=""dh-heading2"">
               <head>
                  <hi style=""font-family:trebuchet ms;font-size:13pt"">ergebnissicherung und outreach</hi>
               </head>
               <p>
                  <hi style=""font-size:11pt"">die ausgearbeiteten ergebnisse der arbeitsgruppen (daten, skripte, anwendungen) werden offen lizensiert und frei zur verfügung gestellt. ein workshopbericht fasst die ergebnisse übersichtlich als blogpost zusammen. gemeinsam mit interessierten teilnehmenden plant die workshopleitung eine ausarbeitung der ergebnisse als empfehlung für mögliche umsetzungen der dhd-abstracts. die konkreten implementierungen sollen während der konferenz online verfügbar gehalten werden und werden von der workshopleitung (und freiwillig teilnehmenden) über soziale medien bekannt gegeben und zur diskussion gestellt. weitere erwünschte kommunikationskanäle (website, email) werden mit der konferenzorganisation besprochen.</hi>
               </p>
            </div>
            <div type=""div2"" rend=""dh-heading2"">
               <head>datenmanagement und infrastruktur</head>
               <p>
                  <hi style=""font-size:11pt"">die einrichtung einer dezidierten organisation auf github.com für den workshop erlaubt das management der verschiedenen entwicklungen und kollaborative arbeitsformen. die dokumentation und das projektmanagement werden ebenfalls über git umgesetzt (githubwiki/  github-projects/issues). die publikation der ergebnisse erfolgt über github-repositories via zenodo. grundsätzlich stehen auch nichtkommerzielle gitlab-instanzen und das dariah-de-repository zur sicherung zur verfügung. das acdh kann für das kurz- und mittelfristige hosting der im rahmen des workshops entwickelten applikationen und services die erforderliche server-infrastruktur zur verfügung stellen. weiter ist die bereitstellung von mehreren instanzen der applikationen voyant und existdb zur nutzung im workshop über dariah-de vorgesehen.</hi>
               </p>
            </div>
            <div type=""div2"" rend=""dh-heading2"">
               <head>teilnehmer*innen</head>
               <p>
                  <hi style=""font-size:11pt"">die praktischen arbeitsphasen der gruppenarbeit erlauben nur kleine gruppengrößen. die zahl der möglichen teilnehmer*innen beträgt daher maximal 25 personen. für die workshopteilnahme werden zwar keine spezifischen technischen oder methodischen grundlagen (jenseits des umgangs mit den tei/xml-basisdaten) vorausgesetzt, jedoch erfordert ein hackathon von den teilnehmenden selbständiges arbeiten und die anwendung vorhandener kompetenzen auf den gegenstandsbereich. der workshop bietet den rahmen und raum für die gemeinsames arbeiten und offenen austausch.</hi>
               </p>
            </div>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note id=""ftn1"" n=""1"" rend=""footnote text"">
                    dies zeigt sich am beispiel der zeitschrift für digitale geisteswissenschaften (zfdg), die als vorbild für die zeitschrift medieval and early modern material culture online (memo) fungierte.
                </note>
            <note id=""ftn2"" n=""2"" rend=""footnote text"">
                    siehe die digitalisierungsklassen iv und v für texte.
                </note>
            <note id=""ftn3"" n=""3"" rend=""footnote text"">
                    die vielgestaltigen tei-kodierungen aus den einreichungen und transformationen wurden einheitlich angeglichen, dokumentiert auf: 
                    <ref target=""https://github.com/gvogeler/dhd2018"">
                  <hi rend=""color(1155cc)"">https://github.com/gvogeler/dhd2018</hi>
               </ref>.
                </note>
         </div>
         <div type=""bibliogr"">
            <listbibl>
               <head>bibliographie</head>
               <bibl>
                  <hi rend=""bold"">borgman, christine l. (2010)</hi>: <hi rend=""italic"">“the digital future is now. a call to action for the humanities”</hi>, in: digital humanities quarterly 3 (4): #p16, <ref target=""http://www.digitalhumanities.org/dhq/vol/3/4/000077/000077.html"">http://www.digitalhumanities.org/dhq/vol/3/4/000077/000077.html</ref> [letzter zugriff 26. september 2018].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">cremer, fabian (2018)</hi>: <hi rend=""italic"">“nun sag, wie hältst du es mit dem digitalen publizieren, digital humanities?”</hi>, in: digitale redaktion (blog), 21.03.2018, <ref target=""https://editorial.hypotheses.org/113"">https://editorial.hypotheses.org/113</ref> [letzter zugriff 26. september 2018].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">dhd-ag ""digitales publizieren"" (2016)</hi>: <hi rend=""italic"">“digitales publizieren”</hi>, working paper, 01.03.2016, <ref target=""http://diglib.hab.de/ejournals/ed000008/startx.htm"">http://diglib.hab.de/ejournals/ed000008/startx.htm</ref> [letzter zugriff 26. september 2018].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">hannesschläger, vanessa / andorfer, peter (2018)</hi>: <hi rend=""italic"">menschen gendern? datenmodellierung zur erhebung von geschlechterverteilung am beispiel der tei2016 abstracts app</hi>, dhd 2018, köln,<ref target=""https://doi.org/10.5281/zenodo.1182576"">https://doi.org/10.5281/zenodo.1182576</ref>
               </bibl>
               <bibl>
                  <hi rend=""bold"">jannidis, fotis /  kohle, hubertus / rehbein, malte (2017)(eds.)</hi>: <hi rend=""italic"">digital humanities. eine einführung</hi>,  stuttgart: j. b. metzler.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">kohle, hubertus (2017)</hi>: <hi rend=""italic"">“digitales publizieren”</hi>, in: <hi rend=""bold"">jannidis, fotis et al. (eds.)</hi>: <hi rend=""italic"">digital humanities. eine einführung</hi>,  stuttgart: j. b. metzler 199-205.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">klaffki, lisa / schmunk, stefan / stäcker thomas (2018)</hi>: <hi rend=""italic"">“stand der kulturgutdigitalisierung in deutschland. eine analyse und handlungsvorschläge des dariah-de stakeholdergremiums 'wissenschaftliche sammlungen'”</hi>, dariah-de working papers 26, göttingen, urn: <ref target=""http://resolver.sub.uni-goettingen.de/purl/?dariah-2018-1"">urn:nbn:de:gbv:7-dariah-2018-1-3</ref>.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">stäcker, thomas (2012)</hi>: <hi rend=""italic"">“wie schreibt man digital humanities?”</hi>, in: dhd-blog, 19.08.2012, <ref target=""https://dhd-blog.org/?p=673"">https://dhd-blog.org/?p=673</ref> [letzter zugriff 26. september 2018].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">stäcker, thomas (2017)</hi>: <hi rend=""italic"">“digital humanities : eine einführung / fotis jannidis, hubertus kohle, malte rehbein (hg.)”</hi>, in:  o-bib. das offene bibliotheksjournal 4(3): 142-148, <ref target=""https://doi.org/10.5282/o-bib/2017h3s142-148"">https://doi.org/10.5282/o-bib/2017h3s142-148</ref>
               </bibl>
               <bibl>
                  <hi rend=""bold"">vogeler, georg (2018)(ed.)</hi>: <hi rend=""italic"">dhd 2018. kritik der digitalen vernunft. konferenzabstracts</hi>, köln: universität zu köln 2018.
                    </bibl>
            </listbibl>
         </div>
      </back>
   </text>

",1.0,1.0,Voyant
11046,2020 - University of Paderborn,University of Paderborn,Digital Humanities zwischen Modellierung und Interpretation,2020,DHd,DHd,Universität Paderborn,Paderborn,,Germany,https://zenodo.org/record/3666690,Zwischen geisteswissenschaftlicher Offenheit und informatischer Explikation: Motivsuche als Herausforderung bei der Arbeit mit digitalen Ressourcen,,Nina Claudia Rastinger;Claudia Resch,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Theoretischer Hintergrund</head>
            <p>Literarische Motive – in ihrer Gestalt als 
                    <hi rend=""italic"">„a theme, character, or verbal pattern which recurs in literature“</hi> (Beckson & Ganz: 1960) – stellen seit Langem einen Untersuchungsgegenstand der Literaturwissenschaft dar. Als 
                    <hi rend=""italic"">„anthrophologische Grundsituationen, die zwar historisch variiert werden, aber in ihrem Kern konstant bleiben“</hi> (Nünning 2013: 542), ziehen sie sich durch die Literaturgeschichte und damit durch den geisteswissenschaftlichen Forschungsbereich. Hiervon angeregte Erkenntnisinteressen umfassen sowohl Fragen danach, wie bestimmte Motive in ausgewählten Werken auftauchen (u. a. Ester et al. 2017, Nölle 2017), als auch danach, wie sich dieses Auftauchen diachron verhält: Wie beständig bzw. flüchtig ist das jeweilige literarische Motiv (Freedman 1971: 126) und inwiefern koppelt es sich an bestimmte Perioden, Textgattungen oder Autor*innen (von Wilpert 2013: 534)? Daran, welcher Aspekt eines Motivs dabei im Wandel der Zeit bestehen bleibt, kann man Wilpert (2013: 533–534) zufolge zudem zwischen verschiedenen Motivarten differenzieren: Bei konstanten Situationen, wie jener des 
                    <hi rend=""italic"">heimkehrenden Sohnes</hi>, handelt es sich um Situationsmotive, während konstant bleibende Charaktere, wie der 
                    <hi rend=""italic"">Menschenfeind</hi>, Typus-Motive konstituieren.
                </p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Forschungsstand</head>
            <p>Obwohl die Motivforschung, wie oben beschrieben, ein vielseitiges Forschungsfeld aufspannt, wurde dieses im Rahmen der Digital Humanities bisher kaum beachtet: Im deutschsprachigen Raum existieren derzeit keine digitalen Korpora, in welchen eine Motivannotation vorgenommen wurde, und auch im nicht-deutschsprachigen Raum scheinen sich derartige Bestrebungen primär auf die Textgattung der Volksmärchen zu beschränken (u. a. Karsdorp et al. 2012, Garcia-Fernandez et al. 2014). Nur in wenigen Ausnahmefällen, wie in der Mittelhochdeutschen Begriffsdatenbank (Springeth 2005), liegt eine semantische Annotation vor, die Themen oder andere motivähnliche Aspekte mit Mark-Up versieht. Begründung findet dieser Umstand der fehlenden Motivannotation vor allem in der vagen Natur des zu Annotierenden. So erfahren Motive nach Best (2008: 349) schließlich erst 
                    <hi rend=""italic"">„im sprachl. Kunstwerk ihre individuelle Ausformung [und sind] somit erst durch Abstraktion faßbar“</hi> und nicht an spezifische sprachliche Ausdrücke gebunden. Die Annotation von literarischen Motiven müsste insofern (noch) manuell vorgenommen werden, wodurch ein hoher Zeitaufwand entstünde sowie eine starke Subjektivität gegeben wäre. 
                </p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Forschungsvorhaben</head>
            <p>Dennoch macht die Überführung der Motivforschung in den Bereich der Digital Humanities Sinn: Korpusbasierte Untersuchungen ermöglichen die systematische Erforschung sowie quantitative Auswertung umfangreicher Textmengen und gewähren somit – vor allem im Hinblick auf die diachron angelegte Motivgeschichte und die Häufigkeiten spezifischer Motive (Freedman 1971: 126) – neue Erkenntnisse. Da es sich bei motivannotierten Korpora zum jetzigen Zeitpunkt jedoch noch um ein Desiderat handelt, muss hierfür ein anderer Zugang gewählt werden und zwar jener der Motivsuche – über folgende Frage: Mithilfe welcher Suchstrategien können Motive in nicht motivisch annotierten Korpora ausfindig gemacht werden?</p>
            <p>Für ebendiese Herausforderung versucht der hier geplante Beitrag mögliche Lösungsansätze aufzuzeigen, wobei der Fokus auf ausgewählten Typus-Motiven, wie dem 
                    <hi rend=""italic"">weisen Salomon</hi> oder dem 
                    <hi rend=""italic"">armen Sünder</hi>, liegt. Anhand dieser sollen exemplarisch diverse Suchstrategien entwickelt werden, mithilfe welcher literarische Motive in digitalen Korpora, wie dem Austrian Baroque Corpus (ABaC:us) oder dem Deutschen Textarchiv (DTA), identifiziert werden können. Die Potentiale der verwendeten digitalen Ressourcen für die Motivsuche werden dabei genauso diskutiert wie ihre auftauchenden Limitationen. Immer mitzubedenken gilt es etwa die bereits angesprochene wechselnde Gestalt von literarischen Motiven: Da es sich bei ihnen um abstrakte Konzepte handelt, welche auf der Textoberfläche eine Vielfalt an sprachlichen Formen annehmen können, sind sie nur schwer über wenige ausgewählte „key words“ aufspürbar. Hierzu trägt auch ihre Komplexität bei: 
                    <hi rend=""italic"">„Motive zeigen Personen und Sachen nicht isoliert, sondern in einem Zusammenhang“</hi> (Frenzel 1978: 29) und bestehen damit immer aus mehreren inhaltlichen Komponenten – wie im Falle von Typus-Motiven, bei welchen Charakteren gewisse Eigenschaften zugeschrieben werden. 
                </p>
            <p>Diesen Problemen versucht das vorliegende Forschungsvorhaben mit vielfältigen Mitteln beizukommen. So wird beispielsweise aus der in vielen Korpora bereits vorhandenen linguistischen Basisannotation, bestehend aus einer Lemmatisierung und einer Wortartenzuordnung (POS-Tagging), Nutzen gezogen: Eigenschaften, wie Weisheit, können etwa als attributive Adjektive operationalisiert und über Abstandsoperatoren nahe eines interessierenden Charakters lokalisiert werden. Zudem können über Open-Source-Anwendungen wie AntConc oder Voyant Tools Kookkurrenzanalysen durchgeführt werden, um Charaktere und Eigenschaften zueinander in Bezug zu setzen und typische Zuschreibungen und Formulierungen sichtbar zu machen. Passend hierzu wird ebenfalls das von Huijnen und Lonij (2016) für die thematische Suche entwickelte Programm „Keyword Generator“ auf seinen Ertrag für die Motivsuche hin befragt werden: Kann die Generierung motivspezifischer Suchwörter anhand bereits erkannter Textpassagen bei der Identifikation weiterer Belegstellen behilflich sein? Diese Frage soll genauso zu beantworten versucht werden wie jene nach der Wahl der adäquaten Suchtermini, für deren Ermittlung sowohl Synonym- als auch Motivdatenbanken zum Einsatz kommen werden. Damit soll der geplante Beitrag letztendlich anhand konkreter Beispiele aus der literaturwissenschaftlichen Praxis verschiedenste Suchstrategien sowie deren Vor- und Nachteile aufzeigen, um die Motivsuche für zukünftige Nutzer*innen zu erleichtern und dadurch zur vermehrten digitalen Motivforschung anzuregen.</p>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Anthony, Laurence</hi> (2019): 
                        <hi rend=""italic"">AntConc (Version 3.5.8)</hi>. Tokyo: Waseda University https://www.laurenceanthony.net/software [letzter Zugriff 25. September 2019].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Berlin-Brandenburgische Akademie der Wissenschaften</hi> (eds.) (2019): 
                        <hi rend=""italic"" space=""preserve"">Deutsches Textarchiv. </hi>Grundlage für ein Referenzkorpus der neuhochdeutschen Sprache. http://www.deutschestextarchiv.de/ [letzter Zugriff 25. September 2019].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Beckson, Karl / Ganz, Arthur</hi> (1960): 
                        <hi rend=""italic"">A Reader's Guide to Literary Terms</hi>. New York: The Noonday Press.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Best, Otto F.</hi> (2008): 
                        <hi rend=""italic"" space=""preserve"">Handbuch literarischer Fachbegriffe. </hi>Definitionen und Beispiele. Frankfurt am Main: Fischer.
                    </bibl>
               <bibl>
                  <anchor id=""Hlk20319381""/>
                  <hi rend=""bold"">Ester, Hans / Mariacher, Barbara / Tax, Evelyne</hi> (eds.) (2017): 
                        <hi rend=""italic"" space=""preserve"">Abschied als literarisches Motiv in der deutschsprachigen Literatur. </hi>Festschrift zu Ehren des 75. Geburtstages von Jattie Enklaar. Würzburg: Königshausen & Neumann.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Freedman, William</hi> (1971): „The Literary Motif: A Definition and Evaluation“, in: 
                        <hi rend=""italic"" space=""preserve"">NOVEL: A Forum on Fiction </hi>4 (2): 123–131.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Frenzel, Elisabeth</hi> (1978): 
                        <hi rend=""italic"">Stoff-, Motiv- und Symbolforschung</hi>. 4., durchges. u. erg. Aufl. Stuttgart: Metzler.
                    </bibl>
               <bibl>
                  <anchor id=""Hlk20307338""/>
                  <hi rend=""bold"">Garcia-Fernandez, Anne / Ligozat, Anne-Laure / Vilnat, Anne</hi> (2014): „Construction and Annotation of a French Folkstale Corpus“, in: 
                        <hi rend=""italic"" space=""preserve"">Proceedings of the Ninth International Conference on Language Resources and Evaluation </hi>2430–2435. 
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Huijnen, Pim / Lonij, Juliette</hi> (2016). „From Keyword Search To Discourse Mining – The Meaning Of Scientific Management In Dutch Vocabulary, 1900-1940“, in: 
                        <hi rend=""italic"">Digital Humanities 2016: Conference Abstracts</hi> 569–570. 
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Karsdorp, Folgert / van Kranenburg, Peter / Meder, Theo / Trieschnigg, Dolf / van den Bosch, Antal</hi> (2012): 
                        <hi rend=""italic"">In search of an appropriate abstraction level for motif annotations</hi>. http://dolf.trieschnigg.nl/papers/CMN.2012.karsdorp.pdf [letzter Zugriff 25. September 2019].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Nölle, Volker</hi> (2017): 
                        <hi rend=""italic"">Der heimliche Blick: Motiv und Modell – eine Matrix innovativer Perspektiven</hi>. Würzburg: Königshausen & Neumann.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Nünning, Ansgar</hi> (ed.) (2013): 
                        <hi rend=""italic"">Metzler Lexikon Literatur- und Kulturtheorie. Ansätze – Personen – Grundbegriffe</hi>. Fünfte, aktualisierte und erweiterte Auflage. Stuttgart: J. B. Metzler.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Resch, Claudia / Czeitschner, Ulrike</hi> (eds.) (2015): 
                        <hi rend=""italic"">ABaC:us – Austrian Baroque Corpus</hi>. http://acdh.oeaw.ac.at/abacus/ [letzter Zugriff 25. September 2019].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Sinclair, Stéfan / Rockwell, Geoffrey</hi> (2019): 
                        <hi rend=""italic"">Voyant Tools (Version 2.4)</hi>. https://voyant-tools.org/ [letzter Zugriff 25. September 2019]. 
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Springeth, Margarete</hi> (2005): „Auf der Suche nach Begriffen und Motiven. Die Mittelhochdeutsche Begriffsdatenbank (MHDBDB) an der Universität Salzburg“, in: Schubert, Martin J. (ed.): 
                        <hi rend=""italic"">Deutsche Texte des Mittelalters zwischen Handschriftennähe und Rekonstruktion</hi> (= Beihefte zu Editio 23). Tübingen: Niemeyer 317–323.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">Von Wilpert, Gero</hi> (2013): 
                        <hi rend=""italic"" space=""preserve"">Sachwörterbuch der Literatur. </hi>Sonderausgabe der 8. verbesserten und erweiterten Auflage 2001. Stuttgart: Alfred Kröner.
                    </bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,digital corpora;literary motif;motif annotation;motif search;search strategies,German,entdeckung;forschungsprozess;inhaltsanalyse;literatur;methoden;text,2020-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div rend=""dh-heading1"" type=""div1"">
            <head>theoretischer hintergrund</head>
            <p>literarische motive – in ihrer gestalt als 
                    <hi rend=""italic"">„a theme, character, or verbal pattern which recurs in literature“</hi> (beckson & ganz: 1960) – stellen seit langem einen untersuchungsgegenstand der literaturwissenschaft dar. als 
                    <hi rend=""italic"">„anthrophologische grundsituationen, die zwar historisch variiert werden, aber in ihrem kern konstant bleiben“</hi> (nünning 2013: 542), ziehen sie sich durch die literaturgeschichte und damit durch den geisteswissenschaftlichen forschungsbereich. hiervon angeregte erkenntnisinteressen umfassen sowohl fragen danach, wie bestimmte motive in ausgewählten werken auftauchen (u. a. ester et al. 2017, nölle 2017), als auch danach, wie sich dieses auftauchen diachron verhält: wie beständig bzw. flüchtig ist das jeweilige literarische motiv (freedman 1971: 126) und inwiefern koppelt es sich an bestimmte perioden, textgattungen oder autor*innen (von wilpert 2013: 534)? daran, welcher aspekt eines motivs dabei im wandel der zeit bestehen bleibt, kann man wilpert (2013: 533–534) zufolge zudem zwischen verschiedenen motivarten differenzieren: bei konstanten situationen, wie jener des 
                    <hi rend=""italic"">heimkehrenden sohnes</hi>, handelt es sich um situationsmotive, während konstant bleibende charaktere, wie der 
                    <hi rend=""italic"">menschenfeind</hi>, typus-motive konstituieren.
                </p>
         </div>
         <div rend=""dh-heading1"" type=""div1"">
            <head>forschungsstand</head>
            <p>obwohl die motivforschung, wie oben beschrieben, ein vielseitiges forschungsfeld aufspannt, wurde dieses im rahmen der digital humanities bisher kaum beachtet: im deutschsprachigen raum existieren derzeit keine digitalen korpora, in welchen eine motivannotation vorgenommen wurde, und auch im nicht-deutschsprachigen raum scheinen sich derartige bestrebungen primär auf die textgattung der volksmärchen zu beschränken (u. a. karsdorp et al. 2012, garcia-fernandez et al. 2014). nur in wenigen ausnahmefällen, wie in der mittelhochdeutschen begriffsdatenbank (springeth 2005), liegt eine semantische annotation vor, die themen oder andere motivähnliche aspekte mit mark-up versieht. begründung findet dieser umstand der fehlenden motivannotation vor allem in der vagen natur des zu annotierenden. so erfahren motive nach best (2008: 349) schließlich erst 
                    <hi rend=""italic"">„im sprachl. kunstwerk ihre individuelle ausformung [und sind] somit erst durch abstraktion faßbar“</hi> und nicht an spezifische sprachliche ausdrücke gebunden. die annotation von literarischen motiven müsste insofern (noch) manuell vorgenommen werden, wodurch ein hoher zeitaufwand entstünde sowie eine starke subjektivität gegeben wäre. 
                </p>
         </div>
         <div rend=""dh-heading1"" type=""div1"">
            <head>forschungsvorhaben</head>
            <p>dennoch macht die überführung der motivforschung in den bereich der digital humanities sinn: korpusbasierte untersuchungen ermöglichen die systematische erforschung sowie quantitative auswertung umfangreicher textmengen und gewähren somit – vor allem im hinblick auf die diachron angelegte motivgeschichte und die häufigkeiten spezifischer motive (freedman 1971: 126) – neue erkenntnisse. da es sich bei motivannotierten korpora zum jetzigen zeitpunkt jedoch noch um ein desiderat handelt, muss hierfür ein anderer zugang gewählt werden und zwar jener der motivsuche – über folgende frage: mithilfe welcher suchstrategien können motive in nicht motivisch annotierten korpora ausfindig gemacht werden?</p>
            <p>für ebendiese herausforderung versucht der hier geplante beitrag mögliche lösungsansätze aufzuzeigen, wobei der fokus auf ausgewählten typus-motiven, wie dem 
                    <hi rend=""italic"">weisen salomon</hi> oder dem 
                    <hi rend=""italic"">armen sünder</hi>, liegt. anhand dieser sollen exemplarisch diverse suchstrategien entwickelt werden, mithilfe welcher literarische motive in digitalen korpora, wie dem austrian baroque corpus (abac:us) oder dem deutschen textarchiv (dta), identifiziert werden können. die potentiale der verwendeten digitalen ressourcen für die motivsuche werden dabei genauso diskutiert wie ihre auftauchenden limitationen. immer mitzubedenken gilt es etwa die bereits angesprochene wechselnde gestalt von literarischen motiven: da es sich bei ihnen um abstrakte konzepte handelt, welche auf der textoberfläche eine vielfalt an sprachlichen formen annehmen können, sind sie nur schwer über wenige ausgewählte „key words“ aufspürbar. hierzu trägt auch ihre komplexität bei: 
                    <hi rend=""italic"">„motive zeigen personen und sachen nicht isoliert, sondern in einem zusammenhang“</hi> (frenzel 1978: 29) und bestehen damit immer aus mehreren inhaltlichen komponenten – wie im falle von typus-motiven, bei welchen charakteren gewisse eigenschaften zugeschrieben werden. 
                </p>
            <p>diesen problemen versucht das vorliegende forschungsvorhaben mit vielfältigen mitteln beizukommen. so wird beispielsweise aus der in vielen korpora bereits vorhandenen linguistischen basisannotation, bestehend aus einer lemmatisierung und einer wortartenzuordnung (pos-tagging), nutzen gezogen: eigenschaften, wie weisheit, können etwa als attributive adjektive operationalisiert und über abstandsoperatoren nahe eines interessierenden charakters lokalisiert werden. zudem können über open-source-anwendungen wie antconc oder voyant tools kookkurrenzanalysen durchgeführt werden, um charaktere und eigenschaften zueinander in bezug zu setzen und typische zuschreibungen und formulierungen sichtbar zu machen. passend hierzu wird ebenfalls das von huijnen und lonij (2016) für die thematische suche entwickelte programm „keyword generator“ auf seinen ertrag für die motivsuche hin befragt werden: kann die generierung motivspezifischer suchwörter anhand bereits erkannter textpassagen bei der identifikation weiterer belegstellen behilflich sein? diese frage soll genauso zu beantworten versucht werden wie jene nach der wahl der adäquaten suchtermini, für deren ermittlung sowohl synonym- als auch motivdatenbanken zum einsatz kommen werden. damit soll der geplante beitrag letztendlich anhand konkreter beispiele aus der literaturwissenschaftlichen praxis verschiedenste suchstrategien sowie deren vor- und nachteile aufzeigen, um die motivsuche für zukünftige nutzer*innen zu erleichtern und dadurch zur vermehrten digitalen motivforschung anzuregen.</p>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listbibl>
               <head>bibliographie</head>
               <bibl>
                  <hi rend=""bold"">anthony, laurence</hi> (2019): 
                        <hi rend=""italic"">antconc (version 3.5.8)</hi>. tokyo: waseda university https://www.laurenceanthony.net/software [letzter zugriff 25. september 2019].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">berlin-brandenburgische akademie der wissenschaften</hi> (eds.) (2019): 
                        <hi rend=""italic"" space=""preserve"">deutsches textarchiv. </hi>grundlage für ein referenzkorpus der neuhochdeutschen sprache. http://www.deutschestextarchiv.de/ [letzter zugriff 25. september 2019].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">beckson, karl / ganz, arthur</hi> (1960): 
                        <hi rend=""italic"">a reader's guide to literary terms</hi>. new york: the noonday press.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">best, otto f.</hi> (2008): 
                        <hi rend=""italic"" space=""preserve"">handbuch literarischer fachbegriffe. </hi>definitionen und beispiele. frankfurt am main: fischer.
                    </bibl>
               <bibl>
                  <anchor id=""hlk20319381""/>
                  <hi rend=""bold"">ester, hans / mariacher, barbara / tax, evelyne</hi> (eds.) (2017): 
                        <hi rend=""italic"" space=""preserve"">abschied als literarisches motiv in der deutschsprachigen literatur. </hi>festschrift zu ehren des 75. geburtstages von jattie enklaar. würzburg: königshausen & neumann.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">freedman, william</hi> (1971): „the literary motif: a definition and evaluation“, in: 
                        <hi rend=""italic"" space=""preserve"">novel: a forum on fiction </hi>4 (2): 123–131.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">frenzel, elisabeth</hi> (1978): 
                        <hi rend=""italic"">stoff-, motiv- und symbolforschung</hi>. 4., durchges. u. erg. aufl. stuttgart: metzler.
                    </bibl>
               <bibl>
                  <anchor id=""hlk20307338""/>
                  <hi rend=""bold"">garcia-fernandez, anne / ligozat, anne-laure / vilnat, anne</hi> (2014): „construction and annotation of a french folkstale corpus“, in: 
                        <hi rend=""italic"" space=""preserve"">proceedings of the ninth international conference on language resources and evaluation </hi>2430–2435. 
                    </bibl>
               <bibl>
                  <hi rend=""bold"">huijnen, pim / lonij, juliette</hi> (2016). „from keyword search to discourse mining – the meaning of scientific management in dutch vocabulary, 1900-1940“, in: 
                        <hi rend=""italic"">digital humanities 2016: conference abstracts</hi> 569–570. 
                    </bibl>
               <bibl>
                  <hi rend=""bold"">karsdorp, folgert / van kranenburg, peter / meder, theo / trieschnigg, dolf / van den bosch, antal</hi> (2012): 
                        <hi rend=""italic"">in search of an appropriate abstraction level for motif annotations</hi>. http://dolf.trieschnigg.nl/papers/cmn.2012.karsdorp.pdf [letzter zugriff 25. september 2019].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">nölle, volker</hi> (2017): 
                        <hi rend=""italic"">der heimliche blick: motiv und modell – eine matrix innovativer perspektiven</hi>. würzburg: königshausen & neumann.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">nünning, ansgar</hi> (ed.) (2013): 
                        <hi rend=""italic"">metzler lexikon literatur- und kulturtheorie. ansätze – personen – grundbegriffe</hi>. fünfte, aktualisierte und erweiterte auflage. stuttgart: j. b. metzler.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">resch, claudia / czeitschner, ulrike</hi> (eds.) (2015): 
                        <hi rend=""italic"">abac:us – austrian baroque corpus</hi>. http://acdh.oeaw.ac.at/abacus/ [letzter zugriff 25. september 2019].
                    </bibl>
               <bibl>
                  <hi rend=""bold"">sinclair, stéfan / rockwell, geoffrey</hi> (2019): 
                        <hi rend=""italic"">voyant tools (version 2.4)</hi>. https://voyant-tools.org/ [letzter zugriff 25. september 2019]. 
                    </bibl>
               <bibl>
                  <hi rend=""bold"">springeth, margarete</hi> (2005): „auf der suche nach begriffen und motiven. die mittelhochdeutsche begriffsdatenbank (mhdbdb) an der universität salzburg“, in: schubert, martin j. (ed.): 
                        <hi rend=""italic"">deutsche texte des mittelalters zwischen handschriftennähe und rekonstruktion</hi> (= beihefte zu editio 23). tübingen: niemeyer 317–323.
                    </bibl>
               <bibl>
                  <hi rend=""bold"">von wilpert, gero</hi> (2013): 
                        <hi rend=""italic"" space=""preserve"">sachwörterbuch der literatur. </hi>sonderausgabe der 8. verbesserten und erweiterten auflage 2001. stuttgart: alfred kröner.
                    </bibl>
            </listbibl>
         </div>
      </back>
   </text>

",2.0,3.0,Voyant
11070,2020 - University of Paderborn,University of Paderborn,Digital Humanities zwischen Modellierung und Interpretation,2020,DHd,DHd,Universität Paderborn,Paderborn,,Germany,https://zenodo.org/record/3666690,Syntaktische Profile für Interpretationen jenseits der Textoberfläche,,Melanie Andresen;Anke Begerow;Lina Franken;Uta Gaidys;Gertraud Koch;Heike Zinsmeister,paper,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <p style=""text-align:left; "">Viele Verfahren des Text Mining und Distant Reading beschränken sich auf eine wortbasierte Auswertung von Texten. Auch wenn auf Basis der Wortformen und ihrer linearen Abfolge bereits neue Perspektiven auf Texte gewonnen werden (z. B. mittels der Voyant Tools, Sinclair / Rockwell 2016), schöpfen diese Methoden das Potential von Texten bei Weitem nicht aus. Insbesondere, wenn die Auswertungsergebnisse Gegenstand weiterführender Interpretationen werden, z. B. um soziale Phänomene zu beschreiben, sehen wir einen Mehrwert in der Auswertung zusätzlicher sprachlicher Strukturen. Konkret verwenden wir syntaktische Annotationen, die präzisere Informationen zu Wortkombinationen liefern können, etwa „X ist Subjekt von Y“ anstelle von „X steht im Kontext von Y“. Zudem bestehen viele syntaktische Relationen über eine längere Distanz an der Oberfläche hinweg und können deshalb nur durch eine syntaktische Perspektive erfasst werden (z. B. Andresen / Zinsmeister 2017). Dies gilt für unterschiedliche Sprachen in unterschiedlichem Ausmaß. Das Deutsche verfügt über deutlich mehr Distanzstrukturen als das Englische, für das die meisten Analyseverfahren ursprünglich entwickelt wurden.</p>
         <p style=""text-align:left; "">In diesem Beitrag vergleichen wir zwei Ansätze zur Berechnung von Kollokationen, einen oberflächenorientierten Ansatz und einen auf Dependenzannotationen basierten. An zwei Fallstudien aus den Fächern Kulturanthropologie und Pflegewissenschaft wird demonstriert, wie die beiden Ansätze eine qualitative Interpretation von Textdaten in Hinblick auf gesellschaftliche bzw. soziale Phänomene unterstützen können. Die Erstellung eines eindeutigen Goldstandards, der eine formale Evaluation erlauben würde, ist bei dieser Art Fragestellung nicht möglich. Stattdessen wird auf qualitative Weise das Potential dieser Analyse zur Bearbeitung der geistes- und sozialwissenschaftlichen Fragestellungen beschrieben.</p>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Syntaktische Profile: Forschungsstand</head>
            <p style=""text-align:left; "">Die Nutzung syntaktischer Informationen zur Charakterisierung von Wortverwendungen ist vor allem in der Lexikografie betrieben worden. Populär wurde das Konzept unter dem Namen „word sketch“ besonders durch die korpuslinguistische Software SketchEngine (Kilgarriff et al. 2004, Kilgarriff et al. 2014). Für ein gegebenes Suchwort wird hier angegeben, welche anderen Wörter besonders häufig in spezifischen syntaktischen Relationen zum Suchwort stehen, z. B. 
                    <hi rend=""italic"">glimpse</hi> als frequentes Objekt von 
                    <hi rend=""italic"">catch</hi> (Kilgarriff et al. 2014: 9). Im Digitalen Wörterbuch der deutschen Sprache (DWDS) kann eine entsprechende Darstellung als sog. DWDS-Wortprofil abgerufen werden (Geyken 2011).
                </p>
            <p style=""text-align:left; "">Weitere Anwendungen gibt es in der Literaturwissenschaft und Linguistik: Googasian / Heuser (2019) vergleichen die syntaktischen Kontexte von Menschen und Tieren in einem Korpus sog. „wild animal stories“. Andresen (2018) nutzt syntaktische n-Gramme für einen Vergleich der Wissenschaftssprachen in den Fächern Literaturwissenschaft und Linguistik. Eine Anwendung auf sozialwissenschaftliche Fragestellungen erfolgt vor allem in den Politikwissenschaften: Anhand syntaktischer Muster wie z. B. Argumentstrukturen oder Mustern der Redewiedergabe werden hier politische Akteure und ihre Positionen identifiziert, miteinander in Relation gesetzt und so Diskurse charakterisiert (van Atteveldt et al. 2008, Kleinnijenhuis / van Atteveldt 2014, Wüest et al. 2011, Blessing et al. 2013). In den Fächern Pflegewissenschaft und Kulturanthropologie steht die Exploration des Mehrwertes syntaktischer Analysen noch aus.</p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Daten und Fragestellungen der Fallstudien</head>
            <p style=""text-align:left; "">Das Potential syntaktisch definierter Kollokationen wird an zwei Fallstudien mit komplementärer Datenlage erprobt. Die erste nutzt ein großes Korpus geschriebener Sprache, das dadurch methodisch eine sehr sichere Grundlage bietet. Die zweite Fallstudie basiert auf einem eher kleinen Korpus mit gesprochener Sprache, was mehr methodische Herausforderungen erwarten lässt.</p>
            <p style=""text-align:left; "">Die kulturanthropologische Fallstudie befasst sich mit dem Themenkomplex der Telemedizin und insbesondere der Frage nach der (Nicht-)Akzeptanz telemedizinischer Anwendungen durch unterschiedliche gesellschaftliche Akteursgruppen. Das hierfür erstellte Korpus umfasst 8.784 Texte mit insgesamt 14,8 Mio. Token und basiert auf einem Webcrawling (Adelmann / Franken 2020). Dafür wurden Webseiten von Krankenkassen, Ärzte- und Patientenverbänden als Ausgangspunkt genutzt und dann Links zu Seiten verfolgt, die mindestens eines von mehreren Wörtern aus einem Wortfeld zur Telemedizin enthalten (Koch / Franken im Druck). </p>
            <p style=""text-align:left; "">Grundlage der pflegewissenschaftlichen Fallstudie ist ein Korpus aus 31 Dialogen, die mit schwerkranken und sterbenden Menschen in palliativer Versorgung geführt wurden. Es handelt sich um Transkripte gesprochener Sprache im Umfang von gut 100.000 Token. Gegenstand der Studie sind die Deutungen von Entscheidungen hinsichtlich der gesundheitlichen Versorgung der Betroffenen.</p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Methode</head>
            <p style=""text-align:left; "">Die Texte beider Korpora werden mithilfe
des Parsers MATE (Bohnet 2010), trainiert auf der Hamburger Dependency
Treebank (Foth et al. 2014), mit Lemmata, Wortarten und syntaktischen
Dependenzen annotiert. Unter Kollokationen verstehen wir „a
combination of two words that exhibit a tendency to occur near each
other in natural language“ (Evert 2008: 1214). Bei der
Operationalisierung von „near each other“ können Kriterien an der
Textoberfläche oder syntaktische Kriterien angesetzt werden: Für den
einfachen Ansatz ohne Annotationen betrachten wir Wörter in einem
Kontextfenster von +/- 3 Wörtern als benachbart, für den syntaktischen
Ansatz Wörter mit einer direkten Dependenzrelation. In beiden Fällen
wird mithilfe des Log-Likelihood-Ratios (LLR, Dunning 1993) berechnet,
welche Kombinationen häufiger im Korpus vorkommen, als basierend auf
den Einzelfrequenzen der Wörter zu erwarten wäre. Im Falle der
syntaktischen Kollokationen werden dafür die Einzelfrequenzen in der
spezifischen syntaktischen Relation genutzt. Die Ergebnisse basieren
auf den Lemmata und werden nach Schlüsselwörtern gefiltert, die für
die jeweiligen Fragestellungen als bedeutsam ausgewählt wurden
(<hi rend=""italic"">T</hi>/<hi rend=""italic"">telemed</hi> bzw. 
<hi rend=""italic"">E</hi>/<hi rend=""italic"">entscheid</hi>). Das hierfür verwendete Analyseskript steht auf GitHub zur Verfügung.<ref n=""1"" target=""ftn1""/> Für die Interpretation werden die Top 10 beider Listen verglichen und nach Bedarf weitere Einträge gesichtet.
</p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Ergebnisse der Fallstudien</head>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Kulturanthropologie</head>
               <p style=""text-align:left; "">Tabelle 1 zeigt die oberflächenbasierten Kollokationen zu Lemmata mit 
<hi rend=""italic"">T</hi>/<hi rend=""italic"">telemed</hi> im kulturanthropologischen Korpus mit den höchsten LLR-Werten. 
<hi rend=""italic"">Telemedizin</hi> ist sehr stark mit dem verwandten Wort 
<hi rend=""italic"">Telematik</hi> assoziiert, was die enge Verknüpfung
der Bereiche anzeigt. Manche Wortpaare sind Bestandteil mehrteiliger
Eigennamen
(<hi rend=""italic"">Bayerische TelemedAllianz, [Zentrum für Telematik
und] Telemedizin GmbH</hi>), die für die Interpretation einen
eingeschränkten Mehrwert haben,
aber doch für den Diskurs potentiell relevante und ggf. bisher unbekannte Akteure sichtbar machen. Mit dem 
<hi rend=""italic"">Tag der Telemedizin</hi> wird ein Fachkongress als wichtiger Begegnungspunkt dieser Akteure aufgeführt. Außerdem liegen allgemeine Konzepte wie 
<hi rend=""italic"">telemedizisch</hi> und 
<hi rend=""italic"">Anwendung</hi> hoch im Ranking.
</p>
               <table rend=""rules"">
                  <head> Tabelle 1: Top 10 der oberflächenbasierten Kollokationen zu
  Lemmata mit <hi rend=""italic"">T</hi>/ <hi rend=""italic"">telemed</hi> im kulturanthropologischen Korpus</head>
                  <row>
                     <cell rend=""DH-Default"">
                        <hi rend=""bold"">Wort 1</hi>
                     </cell>
                     <cell rend=""DH-Default"">
                        <hi rend=""bold"">Wort 2</hi>
                     </cell>
                     <cell rend=""DH-Default"">LLR</cell>
                     <cell rend=""DH-Default"">
                        <hi rend=""bold"">abs. Frequenz</hi>
                     </cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Telematik</cell>
                     <cell rend=""DH-Default"">Telemedizin</cell>
                     <cell rend=""DH-Default"">2044,88</cell>
                     <cell rend=""DH-Default"">468</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">telemedizinisch</cell>
                     <cell rend=""DH-Default"">Anwendung</cell>
                     <cell rend=""DH-Default"">1753,90</cell>
                     <cell rend=""DH-Default"">465</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">bayerisch</cell>
                     <cell rend=""DH-Default"">TelemedAllianz</cell>
                     <cell rend=""DH-Default"">1497,94</cell>
                     <cell rend=""DH-Default"">204</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Telemedizin</cell>
                     <cell rend=""DH-Default"">GmbH</cell>
                     <cell rend=""DH-Default"">1007,39</cell>
                     <cell rend=""DH-Default"">340</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Tag</cell>
                     <cell rend=""DH-Default"">Telemedizin</cell>
                     <cell rend=""DH-Default"">845,74</cell>
                     <cell rend=""DH-Default"">274</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">telemedizinisch</cell>
                     <cell rend=""DH-Default"">Betreuung</cell>
                     <cell rend=""DH-Default"">841,88</cell>
                     <cell rend=""DH-Default"">212</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">bayerisch</cell>
                     <cell rend=""DH-Default"">Telemedallianz</cell>
                     <cell rend=""DH-Default"">731,35</cell>
                     <cell rend=""DH-Default"">97</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">der</cell>
                     <cell rend=""DH-Default"">Telemedizin</cell>
                     <cell rend=""DH-Default"">644,28</cell>
                     <cell rend=""DH-Default"">2533</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Gesellschaft</cell>
                     <cell rend=""DH-Default"">Telemedizin</cell>
                     <cell rend=""DH-Default"">632,95</cell>
                     <cell rend=""DH-Default"">241</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">telemedizinisch</cell>
                     <cell rend=""DH-Default"">Zentrum</cell>
                     <cell rend=""DH-Default"">585,73</cell>
                     <cell rend=""DH-Default"">165</cell>
                  </row>
               </table>
               <table rend=""rules"">
                  <head> Tabelle 2: Top 10 der syntaxbasierten Kollokationen zu Lemmata mit <hi rend=""italic"">T</hi>/<hi rend=""italic"">telemed</hi> im kulturanthropologischen Korpus</head>
                  <row>
                     <cell rend=""DH-Default"">
                        <hi rend=""bold"">Wort 1</hi>
                     </cell>
                     <cell rend=""DH-Default"">Relation</cell>
                     <cell rend=""DH-Default"">
                        <hi rend=""bold"">Wort 2</hi>
                     </cell>
                     <cell rend=""DH-Default"">LLR</cell>
                     <cell rend=""DH-Default"">
                        <hi rend=""bold"">abs. Frequenz</hi>
                     </cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">GmbH</cell>
                     <cell rend=""DH-Default"">ist Apposition von</cell>
                     <cell rend=""DH-Default"">Telemedizin</cell>
                     <cell rend=""DH-Default"">2261,71</cell>
                     <cell rend=""DH-Default"">353</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">telemedizinisch</cell>
                     <cell rend=""DH-Default"">ist Attribut von</cell>
                     <cell rend=""DH-Default"">Anwendung</cell>
                     <cell rend=""DH-Default"">2236,42</cell>
                     <cell rend=""DH-Default"">456</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">bayerisch</cell>
                     <cell rend=""DH-Default"">ist Attribut von</cell>
                     <cell rend=""DH-Default"">TelemedAllianz</cell>
                     <cell rend=""DH-Default"">2002,48</cell>
                     <cell rend=""DH-Default"">204</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Telemedizin</cell>
                     <cell rend=""DH-Default"">ist Genitivattribut von</cell>
                     <cell rend=""DH-Default"">Tag</cell>
                     <cell rend=""DH-Default"">1904,26</cell>
                     <cell rend=""DH-Default"">274</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">telemedizinisch</cell>
                     <cell rend=""DH-Default"">ist Attribut von</cell>
                     <cell rend=""DH-Default"">Betreuung</cell>
                     <cell rend=""DH-Default"">981,87</cell>
                     <cell rend=""DH-Default"">200</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">bayerisch</cell>
                     <cell rend=""DH-Default"">ist Attribut von</cell>
                     <cell rend=""DH-Default"">Telemedallianz</cell>
                     <cell rend=""DH-Default"">967,93</cell>
                     <cell rend=""DH-Default"">98</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">DGTelemed</cell>
                     <cell rend=""DH-Default"">ist Apposition von</cell>
                     <cell rend=""DH-Default"">V.</cell>
                     <cell rend=""DH-Default"">899,83</cell>
                     <cell rend=""DH-Default"">84</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">telemedizinisch</cell>
                     <cell rend=""DH-Default"">ist Attribut von</cell>
                     <cell rend=""DH-Default"">Zentrum</cell>
                     <cell rend=""DH-Default"">772,92</cell>
                     <cell rend=""DH-Default"">163</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Telemedizin</cell>
                     <cell rend=""DH-Default"">ist Apposition von</cell>
                     <cell rend=""DH-Default"">Fachkongress</cell>
                     <cell rend=""DH-Default"">727,37</cell>
                     <cell rend=""DH-Default"">64</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Telemedizin</cell>
                     <cell rend=""DH-Default"">ist Genitivattribut von</cell>
                     <cell rend=""DH-Default"">Möglichkeit</cell>
                     <cell rend=""DH-Default"">720,75</cell>
                     <cell rend=""DH-Default"">144</cell>
                  </row>
               </table>
               <p style=""text-align:left; "">Die syntaktischen Informationen in Tabelle 2 machen den Zusammenhang zwischen den Bestandteilen der Eigennamen in der Relation der Apposition explizit und bieten damit mehr Informationen zur Einordnung dieser Datenpunkte. Die Annotationen ermöglichen außerdem, die Gesamtliste nach bestimmten syntaktischen Relationen zu filtern. Die genannten Appositionen beispielsweise können anhand des Relationslabels ausgeblendet werden. Auch hier gibt es sehr allgemeine Konzepte wie 
<hi rend=""italic"">telemedizinisch</hi> als Attribut von 
<hi rend=""italic"">Anwendung</hi>, die zwar frequent, aber nicht sehr informativ sind. 
<hi rend=""italic"">Telemedizin</hi> als Genitivattribut von 
<hi rend=""italic"">Möglichkeit</hi> weist daraufhin, dass eben deren Möglichkeiten noch Gegenstand des Diskurses sind. In der Durchsicht der Kollokationen jenseits der Top 10 finden sich verwandte Themen des Potentials und der Projekthaftigkeit (<hi rend=""italic"">Potential der Telemedizin</hi>, 
<hi rend=""italic"">Telemedizin-typisches Potential</hi>, 
<hi rend=""italic"">evaluiertes Telemedizinprojekt</hi>, 
<hi rend=""italic"">vielversprechendes Telemedizinprojekt</hi>), die anzeigen, dass sich die Umsetzung der Telemedizin in einer frühen Phase befindet und ihre Akzeptanz als Regelversorgung noch nicht abschließend verhandelt ist.
</p>
               <p style=""text-align:left; "">Auch zur Kollokation 
<hi rend=""italic"">telemedizinisch</hi> als Attribut von 
<hi rend=""italic"">Betreuung</hi> finden sich weiter unten im Ranking
ähnliche Verwendungen zum Thema Betreuung
(<hi rend=""italic"">telemedizinisch betreuen</hi>, 
<hi rend=""italic"">telemedizinisch betreut</hi> …) und Unterstützung
(<hi rend=""italic"">telemedizinisch unterstützt</hi>, 
<hi rend=""italic"">telemedizinisch-unterstützte</hi> (sic!) 
<hi rend=""italic"">Versorgung</hi>, 
<hi rend=""italic"">Telematikunterstützung</hi> …). Dies weist auf die (bisher) eher ergänzende Rolle der Telemedizin im Verhältnis zur medizinischen Regelversorgung hin. 
</p>
               <p>  <lb/>  <lb/>  <lb/> </p>
            </div>
            <div rend=""DH-Heading2"" type=""div2"">
               <head>Pflegewissenschaft</head>
               <p style=""text-align:left; "">Tabelle 3 zeigt die zehn ersten oberflächenbasierten Kollokationen des Dialogkorpus zu Lemmata mit 
<hi rend=""italic"">E</hi>/<hi rend=""italic"">entscheid</hi>. Hier werden zunächst Probleme in den Daten deutlich: Mit 
<hi rend=""italic"">Finan/</hi> liegt ein für gesprochene Sprache typischer Abbruch eines Wortes (vermutlich: 
<hi rend=""italic"">Finanzentscheidung</hi>) vor. Zudem ist 
<hi rend=""italic"">Entscheidungsvariant</hi> eine fehlerhafte Lemmaform zu 
<hi rend=""italic"">Entscheidungsvarianten</hi>. Insgesamt sind die Frequenzen aufgrund der geringen Korpusgröße klein, lassen aber trotzdem hilfreiche Schlüsse für die Analyse zu. Im syntaxbasierten Gegenstück in Tabelle 4 sind zusätzliche Probleme erkennbar, die durch die automatische Verarbeitung gesprochener Sprache entstehen. Die Relation zwischen 
<hi rend=""italic"">hab</hi> und 
<hi rend=""italic"">entscheiden</hi> ist fehlerhaft als adverbial (korrekt: auxiliar) bezeichnet. Allerdings wird ein direkter Zusammenhang zwischen diesen Wörtern erst durch die syntaktischen Annotationen überhaupt erkennbar, da sie im Satz häufig nicht benachbart stehen. Zusätzlich gibt es vollständig falsche Analysen wie die Relation zwischen 
<hi rend=""italic"">entscheidend</hi> und 
<hi rend=""italic"">Puh</hi>.
</p>
               <table rend=""rules"">
                  <head> Tabelle 3: Top 10 der oberflächenbasierten Kollokationen zu Lemmata mit 
                        <hi rend=""italic"">E</hi>/<hi rend=""italic"">entscheid</hi> im pflegewissenschaftlichen Korpus</head>
                  <row>
                     <cell rend=""DH-Default"">
                        <hi rend=""bold"">Wort 1</hi>
                     </cell>
                     <cell rend=""DH-Default"">
                        <hi rend=""bold"">Wort 2</hi>
                     </cell>
                     <cell rend=""DH-Default"">LLR</cell>
                     <cell rend=""DH-Default"">
                        <hi rend=""bold"">abs. Frequenz</hi>
                     </cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Entscheidung</cell>
                     <cell rend=""DH-Default"">treffen</cell>
                     <cell rend=""DH-Default"">33,41</cell>
                     <cell rend=""DH-Default"">7</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">richtig</cell>
                     <cell rend=""DH-Default"">Entscheidung</cell>
                     <cell rend=""DH-Default"">23,06</cell>
                     <cell rend=""DH-Default"">7</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Tablettenform</cell>
                     <cell rend=""DH-Default"">entscheiden</cell>
                     <cell rend=""DH-Default"">21,28</cell>
                     <cell rend=""DH-Default"">4</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">der</cell>
                     <cell rend=""DH-Default"">Entscheidung</cell>
                     <cell rend=""DH-Default"">17,54</cell>
                     <cell rend=""DH-Default"">28</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">dieser</cell>
                     <cell rend=""DH-Default"">Entscheidung</cell>
                     <cell rend=""DH-Default"">13,02</cell>
                     <cell rend=""DH-Default"">7</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">selbst</cell>
                     <cell rend=""DH-Default"">entscheiden</cell>
                     <cell rend=""DH-Default"">12,88</cell>
                     <cell rend=""DH-Default"">4</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Entscheidung</cell>
                     <cell rend=""DH-Default"">überlassen</cell>
                     <cell rend=""DH-Default"">10,87</cell>
                     <cell rend=""DH-Default"">2</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Entscheidungsvariant</cell>
                     <cell rend=""DH-Default"">nebeneinandstellen</cell>
                     <cell rend=""DH-Default"">10,38</cell>
                     <cell rend=""DH-Default"">1</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Finan/</cell>
                     <cell rend=""DH-Default"">Versorgungsentscheidung</cell>
                     <cell rend=""DH-Default"">10,38</cell>
                     <cell rend=""DH-Default"">1</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">entschieden</cell>
                     <cell rend=""DH-Default"">Abraten</cell>
                     <cell rend=""DH-Default"">10,38</cell>
                     <cell rend=""DH-Default"">1</cell>
                  </row>
               </table>
               <p style=""text-align:left; "">Auch für die pflegewissenschaftliche Interpretation bieten die Kollokationen mit den höchsten LLR-Werten erste Anhaltspunkte, die dann durch eine Sichtung der weiteren Rangplätze ergänzt werden können. Die häufigsten Kollokatoren von Entscheidungen stehen für eine Realisierung eigener Entscheidungen der Betroffenen. Die Kollokation 
<hi rend=""italic"">richtig</hi> macht Bewertungen der Entscheidungen sichtbar. Es zeigen sich zudem gegensätzliche Dimensionen des Phänomens, wie „selber entscheiden“ vs. „Entscheidung abgeben“, die hinter der Kollokation mit 
<hi rend=""italic"">überlassen</hi> stehen. Insbesondere die Subjekt-
und Objektrelationen
(<hi rend=""italic"">Entscheidung treffen</hi>, 
<hi rend=""italic"">Entschluss entstehen</hi>, 
<hi rend=""italic"">Entscheidung überlassen</hi>) sind durch die syntaktische Analyse adäquater und theoretisch fundierter abgebildet. Dieser Nutzen wird jedoch durch Fehler in der automatischen Annotation eingeschränkt. Zudem werden diese Relationen in der gesprochenen Sprache mit kürzeren Sätzen möglicherweise auch durch die oberflächenbasierte Analyse besser erfasst als in anderen sprachlichen Registern. Insgesamt betrachtet werden durch den quantitativen Zugang Verwendungszusammenhänge des Phänomens „Entscheidung“ transparent, die wiederum auf wichtige Handlungskontexte in der Versorgungsrealität von schwerkranken und sterbenden Menschen verweisen. 
</p>
               <table rend=""rules"">
                  <head>Tabelle 4: Top 10 der syntaxbasierten Kollokationen zu Lemmata mit 
                        <hi rend=""italic"">E</hi>/
                        <hi rend=""italic"">entscheid</hi> im pflegewissenschaftlichen Korpus </head>
                  <row>
                     <cell rend=""DH-Default"">
                        <hi rend=""bold"">Wort 1</hi>
                     </cell>
                     <cell rend=""DH-Default"">Relation</cell>
                     <cell rend=""DH-Default"">
                        <hi rend=""bold"">Wort 2</hi>
                     </cell>
                     <cell rend=""DH-Default"">LLR</cell>
                     <cell rend=""DH-Default"">
                        <hi rend=""bold"">abs. Frequenz</hi>
                     </cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">entscheiden</cell>
                     <cell rend=""DH-Default"">ist Adverbial von</cell>
                     <cell rend=""DH-Default"">hab</cell>
                     <cell rend=""DH-Default"">33,25</cell>
                     <cell rend=""DH-Default"">9</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">richtig</cell>
                     <cell rend=""DH-Default"">ist Attribut von</cell>
                     <cell rend=""DH-Default"">Entscheidung</cell>
                     <cell rend=""DH-Default"">24,54</cell>
                     <cell rend=""DH-Default"">6</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Entscheidung</cell>
                     <cell rend=""DH-Default"">ist Akkusativobjekt von</cell>
                     <cell rend=""DH-Default"">treffen</cell>
                     <cell rend=""DH-Default"">23,47</cell>
                     <cell rend=""DH-Default"">4</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Entschluss</cell>
                     <cell rend=""DH-Default"">ist Subjekt von</cell>
                     <cell rend=""DH-Default"">entstehen</cell>
                     <cell rend=""DH-Default"">21,93</cell>
                     <cell rend=""DH-Default"">2</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">selbst</cell>
                     <cell rend=""DH-Default"">ist Adverbial von</cell>
                     <cell rend=""DH-Default"">entscheiden</cell>
                     <cell rend=""DH-Default"">18,74</cell>
                     <cell rend=""DH-Default"">4</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">entscheidend</cell>
                     <cell rend=""DH-Default"">ist Adverbial von</cell>
                     <cell rend=""DH-Default"">Puh</cell>
                     <cell rend=""DH-Default"">16,42</cell>
                     <cell rend=""DH-Default"">1</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Tablettenform</cell>
                     <cell rend=""DH-Default"">ist Subjekt von</cell>
                     <cell rend=""DH-Default"">entscheiden</cell>
                     <cell rend=""DH-Default"">16,13</cell>
                     <cell rend=""DH-Default"">2</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Entscheidung</cell>
                     <cell rend=""DH-Default"">ist Akkusativobjekt von</cell>
                     <cell rend=""DH-Default"">überlassen</cell>
                     <cell rend=""DH-Default"">15,59</cell>
                     <cell rend=""DH-Default"">2</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">Entscheidung</cell>
                     <cell rend=""DH-Default"">ist Akkusativobjekt von</cell>
                     <cell rend=""DH-Default"">treff</cell>
                     <cell rend=""DH-Default"">13,52</cell>
                     <cell rend=""DH-Default"">2</cell>
                  </row>
                  <row>
                     <cell rend=""DH-Default"">für</cell>
                     <cell rend=""DH-Default"">ist Präposition zu</cell>
                     <cell rend=""DH-Default"">entscheiden</cell>
                     <cell rend=""DH-Default"">13,46</cell>
                     <cell rend=""DH-Default"">7</cell>
                  </row>
               </table>
            </div>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Schlussfolgerungen</head>
            <p style=""text-align:left; "">Es hat sich gezeigt, dass die Berechnung von Kollokationen auf der Grundlage der sprachlichen Oberfläche bzw. der Syntax für qualitative Fragestellungen informativ sein kann. Für die Auswertung einer spezifischen Fragestellung ist die Assoziationsstärke allein jedoch nicht immer das entscheidende Kriterium. Die Kollokationen geben Hinweise auf Zusammenhänge innerhalb des Korpus, die neue Fragestellungen und Perspektiven generieren können. Gleichzeitig werden durch die Relationsannotationen bereits kleine Datenmengen in erweiterter Form auswertbar.</p>
            <p style=""text-align:left; "">Die beispielhaften Analysen haben gezeigt, dass die syntaktischen Annotationen eine für die Interpretation hilfreiche Differenzierung bieten, indem präziser angegeben wird, in welcher Relation zwei Wörter stehen. Das ermöglicht auch das Filtern nach interessanten Relationstypen. Zudem werden durch den Einbezug der Syntax Relationen zwischen Wörtern in Distanzstellung sichtbar, was insbesondere vom Verb abhängige Satzteile besser sichtbar macht. Andererseits erfordern die syntaktischen Annotationen eine aufwendigere Vorverarbeitung, die mehr Zeit und technische Fähigkeiten erfordert. Außerdem stellen sie eine zusätzliche Fehlerquelle dar. Dies gilt besonders für die gesprochensprachlichen Daten. Eine systematische Überprüfung und Rückbindung an konkrete Korpusbelege ist deshalb wichtig und verbessert die Interpretationsmöglichkeiten aus qualitativer Sicht. </p>
            <p style=""text-align:left; "">Anschließend an diese Arbeiten ist geplant, stärker Kontexte zu aggregieren: In grammatischer Hinsicht wird das durch Koreferenzannotationen erfolgen, die für das pflegewissenschaftliche Korpus bereits vorliegen. Auf semantischer Ebene verfolgen wir den Ansatz, Wortgruppen zu Konzepten zusammenzufassen, z. B. können 
<hi rend=""italic"">Ärztin</hi>, 
<hi rend=""italic"">Arzt</hi>, 
<hi rend=""italic"">Hausarzt</hi>, 
<hi rend=""italic"">Onkologin</hi> usw. auf ein gemeinsames Konzept ÄRZT*INNEN abgebildet werden (vgl. den Ansatz von Wüest et al. 2011). Wir sehen außerdem weitere Anwendungsfälle über die Fächergrenzen hinweg, etwa zur literaturwissenschaftlichen Beschreibung von Geschlechterzuschreibungen, indem die sprachlichen Kontexte weiblicher und männlicher Vornamen verglichen werden.
</p>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note n=""1"" rend=""footnote text"" id=""ftn1"">
               <ptr target=""https://github.com/melandresen/DHd2020""/>
            </note>
         </div>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Adelmann, Benedikt / Franken, Lina</hi> (2020): Thematic web crawling and scraping as a way to form focussed web archives, in: 
                        <hi rend=""italic"">Engaging with Web Archives Conference Book of Abstracts</hi>. To be published at  
                        <ref target=""https://ewaconference.com/"">https://ewaconference.com/</ref>.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Andresen, Melanie</hi> (2018): Sprachliche Variation in der Germanistik: eine n-Gramm-basierte Stilanalyse, in: 
                        <hi rend=""italic"">Book of Abstracts of DHd 2018</hi>. Köln, Deutschland, 311–15.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Andresen, Melanie / Zinsmeister, Heike</hi> (2017): The benefit of syntactic vs. linear n-grams for linguistic description, in: 
                        <hi rend=""italic"">Proceedings of the 4th International Conference on Dependency Linguistics (Depling 2017).</hi> Pisa, Italy, 4–14 http://aclweb.org/anthology/W17-6503.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">van Atteveldt, Wouter / Kleinnijenhuis, Jan / Ruigrok, Nel</hi> (2008): Parsing, Semantic Networks, and Political Authority Using Syntactic Analysis to Extract Semantic Relations from Dutch Newspaper Articles, in: 
                        <hi rend=""italic"">Political Analysis</hi>, 16(4): 428–46 doi:10.1093/pan/mpn006.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Blessing, Andre / Sonntag, Jonathan / Kliche, Fritz / Heid, Ulrich / Kuhn, Jonas / Stede, Manfred</hi> (2013): Towards a Tool for Interactive Concept Building for Large Scale Analysis in the Humanities, in: 
                        <hi rend=""italic"">Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities</hi>. 55–64 https://www.aclweb.org/anthology/W13-2708.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"" space=""preserve"">Bohnet, Bernd </hi>(2010): Very High Accuracy and Fast Dependency Parsing is not a Contradiction, in: 
                        <hi rend=""italic"">Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010).</hi> Beijing, China, 89–97. https://www.aclweb.org/anthology/C10-1011
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Dunning, Ted</hi> (1993): Accurate Methods for the Statistics of Surprise and Coincidence, in: 
                        <hi rend=""italic"">Computational Linguistics</hi>, 19(1): 61–74. https://www.aclweb.org/anthology/J93-1003
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Evert, Stefan</hi> (2008): Corpora and collocations, in: Lüdeling, Anke / Kytö, Merja (Hg.), 
                        <hi rend=""italic"">Corpus linguistics: an International Handbook</hi>, Vol. 2. (Handbücher zur Sprach- und Kommunikationswissenschaft 29). Berlin, Boston: De Gruyter, 1212–1248.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Foth, Kilian A. / Köhn, Arne / Beuck, Niels / Menzel, Wolfgang</hi> (2014): Because Size Does Matter: The Hamburg Dependency Treebank, in: 
                        <hi rend=""italic"">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14)</hi>, 2326–2333. Reykjavik, Iceland. 
                        <ref target=""http://www.lrec-conf.org/proceedings/lrec2014/pdf/860_Paper.pdf"">http://www.lrec-conf.org/proceedings/lrec2014/pdf/860_Paper.pdf</ref>.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Gaidys, Uta / Gius, Evelyn / Jarchow, Margarete / Koch, Gertraud / Menzel, Wolfgang / Orth, Dominik / Zinsmeister, Heike</hi> (2017): hermA: Automated modelling of hermeneutic processes, in: 
                        <hi rend=""italic"">Hamburger Journal für Kulturanthropologie</hi>(7): 119–23.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Geyken, Alexander</hi> (2011): Statistische Wortprofile zur schnellen Analyse der Syntagmatik in Textkorpora, in: Abel, Andrea / Zanin, Renata (Hg.), 
                        <hi rend=""italic"">Korpora in Lehre und Forschung</hi>. Bozen: Bolzano Univ. Press, 129–54.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Googasian, Victoria / Heuser, Ryan J.</hi> (2019): Digital Animal Studies: Modeling Anthropomorphism in Animal Writing, 1870-1930, in: 
                        <hi rend=""italic"">Book of Abstracts of DH 2019</hi> https://dev.clariah.nl/files/dh2019/boa/0458.html.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Kilgarriff, Adam / Baisa, Vít / Bušta, Jan / Jakubíček, Miloš / Kovář, Vojtěch / Michelfeit, Jan, Rychlý / Pavel. / Suchomel, Vít</hi> (2014): The Sketch Engine: ten years on, in: 
                        <hi rend=""italic"">Lexicography</hi>, 1(1): 7–36 doi:10.1007/s40607-014-0009-9.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Kilgarriff, Adam / Rychlý, Pavel / Smrz, Pavel / Tugwell, David</hi> (2004): The Sketch Engine: in: 
                        <hi rend=""italic"">Proceedings of the 11th EURALEX International Congress</hi>. 105–15 https://euralex.org/publications/the-sketch-engine/.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Kleinnijenhuis, Jan / van Atteveldt, Wouter</hi> (2014): Positions of Parties and Political Cleavages between Parties in Texts, in: Kaal, Bertie / Maks, Isa / van Elfrinkhof, Annemarie (Hg.), 
                        <hi rend=""italic"">Discourse Approaches to Politics, Society and Culture</hi>, Vol. 55. Amsterdam: Benjamins, 1–20 doi:10.1075/dapsac.55.01kle. 
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Koch, Gertraud / Franken, Lina</hi> (im Druck): Automatisierungspotenziale in der qualitativen Diskursanalyse. Das Prinzip des Filterns, in: Schilling, Samuel / Klimczak, Peter (Hg.): 
                        <hi rend=""italic"">Die Gesellschaft im Spiegellabyrinth sozialer Medien.</hi> Wiesbaden.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Sinclair, Stéfan / Rockwell, Geoffrey</hi> (2016):
  Voyant Tools. Web.
  <ptr target=""http://voyant-tools.org/""/>. 
</bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Wüest, Bruno / Clematide, Simon / Bünzli, Alexandra / Laupper, Daniel</hi> (2011): Semi-Automatic Core Sentence Analysis: Improving Content Analysis for Electoral Campaign Research, in: 
  <hi rend=""italic"">International Relations Online Working
  Paper</hi>(1).
  <ref target=""https://www.sowi.uni-stuttgart.de/dokumente/forschung/irowp/IROWP_Series_2011_1_Wueest_Clematide_Buenzli_Laupper_Content_Analysis.pdf"">https://www.sowi.uni-stuttgart.de/dokumente/forschung/irowp/IROWP_Series_2011_1_Wueest_Clematide_Buenzli_Laupper_Content_Analysis.pdf</ref>.
</bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,interpretation;kollokationen;syntax,German,annotieren;inhaltsanalyse;kontextsetzung;methoden;text,2020-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <p style=""text-align:left; "">viele verfahren des text mining und distant reading beschränken sich auf eine wortbasierte auswertung von texten. auch wenn auf basis der wortformen und ihrer linearen abfolge bereits neue perspektiven auf texte gewonnen werden (z. b. mittels der voyant tools, sinclair / rockwell 2016), schöpfen diese methoden das potential von texten bei weitem nicht aus. insbesondere, wenn die auswertungsergebnisse gegenstand weiterführender interpretationen werden, z. b. um soziale phänomene zu beschreiben, sehen wir einen mehrwert in der auswertung zusätzlicher sprachlicher strukturen. konkret verwenden wir syntaktische annotationen, die präzisere informationen zu wortkombinationen liefern können, etwa „x ist subjekt von y“ anstelle von „x steht im kontext von y“. zudem bestehen viele syntaktische relationen über eine längere distanz an der oberfläche hinweg und können deshalb nur durch eine syntaktische perspektive erfasst werden (z. b. andresen / zinsmeister 2017). dies gilt für unterschiedliche sprachen in unterschiedlichem ausmaß. das deutsche verfügt über deutlich mehr distanzstrukturen als das englische, für das die meisten analyseverfahren ursprünglich entwickelt wurden.</p>
         <p style=""text-align:left; "">in diesem beitrag vergleichen wir zwei ansätze zur berechnung von kollokationen, einen oberflächenorientierten ansatz und einen auf dependenzannotationen basierten. an zwei fallstudien aus den fächern kulturanthropologie und pflegewissenschaft wird demonstriert, wie die beiden ansätze eine qualitative interpretation von textdaten in hinblick auf gesellschaftliche bzw. soziale phänomene unterstützen können. die erstellung eines eindeutigen goldstandards, der eine formale evaluation erlauben würde, ist bei dieser art fragestellung nicht möglich. stattdessen wird auf qualitative weise das potential dieser analyse zur bearbeitung der geistes- und sozialwissenschaftlichen fragestellungen beschrieben.</p>
         <div rend=""dh-heading1"" type=""div1"">
            <head>syntaktische profile: forschungsstand</head>
            <p style=""text-align:left; "">die nutzung syntaktischer informationen zur charakterisierung von wortverwendungen ist vor allem in der lexikografie betrieben worden. populär wurde das konzept unter dem namen „word sketch“ besonders durch die korpuslinguistische software sketchengine (kilgarriff et al. 2004, kilgarriff et al. 2014). für ein gegebenes suchwort wird hier angegeben, welche anderen wörter besonders häufig in spezifischen syntaktischen relationen zum suchwort stehen, z. b. 
                    <hi rend=""italic"">glimpse</hi> als frequentes objekt von 
                    <hi rend=""italic"">catch</hi> (kilgarriff et al. 2014: 9). im digitalen wörterbuch der deutschen sprache (dwds) kann eine entsprechende darstellung als sog. dwds-wortprofil abgerufen werden (geyken 2011).
                </p>
            <p style=""text-align:left; "">weitere anwendungen gibt es in der literaturwissenschaft und linguistik: googasian / heuser (2019) vergleichen die syntaktischen kontexte von menschen und tieren in einem korpus sog. „wild animal stories“. andresen (2018) nutzt syntaktische n-gramme für einen vergleich der wissenschaftssprachen in den fächern literaturwissenschaft und linguistik. eine anwendung auf sozialwissenschaftliche fragestellungen erfolgt vor allem in den politikwissenschaften: anhand syntaktischer muster wie z. b. argumentstrukturen oder mustern der redewiedergabe werden hier politische akteure und ihre positionen identifiziert, miteinander in relation gesetzt und so diskurse charakterisiert (van atteveldt et al. 2008, kleinnijenhuis / van atteveldt 2014, wüest et al. 2011, blessing et al. 2013). in den fächern pflegewissenschaft und kulturanthropologie steht die exploration des mehrwertes syntaktischer analysen noch aus.</p>
         </div>
         <div rend=""dh-heading1"" type=""div1"">
            <head>daten und fragestellungen der fallstudien</head>
            <p style=""text-align:left; "">das potential syntaktisch definierter kollokationen wird an zwei fallstudien mit komplementärer datenlage erprobt. die erste nutzt ein großes korpus geschriebener sprache, das dadurch methodisch eine sehr sichere grundlage bietet. die zweite fallstudie basiert auf einem eher kleinen korpus mit gesprochener sprache, was mehr methodische herausforderungen erwarten lässt.</p>
            <p style=""text-align:left; "">die kulturanthropologische fallstudie befasst sich mit dem themenkomplex der telemedizin und insbesondere der frage nach der (nicht-)akzeptanz telemedizinischer anwendungen durch unterschiedliche gesellschaftliche akteursgruppen. das hierfür erstellte korpus umfasst 8.784 texte mit insgesamt 14,8 mio. token und basiert auf einem webcrawling (adelmann / franken 2020). dafür wurden webseiten von krankenkassen, ärzte- und patientenverbänden als ausgangspunkt genutzt und dann links zu seiten verfolgt, die mindestens eines von mehreren wörtern aus einem wortfeld zur telemedizin enthalten (koch / franken im druck). </p>
            <p style=""text-align:left; "">grundlage der pflegewissenschaftlichen fallstudie ist ein korpus aus 31 dialogen, die mit schwerkranken und sterbenden menschen in palliativer versorgung geführt wurden. es handelt sich um transkripte gesprochener sprache im umfang von gut 100.000 token. gegenstand der studie sind die deutungen von entscheidungen hinsichtlich der gesundheitlichen versorgung der betroffenen.</p>
         </div>
         <div rend=""dh-heading1"" type=""div1"">
            <head>methode</head>
            <p style=""text-align:left; "">die texte beider korpora werden mithilfe
des parsers mate (bohnet 2010), trainiert auf der hamburger dependency
treebank (foth et al. 2014), mit lemmata, wortarten und syntaktischen
dependenzen annotiert. unter kollokationen verstehen wir „a
combination of two words that exhibit a tendency to occur near each
other in natural language“ (evert 2008: 1214). bei der
operationalisierung von „near each other“ können kriterien an der
textoberfläche oder syntaktische kriterien angesetzt werden: für den
einfachen ansatz ohne annotationen betrachten wir wörter in einem
kontextfenster von +/- 3 wörtern als benachbart, für den syntaktischen
ansatz wörter mit einer direkten dependenzrelation. in beiden fällen
wird mithilfe des log-likelihood-ratios (llr, dunning 1993) berechnet,
welche kombinationen häufiger im korpus vorkommen, als basierend auf
den einzelfrequenzen der wörter zu erwarten wäre. im falle der
syntaktischen kollokationen werden dafür die einzelfrequenzen in der
spezifischen syntaktischen relation genutzt. die ergebnisse basieren
auf den lemmata und werden nach schlüsselwörtern gefiltert, die für
die jeweiligen fragestellungen als bedeutsam ausgewählt wurden
(<hi rend=""italic"">t</hi>/<hi rend=""italic"">telemed</hi> bzw. 
<hi rend=""italic"">e</hi>/<hi rend=""italic"">entscheid</hi>). das hierfür verwendete analyseskript steht auf github zur verfügung.<ref n=""1"" target=""ftn1""/> für die interpretation werden die top 10 beider listen verglichen und nach bedarf weitere einträge gesichtet.
</p>
         </div>
         <div rend=""dh-heading1"" type=""div1"">
            <head>ergebnisse der fallstudien</head>
            <div rend=""dh-heading2"" type=""div2"">
               <head>kulturanthropologie</head>
               <p style=""text-align:left; "">tabelle 1 zeigt die oberflächenbasierten kollokationen zu lemmata mit 
<hi rend=""italic"">t</hi>/<hi rend=""italic"">telemed</hi> im kulturanthropologischen korpus mit den höchsten llr-werten. 
<hi rend=""italic"">telemedizin</hi> ist sehr stark mit dem verwandten wort 
<hi rend=""italic"">telematik</hi> assoziiert, was die enge verknüpfung
der bereiche anzeigt. manche wortpaare sind bestandteil mehrteiliger
eigennamen
(<hi rend=""italic"">bayerische telemedallianz, [zentrum für telematik
und] telemedizin gmbh</hi>), die für die interpretation einen
eingeschränkten mehrwert haben,
aber doch für den diskurs potentiell relevante und ggf. bisher unbekannte akteure sichtbar machen. mit dem 
<hi rend=""italic"">tag der telemedizin</hi> wird ein fachkongress als wichtiger begegnungspunkt dieser akteure aufgeführt. außerdem liegen allgemeine konzepte wie 
<hi rend=""italic"">telemedizisch</hi> und 
<hi rend=""italic"">anwendung</hi> hoch im ranking.
</p>
               <table rend=""rules"">
                  <head> tabelle 1: top 10 der oberflächenbasierten kollokationen zu
  lemmata mit <hi rend=""italic"">t</hi>/ <hi rend=""italic"">telemed</hi> im kulturanthropologischen korpus</head>
                  <row>
                     <cell rend=""dh-default"">
                        <hi rend=""bold"">wort 1</hi>
                     </cell>
                     <cell rend=""dh-default"">
                        <hi rend=""bold"">wort 2</hi>
                     </cell>
                     <cell rend=""dh-default"">llr</cell>
                     <cell rend=""dh-default"">
                        <hi rend=""bold"">abs. frequenz</hi>
                     </cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">telematik</cell>
                     <cell rend=""dh-default"">telemedizin</cell>
                     <cell rend=""dh-default"">2044,88</cell>
                     <cell rend=""dh-default"">468</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">telemedizinisch</cell>
                     <cell rend=""dh-default"">anwendung</cell>
                     <cell rend=""dh-default"">1753,90</cell>
                     <cell rend=""dh-default"">465</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">bayerisch</cell>
                     <cell rend=""dh-default"">telemedallianz</cell>
                     <cell rend=""dh-default"">1497,94</cell>
                     <cell rend=""dh-default"">204</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">telemedizin</cell>
                     <cell rend=""dh-default"">gmbh</cell>
                     <cell rend=""dh-default"">1007,39</cell>
                     <cell rend=""dh-default"">340</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">tag</cell>
                     <cell rend=""dh-default"">telemedizin</cell>
                     <cell rend=""dh-default"">845,74</cell>
                     <cell rend=""dh-default"">274</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">telemedizinisch</cell>
                     <cell rend=""dh-default"">betreuung</cell>
                     <cell rend=""dh-default"">841,88</cell>
                     <cell rend=""dh-default"">212</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">bayerisch</cell>
                     <cell rend=""dh-default"">telemedallianz</cell>
                     <cell rend=""dh-default"">731,35</cell>
                     <cell rend=""dh-default"">97</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">der</cell>
                     <cell rend=""dh-default"">telemedizin</cell>
                     <cell rend=""dh-default"">644,28</cell>
                     <cell rend=""dh-default"">2533</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">gesellschaft</cell>
                     <cell rend=""dh-default"">telemedizin</cell>
                     <cell rend=""dh-default"">632,95</cell>
                     <cell rend=""dh-default"">241</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">telemedizinisch</cell>
                     <cell rend=""dh-default"">zentrum</cell>
                     <cell rend=""dh-default"">585,73</cell>
                     <cell rend=""dh-default"">165</cell>
                  </row>
               </table>
               <table rend=""rules"">
                  <head> tabelle 2: top 10 der syntaxbasierten kollokationen zu lemmata mit <hi rend=""italic"">t</hi>/<hi rend=""italic"">telemed</hi> im kulturanthropologischen korpus</head>
                  <row>
                     <cell rend=""dh-default"">
                        <hi rend=""bold"">wort 1</hi>
                     </cell>
                     <cell rend=""dh-default"">relation</cell>
                     <cell rend=""dh-default"">
                        <hi rend=""bold"">wort 2</hi>
                     </cell>
                     <cell rend=""dh-default"">llr</cell>
                     <cell rend=""dh-default"">
                        <hi rend=""bold"">abs. frequenz</hi>
                     </cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">gmbh</cell>
                     <cell rend=""dh-default"">ist apposition von</cell>
                     <cell rend=""dh-default"">telemedizin</cell>
                     <cell rend=""dh-default"">2261,71</cell>
                     <cell rend=""dh-default"">353</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">telemedizinisch</cell>
                     <cell rend=""dh-default"">ist attribut von</cell>
                     <cell rend=""dh-default"">anwendung</cell>
                     <cell rend=""dh-default"">2236,42</cell>
                     <cell rend=""dh-default"">456</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">bayerisch</cell>
                     <cell rend=""dh-default"">ist attribut von</cell>
                     <cell rend=""dh-default"">telemedallianz</cell>
                     <cell rend=""dh-default"">2002,48</cell>
                     <cell rend=""dh-default"">204</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">telemedizin</cell>
                     <cell rend=""dh-default"">ist genitivattribut von</cell>
                     <cell rend=""dh-default"">tag</cell>
                     <cell rend=""dh-default"">1904,26</cell>
                     <cell rend=""dh-default"">274</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">telemedizinisch</cell>
                     <cell rend=""dh-default"">ist attribut von</cell>
                     <cell rend=""dh-default"">betreuung</cell>
                     <cell rend=""dh-default"">981,87</cell>
                     <cell rend=""dh-default"">200</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">bayerisch</cell>
                     <cell rend=""dh-default"">ist attribut von</cell>
                     <cell rend=""dh-default"">telemedallianz</cell>
                     <cell rend=""dh-default"">967,93</cell>
                     <cell rend=""dh-default"">98</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">dgtelemed</cell>
                     <cell rend=""dh-default"">ist apposition von</cell>
                     <cell rend=""dh-default"">v.</cell>
                     <cell rend=""dh-default"">899,83</cell>
                     <cell rend=""dh-default"">84</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">telemedizinisch</cell>
                     <cell rend=""dh-default"">ist attribut von</cell>
                     <cell rend=""dh-default"">zentrum</cell>
                     <cell rend=""dh-default"">772,92</cell>
                     <cell rend=""dh-default"">163</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">telemedizin</cell>
                     <cell rend=""dh-default"">ist apposition von</cell>
                     <cell rend=""dh-default"">fachkongress</cell>
                     <cell rend=""dh-default"">727,37</cell>
                     <cell rend=""dh-default"">64</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">telemedizin</cell>
                     <cell rend=""dh-default"">ist genitivattribut von</cell>
                     <cell rend=""dh-default"">möglichkeit</cell>
                     <cell rend=""dh-default"">720,75</cell>
                     <cell rend=""dh-default"">144</cell>
                  </row>
               </table>
               <p style=""text-align:left; "">die syntaktischen informationen in tabelle 2 machen den zusammenhang zwischen den bestandteilen der eigennamen in der relation der apposition explizit und bieten damit mehr informationen zur einordnung dieser datenpunkte. die annotationen ermöglichen außerdem, die gesamtliste nach bestimmten syntaktischen relationen zu filtern. die genannten appositionen beispielsweise können anhand des relationslabels ausgeblendet werden. auch hier gibt es sehr allgemeine konzepte wie 
<hi rend=""italic"">telemedizinisch</hi> als attribut von 
<hi rend=""italic"">anwendung</hi>, die zwar frequent, aber nicht sehr informativ sind. 
<hi rend=""italic"">telemedizin</hi> als genitivattribut von 
<hi rend=""italic"">möglichkeit</hi> weist daraufhin, dass eben deren möglichkeiten noch gegenstand des diskurses sind. in der durchsicht der kollokationen jenseits der top 10 finden sich verwandte themen des potentials und der projekthaftigkeit (<hi rend=""italic"">potential der telemedizin</hi>, 
<hi rend=""italic"">telemedizin-typisches potential</hi>, 
<hi rend=""italic"">evaluiertes telemedizinprojekt</hi>, 
<hi rend=""italic"">vielversprechendes telemedizinprojekt</hi>), die anzeigen, dass sich die umsetzung der telemedizin in einer frühen phase befindet und ihre akzeptanz als regelversorgung noch nicht abschließend verhandelt ist.
</p>
               <p style=""text-align:left; "">auch zur kollokation 
<hi rend=""italic"">telemedizinisch</hi> als attribut von 
<hi rend=""italic"">betreuung</hi> finden sich weiter unten im ranking
ähnliche verwendungen zum thema betreuung
(<hi rend=""italic"">telemedizinisch betreuen</hi>, 
<hi rend=""italic"">telemedizinisch betreut</hi> …) und unterstützung
(<hi rend=""italic"">telemedizinisch unterstützt</hi>, 
<hi rend=""italic"">telemedizinisch-unterstützte</hi> (sic!) 
<hi rend=""italic"">versorgung</hi>, 
<hi rend=""italic"">telematikunterstützung</hi> …). dies weist auf die (bisher) eher ergänzende rolle der telemedizin im verhältnis zur medizinischen regelversorgung hin. 
</p>
               <p>  <lb/>  <lb/>  <lb/> </p>
            </div>
            <div rend=""dh-heading2"" type=""div2"">
               <head>pflegewissenschaft</head>
               <p style=""text-align:left; "">tabelle 3 zeigt die zehn ersten oberflächenbasierten kollokationen des dialogkorpus zu lemmata mit 
<hi rend=""italic"">e</hi>/<hi rend=""italic"">entscheid</hi>. hier werden zunächst probleme in den daten deutlich: mit 
<hi rend=""italic"">finan/</hi> liegt ein für gesprochene sprache typischer abbruch eines wortes (vermutlich: 
<hi rend=""italic"">finanzentscheidung</hi>) vor. zudem ist 
<hi rend=""italic"">entscheidungsvariant</hi> eine fehlerhafte lemmaform zu 
<hi rend=""italic"">entscheidungsvarianten</hi>. insgesamt sind die frequenzen aufgrund der geringen korpusgröße klein, lassen aber trotzdem hilfreiche schlüsse für die analyse zu. im syntaxbasierten gegenstück in tabelle 4 sind zusätzliche probleme erkennbar, die durch die automatische verarbeitung gesprochener sprache entstehen. die relation zwischen 
<hi rend=""italic"">hab</hi> und 
<hi rend=""italic"">entscheiden</hi> ist fehlerhaft als adverbial (korrekt: auxiliar) bezeichnet. allerdings wird ein direkter zusammenhang zwischen diesen wörtern erst durch die syntaktischen annotationen überhaupt erkennbar, da sie im satz häufig nicht benachbart stehen. zusätzlich gibt es vollständig falsche analysen wie die relation zwischen 
<hi rend=""italic"">entscheidend</hi> und 
<hi rend=""italic"">puh</hi>.
</p>
               <table rend=""rules"">
                  <head> tabelle 3: top 10 der oberflächenbasierten kollokationen zu lemmata mit 
                        <hi rend=""italic"">e</hi>/<hi rend=""italic"">entscheid</hi> im pflegewissenschaftlichen korpus</head>
                  <row>
                     <cell rend=""dh-default"">
                        <hi rend=""bold"">wort 1</hi>
                     </cell>
                     <cell rend=""dh-default"">
                        <hi rend=""bold"">wort 2</hi>
                     </cell>
                     <cell rend=""dh-default"">llr</cell>
                     <cell rend=""dh-default"">
                        <hi rend=""bold"">abs. frequenz</hi>
                     </cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">entscheidung</cell>
                     <cell rend=""dh-default"">treffen</cell>
                     <cell rend=""dh-default"">33,41</cell>
                     <cell rend=""dh-default"">7</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">richtig</cell>
                     <cell rend=""dh-default"">entscheidung</cell>
                     <cell rend=""dh-default"">23,06</cell>
                     <cell rend=""dh-default"">7</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">tablettenform</cell>
                     <cell rend=""dh-default"">entscheiden</cell>
                     <cell rend=""dh-default"">21,28</cell>
                     <cell rend=""dh-default"">4</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">der</cell>
                     <cell rend=""dh-default"">entscheidung</cell>
                     <cell rend=""dh-default"">17,54</cell>
                     <cell rend=""dh-default"">28</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">dieser</cell>
                     <cell rend=""dh-default"">entscheidung</cell>
                     <cell rend=""dh-default"">13,02</cell>
                     <cell rend=""dh-default"">7</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">selbst</cell>
                     <cell rend=""dh-default"">entscheiden</cell>
                     <cell rend=""dh-default"">12,88</cell>
                     <cell rend=""dh-default"">4</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">entscheidung</cell>
                     <cell rend=""dh-default"">überlassen</cell>
                     <cell rend=""dh-default"">10,87</cell>
                     <cell rend=""dh-default"">2</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">entscheidungsvariant</cell>
                     <cell rend=""dh-default"">nebeneinandstellen</cell>
                     <cell rend=""dh-default"">10,38</cell>
                     <cell rend=""dh-default"">1</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">finan/</cell>
                     <cell rend=""dh-default"">versorgungsentscheidung</cell>
                     <cell rend=""dh-default"">10,38</cell>
                     <cell rend=""dh-default"">1</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">entschieden</cell>
                     <cell rend=""dh-default"">abraten</cell>
                     <cell rend=""dh-default"">10,38</cell>
                     <cell rend=""dh-default"">1</cell>
                  </row>
               </table>
               <p style=""text-align:left; "">auch für die pflegewissenschaftliche interpretation bieten die kollokationen mit den höchsten llr-werten erste anhaltspunkte, die dann durch eine sichtung der weiteren rangplätze ergänzt werden können. die häufigsten kollokatoren von entscheidungen stehen für eine realisierung eigener entscheidungen der betroffenen. die kollokation 
<hi rend=""italic"">richtig</hi> macht bewertungen der entscheidungen sichtbar. es zeigen sich zudem gegensätzliche dimensionen des phänomens, wie „selber entscheiden“ vs. „entscheidung abgeben“, die hinter der kollokation mit 
<hi rend=""italic"">überlassen</hi> stehen. insbesondere die subjekt-
und objektrelationen
(<hi rend=""italic"">entscheidung treffen</hi>, 
<hi rend=""italic"">entschluss entstehen</hi>, 
<hi rend=""italic"">entscheidung überlassen</hi>) sind durch die syntaktische analyse adäquater und theoretisch fundierter abgebildet. dieser nutzen wird jedoch durch fehler in der automatischen annotation eingeschränkt. zudem werden diese relationen in der gesprochenen sprache mit kürzeren sätzen möglicherweise auch durch die oberflächenbasierte analyse besser erfasst als in anderen sprachlichen registern. insgesamt betrachtet werden durch den quantitativen zugang verwendungszusammenhänge des phänomens „entscheidung“ transparent, die wiederum auf wichtige handlungskontexte in der versorgungsrealität von schwerkranken und sterbenden menschen verweisen. 
</p>
               <table rend=""rules"">
                  <head>tabelle 4: top 10 der syntaxbasierten kollokationen zu lemmata mit 
                        <hi rend=""italic"">e</hi>/
                        <hi rend=""italic"">entscheid</hi> im pflegewissenschaftlichen korpus </head>
                  <row>
                     <cell rend=""dh-default"">
                        <hi rend=""bold"">wort 1</hi>
                     </cell>
                     <cell rend=""dh-default"">relation</cell>
                     <cell rend=""dh-default"">
                        <hi rend=""bold"">wort 2</hi>
                     </cell>
                     <cell rend=""dh-default"">llr</cell>
                     <cell rend=""dh-default"">
                        <hi rend=""bold"">abs. frequenz</hi>
                     </cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">entscheiden</cell>
                     <cell rend=""dh-default"">ist adverbial von</cell>
                     <cell rend=""dh-default"">hab</cell>
                     <cell rend=""dh-default"">33,25</cell>
                     <cell rend=""dh-default"">9</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">richtig</cell>
                     <cell rend=""dh-default"">ist attribut von</cell>
                     <cell rend=""dh-default"">entscheidung</cell>
                     <cell rend=""dh-default"">24,54</cell>
                     <cell rend=""dh-default"">6</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">entscheidung</cell>
                     <cell rend=""dh-default"">ist akkusativobjekt von</cell>
                     <cell rend=""dh-default"">treffen</cell>
                     <cell rend=""dh-default"">23,47</cell>
                     <cell rend=""dh-default"">4</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">entschluss</cell>
                     <cell rend=""dh-default"">ist subjekt von</cell>
                     <cell rend=""dh-default"">entstehen</cell>
                     <cell rend=""dh-default"">21,93</cell>
                     <cell rend=""dh-default"">2</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">selbst</cell>
                     <cell rend=""dh-default"">ist adverbial von</cell>
                     <cell rend=""dh-default"">entscheiden</cell>
                     <cell rend=""dh-default"">18,74</cell>
                     <cell rend=""dh-default"">4</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">entscheidend</cell>
                     <cell rend=""dh-default"">ist adverbial von</cell>
                     <cell rend=""dh-default"">puh</cell>
                     <cell rend=""dh-default"">16,42</cell>
                     <cell rend=""dh-default"">1</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">tablettenform</cell>
                     <cell rend=""dh-default"">ist subjekt von</cell>
                     <cell rend=""dh-default"">entscheiden</cell>
                     <cell rend=""dh-default"">16,13</cell>
                     <cell rend=""dh-default"">2</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">entscheidung</cell>
                     <cell rend=""dh-default"">ist akkusativobjekt von</cell>
                     <cell rend=""dh-default"">überlassen</cell>
                     <cell rend=""dh-default"">15,59</cell>
                     <cell rend=""dh-default"">2</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">entscheidung</cell>
                     <cell rend=""dh-default"">ist akkusativobjekt von</cell>
                     <cell rend=""dh-default"">treff</cell>
                     <cell rend=""dh-default"">13,52</cell>
                     <cell rend=""dh-default"">2</cell>
                  </row>
                  <row>
                     <cell rend=""dh-default"">für</cell>
                     <cell rend=""dh-default"">ist präposition zu</cell>
                     <cell rend=""dh-default"">entscheiden</cell>
                     <cell rend=""dh-default"">13,46</cell>
                     <cell rend=""dh-default"">7</cell>
                  </row>
               </table>
            </div>
         </div>
         <div rend=""dh-heading1"" type=""div1"">
            <head>schlussfolgerungen</head>
            <p style=""text-align:left; "">es hat sich gezeigt, dass die berechnung von kollokationen auf der grundlage der sprachlichen oberfläche bzw. der syntax für qualitative fragestellungen informativ sein kann. für die auswertung einer spezifischen fragestellung ist die assoziationsstärke allein jedoch nicht immer das entscheidende kriterium. die kollokationen geben hinweise auf zusammenhänge innerhalb des korpus, die neue fragestellungen und perspektiven generieren können. gleichzeitig werden durch die relationsannotationen bereits kleine datenmengen in erweiterter form auswertbar.</p>
            <p style=""text-align:left; "">die beispielhaften analysen haben gezeigt, dass die syntaktischen annotationen eine für die interpretation hilfreiche differenzierung bieten, indem präziser angegeben wird, in welcher relation zwei wörter stehen. das ermöglicht auch das filtern nach interessanten relationstypen. zudem werden durch den einbezug der syntax relationen zwischen wörtern in distanzstellung sichtbar, was insbesondere vom verb abhängige satzteile besser sichtbar macht. andererseits erfordern die syntaktischen annotationen eine aufwendigere vorverarbeitung, die mehr zeit und technische fähigkeiten erfordert. außerdem stellen sie eine zusätzliche fehlerquelle dar. dies gilt besonders für die gesprochensprachlichen daten. eine systematische überprüfung und rückbindung an konkrete korpusbelege ist deshalb wichtig und verbessert die interpretationsmöglichkeiten aus qualitativer sicht. </p>
            <p style=""text-align:left; "">anschließend an diese arbeiten ist geplant, stärker kontexte zu aggregieren: in grammatischer hinsicht wird das durch koreferenzannotationen erfolgen, die für das pflegewissenschaftliche korpus bereits vorliegen. auf semantischer ebene verfolgen wir den ansatz, wortgruppen zu konzepten zusammenzufassen, z. b. können 
<hi rend=""italic"">ärztin</hi>, 
<hi rend=""italic"">arzt</hi>, 
<hi rend=""italic"">hausarzt</hi>, 
<hi rend=""italic"">onkologin</hi> usw. auf ein gemeinsames konzept ärzt*innen abgebildet werden (vgl. den ansatz von wüest et al. 2011). wir sehen außerdem weitere anwendungsfälle über die fächergrenzen hinweg, etwa zur literaturwissenschaftlichen beschreibung von geschlechterzuschreibungen, indem die sprachlichen kontexte weiblicher und männlicher vornamen verglichen werden.
</p>
         </div>
      </body>
      <back>
         <div type=""notes"">
            <note n=""1"" rend=""footnote text"" id=""ftn1"">
               <ptr target=""https://github.com/melandresen/dhd2020""/>
            </note>
         </div>
         <div type=""bibliogr"">
            <listbibl>
               <head>bibliographie</head>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">adelmann, benedikt / franken, lina</hi> (2020): thematic web crawling and scraping as a way to form focussed web archives, in: 
                        <hi rend=""italic"">engaging with web archives conference book of abstracts</hi>. to be published at  
                        <ref target=""https://ewaconference.com/"">https://ewaconference.com/</ref>.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">andresen, melanie</hi> (2018): sprachliche variation in der germanistik: eine n-gramm-basierte stilanalyse, in: 
                        <hi rend=""italic"">book of abstracts of dhd 2018</hi>. köln, deutschland, 311–15.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">andresen, melanie / zinsmeister, heike</hi> (2017): the benefit of syntactic vs. linear n-grams for linguistic description, in: 
                        <hi rend=""italic"">proceedings of the 4th international conference on dependency linguistics (depling 2017).</hi> pisa, italy, 4–14 http://aclweb.org/anthology/w17-6503.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">van atteveldt, wouter / kleinnijenhuis, jan / ruigrok, nel</hi> (2008): parsing, semantic networks, and political authority using syntactic analysis to extract semantic relations from dutch newspaper articles, in: 
                        <hi rend=""italic"">political analysis</hi>, 16(4): 428–46 doi:10.1093/pan/mpn006.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">blessing, andre / sonntag, jonathan / kliche, fritz / heid, ulrich / kuhn, jonas / stede, manfred</hi> (2013): towards a tool for interactive concept building for large scale analysis in the humanities, in: 
                        <hi rend=""italic"">proceedings of the 7th workshop on language technology for cultural heritage, social sciences, and humanities</hi>. 55–64 https://www.aclweb.org/anthology/w13-2708.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"" space=""preserve"">bohnet, bernd </hi>(2010): very high accuracy and fast dependency parsing is not a contradiction, in: 
                        <hi rend=""italic"">proceedings of the 23rd international conference on computational linguistics (coling 2010).</hi> beijing, china, 89–97. https://www.aclweb.org/anthology/c10-1011
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">dunning, ted</hi> (1993): accurate methods for the statistics of surprise and coincidence, in: 
                        <hi rend=""italic"">computational linguistics</hi>, 19(1): 61–74. https://www.aclweb.org/anthology/j93-1003
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">evert, stefan</hi> (2008): corpora and collocations, in: lüdeling, anke / kytö, merja (hg.), 
                        <hi rend=""italic"">corpus linguistics: an international handbook</hi>, vol. 2. (handbücher zur sprach- und kommunikationswissenschaft 29). berlin, boston: de gruyter, 1212–1248.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">foth, kilian a. / köhn, arne / beuck, niels / menzel, wolfgang</hi> (2014): because size does matter: the hamburg dependency treebank, in: 
                        <hi rend=""italic"">proceedings of the ninth international conference on language resources and evaluation (lrec’14)</hi>, 2326–2333. reykjavik, iceland. 
                        <ref target=""http://www.lrec-conf.org/proceedings/lrec2014/pdf/860_paper.pdf"">http://www.lrec-conf.org/proceedings/lrec2014/pdf/860_paper.pdf</ref>.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">gaidys, uta / gius, evelyn / jarchow, margarete / koch, gertraud / menzel, wolfgang / orth, dominik / zinsmeister, heike</hi> (2017): herma: automated modelling of hermeneutic processes, in: 
                        <hi rend=""italic"">hamburger journal für kulturanthropologie</hi>(7): 119–23.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">geyken, alexander</hi> (2011): statistische wortprofile zur schnellen analyse der syntagmatik in textkorpora, in: abel, andrea / zanin, renata (hg.), 
                        <hi rend=""italic"">korpora in lehre und forschung</hi>. bozen: bolzano univ. press, 129–54.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">googasian, victoria / heuser, ryan j.</hi> (2019): digital animal studies: modeling anthropomorphism in animal writing, 1870-1930, in: 
                        <hi rend=""italic"">book of abstracts of dh 2019</hi> https://dev.clariah.nl/files/dh2019/boa/0458.html.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">kilgarriff, adam / baisa, vít / bušta, jan / jakubíček, miloš / kovář, vojtěch / michelfeit, jan, rychlý / pavel. / suchomel, vít</hi> (2014): the sketch engine: ten years on, in: 
                        <hi rend=""italic"">lexicography</hi>, 1(1): 7–36 doi:10.1007/s40607-014-0009-9.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">kilgarriff, adam / rychlý, pavel / smrz, pavel / tugwell, david</hi> (2004): the sketch engine: in: 
                        <hi rend=""italic"">proceedings of the 11th euralex international congress</hi>. 105–15 https://euralex.org/publications/the-sketch-engine/.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">kleinnijenhuis, jan / van atteveldt, wouter</hi> (2014): positions of parties and political cleavages between parties in texts, in: kaal, bertie / maks, isa / van elfrinkhof, annemarie (hg.), 
                        <hi rend=""italic"">discourse approaches to politics, society and culture</hi>, vol. 55. amsterdam: benjamins, 1–20 doi:10.1075/dapsac.55.01kle. 
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">koch, gertraud / franken, lina</hi> (im druck): automatisierungspotenziale in der qualitativen diskursanalyse. das prinzip des filterns, in: schilling, samuel / klimczak, peter (hg.): 
                        <hi rend=""italic"">die gesellschaft im spiegellabyrinth sozialer medien.</hi> wiesbaden.
                    </bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">sinclair, stéfan / rockwell, geoffrey</hi> (2016):
  voyant tools. web.
  <ptr target=""http://voyant-tools.org/""/>. 
</bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">wüest, bruno / clematide, simon / bünzli, alexandra / laupper, daniel</hi> (2011): semi-automatic core sentence analysis: improving content analysis for electoral campaign research, in: 
  <hi rend=""italic"">international relations online working
  paper</hi>(1).
  <ref target=""https://www.sowi.uni-stuttgart.de/dokumente/forschung/irowp/irowp_series_2011_1_wueest_clematide_buenzli_laupper_content_analysis.pdf"">https://www.sowi.uni-stuttgart.de/dokumente/forschung/irowp/irowp_series_2011_1_wueest_clematide_buenzli_laupper_content_analysis.pdf</ref>.
</bibl>
            </listbibl>
         </div>
      </back>
   </text>

",2.0,3.0,Voyant
11075,2020 - University of Paderborn,University of Paderborn,Digital Humanities zwischen Modellierung und Interpretation,2020,DHd,DHd,Universität Paderborn,Paderborn,,Germany,https://zenodo.org/record/3666690,"Der Datenpool eines frühneuzeitlichen Self-Trackers, oder: Johann Christian Senckenbergs „Observationes“. Ein Distant Reading-Zugang",,Vera Faßhauer,poster / demo / art installation,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Einleitung</head>
            <p>
               <hi style=""font-size:12pt"" space=""preserve"">Aktuelle Diskussionen über ständig wachsende Möglichkeiten der Erfassung, Speicherung und Analyse großer Datenmengen lassen uns vergessen, dass sowohl Wissenschaft als auch Behörden bereits seit Jahrhunderten Praktiken zur Datenerhebung und -verarbeitung entwickelt haben (Borck 2017, Oertzen 2017). So wurden bereits im 17. Jahrhundert astronomische und meteorologische Beobachtungsdaten in Formularen und Tabellen erfasst, in Zahlen und Symbolen kodiert und in Karten und Diagrammen visualisiert (Daston 2011, Mendelsohn 2011, Hess 2011). Auch die empirische Erfassung von personenbezogenen Körperdaten nahm ihren Anfang in den Praxisjournalen frühneuzeitlicher Ärzte, die alle Symptome, Zustandsmerkmale und Reaktionen sowie den Krankheitsverlauf, die Medikamentierung und den Heilungsprozess ihrer Patienten genau dokumentierten (Geyer-Kordesch 1990, Stolberg 2007). Nicht einmal das von den Anhängern der </hi>
               <hi rend=""italic"" style=""font-size:12pt"">Quantified Self</hi>
               <hi style=""font-size:12pt"" space=""preserve"">-Bewegung praktizierte </hi>
               <hi rend=""italic"" style=""font-size:12pt"">Self-Tracking</hi>
               <hi style=""font-size:12pt"" space=""preserve""> (Lupton 2016) gehört gänzlich dem 21. Jahrhundert an, wie die Tagebücher zahlreicher religiöser Selbstoptimierer aus dem 17. und 18. Jahrhundert zeigen.</hi>
            </p>
            <p>
               <hi style=""font-size:12pt"" space=""preserve"">Eines der ambitioniertesten historischen </hi>
               <hi rend=""italic"" style=""font-size:12pt"">Self-Tracking</hi>
               <hi style=""font-size:12pt"" space=""preserve"">-Projekte waren zweifellos die </hi>
               <hi rend=""italic"" style=""font-size:12pt"">Observationes in me ipso factae</hi>
               <hi style=""font-size:12pt"" space=""preserve""> des pietistischen Arztes Johann Christian Senckenberg (1707–1772). In ihnen protokollierte er tagtäglich sein gesamtes Körper- und Seelenleben, um seinen Lebenswandel zu vervollkommnen. Neben Ernährung, Stoffwechsel, Körperaktivität sowie Schlaf- und Ruhephasen notierte er auch alle Reizempfindungen, Körperreflexe und Gemütszustände sowie alle spürbaren Umwelt- und Witterungseinflüsse (Faßhauer 2017). Zeitweise brachte er auf diese Weise täglich bis zu 5.000 Wörter zu Papier, so dass auf den Gesamtzeitraum von dreizehn Jahren gerechnet ca. 14.000 Seiten mit 12.600.000 Wörtern zusammenkamen. Der Beitrag beleuchtet zunächst den epistemischen Zweck dieses riesigen frühneuzeitlichen Datenpools, aus dem derzeit in Frankfurt ausgewählte Bände digital ediert werden (Faßhauer 2018), und setzt sie mit zeitgenössischen Positionen zum Verhältnis von Daten und Theorie in Beziehung. Anschließend wird die Möglichkeit einer Analyse dieser Daten mit modernen </hi>
               <hi rend=""italic"" style=""font-size:12pt"">Distant-reading</hi>
               <hi style=""font-size:12pt"">-Methoden und deren Vereinbarkeit mit dem epistemischen Ziel des religiösen Autors diskutiert.</hi>
            </p>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Selbstbeobachtung und Anti-Rationalismus</head>
            <p>
               <hi style=""font-size:12pt"">Wenn ein Selbstoptimierer des digitalen Zeitalters beschließt, seine Lebenszeit effizient zu nutzen, seinen Körper gesund zu erhalten oder seine Finanzen zu organisieren, bezweckt er damit meist größtmöglichen persönlichen Erfolg und Selbstzufriedenheit im Diesseits. Ein religiöser Self-Tracker des 18. Jahrhunderts hatte hingegen zu allererst sein Seelenheil und seine Erlösung nach dem Tod im Sinn. Diese konnte jedoch nur erlangen, wer die ihm anvertrauten Gottesgeschenke auf Erden treulich verwaltete, pflegte und mehrte. Genau wie für materielle Güter galt dies auch für Gesundheit und Wissenskapital. Nach Auffassung religiöser Gelehrter wie Senckenberg war der Mensch seit dem Sündenfall jedoch geistig so zerrüttet, dass er durch seine Verstandeskräfte zu keinen verlässlichen Erkenntnissen gelangen konnte. Insbesondere theoretische Modelle, die durch „künstliches syllogisiren der Vernunfft“ dem Verstand der Gelehrten („ex mente Doctorum“) entstiegen sind, repräsentierten nur fragmentarisches oder abstraktes Wissen. Zudem müssten ohnehin „alle Regulae universaliores erstlich ab experientia in particularibus“ abgeleitet werden, deren Vielfalt jedoch so viele Ausnahmen aufzeige, „daß die Regulae selbst wieder darüber zernichtet werden“ (Senckenberg 1735: 1r/v, Faßhauer 2017). Sichere medizinische Erkenntnisse waren deshalb nur „ohne Praeoccupation von einer vorher gefassten Hypothesi“ durch mehrfach wiederholte unmittelbare Selbsterfahrung zu erlangen, deren Resultate möglichst vollständig aufgezeichnet und induktiv ausgewertet werden mussten. Auf diese Weise ließen sich Vergleiche mit Aufzeichnungen aus ähnlichen Situationen herstellen, wobei einzelne Faktoren miteinander korreliert, auf ihre Relevanz und Rolle im Gesamtkontext befragt und als mögliche Ursachen oder Auswirkungen anderer Faktoren in Betracht gezogen werden konnten.</hi>
            </p>
            <p>
               <hi style=""font-size:12pt"">Ganz ähnliche Überzeugungen wie Senckenberg äußerte der amerikanische Journalist Chris Anderson, als er im Jahre 2008 das Ende der Theorie und den Beginn des Datenzeitalters verkündete. Auch er ging dabei von der Prämisse aus, dass Theorien die Realität nur verzerrt wiedergäben und letztlich allein in den Hirnen der Wissenschaftler existierten: „The scientific method is built around testable hypotheses. These models, for the most part, are systems visualized in the minds of scientists”. Stattdessen verwies er wie Senckenberg auf die Möglichkeit, durch Erhebung und Speicherung einer möglichst großen Datenfülle ungleich genauere und verlässlichere Aussagen über die Welt in ihrer ganzen Komplexität zu treffen. War der Verzicht auf die Suche nach letztgültigen Kausalitäten bei Senckenberg noch religiös motiviert, entspringt er bei Anderson aus der pragmatischen Erkenntnis, dass die Verfügbarkeit nie gekannter Datenmengen die Notwendigkeit zur Hypothesenbildung schlichtweg erübrige, da sie unter den verschiedensten Gesichtspunkten miteinander korreliert werden könnten. Durch ihre maschinelle Auswertbarkeit gerieten zudem auch Einzelheiten und Muster ins Blickfeld, die der theoriegeleiteten Forschung entgingen: „Correlation supersedes causation, and science can advance even without coherent models, unified theories, or really any mechanistic explanation at all“ (Anderson 2008). Seither wurde gegen Andersons These wiederholt eingewandt, dass bereits die Erstellung von Datensätzen auf theoretischen Prämissen und Selektionskriterien beruhe (boyd/Crawford 2012; Boellstorff 2014). Zudem stelle jede Datenanalyse eine subjektive Interpretation innerhalb bestimmter kultureller, dogmatischer oder ideologischer Kontexte dar, so dass eine hypothesenfreie Datenauswertung unmöglich sei. Die gleiche Problematik lässt sich auch für die Aufzeichnungen Senckenberg aufzeigen: Außer Thermometer und Barometer stand ihm nur sein eigenes Bewusstsein als Messinstanz zur Verfügung, das alle Empfindungen und Wahrnehmungen zwangsläufig subjektiv registrierte, filterte und interpretierte und dabei den protokollierten physischen und seelischen Befindlichkeiten selbst unterworfen war. Auch erfolgte die Aufzeichnung der Daten allein durch seine eigene schreibende Hand, die auf körperliche Irregularitäten ebenso empfindlich reagierte wie auf seelische Erschütterungen und Stimmungsschwankungen. Waren Körper und Bewusstsein anderweitig okkupiert, konnte die Datenerfassung entweder gar nicht oder nur rückwirkend und durch das Gedächtnis vermittelt erfolgen.</hi>
            </p>
            <figure>
               <graphic height=""15.77cm"" n=""1001"" rend=""block"" url=""255_final-e08a8c76be8648de2de651eec8073220.png"" width=""16.23cm""/>
               <head>Abbildungen 1a und 1b: Selbstbeobachtung in Senckenbergs Tagebuch, Dezember 1732 (Manuskript und TEI/XML-Transkription) </head>
            </figure>
         </div>
         <div rend=""DH-Heading1"" type=""div1"">
            <head>Datengetriebenes <hi rend=""italic"">Distant Reading</hi>?  </head>
            <p>
               <hi style=""font-size:12pt"" space=""preserve"">Senckenbergs riesiger Datenpool konfrontiert seine modernen Leser mit einem so mikroskopisch detaillierten Bewusstseinsstrom, dass ein Verständnis seiner Erkenntnisse im </hi>
               <hi rend=""italic"" style=""font-size:12pt"">close reading</hi>
               <hi style=""font-size:12pt"" space=""preserve"">-Modus nahezu unmöglich ist. Die digitale Erschließung einzelner Bände im Rahmen der </hi>
               <hi rend=""italic"" style=""font-size:12pt"">Frankfurter Auswahledition</hi>
               <hi style=""font-size:12pt"" space=""preserve""> ermöglicht nun eine Annäherung an das umfangreiche Textmaterial aus der Makroperspektive (Abb. 1a–b). Der Literaturwissenschaftler Franco Moretti hat dieses Vorgehen bekanntlich als „distant reading“ bezeichnet, da Distanz hier statt eines Hindernisses eine Bedingung der Erkenntnis darstelle: „it allows you to focus on units that are much smaller or much larger than the text“. Die Reduktion von Senckenbergs umfangreichen Beobachtungsdaten auf abstrakte Schemata scheint jedoch zunächst im Widerspruch zu seiner Absicht zu stehen, die ganze Vielfalt der natürlichen Erscheinungsformen unverkürzt zu erfassen. Auch Moretti hat auf dieses Problem hingewiesen: „If we want to understand the system in its entirety, we must accept losing something. We always pay a price for theoretical knowledge: reality is infinitely rich; concepts are abstract, are poor” (Moretti 2013: 48–49). Die irreversible Reduktion von Texten auf abstrakte Schemen, die in der Forschung sogar als gewaltsame Zerstörung des eigentlichen Untersuchungsgegenstandes beschrieben worden ist (Bradley 2012), kann nur in solchen Forschungsumgebungen vermieden werden, die – wie etwa die Voyant Tools (Sinclair und Rockwell 2003) – einen flexiblen Wechsel zwischen der Text- und der Grafikebene und damit zwischen dem </hi>
               <hi rend=""italic"" style=""font-size:12pt"">close</hi>
               <hi style=""font-size:12pt"" space=""preserve"">- und </hi>
               <hi rend=""italic"" style=""font-size:12pt"">distant reading</hi>
               <hi style=""font-size:12pt"" space=""preserve"">-Modus ermöglichen (Jänicke 2016: 20–23). Ein möglicher Ausgangspunkt für eine Fernlektüre ist die Identifizierung von Schlüsselwörtern, die hier anhand von Häufigkeitskriterien erfolgt. Werden in der Liste einzelne Keywords ausgewählt, kann deren Verteilung im Korpus angezeigt werden. Eine Visualisierung der markantesten körperlichen Empfindungen Senckenbergs zwischen August und Dezember 1732 zeigt zum Beispiel, dass im November und Dezember Spannungs- und Druckgefühle vorherrschten, während er im Oktober hauptsächlich Stiche und Zuckungen verspürte, im August und September aber frei von derlei Empfindungen war (Abb. 2). Ein Blick auf die Kollokationen dieser Begriffe zeigt, in welchen Körperteilen sie am häufigsten bemerkbar waren (Abb. 3). Allerdings stellen derlei Festlegungen auf bestimmte Untersuchungszeiträume oder Körperempfindungen bereits Arbeitshypothesen dar, die mit einer bestimmten Erwartungshaltung einhergehen und das Ergebnis dadurch nicht unmaßgeblich präformieren. Die oben aufgezeigte Unmöglichkeit des von Senckenberg projektierten theoriefreien Wissenserwerbs spiegelt sich deshalb unmittelbar in der digitalen Korpusanalyse wider, die gleichfalls nicht rein datengetrieben bzw. ohne hypothetische Vorüberlegungen erfolgen kann. </hi>
            </p>
            <figure>
               <graphic height=""14.12875cm"" n=""1002"" rend=""block"" url=""255_final-2348b0d1ad549aca4fe45aede50a9ee6.png"" width=""16.259527777777777cm""/>
               <head>Abbildung 2: Häufigkeit von Körperempfindungen zwischen August und Dezember 1732 </head>
            </figure>
            <figure>
               <graphic height=""12.49cm"" n=""1003"" rend=""block"" url=""255_final-70f99349f6fbd9d1c169b35a1af56c01.png"" width=""16.26cm""/>
               <head>Abbildung 3: Verteilung von Druckempfindungen auf verschiedene Körperteile </head>
            </figure>
            <p>
               <hi style=""font-size:12pt"">Auch Senckenbergs Essgewohnheiten lassen sich nur mit begriffsbasierten Suchabfragen analysieren. Die Suche nach dem Schlüsselwort „bibi” (ich trank) im Kontext der fünf angrenzenden Wörter zeigt beispielsweise, dass Senckenberg zwar überwiegend Wasser und Tee trank, aber bereits an dritter Stelle der Wein folgte (Abb. 4). Berücksichtigt man jedoch auch andere Getränke wie Kaffee, Bier, Alantwein und Milch sowie die entsprechenden lateinischen Begriffe, so ergibt sich aus dem Verhältnis zwischen alkoholfreien und alkoholischen Getränken das eher moderate Verhältnis von 253 zu 105. Übermäßiger Alkoholkonsum konnte jedoch Gottes Unwillen hervorrufen und durch körperliche und mentale Beschwerden bestraft werden, die sich gleichfalls in den Aufzeichnungen niederschlagen. Die Akribie der Senckenbergischen Selbstbeobachtung ermöglicht es, Vergleiche zwischen den in mehreren solcher Situationen auftretenden Symptomen anzustellen, die als Muster visualisiert und auf Ähnlichkeiten und Abweichungen durch Begleitfaktoren untersucht werden können. Abb. 5 zeigt etwa, dass sich die körperlichen Auswirkungen des Weinkonsums im warmen Monat September (a), den der Diarist für zahlreiche Freiluftaktivitäten nutzte, deutlich von denen im kälteren November (b) unterscheiden, welchen der Autor größtenteils daheim verbrachte. Noch aussagekräftiger sind die Ergebnisse einer Symptomanalyse auf Wochen- oder Tagesbasis. Dabei ist weniger bedeutsam, ob und wie die Symptome kausal zusammenhängen: Wichtiger ist es zu zeigen, dass und auf welche Weise sie gemeinsam auftreten, und wie sich verschiedene Situationen voneinander unterscheiden.</hi>
            </p>
            <figure>
               <graphic height=""14.73cm"" n=""1004"" rend=""inline"" url=""255_final-b2209898e7dc6b342a20553e001ffcbd.png"" width=""16.26cm""/>
               <head>Abbildung 4: Meistkonsumierte Getränke zwischen August und Dezember 1732 </head>
            </figure>
            <figure>
               <graphic height=""9.97cm"" n=""1005"" rend=""inline"" url=""255_final-b04265f323d8366d262be9fdb5a34fe0.png"" width=""16.26cm""/>
               <head>Abbildung 5a: Korrelationen zwischen Weinkonsum und Körperempfindungen in zwei verschiedenen Situationen </head>
            </figure>
            <figure>
               <graphic height=""10.738555555555555cm"" n=""1006"" rend=""inline"" url=""255_final-16e59aeeb78419942694b500eb40073d.png"" width=""16.404166666666665cm""/>
               <head>Abbildung 5b: Korrelationen zwischen Weinkonsum und Körperempfindungen in zwei verschiedenen Situationen </head>
            </figure>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listBibl>
               <head>Bibliographie</head>
               <bibl>
                  <hi rend=""bold"">Anderson, Chris</hi> (2008): “The End of Theory. Will the Data Deluge Make the Scientific Method Obsolete?” in:
  Edge, <ref target=""http://www.edge.org/3rd_culture/anderson08/anderson08_index.html"">http://www.edge.org/3rd_culture/anderson08/anderson08_index.html</ref>
  [letzter Zugriff 20. Dezember 2019].
</bibl>
               <bibl>
                  <hi rend=""bold"">Borck, Cornelius </hi> (2017): „Big Data. Praktiken und Theorien der Datenverarbeitung im historischen Querschnitt“, in:
  <hi rend=""italic"">Zeitschrift für Geschichte der Wissenschaften, Technik und Medizin (NTM) </hi> 25.4, 399–405.
</bibl>
               <bibl>
                  <hi style=""bold"">Boyd, Danah und Crawford, Kate </hi> (2012): “Critical Questions for Big Data”. In:
  <hi rend=""italic"">Information, Communication & Society</hi> 15:5, 662–679, DOI: 10.1080/1369118X.2012.678878 [letzter Zugriff 20. Dezember 2019].
</bibl>
               <bibl>
                  <hi rend=""bold"">Boellstorff, Tom </hi> (2014): „Die Konstruktion von Big Data in der Theorie“. In: Reichert, Ramón (Ed.):
  <hi rend=""italic"">Big Data. Analysen zum digitalen Wandel von Wissen, Macht und Ökonomie</hi>, Bielefeld, 105–131.
</bibl>
               <bibl>
                  <hi rend=""bold"">Bradley, Adam James </hi> (2012): 
  Violence and the Digital Humanities Text as Pharmakon, in:
  <hi rend=""italic"">Proceedings of the Digital Humanities 2012. </hi>
                  <ref target=""http://www.dh2012.uni-hamburg.de/wp-content/uploads/2012/07/HamburgUP_dh2012_BoA.pdf"">http://www.dh2012.uni-hamburg.de/wp-content/uploads/2012/07/HamburgUP_dh2012_BoA.pdf</ref>
  [letzter Zugriff 20. Dezember 2019].
</bibl>
               <bibl>
                  <hi rend=""bold"">Daston, Lorraine </hi> (2011): “The Empire of Observation, 1600–1800”, in: Daston, Lorraine / Lunbeck, Elizabeth (eds.): 
  <hi rend=""italic"">Histories of Scientific Observation</hi>, Chicago/London: University of Chicago Press, 81–113.
</bibl>
               <bibl>
                  <hi rend=""bold"">Faßhauer, Vera </hi> (2017): 
  <hi rend=""italic"">Sacra à Deo in corde discenda, natura ex natura</hi>. Die Observationes Johann Christian Senckenbergs als medico-theologische Aufzeichnungspraktik“, in:
  <hi rend=""italic"">Berichte zur Wissenschaftsgeschichte </hi> 40, 225–246.
</bibl>
               <bibl>
                  <hi rend=""bold"">Faßhauer, Vera </hi> (2018): “Accessing, Editing and Indexing Large Manuscript Collections: The Selected Edition of J. Chr. Senckenberg’s Journals.” In:
  <hi rend=""italic"">Knowledge Organization for Digital Humanities. Proceedings of the 15th Conference on Knowledge Organization WissOrg’17 of the German Chapter of
  the International Society for Knowledge Organization (ISKO)</hi>, 30th November – 1st December 2017, Freie Universität Berlin, ed. Christian Wartena, Michael Franke-Maier and Ernesto de Luca, Berlin 2018, 31–36.
  <ref target=""https://refubium.fu-berlin.de/bitstream/handle/fub188/20535/ProcWissOrg2017.pdf"">https://refubium.fu-berlin.de/bitstream/handle/fub188/20535/ProcWissOrg2017.pdf</ref>
  [letzter Zugriff 20. Dezember 2019].
</bibl>
               <bibl>
                  <hi rend=""bold"">Geyer-Kordesch, Johanna </hi> (1990): „Medizinische Fallbeschreibungen und ihre Bedeutung in der Wissensreform des 17. und 18. Jahrhunderts“, in:
  <hi rend=""italic"">Medizin, Gesellschaft und Geschichte </hi> 9, 7–19.
</bibl>
               <bibl>
                  <hi rend=""bold"">Hess, Volker</hi> (2011): „Das Material einer guten Geschichte. Register, Reglements und Formulare“, in: Dickson, Sheila / Goldmann, Stefan / Wingertszahn, Christof (eds.):
  <hi rend=""italic"">Fakta, und kein moralisches Geschwätz. Zu den Fallgeschichten im Magazin zur Erfahrungsseelenkunde (1783–1793)</hi>, Göttingen: Wallstein, 115–139.
</bibl>
               <bibl>
                  <hi rend=""bold"">Jänicke, Stefan </hi> (2016): 
  <hi rend=""italic"">Close and Distant Reading Visualizations for the Comparative Analysis of Digital Humanities Data</hi>, Diss. Leipzig.
  <ptr target=""http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-207418""/> [letzter Zugriff 20. Dezember 2019].
</bibl>
               <bibl>
                  <hi rend=""bold"">Lupton, Deborah </hi> (2016): 
  <hi rend=""italic"">The Quantified Self. A Sociology of Self-Tracking</hi>, Cambridge: Polity Press.
</bibl>
               <bibl>
                  <hi rend=""bold"">Mendelsohn, J. Andrew </hi> (2011): “The World on a Page: Making a General Observation in the Eighteenth Century”, in: Daston, Lorraine / Lunbeck, Elizabeth (eds.): 
<hi rend=""italic"">Histories of Scientific Observation</hi>, Chicago/London: University of Chicago Press, 396–420.
</bibl>
               <bibl>
                  <hi rend=""bold"">Moretti, Franco </hi> (2013): <hi rend=""italic"">Distant Reading</hi>, London/New York: Verso.
</bibl>
               <bibl>
                  <hi rend=""bold"">Oertzen, Christine von </hi> (2017): „Die Historizität der Verdatung: Konzepte, Werkzeuge und Praktiken im 19. Jahrhundert“, in: 
  <hi rend=""italic"">Zeitschrift für Geschichte der Wissenschaften, Technik und Medizin (NTM) </hi> 25.4, 407–434.
</bibl>
               <bibl>
                  <hi rend=""bold"">Senckenberg, Johann Christian </hi> (1732):
  <hi rend=""italic"">Tagebücher</hi>, Bd. 2: <hi rend=""italic"">Observationes in me ipso factae</hi>, August–Dezember 1732, Senckenbergisches Archiv, Na 31, 2, UB Frankfurt am Main, Digitalisat unter 
  <ptr target=""http://sammlungen.ub.uni-frankfurt.de/senckenberg/content/pageview/5381525""/>
  [letzter Zugriff 20. Dezember 2019].
</bibl>
               <bibl>
                  <hi rend=""bold"">Senckenberg, Johann Christian</hi> (1735): 
<hi rend=""italic"">Briefentwurf an einen unbekannten Empfänger</hi>, 24. Januar 1735, Senckenbergisches Archiv, Mp. 57, UB Frankfurt am Main.
</bibl>
               <bibl>
                  <hi rend=""bold"">Sinclair, Stéfan / Rockwell, Geoffrey</hi> (2003): Voyant Tools. 
  <ref target=""http://‌voyant-tools.org/"">http://‌voyant-tools.org</ref>.
</bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">Stolberg, Michael</hi> (2007): „Formen und Funktionen medizinischer Fallberichte in der Frühen Neuzeit (1500– 1800)“ in: Süßmann, Johannes / Scholz, Susanne / Engel, Gisela (eds.): 
<hi rend=""italic"">Fallstudien: Theorie – Geschichte – Methoden</hi>. Berlin: Trafo, 81–89. 
</bibl>
            </listBibl>
         </div>
      </back>
   </text>

",xml,Creative Commons Attribution 4.0 International,,big data;distant reading;historische datenerfassung;self-tracking,German,daten;inhaltsanalyse;kontextsetzung;manuskript;transkription;visualisierung,2020-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""de"">
      <body>
         <div rend=""dh-heading1"" type=""div1"">
            <head>einleitung</head>
            <p>
               <hi style=""font-size:12pt"" space=""preserve"">aktuelle diskussionen über ständig wachsende möglichkeiten der erfassung, speicherung und analyse großer datenmengen lassen uns vergessen, dass sowohl wissenschaft als auch behörden bereits seit jahrhunderten praktiken zur datenerhebung und -verarbeitung entwickelt haben (borck 2017, oertzen 2017). so wurden bereits im 17. jahrhundert astronomische und meteorologische beobachtungsdaten in formularen und tabellen erfasst, in zahlen und symbolen kodiert und in karten und diagrammen visualisiert (daston 2011, mendelsohn 2011, hess 2011). auch die empirische erfassung von personenbezogenen körperdaten nahm ihren anfang in den praxisjournalen frühneuzeitlicher ärzte, die alle symptome, zustandsmerkmale und reaktionen sowie den krankheitsverlauf, die medikamentierung und den heilungsprozess ihrer patienten genau dokumentierten (geyer-kordesch 1990, stolberg 2007). nicht einmal das von den anhängern der </hi>
               <hi rend=""italic"" style=""font-size:12pt"">quantified self</hi>
               <hi style=""font-size:12pt"" space=""preserve"">-bewegung praktizierte </hi>
               <hi rend=""italic"" style=""font-size:12pt"">self-tracking</hi>
               <hi style=""font-size:12pt"" space=""preserve""> (lupton 2016) gehört gänzlich dem 21. jahrhundert an, wie die tagebücher zahlreicher religiöser selbstoptimierer aus dem 17. und 18. jahrhundert zeigen.</hi>
            </p>
            <p>
               <hi style=""font-size:12pt"" space=""preserve"">eines der ambitioniertesten historischen </hi>
               <hi rend=""italic"" style=""font-size:12pt"">self-tracking</hi>
               <hi style=""font-size:12pt"" space=""preserve"">-projekte waren zweifellos die </hi>
               <hi rend=""italic"" style=""font-size:12pt"">observationes in me ipso factae</hi>
               <hi style=""font-size:12pt"" space=""preserve""> des pietistischen arztes johann christian senckenberg (1707–1772). in ihnen protokollierte er tagtäglich sein gesamtes körper- und seelenleben, um seinen lebenswandel zu vervollkommnen. neben ernährung, stoffwechsel, körperaktivität sowie schlaf- und ruhephasen notierte er auch alle reizempfindungen, körperreflexe und gemütszustände sowie alle spürbaren umwelt- und witterungseinflüsse (faßhauer 2017). zeitweise brachte er auf diese weise täglich bis zu 5.000 wörter zu papier, so dass auf den gesamtzeitraum von dreizehn jahren gerechnet ca. 14.000 seiten mit 12.600.000 wörtern zusammenkamen. der beitrag beleuchtet zunächst den epistemischen zweck dieses riesigen frühneuzeitlichen datenpools, aus dem derzeit in frankfurt ausgewählte bände digital ediert werden (faßhauer 2018), und setzt sie mit zeitgenössischen positionen zum verhältnis von daten und theorie in beziehung. anschließend wird die möglichkeit einer analyse dieser daten mit modernen </hi>
               <hi rend=""italic"" style=""font-size:12pt"">distant-reading</hi>
               <hi style=""font-size:12pt"">-methoden und deren vereinbarkeit mit dem epistemischen ziel des religiösen autors diskutiert.</hi>
            </p>
         </div>
         <div rend=""dh-heading1"" type=""div1"">
            <head>selbstbeobachtung und anti-rationalismus</head>
            <p>
               <hi style=""font-size:12pt"">wenn ein selbstoptimierer des digitalen zeitalters beschließt, seine lebenszeit effizient zu nutzen, seinen körper gesund zu erhalten oder seine finanzen zu organisieren, bezweckt er damit meist größtmöglichen persönlichen erfolg und selbstzufriedenheit im diesseits. ein religiöser self-tracker des 18. jahrhunderts hatte hingegen zu allererst sein seelenheil und seine erlösung nach dem tod im sinn. diese konnte jedoch nur erlangen, wer die ihm anvertrauten gottesgeschenke auf erden treulich verwaltete, pflegte und mehrte. genau wie für materielle güter galt dies auch für gesundheit und wissenskapital. nach auffassung religiöser gelehrter wie senckenberg war der mensch seit dem sündenfall jedoch geistig so zerrüttet, dass er durch seine verstandeskräfte zu keinen verlässlichen erkenntnissen gelangen konnte. insbesondere theoretische modelle, die durch „künstliches syllogisiren der vernunfft“ dem verstand der gelehrten („ex mente doctorum“) entstiegen sind, repräsentierten nur fragmentarisches oder abstraktes wissen. zudem müssten ohnehin „alle regulae universaliores erstlich ab experientia in particularibus“ abgeleitet werden, deren vielfalt jedoch so viele ausnahmen aufzeige, „daß die regulae selbst wieder darüber zernichtet werden“ (senckenberg 1735: 1r/v, faßhauer 2017). sichere medizinische erkenntnisse waren deshalb nur „ohne praeoccupation von einer vorher gefassten hypothesi“ durch mehrfach wiederholte unmittelbare selbsterfahrung zu erlangen, deren resultate möglichst vollständig aufgezeichnet und induktiv ausgewertet werden mussten. auf diese weise ließen sich vergleiche mit aufzeichnungen aus ähnlichen situationen herstellen, wobei einzelne faktoren miteinander korreliert, auf ihre relevanz und rolle im gesamtkontext befragt und als mögliche ursachen oder auswirkungen anderer faktoren in betracht gezogen werden konnten.</hi>
            </p>
            <p>
               <hi style=""font-size:12pt"">ganz ähnliche überzeugungen wie senckenberg äußerte der amerikanische journalist chris anderson, als er im jahre 2008 das ende der theorie und den beginn des datenzeitalters verkündete. auch er ging dabei von der prämisse aus, dass theorien die realität nur verzerrt wiedergäben und letztlich allein in den hirnen der wissenschaftler existierten: „the scientific method is built around testable hypotheses. these models, for the most part, are systems visualized in the minds of scientists”. stattdessen verwies er wie senckenberg auf die möglichkeit, durch erhebung und speicherung einer möglichst großen datenfülle ungleich genauere und verlässlichere aussagen über die welt in ihrer ganzen komplexität zu treffen. war der verzicht auf die suche nach letztgültigen kausalitäten bei senckenberg noch religiös motiviert, entspringt er bei anderson aus der pragmatischen erkenntnis, dass die verfügbarkeit nie gekannter datenmengen die notwendigkeit zur hypothesenbildung schlichtweg erübrige, da sie unter den verschiedensten gesichtspunkten miteinander korreliert werden könnten. durch ihre maschinelle auswertbarkeit gerieten zudem auch einzelheiten und muster ins blickfeld, die der theoriegeleiteten forschung entgingen: „correlation supersedes causation, and science can advance even without coherent models, unified theories, or really any mechanistic explanation at all“ (anderson 2008). seither wurde gegen andersons these wiederholt eingewandt, dass bereits die erstellung von datensätzen auf theoretischen prämissen und selektionskriterien beruhe (boyd/crawford 2012; boellstorff 2014). zudem stelle jede datenanalyse eine subjektive interpretation innerhalb bestimmter kultureller, dogmatischer oder ideologischer kontexte dar, so dass eine hypothesenfreie datenauswertung unmöglich sei. die gleiche problematik lässt sich auch für die aufzeichnungen senckenberg aufzeigen: außer thermometer und barometer stand ihm nur sein eigenes bewusstsein als messinstanz zur verfügung, das alle empfindungen und wahrnehmungen zwangsläufig subjektiv registrierte, filterte und interpretierte und dabei den protokollierten physischen und seelischen befindlichkeiten selbst unterworfen war. auch erfolgte die aufzeichnung der daten allein durch seine eigene schreibende hand, die auf körperliche irregularitäten ebenso empfindlich reagierte wie auf seelische erschütterungen und stimmungsschwankungen. waren körper und bewusstsein anderweitig okkupiert, konnte die datenerfassung entweder gar nicht oder nur rückwirkend und durch das gedächtnis vermittelt erfolgen.</hi>
            </p>
            <figure>
               <graphic height=""15.77cm"" n=""1001"" rend=""block"" url=""255_final-e08a8c76be8648de2de651eec8073220.png"" width=""16.23cm""/>
               <head>abbildungen 1a und 1b: selbstbeobachtung in senckenbergs tagebuch, dezember 1732 (manuskript und tei/xml-transkription) </head>
            </figure>
         </div>
         <div rend=""dh-heading1"" type=""div1"">
            <head>datengetriebenes <hi rend=""italic"">distant reading</hi>?  </head>
            <p>
               <hi style=""font-size:12pt"" space=""preserve"">senckenbergs riesiger datenpool konfrontiert seine modernen leser mit einem so mikroskopisch detaillierten bewusstseinsstrom, dass ein verständnis seiner erkenntnisse im </hi>
               <hi rend=""italic"" style=""font-size:12pt"">close reading</hi>
               <hi style=""font-size:12pt"" space=""preserve"">-modus nahezu unmöglich ist. die digitale erschließung einzelner bände im rahmen der </hi>
               <hi rend=""italic"" style=""font-size:12pt"">frankfurter auswahledition</hi>
               <hi style=""font-size:12pt"" space=""preserve""> ermöglicht nun eine annäherung an das umfangreiche textmaterial aus der makroperspektive (abb. 1a–b). der literaturwissenschaftler franco moretti hat dieses vorgehen bekanntlich als „distant reading“ bezeichnet, da distanz hier statt eines hindernisses eine bedingung der erkenntnis darstelle: „it allows you to focus on units that are much smaller or much larger than the text“. die reduktion von senckenbergs umfangreichen beobachtungsdaten auf abstrakte schemata scheint jedoch zunächst im widerspruch zu seiner absicht zu stehen, die ganze vielfalt der natürlichen erscheinungsformen unverkürzt zu erfassen. auch moretti hat auf dieses problem hingewiesen: „if we want to understand the system in its entirety, we must accept losing something. we always pay a price for theoretical knowledge: reality is infinitely rich; concepts are abstract, are poor” (moretti 2013: 48–49). die irreversible reduktion von texten auf abstrakte schemen, die in der forschung sogar als gewaltsame zerstörung des eigentlichen untersuchungsgegenstandes beschrieben worden ist (bradley 2012), kann nur in solchen forschungsumgebungen vermieden werden, die – wie etwa die voyant tools (sinclair und rockwell 2003) – einen flexiblen wechsel zwischen der text- und der grafikebene und damit zwischen dem </hi>
               <hi rend=""italic"" style=""font-size:12pt"">close</hi>
               <hi style=""font-size:12pt"" space=""preserve"">- und </hi>
               <hi rend=""italic"" style=""font-size:12pt"">distant reading</hi>
               <hi style=""font-size:12pt"" space=""preserve"">-modus ermöglichen (jänicke 2016: 20–23). ein möglicher ausgangspunkt für eine fernlektüre ist die identifizierung von schlüsselwörtern, die hier anhand von häufigkeitskriterien erfolgt. werden in der liste einzelne keywords ausgewählt, kann deren verteilung im korpus angezeigt werden. eine visualisierung der markantesten körperlichen empfindungen senckenbergs zwischen august und dezember 1732 zeigt zum beispiel, dass im november und dezember spannungs- und druckgefühle vorherrschten, während er im oktober hauptsächlich stiche und zuckungen verspürte, im august und september aber frei von derlei empfindungen war (abb. 2). ein blick auf die kollokationen dieser begriffe zeigt, in welchen körperteilen sie am häufigsten bemerkbar waren (abb. 3). allerdings stellen derlei festlegungen auf bestimmte untersuchungszeiträume oder körperempfindungen bereits arbeitshypothesen dar, die mit einer bestimmten erwartungshaltung einhergehen und das ergebnis dadurch nicht unmaßgeblich präformieren. die oben aufgezeigte unmöglichkeit des von senckenberg projektierten theoriefreien wissenserwerbs spiegelt sich deshalb unmittelbar in der digitalen korpusanalyse wider, die gleichfalls nicht rein datengetrieben bzw. ohne hypothetische vorüberlegungen erfolgen kann. </hi>
            </p>
            <figure>
               <graphic height=""14.12875cm"" n=""1002"" rend=""block"" url=""255_final-2348b0d1ad549aca4fe45aede50a9ee6.png"" width=""16.259527777777777cm""/>
               <head>abbildung 2: häufigkeit von körperempfindungen zwischen august und dezember 1732 </head>
            </figure>
            <figure>
               <graphic height=""12.49cm"" n=""1003"" rend=""block"" url=""255_final-70f99349f6fbd9d1c169b35a1af56c01.png"" width=""16.26cm""/>
               <head>abbildung 3: verteilung von druckempfindungen auf verschiedene körperteile </head>
            </figure>
            <p>
               <hi style=""font-size:12pt"">auch senckenbergs essgewohnheiten lassen sich nur mit begriffsbasierten suchabfragen analysieren. die suche nach dem schlüsselwort „bibi” (ich trank) im kontext der fünf angrenzenden wörter zeigt beispielsweise, dass senckenberg zwar überwiegend wasser und tee trank, aber bereits an dritter stelle der wein folgte (abb. 4). berücksichtigt man jedoch auch andere getränke wie kaffee, bier, alantwein und milch sowie die entsprechenden lateinischen begriffe, so ergibt sich aus dem verhältnis zwischen alkoholfreien und alkoholischen getränken das eher moderate verhältnis von 253 zu 105. übermäßiger alkoholkonsum konnte jedoch gottes unwillen hervorrufen und durch körperliche und mentale beschwerden bestraft werden, die sich gleichfalls in den aufzeichnungen niederschlagen. die akribie der senckenbergischen selbstbeobachtung ermöglicht es, vergleiche zwischen den in mehreren solcher situationen auftretenden symptomen anzustellen, die als muster visualisiert und auf ähnlichkeiten und abweichungen durch begleitfaktoren untersucht werden können. abb. 5 zeigt etwa, dass sich die körperlichen auswirkungen des weinkonsums im warmen monat september (a), den der diarist für zahlreiche freiluftaktivitäten nutzte, deutlich von denen im kälteren november (b) unterscheiden, welchen der autor größtenteils daheim verbrachte. noch aussagekräftiger sind die ergebnisse einer symptomanalyse auf wochen- oder tagesbasis. dabei ist weniger bedeutsam, ob und wie die symptome kausal zusammenhängen: wichtiger ist es zu zeigen, dass und auf welche weise sie gemeinsam auftreten, und wie sich verschiedene situationen voneinander unterscheiden.</hi>
            </p>
            <figure>
               <graphic height=""14.73cm"" n=""1004"" rend=""inline"" url=""255_final-b2209898e7dc6b342a20553e001ffcbd.png"" width=""16.26cm""/>
               <head>abbildung 4: meistkonsumierte getränke zwischen august und dezember 1732 </head>
            </figure>
            <figure>
               <graphic height=""9.97cm"" n=""1005"" rend=""inline"" url=""255_final-b04265f323d8366d262be9fdb5a34fe0.png"" width=""16.26cm""/>
               <head>abbildung 5a: korrelationen zwischen weinkonsum und körperempfindungen in zwei verschiedenen situationen </head>
            </figure>
            <figure>
               <graphic height=""10.738555555555555cm"" n=""1006"" rend=""inline"" url=""255_final-16e59aeeb78419942694b500eb40073d.png"" width=""16.404166666666665cm""/>
               <head>abbildung 5b: korrelationen zwischen weinkonsum und körperempfindungen in zwei verschiedenen situationen </head>
            </figure>
         </div>
      </body>
      <back>
         <div type=""bibliogr"">
            <listbibl>
               <head>bibliographie</head>
               <bibl>
                  <hi rend=""bold"">anderson, chris</hi> (2008): “the end of theory. will the data deluge make the scientific method obsolete?” in:
  edge, <ref target=""http://www.edge.org/3rd_culture/anderson08/anderson08_index.html"">http://www.edge.org/3rd_culture/anderson08/anderson08_index.html</ref>
  [letzter zugriff 20. dezember 2019].
</bibl>
               <bibl>
                  <hi rend=""bold"">borck, cornelius </hi> (2017): „big data. praktiken und theorien der datenverarbeitung im historischen querschnitt“, in:
  <hi rend=""italic"">zeitschrift für geschichte der wissenschaften, technik und medizin (ntm) </hi> 25.4, 399–405.
</bibl>
               <bibl>
                  <hi style=""bold"">boyd, danah und crawford, kate </hi> (2012): “critical questions for big data”. in:
  <hi rend=""italic"">information, communication & society</hi> 15:5, 662–679, doi: 10.1080/1369118x.2012.678878 [letzter zugriff 20. dezember 2019].
</bibl>
               <bibl>
                  <hi rend=""bold"">boellstorff, tom </hi> (2014): „die konstruktion von big data in der theorie“. in: reichert, ramón (ed.):
  <hi rend=""italic"">big data. analysen zum digitalen wandel von wissen, macht und ökonomie</hi>, bielefeld, 105–131.
</bibl>
               <bibl>
                  <hi rend=""bold"">bradley, adam james </hi> (2012): 
  violence and the digital humanities text as pharmakon, in:
  <hi rend=""italic"">proceedings of the digital humanities 2012. </hi>
                  <ref target=""http://www.dh2012.uni-hamburg.de/wp-content/uploads/2012/07/hamburgup_dh2012_boa.pdf"">http://www.dh2012.uni-hamburg.de/wp-content/uploads/2012/07/hamburgup_dh2012_boa.pdf</ref>
  [letzter zugriff 20. dezember 2019].
</bibl>
               <bibl>
                  <hi rend=""bold"">daston, lorraine </hi> (2011): “the empire of observation, 1600–1800”, in: daston, lorraine / lunbeck, elizabeth (eds.): 
  <hi rend=""italic"">histories of scientific observation</hi>, chicago/london: university of chicago press, 81–113.
</bibl>
               <bibl>
                  <hi rend=""bold"">faßhauer, vera </hi> (2017): 
  <hi rend=""italic"">sacra à deo in corde discenda, natura ex natura</hi>. die observationes johann christian senckenbergs als medico-theologische aufzeichnungspraktik“, in:
  <hi rend=""italic"">berichte zur wissenschaftsgeschichte </hi> 40, 225–246.
</bibl>
               <bibl>
                  <hi rend=""bold"">faßhauer, vera </hi> (2018): “accessing, editing and indexing large manuscript collections: the selected edition of j. chr. senckenberg’s journals.” in:
  <hi rend=""italic"">knowledge organization for digital humanities. proceedings of the 15th conference on knowledge organization wissorg’17 of the german chapter of
  the international society for knowledge organization (isko)</hi>, 30th november – 1st december 2017, freie universität berlin, ed. christian wartena, michael franke-maier and ernesto de luca, berlin 2018, 31–36.
  <ref target=""https://refubium.fu-berlin.de/bitstream/handle/fub188/20535/procwissorg2017.pdf"">https://refubium.fu-berlin.de/bitstream/handle/fub188/20535/procwissorg2017.pdf</ref>
  [letzter zugriff 20. dezember 2019].
</bibl>
               <bibl>
                  <hi rend=""bold"">geyer-kordesch, johanna </hi> (1990): „medizinische fallbeschreibungen und ihre bedeutung in der wissensreform des 17. und 18. jahrhunderts“, in:
  <hi rend=""italic"">medizin, gesellschaft und geschichte </hi> 9, 7–19.
</bibl>
               <bibl>
                  <hi rend=""bold"">hess, volker</hi> (2011): „das material einer guten geschichte. register, reglements und formulare“, in: dickson, sheila / goldmann, stefan / wingertszahn, christof (eds.):
  <hi rend=""italic"">fakta, und kein moralisches geschwätz. zu den fallgeschichten im magazin zur erfahrungsseelenkunde (1783–1793)</hi>, göttingen: wallstein, 115–139.
</bibl>
               <bibl>
                  <hi rend=""bold"">jänicke, stefan </hi> (2016): 
  <hi rend=""italic"">close and distant reading visualizations for the comparative analysis of digital humanities data</hi>, diss. leipzig.
  <ptr target=""http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-207418""/> [letzter zugriff 20. dezember 2019].
</bibl>
               <bibl>
                  <hi rend=""bold"">lupton, deborah </hi> (2016): 
  <hi rend=""italic"">the quantified self. a sociology of self-tracking</hi>, cambridge: polity press.
</bibl>
               <bibl>
                  <hi rend=""bold"">mendelsohn, j. andrew </hi> (2011): “the world on a page: making a general observation in the eighteenth century”, in: daston, lorraine / lunbeck, elizabeth (eds.): 
<hi rend=""italic"">histories of scientific observation</hi>, chicago/london: university of chicago press, 396–420.
</bibl>
               <bibl>
                  <hi rend=""bold"">moretti, franco </hi> (2013): <hi rend=""italic"">distant reading</hi>, london/new york: verso.
</bibl>
               <bibl>
                  <hi rend=""bold"">oertzen, christine von </hi> (2017): „die historizität der verdatung: konzepte, werkzeuge und praktiken im 19. jahrhundert“, in: 
  <hi rend=""italic"">zeitschrift für geschichte der wissenschaften, technik und medizin (ntm) </hi> 25.4, 407–434.
</bibl>
               <bibl>
                  <hi rend=""bold"">senckenberg, johann christian </hi> (1732):
  <hi rend=""italic"">tagebücher</hi>, bd. 2: <hi rend=""italic"">observationes in me ipso factae</hi>, august–dezember 1732, senckenbergisches archiv, na 31, 2, ub frankfurt am main, digitalisat unter 
  <ptr target=""http://sammlungen.ub.uni-frankfurt.de/senckenberg/content/pageview/5381525""/>
  [letzter zugriff 20. dezember 2019].
</bibl>
               <bibl>
                  <hi rend=""bold"">senckenberg, johann christian</hi> (1735): 
<hi rend=""italic"">briefentwurf an einen unbekannten empfänger</hi>, 24. januar 1735, senckenbergisches archiv, mp. 57, ub frankfurt am main.
</bibl>
               <bibl>
                  <hi rend=""bold"">sinclair, stéfan / rockwell, geoffrey</hi> (2003): voyant tools. 
  <ref target=""http://‌voyant-tools.org/"">http://‌voyant-tools.org</ref>.
</bibl>
               <bibl style=""text-align:left; "">
                  <hi rend=""bold"">stolberg, michael</hi> (2007): „formen und funktionen medizinischer fallberichte in der frühen neuzeit (1500– 1800)“ in: süßmann, johannes / scholz, susanne / engel, gisela (eds.): 
<hi rend=""italic"">fallstudien: theorie – geschichte – methoden</hi>. berlin: trafo, 81–89. 
</bibl>
            </listbibl>
         </div>
      </back>
   </text>

",2.0,4.0,Voyant
11707,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,PRISMS: a new platform for digital Book History,,Alexander Huber;Emma Huber,"paper, specified ""long paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Introduction and motivation</head>
                <p style=""text-align: left; "">In a 2020 talk entitled “A Hornbook for Digital Book History”, Whitney Trettien weaves together many of the strands that have led book history, bibliography, media studies, and the digital humanities to have become deeply entangled in recent years. She convincingly argues for the potential of Book History done digitally “to build connective tissue across scattered collections” and advocates “using the digital tools at our disposal in order to see the big picture of the past”.
                    <note place=""foot"" xml:id=""ftn1"" n=""1"">
                        <p rend=""footnote text"">
                            <ref target=""https://rarebookschool.org/rbs-online/a-hornbook-for-digital-book-history/"">https://rarebookschool.org/rbs-online/a-hornbook-for-digital-book-history/</ref>. She shares this vision of a continuum of print and digital with other influential voices at the intersection of book history, media studies, and the digital humanities, among them Henrike Laehnemann, Sarah Werner, and Matt Kirschenbaum to name but a few.
                        </p>
                    </note>
                </p>
                <p style=""text-align: left; "">It is in this vein that this paper presents the motivation for and realisation of a new open-access open scholarship platform (currently in public beta) named PRISMS.
                    <note place=""foot"" xml:id=""ftn2"" n=""2"">
                        <p rend=""footnote text"">
                            <ref target=""https://www.prisms.digital/"">https://www.prisms.digital/</ref>
                        </p>
                    </note> The aim of the PRISMS Open Scholarship platform is two-fold:
                </p>
                <list type=""ordered"">
                    <item>It offers a publication platform for digital scholarly editions, with full-text (preferably encoded in TEI) and facsimiles, and any accompanying materials, such as introduction, editorial statement, critical apparatus, contextual source materials, bibliography, and indices;</item>
                    <item>It facilitates the semantic annotation of these editions and their related scholarship (in any format) by enabling easy-to-perform formal ontological modelling (based on the CIDOC-CRM family of ontologies
                        <note place=""foot"" xml:id=""ftn3"" n=""3"">
                            <p rend=""footnote text"">
                                <ref target=""http://www.cidoc-crm.org/"">http://www.cidoc-crm.org/</ref>
                            </p>
                        </note>), and thus hopes to contribute to providing the abovementioned “connective tissue” not only for scattered collections, but to overcome the artificial print/digital divide.
                    </item>
                </list>
                <p style=""text-align: left; "">PRISMS was born out of the realization that digital editions do not break with the historicity or materiality of the sources they organize and present, but instead remediate and extend them in ways that enable new forms of access, engagement, presentation, and analysis. PRISMS conceptualizes digital editions as living entities that perform rather than merely document the remediation they engage in.</p>
                <p style=""text-align: left; "">The scholarship that underpins each digital edition provides the essential context for these remediation processes, and collectively they sustain the knowledge network that supports all academic engagement with the texts from any disciplinary viewpoint. PRISMS is designed to allow for the collaborative and collective modelling of this continuum of digital editions and scholarship by placing digital editions, their material and contextual basis, and the resulting academic engagement in a linked context, building on the standards and tools provided by the Semantic Web.</p>
                <p style=""text-align: left; "">We believe that this type of formalization is beneficial for the purposes of this project in at least three ways: firstly, ontologies facilitate modelling with reduced reliance on implicit knowledge through an explicit, shared conceptualization of the domain. Secondly, formal models encourage collaboration as they can be shared, re-used, adapted (forked), enhanced, aggregated, and developed collaboratively. Thirdly, as a form of knowledge representation, visualization, and preservation, formal models support computational processing and ultimately reasoning, and can develop alongside the mental models and human reasoning we engage in as scholars. PRISMS facilitates scholarship that is based on these principles
                    <note place=""foot"" xml:id=""ftn4"" n=""4"">
                        <p rend=""footnote text""> Ground-breaking research projects in this domain include the 
                            <ref target=""https://researchspace.org/"">ResearchSpace</ref> platform and the 
                            <ref target=""https://sphaera.mpiwg-berlin.mpg.de/"">Sphaera CorpusTracer</ref> project.
                        </p>
                    </note>.
                </p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Approach and implementation</head>
                <p style=""text-align: left; "">The PRISMS Open Scholarship platform integrates the task of publishing digital editions with the need for analytical and modelling tools to perform the type of knowledge representation that connects the material, digital, and the scholarship that builds on them. PRISMS aims to support digital editors, book historians, experts in media and cultural studies, librarians, literary scholars, and of course digital humanists, to ensure a wide range of domain expertise, disciplinary practices, and methodological approaches are reflected in the platform. To this end, the PRISMS platform hosts a variety of tools alongside the digital editions, which can be categorized as component tools (such as text-based tools, image-based tools, XML-based tools, etc.) and workbench tools (those available across document types and editions).</p>
                <figure>
                    <graphic n=""1001"" width=""16.002cm"" height=""8.200319444444444cm"" url=""Pictures/d07cb664d746dee79113a8b75d7b65f4.jpeg"" rend=""inline""/>
                    <head>Figure 1 
                        <hi rend=""italic color(474747)"" style=""font-family:Open Sans;font-size:10pt"">Some of the built-in analysis and visualization tools in PRISMS. Voyant-Tools is shown alongside a relation being made between two editions, and some highlighted annotations</hi>
                    </head>
                </figure>
                <p style=""text-align: left; "">The former category of tools is useful for any type of close scholarly work, and in PRISMS these tools include a bookmarking tool, an annotation tool (initially focussing on texts and images, but with a vision to extend annotation capabilities across all media types), the ability to keep research records (and other forms of note-taking, e.g. transcriptions, translations etc.) in the form of notebooks, and integration of Voyant Tools
                    <note place=""foot"" xml:id=""ftn5"" n=""5"">
                        <p rend=""footnote text"">
                            <ref target=""https://voyant-tools.org/"">https://voyant-tools.org/</ref>
                        </p>
                    </note> for statistical analysis and a variety of visualisations of texts. The latter category includes the ability to participate in the shaping of the knowledge graph by modelling concepts and relationships, an easy way to organize research materials, and the ability to download, share, and publish contributions for the benefit of all.
                </p>
                <figure>
                    <graphic n=""1002"" width=""16.002cm"" height=""8.200319444444444cm"" url=""Pictures/a59b90da3ac111e8830619980ef05599.jpeg"" rend=""inline""/>
                    <head>Figure 2 
                        <hi rend=""italic color(474747)"" style=""font-family:Open Sans;font-size:10pt"">Modelling both the material legacies and digital remediation processes with Linked Data technologies and Cytoscape.js</hi>
                    </head>
                </figure>
                <p style=""text-align: left; "">All semantic modelling work, e.g. with regard to the provenance of the material and digital manifestations of an edition, can be performed both directly in a visual representation of the graph using Cytoscape.js or via a set of customizable HTML forms. All resulting triples are stored in an RDF-native graph database (using the abovementioned ontologies) for long-term preservation, collaboration, and re-use. Every user of PRISMS has both read access to this global graph that underpins the platform and unlimited access (via SPARQL Update operations) to a private graph. By default, everything in PRISMS is private. Everything contributors add is immediately visible to them, and they can conduct their scholarship in complete privacy, without delay or interference. Only when the user decides to publish their contributions will they be made available to everyone. Depending on the type of contribution made, there are options to share, download, and/or publish them. All contributions made to the PRISMS platform are stored in standard formats, e.g. the W3C Web Annotation Data Model for annotations and relations.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Contribution and further work</head>
                <p style=""text-align: left; "">PRISMS has been launched with corpora from the EEBO-TCP
                    <note place=""foot"" xml:id=""ftn6"" n=""6"">
                        <p rend=""footnote text"">
                            <ref target=""https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/"">https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/</ref>
                        </p>
                    </note>, ECCO-TCP
                    <note place=""foot"" xml:id=""ftn7"" n=""7"">
                        <p rend=""footnote text"">
                            <ref target=""https://textcreationpartnership.org/tcp-texts/ecco-tcp-eighteenth-century-collections-online/"">https://textcreationpartnership.org/tcp-texts/ecco-tcp-eighteenth-century-collections-online/</ref>
                        </p>
                    </note>, EVANS-TCP
                    <note place=""foot"" xml:id=""ftn8"" n=""8"">
                        <p rend=""footnote text"">
                            <ref target=""https://textcreationpartnership.org/tcp-texts/evans-tcp-evans-early-american-imprints/"">https://textcreationpartnership.org/tcp-texts/evans-tcp-evans-early-american-imprints/</ref>
                        </p>
                    </note>, the DTA extended core corpus
                    <note place=""foot"" xml:id=""ftn9"" n=""9"">
                        <p rend=""footnote text"">
                            <ref target=""https://www.deutschestextarchiv.de/"">https://www.deutschestextarchiv.de/</ref>
                        </p>
                    </note>, and the Taylor Editions
                    <note place=""foot"" xml:id=""ftn10"" n=""10"">
                        <p rend=""footnote text"">
                            <ref target=""https://editions.mml.ox.ac.uk/"">https://editions.mml.ox.ac.uk/</ref>
                        </p>
                    </note> scholarly editions platform. And it is easy to add new editions to the platform either as part of a dedicated digital scholarly editing process, for which training is provided, or by simply adding a IIIF manifest and using a built-in XML-aware or standard text-editor to start transcribing and adding contextual materials. The platform already supports the addition and semantic annotation of a wide range of primary and secondary materials, such as facsimiles in IIIF, a transcription of a source text, a PDF of a journal article, a video of a theatrical performance, an audio book, an image, or a 3D-model of a sculpture mentioned in a text, etc. 
                </p>
                <figure>
                    <graphic n=""1003"" width=""16.002cm"" height=""8.19326388888889cm"" url=""Pictures/ddf3b307349c15c53eafbeda53a58283.jpeg"" rend=""inline""/>
                    <head>Figure 3 
                        <hi rend=""italic color(474747)"" style=""font-family:Open Sans;font-size:10pt"">A view of the PRISMS workbench, with three editions of Faust loaded, and an aggregation of primary and research materials in support of a performance analysis, with a facsimile, two videos, and an audio book</hi>
                    </head>
                </figure>
                <p style=""text-align: left; "">Moving forward, we will continue to work on integrating the digital research and tools important to PRISMS’ users (e.g. reference manager, images taken in reading rooms, items deposited in institutional repositories). With end of the beta phase, the project also intends to provide access to the entire PRISMS knowledge graph through regular data dumps and a public SPARQL endpoint. We envision that over time, PRISMS will evolve both as a powerful discovery tool and a personal research tool.</p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl style=""text-align: left; "">Ciotti, F. (2015) “Digital methods for Literary Criticism.” Lecture slides. University of Rome Tor Vergata, 
                        <ref target=""http://didattica.uniroma2.it/files/scarica/insegnamento/161783-Informatica-Umanistica-Lm-Per-Il-Llea/37175-Slide"">http://didattica.uniroma2.it/files/scarica/insegnamento/161783-Informatica-Umanistica-Lm-Per-Il-Llea/37175-Slide</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">Ciula, A. and Marras, C. (2016) “Circling around texts and language: towards 'pragmatic modelling' in Digital Humanities.” 
                        <hi rend=""titlej"">
                            <hi rend=""italic"">Digital Humanities Quarterly (DHQ)</hi>
                        </hi> 10.3 
                        <ref target=""http://www.digitalhumanities.org/dhq/vol/10/3/000258/000258.html"">http://www.digitalhumanities.org/dhq/vol/10/3/000258/000258.html</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">Ciula, A. and Eide, Ø. (2107) “Modelling in digital humanities: Signs in context.” 
                        <hi rend=""titlej"">
                            <hi rend=""italic"">Digital Scholarship in the Humanities</hi>
                        </hi> 32: i33–i46. 
                        <ref target=""https://doi.org/10.1093/llc/fqw045"">https://doi.org/10.1093/llc/fqw045</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">Ciula, A., Eide, Ø, Marras, C. and Sahle, P. (2018) 
                        <hi rend=""italic"" xml:space=""preserve"">Models and Modelling between Digital and Humanities — A Multidisciplinary Perspective. </hi>
                        <hi rend=""titlej"">Historical Social Research (HSR)</hi> Supplement 31.
                    </bibl>
                    <bibl style=""text-align: left; "">Eide, Ø. (2015)
                        <hi rend=""italic"" xml:space=""preserve""> Media Boundaries and Conceptual Modelling: Between Texts and Maps.</hi> Pre-print manuscript, 
                        <ref target=""https://www.oeide.no/research/eideBetween.pdf"">https://www.oeide.no/research/eideBetween.pdf</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">Kirschenbaum, M. and Werner, S. (2014) “Digital Scholarship and Digital Studies: The State of the Discipline.” 
                        <hi rend=""italic"">Book History</hi> 17, 406-458 
                        <ref target=""https://www.academia.edu/15995371/Digital_Studies_and_Digital_Scholarship_The_State_of_the_Discipline"">https://www.academia.edu/15995371/Digital_​Studies_​and_​Digital_​Scholarship_​The_​State_​of_​the_​Discipline</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">Kräutli, F. and Valleriani, M. (2018) “CorpusTracer: A CIDOC database for tracing knowledge networks.” 
                        <hi rend=""italic"">Digital Scholarship in the Humanities</hi> 33(2): 336-346. 
                        <ref target=""https://pure.mpg.de/rest/items/item_2472866_10/component/file_3002633/content"">https://pure.mpg.de/rest/items/item_2472866_10/component/file_3002633/content</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">Laehnemann, H. (2022) “History of the Book blog.” 
                        <ref target=""https://historyofthebook.mml.ox.ac.uk/"">https://historyofthebook.mml.ox.ac.uk/</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">Oldman, D., Doerr, M. and Gradmann, S. (2016) “Zen and the Art of Linked Data: New Strategies for a Semantic Web of Humanist Knowledge.” In Schreibman, S., Siemens, R., Unsworth, J. (eds.) 
                        <hi rend=""italic"">A New Companion to Digital Humanities.</hi> Malden, MA: Wiley Blackwell, 251-273.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,digital book history;knowledge modelling;scholarly editing,English,"15th-17th century;18th century;19th century;analysis;book and print history;english;global;linked (open) data;literary studies;scholarly editing and editions development, analysis, and methods",2022-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <div type=""div1"" rend=""dh-heading1"">
                <head>introduction and motivation</head>
                <p style=""text-align: left; "">in a 2020 talk entitled “a hornbook for digital book history”, whitney trettien weaves together many of the strands that have led book history, bibliography, media studies, and the digital humanities to have become deeply entangled in recent years. she convincingly argues for the potential of book history done digitally “to build connective tissue across scattered collections” and advocates “using the digital tools at our disposal in order to see the big picture of the past”.
                    <note place=""foot"" xml:id=""ftn1"" n=""1"">
                        <p rend=""footnote text"">
                            <ref target=""https://rarebookschool.org/rbs-online/a-hornbook-for-digital-book-history/"">https://rarebookschool.org/rbs-online/a-hornbook-for-digital-book-history/</ref>. she shares this vision of a continuum of print and digital with other influential voices at the intersection of book history, media studies, and the digital humanities, among them henrike laehnemann, sarah werner, and matt kirschenbaum to name but a few.
                        </p>
                    </note>
                </p>
                <p style=""text-align: left; "">it is in this vein that this paper presents the motivation for and realisation of a new open-access open scholarship platform (currently in public beta) named prisms.
                    <note place=""foot"" xml:id=""ftn2"" n=""2"">
                        <p rend=""footnote text"">
                            <ref target=""https://www.prisms.digital/"">https://www.prisms.digital/</ref>
                        </p>
                    </note> the aim of the prisms open scholarship platform is two-fold:
                </p>
                <list type=""ordered"">
                    <item>it offers a publication platform for digital scholarly editions, with full-text (preferably encoded in tei) and facsimiles, and any accompanying materials, such as introduction, editorial statement, critical apparatus, contextual source materials, bibliography, and indices;</item>
                    <item>it facilitates the semantic annotation of these editions and their related scholarship (in any format) by enabling easy-to-perform formal ontological modelling (based on the cidoc-crm family of ontologies
                        <note place=""foot"" xml:id=""ftn3"" n=""3"">
                            <p rend=""footnote text"">
                                <ref target=""http://www.cidoc-crm.org/"">http://www.cidoc-crm.org/</ref>
                            </p>
                        </note>), and thus hopes to contribute to providing the abovementioned “connective tissue” not only for scattered collections, but to overcome the artificial print/digital divide.
                    </item>
                </list>
                <p style=""text-align: left; "">prisms was born out of the realization that digital editions do not break with the historicity or materiality of the sources they organize and present, but instead remediate and extend them in ways that enable new forms of access, engagement, presentation, and analysis. prisms conceptualizes digital editions as living entities that perform rather than merely document the remediation they engage in.</p>
                <p style=""text-align: left; "">the scholarship that underpins each digital edition provides the essential context for these remediation processes, and collectively they sustain the knowledge network that supports all academic engagement with the texts from any disciplinary viewpoint. prisms is designed to allow for the collaborative and collective modelling of this continuum of digital editions and scholarship by placing digital editions, their material and contextual basis, and the resulting academic engagement in a linked context, building on the standards and tools provided by the semantic web.</p>
                <p style=""text-align: left; "">we believe that this type of formalization is beneficial for the purposes of this project in at least three ways: firstly, ontologies facilitate modelling with reduced reliance on implicit knowledge through an explicit, shared conceptualization of the domain. secondly, formal models encourage collaboration as they can be shared, re-used, adapted (forked), enhanced, aggregated, and developed collaboratively. thirdly, as a form of knowledge representation, visualization, and preservation, formal models support computational processing and ultimately reasoning, and can develop alongside the mental models and human reasoning we engage in as scholars. prisms facilitates scholarship that is based on these principles
                    <note place=""foot"" xml:id=""ftn4"" n=""4"">
                        <p rend=""footnote text""> ground-breaking research projects in this domain include the 
                            <ref target=""https://researchspace.org/"">researchspace</ref> platform and the 
                            <ref target=""https://sphaera.mpiwg-berlin.mpg.de/"">sphaera corpustracer</ref> project.
                        </p>
                    </note>.
                </p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>approach and implementation</head>
                <p style=""text-align: left; "">the prisms open scholarship platform integrates the task of publishing digital editions with the need for analytical and modelling tools to perform the type of knowledge representation that connects the material, digital, and the scholarship that builds on them. prisms aims to support digital editors, book historians, experts in media and cultural studies, librarians, literary scholars, and of course digital humanists, to ensure a wide range of domain expertise, disciplinary practices, and methodological approaches are reflected in the platform. to this end, the prisms platform hosts a variety of tools alongside the digital editions, which can be categorized as component tools (such as text-based tools, image-based tools, xml-based tools, etc.) and workbench tools (those available across document types and editions).</p>
                <figure>
                    <graphic n=""1001"" width=""16.002cm"" height=""8.200319444444444cm"" url=""pictures/d07cb664d746dee79113a8b75d7b65f4.jpeg"" rend=""inline""/>
                    <head>figure 1 
                        <hi rend=""italic color(474747)"" style=""font-family:open sans;font-size:10pt"">some of the built-in analysis and visualization tools in prisms. voyant-tools is shown alongside a relation being made between two editions, and some highlighted annotations</hi>
                    </head>
                </figure>
                <p style=""text-align: left; "">the former category of tools is useful for any type of close scholarly work, and in prisms these tools include a bookmarking tool, an annotation tool (initially focussing on texts and images, but with a vision to extend annotation capabilities across all media types), the ability to keep research records (and other forms of note-taking, e.g. transcriptions, translations etc.) in the form of notebooks, and integration of voyant tools
                    <note place=""foot"" xml:id=""ftn5"" n=""5"">
                        <p rend=""footnote text"">
                            <ref target=""https://voyant-tools.org/"">https://voyant-tools.org/</ref>
                        </p>
                    </note> for statistical analysis and a variety of visualisations of texts. the latter category includes the ability to participate in the shaping of the knowledge graph by modelling concepts and relationships, an easy way to organize research materials, and the ability to download, share, and publish contributions for the benefit of all.
                </p>
                <figure>
                    <graphic n=""1002"" width=""16.002cm"" height=""8.200319444444444cm"" url=""pictures/a59b90da3ac111e8830619980ef05599.jpeg"" rend=""inline""/>
                    <head>figure 2 
                        <hi rend=""italic color(474747)"" style=""font-family:open sans;font-size:10pt"">modelling both the material legacies and digital remediation processes with linked data technologies and cytoscape.js</hi>
                    </head>
                </figure>
                <p style=""text-align: left; "">all semantic modelling work, e.g. with regard to the provenance of the material and digital manifestations of an edition, can be performed both directly in a visual representation of the graph using cytoscape.js or via a set of customizable html forms. all resulting triples are stored in an rdf-native graph database (using the abovementioned ontologies) for long-term preservation, collaboration, and re-use. every user of prisms has both read access to this global graph that underpins the platform and unlimited access (via sparql update operations) to a private graph. by default, everything in prisms is private. everything contributors add is immediately visible to them, and they can conduct their scholarship in complete privacy, without delay or interference. only when the user decides to publish their contributions will they be made available to everyone. depending on the type of contribution made, there are options to share, download, and/or publish them. all contributions made to the prisms platform are stored in standard formats, e.g. the w3c web annotation data model for annotations and relations.</p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>contribution and further work</head>
                <p style=""text-align: left; "">prisms has been launched with corpora from the eebo-tcp
                    <note place=""foot"" xml:id=""ftn6"" n=""6"">
                        <p rend=""footnote text"">
                            <ref target=""https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/"">https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/</ref>
                        </p>
                    </note>, ecco-tcp
                    <note place=""foot"" xml:id=""ftn7"" n=""7"">
                        <p rend=""footnote text"">
                            <ref target=""https://textcreationpartnership.org/tcp-texts/ecco-tcp-eighteenth-century-collections-online/"">https://textcreationpartnership.org/tcp-texts/ecco-tcp-eighteenth-century-collections-online/</ref>
                        </p>
                    </note>, evans-tcp
                    <note place=""foot"" xml:id=""ftn8"" n=""8"">
                        <p rend=""footnote text"">
                            <ref target=""https://textcreationpartnership.org/tcp-texts/evans-tcp-evans-early-american-imprints/"">https://textcreationpartnership.org/tcp-texts/evans-tcp-evans-early-american-imprints/</ref>
                        </p>
                    </note>, the dta extended core corpus
                    <note place=""foot"" xml:id=""ftn9"" n=""9"">
                        <p rend=""footnote text"">
                            <ref target=""https://www.deutschestextarchiv.de/"">https://www.deutschestextarchiv.de/</ref>
                        </p>
                    </note>, and the taylor editions
                    <note place=""foot"" xml:id=""ftn10"" n=""10"">
                        <p rend=""footnote text"">
                            <ref target=""https://editions.mml.ox.ac.uk/"">https://editions.mml.ox.ac.uk/</ref>
                        </p>
                    </note> scholarly editions platform. and it is easy to add new editions to the platform either as part of a dedicated digital scholarly editing process, for which training is provided, or by simply adding a iiif manifest and using a built-in xml-aware or standard text-editor to start transcribing and adding contextual materials. the platform already supports the addition and semantic annotation of a wide range of primary and secondary materials, such as facsimiles in iiif, a transcription of a source text, a pdf of a journal article, a video of a theatrical performance, an audio book, an image, or a 3d-model of a sculpture mentioned in a text, etc. 
                </p>
                <figure>
                    <graphic n=""1003"" width=""16.002cm"" height=""8.19326388888889cm"" url=""pictures/ddf3b307349c15c53eafbeda53a58283.jpeg"" rend=""inline""/>
                    <head>figure 3 
                        <hi rend=""italic color(474747)"" style=""font-family:open sans;font-size:10pt"">a view of the prisms workbench, with three editions of faust loaded, and an aggregation of primary and research materials in support of a performance analysis, with a facsimile, two videos, and an audio book</hi>
                    </head>
                </figure>
                <p style=""text-align: left; "">moving forward, we will continue to work on integrating the digital research and tools important to prisms’ users (e.g. reference manager, images taken in reading rooms, items deposited in institutional repositories). with end of the beta phase, the project also intends to provide access to the entire prisms knowledge graph through regular data dumps and a public sparql endpoint. we envision that over time, prisms will evolve both as a powerful discovery tool and a personal research tool.</p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl style=""text-align: left; "">ciotti, f. (2015) “digital methods for literary criticism.” lecture slides. university of rome tor vergata, 
                        <ref target=""http://didattica.uniroma2.it/files/scarica/insegnamento/161783-informatica-umanistica-lm-per-il-llea/37175-slide"">http://didattica.uniroma2.it/files/scarica/insegnamento/161783-informatica-umanistica-lm-per-il-llea/37175-slide</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">ciula, a. and marras, c. (2016) “circling around texts and language: towards 'pragmatic modelling' in digital humanities.” 
                        <hi rend=""titlej"">
                            <hi rend=""italic"">digital humanities quarterly (dhq)</hi>
                        </hi> 10.3 
                        <ref target=""http://www.digitalhumanities.org/dhq/vol/10/3/000258/000258.html"">http://www.digitalhumanities.org/dhq/vol/10/3/000258/000258.html</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">ciula, a. and eide, ø. (2107) “modelling in digital humanities: signs in context.” 
                        <hi rend=""titlej"">
                            <hi rend=""italic"">digital scholarship in the humanities</hi>
                        </hi> 32: i33–i46. 
                        <ref target=""https://doi.org/10.1093/llc/fqw045"">https://doi.org/10.1093/llc/fqw045</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">ciula, a., eide, ø, marras, c. and sahle, p. (2018) 
                        <hi rend=""italic"" xml:space=""preserve"">models and modelling between digital and humanities — a multidisciplinary perspective. </hi>
                        <hi rend=""titlej"">historical social research (hsr)</hi> supplement 31.
                    </bibl>
                    <bibl style=""text-align: left; "">eide, ø. (2015)
                        <hi rend=""italic"" xml:space=""preserve""> media boundaries and conceptual modelling: between texts and maps.</hi> pre-print manuscript, 
                        <ref target=""https://www.oeide.no/research/eidebetween.pdf"">https://www.oeide.no/research/eidebetween.pdf</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">kirschenbaum, m. and werner, s. (2014) “digital scholarship and digital studies: the state of the discipline.” 
                        <hi rend=""italic"">book history</hi> 17, 406-458 
                        <ref target=""https://www.academia.edu/15995371/digital_studies_and_digital_scholarship_the_state_of_the_discipline"">https://www.academia.edu/15995371/digital_​studies_​and_​digital_​scholarship_​the_​state_​of_​the_​discipline</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">kräutli, f. and valleriani, m. (2018) “corpustracer: a cidoc database for tracing knowledge networks.” 
                        <hi rend=""italic"">digital scholarship in the humanities</hi> 33(2): 336-346. 
                        <ref target=""https://pure.mpg.de/rest/items/item_2472866_10/component/file_3002633/content"">https://pure.mpg.de/rest/items/item_2472866_10/component/file_3002633/content</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">laehnemann, h. (2022) “history of the book blog.” 
                        <ref target=""https://historyofthebook.mml.ox.ac.uk/"">https://historyofthebook.mml.ox.ac.uk/</ref>
                    </bibl>
                    <bibl style=""text-align: left; "">oldman, d., doerr, m. and gradmann, s. (2016) “zen and the art of linked data: new strategies for a semantic web of humanist knowledge.” in schreibman, s., siemens, r., unsworth, j. (eds.) 
                        <hi rend=""italic"">a new companion to digital humanities.</hi> malden, ma: wiley blackwell, 251-273.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>

",2.0,4.0,Voyant
11736,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,"Web Services for Voyant: LINCS, Voyant and NSSI: LINCS, Voyant and NSSI",,Geoffrey Martin Rockwell;Natalie Hervieux;Huma Zafar;Kaylin Land;Andrew MacDonald;Denilson Barbosa;Luciano Frizzera;Mihaela Ilovan;Susan Brown,"paper, specified ""long paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Introduction</head>
                <p style=""text-align: left; "">It is difficult to identify named entities like people and places in long texts and even more difficult to connect the entities that you find to the rich network of information available on the web. In this paper we describe work supported by the LINCS (Linked Infrastructure for Networked Cultural Scholarship) project to make named entity recognition available to scholars through Voyant and its extension Spyral. In this talk we will:</p>
                <p style=""text-align: left; "">First, describe the development of NSSI, a set of named entity recognition (NER) tools that are also available as web services for other tools like Voyant to use.</p>
                <p style=""text-align: left; "">Second, describe how Voyant can use NSSI as a web service to process a text by adding named entity recognition.</p>
                <p style=""text-align: left; "">Third, describe how Spyral, the notebook programming extension of Voyant, can be used for more sophisticated control of the process of named entity recognition, extraction, and use in Voyant. </p>
                <p style=""text-align: left; "">Finally, we will conclude by discussing how NSSI and Spyral will be linked into the LINCS infrastructure to allow scholars to connect their enriched data to that of others.</p>
                <p style=""text-align: left; "">Background on LINCS</p>
                <p style=""text-align: left; "">Humanists tend to be interested in named people, named places and particular organizations over time. NER tools let humanists identify mentions in text referring to the people, places, organizations and other entities discussed in large collections without having to manually comb through them. Good tools like the Stanford Named Entity Recognizer (Finkel et al. 2005) have been available for some time, but are difficult to use if you are not familiar with command line tools and not connected with other resources.</p>
                <p style=""text-align: left; "">The LINCS project, led by Susan Brown at the University of Guelph, is funded by the Canadian Foundation for Innovation to develop shared infrastructure for linked open data. To that end LINCS is working with teams at the University of Alberta and McGill University to develop new NER tools and to connect them to easy-to-use text analysis environments like Voyant.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>NSSI</head>
                <p style=""text-align: left; "">NSSI, or NERVE Secure Scalable Infrastructure, is an application that bundles natural language processing tools, making them simple to use and combine into workflows common to the digital humanities (Zafar 2021). This framework was developed as part of the LINCS project, with the intent to decouple the backend NER tools from the existing Named Entity Recognition Vetting Environment (NERVE) user interface developed by the Canadian Writing Research Collaboratory. This separation allows us to continue using those NER services for NERVE, while making them accessible to other tools such as Voyant and Spyral.</p>
                <p style=""text-align: left; "">NSSI’s design focuses on modularity, with each tool connected as a service that can be used individually or within a larger set of steps. For NER in particular, we have integrated Stanford NER which otherwise requires programming knowledge to use, since it does not come with its own API. With NSSI, a tool such as Spyral can make an API call that includes input text or XML and retrieve the named entities when processing completes. In the presentation we will briefly describe the NSSI infrastructure.</p>
                <figure>
                    <graphic n=""1001"" width=""16.002cm"" height=""10.100027777777777cm"" url=""Pictures/3f40d77e17c9bd0bbd421b51e7b72b5c.png"" rend=""inline""/>
                    <head>Figure 1: Experimental RezoViz NER Interface in Voyant</head>
                </figure>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Voyant and Spyral</head>
                <p style=""text-align: left; "">Voyant Tools is a suite of text analysis and visualization tools that are widely used with over 100,000 users in the last six months. The tools are available in the browser so they don’t need to be installed, though you can download them and run them locally (Rockwell & Sinclair 2016). In the presentation we will show how Voyant can call the NER tools in NSSI and display the found entities as a list for further use. We will also describe the usability testing conducted on ResoViz through the LINCS project.</p>
                <figure>
                    <graphic n=""1002"" width=""16.002cm"" height=""10.100027777777777cm"" url=""Pictures/cd500b7e6df44dca3c7896f1864d715a.png"" rend=""inline""/>
                    <head>Figure 2: ResoViz Social Network Visualization</head>
                </figure>
                <p style=""text-align: left; "">Voyant is also being extended with a notebook programming environment called Spyral (Land et al. 2021; Rockwell et al. 2021). Spyral is, like Observable, an in-browser notebook programming environment that uses JavaScript as the programming language. The difference between Spyral and other notebook environments like Mathematica or Google Colab is that a) the notebooks are maintained on the server so that, again, there is no installation needed and b) Spyral is an extension to Voyant. This means that you can save what you see in Voyant as a notebook with an interactive panel of results embedded in the notebook. Then you can document your results, add more interactive panels, and process the results. In the presentation we will show how Spyral can be used to extend the work with NSSI possible with Voyant and to edit and document results.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Next Steps</head>
                <p style=""text-align: left; "">The paper will conclude by describing the next steps in the larger project, and those are to allow users to connect named entities in their texts to other data about the entities available through the LINCS triple store and other open data resources like Wikidata (Vrandečić 2012). The ultimate goal is to provide scholars with linked infrastructure where data about entities like people or novels can be annotated and connected with that of other projects.</p>
            </div>
            <div type=""div1"" rend=""DH-Heading1"">
                <head>Links</head>
                <p style=""text-align: left; "">Google Colaboratory (Colab): https://colab.research.google.com/ </p>
                <p style=""text-align: left; "">LINCS project: https://lincsproject.ca/</p>
                <p style=""text-align: left; "">Stanford Named Entity Recognizer: https://nlp.stanford.edu/software/CRF-NER.html </p>
                <p style=""text-align: left; "">Voyant Tools: https://voyant-tools.org and Spyral: https://voyant-tools.org/spyral</p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl style=""text-align: left; "">Finkel, J. R., Grenager, T., and Manning C. (2005). Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370. http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf (accessed 21 May 2022).</bibl>
                    <bibl style=""text-align: left; "">Zafar, H. (2021). Linked Data Conversion using Microservices [video file]. Zenodo. https://doi.org/10.5281/zenodo.6551465 (accessed 21 May 2022).</bibl>
                    <bibl style=""text-align: left; "">Land, K., MacDonald, A. and Rockwell, G. (2021). Spyral Notebooks as a Supplement to Voyant Tools. CSDH-SCHN 2021 conference online. http://dx.doi.org/10.17613/2bsr-xp53 (accessed 21 May 2022).</bibl>
                    <bibl style=""text-align: left; "">Rockwell, G. and Sinclair, S. (2016). Hermeneutica: Computer-Assisted Interpretation in the Humanities. Cambridge, Massachusetts, MIT Press.</bibl>
                    <bibl style=""text-align: left; "">Rockwell, G., Land, K., and MacDonald, A. (2021). Social Analytics Through Spyral. Pop! Public. Open. Participatory. no. 3 (2021-10-31). https://popjournal.ca/issue03/rockwell (accessed 21 May 2022).</bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""Subtle_Reference"" xml:space=""preserve"">Vrandečić, D. (2012). Wikidata: A new platform for collaborative data collection. In </hi>Proceedings of the 21st international conference on world wide web, pp. 1063-1064.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,infrastructure;named entity recognition;text analysis;text visualization;web services,English,"20th century;contemporary;english;humanities computing;literary studies;natural language processing;north america;software development, systems, analysis and methods",2022-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <div type=""div1"" rend=""dh-heading1"">
                <head>introduction</head>
                <p style=""text-align: left; "">it is difficult to identify named entities like people and places in long texts and even more difficult to connect the entities that you find to the rich network of information available on the web. in this paper we describe work supported by the lincs (linked infrastructure for networked cultural scholarship) project to make named entity recognition available to scholars through voyant and its extension spyral. in this talk we will:</p>
                <p style=""text-align: left; "">first, describe the development of nssi, a set of named entity recognition (ner) tools that are also available as web services for other tools like voyant to use.</p>
                <p style=""text-align: left; "">second, describe how voyant can use nssi as a web service to process a text by adding named entity recognition.</p>
                <p style=""text-align: left; "">third, describe how spyral, the notebook programming extension of voyant, can be used for more sophisticated control of the process of named entity recognition, extraction, and use in voyant. </p>
                <p style=""text-align: left; "">finally, we will conclude by discussing how nssi and spyral will be linked into the lincs infrastructure to allow scholars to connect their enriched data to that of others.</p>
                <p style=""text-align: left; "">background on lincs</p>
                <p style=""text-align: left; "">humanists tend to be interested in named people, named places and particular organizations over time. ner tools let humanists identify mentions in text referring to the people, places, organizations and other entities discussed in large collections without having to manually comb through them. good tools like the stanford named entity recognizer (finkel et al. 2005) have been available for some time, but are difficult to use if you are not familiar with command line tools and not connected with other resources.</p>
                <p style=""text-align: left; "">the lincs project, led by susan brown at the university of guelph, is funded by the canadian foundation for innovation to develop shared infrastructure for linked open data. to that end lincs is working with teams at the university of alberta and mcgill university to develop new ner tools and to connect them to easy-to-use text analysis environments like voyant.</p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>nssi</head>
                <p style=""text-align: left; "">nssi, or nerve secure scalable infrastructure, is an application that bundles natural language processing tools, making them simple to use and combine into workflows common to the digital humanities (zafar 2021). this framework was developed as part of the lincs project, with the intent to decouple the backend ner tools from the existing named entity recognition vetting environment (nerve) user interface developed by the canadian writing research collaboratory. this separation allows us to continue using those ner services for nerve, while making them accessible to other tools such as voyant and spyral.</p>
                <p style=""text-align: left; "">nssi’s design focuses on modularity, with each tool connected as a service that can be used individually or within a larger set of steps. for ner in particular, we have integrated stanford ner which otherwise requires programming knowledge to use, since it does not come with its own api. with nssi, a tool such as spyral can make an api call that includes input text or xml and retrieve the named entities when processing completes. in the presentation we will briefly describe the nssi infrastructure.</p>
                <figure>
                    <graphic n=""1001"" width=""16.002cm"" height=""10.100027777777777cm"" url=""pictures/3f40d77e17c9bd0bbd421b51e7b72b5c.png"" rend=""inline""/>
                    <head>figure 1: experimental rezoviz ner interface in voyant</head>
                </figure>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>voyant and spyral</head>
                <p style=""text-align: left; "">voyant tools is a suite of text analysis and visualization tools that are widely used with over 100,000 users in the last six months. the tools are available in the browser so they don’t need to be installed, though you can download them and run them locally (rockwell & sinclair 2016). in the presentation we will show how voyant can call the ner tools in nssi and display the found entities as a list for further use. we will also describe the usability testing conducted on resoviz through the lincs project.</p>
                <figure>
                    <graphic n=""1002"" width=""16.002cm"" height=""10.100027777777777cm"" url=""pictures/cd500b7e6df44dca3c7896f1864d715a.png"" rend=""inline""/>
                    <head>figure 2: resoviz social network visualization</head>
                </figure>
                <p style=""text-align: left; "">voyant is also being extended with a notebook programming environment called spyral (land et al. 2021; rockwell et al. 2021). spyral is, like observable, an in-browser notebook programming environment that uses javascript as the programming language. the difference between spyral and other notebook environments like mathematica or google colab is that a) the notebooks are maintained on the server so that, again, there is no installation needed and b) spyral is an extension to voyant. this means that you can save what you see in voyant as a notebook with an interactive panel of results embedded in the notebook. then you can document your results, add more interactive panels, and process the results. in the presentation we will show how spyral can be used to extend the work with nssi possible with voyant and to edit and document results.</p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>next steps</head>
                <p style=""text-align: left; "">the paper will conclude by describing the next steps in the larger project, and those are to allow users to connect named entities in their texts to other data about the entities available through the lincs triple store and other open data resources like wikidata (vrandečić 2012). the ultimate goal is to provide scholars with linked infrastructure where data about entities like people or novels can be annotated and connected with that of other projects.</p>
            </div>
            <div type=""div1"" rend=""dh-heading1"">
                <head>links</head>
                <p style=""text-align: left; "">google colaboratory (colab): https://colab.research.google.com/ </p>
                <p style=""text-align: left; "">lincs project: https://lincsproject.ca/</p>
                <p style=""text-align: left; "">stanford named entity recognizer: https://nlp.stanford.edu/software/crf-ner.html </p>
                <p style=""text-align: left; "">voyant tools: https://voyant-tools.org and spyral: https://voyant-tools.org/spyral</p>
            </div>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl style=""text-align: left; "">finkel, j. r., grenager, t., and manning c. (2005). incorporating non-local information into information extraction systems by gibbs sampling. proceedings of the 43nd annual meeting of the association for computational linguistics (acl 2005), pp. 363-370. http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf (accessed 21 may 2022).</bibl>
                    <bibl style=""text-align: left; "">zafar, h. (2021). linked data conversion using microservices [video file]. zenodo. https://doi.org/10.5281/zenodo.6551465 (accessed 21 may 2022).</bibl>
                    <bibl style=""text-align: left; "">land, k., macdonald, a. and rockwell, g. (2021). spyral notebooks as a supplement to voyant tools. csdh-schn 2021 conference online. http://dx.doi.org/10.17613/2bsr-xp53 (accessed 21 may 2022).</bibl>
                    <bibl style=""text-align: left; "">rockwell, g. and sinclair, s. (2016). hermeneutica: computer-assisted interpretation in the humanities. cambridge, massachusetts, mit press.</bibl>
                    <bibl style=""text-align: left; "">rockwell, g., land, k., and macdonald, a. (2021). social analytics through spyral. pop! public. open. participatory. no. 3 (2021-10-31). https://popjournal.ca/issue03/rockwell (accessed 21 may 2022).</bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""subtle_reference"" xml:space=""preserve"">vrandečić, d. (2012). wikidata: a new platform for collaborative data collection. in </hi>proceedings of the 21st international conference on world wide web, pp. 1063-1064.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>

",17.0,19.0,Voyant
11875,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,"Literary Text Analysis with Spyral Notebooks, a Notebook Environment Companion to Voyant Tools",,Kaylin Catherine Land;Geoffrey Rockwell;Andrew MacDonald;Bennett Kuwan Tchoc;Elliot Damasah,workshop / tutorial,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left; "">Digital literary text analysis is increasingly becoming an integral part of literary studies. However, many tools designed for performing such analysis remain inaccessible to researchers without significant coding and computing skills. Voyant Tools was designed in part to address this gap. Spyral Notebooks are an extension of Voyant Tools and allow researchers to expand upon their findings from Voyant in a notebook environment. Unlike other notebook environments, Spyral Notebooks are accessible without downloading any programs or advanced set-up. Spyral Notebooks are available in an entirely online format. To use Spyral Notebooks, one needs only a connection to the Internet. The notebooks are easily adaptable, shareable, and editable. </p>
            <p style=""text-align: left; "">Spyral is a notebook development environment that is integrated into Voyant Tools. Notebook environments can be thought of as both extensions of traditional research notebooks and as novel tools that integrate documentation, active analysis and presentation of results. At their core, notebooks are made up of three types of blocks or cells that a user can add or delete in a sequence. </p>
            <list rend=""numbered"">
                <item>There are text cells that can contain headings and other text elements found in word processors or browser editors (usually based in HTML) for typing unstructured text. Depending on the notebook environment, the text blocks can be simple or more sophisticated. Spyral Notebooks use HTML for text and offer an in- browser WYSIWYG HTML editor for the text blocks. </item>
                <item>There are code cells where the user inputs code, be it Python, the Wolfram language used in Mathematica, or JavaScript, which is used in Spyral. The code cells can be run in sequence or individually as you debug your code. Code cells can contain as much or as little code as the user desires. </item>
                <item>There are output cells which produce the output of the code you input in the associated code cell. It is important to recognize that the output of the code is dependent on what you have instructed the computer to do; that is, it is not a printout of the code cell but the results of running your code. You thus have to instruct the computer to print out the desired results. </item>
            </list>
            <p style=""text-align: left; "">In our tutorial we introduce participants to Spyral Notebooks. We illustrate how to create a corpus for textual analysis from Voyant Tools or directly in Spyral Notebooks. After walking through the basic mechanisms for using Spyral Notebooks including saving, editing, and sharing notebooks, we move on to more specific features available in Spyral. Participants will learn how to enhance the capabilities of Voyant and go deeper with their textual analysis using Spyral. Finally we provide participants with several tutorial notebooks designed to highlight some of Spyral’s advanced features such as categories for use in sentiment analysis. </p>
            <p style=""text-align: left; "">Spyral Notebooks are a welcome addition to the field of digital humanities as they provide an accessible notebook environment specifically designed for literary text analysis. Spyral Notebooks are thoughtfully designed to serve researchers with limited coding skills who want to take their analysis from Voyant one step further. We especially envisage Spyral proving useful for digital humanities instructors. Spyral provides a useful platform for student work, allowing students to embed their analysis from Voyant, perform more complex analysis using JavaScript, and annotate their code with their thought processes.</p>
        </body>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,javascript;literary text analysis;notebook environments,English,"contemporary;education/ pedagogy;english;global;interface design, development, and analysis;literary studies;text mining and analysis",2022-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left; "">digital literary text analysis is increasingly becoming an integral part of literary studies. however, many tools designed for performing such analysis remain inaccessible to researchers without significant coding and computing skills. voyant tools was designed in part to address this gap. spyral notebooks are an extension of voyant tools and allow researchers to expand upon their findings from voyant in a notebook environment. unlike other notebook environments, spyral notebooks are accessible without downloading any programs or advanced set-up. spyral notebooks are available in an entirely online format. to use spyral notebooks, one needs only a connection to the internet. the notebooks are easily adaptable, shareable, and editable. </p>
            <p style=""text-align: left; "">spyral is a notebook development environment that is integrated into voyant tools. notebook environments can be thought of as both extensions of traditional research notebooks and as novel tools that integrate documentation, active analysis and presentation of results. at their core, notebooks are made up of three types of blocks or cells that a user can add or delete in a sequence. </p>
            <list rend=""numbered"">
                <item>there are text cells that can contain headings and other text elements found in word processors or browser editors (usually based in html) for typing unstructured text. depending on the notebook environment, the text blocks can be simple or more sophisticated. spyral notebooks use html for text and offer an in- browser wysiwyg html editor for the text blocks. </item>
                <item>there are code cells where the user inputs code, be it python, the wolfram language used in mathematica, or javascript, which is used in spyral. the code cells can be run in sequence or individually as you debug your code. code cells can contain as much or as little code as the user desires. </item>
                <item>there are output cells which produce the output of the code you input in the associated code cell. it is important to recognize that the output of the code is dependent on what you have instructed the computer to do; that is, it is not a printout of the code cell but the results of running your code. you thus have to instruct the computer to print out the desired results. </item>
            </list>
            <p style=""text-align: left; "">in our tutorial we introduce participants to spyral notebooks. we illustrate how to create a corpus for textual analysis from voyant tools or directly in spyral notebooks. after walking through the basic mechanisms for using spyral notebooks including saving, editing, and sharing notebooks, we move on to more specific features available in spyral. participants will learn how to enhance the capabilities of voyant and go deeper with their textual analysis using spyral. finally we provide participants with several tutorial notebooks designed to highlight some of spyral’s advanced features such as categories for use in sentiment analysis. </p>
            <p style=""text-align: left; "">spyral notebooks are a welcome addition to the field of digital humanities as they provide an accessible notebook environment specifically designed for literary text analysis. spyral notebooks are thoughtfully designed to serve researchers with limited coding skills who want to take their analysis from voyant one step further. we especially envisage spyral proving useful for digital humanities instructors. spyral provides a useful platform for student work, allowing students to embed their analysis from voyant, perform more complex analysis using javascript, and annotate their code with their thought processes.</p>
        </body>
    </text>

",8.0,8.0,Voyant
11882,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,Low-stakes activities for text analysis instruction in the undergraduate classroom,,Marcela Y. Isuster,"paper, specified ""short paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left; "">The introduction of digital text analysis tools and methodologies in (non-digital) humanities undergraduate courses has been sparsely documented in the literature. Furthermore, most of the times we encounter it, it is done in the context of semester-long or mid-term projects (Boyle and Hall 2016; Ficke 2014), where the stakes for the students are very high. Other times, they include a session on text analysis but no practical application of the tools and methodologies discussed in the course, other than a follow along demonstration.</p>
            <p style=""text-align: left; "">This short paper introduces a middle point between these two extremes through the introduction of low stakes activities and assignments to help student discover and use digital text analysis tools and methodologies.</p>
            <p style=""text-align: left; "">Besides giving students the opportunity to interact with the material in a safe and relaxed manner, low stakes activities help with student retention, confidence, and relationship building (Hamilton 2020; Meer and Chapman 2014). Low stakes activities are also a useful tool to assess comprehension and instruction when the person delivering the lesson is not the regular or official instructor in the course, such as the case of a librarian or a guest speaker. Furthermore, these types of activities are particularly useful for digital humanities instruction because they contribute to scaffolding, a method that has been identified as ideal in this type of instruction (Griffin and Taylor 2017; Isuster 2020; Sample and Schrum 2013; Tracy and Hoiem, 2018).</p>
            <p style=""text-align: left; "">In the context of a Hispanic Studies course, a librarian offered a workshop series on digital text analysis and the web-based reading and analysis environment Voyant Tools. Interspersed with instruction there were a series of low stakes assessments that helped students understand and apply the content of the workshops. Working with the class readings, the librarian created activities that did not rely on having a single answer but encouraged students to discuss and interrogate both the methods and the information used. For example, when preparing a text for text analysis, students debated how different research questions necessitate different text preparation. The activities were completed in groups and were not graded. Results were discussed within the class.</p>
            <p style=""text-align: left; "">The short paper presentation will explore the process of creating and implementing low stakes activities for digital text analysis and other digital humanities instruction. It will discuss the benefits of these types of activities as they pertain to digital humanities instruction and engagement and will share best practices and tips to help attendees create these kinds of activities in their own classrooms, including assignment design and sourcing materials.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl style=""text-align: left;"">Boyle, M. and Hall, C. (2016) ‘Teaching “Don Quixote” in the Digital Age: Page and Screen, Visual and Tactile’, 
                        <hi rend=""italic"">Hispania</hi>, 99(4), pp. 600–614.
                    </bibl>
                    <bibl style=""text-align: left;"">Ficke, S.H. (2014) ‘From Text to Tags: The Digital Humanities in an Introductory Literature Course’, 
                        <hi rend=""italic"">CEA Critic</hi>, 76(2), pp. 200–210. 
                        <ref target=""https://doi.org/10.1353/cea.2014.0012"">10.1353/cea.2014.0012</ref>.
                    </bibl>
                    <bibl style=""text-align: left;"">Griffin, M. and Taylor, T.I. (2017) ‘Shifting expectations: Revisiting core concepts of academic librarianship in undergraduate classes with a digital humanities focus’, 
                        <hi rend=""italic"">College & Undergraduate Libraries</hi>, 24(2–4), pp. 452–466. 
                        <ref target=""https://doi.org/10.1080/10691316.2017.1325346"">10.1080/10691316.2017.1325346</ref>.
                    </bibl>
                    <bibl style=""text-align: left;"">Hamilton, M. (2020) ‘Implementation of a low-stakes daily assessment in a large introductory LAC course’, 
                        <hi rend=""italic"">Teaching and Assessment Symposium</hi> [Preprint]. Available at: 
                        <ref target=""https://digscholarship.unco.edu/posters_2020/4"">https://digscholarship.unco.edu/posters_2020/4</ref>.
                    </bibl>
                    <bibl style=""text-align: left;"">Isuster, M.Y. (2020) ‘From students to authors: Fostering student content creation with Scalar’, 
                        <hi rend=""italic"">College & Undergraduate Libraries</hi>, 27(2-4), pp. 133–148. 
                        <ref target=""https://doi.org/10.1080/10691316.2020.1830908"">10.1080/10691316.2020.1830908</ref>.
                    </bibl>
                    <bibl style=""text-align: left;"">Meer, N.M. and Chapman, A. (2014) ‘Assessment for confidence: Exploring the impact that low-stakes assessment design has on student retention’, 
                        <hi rend=""italic"">The International Journal of Management Education</hi>, 12(2), pp. 186–192. 
                        <ref target=""https://doi.org/10.1016/j.ijme.2014.01.003"">10.1016/j.ijme.2014.01.003</ref>.
                    </bibl>
                    <bibl style=""text-align: left;"">Sample, M. and Schrum, K. (2013) ‘What’s Wrong with Writing Essays: A Conversation’, in Cohen, D.J. and Scheinfedlt, J.T. (eds) 
                        <hi rend=""italic"">Hacking the academy : new approaches to scholarship and teaching from digital humanities</hi>. Ann Arbor, MI: University of Michigan Press, pp. 87–96.
                    </bibl>
                    <bibl style=""text-align: left;"">Tracy, D.G. and Hoiem, E.M. (2018) ‘Scaffolding and Play Approaches to Digital Humanities Pedagogy: Assessment and Iteration in Topically-Driven Courses’, 
                        <hi rend=""italic"">Digital Humanities Quarterly</hi>, 11(4). Available at: 
                        <ref target=""http://digitalhumanities.org:8081/dhq/vol/11/4/000358/000358.html"">http://digitalhumanities.org:8081/dhq/vol/11/4/000358/000358.html</ref>.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,assessment;instruction;pedagogy;text analysis,English,contemporary;curricular and pedagogical development and analysis;education/ pedagogy;english;global;library & information science;text mining and analysis,2022-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left; "">the introduction of digital text analysis tools and methodologies in (non-digital) humanities undergraduate courses has been sparsely documented in the literature. furthermore, most of the times we encounter it, it is done in the context of semester-long or mid-term projects (boyle and hall 2016; ficke 2014), where the stakes for the students are very high. other times, they include a session on text analysis but no practical application of the tools and methodologies discussed in the course, other than a follow along demonstration.</p>
            <p style=""text-align: left; "">this short paper introduces a middle point between these two extremes through the introduction of low stakes activities and assignments to help student discover and use digital text analysis tools and methodologies.</p>
            <p style=""text-align: left; "">besides giving students the opportunity to interact with the material in a safe and relaxed manner, low stakes activities help with student retention, confidence, and relationship building (hamilton 2020; meer and chapman 2014). low stakes activities are also a useful tool to assess comprehension and instruction when the person delivering the lesson is not the regular or official instructor in the course, such as the case of a librarian or a guest speaker. furthermore, these types of activities are particularly useful for digital humanities instruction because they contribute to scaffolding, a method that has been identified as ideal in this type of instruction (griffin and taylor 2017; isuster 2020; sample and schrum 2013; tracy and hoiem, 2018).</p>
            <p style=""text-align: left; "">in the context of a hispanic studies course, a librarian offered a workshop series on digital text analysis and the web-based reading and analysis environment voyant tools. interspersed with instruction there were a series of low stakes assessments that helped students understand and apply the content of the workshops. working with the class readings, the librarian created activities that did not rely on having a single answer but encouraged students to discuss and interrogate both the methods and the information used. for example, when preparing a text for text analysis, students debated how different research questions necessitate different text preparation. the activities were completed in groups and were not graded. results were discussed within the class.</p>
            <p style=""text-align: left; "">the short paper presentation will explore the process of creating and implementing low stakes activities for digital text analysis and other digital humanities instruction. it will discuss the benefits of these types of activities as they pertain to digital humanities instruction and engagement and will share best practices and tips to help attendees create these kinds of activities in their own classrooms, including assignment design and sourcing materials.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl style=""text-align: left;"">boyle, m. and hall, c. (2016) ‘teaching “don quixote” in the digital age: page and screen, visual and tactile’, 
                        <hi rend=""italic"">hispania</hi>, 99(4), pp. 600–614.
                    </bibl>
                    <bibl style=""text-align: left;"">ficke, s.h. (2014) ‘from text to tags: the digital humanities in an introductory literature course’, 
                        <hi rend=""italic"">cea critic</hi>, 76(2), pp. 200–210. 
                        <ref target=""https://doi.org/10.1353/cea.2014.0012"">10.1353/cea.2014.0012</ref>.
                    </bibl>
                    <bibl style=""text-align: left;"">griffin, m. and taylor, t.i. (2017) ‘shifting expectations: revisiting core concepts of academic librarianship in undergraduate classes with a digital humanities focus’, 
                        <hi rend=""italic"">college & undergraduate libraries</hi>, 24(2–4), pp. 452–466. 
                        <ref target=""https://doi.org/10.1080/10691316.2017.1325346"">10.1080/10691316.2017.1325346</ref>.
                    </bibl>
                    <bibl style=""text-align: left;"">hamilton, m. (2020) ‘implementation of a low-stakes daily assessment in a large introductory lac course’, 
                        <hi rend=""italic"">teaching and assessment symposium</hi> [preprint]. available at: 
                        <ref target=""https://digscholarship.unco.edu/posters_2020/4"">https://digscholarship.unco.edu/posters_2020/4</ref>.
                    </bibl>
                    <bibl style=""text-align: left;"">isuster, m.y. (2020) ‘from students to authors: fostering student content creation with scalar’, 
                        <hi rend=""italic"">college & undergraduate libraries</hi>, 27(2-4), pp. 133–148. 
                        <ref target=""https://doi.org/10.1080/10691316.2020.1830908"">10.1080/10691316.2020.1830908</ref>.
                    </bibl>
                    <bibl style=""text-align: left;"">meer, n.m. and chapman, a. (2014) ‘assessment for confidence: exploring the impact that low-stakes assessment design has on student retention’, 
                        <hi rend=""italic"">the international journal of management education</hi>, 12(2), pp. 186–192. 
                        <ref target=""https://doi.org/10.1016/j.ijme.2014.01.003"">10.1016/j.ijme.2014.01.003</ref>.
                    </bibl>
                    <bibl style=""text-align: left;"">sample, m. and schrum, k. (2013) ‘what’s wrong with writing essays: a conversation’, in cohen, d.j. and scheinfedlt, j.t. (eds) 
                        <hi rend=""italic"">hacking the academy : new approaches to scholarship and teaching from digital humanities</hi>. ann arbor, mi: university of michigan press, pp. 87–96.
                    </bibl>
                    <bibl style=""text-align: left;"">tracy, d.g. and hoiem, e.m. (2018) ‘scaffolding and play approaches to digital humanities pedagogy: assessment and iteration in topically-driven courses’, 
                        <hi rend=""italic"">digital humanities quarterly</hi>, 11(4). available at: 
                        <ref target=""http://digitalhumanities.org:8081/dhq/vol/11/4/000358/000358.html"">http://digitalhumanities.org:8081/dhq/vol/11/4/000358/000358.html</ref>.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>

",1.0,1.0,Voyant
11910,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,The case for DH in Literary scholarship,,Elena Pierazzo,"paper, specified ""long paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left; "">Not all Humanities have been equally touched by the digital. For textual scholarship, history and linguistics, for instance, we can have a substantial number of scholarly contributions, particularly when we include experiences embodied in projects and resources. However, comparatively speaking, digital literary criticism has had few followers. An exception are Computational Literary Studies (CLS) that apply quantitative methods to large amount of literary and bibliometric data. Linked to the methods of distant reading [Moretti, 2005], this approach enjoys great success today, while web resources like Voyant, software like Gephi, and programming environments like R, have made text mining very accessible, even for those with limited computer skills. Linked to this approach, stylometry and authorship attribution are also thriving. Particularly mediatized researches are the initiatives that led to ""unmasking"" the identies of Robert Galbraith, a pseudonym of J.K. Rowling, and Elena Ferrante [Joula, 2015; Tuzzi and Cortelazzo, 2018]. However, literary criticism connected to close reading seems almost absent from the DH radar. The CATMA tool, designed to define personalized tagsets for (mainly) literary analysis [Meister 2020], represents a bright exception. Meister, in fact, is one of the few scholars that has engaged with digital literary criticism and digital hermeneutics; the latter has been explored also by Van Zundert (2016) and Ramsey (2011), but from a quantitative perspective. Relatively few scholars in DH have to addressed literary criticism with qualitative approaches, which are, conversely, among the most important for non-digital literary scholars.</p>
            <p style=""text-align: left; "">The reasons for this absence are probably to be found in the controversies about the use of markup within texts that have inflamed the scholarly community since the Eighties. The act of adding explicit markers in the text has been subjected to scrutiny, as it is perceived (rightly) as a harbinger of interpretation and this fact has been (and is, to a certain extent, still) perceived as an invasion, a disfigurement of the text; Cummings (2008) gives a vivid account of the debate and reflects on how it has limited the use of TEI for literary criticism. The argument goes that once the text is marked up, it cannot be reused by others because the interpretation added by the encoder would make it unusable. According to this vision, digital texts must be made available in their most neutral and objective form, and any form of annotation, including editorial, must be avoided. Sperberg-McQueen 1991 and Cummings 2008, amongst others, have tried to address the issue, and I have argued elsewhere on the hermeneutic fallacy of the category of objectivity [Pierazzo 2015]; but these methods remain far from impacting “the Humanities at large” and in particular the literary scholars [Meister 2020]. However, in order to contextualize this debate, one should go back to when this controversy was born. The urgency of those years was to put texts online, to create literary corpora for concordances and the study of word frequencies; at the time, digital acquisition of texts, the transformation of the printed into sequences of characters to be analyzed by computers (Machine Readable Form) was mostly done by hand, with an enormous expenditure of time and energy. The emphasis was therefore on making texts available and on the need of not repeating work. Researchers did not want to work with texts full of manually added codes which then had to be removed just as manually in order to reuse the texts.</p>
            <p style=""text-align: left; "">It is worth noting, though, how this discourse hides the concept of DH as a service: the goal was thought produce resources for others to do “real” research. This argument is not only dangerous, condemning DH to a mere service, but also wrong, as text, any text, can only be the result of the dialectical compromise between the source documents that contains it and scholars that interpret it (even when they “only” transcribe it), and therefore no text can ever be considered objectively neutral [Pierazzo, 2015]. Today conditions have changed: most literary texts are digitally available in many versions, not to mention the plethora of tools and methods to “get rid of” markup, therefore the objections do not stand in the same way.</p>
            <p style=""text-align: left; "">Another obstacle for the uptake of DH in literary studies is the conviction that close reading and critical interpretation only require a reader, a text, and a (printed) essay, and therefore computers, in this context, are useful as typewriters [Kirschenbaum, 2016]. Yet, the lack of experimentation and engagement of the scholarly community in DH for literary analysis does not allow for a clear assessment of the epistemological added value of using computers for one or few texts at a time. But shouldn’t be this the moment for rethinking Digital Literary Studies? Couldn’t we at least try to use markup, ontologies and other methods to understand a text, or answer questions about interpretation?</p>
            <p style=""text-align: left; "">The paper will present some experiences at the University of Tours using TEI markup for the history of ideas, and ontologies and databases for analysis of fictional entities (people and places). We have applied these methods to works by Boccaccio, the Vite by Vasari, and to a small corpus of librettos of the 17th century. These experiments are showing promising results, not only in literary terms, but also on a largely methodological perspective, with colleagues and researchers finding themselves challenged and enticed by DH heuristics.</p>
            <p style=""text-align: left; "">Conditions are ripe for experiences and discussions in order to evaluate the impact of DH in literary studies, particularly in the light of the advancements in HTR and other types of CLS that have the potentials of bringing a large amount of unknown and understudied texts into the literary arena. This could truly change our perspectives and understandings on literature, but we need to sharpen our hermeneutical tools first.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listBibl>
                    <head>Bibliography</head>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Cummings, J.</hi>, 2008. The text encoding initiative and the study of literature. In 
                        <hi rend=""italic"">A companion to digital literary studies</hi> (pp. 451-76), Blackwell.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Kirschenbaum, M.G.</hi>, 2016. What is digital humanities and what’s it doing in English departments?. In 
                        <hi rend=""italic"">Defining Digital Humanities</hi> (pp. 211-220). Routledge.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Juola, P.</hi>, 2015. The Rowling case: A proposed standard analytic protocol for authorship questions. 
                        <hi rend=""italic"">Digital Scholarship in the Humanities</hi>, 30(1): 100-113.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Pierazzo, E.</hi>, 2015. 
                        <hi rend=""italic"">Digital scholarly editing: Theories, models and methods</hi>. Routledge.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Ramsay, S.</hi>, 2011. 
                        <hi rend=""italic"">Reading Machines: Toward and Algorithmic Criticism</hi>. University of Illinois Press.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Sperberg-McQueen, C.M.</hi>, 1991. Text in the electronic age: Texual study and textual study and text encoding, with examples from medieval texts. 
                        <hi rend=""italic"">Literary and Linguistic Computing</hi>, 6(1): 34-46.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Tuzzi, A. and Cortelazzo, M.A.</hi>, 2018. What is Elena Ferrante? A comparative analysis of a secretive bestselling Italian writer. 
                        <hi rend=""italic"">Digital Scholarship in the Humanities</hi>, 33(3): 685-702.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">Van Zundert, J.J.</hi>, 2016. Screwmeneutics and hermenumericals: the computationality of hermeneutics. 
                        <hi rend=""italic"">A companion to digital humanities</hi>. (pp. 331-347) Blackwell.
                    </bibl>
                </listBibl>
            </div>
        </back>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,interpretation;literarary analysis;semantics,English,"contemporary;english;global;literary studies;meta-criticism (reflections on digital humanities and humanities computing);text encoding and markup language creation, deployment, and analysis",2022-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left; "">not all humanities have been equally touched by the digital. for textual scholarship, history and linguistics, for instance, we can have a substantial number of scholarly contributions, particularly when we include experiences embodied in projects and resources. however, comparatively speaking, digital literary criticism has had few followers. an exception are computational literary studies (cls) that apply quantitative methods to large amount of literary and bibliometric data. linked to the methods of distant reading [moretti, 2005], this approach enjoys great success today, while web resources like voyant, software like gephi, and programming environments like r, have made text mining very accessible, even for those with limited computer skills. linked to this approach, stylometry and authorship attribution are also thriving. particularly mediatized researches are the initiatives that led to ""unmasking"" the identies of robert galbraith, a pseudonym of j.k. rowling, and elena ferrante [joula, 2015; tuzzi and cortelazzo, 2018]. however, literary criticism connected to close reading seems almost absent from the dh radar. the catma tool, designed to define personalized tagsets for (mainly) literary analysis [meister 2020], represents a bright exception. meister, in fact, is one of the few scholars that has engaged with digital literary criticism and digital hermeneutics; the latter has been explored also by van zundert (2016) and ramsey (2011), but from a quantitative perspective. relatively few scholars in dh have to addressed literary criticism with qualitative approaches, which are, conversely, among the most important for non-digital literary scholars.</p>
            <p style=""text-align: left; "">the reasons for this absence are probably to be found in the controversies about the use of markup within texts that have inflamed the scholarly community since the eighties. the act of adding explicit markers in the text has been subjected to scrutiny, as it is perceived (rightly) as a harbinger of interpretation and this fact has been (and is, to a certain extent, still) perceived as an invasion, a disfigurement of the text; cummings (2008) gives a vivid account of the debate and reflects on how it has limited the use of tei for literary criticism. the argument goes that once the text is marked up, it cannot be reused by others because the interpretation added by the encoder would make it unusable. according to this vision, digital texts must be made available in their most neutral and objective form, and any form of annotation, including editorial, must be avoided. sperberg-mcqueen 1991 and cummings 2008, amongst others, have tried to address the issue, and i have argued elsewhere on the hermeneutic fallacy of the category of objectivity [pierazzo 2015]; but these methods remain far from impacting “the humanities at large” and in particular the literary scholars [meister 2020]. however, in order to contextualize this debate, one should go back to when this controversy was born. the urgency of those years was to put texts online, to create literary corpora for concordances and the study of word frequencies; at the time, digital acquisition of texts, the transformation of the printed into sequences of characters to be analyzed by computers (machine readable form) was mostly done by hand, with an enormous expenditure of time and energy. the emphasis was therefore on making texts available and on the need of not repeating work. researchers did not want to work with texts full of manually added codes which then had to be removed just as manually in order to reuse the texts.</p>
            <p style=""text-align: left; "">it is worth noting, though, how this discourse hides the concept of dh as a service: the goal was thought produce resources for others to do “real” research. this argument is not only dangerous, condemning dh to a mere service, but also wrong, as text, any text, can only be the result of the dialectical compromise between the source documents that contains it and scholars that interpret it (even when they “only” transcribe it), and therefore no text can ever be considered objectively neutral [pierazzo, 2015]. today conditions have changed: most literary texts are digitally available in many versions, not to mention the plethora of tools and methods to “get rid of” markup, therefore the objections do not stand in the same way.</p>
            <p style=""text-align: left; "">another obstacle for the uptake of dh in literary studies is the conviction that close reading and critical interpretation only require a reader, a text, and a (printed) essay, and therefore computers, in this context, are useful as typewriters [kirschenbaum, 2016]. yet, the lack of experimentation and engagement of the scholarly community in dh for literary analysis does not allow for a clear assessment of the epistemological added value of using computers for one or few texts at a time. but shouldn’t be this the moment for rethinking digital literary studies? couldn’t we at least try to use markup, ontologies and other methods to understand a text, or answer questions about interpretation?</p>
            <p style=""text-align: left; "">the paper will present some experiences at the university of tours using tei markup for the history of ideas, and ontologies and databases for analysis of fictional entities (people and places). we have applied these methods to works by boccaccio, the vite by vasari, and to a small corpus of librettos of the 17th century. these experiments are showing promising results, not only in literary terms, but also on a largely methodological perspective, with colleagues and researchers finding themselves challenged and enticed by dh heuristics.</p>
            <p style=""text-align: left; "">conditions are ripe for experiences and discussions in order to evaluate the impact of dh in literary studies, particularly in the light of the advancements in htr and other types of cls that have the potentials of bringing a large amount of unknown and understudied texts into the literary arena. this could truly change our perspectives and understandings on literature, but we need to sharpen our hermeneutical tools first.</p>
        </body>
        <back>
            <div type=""bibliogr"">
                <listbibl>
                    <head>bibliography</head>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">cummings, j.</hi>, 2008. the text encoding initiative and the study of literature. in 
                        <hi rend=""italic"">a companion to digital literary studies</hi> (pp. 451-76), blackwell.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">kirschenbaum, m.g.</hi>, 2016. what is digital humanities and what’s it doing in english departments?. in 
                        <hi rend=""italic"">defining digital humanities</hi> (pp. 211-220). routledge.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">juola, p.</hi>, 2015. the rowling case: a proposed standard analytic protocol for authorship questions. 
                        <hi rend=""italic"">digital scholarship in the humanities</hi>, 30(1): 100-113.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">pierazzo, e.</hi>, 2015. 
                        <hi rend=""italic"">digital scholarly editing: theories, models and methods</hi>. routledge.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">ramsay, s.</hi>, 2011. 
                        <hi rend=""italic"">reading machines: toward and algorithmic criticism</hi>. university of illinois press.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">sperberg-mcqueen, c.m.</hi>, 1991. text in the electronic age: texual study and textual study and text encoding, with examples from medieval texts. 
                        <hi rend=""italic"">literary and linguistic computing</hi>, 6(1): 34-46.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">tuzzi, a. and cortelazzo, m.a.</hi>, 2018. what is elena ferrante? a comparative analysis of a secretive bestselling italian writer. 
                        <hi rend=""italic"">digital scholarship in the humanities</hi>, 33(3): 685-702.
                    </bibl>
                    <bibl style=""text-align: left; "">
                        <hi rend=""bold"">van zundert, j.j.</hi>, 2016. screwmeneutics and hermenumericals: the computationality of hermeneutics. 
                        <hi rend=""italic"">a companion to digital humanities</hi>. (pp. 331-347) blackwell.
                    </bibl>
                </listbibl>
            </div>
        </back>
    </text>

",1.0,1.0,Voyant
11948,2022 - Tokyo,Tokyo,Responding to Asian Diversity,2022,ADHO,ADHO,,Tokyo,,Japan,https://dh2022.adho.org/,The Linked Editorial Academic Framework: Creating an editorial environment for collaborative scholarship and publication,,Diane Katherine Jakacki;Susan Brown;James Cummings;Mihaela Ilovan;Carolyn Black,"paper, specified ""short paper""","<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left;"">This short paper introduces LEAF (the Linked Editorial Academic Framework virtual research environment), an enhanced and expanded collaborative editorial platform that supports a variety of digital scholarly projects through a pipeline of integrated tools for collaborative production and publication of scholarly and documentary collections. Funded through the Canada Foundation for Innovation and the Andrew W. Mellon Foundation, LEAF aims to address the challenges that face many who undertake and maintain large-scale collaborative DH projects now: namely, the need to ensure that these projects can remain operational and available to editors and audiences over the long-haul. It is only by sharing physical, software, and human infrastructures across institutions that this can be accomplished. In so doing we can support scalability, interoperability, and preservation while allowing for dynamic, iterative, and collaborative editing, and therefore ensure that our materials, collections, and editions will remain viable and accessible. The LEAF team aims to do this by integrating best practices for text encoding, annotation, and metadata standards. This short paper will report on the development of LEAF and the functionalities that it will provide. </p>
            <p style=""text-align: left;"">The implementation, and dissemination of LEAF is built upon a collaboration to extend the Canadian Writing Research Collaboratory (CWRC) built by the Universities of Alberta and Guelph (Susan Brown) with Bucknell University (Diane Jakacki), and Newcastle University (James Cummings) as founding partners. This work enhances CWRC’s functionality through collaborative software development that will ultimately support multiple instances of the LEAF platform in Canada, the US, and the UK. At Bucknell, this work will inform the Liberal Arts Based Digital Edition Publishing Cooperative and the Bucknell Digital Press, funded by an Andrew W. Mellon Digital Publishing Cooperative Implementation grant that will support an expanding portfolio of peer-reviewed digital editions and edition clusters. </p>
            <p style=""text-align: left;"">
                <hi style=""font-size:12pt"" xml:space=""preserve"">The LEAF platform combines hardware, software, and personnel. LEAF is being built on a solid foundation in terms of its data models, core functionality, and code management, so that it is positioned for extension and long-term sustainability. The platform is based on the Islandora 8 framework, which combines Drupal 8 with a Fedora 5 repository for long-term preservation. The LEAF repository will customize and enhance Islandora to enable digital humanities workflows and publication needs. Enhancements include an innovative web-based editing tool that allows users to employ TEI XML along with Web Annotation and IIIF standards-compatible Linked Open Data annotations that enhance discoverability and interoperability. </hi>
            </p>
            <p style=""text-align: left;"">
                The founding LEAF institutions are collaborating to upgrade the existing CWRC environment and produce a fully modular platform that will also be hosted on Bucknell’s servers, further tested at Newcastle University, and offered as containerized open-source code freely available for download and installation by other institutions. In particular, LEAF will facilitate the production and publication of dynamic digital scholarly editions and collections, offering multilingual transcription, translation, and image markup. Entirely browser-based, its functionality includes an in-browser XML markup editor, XML rendering tools, built-in text and data visualization tools including the Voyant Tools suite and its Dynamic Table of Contexts Browser. Overall the LEAF platform will provide a sophisticated interface for digital editions in which the XML markup is leveraged for navigation and active reading, and enhanced with Linked Open Data.
            </p>
        </body>
    </text>

",xml,This text is republished here with permission from the original rights holder.,,collaborative publication;critical infrastructure;preservation,English,"15th-17th century;19th century;20th century;analysis;cultural studies;english;global;literary studies;public humanities collaborations and methods;scholarly editing and editions development, analysis, and methods",2022-01-01,"<text xmlns=""http://www.tei-c.org/ns/1.0"" xml:lang=""en"">
        <body>
            <p style=""text-align: left;"">this short paper introduces leaf (the linked editorial academic framework virtual research environment), an enhanced and expanded collaborative editorial platform that supports a variety of digital scholarly projects through a pipeline of integrated tools for collaborative production and publication of scholarly and documentary collections. funded through the canada foundation for innovation and the andrew w. mellon foundation, leaf aims to address the challenges that face many who undertake and maintain large-scale collaborative dh projects now: namely, the need to ensure that these projects can remain operational and available to editors and audiences over the long-haul. it is only by sharing physical, software, and human infrastructures across institutions that this can be accomplished. in so doing we can support scalability, interoperability, and preservation while allowing for dynamic, iterative, and collaborative editing, and therefore ensure that our materials, collections, and editions will remain viable and accessible. the leaf team aims to do this by integrating best practices for text encoding, annotation, and metadata standards. this short paper will report on the development of leaf and the functionalities that it will provide. </p>
            <p style=""text-align: left;"">the implementation, and dissemination of leaf is built upon a collaboration to extend the canadian writing research collaboratory (cwrc) built by the universities of alberta and guelph (susan brown) with bucknell university (diane jakacki), and newcastle university (james cummings) as founding partners. this work enhances cwrc’s functionality through collaborative software development that will ultimately support multiple instances of the leaf platform in canada, the us, and the uk. at bucknell, this work will inform the liberal arts based digital edition publishing cooperative and the bucknell digital press, funded by an andrew w. mellon digital publishing cooperative implementation grant that will support an expanding portfolio of peer-reviewed digital editions and edition clusters. </p>
            <p style=""text-align: left;"">
                <hi style=""font-size:12pt"" xml:space=""preserve"">the leaf platform combines hardware, software, and personnel. leaf is being built on a solid foundation in terms of its data models, core functionality, and code management, so that it is positioned for extension and long-term sustainability. the platform is based on the islandora 8 framework, which combines drupal 8 with a fedora 5 repository for long-term preservation. the leaf repository will customize and enhance islandora to enable digital humanities workflows and publication needs. enhancements include an innovative web-based editing tool that allows users to employ tei xml along with web annotation and iiif standards-compatible linked open data annotations that enhance discoverability and interoperability. </hi>
            </p>
            <p style=""text-align: left;"">
                the founding leaf institutions are collaborating to upgrade the existing cwrc environment and produce a fully modular platform that will also be hosted on bucknell’s servers, further tested at newcastle university, and offered as containerized open-source code freely available for download and installation by other institutions. in particular, leaf will facilitate the production and publication of dynamic digital scholarly editions and collections, offering multilingual transcription, translation, and image markup. entirely browser-based, its functionality includes an in-browser xml markup editor, xml rendering tools, built-in text and data visualization tools including the voyant tools suite and its dynamic table of contexts browser. overall the leaf platform will provide a sophisticated interface for digital editions in which the xml markup is leveraged for navigation and active reading, and enhanced with linked open data.
            </p>
        </body>
    </text>

",1.0,1.0,Voyant
