
Introduction
One barrier to locating serialized fiction in a digital newspaper archive is the fact that the serialized fiction themselves are not indexed, and individual articles do not have subject terms or tags associated with them that would identify them as fiction. As a result, articles are difficult to find unless the reader browses a large volume of issues or simply hits upon a salutary keyword search. Keyword searching of the collection is more effective for articles on topics in farming or farm life than for works of fiction. Unless the reader is looking for stories by a specific author or for a known story title, keyword searching of fiction is highly ineffective.

While the software used by most archives does a good job of connecting articles in a single issue, the reader does not know where to find the next installment in a serialized work of fiction, so he or she has to find it manually by browsing the collection or doing a keyword search. Finally, while the OCR scanning was done to the highest standard, this is an imperfect process, and much of the content cannot be adequately OCR’d due to background noise and broken letters, features of the original newspapers that impede scanning.

This paper summarized the process of our project that was completed over 15 weeks in the summer of 2012. Our goal was to complete the manual indexing process that had already been started previously, display serialized fiction articles in a new repository, evaluate multiple software packages to see which ones were the most promising for use in the future, and evaluate any automated ways of finding serialized fiction.

Method and Results
Manual Indexing
We experimented with three digital library systems, a Drupal/Fedora based repository (Islandora) (http://islandora.ca/), converting the fiction into TEI P5 and displaying it in the California Digital Library’s eXtensible Text Framework (XTF) (http://www.cdlib.org/services/publishing/tools/xtf/) and Omeka (http://omeka.org) a PHP-based publishing platform for digital library objects. We were unable to get Islandora’s OCR correction module installed so we stopped using it in favor of Omeka. We used XSLT to transform the PrXML into very simple TEI5 files, which we were able to upload to XTF, but the lack of an editor and the intensely manual process of text encoding was also rejected in favor of Omeka. TEI Example: http://uller.grainger.uiuc.edu:8080/xtf/search

Ultimately, we decided on using Omeka with the Scripto Plugin for correction. Serialized fiction articles in one title, the Farmer’s Wife, was manually indexed in a spreadsheet, and graduate assistants converted those stories from PrXML into an exhibit, added Dublin Core metadata and links to the newspaper archive from the new serialized fiction collection. The end result was index of serialized fiction that would increase the accessibility of these articles. Omeka Exhibit: http://uller.grainger.illinois.edu/omeka/

Crowdsourcing OCR correction
The University of Illinois Digital Newspaper collections are in Olive Software ActivePaper Archive, which has a method for administrators to correct text but not users. Omeka provides a plugin called ‘Scripto’ for text correction that we were able to successfully use to correct the text in selected articles. We also evaluated Veridian (http://www.dlconsulting.com/veridian/), which is a commercial digital newspaper library solution used by Trove Digitised Newspapers (National Library of Australia), From the Page (http://beta.fromthepage.com/) and Islandora (http://www.islandlives.ca/). From the Page and Islandora were both very difficult to install and administer, and while not free we felt Veridian was a much better approach and we are evaluating it as part of our future newspaper digitization efforts.

Text Analysis
How can we identify serialized fiction without having to have a human find it, index it in a spreadsheet and manually extract it from the archive? Certain n-grams are common within serialized fiction such as ‘chapter’, ‘the end’, ‘to be continued’ and could be used to simply search for keywords within documents; we could also calculate which words occur most frequently in fiction vs. other types of articles and use those terms to automatically tag articles.

We also evaluated using topic analysis to find fiction. We evaluated the 580 articles we had already identified as serialized fiction using Mallet to find 25 topics with 25 words each. Figure 1. shows the top 25 topics modeled as a network using Gephi, while figure 2 shows the topic words ordered by frequency.


Figure 1


Figure 2

Nodes were ranked by betweenness centrality and topic 14 had the highest at 51,321.01 and its component n-grams along with the other top topics could be used to find serialized fiction in other titles.

One final text analysis technique that could be useful is identifying proper names is Named Entity Extraction. While we made an effort to manually remove names from the topic analysis, as you can see they kept reappearing in the results. By using named entity extraction we could eliminate proper names from the topic analysis to make them more accurate, and to link fiction together by the character’s names. All three of these techniques (keyword frequency, topic analysis, named entity extraction) I plan on evaluating in a future study.

Conclusion
Serialized fiction is an important component of historical newspapers and by making it more accessible to patrons and researchers we can expand the use and usefulness of our digital newspaper collections. The manual indexing approach was relatively inexpensive to accomplish but was time consuming and difficult to do over a large corpus of pages. Two promising approaches to find and digitize serialized fiction in our newspaper archive are adding a crowdsourcing feature to enable users to identify article types and correct mistakes, and utilizing text analysis techniques to identify fiction programmatically. We hope to report on our efforts at the latter at the DH 2013 conference.

References
Bastian M., S. Heymann, and M. Jacomy (2009). Gephi: an open source software for exploring and manipulating networks. International AAAI Conference on Weblogs and Social Media.
Brandes, U. (2001). A Faster Algorithm for Betweenness Centrality. Journal of Mathematical Sociology25 163-177.
Cohen, D. (2008). Introducing Omeka. Dan Cohen's Digital Humanities Blog. http://www.dancohen.org/2008/02/20/introducing-omeka/.
Islandora. http://islandora.ca/.
Island Lives. http://www.islandlives.ca/
McCallum, A. K. (2002). MALLET: A Machine Learning for Language Toolkit. http://www.cs.umass.edu/~mccallum/mallet.
Veridian. http://www.dlconsulting.com/veridian/.
In his 1872 book The Expression of Emotions in Man and Animals (Darwin 1872), Charles Darwin drew our attention to the relationship between human expressions, movements and emotional states, and tried to frame his conclusions by highlighting the similarities between humans and animals. The light he shed on cultural differences with respect to the appearance of the face and variations in expressions perceived across different groups to communicate the same emotions is also important.
More recent scientific evidence highlights the importance that the human beings give to the face. The brain has a specialized amygdala to discriminate scenes in favor of facial expressions, a primitive mechanism to detect potentially dangerous situations (Hariri et al. 2002), that explains our impulse to immediately look at people faces and try to read their facial expression. Finally, the recent discovery of “mirror neurons” (Rizzolatti and Craighero 2004) and their connection with the imitative ability of several primates offers a glimpse about the social construction of emotions, helps to explain the spread of behaviors within human groups, and opens up the possibility of a phenomenology of human expressions.
Nevertheless, the fascination with the human face is not something new in humanities. It has been present since the beginnings of art history; artists have always sought to relate the face to the human body (Chase 2005) and, especially, to the different ways in which how faces reflect the human condition. In this sense, human facial representations contain a human expressions and emotions archive that can help us understand, through a science of the face (Cleese and Ekman 2001), various human condition traits that evolved through time and space. We aim to answer questions about periods in art history, such as the Baroque significance as a culture derived from human expansion. Even if we cannot say that Baroque is just as a historical period, before the discovery of America all faces in art were mostly european; the presence of indigenous faces in paintings is a big disrupt. Another example would be the cultural meaning of the progressive human face disappearing from modern painting. Our methodology analyzes this through facial recognition techniques, data mining, graph theory and visualization and cultural history.
Quantitative analysis of huge amounts of data has provided answers to new and different questions that otherwise couldn’t have been considered (Michel et al. 2011). The study borrows some ideas from the Culturomics (Michel et al. 2011) concept by creating a set of more than 123,500 paintings from all periods of art history, and applying a face recognition algorithm used in Facebook’s photo-tagging system#. The result is a set of over 26,000 faces ready to be analyzed according to several features extracted by the algorithm.
The extracted information accuracy may fluctuate depending on several factors, such as the reproduction quality and size; the thematic content, a portrait is not the same than a hunting scene; the pictorial style, e.g., Cubism versus Figurative art; the lighting, dark and hellish overtones as opposed to daytime or celestial images; or even the contrast between the background and skin tone. At the same time, these elements also provide information about how human faces are depicted by authors, styles and cultures across time. Once a face has been recognized, it provides data about the position of eyes, mouth, chin and ears — what we call the face basic features. If the confidence measure we have reached during the recognition process is satisfactory, we can consider gender, mood, position of lips, range of age or even if the person is wearing glasses — we call these extended features of facial recognition.
 
Figure 1:
Example of face graph for two randomly selected paintings. The red crosses indicate what we called face basic features. The blue lines show the different distances in between the features.
The methodology tackles the study of this huge amount of features in three steps. First, we build a graph with the set of basic features. Second, we look for clusters in the extended features. Third, we compare the graphs and the clusters, corresponding to the basic and extended features respectively, using time intervals and space as factors of the comparison. The selection of the procedures in each of the steps considers different strategies to address the fluctuations in accuracy. What follows is a more detailed explanation of the main mathematical and computational tools used for the processing of the data in each step.
For the basic features graph (Figure 1), we apply the Elastic Bunch Graph Matching (Wiskott et al. 1997) to the extracted data, and then we calculate the resulting weighted graph after applying graph similarity functions like Euclidean Geometry Similarity and Least Square Geometry Similarity. Once the graphs that represent the 26,000 faces are done, we normalize the weights and prune those relationships with weights below the first quartile. Then, a combination of YiFan Hu Multilevel (Hu 2005) and ForceAtlas (Bastian et al. 2009) layouts algorithms are run against the graph to see how the faces are clustered according to their modularity class.
For the extended features clustering, we create a vector for each face and run the K-Means method (Sculley 2010) that is able to cluster unlabeled data, i.e there is no need of a pre-training process but the classification emerges naturally from the data similarities. We check the clusters obtained according to their homogeneity and completeness, according to the definitions given by Rosenberg and Hirschberg (Rosenberg and Hirschberg 2007), in order to establish a good value for V-measure, a conditional entropy-based external cluster evaluation measure that indicates the success of a clustering solution.
Finally, we compare the two sets: the basic features set using graphs and the extended features set using clustering by K-Means method (Sculley 2010). At this point, we are at the perfect position to analyze and characterize each of the groups according to different historical perspectives and cultural questions, for instance, the distinction among styles by giving a minimum set of features that determines its membership. A similar set of features is obtained for a particular interval of time or a specific geographical region. Extending that analysis we are able to study the evolution of these sets of features over time.
In this study, we tackle three important issues that also show the potential of this kind of work to develop new approaches to cultural history and to establish some yardsticks in order to complement the incipient methodology for a Big History (Christian 2004) approach to human culture. First, we show how the analysis of the representation of human faces — both the internal features and in its relative position to the rest of the composition — offers important data to determine periods and borders in the history of art beyond the generalizations supported by the notions of “style”, “genre” and “national history”. Second, we study the correlations between the European expansion overseas from the 16th Century onwards, and the introduction of new human “types” in world paintings, focusing on concepts of identity and gender (with special emphasis on the size and form of the forehead), and relating the results to notions of Baroque, hybridization and globalization (Suarez 2007; Suarez 2008). Finally, we move to the 20th Century and study the disappearance of the human face from art in relation to Ortega y Gasset’s concept of the “dehumanization of art” (Ortega y Gasset 1968) and the artistic and political movements of the first half of the century.
References
Bastian M., S. Heymann, and M. Jacomy (2009). Gephi: an open source software for exploring and manipulating networks. International AAAI Conference on Weblogs and Social Media.
Chase, P. G. (2005). The Emergence of Culture: The Evolution of a Uniquely Human Way of Life. Birkhäuser.
Christian, D. (2004). Maps of time: an introduction to big history. Heldref Publications.
Cleese, J., and P. Ekman. (2001) The Human Face. [BBC] John Cleese, Paul Ekman. http://www.bbc.co.uk/programmes/b00pfqy6
Darwin, C. (1872). The expression of the emotions in man and animals. London: John Murray.
Hariri, A. R., et al. (2002). The Amygdala Response to Emotional Stimuli: A Comparison of Faces and Scenes.NeuroImage 17: 317–323.
Hu, Y. (2005). Efficient, high-quality force-directed graph drawing. Mathematica Journal. mathematica-journal.com.
Michel, J. B., et al. (2011). Quantitative Analysis of Culture Using Millions of Digitized Books. Science 331(6014): 176-182.
Ortega y Gasset, J. (1968). The Dehumanization of Art and Other Essays on Art, Culture, and Literature. Princeton Paperbacks. 128.
Press notice: https://developers.face.com/signup/?g and http://face.com/blog/facebook-acquires-face-com/ Accessed October, 31st , 2012.
Rizzolatti, G., and L. Craighero, (2004). The mirror-neuron system. Annual Revision Neuroscience.annualreviews.org
Rosenberg, A., and J. Hirschberg. (2007). V-measure: A conditional entropy-based external cluster evaluation measure'. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning held June 2007 in Prague. 410–420.
Sculley, D. (2010). 'Web Scale K-Means clustering'. Proceedings of the 19th international conference on World wide web.
Suarez, J. L. (2007). Hispanic Baroque: A Model for the Study of Cultural Complexity in the Atlantic World. South Atlantic Review 72(1): 31-47.
Suarez, J. L. (2008). Complejidad y Barroco. Revista de Occidente. 323. 58-74.
Wiskott, L., et al. (1997). 'Face recognition by elastic bunch graph matching. Pattern Analysis and Machine Intelligence'. IEEE Transactions. 19(7): 775-779.
Archaeology has a long history of innovative work with information and computing technology. While there are a small number examples in the late 1950s, the most influential comes courtesy of James Deetz’s seminal work on Arikara ceramics. Carried out in the early 1960s, Deetz’s project used the IBM704 mainframe at the MIT Computation Laboratory to discover "stylistic coherence" on over two thousand rim sherds from central South Dakota Medicine Crow site. Deetz’s work was extremely important as it suggested that computers were excellent tools for statistical, typological, chronological, or stylistic analysis of large and complex sets of data (a hallmark of archaeology).

Since these early days, digital archaeology has remained intently focused on the analysis, interoperability, and preservation of digital data. By the mid-1980s, however, the personal computer had reached a point where they became effective tools for archaeological visualization and imagery. Desktop applications such as GIS, which allowed for the visualization, analysis, and modeling of socio-spatial data, and CAD, which facilitated the production of detailed and geometrically accurate archaeological maps at various scales without time-consuming redrafting, became central in the digital archaeological ecosystem.

In recent years, along with many other disciplines in the humanities and social sciences, archaeology is entering a new age in which information, computing, and communication technology is having transformative impact on all aspects of the field. The archaeological domains and activities in which digital approaches, methods, and technologies are relevant have grown well beyond the traditional trinity of data, GIS, and CAD. All aspects of research (including field and lab methods), teaching, outreach, publication, and scholarly communication are being impacted in new and unpredictable ways by “digital.” Quite frankly, gone are the days in which digital archaeological methods were siloed off from the main body of scholarly practice. In many ways, one might argue that we have entered an age in which all archaeology is digital archaeology and all archaeologists are digital archaeologists.

In is within this context that this session will highlight a series of innovative projects and practices that represent the forefront of work in digital archaeology. Special attention has been made to highlight projects which represent a variety of domains within digital archaeology including digital data, public engagement, data & topic modeling, crowdsourcing, linked-open data, and digital fieldwork records management. All papers in the session also speak to the changed and changing nature of scholarly and professional practice within archaeology, addressing new approaches to collaboration, community engagement, citizen scholarship, cyberinfrastructure, preservation & access, capacity building, and sharing. A second, but no less important, goal of this session is to challenge the rather curious separation that exists between digital archaeology and the digital humanities by clearly placing the two domains parallel to one another and recognizing the fact that they both have much to learn from one another. The ultimate goal in this regard is to foster and support fruitful discussions and collaboration between digital archaeologists and digital humanists.

Topic Modeling Time and Space: Archaeological Datasets as Discourses
Graham, Shawn|Shawn_Graham@carleton.ca|Carleton University, Canada
Topic modeling is very popular at the moment in the digital humanities. A recent tutorial on getting started with this tool explains them as tools for extracting topics or injecting semantic meaning into vocabularies: "Topic models represent a family of computer programs that extract topics from texts. A topic to the computer is a list of words that occur in statistically meaningful ways. A text can be an email, a blog post, a book chapter, a journal article, a diary entry – that is, any kind of unstructured text" (Graham, Weingart, and Milligan 2012). In that tutorial, 'unstructured' means that there is no encoding in the text by which a computer can model any of its semantic meaning.

Archaeological datasets are rich, largely unstructured bodies of text. While there are examples of archaeological datasets that are coded with semantic meaning through xml and Text Encoding Initiative practices, many of these are done after the fact of excavation or collection. In the field, things can be rather different, and this material can be considered to be 'largely unstructured' despite the use of databases, controlled vocabulary, and other means to maintain standardized descriptions of what is excavated, collected, and analyzed. This is because of the human factor. Not all archaeologists are equally skilled. Not all data gets recorded according to the standards. Where some see few differences in a particular clay fabric type, others might see many, and vice versa. Archaeological custom might call a particular vessel type a ‘casserole’, thus suggesting a particular use, only because in the 19th century when that vessel type was first encountered it reminded the archaeologist of what was in his kitchen – there is no necessary correlation between what we as archaeologists call things and what those things were originally used for. Further, once data is recorded (and the site has been destroyed through the excavation process), we tend to analyze these materials in isolation. That is, we write our analyses based on all of the examples of a particular type, rather than considering the interrelationships amongst the data found in the same context or locus. David Mimno in 2009 turned the tools of data analysis on the databases of household materials recovered and recorded room by room at Pompeii. He considered each room as a 'document' and the artefacts therein as the 'tokens' or 'words' within that document, for the purposes of topic modeling. The resulting 'topics' of this analysis are what he calls 'vocabularies' of object types which when taken together can suggest the mixture of functions particular rooms may have had in Pompeii. He writes, 'the purpose of this tool is not to show that topic modeling is the best tool for archaeological investigation, but that it is an appropriate tool that can provide a complement to human analysis....mathematically concrete in its biases'. The ‘casseroles’ of Pompeii turn out to have nothing to do with food preparation, in Mimno’s analysis.

To date, this is the only example of topic modeling applied to archaeological data. As such, it is novel in the digital humanities for applying the tools of data mining not to texts, but to things. In this paper, I explore the use of topic models on another rich archaeological dataset, the Portable Antiquities Scheme database in the UK. The Portable Antiquities Scheme is a project "to encourage the voluntary recording of archaeological objects found by members of the public in England and Wales". To date, there are over half a million unique records in the Scheme's database. I use topic modeling of this database to tease out archaeological patterns -the discourses of topic modeling, to use Ted Underwood's phrasing - over both time and space. In order to visualize these discourses, I map them both in geographic and relational space, using the network analysis program Gephi. The constellation of ideas (the resultant ‘topics’) that make up the various discourses in the data can be represented as nodes while the strengths of the associations suggested by the topic model can be represented as edges. This two-mode graph (words and ‘topics’ or ‘discourses’) can be queried for deeper structure. I look at the modularity of this graph to determine ‘communities’ of ideas or discourses. I then lay this network against real geographic space by time-slice to understand changes over time and space in the Portable Antiquities Scheme data. I agree with Mimno's suggestion that this is an appropriate tool for the digital archaeologist, but try to understand the limitations, caveats, and lessons for digital humanities more generally, from this application.

The Archaeological Resource Cataloging System (ARCS): An Open-Source Solution to Digitizing an Archaeological Archive
Frey, Jon M.|freyjona@msu.edu|Michigan State University, United States of America
Adams, Brian|adamsb@msu.edu|Michigan State University, United States of America
Schopieray, Scott|schopie1@msu.edu|Michigan State University, United States of America
Over the past few decades, archaeologists have begun to realize the benefits of providing archival records in digital form. Whether information is collected electronically or digitized from pre-existing materials, digital archaeological data should be readily accessible from anywhere in the world. These developments have increased the productivity of scholars who no longer need to visit the actual archive and eased the strain on those projects that must accommodate visiting researchers in addition to their normal daily operations. On the other hand, the creation of digital archives has drastically impacted the ways in which we interact with documents and artifacts that form the basis of archaeological research. Financially constrained projects have lagged behind their better-funded peers in the process of digitization and dissemination of electronic records. As a result, instead of providing greater access to a wider range of archaeological data, the process of archival digitization runs the risk of further privileging the evidence of those surveys and excavations with greater financial resources. In addition, many of the existing digital archaeological archives concentrate upon artifacts to the point that the archaeologist’s field journal, arguably the most important evidence in establishing context, is rarely presented in its original form. Even where diverse forms of information are provided, a digital archive often encourages the study of objects and documents in isolation from one another and without the benefit of the institutional memory that often aids in their interpretation. While a traditional archive allows an individual to conduct their research through physical interaction with a number of different archival materials at once, often in the presence of those who discovered and prepared them, many digital archives simply rely on keyword searches to generate lists of electronic records that to the untrained eye appear to be of equal value as forms of evidence.

In this paper, we present the Archaeological Resource Cataloging System (ARCS), an open-source digital asset management application created for the Ohio State University Excavations at Isthmia in order to address these issues. Developed at Michigan State University through an NEH Digital Humanities Startup Grant, ARCS utilizes a web-based interface that allows authorized users to upload and “tag” digital resources consistently according to generally accepted metadata standards that can be further refined to reflect any project’s unique terminology. These resources can then be searched, sorted into collections and connected to one another through the creation of virtual links without affecting the integrity of the original data. In this way the essential interrelatedness of the various forms of archaeological data is preserved in a flexible electronic format. In order to foster a better sense of community among researchers, each resource is also provided with a discussion tool that allows users to ask questions or identify mistakes, thereby making use of others’ knowledge and experience to cultivate the development of the dataset. In addition, because ARCS depends on the collective effort of a community of users, the system generates a permanent record of all additions and modifications of resources so that errors can be easily corrected and dependable users more clearly identified.

Perhaps most importantly, because it is an open-source application that relies on multiple users to develop and manage the digital assets of an excavation or survey, ARCS offers an affordable option for archaeological projects that lack a dedicated digital archivist or IT specialist. Digital data can be added as it is made available and, once uploaded, new resources can be linked to body of evidence that continues to grow in size and detail. In the end, ARCS not only retains the many benefits of more traditional research involving physical documents at an actual archive but in many ways also speeds and simplifies the process of archaeological investigation.

“All of Us Would Walk Together": Digital Cultural Heritage and the African American Past at Historic St. Mary's City, Maryland
Brock, Terry P.|brockter@msu.edu|Michigan State University, United States of America
In October 2012, Historic St. Mary's City (HSMC) launched a digital exhibit and social media campaign focused on the 19th-century component of their museum. HSMC, an archaeology and living history museum, has traditionally focused its 17th-century component, which was Maryland's first capital city. The digital exhibit, however, allowed the museum to begin interpreting additional centuries without disrupting the 17th-century landscape. Additionally, the digital exhibit, HSMC is able to develop an approach that focuses on public communication and engagement, allows for transparent research methods and interpretations, and provides flexibility when integrating the content into future programming and on-site exhibit. Through a combined approach of a content based digital exhibit, research blog, and social media, the digital exhibit, called "All of Us Would Walk Together", provides an example of digital archaeology that incorporates contemporary concepts of public archaeology through digital exhibitions and research methods. This paper will discuss how this has been put into action.

During the past few decades, community engagement has become a critical component of African American archaeology. Starting with the public excavations at the African American Burial Ground project in New York City, researchers have begun to incorporate local communities and descendants in the development and implementation of research projects and museum exhibits. Establishing a transparent, reciprocal, and pragmatic back-and-forth has become a valued and integral part of the research process for many archaeologists. Online tools have also been used as a means for public engagement, in particular at the Levi Jordan Plantation and Rosewood. More recently, archaeologists have adopted social media as a means for engaging communities and stakeholders, such as at the Michigan State University Campus Archaeology Program, Florida Public Archaeology Network, and at Mt. Vernon. Although each approach has highlighted different topics and methods for engagement, each has found the use of the web and digital social media to be a beneficial means of engaging the public.

At HSMC, the interpretation of the 19th century had not been the primary focus of the museum. This was particularly evident when the structures relating to the 19th century, including a manor home, its outbuildings, and a former duplex slave and tenant quarter, were physically moved to a different location in 1992 due to its conflict with the 17th-century interpretation. While the buildings continued to be used as a bed and breakfast, they were not used as an interpretive component of the museum, causing memory of the 19th century to be lost to the public. Recently, this story has begun to resurface, due to a number of factors at the museum. Included was the reacquisition of the manor home and outbuildings from the owners of the bed and breakfast, and funding opportunities to interpret the duplex quarter through a digital exhibit and a physical exhibit. In addition to interpreting the site, the goal of the exhibits was also to build a relationship with the African American community and to reinstate the 19th-century story into the public consciousness.

The digital exhibit consists of two components: a traditional exhibit space and social media. The exhibit space presents a number of webpages devoted to the interpretation of the historical and archaeological data that has already been analyzed. These pages trace the transition from slavery to freedom for the African Americans who lived on the site, and uses historical and archaeological evidence to develop the narrative. Interspersed are links to blog posts that discuss how the evidence was gathered or used by researchers to draw the conclusions. Additionally, each exhibit page has a comment field, where the public can ask questions, offer their own interpretation, or provide additional commentary. This allows for two-way communication between the public and researchers, while also allowing the public to engage in the interpretive efforts. Lastly, these pages will be linked to the physical exhibit through the use of QR codes or augmented reality to provide more flexibility to the interpretive efforts at the site of the duplex. In doing so, the exhibit becomes flexible, transparent, and engaged physical space, fitting within the parameters of an engaged cultural heritage project.

The use of social media, through a blog, Twitter, and Facebook, adds extra depth to these efforts. The blog provides the most flexibility and transparency by allowing the exhibit space to be amended, added to, or modified in a transparent way. This provides a great deal of flexibility to the site that only a digital exhibit can provide. For example, if the research results in a major change to the exhibit, a blog post can be written to discuss the change and why it happened. In the exhibit, a link to this post can be added to demonstrate that the research process is a fluid, ongoing process. The blog and Twitter are also used to highlight research at comparable sites or examining comparable themes. This ties the archaeological work conducted at this site to larger themes in the discipline, and begins to build relationships with other institutions, while providing additional resources and access to new scholarship to the public. The blog and Twitter are both instrumental in the preservation and exhibit building process, as they make it more transparent: the public can watch and participate in the decisions about what will be included in the exhibit, in addition to understanding what types of constraints are placed on the construction of an exhibit. Twitter also allows lab and fieldwork to be shared in realtime with the public. Lastly, Twitter provides access to a larger, global network, particularly an African American network, a demographic that uses Twitter more than others. Facebook, on the other hand, is being used to connect with HSMC's current online fans through its official Facebook account.

The project itself has a high set of goals, and is approaching it with a multifaceted approach. While the digital component is a crucial step, it is only part of a larger program of public engagement. For example, HSMC has been actively soliciting feedback from local community members and seeking to engage them through the formation of an advisory board. This reiterates an important tenant of engaging in digital public archaeology: that one cannot rely solely on one approach. Nonetheless, the use of the digital space does provide us with additional exhibit and interpretive space, and gives us a great deal of flexibility when dealing with the public, research, and presentation. Most importantly, the use of the digital arena allows this all to be transparent, reciprocal, and dynamic.

An Introduction to the Practices and Initial Findings of the Digital Index of North American Archaeology (DINAA)
Wells, Joshua J.|jowells@iusb.edu|Indiana University, South Bend
Anderson, David G.|dander19@utk.edu|University of Tennessee, Knoxville
Yerka, Stephen J.|syerka@utk.edu|University of Tennessee, Knoxville
Kansa, Eric C.|ekansa@ischool.berkeley.edu|University of California, Berkeley
Whitcher Kansa, Sarah|skansa@alexandriaarchive.org|Alexandria Archive Institute
Noack Myers, Kelsey|kejmyers@indiana.edu|Indiana University, Bloomington
DeMuth, R. Carl|rcdemuth@gmail.com|Indiana University, Bloomington
The Digital Index of North American Archaeology (DINAA) is a project to create interoperability models for archaeological site databases in the eastern United States, funded by the National Science Foundation (#1216810 & #1217240). The core research team consists of researchers from the Department of Anthropology and Archaeological Research Laboratory at the University of Tennessee, the Alexandria Archive Institute, and the Anthropology and Informatics programs at Indiana University. Open Context (http://opencontext.org) will be used as the primary platform for data dissemination for this project. Our aims are to work with the databases held by State Historic Preservation offices and allied federal and tribal agencies in Eastern North America, with the goal of linking data across state lines for research and management purposes. Redacted of sensitive items, such as site location, data linkages will promote extension and reuse by government personnel in state and federal agencies, and domestic and international researchers. The project will mint stable Web-URIs for each site record, and in doing so, we will help lay the foundations for future Linked Open Data applications in North American archaeology, architectural history, and historical studies.

This project repurposes government curated datasets to support innovative humanistic and social science research. Governmental archaeological site files in North America are important loci for documentary information on known archaeological sites. Their most basic function is to contain data about site types and information quality pursuant to heritage preservation legislation at the federal level, but potentially state and local levels as well. However, as a matter of practice these files, often as relational databases, contain many other data fields that describe important archaeological findings, and other data that serve environmental and bureaucratic functions for management and protection of heritage resources. The ways in which data about archaeological sites are recorded and communicated have an important origin in theoretical models about past behavior, and also have important implications on the professional comprehension of the data at large and the use of the data to rank planning and preservation priorities.

Efforts to collect and compile archaeological data have a long history, and information about archaeological sites and collections is maintained by every state and territory. Only rarely, however, have these data been compiled and examined at large geographic scales, especially those crosscutting state lines, and never to the extent and for the research and management purposes proposed in this project. Data from some 15 to 20 states (>half a million sites) east of the Mississippi will be integrated with a common ontology, based on existing standards, and adapted in collaboration with researchers and government personnel in state and federal agencies. The ontology will classify site files according to cultural affiliation and chronology as well as agency assessments of historical significance.

Linkage of site file and other datasets will facilitate studies of past human adaptation spanning large areas, and lead to greater collaboration between archaeologists and scientists in other disciplines. As examples, the linkage of archaeological data at broad new scales will permit, for the first time, the exploration of exciting new research topics, such as how the human populations in North America responded to climate change, population growth, and/or anthropogenic environmental issues over the past 13,000 years.

The availability of output online in the form of maps and data tables (at significantly reduced spatial resolution, to protect sensitive locations) will enhance public awareness, education, and appreciation for scientific research in general and archaeology in particular. The demonstration that primary archaeological data can be integrated and used to address fundamental questions at such scales will stimulate similar efforts worldwide. Finally, by creating translating routines rather than dictating procedures, this project will foster archaeological cooperation through cyberinfrastructure with a high ratio of benefits to costs.

The project helps achieve broader archaeological concerns regarding professional data management training, research ethics and outreach education. It will foster novel networking and data integration among multiple partners, as well as research and educational activities across multiple disciplines and geopolitical boundaries. Publicly accessible data products, at coarse scales to preserve site location security, will also be available for download and reuse as shapefiles, CSV data tables, and RDF and N3 triples for other Web and desktop applications, including desktop GIS investigation. The project will provide specific instructions for the open source GIS applications QGIS gvSIG, and uDIG, and the widely used proprietary application ArcGIS, in order to foster education in geographic information science within archaeology and related disciplines. The project will fund graduate and undergraduate students, and will assist their training in critical information management skills for the 21st century. The project addresses head-on a major challenge facing research communities worldwide: how to link disconnected and incompatible data systems in such a way that the combined data are useful for important scientific research.

The integration of site file data at continental scales in a new and unique informational infrastructure will allow, for the first time, the exploration of the North American archaeological record across multiple temporal periods and geographic regions. The geographic scale and extent of data integration proposed is currently unprecedented in American archaeology yet, we believe we have demonstrated that it is readily achievable. With proper attention these data have the potential for continued growth as developed by the professional archaeological community, and as the resulting datasets become more inclusive they may transform the practice of our profession.

The Portable Antiquities Scheme: a new(ish) model for recording public discovery
Pett, Daniel|DPETT@thebritishmuseum.ac.uk|The British Museum
The Portable Antiquities Scheme (PAS) was inaugurated in 1997, following the revision of the ancient law of Treasure Trove and the subsequent implementation of the Treasure Act in 1996. This project has been through several funding phases all using public money and encourages the voluntary recording of archaeological objects discovered by members of the public in England and Wales (Scotland is subject to different legislation) and items that meet the stipulations of the Treasure Act.

One of the key pillars of the PAS has been its digital presence, which has now been online in some form for over 13 years and this paper will discuss the impact that the digital arm of the project has had on a national and international audience. The PAS has been hailed by many as a model of public archaeological engagement and decried by others for allowing the mining of the archaeological record for personal gain; however this paper will show some of the PAS’ many successes. For example the PAS has had contributions from over 20,000 individuals worldwide, a new facility has been created for contributors to record their own discoveries through taxonomy driven interfaces and over 350 projects utilise these data for informing their research - for example Oxford University’s EngLaID project. The project has also absorbed and enhanced internationally renowned resources such as Oxford University’s Celtic Coin Index and Cardiff University’s Iron Age and Roman coins of Wales database to provide the largest national, single search node for the study of Roman and Celtic coinage.

The project website records a huge array of metadata about objects, that we often have but one chance to record; images, textual description, measurements, spatial data and user generated comments and audit logs. Over 820,000 objects have been recorded on the PAS database and these are made available for all to view, comment and reuse within their own research or their own websites under a Creative Commons ‘by attribution share-alike’ licence. This liberal approach to licensing content has not been seen widely in the UK and European archaeological sector and the launch of the PAS’ Staffordshire Hoard microsite in September 2009 showed how well received this approach would be. Over 250,000 unique visitors in one day used the innovative microsite to learn more about the amazing Anglo-Saxon hoard and many more viewed the images that had been disseminated via Flickr. The PAS has also utilised and archived social media platforms with varying degrees of success and is a major case study within Lorna Richardson’s forthcoming PhD and complements that of its host organisation (the British Museum.)

This paper will show how the PAS website impacts on the public with specific reference to stories of international interest – such as the Staffordshire Hoard, the Frome hoard of 52,503 Roman coins, the Crosby Garrett helmet and the Staffordshire Moorlands Patera. It will also discuss how these successes have been reached on a minimal digital budget (less than £5000 per annum) via the use of open source technology and through the buy in of its audience. The website has been internationally recognised, winning the prestigious Museums and the Web ‘Best of Web’ award for ‘research or online collection’ (recent winners include the V&A and the Metropolitan Museum of Art.)

The author will demonstrate how a variety of digital techniques that have been employed to complement and enhance data collected via our network of archaeologists, volunteers and citizen scientists; for example geo enrichment through the use of Yahoo!, Geonames, Pleiades and Google Maps, text extraction through OpenCalais and Autonomy, the implementation of a Solr based faceted multi-core search engine and also how a wide variety of application programming interfaces (api) have been employed throughout the site. It will also show how the author has been influenced by ground breaking projects such as Open Context, Pleiades and Pelagios and a variety of Museum sector projects.

The paper will also touch on the recent steps towards providing the PAS data through linked data (for example the release of over 50,000 annotations for Pelagios), towards integrating linked data into the site (for example dbpedia enriched resources) and the CIDOC-CRM mapping process. It will also discuss 3D , computed tomography and PTM/RTI imaging projects conducted by Brighton and Southampton universities that have used PAS sourced objects and hoards of coins to provide research material. If time allows, the paper will also demonstrate the success achieved through the use of Flickr as a disseminator for a wide variety of archaeological images; how the author has leveraged news sources such as the Guardian and UK parliamentary records for background debate on the portable antiquities debate (ethical and fiscal) and how it has impacted on the public psyche within the UK and further afield.

msu.seum: A Location Based Mobile Application for Exploring the Cultural Heritage and Archaeology of Michigan State University
Watrall, Ethan|watrall@msu.edu|Michigan State University, United States of America
The spaces we inhabit and interact with on a daily basis are made up of layers of cultural activity that are, quite literally, built up over time. While museum exhibits, historical and archaeological narratives, and public archaeology programs can communicate this cultural heritage, they do not generally allow for rich, place-based, and individually driven exploration by the public. In addition, museum exhibits rarely explore the binary nature of material culture and the preserved record of human activity: the presented information about material culture and the process by which scholarly research has reached those conclusions. In short, the scholarly narrative of material culture, cultural heritage, and archaeology is often hidden from public consumption.

In recent years, mobile devices as well as the development and maturation of augmented reality (broadly construed) have offe
The “preliminaries” section of a 17th-century book encompasses the pages appearing in the printed text before the beginning of the work itself. This information is divided into seven different types of documents: details of publication, documentation of censorship (both civil and ecclesiastical), licensing, selling price, dedications, letters, and errors. The importance of the preliminaries for this project lies in the information present in these sections: the names of the officials signing the documents, their governmental/institutional affiliation, dates, place of issue, and literary circles that appear in the form of dedications and poetry written by various authors and published in their friend’s or associate’s books. In a few pages, the preliminaries give a complete image of the formal process required for the publication of each work of literature. By compiling all this information into a graph database and performing queries specific to various research questions, we have at hand a valuable source of information about the historical networks that influenced the publication of Early Modern Spanish literature.
To get a comprehensive look at this information, we generated lists of every edition of what we consider literary texts (fiction in prose, theatre, poetry, chronicles) published during the 17th -century in the Spanish empire (Jiménez et al. 1980)(Calvo et al. 2003). As shown by the following screen shot, we have focused on acquiring every available edition of each literary work.
 

Sample of one of our acquisitions lists
We then divided the 17th Century into periods corresponding to the different “validos” —royal favorites that served as head of government or “prime ministers” — of the various kings in order to address the changing power structures of the time and their influence in literary production (Hernán et al. 2002). Through interlibrary loans and, in some cases, trips to the libraries that hold the edition, we acquired copies of the pages of each book that make up the preliminaries section. Then, we manually built a graph database using sylvadb.com, an open source software and free graph database management service developed in the CulturePlex Lab. Within Sylva, data was stored and organized using a custom designed system of schemas based on a node/edge relationship system. Finally, we exported the database to Gephi (https://gephi.org/), a software package that allows for visualization and statistical/metric analysis of the network using built-in algorithms and Python based scripting (Bastian et al. 2009). This allows us to detect important communities within the network, key players, important objects, and hubs of production.
For this study, we have unearthed the social networks of publishing and literary creation in 17th-century Spanish literature, focusing particularly on the period during the rule of the Duke of Lerma (1598-1618). Currently the first of our editions lists (Duke of Lerma) consists of 330 editions, out of which we have successfully obtained 228 scanned copies of preliminaries sections: approximately 70% of the total number. Of these scans, 121 have been entered into the database, producing a graph with 1612 nodes and 3472 relationships. Rendered in Gephi using the built-in OpenOrd algorithm, the graph looks like this:
 

The Preliminaries graph rendered in Gephi
Using the algorithms, metric analysis tools, and filters built into Gephi we pinpointed the individuals, governmental and ecclesiastical bodies that influenced publication in this period. Also, by using the concept of “ego network” from social network analysis, we established what we call the “publication network” of some of the authors that interest us (Carrington et al. 2011). A publication network includes the editors, censors, and other individuals important in the formal process of publication, as well as any other individuals that are more directly connected to the author: friends, family, patrons, literary colleagues, etc. We determined the range of the publication network based on the internal data structure of the Preliminaries database as follows. Due to bibliographic concerns (Bowers et al. 1962) and organizational aspects of our data schema, in order to establish a connection between the author and those involved in the approval, licensing, and publication of an edition there are four steps e.g., Author->Work, Work->Edition, Edition->Approval, Approval->Censor. Therefore, to establish an author’s publication network we needed to find neighbors for up to four degrees of separation. Although Gephi does not include ego network filters that extend to four degrees, using its Python based scripting console we were able to code functions that allowed us to isolate subsets within the graph to generate ego networks for any node to n degrees of separation. For instance, in the graph below we can see the publication networks of two authors associated with Mexico; Bernardo de Balbuena, author of Grandeza Mexicana; Juan de Torquemada, author of Monarquía Indiana; and the intersecting nodes in their publication networks:
 

Publications Networks: Balbuena=Black, Torquemada=Grey, Intersecting Nodes=White
Using the above techniques, we set out to find and isolate the main nodes of this social network that made possible the creation and sustainability of a transatlantic network of cultural agents. The first thing that stands out in the graph is Lope de Vega and his powerful, Madrid based publication network (Martínez et al. 2011). Using the Python scripting console, we determined that Lope’s publication network consists of 1083 nodes, or 67% of the nodes in the graph. This information is not new, based on the extremely prolific nature of his literary production we can assume that he was very well connected. However, we can also determine who wasn’t in his publication network. Departing from Lope’s publication network, we were able to locate the successful political and institutional connections that help us explain the central position of institutions such as the House of Zúñiga in the cultural fabric of the period.
 

Publication network: Lope de Vega=Black
To do this we used the scripting console to remove the subset of nodes representing Lope’s publication network from the other nodes that make up the graph, and returned a list of the names of all of the people who are not in Lope’s publication network. A quick review of this list produced some interesting results: we found several authors based in Spain including Gonzalo de Céspedes y Meneses and El Inca Garcilaso; and two authors active in Peru, Diego Dávalos y Figueroa and Pedro de Oña. While a quick look at both Céspedes and El Inca produced interesting results, the two Lima based authors attracted our attention. In this period social circles were highly influenced by geography, and it is logical that these authors find themselves at the periphery of a network centered geographically in Madrid. However, despite geographic concerns both authors remain connected to Lope de Vega’s network. We found that both Oña and Dávalos y Figueroa are connected to Lope’s network at 3 degrees of separation through their dedications to the Viceroy of Peru, Luis de Velasco y Castilla; and at four degrees through Juan de Zúñiga, Diego de Ojeda, and the Order of Santiago:
 

Publication networks: Dávalos y Figueroa=Black, Lope de Vega=Grey, Intersecting nodes=White
In order to contextualize the Peruvian network we compared the aforementioned “Mexican” authors with the “Peruvian” authors. Combining the four social networks into two based on geographic constraints, we found that at 4 degrees of separation there was no direct overlap, so we upped the parameter to 5 degrees of separation and produced the following image:
 

Publication Networks: Intersection between Mexican and Peruvian Networks
As shown here, even at five degrees of separation there are few overlaps between the networks. However, in the above image we begin to notice the importance of the House of Zúñiga. It is well known that the House of Zúñiga was powerful in both Spain and the Americas, and also that certain members of this house were important patrons of the arts and literature (Cátedra 2003; Díez Fernández et al. 2005). Nonetheless, we don’t think that their role in transatlantic literary production has been adequately explored. The political importance of this family in New Spain is obviously important (an Archbishop and a Viceroy); however, the Preliminaries graph illustrates not only the political role this house played in America, but also the importance of political figures/nobility in publication circles and how the members of one house can spread their cultural influence throughout geographic space. To take this concept one step further, we followed the Zúñigas back the Spain. Here we find the Duke of Béjar, Alonso López de Zúñiga y Pérez de Guzmán, and the first part of Don Quixote. It turns out that American authors were not the only artists soliciting support from the House of Zúñiga: Miguel de Cervantes dedicated part 1 of Don Quixote to the famous Duke of Béjar(Rico 2005).
The above samples show the potential of a research model that combines network-based analysis with quantitative and qualitative studies of cultural production, providing evidence of the interaction between political structures and cultural production in the Spanish Empire (Martínez et al 2008). By repurposing bibliographic data, the Preliminaries Project allows us to explore the concept of cultural networks within the framework of transatlantic studies and complexity theory (Wood 2010; Suárez 2007). Furthermore, this study demonstrates the effectiveness of digital humanities methods as a tool to locate previously overlooked areas for further study using a more traditional humanistic approach.
References
1. Pedraza Jiménez, F. B., and M. R. Cáceres. (1980). Manual de literature española. Pamploa: Cénlit.
2. Huerta Calvo, J. (dir.) (2003). Historia del teatro español. Madrid: Gredos.
3. García Hernán, E. (2002). Políticos de la monarquía hispánica (1469-1700). Madrid: Fernández Ciudad.
4. Bastian M., S. Heymann, and M. Jacomy (2009). “Gephi: an open source software for exploring and manipulating networks.” International AAAI Conference on Weblogs and Social Media.
5. Carrington, P. J., and J. Scott (2011). The SAGE Handbook of Social Network Analysis. Los Angeles: Sage.
6. Bowers, Fredson. (1962). Principles of Bibliographic Description. New York: Russell & Russell.
7. Martínez, J. F. (2011).Biografía de Lope de Vega, 1562-1635: un friso literario del Siglo de Oro. Barcelona, PPU.
8. Cátedra, P. M. (2003). La "Historia de la Casa de Zúñiga" otrora atribuida a Mosén Diego de Valera. Salamanca: Gráficas Cervantes.
9. Díez Fernández, J.; I., and G. Santonja. (2005). El mecenazgo literario en la casa ducal de Béjar. Burgos: Instituto Castellano y Leonés de la Lengua.
10. Rico, F. (2005). El texto del "Quijote”: preliminares a una ecdótica del Siglo de Oro. Barcelona: Ediciónes Destino.
11. Martínez Millán, J.;, and M. A. Visceglia (eds.) (2008). La monarquía de Felipe III. >Madrid: Cyan, Proyectos y Producciones Editoriales. Print.
12. Wood, A. T. (2010). Fire, Water, Earth, and Sky: Global Systems History and the Human Prospect. The Journal of the Historical Society. X:3: 287-318.
13. Suárez, J. L. (2007). Hispanic Baroque: A Model for the Study of Cultural Complexity in the Atlantic World. South Atlantic Review. 72(1): 31-47.
Scholars have often maintained a critical distance between their own forms of knowledge making and those of fans, a distance that we find worth interrogating in the setting of digital humanities. After all, the kind of mapping and charting of vast amounts of cultural data that digital humanists are beginning to do seems closely tied to fan practices of collective textual analysis (and production). We argue against a general academic hesitation to seriously incorporate or interrogate fannish intellectual production as compatible with academic work, and we see digital humanities as a promising site for cooperation between fans and academics. Indeed, digital humanities appear indebted to the intellectual productions of fan communities in an age of media convergence.
Specifically, this paper engages with fan material in a way that acknowledges how it may inform and work alongside academic work in television studies. Fan communities’ collective work analyzing and producing digital work around television shows, including mapping characters and their affective relationships, has heavily influenced our own television analysis method, which involves graphing and analyzing social networks of television characters. This method also draws on the academic work of Franco Moretti and other digital humanities scholars who have demonstrated the usefulness of social network analysis of various texts, but who have not discussed the relationships between their analysis and that of fans. We use digital graphing tools to investigate affective relationships, as defined primarily by fans, between television characters across lines of racial or sexual “difference” in ensemble character-driven dramas. Our data sets have in some cases been gleaned from fan forums, from which we have constructed affective social network graphs using tools like Gephi, ManyEyes, and Mathematica. Analysis of these graphs, we found, illuminates trends and “underlying structures” that might otherwise be difficult to notice (Moretti).
To explore the usefulness of graphing social networks, we demonstrated the absence of queer characters and relationships in the television series Lost, a show with a sizable multi-ethnic cast that draws heavily on the sexual, affective, and familial tension of its characters for dramatic effect. First establishing the overwhelming preponderance of heterosexual pairings, we reorganized the nodes so that the most frequently appearing characters gravitate towards the center of the field, and the less frequently appearing characters radiate outwards. As we might expect, characters who appear most frequently have multiple lines of (heterosexual) relationships, while less frequently appearing characters are likelier to be unattached or to have only one relationship over the course of the series. The suggestion of an undergirding heterosexual matrix, visible here, is borne out in the conclusion of the show. In a "flash-sideways," the storyline in a parallel timeline where Oceanic Airlines Flight 815 never crashes, characters recall the moments on the island when they make contact with their principal love interests. This forces a heteronormative pairing as an organizing force (possibly queering the pair of John Locke and Ben Linus, who recall their past lives but without the benefit of a significant other). The graph also makes legible the outliers, exceptions, and outsiders that relate to the frequently appearing characters in ways that support the structuring heterosexual matrix (See Figure 1). One interesting cluster that the graph brings to our attention, for instance, is the set of characters who are positioned as paternal figures to specific nodes.
 
Fig. 1:
Affective and Parental Relations in Lost (Nodal size indicates relative frequency of character’s appearance). Constructed using Gephi.
The lines of paternity as a special category that our graphing highlights, and the general heteronormative drift of the show, also invites us to consider the separateness of maternity as a category with a unique valence. Apart from the running theme of pregnancy dramatized by repeated attempts to capture or rescue the visibly pregnant character, maternity we uncover is an organizing type distinct from paternity. While the characters Kate and Jack both co-parent a child, only Kate’s status as a mother symbolically revokes her candidacy as protector of the island in the finale.
Along similar lines, social network graphs of Friday Night Lights reveal the extent of the characters’ racial segregation in the early seasons. [1] We chose this show to analyze because race is a central theme as characters negotiate racial tensions in a small Texas town. In the case of Friday Night Lights, segregation is apparent through conventional modes of analysis, but a graph makes it more starkly visible. This graph of a Friday Night Lights episode, for example, shows a notable degree of segregation between black and white characters. The mix of such characters on the television screen hides the fact that there are very few interracial conversations, which this graph depicting all interactions in the episode maps clearly. Here we see that the majority of white characters are only connected to other white characters, and, importantly, that the white characters tend to be more strongly socially connected than black characters—they are given more social power (See Figure 2). This graph then provides in one image a sense of how racial interaction plays out across a whole episode.
 
Fig. 2:
Interactions in Friday Night Lights Season 2 Episode 8. Constructed using ManyEyes.
One way we have used social network graphs to heighten visibility of character segregation (or commonality) is through “deformance,” a playful textual reimagining that in some ways resembles remix culture. Deformance is described by Lisa Samuels and Jerome McGann in reference to poetry analysis, as a kind of “reading backwards,” a reconsideration of a text by undoing it, upsetting its order, revealing its gaps (30). In the context of this project, deforming graphs has entailed removing certain networked nodes—and thus removing certain characters from the plot—to see what new connections come to the fore, as well as to reveal the role of those removed nodes in the networked system. If a network falls apart when one node is removed, that often speaks more to the significance of that character than does looking at the graph before he or she is removed from the network. In our analysis of Friday Night Lights, for instance, deforming social networks created from fan-generated relationship information (or from our own fannish data generation) revealed that interactions across race hinged on just one or two key characters. Deforming the episode graph above, for example, by removing the central black character Smash and all those characters who interact exclusively with him, revealed the extent to which he functioned as a “bridge” node connecting two otherwise largely distinct communities of characters (See Figure 3).
 
Fig. 3:
Interactions in Friday Night Lights Season 2 Episode 8, with Smash and characters who speak only to him removed. Constructed using ManyEyes.
As this work demonstrates, we are not interested in fan labor solely as sources for data, but also as rich and challenging sites for methodological exchanges. The intellectual production of fans has been acknowledged and celebrated; indeed Henry Jenkins’s now-classic text on fan cultures, Textual Poachers, highlights the intelligence of audiences and the seriousness of their responses, which engage in familiar practices of literary criticism. Yet to Jenkins, while fans do work that is critical and interpretive, their criticism “is playful, speculative, subjective” and directed to the fan community (284). We see our methodology as a gesture toward the possibility of digital humanities to engage this playful approach seriously, and to consider the importance of the relationships between fans and other interpretive communities.
With academics and fans less strictly monitoring their boundaries in this era of convergence, and with scholars using digital techniques that may formerly have been dismissed as too “playful” or not seriously analytical, a reconsideration of the fan/scholar relationship and possibilities of exchange (or poaching) is called for. Visualization and mapping are widely circulated by fans broaching similar themes to those we explored. In toggling between an episodic mode of analysis and a mode that exceeds the episode, we rely on the community and labor of fans, appropriating their knowledge as the basis of our inquiry. What this suggests is that the digital humanities can anticipate not just a new kind of appreciation of the fan but an acknowledgement of the rich, intellectually productive, and rigorous strategies of knowledge making that new scholarship exploits in its interpenetrative incursions into the terrain of fandom.
References
Jenkins, H. (1992). Textual Poachers: Television Fans & Participatory Culture. New York: Routledge.
Jenson, J. (1992). Fandom as Pathology: The Consequences of Characterization. In Lewis, L. A. (ed), The Adoring Audience: Fan Culture And Popular Media. London: Routledge, 9-30.
Moretti, F. (2011). Network Theory, Plot Analysis. Stanford Literary Lab Pamphlet #2.http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf.
Moody, J. (2001). Race, School Integration, and Friendship Segregation in America. American Journal of Sociology, 107(3): 679–716.
Samuels, L., and J. McGann (1999). Deformance and Interpretation, New Literary History, 30(1): 25-56.
Notes
1. Our interest in using social network graphs specifically to look at racial interaction was partly inspired by James Moody’s 2001 study of integration in high schools, in which he used social network graphs to make visible certain trends of friendship formation between high school students.
Introduction
This paper describes VizOR, a new digital resource currently under development, that visualizes Mark Z. Danielewski’s 2006 novel Only Revolutions. VizOR is built on top of a MySQL database comprised of the complete text of Only Revolutions and is programmed in Python to produce a dynamic, database-driven visualization of the novel. In this paper, we discuss the methods and procedures we are currently developing to create this visualization and highlight the implications of these methods and procedures for theoretical concerns in both digital humanities and media studies. This project, we propose, is both a re-reading of and an exploration of the process of reading Danielewski’s novel: as such, it joins the novel in examining the interrelation of human and machine “reading” and “authorship,” pointing to a procedural understanding of reading and writing and suggesting that both activities occur across a wide variety of actors and platforms.
Context
The form of Danielewski’s novel is unique: the narrative consists of two parallel yet interrelated narratives, one by Hailey and one by Sam, and which one the reader reads depends on how the reader holds the book. If the reader is reading Hailey’s narrative, for instance, she must flip the book upside down to read Sam’s (and vice versa). Apart from the main narrative, each page also contains what we call a chronology section — a date with a list of historical people and events associated with that time or date, which Danielewski crowdsourced from his fans while writing the novel. Thus, each page is divided into four sections: Sam’s narrative, Sam’s chronology (which runs from November 22, 1863 to November 22, 1963), Hailey’s narrative, and Hailey’s chronology (which runs from November 22, 1963 to June 19, 2063).
In addition to its formal innovations, Only Revolutions is interesting for this project because it proliferates numbers, playing with the boundaries between “data” and “narrative.” For example, the numbers 360 and 8 and their factors and multiples are particularly important. Each narrative is 360 pages long, and each narrative and chronology section on each page contains 90 words (90 X 4 = 360). Furthermore, the narrative is divided up into sections of eight pages each, and the number of lines in each narrative section decreases, at regular intervals, from 22 to 14 as the reader progresses through these sections (22 - 14 = 8). “H,” for Hailey, is the eighth letter in the alphabet, and “S,” for Sam, is the eighth letter from the end of the alphabet; “Mark Z. Danielewski” has 16 letters; and Sam and Hailey are described as being “allways sixteen.” There are many, many more examples we could cite here. In this way, Only Revolutions encourages readers — human and machine alike — to count, and count again, the many different numbers that emerge from its pages.
In How We Think: Digital Media and Contemporary Technogenesis, N. Katherine Hayles includes a coda featuring visualizations of Only Revolutions produced with Google Maps. Collaboratively designed with Allen Beye Riddell, these three visualizations trace the geographical “place-names” of Sam and Hailey as they travel throughout the story, and then layer the two to create a composite map of the characters’ movement through the text (243-244). The resulting visualization shows that Hailey and Sam take similar paths across the map and that their overall directionality is nothing if not inconsistent. They move on a whim, together, wherever their overwhelming affection for one another points them. On one hand, VizOR is a response to Hayles’s map-based visualization and is concerned with the assumptions underpinning her choice of visualization platform. On the other hand, the project is addressing a larger trend within the digital humanities: the increased prevalence of data visualization as a mode of literary interpretation.
Due in large part to its often powerful and aesthetically pleasing visual impact, relatively quick learning curve, and overall “cool,” the practice of visualizing textual data has been widely adopted by the digital humanities. This prevalence is evidenced by, for instance, the high frequency of the term “data visualization” in the 2011-2012 Digital Humanities conference abstracts as well as the 2011-2013 Modern Language Association digital humanities panel descriptions. If the first wave of large-scale database projects in the digital humanities is exemplified by the practices of digitizing texts, constructing archives, and determining best practices for digital preservation, then the practice of data visualization is emblematic of the second wave of projects devoted to mining and interpreting this newly available data.
VizOR is influenced by the thinking of scholars and practitioners like Franco Moretti, Matthew Jockers, Jeremy Douglass, and Lev Manovich, as well as by visualization projects like UC San Diego’s “Cultural Analytics” initiative and Stanford’s “Gephi” visualization engine. Like other critical DH projects, VizOR is not only interested in engaging with literature via data visualization, but also in performing this engagement to ask what is at stake in this new mode of interpretation, both in terms of the individual scholar and the digital humanities as a field.
Project Description
The database is designed to be highly flexible, and this architecture allows us to enact the same linguistic layering that occurs while reading the text. We can query a specific word of a particular character’s narrative or chronology, the text from a specific line of a character’s narrative or chronology on a specific page, or the narrative or chronology text from a whole page for a specific character. Querying the database in this way allows us to instantly compare the words and phrases used by one character with those used by the other character on the same page and in the same position. For example, on Sam’s page seven, line five, we see “Gold Eyes with flecks of Green,” while the words in the same position on Hailey’s page seven, line five read,“Green Eyes with flecks of Gold” (Danielewski). VizOR surfaces this kind of reading through its very design, allowing readers to see the similarities and, perhaps more importantly, the differences between the narratives.
The visualization is currently still under development using the Python programming language. Python was selected as the programming language for VizOR due to its general flexibility and its ability to produce dynamic, database-driven visualizations. However, static mock-ups have been designed using Adobe Illustrator and are included at the end of this document.
The visualization reproduces the image of the page numbers in the novel, the two small rotating circles enclosed within the larger circle. In the visualization, however, each of the two smaller circles represents the narratives of Hailey or Sam. All three circles are comprised of absent centers with the text literally expanding outward. The larger circle is comprised of the chronological headings and entries. Clicking on one of the text strings in either narrative will query the database for the corresponding narrative’s string. This correspondence, as well as the inclusion of the Boolean double pipe (symbol for “or”) as link, is shown in the mock-ups at the end of the document. This, in effect, mirrors the layering that occurs during the act of reading. The visualization, though, has the ability to position the corresponding lines on the same plane at the same time, a phenomenon in the novel that is always already delayed or fading away. Rather than attempting to flip the book fast enough to see both sides of the coin, so to speak, VizOR attempts to freeze the text at each moment of mirroring. This moment of pause opens up the possibility of interpretation without disrupting the line’s relationship to other lines or removing it from its context.
Users can navigate the visualization in a number of ways, hovering over any of the terms to magnify chosen words or lines. Once users click on a given line, the circles will rotate to realign the corresponding terms. Further, rotating the encircling chronology forward or backward in time will result in the rotation of the two inner narratives, mirroring the motion of the flip-book page numbers of the print text.Users can navigate the visualization in a number of ways, hovering over any of the terms to magnify chosen words or lines. Once users click on a given line, the circles will rotate to realign the corresponding terms. Further, rotating the encircling chronology forward or backward in time will result in the rotation of the two inner narratives, mirroring the motion of the flip-book page numbers of the print text.
The finished form of the visualization will be searchable and will also contain external hyperlinks. In keeping with the novel’s data-driven construction, produced in some ways by a crowd-sourced or “collaborative” author, the visualization produces a similarly data-driven reading experience. The goal here is to mirror the outward push of the novel, its awareness and incorporation of external databases like online encyclopedias, and the uniquely distributed reading experience of excitedly setting the book down to search for one of its vague historical entries.
Conclusion
In the name of this speed and efficiency, however, these technologies often strip data of its context and idiosyncracies, creating what Tara McPherson has called “a system of interchangeable equivalencies” (35). VizOR, through its emphasis on the distributed processes of reading and writing across different technologies and media, pushes against this seamless homogenization. By highlighting particular, idiosyncratic moments of reading, we hope to activate the messy, "seamy" place where data meets narrative.
References
Barthes, R. (1977). Rhetoric of the Image. Image, Music, Text. New York: Hill and Wang.
Borner, K. (2003). Visualizing Knowledge Domains. Annual Review of Information Science & Technology. 37. Medford, NJ: Information Today, Inc./American Society for Information Science and Technology. 179-255.
Cartwright, L. (2009). Practices of Looking: An Introduction to Visual Culture. Oxford: Oxford University Press.
Danielewski, M. (2006). Only Revolutions. New York: Pantheon.
Drucker, J. (2008). The Virtual Codex from Page Space to E-space. In Schreibman, S., and R. Siemens, (eds). A Companion to Digital Literary Studies. Oxford: Blackwell. http://www.digitalhumanities.org/companionDLS/ (accessed September 21, 2012.
Hayles, N. K. (2012). How We Think: Digital Media and Contemporary Technogenesis. Chicago: The University of Chicago Press.
Lima, M. (2011). Visual Complexity: Mapping Patterns of Information. New York: Princeton Architectural Press.
Manovich, L. and J. Douglass (2007). Cultural Analytics. UC San Diego Software Studies Initiative.http://lab.softwarestudies.com/2008/09/cultural-analytics.html (accessed: September 21, 2012.)
McPherson, T. (2012). U.S. Operating Systems at Mid-Century: The Intertwining of Race and UNIX. In Nakamura, L. and Chow-White, P. A., (eds). Race and the Internet. New York: Routledge.
Terras, M. (2008). Digital Images for the Information Professional. London: Ashgate.
Tufte, E. (1983). The Visual Display of Quantitative Information. Cheshire, CT: Graphics Press.
Vesna, V. (2007). Database Aesthetics: Art in the Age of Information Overflow. Minneapolis: University of Minnesota Press.
Yau, N. (2011). Visualize This: The FlowingData Guide to Design, Visualization, and Statistics. Hoboken: Wiley Press.
Overview
Some humanities scholars argue that the social web “poses a grave threat to the humanities because it lacks the depth, nuance and permanence that make genuine, meaningful interactions about the human condition possible.” They fear that “we risk becoming a world without the comprehensive communication tools needed to keep the humanities alive” (Adamek 2010). Even those who embrace social media all too often dismiss it as a tool that is used primarily for community engagement and self-promotion (Terras 2012). Yet the social web — which we broadly define as the array of technologies that allow individuals to post their thoughts, pictures, and comments in a public forum — when coupled with recent advances in cloud computing, data management and statistical/visual analysis offers significant potential to explore new and enduring humanities questions.
Through careful, rigorous analysis, we believe that the social web affords opportunities for the humanities to realize a contemporary, nuanced understanding of how the public believes our past informs modern society. For example, although there is a strong consensus amongst historians, the broader American public remains conflicted, divided, and confused about the causes of the Civil War. This is evident in any number of recent public polls, such as an April 2011 CNN poll which found that 42% of Americans believed that cause was something other than slavery, and the April 18, 2011 issue of Time, whose cover said “Why We’re Still Fighting the Civil War: The Endless Battle over the War’s True Cause Would Make Lincoln Weep” (CNN 2011; von Drehle 2011). Where polls provide a snapshot of Americans’ views, analysis of online discourse and social media activity affords opportunities to explore modern attitudes towards issues surrounding the Civil War.
Hence, against the backdrop of the on-going sesquicentennial commemorations, this paper examines the intersection of humanities, social sciences, social media, and computing to probe enduring questions around the legacy of the Civil War. To do so, we will illustrate how an ongoing study that uses the Civil War serves as a testbed for examining and developing techniques to conduct traditional types of humanistic inquiry in the context of the social web. Our results demonstrate how careful analysis of the online discourse that occurs across the social web enables deeper understanding of how the Lost Cause Ideology continues to recast the origins of the war and minimize the role of slavery. Simultaneously, we explore how stereotypes of Southerners continue to be propagated and used to shape a memory of the Civil War.
To conduct this study, we demonstrate how contemporary tools developed by industry may be used to analyse the social discourse around the Lost Cause and stereotypes of Southerners. Most scholars researching the social web rely primarily on the use of single keyword search and user-specified hashtags to narrow the size of their datasets to only posts that are immediately relevant to the topic under discussion (for example, Graham 2012; Ross, et al. 2010). However, these methods can be inadequate or problematic for understanding widely diffused social phenomena. For example, when attempting to study a topic such as the Civil War or especially a concept such as “the South” this methodology isn’t adequate as the majority of users do not tag their casual online conversations with these types of metadata nor do they restrict their conversations to a single platform (e.g., Twitter, Facebook etc.). To elicit a holistic view of conversations on the social web, the researcher has to employ strategies that reach across social media platforms and go beyond simple hashtag sets. To overcome these limitations, we leverage software originally developed for business analytics to aggregate content from across the social web (Radian6, 2012) — our search includes not just Twitter and Facebook, but also content from sources such as mainstream news sites, blog posts, and forum posts.
Method
Through our analysis of the social web, we highlight issues and opportunities for scholars who work at the intersection of the humanities, social science, social media, and computing. One issue that must be overcome is that, on a global scale, there are many Souths: not only must we find a way of eliminating references to countries or continents such as South Africa, South America, and the South Pole, we must also filter out conversations related to places such as the South Side of Chicago and even the television show South Park, none of which would be relevant for this research. Similarly, while most Americans would refer to the American Civil War as simply “The Civil War,” there are many similarly named historic and ongoing conflicts that we must filter from our results. The solution is to create “topic profiles” — collections of words that fall into one of three categories: words or phrases that must be present in a post to be included in the results; phrases that must be present along with words from the first category to help categorize results into different topics; and, finally, phrases that, if present in a post, will result in that post being excluded from our results. So, when conducting research into online discussions of southern gender ideals, we create a topic profile similar to the following:
•        1. Posts that CONTAIN any of the following: “the south”, “thesouth”, “southern”
•        2. AND CONTAINS any of the following: “belle”, “lady”, “gentleman”, …
•        3. But DOES NOT CONTAIN any of: “south pole”, “south park”, “south side”, “south africa”, …
By using keywords that were selected following a survey of users of the H-South discussion list, a request from leading scholars (including several former presidents of the Southern Historical Association, of the American South, surveys of a graduate and an undergraduate history class on Southern History at Clemson University, as well as a non- random sample of black and white southerners who were not academics, we have constructed topic profiles for five different themes that might be used to segment contemporary discussion of the South online: Southern Culture (which includes concepts such as honor, hospitality, and accents), Southern Food, Discussions of Gender, Religion, and Southern History. This set of keyword groups results in approximately 300,000 unique hits from the social web each month. Figure 1 illustrates the relative volume of each keyword group during September 2012 and shows how the number of hits varies for each topic across the month. Similar keyword groups have also been constructed for specific events that occurred during the Civil War, such as the attack on Fort Sumter, the Emancipation Proclamation, and the Battle of Gettysburg.
 
Figure 1:
Relative Volume of Five Topic Profiles for September 2012
Analysis
Drilling into this data reveals numerous insights into how users of the social web conceptualize the South and what topics are of interest. Figure 2(a), for example, presents a word cloud of the major terms associated with civil war commemorations, again for the month of September 2012. This word cloud immediately reveals several potential avenues for further investigation, including the centrality of Gettysburg to the online discourse surrounding the Civil War, but also suggests other useful topics to explore. Indeed, although word clouds have frequently been criticized for divorcing words from their context (Harris, 2011), we are able to maintain the connection between the words represented in a cloud and the underlying data. Therefore, if we are interested in further exploring the appearance of the word “history”, we can, as shown in Figure 2(b), drill further into the data to reveal new levels of insight to our topics. At all times, the original posts remain available both to view at an individual level to ensure the relevancy of content and to download for further textual and statistical analysis with dedicated software packages such as R and Gephi.
 
Figure 2:
(a) Word Cloud of Frequently Used Words in the Civil War Profile for September 2012; (b) The same word cloud, instead focused on the word “history”
We can also analyse demographic information, such as age, gender, and location, from the posts we have collected, allowing for a more nuanced understanding of our results. Figure 3 illustrates the location of users in the United States whose posts and tweets were collected from the Civil War topic profile over a seven-day period in mid- September 2012, while Figure 4 presents word clouds of the conversations that occurred in our Civil War topic profile on the 150th Anniversary of the Battle of Antietam by (a) people located in Maryland; (b) those in North Carolina; and (c) users aged between 36 and 45. These charts reveal not only the geographic distribution and volume of conversations, but also regional and demographic differences in the topic, tone, and scope of this online discourse.
 
Figure 3:
Geographic Distribution of Unique Hits from the Civil War Topic Profile in September 2012. A darker shade of orange indicates a greater number of posts from that state.
 
Figure 4:
Word Cloud of conversations that occurred on the 150th Anniversary of the Battle of Antietam by (a) people located in Maryland; (b) those in North Carolina; and (c) users aged between 36 and 45.
References
Adamek, D. (2010). Social Media Flaws and the Humanities. Valley Advocate, 13 December.http://www.valleyadvocate.com/blogs/home.cfm?aid=12863 (accessed 10 October 2012).
CNN Political Unit. (2011.) Civil War Still Divides Americans. CNN, 11 April.http://politicalticker.blogs.cnn.com/2011/04/12/civil-war-still-divides-americans/ (accessed 11 April 2011).
Graham, S. (2012). Mining the Open Web With Looted Heritage — Draft. Electric Archaeology, 8 June 2012.http://electricarchaeology.ca/2012/06/08/mining-the-open-web-with-looted-heritage-draft/ (accessed 15 October 2012).
Harris, J. (2011). Word Clouds Considered Harmful. Nieman Journalism Lab, 13 October.http://www.niemanlab.org/2011/10/word-clouds-considered-harmful/ (10 October 2012).
Radian6 (2012). http://www.radian6.com. accessed 10 October 2012.
Ross, C., M. Terras, C. Warwick, and A. Welsh. (2010). Pointless Babble or Enabled Backchannel: Conference Use of Twitter by Digital Humanists. in2010 Digital Humanities Conference. http://www.ucl.ac.uk/infostudies/claire-ross/Digitally_Enabled_Backchannel.pdf
Terras, M. (2012). The Impact of Social Media on the Dissemination of Research: Results of an Experiment. Journal of Digital Humanities, 1(3) http://journalofdigitalhumanities.org/1-3/the-impact-of-social-media-on-the-dissemination-of-research-by-melissa-terras/ (accessed 10 October 2012).
Von Drehle, D. (2011). 150 Years After Fort Sumter: Why We're Still Fighting the Civil War. Time, 18 April.http://www.time.com/time/magazine/article/0,9171,2063869,00.html (accessed 18 April 2011).
The use of topological graphs, or networks, to represent and analyze the semantic contents of source materials, such as texts and images, has become a signature contribution by the digital humanities to the humanities in general. Specific techniques, such as topic modeling and network analysis, and general approaches, such as macroanalysis and distant reading, exemplify the popularity and effectiveness of methods based on the graph theoretical representation and statistical modeling of cultural materials. However, because of their mathematical complexity and their focus on very large corpora of texts, these methods are beyond the reach of many humanists interested in the interpretation of smaller sets of source materials for cultural meaning. They are also suspect since they introduce ontological commitments that both elide traditional notions of human agency and reframe culture as a set of abstract, metrical dimensions. In this talk, I introduce “common container correlation” (C3) as a relatively simple and transparent interpretive method for the graph theoretical analysis of source materials that may be practiced by both students and more advanced researchers to excavate and make sense of cultural models implicit in textual materials.
C3 may be described as a variation of co-occurrence analysis designed to take advantage of the abundance of encoded cultural materials available to the digital humanist and to allow for the analysis of small sample sizes, such as individual texts. Formally, a common container correlation is just a link, or edge, that is asserted between any two items, regarded as nodes or vertices, that are contained within the same structural container. The set of all such links produces a graph of nodes and links based on their co-occurrence in a common container. In some cases these graphs will have meaning—that is, they will exhibit patterns that lend themselves to structuralist and other forms of interpretive analysis. These patterns may sometimes be correlated with psychological, sociological, or material causes that will be of interest to the humanist.
For example, in a novel marked up with TEI-based schema, we may choose to define the paragraphs of the text as container elements and tagged references to proper names as contained elements. We then assert that all named agents in a given paragraph are related to each other (in the special sense of co-occurring). The set of all of these assertions for all paragraphs will produce a kind of social graph that may then be visualized and analyzed in structural terms. In such a case, it may emerge that two characters consistently appear on opposite sides in multiple instances of a force-directed representation of the graph. This may be evidence of a structural opposition that will have emerged from the statistical distribution of the selected elements. Other approaches may use other container elements, such as scenes, and combinations of contained elements, such as places and people.
The C3 method is easy to implement using available tools. Container and contained elements in XML encoded materials may be extracted using simple XPath statements (by means of a variety of tools) and dumped into tables with columns for container IDs and contained IDs. Such tables may then be transformed using simple SQL queries into various graph data formats for visualization and analysis in tools such as Gephi, GraphViz, SHIVA, and D3. Depending on the intention of the user, the resultant graph may or may not reflect the frequency of edges and vertices in the source data.
In this talk I will describe the C3 method using examples taken from three digital humanities projects with which I have been associated. First, I will describe the application of the method to rhetorical figures (containers) and characters (contained elements) using data from the Princeton Charrette Project. Second, I will describe how undergraduates in an introductory digital humanities course at the University of Virginia created a database relating characters and paragraphs in Austen’s Persuasion. Third, I will describe the use of the method in Stephen Railton’s Digital Yoknapatawpha Project, drawing on data correlating scene containers to people and places as contained elements in William Faulkner’s corpus of fiction. In each case, I will explore the interpretive implications of the algorithms used to visualize the data, taking particular care to describe the specific steps involved in going from markup to data representation to visualization to interpretation. In this way, I hope to connect the discourse on data-driven textual analysis to traditional interpretive methods, such as close reading and structural analysis, in order to produce a genuinely humanistic use of quantitative methods that does not alienate the researcher from the tools of interpretation.

This project uses Neatline and Gephi to demonstrate how digital visualization tools can bring to light new dimensions of modernist studies and periodical studies. Drawing on metadata from as-yet-undigitized letters between various Canadian writers and editors, the project uses geospatial visualizations and network diagrams to interrogate the literary networks and geographical patterning of authors associated with the little magazine Contemporary Verse. In addition to the printed poster, I will have a laptop running Neatline and Gephi, which will allow attendees to interact with the data by exploring it through maps, timelines and network diagrams.

Contemporary Verseran from 1941-1953 and was one of the only vehicles for the publication of modern poetry in mid-century Canada. It was edited by Alan Crawley (1887-1975), whose voluminous correspondence – lodged in various archives across Canada – is an enormously rich resource for the study of the social networks through which poetic currents and aesthetic influences developed. To read it is to get a clear sense of the importance of Crawley in brokering relationships between writers and publishers, shaping the poems that contributors submitted to the journal, and encouraging young writers – particularly younger women who faced considerable difficulties breaking into male-dominated networks for publication and critique – to see their poetry as something worth pursuing.1 

The letters tell an important story of pre-digital cultural empowerment that digital humanities approaches are particularly well suited to uncovering, given that the volume of correspondence lends itself to the kind of distant reading that is made possible by computational analysis of prosopographical and geographical metadata. The new perspectives opened up on this archival material by digital methods are also timely, as scholars of Canadian literature are increasingly engaged in challenging canonical accounts of modernism’s development within the country,2 something which has coincided with a postcolonial turn of sorts within digital humanities more generally.3

The poster, which reports on work in progress into the Crawley letters, is focused around two research questions:

1) How does geography inflect the development of modern poetry in Canada?
Each letter was entered as a record in Omeka and then visualized geospatially using Neatline (www.modmaps.net/mm/neatline/show/crawley-letters). Neatline was configured so that as the timeline slider was moved from earlier to later dates, the map display showed the growth and geographical distribution of Crawley’s network of correspondents. Displaying the correspondence in this way enables investigation of spatial questions that arise at a range of scales from the national to the local, for example the extent to which Crawley’s location on the west coast was able to challenge the dominance of literary networks in the eastern cities of Montreal and Toronto, and the effect of Vancouver’s geography on his ability to participate in literary activities.

2) What is the relationship between literary networks and cultural production?
Gephi was used to create a directed graph with 25 nodes, representing Crawley, various authors with whom he corresponded, other journal editors, and the women who did the bulk of the administrative work for Contemporary Verse. As with the Neatline map, the network diagrams generated by Gephi do not give a complete picture of Crawley’s network of correspondents as archival work is still ongoing, but nonetheless some preliminary suggestive patterns emerge. Crawley’s initial correspondents are more likely to be women, for instance, but as the journal accrues prestige, more established poets become interested in submitting to it, and these poets were more likely to be men. Such a narrative is clearly open to critique – for example it raises methodological questions about which letters were included – but this is a valuable process for raising questions about the partiality of the “data” on which existing narratives about the development of Canadian modernism have relied.

This project forms part of EMiC: Editing Modernism in Canada (editingmodernism.ca). It is also being developed in association with TCLL: Twentieth Century Literary Letters (www.modmaps.net/tcllp), a collaborative project in its early stages which aims to build a digital infrastructure for the discovery, analysis, and visualization of the metadata from a wide range of epistolary materials relating to twentieth century literary figures. As a founding member of TCLL, I am keen to find other scholars working on letter collections who would be interested in joining the project, and one of the aims of showcasing this work at DH2014 is to make connections with others whose metadata could be productively brought into conversation with existing TCLL materials.

References
1. McCullagh, Joan. Alan Crawley and Contemporary Verse. Vancouver: U British Columbia P, 1976. Print; Robertson, George. “Alan Crawley and Contemporary Verse.” Canadian Literature 41 (1969): 87–96. Print; Wilson, Ethel. “Of Alan Crawley.” Canadian Literature 19 (1964): 33–42. Print.

2. Irvine, Dean (2011). Spectres of Modernism. Canadian Literature 209: 6–10. Print.

3. Koh, Adeline, and Roopika Risam (2013). Open Thread: The Digital Humanities as a Historical “Refuge” from Race/Class/Gender/Sexuality/Disability? Postcolonial Digital Humanities. 10 May 2013. Web. 1 Nov. 2013.
>Contexts for Humanities Visualization
Literary scholars are increasingly turning to graphical display of humanistic information as a way to encounter texts in new and engaging ways. Digitally facilitated humanities visualization presents literary critics with opportunities for new insight1, while also opening practitioners to charges of wholesale importation of simplistic scientific methodologies.2 This poster session outlines the rationale, method, and significance of a focused humanities visualization project in order to demonstrate how new techniques of visualization may be undertaken with literary texts to produce new and speculative “aesthetic provocations” in literary studies.3

Materials and Scope
In order to model and theorize how such a project might develop, I am producing a network visualization of the sixteenth-century play Ralph Roister Doister by Nicholas Udall. Using the open-source "interactive visualization and exploration" platform Gephi, I map the relationships between characters in the play based upon dialogue. In other words, this project structurally maps dialogue between characters, producing a network visualization that productively reconfigures the play to provoke new analytical responses. Following the example of Franco Moretti’s work with Hamlet, these models prompt new insight into the “deep structures” of literary works. For Moretti, however, “the most important thing of all” about these reconfigurations is that they can be manipulated: “one can intervene on a model; make experiments.”4 [116, italics in original]. With a baseline visualization of dialogue established, I, following Jerome McGann, selectively intervene and “deform” the re-modeled text to continually reconfigure it. Such processes bring us “to a critical position in which we can imagine things about the texts that we didn’t and perhaps couldn’t otherwise know.”5 [116] Motivated by Drucker and Nowviskie’s call to “engage computing to produce new aesthetic provocations,” I use Roister Doister to understand how humanities visualization may reconfigure our approach to literary inquiry.6 

Methods of Production
In order to map the dialogic relationships between these characters in Gephi, I have created each character as a node in the network; these nodes are connected by directional dialogue originating at a particular node and terminating at another. Thus, the main characters Roister Doister and Merygreeke are nodes 1.0 and 2.0, for example. Within Gephi’s data manipulation environment, a line of dialogue from the former to the latter would be mapped visually as a line [or an “edge”] from node 1.0 to node 2.0. Each exchange is recorded as tabled data in Gephi, which is then used to produce visualizations. Visualizations can be produced for any discrete unit of the text, including scene, act, or the entirety of the play. This project produces visualizations of each of these divisions for comparative purposes. Following Moretti and McGann, I have undertaken selective deformations by, for example, removing various characters at different points, thereby revealing their network centrality. Criticism of Roister Doister has, for the most part, focused heavily on the play’s dramaturgical debt to the classical comedies of Terence and Plautus, the extent to which those formal structures were successfully integrated with “native” elements of English drama, and the play’s debts to the miles gloriosus [“braggart-soldier”] tradition of Classical comedy.78 This project is in part an attempt to revitalize an ossified critical conversation as an example of how new techniques can vigorously re-engage old texts. 

Significance of Anticipated Visualizations
Experimentation in visualization of textual works is a timely one. As can be seen from the growing use of the Voyant suite9 of analysis and visualization tools as well as the popular Mapping the Republic of Lettersproject,10 humanities visualization is a growing area of scholarly concern. Elijah Meeks has argued that “the shift from creating, annotating and analyzing archives to modeling systems can have a profound impact beyond the [admittedly high value of] usability of scholarly material developed during a digital humanities project.”11 [italics mine]. When humanities scholars have reached a certain point of visual literacy, we will begin to engage with such models in profoundly important ways. Indeed, these models may “provide a much more nuanced form of knowledge transmission than the raw datasets or interactive and dynamic applications typically presented as the future of digital scholarly media.”12 This project is an effort to explore how these new forms of knowledge transmission and analysis might impact literary inquiry. 

References
1. Manovich, Lev. (2010). What Is Visualization? Poetess Archive Journal 2(1), n. pag.

2. Drucker, Johanna. (2011). Humanities Approaches to Graphical Display. Digital Humanities Quarterly 5(1), n. page.

3. Drucker, Johanna, and Bethany Nowviskie. (2004). Speculative Computing: Aesthetic Provocations in Humanities Computing. Companion to Digital Humanities. In Schreibman, Susan, Siemens, Ray, & Unsworth, John (eds). Oxford, Blackwell. Accessed 03 April 2012. Available at [journals.tdl.org/paj/index.php/paj/article/view/19/58]

4,5. Moretti, Franco. (2011). Network Theory, Plot Analysis. Stanford, Stanford Literary Lab.

6. Drucker, Johanna, and Bethany Nowviskie. (2004). Speculative Computing: Aesthetic Provocations in Humanities Computing. Companion to Digital Humanities. In Schreibman, Susan, Siemens, Ray, & Unsworth, John (eds). Oxford, Blackwell. Accessed 03 April 2012. Available at [journals.tdl.org/paj/index.php/paj/article/view/19/58]

7. Boas, Frederick S. (1933). An Introduction to Tudor Drama. Oxford, Clarendon.

8. Brooke, C. F. Tucker. (1911). The Tudor Drama: A History of English National Drama to the Retirement of Shakespeare. Boston, Riverside.

9. Voyant Tools. (2012). [voyant-tools.org]. Accessed October 2012.

10. Mapping the Republic of Letters: Navigating Big Data from the Early Modern Period. (2012). Available at [republicofletters.stanford.edu].

11. Meeks, Elijah. (2011). More Networks in the Humanities or Did Books Have DNA? Digital Humanities Specialist. Published 6 December 2011. Accessed 17 April 2012.

12. Meeks, Elijah. (2011). More Networks in the Humanities or Did Books Have DNA? Digital Humanities Specialist. Published 6 December 2011. Accessed 17 April 2012.
“Humanists have always been explorers. They sail not on seas of water but on seas of color, sound, and, most especially, words.” — John B. Smith, 1984

Why is big data such a big deal? Modern digital communications are producing and recording data at a feverish rate, with 90% of the world’s recorded data, most of it unstructured, having been produced in just the last two years (Dragland, 2013). In addition, more traditional texts are being digitized all the time, and Crane (2006) tells us that while the largest academic digital libraries now hold tens of thousands of books, a completed Google Library will likely have more than ten million. To further Smith’s analogy, we are adrift in a sea of data, and we are at risk of floundering.

Front page news events like Edward Snowden’s May 2013 revelations disclosing the secret NSA (America’s National Security Agency) collection and analysis of massive amounts of ostensibly private data both domestically and internationally, are making big data analytics part of the public lexicon.  Major corporations, led by IBM with $1.3 billion in big data revenue in 2012, are scrambling to adopt practices that will allow them to capitalize on the volume of information now being generated.  Global big data related revenues in 2012 were $11.6 billion, and are projected to break $18 billion in 2013 (Kelly et al.).

While one might shudder to think what the NSA will do with all that information, outside of the world of politics and espionage, what can researchers and academics do with the volume of information now, or at least soon to be at our disposal? Matthew Jockers asks, “How do we mine them [texts] to find something we don’t already know?” (University of Nebraska-Lincoln) To utilize such massive corpora and avoid succumbing to the dilemma of what McCormick et al (1987) refer to as “information without interpretation” we need to find the most effective ways for end-users to explore, visualize and interact with big data such that it is both meaningful and understandable, if possible by both the trained and untrained eye. 

Big data analytics and computer-supported visualization offer ways to read collections as our cognitive abilities are stretched to their limit with the sheer volume of data available (Araya, 2003). However, as Franco Moretti has pointed out, what we are reading when we use text mining methods and visualizations are really models of the collections. (Moretti 2013, p. 157) It is important therefore to survey the visual models emerging and question if they can better be designed to suit humanities exploration. In this paper we therefore propose to look at visualization for text mining in the following ways:

Survey text mining visualization in the humanities. Who is using text mining and why? What kinds of visualizations do they find compelling and why?
Identify and Combine commonly presented visualizations for modeling. What visual models could be used for the exploration of large corpora? How could they be combined?  
Model interactive prototypes of different combinations of visualizations for exploration. 
Surveying: In a poster at DH 2013 we presented a framework of text mining tools that are useful in the humanities. Now we will survey the variety of visualizations used to present mining results. We will begin with early discussions of visualization like Smith’s “Computer Criticism” (Style 1978, p. 326), where he notes with agreement Paul de Man’s observation that as late as 1973 there had been no evolution beyond the close reading “techniques of description and interpretation” being used by literary critics since the 1930s or 40s, and suggests “pictorial representation” as one of the potential uses of computer aided text analysis. Brunet in a 1989 article talks about exploiting large corpora and provides a number of examples of visualizations. Our survey will examine advances in practice and understanding in the intervening thirty plus years, leading to contemporary works like Franco Moretti’s Graphs, Maps, and Trees, which, as the title suggests, introduces visual models for literary exploration.

Our survey pays particular attention to recent text mining projects and tools including “The Proceedings of the Old Bailey”, David L. Hoover’s work with cluster analyses at NYU using “MiniTab”, Matt Jockers’ topic modeling work in “Macroanalysis”, the University of Waikato’s “WEKA”, the open-source visualization tool “Gephi”, the UMass machine learning tool “MALLET”, the research tools for textual study reviewed on “TAPoR”as well as some of our own INKE related projects such as “Dynamic Table of Contents”, “CiteLens”, “TextTiles” and “dialR”. 

Identifying and Combining: While we recognize that output will assume a variety of formats including for instance heat maps, topographic plots or scatter plots, our survey suggests that text-mining projects commonly use five principal types of visualization:

1. Dendrograms, showing data clustering within sets
2. Histograms, showing change over time
3. Networkdiagrams, showing how entities are connected within a network
4. Wordclouds, representing topics of words
5. Scatterplots, showing words or parts in an abstract space
Visualizations that are presented in print are typically Spartan, focusing the attention on the results through careful design. All affordances are removed. The same types of visualizations automatically generated from large data sets, however, tend to be too dense to be useful and have to include affordances if meant to be interactive. Simple representative visualizations, like histograms for instance, are insufficient to display complex interrelationships. Dendrograms, especially if you are working with massive data sets, quickly become an illegible mass of inter-connectivity. Word clouds are good at showing the relative frequency of words in a text or topic, but not at comparing one text or topic to another. Network diagrams produce some beautiful results, but suffer from the same difficulties as dendrograms; large data sets quickly lead to illegibility.

“By visualizing information, we turn it into a landscape that you can explore with your eyes, a sort of information map. And when you’re lost in information, an information map is kind of useful.” — David McCandless, 2010

Modeling: Our research now is looking at ways to increase the exploratory power of visualization of large data sets. Used in combination, visualizations can provide otherwise elusive insight and clarity. They can also provide affordances for each other – a histogram can be used to explore a dendrogram. We have developed interactive prototypes using combinations of visualizations, and focusing on not simply allowing, but even encouraging the user to truly explore the (big) data, “function[ing] almost instinctively”, as McCullough (1996) stated, “to serve the process of development”. The more we can encourage users to explore and play with the data, the more likely they are to develop useful insights.


Fig. 1: Combination of a modified dendrogram showing clustering, and a diachronic timeline of 500 philosophy papers.

Figure 1 shows a prototype of a combination of dendrogram and histogram that we developed to visualize the clustering of 500 papers published between 1966 and 2004. The prototype is a combination of a modified dendrogram showing the clusters, and a diachronic visualization displaying the subjects over time. The displayed data is intuitively explorable, and the modified dendrogram is designed to encourage exploration. We developed the R code to prepare the data for interactivity. In Voyant we have developed skins that combine scatter plots and histograms, word clouds and histograms, and network diagrams with other tools. Again, the tools are open for others to recombine.

To conclude, surveying commonly used graphical representations allowed us to identify commonly used visualizations that humanists find useful. To scale these so that they can be used interactively to explore large data sets we have prototyped combinations, where one visualization can be used to explore another and vice versa. The goal is visualizations that help researchers make sense of big data; visualizations that let us explore the forest, not just the trees so that we can draw accurate and appropriate inferences from the data.

References
The Proceedings of the Old Bailey - http://www.oldbaileyonline.org/

Cluster Analysis, Principal Components Analysis (PCA), and T-testing in Minitab - https://files.nyu.edu/dh3/public/ClusterAnalysis-PCA-T-testingInMinitab.html

Jockers, M. Macroanalysis.

Weka 3: Data Mining Software in Java - http://www.cs.waikato.ac.nz/ml/weka/

Gephi - The Open Graph Viz Platform - https://gephi.org/

Mallet - MAchine Learning for LanguagE Toolkit - http://mallet.cs.umass.edu/

TAPoR - Research Tools for Textual Study - http://tapor.ca/

Dynamic Table of Contents - http://www.ualbertaprojects.info/dyntoc/dyntoc_v3_5/Main.html

CiteLens - http://labs.fluxo.art.br/CiteLens/

TextTiles - http://dev.giacometti.me/textTiles/trunk/

dialR - http://research.artsrn.ualberta.ca/~dialr/drMain.html

Allison, S., Heuser, R., Jockers, M., Moretti, F. and Witmore, M. (2012 ). Quantitative Formalism: An Experiment.Pamphlet 1, Stanford Literary Lab. Print.

Araya, A. A. (2003). The Hidden Side of Visualization. Techné: Research in Philosophy and Technology; Vol 7, No 2, Print.

Brunet, É. (1989). L'Exploitation des Grands Corpus: Le Bestiare de la Littérature Française. Literary and Linguistic Computing, Vol. 4, No. 2, p. 121-134. Print.

Crane, G. (2006). What Do You Do with a Million Books? D-Lib MagazineVol. 12 No. 3, Print.

Dragland, Å. (2013). Big Data, for better or worse. SINTEF.no. 22 May 2013. Web. 27 Oct. 2013.

Jockers, M. (2013). Macroanalysis : digital methods and literary history - University of Illinois Press, Urbana, Chicago & Springfield.

Kelly, J., Floyer, D., Vellante, D. and Miniman, S. (2013). Big Data Vendor Revenue and Market Forecast 2012-2017. Wikibon.org. 19 Feb. 2013. Web. 28 Oct. 2013.

McCandless, D. (2010). David McCandless: The beauty of data visualization. TED Talks. Web Video.  25 Oct.  2013. 

McCormick, Bruce H., DeFanti, Thomas A., and Brown, Maxine D. (1987). Visualization in Scientific Computing. Computer Graphics 21, 6 (November). New York: Association for Computing Machinery, SIGGRAPH, Print.

McCullough, M. (1996).  Abstracting Craft: The Practiced Digital Hand; Cambridge, MIT Press, Print.

Moretti, F. (2013). The End of the Beginning: A Reply to Christopher Prendergast.Distant Reading. London: Verso, p. 137-158. Print.

Risen, J. and Poitras, L. (2013). NSA Gathers Data on Social Connections of U.S. Citizens, New York Times, 28 Sep. 2013. Web, 27 Oct, 2013.

Simpson J.; Rockwell, G.; Sinclair, S.; Uszkalo, K.; Brown, S.; Dyrbye, A.; Chartier, R.. (2013). Framework for Testing Text Analysis and Mining Tools.Poster presented at the Digital Humanities 2013 conference at the University of Nebraska-Lincoln. Lincoln, Nebraska, USA.

Smith, J. (1978). Computer Criticism.Style. Vol XII, No 4. Print.

Smith, J. (1984). A New Environment For Literary Analysis.Perspectives in Computing 4. 2/3, (1984): 20-31. Print.

Tufte, Edward. (1983). The Visual Display of Quantitative Information; Cheshire, CT: Graphics Press, Print.

University of Nebraska-Lincoln. (2012). By text-mining the classics, UNL prof unearths new literary insights. UNL News Blog. 23 Aug. 2012. Web. 27 Oct. 2013.
How have text analysis tools in the humanities been imagined in the past? What did humanities computing developers think they were addressing with now dated technologies like punch cards, printed concordances and verbose command languages? Whether the analytic functionality is at the surface, as with Voyant Tools, or embedded at deeper levels, as with the Lucene-powered searching and browsing capabilities of the Old Bailey, the web-based text analysis tools that we use today are very different from the first tentative technologies developed by computing humanists. Following Siegfried Zieliniski's exploration of forgotten media technologies, this paper will look at three forgotten text analysis technologies and how they were introduced by their developers at the time. Specifically we will:

Discuss why is it important to recover forgotten tools and the discourse around these instruments,
Look at how punch cards were used in Roberto Busa’s Index Thomisticus project as a way of understanding data entry,
Look at Glickman’s ideas about custom card output from PRORA, as a way of recovering the importance of output,
Discuss the command language developed by John Smith for interacting with ARRAS, and
Conclude with a more general call for digital humanities archaeology. 
Zieliniski and Media Archaeology
Siegfried Zielinski, in Deep Time of the Media, argues that technology does not evolve smoothly and that we therefore need to look at periods of intense development and then look at the dead ends that get overlooked to understand the history of media technology. In particular he shows how important it is to look at technologies that are not in canonical histories as precursors to “successful” technologies, because they provide insight into the thinking at the time. A study of forgotten technologies can help us understand opportunities and challenges as they were perceived at the time and on their own terms rather than imposing our prejudices. From the 1950s until the early 1990s there was just such a period of technology development around mainframe and personal computer text analysis tools. The tools developed, the challenges they addressed, and the debates around these technologies have largely been forgotten in an age of web-mediated digital humanities. For this reason we recover three important mainframe projects that can help us understand how differently data entry, output and interaction were thought through before born- digital content, output to wall-sized screens, and interaction on a touchscreen. 

Busa and Tasman on Literary Data Processing 
The first case study we will present is about the methods that Father Busa and his collaborator Paul Tasman developed for the Index Thomisticus (Busa could hardly be considered a forgotten figure, but he's often referred to metonymically as a founder of the field, with relatively little attention paid to the specifics of his work and his collaborations). Busa, when reflecting back on the project justified his technical approach as supporting a philological method of research aimed at recapturing the way a past author used words, much as we want to recapture past development. He argued in 1980 that, “The reader should not simply attach to the words he reads the significance they have in his mind, but should try to find out what significance they had in the writer’s mind.” (Busa 1980, p. 83) Concordances could help redirect readers towards the “verbal system of an author” or how the author used words in their time and away from the temptation to interpret the text at hand using contemporary conceptual categories. Concording creates a new text that shows the verbal system, not the doctrine.

Busa’s collaborator Paul Tasman, however, presents a much more prosaic picture of their methodology that focuses on data entry using punch cards so you can actually get concordances of words. He published a paper in 1957 on “Literary Data Processing” in the IBM Journal of Research and Development that focuses on how they prepared their texts accounting for human error and other problems. Tasman writes, “It is evident, of course, that the transcription of the documents in these other fields necessitates special sets of ground rules and codes in order to provide for information retrieval, and the results will depend entirely upon the degree and refinement of coding and the variety of cross referencing desired.” (p. 256) This case study takes us back to a forgotten set of problems (representing text using punch cards) which led to more mature issues in text encoding. In the full presentation we will look closely at the data entry challenges faced by Busa’s team and how they were resolved with the card technology of the time. 

Glickman and Stallman on Printed Interfaces
The second case study we will look at is the development of the PRORA programs at the University of Toronto in the 1960s. PRORA was reviewed in the first issue of CHUM and with the publication of the Manual for the Printing of Literary Texts and Concordances by Computer by the University of Toronto Press in 1966 is one of the first academic analytical tools to be formally published in some fashion. What is particularly interesting, for our purposes, is the discussion in the Manual of how concordances might be printed. Glickman had idiosyncratic ideas about how concordances could be printed as cards for 2-ring binders so that they could be taken out and arranged on a table by users. He was combining binder technology with computing to reimagine the concordance text. Today we no longer think about output to paper as important to tools, and yet that is what the early tools were designed to do as they were not interactive. We will use this case study to recover what at the time was one of the most important features of a concording tool – how it could output something that could be published for others to use. 


Fig. 1: Example of PRORA output from the Manual

Smith and Interaction
One of the first text analysis tools designed to support interactive research was John Smith’s ARRAS. In ARRAS Smith developed a number of ideas about analysis that we now take for granted. ARRAS was interactive in the sense that it was not a batch program that you ran for output. It could generate visualizations and it was explicitly designed to be part of a multi-tasking research environment where you might be switching back and forth between analysis and word processing. Many of these ideas influenced the interactive PC concordancing tools that followed like TACT. In this paper, however, we are not going to focus on all the prescient features of ARRAS, but look at the now rather dated command language which Smith was so proud of. Almost no one uses a command language for text analysis any more; we expect our tools to have graphical user interfaces that provide affordances for direct manipulation. If you need to do something more than what Voyant, Tableau, Lucene, Gephi or Weka let you do, then you learn to program in a language like R or Python. John Smith by contrast, spent a lot of time trying to design a natural command language for ARRAS that humanists would find easy to use and this comes through in his publications on the tool (1984 & 1985). Command languages were, for a while, the way you interacted with such systems and attention to their design could make a difference. Smith tried to develop a command language that was conversational so humanists could learn to use it to explore “vast continents of literature or history or other realms of information, much as our ancestors explored new lands.” (Smith 1984, p. 31) Close commanding for distant reading. 

Conclusions
In the 2013 Busa Award lecture Willard McCarty called us to look to our history and specifically to look at the “incunabular” years before the web when humanists and artists were imagining what could be done. One challenge we face in reanimating this history is that so much of the story is in tools, standards and web sites – instruments difficult to interrogate the way we do texts. This paper looks back at one major thread of development - text analysis tools – not for the entertainment of outdated technology, but recover a way of thinking about technology. We will conclude by discussing other ways back including the need for better documentation about past tools, along the lines of what TAPoR 2.0 is supporting, and the need to preserve tools or at least a record of their usage. 

References
Busa, R. (1980). "The Annals of Humanities Computing: The Index Thomisticus." Computers and the Humanities. 14(2): 83-90.

Glickman, Robert Jay, and Gerrit Joseph Staalman. Manual for the Printing of Literary Texts and Concordances by Computer. Toronto: University of Toronto Press, 1966.

Liu, Alan. (2012) “Where is Cultural Criticism in the Digital Humanities.” In Debates in the Digital Humanities. Ed. Matthew K. Gold. University of Minnesota Press. Liu’s essay is online at <http://dhdebates.gc.cuny.edu/debates/part/11>.

Smith, J. B. (1978). "Computer Criticism." STYLE XII(4): 326-356.

Smith, J. B. (1984). "A New Environment For Literary Analysis." Perspectives in Computing 4(2/3): 20-31.

Smith, J. B. (1985). Arras User's Manual: TR85-036. Chapel Hill, NC, The University of North Carolina at Chapel Hill.

Tasman, P. (1957). "Literary Data Processing." IBM Journal of Research and Development 1(3): 249-256.

Zieliniski, Siegfried. (2008) Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. Cambridge, Massachusetts: The MIT Press.
0. Introduction

Quantitative analysis of literary texts is now well established in authorship attribution. There are continuing lively discussions of method, and the understanding of how classification works best with language continues to evolve, but there are some successes to point to, and most literary scholars accept that when experts disagree on an attribution, a statistical approach can be helpful. The great advantage for quantitative work in this area is that methods can be tested with texts of known origin, so that calibration and validation can be done. Practitioners and traditional scholars can share confidence in rigorous studies with good sampling, controls and validation of various kinds.
There is also the potential for computational stylistics to make a contribution in stylometry beyond authorship attribution and in the wider area of literary interpretation. However, here the problem of validation is acute. If a surprising finding emerges from a quantitative study, how can we tell a chance result, or an artefact of method, from a well-founded finding? How do we judge the robustness of the results and the degree to which conclusions may be generalized? How do we relate analyses based on thousands of long texts to the established understanding of areas of culture based on the intensive study of a few works? The best first options in moving beyond authorship may be other areas of classification, where validation is still possible, like chronology and genre study, but the bigger challenge and greatest rewards will be in interpretation in the wider sense.
This panel approaches the question of validation in computational stylistics beyond authorship attribution through case studies in a variety of languages and literary traditions. Each of the case studies concerns computational stylistics beyond authorship attribution, discusses issues of validation, robustness, and/or interpretation, and offers some considerations of the wider questions of method which arise. The panel will combine brief presentations of the use cases exemplifying the larger issues with ample time for discussion among the panelists and with the audience.
1. Fotis Jannidis & Hugh Craig: Statistical complexity in the language of lowbrow and highbrow novels in German and English

In this study we apply Shannon Entropy and Jensen-Shannon Divergence (Lin, 1991; Rosso et al., 2009) to the language of the novel, using one German and one English corpus from the nineteenth century, and one English corpus from the late twentieth century. We focus on the way low-brow and high-brow novels score on the two measures. We rely on classifications from standard literary histories for the novels and explore the statistical results with close readings of selected passages.
Figure 1 is a graph of Jensen-Shannon Divergence and Shannon Entropy in the more modern English corpus. The novels from the Booker Prize shortlist have generally higher scores on both measures than the other identified groups of low-brow novels (with a p-value of 0.0007 for the t-test on JSD). 

Fig. 1: Jensen-Shannon Divergence and Shannon Entropy scores for 376 novels in the BNC
The pattern for the nineteenth-century corpora is different. In the early- to mid-Victorian English novels, works by Charles Dickens have low scores on both Entropy and Divergence, despite his canonical status. In the 1860-90 German corpus, some popular novels, for example those by Johanna Spyri, author of the famous Heidi books, also have low scores on Entropy and Divergence, but some works by popular authors like Marlitt span the highest to the lowest range of divergence, revealing a hitherto unnoticed variety of styles (the t-test didn’t show a significant difference between the groups). Under some circumstances Entropy, which has been judged not to be a useful measure for author-attribution studies (Hoover, 2003), and Jensen-Shannon Divergence seem to be useful to distinguish between lowbrow and highbrow novels, but it is yet unclear under which circumstances. In the paper we will discuss the overall relationship between the information-theory metrics as applied to language and classifications of the novels in terms of market sectors, relying on standard measures of statistical significance to validate our claims. 
2. Maciej Eder: Bootstrap consensus network: towards a robust visualization in stylometry

Stylometric methodology, developed to solve authorship problems, can easily be extended and generalized to assess different questions in literary history. Explanatory multidimensional methods, relying on distance measures and supported with visualization techniques, are particularly attractive for this purpose. However, they are very sensitive to the number of features (usually: frequent words) analyzed. Even worse, they are either unable to fit dozens of texts on a single scatterplot (e.g. Multidimensional Scaling), or highly dependent on the choice of a linkage algorithm (e.g. Cluster Analysis). The technique introduced in this study combines the concept of network as a way to map large-scale literary similarities (Jockers, 2013), the concept of consensus (Lancichinetti and Fortunato, 2012), and the assumption that textual relations usually go beyond mere nearest neighborship. 

Fig. 2: Two algorithms of mapping textual relations
Particular texts can be represented as nodes of a network, and their explicit relations as links between these nodes. The procedure of linking is twofold. One of the involved algorithms (Fig. 2, top) computes the distances between analyzed texts, and establishes, for every single node, a strong connection to its nearest neighbor (i.e. the most similar text), and two weaker connections to the 1st and the 2nd runner-up (i.e. two texts that get ranked immediately after the nearest neighbor). The second algorithm (Fig. 2, bottom) performs a large number of tests for similarity with different number of features to be analyzed (e.g. 100, 200, 300, …, 1,000 MFWs). Finally, all the connections produced in particular “snapshots” are added, resulting in a consensus network. Weights of these final connections tend to differ significantly: the strongest ones mean robust nearest neighbors, while weak links stand for secondary and/or accidental similarities. Validation of the results – or rather self-validation – is provided by the fact that consensus of many single approaches to the same corpus sanitizes robust textual similarities and filters out apparent clusterings. The idea discussed in this paper can be applied to map large literary corpora (see the contribution by Jan Rybicki, below).
3. Jan Rybicki: Validating a large bootstrap consensus network in literary history

Over five hundred English novels from Swift to Rowling were used to produce a bootstrap consensus network of most-frequent-word frequencies, using a pseudo-bootstrapped cluster analysis from 100 to 1,000 most frequent words with the stylo package (Eder et al., 2013) for R and visualized with GEPHI’s Force Atlas 2 layout algorithm (Bastian et al., 2009). The resulting graph yielded the usual strong authorship signal, but the overall shape exhibited a number of features that make sense in the context of traditional literary history. 

Fig. 3: Bootstrap consensus network of over 500 English novels
The overall shape of the resulting network (Fig. 3) is reminiscent of the earlier-observed “stylistic drift” phenomenon (Burrows, 1994) in its general chronological order and observes certain topographic rules. The texts are roughly ordered from top (early) to bottom (late), with three avenues of transition from the top-most 18th-century writings to late Victorians and modernists: Americans Melville and Hawthorne; Dickens; and the mid-Victorian female writers, with some notable outliers. Most of the latest texts in this set gravitate towards the bottom area of the graph.
With the impact of spelling variation minimized by 100% culling (a procedure by which all words that do not appear in each of the studied texts are rejected from the analysis), this is a clear indication that distant reading by most-frequent-words frequencies can mirror the evolution of literary style over hundreds of texts and hundreds of years and open new perspectives for close reading. After all, the application of statistics to literature is remarkable in that its results can be validated not by statistical means alone, but also, and perhaps above all, by traditional literary history, classification and interpretation.
4. Christof Schöch: Validating and interpreting Principal Component Analysis: A Case-Study from the Analysis of French Enlightenment Plays

This case study investigates issues of validation and interpretation of stylometric results with regard to authorship, genre, date and form, based on a collection of 120 French plays from the French Enlightenment period. Preliminary analyses using Cluster Analysis have suggested that besides authorship, categories like genre (tragedy or comedy) and form (verse or prose) are important stylistic signals in this collection.
To verify this observation, PCA (Jackson, 2003; Diana & Tommasi, 2002) was performed on different subsets of plays. In a subset of 19 comedies in either verse or prose written by five authors between 1712 and 1760, PCA shows strong effects for form (verse or prose) but results appear to vary for different settings (particularly, number of frequent words).
To test the salience of the effects for form and the robustness of the results, the contribution of several variables to the first three principal components was calculated for different settings. More precisely, F-measures of ANOVA tests were calculated for author, form and date in relation to PC1 to PC3 for PCAs based on 5-200 most frequent words.

Fig. 4: F-scores for author, form and data on PC1 to PC3
Figure 4 shows how PC1 is dominated by “form”, with an extremely high F-score, while “author” and “date” hardly contribute. In PC2, form does not play any role, but “author” and also “date” do. Considering its reduced scale, it appears that PC3 does not show any clear trends. 
That the effect for “form” is so concentrated in one major component comes as a surprise, especially because similar effects have not been observed in other domains, such as Early Modern English Drama (Hugh Craig, paper in preparation). However, given the striking robustness of the results, the interpretation of the PCA can proceed with confidence. Further application domains of this method are stylometric investigations of variables such as genre, theme or literary period.
5. Mike Kestemont: Learning Deep Representations of Characters in Literary History

One of the most exciting movements in current Machine Learning is “Deep Learning” (Bengio, 2009). In this field, people attempt to leave the idea of “hand-crafted” features. Older, so-called “shallow” learning techniques – commonly used in stylometry – heavily depend on a researcher’s, typically strongly biased, representation of a problem and will not attempt to optimize or even correct this representation. In “Deep Learning”, the idea is that one should not only learn how to solve a problem, given some input information, but additionally, how the input information is best represented in order to solve the problem. To achieve this, researchers send data through a layered structure of units (“neural network”). At each subsequent layer in this network architecture, the representation of the original input information grows increasingly abstract.

Fig. 5: Country and Capital Vectors Projected by PCA. (Copyright: Mikolov et al. for Google Inc.)
Recent research has demonstrated that Deep Learning yields extremely valuable problem representations. In distributional semantics, for instance, it yields an extremely powerful vector-space model of words. This contribution will survey how such vector spaces have recently been used for advanced analogical reasoning. In a series of breakthrough papers, Mikolov et al. (e.g. 2013) have shown that these models (see Fig. 5) can be used to answer complex questions like: “What is to king, like woman is to man?” (answer: “queen”), or “What is to Warsaw, like France is to Paris?” (answer: “Poland”). I will discuss how this kind of representational learning could be applied to modeling characters from literary history. The main idea is that we should be able to easily answer questions about the archetypical relationships between characters: Who is to Romeo, like Isolde is to Tristan?”. I will argue that this approach offers an exciting new framework to study the “meaning” of literary personas (cf. Bamman et al. 2013) and their cross-novel interrelations.
References

Bamman, D., Brendan O’Connor, B. and Smith, N. (2013). Learning Latent Personas of Film Characters. ACL 2013, Sofia, Bulgaria. 352-361.
Bastian M., Heymann S. and Jacomy M. (2009). Gephi: an open source software for exploring and manipulating networks. International AAAI Conference on Weblogs and Social Media.
Bengio, Y. (2009). Learning Deep Architectures for AI. Foundations and Trends in Information Retrieval 2: 1-127. 
Burrows, J. F. (1994). Tiptoeing into the infinite: testing for evidence of national differences in the language of English narrative.Research in Humanities Computing, 2: 1-33.
Diana, G., and Tommasi, Ch. (2002). Cross-validation Methods in Principal Component Analysis: A Comparison. Statistical Methods and Applications 11/1: 71-82.
Eder, M., Kestemont, M. and Rybicki, J. (2013). Stylometry with R: a suite of tools. Digital Humanities 2013: Conference Abstracts. Lincoln: University of Nebraska-Lincoln, pp. 487-89.
Hoover, D. (2003). Another perspective on vocabulary richness. Computers and the Humanities, 37: 151-78.
Jackson, J.E. (2003).A User’s Guide to Principal Components. Hoboken: Wiley.
Jockers, M. (2013). Macroanalysis: Digital Methods and Literary History. Champaign: University of Illinois Press.
Lancichinetti, A. and Fortunato, S. (2012). Consensus clustering in complex networks. Scientific Reports, 2: 336: 1-7.
Lin, J. (1991). Divergence measures based on the Shannon entropy. IEEE Transactions in Information Theory, 37(1): 145-51.
Mikolov, T., Yih W. and Zweig, G. (2013). Linguistic Regularities in Continuous Space Word Representations. Proceedings of NAACL-HLT 2013, Atlanta, Georgia, 746-751.
Rosso, O., Craig, H., and Moscato, P. (2009). Shakespeare and other English Renaissance authors as characterized by Information Theory complexity quantifiers. Physica A, 388: 916-26.

Visualization As a Bridge to Close Reading: The Audience in The Castle of Perseverance

1. Introduction

In thinking of visualizations and how they can help bridge the gap between traditional close readings of texts and digital projects, Matthew Kirschenbaum asks, "What patterns would be of interest to literary scholars? … How would we evaluate the effectiveness of our visualizations, or the software in general?  Is it succeeding if it surprises us with its results, or if it doesn’t?"1  Many scholars have noted the importance of approaching a visualization project with an appropriate amount of scholarly attention, and it is my intention to show how the activity of creating a visualization of a text and the visualization itself can highlight areas of the text which would benefit from a traditional close reading.234  The text I have chosen for this visualization-prompted close reading is The Castle of Perseverance, a 15th century morality play which takes the form of a locus-and-platea play.  Creating a visualization of the network of characters within the play leads us to inquire into the role of the audience of the play, both the original audience of medieval spectators and contemporary readers or watches of the play.

The Castle of Perseverance stage plan, Macro MS folio 191v.
2. Tool

The tool I have chosen for the visualization is Gephi, an open-source graphic visualization software.  Gephi is "an interactive visualization and exploration platform for all kinds of networks and complex systems, dynamic and hierarchical graphs."5  I have two main reasons for this choice.  The first is that Gephi is a relatively easy program to work with, while also being quite powerful and allowing a nuanced approach to the visualization process.  The second reason for this choice is the set of algorithms which come with Gephi, two of which I will be using in my visualizations.  One measures "Betweenness Centrality," a measure of the size of the network and the average path length between nodes, in this case characters, in the network.  The second algorithm measures modularity and identifies community groups within the network.  Different critical approaches to visualizing will affect the ways in which these two algorithms interpret the data and the resulting visualization will lead to different conclusions.
3. Theoretical Issues

One of the benefits of creating a visualization of the network within a play is that "nothing ever disappears.  What is done, cannot be undone … The past becomes the past, yes, but it never disappears from our perception of the plot."6  In the case of The Castle of Perseverance, however, this permanence of action that exists in a visualization can cause some problems.  Medieval morality plays, by nature, are heavily didactic and often address the audience directly.  In The Castle of Perseverance, over 800 lines, more than one-fifth of the play, are directed at the audience.  Throughout the play, the audience is addressed both directly and indirectly, "all the men that in this werld wold thryve" (521), "Lordyngys" (1425), and "all men" (3694) to give but a few examples.7  Should a visualization of the network of characters in a play include the audience of that play?  They have no speaking role and would certainly not be included in any list of dramatis personae.  Leaving the audience out of the network leads to the network which appears in figure 2.

Fig. 1: The Castle of Perseverance network, without the audience as a node.
If we are troubled by leaving out such a large portion of the play and create a network which includes the audience, the result is figure 3.

Fig. 2: The Castle of Perseverance network with the audience included as a node.
The interesting difference between these visualizations is the members of the various communities as identified by Gephi.  In figure 2, Humanum Genus, the mankind figure, is grouped with the bad angle and most of the seven deadly sins.  In figure 3, however, Humanum Genus is grouped with the good angle and the seven virtues; this is also how the play ends, with Humanum Genus asking for God’s mercy and ascending to heaven.  The audience in some way, according to the communities identified by Gephi, is a factor in Humanum Genus ending the play in God’s grace.  These two maps of the network within The Castle of Perseverance cause us to turn back to the text for a close reading of the ways in which the audience appears as a character through the efforts of the dramatic characters of the play.
References

1. Kirschenbaum, M.Poetry, Patterns, and Provocation: The Nora Project. The Valve. January 12, 2006. Web. October 31, 2013.
2. Jessop, Martyn (2008). Digital Visualization As a Scholarly Activity. Literary and Linguistic Computing 23.3: 281-193. Print.
3. Rieder, Bernhard and Theo Rohle (2012). Digital Methods: Five Challenges. Understanding Digital Humanities. Ed. David M. Berry. New York: Palgrave Macmillan. Print. 67-84.
4. Scully, D. and Bradley M. Pasanek (2008). Meaning and Mining: The Implicit Assumptions in Data Mining for the Humanities. Literary and Linguisting Computing 23.4: 409-424. Print.
www.gephi.org
6. Moretti, Franco. (2011). Network Theory, Plot Analysis. Stanford, Stanford Literary Lab.
7. Eccles, Mark (1969). The Macro Plays. London: Oxford University Press. Print.


    
        
            
                A Study of Symbolic Element Network in National Emblems
                
                    
                        Liu
                        Jyi-Shane
                    
                    Natioanl Chengchi University, Taiwan, Republic of China
                    jyishane.liu@gmail.com
                
                
                    
                        Ning
                        Ke-Chi
                    
                    Natioanl Chengchi University, Taiwan, Republic of China
                    floater.xkernel@gmail.com
                
                
                    
                        Huang
                        Tze-Jung
                    
                    Natioanl Chengchi University, Taiwan, Republic of China
                    4happistar@gmail.com
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    social network analysis application
                    sybolism
                    similarity and divergence
                
                
                    content analysis
                    anthropology
                    networks
                    relationships
                    graphs
                    linking and annotation
                    data mining / text mining
                    English
                
            
        
    
    
        
            Human social life is a repository of symbols in various forms to mark, celebrate, and glorify social groups (Durkheim, 1976). Following the development of modern nationalism, national symbols are devised to identify attachment to specific territories and forge a national unity of particular ethnic and cultural heritage (Smith, 1999). The political, cultural, and ideological identity of an independent country is mostly proclaimed by three symbols: the national flag, the national anthem, and the national emblem (Firth, 1973; Geisler, 2005). The universal adoption of these ethno-cultural identity signifiers provides a great research opportunity to understand the global and regional characteristics of group symbolism in humanity (Butz, 2009; Elgenius, 2011).
            In this paper, we study the similarity and divergence of symbolic elements in ethno-cultural identity with a focus on national emblems. We apply the data analytic method of social network analysis to a total of 205 national emblems such that relational characteristics of symbolic elements and human communities are examined at multiple levels of composite granularity. We conduct exploratory functionalities of the symbolic analytic framework. The results indicate the potential of fruitful discovery in deciphering symbolism in humanity. As a case of application, this study shows the value of digital humanities research in complementing traditional methods of sampled observation and subjective induction.
            National Emblems and Data Model 
            A national emblem is an abstract or representational pictorial pattern that signifies the history, myth, or value of a nation and is regarded as one of the core national symbols. This pictorial pattern is designed to reflect and project a nation’s image with a layout of elementary entities in a set of colors. Every nation and state in the modern world has adopted a national emblem—sometimes also called a great seal or an official coat of arms—of its own for both domestic and international use, such as on the cover of a national passport. Compared to national flags, national emblems, in most cases, seem to contain more elements that encode rich anthropological and ethnic features. For example, the national flag of France (see Figure 1a), is a tri-color layout featuring three vertical bands in blue, white, and red, while the coat of arms of France (Figure 1b) is designed with stronger historical and cultural ingredients and attached with vivid symbolic meaning.
            
                
            
            Figure 1a. French national flag.
            
                
            
            Figure 1b. French national emblem.
            A national emblem is a graphical form of expression that encodes information both at the syntactic and semantic levels. At the syntactic level, we observe the surface forms of the elementary entities and the colors. The French national emblem is composed of entities such as an ax, a rod, an oak branch, a laurel branch, a lion’s head, an eagle’s head, a shield, and a monogram, in yellow (golden) and brown. These elementary entities symbolize justice, wisdom, victory, and name at the semantic level. We retrieve descriptive data of the national emblems of a total of 205 countries from Wikipedia. Each record includes the syntactic data of elementary entities with colors and the semantic data of symbolic meaning. The syntactic data of entities are further aggregated into class and meta-class to provide a three-level granularity of data analysis. For example, ‘ax’ and ‘sword’ at the entity level are aggregated into ‘weapon’ at the class level and ‘man-made objects’ at the meta-class level. Similarly, individual countries are also aggregated into region and continent levels. Table 1 outlines the three levels of data classification, the corresponding data size, and partial data content. 
             Table 1. Syntactic Data Model of National Emblems
            
                
                    Data Type
                    Data Level
                    Data Size
                    Data List
                
                
                    Symbolic Element
                    Meta-class
                    6
                    animal, plant, natural/physical objects, human, man-made objects, human ideas
                
                
                    
                    Class
                    32
                    amphibian, quadruped, bird, celestial body, crop, tree, text, weapon, . . . , etc.
                
                
                    
                    Entity
                    308
                    eagle, lion, sun, moon, star, rice, torch, ribbon, knight, dragon, diamond, . . . , etc.
                
                
                    Color
                    —
                    9
                    black, white, grey, red, blue, green, yellow, orange, brown
                
                
                    Human Community
                    Continent
                    5
                    Africa, Asia, America, Europe, Oceania
                
                
                    
                    Region
                    20
                    East Asia, Southeast Asia, Central Asia, South Asia, Western Asia, North America, Central America, . . . , etc.
                
                
                    
                    Country
                    205
                    Malaysia, Cambodia, Venezuela, Finland, Niger, Oman, Fiji, . . . , etc.
                
            
            Applying Social Network Analysis 
            
                Affiliation Network Model
            
            Social network analysis is a data analytic model that encodes relations among a set of entities and reveals the structural characteristics of their interaction pattern (Wasserman and Faust, 1994). For the national emblem data, social network analysis provides a systematic mechanism to explore and analyze the ideological and cultural relations among human communities. The current study focuses on syntactic data and separates the syntactic variables of symbolic element and color. In network modeling, we construct affiliation networks consisting of a set of symbolic elements (or colors) and a set of human communities. A symbolic element (or color) is linked to a community based on membership in national emblem composition. For dual perspectives, this two-mode bipartite network can also be projected or reduced to two one-mode networks (Newman, 2001; Zhou et al., 2007). One is the element (or color) co-membership network where elements are connected to each other when they share membership of a national emblem composition. The other one-mode network is the community affinity network, where ties represent mutual adoption of the same element (color) in national emblems. All ties in either two-mode or one-mode networks are valued by occurrence frequency to reflect the strength of the relationship. 
            Network Indicators 
            Social network can be analyzed at different levels for inspecting the embedded relations among entities. Among the many proposed network measurements, we adopt a set of core indicators to observe the essential characteristics of similarity and divergence of self-projected ideology and values among global human communities. The calculation of these network indicators and the network visualization are done with Gephi, a network analysis software package developed by Gephi Consortium. 
             • Node level: 
                Degree centrality measures the extent to which a node is involved in extensive relationship. 
                Betweenness centrality estimates how much a node lies on the paths between any two other nodes. Node ranking by both centrality indicators helps classify a node’s location as core, peripheral, or intermediary in the network. 
            
             • Sub-group level: 
                Modularity is a clustering measurement that detects sub-groups where there are more intensive links between nodes within than among the rest of the network (Newman, 2006). This indicator may be useful in identifying families of ideological elements and human communities at different levels. 
            
            Analytical Results
            The integration of a flexible analytical framework of social network analysis and the size and depth of the emblem symbolic data obviously provide a wide horizon of relational characteristics. For example, based on the syntactic data model of Table 1, we can construct two affiliation networks, color-human_community, and symbolic_ element-human_community, one at 1 x 3 and the other at 3 x 3 composite granularity levels. Each affiliation network can also be projected into two one-mode networks. This will result in a total of 36 networks that encode different aspects and various granularities of relational information. Limited by the space of this paper, we demonstrate two distinct analytic functions and present the results of a few interesting discoveries. 
            Discovery by Observation
            We first observe how the nine primary colors are used in national emblems across the world. Figure 2a shows the color-country affiliation network where the size of color nodes is proportional to its degree (frequency of use by country). The top three colors are yellow (149), white (133), and red (129), followed by blue (107), green (73), and black (41). All other colors are significantly minor. 
            
                
            
            Figure 2a. Overview of color-country affiliation network.
            
                
            
            Figure 2b. Unique and strong association of color-region.
            Next we examine the color and region relations and apply the modularity measurement to approximate the separation of cohesive sub-groups (Barber, 2007). As shown in Figure 2b, several unique associative patterns between region and color are identified that provide interesting information. For example, the color blue is a common symbol of Northern Europe, Western Europe, and the Caribbean, while the color green uniquely represents Western Asia and East Africa. This analytic observation based on systematic data exploration can complement traditional symbolism research in sociology and anthropology (Kolsto, 2006; Podeh, 2011).
            For entity-country affiliation, it is observed that a total of 308 unique element entities are used 1,624 times in 205 national emblems with a long-tailed distribution. The top 10 entities are listed in Table 2 and account for 38.2% of occurrence frequency. When the one-mode entity co-membership network is constructed, several cohesive subgroups are identified. One of the interesting patterns of entity association is shown in Figure 3, with shield, eagle, lion, cross, crown, and spear as members of the primary core, and seems to indicate a strong European flavor. 
            
                
                    
                        Table 2. Symbolic element distribution.
                        
                            
                                Rank
                                Entity
                                Freq.
                                Perc.
                            
                            
                                1
                                ribbon
                                110
                                6.8%
                            
                            
                                2
                                shield
                                109
                                6.7%
                            
                            
                                3
                                motto
                                97
                                6.0%
                            
                            
                                4
                                star
                                64
                                3.9%
                            
                            
                                5
                                sun
                                51
                                3.1%
                            
                            
                                6
                                lion
                                44
                                2.7%
                            
                            
                                7
                                nation name
                                43
                                2.6%
                            
                            
                                8
                                eagle
                                36
                                2.2%
                            
                            
                                9
                                crown
                                34
                                2.1%
                            
                            
                                10
                                mountain
                                33
                                2.0%
                            
                        
                    
                    
                
                
                    
                    
                
            
            
                
            
            Figure 3. A sub-group of entity co-membership with European flavor.
            Discovery by Query
            Another way to decode the embedded information from the various affiliation networks is to conduct a focused exploration for specific questions. Suppose we start with the questions of how regions share the use of similar symbolic elements and what the overall diversity is across regions. An entity-region affiliation network may provide partial answers with appropriate granularity. Results of applying network indicators are shown in Table 3, where 20 regions are divided into nine subgroups based on common use of symbolic entities. Several families of regions emerge that reveal different aspects of similarity and divergence in symbolic expression of humanity. Sub-group 1 share a common cultural heritage, even though North America is geographically separated from other members. Members of sub-group 2 are both geographically distant and culturally independent. This grouping presents an interesting phenomenon and calls for further investigation. Sub-groups 3, 4, and 5 seem to be both geographically and culturally connected, while the remaining sub-groups are more isolated from others. Figures 4a and 4b show the entity-region networks of sub-groups 1 and 2.
            Table 3. Regional grouping by common use of symbolic entities.
            
                
                    
                        Sub-
                        group
                    
                    Region(s)
                    Core Entity
                    Degree Centrality
                    Betweenness Centrality
                
                
                    1
                    Southern Europe, North America, Western Europe, Central Europe
                    ribbon
                    18
                    2313.8
                
                
                    
                    
                    shield
                    18
                    2174.9
                
                
                    
                    
                    eagle
                    15
                    1310.0
                
                
                    
                    
                    spear
                    12
                    1032.0
                
                
                    
                    
                    olive
                    11
                    972.4
                
                
                    2
                    South Asia, Northern Europe, North Africa
                    star
                    18
                    2344.3
                
                
                    
                    
                    motto
                    16
                    1930.7
                
                
                    
                    
                    lion
                    17
                    1831.2
                
                
                    
                    
                    moon
                    12
                    990.1
                
                
                    
                    
                    sword
                    12
                    927.9
                
                
                    
                    
                    crown
                    13
                    819.9
                
                
                    3
                    
                        Southeast Asia, Central Asia, 
                        East Asia
                    
                    sun
                    16
                    1838.7
                
                
                    
                    
                    nation name
                    15
                    1736.1
                
                
                    
                    
                    mountain
                    13
                    1280.2
                
                
                    
                    
                    wheat
                    10
                    609.3
                
                
                    
                    
                    river
                    9
                    512.4
                
                
                    4
                    
                        West Africa, 
                        East Africa, Central Africa
                    
                    flag
                    13
                    1250.8
                
                
                    
                    
                    palm (tree)
                    10
                    875.9
                
                
                    
                    
                    ship
                    10
                    734.9
                
                
                    
                    
                    helmet
                    9
                    656.6
                
                
                    5
                    Western Asia, Eastern Europe
                    sea
                    10
                    737.0
                
                
                    
                    
                    arrow
                    7
                    368.3
                
                
                    
                    
                    oak
                    8
                    357.9
                
                
                    
                    
                    horse
                    7
                    270.1
                
                
                    6
                    Caribbean, Oceania
                    chain
                    7
                    375.9
                
                
                    
                    
                    stripe
                    5
                    194.8
                
                
                    7
                    Central America
                    triangle
                    6
                    260.2
                
                
                    
                    
                    cogwheel
                    6
                    258.1
                
                
                    8
                    Southern Africa
                    man
                    6
                    357.9
                
                
                    
                    
                    rifle
                    4
                    130.9
                
                
                    
                    
                    sheep
                    4
                    77.5
                
                
                    9
                    South America
                    bow
                    5
                    270.1
                
                
                    
                    
                    ax
                    5
                    241.2
                
                
                    
                    
                    coffee
                    5
                    185.2
                
            
            Discussion and Conclusion 
            Symbolism is a fundamental form of humanity expression. While a national emblem forms an integral context of correlated symbolic elements, we believe that a single symbolic element is itself a unit of idea expression and can be examined by an objective data analysis. It is the interpretation of analytic results that needs to take into account the contextual information. In this proposal, our purpose is to show the methodological process of analyzing the inter-relations among symbolic elements while cautiously refrain from making conclusive interpretation. 
                Through the co-occurrence of the lexical and geographical presence of the emblem symbols, we utilize social network analysis as an effective tool to reveal the emergent information. 
            
            This study demonstrates the great potential of employing a data analytic framework for explorative discovery in sociology and anthropology. 
                Our approach helps unlock the less than obvious information in the symbol-location relationship by presenting a lexical-based concept in a non-linear fashion, opening it up for interpretations that are not so readily available via standard appearance of materials. Furthermore, it allows the lexical-based concept to speak in its multiplicity. The initial results seem to provide rich implications for better understanding global humanity’s similarities and divergences. Our future work includes interdisciplinary research with sociologists and/or anthropologists for more in-depth investigation and interpretation, as well as an extension to analyzing the relations of symbolic semantic meaning in national emblems. 
            
            
                
            
            Figure 4a. Entity-region network of sub-group 1.
            
                
            
            Figure 4a. Entity-region network of sub-group 2.
        
        
            
                
                    Bibliography
                    
                        Barber, M. J. (2007). Modularity and Community Detection in Bipartite Networks. 
                        Physical Review E,
                        76(6): 066102.
                    
                    
                        Butz, D. A. (2009). National Symbols as Agents of Psychological and Social Changes. 
                        Political Psychology,
                        30(3): 779–804.
                    
                    
                        Durkheim, E. (1976). 
                        The Elementary Forms of the Religious Life. 2nd ed. George Allen and Unwin, London.
                    
                    
                        Elgenius, G. (2011). The Politics of Recognition: Symbols, Nation-Building and Rival Nationalism. 
                        Nations and Nationalism,
                        17(2): 396–418.
                    
                    
                        Firth, R. (1973). 
                        Symbols: Public and Private. George Allen and Unwin, London.
                    
                    
                        Geisler, M. E. (2005). What Are National Symbols—and What Do They Do to Us? In Geisler, M. E. (ed.), 
                        National Symbols, Fractured Identities: Contesting the National Narrative. Middlebury, VT: Middlebury College Press, pp. XIII–XLII.
                    
                    
                        Kolsto, P. (2006). National Symbols as Signs of Unity and Division. 
                        Ethnic and Racial Studies,
                        29(4): 676–701.
                    
                    
                        Newman, M. E. (2001). Scientific Collaboration Networks. II. Shortest Paths, Weighted Networks, and Centrality. 
                        Physical Review E,
                        64(1): 016132.
                    
                    
                        Newman, M. E. (2006). Modularity and Community Structure in Networks. 
                        Proceedings of the National Academy of Sciences,
                        103(23): 8577–82.
                    
                    
                        Podeh, E. (2011). The Symbolism of the Arab Flag in Modern Arab States: Between Commonality and Uniqueness. 
                        Nations and Nationalism,
                        17(2): 410–22.
                    
                    
                        Smith, A. D. (1999). 
                        Myths and Memories of the Nation. Oxford University Press, Oxford, UK.
                    
                    
                        Wasserman, S. and Faust, K. (1994). 
                        Social Network Analysis: Methods and Applications. Cambridge University Press, Cambridge, UK.
                    
                    
                        Zhou, T., Ren, J., Medo, M., and Zhang, Y. C. (2007). Bipartite Network Projection and Personal Recommendation. 
                        Physical Review E,
                        76(4): 046115. 
                    
                
            
        
    



    
        
            
                Building Better Linked Data &amp; Ontology Visualization Tools
                
                    
                        Simpson
                        John
                    
                    University of Alberta, Canada
                    john.simpson@ualberta.ca
                
                
                    
                        Brown
                        Susan
                    
                    University of Alberta, Canada; University of Guelph, Canada
                    sibrown@ualberta.ca
                
                
                    
                        Smith Elford
                        Jana
                    
                    University of Alberta, Canada
                    smithelf@ualberta.ca
                
                
                    
                        Murphy
                        Shawn
                    
                    Semandra, Canada
                    shawn@semandra.com
                
                
                    
                        Brundin
                        Michael
                    
                    University of Alberta, Canada
                    brundin@ualberta.ca
                
                
                    
                        Warren
                        Robert
                    
                    Carleton University; University of Guelph, Canada
                    rwarren@math.carleton.ca
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    Visualization
                    Linked Data
                    Semantic Web
                    Ontologies
                    Tools
                
                
                    interface and user experience design
                    data modeling and architecture including hypothesis-driven modeling
                    ontologies
                    software design and development
                    knowledge representation
                    internet / world wide web
                    visualisation
                    semantic web
                    networks
                    relationships
                    graphs
                    data mining / text mining
                    English
                
            
        
    
    
        
            In 2012 Google indexed an estimated 46 billion web pages (staticbrain, 2014). Although the Web of linked data was only 3/10,000s this size in the same period (Simpson and Brown, 2014), 15.9 million pages is not an insignificant number (Cheng et al., 2013). Furthermore, as an emerging and burgeoning technological field, we can be certain that these numbers have grown significantly since then. With such a vast amount of data in these two Webs, the increasingly pressing concern is how to understand what is contained within them, particularly when it comes to seeing relationships within the content of each. This remains a problem even with linked data and, when warranted, the ontologies that connect such data, even though such data was built to make relationships explicit and available for exploration. This is a problem that others have articulated as well. For instance, Loskyll et al. ask, ‘How can one represent huge collections of knowledge (e.g. ontologies with over 10 million concepts) as browsable trees in a scalable manner and with a clear user interface?’ (2009, 385). While part of the solution would include appropriately curating the original data, this is not feasible with large datasets, and thus a solution must be borne by improved visualization tools and techniques. While there is a clear interest in using visualizations to navigate this information, there is no generally agreed-upon method to aid in this task. Should we render information that would otherwise need to be inferred ‘actually explicitly visible’ (Howse et al., 2011, 259), provide ontologists with an ‘effective solution’ for understanding ‘relations between existing concepts’ (Kocbek et al., 2013, 34), or any number of other purposes? Balancing these concerns remains a challenge despite the fact that visualizations have played an important role in assisting the retrieval and dissemination of both qualitative and quantitative information for centuries (Tufte, 2006; 2001). In this paper, we ask, 
                What features should a Semantic Web visualization tool have to maximize the discovery of new knowledge?
            
            This paper answers this question in two stages: it provides a review and evaluation of 30 existing Semantic Web–related visualization tools, and it reports on the production of a completed network visualization tool that is now entering the user testing phase. 
            
                Review of Linked Data and Ontology Visualization Tools
            
            Before reviewing the features of the network visualization tool we have produced, we will summarize the results of a review of 30 existing tools or techniques that target visualizations of either linked data or ontologies. While it is the case that surveys of visualization tools for linked data and ontologies have been carried out before,
                1 since 2007 there have been none that we know of and none conducted on this scale. The list of tools or techniques that are included in our review is displayed in Table 1.
            
            
                
                    
                        CropCircles
                        (Wang and Parsia, 2006)
                        
                            Cytoscape http://www.cytoscape.org/
                        
                        D3 (Data-Driven Documents)
                        http://d3js.org/
                        Gephi 
                        http://gephi.org/
                        
                            Google Visualization API with RDF 
                        
                        http://data-gov.tw.rpi.edu/wiki/How_to_use_Google_Visualization_API 
                        
                            GraphViz http://www.graphviz.org
                        
                        HyperTree
                        (Bingham and Sudarsanam, 2000)
                        http://kinase.com/tools/HyperTree.html
                        InfoVis ToolKit 
                        http://philogb.github.com/jit/
                        
                            IsaViz http://www.w3.org/2001/11/IsaViz/
                        
                        
                            Jambalaya (Storey et al., 2001)
                        
                        
                            KC-Viz (NeOn Toolkit Plug-in) 
                        
                        (Motta et al., 2011) http://neon-toolkit.org/wiki/KC-Viz
                    
                    
                        Knoocks
                        (Kriglstein and Wallner, 2010)
                        http://enomisk.net/knoocks/
                        LodLive
                        http://en.lodlive.it/
                        NeOn Toolkit 
                        http://neon-toolkit.org/wiki/Main_Page
                        Networkx
                        http://networkx.github.io/ 
                        
                            OntoViz http://protegewiki.stanford.edu/wiki/OntoViz
                        
                        Prefuse
                        http://prefuse.org/
                        ProcessingJS
                        http://processingjs.org/
                        
                            Protégé VOWL Plugin (Lohmann et al., 2014
                            )
                        
                        http://protege.stanford.edu/
                        
                            Protovis http://mbostock.github.com/protovis/
                        
                        
                            RDF Gravity http://semweb.salzburgresearch.at/apps/rdf-gravity/ 
                        
                        
                            RDF2SVG (Rhizomik) http://rhizomik.net/html/redefer/rdf2svg-form/ 
                        
                        Redland
                        http://librdf.org/
                        
                            SIMILE Exhibit / Exhibit 3.0 http://simile-widgets.org/exhibit/ 
                        
                    
                    
                        Sp
                            
                                aceTree
                            
                        
                        (Plaisant et al., 2002)
                        http://www.cs.umd.edu/hcil/spacetree/
                        
                            Spicy Nodes http://www.spicynodes.org/
                        
                        
                            Tableau Public http://www.tableausoftware.com/public
                        
                        
                            Tom Sawyer Software http://www.tomsawyer.com/home/index.php
                        
                        TopBraid Composer 
                        http://www.topquadrant.com/product/TB_Composer.html
                        
                            Visual Browser http://nlp.fi.muni.cz/projekty/visualbrowser/
                        
                        
                            Visual DataWeb (RelFinder, SemLens, gFacet, and tFacet) http://www.visualdataweb.org/tools.php 
                        
                        
                            Voyage RSS Reader http://rssvoyage.com/
                        
                        
                            Welkin http://simile.mit.edu/welkin/
                        
                        
                            Wiki Map Project http://ickn.org/wikimaps/
                        
                        Yago
                        http://www.mpi-inf.mpg.de/yago-naga/yago/index.html
                    
                
            
            Table 1. Linked data and ontology visualization tools.
            A New Visualization Tool
            In 2014, Lohman et al. pointed out, ‘While several visualizations for ontologies have been developed in the last couple of years, they either focus on specific ontology aspects or are hard to read for non-expert users. The silver bullet would be an ontology visualization that is equally comprehensive and comprehensible. It must be printable but also provide intuitive ways to interactively explore ontologies’ (395). While the focus of this comment is clearly on ontologies, the same is true of visualizations for linked data in general. This is just one set of criteria that a so-called silver-bullet visualization tool ostensibly should have. Others include key concept extraction, a rich set of navigation and visualization mechanisms, flexible zooming into and hiding of specific parts of an ontology, history browsing, saving and loading of customized ontology views, essential interface customization support, such as graphical zooming, font manipulation, tree layout customization, clear representations of hierarchy and predicates, and the availability of multiple geometrical views (Motta et al., 2011; Sivakumar and Arivoli, 2011). Clearly, there are a lot of expectations to be met.
            In constructing a new visualization tool for ontologies and linked data, our primary goal was a general one: build an interactive visualization tool that can be used by non-expert users to explore linked data. Of course, such general goals are often riddled with complexities when put into practice, and that was certainly the case here as we worked to address many of the criteria felt to be important by other developers (see above) as well as within our own team. The result has been the tool illustrated in Figure 1, which we are tentatively referring to as HuVis (short for Humanities Visualizer). As the figure makes clear, the interface of the tool is broken out into three areas: a central stage; a tabbed command construction history, documentation, and configuration window on the right; and a text-viewing area on the left that displays text to which the triples are linked.
            
                
            
            Figure 1. The HuVis tool in action.
            In the center of the display are all the entities of the dataset arranged in a circle around a central visualization stage. Entities, represented as nodes, are colour-coded by type and may be dragged onto the stage or reshelved in the ring by dragging with the mouse. When dragged to the stage, all the nodes immediately connected within the pre-loaded linked dataset to the added entity are also added. Nodes may be removed from consideration by dragging them to a discard area, here shown by the small circle of blue dots in the lower right of the figure. To aid users with navigation, the default display of labels is limited to those attached to nodes that are either in the center stage area or those within the range of the mouse pointer. A circular area around the mouse pointer also acts as a magnifying glass, making it easy to differentiate entities even if they are crowded into the display area.
            The command panel in the upper right of the display is a powerful tool for controlling the content of the visualization. With just a few clicks, users may execute commands to display or hide groupings of nodes from the stage area and control the display of those entities. Such selections may be made in conjunction with direct selections of elements in the stage area or outer ring, based on entity type, or based on the predicates by which entities are connected. Other features, such as general instructions for the tool (the Help function) and the controls for the layout of the stage area (Settings), are available in the same window through a tab interface.
            In the top left of the window is the ‘snippet display’. This area shows portions of the text from which the links in the center display area have been drawn. Snippets are revealed by clicking on a link between the two nodes. These snippets are important so that scholars may examine the criteria by which the connections that they are seeing are justified. 
            Conclusions
            Our paper will report on the results of the user testing conducted in early 2015. We will share our insights, as well as a set of recommendations for ontology and linked-data visualization based on the results of this study.
            Note
            1. The review by Katifori et al. (2007) in particular has been highly influential in terms of ontology visualizations.
        
        
            
                
                    Bibliography
                    
                        Bingham, J. and Sudarsanam, S. (2000). Visualizing Large Hierarchical Clusters in Hyperbolic Space. 
                        Bioinformatics,
                        16(7): 660–61, http://bioinformatics.oxfordjournals.org/content/16/7/660.full.pdf.
                    
                    
                        Cheng, G., Liu, M. and Qu, Y. (2013). NJVR: The NanJing Vocabulary Repository. In 
                        Semantic Web and Web Science. Springer, pp. 265–72, http://link.springer.com/chapter/10.1007/978-1-4614-6880-6_23.
                    
                    
                        Howse, J., Stapleton, G., Taylor, K. and Chapman, P. (2011). Visualizing Ontologies: A Case Study. In Aroyo, L., Welty, C., Alani, H., Taylor, J., Bernstein, A., Kagal, L., Noy, N. and Blomqvist, E. (eds), 
                        The Semantic Web. ISWC 2011. Berlin: Springer, pp. 257–72, http://link.springer.com/chapter/10.1007/978-3-642-25073-6_17.
                    
                    
                        Katifori, A., Halatsis, C., Lepouras, G., Vassilakis, C. and Giannopoulou, E. (2007). Ontology Visualization Methods: A Survey. 
                        ACM Computing Surveys,
                        39(4): 10-es, doi:10.1145/1287620.1287621.
                    
                    
                        Kocbek, S., Kim, J., Perret, J. and Whetzel, P. L. (2013). Visualizing Ontology Mappings to Help Ontology Engineers Identify Relevant Ontologies for Their Reuse. In 
                        ICBO, pp. 34–39. http://bionlp.dbcls.jp/redmine/attachments/download/145/kocbek_icbo%20camera.pdf.
                    
                    
                        Kriglstein, S. and Wallner, G. (2010). Knoocks: A Visualization Approach for OWL Lite Ontologies. 
                        CISIS 2010: 4th International Conference on Complex, Intelligent and Software Intensive Systems, pp. 950–55, doi:10.1109/CISIS.2010.55.
                    
                    
                        Lohmann, S., Negru, S. and Bold, D. (2014). The ProtégéVOWL Plugin: Ontology Visualization for Everyone. In 
                        The Semantic Web. ESWC 2014 Satellite Events. New York: Springer, pp. 395–400, http://2014.eswc-conferences.org/sites/default/files/eswc2014pd_submission_19.pdf.
                    
                    
                        Loskyll, M., Heckmann, D. and Glahn, C. (2009). Visualization of Spatial Knowledge with Ontology Trees and Adaptable Search Result Grids in the Era of Web3. 0. In Tochtermann, K. and Maurer, H (eds), Know’09, 
                        9th International Conference on Knowledge Management and Knowledge Technologies, Graz, Austria, pp. 385–90, http://dspace.learningnetworks.org/handle/1820/2060.
                    
                    
                        Motta, E., Mulholland, P., Peroni, S., d’ Aquin, M., Gomez-Perez, J. M., Mendez, V. and Zablith, F. (2011). A Novel Approach to Visualizing and Navigating Ontologies. 
                        The Semantic Web. 
                        ISWC 2011. Springer, pp. 470–86, http://link.springer.com/chapter/10.1007/978-3-642-25073-6_30.
                    
                    
                        Plaisant, C., Grosjean, J. and Bederson, B. B. (2002). SpaceTree: Supporting Exploration in Large Node Link Tree, Design Evolution and Empirical Evaluation. 
                        INFOVIS 2002: IEEE Symposium on Information Visualization, pp. 57–64, doi:10.1109/INFVIS.2002.1173148, ftp://ftp.cs.umd.edu/pub/hcil/Reports-Abstracts-Bibliography/2002-05html/2002-05.pdf.
                    
                    
                        Simpson, J. and Brown, S. (2014). Inference and Linking on the Humanist’s Semantic Web. 
                        INKE, Building Partnerships to Transform Scholarly Publishing. Whistler, British Columbia.
                    
                    
                        Sivakumar, R. and Arivoli, P. V. (2011). Ontology Visualization Protégé Tools: A Review. 
                        International Journal of Advanced Information Technology,
                        1(4), https://idc-online.com/technical_references/pdfs/information_technology/ONTOLOGY%20VISUALIZATION.pdf.
                    
                    
                        Staticbrain. (2014). http://www.statisticbrain.com/total-number-of-pages-indexed-by-google/ (accessed 1 March 2015).
                    
                    
                        Storey, M., Musen, M., Silva, J., Best, C., Ernst, N., Ferguson, R. and Noy, N. (2001). Interactive Visualization to Enhance Ontology Authoring and Knowledge Acquisition in Protégé. In 
                        Protégé: Workshop on Interactive Tools for Knowledge Capture (K-CAP-2001), http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.8402.
                    
                    
                        Tufte, E. R. (2001). 
                        The Visual Display of Quantitative Information. 2nd ed. Graphics Press, Cheshire, CT.
                    
                    
                        Tufte, E. R. (2006). 
                        Beautiful Evidence. Graphics Press, Cheshire, CT.
                    
                    
                        Wang, T. D. and Parsia, B. (2006). CropCircles: Topology Sensitive Visualization of OWL Class Hierarchies. In 
                        Proceedings of the 5th International Conference on Semantic Web, pp. 695–708.
                    
                
            
        
    



    
        
            
                Exploring Large Datasets with Topic Model Visualizations
                
                    
                        Montague
                        John
                    
                    University of Alberta, Canada
                    jmontagu@ualberta.ca
                
                
                    
                        Simpson
                        John
                    
                    University of Alberta, Canada
                    john.simpson@ualberta.ca
                
                
                    
                        Rockwell
                        Geoffrey
                    
                    University of Alberta, Canada
                    geoffrey.rockwell@ualberta.ca
                
                
                    
                        Ruecker
                        Stan
                    
                    Illinois Institute of Technology, USA
                    sruecker@id.iit.edu
                
                
                    
                        Brown
                        Susan
                    
                    University of Alberta, Canada; University of Guelph, Canada
                    sibrown@ualberta.ca
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    Topic Models
                    Graphs
                    Big Data
                    Visualization
                
                
                    philosophy
                    text analysis
                    visualisation
                    networks
                    relationships
                    graphs
                    English
                
            
        
    
    
        
            How can topic model visualizations aid in the exploration of large datasets? The standard method for visualizing topic models involves a two-stage process that begins with the use of a tool specifically designed to construct topic models on a specified corpus. MALLET (McCallum, 2002), a command line tool that implements the Latent Dirichlet Allocation (LDA) algorithm described by Blei et al. (2003), provides output in the form of text files that can be shared directly with other tools and seems to be the default tool of choice for the first stage. Regardless of the topic modeller used, its output is then passed to a separate tool for the second stage in the production of the visualization. These separate tools are either general visualization tools or topic model specific visualization tools. The use of general visualization tools is attractive because it typically requires a lower technical competency and allows the user to leverage skills they already have, but general tools provide fewer targeted affordances for exploration. Still, despite the existence of at least a dozen tools that have been used to visualize topic models, it remains the case that none of them are particularly well suited to exploring large datasets comprising thousands or millions of entries in a corpus distributed over a wide range of time and with a correspondingly large set of candidate topics produced by topic modelling. 
            This paper provides a solution to this problem by describing a tool that has been designed specifically to address this problem and that is now entering production. Before describing the tool and providing a short demonstration of its functionality, we summarize the current landscape of topic model visualizations and provide recommendations based on various use-cases. 
            
                Topic Modelling Visualizations
            
            The visualization methods used across these tools can be fruitfully categorized using a set of modified categories drawn from work done by Katifori et al. (2007) on ontology visualizations: 
             • 
                Traditional charts (Bar, line, pie, scatterplot, etc.): D-VITA (Gunnemann et al., 2013); Tableau (Sharkey and Ansari, 2014); ParallelTopics (Dou et al., 2011); NIPS (Iwata et al., 2008); MetaToMATo (Snyder et al., 2013).
            
             • 
                Network graphs: Gephi (Chen et al., 2013); Topicnet (Gretarsson et al., 2012). 
            
             • 
                Zoomable tools: an unnamed tool by Chaney and Blei (2012). 
            
             • 
                2D matrix: Serendip (Alexander et al., 2014); Termite (Chuang et al., 2012).
            
            The variety of visualization techniques being pursued are evidence of the difficulties related to displaying relevant information from topic modelling, as each visualization method has been developed or chosen with a particular application in mind. The tools that particularly focus on exploring the underlying data and revealing connections are Serendip and MetaToMATo, neither of which is a graph-based visualization, and both of which become overwhelmed by large datasets. Gephi is a general-purpose graph-based visualization tool that has been used to visualize large datasets (Munster, Jockers), but the images it produces are static and it does not easily lend itself to sharing information derived from topic models. Drawing on lessons from a review of all these tools, but Serendip, MetaToMATo, and Gephi in particular, the tool we report on constructing is targeted at exploration using graph-based visualizations of topic models on large corpuses. 
            
                Topic Modelling Philosophy Journals
            
            Motivation for this project has been brought about in part by work done over the past year with a corpus of philosophy journals acquired from JSTOR (Simpson et al., 2014). While investigating this corpus we made use of chart-based visualizations with R after preprocessing with MALLET. While there was much to be learned from the changing prevalence of particular topics over time, there were also a number of things that the visualizations were not able to tell us. In particular, our first visualizations made it difficult to easily see connections between the different topics, to see relations between the different journals, and to have this information present itself with increasing detail as required for the investigation. This challenge arose in large part because the dataset covered 27,536 journal articles from 10 different journals published between 1876 and 2008. Datasets of this size and diversity are becoming increasingly common, and so we moved to pursue a tool capable of visualizing the results of topic modelling. Not finding a suitable one for the exploration of large datasets, we set about designing a new one. 
            
                Visualizations in 3D
            
            MacEachren et al. (1994) suggest that all visualizations are meant to principally perform one of the following tasks: clearly present previously discovered information, analyze a particular dataset, combine multiple datasets, or explore data for knowledge discovery. It is the last category on which our visualization research team has been particularly focused: visualizations that allow the user to not simply understand or analyze a dataset, but to explore it in hopes of uncovering new information. 
            As Lauren Frederica-Klein says in her 2013 digital humanities start-up grant application for the Interactive TOpic Model and MEtadata Visualization (TOME) project
                , regarding Wise et al.’s Galaxies visualization, ‘By forcing all thematic differences into a single two-dimensional presentation, information is inevitably lost’. In an effort to minimize this information loss, we are now creating an exploration tool for the purpose of experiencing a topic-modeled corpus in 3D space, using the free, open-source JavaScript framework 
                Famo.us. 
            
            The 
                Famo.us framework is designed to help ‘create smooth, complex UIs for any screen’, and will provide us some substantial benefits, first and foremost being that its CSS-like styling code is relatively straightforward to learn and implement. Additionally, using physics-based animations, three orientation axes, and opacity control, it is capable of rendering a touch-screen manipulable, three-dimensional environment, the interactivity afforded by which is something we believe will help greatly foster user exploration. As explained by Card et al. (1999), ‘This additional [3rd] dimension projects from the viewpoint toward infinity, creating a large visible workspace’, a decidedly beneficial quality when visually exploring a large dataset. 
            
            Chuang et al. (2012) describe the illegible results of experimenting with topic modelling visualizations where text is displayed directly in the visualization matrix. To reduce the volume of unwanted information, our design implements Healey’s (1996) notion of ‘knowledge discovery’, allowing the user to 
                filter unwanted data, and what Petre and Green (1992) describe as ‘secondary notation; the use of layout and perceptual cues which are not formally part of the notation (elements such as adjacency, clustering, whitespace, labeling and so on)’. Like Healey, we believe this will help ease any display or perception bottleneck and afford the user a unique opportunity to discover and explore unknown trends and relationships in the data. The user will customize the visibility of elements of the dataset and secondary notation by adjusting a series of controls to make alterations to the sensitivity of, for instance, clustering algorithms, labeling, or the visibility of edges in a network. 
            
            Since any single visualization is fundamentally limited in its ability to communicate very complex relationships, like Andrew Goldstone’s DfR browser (2013), and Snyder et al.’s Metadata and Topic Model Analysis Toolkit (MetaToMATo) (2013), our concept merges multiple visualizations into what Snyder et al. term ‘a single faceted browsing paradigm for exploration and analysis of document collections’. Word clouds generated via topic modelling, histograms, line graphs, and network diagramming are all experienced via a zoomable user interface (ZUI). Snyder et al. (2013) explain that ‘effectively exploring and analyzing large text corpora requires visualizations that provide a high level summary’. Figure 1 shows how the viewer will be able to move from that high-level summary, ‘flying’ deeper and deeper into the data, simultaneously experiencing multiple visualizations from unique vantage points, and, as espoused by Chuang et al. (2012), ‘drilling down’ into additional layers of information as desired. 
            
                
            
            Figure 1. As the user navigates deeper into the dataset, more and different relationships are revealed. 
        
        
            
                
                    Bibliography
                    
                        Alexander, E., Kohlmann, J., Valenza, R., Witmore, M. and Gleicher, M. (2014). Serendip: Topic Model-Driven Visual Exploration of Text Corpora, https://graphics.cs.wisc.edu/Papers/2014/AKVWG14/Preprint.pdf. 
                    
                    
                        Blei, D. M., Ng, A. Y. and Jordan, M. I. (2003). Latent Dirichlet Allocation. 
                        Journal of Machine Learning Research, 
                        3: 993–1022. 
                    
                    
                        Card, S. K., Mackinlay, J. D. and Shneiderman, B. (1999). 
                        Readings in Information Visualization: Using Vision to Think. Morgan Kaufmann, San Francisco.
                    
                    
                        Chaney, A. J.-B. and Blei, D. M. (2012). Visualizing Topic Models. 
                        Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media, Dublin, Ireland, 4–7 June 2012, http://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/download/4645%26lt%3B/5021. 
                    
                    
                        Chen, A. T., Sheble, L. and Eichler, G. (2013). Topic Modeling and Network Visualization to Explore Patient Experiences. 
                        Visual Analytics in Health Care, Washington, DC, 16 November 2013, http://ruby.ils.unc.edu/~atchen/pubs/Chen_Sheble_Eichler_VAHC2013.pdf.
                    
                    
                        Chuang, J., Manning, C. D. and Heer, J. (2012). Termite: Visualization Techniques for Assessing Textual Topic Models. Capri Islands, Italy, http://vis.stanford.edu/files/2012TermiteAVI.pdf. 
                    
                    
                        Dou, W., Wang, X., Kraft, T. and Ribarsky, W. (2011). Identifying Topical Trends in Social Media with Topic Modeling. University of North Carolina, Charlotte. http://vialab.science.uoit.ca/textvis2011/papers/textvis%202011-dou.pdf.
                    
                    
                        Frederica-Klein, L. (2013). TOpic Model and MEtadata Visualization Project (TOME), digital humanities startup grant application. 
                    
                    
                        Goldstone, A. (2013). drfbrowser: Take a MALLET to Literary History, http://agoldst.github.io/dfrbrowser/web. 
                    
                    
                        Gretarsson, B., O’Donovan, J., Bostandjiev, S., Höllerer, T., Asuncion, A., Newman, D. and Smyth, P. (2012). Topicnets: Visual Analysis of Large Text Corpora with Topic Modeling. 
                        ACM Transactions on Intelligent Systems and Technology (TIST), 
                        3(2): 23. 
                    
                    
                        Günnemann, N., Derntl, M., Klamma, R. and Jarke, M. (2013). An Interactive System for Visual Analytics of Dynamic Topic Models. 
                        Datenbank-Spektrum, 
                        13(3): 213–23. 
                    
                    
                        Healey, C. (1996). Effective Visualization of Large Multidimensional Datasets. Department of Computer Science Thesis, University of British Columbia. 
                    
                    
                        Iwata, T., Yamada, T. and Ueda, N. (2008). Probabilistic Latent Semantic Visualization: Topic Model for Visualizing Documents. In 
                        Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, 24–27 August 2008, pp. 363–71, http://dl.acm.org/citation.cfm?id=1401937. 
                    
                    
                        Jiao, Z. L., Liu, Q., Li, Y.-F., Marriott, K. and Wybrow, M. (2013). Visualization of Large Ontologies with Landmarks. In 
                        Proceedings of the International Conference on Computer Graphics Theory and Applications and International Conference on Information Visualization Theory and Applications, Barcelona, pp. 461–70, http://www.csse.monash.edu.au/~mwybrow/papers/jiaoivapp2013.pdf. 
                    
                    
                        Katifori, A., Halatsis, C., Lepouras, G., Vassilakis, C. and Giannopoulou, E. (2007). Ontology Visualization Methods Survey. 
                        ACM Computing Surveys,
                        39(4) (2 November): 10–es, doi:10.1145/1287620.1287621. 
                    
                    
                        MacEachren, A. M., Bishop, I., Dykes, J., Dorling, D. and Gatrell, A. (1994). Introduction to Advances in Visualizing Spatial Data. In 
                        Visualization in Geographical Information Systems. London: John Wiley &amp; Sons, pp. 51–59.
                    
                    
                        McCallum, A. K. (2002). MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu. 
                    
                    
                        Petre, M. and Green, T. R. G. (1992). Learning to Read Graphics: Some Evidence That ‘Seeing’ an Information Display Is an Acquired Skill. 
                        Journal of Visual Languages and Computing, 
                        4: 55–70. 
                    
                    
                        Sharkey, M. and Ansari, M. (2014). Deconstruct and Reconstruct: Using Topic Modeling on an Analytics Corpus. In 
                        LAK Workshops, http://ceurws. org/Vol1137/ lakdatachallenge2014_submission_1.pdf. 
                    
                    
                        Simpson, J., Rockwell, G. and Sinclair, S. (2014). The Epidemiology of Ideas. 
                        Canadian Society for Digital Humanities Annual Conference, Brock University, May 2014. 
                    
                    
                        Snyder, J., Knowles, R., Dredze, M., Gormley, M. and Wolfe, T. (2013). Topic Models and Metadata for Visualizing Text Corpora. 
                        Proceedings of the NAACL HLT 2013 Demonstration Session, Atlanta, 10–12 June 2013, pp. 5–9.
                    
                
            
        
    



    
        
            
                Knowledge Networks, Juxtaposed: Disciplinarity in the Encyclopédie and Wikipedia
                
                    
                        Heuser
                        Ryan
                    
                    Stanford University
                    heuser@stanford.edu
                
                
                    
                        Algee-Hewitt
                        Mark
                    
                    Stanford University
                    malgeehe@stanford.edu
                
                
                    
                        Bender
                        John
                    
                    Stanford University
                    bender@stanford.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    encyclopedia
                    knowledge
                    network
                    Enlightenment
                    disciplinarity
                
                
                    historical studies
                    multilingual / multicultural approaches
                    visualisation
                    networks
                    relationships
                    graphs
                    media studies
                    data mining / text mining
                    English
                
            
        
    
    
        
            The disciplines now maintained in universities have changed sharply since the 18th century, when they were still largely theological. However, the 18th century’s academies and encyclopedias set the stage for this transformation. How can the digital humanities help us understand and articulate such changes in the disciplinary structure within systems of knowledge?
            Specifically, our project is designed to use methods of network analysis and visualization to explore commonalities and differences between the 
                Encyclopédie (1751–1772), Diderot’s landmark encyclopedia of the 18th-century Enlightenment, and the Wikipedia of today. One of Diderot’s powerful innovations was a pervasive system of cross-references, networking the many thousands of articles into a navigable system. Wikipedia is built upon a similar system, both hyperlinking its articles together and recommending related articles under the heading of ‘See also’. Our project wagers that a juxtaposition of these cross-reference networks allows us to historicize the particular structures of knowledge interconnectedness as manifested in the 
                Encyclopédie and in its modern Wikipedia descendant. Our results show that, perhaps unintuitively, cross-references more commonly ‘cut across’ demarcations of Enlightenment-born disciplines in Diderot’s 
                Encyclopédie than in Wikipedia.
            
            
                
            
            Figure 1. The article cross-reference network of the 
                Encyclopédie. Each node is an article; each edge, or link, indicates a cross-reference between them. Articles without cross-references are not displayed here. The network was visualized in the software Gephi and colored by its network modularity algorithm with default settings.
            
            Although 65 times larger than the 
                Encyclopédie, Wikipedia, like Diderot’s encyclopedia, works within categories and operates by selection.
                1 One covert but powerful principle of selection within Wikipedia is that of modern disciplines. Although Wikipedia is not specifically organized by disciplines, they figure powerfully. Just one specific instance is that of the well-known article on the ‘philosopher’ in the 
                Encyclopédie, assigning the broadest possible intellectual role to the kind of man of letters Diderot signified by the term. In Wikipedia, by contrast, ‘philosopher’ is specifically about practitioners of the modern formal discipline of philosophy.
            
            Below, we compare these two encyclopedias by combining interpretive readings of cross-reference networks drawn from particular articles (Experiment 1) with a large-scale computational approach (Experiment 2). For the digitized text of the 
                Encyclopédie, we turned to ARTFL, a comprehensive database of thousands of 18th-century French texts, including all 74,000 articles in Diderot’s 
                Encyclopédie (‘ARTFL 
                Encyclopédie Project’, n.d.). Mark Olsen and other researchers involved in the project have computationally identified and hyperlinked the 
                renvois, or cross-references, embedded in its articles (‘Renvois Navigation’, n.d.)
                . Gilles Blanchard and Mark Olsen have published on the relationship between the 
                Encyclopédie’s ‘tree of knowledge’ and its 
                renvois, as two distinct models of knowledge organization, showing which parts of the tree are more strongly connected via cross-references than others. In this project, we juxtapose and compare the cross-reference networks of two encyclopedias created more than two centuries apart, in order to make visible and better understand the historicity of knowledge networks.
            
            Experiment 1: Case Studies of Key Article Networks
            First we drew up a list of articles on a wide range of topics that reflected a diversity of disciplines and that contained a large number of cross-references in both encyclopedias. These articles include, for instance, ‘philosopher’, ‘commerce’, ‘agriculture’, ‘nature’, and ‘existence’. For each selected article, we created a network linking it to all of the articles it cross-references; then we repeated this process only once for each of the cross-referenced articles. The resulting network is a directed cross-reference network with a depth of 2. For this experiment, we considered as cross-references all 
                renvois of the 
                Encyclopédie article, and all links under ‘See also’ in the corresponding English Wikipedia article.
                2
            
            
                
            
            Figure 2. An article cross-reference network, centered on the article ‘commerce’ in the 
                Encyclopédie, emanating two levels deep.
            
            
                
            
            Figure 3: An article cross-reference network, centered on the article ‘commerce’ in the English Wikipedia, emanating two levels deep.
            For example, one of the most striking comparisons we’ve found is the article on ‘commerce’. The 
                Encyclopédie links ‘commerce’ to such diverse topics as art, navigation, and agriculture at the first level, and poetry, mathematics, and construction at the second (Figure 2). Wikipedia’s version of ‘commerce’, by contrast, is tightly disciplinary (Figure 3). At the first level of cross-references, nearly all of the headings are from business, trade, and manufacture. At the second level, at the furthest reach from the core of economics, we have ‘shoplifting’ and ‘classical liberalism’. Clearly, when it comes to ‘commerce’, the unannounced, implicitly defined discipline adhered to by Wikipedia is business management—no room for categories such as tolerance, winter, magnetism, the military, or poetic theory.
            
            In these case studies, we see over and over again an increase in disciplinarity of the sort we see in ‘commerce’—but also a move to technical practices more generally, some of which are disciplinary, some of which are commercial, and some of which are institutional abstractions.
            Experiment 2: Large-Scale Network Comparison
            A potential objection to these English Wikipedia examples lies in our translation of 18th-century French concepts into 21st-century English. The translation of the words from French to English potentially introduces some inconsistencies. With this in mind, we move from particular article networks to a big-data approach, comparing the overall citational structure of the 
                Encyclopédie to that of the French Wikipedia (hereafter 
                Wikipedie). Although the French version is about a third smaller than the English, it is still 20 times larger than the 
                Encyclopédie. In order to compare it to the 
                Encyclopédie, therefore, we reduced both to the subset of headwords appearing in both encyclopedias. This left us with about 16,000 articles. Because the 
                Wikipedie is smaller than its English equivalent, and less likely to have maintained ‘See also’ sections, we decided to consider as cross-references all of the hyperlinks in each article. This is a departure from our earlier practice (see note 2).
            
            For every possible pair of the remaining articles, we calculated the shortest path between them, in the cross-reference networks of both the 
                Encyclopédie and the 
                Wikipedie. Then we looked at which links in each network were the most traversed among all of the shortest paths taken. By shortest path, we mean the smallest number of 
                renvois necessary to reach a second article from a first. These most traversed links are, literally, structurally central to each system of knowledge: if one were navigating either encyclopedia by its cross-references, these would be the connections that you would be most likely to cross. In network theory, this type of centrality is known as 
                betweenness centrality.
            
            Our study of these central links showed a significant difference in the types of relationships so central for the two encyclopedias.
                3 By classifying them in terms of their analogical relationship, we discovered important differences in the way in which the two cross-reference networks are structured. In the 
                Encyclopédie, these relationships are most often characterized by a ‘structuring’ or ‘governing’ analogy (Figure 4). For instance, ‘langue’ is governed by ‘gammaire’, ‘terre’ by its ‘axe’, ‘destin’ by ‘Dieu’, ‘religion’ by ‘foi’, and ‘justice’ by ‘droit’.
                4 In each case, the two concepts are distinct in kind, but they are brought together by a type of relationship in which the first concept depends on the second to give it order and form.
            
            
                
                    
                    
                
            
            Figures 4 (left) and 5 (right): Links with greatest betweenness centrality, in the 
                Encyclopédie (left) and 
                Wikipedie (right) networks, respectively. Here, nodes consist only of articles present in both encyclopedias, and edges only the cross-references between them.
            
            In the 
                Wikipedie, none of the most important links are characterized by this type of analogy (Figure 5). Instead, the most frequent type of relationship is one of membership between two concepts of the same kind. For example, an ‘ecclesiastique’ is a member of the ‘clerge’; ‘vin’ is a type of ‘esprit’; ‘addition’ is a type of ‘arithmetique’.
                5 Although the governing relationships in the 
                Encyclopédie place the entries in large conceptual hierarchies, the taxonomic relationships in the 
                Wikipedie point to disciplinary relationships, where knowledge is interrelated along an axis of specialization and depth. Compared with the 
                Encyclopédie, the 
                Wikipedie is less likely to make the metaphysical connection between arithmetic and, for example, logic; instead, it connects arithmetic to its constituent parts: addition, subtraction, multiplication, and division. We believe this juxtaposition of cross-reference practices and network structures reveals a notable shift toward disciplinarity in modernity, made visible by a combination of computational and hermeneutic methods.
            
            Conclusion
            From our research, then, it appears that the broad inclusivity of Wikipedia is, if anything, more deeply disciplinary than even the planned knowledge system of the 
                Encyclopédie. It is worth wondering about how powerfully Wikipedia is driven by formal disciplines despite its largely informal surface structure. We are finding that where Wikipedia is not more clearly disciplinary, many articles still move toward a greater level of abstraction and institutional formation than in the 
                Encyclopédie.
            
            
                
            
            Notes
            1. The English Wikipedia contains 4.6 million articles. See http://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia (accessed 2 November 2014).
            2. In this experiment, we decided to leave aside the interlinear hyperlinks within most Wikipedia articles, because we believe they more poorly reflect the intentional editorial decisions of Diderot and his collaborators in making the 
                renvois than do the cross-references contained under the ‘See also’ section. We decided to use the English Wikipedia in this experiment because, as the largest Wikipedia, it has received the most editorial attention and is most likely to contain the ‘See also’ section.
            
            3. For this experiment, we annotated the type of analogy evident in the top-15 most central links from the shortest-path traversals of both encyclopedias’ cross-reference networks.
            4. English translation: For instance, language is governed by grammar, Earth by its axis, destiny by God, faith by religion, and justice by right.
            5. English translation: For instance, a clergyman is a member of the clergy; wine is a type of spirits; addition is a type of arithmetic. 
        
        
            
                
                    Bibliography
                    
                        ‘ARTFL Encyclopédie Project’. (n.d.). Morissey, R. and Roe, G., eds. University of Chicago, https://encyclopedie.uchicago.edu/.
                    
                    
                        Blanchard, G. and Olsen, M. (2006). Le Système De Renvoi Dans L’Encyclopédie: Une Cartographie Des Structures De Connaissances Au XVIIIe Siècle. 
                        Recherches Sur Diderot Et Sur L'Encyclopédie,
                        31–32: 45–70.
                    
                    
                        ‘Renvois Navigation’. (n.d.). 
                        ARTFL Encyclopédie Project. Morissey, R. and Roe, G., eds. University of Chicago, https://encyclopedie.uchicago.edu/content/renvois-navigation.
                    
                
            
        
    



    
        
            
                LinkedIn circa 2000 BCE: Towards a Network Model of Pušu-ken’s Commercial Relationships in Old Assyria
                
                    
                        Stratford
                        Edward
                    
                    Brigham Young University, United States of America
                    edward_stratford@byu.edu
                
                
                    
                        Browne
                        Jeremy
                    
                    Brigham Young University, United States of America
                    jeremy_browne@byu.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Short Paper
                
                
                    Assyrian
                    Network Analysis
                    Social Network
                
                
                    archaeology
                    corpora and corpus activities
                    historical studies
                    near eastern studies
                    networks
                    relationships
                    graphs
                    English
                
            
        
    
    
        
            Long before the Silk Road arose from Han Dynasty China, a ‘tin road’ supported an eastern flow of this rare metal from Central Asia to the Aegean basin. Like the Silk Road, the ancient tin road operated as a relay trade where goods were transferred between groups along segments of the route. The basics of the Old Assyrian trade are well understood (Veenhof and Eidem, 2008). From their home city Aššur, on the Tigris in northern Iraq, the Assyrian traders trucked the tin and textiles to the central plateau of modern Turkey, where they sold the goods through dozens of cities across the plateau. But aspects of the commercial organization are still debated (Larsen, 2007; Stratford, 2014). This paper describes a nascent project that applies network analysis to an Old Assyrian corpus to characterize commercial organization. 
            Approximately 23,000 Old Assyrian documents exist (written in cuneiform on clay tablets), most written between 1900 and 1850 BCE. These documents, excavated from the site of Kültepe at intervals since 1884, are stored at more than a dozen European, North American, and Turkish museums. Researchers have transcribed, photographed, and/or catalogued roughly half of them, though fewer have been translated into modern languages. 
            Working from these transcriptions and catalogues, Dr. Edward Stratford selected 538 documents that concerned a merchant named Pušu-ken and his associate, Salim-ahum. Dr. Stratford translated the documents from transcriptions, photographs, and tablets, using the University of Chicago’s Online Cultural and Historical Research Environment (OCHRE) to record the text and its associated metadata. 
            Recent attempts to model Old Assyrian commercial networks (Bamman et al., 2014) have shown promise, but several non-trivial barriers limit the accuracy of these models. First, previous analyses have analyzed co-occurrence on a document-by-document basis. Each document, in fact, describes many ‘occurrences’, such as financial transactions, commercial orders, payment disputes, etc. A more accurate analysis would define associations in terms of interactions between individuals rather than co-occurrence within documents. 
            The second barrier to an accurate analysis of Old Assyrian trade records is the confusion caused by individuals who share informal appellations (see also Bodard et al., 2014). For example, more than 100 individuals in Old Assyrian texts named Aššur-malik can be distinguished by their patronymic designations (e.g., Aššur-malik, son of Al-ahum). However, patronymics were not normally used in Old Assyrian correspondence. Experts familiar with the corpus must rely on context clues—geography, known associates, and specific roles—to deduce the identity in question. 
            Despite its tremendous utility, OCHRE’s ability to link names to individual persons—and thereby distinguish between two individuals with the same name—is still in development. Jeremy Browne worked with Stratford to develop a prototype database and interface to extract names from the corpus and help Stratford quickly map instances of names to specific individuals. 
            
                Building the Database and Preliminary Analysis
            
            This prototype relational database began with a single table to store the translated text segmented into over 2,000 occurrences. Browne wrote routines that extracted from the occurrences almost 1,000 unique non-English words, which Stratford classified as names, places, or technical terms (e.g., 
                minas). The names—which are pertinent to this project—were stored in a new data table, while places and technical terms were set aside for future use. The routines that recorded names also recorded the association between names and occurrences. 
            
            At this point Browne and Stratford could perform a preliminary analysis of the corpus. (Remember, this analysis only considers names, not individuals, within occurrences. The ongoing labor to disambiguate individuals from common names is described below.) Nearly half (287 out of 606, or 47%; see Figure 1) of the names found in the corpus only occur once. While these names represent ‘dead-end’ nodes in the commercial network, they can reveal the importance of the one or more names with which they co-occur. 
            
                
            
            
                
            
            Figure 1. A histogram of name-frequency in the corpus. 
            Visualizing the entire network via Gephi and Elijah Meeks’ GexfD3 library proved problematic because of Pušu-Ken’s and Salim-ahu’s super-degree-centrality. Because the entire corpus was selected based on the documents’ association with those individuals, they are connected to virtually every name in the database by one or two links. Removing Pušu-Ken and Salim-ahu from the dataset revealed a much clearer preliminary picture of the network (see Figure 2). The ring of dots around the periphery represents names and occurrences who only co-occur with Pušu-ken and/or Salim-ahu, but the dots appearing in the intra-space may be very informative. 
            This analysis may provide hints as to which names represent multiple individuals. For example, Figure 3 highlights the subnetwork of Buzuliya, a name whose two occurrences are otherwise geodesically isolated. It is unlikely that one lightly connected person took part in such disparate events, so Buzuliya likely represents two individuals. 
            Figure 4 displays the constellation of occurrences in which the name Lamassi occurs. It is tempting to apply the above criteria here and conclude that this name cannot represent a single individual. However, Lamassi was the name of Pušu-Ken’s wife, who one would expect to occur in a diverse set of interactions. Despite the variety of occurrences in which this name appears, we must be cautious in blindly applying such heuristics. 
            
                
            
            
                
            
            Figure 2. The commercial network sans Pušu-Ken. 
            
                
            
            
                
            
            Figure 3. Indication that the name 
                Buzuliya refers to two people. 
            
            
                
            
            
                
            
            Figure 4. Lamassi’s subnetwork. 
            
                Name Disambiguation Interface
            
            The database was expanded with the addition of a persons table that contained patronymic designations compiled by Stratford. Browne wrote routines that attempted to match these patronymics to names extracted from the corpus. If a given name in a patronymic was not found in any other patronymic, the system would link that name and all of its associated occurrences with that patronymic. Also, if a name only occurred once in the corpus, and no patronymic contained that name, then the name was added as its own patronymic, and its occurrence was linked to that new patronymic. 
            These procedures accounted for almost 300 of the 606 names found in the corpus. As the Old Assyrian expert, Stratford must comb through the remaining names. To assist Stratford, Browne created an AJAX-based web form. The interface presents two select boxes: a box above displaying the names in the corpus, and a box below displaying the known patronymics. There is also a small form at the bottom to add new patronymics to the data table (see Figure 5). 
            When the user clicks on either a name or a patronymic, the text from its associated occurrences appears on the right. For names, the occurrences appear with the name highlighted in blue. The user may click on any of the occurrences displayed for the selected name, and link them to the selected patronymic. 
            
                
            
            
                
            
            Figure 5. The name disambiguation interface. 
            Stratford will disambiguate homonymous individuals, and, by the end of 2014, he and Browne should have more detailed and accurate analyses of this 4,000-year-old commercial network to share. It is possible that, as happened with Jackson’s (2014) analysis of medieval Scottish charters, previously unknown connections and communities will be revealed. 
            
                Moving On
            
            Besides the individual-disambiguated network analysis, Browne and Stratford plan to map specific business transactions and assets to further characterize Old Assyrian commerce and society. Methods that Suen, Luenzel, and Gil (2013) used to analyze an array of media may be adapted to uncover nuances between various types of transactions. The OCHRE metadata for the original clay tablets describe features of handwriting style, so there is an opportunity for ‘transcriptionship’ attribution. Ongoing spectral analyses of the clay tablets may reveal the place of transcription as well. Finally, Stratford and Browne are investigating various metadata standards such as SNAP:DRGN (Bodard et al., 2014) to allow OCHRE and the prototype database to interoperate with other such corpus systems. 
        
        
            
                
                    Bibliography
                    
                        Bamman, D., Anderson, A. and Smith, N. (2013). Inferring Social Rank in an Old Assyrian Trade Network. Paper presented at 
                        DH 2013, Lincoln, NE.
                    
                    
                        Bodard, G., Depauw, M. and Rahtz, S. (2014). SNAP:DRGN—Standard for Networking Ancient Prosopographies: Data and Relations in Greco-Roman Names. Poster presented at 
                        DH 2014, Lausanne.
                    
                    
                        Jackson, C. (2014). Using Social Network Analysis to Reveal Unseen Relationships in Medieval Scotland. Paper presented at 
                        DH 2014, Lausanne.
                    
                    
                        Larsen, M. T. (2007). Individual and Family in Old Assyrian Society. 
                        Journal of Cuneiform Studies,
                        59 (January): 93–106. 
                    
                    
                        Stratford, E. (2014). ‘Make Them Pay’: Charting the Social Topography of an Old Assyrian Caravan Cycle. 
                        Journal of Cuneiform Studies,
                        66 (January): 11–38. 
                    
                    
                        Suen, C., Luenzel, L. and Gil, S. (2013). Extraction and Analysis of Character Interaction Networks From Plays and Movies. Paper presented at 
                        DH 2013, Lincoln, NE.
                    
                    
                        Veenhof, K. R. and Eidem, J. (2008). 
                        Mesopotamia: The Old Assyrian Period. Academic Press, Fribourg. 
                    
                
            
        
    



    
        
            
                Modelling Genre Using Character Networks: The National Tales and Domestic Novels of Maria Edgeworth
                
                    
                        Falk
                        Michael Gregory
                    
                    University of Kent, UK, United Kingdom
                    michaelgfalk@gmail.com
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Poster
                
                
                    Maria Edgeworth
                    network analysis
                    genre
                    novel
                    romanticism
                
                
                    literary studies
                    networks
                    relationships
                    graphs
                    English
                
            
        
    
    
        
            Maria Edgeworth (1768–1849), a once-neglected Anglo-Irish novelist of the Romantic period, is becoming increasingly popular in academic circles. She was an innovative novelist, especially in her use of Irish dialect, witty dialogue, and social analysis.
            Today, scholars tend to divide her novels into two main categories: (1) her ‘national tales’, which take place predominately in Ireland, and explore the Irish national character and the social structure of the Irish countryside; and (2) her ‘domestic novels’, which take place predominately in England, and explore issues of gender, race, and history in the daily lives of English aristocrats (Butler, 1972; Ferris, 2002; Hollingworth, 1997; Kelly, 1989; Voss-Clesly, 1979).
            At first glance, this distinction is compelling, but it also underemphasises similarities between these groups of novels. Her books share common themes, such as an interest in education and historical progress, and a sound approach to her fiction would study both the similarities and differences between her ‘English’ and ‘Irish’ novels.
            Methods
            We can use a new technique—character network analysis—to model the dramatic structure of Edgeworth’s novels, and consider similarities and differences across this common generic distinction. Character network analysis was first introduced to literary scholars by Franco Moretti (2011; 2013), though sociologists had earlier experimented with literary applications of the technique (Stiller et al., 2003).
            Here I apply this technique to three of Edgeworth’s ‘national tales’ and three of her ‘domestic novels’. The raw data was collected by hand, in the form of adjacency matrices for each chapter. Each character constituted a ‘node’ in the network. If they addressed another character in the novel, then an ‘edge’ was drawn between them. Each edge was given a ‘weight’ equal to the number of chapters in which the two characters speak to one another. This data was processed in R using the iGraph package (R Team, 2013; Csardi, 2013). Network diagrams were produced using Gephi (Bastian et al., 2009).
            Results
            Statistical analysis reveals important similarities between the two genres in this small corpus. First, three different measures of ‘centrality’ were calculated: 
                degree centrality (which measures how many edges are connected to each node), 
                betweenness centrality (which how many nodes are connected to one another through a given node) and 
                strength (which is identical to betweenness centrality, but takes account of the ‘edge weights’) (Wasserman and Faust, 1994). Every novel had one character who topped every centrality measure, except for 
                Helen (Table 1). All six novels are 
                Figurenromane, with a single central protagonist. Second, a community detection algorithm was run, which tended to sort the characters in all the novels into different households (Figures 1–6, displayed on poster) (Blondel et al., 2008). All the novels, whether ‘domestic’ or ‘Irish’, depict the world as a network of noble households.
            
            
                
            
            But character network analysis also reveals significant differences between the ‘national tales’ and ‘domestic novels’. Four measurements were taken of the overall structure of the six novel networks: the 
                density (the proportion of possible edges that are present in the network), the 
                transitivity (the probability that A and B will be connected if they are both already linked to C), the 
                betweenness centralisation (which measures how much greater the ‘betweenness’ of the most central node is compared to all other nodes), and the 
                degree centralisation (which performs the same calculation, but on the basis of degree centrality) (Wasserman and Faust, 1994). On these four measurements, significant differences between the two categories of novel emerged (Table 2).
            
            
                
            
            The novels that were set in England or had female protagonists had higher average 
                density and 
                transitivity, suggesting that the ‘domestic novels’ depict smaller, more tightly knit communities, with more interactions between the different minor characters. The novels that were set in Ireland or had male protagonists had higher 
                betweenness and 
                degree centralisation, suggesting that in these novels, which feature more travel, the protagonist is more important for connecting the different regions in the network. 
                Harrington is an interesting case, being the only novel set in England with a male protagonist. An individual value plot suggests there is a complex interaction of setting and gender in this small corpus (Figure 7, displayed on poster).
            
            Conclusion
            Character network analysis is a promising technique, because it produces intuitive results that are easy to reconcile with traditional methods. When we visualise the novels as network diagrams, we can immediately grasp an important similarity between Edgeworth’s ‘national tales’ and ‘domestic novels’: the presence of a dominating protagonist and the network of noble households. When we quantify the novels using statistical analysis, we can start to unpick different factors that are difficult to perceive on a close reading: particularly the hidden structural differences between male and female protagonists, which would bear further scrutiny.
        
        
            
                
                    Bibliography
                    
                        Bastian, M., Heymann, S. and Jacomy, M. (2009). Gephi: An Open Source Software for Exploring and Manipulating Networks. Paper presented at the 
                        International AAAI Conference on Weblogs and Social Media, San Jose, CA, 17–20 May 2009.
                    
                    
                        Blondel, V. D., Guillaume, J.-L., Lambiotte, R. and Lefebvre, E. (2008). Fast Unfolding of Communities in Large Networks. 
                        Journal of Statistical Mechanics: Theory and Experiment, 
                        10.
                    
                    
                        Butler, M. (1972). 
                        Maria Edgeworth: A Literary Biography. Clarendon Press, Oxford.
                    
                    
                        Csardi, G. (2013). iGraph Package: Network Analysis and Visualization (Version 0.6.6). Vienna: CRAN, http://igraph.sourceforge.net.
                    
                    
                        Ferris, I. (2002). 
                        The Romantic National Tale and the Question of Ireland. Cambridge University Press, Cambridge.
                    
                    
                        Hollingworth, B. (1997). 
                        Maria Edgeworth’s Irish Writing: Language, History, Politics. Macmillan, Basingstoke.
                    
                    
                        Kelly, G. (1989). 
                        English Fiction of the Romantic Period, 1789–1830. Longman, London.
                    
                    
                        Moretti, F. (2011). Network Theory, Plot Analysis. 
                        New Left Review, 
                        68: 80–102. 
                    
                    
                        Moretti, F. (2013). ‘Operationalizing’: or, The Function of Measurement in Modern Literary Theory. 
                        New Left Review, 
                        84: 103–19.
                    
                    
                        R Team. (2013). R: A Language and Environment for Statistical Computing. Vienna: R Foundation for Statistical Computing, http://www.R-project.org/
                    
                    
                        Stiller, J., Nettle, D. and Dunbar, R. (2003). The Small World of Shakespeare’s Plays. 
                        Human Nature, 
                        14(4): 397–408.
                    
                    
                        Voss-Clesly, P. (1979). 
                        Tendencies of Character Depiction in the Domestic Novels of Burney, Edgeworth, and Austen: A Consideration of Subjective and Objective World. Institut für Anglistik &amp; Amerikanistik, Universität Salzburg, Salzburg.
                    
                    
                        Wasserman, S. and Faust, K. (1994). 
                        Social Network Analysis: Methods and Applications. Cambridge University Press, Cambridge.
                    
                
            
        
    



    
        
            
                Senga: Participatory Curation and the Mathematical Model of Exhibition
                
                    
                        Kitamoto
                        Asanobu
                    
                    National Institute of Informatics, Japan
                    kitamoto@nii.ac.jp
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    museum
                    participatory curation
                    mathematical model
                    exhibition
                    analysis
                
                
                    interface and user experience design
                    internet / world wide web
                    GLAM: galleries
                    libraries
                    archives
                    museums
                    crowdsourcing
                    English
                
            
        
    
    
        
            Senga is a Web-based participatory curation system designed for the general public. The Senga interface consists of four steps—search, collect, order, and share—to make an image sequence called a tour. We claim that these steps mimic the basic process of curation, so a tour can be considered as an exhibition. Senga is a type of crowdsourcing system similar to crowd annotation, but the uniqueness of Senga is in context-dependence, meaning that an interpretation of an image depends on the neighborhood of an image sequence. Through the analysis of more than 2,500 tours created on Senga since its release in 2007, we showed that it has potential to be used by curators to draw new ideas for the exhibition. We conclude the paper by proposing the mathematical model of exhibitions with a preliminary result on analyzing state transition diagrams and on synthesizing tours. 
            
                What Is Senga? 
            
            ‘Senga’ (Japanese for ‘thousand images’)
                1 is a web-based participatory curation system that allows users to make a ‘tour’ of images and publish it on the Web as an exhibition. Senga is based on the idea of fragmentation and recombination of digitized books. We have digitized more than 200 academically relevant books about the Silk Road (Digital Archive of Toyo Bunko Rare Book
                2), but the academic atmosphere around those books was a barrier to the general public accessing the wealth of cultural heritage information inside the book. Our original motivation was to crop attractive parts of the book to make a collection of image fragments so that users could later collect and recombine them into a new collection for their purposes. Providing image fragments, however, is a common idea exemplified in the British Library’s million photographs at Flickr commons or the Mechanical Curator (Baker, 2013). Our contribution is in designing an interface to make a ‘tour’ of image collection. 
            
            Figure 1 shows four steps to make a tour on Senga—namely, search, collect, order, and share. A user first sets a theme, searches images by various criteria, collects appropriate images, orders them in a meaningful way, and shares them as a tour. We claim that these steps mimic an exhibition’s curation process, and a tour is a basic form of an exhibition. Hence we call Senga a participatory curation system for sharing exhibitions online. 
            
                
            
            Figure 1. Overview of Senga and four steps (search, collect, order, and share) to make a tour. 
            Context-Dependent Annotation 
            Senga can be regarded as a crowdsourcing system similar to crowd annotation. In crowd annotation, a user is asked annotate images with tags (Oomen et al., 2014) and named entities,
                3 and we call it context-independent annotation because metadata description is not affected by how we view the image. On the contrary, Senga deals with context-dependent annotation such as ambiguous or creative interpretations. For instance, a red rectangle is red if it is contrasted with other colors, but is a rectangle if contrasted with other shapes. An ambiguous situation also offers a chance for the exploitation of creative interpretations and contextualization. Senga asks a user to think about the meaning of an image in contrast with other images, which raises users’ interest to look into images. 
            
            Motivation to Participate 
            Senga does not offer a clear goal, however, and users of Senga are expected to find their own goals. After the release of Senga in August 2007 (Kamida and Kitamoto, 2007), we held several outreach events for the general public to use Senga and observed how they understand the concept of the system. The biggest challenge seems to be setting a goal from a pool of images on the interface, or in other words, setting the theme of a tour. Some people quickly decided on a theme and looked intrinsically motivated for improving the quality of the tour. An interesting observation is that the capability of goal-setting does not depend on age; even small kids can quickly find their themes, while some adults got lost and could not concentrate on making the tour. Since October 2011, Senga was permanently installed at Toyo Bunko Museum in Tokyo so that museum visitors can freely use it. To increase the value in the museum, we added the mechanism of extrinsic motivation, namely a free souvenir. A tour made in the exhibition room can be picked up at the museum shop after communicating with the staff to print their tours on postcards. Here a postcard serves as a tool to make a path to the museum shop and keep memories of the museum until the next visit. 
            
                Analysis of User-Generated Tours 
            
            Basic Statistics 
            As of 14 October 2014, Senga produced 2,579 tours. The system provides 3,772 image fragments, and 3,335 images (88%) appear at least in one tour. To analyze the collection of tours from the viewpoint of an exhibition, we first define an ‘exhibition’ as ‘an activity for the general public to arrange artifacts or events with an intention’ (Kawaguchi, 2009 [author’s translation]). This definition suggests that the most important factor of exhibition is ‘intention’, so we focus on two factors—arrangement and title—that are related to the intention of a tour. We check the arrangement and the title for intention and judge if both are consistent. Finally we mark a tour ‘creative’ as long as the arrangement and the title suggest interesting or unique ideas. Evaluation of 2,579 tours was performed by one woman who is not a domain specialist. Such subjective evaluation should be performed by multiple people, but evaluation by a single person is at least consistent and can be used as a good starting point for a preliminary evaluation. As a result, 34% of the tours were judged intentional and consistent, and 6% of the tours (most of them intentional and consistent) were judged creative. In contrast, 41% of the tours were judged intentional but not consistent. 
            Top-Down and Bottom-Up Approaches 
            The first research question is why we have more inconsistent tours than consistent ones. Our hypothesis is that this difference originates in two different approaches to making tours: top-down and bottom-up approaches. In a top-down approach, the theme of a tour is defined in the beginning, so the title could be easily given. On the other hand, in a bottom-up approach, a user starts from collecting images and later tries to give a title that best describes the collection. This is not an easy task for image collections without explicit themes, so a typical solution is to give a title not related to the content, such as the date of the visit. The result suggests a hypothesis that the bottom-up approach is slightly more popular than the top-down approach, which is probably opposite the habit of professional curators. 
            Variety of Contexts 
            The second research question is to measure the variety of contexts at an image level. Figure 2 shows the analysis of image sequences in tours to count images that come just before or just after the target image. Here the leftmost image is the target image, and other images to the right are ordered according to the number of appearances in the neighborhood. In (a), the target image was used in the context of a colorful pattern, but in (b), the target image was used in the context of either animal, pairing, or circular. Multiple interpretations emerged through the comparison of multiple images, but this level of variety is limited to a local context. 
            
            Figure 2. Variety of contexts for a single image.
            Creativity of Users 
            The third research question is to characterize users’ creativity at a tour level. Figure 3 shows an example of creative tours. The first tour has the title ‘couple’, suggesting that the intention is to arrange face images in alternating directions. This interpretation, left or right, is the most important meaning in this tour, but it may have no value in single-image annotation. The second tour has the title ‘fashion show’, suggesting that the intention is to arrange well-dressed people. These creative user-generated tours offer new interpretations for a global context and show potential for the professional curator to draw new ideas for their exhibitions. 
            
                
            
            Figure 3. Creative tours: (a) couple, (b) fashion show. 
            Mathematical Model of Exhibition 
            The final challenge is to represent the collection of tours using a mathematical model to enable deeper analysis. If we assume that an exhibition has meaningful context over a local image sequence in the neighborhood, the exhibition can be mathematically represented as a Markov chain. Here an image is a node, and transition from one image to another is a directed edge between nodes. The union of nodes and edges from all tours constructs a graph structure of all tours. This graph represents collective contexts that Senga has produced, and the analysis of this graph structure leads to the collective analysis and synthesis of tours. Figure 3 shows the state transition diagram of a Markov chain for (a) intentional and consistent tours and (b) others, visualized by Gephi 0.8.2beta (Bastian et al., 2009). Specific shapes depend on the layout algorithm, but it looks as if (a) has more structure than (b), and this impression could be validated by quantitative analysis of the graph. 
            
                
            
            
                
            
            Figure 4. State transition diagrams from (a) creative and consistent tours, (b) other tours. 
            The graph can also be used for synthesis. An example is a random walk on the graph that generates a synthetic tour. Frequency of state transitions is first converted into probability, and a self-avoiding random walk
                4 generates an image sequence as a tour. Figure 5 shows an automatically generated tour. It starts from a pattern image and gradually shifts toward Buddha and finally people, and it gives locally consistent state transitions. 
            
            
                
            
            Figure 5. Automatically generated tour using self-avoiding random walk. 
            Conclusion 
            The paper introduced a participatory curation system, Senga, and proposed a mathematical model of exhibition for analysis and synthesis. Future research challenges include detailed analysis of the mathematical model to characterize creativity. 
            Notes
            1. Senga Silk Road, http://dsr.nii.ac.jp/senga/.
            2. Digital Archive of Toyo Bunko Rare Books, http://dsr.nii.ac.jp/toyobunko/.
            3. Your Paintings Tagger, http://tagger.thepcf.org.uk/.
            4. Eric W. Weisstein, ‘Self-Avoiding Walk,’ from MathWorld—A Wolfram Web Resource, http://mathworld.wolfram.com/Self-AvoidingWalk.html.
        
        
            
                
                    Bibliography
                    
                        Baker, J. (2013). The Mechanical Curator. Digital Scholarship, http://britishlibrary.typepad.co.uk/digital-scholarship/2013/09/the-mechanical-curator.html. 
                    
                    
                        Bastian, M., Heymann, S. and Jacomy, M. (2009). Gephi: An Open-Source Software for Exploring and Manipulating Networks. 
                        International AAAI Conference on Weblogs and Social Media.
                    
                    
                        Kamida, R. and Kitamoto, A. (2007). Senga: A Participatory Archive Based on the Collection and Rearrangement of Images and the Sharing of Slideshows. 
                        IPSJ SIG Computers and the Humanities Symposium 2007, pp. 339–46 (in Japanese).
                    
                    
                        Kawaguchi, Y. (ed.). (2009). 
                        Politics of Exhibition. Suiseisha (in Japanese). 
                    
                    
                        Oomen, J., Gligorov, R. and Hildebrand, M. (2014). Waisda? Making Videos Findable through Crowdsourced Annotations. In Ridge, M. (ed.), 
                        Crowdsourcing Our Cultural Heritage. Ashgate.
                    
                
            
        
    



    
        
            
                Social Media Data: Twitter Scraping on NeCTAR
                
                    
                        Hutchinson
                        Jonathon
                    
                    The University of Sydney
                    jonathon.hutchinson@sydney.edu.au
                
                
                    
                        Hammond
                        Jeremy
                    
                    Intersect, Australia
                    jeremy@intersect.org.au
                
                
                    
                        Martin
                        Fiona
                    
                    The University of Sydney
                    fiona.martin@sydney.edu.au
                
                
                    
                        Yazbek
                        Daniel
                    
                    Intersect, Australia
                    daniel@intersect.org.au
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    big data
                    Social Media Network Analysis
                    cloud compute
                    twitter
                    online communities
                
                
                    corpora and corpus activities
                    internet / world wide web
                    digital humanities - facilities
                    interdisciplinary collaboration
                    visualisation
                    social media
                    networks
                    relationships
                    graphs
                    media studies
                    data mining / text mining
                    English
                
            
        
    
    
        
            The rapid growth in social media communications research in the last five years has seen the assembly of complexly connected datasets of a scale and scope previously rare in humanities scholarship, highlighting the sociocultural intricacies of follower networks (see Weller et al., 2013). Big data inquiry of social media has also been driven by the emergence of publicly accessible, integrated aggregation, indexing, query, and visualisation approaches (Hansen et al., 2011; Burnap et al., 2013). Indeed, the big data moment has challenged humanities researchers to develop innovative methodologies to extrapolate new social knowledge, and brought humanists and computer scientists together in the pursuit of robust eResearch infrastructure and workflows to support these objectives.
            This paper explores a novel social media network analysis (SMNA) research methodology, which resulted in the development of a Twitter visualisation tool for the NeCTAR research cloud. It explores the workflow issues of using large-scale eResearch infrastructure for digital humanities research, and discusses the results of the research program on social media network mapping. In doing so, the authors demonstrate the complexities of interdisciplinary humanities and computer science research collaboration, while revealing new insights made possible through eResearch partnerships.
            SMNA as a methodology requires the development of an integrated workflow using National eResearch Collaboration, Tools and Resources (NeCTAR) as the computing service to host both a Twitter scraper and a network visualisation tool. We used the native Twitter application programming interface (API) to maximise flexibility, minimise costs, and reduce reliance on commercial third-party data providers. The workflow included scripting an automated ‘clean-up’ phase so that hundreds of thousands of raw tweets could be gathered and indexed in a useful format. Analysing the characteristics of social networks across large datasets is computationally taxing, and to solve this problem we set up an interactive visualisation program, Gephi, in the NeCTAR cloud environment. 
            During the discovery and implementation phases of the project we made the following key observations. In Australia, there has been significant investment in eResearch infrastructure, including large-scale storage, data discovery, and high-performance and cloud computer systems. However, these services often have imposing barriers to entry for humanities researchers, who either lack the technological skills, desire, or established collaborative networks to apply these methodologies to their field of research (Meyer and Dutton, 2009). Moreover, eResearch assemblages are generally constructed by computer or physical scientists, who may be unfamiliar with the philosophies and theoretical perspectives of social scientists, and so it is not always clear that these technologies can immediately address the humanities’ ‘big data’ problems (Meade et al., 2013). However, in constructing the NeCTAR-based Twitter research infrastructure and pursuing an extended collaborative approach, the yield from the original research corpus provided more unique and novel results than either discipline could deliver alone. Traditionally, computer science researchers have sought interdisciplinary collaborations in the natural sciences and engineering to find appropriate research problems that balanced scale, complexity, and solvability. The materialisation of big data as a research concern within the humanities now gives software engineers new real-world applications of their techniques, a process that is key to the computer science discipline’s evolution (Hopcroft et al., 2011).
            The research presented in this paper demonstrates that early methodological discussions between humanists and computational specialists strengthened the research design, producing collaboratively designed research questions, while avoiding the high knowledge barriers to lay eResearch entry (Goggin et al., 2014). For example, in this research context, which drew on the expertise of University of Sydney media researchers and NSW Intersect’s computer scientists, a series of new research questions emerged while interrogating the initial social mapping results. The development of a novel computational research workflow emerged from the humanities scholars’ interest in understanding the languages, norms, or rules of communication activities within the Twitter social media platform. Following that research focus, the computer scientists explored a deeper understanding of the relationships between individual users, and the positive and negative communicative sentiment expressed in users’ interactions. The collaboration required significant mutual investment in exploring each research contributor’s agencies and abilities, and ongoing calibration of the research design.
            The resulting workflow and research tool have enabled a rigorous interrogation of the communication activities of complex follower networks within the Twitter platform, and the evolution of a methodology that addresses ethical concerns about big data research raised by boyd and Crawford (2012) along with Ess (2014). It will also underpin the development of an automated research tool that can be rolled out across a number of humanities research projects, and within virtual lab environments.
        
        
            
                
                    Bibliography
                    
                        boyd, d. and Crawford, K. (2012). Critical Questions for Big Data: Provocations for a Cultural, Technological, and Scholarly Phenomenon. 
                        Information, Communication, &amp; Society, 
                        15(5): 662–79.
                    
                    
                        Bradley, J. (2009). What the Developer Saw: An Outsider’s Ciew of Annotation, Interpretation and Scholarship. 
                        New Paths For Computing Humanists,
                        1(1).
                    
                    
                        Burnap, P., Rana, O. and Avis, N.
                         (2013). Making Sense of Self-Reported Socially Significant Data Using Computational Methods.
                        International Journal of Social Research Methodology, Computational Social Science: Research Strategies, Design and Methods,
                        16
                        (2).
                    
                    
                        Ess, C. (2014). At the Intersections between Internet Studies and Philosophy: ‘Who Am I Online?’ 
                        Philosophy &amp; Technology, 
                        25(3): 275–84.
                    
                    
                        Goggin, G., Dwyer, T., Martin, F. and Hutchinson, J. (2014). Finding Mobile Internet Policy Actors in Big Data: Methodological Concerns in Social Network Analysis. Paper presented at 
                        Australasian Association of the Digital Humanities, Expanding Horizons, Perth, Australia, 18–21 March 2014.
                    
                    
                        Hansen, D. L., Shneiderman, B. and Smith, M. A. (2011). 
                        Analyzing Social Media Networks with NodeXL: Insights from a Connected World. Morgan Kaufmann.
                    
                    
                        Hopcroft, J. E., Soundarajan, S. and Wang, L. (2011). The Future of Computer Science.
                         International Journal of Software Informatics, 
                        5(4): 549–65, http://www.ijsi.org/1673-7288/5/i110.htm.
                    
                    
                        Meade, B., Manos, S., Sinnott, R., Fluke, C., van der Knijff, D. and Tseng, A. (2013). Research Cloud Data Communities. 
                        THETA: The Higher Education Technology Agenda 2013, Hobart, Tasmania, 7–10 April 2013, http://eprints.utas.edu.au/16326/1/THETA_2013_Meade_16326.pdf.
                    
                    
                        Meyer, E. T. and Dutton, W. H. (2009). Top‐Down e‐Infrastructure Meets Bottom‐Up Research Innovation: The Social Shaping of e‐Research. 
                        Prometheus,
                        27(3).
                    
                    
                        Weller, K., Bruns, A., Burgess, J., Mahrt, M. and Puschmann, C. (2013). 
                        Twitter and Society. Peter Lang Publishing, New York. 
                    
                
            
        
    



    
        
            
                Generating Navigable Semantic Maps from Social Sciences Corpora
                
                    
                        Poibeau
                        Thierry
                    
                    LATTICE-CNRS, France
                    thierry.poibeau@ens.fr
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    Information extraction; graphs; vizualisation
                
                
                    information retrieval
                    natural language processing
                    semantic analysis
                    text analysis
                    knowledge representation
                    content analysis
                    visualisation
                    networks
                    relationships
                    graphs
                    spatio-temporal modeling
                    analysis and visualisation
                    linking and annotation
                    English
                
            
        
    
    
        
            
                A Practical Example: Mapping the 2007-2008 Financial Crisis
                The 2007-2008 financial crisis was a dramatically complex event and the political responses to this event were at least as complex. These responses can be studied thanks to a huge amount of documents produced by various bodies during the crisis and made available since then. 
                An American initiative aims at studying the response of the American authorities to the crisis through PoliInformatics, defined as “an interdisciplinary field that promotes diverse methodological approaches to the study of politics and government” 
                    (
                    http://poliinformatics.org/
                    ). We participated in the first PoliInformatics challenge, as we describe in following. 
                
                The organizers of the challenge made available a series of documents on the 2007-2008 financial crisis. The shared task consisted in developing solutions to address questions such as “Who was the financial crisis?” or “What was the financial crisis?”. Of course, these questions are too complex to receive a simple and direct answer. So our strategy has been to provide tools to process and visualize the most relevant data, so that experts can easily navigate into this flow of information and make sense of the data. While we believe in semi-automatic corpus exploration, we do not think it is possible or even desirable to provide fully automatic answers to the above questions.
                We have as far as possible used available tools to extract and visualize information. More precisely, we have used the Stanford Named Entity Recognizer (Finkel et al., 2005) and the Cortext platform (
                    http://www.cortext.net/) for information extraction. As for data visualization, we have used Gephi (Bastian et al., 2009) to observe semantic and social networks, and the Cortext platform to observe the evolution of the domain over time. However, these tools are not enough to obtain meaningful representations: for example, new developments are necessary for named entity normalization and linking, esp. to link text with ontologies (Ruiz and Poibeau, 2015). The result should then be filtered following precise, domain-dependent criteria, so as to obtain navigable and readable maps with the most salient information. 
                
            
            
                Technical Overview 
                
                    Named Entity Recognition and Normalization
                    The first step was to extract named entities from the different corpora. Named Entity Recognition is a mature technology that has been used in several Digital Humanities projects (see Van Hooland et al., 2013 for a discussion of some recent projects). However, the use of NER in the analysis of political science texts seems to have been limited, e.g. Grimmer and Stewart’s (2013) survey of text analytics for political science includes no discussion of this technology. 
                    In order to perform entity extraction, we used the Stanford NER, based on Conditional Random Fields, with MUC tags (Time, Location, Organization, Person, Money, Percent, Date) (Finkel et al., 2005). Certain entities appear under different forms. For instance, “Standard and Poor” might occur as “Standard &amp; Poor”, “S&amp;P” or “Standard &amp; Poor’s executive board” (this last sequence in fact refers to a slightly different named entity); in a similar fashion, a person as “Mary Schapiro” may appear as “Schapiro”, or “Miss Schapiro” or “Chairman Schapiro”. We implemented a simple normalization method based on the maximization of common sub-sequence between two strings and obtained qualitatively good results when compared to other more sophisticated algorithms (Gottipati and Jiang, 2011; Rao et al., 2011). Entity linking could then be applied on the result. 
                
                
                    Visualizing entities
                    We used the Gephi software (Bastian et al., 2009) so as to create graphs for each corpus, such that:
                    
                        a node corresponds to a cluster of persons or organizations in the corresponding corpus;
                        an edge between two nodes corresponds to the number of co-occurrences of the two nodes within the same sentence in the corpus.
                    
                    We chose to consider persons and organizations together since they can play a similar role in the event, and metonymy is often used, so that a person can refer to a company (and vice versa). 
                    
                        
                    
                    
                        Figure 1: Visualization of links between entities with Gephi
                    
                    Some results can be seen on figure 1 (which is hardly readable in small format but can be interactively explored by the experts in the field on a computer). Some links correspond to well establish relations like the link between an organization and its CEO (see for ex. the link between Scott Polakoff and OTS, or between Fabrice Tourre and Goldman Sachs). However, we are also able to extract less predictable links that could be of interest for scholars and experts in the field. As an example, we observe a link between the Fed Consumer Advisory Council and the Board of Governors (for ex. Bernanke, Mark Olson, and Kevin Warsh) since the first group of people (the council) warns vigorously the Board of Governors about the crisis. An interesting methodological issue to consider when elaborating these networks is that different automatic linguistic analyses can affect the complexity of the network: For instance, depending on the strategy to calculate intra-document coreference (e.g. whether we consider pronouns referring to an entity as an instance of the entity or not), the amount of edges in the graph will vary. Rieder and Röhle (2012) have discussed interpretation problems for visualizations, and Rieder (2010) has commented on how different graph layout algorithms can lead to representations that promote opposite interpretations of a network for the same corpus. We are as well interested in exploring how the computational linguistics tools employed in order to assess co-occurrences, even before applying a graph layout, can influence the graphs ultimately produced. 
                
                
                    Visualizing temporal evolution
                    The visualizations we produced should be explored and tested by specialists who could evaluate their real benefits. A historic view on the data would also be useful to analyze the dynamics and the evolution of the crisis, through for example the evolution of terms associated with named entities over different periods of time.
                    We tried to explore and visualize the temporal evolution of the financial crisis, more specifically the evolution of the perceived role of organizations over time. To do so, we produced Sankey diagrams of the correlation of organizations and domain related terms in the corpus. With this strategy, Sankey diagrams take into account the temporal evolutions of entities and actions along the crisis.
                    
                        
                    
                    
                        Figure 2: Evolution of the links between named entities and topics over time
                    
                    Figure 2 reveals a modification in the data between 2006 and 2008, a period which approximates the start of the financial crisis. For instance, the stream in purple in this graph reveals many co-occurrences of Fannie Mae and subprime loans for the period 1990-2007 while for the period 2008-2010, Fannie Mae is more closely associated with ’bank regulators’, or ’Federal Financial Services Supervisory Authority’. In a more general way, all the streams of data represented in the diagram are dramatically modified after 2007. 
                
            
            
                Evaluation and Future Work
                The work presented here has been evaluated by a panel of experts in the field. They assessed the utility of the tools and helped us define ways to improve these first results. Perspectives are thus twofold: on the one hand enhance data analysis so as to provide more relevant maps and representations, and on the second hand work closely with domain experts and provide interactive ways of navigating the data. Concerning interactions with experts, it is clear that end users could provide a very valuable contribution in the selection of relevant data as well as in the way they are linked and mapped. Some experiments are currently being done with a focus group gathering social science as well as information science experts. They will assess that the solution is useful and workable and more importantly, will give feedback so as to provide better solutions.
            
            
                Acknowledgements
                This work has received support of Paris Sciences et Lettres (program “Investissements d’avenir” ANR-10-IDEX-0001-02 PSL*) and of the laboratoire d’excellence TransferS (ANR-10-LABX‑0099). Pablo Ruiz is funded thanks to a grant from the Region Ile-de-France.
            
        
        
            
                
                    Bibliography
                    Bastian, M., Heymann, S. and Jacomy M. (2009). Gephi: an open source software for exploring and manipulating networks. In International AAAI Conference on Weblogs and Social Media (ICWSM), San Jose, pp. 361–362.
                    Finkel, J.R., Grenager, T. and Manning, C. (2005). Incorporating non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the conference of the Association for Computational Linguistics, Ann Arbor, pp. 363–370.
                    Gottipati, S. and Jiang, J. (2011). Linking entities to a knowledge base with query expansion. In Proc. of Empirical Methods in Natural Language Processing (EMNLP), Edinburgh.
                    Grimmer J. and Stewart, B.M. (2013). Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts. Political Analysis, 21(3), 267–297. doi:10.1093/pan/mps028. 
                    Latour, B. (2005). Reassembling the Social: An Introduction to Actor-Network-Theory. Oxford: Oxford University Press.
                    Rao, D., McNamee, P. and Dredze, M. (2011). Entity linking: Finding extracted entities in a knowledge base. In Multi-source, Multi-lingual Information Extraction and Summarization, pp. 93–115. Berlin: Springer. 
                    Rieder, B. (2010). One network and four algorithms. http://thepoliticsofsystems.net/2010/10/one-network-and-four-algorithms/, date accessed 25 February 2014. 
                    Rieder, B. and Röhle, T. (2012). "Digital methods: Five challenges." Understanding Digital Humanities: 67-84. London: Palgrave Macmillan.
                    Ruiz, P. and Poibeau, T. (2015). Entity Linking Combining Open Source Annotators via Weighted Voting. Denver: Proceedings of Semeval 2015.
                    Van Hooland, S., De Wilde, M., Verborgh, R., Steiner, T. and Van de Walle, R. (2013). Exploring entity recognition and disambiguation for cultural heritage collections. In Digital Scholarship in the Humanities, Oxford: Oxford University Press. 
                
            
        
    



    
        
            
                Pedagogical Hermeneutics and Teaching DH in a Liberal Arts Context
                
                    
                        Jakacki
                        Diane Katherine
                    
                    Bucknell University, United States of America
                    diane.jakacki@bucknell.edu
                
                
                    
                        Faull
                        Katherine Mary
                    
                    Bucknell University, United States of America
                    faull@bucknell.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    digital humanities pedagogy
                    hermeneutics
                    undergraduate liberal arts
                    peer learning
                    archival studies
                
                
                    archives
                    repositories
                    sustainability and preservation
                    teaching and pedagogy
                    project design
                    organization
                    management
                    text analysis
                    content analysis
                    digital humanities - pedagogy and curriculum
                    maps and mapping
                    networks
                    relationships
                    graphs
                    English
                
            
        
    
    
        
            In the summer 2014 issue of 
                CEA Critic authors Lindsay Thomas and Dana Solomon remarked on the notable lack of discussions of pedagogy in the development of DH in undergraduate institutions (2014). Arguing that DH pedagogy should be something far more than an afterthought, Thomas and Solomon outlined how their undergraduate project RoSE at the University of California, Santa Barbara developed students to be active users and researchers of DH. In the same issue of 
                CEA Critic, E. Leigh Bonds drew on the discussions of Melissa Terras, Stephen Ramsey, Alan Liu, et al. about the fundamental difference between the learning goals of DH courses and those of traditional courses in the humanities (2014). In the case of working with archival materials, how do we teach students to be makers and doers together? Or, in Liu’s terms, how do we develop a pedagogical hermeneutic of ‘practice, discovery, community’ (2009)?
            
            This paper will focus on the teaching experience of Drs. Katherine Faull and Jakacki in the fall of 2014 to show how the planning, design, and execution of a new project-based course—HUMN 100: Digging into the Digital and the Humanities Now!—introduced students to the world of digital humanities through the use of selected digital tools and methods of analysis. This course, taught within the Comparative Humanities program, was designed specifically for first- and second-year students in order to encourage the development of digital habits of mind at the earliest phases of their liberal arts curricular experience (Clement, 2012). Developed to encourage examination and experimentation with a range of digital humanities approaches, students worked with primary archival materials to encourage digital modes of inquiry and analysis. The decision to root the course in a multifaceted analysis of archival materials provided the rare chance for students to engage in the research process typical for a humanities scholar: namely, the discovery of artifacts, the formulation of research questions, followed by the analysis and synthesis of findings culminating in the publication of initial findings in a digital medium. In the process, we introduced students to the basic structure of how to develop a DH research project. 
            For Bucknell University, the focus in digital humanities scholarship and learning has been primarily on spatial thinking (Pacchioli, 2013). It was important to both instructors to emphasize that objective in the development of the course and its learning outcomes, and so they focused upon the importance of finding materials that would be of interest to students ‘in place’. We decided to run the course in two sections, anticipating an opportunity to reflect different perspectives of the instructors’ expertise with DH methods and tools. One section focused on the Colonial mission diaries of the Moravians from Shamokin, Pennsylvania. Written in English, the diary sections selected dealt with interactions between some of the first Europeans to the area and the Native peoples they met and worked among. The other section considered a subset of the diaries of James Merrill Linn, one of the first graduates of the university and a soldier in the American Civil War. Both choices reflect and extend Bucknell’s interest in digital/spatial thinking in terms of its place in the larger historical and cultural narrative. 
            In this paper we demonstrate how scaffolding the course—establishing a parallel structure that spanned both sections—accommodated both core texts while reinforcing the importance of considering how different DH-based methods would strengthen students’ understanding of that subject matter. This approach allowed both instructors to develop more sophisticated and complex course modules while assisting one another through the strengths and skills of each. We will discuss how this challenged us to consider whether we were co-teaching two sections of one course or two courses in collaboration, and where we were successful in identifying moments that offered a richer learning environment for both sections, supporting each other in our separate sections when our own DH expertise and pedagogical approaches were needed by the other. In essence, we had to learn how to teach one another while we were teaching the subject matter to our students. 
            This course was developed to encourage examination and experimentation with a range of digital humanities approaches, and how digital humanists apply computational methods that involve textual analysis and data visualization. The sequencing of the modules was carefully designed so that the ‘product’ of each module then became the ‘data’ of the next module. Throughout the course, students developed a database of names, people, places, and connections that grew organically out of the focus of each specific section. This data, crucial to the success of the students’ assignments, needed some restructuring as we moved onto the next module; for example, students’ close reading in the TEI module led to the development of a prosopography that led to data for entry into Gephi that was then built out in adding geospatial data for GIS. Extensive use was made of online platforms that emphasize important forms of digital engagement, including collaborative online writing environments. Each module ended with a short assignment and also a reflective public-facing blog post that became an unmarked form of intellectual interaction. 
            Student enrollment in the two sections of the course was purposely limited to first- and second-year undergraduates with no background in digital humanities. Accordingly, the course was designed with the goal of not only exposing them to tools of distant and close reading, and network and spatial visualization, but also requiring that they learn to think critically about what each of these methods reveals in the manuscript texts they themselves transcribed; finally, the students were required to produce digital artifacts. 
            This paper will argue that the placement of this course within the interdisciplinary context of the program in Comparative Humanities underscored the program learning goals both of comparativity and interdisciplinarity and the course-specific goals of a new pedagogical hermeneutic. Teaching students to compare meaningfully intellectual materials of different or opposing types, and to theorize the difference between textual and material artifacts, narrative and non-narrative texts, and visual and analytical modes of thought was central to the course. To promote this, each module required students to read key secondary texts that were then integrated into their own reflections on the module. What does Johanna Drucker say about the visual rhetoric of visualization (2014)? How does Elena Pierazzo argue for the epistemic difference of diplomatic editions (2011)? What do Daniel Rosenberg and Anthony Grafton say about the development of timelines and the conceptualization of history (2000)? The interdisciplinary humanistic approach was thus clearly and directly linked to the learning goals of the course. Students also learned to identify, use, and discuss the advantages and disadvantages of different DH methodologies and tools and were encouraged to identify and use key DH terms and concepts. As a result, students learned to develop research questions that could be answered with DH tools and methodologies, and work collaboratively in groups to create projects that related to their own research interests. 
            The development and implementation of HUMN 100—a course without precedent at Bucknell and few guiding models at other undergraduate institutions—was far from easy. While both instructors had co-taught courses before, neither had developed what both agreed was a high-risk, high-profile course that could have significant impact on our colleagues as well as on our students. We knew that we were establishing a foundation that could (hopefully) scale to a much broader presence for the digital humanities across the Bucknell curriculum.
                1 We realized early on in course development that in order for our students to understand the evolving nature of DH research, we would have to reveal our own status as learners. Teaching unfamiliar material—not only across sections, but within a particular class—required an at times uncomfortable degree of transparency. 
            
            However, for all the challenges involved in teaching the class, there were moments of glory. Disengaged students became engaged; solitary learners recognized the essential need to collaborate in order to succeed; participants recognized the transformative nature of the course to their own concepts of the humanities. Students were eager to participate in crowdsourced data collection; they were intrigued to visualize ego-networks as they learned the concepts of network theory; they were excited to see their marked-up transcriptions published in an online digital edition. Through these experiences, they realized that they were creating a community of young DHers and expressed eagerness to take part in more of these learning experiences. 
            We believe that this paper is important to the community that will be present at DH2015 because it provides a model for how digital humanities can and should be taught at the earliest stage of an undergraduate’s university experience, and how this type of learning experience is transformative in terms of demonstrating the interdisciplinarity within the humanities. If such courses are well planned, modestly ambitious, and truly collaborative in both conceptualization and execution, they promote radically new ways of understanding the goals of humanistic enquiry, a new pedagogical hermeneutic for teachers as well as students. 
            Note
            1. In ‘Digital Learning in an Undergraduate Context: Promoting Long-Term Student-Faculty (and Community) Collaboration in the Susquehanna Valley’ (under review), we argue for the importance of such DH methods courses as crucial to the development of a larger curricular program of research-based learning environments that engage students in digital public humanities projects.
        
        
            
                
                    Bibliography
                    
                        Bonds, E. L. (2014). Listening In on the Conversations: An Overview of Digital Humanities Pedagogy. 
                        CEA Critic,
                        76(2) (July): 147–57. 
                    
                    
                        Clement, T. (2012). Multiliteracies in the Undergraduate Digital Humanities Curriculum: Skills, Principles, and Habits of Mind. In Hirsch, B. D. (ed.), 
                        Digital Humanities Pedagogy: Practices, Principles and Politics. Cambridge, UK: Open Book Publishers.
                    
                    
                        Drucker, J. (2014). Interpreting Visualization::Visualizing Interpretation. In 
                        Graphesis: Visual Forms of Knowledge Production. Cambridge, MA: Harvard University Press, pp. 56–137.
                    
                    
                        Liu, A. (2009). Digital Humanities and Academic Change. 
                        English Language Notes, 
                        47(1) (Spring/Summer): 17–35. 
                    
                    
                        Pacchioli, D. (2013). Thinking in Place. 
                        Bucknell Magazine, Fall, http://www.bucknell.edu/x83427.xml.
                    
                    
                        Pierazzo, E. (2011). A Rationale of Digital Documentary Editions. 
                        Literary and Linguistic Computing,
                        26(4): 463–77. 
                    
                    
                        Rosenberg, D. and Grafton, A. 2000. Time in Print. In 
                        Cartographies of Time: A History of the Timeline. New York: Princeton Academic Press, pp. 10–15. 
                    
                    
                        Thomas, L. and Solomon, D. (2014). Active Users: Project Development and Digital Humanities Pedagogy. 
                        CEA Critic,
                        76(2) (July): 211–20. 
                    
                
            
        
    



    
        
            
                Visual First Amendment: A Case Study in Empricial Legal Studies, Visualization, and User Experience
                
                    
                        Sula
                        Chris Alen
                    
                    Pratt Institute, United States of America
                    csula@pratt.edu
                
                
                    
                        Rabina
                        Debbie
                    
                    Pratt Institute, United States of America
                    drabina@pratt.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    visualization
                    statistics
                    law
                    networks
                    usability
                
                
                    project design
                    organization
                    management
                    user studies / user needs
                    law
                    visualisation
                    data mining / text mining
                    English
                
            
        
    
    
        
            This paper uses the Visual First Amendment (VFA) project to highlight the use of data visualization, user experience studies, and computational methods in the study of law. VFA presents visitors with interactive displays that allow them to explore the interrelation of issues, cases, courts, and justices over time, and to consider the broad social and legal changes that have impacted the United States Supreme Court’s ruling on the First Amendment freedoms of religions, speech, press, assembly, and petition. We begin by situating our project in the emerging field of empirical legal studies and describing the open data sources used to create our visualizations. We also discuss several usability tests conducted to improve our visualizations, and the development of the project cycle from concept to dissemination. Explicit attention is given throughout to digital humanities aspects of the project. 
            * * *
            In 1897, future U.S. Supreme Court justice Oliver Wendell Holmes Jr. observed, ‘For the rational study of the law the black-letter man may be the man of the present, but the man of the future is the man of statistics and master of economics’ (Buckler, 2012). Holmes was addressing the need for law to look beyond historical examples to the social realities of the time. His methodological suggestion, however, has found new life in the era of digital humanities. 
            This paper presents the Visual First Amendment (VFA) project, which applies empirical methods and visualization techniques to the study of the First Amendment of the U.S. Constitution. VFA aims to 
             • Provide a deeper understanding of the American constitutional freedoms of religion, speech, press, assembly, and petition.
             • Engage citizens in a discussion of rights, as interpreted now and in the future.
             • Raise awareness of First Amendment issues from a historical and legal perspective.
             • Racilitate research surrounding the First Amendment across a range of education levels and information tasks.
             • Serve as a model for representing Supreme Court data generally. 
            We begin by giving an overview of empirical legal research and similar projects that have been attempted to date. Following that, we describe the data sources and visualization tools behind several VFA interfaces. Finally, we present the results of user experience studies, as well as future directions. 
            
                Background
            
            Traditional legal scholarship consists of detailed analysis of primary sources (e.g., legislation and court rulings), supplemented by secondary sources such as legal scholarship. Empirical legal research, first employed in American legal scholarship in the late 1990s, ‘uses data analysis to study the legal system’ (Georgetown Law, 2014). While this approach is still not widespread, it has been described as ‘the next big thing’ in legal intellectual thought (George, 2006).
            Cane and Kritzer (2010) describe empirical legal research as involving ‘the systematic collection of information (“data”) and its analysis according to some generally accepted method’ and the (possibly numeric) coding or tagging of units of text. This research is often interdisciplinary—examining questions in the intersection of law, society, and economics, as well as judicial behavior and politics—and uses both quantitative/statistical and qualitative methods. Most data used in empirical legal studies are ambient in the sense that they are generated through judicial processes rather than through experiments, which would raise ethical risks in the case of law (Epstein and Martin, 2010). 
            The proliferation of law-related datasets and the increasing availability of data to the general public represent an important step toward increased civic engagement. The U.S. Open Government Initiative makes available over 200 datasets, which are used by journalists, researchers, and the general public for a variety of purposes, among them government oversight and accountability. The Sunlight Foundation, for example, has supported several projects that draw on publicly available data to promote government oversight practices such as tracking the link between campaign contributions and congressional votes on legislation and public policy. 
            To date, however, few attempts have been made to visualize court rulings, and none are focused on presenting data about the First Amendment for a wide variety of audiences: 
             • One of the earliest projects, Visualizing Legal Information (http://www.cs.umd.edu/hcil/west-legal), was created in the late 1990s in the Human Computer Interaction Lab at the University of Maryland. It is no longer updated, and most of the visualizations are no longer functional. 
             • A group of researchers from the University of Oslo is working on an interdisciplinary Law and Visualization project (http://www.jus.uio.no/english/research/areas/law-history/projects/law_visualization.html), which focuses on the relations between law and visualization as expressed in art, architecture, and film. Though the project is described as ‘empirical’, it does not employ data-driven methods, instead examining the experience of aestheticians and their concepts of law. 
             • The Judicial Research Initiative (http://artsandsciences.sc.edu/poli/juri) from the University of South Carolina aims to ‘provide a comprehensive access point to the most recent and cutting-edge research on law and judicial politics.’. This project only has datasets available for download and does not include visualization. 
            Visual First Amendment is unique among these projects as it applies visualization techniques to First Amendment data for a wide range of users. The information presented by the site is also highly contextualized both historically and thematically. 
            Project Design 
            Visual First Amendment draws on three freely available data sources, which have been selected for their recognized excellence in the field: 
             • First Amendment Timeline is compiled by the First Amendment Center, an operating program of the Freedom Forum located at the Newseum and Vanderbilt University. The Timeline is a chronological list of 150-plus American historical events, Supreme Court rulings, and legislation relating to the First Amendment from 1215 to 2011, with narrative descriptions of each event. 
             • Supreme Court Database (SCDB) is a National Science Foundation project hosted by the Center for Empirical Research in the Law at Washington University in St. Louis with nearly a dozen contributing law schools. SCDB contains data on 8,407 cases, 655 of which are classified as First Amendment cases, with over 200 pieces of information coded about each case. While this schema has received some criticism (Shapiro, 2008), it is highly regarded and used by scholars in various publications (Buckler, 2012; Sharma, 2013; Stearns, 2013), including a recent book on behavior of federal judges (Epstein et al., 2013). The project currently covers cases between the 1946 and 2012 terms and has a timetable established for the addition of other years. 
             • Supreme Court Citation Network Data, compiled by James H. Fowler (UCSD) and Sangick Jeon (Stanford), contains 202,167 citations to and from 30,288 Supreme Court majority opinions over the 1800–2002 period.
            The initial VFA prototype contains a range of visualizations based on these data sources. A complete list of these pairings is given in Table 1, and each type of visualization is described below. 
            
                Visualization Type Software Data 
            
            timelines TimelineJS First Amendment Center Timeline 
            statistical charts and graphs Tableau Public SCDB 
            networks Gephi SCDB, Citation Network Data geographic maps Leaflet, QGIS, Tableau Public SCDB 
            Table 1.
            Timelines 
            Timelines are the simplest and most easy-to-read displays used in the project. The current prototype includes a timeline of 100-plus major cases and historical events, as well as curated timelines around specific First Amendment stories. Users can swipe through the timeline using their mouse or touchscreen, and events are divided into a number of categories, including world/national events and various First Amendment freedoms.
            Visual First Amendment Timeline Browser 
            
                
            
            
                
            
            Figure 1. http://visualfa.org/timeline/.
            Statistical Charts and Graphs 
            VFA includes several statistical charts and graphs about First Amendment rulings as well as Supreme Court justices. These visualizations are interactive, allowing the user to select, filter, and manipulate the information presented, and can be shared or embedded across blogs, websites, and social media. 
            Justice Voting Patterns by Ideology 
            
                
            
            
                
            
            Figure 2. http://visualfa.org/statistics/justices-by-ideology/.
            Network Maps 
            Networks are among the most complex yet frequently discussed visualizations in the initial prototype. These graphs illustrate the relationships between court opinions and agreement between justices. Each dot represents a case; hovering over the dot calls up the name of the case, and clicking on it opens a sidebar with full case details. Cases are searchable by name or by topic. 
            Citation Network of Majority Opinions by First Amendment Issue 
            
                
            
            
                
            
            Figure 3. http://visualfa.org/citation-network-maps/network-maps-a-guided-tour. 
            Geographic Maps 
            Geographic maps are some of the first visualizations that young users learn to read, and they bring a new dimension of spatial reasoning to historical and thematic content. Several maps in our initial prototype illustrate the behavior of circuit courts, visually investigating whether these courts merit their reputations as being conservative or liberal in their rulings. A separate map examines the number of cases that arise from each state, filterable by issue, petitioner categories, respondent categories, and more. 
            Circuit Court Map by Decision Ideology 
            
                
            
            
                
            
            Figure 4. http://visualfa.org/circuit-court-map/. 
            User Experience 
            In spring 2014 the Visual First Amendment team conducted an initial round of user experience tests (Pratt IRB #189/2-12-14) at the American Civil Liberties Union and New York Civil Liberties Union offices in downtown Manhattan. Five users were tested using a talk-out-loud method in which they were asked to complete tasks and to narrate their decisionmaking process as they go. The tests were recorded using the Silverback Screen Capture software so that user behavior could be examined later. Users were also asked for general feedback on the site, including its potential audiences. We also spoke more informally with law students, law faculty, and law librarians, who have a special interest in the content of the project. This research has led to a number of site revisions, including 
             • Site navigation with multiple points of entry.
             • Contextual information and details on demand within visualization interfaces.
             • Curated datasets and stories that introduce more advanced interfaces. 
            Future Directions 
            At present, we are working to collaborate with humanities scholars, legal advisors, and media experts on content and visualizations that reflect general/public interest. To further public engagement, the project would benefit from additional user testing with broader audiences as well as the creation of additional tutorial videos and other instructional materials for the website. Finally, we hope to create an integrated user experience across multiple types of visualizations and to link with external resources, such as news stories and reference sources. 
        
        
            
                
                    Bibliography
                    
                        Buckler, K. G. (2012). The Newsworthiness of U.S. Supreme Court Criminal Procedure Cases (1994–2010 Terms): Assessing the Effects of Case Salience and Case Complexity across Elite and Populace Press. 
                        Criminal Justice Review, 
                        39(2): 140–59, http://cjr.sagepub.com/content/39/2/140.short. 
                    
                    
                        Cane, P. and Kritzer, H. M. (2010). In Cane, P. and Kritzer, H. M. (eds), 
                        The Oxford Handbook of Empirical Legal Research. New York: Oxford University Press, pp. 1–7. 
                    
                    
                        Epstein, L., Landes, W. M. and Posner, R. A. (2013). 
                        The Behavior of Federal Judges: A Theoretical and Empirical Study of Rational Choice. Harvard University Press, Cambridge, MA.
                    
                    
                        Epstein, L. and Martin, A. D. (2010). Quantitative Approaches to Empirical Legal Research. In Cane, P. and Kritzer, H. M. (eds), 
                        The Oxford Handbook of Empirical Legal Research. New York: Oxford University Press, pp. 901–25. 
                    
                    
                        George, T. E. (2006). An Empirical Study of Empirical Legal Scholarship: The Top Law Schools. 
                        Indiana Law Journal, 
                        81: 141–61. 
                    
                    
                        Georgetown Law. (2014). Statistics and Empirical Legal Studies Research Guide. http://www.law.georgetown.edu/library/research/guides/empiricallegalstudies.cfm (accessed 1 June 2014). 
                    
                    
                        Shapiro, C. (2008). Coding Complexity: Bringing Law to the Empirical Analysis of the Supreme Court. 
                        Hastings Law Journal,
                        60, http://ssrn.com/abstract=998639. 
                    
                    
                        Sharma, H. (2013). The Ideological Relationship between Elected Officials and Supreme Court Justices, 1946–2012: Reframing the ‘Activism’ Debate to Incorporate Issue-Specific Analysis and Intra-Court Controls. http://ssrn.com/abstract=2277096. 
                    
                    
                        Stearns, M. L. (2013). Constitutional Law in Social Choice Perspective (2013). 
                        Public Choice,
                        157(2013), University of Maryland Legal Studies Research Paper No. 2013–57. 
                    
                
            
        
    


        
            
                Introduction
                Although representing large corpora through the network of persons’ interactions has become quite popular in the Digital Humanities community (Elson et al., 2010), several parameters can have an impact on the resulting network, especially when it is automatically extracted. In this work, we present a step-by-step procedure to extract persons’ networks from documents and select possible configurations in order to increase readability and ease the interpretation of the obtained information. We also discuss some open issues of the task.
                We rely on the same assumption as for word co-occurrence networks: two persons who tend to be mentioned together in a corpus share some commonality or relation from the author’s perspective.
            
            
                The Methodology
                We implemented a novel tool for the automated extraction of a persons’ network from a corpus in the online ALCIDE platform http://celct.fbk.eu:8080/Alcide_Demo/ (Moretti et al., 2014). The module is based on the following steps: i) the corpus is first analysed with the Stanford named entity recognizer (Finkel et al., 2005), in order to recognize persons’ mentions (e.g. John Kennedy, F.D. >Roosevelt, etc.). In the network representation, we assume that persons correspond to nodes and edges express co-occurrence within a given token window; ii) We build a person-person matrix where we assign an edge weight of 1 every time two persons are mentioned together within a certain context window. Every time a co-occurrence is repeated, the weight is increased by 1; iii) The final output is a weighted undirected network where edge weights are the co-occurrence frequency. In the default configuration, name mentions are collapsed onto the same network node only if they have an exact match. In order to allow a more flexible creation of the network, a “Person Management” functionality (Fig. 1) has been implemented, through which users can collapse nodes referring to the same person (e.g. J.F. Kennedy and John Kennedy). This manual check is done through an interface, without the need to access directly the underlying matrix.
                From a technical point of view, the information is stored in a relational database management system, in order to grant multi-user access, good performances and high flexibility regarding the queries. The persons’ co-occurrence matrix is visualized as a network by means of the d3 javascript framework http://d3js.org. During the conversion of the matrix in the json used by d3, the nodes are enriched with additional information, such as the list of documents containing the corresponding entity and the number of connections.
                
                    
                        
                       Fig. 1 View of the Person Management tool
                 
                
                Some settings such as the co-occurrence window type (sentence or token) and width (number of sentences/tokens) are arbitrary, although they have a relevant impact on the extracted network and on its readability. Therefore, the system gives the possibility to change such settings and regenerate the co-occurrence matrix at runtime. In the following sections we will discuss some of these parameters and explain their impact in the light of a use case related to Nixon and Kennedy’s speeches of the 1960 presidential campaign. The corpus consists of 282 documents by Nixon (830,000 tokens) and 598 documents by Kennedy (815,000 tokens)
                    http://www.presidency.ucsb.edu/1960_election.php. All networks displayed in the following sections are screenshots of the system output and are dynamically displayed.
                
                
                    Default configuration
                    In our default configuration, the tool extracts persons’ networks using 1 sentence as a co-occurrence window and collapsing on the same node only name mentions with an exact match. As shown in Fig.2, this basic configuration is enough to highlight the differences between Kennedy’s and Nixon’s networks: the first is much larger and much more connected, with several cliques that tend to emerge from the overall picture. Nixon’s network, instead, is smaller (i.e. less persons are mentioned in his corpus) and less dense. 
                        
                            
                            
                            Fig. 2 Persons co-occurrence network extracted from Kennedy’s (left) and Nixon’s (right) speeches
                        
                    By zooming in the pictures, it is possible to focus on single nodes of interest. For example, if we compare Nixon’s mentions appearing in Kennedy’s speeches, and the other way round (Fig. 3, left and right resp.), we observe that in both cases the opponent is frequently mentioned with ‘enemies’ of the time such as Fidel Castro and Khrushchev. However, this association with negative figures is much more frequent in Kennedy’s speeches (e.g. Nixon is mentioned also with Trotsky and Lenin), probably because Nixon had already a prominent role in US foreign policy being Vice-President. 
                        
                            
                            
                            Fig. 3 Co-occurrence network of “Nixon” mentions extracted from Kennedy’s corpus (left) and of “Kennedy” mentions from Nixon’s (right) speeches
                        
                
                
                    Changing configuration parameters
                    The tool allows users to move from the default configuration to a more customizable one, where it is possible to change the type (sentence or token) and the size of the context window taken into account for the co-occurrences as well as set a threshold to the edges’ weight (number of co-occurrences). By tuning these parameters, it is possible to transform the networks presented in Fig. 2 to obtain a more readable representation, filtering minor nodes and emphasizing information previously hidden by the large amount of information. 
                    Reducing the co-occurrence window increases the probability to extract persons that are more strictly related. At the same time, by increasing the minimum edge weight threshold, we reduce the information visualized, filtering out all the persons co-occurring only once in favour of persons co-occurring consistently through the entire corpus. 
                    Fig. 4 shows the networks, obtained from the data in Fig. 2, generated by setting the maximum token range to 10 (thus, on average, less than the sentence length adopted in Fig. 2) and the minimum edge threshold to 2. The result is a visualization with less but more readable data. In Nixon’s network, we can easily spot some well-defined clusters such as the one grouping the leaders of the communist world (i.e Khrushchev, Stalin, Mao Tse-tung), the cluster of the main representatives of international politics in 1960 (e.g. de Gaulle, Nehru, Adenauer) or a cluster reflecting Nixon’s attitude to refer to previous U.S. Presidents (e.g Andrew Jackson, Thomas Jefferson). Also the network from Kennedy’s corpus is more understandable, including a cluster with prominent Communist politicians (e.g. Khrushchev, Castro, Kadar), but also clusters defining local democratic representatives, for instance those from California (e.g. Pat Brown, John Moss) or those from Pennsylvania (e.g. David Leo Lawrence, Joseph S. Clark).
                    
                        
                            
                            Fig. 4 Persons co-occurrence network extracted from Kennedy’s (left) and Nixon’s (right) speeches using a 10 tokens windows and a minimum threshold of 2.
                        
                    
                    
            
            
                Discussion
                As shown in the above examples, building a persons’ network in an automated fashion implies making some a-priori choices that strongly affect the outcome of the analysis. Such choices are influenced by the type of analysis required by the user. If distant reading is the main goal, the parameters proposed as default by our tool, as shown in Fig. 2, seem to be appropriate. This analysis gives an overview of the overall network dimension and density, and makes it possible to compare two networks at a glance. Instead, if close reading and a qualitative analysis of the connections are more relevant, reducing the window width and displaying only the most connected nodes is necessary. Since in a typical research scenario distant and close reading are both present, and users need to zoom in and out frequently, a tool that changes the network on demand in real time should be implemented. In this respect, Gephi (Bastian et al., 2009), probably the most widely used tool for network analysis in the Digital Humanities community, shows some limitations. Although it provides useful in-built metrics for analysing a network structure, it does not offer the possibility to test different parameters on the fly. Also its integration in an online, browser-based environment is quite complex, as well as the connection to a text analysis pipeline.
                Other issues related to the automated creation of persons’ networks are worth mentioning. Since natural language processing tools are involved in the pre-processing step, users should be aware of the possible mistakes introduced by this pipeline. In particular, Named Entity Recognizers may label as persons other types of entities, wrongly introducing nodes in the network. Other possible mistakes are more difficult to spot and concern homonyms (i.e. nodes that should be split). Such cases can be solved only resorting to a cross-document entity coreference system, where mentions can be resolved by linking them to the entities they refer to. A finer-grained outcome could be achieved integrating also an intra-document coreference system, able to link mentions referring to the same person in a text. Cross- and intra-document coreference would be necessary to ensure that all persons mentioned in a corpus are included in the extracted network. Nevertheless, the impact of automated processing on the quality of the network needs to be further investigated.
            
        
        
            
                
                    Bibliography
                    
                        Bastian M., Heymann S. and Jacomy M.  (2009). Gephi: an open source software for exploring and manipulating networks.
                        International AAAI Conference on Weblogs and Social Media (ICWSM).
                    
                    
                        Elson, D. K., Dames, N. and McKeown, K. R. (2010). Extracting Social Networks from Literary Fiction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden.
                    
                    
                        Finkel J.-R., Grenager T. and Manning, Ch.  (2005). Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-70.
                    
                    
                        Moretti G., Tonelli S., Menini S. and Sprugnoli R. (2014). ALCIDE: An online platform for the Analysis of Language and Content in a Digital Environment. In Proceedings of the First Italian Conference on Computational Linguistics (CLIC-2014), Pisa, Italy.
                    
                
            
        
    


        
            Since 2014, some of the countries that were formerly belligerent of the Great War – most particularly France and UK – have organised a series of commemorations of the First World War, known as the ‘Centenaire’ (France) or the ‘Centenary’ (UK). We can assume that there is a strong link – that cannot let a historian indifferent – between those commemorations, collective memory and historical studies.
            Though studies about collective memory are numerous since the famous works of the French sociologist Maurice Halbwachs (Halbwachs, 1950), few of them are examining how collective memories are being expressed – maybe even transformed – on social networks on-line.
            In the case of the Centenary of the First World War, a set of questions can be asked: What is the on-line echo of the commemoration of the centenary of the 1st World War? What is the behaviour of Memorial/Heritage Institutions about the 1st World War on Twitter? How do they transmit information about the Centenary? Is there an influence of the English predominance on Twitter about the Centenary on how non-english-speaking twitter accounts are considering the 1st World War? Are there specific subjects that are discussed on-line? Which ‘temporalities’ are present in tweets when Twitter users speak about the Great War on-line?
            Though we are not yet able to respond to all those questions, we’ll use our database of tweets in order to answer them at least partially.
            Indeed, since the 1
                st April 2014, around 1.5 millions of tweets containing a hashtag (keyword) linked to the 1st World War were written by over 350 000 Twitter accounts in several languages (mainly English and French). Twitter is a good field to analyse relationships between history and collective memory, memorial institutions and citizens, historians and a wide non-academic audience. We started to explore this database (which is still expanding): we intend to show how a historian can collect, analyse and interpret those tweets, using Digital Humanities methodologies and software in order to answer questions about collective memory of the First World War online.
            
            
                Tools and Methodologies 
                We are using 140dev, a PHP open source script within a LAMP environment to collect tweets through the Twitter streaming API
                    
                        
                            
                                http://140dev.com/ (accessed 4 March 2016).
                            
                        
                    . The tweets are then stored in a MySQL database. Diverse information (tweets and their metadata, hashtags, user information, mentions, retweets) about those tweets can easily be extracted through SQL queries. Those queries can also be used to extract different kind of relations: between tweets, between Twitter users or even between hashtags (
                    ie if a Twitter user mentioned or retweeted another twitter user, if two users wrote the same hashtags, etc). Concerning privacy, we respect the Twitter API Terms.
                
                To analyse tweets, we are using mainly two sets of methodologies/software: social network analysis and network visualisations (with Gephi: mention, retweets or hashtags are considered a link); text analysis through the theory of the 
                    mondes lexicaux (Reinert, 1993) as it is implemented in the IRaMuTeQ software (Ratinaud and Dejean, 2009)
                    
                        
                            
                                http://www.iramuteq.org/ (accessed 4 March 2016) - Interface de R pour les Analyses Multidimensionnelles de Textes et de Questionnaires. IRaMuTeQ is a free software based on python and R. It is available in French, English, German and Spanish (interface and analyses).
                            
                        
                    . The combination of both tools and methodologies has been described by (Smyrnaios and Ratinaud, 2014). IRaMuTeQ, thanks to time-stamped metadata, can also help us working on temporalities. Indeed, clusters that are defined by this software can be projected in time: we can know, day-by-day, the most used kind of tweets.
                    
                        
                             IRaMuTeQ works in dividing the corpus in small segments of text (around 40 words). In our case each segment is a tweet and each tweet is also a text.
                        
                     It helped us, for instance, finding that French fallen soldiers are not described with the same words the 11
                    th of November in comparison to the rest of the year.
                
                The methodologies and tools that remain to be found for this research concern temporalities – even if IRaMuTeQ has helped us answer some question on time. There are several temporalities that are expressed in this corpus: the constant feed of information that is the nature of Twitter; the temporality of each twitter user; the temporality of the Centenary (which is different from one country to the other, and from the Great War temporality); and the temporality of the War itself.
            
            
                First results
                
                    Language
                    English is overwhelmingly present in this corpus. Around 10% only of the collected tweets are not in English. Among those 10%, French is largely in majority and German almost absent, even though German hashtags are collected. The fact that Twitter is an English-based social network does not explain fully this disequilibrium between English and other languages. The Memorial institutions' communication policies on Twitter are better factors to explain it. 
                    The decentralized communication policy of British memorial institutions (the BBC and all its Twitter accounts or the Imperial War Museum for instance) is obviously more efficient than the French centralized communication policy of the 
                        Mission du centenaire. French WW1-related museums do not have Twitter accounts or do have one but do not follow twitter implicit rules such as the use of a general hashtag like #ww1 or the French #pgm.
                    
                
                
                    British and French are not commemorating WW1 the same way
                    The most striking difference between the French corpus and the English one is the fact that both linguistic areas do not commemorate the Great War the same way. There are two major differences between both countries:
                    
                        French are mainly remembering the soldiers (
                            Poilus). British citizens are remembering soldiers, but also battles.
                        
                        The French are focusing on the end of the war, the Armistice, on the 11th November. The British are focusing on the way they entered the war.
                    
                
                
                    English public history and French history amateurs
                    Thanks to the Network visualisations, this corpus also helps understand how public history is present in Britain, in contrary to France where it just begins to appear. The presence of amateurs of history in the French corpus also shows that French historians are not on twitter, in contrary to amateurs who, next to the 
                        Mission du Centenaire, are structuring discussions about the First World War on Twitter.
                    
                
            
            
                Conclusion
                
                    Comparing multilingual corpora
                    To compare our two main corpora (the French one and the English one) that can be extracted from the database, we had to use the two main pieces of software the same way on both corpora and then to ‘humanly’ compare the results. We could not find any tools able to compare two corpora that are in different languages.
                
                
                    Distant reading / Close reading
                    This research project shows that, for historians, it is still important to keep a direct link with each single primary source, as some information can be learned from the interpretation of single tweets. Though methods used in this research are dealing with Franco Moretti’s notion of 
                        distant reading (Moretti, 2007), it proved strategic to be able to go back to every single tweet. The software used, if metadata are kept all along their use, allow this.
                    
                
                
                    Twitter and the rest of the web
                    Why Twitter? The fact that the Twitter API, though sometimes very unstable, is very convenient to use is one of the criteria of this choice. Is it really pertinent in terms of research? Shouldn't we have broader sources? How to extrapolate the project's results to other on-line social networks? Last but not least, the difficulty to anticipate hashtags to be collected might introduce biases in our research.
                
                
                    Future of this research project
                    The question of ‘temporalities’ and their imbrications (the temporality of Twitter / the temporality of users / the temporality of the commemorations / the temporality of the First World War itself) should be the next step of this research. But, as it will require the use of Named Entity Recognition, extending our research to places will be possible as well.
                
            
        
        
            
                
                    Bibliography
                    
                        Halbwachs, M. (1950). 
                        La mémoire collective, Paris: Albin Michel.
                    
                    
                        Moretti, F. (2007). 
                        Graphs, Maps, Trees: Abstract Models for Literary History. London: Verso.
                    
                    
                        Ratinaud, P. and Dejean, S. (2009). IRaMuTeQ: Implémentation de la methode ALCESTE d’analyse de texte dans un logiciel libre [Implementation of the ALCESTE method of text analysis in an open-source software]. 
                        Presentation. Available at: http://repere.no-ip.org/Members/pratinaud/mes-documents/articles-et-presentations/presentation_mashs2009.pdf (accessed 4 March 2016).
                    
                    
                        Reinert, M. (1993). Les ‘mondes lexicaux’ et leur ‘logique’ à travers l’analyse statistique d’un corpus de récits de cauchemars. 
                        Language et Société, 66(1): 5–39. doi:10.3406/lsoc.1993.2632
                    
                    
                        Smyrnaios, N. and Ratinaud, P. (2014). Comment articuler analyse des réseaux et des discours sur Twitter. 
                        tic and société, 7(2). doi:10.4000/ticetsociete.1578
                    
                
            
        
    


        
            
                Introduction
                Big Data is reshaping the historical profession in ways we are only now beginning to grasp. The growth of digital sources since the advent of the World Wide Web in 1990-91 presents new opportunities for social and cultural historians. Large web archives contain billions of webpages, from personal homepages to professional or academic websites, and now make it possible for us to develop large-scale reconstructions of the recent web. Yet the sheer number of these sources presents significant challenges: if the norm until the digital era was to have human information vanish, “now expectations have inverted. Everything may be recorded and preserved, at least potentially” (Gleick, 2012).
                While the Internet Archive makes archived web content available to the general public and mainstream scholarly community through its “Wayback Machine,” (at http://archive.org/web) which allows visitors to enter a Uniform Resource Locator (URL) to visit archived web versions of a particular page, this system is limited: not only do visitors need to know the URL in the first place, but they are limited to individual readings of single webpages.
                By unlocking the Wayback Machine’s underlying system of specialized files, primarily ISO-standardized WebARChive (WARC) files, we can develop new ways to systematically track, visualize, and analyze change occurring over time within web archives. Warcbase, an open-source platform for managing web archives built on Hadoop and HBase, provides a flexible data model for storing and managing raw content as well as metadata and extracted knowledge. Tight integration with Hadoop provides powerful tools for analytics and data processing. Using a case study of one collection, this paper introduces the work that we have been doing to facilitate web archive access with warcbase. We have growing documentation at http://docs.warcbase.org.
            
            
                Project Rationale and Case Study
                In 1996, the Internet Archive launched a complementary research services company, Archive-It, which offers subscription-based web archiving to collecting institutions. 
                The University of Toronto Library (UTL) began collecting a quarterly crawl in 2005 of Canadian political parties and political interest groups (the collections were separate in 2005, merging in 2006) (University of Toronto, 2015). The collection itself has a murky history: UTL had been part of a broader project that would have collected political websites. It fell through, but UTL opted to carry out their crawl on their own and the librarian was responsible for selecting the seed list herself (faculty and other librarians did not respond for calls for engagement). While formal political parties are robustly covered, the “political interest groups” collection was a bit more nebulous: sites were discovered through keyword searches, and some were excluded due to robots.txt exclusion requests. Beyond this brief sketch, we have little information about the decisions made in 2005 to create this collection. This lack of documentation is a shortcoming of this collection model, as if a historian was to use this material in a peer-reviewed paper, questions would be raised about its representativeness.
                If a user wants to use the Canadian Political Parties and Interest Groups Collection (CPP) through Archive-It today, they visit the collection page at https://archive-it.org/collections/227 and enter full-text search queries. In August 2015, our group also launched http://webarchives.ca, based on the British Library’s SHINE front end for web archives; this was a way to facilitate a different form of more casual user access, aimed at the general public (we discuss this in a separate paper).
                The Archive-It portal is limited. There are no readily-available metrics of how many pages have been collected, how they break down by domain and date, and the portal undoubtedly provides skewed results unless the search phrase is dramatically narrowed down.
                Consider the search for “Stephen Harper,” Canada’s Prime Minister between 2006 and 2015 in Figure 1.
                
                    
                    Figure 1: Archive-It Search Portal
                
                The results are decent: Harper’s Facebook page from 2009, a Twitter snapshot from 2010, and some long-form journalism articles and opposition press releases. But amidst the 1,178,351 results, there is no indication as to how the ranking took place, what facets are available, and how things may have changed over the last ten years of the crawl.
                The data is there, but the problem is access.
            
            
                Warcbase: A Platform for Web Archive Analysis
                Warcbase is a web archive platform, not a single program. Its capabilities comprise two main categories:
                
                    Analysis of web archives using the Pig or Spark programming languages, and assorted helper scripts and utilities
                    Web archive database management, with support for the HBase distributed data store, and OpenWayback integration providing a friendly web interface to view stored websites
                
                One can take advantage of the analysis tools (1) without bothering with the database management aspect of Warcbase – in fact, most digital humanities researchers will probably find the former more useful. This paper focuses on the former capabilities, showing how we can use the warcbase platform to carry out text and network analyses.
            
            
                Using Warcbase on Web Archival Collections: Text Analysis
                We have begun to document all warcbase commands on a GitHub wiki, found at 
                    https://github.com/lintool/warcbase/wiki. We begin with installation instructions, and then provide simple scripts written in Apache Spark to run the commands.
                While possible to generate a plain text version of the entire collection, a more fruitful approach has been to generate date-ordered text for particular domains. If a researcher is interested in say, the Green Party of Canada’s evolution between 2005 and 2015, they can extract the plain text for greenparty.ca by running the following script:
                
                    
                
                All they would need to change would be the path/to/input to the directory with their web archive files, the path/to/output for where they want to save the resulting plain-text files, and the greenparty.ca value to whatever domain they might be interested in researching.
                They then receive a date-ordered output of all plain text for that domain (as per the extractCrawldateDomainUrlBody command). It can then be sorted and used in other research avenues. For example, this plain text could be loaded into a text analysis suite such as http://voyant-tools.org/ or other digital humanities environments.
                We have also been experimenting with other visualizations based on the extracted plain text. Computationally intensive textual analysis can be carried out using warcbase itself. Using the Stanford NER package in parallel, we have a script that extracts entities, counts them, and then visualizes them using D3.js to help see overall changes in a web archival collection. Figure 2 below shows the output of the NER visualizer.
                
                    
                    Figure 2: Named Entity Visualization within Warcbase
                
                Finally, another text approach is topic modelling (Blei et al., 2003). LDA works by finding topics in unstructured text. To visualize topic models, we elected to use the Termite Data Server, which is a visual analysis tool for exploring the output of statistical topic models (“uwdata/termite-data-server,” n.d.). As Figure 3 demonstrates, the visualization allows you to get a top-down view at the topics found in a web archive.
                
                    
                    Figure 3: Termite Topic Model
                
                Warcbase presents versatile opportunities to extract plain text and move it into other environments for analysis. Unlike the keyword-based Archive-It portal, we now have data that can be inquired in many fruitful ways.
            
            
                Using Warcbase on Web Archival Collections: Hyperlink Analysis
                Warcbase can also extract hyperlinks. While text can be very important, these sorts of metadata can often be more important: allowing us to see changes in how groups link to each other, what articles and issues were important, and how relationships changed over time.
                Consider Figure 4, which visualizes the links stemming from and between the websites of Canada’s three main political parties.
                
                    
                    Figure 4: Three major political parties in Canada
                
                Above, we can see which pages only link to the left-leaning New Democratic Party (ndp.ca), those that link only to the centrist Liberals (liberal.ca) in the top, and those that only connect to and from the right-wing Conservative Party at right. We can use it to find further information, such as in Figure 5.
                
                    
                    Figure 5: NDP attack
                
                The above links are from the 2006 Canadian federal election. The Liberal Party was then in power and was under attack by both the opposition parties. In particular, the left-leaning NDP linked hundreds of times to their ideologically close cousins, the centrist Liberals, as part of their electoral attacks, ignoring the right-leaning Conservative Party in the process. Link metadata illuminates more than a close reading of an individual website would. It contextualizes and tells stories itself.
                While we have traditionally used Gephi to do analysis, importing material into Gephi from warcbase required many manual steps as documented at https://github.com/lintool/warcbase/wiki/Gephi:-Converting-Site-Link-Structure-into-Dynamic-Visualization. We have been prototyping a link analysis visualization in D3.js, which can run in browser (Figure 6).
                
                    
                    Figure 6: Link Visualization
                
            
            
                Conclusions
                With the increasingly widespread availability of large web archives, historians and Internet scholars are now in a position to find new ways to track, explore, and visualize changes that have taken place within the first two decades of the Web. Warcbase will allow them to do so. This project is among the first attempts to harness data in ways that will enable present and future historians to usefully access, interpret, and curate the masses of born-digital primary sources that document our recent past.
            
        
        
            
                
                    Bibliography
                    
                        Brügger, N. (2008). The Archived Website and Website Philology: A New Type of Historical Document, 
                        Nordicom Review,  29(2): 155–75.
                    
                    
                        Gleick, J. (2012). 
                        The Information: A History, a Theory, a Flood. London: Vintage.
                    
                    
                        University of Toronto. (2015). Archive-It - Canadian Political Parties and Political Interest Groups [WWW Document]. 
                        
                            https://archive-it.org/collections/227
                        .
                    
                    
                        uwdata/termite-data-server. GitHub. 
                        https://github.com/uwdata/termite-data-server.
                    
                    
                        Blei, D. M., Ng, A. Y., Jordan, Michael I. (2003). Latent Dirichlet Allocation. 
                        J. Mach. Learn. Res, 3: 993–1022.
                    
                    
                        Brügger, N. and Finnemann, N. O. (2013). The Web and Digital Humanities: Theoretical and Methodological Concerns. 
                        Journal of Broadcasting & Electronic Media, 57(1): 66–80.
                    
                    
                        Lin, J., Gholami, M. and Rao, J. (2014). Infrastructure for Supporting Exploration and Discovery in Web Archives. 
                        Proceedings of the 23rd International Conference on World Wide Web. doi:10.1145/2567948.2579045.
                    
                
            
        
    


        
            
                Introduction
                University College London (UCL) owns a large corpus of the philosopher and social reformer Jeremy Bentham (1748-1832). Until recently, these papers were for the most part untranscribed, so that very few people had access to the corpus to evaluate its content and its value. The corpus is now being digitized and transcribed thanks to a large number of volunteers recruited through a crowd-sourcing initiative called Transcribe Bentham (Causer and Terras, 2014a, 2014b). 
                The problem researchers are facing with such a corpus is clear: how to access the content, how to structure these 30,000 files, and how to get relevant access to this mass of data? Our goal has thus been to produce an automatic analysis procedure aiming at providing a general characterization of the content of the corpus. We are more specifically interested in identifying the main topics and their structure so as to provide meaningful static and dynamic representations of their evolution over time. 
            
            
                Comparison with other works
                The exploration of large corpora in the Humanities is a known problem for today’s scholars. For example, the recent PoliInformatics challenge addressed the issue by promoting a framework to develop new and original research in text-rich domains (the project focused on political science but can be extended to any sub-field within the Humanities). 
                Specific experiments have recently been done in the field of philosophy, but they mainly concern the analysis of metadata, like indexes or references (Lamarra and Tardella, 2014; Sula and Dean, 2014). Different experiments have nevertheless involved an exploration of large amounts of textual data (see e.g. Diesner and Carley, 2005 on the Enron corpus) with relevant visualization interfaces (Yanhua et al., 2009). 
                In this paper, we propose to explore more advanced natural language processing techniques to extract keywords and filter them according to an external ontology, so as to obtain a more relevant indexing of the documents before visualization. We also explore dynamic representations, which were not addressed in the above-mentioned studies. 
            
            
                Corpus exploration strategy
                
                    The Text analysis module
                    Different scripts have been developed to filter the corpus
                        
                             For example, Bentham sometimes used French in his correspondence and these texts are eliminated via automatic language detection, since we focus on English in this experiment. 
                        . Then documents are assigned a date whenever possible: Since the corpus mostly contains notes and letters, the first date mentioned in the document often refers to the date of the document’s composition (even if this assumption is of course not always true). A large number of documents cannot be assigned a date and are thus not used for the dynamic analysis of the corpus. 
                    
                    To index the corpus and identify meaningful concepts, we first tried to directly extract relevant keywords from the texts. However, traditional techniques like the use of tf-idf (Salton et al., 1983) and c-value (Frantzi et al., 2000) do not seem very efficient in our case. This is not too surprising: it is well known that texts are too ambiguous to provide a sound basis for a direct semantic extraction. Surface variations, the use of synonyms and hyponyms, linguistic ambiguity and other factors constitute strong obstacles for the task. We thus decided to use natural language processing techniques that provide relevant tools to overcome some of these limitations. The tools we employed are either web-based or possible to execute on a personal computer with average specs. 
                    We tried to refine concept extraction by confronting the text with an external, structured database. We used DBpedia (Auer et al., 2007) as a source of structured knowledge (DBpedia is a database made of information extracted from Wikipedia). DBpedia is not a specialized source of information but this guarantees that the approach is not domain or author specific and could be easily used for other corpora. We then used the DBpedia Spotlight Web Service (Mendes et al., 2012; Daiber et al., 2013) to make the connexion between the corpus and DBpedia concepts. This leads to a much more fine grained and relevant analysis than possible with an entirely data-driven keyword extraction. 
                    Based on the outputs of Spotlight, only concepts that occurred at least 100 times, and with a confidence value of at least 0.1 were kept. Spotlight outputs a confidence value between 0 and 1 for each annotation; a 0.1 threshold removes clearly unreliable annotations while maintaining good coverage. Tagging the full corpus with Spotlight (ca. 30,000 documents) took over 24 hours. We called the Spotlight service one document at a time; parallelizing the process can decrease processing time.
                
                
                    The visualization module
                    Once relevant concepts are identified, one wants to produce relevant text representations so as to provide a usable interface to end users. We present here three different kinds of interfaces that show the possible exploitation of the analysis described above. 
                    The corpus is first indexed in a Solr search index
                        
                             https://lucene.apache.org/solr/
                         and accessible through a graphical end-user interface. It is possible to query the corpus by date, using Solr’s faceted search functions
                        
                             https://wiki.apache.org/solr/SolrFacetingOverview
                         (see figure 1).
                    
                    
                    
                        
                        Figure 1: Search interface: users can search via year extracted from the text, which in most cases is the year of writing, allowing users to see texts (especially correspondence) in chronological order.
                    
                    
                    It is also possible to cluster together related keywords, so as to get access to homogeneous sub-parts of the corpora representing specific subfields of Bentham’s activity (see figure 2). 
                    
                        
                        Figure 2: the main topics addressed in the corpus, based on clusters of concepts, showing the main concerns of Bentham's writings, which map closely onto established research areas in Bentham studies. The network was produced by Cortext; colours and fonts were reformatted in Gephi based on Cortext’s gexf-format export
                            
                                 https://gephi.github.io/
                            
                        
                    
                    Dynamic maps are also possible, to see for example the evolution of the different topics addressed in the corpus over time (see figure 3). 
                    
                        
                        Figure 3: A dynamic view of the corpus, computed with the Cortext plarform 
                            (tubes layout), with the evolution of the main topics addressed over time
                        
                    
                
            
            
                Scholarly benefits of these tools for the Transcribe Bentham project
                Since 1958, UCL's Bentham Project has been producing the new, critical edition of the “Collected Works of Jeremy Bentham”. The edition is expected to run to some eighty volumes, the thirty-third of which has recently been sent to the press. The “Collected Works” is based upon texts, which Bentham published during his lifetime, and unpublished texts, which exist in manuscript. It is a major task: UCL's Bentham Papers runs to some 75,000 manuscript pages, while the British Library's has a further 25,000 or so pages. About 40,000 pages have been transcribed to date and, while UCL's award-winning 'Transcribe Bentham' initiative has helped to significantly increase the pace of transcription, a great deal more work needs to be done.
                The first task in producing a volume of the “Collected Works” based upon texts in manuscript is to identify all the relevant pages. Bentham Project editorial staff use the Bentham Papers Database Catalogue, which indexes the manuscript collection by sixteen headings, including date, main heading, subject heading(s), author(s), and so forth. It is, however, entirely possible to miss relevant manuscripts using this method. The subject maps produced for this research promise to complement traditional Bentham Project methods; for instance, Bentham's work on political economy encompasses topics as varied as income tax to colonisation, and the subject maps will make it more straightforward to investigate the nexus between these, and other, subjects.
                The dynamic corpus view, showing the evolution of topics addressed over time, could also prove useful in editorial work as can be shown in two examples. First, an editor at the Bentham Project is currently working on Bentham's writings on convict transportation, though there is some confusion over when exactly Bentham first broached the topic. The dynamic corpus view could help to clear up whether it was only around 1802 when Bentham wrote about transportation, or if he had investigated the subject in any great detail during the 1790s. Second, Bentham became more radical as he aged, and several Bentham scholars have sought to identify the point at which Bentham abandoned his earlier conservatism and 'converted' to political radicalism, and representative democracy; an analysis of Bentham's language at the turn of the century would be instructive in helping clarify this matter.
            
            
                Conclusion
                In this paper, we have presented a first attempt to give a relevant access to a large interdisciplinary corpus in the domain of philosophy, law and history. We have shown that using tools in concept clustering and visualization can provide an alternative way to navigate large-scale corpora, and confirm and visualise scholarly approaches to large scale textual corpora. Exploring how these tools can be effectively used with a corpus such as Bentham's indicates these methods are applicable to other sources as well.
                In the near future, we are planning to refine the linguistic analysis in order to give better representations of the textual content of the corpus. We are also planning experiments with end-user to evaluate in more details the solution and the visualisation techniques used so far in this project. 
            
            
                Acknowledgements
                We want to thank IDEX PSL (Paris Sciences et Lettres, ref. ANR-10-IDEX-0001-02 PSL) as well as the labex TransferS (laboratoire d’excellence, ANR-10-LABX-0099) for supporting this research. Lastly, we would like to thank the reviewers for their insightful comments on the paper. 
            
        
        
            
                
                    Bibliography
                    
                        Auer, S. (2007). DBpedia: A nucleus for a web of open data. 
                        The Semantic Web. Berlin Heidelberg: Springer. 
                    
                    
                        Causer, T. and Terras, M. (2014a). Many hands make light work. Many hands together make merry work: 
                        Transcribe Bentham and crowdsourcing manuscript collections. In Ridge, M. 
                         Crowdsourcing Our Cultural Heritage, Farnham: Ashgate.
                    
                    
                        Causer, T. and Terras, M. (2014b). Crowdsourcing Bentham: Beyond the Traditional Boundaries of Academic History, 
                        International Journal of Humanities and Arts Computing, 8(1): 46-64. 
                    
                    
                        Daiber, J., Jakob, M., Hokamp, C. and Mendes, P. (2013). Improving Efficiency and Accuracy in Multilingual Entity Extraction. Proceedings of the 9th International Conference on Semantic Systems (I-Semantics), Graz. 
                    
                    
                        Diesner, J. and Carley, K. (2005). Exploration of Communication Networks from the Enron Email Corpus. Proceedings of SIAM International Conference on Data Mining, Newport Beach: Workshop on Link Analysis, Counterterrorism and Security, pp. 3- 14,.
                    
                    
                        Frantzi, K., Ananiadou, S. and Mima H. (2000). Automatic recognition of multi-word terms:. the C-value/NC-value method. 
                        International Journal on Digital Libraries, 3(2): 115. 
                    
                    
                        Lamarra, A. and Tardella, M. (2014). Theophilo. A prootype for a thesaurus of philosophy. Lausanne: Digital Humanities 2014. 
                    
                    
                        Mendes, P., Daiber, J., Rajapakse, R., Sasaki, F. and Bizer, C. (2012). Evaluating the Impact of Phrase Recognition on Concept Tagging. Istanbul: Proceedings of the International Conference on Language Resources and Evaluation (LREC 2012).
                    
                    
                        Salton, G., Fox, E. and Wu, H. (1983). Extended Boolean information retrieval. 
                        Communications of the ACM 26.11, pp. 1022-36.
                    
                    
                        Sula, C. and Dean, W. (2014). Visualization of Historical Knowledge structures: an Analysis of the Bibliography of Philosophy. Lausanne: Digital Humanities 2014. 
                    
                    
                        Yanhua, C., Lijun, W., Ming, D. and Jing, H. (2009). Exemplar-based Visualization of Large Document Corpus. IEEE Transactions on Visualization and Computer Graphics (InfoVis2009), 15(6): 1161-68.
                    
                
            
        
    


        
            
                Introduction
                This paper addresses how text-mining, machine-learning and information retrieval algorithms from the field of artificial intelligence can be used to analyze Art-Research archives and conduct (art-) historical research. To gain quick insight into the archive, two aspects are focused on: relations between groups of people using community detection, and global content changes over time using topic modeling. For such archives pre-tagged ground-truth collections are generally not available, and the archives are often too large, geographically distributed, and not always available in digital formats to build such a ground-truth at reasonable costs. To develop and test the validity and relevance of existing tools, close collaboration was established between the AI researchers, museum staff, and researchers in CREATE, a digital humanities project that investigates the development of cultural industries in Amsterdam over the course of the last five centuries.
            
            
                Data
                The research draws on two datasets. The principal dataset is the digitized archive of the Stedelijk Museum Amsterdam, a renowned international museum dedicated to modern and contemporary art and design. The archive of the Stedelijk Museum Amsterdam contains documents from the period 1930-1980. The corpus is a static collection of approximately 160.000 text documents that were digitized using OCR. The second dataset is drawn from Delpher, developed by (Koninklijke Bibliotheek Nederland, 2015). Delpher provides a collection of digitized newspapers, books and magazines that is available for research. A selection of newspapers was made that is used as an additional dataset for this project. Only articles from 1930-1980 that resulted from the query ”Stedelijk Museum” AND ”Amsterdam” were used, forming a set of 18.290 articles.
            
            
                Methodology
                The following methodology uses two approaches to obtain a quick and detailed overview of the content of a digitized archive that contains unstructured information. The first one focuses on the relations between named entities and aims at finding communities in the relation network. The second approach uses time based topic-modeling to get an overview of content changes over time. Finally, a name extraction method is presented that is able to handle multiple causes of name variations.
                
                    Relation networks and community detection
                    In its most basic form, a relation between two named entities can be said to exist when they occur together in the same document. The strength of a relation can be characterized by the number of documents in which both named entities occur. When all the co-occurrences are found, a relation network can be constructed.
                    In addition, sentiment analysis can be done to further characterize a relation. A sentiment score is assigned to each document, indicating the sentiment content of the document. No distinction is made between positive and negative sentiment polarity. The hypothesis is that relations between individuals with a high sentiment are more interesting than relations with a low sentiment. This is because sentiments around trigger-events are often higher than around common-day events. A lexicon based approach is used with lists of language specific sentiment words. The sentiment score of a document is then given by the sigmoid of the count of the sentiment words in the document, normalized by the number of words in the document.
                    Finally, community detection algorithms can be applied to the relation network. These types of algorithms aim at finding clusters of groups of entities that have dense connections between members of the clusters and sparse connections with members of other clusters (Fortunato, 2010). The relation weight measure that is used to calculate the communities, is taken as the product of the strength of the relation, i.e. the number of documents where both entities occur in, and the average sentiment score of the documents of a relation. It was found that combining these two measures, resulted in more meaningful communities.
                
                
                    Time based Topic Modeling
                    In the next approach, topic modeling algorithms are applied to analyze the information content and their evolution over time. Topic modeling tries to discover the underlying thematic structure in a collection of documents. Non-Negative Matrix Factorization (NMF) is being used as a tool for topic modeling (Arora et al., 2012). NMF is an unsupervised method where a matrix is approximated by two low rank non-negative matrices. The extracted semantic feature vectors have only non-negative values and are sparse so they are easily interpretable. Furthermore, NMF is shown to generate more consistent results over multiple runs (Choo et al., 2013), compared to other tools used for topic modeling such as LDA (Blei et al., 2003). 
                    The approach suggested in (Vaca et al., 2014) uses a time-based collective matrix factorization based on NMF and is used in this project. It extends NMF by introducing a topic transition matrix that allows to track topics as they emerge, evolve and fade over time.
                
                
                    Name Extraction
                    The following method was used to extract named entities from a collection of documents in order to build the relation network. It handles different causes of name variations such as OCR induced errors commonly found in digitized document collections, spelling mistakes, name abbreviations and first and last name combinations. 
                    The method makes use of lists of name variations. Starting from a set of names extracted from a name database, such as RKDArtists and (RKD, 2015), the document collection is searched for possible name variations. These variations are found by searching for the last name using a fuzzy search. The similarity between the group of tokens around the found last name, and the original name is then calculated as a similarity score. The similarity score calculation is based on the idea described in (Song and Chen, 2007), which uses a n-gram set matching technique. The lists of name variations can then be evaluated manually or a threshold on the similarity score can be used to identify name variations that correspond to the original name. The method using a threshold of 0.9 on the similarity score was tested on 50 randomly chosen names. The average precision was found to be 81 percent.
                
            
            
                Results
                A relation network was constructed for the document collection of the archive of the Stedelijk Museum Amsterdam. Only artists with the graphic artist qualification in the RKDArtists and database were used. The methods were implemented using available open source software libraries such as the Apache Lucene text search engine library (The Apache Software Foundation, 2015) and the Gephi platform (Bastian et al., 2009). The standard community detection feature in Gephi was used, which is based on the Louvain method (Blondel et al., 2008). The result is shown in Figure 1. The color of the relation between the nodes indicates the average sentiment score of the relation, starting from blue (neutral) to red (high sentiment content). Communities such as group exhibitions, art movements or a group of artists closely related to the museum director, could be identified with the help of a museum expert.
                
                    
                    Figure 1: Found communities for graphic artists in the archive of the Stedelijk Museum
                
                The time based topic modeling algorithm suggested in (Vaca et al., 2014) was implemented in MATLAB and Java. The algorithm was applied to both the archive of the Stedelijk Museum Amsterdam and newspaper articles from the Delpher database. The results are visualized over time in the form of stacked topic rivers (Wei et al., 2010), shown in Figure 2 and Figure 3. Several exhibitions and events could be identified and are annotated on the chart.
                
                    
                    Figure 2: Time based topic modeling for the archive of the Stedelijk Museum Amsterdam
                
                
                    
                    Figure 3: Time based topic modeling for Delpher newspaper articles
                
            
            
                Conclusion
                This paper discusses two approaches to gain insight into a digitized archive. Relation networks of persons with community detection are considered, relying on a robust name extraction method. Furthermore, the evolution of content over time can be explored using time based topic modeling.
                For the humanities researchers in this project, the main aim was to asses the research potential of computational analysis of digitized art archives in general, and the Stedelijk Museum in particular. Two types of preliminary research questions were developed to do so. The first type had to do with identifying patterns of change and continuity, across time and place. These include for instance tracing the position of the Stedelijk Museum as an intermediary in Dutch design industries, or the development of the Stedelijk Museum as an increasingly international player. The second type of question is less concerned with general historical patterns, and more with specific art-historical research questions, regarding for instance (networks of) particular artists, artworks or exhibitions. But before we could start asking such questions to digitized art-historical archives, the quality and accessibility of the texts needed to be established. Secondly, specific methods needed to be explored and adapted in order to clean, identify, retrieve, extract, and structure the texts. The first results presented in this paper demonstrate that even though they may not be clean at the first try or capture all historical nuance, they do help archives to open up and show unexpected relationships and patterns, to answer specific questions, and to get connected with other relevant sources, such RKDartists and Delpher. The community detection in relation with sentiment mining, the topic modeling and name extraction method developed in this project therefore provide a solid basis for the next step in assessing the research potential of art-historical archives: developing in-depth case studies, again in close collaboration with art-historians and historians, allowing the archive to speak up in unprecedented ways, offering access to hidden story lines that subvert and augment prevailing historical narratives.
            
        
        
            
                
                    Bibliography
                    
                        Arora, S., Ge, R. and Moitra, A. (2012). Learning topic models - going beyond SVD. 
                        Foundations of Computer Science (FOCS), 2012 IEEE 53rd Annual Symposium on. IEEE, pp. 1–10.
                    
                    
                        Bastian, M., Heymann, S. and Jacomy, M. (2009). Gephi: an open source software for exploring and manipulating networks. 
                        ICWSM, 
                        8: 361–62.
                    
                    
                        Blei, D. M., Ng, A. Y. and Jordan, M. I. (2003). Latent dirichlet allocation. 
                        The Journal of Machine Learning Research, 
                        3: 993–1022.
                    
                    
                        Blondel, V. D., Guillaume, J.-L., Lambiotte, R. and Lefebvre, E. (2008). Fast unfolding of communities in large networks. 
                        Journal of Statistical Mechanics: Theory and Experiment, 
                        2008(10): P10008.
                    
                    
                        Choo, J., Lee, C., Reddy, C. K. and Park, H. (2013). Utopian: User-driven topic modeling based on interactive nonnegative matrix factorization. 
                        Visualization and Computer Graphics, IEEE Transactions on, 
                        19(12): 1992–2001.
                    
                    
                        Fortunato, S. (2010). Community detection in graphs. 
                        Physics Reports, 
                        486(3): 75–174.
                    
                    
                        Koninklijke Bibliotheek Nederland (2015). Delpher - Boeken Kranten Tijdschriften http://www.delpher.nl/ (accessed 1 November 2015).
                    
                    
                        RKD (2015). Netherlands Institute for Art History https://rkd.nl/en/ (accessed 1 November 2015).
                    
                    
                        Song, S. and Chen, L. (2007). Similarity joins of text with incomplete information formats. 
                        Advances in Databases: Concepts, Systems and Applications. Springer, pp. 313–24.
                    
                    
                        The Apache Software Foundation (2015). Apache Lucene - Welcome to Apache Lucene http://lucene.apache.org/ (accessed 1 November 2015).
                    
                    
                        Vaca, C. K., Mantrach, A., Jaimes, A. and Saerens, M. (2014). A time-based collective factorization for topic discovery and monitoring in news. 
                        Proceedings of the 23rd International Conference on World Wide Web. ACM, pp. 527–38.
                    
                    
                        Wei, F., Liu, S., Song, Y., Pan, S., Zhou, M. X., Qian, W., Shi, L., Tan, L. and Zhang, Q. (2010). Tiara: a visual exploratory text analytic system. 
                        Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, pp. 153–62.
                    
                
            
        
    


        
            With this contribution, I would like to discuss how multi-layered visualizations of epistolary networks can contribute to a better understanding of the circulation of illegal literature and confidential ideas between Catholic Tuscany and the Calvinist Dutch Republic. It questions how intellectual exchanges between these two regions maintained a balance between, on the one hand, the necessity to distribute (prohibited) books and to express controversial ideas and, on the other, social control and the need to avoid the objections of powerful political and religious institutions and individuals. This comparative analysis allows for a sharper focus on the differences and similarities on how intellectuals capitalized on opportunities in the social and religious structures to which they were connected. Indeed, they had to deal with the many tensions between the oppressive catholic environment of the court of Cosimo III and the Dutch Republic, already well known for its relative tolerance and freedom of printing (e.g. Touber, 2014). 
            These personal and societal conflicts forced scholars and booksellers to take strategic measures of secrecy and confidentiality, which in turn depended on what Mauelshagen (2003) also called “networks of trust”. If we pose the question how epistolary networks evolved, understanding changing relationships between people, one might provide insights into aspects of confidentiality. For instance, as relationships grew friendlier, correspondence grew in confidence and trust, while on the other hand one did not correspond with adversaries (Heuvel, et al. 2014). Other examples include studies of Lux and Cook (1998), who claimed that the success of the Dutch Republic depended on what Granovetter (1973) also called “weak ties” instead of central hubs. This implies the importance of intermediaries between communities for the faster distribution of ideas: if you have a particularly close friend from another community, you are more likely to introduce him to your other close friends whom you know that will trust you (Barabási, 2009: 55). 
            If we wish to verify these statements and to understand how intellectuals were able to overcome these confessional and social barriers, the analysis of multi-layered networks of correspondences provides a very interesting addition to archival research. Or in other words, the combination of methods for network analysis for distant reading of large sets of letters with close reading devoted to detect the role of secrecy and confidentiality in epistolary exchanges strengthens historical research. This means that qualitative analysis will uncover how social relations are represented and constructed, sometimes reinforced and sometimes transformed, which is enriched by traditional hermeneutic methods to focus on specific religious and personal features that have influenced those dynamics.
            It is important to consider that full data integration, in particular when dealing with early modern letters, is impossible for reasons of incompleteness, complexity and uncertainty in data. Therefore the focus should not be on analytical and statistical methods of network representations alone, but on approaches that allows us to handle, inquire and interpret these complex historical data. We do not need just networks as static representations, but also networks as interactive interfaces (Heuvel, et al. 2016). To this end, the software tool Nodegoat is used to bring together, explore and contextualise these epistolary exchanges. Nodegoat, differently from network visual-analytical tools such as Gephi, is built around data entry, management and curation processes (
                http://mnn.nodegoat.net/viewer). It enables us to explore and to combine historical networks in various configurations, involving a diverse set of actors. This means that not only persons (scholars and booksellers) constitute relationships but also textual objects, like books with their dedicatees and introduction letters. For instance, the overlay of networks highlights those artefacts that played an important role in the establishment of contacts. If we look at their intersection with their function in the network, new opportunities may rise about how to link book dedications to strategies adopted by scholars. The importance of textual objects as participants in networks has also been stressed by Latour (2005). From the correspondence of the Florentine librarian Antonio Magliabechi, for example, it turned out that the Dutch microbiologist Antoni van Leeuwenhoek (1632–1723) chose to dedicate his work the 
                Arcana Naturae Detecta to Magliabechi. By using this strategy, Leeuwenhoek was able to benefit from Magliabechi’s extensive network for the distribution of his work in Italy. Overlaying different networks sheds light on the role of the 
                Arcana in the network of Magliabechi (fig. 1 and 2). For instance, the image illustrates those correspondences in which Magliabechi mentioned the publication of Leeuwenhoek, showing in this way the diffusion of his publication over time. 
            
            
                
            
            Fig. 1 A visualization of networks around the 
                Arcana Naturae Detecta of Leeuwenhoek (the right yellow node) and Magliabechi. Magliabechi, at the centre of this visualisation, is surrounded by other dedicatees (represented in red), their accompanying books (yellow) and the letters in which the 
                Arcana is mentioned (light blue). Hovering over the nodes and ties opens an overview with the different connections, and specifies the nature of the relationships (see fig. 2) 
            
            
                
            
            Fig. 2 Overview of the 
                Arcana Naturae Detecta, linked to the Short Title Catologue of the National Library of the Netherlands (KB)
            
            Moreover, the analysis of relationships between people can provide insight between direct and indirect transfer of confidentiality via intermediaries. In the Mapping Notes and Nodes in Networks project (Álvarez and Heuvel, 2014) it appeared that the overlay of more networks shed light on the evolution of co-citation networks and introduction networks. As correspondents entered networks of epistolary exchange, they did so not in some ideal egalitarian society, where anyone could join simply by writing a letter, but in a world regulated by social norms and rules of etiquette. In short, letters of introduction were often necessary to be admitted into an epistolary network. Following the evolution of introduction letters alongside a citation network and determine whether a shift takes place in the number of intermediates between correspondents (revealing shrinking degrees of separation), could reveal the importance of introductions in the establishment of epistolary networks. 
            
            Furthermore, Nodegoat is used to bring and contextualise epistolary networks by means of data integration from various data resources such as the Short Title Catalogue Netherlands (STCN) and archival research from archives in the Netherlands and in Italy. The STCN has been queried in order to disambiguate objects and to enhance the interoperability of my data. For example, information on books in epistolary networks can be linked directly to the STCN, which allows me to map the unstructured data in the letters (as titles are often mentioned incomplete) to structured data.
        
        
            
                
                    Bibliography
                    
                        Álvares Francés, L., Heuvel, C. van den (2014). 
                        End report Mapping Notes and Nodes in Networks. https://www.huygens.knaw.nl/wp-content/uploads/2015/05/EndReportMNN.pdf> (accessed 18 December 2016).
                    
                    
                        Barabási, A. (2002). 
                        Linked; the New Science of Networks. Cambridge/Massachusetts: Perseus Publishing. 
                    
                    
                        Granovetter, M. S. (1973). The Strength of Weak Ties, 
                        American Journal of Sociology, 
                        78(6): 1360-1380.
                    
                    
                        Heuvel, C. van den, Vugt, I. van, Kessels, G., Bree, P. van (2016). Deep networks as associative interfaces to historical research, The Future of Historical Network Research (peer reviewed book chapter) in 
                        Ashgate (submitted for publication).
                    
                    
                        Heuvel, C. van den et al. (2015), Modeling Confidentiality and Secrecy in Knowledge Exchange Networks of Letters and Drawings in the Early Modern Europe, 
                        Nuncius, 31: 78–106.
                    
                    
                        Latour, B. (2005). 
                        Reassembling the Social, An Introduction to Actor-Network-Theory. Oxford: University Press.
                    
                    
                        Lux, D and Cook, H. (1998). Closed circles or open networks? Communicating at a distance during the scientific revolution, 
                        History of Science, 
                        36(112): 179-211.
                    
                    
                        Mauelshagen, F. (2003). Networks of Trust and Imagined Community of the Learned, 
                        The Medieval History Journal, 6(1): 1-32.
                    
                    
                        Touber, J. J. (2014). Religious interests and scholarly exchange in the Early Enlightement Republic of Letters: Italian and Dutch scholars 1675-1715, 
                        Rivista di Storia della Chiesa in Italia, 2: 411-436.
                    
                
            
        
    


        
            Over the last two decades, an important amount of work has been carried out by cultural heritage institutions to make their collections available online. How are these digitized collections discovered, discussed and shared on the Web?
                
                     “The future of online digitized heritage: the example of the Great War” is a research project conducted by the BnF, the BDIC and Télécom ParisTech as part of the Cluster of Excellence, Pasts in the present, Investissements d'avenir, réf. ANR-11-LABX-0026-01 (Valérie Beaudouin, Philippe Chevallier, Lionel Maurel, Josselin Morvan, Zeynep Pehlivan, Peter Stirling). 
                
            
            The digitized heritage around the First World War (WW1) is an ideal area for such analysis: many digitized collections, centenary anniversary (2014–2018) and an important activism around the Great War. Family, local and militant history are the main motivations of persons that get involved in the history of WW1 (Offenstadt, 2010).
            Who are the users who publish or discuss around WW1 on the web? Are they amateurs, experts, academics? How do they publish, share and comment digitized documents? These questions matter for the development of Digital Humanities, but also to those in charge of managing the collections. We conduct an exploratory analysis to understand how the activity around the war is visible in the digital space.
            Our research consists of two steps:
            
                Identification and categorization of web sites dedicated to WW1 and network analysis of the links between those sources, in order to draw an overall cartography of Web activity on WW1 and to identify the position of amateurs.
                Focus on one of the main nodes in the web cartography (a forum dedicated to WW1), to analyze the forms of circulations of digitalized documents.
            
            We also conducted a series of semi-structured interviews with participants.
            
            
                Corpus and methodology
                The web is ephemeral: working on web archives allows us to rely on a stabilized corpus, which guaranties the reproducibility of the research (Brügger, 2013). Bibliothèque Nationale de France (BnF), in charge of archiving the French web, created a specific web archive collection dedicated to the centenary of the WW1, based on web sources chosen by librarians. The process of archiving is regularly repeated. Our research relies on the November 2014’s archive (9 698 633 Urls for a total size of 800 Gb).
                The first step of our analysis consists of generating an oriented network graph on web archives to study the relations (materialized by hyperlinks) between web sources related to WW1. We can consider a link as a pragmatic activity of citing or referencing sources (documents, web sites etc.), although in our approach we are not able to qualify the exact nature and function of the link (Saemmer, 2015). 
                To make the large-scale graphs readable, the nodes in the graph correspond to the 
                    seed urls chosen by librarians and all the data crawled from each url is agregated to it. Librarians, in charge of web archiving, qualify the producers (or authors) of the websites. Depending on this categorization, we will distinguish institutional websites (public and official, red) and personal websites (personal and associative, blue). 
                
            
            
                Mapping WW1 on the Web
                The network graph allows to evaluate the place of “amateurs” websites compared to institutional sources. 
                Basic characteristics of our network are calculated using Gephi (Bastian et al., 2009). The network consisted of 514 nodes and 3713 edges with an average degree of 7.22. The average network distance between all pairs of nodes (average path length) is 2.78 edges with a diameter (longest distance) of 8 edges. The clustering coefficient (the degree to which they tend to cluster together) is 0.27 and the modularity index is 0.28. Overall, WW1 French network is made of highly connected pages (∼2.7 edges per node) and shows small-world scale-free network properties (Humphries et al., 2008; Watts et al., 1998) with high clustering coefficient, short average path length and a degree distribution following a “power-law” (smallworldness index = 15).
                More than half of the sources (52%) come from personal websites that are involved in WW1 as a serious leisure, but not as a profession. The institutional sites are less present (36 %). 
                To detect influential actors on our network, we use the 
                    degree centrality which is simply the number of direct relationships that an actor has, the sum of outgoing and incoming links. The network is visualized in Figure 1 by using Gephi, Force Atlas 2 algorithm (Jacomy et al., 2014) as layout with node sizes proportional to their degree centrality. 
                
                
                    
                    Figure 1.Cartography of web sites dedicated to WW1 (degree>30)
                
                The two main actors are centenaire.org, the official site dedicated to the Centenary, on one side and pages1418.mesdiscussions.net, a web forum managed by amateurs, on the other. Around them, we can distinguish two clusters: the red gathers the institutional sites, while the blue (bottom) gathers all the personal web sites that are intensely interconnected. The forum (Pages 14-18) has a specific position: although it is immersed in the middle of the amateur sphere, it is well connected to institutional sources, because users of this forum are mediators to institutional ressources. The forum constitutes a community of practice (Lave and Wenger, 1991): questions and answers on one side and discussions on the other are two kinds of interactions that allow to share and elaborate knowledge collectively (Conein and Latapy, 2008). Thanks to the forum, a lot of personal websites emerged, each of them dedicated to a specific regiment of foot. Experts of those regiments are the authors of those website that accumate documents (public and personal documents and photographies) on each soldier, each battle, each place occupied.
                
                Degree centrality considers only direct relationships, we also use Hubs and Authorities, known as HITS (Kleinberg, 1999) algorithm. A hub is defined as an actor that points to many other actors and an authority is defined as an actor pointed by many others. HITS algorithm calculates two scores for each actor, hub score and authority score, in a mutually reinforcing way based on the idea that a good authority must be pointed to by several good hubs while a good hub must point to several good authorities (Table 1). 
                
                
                    
                        Top 5 Authorities
                        Authority Score
                        Top 5 Hubs
                        Hub Score
                    
                    
                        memoiredeshommes.sga.defense.gouv.fr
                        0.0199
                        pages14-18.mesdiscussions.net
                        0.0379
                    
                    
                        centenaire.org
                        0.0135
                        centenaire.org
                        0.0276
                    
                    
                        crid1418.org
                        0.0127
                        guerre1418.fr
                        0.0251
                    
                    
                        chtimiste.com
                        0.0125
                        combattant.14-18.pagesperso-orange.fr
                        0.0175
                    
                    
                        gallica.bnf.fr
                        0.0124
                        verdun-meuse.fr
                        0.0163
                    
                
                Table 1 : Hubs and Authorities
                Some authorities, like memoiredeshommes.sga.defense.fr and gallica.bnf.fr, have a very specific profile: as documents warehouses, they receive a lot of links but do not point to other resources. The Centenary website is at the same time a top level authority and hub. The forum, pages1418.mesdiscussions.net, and other personal websites are hubs that point to a relatively large number of authorities.
            
            
                Forum activity of citations
                Based on the specific position of the forum, we decided to focus on it. This forum, founded in 2004 by an amateur, has gradually become a reference site in terms of exchanges and discussions on the WW1. It is a highly active platform with about 400,000 messages in 10 years and about 18,000 subscribers.
                
                    
                    Figure 3. Number of messages published in the forum by year
                
                This forum was archived by BnF in January 2015 and we rely on this corpus for our work. For analysing the activity of citing, we classified the citations into four categories: 
                
                    Message_Citation: using a part of previous message 
                    Quote: text inserted using another source
                    Links: hyperlink by using 'link' tag
                    Images: hyperlink by using 'img' tag
                
                As shown by Figure 4, while the usage of quote and message_citations stays stable, the usage of links and images increases over time according to the increase of digitized documents available on line. 
                
                    
                    Figure 4. Citations distribution over time
                
                In the corpus, we identified 255,374 image or link citations, an average of 1 citation for 2 messages. We extracted their domain name. The ten more cited domains, which represent 60% of total links, are shown in Table 2. 
                
                    
                        Netloc
                        Fréquence
                    
                    
                        images.mesdiscussions.net
                        91990
                    
                    
                        pages14-18.mesdiscussions.net
                        9636
                    
                    
                        www.memoiredeshommes.sga.defense.gouv.fr
                        8984
                    
                    
                        gallica.bnf.fr
                        6721
                    
                    
                        www.asoublies1418.fr
                        4979
                    
                    
                        usera.imagecave.com
                        4274
                    
                    
                        74eri.canalblog.com
                        4019
                    
                    
                        www.servimg.com
                        2674
                    
                    
                        imageshack.us
                        2592
                    
                    
                        largonnealheure1418.wordpress.com
                        2407
                    
                    
                        perso.orange.fr
                        1916
                    
                    
                        www.casimages.com
                        1588
                    
                    
                        www.memorial-genweb.org
                        1574
                    
                    
                        www.hiboox.fr
                        1492
                    
                    
                        www.pages14-18.com
                        1387
                    
                    
                        pagesperso-orange.fr
                        1372
                    
                    
                        perso.wanadoo.fr
                        1200
                    
                    
                        albindenis.free.fr
                        1129
                    
                    
                        images.imagehotel.net
                        1041
                    
                    
                        images4.hiboox.com
                        1016
                    
                
                Table 2. Most frequent hosts extracted from url citations
                The most notable result is the importance of image hosting services. Instead of giving a direct link to an online source, people use hosting services (ex: images.mesdiscussions.net). We can estimate that more that 100 000 citations (40% of total) are of this kind where almost half of them point to images. Users are not confident with the life of web sources. They prefer to download and post the picture than to point to a link which may disappear. The image, in this case is directly available and visible into the post. 
                Secondly, the forum itself is the most cited website: due to his long history and intense activity, a lot of questions have already been answered. An activity of knowledge management consists in answering a question by signaling an old topic about the same topic.
                
                    Memoire des hommes and 
                    Gallica are the most cited institutional sites. The first one is a gold mine for genealogists who search for ancestors dead during the war and for historians interested in the battles and regiment history. Gallica is a huge warehouse (4 millions of documents): there is a need to identify what kind of documents people are looking for. We extracted the titles of the documents cited and made a content analysis with Iramuteq (Reinert, 1993) (Figure 4); we identified three kinds of documents: photographs and newspapers, official documents (“journaux officiels”, “décrets”) and documents related to the history of regiments (“historiques des Régiments”) from the department of Defense. 
                
                
                    
                    Figure 5. Text Mining clustering of title documents cited from Gallica
                
                Users of the forum also cite a lot of personal web sites dedicated to specific aspects of the war: the history of the soldiers from a specific regiment or squadron for example.
            
            
                Conclusion 
                To understand what internet users do with digitized archives, we explored systematically the websites discussing WW1. We developed an original methodology combining web and text mining methods applied to web archives. This method is transferrable. It includes expert advice to qualify the relevant variables for qualifying and selecting sources.
                Our analysis shows a great involvment of amateurs (more than half of the websites) in the memory of WW1. They participate to a network of personal websites that gives a specific vision of WW1, more focused on soldiers, regiments, geographic places, objects, remains of war, close to micro-history. A the core of this network, we find the forum, which is the place for interactions and discussions on documents, data, interpretations. 
                In doing research with digitized resources, amateurs collectively increase their professionalism. Although they do not have the status of academics, they acquire methods in exploring and citing their sources, commenting and sharing with other users. Those amateurs, in doing research with digitized resources, play a major role by discovering and citing institutional heritage documents, adding to their value and creating networks and resources.
            
        
        
            
                
                    Bibliography
                    
                        Bastian M., Heymann S. and Jacomy M. (2009). Gephi: an open source software for exploring and manipulating networks,
                         International AAAI Conference on Weblogs and Social Media.
                    
                    
                        Brügger N. (2013). Historical Network Analysis of the Web, 
                        Social Science Computer Review, 31(3): 306-21.
                    
                    
                        Conein B. and Latapy M. (2008). Les usages épistémiques des réseaux de communication électronique: Le cas de l’Open-Source, 
                        Sociologie du Travail, 50(3): 331-52.
                    
                    
                        Humphries, M.D. and Gurney, K. (2008). Network 'small-world-ness': A quantitative method for determining canonical network equivalence, 
                        PLoS ONE,3(4), e0002051.
                    
                    
                        Jacomy, M., Venturini, T., Heymann, S. and Bastian, M. (2014). ForceAtlas2, a Continuous Graph Layout Algorithm for Handy Network Visualization Designed for the Gephi Software. PLoS ONE 9(6), e98679. doi:10.1371/journal.pone.0098679
                        
                        Freeman, L. C. (1977). A Set of Measures of Centrality Based on Betweenness, 
                        Sociometry, 40(1): 35-41.
                    
                    
                        Kleinberg, J. (1999). Authoritative sources in a hyperlinked environment.  Journal of ACM, 46(5): 604-32. DOI=http://dx.doi.org/10.1145/324133.324140
                    
                    
                        Lave, J. and Wenger, E. (1991). 
                        Situated learning: legitimate peripheral participation, Cambridge: Cambridge University Press.
                    
                    
                        Offenstadt, N. (2010). 
                        14-18 aujourd’hui - La Grande Guerre dans la France contemporain, Paris: Odile Jacob.
                    
                    
                        Reinert, M. (1993). Les “mondes lexicaux” et leur “logique” à travers l’analyse statistique d'un corpus de récits de cauchemars », 
                        Langage et société, 66: 5-39.
                    
                    
                        Saemmer, A. (2015). Rhétorique du texte numérique: figures de la lecture, anticipations de pratiques, ENSSIB.
                    
                    
                        Watts, D. J. et Strogatz, S. H. (1998). Collective dynamics of ‘small-world’ networks, 
                        Nature, 393(6684): 440-42.
                    
                    
                        Wenger, E. (1998). 
                        Communities of Practice: Learning, Meaning, and Identity, Cambridge, U.K.: Cambridge University Press.
                    
                
            
        
    


        
         
            Introduction
            Construction and analysis of social networks for historical figures has lately become a popular approach in History and Prosopography (Keats-Rohan, 2007), Sociology (Wetherell, 1998; Roldán Vera and Schupp, 2006) and digital humanities (Rochat, 2015; Yamada, 2015). This approach is especially beneficial for providing a global view and automatic mathematical and statistical analysis for large historical corpora (Rossi et al., 2013), for which researchers are unable to gain much knowledge by even an exhaustive manual exploration. 
            Jewish Biblical and Rabbinic literature is a great source of ancient wisdom and cultural heritage. It includes a large amount of people such as prophets, political and religious leaders, sages and other historical figures. Amazingly, although these people were spread over the world and through different time periods, they were united and connected by the same text - the Bible. Therefore, the aim of this research is to propose and implement a methodology for construction of a cross-generation social network for Jewish sages to explore their inter-relationships on a large scale, using modern computerized tools for text analysis and graph mining (Rossi et al., 2013; Yamada, 2015). 
            
            
            The proposed methodology
            At the first stage we define the corpus of the study and a reliable digital resource for this corpus. We work with the text of Mishna (2
                nd century CE) and Talmud (4
                th-5
                th century CE). Next, the following information is retrieved from existing traditional research sources, such as encyclopedia of Jewish sages (as most of these sources have not been digitized, the person-related data is extracted manually and stored in the digital form):
            
            
                A list of sages for the selected corpora. One of the biggest challenges with sages’ names is their ambiguity and a large number of namesakes (Rochat, 2015; Keats-Rohan, 2007). To tackle this problem we add identifying discriminative features to each name (e.g. father’s name or place of birth). 
                A list of basic relationships between sages, e.g. family relationships, teacher-student, time period, place, possessing a similar political/social/professional role, studying in the same institution, participation in the same event. 
            
            Finally, the above basic relationship list can be further extended with text-based relationships, such as sages who cite each other, disagree, or comment on the same section of the biblical text. This is achieved by automatically learning lexical patterns in which pairs of sages co-occur in texts and using them to extract the corresponding relations. 
            
                
                Figure 1: A fragment of the cross-generation social network for Jewish sages
            
            All the extracted data from multiple independent resources are digitized and integrated in the single database and can be queried and visualized by the common tools (e.g. Gephi (Bastian et al., 2009)). Figure 1 illustrates a fragment of the proposed type of the cross-generation social network for Jewish sages. Complex queries can be further answered by mining the network, e.g. whether a given pair of sages are related and how? What are all the various direct and indirect relationships of a given sage? Whether the same text segment cites sages from different time periods (meaning that it has been edited at a later period)? At the global level the social network helps identify the central figures/communities of the sages in different places, times, schools, dynasties, philosophical approaches, text segments, and citations according to the number of network relationships and their density, centrality and coreness (Rochat, 2015; Keats-Rohan, 2007). The historical data in the built network becomes accessible to researchers from the humanities and will take their research capabilities to the next level. 
            
        
        
            
                
                    Bibliography
                    
                        
                        Bastian, M., S. Heymann, and M. Jacomy. (2009). Gephi: An Open Source Software for Exploring and Manipulating Networks. In 
                        Proceedings of the AAAI Conference on Weblogs and Social Media, Eytan Adar et al. (Eds.), Menlo Park: AAAI Press, pp. 361-62.
                    
                    
                        
                        Barber, Michael J. (2007). Modularity and Community Detection in Bipartite Networks. 
                        Physical Review E, 76: 1-9.
                    
                    
                        
                        Keats-Rohan, K.S.B. (2007). 
                        Prosopography Approaches and Applications: A Handbook. Oxford: Linacre College Unit for Prosopographical Research.
                    
                    
                        Rochat. Y. (2015). Character network analysis of Émile Zola’s Les Rougon-Macquart. In
                        Proceedings of DH 2015, Sydney.
                    
                    
                        Roldán Vera, E. and T. Schupp. (2006). Network analysis in comparative social sciences. 
                        Comparative Education 42(3).
                    
                    
                        Rossi, F., Villa-Vialaneix, N. and Hautefeuille, F. (2013). Exploration of a Large Database of French Notarial Acts with Social Network Methods. 
                        Digital Medievalist, 9.
                    
                    
                        Wetherell, C. (1998). Historical Social Network Analysis. 
                        International Review of Social History, 43: 125-44.
                    
                    
                        Yamada, T. (2015). Detection of People Relationship Using Topic Model from Diaries in Medieval Period of Japan. In
                         Proceedings of DH 2015, Sydney.
                    
                
            
        
    


        
            
                Faceting and Mining Networks 
                 The network graph is one of the best-known and overdetermined of all data visualizations. And it suffers, more than most such modes, from the problem of fetishization. When non-specialists see network graphs, which are information-heavy, aesthetically appealing, cognitively suggestive, and yet curiously hard to read, their first reactions are often along the lines of “Oh wow!”, “Cool!”, or “Neat!”. The corollary to admiration, for members of the academy, however, is often distrust.
                
                     The problem is two-fold. Critical thinkers are justly skeptical of enthusiasm, since enthusiasm can foreclose critical engagement. But the network graph’s prominence as a mode of data visualization, as featured in new outlets like the New York Times, or in prominent projects like
                    Mapping the Republic of Letters 
                    or 
                    Six Degrees of Francis Bacon
                    , makes it uniquely vulnerable to the hermeneutic of suspicion (see Elijah Meeks, “The Digital Humanities as Lightning Rod”). If the digital humanities are 
                    still 
                    distrusted in some more conservative corners of the academy — such as mine, eighteenth century english literary manuscript studies — then network graphs, because they have become synecdochic for digital humanities writ large, are doubly distrusted. 
                
                 This short paper addresses three states of addressing this skepticism head-on, embedding network graph literacy in the context of a larger disciplinary argument. Those three stages correspond to three key perspectives practitioners of network analysis can assume when wanting to persuade skeptics of the probative value of network graphs, to show how condign those graphs are to traditional “analogue” analysis, and to build a persuasive argument within your own field.
                 Those three perspectives are: targeted data gathering; graph design, and argument design.
                 Much of my own work centers on networks of distribution of Jacobite manuscript poetry from 1688 — 1750. Once I knew that I would be using network graphs to illustrate the richness of the social and material facts I had uncovered, I started enriching my dataset with an eye to encoding metadata to reveal those richnesses as precisely as possible. Those metadata categories include size of manuscript, number of copies of each poem in circulation, names of manuscript collectors, languages contained in each manuscript, dates of manuscript, and so on. I entered this metadata on each future node in my two sets of network graphs (one of individual poems, one of manuscripts).
                 Graph design, the next stage, was just as important. Having settled on a layout in Gephi that I felt reliably showed relations among objects in a way that I knew I could explain simply (such as proximity as a function of similarity; distance as an index of relative dis-similarity) I then worked to combat the visual fetish character of the network graph by partitioning the nodes multiple times while keeping them within the same layout. This meant that one my audience had had time to accustom themselves to the layout, successive graphs with differing color schemes, node sizes, and edge weights would show them the facets, the richness, of the dataset. By showing the same graph layout faceted in multiple different ways, my audience could comprehend the layout as a function of multiple and overlapping functions.
                 Argument design, the last stage, is a way of making the procession of network graphs an intuitive progression of visual arguments that draws the audience into, and educates them about, the rhetoric of the network graph precisely by showing its malleability and, in some ways, its very partiality. By showing the protean capacities of the network graph, by showing the extent to which each network graph is itself a reading, skeptics more at home with paleography and stemmatology are able to grasp quite intuitively the resource that is faceted network graph: a flexible and powerful tool to show the richness and interrelatedness of large datasets based on archival discoveries very like their own.
                 This short paper will include slides of my own faceted network graphs to show how we can use this form of argumentation as a kind of nested pedagogy.
                 I enclose here a collection of faceted and mined network graphs made for a colleague’s project on Modernist journals. These bimodal graphs show the interrelation of different contributors to various roughly contemporary journals. By showing the same layout several times, but using node color, node size, edge weight and color, we are able to see:
                which nodes are journalists and which are journals;
                nodes clustered with a granularity of 1.0 (fewest communities of the largest size);
                nodes clustered with a granularity of 0.23 (many smaller communities);
                which journalists contributed the most (edge weight), which journals had the most contributors (node redness and size), and
                which journalists contributed to the most journals (by node size).
                These five network graphs taken together not only supply a much richer picture of the system of writing and publication than a single graph would; by showing the range of information that can be displayed on that graph with a series of simple adjustments, the reader is shown how the graph is a product not of vulgar scientism but of intentional humanism.
                
                    
                        
                        Illustration 1: Lilac nodes are journalists; blue are journals
                    
                
                
                    
                        
                        Illustration 2: Modularity at granularity 1.0; the fewest communities of the largest size.
                    
                
                
                    
                        
                        Illustration 3: Modularity class 0.23; more communities of a smaller size.
                    
                
                
                    
                        
                        Illustration 4: Edge weight shows number of of contributions by a journalist; node redness and size indicates number of contributors.
                    
                
                
                    
                        
                        Illustration 5: Node size indicates number of journals to which a journalist contributed.
                    
                
            
        
        
            
                
                    Bibliography
                    
                        Mapping the Republic of Letters.  (accessed March 7th 2016).
                    
                    
                        Meeks, E. (2012). The Digital Humanities as Lightning Rod.  (accessed March 7th 2016).
                    
                    
                        Six Degrees of Francis Bacon. http://sixdegreesoffrancisbacon.com (accessed March 7th 2016).
                    
                
            
        
    


        
            Visualizations have a central role in the Digital Humanities. The second most popular author-chosen topic word of DH2015 was visualization (cf. Figure 1 from Weingart, 2015). Yet, when one revisits all accepted abstracts at DH2015 with the keyword “vis(z)ualization”, one may notice that not many of these texts indicate which libraries were used for interactivity (only some mention D3), and even fewer had direct links to their websites for testing (many were prototypes). 
            
            
                
                Figure 1. Fragment from graphic with the topical coverage of DH2015 (Weingart, 2015)
            
            One of the more common visualizations are those of relational data. In Katrien Verbert’s more thorough survey of interactive visuals in DH2014's presentations, relations-visualizations represent 50% of all prototypes (that is 29 out of 58). The most popular way to represent relationships are uni- or bi-directional graphs (23 out of 29) and only one of them used a matrix to display connections (see figure 2 from Verbert, 2015). To crack open the discussion on the pros and cons of this visualization technique, I will show how I answered some questions of cultural history, more specifically of Latin American Literary History, with a tailored interactive matrix (to this the visualizations visit: 
                http://www.sgutierrez.seewes.de/).
            
            
                
            
            Figure 2. Visualization techniques used by work presented at DH14 (Verbert, 2015)
            Theoretical Framework
            The history of associations is a goldmine for the intellectual queries of scholars interested in literary and intellectual history. In Latin America, for instance, the appearance of Angel Rama's posthumous book 
                The lettered city (1984), lead to a series of studies concerned with the constitution of enlightened groups, especially in new nation-states. As the capital city of one of the most powerful ex-Spanish Colonies, the lettered network in Mexico City makes an interesting case study. However, despite the valuable monographic studies on this subject (most notably Perales Ojeda, 2000 and Sánchez, 1951), which register around 200 active literary societies during the 19
                th century, no overview on the subject has been possible and not all questions have been resolved. How diverse or homogeneous were these groups? Who were their most recurrent actors? Were certain generations more likely to be part of groups from a certain literary movement? I will propose a way of using visual and interactive displays of literary societies’ membership data to answer these three questions.
            
            Before me, others have sought to gain new insights by exploring the possibilities of data modeling to understand modern sociability. The 
                Berliner Klassik Gesseligkeit Datenbank (The Societies Database of the Berlin Classical Period, 2013) aimed to understand the cultural bloom of the early 19
                th century and Stanford’s 
                Salons Project (2012) was designed to get an understanding of the social composition of the French Enlightenment network. However, to date, there are no online dynamic visualizations of either of these projects.
            Methodology
            a) data collection
            The network's information was obtained by scrapping each associations' entry in the Encyclopedia of Mexican Literature (ELEM). Since ELEM is the most complete source of biographical data for 19th century Mexican writers, it is very unlikely that information about these writers can be found elsewhere; thus, I only considered members with an entry in this source. This procedure leaves out many characters, but it is at least representative of the known characters of the lettered city. It contains information of 51 associations (founded between 1808 and 1894) and of 195 members born between 1781 and 1870.
            b) data model
            The database derived from these two nodes (members and associations) was modelled to answer my research questions, but its metadata is designed to be reusable: members were assigned standard identifiers using Jeff Chiu's VIAF reconciliation service for OpenRefine (Chiu, 2015), and neutral aspects about these nodes -- such as birth and death dates or founding and closing dates—were included. In addition to these neutral aspects, I added two categories that scholars have used to cluster literary characters and societies, namely, generations and literary movements.
            c) visualization
            My first attempt was to follow the most common visualization for networks in the digital humanities, the Gephi-spaguetti (see figure 3). I did everything I could to enhance readability. I set the societies-nodes’ size according to the number of connections they had with other associations and the thickness of the edges to vary depending on how many common members two groups had. Even more, in order to get a chronologically-ordered layout I used Spekkink’s useful plugin, the Event Graph Layout (Spekkink, 2014). From this display, I was able to confirm kinship-relationships between societies. That is, that although persistence was never their 
                forte, when one looks at the number of members that went from an extinct society to the next new one, one gets the impression that despite the ephemeral nature of these groups, there was still a type of continuity among them. 
            
            
                
            
            Figure 3. Network visualization where nodes are 19th century Mexican societies and edges represent the number of common members between them
            Yet, even when I created an interactive graph with Sigma.js it was very hard to read the quantitative differences between my nodes’ connections. On the one side, I was interested in creating a visual display that allowed interactivity, providing end-users with both additional information for each data-point and the possibility to select specific ranges of the network. On the other, I wanted to control the order of my data and the quantities’ color-coding for readability. The solution was provided by a Python-library, Bokeh.
            Results
            The first visual I created was a co-occurrence matrix where each literary association was compared against all others. This display allowed me to understand how many members each pair of associations had in common. In order to enhance the identification of meaningful co-occurrences, I followed the principles of sequential color schemes –where low data values are represented by light colors and high values by dark ones (Wyssen, 2014) – and I assigned different colors and alphas according to the quantities’ distribution of my data: associations’ pairs above or equal to five common members were coded in red, and below five, in blue (see Figure 4). Additionally, I set different and consecutive alpha values to each glyph according to their exact value (intersections of less density had lower alphas). This display was helpful to address the question on the diversity or homogeneity of literary societies: with this tailored visual I was able to identify the homogenous hub of ten literary associations around the 
                Liceo Hidalgo that had a considerable amount of common members, suggesting that although they had different approaches they were nonetheless constituted by recurring members (cf. Figure 5).
            
                
            
            Figure 4. Co-occurrence matrix of literary societies ordered by the sum of common members with other associations
            
                
            
            Figure 5. Selection of societies with the highest common-members’ density
            
                
            
            Figure 6. Associations’ co-occurrence matrix by founding date
            Moreover, when I changed the order of the matrix (by founding date, see Figure 6) I was able to understand these connections in its temporal dimensions. For instance, when zooming on the glyphs for the 
                Liceo Hidalgo (see Figure 7) one can easily identify the previous societies with which this association had enough common members to consider them as predecessors, or which other later groups could be considered as successors for the same reasons. 
            
            
                
            
            Figure 7. Liceo Hidalgos’ co-occurrences, a box-selection of the associations’ matrix by founding date
            Finally, in another color coding of the glyphs (by the literary movement that was in vogue when these societies were established) I could identify which societies of the same period had more common members (see Figure 8). 
            
                
            
            Figure 8. Societies’ co-occurrence matrix with literary movements’ color-coding.
            Conversely, I created another matrix –this time comparing members— which was useful to understand which characters co-occurred more often in the same associations and thus address the second question, namely, which were the most recurrent actors in the network (see Figure 9). The result: thirteen characters formed the core of actors who were most involved (see Figure 10). This information, however, could have been obtained with a simple bar-chart. The difference in perspective that this matrix offers is that it allows the user to see that these characters were not only in many but also similar associations (which can be retrieved by hovering the glyphs), and, additionally, it makes evident how proportionally small this core is when compared to the whole matrix.
            
                
            
            Figure 9. Members’ co-occurrence matrix ordered by maximum summed values.
            
                
            
            Figure 10. Members’ co-occurrence snapshot done with the selection tool of Bokeh’s visuals.
            Finally, to address the second question –namely, the correlation between generations and literary movements–, I created a matrix where associations were ordered by founding date on the y-Axe, and members by birth date on the x-Axe, and where the colors were coded according to their correspondent literary movement (see Figure 11). The dark colors of the glyphs represent the literary movement of a given society (all the blue ones are from the neoclassic movement, for instance), and the light colors in the background represent the members’ generations (for example, in light orange -in a vertical division- are all the members of the 
                Renacimiento generation). Arranging them like this enabled me to take snapshots of different societies and observe the generations’ patterns of membership-adscription. For instance, I could note that although the group formed around the 
                Renacimiento magazine was heavily constituted by its homonym generation (see Figure 12), almost half of its members were born in the timeframe of the previous generation (coded with a light yellow background).
            
            
                
            
            Figure 11. Generations versus literary movements: a co-occurrence matrix
                           
                
            
            Figure 12. Active members in 
                Grupo de la Revista el Renacimiento
            
            Conclusions
            In this paper I have shown how customized visualization of modeled data can enable new readings and lead to new understandings of how societies were formed in a key period of national history. Among other things, matrices help us “see” connections between previous categories of literary history (like generations and literary movements), between societies, but also between members, thus supporting new narratives of the lettered city were the alleged homogeneity of this “elite” group can be seen in a nuanced perspective that integrates complexity without sacrificing abstraction.
        
        
            
                
                    Bibliography
                    Chiu, J. (2015). An OpenRefine Reconciliation Service That Queries VIAF. Java https://github.com/codeforkjeff/refine_viaf.
                    Perales Ojeda, A. (2000).  Asociaciones literarias mexicanas: siglo XIX. 2nd ed. (Al siglo XIX. Ida y vuelta). México: Universidad Nacional Autónoma de México.
                    Rama, A. (1984).  La ciudad letrada. Hanover, N.H., U.S.A.: Ediciones del Norte.
                    Sánchez, J. (1951).  Academias y sociedades literarias de México. University of North Carolina.
                    Spekkink, W. (2014).  Event Graph Layout Wouter Spekkink. http://www.wouterspekkink.org/?page_id=93 (accessed 20 October 2015).
                    Verbert, K. V. K. L. (2015).  On the Use of Visualization for the Digital Humanities. Sydney, Australia http://dh2015.org/abstracts/xml/VERBERT_Katrien_On_the_Use_of_Visualization_for_t/VERBERT_Katrien_On_the_Use_of_Visualization_for_the_Dig.html (accessed 15 December 2015).
                    Weingart, S. (2015). Acceptances to Digital Humanities 2015, (part 2). The Scottbot Irregular http://www.scottbot.net/HIAL/?p=41347 (accessed 23 January 2016).
                    Wyssen, J. (2014). How We Created Color Scales, Website Datavisualization.ch http://datavisualization.ch/inside/how-we-created-color-scales/ (accessed 14 September 2014).
                    (2012). The Salons Project Mapping the Republic of Letters. http://republicofletters.stanford.edu/casestudies/salons.html (accessed 12 November 2014).
                    (2013). Berliner Klassik Geselligkeit-Datenbank Website Berliner Klassik Datenbanken. http://berlinerklassik.bbaw.de/BK/geselligkeit/Suche.html (accessed 25 February 2016).
                
            
        
    


        
            When an earthquake struck Nepal in 2015, the band One Direction sent a tweet encouraging their fans to donate to relief efforts. This one tweet was retweeted a few times, but quickly lost in a flood of other tweets about One Direction’s tour. Simultaneously, an Indian Hindu extremist politician flooded his Twitter stream with rumors that Christian missionaries were coercing conversions from Nepalis in exchange for humanitarian aid. Additionally, an Indian religious group mixed substantial numbers of tweets about a movie they had released with tweets about their relief efforts in Nepal. These are just a few of the users who engaged with the disaster from a distance: they had different motives for tweeting about the disaster, and different levels of engagement with it. We call these users “onlookers:” they tweet about a disaster, but are not directly affected by it.
            This paper analyzes onlooker behavior: it argues that onlookers who will send a few tweets can be predicted by their interests, while onlookers who will tweet heavily about it have few, if any, shared interests. We show that onlookers who primarily tweet about entertainment topics and news topics are likely to mention the disaster, yet send few tweets about it. On the other hand, onlookers who tweet substantially about a disaster after it happens are difficult to identify before the disaster occurs because they do not share common interests aside from the disaster.
            
                Background
                Natural disasters often attract significant attention on Twitter, both by those affected and those who are distant. A substantial amount of research has explored how social media causes users to engage with political, social, and humanitarian problems; however, opinions on social media’s effectiveness—whether it causes users to donate money, stay informed, or participate in campaigns—are mixed. Some argue that displaying concern in social media is more about acquiring social capital than effecting change (Shulman, 2009; Gladwell, 2011; Morozov, 2012; Morozov, 2014), while a Pew Research Center survey finds that social media does create change (Raine et al., 2016). Additionally, many have argued that social media was important though not essential to protests in Egypt (Mazaid, 2011; Tufekci and Wilson, 2012) and other nations (Raine et al., 2016; Shirky, 2011). One analysis found that charities’ use of social media does not increase donations (Malcolm, 2016), while another finds that certain tweeting strategies do (Gasso Climent, 2015) although tweets may not raise awareness about the charity’s causes (Bravo and Hoffman-Goetz, 2015). Where all these studies concur is that social media enable a substantial amount of discourse about crises. The question we explore is how to predict how much attention Twitter users pay to crises: social media presents the opportunity for a user to send a single retweet about a disaster—as many One Direction fans did—or to sustain interest by following other users and sending many tweets about the event over a period of time.
                Additionally, there is little question that social media is useful for those directly affected by disasters. A substantial amount of research finds that social media helps first responders (Regalado et al., 2015; Dugdale et al., 2012; Omilion-Hodges and McClain, 2016; Burns, 2015; Xiao et al., 2015; Kaewkitipong et al., 2016; Meng et al., 2015; Madianou, 2015; Palen, 2008). In fact, specialized algorithms have been developed for that purpose (Pohl et al., 2013a; Pohl et al., 2013b; Platt et al., 2011). Little work examines users who tweet about disasters at a distance, however, despite the large numbers of such users. We examine these onlookers because they produce a large amount of the tweets about humanitarian crises.
            
            
                Method
                We use quantitative text analysis to identify tweets about the earthquake, to cluster onlookers based on shared interests, and to derive a correlation between onlookers’ interests and the number of tweets they sent about the earthquake. This section outlines our methods.
                To attain a broad sample of onlookers, we gathered a dataset of over 5 million tweets sent by around 15,000 users in the three weeks following the Nepal earthquake. We harvested the data from the Twitter REST API by searching for any tweets that mentioned the word “Nepal” from April 24 to May 8. We randomly selected 15,000 users from this set and harvested all of their tweets sent between April 24, 2014 and May 5, 2015. We attempted to capture only English-speaking users to increase the likelihood that we would capture users not directly affected by the earthquake, but we still captured some users who tweeted in multiple languages. This left us with roughly 11,000 onlookers.
                To determine how many tweets a user had sent about the earthquake, we trained a Naive Bayesian classifier using MALLET (McCallum, 2002) on a set of 100 onlookers’ tweets (totaling about 30,000 tweets), marking them as either quake-related or not. We applied the trained classifier to the remainder of the dataset to count each user’s quake-related tweets. Spot checking showed this technique had acceptable accuracy.
                To find shared interests, we used Latent Dirichlet Allocation (Blei et al., 2003), treating all of a user’s tweets as one document. We ran LDA with MALLET with various numbers of topics, and settled on 80. These topics represented a broad span of themes: greetings, news, entertainment, technology, plus four topics directly related to the earthquake. We then looked for connections between onlookers by building an edge list of shared topics, creating a weighted edge between two onlookers if over 25% of both onlookers’ tweets consisted of a shared topic. The edge weight was the product of their affinities to that topic. Using Gephi (Bastian et al., 2009), we then ran a weighted Louvian modularity algorithm (Blondel et al., 2008) over this onlooker graph to generate communities of users.
            
            
                Analysis
                This experiment resulted in 21 communities of onlookers being identified. The communities were labelled using the strongest topics in each. 
                
                    
                        ID
                        Label
                        Average Number of Tweets
                        Users
                        Average Quake-Related Tweets
                    
                    
                        0
                        Foreign language (Spanish)
                        419
                        882
                        9
                    
                    
                        1
                        Japanese Music
                        403
                        135
                        5
                    
                    
                        2
                        Greetings
                        326
                        199
                        5
                    
                    
                        3
                        Portuguese/Fifth Harmony
                        710
                        658
                        7
                    
                    
                        4
                        News Media
                        977
                        11
                        1
                    
                    
                        5
                        News Media
                        652
                        1312
                        22
                    
                    
                        6
                        News/Politics
                        600
                        1236
                        26
                    
                    
                        7
                        Indonesia
                        386
                        416
                        8
                    
                    
                        8
                        Foreign language (unknown)
                        495
                        312
                        5
                    
                    
                        9
                        Unclassified
                        372
                        589
                        25
                    
                    
                        10
                        Dera Sacha Sauda
                        1732
                        54
                        205
                    
                    
                        11
                        News about Russia
                        780
                        30
                        18
                    
                    
                        12
                        Shopping
                        1153
                        226
                        11
                    
                    
                        13
                        Greetings
                        476
                        1010
                        11
                    
                    
                        14
                        Greetings
                        439
                        1347
                        5
                    
                    
                        15
                        Science and animals
                        521
                        108
                        13
                    
                    
                        16
                        One Direction
                        388
                        1085
                        10
                    
                    
                        17
                        Foreign Language (Italian)
                        553
                        47
                        14
                    
                    
                        18
                        TV/Music
                        598
                        722
                        5
                    
                    
                        19
                        Music Videos
                        649
                        14
                        30
                    
                    
                        20
                        Nepal
                        393
                        748
                        125
                    
                
                After pruning out the foreign language communities in the dataset and some that were difficult to classify (Communities 0, 7-9, and 17), we can further group these onlookers into three broad interest groups: Casual Users (Communities 1-3, 12-14, 18, and 19), News and Pundits (4-6, and 13), and Engaged Users (20). We divided these subgroups based on the topics that were strongest in each, but these subgroups correlated with the number of quake-related tweets that each sent. They are described in the table below.
                
                    
                        Category
                        Proportion
                        Quake-Related Tweets/Week
                        Topic Affinities
                    
                    
                        Casual Onlookers
                        46%
                        3
                        Entertainment, greetings
                    
                    
                        News Onlookers
                        25%
                        6
                        News and politics
                    
                    
                        Engaged Onlookers
                        12%
                        10
                        Nepal
                    
                
                
                    Casual Onlookers. Onlookers in these communities showed high activity but low engagement with the disaster, sending an average total of three quake-related tweets a week. Their primary topics of discussion were entertainment, or greetings and positive sentiments. This is the largest group.
                
                
                    News Onlookers. These accounts are either the accounts of professional news outlets or amateur pundits. We find low average quake-related tweets in this group as well: users sent an average of six relevant tweets per week. News outlets generally moved from one topic to another quickly, and pundits only sustained interest in the topics that appealed to them.
                
                
                    Engaged Onlookers. This group sent the most quake-related tweets of all users; the strongest LDA topics in this group were two “Nepal earthquake” topics. However, users in this community had few other topics in common with each other.
                
                This breakdown suggests a model for predicting the number of tweets onlookers send about events. There will be roughly three categories of onlookers: Casual Onlookers, News Onlookers, and Engaged Onlookers. Casual Onlookers will consist of roughly 50% of onlookers, and will send only a few tweets over the first three weeks. Membership in this group is predicted by an interest in entertainment topics. The number of News Onlookers will be half the size of the Casual Onlookers, but they will be roughly twice as engaged. An onlookers’s affinity to this group will be predicted by a general interest in news. Finally, the Engaged Onlookers will send 10-20 times as many tweets as the Casual Onlookers, and will comprise slightly over 10% of onlookers. This group sends the most tweets about an event, but membership in this group cannot be predicted from their preexisting interests.
            
            
                Conclusion
                We find that it is easy to predict shallow engagement with a disaster on Twitter, but difficult to anticipate sustained interest. Onlookers who tweet about entertainment are likely to pass on at least a few messages about donating money because entertainers are likely to post these messages, and fans are likely to retweet them. On the other hand, onlookers who tweet more about an event are likely to have preexisting interests that intersect with a particular aspect of the disaster, but the relevant interests are hard to predict because doing so would require knowing the nature of the disaster ahead of time. For example, to know the Hindu extremist would tweet about rumors of coerced conversions in Nepal, we would have to predict a crisis that would produce such rumors. 
                Additionally, we acknowledge that our model is derived from a single case study. As such, we treat it as provisional pending further experiments. We hope to confirm this model with future work.
            
        
        
            
                
                    Bibliography
                    
                        Bastian, M., Heymann, S., Jacomy, M. et al. (2009). Gephi: an open source software for exploring and manipulating networks. http://www.aaai.org/ocs/index.php/ICWSM/09/paper/viewFile/154./1009 (accessed 22 February 2016).
                    
                    
                        Blei, D. M., Ng, A. Y. and Jordan, M. I. (2003). Latent dirichlet allocation. 
                        The Journal of Machine Learning Research, 
                        3: 993–1022 (accessed 30 June 2014).
                    
                    
                        Blondel, V. D., Guillaume, J.-L., Lambiotte, R. and Lefebvre, E. (2008). Fast unfolding of communities in large networks. 
                        Journal of Statistical Mechanics: Theory and Experiment, 
                        2008(10). doi:10.1088/1742-5468/2008/10/P10008 (accessed 17 October 2013).
                    
                    
                        Bravo, C. A. and Hoffman-Goetz, L. (2015). Tweeting About Prostate and Testicular Cancers: What Are Individuals Saying in Their Discussions About the 2013 Movember Canada Campaign? 
                        Journal of Cancer Education. 1(8). doi:10.1007/s13187-015-0838-8 (accessed 20 February 2016).
                    
                    
                        Burns, R. (2015). Digital Humanitarianism and the Geospatial Web: Emerging Modes of Mapping and the Transformation of Humanitarian Practices Thesis https://digital.lib.washington.edu:443/researchworks/handle/1773/33947 (accessed 20 February 2016).
                    
                    
                        Dugdale, J., Walle, B. Van de and Koeppinghoff, C. (2012). Social media and SMS in the haiti earthquake. ACM Press, pp. 713. doi:10.1145/2187980.2188189. http://dl.acm.org/citation.cfm?doid=2187980.2188189 (accessed 20 February 2016).
                    
                    
                        Gasso Climent, C. (2015). Twitter as a social marketing tool: modifying tweeting behavior in order to encourage donations. Info:eu-repo/semantics/bachelorThesis http://essay.utwente.nl/68039/ (accessed 20 February 2016).
                    
                    
                        Gladwell, M. (2011). 
                        Outliers: The Story of Success. Reprint edition. Back Bay Books.
                    
                    
                        Kaewkitipong, L., Chen, C. C. and Ractham, P. (2016). A community-based approach to sharing knowledge before, during, and after crisis events: A case study from Thailand. 
                        Computers in Human Behavior, 
                        54: 653–66 doi:10.1016/j.chb.2015.07.063 (accessed 20 February 2016).
                    
                    
                        Madianou, M. (2015). Digital Inequality and Second-Order Disasters: Social Media in the Typhoon Haiyan Recovery. 
                        Social Media + Society, 
                        1(2). doi:10.1177/2056305115603386 (accessed 20 February 2016).
                    
                    
                        Malcolm, K. (2016). How Social Media Affects the Annual Fund Revenues of Nonprofit Organizations. 
                        Walden Dissertations and Doctoral Studies http://scholarworks.waldenu.edu/dissertations/2005.
                    
                    
                        Mazaid, P. N. H. and A. D. and D. F. and M. H. and W. M. and M. (2011). Opening Closed Regimes: What Was the Role of Social Media During the Arab Spring?. http://ictlogy.net/bibliography/reports/projects.php?idp=2170 (accessed 15 May 2014).
                    
                    
                        McCallum, A. K. (2002). 
                        MALLET: A Machine Learning for Language Toolkit. Amherst, MA: UMass Amherst http://mallet.cs.umass.edu.
                    
                    
                        Meng, Q., Zhang, N., Zhao, X., Li, F. and Guan, X. (2015). The governance strategies for public emergencies on social media and their effects: a case study based on the microblog data. 
                        Electronic Markets, 
                        26(1): 15–29 doi:10.1007/s12525-015-0202-1 (accessed 20 February 2016).
                    
                    
                        Morozov, E. (2012). 
                        The Net Delusion: The Dark Side of Internet Freedom. Reprint edition. New York: PublicAffairs.
                    
                    
                        Morozov, E. (2014). 
                        To Save Everything, Click Here: The Folly of Technological Solutionism. First Trade Paper Edition edition. New York: PublicAffairs.
                    
                    
                        Omilion-Hodges, L. M. and McClain, K. L. (2016). University use of social media and the crisis lifecycle: Organizational messages, first information responders’ reactions, reframed messages and dissemination patterns. 
                        Computers in Human Behavior, 
                        54: 630–38 doi:10.1016/j.chb.2015.06.002 (accessed 20 February 2016).
                    
                    
                        Palen, L. (2008). Online social media in crisis events. 
                        Educause Quarterly, 
                        31(3): 76–78 (accessed 20 February 2016).
                    
                    
                        Platt, A., Hood, C. and Citrin, L. (2011). From Earthquakes to‘# morecowbell’: Identifying Sub-topics in Social Network Communications. 
                        Privacy, Security, Risk and Trust (passat), 2011 Ieee Third International Conference on and 2011 Ieee Third International Conference on Social Computing (socialcom). IEEE, pp. 541–44 http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6113164 (accessed 17 May 2014).
                    
                    
                        Pohl, D., Bouchachia, A. and Hellwagner, H. (2013a). Supporting Crisis Management via Detection of Sub-Events in Social Networks. 
                        International Journal of Information Systems for Crisis Response and Management (IJISCRAM), 
                        5(3): 20–36 (accessed 17 May 2014).
                    
                    
                        Pohl, D., Bouchachia, A. and Hellwagner, H. (2013b). Social media for crisis management: clustering approaches for sub-event detection. 
                        Multimedia Tools and Applications, pp. 1–32 (accessed 17 May 2014).
                    
                    
                        Raine, L., Purcell, K. and Smith, A. (2016). The Social Side of the Internet | Pew Research Center 
                        Pew Research Center: Numbers, Facts and Trends Shaping Your World http://www.pewinternet.org/2011/01/18/the-social-side-of-the-internet/ (accessed 21 February 2016).
                    
                    
                        Regalado, R. V. J., McHale, K., Dela Cruz, B., Garcia, J. P. F., Ma, C., Kalaw, D. F. and Lu, V. E. (2015). FILIET: An Information Extraction System for Filipino Disaster-Related Tweets. Manila, Philippines: De la Salle University. http://www.dlsu.edu.ph/conferences/dlsu_research_congress/2015/proceedings/SEE/010-HCT_Regalado_RJ.pdf (accessed 20 February 2016).
                    
                    
                        Shirky, C. (2011). Political Power of Social Media - Technology, the Public Sphere Sphere, and Political Change, The. 
                        Foreign Affairs, 
                        90: 28.
                    
                    
                        Shulman, S. W. (2009). The Case Against Mass E-mails: Perverse Incentives and Low Quality Public Participation in U.S. Federal Rulemaking. 
                        Policy & Internet, 
                        1(1): 23–53 doi:10.2202/1944-2866.1010 (accessed 21 February 2016).
                    
                    
                        Tufekci, Z. and Wilson, C. (2012). Social Media and the Decision to Participate in Political Protest: Observations From Tahrir Square. 
                        Journal of Communication, 
                        62(2): 363–79 doi:10.1111/j.1460-2466.2012.01629.x (accessed 15 May 2014).
                    
                    
                        Xiao, Y., Huang, Q. and Wu, K. (2015). Understanding social media data for disaster management. 
                        Natural Hazards, 
                        79(3): 1663–79 doi:10.1007/s11069-015-1918-0 (accessed 20 February 2016).
                    
                
            
        
    


        
            Even before Harper Lee’s “new” book, 
                Go Set A Watchman, was published earlier this year (2015), rumors as to its authorship abounded. Alabama police looked into alleged abuse of Lee’s rights; suspicion suddenly (re)surfaced about the strange fact that one of the greatest bestsellers in American history was its author’s only completed work; Lee’s childhood friendship with Truman Capote (portrayed as Dill in 
                To Kill A Mockingbird) and their later association on the occasion of 
                In Cold Blood fueled more speculations on the two Southern writers’ possible, or even just plausible, collaboration; finally, the role of Tay Hohoff, Lee’s editor on her bestseller, was discussed. Desperate media turned to the usual front for stylometry, Matt Jockers, who graciously ceded this opportunity onto us. A story about our early results appeared in 
                The Wall Street Journal (Gamerman, 2015), and it echoed even in our native Poland, where the country’s major newspaper, 
                Gazeta Wyborcza, also devoted a whole page to this international success of Polish stylometry (Makarenko, 2015).
            
            The truth proved to be at once much less sensational than most of the rumors – and much more interesting. Stylometric evidence is very strong in this case: Harper Lee is the author of both 
                To Kill A Mockingbird and 
                Go Set A Watchman. The first method applied here was part of stylo, a stylometric package (Eder et al., 2013) for R (R Core Team, 2014): series of most-frequent word frequencies in a collection of texts were compared using Burrows’s Delta measure of distance (Burrows, 2002); Delta distances were compared for each pair of the texts in this corpus by cluster analysis, and the results of clustering were used to create a bootstrap consensus tree. The resulting Fig. 1 shows the two Harper Lee books as two nearest neighbors just as it does the other authors included for comparison here. More importantly, perhaps, Truman Capote is far away. Most importantly, her editor’s only available book, 
                Cats and Other People, betrays no similarity to her charge. Since this sort of diagram is oriented at deciphering the strongest signal in word usage, authorship, the various rumors should be finally set at rest – the more so as the two Harper Lee novels have always been each other’s nearest neighbors in a whole series of rigorous machine-learning classification tests performed using stylo’s “classify” function.
            
            
                
                    
                    Figure 1: Harper Lee and selected authors of the American South, compared at 100–2000 most frequent words
                
            
            Lesser affinities between texts are preserved in Fig. 2, which presents a network analysis of the same data treated with an enhanced version of the aforementioned consensus statistical method (Eder, 2015b) and produced with the Force Atlas 2 layout (Jacomy et al., 2014) in Gephi (Bastian et al., 2009). The degree of similarity is shown by the thickness of the curves that connect the particular texts: the thicker the line, the stronger the similarity. Additionally, the algorithm also spatially distributes the nodes (representing each text) to provide an additional visualization effect.
            
                
                    
                    Figure 2: Network analysis of the same collection of novels
                
            
            It is no surprise that this diagram echoes the previous one as far as the strongest similarities are concerned. Lee is still Lee; now, Faulkner stands almost alone. But then the lesser forces, represented by the slightly narrower connections, also count. The first thing that strikes the eye in the Lee neighborhood is the 
                Watchman’s affinity to 
                In Cold Blood and a more heterogeneous pattern for the 
                Mockingbird: the book researched by Capote with Lee is still linked to her 1960 bestseller, but now only by the minutest of lines. This rephrases the Lee/Capote question in a more interesting way. Is there a drop of Capote in Lee? Perhaps not in the entirety of her work – perhaps just in a passage or two. This should be answered with a modification of the method: since it is difficult to see overlapping stylometric signals in an entire novel, one can see much more when the novel is split into equal and smaller fragments; then, the usual stylometric analysis is applied to the particular slices according to the “rolling.classify” procedure (Eder, 2015a).
            
            The most reasonable texts to be thus compared to 
                To Kill A Mockingbird are Capote’s 
                In Cold Blood (since Lee helped with the research for that book), Lee’s own 
                Go Set A Watchman (to see how much of the 
                Watchman might be found in the 
                Mockingbird) and Tay Hohoff’s 
                Cats and Other People (to find out how much of Lee’s rewriting of her original proposal might have been influenced by her experienced editor). This is presented in Fig. 3, and the result is quite interesting.
            
            
                
                    
                    Figure 3: To Kill a Mockingbird contrasted sequentially against Capote’s In Cold Blood (red), Hohoff’s Cats and Other People (blue) and Lee’s Go Set A Watchman (green). The lower band represents the strongest authorial signal; the upper band (in less intense colors) is the second-strongest signal
                
            
            The signal in a little more than a half of the segments in 
                To Kill A Mockingbird is that of the novel she originally brought to be published by Lippincott. It is highly significant that its longest stretch coincides with the trial that was only mentioned in the 
                Watchman and became the focus of the book in the 
                Mockingbird. This seems to suggest that while this refocusing of the book was made following the advice of the editor, the rewriting was indeed done by Harper Lee.
            
            The rest of the 
                Mockingbird is a veritable mosaic of her own and her editor’s hand. Tay Hohoff’s impact seems to be especially visible towards the end of the story, and it coincides with the novel’s climax in Chapter 28: Scout, dressed in her elaborate and cumbersome ham costume, is attacked by Bob Ewell, who, following the struggle with Jem and then with Arthur “Boo” Radley, is left with his own knife stuck under his ribs.
            
            We will never know, of course, whether Tay Hohoff really wrote that scene (and the others that seem to bear her mark) for Lee. But it is sensible to argue that while 
                To Kill A Mockingbird is obviously a novel by Harper Lee, traces of someone who helped her along the way for two whole years – and who, at one point, talked the author into running down to the street to collect the manuscript that had been flung through the window in frustration (Shields, 2006: 121) – must be there somewhere. The results produced by the different functions of stylo are not in conflict when they show the overall strength of the 
                Watchman signal in the 
                Mockingbird and the possible echoes of Hohoff (or even, at the very onset of the novel, of Capote) in selected segments. Rather, they seem to provide new insights into the traces of various people involved in the making of a novel – and into how some of these traces may be identified and discerned by stylometry. It is equally sensible to find such traces in a work of a very particular kind: a novel that has been reprocessed almost beyond recognition in a long process of authorial and editorial collaboration; where the final version keeps the setting and the characters of the first, but changes its focus, its historical moment in time and, perhaps more importantly, its ideological message.
            
        
        
            
                
                    Bibliography
                    
                        Bastian M., Heymann S., Jacomy M. (2009). Gephi: an open source software for exploring and manipulating networks. 
                        International AAAI Conference on Weblogs and Social Media.
                    
                    
                        Burrows, J. (2002). “Delta”: A measure of stylistic difference and a guide to likely authorship. Literary and Linguistic Computing, 
                        17: 267–87.
                    
                    
                        Eder, M. (2015a). Rolling stylometry. 
                        Digital Scholarship in the Humanities, 
                        30, first published online: 7 April 2015, doi: 10.1093/llc/gqv010.
                    
                    
                        Eder, M. (2015b). Visualization in stylometry: cluster analysis using networks. 
                        Digital Scholarship in the Humanities, 
                        30, first published online 3 December 2015, doi: 10.1093/llc/fqv061.
                    
                    
                        Eder, M., Kestemont, M. and Rybicki, J. (2016). Stylometry with R: a package for computational text analysis. 
                        R Journal, 
                        8, first published online 30 December 2015, 
                        .
                    
                    
                        Gamerman, E. (2015). Data Miners Dig Into “Watchman”. 
                        The Wall Street Journal, 17 July 2015: D5, 
                        .
                    
                    
                        Jacomy, M., Venturini, T., Heymann, S. and Bastian, M. (2014). ForceAtlas2, a continuous graph layout algorithm for handy network visualization designed for the Gephi software. 
                        PLoS ONE, 
                        9(6): e98679, doi:10.1371/journal.pone.0098679.
                    
                    
                        Makarenko, V. (2015). Literackie śledztwa Polaków. 
                        Gazeta Wyborcza, 31 July 2015: 18.
                    
                    
                        R Core Team (2014). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Wien, 
                        .
                    
                    
                        Shields, C. J. (2006). 
                        Mockingbird: A Portrait of Harper Lee. New York: Henry Holt and Co. 
                    
                
            
        
    


        
            Jonathan Edwards (1703-1758) is generally considered the most eminent and versatile thinker in early American history. His impact on the shaping of the theological thought and the preaching tradition of the colonial period was profound and long-lasting. Today he remains one of the best studied figures of the American past and different elements of his impressive output are continually reprinted by both academic and commercial publishing houses. Over his life Edwards authored more than a thousand sermons, hundreds of letters and a number of theological treatises. The Jonathan Edwards Research Center at Yale University edited and published most of these texts in their complete form as 
                The Works of Jonathan Edwards, led by Harry Stout as general editor and Kenneth Minkema as executive editor. The series of almost thirty volumes is described by Phillip Gura, a former editor of 
                Early American Literature, as the “most important editorial project in American cultural history in the past 50 years” (2004, 149). Edwards’ life is so well documented that there are hardly any stones unturned in the life of the Northampton divine. Especially, his relevance for the events of the Great Awakening, a powerful social-religious movement of colonial America, underwent close scrutiny and the most notorious sermon of America, “Sinners in the Hands of an Angry God” which he authored, has been the studied linguistically, rhetorically and stylistically. 
            
            Like most people of his age, Edwards was a diligent diary-keeper and an avid letter-writer. His private texts offer a comprehensive insight into his daily struggles and ambitions – in consequence, the very writing and publishing process of his texts is relatively well documented. Yet, surprisingly – in spite of such extensive research conducted upon Edwards – the relationship between him and Thomas Foxcroft (1697-1769), his editor and literary agent has not been extensively studied. 
            Foxcroft was a minister at First Church in Boston, Massachusetts and Jonathan Edwards's ally in the pro-revival debate. Their collaboration began most probably in 1849; Edwards had great trust in his erudition and skill to carry out the authorial intent expressed point-by-point in his commentaries to the suggestions of corrections. Foxcroft sometimes included Edwards's correction verbatim, exactly as indicated by the author, at other times, he paraphrased them, while preserving the author's thought. Edwards entrusted Foxcroft with the editing, the correction and the publication process – as he writes in a letter sent from Stockbridge – a small mission he was sent to after the dismissal from his own parish of Northampton: “I should be glad that you would endeavor that this book may be printed in a pretty good paper and character, and may be printed correctly, and that particular care may be taken that the printer don't skip over a whole line as they sometimes do. And if the bookseller can be agreed with to let me have a number for the copy, it would be pleasing”. (30 June, 1752). Edwards also consulted Foxcroft about the correctness of his interpretation of other authors: "(…) it is very difficult, and almost impossible, for another to enter into all the views of a writer, or to know everything he has in view in all that he says; and therefore a little variation of sentiment, may much thwart and disappoint his design, insensibly to another. But this I should take as a very friendly part and much desire, that if you observe, that in any instances I have mistaken Mr. Williams' meaning, and misrepresented him, or in any respect injured him (…)" (30 June, 1752). The extent to which the style of the editor (whose idiosyncratic style can be described on the basis of numerous publications he himself authored) permeated the author's writings in this case has not been determined. The influence of Foxcroft's thought and style in Edwards's writings seems to be potentially very strong and demands close investigation and the stylometric approach seems a most fitting tool to be employed for such a study. 
            The analysis was performed with two quantitative methods: frequencies of most frequent words were compared between the texts using the Delta procedure (Burrows 2002); then, an analogous procedure (this time using Support Vector Machines) was used to look for traces of the editor’s signal in consecutive segments of several treatises by Edwards (“rolling.classify,” Eder 2015a). The analyses were performed with 
                stylo (Eder et al. 2013), a package for R, the statistical programming environment (R Core Team 2014), postprocessed with Gephi network analysis software (Bastian et al. 2009).
            
            A general view of stylometric similarities and differences between the writings of Edwards and Foxcroft is presented in the network diagram in Fig. 1. It shows, above all, a good separation of the signal of the two preachers, especially when Edwards’s spiritual texts; sermons, treatises and Biblical comments are concerned, these, in turn, exhibit a degree of separation by subgenre – as opposed to Foxcroft’s generally more uniform stylometry.
            
                
                Figure 1. Network analysis of texts by Edwards (red) and Foxcroft (green).
            
            In the more detailed search for the editor’s signal with the “rolling.classify” method, longer texts by Edwards, i.e. his treatises, were compared against his own signal averaged over the rest of his 
                oeuvre and against that of Foxcroft, bearing in mind the suggested caesura of 1749. Sure enough, consecutive segments of Edwards’s works written before that date exhibited no traces of the editor (as exemplified by Fig. 2), and then surfaced in a series of works (as visible in Fig. 3). Interestingly, the editor’s signal disappears again in 1758, the year of Edwards’s (not Foxcroft’s) death.
            
            
                
                Figure 2. Consecutive segments of Edwards's 
                    Mind (1723); throughout the work, Edward's signal (red) dominates over the (absent) signal of Foxcroft.
                
            
            
                
                Figure 3. Consecutive segments of Edwards's Humble Inquiry (1749); in many other fragments, dominated by Edwards (red), Foxcroft's impact is still visible. The lower band shows the strongest signal; the upper, the second strongest.
            
            These results have two significant consequences. The first is that we have now produced a quantitative confirmation of the extent of collaboration between two major colonial authors. But the fact that the quantitative agrees so well with the qualitative (or historical) evidence also shows that editorial traces can indeed be found with stylometry, perhaps to a greater degree than we might have anticipated.
            Acknowledgements
            This research is part of project K/PBO/000331, supported by Poland’s National Science Center.
        
        
            
                
                    Bibliography
                    
                        Bastian M., Heymann S. and Jacomy M. (2009). 
                        Gephi: an open source software for exploring and manipulating networks. International AAAI Conference on Weblogs and Social Media.
                    
                    
                        Burrows, J. (2002). Delta: A measure of stylistic difference and a guide to likely authorship. 
                        Literary and Linguistic Computing, 17: 267-87.
                    
                    
                        Eder, M., (2015a). Rolling stylometry. 
                        Digital Scholarship in the Humanities, 
                        30, first published online 7 April 2015, doi: 10.1093/llc/gqv010.
                    
                    
                        Eder, M. (2015b). Visualization in stylometry: cluster analysis using networks. 
                        Digital Scholarship in the Humanities, 
                        30, first published online 3 December 2015, doi: 10.1093/llc/fqv061.
                    
                    
                        Eder, M., Kestemont, M. and Rybicki, J. (2013). Stylometry with R: a suite of tools. 
                        Digital Humanities 2013: Conference abstracts, University of Nebraska-Lincoln, pp. 487-89.
                    
                    
                        Gura, Philip F. (2004). Jonathan Edwards in American Literature, 
                        Early American Literature 39(1): 147-166.
                    
                    
                        Jacomy, M., Venturini, T., Heymann, S. and Bastian, M. (2014). ForceAtlas2, a Continuous Graph Layout Algorithm for Handy Network Visualization Designed for the Gephi Software. 
                        PLoS ONE, 9(6): e98679. doi:10.1371/journal.pone.0098679.
                    
                    
                        R Core Team (2014). 
                        R: A language and environment for statistical computing. R Foundation for Statistical Computing, Wien, 
                        http://www.R-project.org/.
                    
                
            
        
    


        
            Stylometry based on quantitative analysis of linguistic features such as most frequent words, lemmata, or parts of speech, is a time- and research-proven tool in authorship attribution and plagiarism detection, and is now also used in more general literary studies as part of the distant reading revolution. It has been particularly successful in grouping long texts by their authors in both supervised and unsupervised machine learning tests – and the appeal of this material to stylometrists is understandable in that novels are easily available and easily definable chunks of linguistic (and literary) material, and, despite rumors on the death of the author, most readily associated by the general reader with a single creative figure. And when they are not, discovering the fingerprints of more than one hand in collaborative works is another favorite pastime of stylometrists.
            While perhaps equally avidly studied, the authorship signal in drama is often more problematic. This is probably why the most famous question, that of Shakespearean authorship, is so complex and so hotly contested – as evident, for instance, in a fairly recent debate (Craig and Kinney, 2009; Vickers, 2011; Hoover, 2012). Other difficulties in this genre include the “codification of … literary discourse” in certain literary periods and the fact that the same authors might write drama both in prose and in verse (Schöch, 2013, 2014); also, it may be supposed that, as dramatists create their characters through dialogue, there is a more or less conscious effort on their part to differentiate their style. This last phenomenon has also been researched in novels (Burrows, 1987) and translations of novels and drama, and the results could be equally problematic (Rybicki, 2006, 2007, 2008).
            Even more distortion may be expected in a somewhat similar genre, that of film and TV dialogue – and its textual reflection in intralingual subtitles and interlingual translations. The final shape of filmic speech ascribed to a given screenwriter can be influenced by other agents, such as directors or actors. It can be further transformed in the process of intralingual subtitling, especially that performed by “fansubbers”, who do not necessarily reflect the exact dialogue spoken onscreen. This becomes even more of a problem in the case of interlingual translations, which by nature condense (subtitles, voice-over) or rework (dubbing) the original message, being at times anonymous versions of questionable quality (“fansubs”).
            Quantitative methods have already found their way into audiovisual translation research (Pérez-González 2014; Baños 
                et al., 2013), as exemplified by such projects as Pavia Corpus of Film Dialogue, used to examine sociolinguistic and pragmatic features of dubbed Italian (Freddi and Pavesi, 2009), or Forlixt1, a multimodal corpus which helps to investigate the interplay of verbal and non-verbal semiotics of the film (Valentini, 2006, 2008).
            
            In comparison with the above attempts, our research was done on a specialized corpus of historical films and TV (mini)series. This choice was based on the assumption that the subgenre has unique characteristics which find reflection in film dialogue: namely, it authenticates the represented reality and it also adheres to the codes of realism existing in particular countries. This, in turn, made us look for the same phenomena in (English) originals and (Italian and Polish) translations, since translated dialogue, too, is shaped by stylistic necessities of the genre, culture-specific images of the past dominant in the target context, but also by norms and conventions of audiovisual translation in a given language/culture/country. The exact composition of the corpus is given in the table below:
            
                
                    Original
                    Polish voice-over
                    Polish “official” subtitles
                    Polish fansubs
                    Italian dubbing
                    Italian “official”subtitles
                    Italian fansubs
                
                
                    
                        The Tudors Season 1 (2005)
                    
                    +
                    +
                    
                    +
                    +
                    +
                
                
                    
                        The Tudors Season 2 (2007)
                    
                    +
                    +
                    +
                    +
                    +
                    +
                
                
                    
                        Elizabeth I (2005) (miniseries)
                    
                    
                    +
                    
                    
                    +
                    
                
                
                    Elizabeth (1998)
                    +
                    +
                    
                    +
                    +
                    +
                
                
                    Elizabeth. Golden Age (2007)
                    +
                    +
                    
                    +
                    +
                    +
                
                
                    
                        The Other Boleyn Girl (2008)
                    
                    
                    +
                    
                    +
                    +
                    +
                
                
                    
                        The Private Lives of Elizabeth and Essex (1939)
                    
                    
                    
                    +
                    +
                    +
                    +
                
                
                    Anne of a Thousand Days
                    
                    +
                    
                    +
                    
                    
                
                
                    
                        Wolf Hall (2015)
                    
                    
                    
                    
                    
                    
                    +
                
            
            From the point of view of film and audiovisual translation studies, our research project explores the concept of film/television genre and its distinctive features, focusing on the functions of film dialogue and linguistic/stylistic strategies used by screenwriters to fulfil them (Kozloff, 2000; Jaeckle, 2013). The first stage of our investigation consisted in a contrastive stylometric analysis of the extended Anglophone corpus, composed of both historical and non-historical film scripts, in order to verify our preliminary hypothesis about the genre-related specificity of film dialogue. We proceeded, then, to the analysis of parallel corpora of scripts in all available translations into Polish (voice-over, official and amateur subtitles) and Italian (dubbing, official and amateur subtitles). All this was done with several quantitative methods previously developed and used on other textual material, i.e. literary texts. In particular, frequencies of words from various frequency strata were compared between texts in each of the languages studied using the Delta procedure (Burrows, 2002). The analyses were performed with 
                stylo (Eder et al., 2013), a package for R, the statistical programming environment (R Core Team, 2014), later also postprocessed with Gephi network analysis software (Bastian et al., 2009).
            
            On the basis of these tests several observations could be made. As concerns the screenwriters, they tend to adapt the dialogues to the requirements of historical genre and the presented epoch. This is visible in Fig. 1, where the authorial signal seems to disappear whenever a given writer worked on two films and/or series set in different eras or belonging to a different, i.e. non-historical genre.
            
                
                Figure 1. Network analysis diagram of historical and non-historical scripts.
            
            As concerns audiovisual translations, we arrived at rather unexpected conclusions. We compared versions of individual episodes of TV series and films in Italian (dubbing, subtitles) and Polish (voice-over, subtitles) by analyzing frequencies of single words and part-of-speech 5-grams; the latter measure was a rough approximation of syntax (Górski et al., 2014). As far as Italy is concerned, we noticed an astounding uniformity of style regardless of technique, be it subtitles or dubbing: translations of individual episodes of TV series and films clustered together in analyses of both word and part-of-speech frequencies. By contrast, Polish subtitles and voice-over scripts of the same episodes clustered together for single-word frequencies, while the latter formed their own clusters in part-of-speech 5-gram analysis. This is shown in Fig. 2 and 3.
            
                
                Figure 2. Bootstrap consensus tree for most frequent words in Polish subtitle and voice-over translations in a corpus of Elizabethan-era films and TV series.
            
            
                
                Figure 3. Bootstrap consensus tree for most frequent part-of-speech 5-grams in Polish subtitle and voice-over translations in a corpus of Elizabethan-era films and TV series.
            
            Obviously, the similarities between dubbing and subtitles in Italian may stem from the fact that the latter are based on the former. However, the fact that even amateur subtitles, which usually are published before the release of the dubbed version, show a considerable affinity to dubbing, demonstrates high normalization of the formal language used in Italian historical films and television series. 
            All these results confirm our preliminary hypothesis that film genre influences the strategies of creating film dialogues and their translations. Although we believe that stylometric and computational analysis should not be the end in itself, it seems invaluable in audiovisual translation studies, encouraging closer qualitative analysis of the original and translated scripts. It invites further investigation of such issues as:
            
                the importance of cultural norms and conventions in film translation;
                significant intercultural differences in translation strategies used by subtitlers;
                complex relations between dubbing and subtitles, official and amateur subtitles, voice-over and subtitles
                cultural development of written / spoken language in a given country and the salient stylistic trends in audiovisual translation.
            
            Acknowledgements
            This research is part of project 2013/11/B/HS2/02890, supported by Poland’s National Science Center.
        
        
            
                
                    Bibliography
                    
                        Baños, R., Bruti, S. and Zanotti, S. (eds). (2013). 
                        Perspectives: Studies in Translatology. Special Issue: 
                        Corpus Linguistics and Audiovisual Translation: In Search of an Integrated Approach. 21(4).
                    
                    
                        Bastian M., Heymann S. and Jacomy M. (2009). 
                        Gephi: an Open Source Software for Exploring and
                    
                    
                        Manipulating Networks. International AAAI Conference on Weblogs and Social Media.
                    
                    
                        Burrows, J. (1987). 
                        Computation into Criticism: A Study of Jane Austen’s Novels and an Experiment in Method. Oxford: Oxford U. Press.
                    
                    
                        Burrows, J. (2002). Delta: A measure of stylistic difference and a guide to likely authorship. 
                        Literary and Linguistic Computing, 17: 267-87.
                    
                    
                        Craig, H., and Kinney, A. eds. (2009). 
                        Shakespeare, Computers, and the Mystery of Authorship. Cambridge: Cambridge U. Press.
                    
                    
                        Eder, M. (2015). Visualization in Stylometry: Cluster Analysis Using Networks. 
                        Digital Scholarship in the Humanities, 
                        30, first published online 3 December 2015, doi: 10.1093/llc/fqv061.
                    
                    
                        Eder, M., Kestemont, M. and Rybicki, J. (2013). Stylometry with R: a Suite of Tools, in 
                        Digital Humanities 2013: Conference abstracts, University of Nebraska-Lincoln, pp. 487-89.
                    
                    
                        Górski, R., Eder, M. and Rybicki, J. (2014). Stylistic fingerprints, POS tags and inflected languages: a case study in Polish, in 
                        Qualico 2014: Book of Abstracts. Olomouc: Palacky University, pp. 51–53.
                    
                    
                        Freddi M, and Pavesi, M. (2009). The Pavia Corpus of Film Dialogue: Methodology and Research Rationale; in Freddi, M. and Pavesi M. (eds). 
                        Analyzing Audiovisual Dialogue: Linguistic and Translational Insights, Bologna: Clueb, pp. 95-100. 
                    
                    
                        Hoover, D. (2012). The Rarer They Are, the More There Are, the Less They Matter. 
                        Digital Humanities 2012: Conference abstracts, University of Hamburg. Hamburg U. Press, pp. 218-221. 
                    
                    
                        Jacomy, M., Venturini, T., Heymann, S. and Bastian, M. (2014). ForceAtlas2, a Continuous Graph Layout Algorithm for Handy Network Visualization Designed for the Gephi Software. 
                        PLoS ONE, 9(6): e98679. doi:10.1371/journal.pone.0098679.
                    
                    
                        Jaeckle, J. (ed.). (2013). 
                        Film Dialogue. London & New York: Wallflower Press.
                    
                    
                        Jockers, M. (2013). 
                        Macroanalysis. Digital Methods and Literary History. Champaign: U. of Illinois Press.
                    
                    
                        Kozloff, S. (2000). 
                        Overhearing Film Dialogue. Berkley: University of California Press.
                    
                    
                        Pérez-González, L. (2014). 
                        Audiovisual Translation Theories, Methods and Issues. London and New York: Routledge. 
                    
                    
                        R Core Team (2014). 
                        R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Wien, 
                        http://www.R-project.org/. 
                    
                    
                        Rybicki, J. (2006). Burrowing into Translation: Character Idiolects in Henryk Sienkiewicz's 
                        Trilogy and its Two English Translations. 
                        Literary and Linguistic Computing 21(1), 91-103.
                    
                    
                        Rybicki, J. (2007). Twelve Hamlets: A Stylometric Analysis of Major Characters' Idiolects in Three English Versions and Nine Translations, in 
                        Digital Humanities 2007: Conference Abstracts, University of Illinois, Urbana-Champaign, p. 191.
                    
                    
                        Rybicki, J. (2008). Does Size Matter? A Re-examination of a Time-proven Method, in 
                        Digital Humanities 2008: Conference abstracts, University of Oulu, p. 184.
                    
                    
                        Schöch, C. (2013). Fine-Tuning our Stylometric Tools: Investigating Authorship and Genre in French Classical Drama, in 
                        Digital Humanities Conference 2013, Lincoln, Nebraska, USA.
                    
                    
                        Schöch, C. (2014). Corneille, Molière et les autres. Stilometrische Analysen zu Autorschaft und Gattungszugehörigkeit im französischen Theater der Klassik, in Schöch, C. and Schneider, L. (eds). 
                        Literaturwissenschaft im digitalen Medienwandel, Mainz/Berlin: Philologie im Netz, pp. 130-57. 
                    
                    
                        Valentini, C. (2006). A Multimedia Database for the Training of Audiovisual Translators. 
                        JoSTrans: The Journal of Specialised Translation 6. http://www.jostrans.org/issue06/art_valentini.php.
                    
                    
                        Valentini, C. (2008). Forlixt1: The Forli Corpus of Screen Translation: Exploring Macrostructures; in Chiaro, D. Heiss, Ch. And Bucaria, Ch. (eds). 
                        Between Text and Image. Updating Research in Screen Translation. Amsterdam & Philadelphia: John Benjamins, pp. 37-51. 
                    
                    
                        Vickers, B. (2011). Shakespeare and Authorship Studies in the Twenty-First Century. 
                        Shakespeare Quarterly 62: 106-42.
                    
                
            
        
    


        
            Digital Humanities has seen slow adoption in the Slavic language and literature fields in North American academia. This issue frames our project, the Digital Émigré, a digital resource for exploring Russian émigré periodical literature. Our project has a threefold aim. As periodical studies scholars, we want to enable access to Russian émigré journals for new audiences. As digital humanists, we believe that DH tools and methodologies can facilitate new forms of knowledge about twentieth-century Russian, and more broadly diaspora, literary and cultural history. Finally, as Slavists, we hope our project will be a hub for discussion about the applicability of DH theory and practice for scholars working with Russian-language material.
            At this pilot stage, Digital Émigré is a web-based searchable database of article-level metadata of Russian-language journals published outside of Russia in the twentieth century. Our pilot contains four titles (approximately 100 issues and 1,500 articles): 
                Novoselye and 
                Novyi zhurnal were published in the 1940s in New York, and 
                Sintaksis and 
                Kontinent, in the late 1970s and 1980s in Paris. Our pilot site provides insight into literary culture at both the beginning and end of the Cold War, bookending the twentieth-century Russian diaspora experience. Digital Émigré is intended to scale, and will eventually contain additional titles and new functionality. 
            
            We will highlight the main scholarly avenues that DH methods allow us investigate, such as mapping networks of co-publication, tracking evolving political, social and cultural concerns of émigrés over the course of the Cold War, demonstrating the increased opportunities for émigré women as editors and contributors, and highlighting the proportion of original vs. re-printed work in émigré publications. This way, our project encourages experimentation that will enrich the study of Slavic periodical culture: accessing journals through their data can challenge narratives that are often framed by retroactive canonization, close reading and focus on individual authors. Digital Émigré thereby bridges philological approaches and sociological questions about intellectual networks and communities of artistic production. 
            The poster address the project’s core technical design: our strategy for data modeling and management and database design.  We will also present our plans for next steps, which is to provide full-text access and to federate our titles with other digital periodical collections. For this, we are designing a TEI schema modeled on major periodical studies digital collections -  specifically the Blue Mountain Project at Princeton University (
                http://bluemountain.princeton.edu and the Yellow 90’s Online at Ryerson University (
                http://www.1890s.ca)  
            
            We will also discuss the specific challenges of working with Russian language material and Cyrillic script, such as character encoding, transliteration, translation, and  tokenizing and stemming. These issues can be barriers to success when working with popular DH tools that are developed primarily for Western scripts and languages, and we will show our solutions for using some well-known tools for: data normalization (OpenRefine), text analysis (Voyant), network analysis (Gephi), visualization (Raw, Palladio), and topic modeling (MALLET).
            Digital Émigré is committed not only to the exploration of the intellectual experience of diaspora cultural life. As a digital humanities project, it is itself invested in building intellectual communities around the engagement with this material and its afterlife. It aims to foster contact between scholars working with Russian and other Slavic languages internationally, especially through the discussion of issues of interoperability and creating multilingual digital research environments.
        
    


        
            Gephi is a free and open source network analysis software used, among other things, in social network analysis. This workshop is intended for beginners as well as confirmed users. First, we offer an introduction to the basics of Gephi, then we explore through practice the question of visual network interpretation. We provide a dataset of both Twitter hashtags and Twitter followers graphs on various topics related to the DH community.
            
                Why network interpretation matters
                Reading a network visualization can sometimes be harder than simply producing it. Once the graph has been produced, what are we supposed to look at? Nowadays, it is common to learn how to use social network analysis software such as Gephi via online tutorials, but it is often difficult to learn how to interpret the results. Based on the experience of members of the software development team and Gephi power users, we offer this workshop to help users interpret their results.
                Network visualizations are exploratory rather than explanatory. As a scholar, it is important to leverage network visualization in order to find interesting insights inside your data. Exploring a network requires mobilizing external knowledge on data’s context. Exploration is about generating, and not validating, hypotheses. Networks do not carry a single, clear message, and it is as fruitful for analysis as it is bad for communication. Dispelling this misunderstanding is very important if you want to fully benefit from a tool like Gephi.
                The idea that a tool can analyze things for you is another misunderstanding we can help tackle. Gephi allows you to explore multiple facets of your data, but the interpretation remains to be done by the user him/herself. Users have to spend time with their data, and a workshop is a good place to introduce this data-care principle.
                Once you know what to look for in a network, you will capable of finding insight but you still have to excavate some evidence. Network metrics are more capable of doing so than the visualization itself. In this workshop we will also learn to match visual features with metrics so that you can provide evidence for what you have seen. For instance, observed clusters are proven to be modularity clusters in the sense of Newman (Noack, 2009).
            
            
                Workshop schedule
                
                    Part 1: Visual network analysis in a nutshell
                    We start the workshop with a presentation about why and how we visualize networks (Jacomy et al., 2014) and how we interpret them (Venturini et al., 2015) through a Exploratory Data Analysis method (Tukey, 1977)
                
                
                    Part 2: Gephi practice
                    
                            
                                
                                Figure 1: Participants will learn how to produce a readable Gephi map
                            
                    
                    In this part we explain Gephi through examples. We manipulate Gephi on screen while participants execute the same operations on their computers, using the provided datasetsAckland, R. 2013. “Web social science: Concepts, data and tools for social scientists in the digital age.” SageAckland, R. 2013. “Web social science: Concepts, data and tools for social scientists in the digital age.” Sage (Grandjean, 2015). The complete chain of usage will be addressed by illustrating Gephi features from basics (software installation, layout, data table…) to advanced (computing statistics, filtering, exporting…).
                
                
                    Part 3: Guided practice
                    Each group makes a visualization and wraps it up in a few slides using screenshots (Girard et al., 2015). The trainers provide practical help to participants.
                
                
                    Part 4: Collective discussion
                    Each group presents its findings, and we leverage these live examples to discuss the interpretation process through networks and notably its robustness compared to the visualisation choices.
                    This workshop is supported by DIME-WEB part of DIME-SHS research equipment financed by the EQUIPEX program (ANR-10-EQPX-19-01).
                
            
        
        
            
                
                    Bibliography
                    
                        Girard, P., Jacomy, M. and Plique, G. (2015). Manylines, a graph web publication platform with storytelling features Paper presented at the graph dev room, FOSDEM, Bruxelles, Belgique. https://archive.fosdem.org/2015/schedule/event/graph_manylines/ (accessed 14 March 2016).
                    
                    
                        Grandjean, M. (2015). GEPHI – Introduction to Network Analysis and Visualization, 
                        Martin Grandjean. http://www.martingrandjean.ch/gephi-introduction/ (accessed 14 March 2016).
                    
                    
                        Jacomy, M., et al. (2014). ForceAtlas2, a Continuous Graph Layout Algorithm for Handy Network Visualization Designed for the Gephi Software. (Ed.) Muldoon, M. R., 
                        PLoS ONE, 
                        9(6): e98679 doi:10.1371/journal.pone.0098679.
                    
                    
                        Noack, A. (2009). Modularity clustering is force-directed layout. 
                        Physical Review E, 
                        79(2): 026102 doi:10.1103/PhysRevE.79.026102.
                    
                    
                        Tukey, J. W. (1977). 
                        Exploratory Data Analysis. Addison-Wesley Publishing Company.
                    
                    
                        Venturini, T., Jacomy, M. and Pereira, D. (2015). Visual Network Analysis: the Example of the Rio+20 Online Debate. Working paper. http://www.medialab.sciences-po.fr/wp-content/uploads/2015/06/VisualNetwork_Paper-10.pdf.
                    
                
            
        
    
Introduction
As opposed to the former idea of creative autonomy, in recent years, humanities research tends to investigate cultural contexts and circumstances, inspirational models, and the ways that knowledge, experience and expertise have been transferred over time. We address the question of "creative transfer" within the field of music. Due to the everlasting significance

of musical works, relationships between musicians -the entry point for such an investigation - are well documented in archives, libraries and museums. In print media, usually only a single relation between two musicians is narrated. Furthermore, it is common for the biography of only one of the two musicians to report on the relationship. Larger overviews of social networks between several musicians seldom exist. Although some digital resources exist, these are often reduced to the milieux of popular musicians like Mozart and Beethoven.

Since 2005, musicologists of the project Bavarian Musicians Encyclopedia Online (Bayerisches Musiker Lexikon Online, BMLO) have systematically collected biographical data (an example is given in Figure 1) and examined relationships between musicians from print media - a tedious work that results in a unique database of great value for musicology. The BMLO contains musicians from all kinds of musical professions (e.g., composers, singers, musicologists, instrument makers, ...), most of whom had an active lifetime period living in Bavaria or a considerable influence on Bavaria. Now providing information about around 28,000 musicians, the BMLO has achieved global scope, one that is underpinned by the many musicologists worldwide who use the BMLO for their daily work.



Schumann, Robert (Robert Alexander)

* 8. Juni 1810 Zwickau, t 29. Juli 1856 Bonn Geschlecht männlich

Konfess i o n evangel isch-lutherisch Beziehungen

Schumann, Clara (1819-1896), Pianistin

Wieck, Friedrich (1785-1873), Musikpädagoge Schüler

Nottebohm, Martin Gustav (1817-1882), Musikforscher Wasielewski, Wilhelm Joseph von (1822-1896), Musikforscher

Kollegen

Hauptmann, Moritz (1792-1868), Geiger Mendelssohn Bartholdy, Felix (1809-1847), Komponist

Panofka, Heinrich (1807-1887), Gesangslehrer Chopin, Frédéric (1810-1849), Komponist Joachim, Joseph (1831-1907), Geiger Brahms, Johannes (1833-1897), Komponist

Musikalische Berufe Pianist, Komponist Weitere Tätigkeiten Journalist, Jurist Träger/Sparte Medien

Wirkungsortc Bayreuth, Berlin, Bonn, Colditz, Dresden, Düsseldorf, Frankfurt/Main, Hamburg, Heidelberg, Karlsbad, Köln, Kreischa, Leipzig, Mailand, Maxen, Moskau, München, St. Petersburg, Wien, Zwickau

Figure 1: Biographical information about Robert Schumann

in the BMLO. Alongside information about a musician's lifetime, denomination, professions or places of activity, the database provides a number of relationships by type. Next to his partner Clara Schumann, further relations are listed to Robert Schumann's father in law, to his students, colleagues, and other acquainted musicians in his social network

In earlier works, we developed visual interfaces on the basis of the BMLO data for profiling musicians (Jä-nicke et al, 2016), and for the distant reading of musicians' biographies (Khulusi et al, 2016). However, the social network inherent in the BMLO has remained untouched so far. Using the BMLO, only the social network of single musicians can be observed, as is the case when using print media. In order to facilitate an extensive analysis of the entire social network concealed in the BMLO, we designed a visualization that brings together all of the relationships in the form of an interactive social network graph. In contrast to previous means of investigating the transfer of musical knowledge, we allow for the dynamic exploration of relationships among musicians over generations.

Graph Topology

Information regarding relationships to other musicians in the database is provided for 9,805 musicians of the BMLO. Only one relation exists for around 46,5% of these musicians, and just 261 musicians have ten or more relations. Adolf Wilhelm August Sandberger is the musician with most relations (97). The average number of relations for musicians is 2.6. The resultant graph structure of the social network consists of 1,420 connected components, the largest component connects 5,539 musicians, the second largest only 56 musicians - 1,385 connected components contain less than ten musicians.

Due to the above mentioned topological features of the graph, the typical, straightforward visualization using a force-directed layout approach, e.g., by using tools such as Gephi (Bastian, 2009), leads to a global overview of the social network (see Figure 2). However, local structures are hardly readable, which makes an interactive exploration nearly impossible. The objective of this work was to develop a graph design that makes the social network of musicians visually accessible for the first time, and, moreover, capable of being explored in accordance with the research questions of the collaborating musicologists. We focused on the largest connected component that causes the greatest challenges for this task.


Figure 2: The largest connected component with 5,539 musicians visualized using Gephi.


Graph & Interface Design to Analyze Teacher-Student Relationships

The preliminary step when generating the social network graph is filtering according to the underlying research question. At first, a filtering can be done by relationship type(s). Second, it is possible to focus exclusively on musicians with specific professions (e.g., instrumentalists). In the following discussion, we focus on the motivating example for this work: the analysis of teacher-student relationships to investigate how musical knowledge, experience and expertise have been transferred over time. The corresponding filter keeps 3,994 musicians, the largest connected component of this sub-network - the research object of the musicologists - contains 2,769 teachers and students. The Gephi output for this graph is given in Figure 3.


Figure 3: The largest connected component of the teacher-student network with 2,769 musicians visualized using Gephi.


Although the structures are slightly finer due to the reduced number of nodes and edges, the highly connected part in the interior of the graph remains cluttered. Here, we list our design decisions applied in order to generate a readable graph (see Figure 4) and a navigable interface.


Figure 4: The largest connected component of the teacher-student network with 2,769 musicians (608 nodes) visualized with our method.

• Temporally aligned graph: It was particularly important for the musicologists that the graph layout includes a temporal dimension, so that relations can be chronologically analyzed from left-to-right. Therefore, we applied a force-directed graph layout and used fixed x-values that represent a time-stamp,

which reflects the middle of a musician's creative lifetime (see Janicke 2016), on a horizontal time axis. As a result, the nodes only spread vertically and the chronological order remains intact.

•    Node grouping: Because the underlying research question investigates transfer paths of musical knowledge, we hide the nodes of musicians who never had the role of a teacher. Still, these musicians are grouped to their teachers, and can be accessed in the exploration process. This design decision reduces the number of nodes to be displayed from 2,769 to 608.

•    Node layout: To illustrate the significance and the influence of personalities, the sizes of nodes reflect the number of students of the corresponding teachers, which makes teachers with many students salient. Per default, node labels are hidden, but for navigation purposes, a user-defined number of node labels with the corresponding musicians names can be shown on demand. Either the most popular musicians or the teachers with most students can be highlighted.

•    Interactivity: Hovering over a node shows the corresponding musician and two lists of students (those who became teachers and those who did not) in a popup box. Clicking a node highlights all connections to a teacher's students who became themselves teachers. This way, transfer paths of musical knowledge can be assembled interactively.

•    Musical profession analysis: For the selected (via mouse click) musicians in the graph, the evolution of musical professions can be analyzed. Therefore, all musical professions of the teachers' students are listed by decreasing frequency. For each profession, a bar chart illustrates when they have been pursued.

Analysis of Teacher-Student Relationships
This section outlines a usage scenario of the teacher-student network taking the example of Adolf Wilhelm August Sandberger who established musicology as a subject of study in Munich.

First, we compare Sandberger to one of his teachers, Joseph Rheinberger, both being the teachers with the highest numbers of students (the BMLO lists 97 students for Sandberger and 87 students for Rheinberger). Of special interest was the comparative analysis of the musical professions of their students in order to assess the similarity of both teachers' studentries. Figure 5 shows the two selected teachers in the social network, and a view at the summarized musical professions of their students is given. While composition was the major musical profession of Rheinbergers students (70x), this number drops for Sandbergers

students (52x). On the other hand, the number of musicologists increase (10x    65x). Other significant

changes can be seen for the professions choirmaster (29x    9x), organist (19x    8x), music writer (12x

29x) and music editor (5x    26x). Thus, the visualiza

tion reflects a change of the musical profile of both studentries from composition to composition science- a hypothesis that could be verified with our system.

Second, we examined the change of teaching since Sandberger established musicology in Munich. Therefore, we observed the musical professions of the students of Sandberger and his successors in Munich, Rudolf von Ficker, Thrasybulos Georgios Georgiades and Theodor Göllner (see Fig. 6). While the musicologist is the most frequent taught musical profession, the composer gets less and less important. The last teacher Theodor Göllner even had no student with the composer as musical profession. Thus, the change from composition to composition science that already started with Sandberger compared to Rheinberger, steadily continued with Sandberger's successors.


Figure 5: Comparing the students of Joseph Rheinberger and Adolf Wilhelm August Sandberger.


Figure 6: Temporal change of teaching in Munich.

Conclusion

Through close collaboration with computer scientists and musicologists implementing a user-centered design approach, we developed a visualization that allows for the dynamic, interactive exploration of the social network of musicians, focusing primarily on teacher-student relationships. In contrast to out-of-the-box tools like Gephi, we took the research questions of the collaborating musicologists into account when designing the graph and the user interface. Although detailed information about individual relation periods between musicians as well as the taught musical professions are not included in the underlying database, the provided interface facilitates a novel view on the social network of musicians, which allows to draw conclusions on the question of the transfer of musical knowledge.

The value of our system for users of the BMLO is not only that social networks are visualized for the first time, but also that the graph may be filtered in accordance with the way that specific research questions can be investigated. Next to teacher-student relationships, familial or labor relationships also create valuable networks to be explored. Furthermore, it is possible to analyze sub-networks concerning musical professions, and to combine relationship types with musical professions. For example, when combining teacher-student relationships with the musical profession instrumentalist (see Fig. 7), Wolfgang Amadeus Mozart shows up at the beginning of the instrument playing knowledge transfer.


Figure 7: Teaching instrumentalists.

Bibliography
Bastian M., Heymann S., and Jacomy M. (2009). Gephi: an

open source software for exploring and manipulating

networks. International AAAI Conference on Weblogs and Social Media.

Jänicke, S., Focht, J., and Scheuermann, G. (2016). Interactive Visual Profiling of Musicians. IEEE Transactions on

Visualization and Computer Graphics (TVCG), 22(1):200— 209, Jan 2016.

Khulusi, R., and Jänicke, S. (2016). On the Distant Reading of Musicians' Biographies. In Digital Humanities 2016: Conference Abstracts. Jagiellonian University & Pedagogical University, Krakow, pp. 818-820.

Beaudouin, V., and Pehlivan, Z. (2016). The Great War on the Web: the Making of Citing and Referencing by Amateurs. In Digital Humanities 2016: Conference Abstracts. Jagiellonian University & Pedagogical University, Krakow, pp. 433-436
The Victoria Press began as an outgrowth of SPEW (Society for Promoting the Employment of Women), a group of Victorian feminists who sought to provide new avenues for women’s work in the printing industry. SPEW activists, led by Emily Faithfull, set up a Press where women worked as compositors (Tusan, 2004; Fredeman, 1974). Presswomen printed anthologies, tracts, and feminist periodicals, including the monthly English Woman’s Journal (1858-1864), which published mainly contributions by women. Publication of the English Woman’s Journal (EWJ) spanned “the period between the failed attempt to reform legislation that prevented married women holding property in 1857 and the equally unsuccessful attempt to win female suffrage in 1867” (Mussell, 2008). Articles in the EWJ promoted both of these reform measures and advocated for female employment in different fields, as well as other contemporary feminist causes. My use of the term “feminist,” though anachronistic to the movement, has been accepted by other scholars writing about the period, who choose the word to demonstrate an active and purposeful engagement with the continuous struggle for women’s rights (see Phegley, 2004; Mussell, 2012; Frawley, 1999).

My digital project, The Victoria Press Circle, funded by Princeton’s Center for Digital Humanities, offers open-access network visualizations of the women and men involved in the Victoria Press, based on contents of the EWJ and three anthologies printed at the Press between 1861 and 1863. The Victoria Press Circle’s first aim is reconstruction: the project helps to establish the history of the Victoria Press, since there is no existing archive. This is especially important since the EWJ includes a high percentage of unsigned contributions (about 40 percent). None of the women who published in the EWJ currently have significant digital representation. Identifying them as individuals combats the critical undervaluing of texts in female-produced periodicals, and studying them as a group highlights authors who may not receive attention individually.

Furthermore, this project demonstrates collaboration. A network-focused approach is particularly appropriate because the Victoria Press was constructed on a material model of collaborative female labor. Its founders explicitly attempted to build a hub of social interaction around the Press, creating venues to promote women’s rights. SPEW members saw their office as a meeting place for women advocating for female employment:

It is also the intention of the Society to render their office a depot for information of every kind relating to the employment of women. Curious and interesting facts will be collected. Extracts from newspapers, pamphlets, and speeches on the subject, will be gathered together, and kept for the inspection of members of the Society (EWJ, 1859).

In creating network graphs, I am reconstructing how the Presswomen built a social network for themselves, not imposing intentionality on their project (Weingart, 2013). All of my data will be freely available and downloadable in .csv format for other researchers to access and use.

Ultimately, The Victoria Press Circle’s open-access website will display at least three network visualizations, constructed in Cytoscape, of those involved in the Press: one composite graph for the three anthologies; one graph for the EWJ; and one combined visualization for all the publications. Cytoscape, though a tool designed for biomolecular analysis, is more flexible than Gephi for social network analysis, especially for specific functions of filtering based on node and edge attributes and on network statistics (see Shannon, 2003).

In addition to literary contributors, my visualizations include compositors, engravers, printers, editors, and paper manufacturers. Marianne Van Remoortel (2015) has helped in identifying names of compositors from newspaper reports and census data. While the individual model of many digital archives privileges authors and minimizes others involved in literary production, the women of the Press were working at every level of print culture to advance their social aims. By valuing all types of contributions equally, my visualizations illustrate their collaborative effort. The Presswomen’s project echoes through current debates in digital humanities about the necessity of learning to code for engagement in DH work (Dinsman, 2016). I believe that programming can be a feminist act for scholars, just as involvement in print culture was a feminist act for Victorian female authors. The artisan practice of printing is analogous to the artisan practice of coding, and both are affected by experiences of gender, race, class, and sexuality. I hope to use my project to show how women worked with their hands and their pens in tandem, and how I’m continuing that work today.

Weingart, S. (2013). “Networks Demystified 8: When Networks Are Inappropriate,” The Scottbot Irregular. https://scottbot.net/networks-demystified-8-when-networks-are-inappropriate/ (accessed 15 October 2016).

Bibliography

( ---) (1859).“Association for Promoting the Employment of Women,” English Woman’s Journal IV(19): 59. http://ncse-viewpoint.cch.kcl.ac.uk/ (accessed    10

September 2016).

Dinsman, M. (2016). “The Digital in the Humanities: An Interview with Marisa Parham.” Los Angeles Review of Books. https://lareviewofbooks.org/article/digital-humanities-interview-marisa-parham/ (accessed 8 February 2017).

Frawley, M. (1999). “Feminism, Format, and Emily Faithfull’s Victoria Press Publications.” Nineteenth-century feminisms 1: 39-63.

Fredeman, W. (1974). “Emily Faithfull and the Victoria Press: An Experiment in Sociological Bibliography,” The Library s5- XXIX(2): 1439-164. doi:10.1093/library/s5-XXIX.2.139 (accessed 13 March 2016).

Mussell, J. (2008). “NCSE: English Woman’s Journal (18581864).” http://www.ncse.ac.uk/headnotes/ewj.html (accessed 10 September 2016).

Mussell, J. (2012). The Nineteenth-Century Press in the Digital Age. Palgrave Studies in the History of the Media. New York: Palgrave Macmillan.

Phegley, J. (2004). Educating the Proper Woman Reader: Victorian Family Literary Magazines and the Cultural Health of the Nation. Columbus: Ohio State University Press.

Van Remoortel, M. (2015). Women, Work and the Victorian Periodical: Living by the Press. New York: Palgrave Macmillan.

Shannon, P. et al. (2003). “Cytoscape: A Software Environment for Integrated Models of Biomolecular Interaction Networks,” Genome Research 13(11): 24982504. doi:10.1101/gr.1239303 (accessed 15 October 2016).

Tusan, M. (2004). “Performing Work: Gender, Class, and the Printing Trade in Victorian Britain,” Journal of Women’s History 16(1): 103-126. doi:10.1353/jowh.2004.0037 (accessed 10 September 2016).
Aim

This paper explores the possibilities of applying computer-assisted methods to the field of Nordic manuscript studies, with a special emphasis on a network analysis—in a broad sense—of manuscript context. A case study of one Icelandic legendary saga's manuscript tradition is used to test the hypothesis that the manuscript context can carry information about ethnic genre associations of the text (on ethnic genre in Old Norse literature see: Harris, 1975; on legendary sagas as a genre see: Quinn, 2006).

Research Questions

Hromundar saga Gripssonar traditionally belongs in the corpus of legendary sagas (fornaldarsogur); it was included in the second volume of Rafn's (1829) Fornaldarsogur Nordrlanda, and in Bjorner's (1737) Nordiska kampa dater. The saga as it is known today, however, is a post-medieval re-working of a metrical version of the story known as rlmur (Brown, 1946), and is probably not much older than seventeenth century. Therefore it does not necessarily fit well with the other texts included the corpus of legendary sagas, as they usually date from the fourteenth and fifteenth centuries (Driscoll, 2005:207). This makes Hromundar saga Gripssonar an interesting case study for investigations of the text's genre affiliation in the extant manuscripts preserving the saga. Does it appear frequently in manuscripts with the older legendary sagas or with younger rimur-based narratives? To answer this question, I first examine the manuscript context of legendary sagas as a corpus, based on collaborative research with Rowbotham and Wills (Kapitan et al., 2017); second, I examine the position of Hromundar saga Gripssonar within the corpus and its relationships with other texts.

Much discussion in the field of Old Norse studies centers on whether the legendary sagas deserve to be considered a separate literary genre, or should instead be analyzed as chivalric literature (Quinn, 2006). One of the main reasons for these considerations seems to be the fact that the term fornaldarsogur is not attested in the medieval texts; it was introduced in the early nineteenth century by C.C. Rafn, who published a collection of texts under the title Fornaldarsogur Nordrlanda (Rafn, 1829). Rafn's selection of texts and the definition of fornaldarsogur as a corpus of texts dealing with events taking place in Scandinavia before the settlement of Iceland, however, was not detached from pervious scholarship of early eighteenth century (Lavender, 2015). The current discussion on the legendary sagas as a corpus (or a genre) is polarized around contradicting opinions. Some scholars suggest that the legendary sagas had to be considered a separate category in pre-modern period, because they are frequently bound together in the manuscripts (Gudmundsdottir, 2001:cxlvii; Mitchell, 1991:21) while others, using the same argument, emphasize strong connections between the legendary sagas and the chivalric sagas (Driscoll, 2005:193). Additional problems arise when classifying the generic hybrids (Rowe 1993; 2004) appearing within the corpus, or distinguishing between the legendary sagas (fornaldarsogur) and the late legendary sagas (fornaldarsogur sldari tlma; Driscoll 2005). Even though scholars eagerly turn towards the manuscript context to support their claims regarding the genre classification, no comprehensive overview of the legendary sagas' codicological context has yet been presented. This gave rise to the project Stories for all times conducted at the University of Copenhagen, which created a complete catalogue of manuscripts preserving legendary sagas. The catalogue contains 818 TEI-conformant XML-based manuscript descriptions with over 8000 items, 1764 of which are classified as legendary sagas and 920 as chivalric sagas. This amount of data is much too large to be analyzed manually, therefore it is necessary to apply computer-assisted analysis in order to draw some general conclusions regarding this corpus and the relationships between these texts.

Methods

The first part of my paper, which aims to establish the position of Hromundar saga Gripssonar within the wider context of the manuscript, draws on the network analysis of the corpus, conducted in collaboration with Rowbotham and Wills (Kapitan et al., 2017). There, the codicological context of a text was considered as a system of relationships between texts, and following Hall's (2013) approach in his network of chivalric sagas, texts were represented as nodes, manuscripts as edges, both visualized with the free visualization software Gephi. The second part is based on database queries aimed at obtaining detailed information about particular manuscripts and their contents. The main focus of the analysis was to examine the manuscripts preserving the complete texts of Hromundar saga Gripssonar in Icelandic, therefore the manuscripts containing excerpts and translations were ignored. The distribution of texts appearing frequently alongside Hromundar saga Gripssonar by century has been obtained using XPath queries of the online catalogue Stories for all times. Main Findings

As a result of this research, the hypothesis can be confirmed: generally, texts belonging to one genre appear most frequently in manuscripts with other texts belonging to the same genre. However, an interesting transmission history of Hromundar saga Gripssonar suggests a close association of this saga with the late legendary sagas, and in particular Bragda-Olvis saga. Both Bragda-Olvis saga and Hromundar saga Gripssonar are post-medieval reworkings of older metric versions of the stories (rimur), and for both texts the manuscript AM 601 b 4to (Arni Magnuisson Institute, Reykjavik) was suggested as the witness carrying the best text of the saga (Andrews, 1911; Brown, 1946; Hooper, 1934; Hooper, 1932). Even though Hromundar saga Gripssonar appears most frequently with texts classified as late legendary sagas in pre-1800 manuscripts, after 1800 the texts classified as (traditional) fornaldarsogur start to dominate. The late Bragda-Olvis saga dominates the pre-1800 setting, but the distribution changes in the nineteenth century when Porsteins saga Vikingssonar, Starkadar saga gamla, Fridpjofs saga ins fr&kna, and Halfs saga Halfsreka appear more frequently (as presented on figure below). Starkadar saga gamla is a late-eighteenth century saga written by Snorri Bjornson (1710-1803), utilizing traditional legendary motifs of Saxo's Gesta Danorum and legendary sagas (Driscoll, 2009:209; Simek and Hermann Palsson, 1987:331), so its co-occurrence with other legendary sagas starting from the eighteenth century onwards is not surprising. The three remaining texts that started to appear more frequently alongside Hromundar saga Gripssonar in nineteenth-century manuscripts were all published in the same volume of Rafn's Fornaldarsogur Nordrlanda, in which Hromundar saga Gripssonar was published (volume II); likewise Fridtjofs saga ins fr^kna, and Halfs saga Halfsreka appeared in Bjorner's edition from 1737 together with Hromundar saga Gripssonar. This shows how printed editions influenced the saga's transmission in the manuscript form. A text, which once showed strong connections to another rimurbased narrative, became detached from its previous setting and gained new, print-influenced context after becoming part of printed editions.


Figure 1. Texts appearing frequently with Hromundar saga Gripssonar in manuscripts by century

Relevance

The topic of this paper fits in the advertised panel "Quantitative stylistics and philology, including big data and text mining studies," as it employs database quarrying and network analysis of significant amount of data.

Acknowledgements

The author of this paper owes a great debt to Tarrin Wills for merging the data from various databases (fasnl.ku.dk, skaldic.abdn.ac.uk, handrit.is, onpweb.nfi.sc.ku.dk) to obtain the input data for the network analysis, and to Tim Rowbotham, who worked on standardizing the uniform titles and genre classifications in the XML-files in the Stories for all times catalogue.

Bibliography

Gudmundsdottir, A. (2001). Ulfhams Saga. (Stofnun Arna

Magnussonar A Islandi 53). Reykjavik: Stofnun Arna

Magnussonar a Islandi.

Andrews, A. L. (1911). Studies in the fornaldarsogur

Nordurlanda. Modern Philology, 8: 527-44.

Bjorner, E. J. (1737). Nordiska kampa dater i en sagoflock samlade om forna kongar och hjaltar. Volumen historicum, continens variorum in orbe hyperboreo antiquo regum, heroum et pugilum res praeclare et mirabiliter gestas. Accessit, praeter conspectum genealogicum Svethicorum regum et reginarum accuratissimum etiam praefatio. Stockholmiae: typis J.L., Horrn.

http://books.google.com/books?id=9nZUAAAAYAAJ (accessed 28 January 2016).

Brown, U. (1946). The saga of Hromund Gripsson and Porgilssaga. Saga-Book, 13: 51-77.

Driscoll, M. J. (2005). Late prose fiction (lygisogur). A Companion to Old Norse-Icelandic Literature and Culture. Oxford: Blackwell Publishing Ltd, pp. 190-204.

Driscoll, M. J. (2009). Editing the Fornaldarsogur NorSurlanda. A Austrvega, Saga and East Scandinavia, Preprints of the 14th International Saga Conference Uppsala 9th - 15th August 2009. Gavle: University of Gavle, pp. 207-12.

Gephi. (n.d.) Gephi: The open graph viz platform: https://gephi.org

Hall, A. and Parsons, K. (2013). Making stemmas with small samples, and digital approaches to publishing them: testing the stemma of KonraSs saga keisarasonar. Digital Medievalist, 9.

Handrit. (n.d.) Online catalogue handrit.org: http://handrit.org

Harris, J. (1975). Genre in the saga literature: A Squib. Scandinavian Studies, 47(4): 427-36.

Hooper, A. G. (1932). “BragSa-Qlvis saga” now first edited. Leeds Studies in English, 1: 42-54.

Hooper, A. G. (1934). Hromundar saga Gripssonar and the Griplur. Leeds Studies in English, 3: 51-56.

Kapitan, K. A., Rowbotham, T. and Wills, T. (2017). Visualising genre relationships in Icelandic manuscripts. Conference Abstracts. Gothenburg: The University of Gothenburg, pp. 59-62.

Lavender, P. (2015). The Secret Prehistory of the Fornaldarsogur. The Journal of English and Germanic Philology, 114(4): 526-51.

Mitchell, S. (1991). Heroic Sagas and Ballads. Ithaca and London: Cornell University Press.

Nordisk Forskningsinstitut (n.d). The “Stories for all time” Project. http://fasnl.ku.dk/

Quinn, J. (2006). Interrogating Genre in the Fornaldarsogur: Round-Table    Discussion’.    Viking and Medieval

Scandinavia, 2: 276-96.

Rafn, C. C. (1829). Fornaldarsögur Nordrlanda. Kaupmannahofn.

Rowe, E. A. (1993). Generic Hybrids: Norwegian ‘Family’ Sagas and Icelandic 'Mythic-Heroic' Sagas. Scandinavian Studies, 65(4): 539-54.

Rowe, E. A. (2004). 'Porsteins ^attr uxafots, Helga ^attr Porissonar,’ and the Conversion '^iettir'. Scandinavian Studies, 76(4): 459-74.

Simek, R. and Palsson, H. (1987). Lexikon Der Altnordischen Literatur, Die Mittelalterliche Literatur Norwegens Und Islands. (Kroners Taschenausgabe 490). Stuttgart: Kroner.

Skaldic Project Academy Body. (n.d.) Skaldic project: http://skaldic.abdn.ac.uk/db.php?

University of Copenhagen (n.d.) Ordbog over det norr0ne prosasprog Registre:

http://onpweb.nfi.sc.ku.dk/mscoll_d_menu.html
Introduction

nodegoat allows scholars to build datasets based on their own data model and offers relational modes of analysis with spatial and chronological forms of contextualisation. By combining these elements within one environment, scholars are able to instantly process, analyse and visualise complex datasets relationally, diachronically and spatially; trailblazing. nodegoat follows an object-oriented approach throughout its core functionalities. Borrowing from actor-network theory this means that people, events, artefacts, and sources are treated as equal: objects, and hierarchy depends solely on the composition of the network: relations. This object-oriented approach advocates the self-identification of individual objects and maps the correlation of objects within the collective.

Research Environment

nodegoat is a web-based research environment that facilitates an object-oriented form of data management with an integrated support for diachronic and spatial modes of analysis. This research environment has been developed to allow scholars to design custom relational database models. nodegoat dynamically combines functionalities of a database management system (e.g. Access/FileMaker) with visualisation possibilities (e.g. Gephi/Palladio) and extends these functionalities (e.g. in-text referencing, LOD-module) in one web-based GUI. As a result, nodegoat offers researchers an environment that seamlessly combines data management functionalities with the ability to analyse and visualise data.

The explorative nature of nodegoat allows researchers to trailblaze through data; instead of working with static ‘pushes’ - or exports - of data, data is dynamically ‘pulled’ within its context each time a query is fired. The environment can be used in self-defined collaborative configurations with varying clearance levels for different groups of users.

As a result of nodegoat's object-oriented set-up, everything is an object. In the case of a research project on correspondence networks, this means that a researcher would define three types of objects in nodegoat: 'letter', 'person', 'city'. Each object relates to an other object via relations (e.g. a letter relates to persons to identify the sender/receiver and this letters has been sent from/received in a city). In an extended research process, researchers could also define themselves as objects in the dataset, their sources or other datasets. Due to the focus on relations and associations between heterogeneous types of objects, the platform is equipped to perform analyses spanning multitudes of objects. By enriching objects with chronological and geospatial attributed associations, the establishment and the evolution of networks of objects is fully contextualised. In nodegoat, these contexts and sets of networked data can be instantly visualised through time and space.

This open-ended approach makes nodegoat different from tools like the Social Networks and Archival Context Project, Alan Liu’s Research Oriented Social Environment, the Software Environment for the Advancement of Scholarly Research, Prosop, or tools with a main focus on coding of qualitative data as seen in various computer-assisted qualitative data analysis software. With its object-oriented approach, nodegoat facilitates the aggregation of collections, coding of texts, and analysis of networks, but models these methods towards the creation and contextualisation of single objects that move through time and space. Facts & Figures

nodegoat is conceptualised and built by the independent research firm LAB1100, based in The Hague, The Netherlands. In order to share the functionalities of nodegoat with the scholarly community, scholars and research institutes are invited to use nodegoat for their own research purposes. Over 300 scholars have a personal research domain on nodegoat.net. Over 15 institutional partnerships have been established with universities, research institutes, and museums in The Netherlands, Belgium, Luxembourg, and Germany.

A nodegoat user forum and FAQ is hosted on the Historical Network Research website. In the course of 2018 an open source package of nodegoat will be released within the wider framework of the nodegoat community.

Examples of projects in nodegoat

Over 12.000 battles as described by Wikidata and DBPedia users visualised in nodegoat,

http://nodegoat.net/blog.s/14/a-wikidatadbpedia-geography-

of-violence.

Project Mapping Notes and Nodes in Networks' in collaboration with Huygens ING, University of Amsterdam, & KNiR


The whereabouts of over 20.000 people visualised through time and space in nodegoat http://mnn.nodegoat.net/viewer.p/1/47/scenario/17/geo/

illustration of a personal research dataset in nodegoat


Geographical network visualisation in nodegoat by Tobias Winnerling for the project 'Wer Wissen Schafft'

A Wikidata/DBpedia Geography of Violence
Modelling Literary Scholarship
The availability of digitised full-text resources, as well as bibliographical data in standard database format, has recently opened a new chapter in the sociology of literature by revaluating empirical approaches and data-driven scholarship. The road to this “empirical turn” in literary scholarship has been paved by such scholars as Franco Moretti (2005, 2013) and Matthew Jockers (2013), who showed how empirical data like bibliographical records, annotations, title words, genre categorization, etc., may help in generating new knowledge about literary periods. This approach gathered its momentum as other works exploring the possibility of using such data to answer particular research questions emerged. Due to the shortage of space we will name just a few that have the most influence on this paper, dividing them into three research strands. Firstly, the use of bibliographical data for statistical inferences on literary processes, e.g Bode’s (2012) rereading of Australian literary history through the data from AustLit (Australian Literary Bibliography). Secondly, the study of author co-occurrences and mutual references, e.g. visualising literary circles on the basis of such data by Long and So (2013a, 2013b). Thirdly, the application of topic modelling to uncover pertinent issues in literary scholarship, e.g. Goldstone and Underwood’s analyses of the evolution of American literary scholarship on the example of PMLA (2012) and seven major literary journals (2014). In combining those approaches into a macroanalytical study of Teksty Drugie, we also adopted the rationale introduced by the 40th anniversary internet edition of Signs, a literary journal dedicated to feminist criticism.

Aim
The aim of this study is to apply macroanalytical methods to trace the chronology of transformations of Polish literary studies using the example of Teksty Drugie. We hypothesise that the collection of papers published in a leading academic journal on literary scholarship can serve as a reliable approximation to chronological changes and/or breaks in Polish literary theory at the turn of the 20th century. We will first trace the topics present in the journal and then analyse them in diachronic perspective. We will focus on the influence of extra-textual events and phenomena on literary scholarship.

We believe that 25 years is a sufficient timespan to observe linguistic differences which are not caused by regular language change. Other projects conducted by the authors of this paper show that a language change (in Polish) typically spans many decades rather than a mere 25 years (e.g. Eder & Górski, 2016). Furthermore, we deal with conventionalised language of scholarship, so the use of certain terms often relates to a given paradigm rather than to a language in general. Nevertheless, we are aware of possible changes of meaning of keywords while interpreting topic models.

Material
Teksty Drugie is a Polish literary journal dedicated to literary scholarship. It has been published since 1990 by the Institute of Literary Research of the Polish Academy of Sciences. It focuses on literary theory, criticism and cultural studies, while also publishing articles by authors from neighbouring disciplines (philosophy, sociology, anthropology). The journal publishes monographic issues dedicated to particular topics or approaches within literary and cultural studies. All those features make it a good example for exploring the vicissitudes of Polish literary scholarship.

The corpus consists of the entire collection of papers published in Teksty Drugie (excluding letters, surveys, notes, etc.) in the years 1990-2014 (2,553 texts, 11,310,638 words). The material covering the years 1990-1998 was digitised, OCR-ed, and then manually edited, in order to exclude running heads, editorial comments, and so forth. Obviously, some textual noise - e.g. a certain number of misspelled characters - could not be neutralised. The material from 1999 onwards was digitally-born, but even though a small number of textual issues might have occurred. We believe, however, that distant reading techniques are resistant to small amounts of systematic noise (Eder, 2013).

Given the nature of Polish, which is highly inflected, lemmatization was necessary for a reliable processing of texts. The corpus has been lemmatised with LEM 1.0. (Literary Exploration Machine) developed by CLARIN-PL (see: Piasecki, Walkowiak, Maryl 2017). Method

To scrutinise the formulated hypothesis, we applied one of the methods of information retrieval that recently attracts a good share of attention in Digital Humanities circles, namely topic modelling in its classical variant known as Latent Dirichlet Allocation (LDA). The method, introduced by Blei (2012), allows for finding co-occurring cohorts of words that presumably reveal (latent) semantic relations.

The experiments were performed using a tailored script in the R programming language, supplemented by the package ‘stylo’ (Eder et al., 2016) for text preprocessing, and the package ‘mallet’ (McCallum, 2002) for the actual LDA analysis. A bimodal network of the relations between topics were produced using the software Gephi (Bastian et al., 2009).

Topic modelling relies on the assumption that particular topics are defined by words co-appearing in a given context. Hence, the definition of “context” is crucial to allow for any reliable observations. A few different solutions have been suggested (e.g. Blei, 2012; Jockers, 2013). In our approach, we did not split input texts into smaller samples, which was motivated by the fact that the vast majority of the studies published in Teksty Drugie are rather short.

Other parameters used in the study included: a stop word list containing 327 words (mostly function words, numerals, and very common adverbs), 100 topics extracted in 1,000 iterations, with the obvious caveat that this choice was arbitrary.

Results
A general overview of the obtained results shows a few interesting patterns. Firstly, we analysed and categorised the topics on the basis of their predominant words. The categories are as follows: literary theory (e.g. literature, fiction, text), poetics (e.g. verse, novel, short story, rhetoric) and methodological approaches (e.g. deconstruction, comparative literature, postcolonial studies, psychoanalysis); history of literature (e.g.

romanticism, contemporary poets) and cross-cutting research themes (e.g. death, politics, literacy).

A thorough exploration of such models requires a topographical visualisation capable of showing the connections between various topics, which often share a key word (cf. Goldstone and Underwood, 2012). The network (Fig. 1) is too large to be adequately rendered in this paper (a higher resolution image of Figure 1 is available online), yet even without the knowledge about concrete topics presented, we may see (partly thanks to ForceAtlas2 layout, which highlighted this feature) that groups of topics in our corpus are concentrically distributed. This onion-like distribution allows us to distinguish between the central topics (i.e. those who appear in many different papers) and those who appear less often or sporadically and hence are not particularly well-connected with other topics. For instance, in the geometrical centre of the network we may find topics and words pertinent to literary scholarship:    literature, literary, comparative

literature, national literatures, Jewish studies, fiction, together with some names of contemporary authors. Outliers are also interesting, and could be assigned to 3 groups: (1) expressions in foreign languages, (2) particular research topics or discourses which introduce quite a hermetic language, not shared in other topics, (3) noise (e.g. word bits generated through some errors in OCR).


Figure 1. Relationships between topics in Teksty Drugie.

Yet it has to be noted that even the most accurate rendering of the topical distribution is still only a static snapshot insensitive to changes. In order to see the evolution of topics, we need to visualise them on a temporal axis. Due to a shortage of space we present here only a few examples, to show the application of our method. All dot plots are presented below with a trend line based on two period moving average.

Fig. 2 represents the gradual shift of interest from more literature-oriented approaches, to the cultural ones. Both red (topic 19: literature, literary, writer, work) and green (topic 5: literature, research, theory) seem to be dominating until approx. 2007, when the blue line (topic 49: culture, cultural, social) overtakes the green line for the first time. Three years later it becomes the dominant approach, marking the shift in the overall content of Teksty Drugie.

Literature vs. culture


° Culture, cultural, social ° Literature, literary, writer ° Literary research, theory

Fig. 2. A temporal distribution of three topics related to literature and culture.

Topic analysis allows us to not only trace the evolution of the journal itself but also to see how the real-world events shape the topics undertaken by literary scholars. Fig. 3 shows the influence of the political transformation in Poland on the content of Teksty Drugie. We see a similar pattern in trends of all topics presented: grey (topic 60: power, society, state, fight, war, law), red (topic 36: political, communism, Polish People’s Republic), blue (topic 7: Polish, Pole, national), yellow (topic 94: censorship, exile, novel, positivism, country, London, political). All of them are quite important in the early 1990s and the interest gradually fades until the end of this decade. The spikes around 2001/2002 are caused by the publication of monographic issues which make certain topic more dominant. E.g. Issue No.1-2/2000 was dedicated to socialist realism hence the spike of “communism-related” issue in that year.

This trend shows how political events (namely the transformation and forming of the new democracy) are dominating even the literary scholarship. It could be also the case that more politically charged issues (e.g. history of censorship in Poland) could have been published only after the fall of the communism, hence so many articles in that period.

Political transformation


Axis Title

o Polish, Pole, national

o political, communism, Polish People's Republic o power, society, state, fight, war, law

_o censorship, exile, novel, positivism, country, London, political

Fig. 3. Temporal shift of topics related to politics.

The last trend we would like to discuss is the emergence of the Holocaust studies in Teksty Drugie. As we can see in the Fig. 4, the red trend line (topic 59: Jew, Jewish, antisemitic) is visible on the fairly same level all through the 25 years, whereas the blue one (topic 18: testimony, Holocaust) is virtually nonexistent until 2001.

Holocaust studies vs. Jewish studies


o Holocaust studies    o Jewish studies

Fig. 4. Temporal distribution of topics related to Jewish studies and the Holocaust.

This sudden boom can be linked to the publishing of the Polish edition of Neighbors by Jan Gross (2000) and the investigation into the role of Polish civilians in the genocide perpetrated in the city of Jedwabne during the World War II. This case opened a long process of re-investigating the troubled Polish-Jewish past, which could be traced also in the issues of Teksty Drugie.

Conclusions
In this study we tried to show how extra-textual events influence the content of literary scholarship on the example of Holocaust studies and political transformation, which entailed the prevalence of topics related to politics, power, society, state, and communism in the early 1990s. In the subsequent studies we plan to compare the results of topic modelling with bibliographical data in order to check whether the dominance of a certain topic stems from the large number of scholars who pursue it, or if it instead depends on the fact that a small group of authors published more often than others. Acknowledgement

The authors wish to thank Dr Tomasz Walkowiak for his extensive help with cleaning and lemmatizing the corpus.

Bibliography
Bastian, M., Heymann, S. and Jacomy, M. (2009). Gephi: An open source software for exploring and manipulating networks. Proceedings of the Third International ICWSM Conference. San Jose, pp. 361-62.

Bode, K. (2012). Reading by Numbers: Recalibrating the Literary Field. London & New York: Anthem Press.

Blei, D. M. (2012). Probabilistic Topic Models. Communications of the ACM, 55(4): 77-84.

Eder, M. (2013). Mind your corpus: systematic errors in authorship attribution. Literary and Linguistic Computing, 28(4): 603-14.

Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: a package for computational text analysis. R Journal, 8(1): 107-21.

Eder, M., Gorski, R. (2016). Historical Linguistics’ New Toys, or Stylometry Applied to the Study of Language Change. In Digital Humanities 2016: Conference Abstracts. Jagiellonian University & Pedagogical University, Krakow, pp. 182-184.

Goldstone, A. and Underwood, T. (2012). What can topic models of PMLA teach us about the history of literary scholarship?. Journal of Digital Humanities, 2(1).

Goldstone, A. and Underwood, T. (2014). The quiet transformations of literary studies: What thirteen thousand scholars could tell us. New Literary History, 45(3): 359-84.

Gross, J. T. (2000). Sqsiedzi: Historia zagtady zydowskiego miasteczka. Sejny: Fundacja Pogranicze.

Jockers, M. L. (2013). Macroanalysis: Digital Methods and Literary History. University of Illinois Press.

Long, H. and So, R. (2013a). Network science and literary history. Leonardo, 46(3): 274-274.

McCallum, A. K. (2002). MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu/.

Moretti, F. (2005). Graphs, Maps, Trees: Abstract Models for a Literary History. New York: Verso.

Moretti, F. (2013). Distant Reading. New York: Verso Books.

Piasecki, M., Walkowiak, T., Maryl, M. (2017). Literary Exploration Machine (LEM 1.0) - New Tool for Distant Readers of Polish Literature Collections. Paper accepted for presentation at ADHO Digital Humanities conference at McGill Universiy, Montreal.

So, R. and Long, H. (2013b). Network analysis and the sociology of Modernism. Boundary 2, 40(2): 147-82.
Introduction
Besides time, characters and plot, space is one of the main components in storytelling. But despite its importance as a category for the setting of narrative action and unlike the other mentioned categories, the conceptualisation of space has long been neglected in narratological research. This holds true even after the so-called spatial turn (Soja 1990) in cultural history, that lead to a renewed interest and to fruitful insights into space as a metaphorical concept. However, a systematic description of the means by which space is created in narratives is still in its beginnings (e.g. Den-nerlein 2009, Piatti et. al. 2009).

This is at least partly due to the fact that space poses substantial problems for modeling. The creation of space in narratives is often dynamic and based on implicit information: Rather than constructing a given, mathematical space beforehand, stories tend to evolve their setting in relation to its characters that constitute space through their actions. Spatial information in stories therefore highly depends on the characters that act, move or perceive within it. Especially in fiction, this also means that spatial information is often fuzzy and imprecise (Piatti et. al. 2009), since narrators quite frequently are more interested in telling a story than designing a detailed, coherent setting for it. Whereas these problems are hard to handle in traditional literary studies, they present serious yet interesting challenges for a digital formalization.

In our paper, we will illustrate the complex tasks that have to be tackled by a digital narratology of space based on an exemplary annotation workflow, that we will outline for the description of spatial elements in Jules Verne's Around the world in Eighty Days.

Challenges for a digital narratology of space
We describe the following major problems that can be grouped into a chain of work-tasks:

1. Basic information on the setting of a narrative can be retrieved by extracting the place names from a text (NER). However, an automatic extraction is flawed by well-known problems of disambiguation (cf. for example, Barbaresi / Biber 2016). Since space in narration is highly dependent on characters that are placed in it, these entities have to be detected as well.

2. Place names are not the only kind of spatial information that can be found in texts. Besides others, space is also constituted with the help of nouns that do not necessarily have an inherent spatial component (for example, a car in a text can be the subject of a description, but it turns into a space marker if someone enters it).

3. Spatial entities (names and nouns) can be referred to by co-reference.

4. Spatial entities in a text are not always the setting of narrative actions. Place names or nouns can also only be mentioned, dreamed of, remembered, reflected on etc. This different functionality has to be taken into account when it comes to the automatic generation of literary maps (eg. Moretti 1998, Piatti 2008).

To capture this opposition, Dennerlein (2009) separates event regions from mentioned spatial objects in her conception. Event regions are defined as spatial zones, where events take place. In contrast, mentioned spatial objects contain all spaces that are not event-related. Piatti et. al. (2009) develop a similar model: Their concept of setting closely corresponds with event regions, whereas projected space and marker give a finer differentiation of the notion of mentioned spatial objects (cf. Figure 1).

Dennerlein

Piatti

Event regions

-    Spatial zone, where an event takes place

-    The event determines the extension of the place (wide focus)

Setting

-    Characters need to be present

-    Each single action of a character at a current setting constitute this kind of place (narrow focus)

Mentioned spatial objects

-    Contains all spaces that are not part of an event

-    This includes commenting, arguing, reflections or descriptions

Projected space

- Characters are not present in this location, but they dream of, remember, or long for it

Marker

-    Describes a location that is only mentioned

-    It has no significance for the story or the character

-    Markers indicate the geographical range and horizon of the fictional space

Figure 1: Comparison of spatial concepts from Katrin Dennerlein Barbara Piatti

Annotation Workflow


The complexity of spatial information demands for a multi-faceted approach. Figure 2 shows a spreadsheet with the beginning of chapter 14 of Verne's Around the World in Eighty Days that has automatically, semi-automatically and manually been enriched with multiple layers of annotation.

First, the text was tokenized, lemmatized and POS-tagged (columns 1-4).

Second, Named Entity Recognition (NER) was applied to the text (columns 5, both steps were performed with Weblicht [2012]). The NER also identifies the names of the characters. The results of the NER have to be corrected manually. To improve the automation of this step the exploitation of other toponym-ical bases like Geonames and OpenStreetMap will be discussed.

Thirdly, to generate column 6, we used theme-specific wordlists that we built on the base of existing lexicological ontologies (GermaNet for German Texts [Hamp/Feldweg 1997, Henrich/Hinrichs 2010]. English Wordlists as shown were provisionally generated manually, but can be built in a similar way). These wordlists were used to automatically tag the text. In Fig. 1, ‘valley' has been annotated as LSC, which means that it belongs to the word field landscape. So far, we created wordlists for landscape and architecture (in German), which cover a high amount of place nouns. The annotation can be manually supplemented and new wordlists can be created.

Fourthly, in column 7, ‘the beautiful valley of the Ganges' was (manually) annotated as event region according to the model of Dennerlein (2009). An automatic differentiation between event regions and mentioned spatial objects will be a challenging task. However, we consider a rule-based extraction of dependency paths to approach the problem

Figure 3 shows a parse tree of the sentence from Verne's text. The pattern [Character - SUBJ - Verb of Motion - OBJ - place noun] is likely to indicate an event region. By gaining several similar patterns with high precision regarding the identification of event regions, we assume to collect features for a future implementation of machine learning methods.

Fifthly, in column 8, coreferences were annotated manually. We will consider different kinds of co-references: Spatial entities can be referred to by nouns (e.g. ‘Paris / ‘city') or pronouns. Also, certain deictics (‘here', ‘there') might refer to spatial antecedents. However, a reliable automatic coreference resolution, which would be highly desirable for many kinds of narratological analysis, is out of the scope of this paper.

Finally, we included a column (9) for annotations that are based on the modified tag-set of the ISOSpace-standard (Pustejovsky et. al. 2011a und b), as they are presented in the SpaceEval Annotation Guidelines (2014).

Visualizations and Outlook
With the help of this semi-automatic and multi-layered method, we hope that we can make use of the strength of the different approaches and combine their advantages (it would be highly beneficial, for example, to combine Dennerlein's category with an ISOspace-annotated text to enhance their rule-based detection).

The potential of their combination shall be demonstrated by two examples of visualizations that draw on named entities and wordlists.

A network of spatial markers
As Piatti et. al. (2009) pointed out, the impreciseness and semantic potential of spatial information in literary texts sometimes demand visualizations other than geographical maps.

Phileas Fogg descends the whole length of the beautiful valley of the Ganges

without ever thinking


VMOO^^PMOD^^^OE seeing



Figure 4 shows a co-occurrence network of characters and place markers in Around the world in Eighty Days: Characters appear in red, place names in yellow. Place nouns are divided into the sub-categories landscape (green), architecture (grey) and transport (blue). In a straightforward approach, we established edges whenever a character and a spatial marker appear in the same sentence. The nodes have been sized according to their degree (the number of their connections), which can be related to Juri Lotman's (Lotman 1977) concept of mobile vs. immobile characters: Characters which are connected to many places like Phileas Fogg and Passepartout are more likely to be main characters than characters with a lesser degree.

(The visualization was established with Gephi [Bastian et al. (2009)]).

Word-list-based Frequency Analyses
Figure 5 shows the distribution of landscape and architecture terms over the whole text of Around the world in Eighty days compared to a corpus of 451 German novels taken from the TextGrid-Repository (Text-Grid Konsortium 2006-2014, licensed CC-BY-4.0), which cover a time range from 1700 to 1920.

Every text was chunked into 10 ‘segments' (x-axis), for which we calculated the relative percentage of the vocabulary from the corresponding word field (‘value', y-axis). The graph shows a noticeable peak in the use of architectural vocabulary in the last third of the text, which can serve as a starting-point for a close reading of the text. However, to take full advantages of distant reading techniques for spatial analysis, more refined methods and annotated corpora are necessary. We hope that these methods can be developed by considering the challenges outlined in our basic model in this

paper.


Barbaresi, A., and Biber, H. (2016): „Extraction and Visualization of Toponyms in Diachronic Text Corpora.“, in: Digital Humanities 2016, Jul 2016, Cracovie, Poland, Digital Humanities 2016 Conference Abstracts, 732-734 http://dh2016.adho.org/

Bastian, M., Heymann, S., and Jacomy, M. (2009): Gephi: an open source software for exploring and manipulating

networks. International AAAI Conference on Weblogs and Social Media.

Dennerlein, K.(2009): Narratologie des Raumes. Berlin: de Gruyter.

Hamp, B., and Feldweg, H.(1997): „GermaNet - a Lexical-Semantic Net for German.“, in: Proceedings of the ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications. Madrid

Henrich, V., and Erhard, H. (2010): "GernEdiT - The GermaNet Editing Tool", in: Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC 2010). Valletta, Malta, May 2010, 22282235.

Lotman, J. (1977): The Structure of the Artistic Text. Translated from the Russian by Ronald Vroon. Ann Arbor: University of Michigan, Department of Slavic Languages and Literatures.

Moretti, F. (1998): Atlas of the European novel. 1800-1900. London / New York: Verso.

Piatti, B. (2008): Die Geographie der Literatur. Schauplätze, Handlungsräume,    Raumphantasien.    Göttingen:

Wallstein.

Piatti, B., Bär, H. R., Reuschel, A.-K., Hurni, L., Cartwright, W. (2009): „Mapping Literature: Towards a Geography of Fiction.“, in: Cartwright, William / Gartner, Georg / Lehn, Antje (Eds.): Cartography and Art. Berlin / Heidelberg, Springer 2009, 179-194.

Pustejovsky, J., Moszkowicz, J. L., Verhagen, M. (2011a): ISO-Space: The annotation of spatial information in language, in: Proceedings of the Joint ACL-ISO Workshop on

Interoperable Semantic Annotation, 1-9.

Pustejovsky, J., Moszkowicz, J. L., Verhagen, M. (2011b):

Using ISO-Space for Annotating Spatial Information. In: Proceedings of the International Conference on Spatial Information Theory

Soja, E. (1990): Postmodern Geographies. The Reassertion of Space in Critical Social Theory. London / New York:

Verso.

SpaceEval Annotation Guidelines (2014) http://jamespusto.com/wp-content/up-loads/2014/07/SpaceEval-guidelines.pdf

TextGrid Konsortium (2006-2014). TextGrid: Virtuelle

Forschungsumgebung für die Geisteswissenschaften. Göttingen: TextGrid Konsortium. textgrid.de.

WebLicht (2012): CLARIN-D/SfS-Uni. Tübingen 2012. WebLicht: Web-Based Linguistic Chaining Tool. Online. https://weblicht.sfs.uni-tuebingen.de/
Introducción
Encontrar relaciones entre los encabezamientos que se asignan a una obra monográfica es un problema histórico en el ámbito de búsqueda y recuperación de información. Por un lado, los documentos rara vez pueden ser representados con un solo tema; por otro, el número de temas que se puede asignar a una obra es virtualmente infinito (Green, 2001). En la intersección de las Humanidades Digitales y la Bibliotecología han existido diversos esfuerzos por mejorar la calidad de las ontologías de estos temas (Nurmikko-Fuller et al, 2016), su evaluación (Harper, 2016) y visualización (Duguid, 2015). Sin embargo, a nuestro conocimiento, no se han hecho estudios que aprovechen métodos innovadores para indagar relaciones entre los encabezamientos de materia. En esta comunicación breve, presentamos los resultados preliminares de un primer acercamiento al tema, que aprovecha el área de especialidad de cada participante del equipo --humanidades digitales, ciencia de datos y bibliotecas-- para analizar 249,899 registros de una de las colecciones más importantes de Ciencias Sociales y Humanidades de América Latina: la del catálogo de la Biblioteca Daniel Cosío Villegas de El Colegio de México.

Metodología

A través del portal de analíticas del Grupo Ex Libris, se extrajeron los encabezamientos de materia de todos los 249,899 registros de libros de la colección de la Biblioteca Daniel Cosío Villegas. Los encabezamientos de materia fueron subdivididos a su vez en tres niveles a partir de los subencabezamientos, sin distinguir entre

sus tipos -geográficos, cronológicos y de forma (ver Salta et al., 2015)- sino sólo tomando en cuenta su posición (primer subencabezamiento, segundo, etcétera). Por ejemplo, México--Historia--1821-1861 fue dividido en: México, Historia, 1821-1861.

Se estudió la relación entre temas utilizando técnicas de minería de reglas de asociación. Estas procuran descubrir implicaciones de la forma I i donde I es un conjunto de objetos y i es un objeto en particular, ambos tomados de un universo de objetos, en este caso temas. El soporte de I se define como el número de registros para los cuales I es subconjunto. La confianza se define como el soporte de I U i entre el soporte de I (Leskovec, 2010).

Se debe notar que la frecuencia de los temas asociados a los registros es sumamente baja como se puede observar en la Tabla 1, lo cual puede deberse a que, tratándose de una biblioteca especializada en ciencias sociales y humanidades los temas que se asignan son muy específicos, a fin de que el usuario especializado pueda encontrar lo que realmente le sirve.

Percentiles

Tema

25%

50%

75%

85%

95%

99%

1

1

1

3

5

22

129

2

1

1

3

6

27

219

3

1

1

3

6

28

170

Tabla 1

Asimismo, es de notar que 231,052 (92.45%) de los registros tienen un encabezamiento de materia; 152,414 (treinta por ciento menos) llega a tener dos encabezamientos de materia y sólo 29.89% tuvo tres. Por este motivo, los encabezamientos se concatenaron verticalmente para observar indistintamente las relaciones entre éstos. Se utilizó el algoritmo a priori y la elección de los umbrales se llevó a cabo de manera manual; se generaron 13 conjuntos de reglas de asociación con variaciones en los umbrales de confianza y soporte. Cada uno de estos conjuntos de reglas de asociación induce un grafo que se puede visualizar y explorar como se muestra más adelante. Umbrales demasiado permisivos inducen redes que tienen demasiadas relaciones como para poderse explorar manualmente y umbrales demasiado restrictivos inducen redes que no tienen suficientes relaciones como para poder decir algo interesante sobre la estructura de los datos en su totalidad. Finalmente se eligió una red que presenta un balance entre cantidad de información e interpretabi-lidad. El ‘soporte' mínimo fue de 0.0001 (ver Gráfico 1) y la ‘confianza' mínima de 0.4 (ver Gráfico 2) y la matriz de incidencia derivada de las reglas encontradas se utilizó para generar un grafo para la exploración visual del conjunto de asociaciones descubiertas. Para crear esta versión gráfica utilizamos la exportación de R a Gephi (Yon and Yon, 2015), la ‘confianza' como un peso para los vértices y Fruchterman Reingold (1991) como algoritmo para el diseño. Dimos color a los nodos de acuerdo con su modularidad, es decir, de acuerdo a las “comunidades” de nodos que se crean por la fuerza de sus relaciones (Blondel et al, 2008). La alta modularidad de la red prueba lo conectados que están los nodos en sus grupos y lo desconectados que están de nodos fuera de su red.

Resultados
Como hemos mencionado antes, los encabezamientos fueron divididos en los subencabezamientos que los anidan. Retomando el ejemplo anterior: “México--Historia--1821-1861” fue codificado como:

• Subject 1.1 - México • Subject 1.2 - Historia • Subject 1.3 - 1821-186

Este modelado de los datos, fue pensado para permitir una cierta exploración “gramática” de la asignación temática. Es decir, que permitiera ver qué niveles “sintácticos” se relacionan en qué orden con otros niveles. En números, la red tiene 394 nodos (subencabezamientos) y 339 vértices (asociaciones). De los nodos, 203 son del primer nivel, 109 del segundo, 33 de la combinación de un encabezamiento del primer nivel con el tercero, y cuatro de la combinación del primer nivel con el cuarto. El total asociaciones o reglas de implicación (si encabezamiento I aparece también i) fue de 339. De éstas la mayoría ocurre sólo en 25 registros, es decir, tuvieron un soporte bajo (ver Gráfico 1). Sin embargo, esto no es tan poco considerando lo que hemos dicho antes de la naturaleza especializada de esta biblioteca. Por otro lado, las confianzas observadas presentan una distribución menos concentrada que la de los soportes (ver Gráfico 2).


Gráfico 1

Distribución de las confianzas de las reglas encontradas

0.3    0.4    0.5    0.6    0.7    0.8    0.9    1.0

Confianza

Gráfico 2

De la red de grafo interactiva que obtuvimos con el

uso de Gephi y el plug-in de Sigma.js, pudimos identificar que el nodo con mayores asociaciones o reglas es ‘Historia' en su posición como “Subject 1.2” y que entre sus asociaciones existen dos nodos de distinta modu-laridad y nivel (ambos “Subject 1.1”): ‘México' (ver Imagen 1) y ‘España' (ver Imagen 2).


Imagen 1


Imagen 2

A su vez, la plataforma permite explorar más a fondo el encabezamiento ‘España' y darse cuenta, por ejemplo, de que este tema en primera posición tiene fuertes relaciones con subencabezamientos de la tercera dimensión que corresponden a los periodos históricos relevantes en la historia de ese país:


En resumen, este tipo de exploración permite al usuario familiarizarse con las reglas “gramaticales” de la asignación temática pues puede “ver” tanto los niveles “sintácticos” de los temas como las formas en que se relaciona con otros, además de que incluye un botón

de búsqueda de encabezamientos que permite interactuar de manera directa con el grafo (disponible en linea).

Reflexión final
Nosotros, como lo sugieren Nurmikko-Fuller et al., estamos conscientes de que si las bibliotecas quieren dar acceso a recursos de información relevantes para nuevas áreas de investigación, deben evolucionar a métodos más sofisticados y semánticos de asignación temática para proporcionar nuevos puntos de acceso que correspondan más al lenguaje natural y que permitan identificar las relaciones temáticas con mayor claridad.

Sin embargo, en lo que este paso puede ser dado en México y Latinoamérica, creemos que el uso de herramientas y métodos de las humanidades digitales pueden ayudar a analizar los datos generados en la organización de la información e incluso útil para la formación del catalogador, que aprende a asignar-elaborar los temas y con esta herramienta podría tener un acceso visual a la “sintaxis temática” de ciertos términos. En este mismo sentido, un acercamiento así, podría ser usado como elemento pedagógico de los cursos de investigación documental en el que los estudiantes deben aprender a familiarizarse con los lenguajes controlados. Otra aplicación de este trabajo, podría ser en la evaluación de colecciones para determinar las fortalezas y carencias temáticas, de acuerdo con la especialidad que la biblioteca declara. Análisis más detenidos pueden ayudarnos a determinar la representación cronológica, autoral, lingüística o geográfica de un acervo. En fin, consideramos que al continuar el análisis y desarrollo de este proyecto podremos aportar otro tipo de metodología no sólo para evaluar las colecciones sino para acercarse a ellas.

Bibliografía
Blondel, V., et al. (2008). “Fast unfolding of communities in large networks”, Journal of Statistical Mechanics: Theory and Experiment, P1008.

Duguid, T. (2015), "BigDIVA: Big Data, Big Visuals, Big Searches, and Big Results." Texas Digital Humanities Conference 2015. University of Texas Arlington, Texas.

Fruchterman, T. M., & Reingold, E. M. (1991). Graph drawing by force-directed placement. Software: Practice and experience, 21(11), pp. 1129-64.

Green, R. (2001). “Relationships in the organization of knowledge: an overview.” Relationships in the organization of knowledge. Springer Netherlands, pp. 3-18.

Nurmikko-Fuller, T., Jett, J., Cole, T., Maden, C., Page, K., Downie, J. (2016). “A Comparative Analysis of Bibliographic Ontologies: Implications for Digital Humanities”. Digital Humanities 2016: Conference Abstracts. Jagiello-nian University & Pedagogical University, Krakow, pp. 639-42.

Leskovec, J., Rajaraman, A., Jeffrey, U. (2010). Mining of

Massive Datasets. Cambridge University Press, U.K., pp.

205-14.

Salta, G., Cravero C., Saloj, G. (2005) “Lista de encabezamientos de materia de la Biblioteca del Congreso de los

Estados Unidos: características generales”. Información,

Cultura y Sociedad, 12. pp. 85-97

Yon, G. V., & Yon, M. G. V. (2015). Package ‘rgexf'.
Introduction

Twitter data have been growingly used as a source for scholarly studies in various disciplines in recent years (Williams et al., 2013). The value of such borndigital data as primary source materials for future researches in history is already being acknowledged (Webster, 2015, Steinhauer, 2015). But, at least for now, historians seem rather reluctant to make use of them, although some recent works deal with the perception and memory of the past on Twitter (Clavert, 2016, Turgeon, 2014) or propose both documentation and analysis of present time events (Ruest and Milligan, 2016).

One possible reason of this reluctance could be the attachment of historians to traditional archival collections, e.g. those organized by professional archivists. But the creation of archives for social network sites data is not yet systematic and still in the beginning. As for the global Twitter archive of the Library of Congress, it is unknown when it will be functional (Zimmer, 2015). As it is possible for one to retrieve Twitter datasets, a second reason of this reluctance could be the need for acquaintance with basic methods and tools for gathering, understanding and analyzing born-digital data. However, not all historians are trained to digital humanities and quantitative methods that provide for such skills. Last but not least, the main reason could be the relation historians have with

time. It has always been difficult to define the moving

frontier between the present time and the recent past in contemporary history (Bedarida, 2003). As instant ephemera data that belong in the very present time, Twitter data precisely underline the difficulty for historians to define their own territory in these temporalities.

Nonetheless, for historians concerned with contemporary historical events - historical in the sense of a conjuncture that reveals a before and an after (Le Goff, 1999) - Twitter provides an original documentation. This documentation is generated in real time; organized around folksonomies - the hashtags - that reveal a direct perception from below; but also in close relation with media coverage. Since the creation of Twitter, a series of hashtagged global events (#IranE-lection, #15M, #Occupy, the 2011 Arab revolutions under various hashtags, the 2015 terrorist attacks in Paris...) update the concept of the monster-event (Nora, 1972) in that they are produced, lived, transmitted and shared in real-time around the globe - or at least in its connected parts. In spite of the known biases, mainly the fact that it is mainly used by relatively young and highly educated adults (Pew Research, 2016), Twitter offers an original kind of non-institutional primary sources (tweets) that can be complementary to the traditional ones the historians use.

This paper is a tentative to document and to provide a first analysis, based on Twitter primary sources, of the Greek referendum of 2015. This event has already obtained a distinctive status in the “before” and the “after” the current crisis marks in Greece's posttransition to democracy history (after 1974) (Avgher-idis et al., 2015), although time and future historians will definitely tell. The paper considers the transnational phase of this event, which followed the Greek vote in favour of the “no” to further austerity measures, included negotiations in the instances of the EU and ended with the agreement of the Greek government to conclude a third harsh austerity programme. Our main research hypothesis is that the imbrication of different hashtags reveals different temporalities that allow researchers to construct regimes of historicity of an event.

Event background

In the aftermath of the 2008 financial crisis, Greece, an EU and Eurozone member, began going through a severe debt crisis that revealed the structural weaknesses of the European monetary union and soon expanded to other weak members (Portugal, Ireland, Cyprus and Spain). Since 2010, the crisis has been managed through the setup of a European financial assistance mechanism in exchange for national programmes of structural reforms and budgetary cuts. Two such programmes were applied to Greece in 2010 and in 2012, plus a debt restructuring, that were monitored by the European Commission, the European Central Bank and the International Monetary Fund (Papaconstantinou, 2016; Zettelmeyer, 2013). The ongoing crisis provoked profound social and political transformations in the country that brought to power a coalition government led by the radical Left party of Syriza in January 2015. Syriza won the election with the promise to put an end to austerity politics. The party emerged in the context of the post-2008 crisis that shook the countries of Southern Europe and the 2011 Indignant movements, just like Podemos in Spain. Thus, the referendum of July 2015 was far from being significant only in the context of the Greek crisis as an effort of the new government to ameliorate the terms of the Greek programmes, as it put at stake different visions for the EU and its crisis management politics.

Data collection and analysis: method and tools

Tweets using the hashtag #greferendum were collected with NodeXL, an add-in to MS Excel (Smith et al., 2009). The collect was setup once daily from July, 6 to July, 16 2015. The size of the gathered sample was determined by the capacities of the tool, that can collect a maximum of around 20,000 tweets at once. A total of 204,714 tweets were collected of which 139,945 are retweets (68,36 %), 8, 686 responses (4,24 %), 56,086 mere tweets (27,39 %). Minor collects were also launched for other related hashtags (mainly #thisisacoup). Hashtag data were treated with Open-Refine and further explored with R software.

Statistical analysis of textual data (tweets) was made with TXM-Textometry software (Heiden, 2010). The corresponding dataset had been previously encoded following the TEI P5/XML standard with use of the OxGarage service.

Social network analysis and visualizations were

made with Gephi software (Bastian et al., 2009).

Data analysis: first findings

Hashtags

The first part of the research focused on reading the hashtags of the dataset. The hashtag #greferen-dum was used with a variety of hashtags, a total of some 12,000 words (all languages and variants included). A first study focused on the hashtags with a frequency over 99, which gave a total of 158 words. After an elementary typology was established, it was possible to distinguish: geographic names, names of persons, institutions, common names, neologisms that came out of contractions (such as “greferendum”), short phrases that had the function of commentary.

The use of hashtags varied between tag and commentary, or included both functions at once (Bruns and

Burgess, 2011). The big majority of hashtags are in English (112 out of 158). However, in the thirty most frequent hashtags of the dataset, it is possible to find

words in Spanish, Italian, French, and German. By consequence, the linguistic communities that participated in the global interactions were the ones that were the most concerned by the crisis. As for the Greek language, it is not entirely absent as such, but it is mainly present in its greeklish form: Greek words used as hashtags but written in Latin alphabet.

A close reading of the thirty most frequent hashtags with parallel consideration of the associations of words (coocurrencies) shows the tractations that followed the Greek referendum were basically perceived as an intergovernmental affair with the EU actors occupying a secondary position.

An interesting case is the emergence of the hashtag #thisisacoup as an act of solidarity of Spanish militants of Barcelona en Comu towards the Greek government during the Eurogroup and the Euro Summit negotiations (12-13 July). The corresponding dataset is more oriented to the expression of personal opinion than the dissemination of information with hashtags in the form of phrases that function more as commentary than tags (such as #yovoycongrecia).

Domains

The most tweeted domains were twitter.com (7,352 tweets) and theguardian.com (7,217 tweets). In general, it is possible to distinguish two main tendencies. First, the dissemination of information in social media (Twitter, YouTube, Instagram, Facebook ). Second, the dissemination of authoritative information (international media, specialized independent blogs, personal blogs).

Communities detection

The network of the #greferendum corpus is composed by 103,733 nodes and 204,713 relations. After nodes with a degree higher than 10 were isolated (around 4% of the total), 326 communities were detected with Gephi (Louvain method). These communities need to be further explored, however the first findings for the most important of them show that affinities developed around sources of information (media), political and/or intellectual personalities, professional communities, and also linguistic communities. Conclusion

The network and the detected communities seem to have been structured around the dissemination of information but also political affinities and/or mili-tantism. However, further exploration is necessary in order to better understand the network structure.

A quantitative analysis of the tweets, with emphasis on the associations between the hashtags, indicate coexistence of different temporalities within the temporality of the 2015 Greek referendum that are principally related to the Eurozone crisis, the associated national sub-crisis, and post-2008 anti-austerity movements. In this sense, Twitter primary sources offer insights from a transnational scale.

Bibliography

Avgheridis, M., Gazi, E. and Kornetis, K. (eds) (2015).

M£TanoAÎT£uan. H EÀÀafta oto p.£Taixp.io Svo auiivoiv

[Metapolitefsi. Greece Between Two Centuries]. Athens:

Themelio, pp. 15-18 and 335-66.

Bastian, M., Heymann, S., Jacomy, M. (2009). “Gephi: An

Open Source Software for Exploring and Manipulating

Networks.” International AAAI Conference on Weblogs and Social Media. San Jose, California: Association for the Advancement of Artificial Intelligence, pp. 361-62

Bédarida, F. (2003). Histoire, critique et responsabilité.

Brussels: Complexe, p. 64

Bruns, A. and Burgess, J. (2011). “The Use of Twitter

Hashtags in the Formation of Ad Hoc Publics”. Proceedings of the 6th European Consortium for Political Research

(ECPR) General Conference. Reykjavik: University of Iceland. Available at: http://eprints.qut.edu.au/46515/

Clavert, F. (2016). “#ww1. The Great War on Twitter.” Digital Humanities 2016: Conference Abstracts. Krakow: Jagi-ellonian University & Pedagogical University, pp. 46162. Available at: http://dh2016.adho.org/abstracts/378

Hanneman, R. A. and Riddle, M. (2005). Introduction to social network methods. Riverside, California: University of California, 2005

Heiden, S. (2010). “The TXM Platform: Building OpenSource Textual Analysis Software Compatible with the TEI Encoding Scheme.” 24th Pacific Asia Conference on Language, Information and Computation. Sendai: Institute for Digital Enhancement of Cognitive

Development, Waseda University, pp.389-398. Available at: https://halshs.archives-ouvertes.fr/halshs-

00549764/document

Le Goff, J. (1999). “Les « retours » dans l'historiographie française actuelle”. Les Cahiers du Centre de Recherches

Historiques,    22. Available at: http://ccrh.re-

vues.org/2322 ; DOI:10.4000/ccrh.2322

Nora, P. (1972). "L'événement monstre". Communications, 18: 162-72

Papaconstantinou, G. (2016). Game Over: The Inside Story of the Greek Crisis. Middleton, Delaware: CreateSpace

Pew Research Center (2016). “Social Media Update 2016”. Available    at:    http: //www.pewinter-

net.org/2016/11/11/social-media-update-2016/

Ruest, N. and Milligan, I. (2016). “An Open Source Strategy for Documenting Events: The Case Study of the 42nd Canadian Federal Election on Twitter.” Code4Lib Journal, 32. Available at http://iournal.code4lib.org/arti-cles/11358

Smith, M., Shneiderman, B., Milic-Frayling, N., Rodrigues, E.M., Barash, V., Dunne, C., Capone, T., Perer, A. and Gleave, E. (2009). “Analyzing (Social Media) Networks with NodeXL.” Proceedings of the Fourth International Conference on Communities and Technologies. New York: ACM, pp. 255-64

Steinhauer, J. (2015). “Preserving Social Media for Future Historians.” Insights. Scholarly Work at the John W. Kluge Center.    Available    at:

https://blogs.loc.gov/kluge/2015/07/preserving-so-cial-media-for-future-historians/

Turgeon, A. (2014). “Comment travailler la mémoire sur Twitter”. Études canadiennes / Canadian Studies, 76. Available at: http://eccs.revues.org/216

Webster, P. (2015), “Will Historians of the Future Be Able to Study Twitter ?”. Webstory, Peter Webster's Blog. Available at: https://peterwebster.me/2015/03/06/fu-ture-historians-and-twitter/

Williams, Sh.-A., Terras, M. and Warwick, C. (2013). “What people study when they study Twitter: Classifying Twitter related academic papers.” Journal of Documentation, 69 (3): 384-410

Zettelmeyer, J. , Trebesch, Ch. and Gulati, M. ( 2013) “The Greek debt restructuring: an autopsy”. Economic Policy, 28 (75): 513-63.

Zimmer, M. (2015). “The Twitter Archive at the Library of Congress: Challenges for information practice and information policy." First Monday 20 (7). Available at: http://firstmonday.org/ois/index.php/fm/arti-cle/view/5619/4653
Revivalism has always played a significant role in the social functioning of religion in the US. As argued by McClymond, "religious revivals are as American as baseball, blues music, and the stars and stripes" (2010, 306). This strong presence of revivalism in the American religious landscape translates into the considerable significance of revival preaching not only for the American pulpit practice, but also for American culture in general. One could argue that a certain continuity is discernible in the American revival tradition, and preachers of consecutive "Awakenings", starting with the Great Awakening, have utilized similar communicative strategies, analogous sets of cultural references, as well as persuasive ploys to forward the "New Birth" to their hearers and to spread the revival zeal. Billy Graham, the most celebrated televangelist of the 20th century, testified to the importance of this tradition when in 1949 in Los Angeles, during the "Canvas Cathedral" Crusade, he delivered to a contemporary audience the most notorious sermon of the Great Awakening (and, perhaps, of America's entire pulpit oratory), Jonathan Edwards's Sinners in the Hands of an Angry God. Graham was by no means a pioneer in this respect, as the 19th century American revivalists, like Charles Finney, eagerly fell back on the rhetorical heritage of the first Great Awakening sermons.

Crowds of thousands of people, the emotional reactions of the audiences which often bordered on mass hysteria, fervent theological debates and a surplus of publications played a significant role in the shaping of the early American ecclesiastical order. Similarly, the American rhetorical tradition was strongly informed by the revival developments in pulpit oratory, especially in the context of both the oratory of the American Revolution and the Civil War. New forms of preaching manifested the power of the spoken word, and propelled the dynamics of public debate in a period which was to prove vital for the development of American identity.

The study of American preaching tradition from the diachronic perspective seems particularly important. Different groups of preachers in consecutive Great Awakenings, (First 1735-1750, Second 1790-1840, Third 1850-1900, Fourth 1960-1980) appropriated the rhetorical models employed by the previous generations and built upon their output.

However, because of the sheer size of the available corpus, the comparative study of the preachers of all Great Awakening has been so far impossible. Stylome-try based on word usage makes it possible to highlight the connections between particular groups of preachers, as well as to demonstrate the evolution of the American revival preaching tradition; and to confront these findings with the existing attempts at classifica-tion/chronology (Stout 1986).

In our study, 42 collections of sermons by individual preachers (one text file per preacher) have been collected, digitized and modernized to avoid an excessive chronological bias due to spelling differences; of course, purely linguistic bias could not be entirely eliminated. Of these, 13 are traditionally classified as First Awakening; 9 as Second; 19 as Third; the most traditionally controversial group, Fourth, is represented by Billy Graham alone.

Classification was made by comparing distances, or differences, between most-frequent-word frequencies of the texts using Burrows's Delta procedure (2002); the distance measure applied was “Cosine Delta” as proposed by Jannidis et al. (2015), implemented in the stylo package (Eder et al. 2015, 2016) for R (). The word frequencies were submitted to a consensus procedure of cluster analysis at word frequency vectors of 100 to 2000 most frequent words, and these results then served to produce network diagrams in Gephi (Bastian et al. 2009) using the gravitational Force Atlas 2 algorithm (Jacomy et al. 2014). This produces a network or a “map” of data points for the individual preachers' sermons; the closer and the thicker the links between them, the more similar they are.

Figure 1 presents such a network graph for the 42 preachers. The color coding follows the traditional division into four “Awakenings” (from First, green, through Second, yellow, Third, red, and Fourth, purple). A very strong evolutionary pattern emerges;

apart from minor imperfections, the network shows a clear evolution from the earliest preachers to the most modern one, Graham. At the same time, the grouping of the data points suggest that a different classification might also exist in the dataset. This is why another algorithm available in Gephi, “Modularity,” was used to

discover these “communities,” or groups (Blondel et al.

2008).


Figure 1. The four Great Awakenings in traditional classification (top); divided by modularity into 3 (center) and

4 (bottom) groups.

This produces two alternatives to the traditional division of the Revivalist movements(s). In the first of

these (center), what is usually referred to as the First and the Second Great Awakening would be merged into a single group, while the traditional Third Awakening splits into early and late phases; the latter of

which now also included the single representative of the Fourth. Perhaps more interestingly, computergenerated four Awakening communities (bottom) suggest a First only limited to some of Edwards's contemporaries, a Second that extends a little more than traditionally into the past, and again a limited early Third with an expanded Fourth.

This is of course not to say that the above results

invalidate the traditional, or historical, division of American revivalist writing. But the stylistic (or, at least, stylometric) divisions are no less valid. Most-frequent-word usage is a significant element of distant reading; and such a distant reading of the Revivalism

opens new avenues for close reading of the American homiletic tradition.

open source software for exploring and manipulating networks. International AAAI Conference on Weblogs and Social Media.

Blondel, V., Guillaume J.-L., Lambiotte, R., Lefebvre, E.

(2008). Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment (10), P10008    (12pp)    doi:    10.1088/1742-

5468/2008/10/P10008.

Burrows, J. (2002). ‘Delta': A measure of stylistic difference and a guide to likely authorship. Literary and Linguistic Computing, 17: 267-287.

Eder, M., Kestemont, M. and Rybicki, J. (2013). Stylometry with R: a suite of tools. Digital Humanities 2013: Conference abstracts, University of Nebraska-Lincoln, 487-489.

Eder, M., Rybicki, J., Kestemont, M. (2016). "Stylometry with R: A Package for Computational Text Analysis," R Journal 8 (1): 107-121.

Jacomy, M., Venturini, T., Heymann, S. and Bastian, M.

(2014). ForceAtlas2, a Continuous Graph Layout Algorithm for Handy Network Visualization Designed for the Gephi Software. PLoS    ONE,    9(6): e98679.

doi:10.1371/journal.pone.0098679.

McClymond, M. (2010). Revivals, in P. Goff (ed.), The Blackwell Companion to Religion in America, Chichester: Wiley-Blackwell.

R Core Team (2014). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Wien, http://www.R-project.org/.

Stout, S. (1986). The New England Soul. Preaching and Religious Culture in Colonial New England. New York: Oxford

University Press.

Acknowledgements
This research is part of project K/PBO/000331, supported by Poland's National Science Center.

Bibliography
Bastian, M., Heymann, S., Jacomy, M. (2009). Gephi: an
Introduction
This paper presents the prototype of a “Database of Belarusian Literary Periodicals” developed at the University of Oldenburg, Germany. The project addresses the hitherto under-researched area of Belarusian literature from a field theoretical perspective (Bourdieu, 2002) in combination with a quantitative approach to Literary History (Moretti, 2005).

Belarusian Literature
The case of Belarusian literature is interesting in particular because it constitutes a highly unstable literary field. Due to historical reasons the formation of the literary field and the rise of a literary market dates back only to the beginning of the 20th century. During the first half of the 1920s the establishment of proletarian society in Soviet Belarus entailed the promotion of Belarusian language and literature, which were then consolidated by specific institutions. But already from the second half of the 1920s, cultural politics changed

and literary life came under increasing ideological

control ending in the infamous ‘cleansings’ ("cistki”) of

the 1930s.

Literary Periodicals
Research on the formation of literary markets and fields has illustrated the crucial role of literary periodicals (Bourdieu, 2002; van Rees, 2012). We consider these magazines media that allows us to reconstruct and to analyze the specific structure and internal development of the Belarusian literary field. They allow us to trace the configurations of authors within groups and magazines, the trajectories of single authors and/or literary groups in the field, the differentiation of the literary genre system, the formation of literary criticism and so forth.

The Database
The prototype of the database includes the four most important Belarusian literary periodicals published between 1922 and 1939: Maladnjak, Polymja, Uzvissa and Kalos'se (cf. Kohler, 2016). For the time being, we focus on capturing these periodicals’ tables of contents, assuming that a systematic analysis of literary periodicals does not necessarily require the literary texts themselves but that the corresponding ‘paratexts’ suffice (cf. Genette, 1989).

The database currently comprises the tables of contents of 252 issues that were transcribed manually and were encoded according to the TEI-Guidelines. The database itself is set up as an application for the opensource XML database eXistdb. An extensive range of search queries can be performed on the corpus, which enables users to identify quantitative characteristics of the periodicals and its contributing authors. It allows for the export of the data as a dynamic network graph in the gefx-format (Gephi).

Pilot study: corpus, questions, and method
For the pilot study we specifically focus a) on the period 1922-1932 (the year 1932 marks the end of literary diversity: groups and magazines dissolved and fused into the ‘one’ Writers’ Association with its official periodical Polymja), and b) on the three periodicals in Soviet Belarus (Kalos'se was established only in 1934 and was published in the Polish part of the country). Taking into account this focus, the pilot study deals with a complete corpus of 189 issues.

We focused on the analysis of c) authors’ trajectories (fluctuations between periodicals), including the question of "splinter groups’ trajectories” and of d) hi-erarchization of authors. These questions are interlinked with the questions, e) whether an author’s movement from one periodical to another brings an increase of his/her publication frequency, and how the constellation of authors changed in relation to such movements.

Step 1: Identification of authors: Problems we had to overcome in this respect were variations in spelling and the frequent use of acronyms and pseudonyms. The use of pseudonyms is in some cases not only linked to the periodical (some authors published in one magazine

using their real name and under pseudonyms in the others) but also to the author's role (e.g. writer vs. critic/reviewer).

Step 2: Identified authors were linked to a <person> record, where additional information on each identified author is stored. If available, these entries were linked to authority files (VIAF). As the coverage of Belarusian authors in the relevant authority files is rather incomplete, the authors were also linked to the corresponding Wikidata entries, to provide for an external unique identifier.

Step 3 (question ‘d'): We identified frequently published authors and compared their rankings in the periodicals at several points in time but also focused on literary reviews and translations.

Step 4 (questions ‘c', ‘e', ‘f'): We observed a relatively high fluctuation of authors and splinter groups between the periodicals, including also the authors that published most frequently. In analyzing the movements of authors, it was possible to complement hitherto lacking insights and background knowledge about the journals' interconnectedness.

Bibliography
Bourdieu, P. (2002). The Rules of Art: Genesis and Structure of the Literary Field. Cambridge: Polity Press.

Genette, G. (1989). Paratexte. Frankfurt a. Main.

Kohler, G.-B. (2016). “‘Success' and ‘Failure' of Literary Collaboration between Authors in Belarus in the 1920s.” In Butler, M., Hausman, A. and Kirchhofer, A. (eds), Precarious Alliances. Cultures of Participation in Print and

Other Media. transcript, pp. 207-40.

Moretti, F. (2005). Graphs, Maps, Trees: Abstract Models for a Literary History. London - New York: Verso.

Van Rees, K. (2012). “Field, Capital and Habitus. A Relational Approach to ‘Small' Literatures.” In Kohler, G.-B., Navumenka, P. and Grüttemeier, R. (eds), Kleinheit als

Spezifik. Beiträge zu einer feldtheoretischen Analyse der belarussischen Literatur im Kontext ‘kleiner' slavischer Literaturen. Oldenburg, pp. 5-56.
Cette contribution s'inscrit dans la continuité d'un un rapport sur les nouveaux modèles éditoriaux du libre accès, remis en octobre 2016 au Ministère de la recherche français.

En 2016, le mouvement libre accès se trouve en effet à un tournant : il entre dans la loi, via la mise en place de droits de republication indépendamment des contraintes fixées par l'éditeur en Allemagne, en Italie, tout récemment en France et peut-être bientôt au Canada. La part de publications scientifiques en accès ouvert atteint une taille critique : plus de 50% des publications européennes de 2011 étaient disponibles en lignes.

Au vu de cette accélération récente, s'ouvre une question prégnante : quel libre accès ? Car nous nous trouvons aujourd'hui au carrefour de plusieurs voies (ce dont la typologie des couleurs — vertes ou dorées — rend très imparfaitement compte).

Certaines institutions scientifiques, comme le Max Planck Institute, préconisent aujourd'hui un journal flipping : les budgets consacrés à l'acquisition des collections seraient directement reversés aux éditeurs sous la forme de « droits à publier ». La bascule n'affecte ici que les flux financier : le libre accès n'a, a priori aucune portée éditoriale.

Parallèlement, s'ébauche une vague aspiration à un model flipping : un changement profond des pratiques et des supports de diffusion de la recherche. C'est là l'objet de notre rapport : voir si, des mutations a priori distinctes comme l'évaluation ouverte (open peer review), les outils d'éditions open source (comme Open Journal System), l'ouverture des données de la recherche, les formes d'écriture interactives (carnets de codes) ou les modèles de diffusion non commerciaux (comme Open Library of Humanities) peuvent converger dans des paradigmes alternatifs cohérents.

Pour préparer notre rapport, nous avons constamment mobilisé, à différent niveaux, des méthodes nouvelles venues de deux champs emblématiques des « humanités numériques » les digital studies et, dans une moindre mesure, la lecture distante (ou distant reading). Notre interrogation est ici méthodologique : ce que les humanités numériques peuvent apporter à notre compréhension du libre accès, en tant que « phénomène éditorial » au plein sens du terme (Cette question transparaît indirectement dans des travaux revendiquant leur rattachement au champ tel que l'ouvrage collectif Vitali-Rosati et Sinatra, 2014; ou Magis et Granjon, 2016) Cet enjeu reste sous-jacent à la version finale de notre rapport — le format se prêtant mal à ce type d'interrogations épistémologiques. Nous souhaitons l'aborder plus directement ici sous trois angles :

Un « tournant matériel » du libre accès : l'expression fait directement écho au materialism turn des digital studies (Kirschenbaum et Werner 2014). Aux vertiges du cyberspace a succédé une saisie minutieuse du contexte socio-technique des pratiques et usages informatisés. L'étude rétrospective des conditions d'apparition du libre accès nécessitait un tel déplacement conceptuel. La « révolution numérique » de l'édition scientifique est une révolution longue : dès les années 1960, l'ordinateur devient un outil essentiel du processus d'éditorialisation. Cette informatisation précoce a joué un rôle déterminant dans l'apparition des grands oligopoles aujourd'hui dominant (à l'exception, finalement partielle, des SHS), mais aussi dans la généralisation de plusieurs pratiques aujourd'hui naturalisées (telles que le « peer review » formalisé — en son sens actuel l'expression n'apparaît pas avant les années 1970). La démocratisation, tout aussi précoce en milieu universitaire, des outils informatiques permet aussi le déploiement d'alternatives, dès l'apparition du premier site web en 1990 (Tim Berners-Lee s'en sert également comme « dépôt personnel » de ses propres articles). Il y a ainsi, tacitement, une correspondance éditoriale directe du web et de la publication en libre accès (au sens où, originellement, l'un a été conçu pour l'autre). Si l'architecture du réseau encourage la circulation des textes scientifiques, elle n'empêche pas l'émergence de formes de contrôle de cette circulation. La question des infrastructures est ouvertement posée aujourd'hui, à mesure que les grands éditeurs scientifiques réinventent leurs rôle sur le modèle de Facebook ou Google (ce qui se traduit, dans le cas d'Elsevier, par le rachat de réseaux sociaux académiques ou d'archives ouvertes comme SSRN). Un essai de text mining sur les billets récents de 200 blogs indexés par l'Open Access Directory révèle une prégnance des enjeux éditoriaux et "matériels" : traitement et publication des données, relations avec les bibliothèques, définition des formats (articles, post,

work), formes d'évaluations (« peer » et « review »

sont les principaux débouchés du terme « article »)... Les questions économiques, par contraste, passent au second plan : les occurrences de « green » ou de « gold » sont faibles (pas plus de 100 occurrences) et, surtout, il n'y a pas de « nœud » spécifique au sein du réseau pour rendre compte des modèles économiques.


Réseau de co-similarité dans les blogs référencés par l'open access directory.Réalisé avec Iramuteq (et Gephi pour la mise en forme).

Le libre accès comme processus d'éditorialisa-tion. Un texte sous licence libre n'est pas simplement librement consultable mais librement reproductible. En accord avec Bruno Bachimont, nous faisons le constat d'un lien profond entre la mobilité documentaire construction du discours : « La finalité n'est plus de retrouver des documents, mais d'en produire de nouveaux, à l'aide des ressources retrouvées. On passe ainsi de l'indexation pour la recherche à l'indexation pour la publication. Comme cette dernière s'effectue selon des règles et des normes, on parlera plutôt d'édi-torialisation, pour souligner le fait que les segments indexés sont enrôlés dans des processus éditoriaux en vue de nouvelles publications» (Bachimont 2007). De nouvelles formes d'évaluations ouvertes se trouvent ainsi totalement émancipées du cadre pré-déterminé de la revue : sur les nouveaux modules d'évaluation potentiellement associés aux archives ouvertes ou sur des initiatives communautaires comme PubPeer, le texte scientifique « mobile » peut être consulté et analysé à tout moment, par quiconque. La pratique du text mining et de la lecture distante implique également une remobilisation a posteriori des textes : un projet comme ContentMine extrait des millions de données structurées à partir de grands corpus scientifiques ; ces données peuvent à leur tour migrer d'un projet à l'autre (par exemple sur Wikidata), au gré des liaisons du « web sémantique ». La mobilité devient le point cardinal de nouveaux modèles éditoriaux encore indistincts. Nous évoquerons ici les principaux cadres et acteurs de ce processus de décantations des normes éditoriales (en insistant plus particulièrement sur le rôle central mais occulté des outils d'édition comme Editorial Manager ou Open Journal System).

Le libre accès comme projet politique. Ce troisième volet invoque moins une « méthode » à proprement parler des humanités numériques qu'une introspection récurrente : le rôle fondamental et contradictoire des « communautés » dans le processus de recherche (par opposition aux « institutions », Mou-nier 2015). Tournant matérialiste et tournant éditorial du libre accès convergent vers la nécessité de définir des infrastructures et des lieux de collaborations : en somme il est question de passer du texte comme bien commun passif, conçu pour être librement partageable par opposition aux textes enclos sous paywall, au produit d'un commun, soit d'une communauté qui assure sa publication, sa diffusion et sa préservation et se fixe ses propres règles de gouvernance. Wikipédia, Wikidata ou OpenStreetMap montrent qu'une telle forme d'organisation est viable à grande échelle — et que sans être intrinsèquement « scientifiques », elles contribuent directement à la dissémination des connaissance (voire à leur recueil et leur structuration dans le cas de Wikidata). L'Open Library of Humanities ouvre ici la perspective de constructions hybrides associant coordination institutionnelle (entre 200 bibliothèques et organisations universitaires garantes de l'indépendance du projet) et structure partiellement communautaire. Le champ politique du libre accès est également animé par des collectifs plus restreints. Longtemps actives dans l'ombre, les bibliothèques clandestines se sont imposés comme des acteurs essentiels de la diffusion des connaissance : le site Sci-Hub référence plusieurs dizaines de millions

d'articles, en grande partie consultés par des pays du

Sud (comme l'Iran ou l'Inde - cf. les recherches approfondies de Cabanac 2016).

L'exemple d'Open Library of Humanities nous amène à interroger, en conclusion, le rôle joué par les humanités numériques au sein du mouvement du libre

accès. L'émergence d'un carrefour interdisciplinaire faisant notamment se côtoyer les sciences informatiques et les approches élargies de l'objet textuel (de la bibliographie matérielle au tournant sémotique) affecte aujourd'hui significativement un mouvement jusqu'ici surtout centré sur les sciences naturelles. Le thème retenu cet année par DH2017 en offre un témoignage parmi d'autres — et la présentation s'achèvera sur cet exercice méta-communicationnel...

Bibliographie

Bachimont, B. (2007) « Nouvelles tendances applicatives : de l'indexation à l'éditorialisation », in L'indexation multimédia, Paris, Hermès, 2007. Consulter à l'adresse http:// cours.ebsi.umon-treal.ca/sci6116/ Ressources files /Bachimont-FormatHerme%CC%80s.pd

Cabanac, G. (2016) : « Bibliogifts in LibGen? A study of a text-sharing platform driven by biblioleaks and crowdsourcing », Journal of the Association for Information Science and Technology, 67(4), 2016, p. 874-884.

Kirschenbaum, M., et Werner, S. (2014) « Digital Scholarship and Digital Studies: The State of the Discipline », Book History, vol. 17:1, p. 406-458.

Magis, C., et Granjon, F. (2016) « Numérique et libération de la production scientifique », Variations. Revue internationale de théorie critique, 19.

Mounier, P. (2015) « Une «utopie politique» pour les humanités numériques? », Socio. La nouvelle revue des sciences sociales, 2015:4, p. 97-112.

Vitali-Rosati, M., et Sinatra, M. E. (2014) (dir.), Pratiques de l'édition numérique, Montréal, Presses de l'Université de Montréal.
Introduction
This project uses network graphs to depict musicians' careers in late seventeenth-century Venice. The

current network graph, viewable on the Musicians in Venice web page, demonstrates relationships between musicians and the institutions that employed them. The graph is bimodal, with nodes representing musicians (“people” nodes) and institutions (“place” nodes). Archival texts are incorporated into the visualization, with transcriptions of records that indicate musicians' activity included in the node attributes. The entire project is text-based, with data derived from XML transcriptions of archival records in addition to assigned metadata. The current website is a proof of concept for a larger project that would demonstrate ways of displaying text as part of the network graph and using texts in creating network visualizations.

The current graph focuses on the career of the composer Giovanni Legrenzi. Legrenzi worked for several prominent Venetian institutions from the early 1670s to his death in 1690 and ultimately was appointed to the most prestigious musical posts in the city. The musicians he worked with also served multiple institutions, either simultaneously or in succession, and their relationships with other musicians, patrons, and administrators often facilitated their movement between institutions. Legrenzi's Venetian career presents an excellent case study of these musical connections in late seventeenth-century Venice, and studying these connections demonstrates how networks of musicians functioned in this time period. This study also provides a representative sample of how network visualization effectively demonstrates patterns in musicians' careers in Venice.

The texts in the network graph include transcribed administrative documents, mostly unpublished, from the Venetian institutions where Legrenzi was active between 1670 and 1690. These are primarily payment, hiring, and termination records, which document the activity of the musicians employed by or affiliated with these institutions. Several generations of musicologists have used these documents to identify where different musicians were working, when they were employed, and in what capacity (Bonta, 1964; Moore, 1981; Termini, 1981; and Selfridge-Field, 1994). In this sense, this treatment of the documents is standard to the field, but applies DH methodology and processes to create visual representations the data. This provides a new perspective on the sources. For instance, grouping the “person” nodes by centrality results in multiple sub-groups, and proximity among these nodes demonstrates shared institutions and implied communities. (Hanneman and Riddle, 2005: Chapter 10) (Mary Russell Mitford's text-based network analysis of Robert Southey’s Thalaba the Destroyer provides an excellent example of this. In addition to demonstrating the behavior of agile, well-connected nodes, the graph displays modules of nodes as units of analysis (Wasserman and Faust, 1994: 4). Methodology

The transcriptions of the original documents were encoded in XML with tags and attributes determined by the Text Encoding Initiative (TEI). I used the XML markup to tag the information I wanted to include in my data while retaining the complete original text. In general, I tagged the name of the musician, the name of the institution with which they were connected, and the date and location of the connection.

9307

9308    v    <div type=”entry" xml:id=”LD0_17” rt=”17">

9309    v    <dateline>

«date when*"1677-02-04">Adi 4 Feb[ra]ro 1677 Cong[regazion]e

9311    Gent[il]e«/date>

9312    |    </dateline>

9313    v |    «pxnote place*"margin">Legrenzi</note> Intesi li sentiment! del Sig[no]r

9314    |    Maestro «name type=”person" ref=''#GioLeg">Legrenzi«/name>; che accetto di

9315    continuar a far gli Oratorj secondo il solito; fu eletto di nuovo il

9316    Pre[te] D. «name type=“person" ref=”#BarGri">Bartol[ome]o

9317    |    Gritti«/name>; accid passasse I’istesso uffizio col Stg[no]r

9318    |    «name type=”person" ref=”#GioPes”>Giovanni da Resaro«/name>; con

9319    |    q[ue]sto pero; ch’ egli averse solo da ingerirsi nelle funzioni

di «name type="place" ref=”#fava''>Chiesa</name> solam[ertt]e; e fu

9321    |    preso con tutti li voti.</p>

9322    </div>

9323

XML markup of primary document text

I then extracted the information for the network from the XML document using an XSLT. For this project, the stylesheet extracted the data assembled in the person index of the XML document and combined it with the entire text that documented each event. The resultant document was a CSV file of raw data that had to be separated into sets of nodes and edges to be read by the visualization platform. The data visualization for this project was generated using Gephi, which is an open-source platform for exploratory data analysis. For this project, I generated the network graph in Gephi by importing two CSV files: one with the nodes and their attributes and ID numbers, and one with the edges and the source and target data. The online version of the network graph includes all the text associated with a “person” node in the network as a node attribute. When the user selects a person node in the online version, it displays the transcriptions for every document mentioning that person in the Information Pane. (I exported the data using the SigmaExporter

plugin for Gephi. Version 0.9.0 from Gephi Thirdpar-

ties Plugins. The code is available on Github.)

Displaying all the text associated with a person node presented a challenge as the XSLT exported each connection between a musician and an institution as a new line in the CSV. For instance, if a musician was hired by a church in 1674, received a raise in 1678, and retired in 1686, there would be three lines in the CSV. When imported into Gephi, this would create three connecting edges between the person node and the place node. This created a misleading network graph, as the number of documented connections between people and places depended on the nature and availability of the records. To combine all the texts associated with the events for each person in the index, I concatenated the raw data using a PHP script and saved the results as a TSV file that could be imported into Gephi. As a result, the entire context of the decrees and decisions surrounding that musician's activity appears in the Information Pane, which also displays and links to the institutions associated with that musician. The

PHP script also included HTML formatting to better distinguish the entries from one another in the visualization. Once my nodes and edges were imported into Gephi, I used a layout that grouped the nodes by degree of centrality (Algorithm in Gephi based on Blondel, Guillaume, Lambiotte, and Lefebvre, 2008. Resolution in Gephi based Lambiotte, Delvenne, and Barahona, 2009).

• ©•    Gephi 0.9.1 - O829.gephi


Network graph in Gephi

Next steps

The current graph is the foundation of a more extensive project that uses musicians and musical activity in Early Modern Venice to benefit scholars working with TEI in similar for-hire environments. In these environments, hubs of activity define the relationship between individual practitioners in the historical equivalent of a sharing economy. In the long-term, I propose an online data repository and network graph of musicians working in seventeenth-century Venice that would eventually provide a roadmap for scholars embedded in the TEI transcription model, but with an interest in an automated process for applying TEI text analysis to network analysis.

In addition to the bimodal graph that highlights relationships between musicians and their employing institutions, I also hope to create a unimodal graph documenting relationships among the musicians. Creating the unimodal network and a supporting web application will be a challenge as relationships between individuals can be complicated and dynamic. This will take considerable work in effectively diagramming entity relationships and building web applications that go beyond the “out of the box” functionality of the Gephi export plugin. The website for the Six Degrees of Francis Bacon project provides an excellent example of a unimodal graph; here the relationships are de-

fined by circumstances (such as “father to” or “colleague of”) or actions (such as “met” or “wrote to”).

Linked Jazz, which uses documents as node attributes to demonstrate relationships and meaningful connections between jazz musicians in an interactive network graph, is also an excellent example. Both sites feature custom-designed entity relationships and interactive features in their web design that I want to emulate in my own network graph.

For the unimodal version, I would expand the data beyond Legrenzi's Venetian career, increasing the time frame and the number of institutions represented in the graph, and include different kinds of sources, such as periodicals, correspondence, and notarial records. Developing relationship typologies is such a crucial component to the project. This will require creating custom node metadata based on the information from the primary sources and agile development as the number and variety documents expand. The result will be a project that can better serve other scholars of Venetian music and culture.

Bibliography
Bonta, S. (1964). “The Church Sonatas of Giovanni Legrenzi.” Ph.D. diss., Harvard University.

Hanneman, R. and Riddle, M. (2005). Introduction to Social Network    Methods.    2005. http://www.fac-

ulty.ucr.edu/~hanneman/nettext/C10_Centrality.html

Moore, J. H. (1981). Vespers at St. Mark's: Music of Alessandro Grandi, Giovanni Rovetta, and Francesco Cavalli. Ann Arbor, Michigan: UMI Research Press.

Selfridge-Field, E. (1994). Venetian Instrumental Music from Gabrieli to Vivaldi. New York: Dover.

Termini, O. (1981). “Singers at San Marco in Venice: The Competition between Church and Theatre (c1675 -c1725).” Royal Musical Association Research Chronicle 17: 65-96.

Wasserman, S. and Faust, K. (1994). Social Network Analysis: Methods and Applications. Cambridge: Cambridge University Press.
Scholars conducting historical research are provided with a growing range of digital humanities tools, supporting different phases of the research process: there is software for extracting text from documents

(such as pdftotex, available on many Linux distributions or as part of Poppler), run OCR processes on images (for example Tesseract), tools for the creation, analysis, and visualization of datasets (for instance Nodegoat, Palladio, or Visualeyes), or software to work with annotations (for example Annotation Studio) or networks (such as Gephi or Cytoscape). Programming libraries are being developed to serve the needs of humanity scholars, like Spacy or Tethne. There are several repositories (such as HathiTrust or the Europeana) that provide access to sources and can easily be integrated into other services through APIs. Many tools, however, work well as self-contained units that scholars can use as singular parts of their research process, but cannot easily be combined into an integrated workflow by the researcher. Existing and new tools are developed using different languages and programming frameworks depending on requirements, skillset, and preference of the original developer, making reuse and integration harder for the developer seeking to combine several tools. Moreover, since most tools are developed independently of each other, many efforts are repeated by reimplementing functionality that is already provided by a different piece of software.

In this workshop, we would like to gather developers and programming-literate scholars to share their tool-building experiences and to present our first practical steps to create a system integrating multiple tools to work with historical documents from scan to analysis. The workshop is intended as a starting point for future exchange and cooperation for digital humanities developers.

In the summer of 2016, the Digital Innovation Group at Arizona State University (ASU) and the Max Planck Institute for the History of Science (MPIWG) started to combine their efforts in developing software for the history of science. One outcome of this collaboration is a research system that allows users to manage their documents, automatically runs OCR on uploaded files, provides an image viewer for uploaded and extracted images, and integrates document management with a multi-user Jupyter notebook server for writing analysis and visualization scripts. Rather than one big system, however, the research system is comprised of several integrated services developed independently of each other using different programming languages and frameworks.

For the Digital Humanities Conference 2017, we propose a full-day workshop with the goal to connect different tools and services to build a tool infrastructure for historical research.

The first half of the workshop will give tool developers a chance to present their software. Every presenter will be allowed 10 minutes for their presentation and 5 minutes for questions. ASU and MPIWG will present the different components of the developed research system. Specifically, we will present the following projects:

•    Collaborative Jupyter Notebooks: a Ju-

pyter Notebook server that allows sharing and publishing of notebooks based on Next-cloud and Dataverse.

•    DocuManager: an environment for annotating, correcting and searching digital documents, in particular for OCR text in ALTOXML and HOCR.

•    Giles Ecosystem: an Apache Kafka-based service to extract images and texts from documents and run OCR procedures on them.

•    Digilib: a Java-based IIIF-compliant image server and viewer.

The second half will be dedicated to discussing how different tools can be connected and integrated, and how we can build a community around those tools.

We envision the results of this workshop to be a concrete roadmap of how different tools will be integrated. We will define interfaces and API requirements, and if possible start development work during the workshop. Second, we will develop an organizational strategy for cooperation and collaboration among different projects. To aid organization, we will provide a Jira and Confluence project that participants can use during and after the workshop to organize collaboration.

We plan on organizing a follow-up meeting at the end of 2017 at Arizona State University to review progress since the initial workshop and plan next steps. If the collaboration is successful, we hope to establish regular meetings and expand the group to connect more tools and services.

Participants/Call
We will send out a call for participation in form of a short tool presentation for the first part of the workshop. We will ask presenters to focus on the technical perspective of their tool answering the following key questions:

• What is the general workflow of the tool?

• What is the core functionality of the tool?

• What input and output formats does the tool accept? Or what interfaces does it expose?

• What license model was chosen?

• What features are still missing and what are the next development goals?

• How is maintenance and development of the tool organized?

The deadline for the call is July 1st, 2017. We plan to accept 5-10 submission for our call, based on the usefulness of the tool and the potential for integration with other tools, which would all fit the first half of the workshop.

Audience
The target audience for this workshop are developers, historians with programming background, scholars with a technical background, and generally people involved in the development of tools to support historical research.

• Dirk Wintergrün (Max Planck Institute for the History of Science, Germany)

• Julia Damerow (Arizona State University, USA)

• Robert Casties (Max Planck Institute for the History of Science, Germany)

• Malte Vogl (Max Planck Institute for the History of Science, Germany)

Confirmed Presenters
Introduction
In this half-day tutorial we will offer a full-fledged, implemented and tested workflow that has been developed in the interdisciplinary Center for Reflected

Text Analytics (CRETA, a research center connecting both scholars from Humanities/Social Sciences and Computational Linguistics at the University of Stuttgart). Our focus is the valid and reliable identification of various kinds of entities and segments from raw, un-annotated texts and the extraction of specific relational information via network visualizations. Given the recent interest in networks for data representation and visualization (e.g., Gephi-tutorial at DH 2016), we argue that the following three-step-workflow is applicable to many research questions in the Social Sciences and Humanities:

1.    Detection of entity references in texts of different genres (e.g. references to chancellor Merkel in parliamentary debates),

2.    Segmentation of the texts guided by research questions (e.g. parts of a parliamentary speech dealing with the Greek financial crisis), and

3. Creation of networks of entities that co-occur within a segment (e.g. references to national or international organizations in a parliamentary debate dealing with the issue of wars and military interventions).

This workflow is one example of modularizing complex research questions into concrete steps and can moreover be combined with computational methods for the semi-automatic analysis of very large text corpora. The concepts of “entity” and “segment” are sufficiently generic to allow the same set of tools to be employed in different research questions originating from different fields of research. The tutorial is therefore not aimed at a specific Humanities or Social Sciences discipline and instead open to all researchers interested in the analysis of entity relations in large amounts of textual data.

In our tutorial we will make use of the web-based annotation tool CRETAnno developed to support semi-automatic annotation. CRETAnno provides tools for annotation and continuous assessment of inter-annotator agreement, thereby facilitating the production of reliable and valid data. Our tool facilitates the annotation of large text corpora: After some training instances are annotated, a machine learning model can be trained to predict new instances on additional texts, which can then be corrected and used as additional training material. This way, large texts can be annotated (relatively) quickly, given systematic manual annotation and clear annotation guidelines. This 3-step approach is currently investigated within the Center for Reflected Text Analytics (CRETA) on four distinct text corpora, connected to diverse research questions in different disciplines. Although establishing broadly applicable workflows has its merits (Kuhn & Reiter, 2015), we believe it is important to be able to “parameterise” them to take into account the specificities of a concrete research question. Research questions should govern the definition of entities, segments and weighting criteria in the network. In the tutorial, participants will be free to bring in (and work on) their own research questions (within the time limits of the tutorial).

Entity Reference Detection
Every concept of interest within a real or fictional world can be considered as an entity. Words in a text refer to th ese entities and are therefore called entity references. We have established annotation guidelines that distinguish six entity classes, oriented at the research questions within CRETA: Person, Location, Organization, Work (e.g., a piece of art), Event and Abstract Concept (e.g., art).

While these entities are semantically diverse, their linguistic representation in texts is similar: References are either proper nouns (Hillary Clinton/EU), pronouns (she/it) or appellative noun phrases (an American politician / the international organisation).Most of the entity references consist of a few words, but we generally opt for annotating full noun phrases (e.g., the British people after having voted for the Brexit). In order to be able to link entities semi-automatically, we focus on appellative noun phrases and proper nouns, and ignore pronouns (see below).

The notion of “entity reference” we are aiming for differs from what is known in Named Entity Recognition (NER) and Coreference Resolution (CR). In NER, only proper nouns are detected, while CR also aims to resolve pronouns. Our notion of entity reference detection is aiming for the middle ground. By excluding pronouns, we also exclude the most ambiguous words, whose co-reference properties typically can only be judged in context of their appearance. Appellative NPs contain enough information such that we can establish their identity with proper nouns with relatively simple lists and rules.

Text Segmentation
Researchers from Humanities or Social Sciences generally want to inquire either the interaction between entities (within certain contexts) or between entities and the contexts themselves. Text segmentation is our way of operationalising this context. The notion of segment -- again -- is a generic one, to be adapted to specific research questions and/or theoretical assumptions made within a discipline or research area. Different kinds of segmentation are distinguishable: A segmentation according to structural units like chapters (narratives), speeches (minutes of parliamentary debates) or acts (dramatic texts) relies on the proper detection of such segments in the original texts and is therefore highly intertwined with the concrete text format at hand. Although machine learning models can be trained to perform such tasks, they likely do not generalize well to new texts. Even in TEI-encoded dramatic texts (which are strongly structured), there are a lot of options how to encode acts. We therefore aim for making it easy for researchers from Humanities and Social Sciences to detect such segments using metadata (e.g. dates of publication of a newspaper article or a parliamentary debate), text-specific regular expressions and/or rules.

A second kind of segmentation is segmentation according to content criteria. Depending on text genre and research question, this can mean segmentation by topic, narrative level, plot, time, location etc. One possible application is the segmentation of newspaper content according to various topics (Kantner & Overbeck 2017, forthcoming).

Structurally, segment annotations differ from entity reference annotations by being longer and thus sparser within a text. This has consequences for the semi-automatic support, because annotating a sufficient number of training instances requires more text to be read (and analysed with respect to its segmentation) and thus takes more time. CRETAnno therefore supports a number of unsupervised segmentation algorithms that can be used directly. In addition, researchers can specify text patterns using regular expressions and simple rules and thus focus the segmentation on the specific research question they have. Entities + Segments = Networks

Given entity reference and segment annotations, it is only a small step to extract network-like data based on co-occurrence. As the entity reference annotation does not include links between annotations referring to the same entity, we developed a small tool to mark co-reference, given the annotated entity references. Currently, this has to be done manually, but we will explore automatisation possibilities in the future. Given that we can already identify string-identical entity references automatically, it is a manageable workload.

CRETAnno offers an interface to the graph exploration

software Gephi, which can be used to edit, explore, inspect and visualise the network (the tutorial covers the annotation, ex- and import, but only basic functionality of Gephi.).

Tutorial
Participants will have the opportunity to work on texts of their own choosing within the first half of the workshop. To that end, they will be asked to submit their texts before the workshop. We will supply hands-on material to participants that do not submit. The tutorial focuses on hands-on sessions and active participation.

Appendix
Tutorial Instructors

All submission authors work jointly in the Center for Reflected Text Analytics (CRETA) at Stuttgart University, Germany.

Sandra Murr

Sandra Murr , is a PhD candidate in the Department of modern German literature at the University of Stuttgart. Within CRETA, she analyzes literary works of the productive reception of J. W. v. Goethe's Sorrows of the Young Werther, the so-called Wertheriaden, focusing on the analysis of the central character constellation with respect to emotions.

Maximilian Overbeck

Maximilian Overbeck is a PhD candidate in Political Science at the Chair of International Relations and European Integration at the University of Stuttgart. In his PhD he analyses Western debates on religion in the context of wars and armed conflicts where he uses highly innovative computational-linguistic approaches for the valid and reliable analysis of large newspaper corpora.

Nils Reiter

Dr. Nils Reiter works at the Department of Natural Language Processing and coordinates the scientific work in CRETA. Since his PhD thesis with the title Discovering Structural Similarities in Narrative Texts using Event Alignment Algorithms ( Link ), he is working in and for the Digital Humanities area, with a particular focus on literary texts, annotation and the operationalisation of Humanities research questions.

Target Audience

Any student or scholar interested in qualitative and quantitative text analysis is invited. Prior knowledge in text analysis techniques is not obligatory but might be helpful. Programming skills are not necessary, but familiarity with Gephi is helpful. We welcome 20 to 30 participants.

Acknowledgements
We at CRETA are grateful to the German Federal Ministry of Education and Research (BMBF) for its generous funding in the years 2016 until 2018 (project ID: 01UG1601).

Bibliography
John, M., Lohmann, S., Koch, S., Worner, M., Ertl, T. (2016)

Visual Analytics for Narrative Text: Visualizing Characters and their Relationships as Extracted from Novels. Proceedings of the 7th International Conference on Information Visualization Theory and Applications (IVAPP '16). SciTePress, 2016.

Kuhn, J., and Reiter, N. (2015). A Plea for a Method-Driven Agenda in the Digital Humanities. In Proceedings of Digital Humanities 2015, Sydney, Australia, June 2015.

Kantner, C., Overbeck, M. (2017, forthcoming): „Die Analyse ,weicher‘ Konzepte mit ,harten‘ korpuslinguistischen Methoden. In: J. Behnke, A. Blaette, J.-U. Schnapp & C. Wagemann (eds.) Big data? New Data. Baden-Baden: Nomos Verlag.

Overbeck, M., (2015). Observers turning into participants: Shifting perspectives on religion and armed conflict in Western news coverage. The Tocqueville Review/La revue Tocqueville, 36, 95-124.

Reiter, N. (2015) Towards Annotating Narrative Segments. Proceedings of the 9th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 34-38, Beijing, China, July 30, 2015.

        
            
                Introduction
                Recent results of computer-aided research suggest that characters in novels – measured by their character speech – can be laid out stylistically distinct from other characters of the same novel (Hoover, 2017; Fields, Bassist, Roper, 2017). Thus, experienced authors are able to create characters with ‘distinctive voices’ which can be identified by word frequencies. Unlike stylometrically determined signals in respect to author, genre or period, it is then an intratextual criterion for similarity and disparity. The study’s subject is therefore not a large text corpus of different authors and periods, but a single literary text that comes into analytical focus. This approach to text selection is oftentimes called ‘microanalysis’ (Hoover, 2017). The term does not only differ from buzzwords such as ‘big data’, it also emphasizes the differences to concepts such as ‘macroanalysis’ (Jockers, 2013) and ‘distant reading’ (Moretti, 2000; 2005) despite their comparable quantitative techniques.
                Surprisingly, studies on the stylistic differentiation of character speech are mostly limited to novels even though the structure of dramatic texts makes a quantitative examination of dramatic character speech easier: The speech is neither sorted nor commented nor framed by a narrator. By consequence and in contrast to narrative texts, the character speech can be isolated automatically. Initial approaches are already available: E.g., John Burrows and Hugh Craig show that individual drama characters can indeed be successfully assigned to an author’s signal (Burrows, Craig, 2012). Both argue against critics who question a successful attribution of dramatic texts to an author, as Masten (1997) does who claims that the lack of narrators would lead to many indistinguishable voices.
            
            
                Distinctive Character Speech in Dramatic Texts?
                
                    Figure 1
                    
                        
                            Figure 1, 
                            2 and 
                            6 were generated using the ‘stylo’ package for R. 
                            Figure 3, 
                            4 and 
                            5 were created using the ‘DramaAnalysis’ package for R (Nils Reiter, Marcus Willand). https://github.com/quadrama/DramaAnalysis. The visualization of 
                            Figure 2 was done in Gephi.
                        
                     is based on David Hoover’s approach in 
                    The Microanalysis of Style Variation (2017) but is applied to the genre of drama. The hierarchical cluster analysis in 
                    Figure 1 illustrates the various characters of Gotthold Ephraim Lessing’s 
                    Minna von Barnhelm, 
                    oder das Soldatenglück (1767) in regard to their similarity. As one of the plays of “Lessing’s maturity” (Worvill, 2005: 177) 
                    Minna von Barnhelm seems to be an appropriate drama to discuss its characters and their speech. Michael Metzger, e.g., argues that Lessing created “a characteristic pattern of language for each of the various roles he has written” (Metzger, 1966: 196; see also Worvill, 2005; Asmuth, 2009).
                
                The stylometric analysis is based on word frequency lists which are extracted from the individual characters’ utterances. With the help of ‘Cosine Delta’, that is claimed to achieve more reliable results than ‘Burrows’s’ or Argamon’s Delta’ (Evert et al., 2017), the speeches’ relative stylistic similarity is calculated by means of word frequencies. Contrary to Hoover’s approach, the character speech is not divided into artificial segments of 1500 words each but by its ‘naturally given’ act boundaries.
                    
                         The act boundaries are marked with underscores in the illustration. 
                     This is helpful for the interpretation of the stylometric results based on the conditions of their emergence, such as the co-presence of characters. The procedure’s disadvantages are the speech segments’ inconsistencies: Some segments fall below a length of 700 words and must be excluded.
                    
                         To compare: Fields, Bassist and Roper use segments of only 200 words each.
                     It also eliminates the so-called possibility of ‘randomization’, as it is practiced by Hoover: the individual character speeches’ word distribution is randomly assigned to the segments in order to ‘normalize’ outliers. However, one should be cautious regarding the random distribution of words since potentially better results can only be measured by the underlying hypothesis.
                
                
                    
                
                
                    Figure 1: Dendrogram of 
                    Minna von Barnhelm, 1000 MFW, no culling, Cosine Delta, Ward Clustering.
                    
                         Although some speech segments fall below a length of 1000 words, it should still be feasible to use a vector length of 1000 MFW (Eder, 2017b). The results of 
                            Figure 2 support this hypothesis, but a larger scale study on this topic is a future task.
                        
                    
                
                
                    Minna von Barnhelm’s stylometric analysis indicates certain signs of stylistically distinctive character speech: E.g., Tellheim’s speech – he is the male protagonist of the play – from Act 1 and 3 is grouped in immediate vicinity. The same holds true for the speech in Act 3 and 5 taken from Paul Werner. However, most of the speech segments seem to follow a different criterion. This is particularly evident for the uppermost section of the chart: The speeches by Major Tellheim, Minna von Barnhelm, Franziska (Minna’s chambermaid) and the landlord (Wirt) are grouped on a contiguous branch, i.e. they resemble the other segments stylistically. Those four segments of speech belong to the drama’s second act. There are other examples that seem to confirm act boundaries as an important factor for the analysis’ results. The most striking ones are those of Tellheim and Minna in both Act 4 and 5. The analysis shows that the results by Hoover, Fields, Bassist and Roper cannot be transferred to Lessing’s dramatic text directly.
                
                A single dendrogram, however, must not be more than a first indication for the assessment of the hypothesis. To avoid a potential ‘cherry picking’ problem at this point, further stylometric analyses on an expanded corpus were conducted.
                    
                         I analyzed 13 texts – three by Lessing, four by Friedrich Schiller, three by Johann Wolfgang Goethe and three by Friedrich Hebbel – with a total of 175 speech segments. Parameters used: 1000 MFW, no culling, Cosine Delta, Ward Clustering. The visualization is not shown in the paper. 
                     Both, the author’s signal (175 of 175 segments matching) and the text unity (171 of 175 segments matching) can be clearly identified. Thus, the cluster analysis does not seem to be influenced negatively by the relatively small sizes of the speech segments. 
                    Figure 2, a network plot that uses the same corpus, consolidates this finding.
                    
                         See Eder (2017a) for advantages of stylometrics visualized by network plots. 
                    
                
                
                    
                
                
                    Figure 2: Stylometric network of 13 dramas. 500–1500 MFW, no culling, Cosine Delta, three nearest neighbors. Node sizes represent average degree, node colors represent modularity rank.
                
            
            
                Co-presence and Character Semantics
                Stylometrics are not the only method to determine relative similarities within a text corpus. The extent to which they are suitable to discuss open questions – in contrast to, e.g., author attribution – remains to be examined. If parameters such as distance measures, word size or culling must be redefined with respect to the text corpus, ‘cherry picking’ would then become inherent to the method (Schöch, 2014; Jannidis 2014; Eder, 2013). It is therefore necessary to compare the established observations to other quantitative methods. This is done by means of analyzing co-presence and semantics of character speech.
                
                    Figure 3 illustrates the speech parts of the six most important characters in Lessing’s 
                    Minna von Barnhelm. The following investigation focuses on the protagonists Tellheim and Minna. In the second, but especially in the fourth and fifth act, Tellheim and Minna are mainly co-present. This structural data correlates with the observations in 
                    Figure 1. The speech segments of those acts are grouped closely together, while Tellheim’s speech in Act 1 and 3 is clearly separated. In these two acts Tellheim and Minna are not co-present.
                
                
                    
                
                
                    Figure 3: Co-presence in 
                    Minna von Barnhelm.
                
                The observation that stylistic similarities of the character speech is related to structural characteristics challenges earlier research and demands further investigation: Is it possible to expand or specify this finding? A semantic word field analysis, as used by Willand and Reiter (2017), serves to operationalize the thematic conception of character speeches.
                    
                         For this purpose, five dictionaries on the topics of family, war, love, ratio and religion were created, enlisting 65 to 110 words each. The words were used in dramas between 1770 and 1830 (Willand, Reiter 2017).
                    
                    Figure 4 illustrates two diagrams that compare different segments of Tellheim’s and Minna’s speech. The figure on the left compares Tellheim’s speech in Act 1 and 5. It indicates significant semantic differences in those segments that also showed little similarity in terms of style. The themes ‘love’ and ‘ratio’ are given greater weight in Act 5, while the context of ‘family’ is invoked less frequently. All in all, one can clearly detect a discrepancy in the semantic fields’ word frequencies.
                
                
                    
                
                
                    Figure 4: Semantic fields in 
                    Minna von Barnhelm.
                
                The diagram on the right shows the semantic fields of Minna and Tellheim in the fifth act. Compared to the diagram on the left, the two speeches of Tellheim and Minna seem to correlate better with each other, especially considering the word fields ‘love’ and ‘ratio’. None of the word fields is conspicuous due to extreme differences. Whether this observation can actually be used as a marker for similar topics or not has to be proofed within a larger text corpus. By consequence, this would be useful to determine a threshold value to mark similarity and disparity. I started this task using the Euclidean distance to measure the similarity between different segments of character speech in 
                    Minna von Barnhelm. It results in the following values of similarity:
                
                
                    
                
                
                    Figure 5: *denotes co-presence, yellow colored values are nearest neighbor segments as taken from the stylometric analysis (
                    Figure 1), lower values display a higher similarity.
                
                Average of the four nearest neighbor segments: 0,008703
                    Average of Tellheim’s segments (without nearest neighbors): 0,012026667
                
                The difference of the two groups’ average margin is a value of 0,00332, or 38,2 percent. Although the sample size is still small, one dramatic text only, this seems to be quite a significant result. Thus, the word field semantics do at least provide an indication that style, theme and presence of characters are related to some extent.
            
            
                Conclusion
                A closer examination of the character speech in 
                    Minna von Barnhelm has shown that it is plausible to combine different analytical methods. Thus, the investigation benefits from their respective strengths. Herein, results can be validated and opened for broader questions. In the chosen dramas, co-presence seems to have an impact not only on style but also on the semantics of character speech. The segments spoken by the two protagonists in Act 5 of 
                    Minna von Barnhelm exemplify this thesis. These results differ from Hoover’s and suggest having a closer look on co-presence and its influence on the distinctiveness of character speeches in dramas as well as in novels. The absence of a narrator in dramatic texts is one possible starting point to explain the differences outlined in this paper. 
                
            
        
        
            
                
                    Bibliography
                    Asmuth, B. (2009). 
                        Einführung in die Dramenanalyse. 7th ed. Stuttgart, Weimar: J. B. Metzler.
                    
                    Bastian, M., Heymann, S. and Jacomy, M. (2009). Gephi: An Open Source Software for Exploring and Manipulating Networks. 
                        International AAAI Conference on Web and Social Media, North America.
                    
                    Burrows, J. and Craig, H. (2012). Authors and characters. 
                        English Studies, 93(3): 292–309.
                    
                    Eder, M. (2013). Computational Stylistics and Biblical Translation: How Reliable Can a Dendrogram Be?. In Piotrowski T. and Grabowski Ł. (eds.), 
                        The Translator and the Computer. Breslau: WSF Press, pp. 155–170.
                    
                    Eder, M. (2017a). Visualization in Stylometry: Cluster Analysis Using Networks. 
                        Digital Scholarship in the Humanities, 32 (1): 50–64.
                    
                    Eder, M. (2017b). Short Samples in Authorship Attribution: a New Approach. 
                        Digital Humanities 2017. Conference Abstracts. Montréal: McGill University and Université de Montréal, pp. 221–224. 
                    
                    Eder, M., Kestemont, M. and Rybicki, J. (2013). Stylometry with R: a Suite of Tools. In: 
                        Digital Humanities 2013. Conference Abstracts. Lincoln: University of Nebraska, pp. 487–489.
                    
                    Evert, S., Proisl, Th., Jannidis, F., Reger, I., Pielström, S., Schöch, Ch. and Vitt, Th. (2017). Understanding and Explaining Delta Measures for Authorship Attribution. 
                        Digital Scholarship in the Humanities. https://doi.org/10.1093/llc/fqx023.
                    
                    Fields, P. J., Bassist, L. and Roper, M. (2017). Characters in 19th Century Novels Display Distinctive Voices as Seen by Stylometric Analysis. In 
                        Digital Humanities 2017. Conference Abstracts. Montréal: McGill University and Université de Montréal. https://dh2017.adho.org/abstracts/494/494.pdf.
                    
                    Hoover, D. (2017). The Microanalysis of Style Variation. 
                        Digital Scholarship in the Humanities. https://doi.org/10.1093/llc/fqx022.
                    
                    Jannidis, F. (2014). Der Autor ganz nah. Autorstil in Stilistik und Stilometrie. In Schaffrick M. and Willand M. (eds.), 
                        Theorien und Praktiken der Autorschaft. Berlin, Boston: De Gruyter, pp. 169–195.
                    
                    Jockers, M. (2013). 
                        Macroanalysis: Digital Methods and Literary History. Urbana, Chicago, Springfield: University of Illinois Press.
                    
                    Masten, J. (1997). 
                        Textual Intercourse: Collaboration, Authorship and Sexualities in Renaissance Drama. Cambridge: UP.
                    
                    Metzger, Michael M. (1966). 
                        Lessing and the Language of Comedy. The Hague, Paris: Mouton.
                    
                    Moretti, F. (2000). Conjectures of World Literature. 
                        New Left Review, 1: 54–68.
                    
                    Moretti, F. (2005). 
                        Graphs, Maps, Trees: Abstract Models for a Literary History. London: Verso.
                    
                    Schöch, Ch. (2014). Corneille, Molière et les autres. Stilometrische Analysen zu Autorschaft und Gattungszugehörigkeit im französischen Theater der Klassik. 
                        Literaturwissenschaft im digitalen Medienwandel. Beihefte zu Philologie im Netz, 7: 130–157.
                    
                    Willand, M. and Reiter, N. (2017). Geschlecht und Gattung: Digitale Analysen von Kleists ‘Familie Schroffenstein’. 
                        Kleist Jahrbuch, 2017: 177–195.
                    
                    Worvill, R. M. (2005). 
                        ‘Seeing’ Speech: Illusion and the Transformation of Dramatic Writing in Diderot and Lessing. Oxford: Voltaire Foundation.
                    
                
            
        
    

        
            
                Introduction
                Books written by and marketed towards women have been analyzed mostly in the context of popular culture (
                    Radway, 1987; Hollows, 2000; Modleski, 2008). In literary criticism however, fictional work by women is regularly held up to such ‘women’s novels’ to measure the quality 
                    (van Boven, 1992; Vogel, 2001; Groos, 2011). This connection made between female author gender and popular feminine novels is likely based on bias, but it is not yet well-researched in computational stylistics. In this paper we present a pilot study for examining this potential bias, through the combination of a reader survey and text analysis. 
                
            
            
                Related work
                Although computational stylistics is now quite common in analysis of fiction 
                    (i.e. Semino and Short, 2004), ‘women’s’ genres are not researched often in relation to literature. 
                    Jautze et al. (2013) focuses on differences between the syntactic make-up of sentences in literary novels and so-called ‘chick lit’ (cf. Ferriss and Young, 2013); 
                    Montoro (2012) performs computational-linguistic analysis on chick lit as opposed to a BNC sampler corpus – but not to literary fiction specifically. 
                
            
            
                Women’s books 
                What is the relationship between books by women and ‘women’s books’ according to readers? We examine this through results of the National Reader Survey 
                    (2013). Respondents were supplied with a list of 401 recent Dutch-language novels (translated and originally Dutch, published between 2007-2012) that were most often loaned from libraries and bought from bookstores between 2009-2012 (Koolen et al., in preparation).
                    
                         Note that the Riddle corpus’ novels show the one-sidedness of the market: it consists of few genres, there are very few novels by people of color, it contains mostly European and North-American novels. 
                    ,
                    
                         The factor of translation will be taken into account in further development of this pilot, for information on effects within the larger project, see van Dalen-Oskam, 2016. 
                     Respondents supplied ratings of literary quality on books they had read (on a scale of 1-7) and were allowed to motivate one of their scores. 
                
                 Overall, works by female authors are judged to have lower literary quality (M=3.92, SD=0.81) than those by male authors (M=4.73, SD=1.04); t(344)=-8.34, p &lt; 0.01. This is partially caused by romantic novels, which are mainly written by women (M=3.02, SD=0.60).
                    
                         To distinguish genres, we roughly base ourselves on Dutch publishers’ assignments of genre, which is done through a uniform classification system in the Netherlands. 
                     More surprisingly, within general fiction female authors’ works scores’ (M=4.55, SD=0.84) are significantly lower than for male’s (M=5.53, SD=0.73); t(120)=-7.60, p&lt;0.01. 
                
                An analysis of the motivations shows that the concept of the ‘women’s book’ (‘vrouwenboek’) and similar gendered terms are used dozens of times to explain what literary quality is 
                    not; a male equivalent is mentioned twice (‘men’s book’, ‘boy’s book’). Examples of novels referred to as ‘women’s’ book’ are translations of 
                    Eat, Pray, Love by Gilbert (general fiction), 
                    Remember Me? by Kinsella (romantic fiction) and 
                    The Ice Princess by Låckberg (suspense). Thus, works by female authors are equated with ‘women’s books’ regardless of the novel’s own genre. Perceived connections that respondents provide are: bad story (about love), a simple style, no deeper layers, etc.. But how much do ‘women’s books’ differ from novels that are perceived as literary? And are they more strongly connected to other female-authored novels than to male-authored ones?
                
            
            
                Text analysis
                We perform two experiments as a first exploration. We compare present-day romantic novels by female authors (R), predominantly chick lit, to general fiction by women (GF) and general fiction by men (GM). We select the lowest scoring novels in the romantic genre and the highest in the general fiction genre (i.e. the most ‘literary’ ones according to our respondents), to find the clearest contrast (cf. Table 1). We use only one novel per author, unless the author uses a different pen name (Kinsella/Wickham). 
                
                    Table 1. Division of books in the sub-corpus
                    
                        
                            Genre / gender author (av. rating literariness)
                        
                        
                            Transl. from English
                        
                        Originally Dutch
                    
                    
                        Romantic / female (2.8)
                        10
                        2
                    
                    
                        General fiction / female (5.2)
                        10
                        2
                    
                    
                        General fiction / male (5.9)
                        10
                        2
                    
                
                
                    Experiment 1: style
                    As we have shown, the style of ‘women’s books’ is seen as inferior. We use stylometric analysis to explore this notion, adding Gilbert’s 
                        Eat, Pray, Love to this experiment (cf. Section 3); a hybrid of general fiction and romance. Stylometric analysis is most often used to perform authorship recognition, but has been successfully applied to identify gender 
                        (Rybicki, 2015) and fictional genres (Allison et al., 2011). We apply the method detailed in Eder (2017). First, with R-package Stylo (Eder et al., 2016), we construct a bootstrap consensus tree based on the 100 through 1,000 most frequent words with 100-word intervals, using Classic Delta to calculate stylistic similarity (cf. Eder, 2017). Second, we use network analysis and visualization tool Gephi to visualize the novels’ connectedness (Bastian et al., 2009). Color-codes are based on modularity, which visualizes groupings of greater inner coherence (Blondel et al., 2018). Finally, we apply the ForceAtlas2 algorithm to make groupings more visually distinct. 
                    
                    
                        
                        Network visualization of the novels’ stylistic proximity (R = romantic, GF = general fiction/female author, GM = general fiction/male author). Colors indicate groupings based on modularity
                    
                    Fig. 1 shows six clusters. Part of the romantic novels (blue, soft pink) are indeed separated from the general fiction (other colors); Stockett’s 
                        The Help is stylistically connected strongest to romantic novels. General fiction by female and male authors hardly form clusters of their own. Except for one ‘male’ cluster which contains a Barnes’ novel and an outlier: Gilbert’s novel – which is seen as a ‘women’s novel’ by our respondents. Weiner, known for opposing the ‘chick lit’ label to her work 
                        (Mead, 2014) has a stronger connection to general fiction. In other words, stylistically seen, part of the romantic novels appear to have a specific signature, but most novels by female authors are not obviously stylistically connected to them.
                    
                
                
                    Experiment 2: sentiment 
                    We now use Linguistic Inquiry and Word Count (LIWC), a word list analysis tool, which has a dictionary for Dutch (Boot et al., 2017) and has been applied to literary fiction in genre analysis (Nichols et al., 2014). LIWC contains a number of content and sentiment-related categories that are of interest. Attention to physical appearance, a (heterosexual) love story, work and friendship and have been identified as themes of chick lit novels 
                        (Gill and Herdieckerhoff, 2006), which are the main component of the romantic genre in this corpus. We report significant differences on salient categories in an independent 
                        t-test between averages of groups (p &lt; 0.01). 
                    
                    
                        Table 2. Significant differences (p &lt; 0.01) between groups
                        
                            
                                LIWC category
                            
                            
                                Romantic-Gen. Female
                            
                            
                                Romantic-Gen. Male
                            
                            
                                Gen. Female-Gen. Male
                            
                        
                        
                            Articles
                            
                             X
                            
                        
                        
                            Prepositions
                            
                             X
                            
                        
                        
                            Affect
                             X
                             X
                            
                        
                        
                            Posemo
                             X
                             X
                            
                        
                        
                            Negemo
                            
                            
                            
                        
                        
                            Social
                            
                             X
                            
                        
                        
                            Communication
                             X
                             X
                            
                        
                        
                            Friends
                             X
                             X
                            
                        
                        
                            Job
                             X
                            
                            
                        
                        
                            Swearwords
                             X
                            
                            
                        
                    
                    Table 2 shows that romantic novels differ from general fiction in some ways: more positive emotions, but no significant difference in negative emotions, more words pertaining to friendship. The romantic novels differ in other ways from either the female or the male-authored literary novels: there are more job-related words in the romantic novels than in female-authored general fiction; less articles and prepositions than male-authored general fiction. Female-authored literary novels and male-authored ones do not significantly differ on any category. This might indicate that when comparing literary fiction to romantic novels, readers choose to focus on commonalities with female authors and differences with male authors, whereas differences between female authors and commonalities with male authors are overlooked. However, we need to be careful with interpretations of 
                        t-tests in LIWC 
                        (cf. Koolen and van Cranenburgh, 2017). Additional analysis will need to be performed to identify within-group differences. Finally, physicality and the body do not appear to be specific to romantic novels. This finding corroborates earlier research, see Montoro (2012) and 
                        Koolen (2018).
                    
                
            
            
                Conclusion
                Romantic novels appear to be more different from all general fiction than the general fiction differs among authors of female and male gender. They contain signature elements, albeit not all the expected ones (positive emotions and friends, not attention to appearance). Part of the romantic novels are clearly different from general fiction stylistically, but a number of them cluster with male-authored general fiction; most notably work by Gilbert and Weiner. Although further testing is needed, they show that computational stylistic analysis might be used to paint a more objective picture of the actual style of contemporary novels by female authors and the relationships between them. We offer a speculation: if we consider the romantic novels in this corpus to be ‘women’s novels’, there are a several indications that commonalties between female-authored general fiction and romantic novels are stressed heavily and this might be a reason female authors’ novels are judged to have less literary quality. Nevertheless, we do not aim to assert ‘low’ literary quality of the romantic novels, either. To examine gendered quality perceptions further, we will include other fictional genres in future research.
            
        
        
            
                
                    Bibliography
                    
                        Allison, S. D., Heuser, R., Jockers, M. L., Moretti, F. and Witmore, M. (2011). Quantitative formalism: an experiment. 
                        Stanford Literary Lab
                        https://litlab.stanford.edu/LiteraryLabPamphlet1.pdf (accessed 27 November 2017).
                    
                    
                        Bastian, M., Heymann, S. and Jacomy, M. (2009). Gephi: an open source software for exploring and manipulating networks. 
                        International AAAI Conference on Weblogs and Social Media, 
                        8: 361–62.
                    
                    
                        Blondel, V. D., Guillaume, J.-L., Lambiotte, R. and Lefebvre, E. (2008). Fast unfolding of communities in large networks. 
                        Journal of Statistical Mechanics: Theory and Experiment, 
                        2008(10): P10008.
                    
                    
                        Boot, P., Zijlstra, H. and Geenen, R. (2017). The Dutch translation of the Linguistic Inquiry and Word Count (LIWC) 2007 dictionary. 
                        Dutch Journal of Applied Linguistics, 
                        6(1): 65–76.
                    
                    
                        Eder, M. (2017). Visualization in stylometry: cluster analysis using networks. 
                        Digital Scholarship in the Humanities, 
                        32(1): 50–64.
                    
                    
                        Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: a package for computational text analysis. 
                        R Journal, 
                        8(1): 107–21.
                    
                    
                        Ferriss, S. and Young, M. (2013). 
                        Chick Lit: The New Woman’s Fiction. New York: Routledge.
                    
                    
                        Gill, R. and Herdieckerhoff, E. (2006). Rewriting the romance: new femininities in chick lit?. 
                        Feminist Media Studies, 
                        6(4): 487–504.
                    
                    
                        Groos, M. (2011). Wie schrijft die blijft? Schrijfsters in de literaire kritiek van nu (Who writes remains? Female writers in today’s literary criticism). 
                        Tijdschrift Voor Genderstudies, 
                        3(3): 31–36.
                    
                    
                        Hollows, J. (2000). 
                        Feminism, Femininity and Popular Culture. Oxford: Manchester University Press.
                    
                    
                        Jautze, K., Koolen, C., van Cranenburgh, A. and de Jong, H. (2013). From high heels to weed attics: a syntactic investigation of chick lit and literature. 
                        Proceedings of the Workshop on Computational Linguistics for Literature. Atlanta, GA, USA: Association for Computational Linguistics, pp. 72–81.
                    
                    
                        Koolen, C. (2018). 
                        Reading Beyond the Female: the Relationship between Perception of Author Gender and Literary Quality. Amsterdam: University of Amsterdam.
                    
                    
                        Koolen, C. and van Cranenburgh, A. (2017). These are not the stereotypes you are looking for: bias and fairness in authorial gender attribution. 
                        Proceedings of the First Workshop on Ethics in Natural Language Processing. Valencia, Spain: Association for Computational Linguistics, pp. 19–29.
                    
                    
                        Koolen, C., van Dalen-Oskam, K., van Cranenburgh, A., Nagelhout, E. and de Jong, H. (in preparation). Literary quality in the eye of the Dutch reader: the National Reader Survey and its results.
                    
                    
                        Mead, R. (2014). Written off: Jennifer Weiner’s quest for literary respect. 
                        The New Yorker
                        https://www.newyorker.com/magazine/2014/01/13/written-off (accessed 27 November 2017).
                    
                    
                        Modleski, T. (2008). 
                        Loving with a Vengeance: Mass Produced Fantasies for Women. New York: Routledge.
                    
                    
                        Montoro, R. (2012). 
                        Chick Lit: The Stylistics of Cappuccino Fiction. London, New York: Bloomsbury Publishing.
                    
                    
                        National Reader Survey (2013). 
                        Het Nationale Lezersonderzoek. 
                        https://www.hetnationalelezersonderzoek.nl/ (accessed 26 April 2018).
                    
                    
                        Nichols, R., Lynn, J. and Purzycki, B. G. (2014). Toward a science of science fiction: applying quantitative methods to genre individuation. 
                        Scientific Study of Literature, 
                        4(1): 25–45.
                    
                    
                        Radway, J. A. (1987). 
                        Reading the Romance: Women, Patriarchy and Popular Literature. London: Verso.
                    
                    
                        Rybicki, J. (2015). Vive la différence: tracing the (authorial) gender signal by multivariate analysis of word frequencies. 
                        Digital Scholarship in the Humanities, 
                        31(4): 746–61.
                    
                    
                        Semino, E. and Short, M. (2004). 
                        Corpus Stylistics: Speech, Writing and Thought Presentation in a Corpus of English Writing. New York: Routledge.
                    
                    
                        van Boven, E. (1992). 
                        Een Hoofdstuk Apart: ‘Vrouwenromans’ in de Literaire Kritiek 1898-1930 (A Separate Chapter: 'Women’s Novels’ in Literary Critique 1898-1930). Amsterdam: Sara/Van Gennep.
                    
                    
                        van Dalen-Oskam, K. (2016). ‘Could be the translation, of course’. Analysing the perception of literary fiction and literary translations. 
                        Digitalität in Den Geisteswissenschaften. Loveno di Menaggio, Italy.
                    
                    
                        Vogel, M. (2001). 
                        ‘Baard Boven Baard’: Over Het Nederlandse Literaire En Maatschappelijke Leven 1945-1960 (’Beard over Beard’: On Dutch Literary and Societal Life 1945-1960). Maastricht: Maastricht University.
                    
                
            
        
    

        
            We present ongoing research using data visualization and complex network analysis to historicize the production of three periodicals: 
                Fireweed, Fuse, and 
                Border/Lines. Computational methods allow for the visualization of metadata describing these magazine issues as a complex network – but what do these visualizations reveal about real social relations involved in the production and circulation of these magazines? 
            
            
                Fireweed, Fuse, and 
                Border/Lines emerged between 1976 and 1986 in Toronto, Canada, from a hotbed of lesbian and gay liberation, feminist and cultural race politics, thereby circulating in relation to transnational social, political and cultural movements (Butling and Rudy, Gonosko and Marcellus, Monk, Robertson). Whereas digital art historical scholarship often applies computational methods to the analysis of visual images (Zorich, Manovich), this paper instead applies complex network analysis to bibliographic metadata describing artist-led magazine publishing. We propose that there is a correlation between the magazine as a site of imagined community (as a discursive site where artistic scenes and poetic community are formed) (Allen, 12-17); and the complex networks visualized from metadata describing production teams and content of each printed issue (Knight, Long, Lincoln, Liu). 
            
            At this time, we have completed the data gathering stage. Prior to our initiative, 
                Fireweed and 
                Fuse were not digitized, nor were they comprehensively indexed on digital platforms. A complete data set was created using human cataloguers and a pre-existing metadata schema developed for the e-artexte open repository of publications on contemporary art. 
                Border/Lines was previously digitized, and housed in an open journal repository. However, this online collection is not complete, further, it was not possible to extract the metadata from this platform in a consistent format
                . Contributor names and roles were indexed for each magazine issue (editor, author, translator, etc.). Many of the contributor names and roles already exist within the e-artexte authority files, and standard indexing protocols were expanded to include roles that are not usually recorded in the metadata (members of editorial committees, designers, typesetters, etc.). 
            
            Once indexed in e-artexte, the data became publicly accessible and exportable into various formats, including EPrints XML. A conversion to Graph GML files used Apache Pig Latin scripts (Neugebauer). The resulting Graph GML data was imported into Gephi.
            To borrow an expression from Hoyt Long’s mapping of literary community, resulting graphs encourage a “sliding back and forth” between the macroscale of the generated graphs and the microscale of the discourse of the artistic and poetic communities represented (316). 
            A Multi Modal graph (Figure 1) maps relationships between individual magazine issues, contributors (writers, editors, and designers, etc.), artists as subjects of articles, and publishers. Edges were assigned a colour according to magazine title. Node size has been mapped to betweenness centrality, with a filter applied to a range higher than .01. 
            Lisa Steele and Clive Robertson feature prominently as contributors to 
                Fuse magazine, with a high degree of betweenness centrality. This is not a surprise, as both authored multiple articles in the magazine, are founding editors and key figures in the Toronto artistic and activist scenes bridged through the magazine’s content (Robertson, Monk). More remarkable is the prominence of Lynne Fernie in the network, best known for later success as the director of documentary films addressing LGBTQ histories. Fernie’s high degree of betweenness centrality and position as a connector between the cluster of nodes surrounding 
                Fuse magazine and 
                Fireweed, provides a bridge between these two magazines as spaces that shared an impulse towards lesbian and feminist liberation. Poet and activist Dionne Brand, who works at the intersection of race and gender, also bridges 
                Fuse and 
                Fireweed. Cultural policy analyst Jody Berland, and gay activist and environmentalist Alexander Wilson bridge 
                Fuse and 
                Border/Lines. Feminist cultural historian Rosemary Donegan bridges all three discursive spaces
                .
            
            A second graph, a Single Mode Contributor Projection will map relationships between individual contributors through their frequency of co-occurence in magazine issues. The graph will be filtered through edge weight, which represents co-occurrence in a minimal number of journal issues. We will colour the graph through community detection on this network of contributor relations using the modularity functionality in Gephi (
                Blondel et al.).
            
            We anticipate that contributors with a high betweenness centrality will emerge as catalysts for artistic community as it is represented by the discursive spaces of these magazines. Although some of these names may be iconic, “famous” artists and writers, other careers may not have had the same trajectory of visibility. Additional graphs will be generated by publication year to illustrate how the network structure and centrality measures changed over time. 
            
                
            
            Figure 1. A Multi Modal graph (Figure 1) maps relationships between contributors and issues of magazines with a high degree of betweenness centrality: 
                Fuse (purple) previously titled, Centerfold (orange); Fireweed (green), and 
                Border/Lines (blue).
            
        
        
            
                
                    Bibliography
                    
                        Allen, G. (2016). 
                        The Magazine. Cambridge, Mass.: MIT Press. 
                    
                    
                        Butling, P. and Rudy, S. (2005). 
                        Writing in Our Time: Canada’s Radical Poetries in English (1957-2003). Waterloo, Ont.: Wilfred Laurier University Press. 
                    
                    
                        Gonosko, G. and Marcellus, K. (2005). Dead Downtown: Writing the Cultural Obituary of the Alternative Press. 
                        Topia, 14: 23-35. 
                    
                    
                        Knight, A. R. (2017). Putting them on the map: Mapping the Agents of the Colored Co-operative Publishing Company. https://www.arcgis.com/apps/MapSeries/index.html?appid=665eb933117f4ed68f0535b4560b5744
                    
                    
                        Lincoln, M. (2016). “Social Network Centralization Dynamics in Print Production in the Low Countries, 1550-1750” 
                        International Journal of Digital Art History 2: 134-157.
                    
                    
                        Long, H. (2015). “Fog and Steel: Mapping Communities of Literary Translation in an Information Age” 
                        The Journal of Japanese Studies, 41(2): 281-316. DOI 10.1353/jjs.2015.0062
                    
                    
                        Liu, A. (2012). “Friending the Humanities Knowledge Base: Exploring Bibliography as Social Network in RoSE.” NEH Office of Digital Humanities White Paper. https://rosedocumentation.files.wordpress.com/2012/07/rose-white-paper-as-submitted-to-neh.pdf
                    
                    
                        Manovich, L. (2015). “Data Science and Digital Art History” 
                        International Journal of Digital Art History 1:12-34. DOI: 10.11588/dah.2015.1.21631 
                    
                    
                        Neugebauer, T. (2017). “EPrintsData2GML” Eprints Interest Group, 2017 International Conference on Open Repositories. 
                        https://github.com/photomedia/EPrintsData2GML
                    
                    
                        Monk, P. (2016). 
                        Is Toronto Burning? 
                        Three Years in the Making (and Unmaking) of the Toronto Art Scene. Toronto: AGYU.
                    
                    
                        Robertson, C. (2006). 
                        Policy Matters: Administrations of Art and Culture.
                        Toronto: YYZ Books. 
                    
                    
                        Blondel, V.D. et al.
                         (2008). Fast unfolding of communities in large networks. 
                        Journal of Statistical Mechanics: Theory and Experiment 10: 10008-10020. DOI 10.1088/1742-5468/2008/10/P10008 
                    
                    
                        Zorich, D. (2012). “Transitioning to a Digital World: Art History, Its Research Centers, and Digital Scholarship,” 
                        
                            Kress
                            Foundation
                            .
                        
                        http://www.kressfoundation.org/news/Article.aspx?id=35338
                    
                
            
        
    

        
            There has been a longstanding debate over the cetology sections in Herman Melville's Moby Dick. These chapters, which are interwoven into the mid-section of the novel, are curiously devoid of characters or plot development and instead describe whaling biology and behavior. Some Melville scholars, including Charles Olson and Lawrence Buell, have suggested that the novel might have been written as two separate texts that were spliced together in the final stages. As the original manuscripts have been lost, this has never been confirmed. However, I hope to show that the way in which the chapters cluster together reveals that the novel does indeed have two unique stylistic signatures. This is perhaps compelling evidence in favor of the “two Moby Dicks,” a phenomenon that has been much speculated upon but never proven.
        
        
            
                
                    Bibliography
                    
                        Bastian M., Heymann S., Jacomy M. (2009). 
                        Gephi: an open source software for exploring and manipulating networks. International AAAI Conference on Weblogs and Social Media.
                    
                    
                        Eder, Maciej. Kestemont, Mike and Rybicki, Jan. (2015). 
                        ‘Stylo’: a package for stylometric analyses.
                    
                
            
        
    

        
            Publicaciones y exposiciones online, así como otros recursos interactivos, se encuentran entre los recursos online más utilizados por museos de arte en todo el mundo para transmitir historias vinculadas a obras de arte y colecciones. Los formatos tradicionalmente utilizados por museos para contar la historia del arte están siendo reconceptualizados a través de las cualidades y funcionalidades que nos ofrece el medio digital. El proceso experimental que aquí se expone nace con el objetivo de visualizar las particularidades que definen las narrativas generadas en los recursos online de museos de arte. Una selección de seis recursos online representativos de las tipologías más comunes producidos por museos de los Estados Unidos, España y Reino Unido* han sido la base para, por un lado, recabar datos sobre la perspectiva de los productores, y por otro, la de los usuarios especializados—una audiencia de perfil académico/investigador en el área de la historia del arte. Los datos se obtuvieron a través de dos métodos: entrevistas con los productores involucrados en la creación de los recursos online seleccionados, y en el caso de los usuarios, a través del protocolo conocido como “pensamiento en voz alta” (thinking aloud protocol) que ayuda a capturar información relevante a los procesos de navegación de los recursos online. Ambos procedimientos fueron grabados y transcritos para un facilitar el análisis posterior. Estos datos que se codificaron y analizaron desde una perspectiva narratológica permitiendo la observación de elementos configurantes de las narrativas: autoría, recepción como lectoespectador, estructuración, espacialidad, cronología. 
            Aunque en la investigación doctoral que da origen a los datos utilizados en esta propuesta se siguió una metodología cualitativa de corte más tradicional, en este póster se expone una aproximación experimental basada en la visualización de los códigos extraídos de las transcripciones. La visualización ofrece una visión complementaria al análisis inicial de los datos, orientado a presentar resultados de forma discursiva. De acuerdo con esta premisa, el póster compararía las posibilidades de análisis y presentación de las visualizaciones con el formato discursivo. Un análisis visual de los códigos revela aspectos cuantitativos de los datos, así como las conexiones entre los códigos de forma más explícita. En un cierto sentido, se presenta un resumen visual o vista general. La visualización de datos puede ayudar en la identificación de aspectos que habían sido obviados tras el empleo de la metodología más tradicional, y potencialmente, puede ofrecer nuevas conclusiones en la investigación. 
            La modalidad de visualización que se emplea en esta propuesta ha sido diseñada partiendo de diferentes metodologías y herramientas de visualización de datos. En primer lugar, toma como punto de partida en el uso de diagramas como herramienta de análisis narratológico (Ryan, 2007). Aunque el diseño de la metodología usada para visualizar datos en este póster emplea específicamente el procedimiento conocido como “map analysis” (Carley, 1993), éste permite la comparación de textos en base a los códigos extraídos y las relaciones entre ellos. Por otro lado, el trabajo de Luther (2017) propone un modelo y herramienta de visualización centrado en la representación de aspectos cualitativos y cuantitativos, éste fue desarrollado con el objetivo de estudiar aspectos de temática socio-histórico artística. No obstante, como herramientas se han elegido Gephi y d3.js ya que permiten representar la frecuencia de los códigos e interrelaciones. Las visualizaciones de este póster representan por separado los datos tanto de productores como de usuarios especializados de los recursos online, permitiendo comparar los seis recursos online. Las visualizaciones permiten estudiar las diferencias y similitudes existentes entre las perspectivas de los creadores, desde un punto de vista referente a la autoría, y las perspectivas de la audiencia especializada, como lectoespectadores de los recursos online. Conclusiones derivadas del proceso de visualización de datos serán argumentadas en el póster. Las visualizaciones se conciben como generadoras de discusiones además de ser una representación de la investigación llevada a cabo, éstas podrán ser consultadas en http://m-hidalgo.com. 
            Este trabajo es también resultado del proyecto de I+D: "HAR2014-51915-P. Catálogos artísticos: Gnoseología, epistemologías y redes de conocimiento. Análisis crítico y computacional”.
            *Los estudios de son recursos digitales de las siguientes instituciones: Museo Nacional del Prado, Museo Centro de Arte Contemporáneo Reina Sofía, National Gallery, Londres, National Gallery of Art, Washington DC, Metropolitan Museum of Art y MoMA.
        
        
            
                
                    Bibliography
                    
                        Carley, K. M. (1993). Coding choices for textual analysis: A comparison of content analysis and map analysis. Sociological Methodology, 23, 75–126. http://www.casos.cs.cmu.edu/publications/papers/carley_1993_codingchoices.PDF 
                    
                    Drucker, J., (2014). Graphesis. Visual Forms of Knowledge Production. Cambridge, MA.: Harvard University Press.
                    Flick, U., (2010). An Introduction to Qualitative Research. London: Sage Publications.
                    
                        Gee, K. (2001). The ergonomics of hypertext narrative: usability testing as a tool for evaluation and redesign. 
                        ACM J. Comput. Doc. 25, 1 (February 2001), 3-16. DOI=
                        
                            http://dx.doi.org/10.1145/383948.383950
                        
                    
                    
                        
                            Luther, A.
                         (2017). The Entity Mapper: A Data Visualization Tool for Qualitative Research Methods. 
                        
                            Leonardo
                        , Volume 50, Issue 3, June 2017. Cambridge: MIT Press, p.268-271. Doi:
                        
                             10.1162/LEON_a_01148
                        . Abstract available at: 
                        
                            http://www.mitpressjournals.org/doi/abs/10.1162/LEON_a_01148
                        
                    
                    
                        
                            Mann, L.
                         (2016). Online scholarly catalogues: Data and insights from OSCI. 
                        
                            MW2016: Museums and the Web 2016
                        . Consulted November 26, 2017.
                    
                    
                        http://mw2016.museumsandtheweb.com/paper/online-scholarly-catalogues-data-and-insights-from-osci/
                    
                    
                        
                            Ryan, M.
                         (2007). Diagramming narratives. Semiotica. 165: 1.4, 11-40. 
                    
                    
                        
                            Warwick, C.
                         (2013). Studying users in digital humanities. Terras, Melissa; Nyhan, Julianne, and Vanhoutte, Edward, eds., Defining Digital Humanities. A Reader. London: Routledge 
                        https://blogs.ucl.ac.uk/dh-in-practice/chapter-1/
                    
                
            
        
    

        
            Este panel se propone avanzar, desde un punto de vista metodológico, en el análisis a gran escala de la revista como institución cultural, una aproximación que puede cuestionar centros de producción literaria y revelar dinámicas de relación hasta ahora desconocidas entre las mal denominadas literaturas periféricas y los centros culturales hegemónicos. Para ello, los coordinadores de este panel proponemos cuatro presentaciones que abordan el estudio de revistas culturales históricas del ámbito hispánico y lusófono a través de diversos estudios de caso que utilizan herramientas digitales y combinan intereses disciplinares en los campos de la historia de las ideas, la historia cultural, los estudios de traducción, y la literatura comparada desde una perspectiva transnacional. El panel se propone dar muestra del estado actual del estudio de revistas culturales en los espacios hispánico y lusófono a través del uso de herramientas digitales que permitan avanzar en la discusión metodológica.
            A este respecto, las propuestas de comunicación que se presentan se enmarcan en proyectos de investigación en curso vinculados a distintas universidades de Bélgica, Alemania, Portugal y España (Universiteit Antwerpen, Universität Tübingen, Universidade Nova de Lisboa y Universitat Oberta de Catalunya, respectivamente). Estos proyectos privilegian a la revista cultural como objeto de estudio y emplean distintas herramientas y metodologías digitales, dando sobradas muestras de la riqueza de perspectivas analíticas dentro del campo de las Humanidades Digitales (digitalización de materiales impresos y POS-tagging, construcción de repositorios electrónicos y de portales de investigación, bases de datos relacionales, geolocalización y visualización). Los autores de las distintas presentaciones comparten un compromiso similar en la colaboración científica entre pares, gracias a la publicación en abierto de los datos recogidos en sus investigaciones (
                open source). Las comunicaciones presentarán los respectivos proyectos en curso, ejemplificando con estudios de caso que de ellos se derivan.
            
            En concreto, las comunicaciones del panel abordarán los siguientes objetos:
            
                La creación de un repositorio de textos a partir de revistas en las Filipinas entre 1850 y 1945, dentro del marco del proyecto “Digitization of Philippine Rare Periodicals and Training in DH”, con el propósito de facilitar el futuro estudio de textos históricos a través de herramientas digitales. Por medio de un análisis textual computacional, esta comunicación ejemplificará la utilidad de este repositorio con un estudio sobre la actitud que adopta la sociedad de habla filipina respecto de otros países en tres momentos concretos del siglo 
                    xx.
                
                La presentación del entorno digital “Revistas culturales 2.0” al servicio de investigadores de revistas culturales históricas en lengua española. En base a este portal, la comunicación presentará un estudio de redes sociales entre los autores, revistas y géneros literarios con el objetivo de centrarse en textos programáticos (editoriales, prólogos o manifiestos).
                La presentación de la base de datos de “Revistas de Ideias e Cultura” portuguesas del siglo 
                    xx que combina aproximaciones a partir de la historia de las ideas, la biblioteconomía y la ciencia de la información. La comunicación abordará las redes de recepción en revistas portuguesas en base a las obras y los nombres citados en ellas.
                
                La identificación y análisis de las traducciones literarias publicadas en revistas hispánicas en el primer tercio del siglo 
                    xx con el objetivo de descubrir publicaciones hasta ahora desconocidas y revelar las relaciones literarias y editoriales entre distintos órganos de la prensa cultural hispánica a escala transnacional. Este estudio se realizará a partir de los datos recogidos en el VRE “MapModern” sobre revistas clave españolas e hispanoamericanas, mediadores culturales, y su participación en eventos y organizaciones culturales internacionales.
                    
                
            
            
                Philippines at the crossroads: enhancing research on Philippine periodicals and finding transnational attitudes in them
                
                    Rocío Ortuño Casanova, Universiteit Antwerpen
                
                Key Words: Philippines, online repository, OMEKA, IIIF
                The Philippines has been historically the centre of intercontinental, cultural, and economic relations: between Asia and Europe (Spain) both, in the time of the Spanish invasion (1565-1898) and nowadays 
                    (Montobbio, 2004: 11, 13); between America and Asia since the Manila galleon (1565-1815) 
                    (Giráldez 2015)
                    ; and during the US invasion of the country (1902-1941,1946) (Kramer 398-407, 
                    San Juan 2000). However, the scarce research performed so far on the Philippines, and the difficulty of access to textual materials from the country have became two major problems for the study of these relations, in which the Philippines constitutes a blind spot. In order to address these problems, the 
                    AC/DC research group of the University of Antwerp is developing a 
                    project in partnership with the University of the Philippines and funded by 
                    VLIR-UOS to create an online repository of periodical publications in the Philippines, and to offer training in DH to potential users of this repository.
                
                This talk is structured into two parts. The first one will provide an overview of the digitization scene in the Philippines, and will present the VLIRUOS TEAM project 
                    “Strengthening Digital Research at the University of the Philippines System: Digitization of Philippine rare newspapers and magazines (1850-1945), and training in Digital Humanities”. The second part will offer an example of what kind of research results we expect to achieve with it.
                
                The project has the initial objectives of (1) making written documentation available online for perusal of researchers both, from the Philippines and abroad. (2) Increasing academic research on humanities in the Philippines by the diffusion of DH methodologies. Therefore, two actions will be implemented:
                
                    The creation of an online repository of Philippine periodicals published between 1850 and 1945 and hosted at the University of the Philippines. Although the University of Santo Tomás is also uploading their 
                        rare periodicals collection, and there are other incipient projects for digitization in the Philippines, this repository will differentiate itself by considering three aspects:
                    
                    
                        A social aim: how can this repository be useful to a wide Filipino public? 
                    
                    
                        Becoming useful to a range of researchers: how can we process the texts and what metadata are necessary to facilitate research for scholars from different disciplines such as linguistics, history or literature? 
                    
                    
                        Facing the challenges of the Philippine context such as 
                        slow internet or multilingualism in periodical publications.
                    
                    Organization of training session in DH and implementation of projects in four campuses of the University of the Philippines.
                
                One of the main objectives of the project is producing interdisciplinary research on the Philippines, based on the digitized materials, using digital tools. In this talk, one example of the kind of research results that we expect to achieve will be provided. We will show an analysis of adjectives related to Spain, China and the US in 1918 (end of World War I), 1930 (after the Crack) and 1936 (between the declaration of the Philippines as a Commonwealth state and the beginning of the Spanish Civil war) in the Philippine cultural magazine 
                    Excelsior, obtained with POS tagging of the text. It aims to find the attitude of the Philippine speaking society towards other countries at the beginning of 20
                    th century with computational 
                    text analysis (Computer linguistics). The data obtained allows to reach conclusions on historical and literary trends
                    .
                
                References:
                
                    Giráldez, A. (2015). 
                    The Age of Trade: The Manila Galleons and the Dawn of the Global Economy. Lanham, Maryland: Rowman &amp; Littlefield.
                
                
                    Kramer, P. (2006). 
                    The Blood of Government: Race, Empire, the United States, and the Philippines. Quezon City: Ateneo de Manila University Press.
                
                
                    Montobbio, M. (2004). 
                    Triangulando la triangulación: España/Europa-América Latina-Asia Pacífico. Barcelona: Cidob/ Casa Asia.
                
                
                    Roque, A. (2012). Towards a computational approach to literary text analysis. 
                    Workshop on Computational Linguistics for Literature. Montréal, Canada: Association for Computational Linguistics, June 8, 2012, pp. 97–104.
                
                
                    San Juan, E. (2000). 
                    After Postcolonialism: Remapping Philippines-United States Confrontations. Lanham, Maryland: Rowman &amp; Littlefield.
                
                
                    
                
            
            
                
                    Revistas culturales 2.0 – Portal e investigación
                
                
                    Teresa Herzgsell y Claudia Cedeño
                    , Universität Tübingen
                
                
                    Revistas culturales 2.0
                     es un portal creado en 2014 y desde entonces en uso. Está diseñado como un entorno virtual para investigadores de revistas culturales históricas en lengua española. Técnicamente, el portal se basa en el gestor de contenidos Drupal 8.0, en el que se han implementado funcionalidades específicas. Así se permite por una parte a los usuarios registrados etiquetar los materiales digitalizados (provenientes de los fondos de revistas culturales latinoamericanos del Instituto Iberoamericano de Berlín) mediante formularios prestablecidos. Y por otra parte se posibilita el intercambio de datos entre el entorno virtual y bibliotecas, y otros grupos de investigación. Las funciones que el portal ofrece actualmente son: 
                    Blog
                    , 
                    Biblioteca Virtual
                    , 
                    Red de Participantes
                    , 
                    Publicaciones
                    , 
                    Bibliografía
                     y 
                    Enlaces comentados
                    . En conjunto tienen el objetivo de construir puentes entre las bibliotecas y sus colecciones digitales, grupos de investigación y el público general interesado. Tenemos paralelamente dos tareas básicas: la de orientar sobre proyectos actuales de digitalización de revistas culturales hispánicas, tanto de España como de América Latina, y otra, no menos importante, la de impulsar el uso de herramientas y tecnologías de las humanidades digitales en la investigación.
                
                
                    Fruto de este último objetivo es la línea de investigación actual titulada 
                    Del modernismo a las vanguardias: procesos de modernización y redes transnacionales en revistas culturales de la modernidad
                    , proyecto grupal financiado desde 2017 por la Fundación Alemana de Investigación Científica (DFG). Este se conforma por dos subproyectos estrechamente entrelazados entre sí, con enfoque sobre el modernismo y las vanguardias, respectivamente. En ellos la revista cultural es considerada como una red que pone en contacto no solamente diferentes actores (colaboradores que pueden ser escritores, pintores, directores, etc.), sino también diferentes géneros textuales. El desarrollo de tales géneros está marcado tanto por las dinámicas intrínsecas del campo literario, como de factores externos como son el valor económico de la literatura en cuanto mercancía que se quiere vender a un público. Para poner en práctica este enfoque que conceptualiza la revista como red usamos métodos cuantitativos de análisis y visualización de redes basados en datos, métodos de muy reciente aplicación en el campo de la investigación de revistas (estudios pioneros en este sentido son So y Long, 2013 y Murphy et al., 2014).
                
                
                    Sobre la base de metadatos hemerográficos estructurados, se analizan con 
                    Gephi
                     revistas como redes bimodales (distribución autor-género y autor-revista). Sin embargo, la visualización en nuestro proyecto de investigación no es el objetivo final, sino un paso intermedio que permite reconocer patrones formales que nos dirigen a la hora de realizar después lecturas intensivas, enfocadas especialmente en textos programáticos como editoriales, prólogos, manifiestos, etc. En conjunto, pues, nuestra metodología es mixta y combina enfoques cuantitativos y cualitativos, lecturas distantes con cercanas.
                
                
                    Bibliografía:
                
                
                    Murphy
                    ,
                     J. S. et al. (2014). Visualizing Periodical Networks. 
                    The Journal of Modern Periodical Studies, V(1) (Special Issue).
                
                
                    So, R. J. y Long, H. (2013). Network Analysis and the Sociology of Modernism. 
                    Boundary 2, 40: 147-82.
                
                
                    Enlaces
                    :
                
                
                    
                        https://www.revistas-culturales.de/
                    .
                
                
                    http://gepris.dfg.de/gepris/projekt/327964298?language=en
                    
                
            
            
                Portuguese magazines of ideas and culture in the early decades of the 20
                    th century: reception networks within different political and artistic movements
                
                
                    Joana Malta and Pedro Lisboa, Universidade Nova de Lisboa
                
                For almost two decades now, the Seminário Livre de História das Ideias (Free Seminar of History of Ideas) research group has been working on building a comprehensive and extensive database containing some of the most important 20
                    th century Portuguese periodicals. The Magazines of Ideas and Culture project has adopted a multidisciplinary approach from the beginning, making use of knowledge from fields such as history of ideas, library science, and information science, with the aim of building a relational database that contains exhaustive information on authorship, quoted authors and works, subjects, concepts, and geographical names for all articles printed in these publications. For methodological purposes, any autonomous printed piece, whether an essay, a poem, an aphorism, a news story, etc., is considered an article. Final users of the database are thus presented with a corpus of texts and contextualising metadata that structure the more or less explicit programmatic and doctrinarian aspects of these magazines, from a perspective grounded in the fields of history of ideas and conceptual history.
                
                Not only do we propose to present the platform and the project, but also to show some of the possible outcomes obtained by using this information. Working mainly with data on quoted names and works, we bring to light common reception networks from magazines published by artists and intellectuals with diverse origins such as the republican Renascença Portuguesa, the anarchist movement, or the Modernist literary currents. We will be using information on all quoted names and works from every article published in the following groups of magazines: 
                    Nova Silva (1907), 
                    A Águia (1910-1932), and 
                    A Vida Portuguesa (1912-1915), all related to the Renascença Portuguesa movement; 
                    Atlantida (1915-1920), published both in Portugal and in Brazil; 
                    A Sementeira (1908-1919), 
                    Germinal (1916-1917), 
                    Suplemento Literário Ilustrado do Jornal A Batalha (1923-1927), and 
                    Renovação (1925-1926), from the anarchist camp; and 
                    Orpheu (1915), 
                    Eh Real! (1915), 
                    Exílio (1916), 
                    Centauro (1916), 
                    Sphinx (1917), 
                    A Tradição (1917) and 
                    Portugal Futurista (1917), magazines that were connected, in one form or another, to the Modernist movement. These publications had very different underlying programmatic and doctrinarian backgrounds, as well as a very asymmetric presence within the context of Portuguese magazines, both with periodicals that existed for extended periods (
                    e.g., 
                    A Águia was published from 1910 to 1932) and those with very short timespans (
                    e.g., a single issue of 
                    Portugal Futurista was published). We identify differences and similarities, both profound and subtle, between the intellectual and cultural frameworks underlying the selected group of magazines and the movements they belong to, as a means to gain better understanding of the ideological foundations of Portuguese political, cultural, artistic, and intellectual movements of the first decades of the 20
                    th century.
                
                References:
                
                    Andrade, L. C. de (2003). Introdução: quatro notas breves. In Andrade, L. C. (ed), 
                    Revistas, Ideias e Doutrinas. Leituras do Pensamento Contemporâneo. Lisbon: Livros Horizonte, pp. 11-18.
                
                
                    Andrade, L. C. de (1999). O Substantivo “intelectuais”. 
                    Cadernos de Cultura, 2: 23-41.
                
                
                    Andrade, L. C. de (2016). Revistas de Ideias e Cultura, 
                    http://ric.slhi.pt/A_Aguia/um_voo_singular_e_longo (accessed 22 November 2017).
                
                
                    Carrington, P. J. et al. (eds) (2005). 
                    Models and Methods in Social Network Analysis. Cambridge: Cambridge University Press.
                
                
                    Castro, Z. O. de (1996). Da história das ideias à história das ideias políticas. 
                    Cultura: Revista de História e Teoria das Ideias, VIII (2): 11-21.
                
                
                    Charle, C. (2004). 
                    Le Siècle de la Presse (1830-1939). Paris: Éditions du Seuil.
                
                
                    Freire, J. (1981). “A Sementeira”, do arsenalista Hilário Marques. 
                    Análise Social, XVII (67-68): 767-826.
                
                
                    Frigessi, D. (ed) (1979). 
                    La Cultura Italiana del '900 Attraverso le Riviste. Turin: Giulio Einaudi.
                
                
                    Guimarães, L. et al. (2013). 
                    Atlântida. A invenção da comunidade luso-brasileira. Rio de Janeiro: Contracapa.
                
                
                    Lisboa, P. (2015). Edição electrónica de revistas históricas. O caso de A Águia. In Rollo, M. F. and Amaro, A. R. (eds), 
                    República e Republicanismo. Coimbra: Caleidoscópio, pp. 133-145.
                
                
                    Martins, A. L. (ed) (2012). 
                    História da Imprensa no Brasil. São Paulo: Contexto.
                
                
                    Pluet-Despatin, J. et al. (eds) (2002). 
                    La Belle Époque des Revues. Paris: IMEC.
                
                
                    Santos, A. R. dos (1990). 
                    A Renascença Portuguesa: um movimento cultural portuense. Porto: Fundação Eng. António de Almeida.
                
                
                    Scott, J. (2000). 
                    Social Network Analysis: a handbook. London: Sage.
                
                
                    Seminário Livre de História das Ideias, Revistas de Ideias e Cultura, 
                    http://ric.slhi.pt/
                
                (accessed 22 November 2017).
                
                    Tebbel, J. and Zuckerman, M. E. (1991). 
                    The Magazine in America, 1741-1990. New York: Oxford University Press.
                
                
                    Wasserman, S. and Faust, K. (1994). 
                    Social Network Analysis: methods and applications. Cambridge: Cambridge University Press.
                    
                
            
            
                La traducción en revistas literarias hispánicas: una reflexión metodológica a partir del empleo de herramientas digitales
                
                    Diana Roig Sanz, Laura Fólica y Ventsislav Ikoff
                    , Universitat Oberta de Catalunya
                
                
                    El estudio de las publicaciones periódicas ha contado con una tradición académica generalmente acotada a los límites del espacio nacional; sin embargo, el giro transnacional que han experimentado las Humanidades y las Ciencias Sociales en los últimos años y la aparición de nuevas herramientas informáticas han permitido nuevos estudios con un marco interpretativo transnacional y a gran escala (Tolonen, 2016; Schelstraete y 
                    Von Remoortel, 2017) que ha revitalizado la bibliografía existente sobre el estudio de la prensa cultural.
                
                Ante este estimulante panorama, la reflexión sobre cómo analizar las traducciones diseminadas en las páginas periódicas no goza todavía del mismo interés y los investigadores se siguen abocando sobre todo a los aspectos temáticos, semánticos o semióticos de las publicaciones. No obstante, las traducciones publicadas en la prensa periódica constituyen un objeto de extrema y original riqueza a la hora de estudiar el proceso de recepción y transferencia de literaturas extranjeras, la difusión de autores y obras, la creación de redes transnacionales, así como el establecimiento o cuestionamiento de cánones literarios. Por otra parte, las herramientas digitales y el uso de los macrodatos permiten explorar estos aspectos desde una perspectiva espacial y temporal más amplia e inclusiva, que promueve la incorporación de voces hasta ahora desconocidas y de relaciones insospechadas, así como el cuestionamiento de distinciones estancas como la de "centro" y "periferia", no sólo en el campo cultural hispánico sino en el espacio internacional.
                
                    Esta comunicación se propone reflexionar sobre el uso de herramientas digitales y metodologías basadas en 
                    big data
                    , con el objetivo de rastrear y analizar las traducciones literarias publicadas en una selección de revistas culturales hispánicas de la primera mitad del siglo 
                    xx
                    .
                
                
                    Este análisis a gran escala se realizará a partir de los datos recopilados en una base de datos relacional, basada en el entorno digital de exploración 
                    
                        Nodegoat
                    
                     y creada para el proyecto 
                    
                        MapModern
                    
                    , Esta base nos permitirá trabajar con distintas publicaciones y geografías del ámbito hispánico: por ejemplo, las revistas españolas 
                    La Gaceta literaria
                     y 
                    Revista de Occidente
                    , las argentinas 
                    Sur
                     y 
                    Proa
                    , la mexicana 
                    Los Contemporáneos
                    , la chilena 
                    Ercilla
                    , entre otras.
                
                En este sentido, entre los problemas metodológicos que advertimos en el estudio de las traducciones publicadas en prensa periódica cabe señalar el límite del recorte o escala, la ausencia de datos o disparidad en sus grados de exhaustividad, la representatividad de los mismos, los errores generados a partir de la falta de coherencia entre índices y el contenido de las publicaciones, los problemas relativos a la digitalización de los materiales con tecnologías automáticas como OCR, la posible distorsión generada por las visualizaciones.
                
                    En definitiva, la reflexión metodológica sobre el estudio de las traducciones presentes en publicaciones periódicas nos permitirá arrojar luz a nuevas relaciones culturales entre estos órganos de prensa cultural en una escala hispánica muy amplia y en la diacronía del inicio del siglo 
                    xx
                    , nos permitirá descubrir mediadores/traductores, así como traducciones y originales hasta ahora no estudiados, promoviendo, de este modo, una apertura de los estudios literarios y culturales en espacios menos dominantes.
                
                Bibliografía:
                
                    Fólica, L. (2010). La traducción literaria en el periodismo cultural: representaciones de autores, traductores y lengua. 
                    Avatares de la comunicación y la cultura, 1: 122-143
                    .
                
                
                    Jockers, M. (2013). 
                    Macroanalysis: Digital Methods and Literary History. Springfield: University of Illinois.
                
                
                    Lafleur, H., et al. (2006). 
                    Las revistas literarias argentinas (1893-1967). Buenos Aires: El 8vo Loco.
                
                
                    Sanz Roig, D. (2016). Hacia una nueva historia literaria: redes, mediadores culturales y humanidades digitales. 
                    Puentes de Crítica: 40-49.
                
                
                    Sanz Roig, D. y Meylaerts R. (2016). Edmond Vandercammen, médiateur culturel: le monde hispanique et le réseau du 
                    Journal des Poètes. 
                    Lettres Romanes, 70 (3-4): 405-433.
                
                
                    Meylaerts, R., Sanz Roig D., Gonne, M. y Lobbes, T. (2016). Cultural Mediators in Cultural History: What do we learn from studying mediators’ complex transfer activities in interwar Belgium. In. Brems, E., Réthelyi, O. y Van Kalmthout, T. (eds), 
                    The Circulation of Dutch literature. Leuven: Leuven University Press.
                
                
                    Sanz Roig, D. y Meylaerts R. (en prensa). Paul Vanderborght and 
                    La Lanterne sourde: networks and cultural mediation with the Spanish and Latin-American critics and translators. In D’haen, T. y Vandesbosch, D. (eds). 
                    Literary Transnationalism(s). Leiden/Boston: Brill.
                
                
                    Schelstraete, J., y Van Remoortel, M. (2017). Towards a sustainable and collaborative data model for periodical studies. 
                    Media History.
                
                
                    Tolonen, M. (2016). Printing in a Periphery: a Quantitative Study of Finnish Knowledge Production, 1640-1828. 
                    Digital Humanities 2016: Conference Abstracts. Jagiellonian University y Pedagogical University, Cracovia, 11–16 julio 2016.
                
            
        
    

        
            Theatre studies is a largely under-discussed topic in digital humanities research projects. It's lagging behind the first wave of digital humanities scholarship, «  focus[ing] on large-scale digitization projects and the establishment of technological infrastructure » (Presner, 2010). Theatre studies remains on the fringe of a growing phenomenon : culture analytics. In the context of big and complex datasets, culture analytics « is the data-driven analysis of culture » (IPAM, 2016). I suggest the expression « theatre analytics » (Bardiot, 2017). To paraphrase the culture analytics definition, theatre analytics is the data-driven analysis of theatre, whether it concerns theatre history (Caplan, 2016), drama or mise-en-scène. To understand what quantitative methodologies can bring to the knowledge of theatre, I propose a case study of Merce Cunningham. What can we learn about Merce Cunningham, one of the most influential 
                choreographers of the 20th century, thanks to theatre analytics ? A leader of the American avant-garde throughout his seventy year career from 1938 to 2009, he establishes in 2000, in the twilight of his career, the Merce Cunningham Trust, in order to preserve the integrity of his work. At the same time, he decides to dissolve the Merce Cunningham Dance Company (MCDC) two years after his death and a legacy tour. This is an unprecedented initiative. On one hand, it demonstrates exceptional effort and dedication to document the works. On the other hand, it challenges the ephemeral nature of performing arts : 86 out of 183 choreographies are documented with “digital Dance Capsules” “so that it may be performed in perpetuity”(Dance Capsules, n.d.). By the way, two groups of works are defined : the canon (key works with extensive documentation in order to perform them again and again) ; the auxiliary (minor works with no documentation available to the public and 
                de facto impossible to replay).
            
            The data was collected from the Merce Cunningham Trust website. It concerns theatre production and cast, Dances Capsules documentation and the history of the MCDC. The dataset contains 183 works from 1938 to 2009 (including 86 Dance Capsules) and 347 people. We can identify three main data categories : people, works and documentation. What can we infer from beyond the data about the MCDC history, Cunningham's aesthetics and documentation strategies ?
            Measuring means measuring instruments. I used various and complementary tools in order to vary the approaches and analysis of the same dataset : Gephi for network analysis ; Palladio for geographic and temporal representation ; spreadsheet (Excel, Open Office, Datamatic) for statistics analysis. This paper will present the first results of this research, part of it conducted with students during a graduate « introduction to digital humanities » course. Statistical diagrams show three different periods of Cunningham's work ; a stylistic signature with a preference for pieces that are 30 minutes long, and for soli, sextets and works with 13 to 15 dancers ; a general trend towards more dancers and more length ; the special place of soli in order to articulate the canon and the auxiliary ; the organization of documents in the Dance Capsules. Network analysis let me define two different ways of collaboration, the « star » and the « spiral », and raises awareness on pivotal dancers. Geographic representation highlights relations between Europe and the United States.
            In a wider historical perspective, it would be interesting to compare these preliminary results with other datasets. One example : two patterns have been identified in the Cunningham collaborations network : the star (figure 1), with discontinuous, centralized collaborations and groups separated from each other; the spiral (figure 2), with continuous, collective collaborations and one group growing organically. The change from the star to the spiral takes place when the company is created. Do these patterns characterize other choreographers and directors careers ? Is the creation of the company the main factor causing the evolution from the first pattern towards the second one ? While a well-worn issue – we do know that the creation of a company plays a crucial role in a career – the fact remains that “theatre analytics” let us visualize the patterns this break constitutes (or maybe not) and define different ways of collaborations.
            
                
                    
                
            
            Figure 1 : Merce Cunningham's collaborations network before 1954. The star pattern. 
            Pink, dancers; orange, composers ; green, stage designers ; blue, choreographer.
            
                
                    
                Figure 2 : Merce Cunningham's collaborations network after 1954. The spiral pattern.
            
        
        
            
                
                    Bibliography
                    Dance Capsules - Merce Cunningham Trust 
                         (accessed 29 May 2018).
                    
                    Merce Cunningham Dance Capsules 
                         (accessed 29 May 2018).
                    
                    
                        Bardiot, C. (2017). Arts de la scène et culture analytics. (Ed.) Galleron, I. 
                        Revue d’historiographie du Théâdre. Etudes théâtrales et humanités numériques(4): 11–20.
                    
                    
                        Caplan, D. (2016). Reassessing Obscurity: The Case for Big Data in Theatre History. 
                        Theatre Journal, 
                        68(4): 555–73.
                    
                    
                        Tangherlini, T. R. (ed). (2016). 
                        Culture Analytics : White Papers. 
                        .
                    
                    
                        Presner, T. (2010). Digital Humanities 2.0: a report on knowledge. 
                        Connexions Project.
                    
                
            
        
    

        
            Las manifestaciones afrolatinoamericanas y sus conexiones con el mundo digital han comenzado a generar un creciente interés en diversos campos de estudio: las humanidades digitales, los estudios culturales, literarios y antropológicos entre otros. A pesar del interés, el estudio de tal intersección se encuentra en una etapa inicial debido a factores como a) las limitaciones de acceso a herramientas digitales por parte de algunos agentes y comunidades identificadas y auto-identificadas como afrolatinoamericanas/afrolatinas; b) limitaciones en la consecución de derechos de autor de algunas piezas y manifestaciones cuya distribución e intercambio digital se hace más difícil; y c) falta de innovación en la forma de clasificar piezas y manifestaciones que, en muchos casos, no coinciden con la tradición letrada que subyace al proceso de archivo ya sea digital o no. Tales limitaciones han hecho más difícil la consolidación de propuestas analíticas que, desde las humanidades digitales, den cuenta del estado y evolución de las culturas afrolatinoamericanas, así como de sus aportes a nivel de conocimiento en espacios locales, regionales y globales. 
            Algunas formas de revertir dichas limitaciones ha sido el desarrollo de iniciativas y colecciones digitales por parte las mismas comunidades afrolatinoamericanas en cooperación con entidades académicas, agencias multilaterales, gubernamentales, intergubernamentales y no gubernamentales. Tales iniciativas muestran la diversidad de manifestaciones generadas desde dichas comunidades; manifestaciones que son fundamentales para su identificación, visibilización y, sobre todo, consideración dentro de un modelo de justicia social que, como el contemporáneo, se centra en el reconocimiento de los derechos humanos. Asimismo, dichas adaptaciones tecnológicas se convierten en una forma de lo que Steve E. Jones determina como ‘eversion” (Jones, 2016) o la consolidación de unas realidades híbridas entre lo digital, lo análogo y lo performático. Algunos de los proyectos más importantes en este ámbito son, entre otros, Digital Portobelo, Mueseu Afro Digital Río de Janeiro o Proyecto Afrolatin@, a partir de los cuales se hacen evidentes diversas formas de ser afrolatinoamericano, así como diversas formas de representación y expresión de sujetos cuya identificación intersecta varios espacios discursivos, políticos y de acción. Algunos de los puntos positivos de dichas plataformas y colecciones es que a) son espacios en constante construcción –actuales y constantemente actualizados- y b) permiten ver procesos de acceso, creatividad, justicia simbólico-social que las comunidades están persiguiendo y han perseguido por largo tiempo. Sin embargo, el carácter de construcción constante de dichas plataformas es, al mismo tiempo, un aspecto negativo dado que el flujo de información se convierte en un desafío para unas humanidades digitales cuyo modelo se ha centrado en la digitalización y análisis de información canónica, única, extraordinaria (Manovich, 2016). Las plataformas generadas por parte de esas comunidades afrolatinoamericanas, por el contrario, registran el flujo de la cultura en el presente que no ha sido propiamente abordado por las humanidades ya sean análogas o digitales. En el caso de la intersección entre estudios afrolatinoamericanos y estudios digitales, el proceso de análisis ha estado mucho más rezagado no solo por la falta de bases de datos o de construcción de archivos digitales, sino por la falta de interés y apoyo para construirlos y, a partir de allí, desarrollar metodologías innovadoras de análisis (Gomez, 2011). 
            De acuerdo con el panorama descrito, esta presentación corta dará cuenta del proceso de investigación e implementación metodológica llevado a cabo a partir de 
                Manuel Zapata Olivella Collections, una colección digital desarrollada por la biblioteca de la Universidad de Vanderbilt. Manuel Zapata Olivella fue uno de los escritores y activistas afrolatinoamericanos más importantes del siglo XX, cuya obra y pensamiento han influido al movimiento afrolatinoamericano contemporáneo. Sus cartas, manuscritos y documentación personal como escritor, artista y activista habían quedado en un archivo personal manejado por su familia. Sólo hasta el 2008 la Universidad de Vanderbilt adquirió el fondo y desarrolló una colección digital en el cual se hacen visibles varios de sus documentos y proyectos tanto etnológicos como antropológicos. Entre los archivos digitalizados se encuentran los documentos –cartas, panfletos, memorias, comunicaciones personales, fotografías y audios- del 
                Primer Congreso de Cultura Negra de las Américas, realizado en Colombia en 1978. El proyecto, llevado a cabo con apoyo de la Universidad de Indianápolis, consistió en el análisis digital de dicha documentación y del Congreso como uno de los nodos centrales de la acción política, literaria y cultural afrolatinoamericanas del siglo XX y XXI. El proyecto buscaba a) responder preguntas tales como: ¿Cuáles fueron las redes artísticas y textuales que permitieron la emergencia del Congreso?, ¿Cuáles fueron los discursos socio-culturales latinoamericanos con los cuales el congreso desarrolló un diálogo y logró establecer su propio conjunto de valores y códigos para explicar lo afrolatinoamericano?, ¿Cuáles de los valores políticos y estrategias estéticas creadas y adoptadas por el Congreso devinieron patrones de acción y fueron transmitidas al movimiento afrloatinoamericano de la era digital?. Asimismo, el proyecto buscaba b) desarrollar propuestas metodológicas digitales para comenzar a entender la complejidad e interconexión –en tiempo y espacio- del movimiento afrolatinoamericano. Esta última actividad se desarrolló a través de la implementación de mapas de tópicos y el uso de plataformas digitales para visualizar la información de forma inter-relacional –Vg. Scalar, Wandora, Gephi, etc.-, considerando la diversidad de materiales en el ecosistema informativo de la tradición afrolatinoamericana. 
            
            La presentación entonces mostrará los resultados de esa investigación a través del mapeo de textos, de agentes, instituciones y sistemas de valores relacionados para, finalmente, conectarlo con las propuestas ideológicas fundamentales del movimiento afrolatinoamericano surgido de la Conferencia Mundial Contra el Racismo realizada en Durbán en 2001. A través de esta presentación se discutirán no solamente los hallazgos de la investigación en particular sino, sobre todo, las perspectiva de unas humanidades digitales afrolatinoamericanas que, aunque se incluyan en las discusiones regionales (RedHD, Humanidades digitales en Latinoamérica) intentan ir más allá, en busca de la conexión entre activismo e investigación académica con un objetivo claro: la justicia social y la descolonización del conocimiento.
        
        
            
                
                    Bibliografía
                    
                        Gómez F. P. (2011). La colección Manuel Zapata Olivella. 
                        Revista de estudios colombianos, 37-38: 117-118.
                    
                    
                        Jones E. S. (2016). The Emergence of the Digital Humanities. 
                        Debates in the Digital Humanities, University of Minnesota Press. 
                        http://dhdebates.gc.cuny.edu/debates/text/52
                    
                    
                        Manovich, L. (2016). The Science of Culture? Social Computing, Digital Humanities and Cultural Analytics. 
                        Journal of Cultural Analytics. Doi: 
                        
                            10.22148/16.004
                        
                    
                
            
        
    

        
            
                Introduction
                The study of communication networks, specifically road networks, is a topic of broad interest to the scholarly community. It allows researchers to draw conclusions that range from historical events (Antrop, 2004; Trombold, 1991) to transit and traffic (Bash et al., 2017; Yang and Yagar, 1995), while adding a tangible and understandable dimension to their work. The appearance of Geographical Information Systems (GIS) made it possible to perform such analysis efficiently and accurately. It is just recently that the study of topological and growth properties of road networks are giving us the chance of understanding the bigger picture of cities (Antrop, 2005; Kasanko et al., 2016).
                In the American landscape, network analysis of road networks has shown evidence that the construction of interstate highways affected the political and geographic polarization of cities, undermining representation and posing a threat to democracy itself (Nall, 2015; 
                    Ejdemyr et al., 2005). Most of these studies, however, rely on “the only rigorous year-to-year record of the construction of interstate highways and the incorporation of existing freeways into the system” (Nall, 2018), the Federal Highway Administration PR-511 database (FHWA PR-11). While the FHWA PR-11 is the most complete database available, it is based on highway construction records, which oftentimes misrepresent the complexity of turning political promises into reality, and does not include data on the development of road networks before the interstates. One way to approach this lack of data is to resort to roadmap collections, which might be a better proxy to understand the reality of transportations networks. Unfortunately, despite the number of digitized and scanned map collections, the lack of their availability in standard network data formats still represents a burden for the study of historical road networks. Although network analysis tools exist, we are not able to fully leverage their potential regarding historical datasets without a huge amount of manual work to generate network data.
                
                As an alternative, modern approaches of road extraction from maps promise fully automated methods that rarely generalize well (Mena, 2003, 
                    Sharma et al., 2013), or rely on good quality labeled data (Isola et al., 2016), which is non existent or very difficult and costly to gather. We are then left to semi-automated methods where the researcher is guided to enter some crucial information needed for the automated process to start. However, these methods are usually conceived for satellite imagery or raster images of maps, lacking proper support for the variety of style and format found when dealing with collections of historical maps, and producing vector information not in network format. In order to fill this gap, we are presenting Histonets, a web-based platform to assist in the conversion of historical maps into digital networks, turning intersections into nodes and roads into edges.
                
            
            
                Methodology
                The platform begins with a login screen, after which each researcher can create a number of collections of images of maps by linking them from IIIF-compliant repositories. Furthermore, researchers are able to create settings for similar images (according to their criteria). Once images are selected, the pipeline for the Histonets platform is comprised of 4 steps: image preparation and cleaning, pattern matching, pathfinding, and graph correction. Cleaning can be fully automated or fine-tuned by adjusting the parameters of several actions to be applied. Once clean, image color depth is reduced by an automatic color clustering algorithm that only needs the final number of colors (defaults to 8).
                With the image clean and posterized, the pattern matching step begins. In order to identify intersections and corners that will eventually become the nodes of the graph, researchers must circle around them, and, with a couple of samples, Histonets will try to find other instances in the images, taking into account rotation and orientation of the templates. Identifying roads is done by selecting their colors and a threshold. Areas under a certain threshold are removed as well. A final preview of the resulting graph is shown for the whole image. If the graph complies with the expectations the researcher can start a batch process to apply the same parameters to the whole collection. The tasks can be monitored and canceled. The final result of the process for each image map is a downloadable file in a compatible graph format, including Gephi and GraphML (sees Figure 1).
                
                     Figure 1. Sample of image input (upper left), internal output (upper right), and final graph as produced by Histonets (lower)
                
            
            
                Discussion
                Although in early stages, Histonets has already proved to reduce substantially the amount of hours of manual labour, cutting down the time needed to process an entire collection. Moreover, the easy parallelization built-in in Histonets is only limited by the computational resources available, making it easier for cloud or high performance computing center deployments to further boost its performance. However, without a proper benchmarking framework it is still difficult to assess its accuracy and completeness. One of our goals moving forward is to test and measure these factors, and adjust the platform for greater reliability.
                While Histonets, as a whole pipeline, is focused specifically on extracting road networks from historical maps, collaborators have already identified uses outside of Political Science or History. As a general low-barrier and user friendly computer vision application, we have shown it to be useful for identifying capital letters in Medieval manuscripts, counting glyphs in Egyptian hieroglyphs, or even identifying architectural features. With its balance between meeting specific research needs and generalizable applicability, Histonets has a bright future as an adaptable tool in the Digital Humanities.
            
        
        
            
                
                    Bibliography
                    
                        Antrop, M. (2004) Landscape change and the urbanization process in Europe. 
                        Landscape and urban planning
                        67.1, pp. 9-26.
                    
                    
                        Antrop, M. (2005) Why landscapes of the past are important for the future. 
                        Landscape and urban planning
                        70.1, pp. 21-34.
                    
                    
                        Bast, H., et al. (2017) Fast routing in road networks with transit nodes. 
                        Science
                        316.5824, pp. 566-566.
                    
                    
                        Champion, T. (2001) Urbanization, suburbanization, counterurbanization and reurbanization. 
                        Handbook of urban studies
                        160: 1.
                    
                    
                        Ejdemyr, S., Nall, C., and O’Keefe, Z.
                         (2015) Building Inequality: The Permanence of Infrastructure and the Limits of Democratic Representation.
                    
                    
                        Isola, P., et al. (2016) Image-to-image translation with conditional adversarial networks. 
                        arXiv preprint arXiv:1611.07004.
                    
                    
                        Mena, J. B. (2003) State of the art on automatic road extraction for GIS update: a novel classification. 
                        Pattern Recognition Letters
                        24.16, pp. 3037-3058.
                    
                    
                        Nall, C.
                         (2015) The political consequences of spatial policies: How interstate highways facilitated geographic polarization.
                        The Journal of Politics
                        77.2
                        , pp. 394-406.
                    
                    
                        Nall, C. (2018) The Road to Inequality: How the Federal Highway Program Polarized America and Undermined Cities. Cambridge University Press.
                    
                    
                        Kasanko, M., et al. (2016) Are European cities becoming dispersed?: A comparative analysis of 15 European urban areas. 
                        Landscape and urban planning
                        77.1, pp. 111-130.
                    
                    
                        Sharma, N, Bedi, R., and Dogra, A. K.
                         (2013) A Survey on Road Extraction from Color Image using Vectorizaton.
                        IJRET: International Journal of Research in Engineering and Technology
                        2.10
                        .
                    
                    
                        Trombold, C. D. (1991) ed. 
                        Ancient road networks and settlement hierarchies in the New World. Cambridge University Press.
                    
                    
                        Yang, H., and Yagar, S. (1995) Traffic assignment and signal control in saturated road networks. 
                        Transportation Research Part A: Policy and Practice
                        29.2, pp. 125-139.
                    
                
            
        
    

        
            Background
            The infrastructures that we use to navigate the world often become invisible as they become indispensable (Bowker and Star, 2000). However, critical examination of information systems is necessary to understand their implicit biases, and the ways that they invite some types of engagement and restrict others. Structures of power continue to be replicated in the ways that technologies are deployed in our lives (Noble, 2016; Tufekci, 2016), and the inability to access and assess the standards which make digital communication possible risks the uncritical perpetuation of those power structures (Drabinski, 2013). The moments of rupture, when an established system takes on a new facet with unintended consequences, can be an important moment of visibility, where we are able to reveal its ideological foundations, and the ways that its users adapt their own behaviors to it, or push back against its uncomfortable constraints (Raley, 2006; Marino, 2007). The introduction of emojis to the Unicode Standard, and their widespread adoption over the decade from 2006-2017 is one such moment of transition.
             Scholars of standards and standardization argue that the input of users is necessary for a standard to meet the needs of those users (Foray, 1994), and while the process of adding content to the Unicode Standard remains rigid, the unicode.org website provides an explicit record of the development and evolution of the face that Unicode presents to its users, and is able to be read as a text which reveals the contemporary state of Unicode and the cultural ideologies which shape it.
            Methodology
            While major language- and script-based additions are made with each update to the Unicode Standard, my analysis focuses on changes to the unicode.org website, and its role as an intermediary document between the Consortium, the Standard itself, and everyday users. The introduction of emojis in various updates to the Standard has resulted in changes to the content and structure of the unicode.org website that reflect an increased engagement with end users, which I argue is the result of increased semantic value of emoji characters for the user
                
                     A notable exception to this semantic shift is written Chinese, which is already a semantic-character-based language, as opposed to syllable- or alphabet-based, as are the rest of the world’s major languages. Thomas S. Mullaney gives a thorough historical analysis of the implication of this on text-encoding technologies in 
                        The Chinese Typewriter (MIT Press, 2017). 
                    
                , as compared to an individual character in a language's written script. It is my intention, through this analysis, to describe the types of changes that happen to the governing body and public documents of Unicode as major changes happen to the Standard itself.
            
            A timeline was created of the dates of major updates to the Unicode Standard since its introduction in 1991, using the official release dates for updates to the Unicode Standard as maintained by the Unicode Consortium. I cross-reference this document with the rollout of each new version by the major platforms
                
                     https://unicode.org/emoji/format.html#col-vendor lists the major “vendors” of emojis, or platforms with proprietary visual displays of emojis. These vendors are Apple, Google, Twitter, Facebook, Facebook Messenger, Windows, and Samsung.
                , with a particular emphasis on updates featuring new emoji characters, beginning with Unicode 6.0 in 2010
                
                     While the first major batch of emojis were incorporated into Unicode in 2010, and the first official “Emoji 1.0” release was in 2015, work has been done within the standard since late 2006 to consider the addition and management of emoji-like characters within Unicode— hence the specific 2006-2017 emphasis of this research. (https://www.unicode.org/reports/tr51/#Introduction)
                . 
            
            With this timeline in mind, I scraped the unicode.org domain using Python and the Beautiful Soup
                
                     https://www.crummy.com/software/BeautifulSoup/
                 library to collect the URLS of all the unique pages under the parent domain, as well as a table of links between those pages. This serves as a source-target list for the creation of a network visualization of the unicode.org domain, using the network visualization software Gephi.
                
                     http://gephi.io
                 This process is repeated using archived versions of the unicode.org site, available from the Internet Archive’s Wayback Machine
                
                     https://web.archive.org/
                , resulting in several structural snapshots of the unicode.org website over time, which can then be overlaid and compared to one another to note particular areas of change within the site.
            
            Additionally, using points of change within the site structure as a guide, I also collect and code page content data to reflect the type of changes made to those pages during each major update. This coding is done on two axes: The first labels each change as being content- or structure-based (eg. adding text or links to a page, respectively), and the second designates which aspect of the Standard and/or Consortium is being addressed by the change. Examples of this second type of labelling would be “Emoji,” “Membership,” “Meta-Documentation,” or “Language Scripts.” This coding is done in two phases— an initial survey of this data in order to formally create labelling categories, and then a closer examination of the updates to apply those labels.
            Discussion and next steps
            This research project addresses issues of digital infrastructure from a unique angle: one that considers the socially-constructed nature of technology, as well as the meta-narrative of maintenance and upkeep of a system that has become crucial to our ability to communicate in a digital world. Through analysis of the secondary documents relating to the Unicode Standard, it is possible to gain invaluable insights into the ways that knowledge is organized collectively and continuously, as well as the embedded values that shape who can access and influence that knowledge.
            
                This case study will provide a foundation for more expansive examination of systems of digital infrastructure. It is a beginning point both for further analysis of the adoption and adaptation of Unicode (and emojis in particular), but also as a framework for examining other forms of scaffolding which uphold the content of digital spaces. 
            
        
        
            
                
                    Bibliography
                    
                        Bowker, G. C., and Star
                        , S. L. (2000). 
                        Sorting things out: Classification and its consequences
                        . Cambridge: MIT Press.
                    
                    
                        Drabinski, E. (2013). Queering the catalog: queer theory and the politics of correction. 
                        The Library Quarterly 83(2): 94-111. doi:10.1086/669547
                    
                    
                        Foray, D
                        . (1994). Users, standards and the economics of coalitions and committees. 
                        Information Economics and Policy
                        , 
                        6
                        (3): 269-293.
                    
                    
                        Marino, M. C. (2007, December 4). Critical code studies. 
                        Electronic Book Review. Retrieved from http://electronicbookreview.com/thread/electropoetics/codology
                    
                    
                        Noble, S.U. (2016). A future for intersectional black feminist technology studies. 
                        The Scholar &amp; Feminist Online. 13.3 - 14.1. Retrieved from: http://sfonline.barnard.edu/traversing-technologies/safiya-umoja-noble-a-future-for-intersectional-black-feminist-technology-studies/0/ 
                    
                    
                        Raley, R. (2006). Code.surface || Code.depth, 
                        Dichtung Digital. Retreived from http://www.dichtung-digital.org/2006/01/Raley/index.htm
                    
                    
                        Tufekci, Z. (2016, June). 
                        Machine intelligence makes human morals more important. [Video file]. Retrieved from https://www.ted.com/talks/zeynep_tufekci_machine_intelligence_makes_human_morals_more_important
                    
                
            
        
    

        
            
                Introduction
                Even-Zohar's polysystem theory is a well-established approach to understanding how entire translated literatures interact (or not) with the body of the receiving native literary culture. Even-Zohar identifies a number of possible interactions depending on the relative "strength" and "age" of the two (or more) literatures, and translated literatures may assume "peripheral" or "central" positions within the target literary polysystem. According to this scholar, translations are usually peripheral to native literature; but he also cites examples where a given literary polysystem places some imported subsystems in a central position, while other “foreign imports” remain in the periphery (Even-Zohar 1990).
                Even-Zohar thus deals with literary creation en masse rather than, as is often the case in academic approaches to literary translation, on single books original and translated. The obvious parallel to Distant Reading has already been drawn (Helgesson and Vermeulen 2015, 25-26); but it might also be tempting to do the same for a related approach, macroanalysis, if we are to follow the distinction made by the exponent of the latter term (Jockers 2013, 48). Both bring together investigations into masses of literary material unattainable by traditional close reading; yet macroanalysis looks inside many books at once using quantitative methods applied to their lexical layers that have been called “stylometry” well before both Moretti and Jockers.
            
            
                Material
                From our personal mixed Polish-Italian perspective, few cases could serve as a better pretext to try to negotiate this marriage between polysystem theory and computational stylistics than that of 
                    Quo vadis (1896), the historical romance by Henryk Sienkiewicz, Poland’s first literary Nobel Prize winner of 1905. Its international success – long gone with the wind but unparalleled by any other Polish novel to this day – resulted in a veritable explosion in terms of numbers of translations into various languages. In many countries, several different translations simultaneously vied for the public’s attention. Yet “several” does not even begin to describe the situation in Italy, where at least three hundred different editions can be still found today (Woźniak 2016). In the first two years of the existence of 
                    Quo vadis on the Italian market (1899-1900), as many as eight different translations were already available to the readers (Berti and Gagetti 2016). 
                
                No wonder: not only was the novel set in the Italian capital and not only did it deal with a subject already very present in Italian culture old and new; the book’s (and its author’s) brand of conservative Catholicism must have appealed to some of the most influential circles of the country. Yet the novel was also praised by some of Italy’s progressive critics, who saw, in Sienkiewicz’s persecuted Christians, the struggle of their contemporary revolutionary movements, and who liked to read his depiction of Imperial Rome’s decadence as a diatribe against the existing power structure (Marinelli 1984). 
                This profusion of Italian renderings is also the reason why building their representative selection was no easy task. Only a single translation was available online; a search in Polish and Italian libraries provided almost seventy candidate texts: signed or unsigned by a translator, published by a variety of publishers, often in several somewhat different editions. In the end, twenty-four translations produced until mid-20th c. have been identified as more or less independent of each other, although some of these still share over 50% of material, as evidenced by comparison of texts for identical 5- or longer word clusters with 
                    WCopyFind (Bloomfield 2011-2016). When applied to genuinely different translations, the similarity ratio is of the order of 5-7%.
                
                The natively Italian literary polysystem was represented by close to 1300 different literary texts, mostly selected and adapted from 
                    Progetto Manuzio, one of the most comprehensive Internet collections of electronic texts in Italian. To include as many texts as possible, this set of Italian writing included dramas, epic poems and opera libretti as well as novels and novellas from the 15th to the 21st century. Several translations of other novels by Sienkiewicz were also added to the collection, and another big body of translations of a single author, Shakespeare, was included as well.
                
            
            
                Methods
                The stylometric method applied has been described by Eder (2017) and applied to other literary corpora by Rybicki (2014, 2016). Basing on Burrows’s Delta procedure (2002), a list of most-frequent words (MFWs) is produced for the entire corpus. These words are then counted in the individual texts, and their frequencies are compared in text pairs to produce a matrix of distance measures; in this study, the distances were established by means of the modified Cosine Delta (Smith and Aldridge 2011), which is now seen as the most reliable version (Evert et al. 2017). The distance matrix then undergoes Cluster Analysis (Ward’s hierarchical clustering), resulting in grouping the texts into “clusters” of greatest similarity; this is repeated for reiterations from 100 to 2000 MFWs at 100-word increments, and a consensus between the individual iterations is produced to show each text’s most consistent nearest neighbors, next-to-nearest neighbors and next-to-next-to nearest neighbors. The procedure is performed by means of 
                    stylo (Eder et al. 2016), a stylometric package for 
                    R (R Core Team 2016). The results are visualized by means of network analysis, applying the “Force Atlas 2” gravitational algorithm (Jacomy et al. 2008) in 
                    Gephi (Bastian et al. 2009) to the above-mentioned scores. Instead of applying a “human-made” classification of the resulting network of nodes and edges (i.e. identifying authors, genres and literary periods based on external and traditional literary history), the task of dividing the network into groups of greatest internal similarity was entrusted to 
                    Gephi’s modularity function, which finds communities within a weighted network (Blondel et al. 2008). The main experiment was conducted by successively increasing the number of communities shown until the expected separate cluster of translations of 
                    Quo vadis became a separate entity in the network, and the degree of its discreteness could thus be assessed.
                
            
            
                Results
                Dividing the network into just two modularity groups failed to isolate Sienkiewicz from the main Italian community. Instead, the main division was that between 19
                    th/20
                    th-century novels, translated or originally Italian, and everything else – the one notable exception to this rule was the prose of Pirandello, classified with the earlier texts. At three modularity groups, Italian drama detached itself from early prose. At four, the first writer became a separate community, but this was the native Deledda rather than the alien Sienkiewicz. At five, 19
                    th- and 20
                    th/21
                    st-century novels became two distinct groups; at six, another native Italian, Salgari, received his own class; at seven, pre-19
                    th-century works detached themselves from later prose. It is only at ten communities that a translated rather than an Italian author became a separate subsystem (to use Even-Zohar’s term) – in fact, not one but two: Sienkiewicz (not just his 
                    Quo vadis) and Shakespeare (Fig. 1). 
                
            
                    
                        
                        Figure 1. Network analysis of distances between most-frequent-word usage. Thick and short lines (edges) denote small distance (or high similarity). For simplicity, only the final 10-community modularity is shown.
                    
                    

            
                Discussion
                It seems too much of a coincidence that two major subsystems (translations of Shakespeare and translations of Sienkiewicz) become separated from the main body of literature in Italian at the same time, and that this happens only after two native authors receive their own subsystems. If such a mechanism were to be observed in even more extensive collection of texts (when they finally become available), Even-Zohar’s hypothesis of the usually peripheral position of translated literature could find its stylometric illustration. At the same time, this experiment confirms not only that original novels are more similar to translated ones than the former to original drama; but also that certain original authors are more different from other original authors than those translated from another language.
                Obviously, this hypothesis must be tested in the future in other literary polysystems to claim that the affinity between polysystem theory and macroanalysis is anything more than metaphorical. Even-Zohar speaks of reception of literary works within a broader national culture; macroanalysis counts context-free words. Still, in its attempts to bring distant and close reading together, stylometry has been clutching at even weaker straws. Stylometrists continue to make similar leaps (of faith?) between their graphs and trees and networks on the one hand, and traditional literary history on the other. They usually believe that frequencies of very frequent words provide insights into more abstract characteristics of texts than their mere lexical or even grammatical difference: and these abstracts so far include authorship, genre, chronology, or gender. This study might just have added a new one. At the very least, it is an invitation to apply Even-Zohar’s concepts in various “distant” approaches to literature.
            
            
                Acknowledgements
                This research was made as part of the project: “Miejsce 
                    Quo vadis? w kulturze włoskiej. Przekłady, adaptacje, kultura popularna” (0136/ NPRH4/H2b/83/2016), funded by Poland’s National Program for Advances in the Humanities (NPRH).
                
            
        
        
            
                
                    Bibliography
                    
                        Bastian, M., Heymann, S., and Jacomy, M. (2009). “Gephi: an open source software for exploring and manipulating networks.” 
                        Proceedings of the International AAAI Conference on Weblogs and Social Media, San Jose, Ca.
                    
                    
                        Berti, G. de, and Gagetti, E. (2016). “La fortuna di ‘Quo vadis’ in Italia nel primo quarto del Novecento.” In Woźniak, M., Biernacka-Licznar K., eds, 
                        Quo Vadis. Da caso letterario a fenomeno di massa. Ispirazioni - adattamenti - contesti. Roma: Ponte Sisto, 50-59.
                    
                    
                        Blondel, V.D., Guillaume, J.-L., Lambiotte, R., Lefebvre, E. (2008). “Fast Unfolding of Communities in Large Networks,“ 
                        Journal of Statistical Mechanics: Theory and Experiment 10: 1000.
                    
                    
                        Bloomfield, L. (2011-2016). 
                        WCopyFind. The Plagiarism Resource Site, http://plagiarism.bloomfieldmedia.com. Accessed 24. Nov. 2017.
                    
                    
                        Burrows, J.F. (2002). “Delta: A Measure of Stylistic Difference and a Guide to Likely Authorship.” 
                        Literary and Linguistic Computing 17: 267-287.
                    
                    
                        Eder, M. (2017). “Visualization in stylometry: Cluster analysis using networks.” 
                        Digital Scholarship in the Humanities 32(1): 50-64.
                    
                    
                        Eder, M., Kestemont, M., and Rybicki, J. (2016). “Stylometry with R: A package for computational text analysis.” 
                        The R Journal 8(1): 107–121.
                    
                    
                        Even-Zohar, I. (1990). “The Position of Translated Literature within the Literary Polysystem.” In 
                        Polysystem Studies [= Poetics Today] 11(1): 45-51. 
                    
                    
                        Evert, S., Proisl, T., Jannidis, F., Reger, I., Pielström, S., Schöch, C., and Vitt, T. (2017). “Understanding and explaining Delta measures for authorship attribution.” 
                        Digital Scholarship in the Humanities 32 (sup. 2): 4-16.
                    
                    
                        Helgesson, S. and Vermeulen, P. (2015). „Introduction. World Literature in the Making.” In Helgesson, S. and Vermeulen, P. eds, 
                        Institutions of World Literature, Writing, Translation, Markets. London: Routledge, 1-22.
                    
                    
                        Jacomy, M., Venturini, T., Heymann, S., and Bastian, M. (2008). “ForceAtlas2, a Continuous Graph Layout Algorithm for Handy Network Visualization Designed for the Gephi Software.” 
                        PLoS ONE 9(6): e98679. DOI=10.1371/journal.pone.0098679.
                    
                    
                        Jockers, M. (2013). 
                        Macroanalysis. Digital Methods and Literary History, Champaign: University of Illinois Press 2013.
                    
                    
                        Marinelli, L. (1984). “’Quo vadis.’ Traducibilità e tradimento,” 
                        Europa Orientalis 3: 131-146.
                    
                    
                        R Core Team. (2016). 
                        R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/. 
                    
                    
                        Rybicki, J. (2014). “Pierwszy rzut oka na stylometryczną mapę literatury polskiej,” 
                        Teksty drugie 2: 106-128.
                    
                    
                        Rybicki, J. (2016). “Vive la différence: Tracing the (Authorial) Gender Signal by Multivariate Analysis of Word Frequencies,” 
                        Digital Scholarship in the Humanities 31(4): 746-761.
                    
                    
                        Smith, P. and Aldridge, W. (2011). “Improving authorship attribution: Optimizing Burrows’ Delta method.” 
                        Journal of Quantitative Linguistics, 18(1): 63–88.
                    
                    
                        Woźniak, M. (2016). “Quo vadis: da caso letterario a fenomeno di massa. Dove ci ha portato Sienkiewicz?” In Woźniak, M., Biernacka-Licznar K., eds, 
                        Quo Vadis. Da caso letterario a fenomeno di massa. Ispirazioni - adattamenti - contesti. Ponte Sisto, Roma, 6-15.
                    
                
            
        
    






Preliminaries: The Social Networks of Literary Production in the Spanish Empire During the Administration of the Duke of Lerma (1598-1618)



Brown
,
David Michael

University of Western Ontario, Canada
dbrow52@uwo.ca



Suárez
,
Juan Luis

University of Western Ontario, Canada
jsuarez@uwo.ca




University of Nebraska-Lincoln

Center for Digital Research in the Humanities

319 Love Library
University of Nebraska–Lincoln
Lincoln, NE 68588-4100
cdrh@unl.edu


Lincoln, Nebraska

University of Nebraska-Lincoln
Lincoln, NE 68588-4100







The Preliminaries Project is an ongoing study that uses social network analysis to better understand the publication of literary texts in the 17th century Spanish Empire. Using the metadata provided in the preliminaries sections of these texts, we have constructed a series of graph databases that allow us to organize this information in a custom tailored and meaningful way. The data is exported to Gephi graph visualization software for analysis to discover patterns, key communities, and structures not apparent in traditional humanistic studies. We then use traditional humanistic techniques to study important points located in the graph. This study demonstrates the utility of social network analysis in understanding Early Modern publication in the Spanish Empire. By using modern tools to analyze large data sets, we show the effectiveness of digital humanities as a tool to locate previously overlooked areas for further study using a more traditional humanistic approach.



No source: created in electronic format.




LP20





Paper


Long Paper


Hispanic Studies
Social Networks
Transatlantic Communities
Publication
Gephi


databases & dbms
historical studies
literary studies
metadata

data modeling and architecture including hypothesis-driven modeling

spanish and spanish american studies
bibliographic methods / textual studies
cultural studies
visualisation






Laura Weakly
Initial encoding







The “preliminaries” section of a 17th-century book encompasses the pages appearing in the printed text before the beginning of the work itself. This information is divided into seven different types of documents: details of publication, documentation of censorship (both civil and ecclesiastical), licensing, selling price, dedications, letters, and errors. The importance of the preliminaries for this project lies in the information present in these sections: the names of the officials signing the documents, their governmental/institutional affiliation, dates, place of issue, and literary circles that appear in the form of dedications and poetry written by various authors and published in their friend’s or associate’s books. In a few pages, the preliminaries give a complete image of the formal process required for the publication of each work of literature. By compiling all this information into a graph database and performing queries specific to various research questions, we have at hand a valuable source of information about the historical networks that influenced the publication of Early Modern Spanish literature.


To get a comprehensive look at this information, we generated lists of every edition of what we consider literary texts (fiction in prose, theatre, poetry, chronicles) published during the 17th -century in the Spanish empire (Jiménez et al. 1980)(Calvo et al. 2003). As shown by the following screen shot, we have focused on acquiring every available edition of each literary work.




Sample of one of our
acquisitions
lists



We then divided the 17th Century into periods corresponding to the different “validos” —royal favorites that served as head of government or “prime ministers” — of the various kings in order to address the changing power structures of the time and their influence in literary production (Hernán et al. 2002). Through interlibrary loans and, in some cases, trips to the libraries that hold the edition, we acquired copies of the pages of each book that make up the preliminaries section. Then, we manually built a graph database using sylvadb.com, an open source software and free graph database management service developed in the CulturePlex Lab. Within Sylva, data was stored and organized using a custom designed system of schemas based on a node/edge relationship system. Finally, we exported the database to Gephi (
https://gephi.org/
), a software package that allows for visualization and statistical/metric analysis of the network using built-in algorithms and Python based scripting (Bastian et al. 2009). This allows us to detect important communities within the network, key players, important objects, and hubs of production.


For this study, we have unearthed the social networks of publishing and literary creation in 17th-century Spanish literature, focusing particularly on the period during the rule of the Duke of Lerma (1598-1618). Currently the first of our
editions
lists (Duke of Lerma) consists of 330 editions, out of which we have successfully obtained 228 scanned copies of preliminaries sections: approximately 70% of the total number. Of these scans, 121 have been entered into the database, producing a graph with 1612 nodes and 3472 relationships. Rendered in Gephi using the built-in OpenOrd algorithm, the graph looks like this:



The Preliminaries graph rendered in Gephi


Using the algorithms, metric analysis tools, and filters built into Gephi we pinpointed the individuals, governmental and ecclesiastical bodies that influenced publication in this period. Also, by using the concept of “ego network” from social network analysis, we established what we call the “publication network” of some of the authors that interest us (Carrington et al. 2011). A publication network includes the editors, censors, and other individuals important in the formal process of publication, as well as any other individuals that are more directly connected to the author: friends, family, patrons, literary colleagues, etc. We determined the range of the publication network based on the internal data structure of the Preliminaries database as follows. Due to bibliographic concerns (Bowers et al. 1962) and organizational aspects of our data schema, in order to establish a connection between the author and those involved in the approval, licensing, and publication of an edition there are four steps e.g., Author->Work, Work->Edition, Edition->Approval, Approval->Censor. Therefore, to establish an author’s publication network we needed to find neighbors for up to four degrees of separation. Although Gephi does not include ego network filters that extend to four degrees, using its Python based scripting console we were able to code functions that allowed us to isolate subsets within the graph to generate ego networks for any node to
n
degrees of separation. For instance, in the graph below we can see the publication networks of two authors associated with Mexico; Bernardo de Balbuena, author of
Grandeza Mexicana
; Juan de Torquemada, author of
Monarquía Indiana
; and the intersecting nodes in their publication networks:




Publications Networks: Balbuena=Black, Torquemada=Grey, Intersecting Nodes=White



Using the above techniques, we set out to find and isolate the main nodes of this social network that made possible the creation and sustainability of a transatlantic network of cultural agents. The first thing that stands out in the graph is Lope de Vega and his powerful, Madrid based publication network (Martínez et al. 2011). Using the Python scripting console, we determined that Lope’s publication network consists of 1083 nodes, or 67% of the nodes in the graph. This information is not new, based on the extremely prolific nature of his literary production we can assume that he was very well connected. However, we can also determine who
wasn’t
in his publication network. Departing from Lope’s publication network, we were able to locate the successful political and institutional connections that help us explain the central position of institutions such as the House of Zúñiga in the cultural fabric of the period.



Publication network: Lope de Vega=Black


To do this we used the scripting console to remove the subset of nodes representing Lope’s publication network from the other nodes that make up the graph, and returned a list of the names of all of the people who are not in Lope’s publication network. A quick review of this list produced some interesting results: we found several authors based in Spain including Gonzalo de Céspedes y Meneses and El Inca Garcilaso; and two authors active in Peru, Diego Dávalos y Figueroa and Pedro de Oña. While a quick look at both Céspedes and El Inca produced interesting results, the two Lima based authors attracted our attention. In this period social circles were highly influenced by geography, and it is logical that these authors find themselves at the periphery of a network centered geographically in Madrid. However, despite geographic concerns both authors remain connected to Lope de Vega’s network. We found that both Oña and Dávalos y Figueroa are connected to Lope’s network at 3 degrees of separation through their dedications to the Viceroy of Peru, Luis de Velasco y Castilla; and at four degrees through Juan de Zúñiga, Diego de Ojeda, and the Order of Santiago:




Publication networks: Dávalos y Figueroa=Black, Lope de Vega=Grey, Intersecting nodes=White



In order to contextualize the Peruvian network we compared the aforementioned “Mexican” authors with the “Peruvian” authors. Combining the four social networks into two based on geographic constraints, we found that at 4 degrees of separation there was no direct overlap, so we upped the parameter to 5 degrees of separation and produced the following image:




Publication Networks: Intersection between Mexican and Peruvian Networks



As shown here, even at five degrees of separation there are few overlaps between the networks. However, in the above image we begin to notice the importance of the House of Zúñiga. It is well known that the House of Zúñiga was powerful in both Spain and the Americas, and also that certain members of this house were important patrons of the arts and literature (Cátedra 2003; Díez Fernández et al. 2005). Nonetheless, we don’t think that their role in transatlantic literary production has been adequately explored. The political importance of this family in New Spain is obviously important (an Archbishop and a Viceroy); however, the Preliminaries graph illustrates not only the political role this house played in America, but also the importance of political figures/nobility in publication circles and how the members of one house can spread their cultural influence throughout geographic space. To take this concept one step further, we followed the Zúñigas back the Spain. Here we find the Duke of Béjar, Alonso López de Zúñiga y Pérez de Guzmán, and the first part of Don Quixote. It turns out that American authors were not the only artists soliciting support from the House of Zúñiga: Miguel de Cervantes dedicated part 1 of Don Quixote to the famous Duke of Béjar(Rico 2005).


The above samples show the potential of a research model that combines network-based analysis with quantitative and qualitative studies of cultural production, providing evidence of the interaction between political structures and cultural production in the Spanish Empire (Martínez et al 2008). By repurposing bibliographic data, the Preliminaries Project allows us to explore the concept of cultural networks within the framework of transatlantic studies and complexity theory (Wood 2010; Suárez 2007). Furthermore, this study demonstrates the effectiveness of digital humanities methods as a tool to locate previously overlooked areas for further study using a more traditional humanistic approach.







1.
 Pedraza Jiménez, F. B., and M. R. Cáceres.
(1980).
Manual de literature española
. Pamploa: Cénlit.


2.
Huerta Calvo, J. (dir.)
(2003).
Historia del teatro español
. Madrid: Gredos.


3.
García Hernán, E.
(2002).
Políticos de la monarquía hispánica (1469-1700)
. Madrid: Fernández Ciudad.


4.
Bastian M., S. Heymann, and M. Jacomy
(2009). “Gephi: an open source software for exploring and manipulating networks.” International AAAI Conference on Weblogs and Social Media.


5.
Carrington, P. J., and J. Scott
(2011).
The SAGE Handbook of Social Network Analysis
. Los Angeles: Sage.


6.
Bowers, Fredson.
(1962).
Principles of Bibliographic Description
. New York: Russell & Russell.


7.
Martínez, J. F. 
(2011).

Biografía de Lope de Vega, 1562-1635: un friso literario del Siglo de Oro

. Barcelona, PPU.


8.
 Cátedra, P. M. 
(2003).

La "Historia de la Casa de Zúñiga" otrora atribuida a Mosén Diego de Valera

. Salamanca: Gráficas Cervantes.


9.
Díez Fernández, J.; I., and G. Santonja.
(2005).
El mecenazgo literario en la casa ducal de Béjar
. Burgos: Instituto Castellano y Leonés de la Lengua.


10.
Rico, F. 
(2005).

El texto del "Quijote”: preliminares a una ecdótica del Siglo de Oro

. Barcelona: Ediciónes Destino.


11.
Martínez Millán, J.;, and M. A. Visceglia (eds.)
(2008).
La monarquía de Felipe III
. >Madrid: Cyan, Proyectos y Producciones Editoriales. Print.


12.
Wood, A. T.
(2010). Fire, Water, Earth, and Sky: Global Systems History and the Human Prospect.
The Journal of the Historical Society
. X:3: 287-318.


13.
Suárez, J. L.
(2007). Hispanic Baroque: A Model for the Study of Cultural Complexity in the Atlantic World.
South Atlantic Review
. 72(1): 31-47.






AbstractTo aid musicological research, we extract named entities from a German musicological encyclopedia (MGG) with Named Entity Recognition, and link these entities in a social network by the entities that they co-occur with. We offer two network plots that reveal details about the history of musicology, of (a) musicologists that have a lexicon entry each, and (b) the composers that are mentioned in these entries.1 IntroductionOver the 18th and 19th century the academic discipline of musicology had become a central stage for negotiating the value of certain music and aesthetics. These negotiations have also led to the exclusion and deprecation of particular cultural elements, be they issues, music, or people. This discourse has been reflected in the encyclopedia ‘Die Musik in Geschichte und Gegenwart’ (Music in History and Present, MGG, comparable to the New Grove Dictionary of Music and Musicians). It contains 1282 biographical entries of musicologists, representing the Germany centered Western European discourse on musicological knowledge of the last 120 years.To analyze this text corpus at scale, we train named entity (NE) taggers based on BERT (Devlin et al., 2018) with the deepset.ai toolkit. To evaluate our NE taggers, we manually annotate texts from the encyclopedia.Previous manual annotations showed that we might expect data that enables us to analyse the professional network of musicologists exposed in the MGG Online (van Dyck-Hemming and Wald-Fuhrmann, 2019).This approach gives us an overview of socialgroupings, main actors and most negotiated composers (vanDyck-Hemming and Wald-Fuhrmann, 2016), (Latour, 2014). We offer two graphs. In Figure 2 we showthe network of musicologists with encylopedia entries, and in Figure 3 these musicologists with composersmentioned in the entries. If a certain node is not shown (like composers in Figure 2), then it is reduced toan edge. We calculate the importance of a certain actors by eigenvector centrality.2 Named EntitiesWe tune pre-trained BERT models (Devlin et al., 2018) with a sequence classification layer on theCoNLL-2003 Named Entity Dataset (Sang and De Meulder, 2003) and achieve a competitive .85 F1-score. See table 1 for an overview of the F1-scores of our BERT tagger, on conll03 itself and then tested on manual annotation of students.The students annotated around 3000 tokens with the classes Person (PER), Geopolitical Entity (GPE/LOC), Organizations (ORG), other (OTH), temporal expressions (TIME) and professional field, or discipline (FACH). We calculated Cohen kappabetween the annotators on document level. Over all classes, agreement ranges between .5 and .8. Whenremoving FACH, the agreement increases to .7 to .95, suggesting that future research should implementproper guidelines for this label.Our best model achieves a F1-score of 85% on conll03 itself, but on the manual annotation (withTIME, FACH, OTH removed), it only achieves 47% F1-macro.Especially ORG is not reliably detected. Precision figures are also somewhat low, much to our surprise,as manual inspection of the automatically annotated data showed exceptionally good precision.We therefore suspect that (a) the manual annotation (of spans) needs some improvement by better tailoring the guidelines to the used model, and (b) paying special attention to punctuation marks (commas, brackets) that are ubiquitous in encyclopedic articles. Since manual inspection revealed good performance of the model, we use it to extract entities for subsequent processing steps.2.2 Name StandardizationWe automatically removed first names and thus only used surnames. See Figure 1 for an overview ofthe cleaning pipeline. Unfortunately, only relying on last names leads to the conflation of some names, e.g., Hermann and Amalie Abert, the latter being the first female academic musicologist in Germany.2.3 Social Network ExtractionTo obtain a social network, we connect all names that occur with each other in a particular encyclopediaarticle. The network visualization was performed with the software Gephi (Bastian et al., 2009). Wefocused on the Largest Connected Component (Newman, 2010) and discarded nodes with a degree lessthan three. The Modularity (Blondel et al., 2008) measure was used to color-encode modules withinthe network, while the Eigenvector Centrality (Newman, 2010) embraced nodes of higher importance byincreasing their node size. Finally we applied the OpenOrd (Martin et al., 2011) or ForceAtlas2 (Jacomyet al., 2014) layout algorithm to draw the final graphics. In addition to a layout, those algorithms producea visual clustering that give further insights.3 Social networks of influential actors and geographical groupingFigure 2 illustrates a graph of musicologists that are connected by the entities that occur in theirbiography, using only names that occur in the title of an encyclopedic article. We find a central groupof researchers that minted German musicology. This group spans from Hermann and Amalie Abert overArnold Schering, to Hugo Riemann and Heinrich Kretzschmar. The likewise important Erich Hornbosteland Curt Sachs represent the field of systematic musicology, other researchers sharing their interests aregrouped around them. Guido Adler, who is loosely connected to many names, likely because he foundedthe first institute of musicology in Vienna, drifted off center to house his students that are also connectedto the center.We even could partly validate a study based on quantified historical data on Carl Dahlhaus and HansHeinrich Eggebrecht who represent the two main figures of German musicology from 1965 to 1995. Weobserve a clear distinction between both researchers and are able to identify a significantly bigger groupconnected to the younger Dahlhaus.3.1 Musicology and its issueIn additon to the musicologists, the data basis of Figure 3 also includes all names of composers. Here, the visualization shows a network of musicology and its main issues. Composers from the Bach-family or Mozart are the issuesof musicology as an academic discipline. So, the overwhelming dominance of names like ‘Mozart’,‘Bach’, ‘Beethoven’ and ‘Wagner’ represent the dominance of theresearch that musicologists have done on these composers and their music. Especially Bach resp. theBach-family seems to be the one issue deeply connected to the earliest and most important musicologistsnamely Johann Forkel, Philip Spitta, and Guido Adler. Moreover, there are numerous edges (identifiable via colour) linking this group with younger musicologists like Arnold Schering, Hermann Abert, Friedrich Blume and others who shaped musicology of the 1920s.4 ConclusionWe presented a preliminary study of a clearly defined corpus of prosopographical texts, from which we extracted Named Entities and linked these to visualize the social network of musicologists and their relations to certain issues: the composers. This approach is useful for historical research to confirm hypothesis that were draw from arduous manual work. Regarding musicology, we could confirm quantitatively what we expected and already knew from conventional historical research. The overwhelming dominance of a few composers also allows us to reflect about the knowledge standards that musicology has so far relied upon.
The increasing value of information visualization techniques to support investigating quantitative research questions in humanities applications is well-documented (Jänicke et al., 2017; Windhager et al., 2019). At the same time, it is important to make (digital) humanities scholars literate in dealing with visualizations to ensure that accurate conclusions can be drawn. In order to make younger generations ready for interdisciplinary work in a digital humanities context, I taught a module that attracted both computer science and humanities students. This article reflects on the most important aspects in teaching the course throughout three years.Cohort of studentsTeaching needs to be flexibly organized in dependency on the backgrounds of students joining a course. A course might be offered exclusively for either humanities or computer science students, or it can attract students with diverse study subjects. I faced different constellations, and emphasized different aspects according to the demands of the cohort. Whereas humanities students profit from more intense discussions on computational thinking and data modeling, it is especially helpful for computer science students to get to know typical research interests and traditional workflows of humanities scholars. The major focus should be to ensure that students, independent of their backgrounds, learn to “speak the same language” using the same terminology.Teaching visualization theoryTo serve students with an easy-to-digest overview of visualization design, I recommend Tamara Munzner’s book “Visualization Analysis and Design” (Munzner, 2014). It provides an introduction to data and task abstraction that are necessary to comprehend and to develop new or adapt existing visualizations. It further discusses how data features can be appropriately mapped to visual features, and how users can interact with visualizations. Also, emphasis should be devoted to Shneiderman’s Information Seeking Mantra “Overview first, zoom and filter, then details-on-demand” (Shneiderman, 1996) as it encapsulates the general idea of quantitative data analysis without losing the materials a visualization is composed of. This rather theoretical frame should be accomplished with discussing visualization techniques that are of particular importance for digital humanities research: geographical maps, timelines, tag clouds, heat maps and graphs.Ready-to-use-based vs. development-driven visualizationsNext to theoretical contents, the course should have a strong focus on practical work. It is known that different approaches to make use of visualization for knowledge discovery in digital humanities applications exist (Jänicke, 2016). The first approach is to apply ready-to-use tools like Voyant (Sinclair and Rockwell, 2020) for quantitative visual text analysis or Gephi (Bastian et al., 2009) for graph visualization. While the advantage of using such frameworks is generating arguable visual output in a short amount of time, research interests might deviate from what the tool can provide. Further, scholars need to learn how to use potentially complex tools and how to interpret upcoming results. In contrast to applying existing tools to generate visualizations, (digital) humanities and visualization scholars might also engage with each other aiming to generate a new visual vocabulary to expedite knowledge discovery, thereby facing the problems of interdisciplinary collaborations. While the first approach should be carried out simultaneously to the theoretical sessions, I recommend to conduct interdisciplinary student projects in the second part of the course.Supervisory roles during project workTraining to apply the learned visualization-related terminology should be the main focus of a project, while the visualization result itself plays a secondary role. Especially in a setting with mixed backgrounds, students should be engaged to think about potentially interesting project ideas. Each conducted project should include at least one participant with a (digital) humanities and one with a computer science background to expedite interdisciplinary exchange. This constraint generates different supervisory roles:The Mediator: Projects involve students having a computer science and a humanities background alike. The entire project is managed by the students, whereas the teacher might supervise in the form of a mediator during meetings. While students face typical pitfalls of interdisciplinary projects, such projects still brought forth considerable results, one of which is shown in Figure 1.The Real & the Fake Humanities Scholar: When the number of students with a humanities background is too low, project groups that only include computer science students need to be complemented with domain experts. For some of the projects, I was able to involve partners from the humanities, the real humanities scholars, with research interests targeted towards available data sets. This setting generated very good results (see Figure 2) and guaranteed the steepest learning curve for computer science students as they cooperated with domain experts experienced in digital humanities. However, for some projects I, educated in computer science, needed to act as a humanities scholar, being the least favorable setting as only fake interdisciplinary discussions are possible.The Helper: On the other hand, the number of computer science students joining a course might be limited. In that case, I could advise students in the role of a computer scientist, better suitable considering my own background. The focus in such projects was rather on data modeling and acquisition as well as applying existing tools and libraries than developing new solutions. An example is shown in Figure 3.Figure 1: The mediator project developed a method to semi-automatically extract biographical information about members of the German Bundestag in 2019, and the adapted stream visualization allows multifaceted exploration of biographical features.Figure 2: The real humanities scholar project focused on the development of an interactive tag cloud that supports composing engineering branches based on the study subjects of engineering professors. More information can be found in a related publication on the project (Meinecke and Jänicke, 2018).Figure 3: The helper project focused on the contents published at three German websites known for publishing fake news articles. For comparatively analyzing the results, the TagPies visualization (Jänicke et al., 2018) was adapted.I recommend making such courses accessible to all students as it prepares them best for potential future collaborations in a digital humanities context. More detailed information on theoretical contents, conducted student projects and course reflections can be found in my related IVAPP article (Jänicke, 2020).
Culture Analytics is a collaborative, translational data science that explores culture and cultural interaction as a multi-scale / multi-resolution phenomenon. The macroscopic view, that allows a researcher to move from the microscale of close reading, up through the mesoscales, and on to the macroscale of distant reading, is a hallmark of the field. Following the 2019 workshop which focused on the use of time series as a way to understand and visualize humanities data, the 2020 workshop is dedicated to how network exploration and analysis can be similarly used to understand and visualize. The questions that focus the workshop are: How can network visualisations provide a distant viewing of data? What are the different steps to build a network? How to read it? Similar to 2019, the workshop will begin with an introduction to culture analytics and network analysis offered as a framework for the rest of the workshop. Afterwards, there will be two brief presentations of of research examples, one involving a small dataset and another one with big data. This will be followed by a hands-on tutorial with Gephi that will introduce participants to the basics of network exploration and analysis. Gephi is a popular network visualization and analysis tool. It helps scholars investigate empirical phenomena that can be seen as nodes and links, entities and relations. Drawing from graph theory and information design, it helps shifting the focus from the entities to their relations. It repurposes computational metrics to describe and characterize the topology of empirical networks.
This project attempts to apply the techniques of social network analysis (SNA) and visualization to the representations of rabbinic interactions in the Babylonian Talmud, a sprawling text written in Hebrew and Aramaic and probably redacted in Babylonia (modern day Iraq) in the sixth century CE. Our goals are (1) to develop a workflow and methodology allowing us to visualize and analyze the interactions between rabbis as represented in the Babylonian Talmud; (2) to see if we could learn something new about the relationship between rabbis as represented in the Talmud and/or the process of its redaction; and (3) to present a public-facing interface allowing scholars to interact directly with our visualization.Many of the research questions that drive this project go back more than a century. Pioneering work in Jewish studies (especially Albeck (1969); Margolioth (1987)) has attempted to detail the relationships between some of these rabbis. This work remains valuable, although it sometimes uses outdated methodological assumptions. Some of the relationships, for example, are reconstructed on the basis of stories about rabbis that most scholars today would understand as late, fictional creations. So too, scholars have long tried to understand the process by which the Babylonian Talmud was redacted (for summary of the scholarship on this, see Rubenstein 2013). Historians have also tried to understand the rabbis as a “network”, although without applying the quantitative tools now available (Hezser (1997); Lapin (2012)).In this part of the project, we focused our attention on citation chains. Rabbis frequently say things in the name of other rabbis (e.g., “Rabbi X said in the name of Rabbi Y who said in the name of Rabbi Z” – these chains usually consist of two or three names but can go up to nine!). By focusing on simply the names in these chains (and not the content of what they reported), our work intersects with that of Zhitomirsky-Geffet and Prebor (2018) and Josh Waxman (2019). At the same time, both our workflow and the kinds of questions that we were asking of the network as a whole make it distinct.The first step in our workflow was to identify each instance of a citation chain in the Hebrew/Aramaic text. In order to do this, we compiled a list of the names of all rabbis mentioned in ancient rabbinic literature (along with any aliases that they had) and assigned each a unique numeric identifier. The list was created through both automated and manual processes. Then, we created and ran a pattern matching program on a digital version of the “standard” printed edition (Vilna) of the Babylonian Talmud text to identify instances of rabbinic names and citation chains. Once identified, the program split the citations into “source” and “target” rabbis so we could identify who was citing whom. The results of the automated process were highly accurate as we verified through manual review of a statistically significant sample.The program identified 5,245 citation instances. When grouped into unique interactions (e.g., Rabbi X may cite Rabbi Y twenty times, but we counted that as one unique interaction), we were left with 630 rabbis (our nodes) and 1217 unique interactions (our edges). We loaded our node and edges files into Gephi (Gephi) and UCINET. A visualization can be seen in Figure 1, which (using a Force Atlas 2 layout) groups the more connected rabbis toward the center.Figure 1: Graph of All Rabbis in Babylonian Talmud Who Appear in Citation ChainsWe have two major research findings. First, and less surprisingly, when separated into Modularity Classes through an unsupervised algorithm, the rabbis relatively cleanly separated into groups that clustered around rabbinic figures who themselves had many connections, which looks like a “school” structure (see Figure 2). Previous research has led us to expect this. Second, and more surprisingly, the rabbis at the centers of each of those circles were themselves densely and directly connected to each other. These ten or fifteen rabbis, over four centuries and two geographical locales, served as the backbone for the rabbinic network. It is still unclear to us whether these connections represent real social interactions or can better be explained as the result of later editing and redactional decisions and conventions.Figure 2: Rabbis in Citation Chains in Babylonian Talmud in Modularity ClassesThere are problems inherent in this data. The rabbis are themselves sometimes unsure about an attribution (and explicitly argue about it). Since rabbis sometimes shared names or nicknames, it is sometimes impossible to match with certainty a name with a distinct individual; in such cases we assigned the shared name to the more prominent rabbi. Moreover, we are just using one, easily available text. Manuscripts sometimes record these attributions differently. We feel that given the macro approach we took to this network, these problems become less significant. Nevertheless, they need to be better taken into account in future, more fine-grained analyses. By the time this is published, we should have our data and code freely available on Github. It may take us longer to develop a public-facing interface, perhaps along the lines of “The Six Degrees of Francis Bacon.” We will also extend our approach to other interactions in the Babylonian Talmud (e.g., when a rabbi asks a question of another rabbi); to other rabbinic texts from this period; and to other manuscript versions of these texts.
The usefulness of computational linguistic tools, such as named entity recognition (NER) systems, in linguistic or literary studies of under-resourced languages is an area that is still relatively unexplored. We applied NER systems to one Afrikaans novel and two scanned dramas, one in Tshivenḓa and one in Xitsonga. Personal relations are identified through character name co-occurence in sentences and these relationships are visualized using Gephi, following the approach by Van de Ven et al. (2018). The research identified several practical problems: low quality OCR, low quality NER, limited amounts of NE and language specific issues.

        
            The CWRC-Writer XML/RDF editor is the centerpiece of the Canadian Writing Research Collaboratory (CWRC) platform for the production, hosting, and dissemination of digital humanities scholarship. In development since 2011 and launched with the platform in 2016, the browser-based editor has reached maturity and stability. Well prior to this, the team had begun strategizing towards sustainability. We outline this strategy while highlighting features of the editor.
            Compared to some outcomes of digital dumanities tool building – such as gaining new insights into one’s own research – the effort of turning a tool into a sustainable, generalized service is less glamorous, more laborious, and less acknowledged. Tool-building is considered part and parcel of the scholarly work of DH 
                (Schreibman and Hanlon, 2010) and is beginning to be recognized by academic reward systems. Yet scant support and rewards accrue once software is up and running. This situation has changed little over the years, despite increasing concern regarding digital infrastructure sustainability generally 
                (Eghbal, 2016; Maron and Pickle, 2014) and attention to “care and repair” within DH 
                (Nowviskie, 2015; Sayers). Like all software, DH tools require maintenance, enhancement and updates, which is to say, continued funding and expertise. 
            
            Pursuing uptake seems like a natural approach to the sustainability dilemma, since:
            
                it is easier to demonstrate the success of a tool and to justify further resource allocation in light of increases in use; and
                adopters of a tool are invested in its survival and might put resources towards sustainability.
            
            However, uptake is no guarantee of sustainability. As observed by Cameron Neylon, many scholarly infrastructures are public goods, and “Finding sustainability models to support them is a challenge due to free-loading, where someone who does not contribute to the support of an infrastructure nonetheless gains the benefit of it” 
                (2017: 3). Nevertheless, unused tools are poorly positioned to request continued funding or support.
            
            The uptake or adoption of existing DH software by new users is far from guaranteed, even if it fulfills a need that it is well-documented in the research community where it originates. Fred Gibbs and Trevor Owens crystallize the ways in which tool uptake is hindered by multiple factors 
                (2012). Significant problems include:
            
            
                
                    managing expectations, while also scaling up functionality from local to more general needs (Koeser and Hicks, 2018);
                
                limited 
                    learning resources (examples, user documentation);
                
                unintuitive or complex 
                    user interfaces that discourage novice users;
                
                lack of support for 
                    standards and interoperability.
                
                community building
            
            Together with more mundane but important activities like code maintenance, stable hosting, and systems administration, these factors create challenges that can prove fatal to promising technologies. Some are proclaimed at digital humanities conferences but seldom heard from again, while others like Paper Machines 
                (Guldi and Johnson-Roberson, 2012) show immense promise but do not develop into fully robust tools. Even mature tools with uptake from a wide range of users, such as Gephi, live quite precarious lives 
                (Jacomy, 2018). In short, the challenges of sustaining tools are manifold. We use the above points as a rubric for reflecting on CWRC-Writer’s engagement with the challenges of uptake.
            
            
                Scaling features and expectations 
                The modular CWRC-Writer exists in several types of installation to suit users from novices to technical experts: 
                
                    CWRC-Writer: available to researchers within the CWRC platform, where it is integrated with an Islandora repository, 
                    Git-Writer (
                        cwrc-writer.cwrc.ca): uses GitHub’s file storage, versioning, and authentication to allow anyone to edit GitHub-hosted XML documents (Fig. 1).
                    
                    Installations by third parties in other software stacks.
                        
                    
                
                
                    
                
                Fig. 1. Git-Writer document loading interface
                To support a wide variety of users, CWRC-Writer provides these core features:
                
                    an interface that renders XML in a human-readable layout using CSS (Fig. 2);
                    XML tagging, with or without tags showing, with validation and error identification;
                    raw/source XML editing for experts;
                    entities tagging in XML and/or Web Annotation RDF with built-in authority lookups.
                
                Members of the DH community, as well as literary and cultural studies scholars using XML for their texts, were involved from the beginning in the design of the tool. The user group comprises both power-users – researchers with decades of experience in markup – and novice or occasional users with little familiarity with DH. CWRC-Writer was designed from the outset as a light-weight editor to allow novices to tag XML documents and link them to named entity authorities, such as the Virtual International Authority File, in a manner that would avoid the steep learning curve associated with other, more complex editing tools 
                    (Brown, 2015). 
                
                This lightweight usage is our main use case. CWRC-Writer does not aim to replace a full-featured XML editor for heavy-duty markup or transformations. The complexity of managing XML through an HTML front-end mean that major restructuring, for instance, is very tricky. To ensure that available affordances are aligned with the needs of the users, CWRC-Writer offers three different editing modes: 
                
                    A default combined XML &amp; RDF mode creates both XML tags and Web Annotations identifying entities in the same span of text; 
                        external named entity identifiers are mapped onto the equivalent tags within supported XML schemas - which include established 
                        
                            TEI customizations
                        
                         and other schemas employed by CWRC-supported projects. 
                    
                    RDF-only mode for Web Annotations that leave the body of the XML file untouched.
                    XML-only mode for tagging without adding any Web Annotations. 
                
                
                    
                
                Fig. 2. CWRC-Writer document showing application of CSS
                In conjunction with particular user communities, we are extending CWRC-Writer functionality based on a document’s schema declaration; for example, for EpiDoc files, a popup editor for translations will allow users to create or tag a translation while viewing it side-by-side with the original. To support transcription, side-by-side display of the XML and images allows transcribers to view the scanned manuscript within the tool (Fig. 3).
                
                    
                
                
                    Fig. 3. Editing interface (XML &amp; RDF mode) with side-by-side display of manuscript scan
                    
                
            
            
                Learning resources
                We mitigate the challenge of a new interface by providing extensive, searchable 
                    
                        user documentation
                     (produced with the DITA documentation standard) and 
                    
                        tutorial videos
                    , as well as virtual office hours for real-time support. Learning to apply markup is a major challenge for the uninitiated, so there are sandbox templates for fooling around. Projects can create customized document templates that can be used to kickstart content creation and editing. These can provide highly detailed instructions, in order to promote consistency and best practices. 
                
            
            
                A user-friendly interface 
                From 2012 on, CWRC-Writer has undergone successive rounds of user testing, which have informed feature development and UX improvements. Two extensive rounds of survey-based user-testing were conducted before 2016, followed by numerous informal consultations and feedback from users and workshop participants. CWRC-Writer code is available in GitHub and a ticketing template allows adopters to submit both feature requests and bug reports. Formal announcement of the GitHub version in 2019 will be followed by another round of systematic user testing. 
            
            
                Standards and interoperability 
                CWRC-Writer editor adheres to the standards for both markup and Web Annotation. An integrated XML validator allows users to validate against the declared schema as they work on the document. TEI is supported in all version of the editor. RDF annotations adhere to the Web Annotation Data model, a W3C Recommendation that is being widely adopted within DH and in the scholarly publishing community as a standard for annotation data.
            
            
                Promoting a community of users 
                In addition to passive adopters, who employ CWRC-Writer as made available through CWRC or GitHub, we have projects joining CWRC primarily thanks to its integration of the editor with other tools. There is growing interest from members of the DH community considering it for use in TEI editing projects, as components of library-based DH tool suites, or for teaching XML. The Center of Digital Humanities Research at Texas A&amp;M has produced a containerized version and has installed it on top of Fedora 4 as part of a larger toolkit. Bucknell University is installing a version of the Git-Writer to support diverse local DH projects, and other institutional installations are planned. External partners were consulted for the development of Git-Writer, and the code is configurable, modular, and well documented in order to permit installation in a range of software environments. Users currently cohere around specific projects. We hope a broader CWRC-Writer community will develop as numbers grow, and be joined by a community of developers familiar with and willing to contribute to upkeep. However, the experience of other projects indicates that this is a major challenge.
            
            
                Future developments
                CWRC-Writer has for several years now, since its launch within CWRC, been thinking hard about how to promote uptake and long-term sustainability. Our development roadmap is constructed around current and oncoming user needs. We will continue to adapt our strategy in response to insights gained from further user testing and feedback from the community following the launch of the Git-Writer to the DH community.
            
        
        
            
                
                    Bibliography
                    
                        Brown, S. (2015). Remediating the Editor. 
                        Interdisciplinary Science Reviews, 
                        40(1): 78–94 doi:10.1179/0308018814Z.000000000106.
                    
                    
                        Eghbal, N. (2016). 
                        Roads and Bridges: The Unseen Labor Behind Our Digital Infrastructure. Ford Foundation https://fordfoundcontent.blob.core.windows.net/media/2976/roads-and-bridges-the-unseen-labor-behind-our-digital-infrastructure.pdf.
                    
                    
                        Gibbs, F. and Owens, T. (2012). Building Better Digital Humanities Tools: Toward broader audiences and user-centered designs. 
                        Digital Humanities Quarterly, 
                        006(2).
                    
                    
                        Guldi, J. and Johnson-Roberson, C. (2012). 
                        Paper Machines. http://papermachines.org/.
                    
                    
                        Jacomy, M. (2018). Is Gephi obsolete? Situation and perspectives. 
                        Gephi Blog https://gephi.wordpress.com/2018/11/01/is-gephi-obsolete-situation-and-perspectives/ (accessed 27 November 2018).
                    
                    
                        Koeser, R. S. and Hicks, Benamin (2018). Bridging Digital Humanities Internal and Open Source Software Projects through Reusable Building Blocks. Mexico City, Mexico https://dh2018.adho.org/en/bridging-digital-humanities-internal-and-open-source-software-projects-through-reusable-building-blocks/ (accessed 27 November 2018).
                    
                    
                        Maron, N. L. and Pickle, S. (2014). 
                        Sustaining the Digital Humanities Host Institution Support beyond the Start-Up Phase. ITHAKA.
                    
                    
                        Neylon, C. (2017). Sustaining Scholarly Infrastructures through Collective Action: The Lessons that Olson can Teach us. 
                        KULA: Knowledge Creation, Dissemination, and Preservation Studies, 
                        1(1): 3 doi:10.5334/kula.7.
                    
                    
                        Nowviskie, B. (2015). On capacity and care 
                        Bethany Nowviskie http://nowviskie.org/2015/on-capacity-and-care/ (accessed 27 November 2018).
                    
                    
                        Sayers, J. From Make or Break to Care and Repair 
                        MLab in the Humanities. http://maker.uvic.ca/inke16/ (accessed 27 November 2018).
                    
                    
                         (2015). Remediating the Editor. 
                        Interdisciplinary Science Reviews
                        40, (1): 78–94 doi:10.1179/0308018814Z.000000000106.
                    
                    
                        Eghbal, N. (2016). 
                        Roads and Bridges: The Unseen Labor Behind Our Digital Infrastructure. Ford Foundation https://fordfoundcontent.blob.core.windows.net/media/2976/roads-and-bridges-the-unseen-labor-behind-our-digital-infrastructure.pdf.
                    
                    
                        Gibbs, F. and Owens, T. (2012). Building Better Digital Humanities Tools: Toward broader audiences and user-centered designs. 
                        Digital Humanities Quarterly, 
                        006(2).
                    
                    
                        Guldi, J. and Johnson-Roberson, C. (2012). 
                        Paper Machines. http://papermachines.org/.
                    
                    
                        Jacomy, M. (2018). Is Gephi obsolete? Situation and perspectives. 
                        Gephi Blog https://gephi.wordpress.com/2018/11/01/is-gephi-obsolete-situation-and-perspectives/ (accessed 27 November 2018).
                    
                    
                        Koeser, R. S. and Hicks, Benamin (2018). Bridging Digital Humanities Internal and Open Source Software Projects through Reusable Building Blocks. Mexico City, Mexico https://dh2018.adho.org/en/bridging-digital-humanities-internal-and-open-source-software-projects-through-reusable-building-blocks/ (accessed 27 November 2018).
                    
                    
                        Maron, N. L. and Pickle, S. (2014). 
                        Sustaining the Digital Humanities Host Institution Support beyond the Start-Up Phase. ITHAKA.
                    
                    
                        Neylon, C. (2017). Sustaining Scholarly Infrastructures through Collective Action: The Lessons that Olson can Teach us. 
                        KULA: Knowledge Creation, Dissemination, and Preservation Studies, 
                        1(1): 3 doi:10.5334/kula.7.
                    
                    
                        Nowviskie, B. (2015). On capacity and care 
                        Bethany Nowviskie http://nowviskie.org/2015/on-capacity-and-care/ (accessed 27 November 2018).
                    
                    
                        Sayers, J. From Make or Break to Care and Repair 
                        MLab in the Humanities. http://maker.uvic.ca/inke16/ (accessed 27 November 2018).
                    
                    
                        Schreibman, S. and Hanlon, A. M. (2010). Determining Value for Digital Humanities Tools: Report on a Survey of Tool Developers. 
                        Digital Humanities Quarterly, 
                        004(2).
                    
                
            
        
    

        
            Literature and history, writ large, are fields with prominent voices in the digital humanities community. Art history, film studies, archaeology, and anthropology form recognizable disciplinary clusters within the international discourse of digital humanities as well. Since the mid-20th century, “area studies” have offered their own interdisciplinary organizing principle for drawing together scholars who work with a variety of materials and approaches. While on one hand, this arrangement would appear to be naturally conducive to international collaboration, geopolitical tensions and restrictions have at times formed a barrier between area studies scholars in the United States, and their colleagues working in and around the “areas” in question. This has been especially true in the field of Slavic and East European studies, which have been knit together in the United States through two major professional organizations that have -- through a combination of circumstance, necessity, and then habit -- cultivated a distinct and somewhat insular scholarly community in Slavic studies.
            The fall of the Iron Curtain, the fluctuations in the perceived threat of Russia to the United States, and the spread of the Internet as a medium for communication have provided scholars worldwide with opportunities to engage with Slavic area studies through a global and collaborative lens in ways that were not conceivable earlier in the history of the field. While many U.S. scholars have taken advantage of the possibility to travel to Slavic and East European countries, the scholarly networks of citation and discourse remain centered within the communities formed by the U.S.-based professional organizations.
            The emergence of digital humanities tools and methodologies has provided an opportunity for rethinking the collaborative landscape for Slavic and East European studies. Much as in the humanities as a whole, the percentage of scholars in the field actively using digital tools and methodologies is fairly small. For Slavic and East European studies, however, any effort to develop a community around digital humanities is compounded by the comparatively small overall size of the field. Indeed, while there have been efforts to run a digital humanities interest group through one of the major professional organizations, the work of sustaining the group has fallen on one or two people, as there is not enough of a critical mass to spread around the necessary but time-consuming work of cultivating and growing the group. Rather than working within existing scholarly organizational frameworks, some U.S.-based scholars have turned towards international collaboration to further their engagement with digital tools and methodologies. This panel brings together Slavists and East Europeanists from the United States, Western Europe, and Russia whose research, teaching, and infrastructure development is shaped by engagement with colleagues who share similar materials and methods despite widely varying national and institutional contexts. In addition to presenting highlights of their own work, panel participants will reflect on the ways in which digital humanities provides a different organizing principle for their scholarly networks and community. 
            
                Cultural Heritage and Critical Multiperspectivity: Building Research Infrastructures on Eastern Europe
            
            Due to the constantly increasing number of digitally available sources, those working in the fields of History and Cultural Studies are faced with a significant challenge, namely, to develop new practices and procedures around the verification of sources and the provision of digital material. Of central importance here are questions relating to the provenance, validity, and critical analysis of sources. These matters are particularly pertinent in the case of East Europe, where transnational conflicts around shared historical and cultural heritage have given rise to a special need for research-based information, digital source criticism, and questions of research ethics.
            As a leading European research institute on the history of Eastern Europe, we continue to develop our shared research infrastructure in collaboration with other partners, and we need to keep these goals in mind. This presentation aims at addressing the challenges and opportunities of building up a multi-national infrastructure for a digital and critical documentation of cultural heritage in Eastern Europe. This process involves not only developing research software and a standardized vocabulary, but also mapping out multi-perspective approaches relating to the exegesis of digital sources within metadata structures and, last but not least, providing dialogue-based formats for reflecting on historical sources in the digital age.
            
                Being an Englishman in New York, or How to Launch an International DH Lab in St. Petersburg
            
            DHlab, the international digital humanities lab launched in at our technical university in St. Petersburg (Russia) in October 2018, is dedicated to exploring how technology facilitates new possibilities in understanding society and global culture. This is a collaboration between our institution and a private university in the United States, and intends to draw in partners from DH communities all over the world. In this presentation, we will discuss our priorities when starting a humanities-focused initiative at a technical university. These include 1) the importance of humanistic disciplines in the discourse of Information Technologies, Computer Science and Engineering; 2) motivating women in science; 3) applying STEM methods to Arts and Humanities research problems; 4) managing different approaches while working on digital humanities projects; 5) involving students of different majors and interests in interdisciplinary projects; and 6) highlighting digital humanities among wide audience using social networks. We will share our experience in training the next generation of DH scholars, developing innovative DH projects, contributing to creating useful software, providing women within Humanities and STEM with equal opportunities to conduct interdisciplinary research etc.
            We will also discuss one of our most significant projects, a web application that maps historical and cultural heritage data about key landmarks of St. Petersburg, Russia. With input from scholars of history, library science, cultural studies and information technologies, the project team has conducted semantic analysis on a large, multilingual textual corpus that includes memoirs, documentaries and periodicals, and uses Text Encoding Initiative (TEI) to encode information about people, relationships, and events, and Uniform Resource Identifiers (URIs) to identify locations. All landmarks are being mapped onto an interactive city map of St. Petersburg with a user-friendly interface to facilitate easy navigation and filtering.
            
                Edges of Slavic Studies: Network Analysis of an Areas Studies Field from the Cold War to the Present
            
            This presentation uses DH tools to turn a lens to the Slavic Studies professional community. In 2018, the Association for Slavic, East European and Eurasian Studies (ASEEES) the main North American-based international academic professional organization in the field, celebrates its 70 th anniversary. As part of the anniversary, the association published the programs for thirty-four of its conventions. These programs are a valuable guide not only to individual contributions but to the institutional networks embedded in the Slavic Studies field. This paper uses network analysis to analyze the shifting intellectual motivations and professional ties in Slavic Studies.
            Before the fall of the USSR, Slavic Studies was not only an academic field but a front in the Cold War. US government funding motivated by fear of communism also produced works in the humanities with no immediate political value and created a new field of area studies (Engerman 2009). Network analysis can assess Engerman’s qualitative research and go beyond it. After the Cold War ended, Slavic Studies undertook a “post-mortem” of the USSR. How did the constellation of intellectual and professional networks shift? After the financial crisis of 2007-08 that brought cuts in research spending, how did Slavic Studies change? What institutions remained in the field and with what institutional concerns? Similarly, network analysis can help understand early academic reactions to Russia’s more aggressive stance in the world since 2011.
            This study can also contribute to understanding how digital humanities is growing in fields outside of Western Europe and North America. By isolating the networks in panels whose papers use relevant keywords (e.g., “digital,” “dh,” “computational”) the paper will ask how the impact of digital humanities has and has not impacted the Slavic field.
            
                International DH Collaboration as Pedagogy
            
            Pedagogy has long been a major topic of interest in the DH community. An opportunity for engaging students in DH that has not been addressed is through involvement in international partnerships. In this presentation, we discuss our institution’s emerging Slavic DH Working Group, our decision to prioritize collaboration with international partners, and our efforts to include both graduate and undergraduate students in our work. As an alternative - or supplement - to the curricular or project-based learning models that are most common in many DH programs, involvement in international partnerships provides not only skills, competencies and exposure to professional practice, but opportunities for cross-cultural knowledge exchange that are key for the advancement of both field-specific and DH scholarship.
            The Slavic DH Working Group at our research university is the only one of its kind in the United States. Established in 2017 through the joint effort of faculty, graduate students, librarians and DH staff, the Slavic DH Working Group is a community of scholars at all levels, from various disciplines, and engaged in a variety of professional fields. Promoting the exploration of digital humanities in Slavic, East European and Eurasian Studies, the group meets monthly during the academic year for events, trainings, and mentorship . In this presentation, we will discuss our collaboration with a research institute in Germany whose focus is Eastern European history. In the summer of 2018, we took eight members of the Slavic DH Working Group - including two graduate students and two undergraduates - to Germany for a 5-day DH workshop called “Digital Mapping Eastern Europe.” Besides our US group, the workshop brought in early-career scholars from Germany, Hungary, Poland, Czech Republic, Ukraine, Belarus, Russia and Israel. We will discuss how the format of the workshop allowed the graduate students to make significant strides in their dissertation work. The workshop was a unique opportunity for what Geoffrey Rockwell and Stefan Sinclair call “acculturation” (Rockwell and Sinclair, 2012) - not just academic professionalization, but broader exposure to the culture of a field that includes different types of jobs, and in various national and international contexts. We will also discuss the opportunities this experience afforded the undergraduate participants, and the kind of both collaborative and independent work that the workshop generated. By contrast to the more conventional contexts in which undergraduates learn DH - either in the classroom setting or as paid laborers on faculty-led DH projects - participation in the international exchange allowed students to become involved in the community as scholars in their own right.
            We will suggest that international collaborations in DH are particularly beneficial for smaller fields like Slavic where use of digital methods are relatively new, and where formal and fruitful international exchange by groups of scholars can difficult to sustain. Bringing DH into the graduate and undergraduate Slavic Studies experience exposes students to new scholarly approaches while animating Russian and East European cultural heritage and collections for young scholars.
            
                Conversational Versus Co-occurrence Models in Networking the Russian Novel
            
            The Russian novel has been traditionally regarded as the novel of ideas, in which the conflicting views on the national identity and Russia’s relationship with the “other” are presented in the dramatized narrative. Especially after Mikhail Bakhitn’s pioneering ideas of dialogism and polyphony of Fyodor Dostoevsky’s novels, it became common to view the novel as a balanced dramatization of conflicts among polar opposites. In my ongoing research project on the network novel in the late imperial Russian culture, I approach Tolstoy’s and Dostoevsky’s novels as models of an emerging liberal society. These narratives are not merely depictions of individual experiences manifested in text, but also represent complex societies whose imaginary social forms can be quantified and analyzed.
            In British and American nineteenth-century novels, a shift occurred between the novel of domesticity and the network novel which exaggerated contrast within texts to expose the search for a resolution in the world outside. This study aims to examine a corpus of Russian novels to see whether they have the same clear dichotomy between a nuclear family type and a social panorama type.
            This study takes the approach of comparing the conversational and with co-occurrence methods for collecting data from the novels. The conversational method is based on the participation of the characters in a “speech act,” defined as a continuous span of narrative time featuring a set of characters co-located in space and time, where they take turns speaking, are mutually aware of each other, and each character’s speech is intended for the other to hear. The co-occurrence method uses the automatic extraction of interactions based on the appearance of two characters in one sentence or short paragraph. This study compares networks created in Gephi using each method and argues that the co-occurrence of the characters in one scene does not yield significantly different results compared to measuring associations via speech acts. The relative simplicity of identifying speech acts, versus identifying all instances of speaking and non-speaking characters’ co-location in space and time, makes an approach based on speech acts even more attractive.
            Examining the effectiveness of different methods for generating networks from novels has implications for technical developments that can further research in Russian literature. If speech act-based networks reliably capture the nodes and edges of the Russian realist novels, that presents an opportunity to develop algorithms that can identify speech acts with a reasonable degree of accuracy. This, in turn, has implications for corpus development, and ensuring that those corpora are formatted in machine-readable ways (e.g. encoded in UTF-8 and available as plain text files, rather than in proprietary formats). This talk will reflect on experiences, challenges, and successes in sharing corpora and tools with an international group of colleagues who use similar digital approaches in their research on Russian literature.
            
                Rethinking Scholarly Networks Through the Lens of Digital Humanities
            
            Scholarly societies are core social and community infrastructure in the humanities. Presenting at conferences sponsored by these societies is a crucial component for gaining visibility in a field, and bolsters the legitimacy of one’s candidacy for jobs, awards, and other forms of recognition. In North America, these societies are typically scoped nationally, and attending a major annual conference for a society is an expensive endeavor — a situation only compounded for scholars who work interdisciplinarily. The professional necessity of directing significant amounts of one’s travel budget towards these conferences, combined with the limits of their national scope, impede international collaboration, particularly for early-career scholars.
            Digital humanities provides a different axis for finding and engaging with colleagues across national boundaries. Particularly for the text-centric scholarship commonly found in literature and history, building corpora is time-consuming and expensive endeavor. Once completed, such corpora can be a transformative resource for colleagues pursuing a wide range of research questions. For scholars whose work involves applying natural-language processing tools to languages other than modern English, there are resources like stopword lists, word embeddings, and pre- and post-processing scripts that can meaningfully be shared between projects.
            Collaborative development of tools, resources, taxonomies, and infrastructure that are directly related to Slavic studies is itself a significant step forward for the field -- but in the long run, perhaps the most significant impact of DH-centered collaboration is the “downstream effects” of the relationships established between scholars. Scholarly networks are valuable not only for research collaboration, but also as a source of advice when dealing with institutional challenges, and connections and opportunities for one’s students.
            This paper will draw together unifying themes from the presentations given by other panelists and consider the ways in which digital humanities could reshape the current insular, nationally-centered landscape of Slavic and East European area studies in the U.S., and foster the emergence of new international scholarly networks that have an impact beyond the field of Slavic studies itself. It will also reflect on the ways that existing disciplinary scholarly societies could take a more proactive role in breaking down the national boundaries that have emerged through their current structure, for instance, by fully engaging with digital tools and platforms with roots in the digital humanities / scholarly communications communities, such as Humanities Commons.
        
        
            
                
                    Bibliography
                    Engerman, D. (2009). 
                        Know Your Enemy: The Rise and Fall of America's Soviet Experts. Oxford, UK: Oxford University Press.
                    
                    Rockwell, G. and Sinclair, S. (2012). Acculturation and the Digital Humanities Community. In Hirsch, B. (ed), 
                        Digital Humanities Pedagogy: Practices, Principles and Politics. Cambridge: UK, OpenBook Publishers.
                    
                
            
        
    

        
            
                Introduction
                Although there have been some infrastructural developments of late, the main 
                    modus operandi in digital literary studies is still to apply a certain research method to an ephemeral corpus. In a best-case scenario, the results are 
                    somehow reproducible, in a worst-case scenario they are not reproducible at all. At best, there is an openly accessible corpus in a standard format such as TEI, another markup language, or at least TXT. At worst, the corpus is not even accessible, i.e., the research results cannot be questioned.
                
                However, there are signs that this is slowly changing. Some projects provide interfaces that allow for multiple ways of access to corpora. One of these projects is DraCor, an open platform for research on (European) drama, which will be introduced in this paper (accessible at 
                     or via its GitHub repositories or its API). DraCor transforms existing text collections into 'Programmable Corpora' – a new term we bring into play with this talk.
                
            
            
                Building Blocks
                
                    Vanilla Corpora
                    Similar to the COST Action on European novels (Schöch et al. 2018), the DraCor project seeks to establish a bundle of multilingual drama corpora encoded in basic TEI as basis for digital comparative studies. To date, the platform enables access to a Russian-language (
                        ) and a German-language corpus of plays (
                        ). Similar to Paul Fièvre's collection "Théâtre classique", these corpora are designed as vanilla corpora, which initially contain hardly any special markup beyond the necessary, but are freely available and can therefore be forked, enriched and expanded. To demonstrate that other corpora can be easily linked to the platform, we forked the Shakespeare Folger Corpus and the Swedish Dramawebben corpus and connected it, and all existing extraction and visualisation methods of the platform are readily applicable to the newly added corpora (
                         and 
                        ). Other corpora of dramatic texts are to follow; the only prerequisite is that they are encoded in TEI.
                    
                    The advantages of a freely available corpus hosted on GitHub are obvious. Not only can the corpus be cloned and loaded directly into an XML database like eXist-db. Using the SVN wrapper from GitHub, the entire corpus can also be downloaded directly, in its current state and without version history if this is not needed:
                    svn export https://github.com/dracor-org/rusdracor/trunk/tei
                    An openly accessible GitHub repository also means that pull requests for error correction are possible and welcome.
                
                
                    XML Database (eXist-db) and Frontend
                    DraCor relies on eXist as XML database to process TEI files and to provide functions for researching the corpora. The frontend is built with React (
                        ), it is responsive and easily extensible. However, the focus is not on the GUI, but on the API (on the general differences between these two approaches to interfaces cf. Bleier/Klug 2018).
                    
                
                
                    API
                    To come close to the ideal and the possibility of applying "all methods to all texts" in a simple manner (Frank/Ivanovic 2018), it takes more than open corpora. The article by Frank/Ivanovic advocates SPARQL endpoints (for which there is also a readily-available app for eXist-db: 
                        ).
                    
                    DraCoroffers such endpoint, but also features a rich general API documented and explained via Swagger (
                        ). In a subarea of corpus philology, the digital scholarly editions, discussions about more proactive use of APIs have already begun (for background information cf. Bleier/Klug 2018), the Folger Digital Texts API may serve as an example (
                        ). The advantage of a more modern solution like Swagger is that API queries can be executed live and directly and that the output can be controlled more precisely.
                    
                    A simple use-case scenario would look like this: using RStudio you can throw a quick glance into a corpus with just a few lines of code, maybe regarding the development of the number of characters in Russian drama between 1740 and 1940, stored in the metadata table (
                        ). This table, which can be obtained in JSON or CSV format, is read into a Data.Table, whereupon the values of two columns (year of publication and number of speakers) can simply be visualised via ggplot (Fig. 1).
                    
                    
                        
                            
                            Figure 1: Number of characters per play in chronological order (source: RusDraCor).
                        This very simple example is able to show the starting point of a decisive structural diversification of the Russian drama landscape. Pushkin's historical drama "Boris Godunov" (1825), result of his reading of Shakespeare, features speech acts of 79 characters, a number previously unthinkable in Russia drama.
                    
                    However, the possibilities are not limited to using ready-made API functions. New research ideas always create new needs for easily obtainable and reproducible data and metrics; the API can be extended accordingly, i.e., new research ideas can be implemented centrally in the API layer. This is made even easier by the fact that Apache Ant can be used to rebuild the entire development environment on your own system.
                    In addition to structural data and metadata, full texts without markup can also be obtained, e.g., if methods such as stylometry or topic modelling are the purpose, i.e., methods that need a "bag of words" and do not require markup.
                    All in all, the structure and documentation of open APIs makes it much easier to reproduce research results, which up to now has often been a time-consuming (or impossible) process.
                
                
                    Shiny App
                    An example of the versatility of the DraCor API is the Shiny App created by Ivan Pozdniakov (
                        ). Shiny is a framework based on R, which makes it possible to display interactive visualisations in the browser. The DraCor Shiny App does just that, relying entirely on the DraCor API for data retrieval. Thus, visualisations of the current database can be used for teaching and research purposes, but also for easier data correction.
                    
                
                
                    Didactics
                    The formalisation of literary texts, for example via markup, is not self-explanatory. Although the community can rely on some standards, every operationalisation depends on the actual research question. To give an example: if you would like to extract network data based on character interactions in a literary text, you would have dozens of different ways of doing this (e.g., Grayson et al. 2016 test different extraction methods for novels and compare the results). This also applies to plays. In order to sharpen the senses for this in teaching, we developed the tool "ezlinavis" (
                        ) and integrated it into the DraCor toolchain. Network data can be extracted from literary texts manually, also to raise the awareness for the contingency of this process, an important preliminary step to the eventual step of operationalisation.
                    
                    In addition to an approach to the gamification of the process of correcting TEI-encoded corpora (Göbel/Meiners 2016), we also developed a card game for teaching purposes in order to playfully train the understanding of network values (Fischer at al. 2018).
                    These didactic tools wrapped around the platform are an integral part of the whole project as they are based on the project data and operationalisations. While building the platform, it was important to recognise that data can take several forms and be equally important for research and teaching.
                
                
                    Linked Open Data
                    The TEI files contain GND [Integrated Authority File of the German National Library] and Wikidata identifiers for both authors and works. In this way, various data and facts that lie beyond one's own corpus work can be included. Something like an automatically created gallery of authors has a more illustrative character (de la Iglesia/Fischer 2016). But using the same identifiers, we can also determine if a corpus has a regional bias. Via Wikidata, we can easily display the distribution of the authors' places of birth and death on a map (by doing so, we could rule out that our German-language corpus GerDraCor has a regional bias, cf. Göbel/Fischer 2015).
                    Similarly, the Wikidata ID of the plays can be used to find out where they were first performed (example query: 
                        ), i.e., aspects of the performance history can be switched on, even though they are not the focus of the core project and based on data curated elsewhere.
                    
                
                
                    Infrastructure Instead of Rapid Prototyping
                    Projects like DraCor seek to provide the digital literary studies with a reliable and extensible infrastructure so that the research community can focus on research questions.
                    An important conclusion for us was that we would give up the further development of our all-in-one Python script collection "dramavis" (Kittel/Fischer 2014–2018 and Fischer et al. 2017), which we have been developing for four years now. From here on, we would rather devote our time to the API. "Dramavis" followed the idea of rapid prototyping and had to do all by itself, including the preprocessing of data (Trilcke/Fischer 2018), which is not untypical in the Digital Humanities. The code base has grown quite a bit in the meantime and its maintenance has become difficult and often enough led away from actual research questions.
                
            
            
                Outlook
                In allusion to the project "ProgrammableWeb" – which maintains a database of open APIs and whose slogan is: "APIs, Mashups and the Web as Platform" (accessible at 
                    ) – we propose the term 'Programmable Corpora' for research-oriented corpora providing an API.
                
                Programmable Corpora facilitate the implemention of research questions around corpora. It is to be expected that infrastructural efforts of this kind will pay off for the entire community with effects such as those listed by John Womersley in his presentation at the ICRI2018 conference in Vienna: a) dramatically increase scientific reach; b) address research questions of long duration requiring pooled effort; c) promote collaboration, interdisciplinarity, interaction.
                The are numerous ways to connect to Programmable Corpora, no matter if you don't want to code at all and only need a CSV file for Excel or LibreOffice Calc or a GEXF file for Gephi, if you want to research a corpus via its connections to the Linked Open Data cloud or just want to get specific data for your R or Python script without having to worry about the corpus and its maintenance and reproducibility (all this remains an option, though). Programmable Corpora make it easier to decide on which level of the platform your own research process starts.
            
        
        
            
                
                    Bibliography
                    
                        Bleier, Roman; Klug, Helmut W. (2018): Discussing Interfaces in Digital Scholarly Editing. In: 
                        Digital Scholarly Editions as Interfaces. BoD, Norderstedt, pp. V–XV. URL: 
                        
                    
                    
                        de la Iglesia, Martin; Fischer, Frank (2016): 
                        The Facebook of German Playwrights. URL: 
                        
                    
                    
                        Fischer, Frank; Dazord, Gilles; Göbel, Mathias; Kittel, Christopher; Trilcke, Peer (2017): Le drame comme réseau de relations. Une application de l‘analyse automatisée pour l’histoire littéraire du théâtre. In: 
                        Revue d'historiographie du théâtre. № 4 (2017). URL: 
                        
                    
                    
                        Fischer, Frank; Kittel, Christopher; Milling, Carsten; Schultz, Anika; Trilcke, Peer; Wolf, Jana (2018): Dramenquartett – Eine didaktische Intervention. In: 
                        Conference proceedings of 
                        DHd2018. University of Cologne, pp. 397–398. DOI: 
                        
                    
                    
                        Göbel, Mathias; Fischer, Frank (2015): 
                        The Birth and Death of German Playwrights. URL: 
                        
                    
                    
                        Göbel, Mathias; Meiners, Hanna-Lena (2016): Play(s): Crowdbasierte Anreicherung eines literarischen Volltext-Korpus. In: 
                        Conference proceedings o
                        f
                         
                        DHd2016. University of Leipzig, pp. 140–143. URL: 
                        http://www.dhd2016.de/abstracts/vortr%C3%A4ge-007.html
                    
                    
                        Grayson, Siobhán; Wade, Karen; Meaney, Gerardine; Greene, Derek (2016): The Sense and Sensibility of Different Sliding Windows in Constructing Co-Occurrence Networks from Literature. In: 
                        2nd IFIP International Workshop on Computational History and Data-Driven Humanities. Trinity College Dublin 2016. PDF: 
                        
                    
                    
                        Kittel; Christopher; Fischer, Frank (2014–2018): 
                        dramavis. Python script collection. Repository: 
                        
                    
                    
                        Schöch, Christoph et al. (2018): Distant Reading for European Literary History. A COST Action [Poster]. In: 
                        DH2018: Book of Abstracts / Libro de resúmenes. Mexico: Red de Humanidades Digitales A. C. URL: 
                        
                    
                    
                        Trilcke, Peer; Fischer, Frank (2018): Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen. In: Martin Huber and Sybille Krämer (eds.): 
                        Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden. (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3). DOI: 
                        
                    
                
            
        
    

        
            Network interpretation is a widespread practice in the digital humanities, and its exercise is surprisingly flexible. While there is now a wide variety of uses in different fields from social network analysis (Ables et al., 2017) to the study of document circulation metadata (Grandjean, 2016) or literature and linguistic data (Maryl and Elder, 2017), many projects highlight the difficulty of bringing graph theory and their discipline into dialogue. Fortunately, the development of accessible software (Bastian et al., 2009), followed by new interfaces (Rosa Pérez et al., 2018; Wieneke et al., 2016), sometimes with an educational dimension (Beaulieu, 2017; Xanthos et al., 2016), has been accompanied in recent years by a critical reflection on our practices (Weingart, 2011; Kaufman et al., 2017), particularly with regard to visualisation. Yet, it often focuses on technical aspects.
            In this paper, we propose to shift this emphasis and address the question of the researcher’s interpretative journey from visualisation to metrics resulting from the network structure. Often addressed in relation to graphical representation, when it is not used only as an illustration, the subjectivity of translation is all the more important when it comes to interpreting structural metrics. But these two things are closely related. To separate metrics from visualisation would be to forget that two historical examples of network representation, Euler (1736) and Moreno (1934), are not limited to a graphic reading (the term “network” itself would only appear in 1954 in Barnes’ work). In the first case, the demonstration was based on a degree centrality measurement whereas in the second case the author made the difference between “stars” and “unchosen” individuals while qualifying the edges as inbound and outbound relationships.
            This is why this paper propose to examine the practice of visual reading and metrics-based analysis in a correspondence table that clarifies the subjectivity of the translation while presenting possible and generic interpretation scenarios.
            
                Visual approach: making the global structure readable
                The way we read networks has changed over time. Historically the question of network readability was asked in terms of aesthetic criteria. In the word of Jacob Moreno “the fewer the number of lines crossing, the better the sociogram”. Even in the nineties, when giving birth to the modern layout algorithm, Früchterman and Reingold (1991) aimed at “minimizing edge crossings” and “reflecting inherent symmetry”. However these criteria do not seem so crucial to practices observed nowadays in digital humanities (and beyond).
                
                    
                
                
                    Fig. 1 Different contexts for network visualisation in DH2016, DH2017 and DH2018 abstracts.
                
                Looking at recent papers in digital humanities, networks appear to have a wide range of usages. Their visualisations are either self-sufficient [fig. 1.a.] (Algee-Hewitt, 2018; Pino-Diaz and Fiormonte, 2018; Verhoeven et al., 2018; Marraccini, 2017), an optional help to understanding [fig 1.b.] (Colavizza et al., 2016) or strongly connected to the text. Some authors use them to highlight the position of a specific node [fig. 1.c.] (Moretti et al., 2016), to compare layouts [fig. 1.d.] (Sozinova, 2016) or the layout of the same graph in time [fig. 1.e.] (Wright, 2016). They may aim at visualising communities [fig. 1.f.] (Rybicki et al., 2018; Torres-Yepez and Zreik, 2018), mapping a general structure [fig. 1.g.] (Gao et al., 2017), tracking density patterns [fig. 1.h.] (Gao et al., 2018) or monitoring algorithms like modularity clustering [fig. 1.i.] (Choinski and Rybicki, 2017). These usages reveal a different perspective in network visualisation where we expect the visual to translate underlying relational structures.
                It helps to give different names to these two different approaches. We call 
                    diagrammatic the perspective where the network is a diagram that we read by following paths. We do not want the edges to cross and we use aesthetic criteria to bring clarity. It was Moreno’s perspective, and is still relevant to small networks and local exploration. Then we call 
                    topological the perspective where the network is a structure that we read by detecting patterns. We expect the visualisation to help us retrieve structural features like clustering or centralities. It is a common practice in digital humanities, more holistic and relevant to larger networks. Aside or in complement, classic data visualisation is also employed to visualise non-relational structures (node attributes, etc.).
                
                
                    
                
                
                    Fig. 2 Various layouts do not follow a force-driven algorithm to make non-relational dimensions of the data explicit.
                
                In the topological perspective, a standard procedure is to assign nodes a position using a force-driven algorithm. This family of algorithms is known for displaying clusters that match a widely used measure of community detection, modularity clustering (Noack, 2009). Its translation remains however difficult to interpret locally, as we can never give a simple explanation for a node’s position. Classic data visualisation also translates non-relational structures, by itself or combined with a relational perspective. Different structural features may require different visualisations: the examples of fig. 2 shows curated visualisations using categories [fig. 2.a boys and girls, in the famous example of (Moreno, 1934)], temporality [fig. 2.b] (Jänicke and Focht, 2017) or hierarchy [fig. 2.c] (Grandjean, 2017). Though very different from force-driven placement, they display better certain structural features.
            
            
                Objectifying the structure with metrics
                Often opposed to visual interpretation, of which they would be a more objective and reliable representation, centrality measures have a history that goes back to more than half a century and shows that they are not immutable and require constant adaptation to usage Moreover, Freeman (1979) insists on the fact that the notion of “centrality” is the result of several intuitive conceptions. To remind that these metrics are based on “intuition” means to recognize that they have no meaning in themselves and that their interpretation must be rediscussed - and therefore translated - according to the context. This paper thus proposes to list and evaluate to which extent these metrics are applicable to humanities and social science data and can, if necessary, be “translated” into this language to complement visual analyses.
                
                    
                
                
                    Fig. 3 Three levels of interpretation that can be articulated: visual analysis (examples top left), use of global metrics (examples bottom right) and use of local metrics (highlighted nodes).
                
                
                    Global properties
                    Statistical analysis allows for comparing networks across multiple dimensions at once (Tank and Chen, 2017). For instance, comparing the 
                        number of nodes and edges of different graphs of the same type (Trilcke et al., 2016) can be a ranking tool that is directly translatable into natural language. In addition to that, studies suggest that 
                        density (the number of edges in relation to the number of nodes) is relevant to analyse character networks, especially when compared within a homogeneous collection (Evalyn and Gauch, 2018; Grandjean, 2015). This is also the case when measuring 
                        average path length (Trilcke et al., 2016).
                    
                
                
                    Local properties
                    With regard to local measures, the 
                        degree (number of neighbouring nodes) is the simplest 
                        centrality, and the only one systematically used between the late 1950s and early 1970s, before the development of more diversified metrics (Freeman, 1979). Its simplicity allows for a transparent translation: in a literary network, for example, it counts the number of times one character speaks to another (Jannidis et al., 2016).
                    
                    The notion of 
                        betweenness centrality disrupts the conception of what the “centre” of a network may consist of. Its ability to reveal structural elements bridging large, immediately visible clusters makes it popular in the social sciences since the emergence of Granovetter’s concept of “weak ties” (Granovetter, 1973). Betweenness is very closely linked to the notion of circulation: it counts the shortest paths to detect intermediate “bridges” or “key passages” capable of opening or locking certain parts of the network (Tayler and Neugebauer, 2018). Depending on applications, these are therefore both positions of power and vulnerable places.
                    
                    The 
                        closeness centrality allows to highlight the “geographical” middle of the graph. In networks of a certain density and when they are not divided into several distinct communities, the closeness is generally fairly evenly distributed and allows a good translation of the notions of “center” and “periphery”.
                    
                    For its part, the 
                        eigenvector centrality is quite complicated to translate since it works iteratively and is very much dependent on the structural context at short and medium range around a node. “Prestige” or “influence” centrality, named “power” centrality by its author (Bonacich, 1972), it qualifies a node’s environment while operating in cascade: a well connected node gives its neighbours a part of its authority capital, and so on. It is therefore particularly useful when trying to analyze the hierarchy of the nodes in a graph (Piper et al., 2017). The most well-known use of this measure is the backbone of the Google search engine: the PageRank algorithm (Brin and Page, 1998).
                    
                
            
            
                Towards mixed approaches
                This contribution proposes a table of correspondence between the concepts of graph theory and the practice of visual network analysis in the social science and humanities. This effort must not be understood as a demarcationist attempt at telling the right method from the wrong. The “dictionary” is not exhaustive and only aims at helping to bridge two worlds that have more in common that what meets the eye. By focusing on translating methods, we want to stress that crossing points are real even though they do not come without issues, and thus require our methodological attention. 
                We also note that the analysis should not be limited to a catalogue of well-known methods (basic centralities, etc.) but that approaches combining several of those should be encouraged to obtain an optimal and innovative “translation”. In this way, we could compare metrics (Escobar and Schauf, 2018) or combine them to establish rankings (Fischer et al., 2018; Grandjean, 2018: 328). Furthermore, the enrichment of the networks by means of categories that are not dependent on the structure, like the gender of individuals in a social network (Dunst and Hartel, 2017) or the discipline of projects in a scientometric analysis (Grandjean et al., 2017), allows to test translation and interpretation hypotheses by avoiding the blind approach of testing all possible graph metrics.
            
        
        
            
                
                    Bibliography
                    
                        Ables M. et al. (2017). Using Archival Texts to Create Network Graphs of Musicians in Early Modern Venice, 
                        Digital Humanities 2017, Montreal. 
                    
                    
                        Algee-Hewitt M. A. (2018). The Hidden Dictionary: Text Mining Eighteenth-Century Knowledge Networks, 
                        Digital Humanities 2018, Mexico City. 
                    
                    
                        Barnes J. A. (1954). Class and Committees in a norwegian Island Parish, 
                        Human Relations, 7, 39
                        ‑58.
                    
                    
                        Bastian M. et al. (2009). Gephi: an open source software for exploring and manipulating networks, 
                        International AAAI Conference on Weblogs and Social Media, 361-362.
                    
                    
                        Beaulieu M.-C. (2017). Perseids and Plokamos: Weaving pedagogy, data models and tools for social network annotation, 
                        Digital Humanities 2017, Montreal. 
                    
                    
                        Bonacich P. (1972). Factoring and weighting approaches to status scores and clique identification, 
                        The Journal of Mathematical Sociology, 2, 1, 113
                        ‑120.
                    
                    
                        Brin S. and Page L. (1998). The Anatomy of a Large-Scale Hypertextual Web Search Engine, 
                        Seventh International World-Wide Web Conference, Brisbane.
                    
                    
                        Choinski M. and Rybicki I. (2017). Networks of the Great Awakenings: Classification of Puritan Sermons by Word Usage Statistics, 
                        Digital Humanities 2017, Montreal.
                    
                    
                        Colavizza G. et al. (2016). A Method for Record Linkage with Sparse Historical Data, 
                        Digital Humanities 2016, Krakow.
                    
                    
                        Dunst A. and Hartel R. (2017). Corpora and Complex Networks as Cultural Critique: Investigating Race and Gender Bias in Graphic Narratives, 
                        Digital Humanities 2017, Montreal.
                    
                    
                        Escobar Varela M. and Schauf A. (2018). Network Analysis Shows Previously Unreported Features Of Javanese Traditional Theatre, 
                        Digital Humanities 2018, Mexico City.
                    
                    
                        Euler L. (1736). Solutio Problematis ad Geometriam Situs, 
                        Opera Omnia, 7, 128-140.
                    
                    
                        Evalyn L. and Gauch S. (2018). Analyzing Social Networks Of XML Plays: Exploring Shakespeare’s Genres, 
                        Digital Humanities 2018, Mexico City. 
                    
                    
                        Fischer F. et al. (2018). To Catch A Protagonist: Quantitative Dominance Relations In German-Language Drama (1730–1930), 
                        Digital Humanities 2018, Mexico City. 
                    
                    
                        Freeman L. C. (1979). Centrality in Social Networks: Conceptual Clarification, 
                        Social Networks, 1, 215
                        ‑239.
                    
                    
                        Früchterman T. M. and Reingold E. M. (1991). Graph drawing by force-directed placement, 
                        Software: Practice and Experience, 21, 1129-1164.
                    
                    
                        Gao J. et al. (2017). The Intellectual Structure of Digital Humanities: An Author Co-Citation Analysis, 
                        Digital Humanities 2017, Montreal.
                    
                    
                        Gao J. et al. (2018). Visualising The Digital Humanities Community: A Comparison Study Between Citation Network And Social Network, 
                        Digital Humanities 2018, Mexico City.
                    
                    
                        Grandjean M. (2015). Network Visualization: Mapping Shakespeare’s Tragedies, http://www.martingrandjean.ch/network-visualization-shakespeare.
                    
                    
                        Grandjean M. (2016). Archives Distant Reading: Mapping the Activity of the League of Nations’ Intellectual Cooperation, 
                        Digital Humanities 2016, Krakow.
                    
                    
                        Grandjean M. (2017). Multimode and Multilevel: Vertical Dimension in Historical and Literary Networks, 
                        Digital Humanities 2017, Montreal.
                    
                    
                        Grandjean M. et al. (2017). Complex Network Visualisation for the History of Interdisciplinarity: Mapping Research Funding in Switzerland, 
                        Digital Humanities 2017, Montreal.
                    
                    
                        Grandjean M. (2018). 
                        Les réseaux de la coopération intellectuelle. La Société des Nations comme actrice des relations scientifiques et culturelles dans l’entre-deux-guerres, Lausanne.
                    
                    
                        Granovetter M. S. (1973). The Strength of Weak Ties, 
                        American Journal of Sociology, 78, 1360
                        ‑1380.
                    
                    
                        Jänicke S. and Focht J. (2017). Untangling the Social Network of Musicians, 
                        Digital Humanities 2017, Montreal.
                    
                    
                        Jannidis F. et al. (2016). Comparison of Methods for the Identification of Main Characters in German Novels, 
                        Digital Humanities 2016, Krakow.
                    
                    
                        Kaufman M. et al. (2017). Visualizing Futures of Networks in Digital Humanities Research, 
                        Digital Humanities 2017, Montreal.
                    
                    
                        Marraccini M. (2017). The Victoria Press Circle, 
                        Digital Humanities 2017, Montreal.
                    
                    
                        Maryl M. and Eder M. (2017). Topic Patterns in an Academic Literary Journal: The Case Of Teksty Drugie, 
                        Digital Humanities 2017, Montreal.
                    
                    
                        Moreno J. L. (1934). 
                        Who Shall Survive? A New Approach to the Problem of Human Interrelations, Nervous and Mental Disease Publishing.
                    
                    
                        Moretti G. et al. (2016). Building Large Persons’ Networks to Explore Digital Corpora, 
                        Digital Humanities 2016, Krakow.
                    
                    
                        Noack, A. (2009). Modularity clustering is force-directed layout. 
                        Physical Review E, 79(2), 026102.
                    
                    
                        Pino-Díaz J. and Fiormonte D. (2018). La Geopólitica De Las Humanidades Digitales: Un Caso De Estudio De DH2017 Montreal, 
                        Digital Humanities 2018, Mexico City. 
                    
                    
                        Piper A. et al. (2017). Studying Literary Characters and Character Networks, 
                        Digital Humanities 2017, Montreal.
                    
                    
                        Rosa Pérez J. et al. (2018). Histonets, Turning Historical Maps Into Digital Networks, 
                        Digital Humanities 2018, Mexico City. 
                    
                    
                        Rybicki J. et al. (2018). Polysystem Theory And Macroanalysis. A Case Study Of Sienkiewicz In Italian, 
                        Digital Humanities 2018, Mexico City. 
                    
                    
                        Sozinova O. (2016). Complex Networks-Based Approach to Russian Rhyme History Description: Linguostatistics and Database, 
                        Digital Humanities 2016, Krakow.
                    
                    
                        Tang M.-C. and Chen K.-H. (2017). A cross-language comparison of co-word networks in Digital Library and Museum of Buddhist Studies, 
                        Digital Humanities 2017, Montreal.
                    
                    
                        Tayler F. and Neugebauer T. (2018). Complex Networks Of Desire: Mapping Community In Visual Arts Magazines Fireweed, Fuse, And Border/Lines, 
                        Digital Humanities 2018, Mexico City. 
                    
                    
                        Torres-Yepez L. and Zreik K. (2018). Estudio Exploratorio Sobre Los Territorios De La Biopirateria De Las Medicinas Tradicionales En Internet : El Caso De America Latina, 
                        Digital Humanities 2018, Mexico City. 
                    
                    
                        Trilcke P. et al. (2016). Theatre Plays as ‘Small Worlds’? Network Data on the History and Typology of German Drama, 1730–1930, 
                        Digital Humanities 2016, Krakow.
                    
                    
                        Verhoeven D. et al. (2018). Solving the Problem of the “Gender Offenders”: Using Criminal Network Analysis to Optimize Openness in Male Dominated Collaborative Networks, 
                        Digital Humanities 2018, Mexico City. 
                    
                    
                        Weingart S. B. (2011). Demystifying Networks, Parts I &amp; II, 
                        Journal of Digital Humanities, 1, 1. 
                    
                    
                        Wieneke, L. et al. (2016). Introducing HistoGraph 2: Exploration of Cultural Heritage Documents Based on Co-Occurrence Graphs, 
                        Digital Humanities 2016, Krakow.
                    
                    
                        Wright C. (2016). The Formation of Australia’s Economic History Community, 1950–1970: A Multidimensional Network Analysis, 
                        Digital Humanities 2016, Krakow.
                    
                    
                        Xanthos A. et al. (2016). Visualising the Dynamics of Character Networks, 
                        Digital Humanities 2016, Krakow.
                    
                
            
        
    

        
            Traditional Chinese Phonology, lacking of alphabetic system of phonetic notation such as IPA, had to deal with large written materials in Chinese characters, and used Chinese characters as a tool to analyze sounds of words. This brings up a significant feature of its study, that is, the relationships of words’ sounds are more important than their phonetic values.
            Xìli
                án (literally: "inter-link") is one of the most important methods in traditional Chinese phonology. Its fundamental is to build networks of Chinese characters having same syllabic elements. This paper takes Xìli
                án of Fǎnqiè in 
                Gu
                á
                ngyùn as an example to show how to use network analysis and visualization software to improve traditional Chinese phonology study.
            
            In general, Chinese characters are monosyllabic. A Chinese syllable can be divided into three parts: the Initial (Shēngmǔ), the Final (Yùnmǔ), the Tone (Shēngdi
                ào). The final can be further subdivided into the Medial (Jièyīn), the Main Vowel (yùnfù) and the Coda (yùnwěi), while the Medial and Coda are optional.
            
            
                
            
            Fǎnqiè is a Chinese method to indicate the pronunciation of a monosyllabic character by using two other characters. The first one, known as Fǎnqiè-Sh
                àngzì, has the same initial as the desired character, known as Bèiqièzì. And the second one, known as Fǎnqiè-Xi
                àzì has the same final and tone as Bèiqièzì. Here is an example. “端(du
                ān), 多(du
                ō)官(gu
                ān)切”. The Bèiqièzì is “端”. The Fǎnqiè-Sh
                àngzì is “多” indicating that the initial of “端” is “d”. The Fǎnqiè-Xi
                àzì is “宗” indicating that final of “冬” is “u
                ɑn” and the tone is “-”.
            
            Naturally, any Fǎnqiè was meaningful when it was created, but may not keep its perfection as time goes by, due to phonetic changes. For example, “東(dōng), 德(dé)紅(hóng)切”. “德” still has the same initial as “東”, while “紅” has the same final but different tones. Thus, the systematic use of such Fǎnqiè in ancient rime dictionaries is an invaluable resource for the work of historical linguists.
            
                Guǎngyùn is a Chinese rime dictionary compiled in 1008AD. It is a revision and expansion of 
                Qièyùn, the influential rime dictionary published in 601AD. It is generally accepted that 
                Qièyùn recorded the voice of Chinese at that time, maybe not as a spoken language, but rather how characters should be pronounced when reading the classics. So 
                Guǎngyùn, as the most accurate available account of 
                Qièyùn in the past, was used by traditional scholars as a major source on the reconstruction of 
                Qièyùn system, the code name of Middle Chinese.
            
            
                Guǎngyùn is split into four tones in five volumes (Two for Píng tone, and one each for other three: Shǎng, qù, rù). Each tone is split into rimes, with total of 206 rimes (including final and tone). Each rime is divided into groups of homophonous characters, with a character as the representation, and the pronunciation of each group given in 
                Fǎnqiè formula.
            
            It is Chénlǐ, in his masterpiece 
                Qièyùn-Kǎo published in 1842, who first introduced Xìli
                án method in the study of Fǎnqiè in 
                Guǎngyùn. The Principle of Fǎnqiè-Xìli
                án comes from the idea that the relation between Bèiqièzì and Fǎnqiè-sh
                àngzì or Fǎnqiè-xi
                àzì is an equivalence relation.
            
            In mathematics, an equivalence relation is a binary relation that is reflexive, symmetric and transitive. Any equivalence relation provides a partition of the underlying set into disjoint equivalence classes. Two elements of the given set are equivalent to each other if and only if they belong to the same equivalence class.
            That means, if we look for the Fǎnqiè-sh
                àngzì of each Fǎnqiè-sh
                àngzì and link them together with one another, we can obtain equivalence class of Fǎnqiè-sh
                àngzì representing a same initial. By systematically applying this method, it becomes possible to make classes of Fǎnqiè-sh
                àngzì for the initials, and Fǎnqiè-xi
                àzì for the rimes (including the final and tone) of 
                Guǎngyùn. When two classes were unable to link each other by any method, it may conclude that they represent distinct initials and distinct rimes.
            
            
                
                Figure 1: A page in 
                    Guǎngyùn
                
            
            In old days, Fǎnqiè-Xìli
                án of Guǎngyùn would cost a lot of time and the process is hard to display. So faced the same materials, researchers had to put lot of energy in repeating works to verify others’ results. It was often difficult to find out what went wrong when there were disagreements. Today, with the help of the network analysis and visualization software package like Gephi, it becomes much more easily to display one’s own work or review other’s work in both researching and teaching. Fǎnqiè-sh
                àngzì (or Fǎnqiè-xi
                àzì) are the nodes. The equivalence relation between them are undirected links. Then the components in the undirected network are equivalence classes of Fǎnqiè-sh
                àngzì (or Fǎnqiè-xi
                àzì) which represent different initials (or rimes).
            
            The process is simple and repeatable. The first step is to convert the text of 
                Guǎngyùn to a structural form, see Table 1. The second step is to find the no repeated Fǎnqiè-sh
                àngzì (or Fǎnqiè-xi
                àzì) appeared in Guǎngyùn. The third step is to find the Fǎnqiè-sh
                àngzì of each Fǎnqiè-sh
                àngzì (or Fǎnqiè-xi
                àzì of each Fǎnqiè-xi
                àzì) in step 2, see Table 2. The fourth step is to transform the equivalence relations between Bèiqièzì and Fǎnqiè-sh
                àngzì or Fǎnqiè-xi
                àzì into links between nodes representing these characters. The fifth step is to convert those links in step 4 to a network with a network analysis and visualization software package, like Gephi, see Figure 2. The sixth step is find all the components in the network, Figure 3 shows an example.
            
            However, it is not the end of our study of Middle Chinese Phonology, but only the beginning. Over more than 100 years, there is still no consensus on exactly how many initials and finals in Middle Chinese. The reasons are complicated, partly due to different versions of Guǎngyùn, partly due to various origins of Fǎnqiè in Guǎngyùn. Owing to digital method, it becomes much more convenient to demonstrate the works of key scholars and to locate the exact Fǎnqiè that caused their disagreements. 
            
                Table 1: The Structural Table of 
                    Guǎngyùn
                
                
                    Rhyme
                    Homophonous Group
                    
                        F
                        ǎ
                        nqiè-sh
                        à
                        ngzì  
                    
                    
                        F
                        ǎ
                        nqiè-xi
                        à
                        zì
                    
                
                
                    上平1東
                    
                        東菄鶇䍶
                        𠍀
                        倲
                        𩜍𢘐
                        涷蝀凍鯟
                        𢔅
                        崠埬
                        𧓕
                        䰤
                    
                    德
                    紅
                
                
                    上平1東
                    
                        同仝童僮銅桐峒硐
                        𦨴𧱁
                        筒瞳㼧
                        𤭁
                        罿犝筩潼曈洞侗橦烔䴀挏酮鮦㼿
                        𦏆𦍻
                        眮蕫穜衕
                        𩍅𢈉
                        䆚哃
                        𢏕
                        絧
                        𨝯𨚯𪔝𩦶𪒿
                    
                    徒
                    紅
                
                
                    上平1東
                    
                        中衷忠
                        𦬕
                    
                    陟
                    弓
                
                
                    上平1東
                    
                        蟲沖种盅爞
                        𦬕
                        翀
                    
                    直
                    弓
                
                
                    上平1東
                    
                        終衆(眾)潨
                        𣧩𧑄𩅧
                        䝦䶱䈺螽鼨蔠柊鴤泈
                    
                    職
                    戎
                
                
                    ……
                    ……
                    ……
                    ……
                
            
            
                Table 2: The Fǎnqiè-sh
                    àngzìof each Fǎnqiè-sh
                    àngzì in 
                    Guǎngyùn
                
                
                    ID
                    
                        F
                        ǎ
                        nqiè-sh
                        à
                        ngzì in 
                        Guǎngyùn
                    
                    
                        F
                        ǎ
                        nqiè-sh
                        à
                        ngzì of F
                        ǎ
                        nqiè-sh
                        à
                        ngzì
                    
                
                
                    1
                    德
                    多
                
                
                    2
                    徒
                    同
                
                
                    3
                    陟
                    竹
                
                
                    4
                    直
                    除
                
                
                    5
                    職
                    之
                
                
                    6
                    敕
                    恥
                
                
                    7
                    鋤
                    士
                
                
                    8
                    息
                    相
                
                
                    9
                    如1
                    人
                
                
                    ……
                    ……
                    ……
                
            
            
                
                Figure 2: The Network of Fǎnqiè-sh
                    àngzi
                
            
            
                
                Figure 3: A Component in the Network of Fǎnqiè-shàngzi
            
        
        
            
                
                    Bibliography
                    Chen Pengnian, etc. (Song Dynasty). 
                        S
                        ò
                        ngb
                        ě
                        n Gu
                        ǎ
                        ngy
                        ù
                        n &amp; Y
                        ǒ
                        ngl
                        ù
                        b
                        ě
                        n Y
                        ù
                        j
                        ì
                        ng. Nanjing: Jiangsu Education Press, 2005.
                    
                    Chen Li (Qing Dynasty). 
                        Qi
                        è
                        y
                        ù
                        n K
                        ǎ
                        o. Beijing: Beijing China Bookstore, 1984. 
                    
                    Huang Yiqing, Wang Ning, Cao Shujing.
                         General Introduction to Traditional Ancient Phonology. Beijing: Commercial Press, 2015.
                    
                
            
        
    

        
            
                Introduction
                The thirteenth century in the Czech lands is undoubtedly the most interesting period for the nobility. After the prince period (until 1198), the throne is surrounded by “magnate” families with precisely defined family relations, and the lower nobility rise in numbers. It is the last period when a staggering social rise is possible for a broader number of aristocrats and warriors. Over the century, the Estates are relatively precisely established (Novotný, 1937; Vaníček, 2002; Žemlička, 2002).
                When the economic and social gap between the lower and higher nobility widens, the political development becomes dynamic. After an almost invariable group of families around the monarchs has been established, the impossibility of political upheaval led to the uprising of part of the nobility and the civil war in 1248-1249.
                Using social network analysis (Wasserman and Faust, 1994) we attempt to describe polarization within the nobility, identify who joined the uprising in the ranks of Přemysl Otakar II (Ottokar II), and how it influenced their chances to be appointed to high-ranking positions within the kingdom.
                Whereas social network analysis has been qualitatively used by some medieval historians (e.g., Ruffini-Ronzani, 2016), current scholarship on the Czech civil war (e.g., Jan 2008) focuses on individuals. It hypothesises about cliques around Václav I and Ottokar II based only on the holders of offices during their reigns. Our analysis relies on more detailed data (co-occurrences in the charters) and more advanced method (centrality measures coupled with clustering).
            
            
                Data
                The data concerning relations between Václav I and Ottokar II and the Bohemian nobility were collected manually from the charters released between year 1198 and 1283. In total, we collected data on approximately 2300 noblemen from 568 charters. Identification of individuals was at times ambiguous – for example, Jan, the son of George, and Jan of Brno appearing within a few years, may be one, two or three men – leading to arbitrary choices.
                A cross-check with other sources was not possible: a) there are only few charters common to 
                    Regesta Imperii or 
                    Monumenta Germaniae Historica and Czech sources; b) in the narrative sources, very brief 
                    Annales of 1198-1278 and longer 
                    The Stories of Wenceslaus I, there are only five person mentioned, two of them unknown to the charters.
                
                At this time, we cannot follow social relations in its own sense, we can only determine the agnatic kinship (women in charters are exception), the kinship between individual generations can only be thought of to discover names typical of another genus.
                
                    
                    The diagonal provides the number of noblemen included in a network for a given period. The off-diagonal tiles show how many noblemen appear in two given periods
                
            
            
                Methods
                The primary concern of this paper lies within the relations between noblemen. From the charters, we extracted weighted networks of noblemen (as network nodes) and their co-occurrence in charters (as links; the weight of a link is proportional to the number of co-occurrences) in four periods 1240-47, 1248-49, 1250-1253, 1254-1257. The length of the first and last period was chosen so as to build networks of comparable sizes. We analysed only the largest connected components of such networks, in order to meaningfully define quantities such as shortest paths connecting any two people or their centrality indices (which are measures of nodes’ importance in the network or simply: their ranking).
                
                    
                    Changes in centrality of nodes in the network. 
                        Left: cluster of people thriving under Václav I, whose position declined under Ottokar II. 
                        Right: cluster of people who gained their position during rebellion; two example noblemen are indicated
                    
                
                We used node strength (sum of weights of its links) as a proxy for its centrality. Next, to each person appearing in the networks we attributed a vector of four values: their network strength in the four consecutive periods. These vectors were agglomeratively clustered with the use of so called “chessboard distance” into groups of noblemen whose centrality underwent similar changes due to the changes of reign.
                Analysis was performed in: Wolfram Mathematica 11.3; network visualisation in: Gephi 9.2.
            
            
                Results
                In Figure 1, we show that the networks of decision-makers in the consecutive periods overlap at most in one third, indicating considerable rotation of posts. Next, as shown in Figure 2, we automatically found two groups of people: benefitting or losing from the uprising. Using that information we extracted the names of noblemen hypothetically loyal to Václav I or opposing him. In Figure 3, we show one of the analysed networks, notably with future rebels in vicinity of Ottokar II, and some filial and brotherly kinships within the highest-ranking noblemen.
            
            
                Conclusions and outlook
                The results show that almost 800 years later we can still identify the people involved in coup d’état and how it influenced their power in an important period of Czech history. We aim at extending this study by incorporating other information from the charters, e.g., their geographical location, the posts held by the noblemen or family membership. Methodologically, we plan to explore other centrality measures as well as community detection to corroborate the results with different techniques, and use bootstrap approach by generating ensembles of random networks with realistic properties to further assess statistical significance of the results. In terms of historical sociology, an interesting task would be to compare characteristics of the above networks with other known – contemporary or historical – social networks, and obtain a complementary insight into the (power) relations in medieval societies.
                
                    
                    Network constructed from charters issued in 1240-1247. The size of nodes is proportional to their strength. The kings (
                        purple) and 10 noblemen with highest centrality are labelled. 
                        Red nodes correspond to the cluster that thrived during rebellion. 
                        Light green nodes correspond to the cluster of people thriving under Václav I. Insets A and B show Přemysl II’s and Václav I’s subnetworks, respectively
                    
                
            
        
        
            
                
                    Bibliography
                    Jan, L. (2008). Domácí šlechtická opozice a přemyslovští králové 13. věku. In: Rituál smíření. Konflikt a jeho řešení ve středověku, ed. Nodl, M. and Wihoda, M. Brno, pp. 85–100.
                    Novotný, V. (1937). České dějiny I.4. Rozmach české moci za Přemysla Otakara II. (1253–1278). Praha.
                    Ruffini-Ronzani, N. (2016). L’analyse de réseaux, un outil pour relire l’émergence des principautés territoriales? Premières réflexions sur le cas hennuyer (mil. XIe-début XIIe siècle). In: Retour Aux Sources. Quatrième Rencontre Du Groupe “Réseaux et Histoire”.
                    Vaníček, V. (2002). Velké dějiny zemí Korun české 3. Praha – Litomyšl.
                    Wasserman, S. and Faust, K. (1994). Social Network Analysis: Methods and Applications. Cambridge: Cambridge University Press.
                    Žemlička, J. (2002). Počátky Čech královských 1198–1253. Proměna státu a společnosti. Praha.
                
            
        
    

        
            
                Introduction
                Over the last decades, with technological advancements such as growing digitalization and the development of social media platforms, the act of reading has transformed into an interactive experience (Cordón-García et al., 2013; Merga, 2015), where the Internet plays a key role (Murray, 2018). Social reading platforms like Goodreads and Wattpad are online environments where millions of people from all over the world come to share their love for the written word. Members come together to discuss what they have read and what they judge as good or bad literature, they recommend books to one another, and even try their hand at writing fiction.
                While a growing number of studies have been dedicated to this phenomenon (Nakamura, 2013; Ramdarshan Bold, 2016), so far only a few have adopted computational methods (Faggiolani and Vivarelli, 2016; Thelwall and Kousha, 2017) and none has combined these methods with empirical approaches to the study of literature and its effects.
                As the online environment is very different from the literary field as we know it, showing new types of complex interactions, we need to explore social reading and writing from both social and content perspectives. Social questions that should be investigated include interaction among users, questions of power (sources of literary authority), the effects on literacy and on reading behaviors, the changing system of social values (and of aesthetic evaluation). Content questions include questions about style, about the distribution and originality of comments, about the affective, reflective or social nature of content.
                With this panel, we will showcase the potential of studying social reading through the combination of multiple and interrelated approaches: from purely statistical, data-driven, and stylometric analyses, through qualitative and quantitative surveys of key users and a theory-driven qualitative taxonomy of reading valuation, towards a combination of the empirical and the computational, supported by a sound theoretical/methodological awareness. The substantial variety of case studies in four languages (English, German, Italian, and Dutch) will reflect the diversity of social reading, which can and should be studied from multiple points of view as well as with an array of methodological tools.
            
            
                Visualizing Wattpad
                Federico Pianzola
                Simone Rebora
                With this abstract, we focus on 
                    
                        Wattpad
                    , where social reading takes the form of a “discussion in the margin” (Stein, 2010), i.e. where texts are commented upon by millions of users, paragraph by paragraph. A central goal of our study is to understand how the corpus of the most appreciated texts on 
                    Wattpad is different from the canonical corpus of Western literature. For a preliminary analysis, we focused on the categories of “Classics” and “Teen Fiction” (TF).
                
                
                    Analysis I. For a first understanding of readers’ engagement, we examined the progression of the number of comments over the 20 most-commented books for each genre. Figures 1 and 2 show that TF is more stable, whereas for many Classics the majority of comments is on the first chapters. Also the total numbers are very different: 2,569,405 comments for the first TF title and just 42,013 for 
                    Pride and Prejudice.
                
                
                    
                
                Figure 1. Progression of the number of comments over the 20 most-commented Classics books
                
                    
                
                Figure 2. Progression of the number of comments over the 20 most-commented TF books
                
                    Analysis II. For a deeper understanding of reader response, we adopted sentiment analysis and compared the “emotional arcs” (Reagan et al., 2016) of both paragraphs and comments. The analysis was performed using the 
                    
                        Syuzhet
                     package on 6 books per genre: all details and limitations of the approach are described in (Rebora and Pianzola, 2018). Figures 3 and 4 show a better attunement between paragraphs and comments in TF: the highest Pearson's correlation (0.807; p-value &lt; 2.2e-16) was reached by TF title #3.
                
                
                    
                
                Figure 3. Emotional arcs for paragraphs and comments of six Classics books
                
                    
                
                Figure 4. Emotional arcs for paragraphs and comments of six TF books
                
                    Analysis III. To explore the relations between users while reading the books, we adopted network analysis (Scott, 2017) based on a very simple rule: the more two users reply to each other’s comments, the stronger is their connection. The visualizations were realized through the 
                    
                        Gephi
                     platform. Figures 5 and 6 show the networks of the most commented Classics and TF titles. To make the samples comparable, the networks were reduced to the 1,000 strongest connections. As evident, Classics readers tend to group in a single cluster, while TF readers group in multiple sub-clusters. This phenomenon seems to suggest that reading classics enhances the formation of a more homogeneous community.
                
                
                    
                
                Figure 5. Network graph of the 1,000 strongest connections between 
                    Pride and Prejudice readers
                
                
                    
                
                Figure 6. Network graph of the 1,000 strongest connections between 
                    The Bad Boy’s Girl readers
                
                
                    Analysis IV. To visualize 
                    Wattpad’s user distribution geographically, we analyzed a sample of 300,000 user profiles. Out of these, only 34.3% provide locations, many of which are fictitious (e.g. “Hogwarts” and “Wonderland”). We converted these locations into standardized state names with the help of the 
                    
                        Google
                    
                     
                    
                        Maps Place Autocomplete
                     API. Notwithstanding some errors (e.g. “Hell” was located in Denmark), the analysis provided confirmation to the supposed relevance of states like India and Philippines (Miller, 2015), together with the USA (see Fig. 7).
                
                
                    
                
                Figure 7. Geographic map of Wattpad users
                All these analyses are to be considered as preliminary to a more extensive and detailed study, but they efficiently showcase the high potential of investigating the social reading phenomenon through visualizations. 
                    Wattpad data urge us to rethink concepts like "world literature" and its dynamics. Of course, we cannot generalize these findings for books circulating through traditional publishing channels, but we can gain interesting insights for a more in-depth critical reflection on publishing and reading in the 21st century.
                
            
            
                Sources of authority in online book reviews
                Peter Boot
                It is a commonplace to state that with the advent of Amazon, book blogs and readers’ communities the role of traditional authorities in the literary field has become less important (McDonald, 2007). Rather than following the lead of professional critics, teachers or professors, readers are said to take advice from fellow readers. In my contribution to this panel, I will look at the evidence that a corpus of online Dutch book reviews can provide for answering the question which persons or institutions are considered authoritative by online reviewers.
                 In a pilot investigation, we have looked at references to possible authorities from a number of domains: traditional critics, newspapers, prizes, television programs, the book trade (publishers, booksellers, libraries), authors, teachers, websites and private contacts.
                Reviews were downloaded from some prominent weblogs, some dedicated (mass) review sites, the online magazine 8weekly and, for contrast, the most prominent Dutch newspaper NRC (Boot, 2013). We used collections of search terms and regular expressions to search the downloaded reviews using AntConc. Irrelevant hits were removed manually. A subset of the remaining results was annotated and assigned a 
                    type (type of person or institution that is assigned authority), 
                    role (role of reference to authority in review, e.g. the authority is mentioned as supporting the reviewer’s view or as the one who advised the reviewer to read the book) and 
                    agreement (whether the reviewer accepts the view of the authority). In all 1518 references to some form of authority were annotated. Because of many practical limitations and ad-hoc decisions, the procedure won’t give us any firm numbers but it does give a clear indication of sources of authority that are recognised in the online domain.
                
                
                    
                
                Figure 1. Main sources of authority in online reviews
                The main findings are summarized in Figure 1. The main sources of authority in the online reviews are companies (publishers and bookshops) and literary prizes. Authors (of the reviewed book or others) are also important. Online critics (‘peers’) are not unimportant, but print critics are hardly mentioned. Personal contacts (family and friends) are more important than print critics. Frequently, people refer to a medium rather than to a critic by name, but here too, online media are more often mentioned than print media. Teachers are almost invisible.
                 Much can be said about this. For one thing, we counted all references equal but some will weigh heavier than others. There are also important differences between the online platforms. The collection we used did not include reviews from booksellers’ sites. For now, though, the most important conclusion is that no, traditional critics are not highly valued on online platforms; however, the beneficiaries are commercial companies rather than fellow users of online platforms.
            
            
                Classifying the style(s) of criticism. A computational analysis of Italian book reviews
                Massimo Salgaro
                Simone Rebora
                In this abstract, we use a corpus of book reviews (Salgaro and Rebora, 2018) to answer the following question: how do professional critics, journalists and passionate readers differ in the writing of reviews and what features can be used to identify them?
                The corpus is divided into three subsets: reviews published on social reading platforms (source: 
                    aNobii), in paper magazines (
                    Il Sole 24 Ore), and in scientific journals (
                    Between, 
                    Osservatorio critico della germanistica, and 
                    OBLIO). All sub-corpora have an approximate size of 650,000 tokens.
                
                First, we adopted stylometry to classify the texts. As demonstrated by (Eder, 2015), the first element to influence the quality of a stylometric classification is text length. Preliminary tests—ran with the 
                    Stylo R package (Eder et al., 2016) on 5,000-word-long text chunks—showed how Cosine Delta distance, based on just 50 MFW, was able to almost perfectly separate the three subgroups (see Fig. 1). Considered the high variance of text length in our corpus (mean = 259 words; SD = 363 words), we artificially generated a series of sub-corpora composed by text chunks of the same length (varying between 50 and 5,000 words) and we evaluated clustering quality through the adjusted Rand index in the 
                    PyDelta Python library. Figure 2 confirms how Cosine Delta distance with 2,000 MFW is the best performing classifier (Evert et al., 2017), but also 200 MFW (i.e., mainly function words) reach a similar—and, in some cases, even better—efficiency. As for text length, clustering quality is quite poor below 1,000 words, while a plateau is reached at about 3,000 words.
                
                
                    
                
                Figure 1. Network graph of the corpus (Cosine Delta, 50 MFW, ForceAtlas2 in 
                    Gephi). In green: 
                    aNobii; in violet: 
                    Sole 24 Ore; in red: 
                    Between, 
                    Osservatorio, and 
                    OBLIO.
                
                
                    
                
                Figure 2. Clustering quality per slice length and MFW used (distance: Cosine Delta)
                To improve the results for shorter chunks, we developed a framework for a machine learning classifier, by operationalizing a series of traditional definitions of literary criticism (e.g. Eco, 1979; Gardt, 1998; Rodler, 2004; Colussi, 2017). An extensive lexicon of literary criticism (Beck et al., 2007) was translated into Italian; selections of terms related to mental imagery and emotional aesthetic response were extracted from questionnaires and tools in empirical aesthetics (e.g. Knoop et al., 2016; Fialho et al., in press), translated into Italian, and expanded through the 
                    fastText Italian word-embedding model (Joulin et al., 2017). These resources—together with selected features in the 
                    LIWC Italian dictionary—were used to measure emotional and cognitive involvement with the reviewed text. The measurements were combined with the results of the stylometric analysis (Cosine Delta, 2,000 MFW) and used to train an SVM classifier.
                
                For a corpus composed by 500-word-long chunks (the length of this abstract), the sole stylometric analysis reached an attribution accuracy of 90.1%, while the SVM classifier scored 93.2%. A slight but promising improvement, if we consider the simplicity of the framework—that can and should be refined further.
                With this paper, we hope to have cast the groundwork for a research that might fruitfully combine computational methods and literary theory to study the “style of criticism” of professional and non-professional readers.
            
            
                Shared Reading. Digital Reading and Writing of Literature
                Gerhard Lauer
                Maria Kraxenberger
                In the digital age, the practice and habits of reading change fundamentally. Not few speak of the end of the book and of (deep) reading (e.g., Wolf 2007). However, and contrary to this popular statement, reading and writing literature has never been practiced as intensely as within digital societies (Lauer 2018, 2019) – a development that involves in particular young, adolescent readers.
                In preliminary studies, we categorized various reading and writing platforms and conducted a first study on Harry Potter-fanfiction, using a personality questionnaire (Paulus 2016). Results indicate a dominance of romance as well as of fantasy genres (see also Odag 2008) and that authors of fanfiction are usually between 14 and 20 years old. They show both introverted and extroverted personality traits and come from diverse educational backgrounds. Generally, they can be considered rather empathetic. In a similar vein, other reading research suggests that dealing with literary texts increases performance in Theory of Mind, and thus might the support the development of prosocial skills such as empathy (Kidd &amp; Castano 2013).
                Despite justified criticism on over-generalizations (e.g., van Kuijk et al. 2018), research on the emerging changes towards digital, joint reading of texts with presumably much lower literary demands and its potential implications is missing. This is the more surprising, since first explorative studies in schools indicate that digital formats can be used very well for education through writing and reading (Bertschi-Kaufmann &amp; Graber 2007, Wampfler 2016) and not only shed light on, but also foster a variety of effects. Nevertheless, it is still unclear how and with what consequences digitization affects the reading behavior of adolescents, including complex processes such as (higher) language acquisition, establishments of social peer-groups and their interactions, as well as their understanding and appreciation of literature and the written word.
                Our project focuses on the changing social processes that accompany the reading and writing of literature in the digital age. In doing so, we focus on the young readers and their distinct reading and writing socialization on digital reading (and writing) platforms such as Wattpad. Based on a social interactionist-reading model and the theoretical concept of “scaffolding” (see, for example, Bruner 1983, Nation 2018, Quasthoff 2011) the project integrates descriptive, qualitative and quantitative methods in a multi-methodological approach.
                We present a questionnaire that combines established scales from social psychology with items used in reading research to explore online and offline reading habits. Using new methods of field research enables a better understanding of how intensively young readers think and feel about reading and writing literature and what kind of values are used to talk about literature in social media. To arrive at a more comprehensive picture of reading in the digital age, we provide a general theoretical framework that helps to integrate the thus acquired data.
            
            
                Where’s your attention? An empirical assessment of Web 2.0 users’ literary values
                J. Berenike Herrmann
                Thomas Messerli
                Traditionally, reading fiction has been seen as a tool for honing crucial sense-making capacities, enabling an integrated sensual and intellectual personal development (
                    judicium sensitivum, Wolff, 1738; cf. Lauer, 2012). Web 2.0 users, on the other hand, are understood not so much as interested in the challenges and pleasures of literature-as-art, but in the easy gratification of popular genres, driven by an economy of attention (Franck, 2004). Users’ book reviews are held to show an affirmative bias, documenting the lack of a deep and discerning reading engagement (Ingold, 2014). Mirroring a stated trend towards a “digestible” documentary and authentic literature (Röhricht, 2016), lay reviewers are held to operate with a heavy content bias and to neglect formal criteria.
                
                However, many of these verdicts lack comprehensive empirical support, leaving questions such as the following ones open: What do web 2.0 readers actually do when judging literary value? What are the grounds for value judgements? Do readers apply aesthetic and ethical premises? Do they prioritize content over formal criteria? In our paper, we will scrutinize a corpus of lay literature reviews published on the German reading platform lovelybooks.de for the premises underlying users’ valuation statements.
                Using sentiment analysis and rule-based techniques as a bootstrap methodology for semi-automatic identification of valuation statements in our corpus (ca. 1 Mio reviews), our exploratory case study analyzes the lovelybooks.de categories ‘classics’ and ‘novels’ Combining quantitative and qualitative methods, we examine how users evaluate literary texts in terms of quality criteria such as subjectively perceived effects, but also more formal ones, including authorial style, literary character motivation and plot construction (motivation, narrative arc, suspense).
                In addition to semi-automatic coding and running KWICs of recurrent phrases and keywords, we analyze a number of reviews as integral cases. How do users develop their (subjective) assessments? How do assessments appear in terms of tone and habitus? How do users prioritize the different categories for their evaluation? The qualitative coding is aimed at fine-tuning a coding schema that adapts the axiological model by Heydebrand &amp; Winko (1996) for web 2.0 lay reviews.
                Our first results do indeed point to users’ heightened subjectivity and a tendency to plain statements, reporting subjectively perceived effects (
                    a bit too melancholic, I found it a pity) and marking taste (
                    I liked it well). Yet, we found considerable attention to formal aspects such as plot construction (
                    some of the events were not plausible to me) and character motivation (
                    the characters were well crafted), as well as critical (not only affirmative) judgements. What is more, our results indicate that many users do apply a plain and subjectively toned language, but handle a reflected taxonomy of value criteria to support their taste judgements, expressed by statements such as 
                    Noch schwerer wog bei der Sternevergabe … [The awarding of stars was even more influenced by…], 
                    Ein Punkt, der mir sehr wichtig ist [A point that is very important to me]. We thus suggest that instead of propelling a decline of intellectual scrutiny and differentiation, online reading platforms offer a potential for personal insight and development. Our study is one of the first few data-driven approaches to scrutinize the literary values underlying people’s engagement with literary texts in a participatory culture.
                
            
            
                Empirical Goodreads
                Simone Rebora
                Moniek Kuijpers
                Piroska Lendvai
                Our research aims to complement computational linguistics with methods used in empirical literary studies to investigate the experience of “getting lost in a book”. Using software from the field of natural language processing, we match the 18 statements from the Story World Absorption Scale (SWAS, cf. Kuijpers et al., 2014) – a questionnaire used to identify absorbing reading experiences – to reader reviews posted on the online platform
                     
                    
                        Goodreads
                    . The SWAS taps into four different aspects of absorbed reading, namely sustained concentration (Attention), vivid imagery of the story world (Mental Imagery), feelings of empathy or sympathy for the characters of a story (Emotional Engagement) and the sensation of having made a deictic shift from real world to story world (Transportation). The reader reviews on Goodreads more often than not include descriptions of people’s experiences with certain books and evaluations of their reading experiences, and therefore it could be argued that they fall somewhere between accounts of reader response and literary criticism. They certainly constitute a rare treasure trove of qualitative, user-generated data on reading and reading evaluation.
                
                The aims of our project are twofold: 1) validating the SWAS, and 2) enabling comparative analyses of absorption across different books, genres, and reader groups.
                We have performed a manual analysis on 180 Goodreads reviews of three contemporary blockbuster novels (representative of the Fantasy, the Romance, and the Thriller genre), confirming that, in many cases, SWAS statements and particular sentences in Goodreads reviews overlap substantially.
                     For example, 
                    
                        one reviewer
                     writes: “I’m so absorbed in the world Martin produced out of his wits” (a sentence that matches with SWAS statement A3: “I felt absorbed in the story”); while
                     
                    
                        another reviewer
                     expresses her identification with the main character: “I went through all the emotional ups and downs right along with her” (matching with EE4: “I felt how the main character was feeling”). A total of 130 matching sentences were identified (for details, see Fig. 1).
                
                
                    
                
                Figure 1. SWAS/Goodreads matches. 
                    A1-A5: Attention, 
                    T1-T5: Transportation, 
                    EE1-EE5: Emotional Engagement, 
                    MS1-MS3: Mental Imagery
                
                In order to extend the analysis to the entire Goodreads corpus, which collects about 80 million book reviews, we combine two technologies: textual entailment detection software, i.e., EOP (Magnini et al., 2014) and text reuse detection software, i.e., TRACER (Büchler et al., 2018). Preliminary experiments (Rebora et al., 2018) show that both tools need adaptation and training for this specific task, as our best “out of the box” recall score is 0.28, while training on the manually-annotated reviews increases recall to 0.49.
                Based on the 130 identified sentences plus the 18 SWAS statements, we defined a provisional “absorption lexicon” and expanded it through a word-embedding model (Mikolov et al., 2013) based on 2.5 million reviews on Goodreads (total tokens ~ 400 million). Figure 2 shows a synthetic representation of this lexicon.
                
                    
                
                Figure 2. Absorption lexicon (word dimensions symbolize their weights).
                The lexicon was used to identify, through a standard “bag-of-words” approach, the reviews that showed the highest levels of absorption. The results, while significant in themselves (see Figs. 3 and 4 for sample visualizations), are used as the starting point for extensive semi-automatic annotation work – with four annotators working in parallel for a total of 18 months, starting in December 2018. Our goal will be that of producing a ground truth corpus as training data for machine learning algorithms, towards a fully-automated matching of Goodreads reviews with the different aspects of absorbed reading.
                
                    
                
                Figure 3. Network graph based on the absorption scores for the four SWAS categories.
                
                    
                
                Figure 4. Zoom of Fig. 3.
            
        
        
            
                
                    Bibliography
                    
                        Beck, R., Kuester, H. and Kuester, M. (2007). 
                        Basislexikon anglistische Literaturwissenschaft. Paderborn: Fink.
                    
                    
                        Bertschi-Kaufmann, A. (2007). 
                        Lesekompetenz, Leseleistung, Leseförderung Grundlagen, Modelle und Materialien. Seelze-Velber: Klett, Kallmeyer
                    
                    
                        Boot, P. (2013). The desirability of a corpus of online book responses. Second Workshop on Computational Linguistics for Literature. Retrieved 2015-12-27 from http://www.aclweb.org/anthology-new/W/W13/W13-1405.pdf
                    
                    
                        Bruner, J. S. (1983). 
                        Child’s Talk: Learning to Use Language. Oxford: Oxford University Press.
                    
                    
                        Büchler, M., Franzini, G., Franzini, E. and Bulert, K. (2018). TRACER – a multilevel framework for historical text reuse detection, 
                        Journal of Data Mining and Digital Humanities – Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages [in press].
                    
                    
                        Colussi, D. (2017). 
                        Stili della critica novecentesca: Spitzer, Migliorini, Praz, Debenedetti, Sereni. Roma: Carocci.
                    
                    
                        Cordón-García, J.-A., Alonso-Arévalo, J., Gómez-Díaz, R. and Linder, D. (2013). 
                        Social Reading. Oxford: Chandos.
                    
                    
                        Eco, U. (1979). 
                        Lector in fabula: la cooperazione interpretativa nei testi narrativi. Milano: Bompiani.
                    
                    
                        Eder, M. (2013). Does size matter? Authorship attribution, small samples, big problem. 
                        Digital Scholarship in the Humanities, 
                        30(2): 167–82 doi:10.1093/llc/fqt066.
                    
                    
                        Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: A Package for Computational Text Analysis. 
                        The R Journal, 
                        8(1): 107–21.
                    
                    
                        Evert, S., Proisl, T., Jannidis, F., Reger, I., Pielström, S., Schöch, C. and Vitt, T. (2017). Understanding and explaining Delta measures for authorship attribution. 
                        Digital Scholarship in the Humanities, 
                        32(suppl_2): ii4–ii16, doi:10.1093/llc/fqx023.
                    
                    
                        Faggiolani, C. and Vivarelli, M. (eds). (2016). 
                        Le Reti Della Lettura : Tracce, Modelli, Pratiche Del Social Reading. Milano: Editrice Bibliografica.
                    
                    
                        Fialho, O., Hoeken, H. and Hakemulder, F. (in press). Literary Imagination and Changing Perceptions of Self and Others: an Explanatory Model of Transformative Reading.
                    
                    
                        Franck, G. (2004). 
                        Ökonomie der Aufmerksamkeit : ein Entwurf ([8. Aufl.]). München: C. Hanser.
                    
                    
                        Gardt, A. (1998). Die Fachsprache der Literaturwissenschaft im 20. Jahrhundert. In Hoffmann, L., Kalverkämper, H. and Wiegand, H. E. (eds), 
                        Fachsprachen. Berlin, New York: de Gruyter, pp. 1355–62.
                    
                    
                        Heydebrand, R. von and Winko, S. (1996). 
                        Einführung in die Wertung von Literatur : Systematik - Geschichte - Legitimation. Paderborn, Zürich [etc.]: Schöningh.
                    
                    
                        Ingold, F. P. (2014). Laienherrschaft – in Klagenfurt und anderswo. Volltext, 3. Retrieved from
                         
                        
                            https://www.lyriktext.de/ingold-essays/laienherrschaft-n-in-klagenfurt-und-anderswo/
                        
                    
                    
                        Joulin, A., Grave, E., Bojanowski, P. and Mikolov, T. (2017). Bag of Tricks for Efficient Text Classification. 
                        Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. Association for Computational Linguistics, pp. 427–431.
                    
                    
                        Kidd, D. C. and Castano, E. (2013). Reading literary fiction improves theory of mind. 
                        Science, 
                        342(6156): 377–380.
                    
                    
                        Knoop, C. A., Wagner, V., Jacobsen, T. and Menninghaus, W. (2016). Mapping the aesthetic space of literature ‘from below’. 
                        Poetics, 
                        56: 35–49 doi:10.1016/j.poetic.2016.02.001.
                    
                    
                        Kuijpers, M., Hakemulder, F., Tan, E.E. and Doicaru, M.M. (2014). Exploring absorbing reading experiences. Developing and validating a self-report scale to measure story world absorption. 
                        Scientific Study of Literature, 
                        4(1): 89–122.
                    
                    
                        Lauer, G. (2012). Die Sinne und die Einbildungskraft. Zu Johann Gebhard Ehrenreich Maaß' Versuch über die Einbildungskraft im Kontext der Frühgeschichte der Psychologie. In Décultot, E. and Lauer, G. (eds). 
                        Kunst und Empfindung. Zur Genealogie einer kunsttheoretischen Fragestellung in Deutschland und Frankreich im 18. Jahrhundert. Heidelberg: Winter, pp. 157-173
                    
                    
                        Lauer, G. (2018). Instagram-Poesie. Über das digitale Popmärchen Rupi Kaur. 
                        Neue Zürcher Zeitung, 1. Juni 2018, 
                        
                            https://www.nzz.ch/feuilleton/gefuehl-ist-alles-lyrik-iminternet-ld.1369814
                         (accessed 27-11-2018).
                    
                    
                        Lauer, G. (2019). 
                        Lesen im digitalen Zeitalter. Darmstadt [in press].
                    
                    
                        Magnini, B., Zanoli, R., Dagan, I., Eichler, K., Neumann, G., Noh, T. G. and Levy, O. (2014). The Excitement Open Platform for textual inferences. In 
                        Proceedings of ACL Demo Session. Baltimore: ACL, pp. 43–48.
                    
                    
                        McDonald, R. (2007). 
                        The death of the critic. London, New York: Continuum International Publishing Group.
                    
                    
                        Merga, M. K. (2015). Are Avid Adolescent Readers Social Networking About Books?. 
                        New Review of Children’s Literature and Librarianship, 
                        21(1): 1–16 doi:10.1080/13614541.2015.976073.
                    
                    
                        Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. and Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In 
                        Advances in neural information processing systems, pp. 3111-3119.
                    
                    
                        Miller, M. (2015). What Wattpad Brings to the Publishing Table. PUB800, 
                        
                            https://tkbr.publishing.sfu.ca/pub800/2015/12/what-wattpad-brings-to-the-table/
                         (accessed 27-09-2018).
                    
                    
                        Murray, S. (2018). 
                        The Digital Literary Sphere: Reading, Writing, and Selling Books in the Internet Era. Baltimore: Johns Hopkins University Press.
                    
                    
                        Nakamura, L. (2013). ‘Words with friends’: Socially networked reading on Goodreads. 
                        Pmla, 
                        128(1): 238–43 doi:10.1632/pmla.2013.128.1.238.
                    
                    
                        Nation, K. (2018). What Teachers Need to Know about Shared Reading. 
                        Times Education Supplement, 9 March 2018, 
                        
                            https://www.tes.com/news/what-teachers-need-know-about-shared-reading
                         (accessed 27-11-2018).
                    
                    
                        Odağ, Ö. (2008). Of men who read romance and women who read adventure-stories… An empirical reception study on the emotional engagement of men and women while reading narrative texts. In Auracher, J. and Peer, W. van (eds), 
                        New Beginnings in Literary Studies. Newcastle: Cambridge Scholars Press, pp. 308–29.
                    
                    
                        Paulus, C. (2016). Saarbrücker Persönlichkeits-Fragebogen (SPF). Based on the Interpersonal Reactivity Index (IRI), V6.2.
                    
                    
                        Quasthoff, U. M. (2011). Diskurs- und Textfähigkeiten. Kulturelle Ressourcen ihres Erwerbs. In Hoffmann, L., Leimbrink, K. and Quasthoff, U. M. (eds), 
                        Die Matrix der menschlichen Entwicklung. Berlin: De Gruyter, pp. 210–51.
                    
                    
                        Ramdarshan Bold, M. (2016). The return of the social author: Negotiating authority and influence on Wattpad. 
                        Convergence: The International Journal of Research into New Media Technologies doi:10.1177/1354856516654459. http://con.sagepub.com/cgi/doi/10.1177/1354856516654459.
                    
                    
                        Reagan, A. J., Mitchell, L., Kiley, D., Danforth, C. M. and Dodds, P. S. (2016). The emotional arcs of stories are dominated by six basic shapes. 
                        EPJ Data Science, 
                        5(1): 31.
                    
                    
                        Rebora, S., Lendvai, P. and Kuijpers M. (2018). Reader experience labeling automatized: Text similarity classification of user-generated book reviews. In 
                        EADH 2018 Book of Abstracts [in press]
                    
                    
                        Rebora, S. and Pianzola, F. (2018). A New Research Programme for Reading Research: Analysing Comments in the Margins on Wattpad. 
                        DigitCult - Scientific Journal on Digital Cultures, 
                        3(2): 19–36 doi:10.4399/97888255181532.
                    
                    
                        Rodler, L. (2004). 
                        I termini fondamentali della critica letteraria. Milano: B. Mondadori.
                    
                    
                        Röhricht, K. (2016). 
                        Wettlesen um den Ingeborg-Bachmann-Preis : Korpusanalyse der Anthologie “Klagenfurter Texte” (1977-2011). Innsbruck, Wien, Bozen: Studien Verlag.
                    
                    
                        Salgaro, M. and Rebora, S. (2018). Measuring the ‘Critical Distance’. A Corpus -Based Analysis of Italian Book Reviews. In Spampinato, D. (ed), 
                        AIUCD2018 - Book of Abstracts. pp. 161–63 doi:10.6092/unibo/amsacta/5997. http://amsacta.unibo.it/id/eprint/5997
                    
                    
                        Scott, J. (2017). 
                        Social Network Analysis. Los Angeles; London; New Delhi; Singapore; Washington, DC; Melbourne: Sage.
                    
                    
                        Stein, B. (2010). A Taxonomy of Social Reading: a proposal http://futureofthebook.org/social-reading/.
                    
                    
                        Thelwall, M. and Kousha, K. (2017). Goodreads: A social network site for book readers. 
                        Journal of the Association for Information Science and Technology, 
                        68(4): 972–83 doi:10.1002/asi.23733.
                    
                    
                        Van Kuijk, I., Verkoeijen, P., Dijkstra, K. and Zwaan, R. A. (2018). The Effect of Reading a Short Passage of Literary Fiction on Theory of Mind: A Replication of Kidd and Castano (2013). 
                        Collabra: Psychology, 
                        4(1): 7 doi:10.1525/collabra.117.
                    
                    
                        Wampfler, P. (2016). Schreiben in sozialen Netzwerken - vier Szenarien. In Knopf, J., Abraham, U. and Schneider Verlag Hohengehren GmbH (eds), 
                        Deutsch Didital Band 2 Praxis. Baltmannsweiler: Schneider Verlag Hohengehren, pp. 84–90.
                    
                    
                        Wolf, M. (2007). 
                        Proust and the Squid: The Story and Science of the Reading Brain. New York, N.Y: HarperCollins.
                    
                    
                        Wolff, C. (1738). 
                        Psychologia empirica, methodo scientifica pertractata, qua ea, quae de anima humana indubia experientiae fide constant, continentur et ad solidam universae Philosophiae practicae ad Theologice naturalis tractationem via sternitur. Autore Christiano Wolfio. Potentissimi Suecorum regis etc. Consiliareo Regiminis Mathematum ad Philsosophia Professore Primario in Academia Marburgensi etc. Editio nova priori emendatior. Francofurti; Lipsiae: In Officina Libraria Rengeriana.
                    
                
            
        
    

        
            
                
                    Introduction
                    
                        This paper fleshes out the relations between the conceptual trinity of modernity, civilization and Europe (MCE) using digital history techniques. The idea of Europe as it emerged during the early modern period and developed over the nineteenth and twentieth centuries is often said to coincide with both ‘civilization’ and ‘modernity’ (Murray-Miller, 2018; Eisenstadt 2001). In the latest contribution to this topic, Murray-Miller argues that ‘the intertwining of these concepts is so extensive that, historically, one has typically served as a metonym for the other’ (Murray-Miller, 2018: 418-422). In this research we elaborate on this conceptual entanglement and evaluate the semantic boundaries that are said to define the MCE trinity. Based on a computational analysis of four Dutch newspapers spanning the period 1840-1990 we conclude that, in contrast to what the literature claims, semantic relations among the MCE elements are hardly visible and when they do, they are far from a ‘trinity’. 
                    
                    
                        Histories of concepts such as civilization and modernity are often based empirically on a selection of semantically dense works written by a limited number of intellectuals. While we do not claim that newspapers fully represent the historical ‘Zeitgeist’ we do argue that they reflect a broader swath of periodically iterated public opinion, and thus help us understand the development of concepts in a broader segment of society. This research shows how the ‘streetlight effect’ in intellectual history can lead to what in effect is a eurocentric fallacy in the study of modernity and civilization.
                    
                    
                        By investigating conceptual interrelations we add to the emergent field of digital conceptual history. Earlier historians of concepts already recognized the value of tracing patterns in word use to understand conceptual change (Reichardt, 1985). Digital datasets and computational methods now enable more advanced enquiries into frequencies and distributions that reveal important aspects of conceptual change (Kenter et al., 2015; Recchia et al., 2016).
                    
                
                
                    Corpora
                    
                        We restrict ourselves to four Dutch-language newspapers issued between 1800 and 1990: Algemeen Handelsblad (AH, 1828-1970), Leeuwarder Courant (LC, 1800-1990), Nieuwe Rotterdamsche Courant (NRC, 1844-1869, 1909-1929, 1970-1990; AH and NRC merged in 1970 as NRC Handelsblad), and De Telegraaf (TEL, 1893-1990). These newspapers are fairly representative of the Dutch newspaper landscape and cover a large part of the nineteenth and twentieth centuries (Wijfjes, 2004). In this period, the form and content of the newspapers in questions changed significantly: the size and regularity increased, political affiliations became more explicit and commercialization contributed to a focus on local, regional and national matters (De Graaf, 2010). Although we find these changes not to ‘affect’ our concepts, we are aware that our selection consists of newspapers distributed nationally. Comparisons with regional newspapers, or newspapers more strongly attached to political ideologies would be fruitful for further research.
                    
                
                
                    Methodology
                    
                        Considering the variable and imprecise meaning and usage of ‘Europe’, we approach the ‘trinity’ from the perspective of modernity and civilization. We first aggregate bigrams that contain modern
                        *
                        , beschaafd
                        * 
                        (‘civilized’) and beschaving
                        * 
                        (‘civilization’) and subsequently analyze bigram-frequency, -productivity (number of different bigrams used) and -creativity (number of ‘new’ bigrams introduced) to identify change and stability in the word usage. We complement these methods with PMI-collocations. Subsequently, we look into our concept’s meanings and interrelationships by employing word embeddings (Mikolov et al., 2013). Since the introduction of this method, much work has been undertaken to employ word embeddings in the study of diachronic word evolution (Kutuzov et al., 2018; Tang, 2018)). We analyze the concepts in five periods (1840-1869, 1870-1899, 1900-1939, 1950-1969, 1970-1990), a periodization based on the trends observed in the bigram frequencies and the availability of newspaper data (Hamilton, Leskovec and Jurafsky, 2016). Because the corpus size increases considerably over time we use random sampling to obtain equally sized input data for our vector space models. After aligning the models, we extract the most similar terms that appear in all the models in a given period and rank them based on the average cosine distance. Entanglement is investigated using bigrams that can be seen as combinations of two concepts (i.e. ‘modern europa’ or ‘moderne beschaving’), collocations and a network-based approach to word embeddings. We extract the thirty words most similar those thirty words most similar words to the adjectives ‘modern’, ‘civilized’ and ‘european’ across five periods. Using the open-source visualization software Gephi we visualized the results, using the resulting 13,500 words as nodes in a network, and the relations between them as edges. The entanglement thus becomes visible through the degree of connectedness of the nodes, as well as the overall network density.
                    
                
                
                    Findings
                    We show how during the nineteenth century the concept of modernity experienced an interpretative shift from modernity as ‘the present’ to modernity as a stage in history. Bigrams show that the adjective ‘modern’ was also increasingly combined with abstract phenomena such as state, time and freedom. Similarly, the concept of civilization shifted from being associated with enlightenment and prosperity to a geographically locatable counterpart of barbarism. Elements of purity and superiority followed this localization of civilization in the decades surrounding 1900, only to be combined with notions of global values, science and the history of humankind in the second half of the twentieth century.
                    
                         While these conceptual developments hint at a tightly woven idea of a modern European civilization, this idea is hardly corroborated by our data. If we assume that to be modern meant to be European, and to be European meant to be civilized, we would to find overlap in semantically similar words. Semantically similar words to ‘modern’, ‘civilized’ and ‘european’, however, seldom overlap. Signs of an MCE trinity are only spotted in the period 1870-1899, when colonialism and a rapidly changing industrial society produced new ideas about a superior and civilized European modernity. Outside this period, conceptual connections are present, but seldom between all three components of the trinity. Even if the trinity is assumed to be present, it was far from an equally sized triangle: Europe only marginally features in network representations of semantically similar words (Figure 1). Instead, it appears as a relatively insignificant word. Civilization and modernity hardly figure as spatial categories in newspapers, and in so far as they do, the most important concept is not ‘Europe’ but ‘the West’ (Geulen, 2011; Mishkova and Trencsényi, 2017; Bavaj and Steber, 2011, Bonnet, 2004).
                    
                
                
                    Conclusion
                    
                        Instead of an extensive and constant entanglement of modernity, civilization and Europe our research shows intermittent and alternating connections. Given that these results differ from research based on elite discourse, this paper demonstrates the need for digital research into conceptual interrelationships. In the case of the MCE trinity, this leads to a misreading of ‘eurocentrism’. Digital methods counter this fallacy and show how conceptual configurations assume different forms in different sources and social groups.
                    
                
            
        
        
            
                
                    Bibliography
                    
                        Bavaj, R. and Steber, M. (eds). (2015). 
                        Germany and ‘The West’, the History of a Modern Concept. New York/Frankfurt: Bergbahn.
                    
                    
                        Bonnet, A. (2004). 
                        The Idea of the West, Culture, Politics and History. Houndmills: Palgrave Macmillan.
                    
                    
                        De Graaf, R. (2010). 
                        Journalistiek in Beweging. Veranderende berichtgeving in kranten en pamfletten (Groningen en 's-Hertogenbosch 1813-1899). Amsterdam: Bert Bakker.
                    
                    
                        Geulen, S. (2011). Plädoyer für eine Geschichte der Grundbegriffe des 20. Jahrhunderts 
                        Zeithistorische Forschungen/Studies in Contemporary History, 7(1): 79-97.
                    
                    
                        Hamilton, W. L., Leskovec, J., and Jurafsky, D. (2016).  Diachronic word embeddings reveal statistical laws of semantic change.  
                        arXiv:1605.09096v6 [cs.CL] http://arxiv.org/
                    
                    abs/1605.09096v6 (accessed 19 April 2019).
                    
                        Ifversen, J. (2011). About key concepts and how to study them. 
                        Contributions to the History of Concepts, 6(1): 65-88. 
                    
                    
                        Kenter, T., Wevers, M., Huijnen, P., and Rijke, M. de. (2015). Ad Hoc Monitoring of Vocabulary Shifts over Time. 
                        Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. 1191-2000.
                    
                    
                        Kutuzov, A., Øvrelid, L., Szymanski, T. and Velldal, E. (2018). Diachronic word embeddings and semantic shifts: a survey. 
                        arXiv:1301.3781v3 [cs.CL].
                    
                    
                        Mikolov, T., Chen, K., Corrado, G. and Dean, J. (2013). Efficient estimation of word representations in vector space.
                         arXiv:1301.3781v3 [cs.CL] http://arxiv.org/
                    
                    abs/1301.3781v3 (accessed 19 April 2019).
                    
                        Mishkova, D. and Trencsényi, B. (eds). (2017). 
                        European Regions and Boundaries, A Conceptual History. New York/Frankfurt: Bergbahn.
                    
                    
                        Murray-Miller, G. (2018) Civilization, Modernity and Europe: The Making and Unmaking of a Conceptual Unity. 
                        History, 103:418-433.
                    
                    
                        Recchia, G., Jones, E., Nulty, P., Regan, J. and de Bolla, P. (2016). Tracing Shifting Conceptual Vocabularies Through Time. 
                        European Knowledge Acquisition Workshop. 19-28.
                    
                    
                        Reichardt, R. (1985). Introduktion. In Reichardt, R. (ed). 
                        Handbuch politisch-sozialer Grundbegriffe in Frankreich 1680-1820. Frankfurt: Verlag.
                    
                    
                        Tang, X. (2018). A state-of-the-art of semantic change computation. 
                        Natural Language Engineering, 24(5)  1-28.
                    
                    
                        
                        Wijfjes, H. (2004).  
                        Journalistiek in Nederland 1850-2000. Beroep, cultuur en organisatie. Amsterdam: Boom.
                    
                
            
        
    

        
            
                1. Introduction
            
            
                This paper presents an attempt to help understand the complex relationships between writers, their texts and their translators. A text by a writer may be translated by a single translator, or rival translations of the same text may appear; or a writer’s one text may be translated by one translator, and another of his texts by another translator; all this creates a complex mesh of connections that begs to be analyzed as a social network. The fact that some translators limit themselves to just one source language and one target language – the usual situation – does not mean that exceptions do not exist: Polish-Jewish anarchist poet Joseph Bovshover translated Polish into English (Sienkiewicz) and English into Yiddish (Shakespeare); Polish translator Ireneusz Kania translates (directly!) from 20 different languages (including Sanksrit and Tibetan). One of the main aims of this reconnaissance was to establish the degree of clustering by language, i.e. whether single-source-language communities are also interconnected.
            
            
                2. Material and methods
            
            
                Data on writers and translators was obtained from UNESCO’s Index Translationum, a large database of existing translations of texts from numerous domains (Bosser 2000). The data for this study was limited to 
                literary
                translations into
                Polish
                , a total of almost 18,000 individual editions of novels or collections of short stories by 8290 authors and 6582 translators from 155 languages. Editions of the same translations by the same translators were treated as separate entities, since this allows to discover authors, texts, translators and translations of particular cultural significance (or merely popularity). The network, prepared in Gephi (Bastian et al. 2009), used the number of editions of texts of an author translated by a translator as weights. 
            
            
                3. Simple measures
            
            
                It is not surprising that a lion’s share of translations into Polish as recorded in the Index Translationum was from English-language literature: a total of 12190 editions, or 67.73%; this domination occurs in most cultures nowadays; in Poland, English has been the most-translated language since the 1930s (Krajewska 1972, 14). This is also visible in the most-translated (or, more precisely, most-published) authors: of the top 10, only Hedwig Courts-Mahler and Jules Verne wrote in a language other than English (respectively German and French); of the top 5, only Jack Higgins was male, preceded by Barbara Cartland (the top scorer with 75 published translations), Nora Roberts, Agatha Christie and Danielle Steel; the top 10 also included Lucy Maud Montgomery, Graham Masterton, André Norton and various Disney products (treated jointly here). Authors deemed as “classical”, “canonical” or “artistic” only began to appear at the bottom of the second ten: Philip K. Dick came as 19
                th
                , followed by Mark Twain, Jack London and Hans Christian Andersen; Dickens was 27
                th
                ; Tolkien, 41
                st
                ; Conrad, 46
                th
                ; Ursula K. Le Guin, 54
                th
                , beat Shakespeare, 56
                th
                . German and French literature came far behind English and close to each other, with, respectively, 6.91% and 6.77% of the entire database, followed by Russian (4.22%), Spanish (2.16%) and Italian (1.90%). Apparently, Poland’s most prolific and ubiquitous translator is one Małgorzata Fabianowska, the translator of 73 editions by 71 authors (Disney products and books based on films), followed by Jacek Manicki, with 66 translators of such authors as Stephen King, Robert Ludlum and John Grisham. The fact than none of Poland’s celebrity translators appeared in any top positions shows the extent to which quantity does not equal quality in literary translation.
            
            
                3. Social network analysis
            
            
                Figure 1 presents a three-dimensional network visualization of the data, produced with the Fruchterman-Reingold force-directed algorithm (1991) and edited with UCSF Chimera (Pettersen et al. 2004).
            
            
                
                Figure 1. Writer-translator network. Nodes are writers and translators; edges are translations.
            
            
                As is evident from the general description of the content of the database, the large community in the middle of the sphere shows a strongly-interconnected web of translations from the English. By comparison, other major literatures (German, French, Russian, Spanish and Italian) are much less represented. There is precious little connection between the source languages (despite the above-mentioned existence of multilingual translators). The debris at the external limits of the sphere are instances of one-time translators of texts by authors translated only once, and thus unconnected with the community of a given source language; instances of a single author translated just twice or thrice and/or by just one or two translators, etc. Figure 2 presents an example of this: 
            
            
                
                Figure 2. A small community of two French authors, Peyremaure and Reznikoff sharing a translator, Olędzka; Reznikoff was also translated by Wasitowa.
            
            
                Whenever larger communities are less interconnected, it is feasible to follow the connections within. Figure 3 does this for a subset of the French community. Jean-Paul Sartre is thus connected via his translators to such writers as Raymond Queneau and Jacques Prévert (both
                Satrapes
                of the Pataphysical Club), indicating that sharing translators might follow a chronological if not a generational or an ideological logic. But a somewhat more versatile translator, Jacek Trznadel, also links Sartre with Marquis de Sade. Numerous other interesting connections could be observed.
            
            
                
            
            
                4
                . Conclusions
            
            
                Social network analysis thus seems a good tool to examine relationships within a large database such as the Index Translationum, relationships which are not accessible through direct retrieval of its information. The examples of connections mentioned above obviously do not even begin to describe the extent to which visualizing these networks can be helpful to write, perhaps, an entirely new history of literary translation. Once similar network are drawn for other target languages (and perhaps for more reliable databases of translations than the Index Translationum, which, for all its size, has some issues of reliability and representativeness), mathematical social network values (such as homophily, density, or clustering coefficient) could be compared between the literary cultures of these languages to produce quite a new type of distant reading of the phenomenon of literary translation.
            
        
        
            
                
                    Bibliography
                    
                        Bastian, Mathieu, Sebastien Heymann and Mathieu Jacomy. 2009. “Gephi: an open source software for exploring and manipulating networks.” 
                        Proceedings of the International AAAI Conference on Weblogs and 
                        Social
                         Media
                        , San Jose, Ca.
                    
                    
                        Bosser, Sylvie. 2000. “The Index translationum on CD-ROM: an analytical tool.” In 
                        World culture report 2000: cultural diversity, conflict and pluralism
                        , 274-275.
                    
                    
                        Fruchterman, Thomas M. J. and Edward M. Reingold, Edward M. 1991. “Graph Drawing by Force-Directed Placement.”
                        Software – Practice &amp; Experience
                        21 (11): 1129-1164.
                    
                    
                        Hanneman, Robert A. and Mark Riddle. 2011. “Concepts and Measures for Basic Network Analysis.” 
                        The Sage Handbook of Social Network Analysis
                        . SAGE, 346–347.
                    
                    
                        Krajewska, Wanda. 1972. 
                        Recepcja literatury angielskiej w Polsce w okresie modernizmu (1887-1918) : informacje, sądy, przekłady
                        . Wrocław: Ossolineum.
                    
                    
                        Pettersen, Eric F., Thomas D. Goddard, Conrad C. Huang, Gregory S. Couch, Daniel M. Greenblatt, Elaine C. Meng and Thomas E. Ferrin. 2004. “UCSF Chimera – a visualization system for exploratory research and analysis.” Journal of Computational Chemistry 13: 1605-1612.
                    
                
            
        
    

        
            This paper is the first official report on a Marie Curie project entitled 
                Last Letters from the World Wars: Forming Italian Language, Identity and Memory in Texts of Conflict, which started in September 2018. This project deals with a linguistic and thematic analysis of the last letters of people sentenced to death during the First and the Second World Wars, conducted with digital humanities tools. In this very first part of the project, I am preparing the lexicon analysis that will be the focus of my methodology. I am also creating a geographical representation of the corpus because this project is intrinsically geographical in its approach. Indeed, mapping appears necessary considering the linguistic particularities of the Italian territory, which is characterised by a linguistic diversity on the local level. 
            
            In this part of my research activities, I am developing some tools that are specific to the analysis of the Second World War. Notably, I have already collected and georeferenced four datasets (heretofore named as DS) regarding the Italian Resistance against fascism and the Nazi occupation. I have reworked or created data on this topic thanks to my collaboration with the staff of several Italian institutions and archives:
            
                DS_1 - The network of all the massacres committed by Nazis and Fascists since the beginning of the Italian dictatorship. This data was originally collected in the framework of a previous project, 
                The Atlas of the massacres of Nazis and Fascists (http://www.straginazifasciste.it/), by the Central Institute of the Resistance in Milan. The original project led to the creation of a basic database, managed by a SQL system that communicates with Google's API. I added dates and a new style to this project.
            
            
                DS_2 - The political and racial arrests and deportations committed by Nazis and Fascists against civilians and the partisan army. This data is contained in the database of the Italian Documentation Centre of Jewish Culture, with which I have started a collaboration in order to create a mapping of their very rich material.
            
            
                DS_3 - The entire corpus of letters written by partisans, generally Italians, before their execution. This data is directly connected with the project Last Letters. The letters are georeferenced and analysed so that an historical and linguistic analysis could be applied.
            
            
                DS_4 - The movements of the Italian fighters who participated in the Spanish Civil War beside the Spanish Republic against Franco. This data is collected by the Ferruccio Parri Institute, which asked me to exploit it digitally.
            
            These four datasets present a huge amount of data (approximately between 20.000 and 30.000 elements already categorised and georeferenced) that perfectly display the repression acted by Nazis and Fascists on members of the Resistance and populations, but also the activities of Italian partisans in reaction to this oppression. Indeed, the letters were usually written in the areas where Fascists had crushed their opponents through military attacks. 
            This research has already appeared that a digital mapping approach is essential in dealing with this historical data. This method has never been applied before, and can highlight new connections and differences between Nazi and Fascist abuses. This map, which I have already built, 
                https://goo.gl/BZSo3L, shows how a digital visualisation of the massacres can help to understand in what parts of the country repression was led by Germans or Italians. These peculiarities highlight the dualism of the Italian Resistance, which can be considered both as a phenomenon of resistance and a civil war. This is a very debated problem in historiography, which is linked with ideological positions. Indeed, admitting that Italy was involved in a civil war means that partisans were soldiers of a regular army, as opposed to a group of patriots who fought against foreign invasion. If partisans were construed as a regular army, their war against the Italian Social Republic could be considered a war against other Italians and not against Fascism, which was defeated on 8 September 1943 with the armistice signed between the Italian army and the Allies.
            
            The data I have collected so far will be presented using a combination of digital tools such as Gephi, Carto and Google Earth. I am particularly interested in the construction of networks in order to understand what categories of people were mainly repressed. I have georeferenced the data with R and Openrefine and, after correcting coordinates by checking on the maps the potential errors every ten records, I have built datasets divided into several categories regarding politics, social background and gender. Through the use of network analysis combined with spatial analysis, I would like to shed light on the differences between Nazi and Fascist repression, insofar as different categories of people were targeted by one or the other. 
            The idea is to compose the very first atlas of Fascist and Nazi repression in Italy using only open source tools. The mapping is thought and coded mainly with the use of R, without the use of Shiny, but also in JavaScript for some special layers, especially the chronological ones. 
            The meaning of this work is to shed a new light on this period considering that, even in the archives, the actions of Nazis and Fascists were often covered for political reasons after the war. For instance, at the end of the Second World War, politicians probably understood that it was impossible to punish everyone for their crimes and therefore, they often had files disappear from the dossiers. The case of the “cupboard of shame” is a very famous example of these tendencies: in 1994, a wooden cabinet was discovered inside a large storage room in the Palazzo Cesi-Gaddi, in Rome which, at the time, housed the chancellery of the military attorney's office. The cabinet contained an archive of 695 files documenting war crimes perpetrated on Italian soil under Fascist rule and during Nazi occupation after the armistice on September 8, 1943, between Italy and the Allied armed forces. The cupboard’s doors were locked and facing the wall when they were discovered.
            As this case shows, Italy still has trouble dealing with its past. The Fascist regime represents an uneasy heritage for today’s political parties and citizens. Mapping the Fascist repression of that time is a way to display the historical responsibilities of Fascism through the violence expressed against racial and political categories, and the use of a digital map enables us to visualize the phenomenon under a new light. Connecting those historical events with the geographical places where they happened facilitates the emergence of patterns in the repression of populations in the course of the most tragic event of twentieth-century Italian history.
        
        
            
                
                    Bibliography
                    
                        Roberto Battaglia and Giuseppe Garritano, 
                        Breve storia della Resistenza in Italia, 7. ed, Biblioteca di storia 20. secolo (Roma: Editori Riuniti, 2007).
                    
                    
                        Gianluca Fulvetti and Paolo Pezzino, eds., 
                        Zone Di Guerra, Geografie Di Sangue: L’Atlante Delle Stragi Naziste e Fasciste in Italia: (1943-1945) (Bologna: Società editrice Il mulino, 2016).
                    
                    
                        Franco Moretti, 
                        Graphs, Maps, Trees: Abstract Models for a Literary History (London ; New York: Verso, 2005).
                    
                    Philip Cooke, 
                        The Legacy of the Italian Resistance (New York: Palgrave Macmillan US : Imprint : Palgrave Macmillan, 2011).
                    
                    
                        Emanuele Sica and Richard Carrier, eds., 
                        Italy and the Second World War: Alternative Perspectives, History of Warfare 121 (Leiden ; Boston: Brill, 2018).
                    
                    
                        Ishay Landa and Michal Aharony, ‘Fascism, Extremism, and Extermination’, 
                        The Journal of Holocaust Research 33, no. 1 (January 2019): 1–3.
                    
                    
                        Patrick Bernhard, ‘Blueprints of Totalitarianism: How Racist Policies in Fascist Italy Inspired and Informed Nazi Germany’, 
                        Fascism 6, no. 2 (8 December 2017): 127–62.
                    
                    
                        Richard Bessel, ed., 
                        Fascist Italy and Nazi Germany: Comparisons and Contrasts (Cambridge [England] ; New York, NY, USA: Cambridge University Press, 1996).
                    
                    
                        Ruth Ben-Ghiat, 
                        Fascist Modernities: Italy, 1922-1945, Studies on the History of Society and Culture 42 (Berkeley, Calif London: University of California Press, 2004).
                    
                
            
        
    

      
         
            Netzwerke und Historische Epistemologie
            Methoden der Netzwerkanalyse kommen immer stärker als heuristisches Tool zum
          Einsatz, wenn es darum geht historische Prozesse zu beschreiben und zu
          analysieren. Netzwerktheorie stellt hierbei eine Möglichkeit dar, um
          systematisch Wissensstrukturen und die Formation und Transformation von
          Wissenssystemen zu beschreiben. Methodische Ansätze können hierbei von den
          Sozialwissenschaften, der Innovationsforschung sowie aus der Informatik und der
          Graphentheorie übernommen werden. Zunächst machten die meisten Ansätze in der
          historischen Forschung hauptsächlich von den qualitativen Konzepten gebrauch und
          weniger von den quantitativen Methoden, die die Netzwerktheorie liefert. Ein
          eindrucksvolles Beispiel zeigt sich im Werk von Irad Malkin in seinen Arbeiten
          über die griechische Antike (Malkin 2011). Mit der verbesserten Verfügbarkeit
          von Tools für die qualitative Analyse verändert sich diese Situation nun
          deutlich. Die Anzahl der Fallstudien hat mittlerweile die Größe erreicht, dass
          erste Theoretisierungen dieser Methode in der historischen Forschung unternommen
          wurden (van den Heuvel 2015). Große Impulse gehen von der sich um http://historicalnetworkresearch.org organisierenden Gruppe aus.
          Netzwerktheorie hat insbesondere ein großes Potential in der
          Wissenschaftsgeschichte insbesondere der historischen Epistemologie, die
          Entwicklung von Wissen als eine Verknüpfung von sozialen, materiellen und
          kognitiven Wissenssystemen sieht. Diese Systeme lassen sich als Netzwerke im
          Sinne der Netzwerktheorie verstehen. Dazu schlagen wir drei miteinander
          interagierende Netzwerke vor: ein soziales Netzwerk, ein semiotisches Netzwerk,
          sowie das darüber liegende epistemische Netzwerk: grob gesprochen ein Netzwerk
          von Akteuren, ein Netzwerk der Repräsentation von Wissen, das sich in
          materiellen Objekten oder auch kodifizierten Verfahren darstellt und schließlich
          das eigentliche kognitive Netzwerk. Wir werden in unserem Beitrag die ersten
          Ergebnisse zweier Fallstudien vorstellen. Dabei liegt unser Schwerpunkt weniger
          auf den konkreten Ergebnissen, sondern darauf zu zeigen, wie sich der Prozess
          der Übersetzung von historischen Fakten und Annahmen in netzwerktheoretisch
          auswertbare Daten darstellt und damit die Digital Humanities dazu beitragen,
          interdisziplinäre geisteswissenschaftliche Forschung zu ermöglichen. 
         
         
            Fallstudien
            In der ersten Fallstudie gehen wir der Frage nach, wie sich eine bestimmte
            Wissenstradition in der frühen Neuzeit mittels gedruckter Traktate in der Zeit
            von 1472 bis in das Jahr 1650 in ganz Europa verbreiten konnte. Es geht hierbei
            um die mit der Sphaera des Sacrobosco (Thorndike 1949),
            einem ursprünglich handgeschriebenen grundlegenden Text, verbundenen
            Wissenstradition. Ursprünglich ein Text über Astronomie und Kosmologie wurde
            dieser im Laufe seiner Editionsgeschichte immer wieder durch zusätzliche Texte
            erweitert und umfänglich kommentiert. Es gehörte zum verbindlichen Wissenskanon
            der Universitäten dieser Periode. In der Zeit von 1472 bis 1650 haben wir bisher
            363 Editionen identifiziert, die in ganz Europa veröffentlicht wurden. Welches
            sind die Voraussetzungen, die eine solche Verbreitung ermöglichten und welche
            Wissensinhalte wurden verbreitet und wie veränderten sich diese über die Zeit?
            Wir sehen in der Netzwerktheorie einen vielversprechenden Ansatz diese Fragen zu
            klären. Dazu haben wir ein erstes Netzwerk von wesentlichen Akteuren, den
            Verlegern, identifiziert, die maßgeblich für die Verbreitung des Traktats waren.
            Von welcher Form die Interaktionen zwischen den Verlegern waren, können wir
            bisher nur in sehr begrenztem Rahmen sicher bestimmen. Gleiches gilt für Ihre
            Rolle in den lokalen Wissensnetzwerken, in die sie eingebunden sind. Es ist
            jedoch möglich, Hypothesen über die Einflussbereiche aufzustellen, die die
            Kanten in dem Netzwerk rechtfertigen. Diese Hyptohesen lassen sich mit
            Netzwerktools in unserem Falle durch den Einsatz von Skripten in iPython (cf.
            Pérez / Granger 2007) unter Benutzung der Pakete networkx (cf. NetworkX
            developer team 2014) und graph-tool (cf. Peixoto) testen und modifizieren, in
            unserem Beitrag werden wir die von uns angewandten Verfahren darstellen. Die
            Skripte werden in Zukunft auf der Webseite des Projektes veröffentlicht werden.
            Neben den Verlegern gibt es ein weiteres Netzwerk, das eigentliche Netzwerk der
            Publikationen besser der Publikationsereignisse, diese stellen im Rahmen der
            angerissenen Theorie der Wissenssysteme eine andere Kategorie dar, sie sind
            materieller Ausdruck von Wissen, gehören damit zum semantischen Wissenssystem.
            Auch hier stellt sich die Frage, wie diese Editionen miteinander in Beziehung
            stehen. Offensichtlich stehen diese Beziehungen in enger Verbindung mit dem
            Akteursnetz. Jedoch gibt es auch andere Beziehungen, wie zum Beispiel die in den
            Editionen enthaltenen zusätzlichen Traktate und Kommentare, oder Abweichungen
            von Vorgängern. Auch hier sind wir bisher nur in der Lage Hypothesen
            anzustellen, aber auch hier zeigen uns Methoden der Netzwerktheorie bereits
            jetzt Wege auf, wie diese sich überprüfen lassen. Schließlich ist die
            entscheidende Frage: Welches Wissen wird durch die Traktate in welcher Form
            vermittelt und wie trägt dieses zur Wissensorganisation in der frühen Neuzeit
            bei? Wie oben beschrieben war das Traktat über die Sphaera ursprünglich ein
            Traktat über Astronomie und Kosmologie, die Ergänzungen umfassen jedoch einen
            wesentlich erweiterten Wissensbereich. Die Themenfelder, um die das Traktat
            erweitert wurden, umfassen Felder der mathematischen Astronomie,
            Kalenderberechnungen, die Benutzung und in einigen Bereichen konkrete Anleitung
            für die Konstruktion von astronomischen Instrumenten, nautische Astronomie und
            Geographie, Kartographie, Meteorologie, Arithmetik, Geometrie, der Konstruktion
            und des Gebrauchs von mathematischen Instrumenten zur Ausführung arithmetischer
            Berechnungen, Astrologie, Literatur, angewandte Optik und Mechanik. Die Antwort
            auf die Frage, welche Inhalte an welchen Stellen in die einzelnen Editionen
            eingeflossen sind, gibt Aufschlüsse über die Veränderung der frühneuzeitlichen
            Wissensstruktur. Auch hier kann Netzwerktheorie nach unserer Überzeugung einen
            wesentlichen Beitrag dazu leisten, schlüssige Antworten aufzufinden. Wir
            untersuchen dazu welche Ko-Autoren in welchen Traktaten genannt werden,
            insbesondere Clavius und Pedro Nuñes, und wie sich diese Subnetze ausprägen.
            Diese Koautoren stehen jeweils für bestimme Wissensgebiete geben also ein Indiz
            dafür welche Wissensbereich sich neu etablierten. Auch für diese Analysen haben
            wir Skripte entwickelt, die helfen diese Netzwerke zu visualisieren und deren
            Charakteristika zu untersuchen. Erste Ergebnisse der Netzwerkanalyse lassen
            Rückschlüsse auf die Stabilität des Wissensnetzwerkes zu. Wir können im
            wesentlichen drei Phasen deutlich identifizieren: eine noch instabile radiale
            Verbreitung, eine relative lange Phase der Stabilität und schließlich eine Phase
            der Reduktion. 
            
               
               
                  Abb. 1: Minimaler verbundener Graph der Editionen der
            Sphaera. Kanten spiegeln hier die chronologische Ordnung wieder. Dieser Graph
            legt die Randbedingungen für die weiter Analyse der Einflussbereiche der
            Editionen fest. Blaue Punkte stellen die lateinischen Editionen, rote die
            Editionen in lokalen Sprachen dar.
            
            
               
               
                  Abb. 2: The graph shows the role of editions where Clavius
            is given as one co-author (squares). The color indicates the publishers which
            are clustered by city. Only the places where Clavius is a co-author have labels.
            The big unnamed cluster in the middle is Paris.
            
            Die zweite Fallstudie widmet sich einem anderen Bereich. Dem epistemischen
              Netzwerk der Allgemeinen Relativitätstheorie. Kurz umrissen geht es hierbei um
              die Analyse des Prozesses, der dazu führte, dass die ART in der Nachkriegsphase
              von einem Randproblem zu einem zentralen Ankerpunkt der theoretischen Physik
              wurde. Dieser Prozess wurde auch als "Renaissance der allgemeinen Relativität"
              (Will 1983) bezeichnet. Aus einem zersplitterten lose gebundenen Netzwerk von
              Einzelpersonen wird in dieser Zeit ein stabiles Netzwerk, dass von Institutionen
              getragen wird. Darauf aufbauend entwickelt sich ein Netzwerk materieller
              Kommunikation in Form von neuen Journalen und Konferenzreihen, sowie eine neue
              disziplinäre Sprache, die das Wissen über die ART kodifizierte. Die ART
              entwickelte sich so zur unangefochtenen Theorie von Raum und Zeit (Blum 2015).
              Zum jetzigen Zeitpunkt haben wir ein Netzwerk aus über 500 Akteuren
              identifiziert und ihre Beziehungen klassifiziert. Mit Methoden der
              Netzwerkanalyse können wir zeigen, welche Akteure über die Zeit an Bedeutung
              gewonnen und verloren und die Formation von Subclustern zeigen. Auch hier werden
              wir in unserem Beitrag im wesentlichen darauf eingehen, wie der konkrete Prozess
              der Umwandlung von historischen Fakten und Hypothesen in netzwerktheoretische
              Konzepte als Anwendung der DH in der historischen Forschung vollzogen wird.
              Insbesondere werden wir auf die Überlegungen und Schwierigkeiten eingehen,
              sinnvolle Verknüpfungen zwischen den Akteuren zu definieren und insbesondere zu
              Gewichten, d. h. in der Sprache der Netzwerktheorie, weak and strong ties zu
              idenfizieren. Von besonderer Bedeutung ist auch hier die Frage des Umgangs mit
              der dynamischen Entwicklung des Netzwerkes. Wie können diese so visualisiert
              werden, dass der historisch Forschende daraus Rückschlüsse ziehen kann, wie
              können diese Tools so zur Verfügung gestellt werden, dass sie auch direkt durch
              den Forscher nutzbar sind? In unserem Fall haben wir dazu unterschiedliche
              Ansätze verfolgt, erneut die Umsetzung mittels iPython, und die Visualisierung
              dann in Gephi und Cytoscape.
            
               
               
                  Abb. 3: Visualization of the network of collaborations of
              scientists who worked on general relativity in the post-war period. Node size is
              proportional to the degree centrality. The color of the nodes is a
              representation of betweenness centrality (from lighter to darker). 
            
         
      
      
         
            
               Bibliographie
               
                  Drucker, Johanna (2013): "Performative Materiality
                and Theoretical Approaches to Interface", in: Digital
                Humanities Quartely 7,1 http://www.Digitalhumanities.org/dhq/vol/7/1/000143/000143.html
                [letzter Zugriff 03. September 2015]. 
               
                  Edwards, Charlie (2012): "The Digital Humanities and
                Its Users", in: Gold, Matthew K. (ed.): Debates in the
                Humanities
                  http://dhdebates.gc.cuny.edu/debates/text/31 [letzter Zugriff 03.
                  September 2015]. 
               
                  Kirschenbaum, Matthew (2008): Mechanisms. New Media and the Forensic Imagination. Cambridge: MIT
                  University Press.
               
                  Lauer, Gerhard (2013): "Die digitale Vermessung der
                  Kultur. Geisteswissenschaften als Digital Humanities", in: Geiselberger,
                  Heinrich / Moorstedt, Tobias (eds.): Big Data. Das
                  neue Versprechen der Allwissenheit. Berlin: Suhrkamp. 
               
                  Manovich, Lev (2001): Language of
                  New Media. Cambridge: MIT Press.
               
                  Warwick, Claire (2012): "Studying users in digital
                  humanities, in: Warwick, Claire / Terras, Melissa / Nyhan, Julianne (eds.):
                  Digital Humanities in Practice. London: Facet
                  Publishing. 
            
         
      
   



      
         Die zunehmende Mengen an Volltexten in den Geschichtswissenschaften und vor allem
        auch in der Mediävistik bietet neue Chancen für die Forschung, erfordern aber auch
        neue Methoden und Sichtweisen. Der Beitrag möchte die Verwendung von
        Graphdatenbanken für die Speicherung von Erschließungsinformationen vorstellen. 
            Momentan werden digitale Quellen und die mit ihnen verbundenen
        Erschließungsinformationen meist in XML oder in SQL-Datenbanken abgelegt. XML hat
        sich als Standard bewährt und findet in vielen Editionsprojekten als Datenformat
        Verwendung während Datenbanken auf Websites meist auf SQL-Datenbanken als
        Daten-Repositories zurückgreifen. XML-Dateien sind in der Regel bis zu einem
        gewissen Grade noch verständlich lesbar, bei SQL-Datenbanken ist die Lesbarkeit
        ohne Kenntnis der zu Grunde liegenden Datenstrukturen in der Regel nicht mehr
        gegeben. Dies liegt nicht zuletzt auch an den Architekturen der Datenbanken: um
        optimale Performance zu erhalten werden die Datenstrukturen normalisiert. Hier kommt
        es für die optimalen Nutzungsmöglichkeiten entscheidend auf die Gestaltung des
        Frontends der Datenbank an. Oft sind die User-Interfaces jedoch vor allem auf die
        Bedürfnisse jener Personen ausgerichtet, die die Datenbank selbst erstellt haben. Da
        diese Personen in der Regel die Datenstrukturen tief durchdrungen haben, kann es bei
        der Gestaltung des Frontends leicht zu einseitigen Ausrichtung auf Experten-Nutzer
        kommen. Solche Nutzer wissen bereits vor der Suchanfrage wie ihr Ergebnis aussieht.
        In den Fachwissenschaften wird eine solche Anfrage als CIN-Anfrage bezeichnet
        (concrete information need). Davon zu unterscheiden sind POIN-Anfragen
        (problem-oriented information need), bei denen der Nutzer ohne tiefere Kenntnisse
        des Datenmaterials und den zu Grunde liegenden Strukturen eine Anfrage startet (Vgl.
        hierzu Frants / Shapiro / Voiskunskii: 1997). Die Ausrichtung auf CIN-Anfragen zeigt
        sich auch in den größeren Quellenportalen zur Mediävistik (Vgl. Kuczera 2014). Hier
        ist die Verwendung von Graphdatenbanken ein alternativer Ansatz für die Speicherung
        von erschließendem Wissen. 
            In SQL-Datenbanken sind die Informationen in Tabellen abgelegt, die
        untereinander verknüpft sind. Graphdatenbanken folgen hier einem völlig anderen
        Ansatz. In einem Graph gibt es Knoten und Kanten. Vergleicht man die Knoten mit
        einem Eintrag in einer Tabelle einer SQL-Datenbank, wäre eine Kante eine
        Verknüfung zwischen zwei Tabelleneinträgen. Im Unterschied zu SQL- Datenbanken
        können Knoten und Kanten jeweils Eigenschaften haben. 
         
            
            
               Abb. 1: Direkte Verwandschaftsverhältnisse Karls des
          Großen als Graph visualisiert
         
         Daneben lassen sich die in Graphdatenbanken abgelegten Informationen sehr gut
          visualisieren. Gerade komplexere Datenbestände können hier sinnvoll für den
          Wissenschaftler erschlossen werden. Explorative Erschließungsmöglichkeiten
          erleichtern hierbei den Zugriff auf weitergehende Wissensdomänen des Repositoriums
          (Vgl. Kuczera 2015).
         Das Datenmodell einer Graphdatenbank bildet quasi die semantische Repräsentation des in der Datenbank abgelegten Wissens. Ergänzt man die Eigenschaften der Knoten mit Identifikatoren wie den Angaben aus der GND oder legt man den Verknüpfungsstrukturen fachspezifische Ontologien zu Grunde können die Informationen der Graphdatenbank auch für automatisierte Abfragen über das Internet erschlossen werden.
         In der Posterpräsentation werden in einem ersten Beispiel die Strukturen einer Graphdatenbank erläutert und anschließend mit der Graphenrepräsentation der Register der Regesten Kaiser Friedrichs III. und der genealogischen Datenbank Nomen-et-Gens Anwendungsbeispiele vorgestellt.
         
            
            
               Abb. 2: Beispielgraph zu Karl dem Großen, Einhard und
            der Vita Karoli Magni
         
      
      
         
            
               Bibliographie
               
                  Frants, Valery I. / Shapiro, Jacob / Voiskunskii, Vladimir
                G. (1997): Automated information retrieval.
                Theory and methods (= Library and information science). San Diego: Academic
                Press.
               
                  Kuczera, Andreas (2014): "Digitale Perspektiven
                mediävistischer Quellenrecherche", in: mittelalter.hypotheses.org
                  http://mittelalter.hypotheses.org/3492 [letzter Zugriff 28.
                  September 2015].
               
                  Kuczera, Andreas (2015): "Graphdatenbanken für
                  Historiker. Netzwerke in den Registern der Regesten Kaiser Friedrichs III.
                  mit neo4j und Gephi", in: mittelalter.hypotheses.org
                  http://mittelalter.hypotheses.org/5995 [letzter Zugriff 28.
                    September 2015].
            
         
      
   



      
         
            Kurzbeschreibung
            Im Verlauf der letzten 10 Jahre hat die Menge an digital verfügbaren,
          fachwissenschaftlich annotierten Volltexten für die historische Forschung stark
          zugenommen. Damit einher geht auch eine Veränderung sowohl der Nutzungsformen
          digitaler Quellen als auch der Möglichkeiten der historischen Arbeitsweise.
          Bestand um die Jahrtausendwende noch enger Kontakt zwischen Historiker_innen und
          Quellen, nimmt dies mit zunehmender Digitalisierung perspektivisch ab. Hat der /
          die Forscher_in früher die für seine Forschungsfragen relevanten Quellen in der
          Regel alle mindestens einmal gelesen, scheint dies bei den heute
          recherchierbaren Mengen an digitalen Quellen kaum noch möglich. Ein Hauptproblem
          ergibt sich hier aus der Schnittstelle zwischen Forscher_innen und den im Netz
          erreichbaren Quellendatenbanken. Die Suchinterfaces der Datenbanken sind oft für
          die Nutzung durch Expert_innen des jeweiligen Materials optimiert. Dies ist auf
          der einen Seite zu begrüßen, da sie den Fachwissenschaftler_innen damit besten
          Zugriff auf das Material gewähren. Daneben sollten aber weitere
          Zugriffsmöglichkeiten für übergreifende Text-Mining- oder Big-Data-Recherchen
          bereitgestellt werden, mit denen verschiedene Quellenkorpora parallel im
          Hinblick auf übergreifende Fragestellungen untersucht werden können.
            Neben Such- bzw. sammlungsorientierten Zugriffen auf die Daten wird die Fähigkeit, mittels bestimmter Visualisierungsmethoden und -instrumente neue Zusammenhänge in den Daten zu erkennen und diese dann für die historische Analyse zu nutzen immer wichtiger. Insbesondere im Bereich der graphorientierten Visualisierungsmethoden ist im Moment ein regelrechter Boom an Softwarebibliotheken und Online-Tools zu beobachten. Auch in den einschlägigen Forschungsumgebungen bzw. Forschungsinfrastrukturen für die Geisteswissenschaften wie TextGrid und DARIAH werden zunehmend Visualisierungsinstrumente für annotierte Fachdaten eingebunden.
            Im Workshop zweier Partnerinstitutionen aus dem DARIAH-DE Cluster
            "Fachwissenschaftliche Annotationen" (Salomon Ludwig Steinheim-Institut für
            deutsch-jüdische Geschichte / Akademie der Wissenschaften und der Literatur
            Mainz, Digitale Akademie) soll es mit konkretem Praxisbezug um die Potentiale,
            Methoden und Instrumente zur Visualisierung von historischen Fachdaten gehen.
            Als Anwendungsbeispiel dienen die historischen Fachdatenrepositorien, die beide
            Partner in den Workshop mit einbringen (bspw. Epidat –
            epigraphische Datenbank; die Deutschen
            Inschriften Online; die Regesta Imperii Online). 
            Visualisiert werden zunächst klassische Fragestellungen, wie beispielsweise die
              nach bestimmten Familienbeziehungen, nach Orts- oder Zeitbezügen in den Daten.
              Genutzt werden Instrumente wie beispielsweise die Graphdatenbank neo4j, das
              Netzwerkvisualisierungsinstrument Gephi, aber auch JavaScript-Tools auf Basis
              von sigma.js,
              dracula oder auch  d3 . Der Workshop wird
              schrittweise vorgehen. Ein grundlegendes Verständnis für TEI/XML, JSON, RDF
              sowie JavaScript-Webtechnologien ist für die Teilnahme am Workshop hilfreich,
              aber nicht zwingend. Nach einer allgemeinen Einführung in den Bereich des
              fachwissenschaftlichen Annotierens wird es um praktische Beispiele gehen, die in
              den Daten vorhandenen Annotationen für verschiedene Visualisierungsinstrumente
              aufzubereiten und dann die jeweiligen Visualisierungen zu erzeugen. In kurzen
              Impulsreferaten werden sich die Workshop-Teilnehmer_innen die gemeinsam
              erarbeiteten Visualisierungen und die Instrumente, mit denen diese
              Visualisierungen erzeugt worden sind, gegenseitig vorstellen. 
            Im Fazit soll der Workshop ein Bewusstsein und auch schon erste Fähigkeiten
                entwickeln, sich mit Hilfe von Visualisierungsinstrumenten neue Sichten auf das
                Quellenmaterial und somit neue Forschungsperspektiven zu eröffnen. Deutlich
                werden wird aber auch, dass dieser Ansatz nicht automatisch zu einer
                „Antwort-Maschine“ führt, die dem / der Wissenschaftler_in die interpretative
                Arbeit abnimmt. Vielmehr können sich durch Visualisierungen neue
                Interpretationsmöglichkeiten des Quellenmaterials bieten, die vorher einfach auf
                Grund der Datenmasse nicht sichtbar gemacht werden konnten.
            Weiterführende Literatur:
            Kuczera, Andreas (2015): Graphdatenbanken für Historiker.
                Netzwerke in den Registern der Regesten Kaiser Friedrichs III. mit neo4j und
                Gephi http://mittelalter.hypotheses.org/5995.
              
            Schrade, Torsten (2013): "Datenstrukturierung", in: Frietsch, Ute / Rogge, Jörg
                (eds.): Über die Praxis des kulturwissenschaftlichen
                Arbeitens. Ein Handwörterbuch. Bielefeld: transcript 91–97.
         
         
            Teilnehmerzahl
            10 - 15 Personen
         
         
            Benötigte Ausstattung
            
               WLAN-Zugang
               Beamer
               Workshop-Teilnehmer sollten ihre eigenen Laptops mitbringen
            
         
      
   



      
         
            
         
         
            Beispiele Graphbasierter Erschließung der Datenbank Nomen et Gens
            Die Datenbank Nomen et Gens ist aus einem DFG-Projekt hervorgegangen und verzeichnet Quellen und die in Ihnen belegten Personen für die vier Jahrhunderte vor der Zeit Karls des Großen. Das Abfragefrontend der Mysql-Datenbank ist im Internet unter www.nomen-et-gens.de zu erreichen.
            Ein Teil der Datenbank wurde im Vorfeld eines DFG-Antrages in eine Graphdatenbank konvertiert um neue Auswertungsmöglichkeiten zu testen, die im Rahmen dieses Posters dargestellt werden sollen.
            Frage an die Datenbank:
            Zeige mir die Quelle „Annales Petaviani“ und alle Personen, die in dieser Quelle belegt sind und alle Quellen, in denen diese Personen wiederum gemeinsam belegt sind. 
            
               
                  
                  Abbildung 1: Visualisierung der Graphdatenbankabfrage.
               
            
            Bewertung des Abfrageergebnisses:
            
               Der interessanteste Information der Visualisierung ist die traversale Abfrage zu Personen in einer Quelle, die wiederum in einer Quelle gemeinsam vorkommen. Die Abfrage „Zeige mir alle Personen in einer Quelle" funktioniert ja auch mit einer relationalen Datenbank. Aber es gibt keine Möglichkeit, auf die Weise auch noch gleich zu sehen, in welcher Quelle eine Person außerdem noch steht. Insofern fügt die Graphdatenbank hier eine "Dimension" hinzu. 
               Verwandtschaftliche Beziehungen zwischen Personen stehen beim jeweiligen Personeneintrag und sind damit immer nur bis zum nächsten Glied zu sehen.
               Für Frühmittelalterhistoriker besonders interessant sind gemeinsam urkundlich belegte Personen. Eine solche Abfrage ist über die relationale Datenbank nur schwer umfassend durchzuführen.
               Die Visualisierung aus der Graphdatenbank veranschaulicht die Überlieferungssituation von einzelnen Personen. Der umkringelte Radbod ist, anders als z.B. Bonifatius zu dem es sehr viele Belege gibt, nur schwer historisch fassbar.
            
         
      
      
         
            
               Bibliographie
               
                  Kuczera, Andreas (2016c): 
                        „Digital Editions beyond XML – Graph-based Digital Editions“, 
                        in: 
                        Proceedings of the 3rd HistoInformatics Workshop on Computational History (HistoInformatics 2016) 37–46
                        http://ceur-ws.org/Vol-1632/paper_5.pdf.
                    
               
                  Kuczera, Andreas (2016b): 
                        „Graphbasierte digitale Editionen“, 
                        in: 
                        Mittelalter: Interdisziplinäre Forschung und Rezeptionsgeschichte 19. April 2016 
                        mittelalter.hypotheses.org/7994
               
               
                  Kuczera, Andreas (2016a): 
                        „Endcoding and Presenting Historical Biographical Data with Graph Data Bases“,
                        in: 
                        CO:OP. The Creative Archives' and Users' Network
                  https://coop.hypotheses.org/297.
                    
               
                  Kuczera, Andreas (2015): 
                        „Graphdatenbanken für Historiker. Netzwerke in den Registern der Regesten Kaiser Friedrichs III. mit neo4j und Gephi“,
                        in: 
                        Mittelalter: Interdisziplinäre Forschung und Rezeptionsgeschichte 5. Mai 2015
                        mittelalter.hypotheses.org/5995
               
               
                  Kuczera, Andreas (2014b): 
                        „Big Data History“, 
                        in: 
                        Mittelalter: Interdisziplinäre Forschung und Rezeptionsgeschichte 10. Oktober 2014
                        mittelalter.hypotheses.org/3962
               
               
                  Kuczera, Andreas (2014a): 
                        „Digitale Perspektiven mediävistischer Quellenrecherche“, 
                        in: 
                        Mittelalter: Interdisziplinäre Forschung und Rezeptionsgeschichte 18. April 2014
                        mittelalter.hypotheses.org/3492
               
            
         
      
   



      
         
         
            Ausgangslage
            Der Einfluss der unter Digital Humanities (DH) zusammengefassten digitalen Theorien und Methoden auf die geisteswissenschaftlichen Disziplinen wächst stetig. Digitale Projekte erleben in den Geisteswissenschaften einen rasanten Aufschwung (Koller 2016: 43). Damit einher geht der Bedarf an Absolventen geisteswissenschaftlicher Fächer, die bereits während ihres Studiums Kompetenzen im Bereich der Digital Humanities erwerben konnten. Bereits 2013 forderten Vertreter/innen im „Manifest für die DH“ eine Etablierung „digitale[r] Trainingsprogramme in den Geisteswissenschaften“ (DH-Manifest: 2013), angepasst an die unterschiedlichen Bedürfnisse der Fachbereiche und die jeweiligen Karrierestufen. Auch der DHd misst der Ausgestaltung der IT-Ausbildung von Studierenden eine gesteigerte Bedeutung zu. Die Arbeitsgruppe zur Erarbeitung eines „Referenzcurriculums Digital Humanities“
                    2 beschäftigt sich mit der Suche nach einer 
                    bestpraxis, von der Anwender und Institutionen gleichermaßen profitieren (Sahle 2013; Thaller 2015: 3).
                
            Zahlreiche Universitätsstandorte haben auf die neuen Anforderungen mit der Einrichtung unterschiedlich ausgestalteter DH-Studiengänge reagiert (Bartsch/Borek/Rapp 2016: 173; DH Course Registry). Trotz dieser neugeschaffenen Angebote besteht ein zusätzlicher Bedarf an informationstechnologischer Ausbildung in der Breite (Ehrlicher 2016: 625). Zunehmend wird auch in „klassischen“ geisteswissenschaftlichen Berufsfeldern Sicherheit im Umgang mit Software und digitalen Technologien vorausgesetzt. Dieses Grundverständnis digitaler Methoden kann nicht mehr ausschließlich im Selbststudium angeeignet werden (Spiro 2013: 332; Sahle 2016: 79).
         
         
            Projektziele und Rahmenbedingungen
            Hier setzt das Projekt „Digitaler Campus Bayern – Digitale Datenanalyse in den Geisteswissenschaften“ an, welches von der IT-Gruppe Geisteswissenschaften (ITG) der Ludwig-Maximilians-Universität München (LMU) seit Beginn dieses Jahres durchgeführt wird. Grundgedanke ist eine IT-Grundausbildung („
                    IT for all“), welche die Studierenden problemorientiert in die Anwendung digitaler Methoden einführt. Ausgehend von fachwissenschaftlichen Fragestellungen werden Lehrveranstaltungen mit IT-Inhalten in Kooperation mit verschiedenen geschichtswissenschaftlichen Disziplinen und der Kunstgeschichte konzipiert. Dabei soll eine möglichst umfassend angelegte Grundlagenvermittlung in Erfassung, Modellierung, Analyse und anschließender Visualisierung von Daten erfolgen (Lücke/Riepl 2016: 77). Das Verständnis digitaler Methoden steht ebenso im Vordergrund wie eine fachliche Reflexion ihrer Potentiale (Rehbein 2016: 17).
                
            Die Situation der DH an der LMU gestaltete sich bis Projektbeginn (Januar 2016
                    3) ambivalent. In den vorgenannten Studiengängen wurden regelmäßig Überblicksveranstaltungen zur Einführung in die Informatik für Historiker bzw. Kunsthistoriker angeboten. Eine praktische Umsetzung des theoretischen Wissens konnte im Rahmen dieser Veranstaltungen jedoch nicht geleistet werden. Demgegenüber werden durch die ITG, die auf langjährige und umfangreiche Erfahrungen im Bereich des digitalen Projektmanagements
                    4 verweisen kann, optimale Voraussetzungen für eine fortan praxisnahe IT-Ausbildung geschaffen.
                
            Als Mitglied im Münchner Arbeitskreis für digitale Geisteswissenschaften (dhmuc)
                    5 kooperiert die IT-Gruppe zudem fach- und institutionsübergreifend mit zahlreichen kulturellen Einrichtungen. An der Schnittstelle zur universitären Lehre ist es möglich, die „
                    IT for all“-Ausbildung geisteswissenschaftlicher Studierender auf die Anforderungen und Wünsche der potentiellen Arbeitgeberseite im (digitalen) Kultur-, Wissenschafts- und Informationssektor auszurichten.
                
         
         
            Interaktive Lehr- und Lernumgebung 
                    DHVLab
            
            Für die praktische Umsetzung kommt eine interaktive Lehr- und Lernumgebung, das 
                    Digital Humanities Virtual Laboratory – kurz 
                    DHVLab – zum Einsatz
                    6. Die im Entstehen begriffene Plattform umfasst mehrere Komponenten, die im Folgenden vorgestellt werden sollen:
                
            
               Virtuelle Rechenumgebung
               Die virtuelle Rechenumgebung ist das „Herzstück“ der Ausbildungsplattform. Auf dem virtuellen Desktop werden in Abstimmung mit dem/der Kursleiter/in Software und Tools installiert. Dadurch wird die sukzessive Installation durch die Teilnehmer/innen obsolet, wodurch Probleme aufgrund unterschiedlicher Betriebssysteme und Versionierungen vermieden werden. Bei Anmeldung im 
                        DHVLab erhält jede/r Teilnehmer/in eine eigene SQL-Datenbank. Gleichzeitig werden strukturierteDatensammlungen vorgehalten. Diese sind für die Kursteilnehmer/innen zugänglich und für eigene oder im Kurs behandelte Fragestellungen verwendbar. Im Laufe der Lehrveranstaltung können neue Forschungsfragen ausgearbeitet und ein grundsätzliches Verständnis für den sinnvollen Einsatz von Tools und Software
                        7 in den Geisteswissenschaften entwickelt werden.
                    
            
            
               Ausbildungsmaterialien
               Im vergangenen Semester wurde das System testweise in vorgenannten Einführungsveranstaltungen eingesetzt. Die bei der Evaluation gesammelten Erfahrungen fließen unmittelbar in die Erstellung bzw. Erweiterung der Ausbildungsmaterialien. Anhand praxisnaher Manuale wird IT-Grundlagenwissen, in einzelne Lehreinheiten gegliedert, anschaulich dargestellt und erklärt. Die Erstellung von Lehrvideos und Übungsaufgaben ist vorgesehen. Aus diesem Portfolio können Dozentinnen und Dozenten Module entsprechend ihrer fachwissenschaftlichen Schwerpunktsetzung und der Voraussetzungen der Teilnehmer/innen auswählen. Die Seminarplanung und -durchführung erfolgt stets in enger Abstimmung mit den Projektmitarbeitern.
            
            
               Publikationsumgebung
               Für die Vor- und Nachbereitung der einzelnen Sitzungen steht ein WordPress-Blog zur Verfügung. Dort können die Kursleiter/innen Materialen einstellen, die Studierenden ihren Erkenntnisfortschritt und Analyseergebnisse dokumentieren. Dabei erlernen sie gleichzeitig 
                        in praxi das wissenschaftliche Bloggen als innovative Form des Publizierens. Eine abschließende Publikation der studentischen Seminararbeiten ist auf dieser Plattform möglich.
                    
            
            
               Datenrepositorium
               In einem gesonderten Bereich der Datenbankumgebung werden die von den Studierenden im Rahmen einer Lehrveranstaltung erarbeiteten Datenbestände modelliert und nachhaltig abgelegt. Langfristiges Ziel ist der Aufbau eines Forschungsdatenrepositoriums. Nachfolgende Kurse mit ähnlichen Seminarthemen können auf diese Datensammlungen zugreifen, für die eigene Forschungsarbeit verwenden und dadurch sukzessive erweitern. Unterstützung erfährt die ITG durch die Universitätsbibliothek der LMU als Kooperationspartnerin auf dem Gebiet der nachhaltigen und nachnutzbaren elektronischen Publikation von Forschungsdaten.
            
            
               Entwicklung eigener Analyse- und Softwarekomponenten
               Mit dem 
                        DHVLab Analytics Center wurde eine Webanwendung entwickelt, die dazu dient, konkrete geisteswissenschaftliche Fragestellungen mithilfe quantitativer statistischer Methoden zu beantworten, sowie im Stile eines explorativen Werkzeuges neue Forschungsansätze zu eröffnen. Das 
                        Analytics Center kombiniert einführende deskriptive Analysen mit komplexeren Methoden der multivariaten Statistik. Neben diesem Analysetool entsteht derzeit eine Editionsumgebung, die speziell auf die Anforderungen von Studierenden und Promovierenden ausgerichtet wird. Diese wird erstmals im Sommersemester 2017 in einer Übung zur Edition mittelalterlicher Urkunden zum Einsatz kommen. Die Entwicklung weiterer Instrumente ist geplant.
                    
            
         
         
            Der Einsatz der Plattform in der Lehre
            Nach der technischen Realisierung der Plattform und dem Aufbau grundlegender Ausbildungsmaterialien im ersten Projekthalbjahr kommt das System im Wintersemester 2016/2017 erstmals in eigens konzipierten Lehrveranstaltungen zur Anwendung. In der Kunstgeschichte soll das 
                    Analytics Center in einem Seminar zur Beschäftigung mit informatischen und mathematischen Verfahrensweisen anregen. Parallel dazu erfolgt eine Einführung in die Statistiksoftware 
                    RStudio. Ein geschichtswissenschaftliches Hauptseminar beschreitet den Weg von der Originalquelle über die strukturierte Aufnahme und Modellierung von Forschungsdaten sowie die Einführung in die Arbeit mit relationalen Datenbanken hin zur Georeferenzierung. Die in den Seminaren gewonnenen Erfahrungen und Erkenntnisse fließen unmittelbar in die Verbesserung und Ausweitung des bestehenden Lehrmaterials ein (u.a. Erstellung von Anwendungsszenarien). Neben den genannten Kursen wird die Plattform bereits in zahlreichen Lehrveranstaltungen als technische Grundlage verwendet
                    8.
                
         
         
            Konzeption eines fachspezifischen DH-Curriculums
            Die sukzessive wachsende Plattform und die aktuell angebotenen Kurse dienen als Grundlage für eine Institutionalisierung der IT-Grundausbildung in Form eines fachspezifischen DH-Curriculums. Das Konzept für das geplante freiwillige Zusatz-Zertifikat wird derzeit in der Projektgruppe erarbeitet und baut auf Erfahrungen vergleichbarer Angebote im deutschsprachigen Raum auf
                    9. Angedacht ist eine Kombination aus Veranstaltungen, die explizit IT-Grundlagenwissen vermitteln, und praxisorientierten Kursen, in denen die erlernten IT-Inhalte auf fachwissenschaftliche Gegenstände angewendet werden. Wichtig erscheint eine ausgewogene Verschränkung von 
                    eLearning-Angeboten und Präsenzveranstaltungen, da insbesondere letzteren durch den intensiven Austausch der Studierenden mit DH-Spezialisten ein großer Beitrag zum Lernerfolg beigemessen wird
                    10.
                
         
         
            Grundlage einer nachhaltigen IT-Didaktik
            Neben der Langzeitarchivierung der Forschungsdaten wird auch die Nachhaltigkeit der informationstechnologischen Infrastruktur (Serveranlage mit redundant ausgelegten File-, Datenbank- und Web-Servern sowie ausreichenden Storages) durch die IT-Gruppe Geisteswissenschaften dauerhaft gewährleistet. Die Architektur des 
                    DHVLab ist flexibel und skalierbar gestaltet, sodass sie weiter ausgebaut werden kann (bei Bedarf ist ein Hosting der Server am Leibniz-Rechenzentrum in Garching bei München möglich). Für eine nachhaltige IT-Didaktik spielt neben der langfristig gesicherten technischen Infrastruktur insbesondere die inhaltliche Kontinuität eine entscheidende Rolle. Die im Rahmen des Projektes erarbeiteten Lehreinheiten werden dauerhaft zur Verfügung gestellt. Thematisch sind sie so zu gliedern und fachlich anzupassen, dass eine spezifische Auswahl für eine Lehrveranstaltung und damit eine Integration in ein geisteswissenschaftliches Einzelfach möglich ist. Die IT-Gruppe stellt auch nach Ende der Projektlaufzeit die unterstützende Begleitung der Lehrveranstaltungen sicher. Der Vortrag möchte zur Diskussion anregen, inwiefern sich die Anpassung der Materialien an die sich rasch wandelnden Anforderungen im Bereich der Digital Humanities möglichst effizient gestalten lässt. IT-Didaktik scheint nur dann einen Anspruch auf Nachhaltigkeit zu besitzen, wenn sie sich in einem steten Anpassungsprozess befindet.
                
            Ganz im Sinne des „Digitalen Campus Bayern“ ist das Münchener Pilotprojekt auf eine Ausweitung auf andere Studienstandorte ausgerichtet. Die Plattform wird beispielsweise ab 2017 in einem im Aufbau befindlichen Kooperationsprogramm zur DH-Ausbildung der Universitäten Erlangen, München und Regensburg zum Einsatz kommen. Alle Module des 
                    DHVLab können kollaborativ von anderen Hochschulen genutzt werden, um umfassende Sammlungen von Tutorials, Aufgaben, Softwarebeschreibungen, Anwendungsszenarien sowie Sammlungen fachwissenschaftlicher Objekt- und Metadaten aufzubauen und gemeinsam zu pflegen.
                
         
      
      
         
            Vgl. 
                            http://www.dh-curricula.org/index.php?id=1 [letzter Zugriff: 30. November 2016].
                        
             Die Projektlaufzeit beträgt zwei Jahre. Das Vorhaben ist Teil eines Förderprogramms, welches das Bayerische Wissenschaftsministerium aufgelegt hat. Vgl. 
                            https://www.km.bayern.de/pressemitteilung/9340/.html [letzter Zugriff: 30. November 2016].
                        
             Vgl. die Übersicht unter 
                            www.itg.lmu.de/projekte [letzter Zugriff: 30. November 2016].
                        
             Vgl. 
                            http://dhmuc.hypotheses.org/uber [letzter Zugriff: 30. November 2016].
                        
             Für die Dokumentation der technischen Infrastruktur vgl. 
                            http://dhvlab.gwi.uni-muenchen.de/index.php/Category:
               Architektur [letzter Zugriff: 30. November 2016].
                        
             Derzeit stehen in der virtuellen Umgebung u.a. folgende Software und Programme zur Verfügung: LibreOffice-Paket, OCRFeeder und Ocrad (Texterkennung), Python (PyCharm), RStudio (Statistik), Gephi (Visualisierung), epcEdit (XML-Editor), AntConc und TreeTagger (Korpuslinguistik).
             Vgl. die Zusammenstellung auf der Projektseite: 
                            http://dhvlab.gwi.uni-muenchen.de/index.php/Das_DHVLab_im_Einsatz [letzter Zugriff: 30. November 2016].
                        
             Vgl. insbesondere die Angebote in Köln (
                            http://www.itzertifikat.uni-koeln.de/), Passau (
                            http://www.phil.uni-passau.de/zertifikat-dh/) und Stuttgart („Das digitale Archiv“, 
                            http://www.uni-stuttgart.de/dda), letztgenanntes als Vorläufer eines DH-Masterstudienganges [letzter Zugriff: 30. November 2016].
                        
             Vor diesem Hintergrund erscheinen grundständige 
                            eLearning-Angebote wie „The Programming Historian“ (
                            http://programminghistorian.org/) für einen autodidaktischen Einstieg begrüßenswert. Die Initiatoren des DHVLab sind jedoch der Auffassung, dass eine umfassende Präsenzausbildung nicht ersetzt werden kann.
                        
         
         
            
               Bibliographie
               
                  Bartsch, Sabine / Borek, Luise / Rapp, Andrea (2016): 
                        „Aus der Mitte der Fächer, in die Mitte der Fächer: Studiengänge und Curricula – Digital Humanities in der universitären Lehre“,
                        in: 
                        Bibliothek – Forschung und Praxis 40 (2): 172–178 10.1515/bfp-2016-0030.
                    
               
                  DARIAH-EU: 
                        Digital Humanities Registry – Courses
                  https://dh-registry.de.dariah.eu/ [letzter Zugriff 30. November 2016].
                    
               
                  DHI Paris (Teamaccount) (2013): 
                        „Wissenschaftlicher Nachwuchs in den Digital Humanities: Ein Manifest“,
                        in: 
                        Digital Humanities am DHIP, 23. August 2013 
                        http://dhdhi.hypotheses.org/1995 [letzter Zugriff 30. November 2016].
                    
               
                  Ehrlicher, Hanno (2016): 
                        „Fingerübungen in Digitalien. Erfahrungsbericht eines teilnehmenden Beobachters der Digital Humanities aus Anlass eines Lehrexperiments“, 
                        in: 
                        Romanische Studien 4: 623–636 
                        http://www.romanischestudien.de/index.php/rst/article/view/88 [letzter Zugriff 30. November 2016].
                    
               
                  Koller, Guido (2016): 
                        Geschichte digital: Historische Welten neu vermessen. 
                        Stuttgart: Kohlhammer.
                    
               
                  Lücke, Stephan / Riepl, Christian (2016): 
                        „Auf dem Weg zu einem Curriculum in den Digital Humanities“,
                        in: 
                        Akademie Aktuell 57 (1): 74–77 
                        http://badw.de/fileadmin/pub/akademieAktuell/2016/56/0116_17_Riepl_V04.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Rehbein, Malte (2016): 
                        Geschichtsforschung im digitalen Raum. Über die Notwendigkeit der Digital Humanities als historische Grundwissenschaft. (Preprint) 
                        http://www.phil.uni-passau.de/fileadmin/dokumente/lehrstuehle/rehbein/Dokumente/GeschichtsforschungImDigitalenRaum_preprint.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Sahle, Patrick (2013): 
                        DH studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities (= DARIAH-DE Working Papers 1). 
                        Göttingen: GOEDOC 
                        http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-1.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Sahle, Patrick (2016): 
                        „Digital Humanities als Beruf. Wie wird man ein „Digital Humanist“, und was macht man dann eigentlich?“, 
                        in: 
                        Akademie Aktuell 57 (1): 78–83 
                        http://badw.de/fileadmin/pub/akademieAktuell/2016/56/0116_18_Sahle_V04.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Spiro, Lisa (2012): 
                        „Openingup Digital Humanities Education“, 
                        in: Hirsch, Brett D. (ed.):
                        Digital Humanities Pedagogy: Practices, Principlesand Politics 331–363 
                        http://www.openbookpublishers.com/product/161/ [letzter Zugriff 30. November 2016].
                    
               
                  Thaller, Manfred (2015): 
                        „Panel: Digital Humanities als Beruf – Fortschritte auf dem Weg zu einem Curriculum“, 
                        in: 
                        Digital Humanities als Beruf: Fortschritte auf dem Weg zu einem Curriculum, vorgelegt auf der Jahrestagung 2015 3–5 
                        https://www.digitalhumanities.tu-darmstadt.de/fileadmin/dhdarmstadt/materials/Digital_Humanities_als_Beruf_-_Stand_2015.pdf [letzter Zugriff 30. November 2016].
                    
            
         
      
   



      
         Problemstellung
         Im Anschluss an den 1990 durch den Humangeographen Edward Soja ausgerufenen ‚Spatial Turn’ (Soja 1990) haben sich zahlreiche kulturwissenschaftliche Forschungsarbeiten mit einer Beschreibung des Raums beschäftigt. In der Literaturwissenschaft fanden dabei u.a. kartografische Darstellungen große Resonanz: Franco Moretti etwa untersuchte in seinem „Atlas of the European Novel“ Orte der literarischen Produktion und Rezeption (Moretti 1998), Barbara Piattis Studie „Die Geographie der Literatur“ richtete den Fokus auf die Illustration einer konkreten literarisch thematisierten Gegend (die Zentralschweiz, vgl. Piatti 2008). Besondere Aufmerksamkeit wurde literarischen Karten auch im Kontext der Digital Humanities zu Teil, in denen geografische Informationssysteme (GIS) zum Einsatz kommen (typische Workflows beschreiben Gregory et. al. 2015)
         Den meisten dieser Ansätze ist dabei gemein, dass sie für ihre Datengrundlage in erster Linie auf konkrete Nennungen von Ortsnamen (Toponymen) rekurrieren und weitere Ortsmarker weniger stark berücksichtigen. An der Konstitution literarischer Räume sind jedoch in der Regel auch komplexere Faktoren beteiligt, zu deren Beschreibung bereits erste narratologische Ansätze vorliegen (etwa von Kathrin Dennerlein [2009 und 2011] oder Gabriel Zoran [1984], vgl. auch die Überlegungen bei Piatti [2008]), die jedoch im Kontext der Digital Humanities bislang noch zu wenig Beachtung gefunden haben.
         In unserem Beitrag möchten wir diese Ansätze aufgreifen, um das Instrumentarium der digitalen Textanalyse hinsichtlich der Kategorie des Raums zu schärfen und zu erweitern. Dazu scheinen uns insbesondere zwei Aspekte von Bedeutung: Zum einem die Unterscheidung von Raummarkierungen hinsichtlich ihrer Handlungsrelevanz (I), zum anderen die Ausweitung der Analyse auf räumliche Begriffe, die über bloße Namensnennungen hinausgehen (II). Für beide Problemfelder präsentieren wir erste Verfahren zur automatischen Auswertung und geben Ausblicke auf die Möglichkeiten einer vergleichenden Analyse.
         
            I. Differenzierung von Räumen nach Handlungsrelevanz
         
         Im Anschluss an Dennerleins Narratologie des Raumes hat sich insbesondere der Terminus der 
                räumlichen Gegebenheit als Grundeinheit zur Bezeichnung von Ort und Raum durchgesetzt. Sind räumliche Gegebenheiten der Schauplatz eines konkreten Ereignisses, werden sie als 
                Ereignisregionen spezifiziert. Diese haben als die zentralen handlungsrelevanten Räume besondere Bedeutung gegenüber 
                erwähnten räumlichen Gegebenheiten, die bei nicht-situationsbezogener Thematisierung von Raum entstehen. In ähnlicher Weise unterscheiden Piatti (2008) sowie Piatti et. al. (2011) zwischen 
                Schauplatz und 
                projizierten Orten.
            
         Am Beispiel von Jules Vernes „Reise um die Erde in 80 Tagen“ lässt sich die Wichtigkeit dieser Unterscheidung aufzeigen: So bietet z.B. Kapitel 14 eine Zugfahrt durch das Gangestal mit Aufenthalten in Allahabad und Benares sowie der Ankunft in Calcutta. Genannt werden im Text jedoch auch weitere Städte Indiens und das nächste Ziel Hongkong; eine Vielzahl der extrahierten Ortsnamen bezieht sich somit nicht auf den Handlungsort des Kapitels.
         Erst die kategoriale Trennung der Raummarkierungen in Ereignisregionen und erwähnte räumlichen Gegebenheiten ermöglicht die valide Rekonstruktion einer Reiseroute, die dann in einer GIS-Darstellung visualisiert werden kann (Abbildung 1). 
         
            
         
         Eine automatische Unterscheidung dieser Kategorien muss daher das Fernziel computerunterstützter Raumuntersuchungen sein. Ansätze zu einer solchen Differenzierung suchen wir in der Anwendung von computerlinguistischen Methoden der Relationsextraktion, bei denen sich aus strukturellen Auffälligkeiten Regeln zur Klassifikation von Ereignisregionen und erwähnten räumlichen Gegebenheiten ableiten lassen. Hierzu zwei Beispiele: 
         1. Das 14. Kapitel der „Reise um die Erde in 80 Tagen“ beginnt mit dem in Abbildung 2 dargestellten Satz:
         
            
         
         Vor allem der Hauptsatz mit dem Pfad [Figur – SUBJ – Bewegungsverb - OBJ – Raumnomen] zwischen "Phileas Fogg" und "Gangesthal" kennzeichnet letzteres als Ereignisregion. Dabei stellt insbesondere die Verbkategorie ein Indiz für die Klassifikationsentscheidung dar: Statische Verben ("stehen", "sitzen") oder Verben der Bewegung ("gehen", "fahren") zeugen in spezifischen Satzstrukturen häufig von einer Ereignishaftigkeit im Gegensatz zu Verben der Kognition ("denken"). 
         Zur Einteilung der Verben nutzen wir das von der Universität Tübingen entwickelte lexikalisch-semantische Netz 
                GermaNet (Hamp/Feldweg 1997, Henrich/Hinrichs 2010), in dem eine Systematisierung vorliegt, die auf den Verbkategorien von Levin und dem Valenzwörterbuch von Schumacher basiert (Levin 1993, Schumacher 1986).
            
         2. Findet ein erzähltes Ereignis innerhalb einer Bewegung im Raum statt, können mehrere Räume zu einem 
                Bewegungsbereich zusammengefasst werden. Diese können bei Strukturen wie der folgenden leicht maschinell erkannt werden:
            
         "Ich fuhr [...] 
                von Königstadt, wo unser Hauptbüro war, 
                nach Gründerheim, wo wir eine Nebenstelle hatten. 
                Dort holte ich dringende Korrespondenz, Gelder und schwebende Fälle." (Böll, Heinrich: Über die Brücke)
            
         Im zweiten Satz findet sich zudem eine Referenz auf das Antezedens "Gründerheim". Eigentlich werden derartige deiktische Adverbialausdrücke ("hier", "da" und "dort") bei der Koreferenz-Resolution nicht berücksichtigt. Deshalb planen wir eine Erweiterung der Trainingssets bestehender Koreferenz-Systeme hinsichtlich dieser Termini. 
         Die hier an zwei Beispielfällen dargestellten Regeln zur Unterscheidung narratologischer Raumeinheiten sollen in Zukunft kontinuierlich erweitert und zunächst so gestaltet werden, dass sie eine hohe Präzision erzielen. Anschließend können sie als Features für spätere maschinelle Lernverfahren verwendet werden.
         
            II. Anreicherung der Raumbeschreibung
         
         Netzwerkvisualisierung von Raumnomen
         Eine kartografische Darstellung, wie sie in Abbildung 1 ersichtlich ist, bleibt auf realweltliche Ortsnamen beschränkt und lässt wesentliche Aspekte der Raumdarstellung außer Acht. Einen ersten Ansatz, die Vielfältigkeit der tatsächlichen Handlungsräume und ihrer Zusammenhänge abzubilden, bietet das Netzwerk in Abbildung 3. Hier werden für das angesprochene Kapitel 14 neben den Toponymen auch unspezifische Raumnomen als Knoten berücksichtigt und Verbindungen immer dann etabliert, wenn zwei Raumbegriffe gemeinsam in einem Satz auftreten.
         Insbesondere landschaftliche (grün) und architektonische Raumnomen (grau) stellen relevante Klassen von Raummarkern dar, die neben den konkreten Ortsnamen zentrale Komponenten der literarischen Raumbeschreibung bilden. Als räumliche Gegebenheiten kommen aber auch bewegliche Objekte, in denen sich Figuren aufhalten können, in Frage, wie die in der Grafik blau markierten Fahrzeuge. 
         
            
         
         Abbildung 3: Netzwerk
         Lexikon und Taxonomien für Raumbegriffe
         Zur lexikalischen Erfassung von realweltlichen Toponymen greifen wir auf die Named-Entitiy-Recognition von 
                Weblicht (2012) zurück, deren Ergebnisse wir mit dem Rückgriff auf die frei zugänglichen Datenbanken 
                GeoNames (www.geonames.org) und 
                OpenStreetMap (www.openstreetmap.org, Datendownload über www.geofabrik.de) zu verfeinern trachten. Aufgrund des Problems der möglichen Ambiguität von Ortsnamen (Leidner / Liebermann 2011, Gregory / Hardie 2011) ist jedoch eine manuelle Nachbearbeitung nötig. Listen von unspezifischen Raumnomen („Berg“, „Bach“, etc.) erstellen wir (ebenfalls semi-automatisch) auf der Basis von 
                GermaNet.
            
         Innerhalb dieses Lexikons planen wir zudem eine Einordung der Raumbegriffe in spezifische Taxonomien:
         
            i) Vertikale Raum-Ort-Hierarchie
         
         Narratologisch wird unter dem Begriff 
                Raum ein umfassendes Gebiet in der erzählten Welt verstanden, welches ein Innen und Außen besitzt und wiederum lokalisierbare, punktuelle 
                Orte beinhaltet (Dennerlein 2009). Diese Zuordnung erfolgt jedoch meist relational zur Erzählsituation: In Alfred Döblins Roman 
                Berlin Alexanderplatz bildet die Stadt den Raum mit einzelnen Plätzen und Straßen als Orten. Unter einer geringfügigen Erweiterung des Erzählspektrums wäre Berlin aber potentiell nur ein Ort unter vielen im übergeordneten Raum Deutschland.
            
         Statt einer festen Zuschreibung nähern wir uns dem Verhältnis von Ort und Raum über eine vertikale Taxonomie von räumlichen Gegebenheiten an, die von der Planetenebene bis zu jenen Objekten reicht, in denen sich unter Annahme faktualer Gesetzmäßigkeiten keine Figur mehr aufhalten kann. Im Sinne des 
                principles of minimum departure (Ryan 1980) kann dabei so lange von einer nach realweltlichen Gesetzen eingerichteten Erzählwelt ausgegangen werden, bis deren bewusste Aufhebung innerhalb fiktionaler Texte in spezifischen Fällen eine gezielte Anpassung der Taxonomie erfordert (z.B. bei Aladins Flaschengeist in der Wunderlampe).
            
         Abbildung 4 zeigt basierend auf den kapitelweise extrahierten Ereignisregionen in „Reise um die Erde in 80 Tagen“ die obersten taxonomischen Stufen: 1. Kontinent, 2. Land, 3. Stadt bzw. landschaftliche Region (inklusive Transportmittel, markiert in blau). 
         
            
         
         Während in den ersten beiden Ebenen dieses Beispiels ausschließlich Toponyme vorkommen, beinhaltet zumindest die dritte Stufe in der Nominalphrase „Latenenwald bei Allahabad“ ein unspezifisches Raumnomen. Weitere allgemeine Begriffe wären vor allem auf einer hier nicht dargestellten vierten Ebene zu finden (z.B. „Sumpf“, „Bach“, „Weizenfeld“ innerhalb der Landschaft „Behar“, vgl. Abb. 3).
         Zur automatischen Erstellung einer solchen Hierarchie bieten sich bei Toponymen die in 
                GeoNames vorhandenen Metadaten an, in denen bei jedem Stadt-Eintrag Informationen zu Land und Kontinent vorhanden sind. Bei unspezifischen Raumnomen eignet sich hingegen die hierarchische Struktur von 
                GermaNet. So stellt etwa der Begriff „Bach“ ein Hyponym zum übergeordneten Synset „Wasser/Gewässer“ dar, letzteres besitzt wiederum die Hyperonyme „Land/Gegend/Gefilde“.
            
         
            ii) Wortfelder
         
         Wie in Abb. 3 ersichtlich, speisen sich Raumnomen zu großen Teilen aus den Wortfeldern Architektur und Landschaft. Die folgende Analyse basiert auf semiautomatisch erstellten Wortlisten, die auf der Basis von 
                GermaNet durch die Auswertung der entsprechenden Synsets und Implikationen (Hyperonymie und Hyponymie) von zentralen Begriffen aus beiden Wortfeldern gewonnen wurden.
            
         Makroperspektive
         Das Potential digitaler korpusgestützter Raum-Analysen soll anhand des Vergleichs dreier ‚Berlin-Romane’ exemplarisch aufgezeigt werden. Dazu werden die Texte in jeweils zehn Segmente aufgeteilt und hinsichtlich der Frequenz spezifischer Raumbegriffe aus den Wortfeldern Architektur und Landschaft untersucht:
         Dabei lassen sich deutlich höhere Anteile des architektonischen Vokabulars gegenüber dem Segmentmittelwerten eines Vergleichskorpus erkennen, das aus 451 im Textgrid-Repository enthaltenen Romanen besteht (Abbildung 5, oben). 
         
            
         
         Während die Verteilung des architektonischen Wortschatzes in Hesses „Heimliches Berlin“ nur temporäre Spitzen zeigt, sind die Segmentverteilungen von „Berlin Alexanderplatz“ und Wilhelm Raabes „Die Chronik der Sperlingsgasse“ gegenüber der mittleren Verteilung des Korpus signifikant verschieden. Dies wurde sowohl mit dem Wilcoxon-Rangsummentest (Annahme der Varianzhomogenität und Gleichverteilung zwischen den Sampleverteilungen) sowie dem Mood's Median-Test (keine Verteilungsannahme) überprüft (Abbildung 6).
         Das landschaftliche Vokabular hingegen liegt bei den Berlin-Romanen tendenziell etwas unter dem Mittel des Korpus, allerdings sind die Abweichungen nur im Fall von „Berlin Alexanderplatz“ eindeutig signifikant (Abb. 5 unten, Abb. 6)
         
            
         
         Ungeachtet dieser Unterschiede sind die Zusammenhänge zwischen beiden Wortfeldern auffällig (Abbildung 7). Die Spearman-Korrelation zwischen architektonischen und landschaftlichen Begriffen bei „Berlin Alexanderplatz“ beträgt 0.5030488 und bei „Die Chronik der Sperlingsgasse“ sogar 0.7454545. So kann trotz abweichender Anteile der Wortfelder hinsichtlich ihrer Frequenz eine starke Verflechtung spezifischer Klassen von Räumen angenommen werden.
         
            
         
         Ausblick
         Die vorgestellten Ansätze verstehen sich als Anregung für die Entwicklung eines differenzierten Instrumentariums der digitalen Raumanalyse, das in Zukunft weiter ausgebaut werden soll und die Grundlage für die Behandlung weiterführender literaturwissenschaftlicher Fragestellungen bildet, die etwa Aspekte der Semantisierung von Räumen (Lotman 1972), des raumzeitlichen Entwurfs von Erzählwelten (Bachtin 1989) und der Bedeutung von Raumkonstellationen für die Gattungspoetik beinhalten (vgl. zusammenfassend Nünning 2009). 
      
      
         
            
               Bibliographie
               
                  Bachtin, Michail Michailowitsch (1989): 
                        Formen der Zeit im Roman. Untersuchungen zur historischen Poetik. Ed. von Kowalski, Edward / Wegner, Michael.
                        Frankfurt am Main: Fischer.
                    
               
                  Bastian Mathieu / Heymann Sebastian / Jacomy Mathieu (2009): 
                        „Gephi. An open source software for exploring and manipulating networks“, 
                        in: 
                        International AAAI Conference on Weblogs and Social Media.
                    
               
                  Dennerlein, Katrin (2009): 
                        Narratologie des Raumes. 
                        Berlin: de Gruyter.
                    
               
                  Dennerlein, Katrin (2011): 
                        „Raum“,
                        in: Matías Martínez (ed.): 
                        Handbuch Erzählliteratur: Theorie, Analyse, Geschichte. 
                        Stuttgart / Weimar: Metzler 158–165.
                    
               
                  Gregory, Ian / Hardie, Andrew (2011): 
                        „Visual GISting: bringing together corpus linguistics and Geographical Information Systems“, 
                        in; 
                        LLC 26: 297–314. 
                    
               
                  Gregory, Ian / Cooper, David / Hardie, Andrew / Rayson, Paul (2015): 
                        „Spatializing and Analyzing Digital Texts. Corpora, GIS, and Places“, 
                        in: David Bodenhamer / John Corrigan / Trevor Harris: 
                        Deep Maps and Spatial Narratives. 
                        Bloomington: Indiana University Press 150–178. 
                    
               
                  Hamp, Birgit / Feldweg, Helmut (1997): 
                        „GermaNet - a Lexical-Semantic Net for German“, 
                        in: 
                        Proceedings of the ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications.
                        Madrid.
                    
               
                  Henrich, Verena / Hinrichs Erhard (2010): 
                        „GernEdiT - The GermaNet Editing Tool“, 
                        in: 
                        Proceedings of LREC 2010 2228–2235.
                    
               
                  Leidner, Jochen / Lieberman, Michael (2011): 
                        „Detecting Geographical References in the Form of Place Names and Associated Spatial Natural Language“, 
                        in: 
                        SIGSPATIAL Special 3: 5–11.
                    
               
                  Levin, Beth (1993): 
                        English Verb Classes and Alternations. 
                        University of Chicago Press.
                    
               
                  Lotman, Juri (1972): 
                        Die Struktur literarischer Texte. 
                        München: Fink.
                    
               
                  Moretti, Franco (1998): 
                        Atlas of the European novel. 1800-1900. 
                        London / New York: Verso.
                    
               
                  Nünning, Ansgar (2009): 
                        „Formen und Funktionen literarischer Raumdarstellung: Grundlagen, Ansätze, narratologische Kategorien und neue Perspektiven“, 
                        in: Wolfgang Hallet / Birgit Neumann (eds.): 
                        Raum und Bewegung in der Literatur: Die Literaturwissenschaften und der Spatial Turn. 
                        Bielefeld: Transcript 33–52.
                    
               
                  Piatti, Barbara (2008): 
                        Die Geographie der Literatur. Schauplätze, Handlungsräume, Raumphantasien. 
                        Göttingen: Wallstein.
                    
               
                  Piatti, Barbara / Reuschel, Anne-Kathrin / Hurni, Lorenz (2011): 
                        „A Literary Atlas of Europe – Analysing the Geography of Fiction with an Interactive Mapping and Visualisation System“, 
                        in: 
                        Proceedings of the 25th International Cartographic Conference. Paris.
                    
               
                  Ryan, Marie Laure (1980): 
                        „Fiction, Non-Factuals, and Minimal Departure“, 
                        in:
                        Poetics 8: 403–422.
                    
               
                  Schumacher, Helmut (1986): 
                        Verben in Feldern: Valenzwörterbuch zur Syntax und Semantik deutscher Verben. 
                        Berlin / New York: de Gruyter Verlag,
                    
               
                  Soja, Edward (1990): 
                        Postmodern Geographies: The Reassertion of Space in Critical Social Theory.
                        London / New York: Verso.
                    
               
                  WebLicht (2012): 
                        CLARIN-D/SfS-Uni. Tübingen 2012. WebLicht: Web-Based Linguistic Chaining Tool. 
                        https://weblicht.sfs.uni-tuebingen.de/ [letzter Zugriff 1. Dezember 2016]
                    
               
                  Zoran, Gabriel (1984): 
                        „Towards a theory of space in narrative“, 
                        in: 
                        Poetics Today 5: 309–335.
                    
            
         
      
   



      
         Mit der Thematik der digitalen Nachhaltigkeit sind Gedächtnisinstitutionen, die sich die umfassende digitale Sicherung und Erschließung des Kulturerbes zum Ziel setzen, gleichermaßen konfrontiert wie Forschungsprojekte, bei denen digitale Daten generiert, computergestützt ausgewertet und in Forschungsdatenbanken zugänglich gemacht werden. Dabei liegt das Hauptaugenmerk nicht nur auf der Langzeitsicherung und Archivierung; die größeren Herausforderungen stellen sich vor allem bei den Aktualisierungen der Datenmodelle, Analysemethoden und Präsentationsformen, um die neuen Werkzeuge der Digital Humanities bestmöglich einsetzen zu können. Die Überlegungen im Vorfeld derartiger Relauncharbeiten bewegen sich erfahrungsgemäß zwischen vorsichtiger Adaptierung und radikalem Umbau der vorliegenden Datenarchitektur. Dass dabei alle vorhandenen Informationen verlustfrei übertragen werden sollen, versteht sich von selbst. Der folgende Beitrag möchte die Transformation von Daten eines Langzeitprojekts in eine neue Datenarchitektur für eine Bilddatenbank vorstellen, bei der eine Graphdatenbank zum Einsatz kommt:
         Am Institut für Realienkunde des Mittelalters und der frühen Neuzeit (IMAREAL), einem interdisziplinär ausgerichteten Forschungsinstitut, das Teil der Universität Salzburg ist, wird die materielle Kultur des Mittelalters und der frühen Neuzeit untersucht. Bildquellen bilden dabei neben Schriftquellen und überlieferten Objekten die Grundlagen der Analysen. Mit dem Aufbau der Bilddatenbank REALonline wurde am IMAREAL in den 1970ern auf der Grundlage der von Manfred Thaller speziell für die Anforderungen der historischen Grundwissenschaften entwickelten Datenbanksysteme begonnen – zunächst 
                Descriptor und in weiterer Folge 
                
               Κλειώ
             (Thaller 1980 u. 1989). Der Datenbestand von REALonline wurde seither und wird weiterhin kontinuierlich erweitert, damit dargestellte Dinge und ihre Kontexte erforscht werden können. Die Datenbank ist seit 2002 unter 
                http://tethys.imareal.sbg.ac.at/realonline online verfügbar (Matschinegg 2004). Anhand der Datenbank ist es möglich, die Bedeutung und Funktion der materiellen Kultur im Bilddiskurs zu untersuchen: Welche Objekte waren zu welchen Zeiten in welchen Gesellschaften und Kontexten als visuelle „Requisiten“ gegenwärtig oder vor- und damit auch darstellbar? Wie wurden Dinge im Bild verhandelt und welche Rolle nehmen sie innerhalb von ins Bild überführten Narrativen ein? 
            
         Um diese Fragen beantworten zu können, wurde am IMAREAL entschieden, neben den Metadaten zum Werk bzw. Bildträger systematisch 
                alle im Bild dargestellten Elemente auszuzeichnen (Abb. 1). Im Gegensatz zu anderen Bilddatenbanken wird das Dargestellte nicht nur mit einigen wenigen Schlagwörtern erfasst. Diesem Umstand ist es zu verdanken, dass die in REALonline erhobenen Daten sowohl im Rahmen von interdisziplinären Forschungen zur materiellen Kultur ausgewertet werden können, als auch in unterschiedlichen geisteswissenschaftlichen Untersuchungskontexten und für Kulturerbedokumentationen eine wertvolle Ressource darstellen.
            
         
            
         
         Abb. 1: Erfassungsschema der Metadaten in REALonline
         Im Modell für die Erfassung der dargestellten Entitäten im Bild werden folgende Informationen erhoben: Für Subjekte werden neben dem Subjektnamen die Kategorien Geschlecht, Beruf bzw. Stand und Gestik erfasst. Bei Objekten werden der Objektname und die Informationen zu Farbe, Material und Form erhoben. Weiters wird die Struktur dieser Metadaten zu den Bildinhalten festgehalten und kann damit im Rahmen von Analysen abfragbar gemacht werden: Direkte Subjekt-Objekt- bzw. Objekt-Objekt-Relationen werden erfasst, um am Körper getragene bzw. von Figuren gehaltene Objekte zu dokumentieren oder einen Bezug zwischen einzelnen dargestellten Dingen (etwa ein auf einem Tisch stehender Krug) in den Daten abbilden zu können. Darüber hinaus können sowohl Körperteile als auch Teile von Objekten als Metadaten zum Dargestellten gespeichert werden (Jaritz 1993: 23-43).
         Graphdatenbanken eignen sich u.a. besonders dafür, die vielfältigen Beziehungen zwischen Personen oder Personen und Gegenständen bzw. Ereignissen sowie auch zwischen den Dingen untereinander möglichst flexibel abzubilden und sind nun auch in der historischen Forschung im Kommen; als Software wird oft Neo4j eingesetzt (Raspe 2014, Kaufmann & Andrews 2015, Kuczera 2015). Die verzweigte Struktur der erfassten Metadaten in REALonline ist einer der Hauptgründe, warum für die neue Datenarchitektur ein property-graph-Modell gewählt wurde. Ein weiterer Leitgedanke war, dass das Beziehungsnetz von Subjekten, Objekten und Handlungen in mittelalterlichen und frühneuzeitlichen Bildern anhand des Modells eines verzweigten Graphen besser veranschaulicht werden kann als in einer langen Liste mit Metadaten. Die Graphdatenbank bietet im Fall von REALonline sowohl bei der Präsentation für die Nutzer_innen im Frontend, für die Abfrage und Darstellung der Abfrageergebnisse als auch für die Eingabe der Daten im Backend (siehe Abb. 2) eine Verbesserung der Usability gegenüber dem zuvor verwendeten hierarchischen Datenbankmodell.
         
            
         
         Abb. 2: Screenshot des Graphen zum Dargestellten im Bild (Backend)
         In unserem Projekt hat sich die Kombination von Neo4j für die Modellierung und Abfrage der „beziehungsrelevanten“ Daten mit einer NoSQL-Mongo-Dokumentendatenbank angeboten (Abb. 3). Diese Lösung baut einerseits auf dem in der Praxis bereits bewährten softwareseitigen Ineinandergreifen bei der semantischen Transformation der Informationen auf und bietet gleichzeitig die Möglichkeit zur Speicherung und Abfrage von werkgeschichtlich wie auch projektgeschichtlich relevanten Informationen zu den einzelnen Bilddokumenten, die im Verlauf dieses Langzeitprojektes erhoben wurden und laufend erweiterbar bleiben sollen.
         
            
         
         Abb. 3: Datenarchitektur von REALonline
         Im Vortrag möchten wir aber auch auf die Herausforderungen hinweisen, denen wir uns im Zuge des Entwurfs der Datenarchitektur von REALonline stellen mussten: So war etwa in der Struktur des hierarchischen Datenmodells die Information zur dargestellten Handlung auf derselben Ebene angesiedelt wie die Entitäten Subjekt und Objekt. Beim Datenexport aus der bis dato verwendeten 
                
               Κλειώ
            -Datenbank und dem Import in Neo4j konnte – nachdem in diesem Fall keine automatisierten Zuweisungen der Handlung zu Personen bzw. Objekten möglich waren – dieser Umstand nur in das neue Datenmodell mitübernommen werden. Mit der Entscheidung für eine Graphdatenbank ist dennoch gewährleistet, dass in einem weiteren Schritt Informationen, wie jene zur dargestellten Handlung, statt in Knoten in die Kanten des Graphen gelegt werden können und damit die Struktur von RDF-Triples (Subjekt-Prädikat-Objekt) bekommen. 
            
         Aufgrund der zeitintensiven Datenerhebung war ein wichtiger Aspekt des Relaunchs, die Dateneingabe so effizient wie möglich zu gestalten. Der Beitrag wird die gefundene Lösung präsentieren. Langfristig gesehen sollte versucht werden, den Zeitaufwand für die Erhebung von Metadaten zu den auf historischen Bildern dargestellten Elementen zu minimieren. Daher möchten wir die in REALonline während mehr als 40 Jahren erhobenen Informationen als Trainingsdaten in transdisziplinäre Projekte zwischen den Geisteswissenschaften und der Computer Vision – insbesondere zur (semi-)automatisierten Bilderkennung – einbringen, so dafür Fördermittel eingeworben werden können.
         Mit dem Relaunch von REALonline kann die Menge der erhobenen Metadaten zum im Bild Dargestellten (aktuell sind innerhalb von 23316 Datensätzen 1.165562 Begriffe dazu erfasst) besser zugänglich gemacht werden: Abfragen der Graphdatenbank und Visualisierungen (z.B. mit Software wie 
                gephi–The Open Graph Viz Platform oder 
                yEd graph editor) dieser Ergebnisse können komplexe Zusammenhänge innerhalb der Bilddetails aufdecken oder Aufschlüsse zu Mustern sowie „Ausreißern“ in unterschiedlichen Samples liefern, die nicht nur als Resultate statistischer Auswertungen verstanden werden sollen, sondern vor allem dazu dienen können, neue Fragen in der (interdisziplinären) Forschung anzustoßen. Beispielsweise wurde in 154 Datensätzen das Bildthema „Geißelung Christi“ erfasst. Bei der Erschließung dieser Datensätze wurden wiederum 516 Objekte verzeichnet, die von Figuren im Bild in der Hand gehalten werden. Die Visualisierung (Abb. 4) beschränkt sich auf jene Objekte, die nur einmal vorkommen (gelb) und die ihnen übergeordneten Thesauruskategorien (grün, dunkelrot). Während die meisten Objekte dem gängigen Narrativ „Geißelung“ zugeordnet werden können, sind die Objekte 
                Münze und 
                Geldbeutel nur über einen Konnex zum mittelalterlichen Drama erklärbar (Nicka 2014, 280–282): Die Darstellung einer Bezahlung der Geißler Christi, die nur auf einem Flügelaltar im niederösterreichischen Pöggstall im Bild festgehalten wurde, kennen wir ansonsten nur aus Passionsspielen, wo jüdische Protagonisten negativ gekennzeichnet werden, indem sie den Gerichtsknechten Geld geben, um besonders fest mit den Ruten zuzuschlagen (siehe auch Abb. 2). 
            
         
            
         
         Abb. 4: Visualisierung der dargestellten Objektbegriffe und Körperbezeichnungen aus den Bildbeschreibungen in ihrer Zuordnung zur jeweiligen Thesauruskategorie
         Abschließend bleibt zu erwähnen, dass sich mit der Notwendigkeit, eine gut eingeführte, aber in ihren technischen Funktionalitäten nicht mehr zeitgemäße Bilddatenbank zu modernisieren, auch die Chance zur besseren Nutzung der umfangreichen Datenbestände verbinden lässt. Im Zuge der Relaunchvorarbeiten haben wir die Konzepte und Lösungsansätze aufgegriffen, die gegenwärtig in den Digital Humanities diskutiert und getestet werden. Wir haben die Umsetzung in enger Zusammenarbeit mit den Grazer Entwicklerfirmen complement.at und zedlacher.net realisiert. Der Beitrag gibt einen knappen Überblick über die wichtigsten Entscheidungsfindungsprozesse sowie die Schwierigkeiten und Potentiale, die bei der Überführung in die neue Datenarchitektur und die gewählte Frontend-Lösung entstanden sind. Der Aspekt der Nachhaltigkeit hat dabei von Anfang an eine große Rolle gespielt; sowohl bei der Erhaltung aller vorhandenen Informationen als auch bei der nachhaltigen Nutzbarkeit der erhobenen Daten. So ist die Zitierbarkeit der Daten über einen PID (persistent identifier) mit einem handle gewährleistet und die Metadaten werden mit einer 
                Creative Commons by-nc-sa 4.0-Lizenz zur Verfügung gestellt. Die neue Online-Version von REALonline wird gegenwärtig getestet und optimiert und 2017 freigeschaltet.
            
      
      
         
            
               Bibliographie
               
                  Jaritz, Gerhard (1993): 
                        Images: A Primer of Computer-Supported Research with Κλειώ IAS. Halbgraue Reihe zur historischen Fachinformatik A 22. 
                        St. Katharinen: Scripta Mercaturae Verlag. 
                    
               
                  Kaufmann, Sascha / Andrews, Tara Lee (2016): 
                        „Bearbeitung und Annotation historischer Texte mittels Graph-Datenbanken am Beispiel der Chronik des Matthias von Edessa“,
                        in:
                        DHd 2016: Modellierung - Vernetzung - Visualisierung 176–178
                        http://dhd2016.de/boa.pdf [letzter Zugriff 20. August 2016].
                    
               
                  Kuczera, Andreas (2015): 
                        „Graphdatenbanken für Historiker. Netzwerke in den Registern der Regesten Kaiser Friedrichs III. mit neo4j und Gephi“, 
                        in: 
                        Mittelalter. Interdisziplinäre Forschung und Rezeptionsgeschichte, 5. Mai 2015, 
                        http://mittelalter.hypotheses.org/5995 (ISSN 2197-6120) [letzter Zugriff 20. August 2016].
                    
               
                  Matschinegg, Ingrid (2004): 
                        „REALonline – IMAREAL's Digital Image-Server“, 
                        in: 
                        [Enter the Past]. The E-way into the Four Dimensions of Cultural Heritage. CAA 2003 | Computer Applications and Quantitative Methods in Archaeology | Proceedings of the 31st Conference, Vienna, Austria, April 2003 (BAR International Series 1227). 
                        Oxford: archaeopress, 214-216.
                    
               
                  Nicka, Isabella (2014): 
                        „Interfaces. Berührungszonen von Transzendenz und Immanenz im spätmittelalterlichen Sakralraum“, 
                        in: Meyer, Marion / Klimburg-Salter, Deborah (eds.):
                        Visualisierungen von Kult. 
                        Wien / Köln / Weimar: Böhlau, 260-293, Abb. auf 438-444.
                    
               
                  Nicka, Isabella (im Erscheinen): 
                        „REALonline–Explore and Find Out. Wohin führt das Digitale die Kunstgeschichte?“,
                        Beitrag zum Tagungsband der vom 6.-8. Nov. 2015 in Wien abgehaltenen Konferenz „Newest Art History“. Wohin geht die jüngste Kunstgeschichte?
               
               
                  Raspe, Martin (2014): 
                        Zuccaro. Ein modernes, konfigurierbares Informationssystem für die Geisteswissenschaften.
                        http://zuccaro.biblhertz.it/dokumentation/zuccaro [letzter Zugriff 20. August 2016].
                    
               
                  Thaller, Manfred (1980): 
                        „Descriptor: Probleme der Entwicklung eines Programmsystems zur computerunterstützten Auswertung mittelalterlicher Bildquellen“, 
                        in: 
                        Europäische Sachkultur des Mittelalters: Gedenkschrift aus Anlaß des 10jährigen Bestehens des Instituts für mittelalterliche Realienkunde Österreichs (Veröffentlichungen des Instituts für mittelalterliche Realienkunde Österreichs 4 / Sitzungsberichte der Akademie der Wissenschaften, Phil.-Hist. Klasse 374). 
                        Wien: Verlag der Österreichischen Akademie der Wissenschaften, 167–194.
                    
               
                  Thaller, Manfred (1989): 
                        Κλειώ.
                        Ein Datenbanksysten. Halbgraue Reihe zur historischen Fachinformatik B1.
                        St. Katharinen: Scripta Mercaturae Verlag.
                    
            
         
      
   



      
         
            Einleitung
            Im vorliegenden Abstract stellen wir eine Methode sowie erste Ergebnisse der Analyse von Entitäten-Assoziationen realer Leserinnen und Leser vor.
            Literaturwissenschaftliche Rezeptions-, Lese- und Lesertheorien gehen seit ihren hermeneutischen und wirkungsästhetischen Anfängen (Schleiermacher 1838, insb. 309f.; Iser 1976) von professionellen (Dijkstra 1994), informierten (Fish 1970, 86), Modell- (Eco 1979) oder sogar idealen (Schmid 2005) Lesern aus (vgl. Willand, 2014). Diesen wird die Kompetenz zugeschrieben, idealerweise sämtliche Textmerkmale referentialisieren zu können, wobei je nach literaturtheoretischer Provenienz unterschiedliche Kontexte die Grundlage der Zuschreibungen an den Text bilden. Dazu gehören u.a. Informationen über den Autor oder über die sozialhistorischen Bedingungen der Textproduktion, über die Rezeptionsbedingungen, über Vorgänger- oder zeitgenössische Texte oder über Wissen aus dem Bereich der Literaturwissenschaftlerin bzw. des Lesers selbst. 
            An bestimmte Wissensbestände dieser 
                    realen Leserinnen und Leser literarischer Texte können wir uns durch eine computergestützte empirische Analyse von Rezeptionszeugnissen aus sozialen Medien annähern. Konkret ist unser Ziel die Rekonstruktion und Analyse der von literarischen Texten ausgelösten Assoziationen. Dabei beschränken wir uns auf die Assoziationen, die reale oder fiktive Entitäten betreffen, also etwa Personen des öffentlichen Lebens oder Figuren aus fiktionalen Werken.
                
            Die Plattform Goodreads bietet Leserinnen und Lesern die Möglichkeit des freien schriftlichen Austauschs über literarische Texte in einer großen Community. 55 Mio. Mitglieder haben bis 2017 über 50 Mio. Reviews geschrieben, wobei die Besprechungen die Inhalte der Bücher selbst und nicht - wie etwa bei Verkaufsplattformen wie Amazon - die Distribution, den Preis o.ä. fokussieren (Piper et al. 2015). 
         
         
            Verarbeitung
            Als Grundlage unserer Analysen  wurden die Reviews in einer lokalen Datenbank gespeichert. 
            Die Datenbank enthält 1,3 Millionen englischsprachige Reviews zu 5.481 besprochenen Texten. Die Reviews umfassen insgesamt etwa 150 Mio. Tokens, d.h. uns steht eine große Datenmenge zur Extraktion der Entitäten zur Verfügung. In einem ersten Schritten wurden die Reviews bereinigt: HTML-Tags wurden entfernt und Wiederholungen von mehr als dreimal dem gleichen Zeichen oder Wort auf drei reduziert. 
            Zur Extraktion der Entitäten aus den Reviews haben wir den Stanford Named Entity Recognizer (Finkel et al., 2005) verwendet. Der Tagger klassifiziert die gefundenen Entitäten in mehrere Klassen. Für uns ist die Klasse „PERSON“ relevant, da diese alle gefundenen Entitäten von Personen enthält.
            Im nächsten Schritt disambiguieren wir die extrahierten Entitäten, da z.B. ein Name wie “Harry” auf viele mögliche Träger des Namens verweisen kann. Mit Hilfe von UKB (Agirre et al., 2009) und UKB-wiki (Agirre et al., 2015) können den Entitäten Wikipedia-Seiten zugeordnet werden, welche die möglichen Entitäten repräsentieren. Für diese Disambiguierung verwendet UKB den PageRank-Algorithmus (Page et al. 1999), der Dokumente nach ihrem Verlinkungsgrad bewertet. Sobald Namen wie „Ron“ und „Dumbledore“ im selben Kontext erwähnt werden, wird die Wahrscheinlichkeit größer, dass mit “Harry” 
                    Harry Potter, mit “Ron” 
                    Ron Weasly und mit “Dumbledore” 
                    Albus Dumbledore aus der Romanreihe 
                    Harry Potter gemeint sind, weil diese Entitäten aus dem selben Kontext kommen und dies in der Wissensbasis Wikipedia durch Verlinkungen explizit ablesbar und quantifizierbar ist.
                
            UKB-wiki stellt einen herunterladbaren Graphen zur Verfügung, der Wikipedia-Seiten und Links auf andere Wikipedia-Seiten repräsentiert. In einem mitgelieferten Wörterbuch sind Entitäten mit allen möglichen Entitäten (Verweise auf Wikipedia Seiten) aufgeführt. 
            Die auf diese Weise gewonnen Wikipedia-Einträge wurden anschließend hinsichtlich des ontologischen Status der referenzialisierten Entität kategorisiert, also ob es sich um eine reale Person oder fiktionale Figur handelt. Dazu wurde die Wissensbasis DBpedia
                     verwendet, die die Daten aus Wikipedia strukturiert und maschinenlesbar kodiert. Da die Disambiguierung Wikipedia-Einträge liefert, können wir anhand dieser die auf den zugehörigen DBpedia-Eintrag zugreifen. Über DBpedia lassen sich neben ontologischen Kategorien  auch andere Eigenschaften extrahieren, die für eine Analyse ggf. interessant sind, etwa Geschlecht oder Relationen zu anderen Figuren.
                
            Die extrahierten Daten werden zunächst als Tabelle gespeichert und erlauben somit eine flexible weitergehende Verarbeitung, etwa in einem Netzwerk. Eine Zeile der Tabelle besteht aus dem Werktitel, der disambiguierten Entität (Verweis auf Wikipedia Seite), einer Liste der extrahierten Entitäten aus den Reviews, einer Liste von Review-IDs, um nachvollziehen zu können in welchen Reviews der Name erwähnt wird, der Anzahl der Erwähnungen und der Angabe ob es sich um eine Figur handelt oder nicht.
            
               
                  Titel
                  Disambiguierte Entität (Verweis auf Wikipedia Seite)
                  Extrahierte Entität
                  Review IDs
                  Anzahl der Erwähnungen
                  Handelt es sich um eine fiktionale Figur?
               
               
                  The Hound of the Baskervilles
                  Agatha_
                            Christie
                  christie, agatha_
                            christie, agatha_
                            christy
                  4707841
                            20, …, 1886
                            08568
                  20
                  False
               
               
                  The Hound of the Baskervilles
                  Spock
                  spock
                  429714
                            73
                  1
                  True
               
               
                  The Hound of the Baskervilles
                  Robert_Downey,_Jr.
                  robert_downey_jr, robert_downey
                  107754
                            3609, …, 125
                            0976986
                  18
                  False
               
               
                  The Hound of the Baskervilles
                  Ann_Radcliffe
                  ann_radcliffe
                  435380
                            655
                  1
                  False
               
            
            
               Tabelle 1: Auszug aus den extrahierten Daten. Die extrahierten Entitäten stammen aus den Reviews zu
                     The Hound of the Baskervilles.
                
            
               Zwischenergebnisse
               Um ein exemplarisches Resultat zu präsentieren, haben wir Reviews zu “The Hound of the Baskervilles” (deutsch: “Der Hund von Baskerville”) analysiert. Unter den häufig erwähnten Entitäten finden sich erwartungsgemäß Sherlock Holmes, Dr. Watson, sowie der Autor Arthur Conan Doyle. Weitere häufig erwähnte Figuren aus der fiktionalen Welt des Sherlock Holmes' sind James Mortimer und Charles Baskerville. Aber auch Professor Moriarty wird häufig erwähnt, obwohl er in diesem Buch der Sherlock-Reihe gar nicht auftaucht. Das System erzeugt jedoch auch Fehler. Beispielsweise wird der Antagonist Stapleton zwar sehr oft erwähnt, da zu ihm aber kein eigener Wikipedia-Eintrag existiert, wird er fälschlicherweise mit dem Footballspieler Frank Stapleton verknüpft. Henry Baskerville, der Sohn von Charles und Erbe des Anwesens, wird im Buch fast durchgehend als Sir Henry bezeichnet, und kommt mit diesem Namen ebenfalls häufig in den Reviews vor. Da auch für ihn kein eigener Wikipedia-Eintrag existiert und der Name Henry extrem mehrdeutig ist, werden eine Reihe klar falscher Entitäten verknüpft: Henry II. von Frankreich; Henry County (Alabama); oder Henry I. von England.
               Bemerkenswert sind insbesondere jedoch die referenzialisierten extra-textuellen Entitäten, also diejenigen, die nicht aus der fiktionalen Welt Sherlocks stammen. Es finden sich etwa 
                        Hercule Poirot und 
                        Agatha Christie unter den erwähnten Entitäten, was als klares Zeichen dafür gesehen werden kann, dass die Leserinnen und Leser den Text vor dem Hintergrund eines starken Gattungsbewusstseins rezipieren. Dafür spricht auch, dass mit 
                        Benedict Cumberbatch, 
                        Robert Downey, Jr., 
                        John Barrymore und 
                        Jeremy Brett gerade die Schauspieler unter den assoziierten Referenzen vertreten sind, die in einer der vielen Verfilmungen die Rolle des Sherlock Holmes verkörpert haben.
                    
            
            
               Fehleranalyse
               Das am häufigsten auftretende Problem ist das Fehlen eines Wikipedia-Eintrages für eine Figur. In der englischsprachigen Wikipedia sind fiktionale Figuren zwar nicht per se davon ausgeschlossen -- Richtschnur hier ist deren “Notability”. Viele Figuren sind jedoch nur auf den Einträgen des entsprechenden Werks erwähnt. Da der Algorithmus nicht in der Lage ist, 
                        keinen Eintrag zu liefern, wird in solchen Fällen eben ein anderer Eintrag verwendet, auch wenn dieser relativ weit entfernt sein mag. Eine technische Lösung wäre sicher, nur ab einem gewissen Schwellwert eine Disambiguierung vorzunehmen, und die nicht-disambiguierten Einträge zumindest als solche erkennen zu lassen. Eine andere Möglichkeit läge in der (zusätzlichen) Verwendung von Literaturlexika, die (womöglich) eine größere Abdeckung zu fiktionalen Figuren aufweisen. Beide Optionen werden wir in zukünftigen Arbeiten genauer untersuchen. 
                    
               Da es sich bei den Reviews letztlich um Inhalte aus einem sozialen Medium handelt, kommt es auch vor, dass Namen falsch geschrieben werden oder gar der gesamte Text schriftsprachliche Konventionen übergeht. Prima vista sind diese Fälle im Vergleich zu Buchrezensionen zwar häufig anzutreffen, wir können das Problem aber umgehen, indem wir nur diejenigen Erwähnungen berücksichtigen, die mehr als einmal vorkommen. Festzuhalten bleibt aber ebenfalls, dass die Texte im Vergleich zu z.B. Twitter-Daten deutlich sauberer sind.
               Eine weitere mögliche (jedoch noch nicht tatsächlich beobachtete) Fehlerquelle liegt in der Natur des PageRank-Algorithmus: Wenn eine Figur in einem Werk existiert, ein Leser oder eine Leserin jedoch explizit z.B. eine Person des öffentlichen Lebens mit dem gleichen Namen erwähnt, wird der Algorithmus diese Erwähnung eher der Figur zuschlagen, da diese dichter mit anderen Figuren verknüpft ist. 
            
         
         
            Auswertung als Netzwerk
            Die oben extrahierten Daten erlauben Auswertungen auf vielfältige Weise. Exemplarisch konzentrieren wir uns hier auf eine Form, in der von Lesern zugeschriebene Gemeinsamkeiten zwischen literarischen Texten untersucht werden. Die Texte und die ihnen zugeschriebenen Assoziationen werden dabei als Knoten in einem Netzwerk repräsentiert. Ein Text ist also mit allen ihm zugeschriebenen Assoziationen verbunden, wobei das Gewicht der Kante die Anzahl der Reviews angibt, in denen eine bestimmte Assoziation auftaucht. 
            Durch diesen Aufbau ergeben sich Kerneigenschaften des Netzwerkes, die bei der Analyse zu beachten sind: Ein Teil der erwähnten Entitäten sind 
                    intratextuelle Referenzen, d.h. Figuren aus dem jeweiligen Text selbst (Veldhues, 1995). Auch wenn diese keine 
                    intertextuellen Assoziationen und damit nur sekundäres Extraktionsziel sind, behandeln wir sie als gleichwertige Assoziationen
                    . 
                
            Figuren, die in mehr als einem Werk auftauchen (z.B. 
                    Sherlock Holmes oder 
                    Harry Potter) bilden eine hoch gewichtete Verbindung zwischen den Texten einer literarischen Reihe, wobei Reihen durch die von ihnen geteilte fiktionale Welt markiert sind. Als gemeinsamer Assoziationsraum sind sie aufgrund der hohen Gewichtung auch angemessen im Netzwerk repräsentiert.
                
            Durch die gemeinsame Darstellung der Werke und assoziierten Entitäten ergeben sich -- bei Auswahl eines geeigneten Layout-Algorithmus z.B. in Gephi
                     -- eng zusammenhängende Gruppen von Werken. Das hier exemplarisch angeführte Resultat eines engen Zusammenhangs repräsentiert jedoch nicht bestimmte Texteigenschaften selbst, sondern lediglich von Leserinnen und Lesern gemeinsam gemachte Zuschreibungen an diese Texte.
                
            Das hier beschriebene Netzwerk wird im Zuge der Konferenz frei zugänglich gemacht.
            
               
                  
                  Abbildung 1: Assoziationen zu Conan Doyles The Hound of the Baskervilles, extrahiert aus den Reviews von Benutzern. Die Abbildung zeigt zur Illustration sämtliche assoziierte Entitäten, unabhängig von der Häufigkeit.
               
            
         
         
            Nächste Schritte
            Durch den Zugriff auf bisher undenkbar große Rezeptionsdatenmengen erhält die empirische Leseforschung einen sie fundamental erweiternden Impetus, war sie methodisch betrachtet bisher überwiegend auf Fragebögen
                     und peripheriephysiologische Messungen angewiesen, jüngst gestützt durch bildgebende Verfahren. Computerlinguistische Methoden der Sprach- und Korpusverarbeitung versprechen nicht nur die Analyse unlesbarer Mengen an Rezeptionszeugnissen, sondern auch eine Modellierung leserattribuierter Kontexte literarischer Texte und somit einen ersten Einblick in die bisher unbeantwortete Frage, mit welchem Vorwissen echte Leser eigentlich lesen.
                
            In diesem Sinne präsentiert das eingereichte Paper erste, jedoch bereits substanzielle Ergebnisse. 
            Die nächsten Schritte leiten sich direkt aus der oben diskutierten Fehleranalyse ab. Zum einen soll die Wissensbasis um fiktionale Figuren aus den Werken erweitert werden (was z.B. über 
                    named entity recognition über den Volltexten machbar wäre). Zum anderen soll der Algorithmus in die Lage versetzt werden bestimmte (fehlerhafte) Zuweisungen zurückzuweisen, etwa mit einem zu definierenden 
                    threshold.
                
         
      
      
         
            http://wiki.dbpedia.org/
            Das Filtern von innertextuellen Figuren ist technisch möglich (Beck, 2017), aber zeitaufwändig und für die hier vorgestellte Nutzung als Explorationswerkzeug letztlich unnötig.
            https://gephi.org
            Groeben 1979; Baurmann 1981; Funke 2003; Christmann u. Schreier 2003; Wübben 2009 u.v.m.
         
         
            
               Bibliographie
               
                  Agirre, Eneko / Soroa, Aitor (2009): “Personalizing PageRank for Word Sense Disambiguation”, in: Proceedings of the 12th conference of the European chapter of the Association for Computational Linguistics (EACL-2009). Athens, Greece.
                    
               
                  Agirre, Eneko / Barrena, Ander / Soroa, Aitor (2015): Studying the Wikipedia Hyperlink Graph for Relatedness and Disambiguation. http://arxiv.org/abs/1503.01655
                    
               
                  Beck, Jens (2017): How do People Read Literature? - Detection and Identification of Names in Book Reviews. Bachelor’s thesis, Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart.
                    
               
                  Baurmann, Jürgen (1981). „Textrezeption empirisch. Wege zu einem ziel, behelfsbrücken oder holzwege?". Rezeptionspragmatik. Beiträge zur Praxis des Lesens. Uni-Taschenbücher. Band 1026. Hrsg. v. Gerhard Köpf, 201–218. München.
                    
               
                  Christmann, Ursula / Margrit Schreier (2003). „Kognitionspsychologie der Textverarbeitung und Konsequenzen für die Bedeutungskonstitution literarischer Texte". Regeln der Bedeutung. Zur Theorie der Bedeutung literarischer Texte. Revisionen. Hrsg. v. Fotis Jannidis, Gerhard Lauer, Matías Martínez & Simone Winko, 246–284. Berlin.
                    
               
                  Dijkstra, Katinka (1994): Leseentscheidung und Lektürewahl. Empirische Untersuchungen über Einflussfaktoren auf das Leseverhalten. Berlin.
                    
               
                  Dimitrov, Stefan / Zamal, Faiyaz / Piper, Andrew / Ruths, Derek (2015): “Goodreads vs Amazon: The Effect Of Decoupling Book Reviewing And Book Selling", in: International Conference on Web and Social Media (ICWSM-14).
                    
               
                  Eco, Umberto (1979): The Role of the Reader. Explorations in the Semiotics of Texts. Bloomington, IN.
                    
               
                  Finkel, Jenny Rose / Grenager, Trond / Manning, Christopher (2005): “Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling”, in: 
                        Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370.
                    
               
                  Fish, Stanley E. (1970): „Literature in the Reader: Affective Stylistics“, in: 
                        New Literary History 1(2): 123–162.
                    
               
                  Funke, Mandy (2003). „Das Abenteuer der Fragebögen. Aspekte zur empirischen Wirkungsforschung in der DDR". Wissenschaft und Systemveränderung. Rezeptionsforschung in Ost und West – Eine konvergente Entwicklung? Euphorion. Band 44. Hrsg. v. Wolfgang Adam, Holger Dainat & Gunther Schandera, 119–126. Heidelberg.
                    
               
                  Groeben, Norbert (1979). „Zur Relevanz empirischer Konkretisationserhebungen für die Literaturwissenschaft". Empirie in Literatur- und Kunstwissenschaft. Grundfragen der Literaturwissenschaft. Hrsg. v. Siegfried J. Schmidt, 43–82. München.
                    
               
                  Iser, Wolfgang (1976). Der Akt des Lesens. Theorie ästhetischer Wirkung. Band 636. München.
                    
               
                  Page, Lawrence / Brin, Sergey / Motwani, Rajeev / Winograd, Terry (1999): “The PageRank Citation Ranking: Bringing Order to the Web”
                        , technical Report. Stanford InfoLab.
                    
               
                  Schleiermacher, Friedrich (1838): Hermeneutik und Kritik mit besonderer Beziehung auf das Neue Testament. Aus Schleiermachers handschriftlichem Nachlasse und nachgeschriebenen Vorlesungen herausgegeben von Friedrich Lücke. In: Friedrich Schleiermacher’s sämmtliche Werke. Berlin: Reimer.
                    
               
                  Schmid, Wolf (2005): Elemente der Narratologie. Narratologia. Band 8. Berlin.
                    
               
                  Veldhues, Christoph (1995): "Gleich- und Gegenüberstellung".Intratextuelle und intertextuelle Bedeutung in der Literatur. Zeitschrift für französische Sprache und Literatur 40/3 (1995), 243-267.
                    
               
                  Willand, Marcus (2014): Lesermodelle und Lesertheorien. Historische und systematische Perspektiven. Narratologia. Band 41. Berlin.
                    
               
                  Wübben, Yvonne (2009). „Lesen als Mentalisieren? Neuere kognitionswissenschaftliche Ansätze in der Leseforschung". Literatur und Kognition. Bestandsaufnahmen und Perspektiven eines Arbeitsfeldes. Poetogenesis. Band 6. Hrsg. v. Martin Huber & Simone Winko, 29–44. Paderborn.
                    
            
         
      
   



      
         Dieser Posterbeitrag veranschaulicht die Interaktion zwischen computerlinguistischen Methoden und Regestenforschung. Es wird eine Anwendung vorgestellt, die bereits in einem graphbasierten Format vorliegendene Regesten webbasiert anzeigt und es erlaubt, Registereinträge im Text zu verorten. Die daraus entstandene Datenbasis hilft dabei neues Wissen zu generieren, so können z.B. Verwandtschaftsbeziehungen automatisch erkannt und in den Regesten-Graph integriert werden.
         Das im Rahmen des Bund-Länder-geförderten Akademienprogramms angesiedelte Grundlagenforschungsprojekt Regesta Imperii erstellt deutschsprachige Inhaltsangaben (sog. Regesten) von Kaiser-, Königs und Papsturkunden, begonnen von Karl dem Großen bis hin zu Kaiser Maximilian. Seit Projektbeginn 2001 wurden 1829 Regesten erstellt und digitalisiert und stehen inzwischen als Volltext im Internet zur Verfügung. Die Publikation der digitalisierten Register befindet sich gerade in Vorbereitung.
         Neben den Regesta Imperii sind immer mehr Editionen und Regestenwerke als Volltext im Internet zugänglich und können über Suchmasken abgefragt und genutzt werden. Die Nutzungsart unterscheidet sich zumeist aber nicht grundlegend von einer analogen Nutzung des Buches: Das Register wird aufgeschlagen und man kann anschließend die einem Registereintrag zugeordneten Urkunden oder Regesten aufrufen und lesen.
            
         
                Mit der Nutzung von Graphentechnologien in den digitalen Geisteswissenschaften werden neue Nutzungs- und Analyseformen der bereits vorhanden digitalen Editions- und Regestenwerke möglich. Die Digitale Akademie, Mainz (
                
               www.digitale-akademie.de
            ) hat auf ihrer Seite
                
               www.graphentechnologien.de
            einige beispielhafte Anwendungsszenarien für die Nutzung von Graphdatenbanken zur Erschließung von Onlineregesten vorgestellt. Für dieses Beispielprojekt wurden die Regesten Kaiser Friedrichs III. in eine Graphdatenbank konvertiert, anschließend das zugehörige Register digitalisiert und in die Graphdatenbank integriert. Im Graphenmodell ist es über die Abfrage nun möglich herauszufinden, in welchem Regest eine Person genannt wird und eine Analyse der gemeinsam mit ihr im Regest genannten Personen zu veranlassen (Abbildung 1). Das Graphenmodell erlaubt zudem die weitere Ergänzung von Kanten zwischen den Registerknoten. So ist es beispielsweise möglich, dass zwei Personenknoten, die Vater und Sohn darstellen, durch eine KIND-Kante ergänzt werden, um so deren Verwandtschaftsbeziehung explizit im Graphen zu repräsentieren. Mit solchen Zusatzinformationen kann das Register als Erschließungswerkzeug immer weiter wachsen.
            
         
            
               
               Abbildung 1 Graphbasierte Repräsentation von Regesten (gelb) mit zugehörigen Registereinträgen (rot,blau,grau).
                    
            
         
         Methoden aus der Computerlinguistik helfen diesen manuell sehr aufwendigen Grapherweiterungsschritt semi-automatisch durchzuführen. Dazu werden im ersten Schritt
                alle Regesten mit einer auf der Clarin-D Infrastruktur basierenden Sprachverarbeitungs-Pipeline (Malow 2012) maschinell verarbeitet: Auflösung von Abkürzungen, Tokenisierung, Part-of-Speech Tagging, Parsing und Entitätenerkennung. Im zweiten Schritt werden die Texte der einzelnen Regesten in einer interaktiven Webanwendung ausgewertet. Für jedes Regest werden alle Registereinträge (Personen, Orte, usw.), die während der Digitalisierung manuell mit Regesten verbunden wurden, angezeigt. Abbildung 2 stellt rechts den Regestentext und links alle verbundenen Registereinträge dar. In den Ausgangsdaten ist nicht gespeichert wie die Regeistereinträge im Text erwähnt werden, z.B. dass
                sich „der Stadt“ im Text auf den Eintrag „Aachen“ bezieht. Unsere Anwendung ermöglicht es hingegen jede im Text erkannte Entität mit den Registereinträgen per einfachem Mausklick zu verbinden. Mittels dieses Verfahrens konnten bereits ca. 4000 Registereinträge mit passenden Textstellen verbunden werden.
            
         
            
               
               Abbildung 2: links: Regesttext, erkannte Entitäten sind hervorgehoben (bereits neu verortete Textstellen zum Eintrag sind grün); rechts: verknüpfte Registereinträge zum Regest
                    
            
         
         
                Die neu geschaffene annotierte Datenmenge bildet somit einerseits eine wichtige Grundlage, für die Regestenforschung, da nun sehr einfach Wissen aus Texten mit den verknüpften Registereinträgen automatisch abgeleitet werden kann: z.B. die Vater-Sohn-Relation zwischen Colyn Beisse und Johann Beissel (Abbildung 3).
            
         
                Andererseits bietet dieser Datensatz für die computerlinguistische Forschung weitere Anknüpfungspunkte. Mittels Distant Supervision (Blessing 2012) können aus den verknüpften und im Text verankerten Relationen zu den Registereinträgen neue Modelle trainiert werden, die wiederum auf nicht manuell annotierten Textstellen der Regesten Anwendung finden. Durch diesen iterativen Ansatz können sukzessive große
                Regestensammlungen mit immer neuem Wissen angereichert werden und somit eine Grundlage für neue Analyseformen bieten.
            
         
            
               
               Abbildung 3: Depedenzanlayse, die zeigt wie Verwandtschaftsrelationen direkt aus der Analyse extrahiert werden können. In diesem Beispiel ist Colyn Beissel der Sohn von Johann Beissel.
                    
            
         
      
      
         
            
               Bibliographie
               
                  Blessing, Andre / Schütze, Hinrich 
                        (2012) Crosslingual Distant Supervision for Extracting Relations of Different Complexity.
                        In Proceedings of the 21st ACM International Conference on Information and Knowledge Management
               
               
                  Kuczera, Andreas
                        (2017) Herrscherhandeln in den Regesta Imperii. Beispielprojekt an den Regesten Kaiser Friedrichs III. URL:
                        
                        (abgerufen am 14.09.2017)
                    
               
                  Kuczera, Andreas 
                        (2016): Graphdatenbanken für Historiker. Netzwerke in den Registern der Regesten Kaiser Friedrichs III. mit neo4j und Gephi, in: Mittelalter. Interdisziplinäre Forschung und Rezeptionsgeschichte, 05.05.2015. URL:
                        
                        
                  
                     http://mittelalter.hypotheses.org/5995
                  .
                    
               
                  Mahlow, Cerstin / Eckart, Kerstin / Stegmann, Jens / Blessing, Andre / Thiele, Gregor / Gärtner, Markus / Kuhn, Jonas
                        (2014) Resources, Tools, and Applications at the CLARIN Center Stuttgart in Proceedings of the 12th Konferenz zur Verarbeitung natürlicher Sprache  11-21
                    
            
         
      
   



      
         
            Introduction and Motivation
            The emergence of computational methods of text processing has created new paradigms of research in literary studies in recent years (Jockers & Underwood, 2016), for instance 
                    distant reading to find patterns and regularities (Moretti, 2005). Network analysis and extraction of information about relations between characters from literary texts is an example for distant reading methods. Such information can not only be helpful for better understanding of character interactions but can also facilitate the comparison of thereof in different texts.
                
            Existing tools of text analysis and network visualization such as Voyant
                     or Gephi
                     are either missing modules for character network analysis or require preliminary steps on data preprocessing from the user and therefore are not easy-to-use for some humanities scholars who lack programming skills. Interactive tools in addition often lack features to ensure reproducibility of results.
                
            We present our ongoing effort on closing this gap by developing a literary analysis reporting tool 
                    rCAT
               
                  
               , whose primary purpose is to provide an easy-to-use, stable, and reusable solution for automatic extraction of relational information from text and to characterize these relationships automatically to provide the user with deeper qualitative insight. We opt for implementation as a web-based reporting tool instead of an interactive tool for two reasons: (1) automatically generated reports in PDF format can serve as a stable foundation for discussion and can be reused in publications and visualizations easily, and (2) the results are clearly connected to the chosen input parameters such that reproducibility of results is ensured.
                
            As a use-case study, we apply 
                    rCAT to Johann Wolfgang von Goethe's epistolary novel 
                    Die Leiden des jungen Werthers. On the basis of this epistolary novel, we show that not only the network can be generated, but also the characteristic triangular relationship of the protagonists is easily identified. The goal is to automatically determine this triad in the original text and in the adaptations that have been published since the publication of 
                    Werther in 1774.
                
         
         
            
               Previous Work
                
            Previous research on social networks in literary fiction generally fall into one of the two categories: (1) works that explore methods for extracting and formalizing character networks (
                    cf., Elson et al. (2010), Agarwal et al. (2012, 2013), Park et al. (2012)), and (2) works that primarily focus on qualitative implications of network analysis (
                    cf., Rydberg-Cox (2011), Moretti (2011), Nalisnick & Baird (2013), Jayannavar et al. (2015)). It is common to address both tasks at the same time, as in Beveridge & Shan (2016), who introduce a number of formal measures for analyzing the centrality of the characters in 
                    Game of Thrones books, which results in both expected and surprising findings. 
                
            Building on graph theory extensively elaborated in the past fifty years (e.g., Bondy and Murty, 1976 or West, 2001), our work is similar to Beveridge & Shan (2016), in particular, in terms of the weighted degree measure, and to Park et al. (2012), in terms of distance measure for detecting closely related characters in a text.
         
         
            
               Methods
                
            In the following, we explain the different components in 
                    rCAT, which are available for text analysis. After that, we discuss the results based on a use-case study.
                    
            
            
               Character lists and character identification
               To detect character mentions in the text we use a fundamental named-entity recognition approach based on dictionaries. This approach is suitable for scholars who analyze texts they already know. Consequently, we opt for a transparent and simple character recognition procedure: The user provides a list of character names to be included in the analysis specifying a canonical name form and all variations thereof she would like to take into account (
                         e.g., “Lotte” is the canonical name and “Lotten”, “Lottens”, “Lottgen”, “Lottchen”, “Charlotten S.”. are its variants).
                     
            
            
               Relation detection and context words
               We define the closeness of relationship between two characters using a 
                    distance measure
                  dist
                  X
                  (p,q), where 
                    p and 
                    q are the strings corresponding to these characters and 
                    X is the number of tokens between them (Park et al., 2012). In addition, we introduce the 
                    context measure
                  cont
                  Y
                  (p,q), where 
                    p and 
                    q are the strings corresponding to these characters and 
                    Y is the number of tokens before the character 
                    p and after the character 
                    q. While the former measure allows for detecting those characters that are closely related to each other, the latter one enables a contextual analysis of their relationship.
                
            
            
               Network analysis
               We visualize the network of characters with an undirected graph 
                    G=(V,E), where 
                    V are the vertices, each vertex corresponding to one character, and each edge 
                    E=(V
                  i,
                  ,V
                  j
                  ) corresponding to relations between pairs of characters. We output the following measures for each character node: 
                    degree, 
                    edge weight, 
                    weighted degree and 
                    density. The degree is the number of edges occurring with a given vertex. The edge weight, 
                    w
                  i,j
                   ≥ 0, is defined as the number of interactions between the vertices V
                    i and V
                    j. The weighted degree is the sum of weights of the edges occurring with a vertex 
                    i. Density is the ratio of occurring edges between two vertices and all possible vertex pairs.
                    
            
            
               Word clouds
               Word clouds are an approach to visualize the vocabulary of a text. The size of one word corresponds to its frequency. We use two different kinds of word clouds: For each character in the character list, we show word clouds based on the context of a window size 
                        n. For each pair of characters occurring in the network, we present a word cloud based on the words between them as well as on the words found in the context. Both types of word clouds can be filtered to the specific word fields (words from specific domains) which is helpful in gaining a focused insight into the characters relations.
                    
            
            
               Word Field developments
               We plot the timeline of multiple predefined world fields (specified by word lists) in the text. This feature is helpful in representing how certain fields (
                      e.g., concepts, emotions) develop throughout the narrative (Kim et al., 2017).
                   
            
            
               Implementation
               The tool was developed using Python v.3.6 and the Flask
                     web development framework. The tool outputs a single PDF report. The resulting document contains information from the analysis modules described in the previous section. Network graphs included in the report are generated with 
                    graphviz. Additionally, the tool can generate a CSV file that can be used as input to Gephi. 
                    
            
         
         
            
               Use-case Demonstration
                
            For a use-case analysis, we apply 
                    rCAT to 
                    Die Leiden des jungen Werther by Johann Wolfgang Goethe with the following parameters: X=8, Y=5, stop words removed (previous work focused on this analysis without rCAT, 
                    cf. Murr, 2017).
                
            In Goethe's epistolary novel, the protagonist Werther describes his unhappy love for Lotte, who is engaged to Albert. The characteristic triangular relationship in the novel arises from this constellation (protagonist - beloved woman - antagonist). With 
                    rCAT we expect to identify and characterize this relationship. Figures 1 and 2 show a sample network analysis output (tables are shown only partly).
                
            The protagonist Werther shows a degree of 21, which is the number of characters with whom he interacts. The closest relationship measured by edge weight (Figure 2) is observed between Werther and Lotte (81 interactions). The antagonist Albert has a low degree of 3. However, his weighted degree is 36 (third highest after Werther and Lotte), which confirms his important role in the triangular relationship.
            
               
                  
                  Illustration 1: Degrees and weighted degrees for most important characters of Goethe’s Werther
               
            
            
               
                  
                  Illustration 2: Edge weights
               
            
            
               
                  
                  Illustration 3: Complete network of Goethe’s Werther
               
            
            Highlighted in red is the typical triangular relationship in Goethe’s novel, which corresponds to the three highest weighted degrees. In further steps, we will use 
                    rCAT to analyze the adaptations of Goethe's novel with a focus on this triad.
                
            To better characterize the edges, the tool outputs top-
                    n word clouds sorted by edge weight (
                    n is specified by the user) for character pairs and by degree for single characters. Figure 4 and 5 show examples of the word clouds for character pairs filtered to the words from the emotion domain.
                
            
               
                  
                  Illustration 4: Word clouds for Werther-Lotte
               
               
                  
                  Illustration 5: Werther-Albert
               
            
            The word clouds enable first conclusions about the relationships of the characters. Werther and Lotte's word cloud characterizes their ambivalent relationship. The key words "Leidenschaft" and "Freude" reflect Werther's love, whereas the mentions of "sterben" and "Verblendung" are characteristic of the unrequited love, which leads Werther into his "disease unto death". As Werther and Albert’s word cloud reveals, their relationship is dominated by the "Unruhe" that Werther feels through his adversary. 
            Additionally, the tool plots the development of the narrative (not bound to specific characters) based on the word fields, an example of which is shown on Figure 6. In this case we used words from the emotion domain (with emotion dictionaries by Klinger et al. (2016)).
            
               
                  
                  Illustration 6: Word field development for Goethe’s Werther
               
            
            The word field development can highlight the prevalence of individual emotion domains across the text. The accumulation of the negative emotion words (Wut,Trauer, Furcht) towards the end suggests, for example, that Goethe’s novel has no “happy ending”. The striking rash on “Freude”, however, captures the last happy hours Werther spends with Lotte in the second part of the narration before he kills himself.
            
               
                  Future Work
                    
               The next version of the tool will include a character-oriented word field development calculated and plotted for the main characters of the stories. In addition, future releases will include more analysis features and bulk file processing.
            
         
      
      
         
            
               
            
            
               
            
            
               
                  www.ims.uni-stuttgart.de/data/rcat
               
            
            
               
            
         
         
            
               Bibliographie
               
                  Agarwal, A. / Corvalan, A. / Jensen, J. / Rambow, O. (2012): “Social Network Analysis of Alice in Wonderland”, in: CLfL@ NAACL-HLT 88-96.
               
                  Agarwal, A. / Kotalwar, A. / Rambow, O. (2013): “Automatic Extraction of Social Networks from Literary Text. A Case Study on Alice in Wonderland”, in: IJCNLP 1202-1208.
               
                  Beveridge, A. / Shan, J., (2016): “Network of thrones”, in: Math Horizons, 23(4): 18-22.
               
                  Bondy, J.A. / Murty, U.S.R. (1976): Graph theory with applications (Vol. 290). London: Macmillan.
               
                  Burrows, J.F. (1987): “Word-patterns and story-shapes: The statistical analysis of narrative style”, in: Literary & Linguistic Computing, 2(2): 61-70.
               
                  Elson, D.K. / Dames, N. / McKeown, K.R. (2010): “Extracting social networks from literary fiction”, in: Proceedings of the 48th annual meeting of the association for computational linguistics 138-147. Association for Computational Linguistics.
               
                  Heuser, R., F. Moretti / E. Steiner (2016): The Emotions of London. Technical report. Stanford University. Pamphlets of the Stanford Literary Lab.
               
                  Jayannavar, P. / Agarwal, A. / Ju, M. and Rambow, O. (2015): “Validating Literary Theories Using Automatic Social Network Extraction”, in CLfL@ NAACL-HLT 32-41.
               
                  Jockers, M.L. / Underwood, T. (2016): “Text‐Mining the Humanities”, in: Schreibman, Susan / Siemens, Ray / Unsworth, John (eds.): A New Companion to Digital Humanities 291-306.
               
                  Kim, E. / Padó, S. / Klinger, R. (2017): “Investigating the Relationship between Literary Genres and Emotional Plot Development”, in: Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature 17-26.
               
                  Klinger, R. / Sulliya S.S. / Reiter N. (2016): “Automatic Emotion Detection for Quantitative Literary Studies -- A Case Study on Kafka's ‘Das Schloss’ and ‘Amerika’”, in: Digital Humanities (DH), Conference Abstracts, Kraków, Poland, 2016.
               
                  Michel, J.B. / Shen, Y.K. / Aiden, A.P. / Veres, A. / Gray, M.K. / Pickett, J.P. / Hoiberg, D. / Clancy, D. / Norvig, P. / Orwant, J. / Pinker, S. (2011): “Quantitative analysis of culture using millions of digitized books”, in: science, 331(6014) 176-182.
               
                  Moretti, F. (2005): Graphs, maps, trees: abstract models for a literary history. Verso.
               
                  Moretti, F. (2011). Network theory, plot analysis. Stanford Literary Lab Pamphlet Series 2. Available at: https://litlab.stanford.edu/LiteraryLabPamphlet2.pdf
               
                  Murr, S. / Barth, F. (2017): Digital Analysis of the Literary Reception of J.W. v. Goethe’s ‘Die Leiden des jungen Werthers’, in: Digital Humanities (DH), Conference Abstracts, Montreal, Canada 2017.
               
                  Nalisnick, E.T. / Baird, H.S. (2013): “Extracting sentiment networks from Shakespeare's plays”, in: Document Analysis and Recognition (ICDAR), 2013 12th International Conference on IEEE 758-762.
               
                  Park, G.M. / Kim, S.H. / Cho, H.G. (2013): “Structural analysis on social network constructed from characters in literature texts”, in: Journal of Computers, 8(9): 2442-2447.
               
                  Rydberg-Cox, J., (2011): “Social networks and the language of greek tragedy”, in: Journal of the Chicago Colloquium on Digital Humanities and Computer Science (Vol. 1, No. 3).
               
                  West, D.B. (2001): Introduction to graph theory (Vol. 2). Upper Saddle River: Prentice Hall.
            
         
      
   



      
         
            Einleitung
            Obwohl sich infrastrukturell einiges getan hat, sieht ein typischer Operationsmodus der digitalen Literaturwissenschaft immer noch so aus, dass eine bestimmte Forschungsmethode auf ein oft nur ephemeres Korpus angewandt wird. Im besten Fall ist das Ergebnis 
                    irgendwie reproduzierbar, im schlechtesten Fall gar nicht. Im besten Fall gibt es ein offen zugängliches Korpus in einem Standardformat wie TEI, einer anderen Markup-Sprache oder zumindest als txt-Datei. Im schlechtesten Fall ist das Korpus gar nicht zugänglich, d. h., die Forschungsergebnisse müssen einfach hingenommen werden.
                
            Doch seit kurzem gibt es Anzeichen, dass sich dies ändert. Einige Digital-Humanities-Projekte stellen Schnittstellen zu stabilen Korpora zur Verfügung, über die man mannigfaltige Zugriffsmöglichkeiten bekommt und reproduzierbar arbeiten kann. Eines dieser Projekte ist DraCor, eine offene Plattform zur Dramenforschung, die in diesem Vortrag vorgestellt werden soll (zugänglich unter 
                    bzw. über die Repos und verschiedene Schnittstellen). DraCor transformiert vorliegende Textsammlungen zu ›Programmable Corpora‹ – ein neuer Begriff, den wir mit diesem Vortrag ins Spiel bringen möchten.
                
         
         
            Die Bausteine
            
               Vanillekorpora
               Ähnlich wie die COST Action zu europäischen Romanen (Schöch et al. 2018), versucht das DraCor-Projekt als Basis für eine digitale Komparatistik einen Stamm an multilingualen Dramenkorpora aufzubauen, die in basalem TEI kodiert sind. Ein selbst betriebenes russischsprachiges 
                    () und ein deutschsprachiges Korpus 
                    () dienen dabei als Einstieg. Diese Korpora sind, ähnlich wie die Sammlung »Théâtre classique« von Paul Fièvre, im weitesten Sinne als Vanillekorpora angelegt, die über das notwendige Markup hinaus zunächst kaum weitere spezielle Auszeichnungen enthalten, allerdings frei zur Verfügung stehen und damit fork- und erweiterbar sind. Zur Demonstration, dass auch andere, reicher kodierte Korpora dazugebunden und sofort alle bereits bestehenden Extraktions- und Visualisierungsmethoden der Plattform angeboten werden können, wurden das Shakespeare Folger Corpus sowie das schwedische Dramawebben-Korpus geforkt und angedockt 
                    (bzw. 
                        ). Dramenkorpora in weiteren Sprachen sollen folgen; einzige Voraussetzung dabei ist jeweils, dass diese in TEI vorliegen.
                    
               Die Vorteile von frei auf GitHub gehosteten Korpora liegen auf der Hand. Unabhängig von den letztlich durch die Plattform zur Verfügung gestellten Schnittstellen können die Korpora alternativ durch Klonen oder andere Downloadmethoden, etwa über den SVN-Wrapper von GitHub, direkt bezogen und individuell weiterverarbeitet werden. Ein offen zugängliches GitHub-Repositorium heißt auch, dass Pull Requests zur Fehlerkorrektur und Forks für Erweiterungen möglich und erwünscht sind.
            
            
               XML-Datenbank (eXist-db) und Frontend
               DraCor als Plattform setzt auf die eXist-Datenbank, um die TEI-Dateien zu verarbeiten und Funktionen zur Beforschung der Korpora zur Verfügung zu stellen. Das Frontend wurde mit ReactJS gebaut, ist responsiv und einfach erweiterbar. Der Schwerpunkt liegt aber nicht auf der GUI, sondern auf der API (vgl. generell zur Unterscheidung zwischen beiden Schnittstellenansätzen Bleier/Klug 2018).
            
            
               API und Entwicklungsumgebung
               Um dem Ideal und der Möglichkeit nahe zu kommen, auf einfache Weise »alle Methoden auf alle Texte« anwenden zu können (Frank/Ivanovic 2018), braucht es mehr als offene Korpora. Der zitierte Text von Frank/Ivanovic macht sich hinsichtlich dessen für SPARQL-Endpunkte stark; auch DraCor bietet einen solchen an, besitzt darüber hinaus aber eine reiche API, die über Swagger dokumentiert und erläutert wird 
                    (). In einem Teilbereich der Korpusphilologie, den Digital Scholarly Editions, hat die Diskussion um eine proaktivere Nutzung von APIs bereits begonnen (zur Vorgeschichte vgl. wiederum Bleier/Klug 2018), als Beispiel hierfür diene die Folger Digital Texts API 
                    (), über die man sich spezifische Querys zusammenbauen kann. Der Vorteil einer moderneren Lösung wie Swagger besteht darin, dass API-Querys live und direkt ausgeführt und die Outputs genauer kontrolliert und gesteuert werden können.
                    
               Ein einfaches Use-Case-Szenario sieht dann so aus, dass man etwa im RStudio mit zwei, drei Zeilen Code einen Blick in ein Korpus werfen kann, etwa über die zeitliche Entwicklung der Anzahl der Charaktere im russischen Drama zwischen 1740 und 1940, die in der Metadatentabelle festgehalten sind 
                    (). Diese Datei, beziehbar im JSON- oder CSV-Format, wird in eine Data.Table eingelesen, woraufhin die Werte zweier Spalten (Erscheinungsjahre und Number of Speakers) einfach über ggplot visualisiert werden können (Abb. 1).
                    
               
                  
                     
                     Abbildung 1: Anzahl der Charaktere pro Drama in chronologischer Ordnung (Quelle: RusDraCor).
                  Anhand dieses sehr simplen Beispiels zeigt sich dann recht deutlich, dass sich mit Puschkins an Shakespeare angelehntem historischen Drama »Boris Godunow« (1825), in dem Sprechakte von 79 Charakteren vorkommen, eine strukturelle Diversifizierung der russischen Dramenlandschaft Bahn bricht.
                    
               Die Möglichkeiten beschränken sich aber nicht darauf, vorgefertigte API-Funktionen zu benutzen. Neue Forschungsideen zeitigen immer auch neue Bedarfe an einfach bezieh- und reproduzierbaren Daten und Metriken; die API kann dementsprechend erweitert werden. Dies wird dadurch erleichtert, dass über Apache Ant die gesamte Entwicklungsumgebung auf dem eigenen System nachgebaut werden kann.
               Durch bereits implementierte Funktionen können neben Struktur- und Metadaten etwa auch Volltexte ohne Markup bezogen werden (auch Untermengen von Volltexten wie Regieanweisungen), etwa wenn Methoden wie die Stilometrie oder das Topic Modeling der Endzweck sind, also Methoden, die nach dem »bag of words«-Prinzip arbeiten, für das kein Markup vonnöten ist.
               Insgesamt wird durch den Aufbau und die Dokumentation offener APIs die bisher oft aufwendige Reproduzierbarkeit von Forschungsergebnissen erheblich erleichtert.
            
            
               Shiny App
               Ein Beispiel für die vielseitigen Nutzungsmöglichkeiten der DraCor-API ist die Shiny App, die Ivan Pozdniakov aufgesetzt hat 
                    (). Shiny ist ein auf R basierendes Framework, das es ermöglicht, interaktive Visualisierungen im Browser darzustellen. Die DraCor-Shiny-App tut genau dies und setzt dabei vollkommen auf die DraCor-API für den Datenbezug. So kann zu Lehr- und Forschungszwecken, aber auch zur einfacheren Datenkorrektur, auf Visualisierungen des aktuellen Datenbestandes zugegriffen werden.
                    
            
            
               Didaxe
               Das Markup oder andere Formalisierungen literarischer Texte sind nicht selbsterklärend. Zwar gibt es einige Standards, aber die jeweilige Operationalisierungslösung hängt von der Forschungsfrage ab. Allein das Extrahieren von Figurennetzwerkdaten ist auf viele Arten und Weisen möglich, was dazu führt, dass etwa alle von verschiedenen Forschungsgruppen extrahierten Netzwerke aus Shakespeares »Hamlet« zu leicht verschiedenen Ergebnissen kommen. Selbst für Dramen ist dies also schon ein nicht-trivialer Akt, von Romanen dann ganz zu schweigen (beispielhaft seien Grayson et al. 2016 genannt, die verschiedene Extraktionsmethoden für Romane durchtesten und die Ergebnisse vergleichen). Um diese Erkenntnis schon in der Lehre zu fördern, wurde das Tool »Easy Linavis« 
                    () entwickelt und in die DraCor-Toolchain integriert. Per Hand können Netzwerkdaten aus Texten extrahiert und dabei das Bewusstsein für die Kontingenz dieses Vorgangs geschärft werden, eine wichtige Vorstufe zur Operationalisierung.
                    
               Neben einem Ansatz zur Gamifizierung des TEI-Korrekturvorgangs (Göbel/Meiners 2016) haben wir für Lehrzwecke auch ein Dramenquartett entwickelt, um spielerisch das Verständnis von Netzwerkwerten zu trainieren (Fischer at al. 2018).
               Die aufgezählten, um die Plattform herumgruppierten didaktischen Mittel sind integraler Bestandteil des ganzen Projekts, da sie auf dessen Daten und Operationalisierungen aufsetzen. Wichtig dabei war die Erkenntnis, dass Daten mehrere Gestalten annehmen und für Forschung und Lehre gleichermaßen von Bedeutung sein können.
            
            
               Linked Open Data (LOD)
               Im TEI-Code sind PND- bzw. Wikidata-Identifier sowohl für Autor*innen als auch für die Werke hinterlegt. Auf diese Weise lassen sich verschiedene Realien, die außerhalb der eigenen Korpusarbeit liegen, hinzufügen. Eine automatisch erstellte Autor*innengalerie hat dabei noch eher illustrativen Charakter (de la Iglesia/Fischer 2016).
               Darüber hinaus kann man aber zum Beispiel feststellen, ob es nicht einen unbewussten regionalen Bias im Korpus gibt. Dafür lässt man sich über die Wikidata-Identifier die Verteilung der Geburts- und Sterbeorte der Autor*innen auf einer Karte anzeigen. So konnte dann für das deutschsprachige Korpus GerDraCor ausgeschlossen werden, dass es einen solchen Bias gibt, da sich die Orte relativ gleichmäßig über die (historisch) deutschsprachigen Gebiete verteilen (Göbel/Fischer 2015).
               Ebenso lässt sich über die Wikidata-ID der Stücke herausfinden, wo diese uraufgeführt worden sind (Beispiel-Query: 
                        ), d. h., Aspekte der Aufführungsgeschichte lassen sich zuschalten, obwohl diese gar nicht im Fokus des Kernprojekts liegen. Programmable Corpora verbinden sich also auch mit der Welt um sie herum, was sie u. a. von den nach innen gerichteten Workbenches der Korpuslinguistik unterscheidet.
                    
            
            
               Infrastruktur statt Rapid Prototyping
               Projekte wie DraCor versuchen nichts anderes als den digitalen Literaturwissenschaften eine verlässliche und ausbaufähige Infrastruktur zu geben, damit sie sich stärker auf eigentliche Forschungsfragen konzentrieren und reproduzierbare Ergebnisse hervorbringen können.
               Eine wichtige Folgerung für uns war, dass wir die Weiterentwicklung unserer seit vier Jahren entwickelten all-in-one Python-Skriptsammlung 
                        dramavis aufgeben und uns lieber der Arbeit an der API widmen. 
                        Dramavis (Kittel/Fischer 2014–2018 sowie Fischer et al. 2017) folgte dem in den Digital Humanities nicht untypischen Rapid Prototyping mit direkter Verarbeitung literarischer XML-Daten (Trilcke/Fischer 2018) und einer mittlerweile stark gewachsenen Codebasis, die alles auf einmal kann, deren Maintenance aber immer schwieriger geworden ist und oft genug von den eigentlichen Forschungsfragen weggeführt hat.
                    
            
         
         
            
               Fazit
                
            In Anlehnung an das Projekt »ProgrammableWeb« – das eine Datenbank von offenen APIs unterhält und dessen Slogan lautet: »APIs, Mashups and the Web as Platform« (zugänglich unter 
                    ) – schlagen wir für infrastrukturell-forschungsorientierte, offene, erweiterbare und LOD-freundliche Korpora den Begriff ›Programmable Corpora‹ vor.
                
            Programmable Corpora erleichtern es, Forschungsfragen auf viele Arten und Weisen um Korpora herum programmieren zu können. Es steht zu erwarten, dass sich infrastrukturelle Anstrengungen dieser Art für die gesamte Community auszahlen mit Effekten, wie sie John Womersley in seiner Präsentation auf der ICRI2018 in Wien aufgezählt hat: a) dramatically increase scientific reach; b) address research questions of long duration requiring pooled effort; c) promote collaboration, interdisciplinarity, interaction.
            Der Anschlussmöglichkeiten sind viele, egal ob man gar nicht programmieren möchte, sondern nur eine GEXF-Datei für Gephi benötigt, ob ein Korpus über seine Verbindungen zur Linked Open Data Cloud beforscht oder einfach aus R oder Python heraus bestimmte Daten bezogen werden sollen, ohne dass man sich mit dem Korpus und dessen Maintenance und Reproduzierbarkeit selbst kümmern muss (all dies bleibt natürlich aber eine Option). Programmable Corpora erleichtern die Entscheidung, auf welcher Ebene der eigene Forschungsprozess einsetzt.
         
      
      
         
            
               Bibliographie
               
                  Bleier, Roman / Klug, Helmut W. (2018): Discussing Interfaces in Digital Scholarly Editing. In: Digital Scholarly Editions as Interfaces. BoD, Norderstedt, S. V–XV. URL: 
               
               
                  de la Iglesia, Martin / Fischer, Frank (2016): The Facebook of German Playwrights. URL: 
               
               
                  Fischer, Frank / Dazord, Gilles / Göbel, Mathias / Kittel, Christopher / Trilcke, Peer (2017): Le drame comme réseau de relations. Une application de l‘analyse automatisée pour l’histoire littéraire du théâtre. In: Revue d'historiographie du théâtre. № 4. URL: 
               
               
                  Fischer, Frank / Kittel, Christopher / Milling, Carsten / Schultz, Anika / Trilcke, Peer / Wolf, Jana (2018): Dramenquartett – Eine didaktische Intervention. In: Konferenzabstracts zur DHd2018, Universität zu Köln. S. 397 f. DOI: 
               
               
                  Göbel, Mathias / Fischer, Frank (2015): The Birth and Death of German Playwrights. URL: 
               
               
                  Göbel, Mathias / Meiners, Hanna-Lena (2016): Play(s): Crowdbasierte Anreicherung eines literarischen Volltext-Korpus. In: Konferenzabstracts zur DHd2016, Bern/CH. S. 140–143. URL: http://www.dhd2016.de/abstracts/vortr%C3%A4ge-007.html
               
               
                  Grayson, Siobhán / Wade, Karen / Meaney, Gerardine / Greene, Derek (2016): The Sense and Sensibility of Different Sliding Windows in Constructing Co-Occurrence Networks from Literature. In: 2nd IFIP International Workshop on Computational History and Data-Driven Humanities. Trinity College Dublin 2016. PDF: 
               
               
                  Kittel, Christopher / Fischer, Frank (2014–2018): dramavis. Python-Skriptsammlung. Repo: 
               
               
                  Schöch, Christoph et al. (2018): Distant Reading for European Literary History. A COST Action [Poster]. In: DH2018: Book of Abstracts / Libro de resúmenes. Mexico: Red de Humanidades Digitales A. C. URL: https://dh2018.adho.org/en/?p=11345
               
               
                  Trilcke, Peer / Fischer, Frank (2018): Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen. In: Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden. Hrsg. von Martin Huber und Sybille Krämer (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3). DOI: 
               
            
         
      
   



      
         Digital Humanities in die Lehre und Ausbildung stärker zu integrieren, ist eine vielfach geäußerte Forderung im Rahmen von DH und geisteswissenschaftlichen Fachverbänden (Sahle 2017). Während in den ersten beiden Jahrzehnten der Digitalisierung vor allem der Wandel von der analogen zur digitalen Erschließung und Präsentation, Open Access, Blended Learning und digitales Publizieren im Mittelpunkt der Forschung stand, haben sich Ansätze und Themen zur universitären Vermittlung von DH-Technologien in der jüngeren Vergangenheit stark verändert. Mit den Digital Humanities ist eine Community entstanden, die innovative neue Methoden, vor allem aber eine Vielzahl von Tools und Werkzeugen bereitstellt. Sie entspringen nicht einem einzelnen fachspezifischen Kontext, sondern setzen auf interdisziplinäre Konzepte und vor allem vertiefte informatische Kenntnisse. Mittlerweile lässt sich in der Fachlandschaft eine Etablierung neuer Formen von Studiengängen und Curricula beobachten, die solche spezifischen DH-Anwendungen vermitteln und so zur Ausbildung der dringend benötigten DH-Spezialisten beitragen. Grundsätzlich deckt diese Form der parallelen Ausbildung von DH-Spezialisten zu den eigentlichen Fachwissenschaften daher einen sehr wichtigen Bedarf ab (Sahle 2013).
         Dennoch wirft diese Entwicklung auch Schwierigkeiten auf, da sie die Entkoppelung von an der DH orientierten Wissenschaftlern und eigentlicher geisteswissenschaftlichen Fachwissenschaft zusätzlich verschärft (Hohls 2017). Da viele DH-Anwendungen heute noch keinem Fachkanon oder Standards unterliegen, bleibt die Einarbeitung in solche Methoden und Tools ein arbeitsintensiver Prozess interdisziplinärer Verständigung, der meist individuell geleistet werden muss. Dies führt häufig zu Schwierigkeiten im Vermittlungsprozess. In der jüngeren Diskussion wird dies gern mit dem Bild des DH-affinen „Hackers“ charakterisiert, der in der Fachwissenschaft dem interessierten „Laien“ gegenübertritt. Die Spannungen zwischen beiden Gruppen (Enthusiasten und Skeptiker) haben sich in den letzten Jahren verschärft. Auf dem letzten Historikertag in Münster (September 2018) war die Skepsis gegen neue Methoden des digitalen Arbeitens in mehreren Sessions genauso fassbar, wie letztlich die Forderung danach, dass die DH auch fachlich für die einzelnen Fachverbände konkrete Antworten liefern müsse und nicht nur eine grundlegende Erschließungsfunktion besitzen dürfe. Innerhalb der einzelnen Fachverbände muss geklärt werden, inwieweit digitale Geisteswissenschaft Bestandteil der klassisch fachbezogenen Ausbildung werden kann und soll (Fickers 2014, S. 27, Schulz 2018, S. 79f.). Das gilt nicht nur für die spezifischen Forschungsrichtungen der Hilfs- und Grundwissenschaften und der Quellenkritik, sondern letztlich für alle Teilbereiche der fachbezogenen Forschung (Schlotheuber/Bösch 2015). Während in den Digital Humanities eher die hilfswissenschaftlichen Traditionen der Erschließung, Annotation, Editorik, Messung und Visualisierung wichtige Dimensionen repräsentieren, bleibt für die Geschichtswissenschaft die digitale Quellenkritik, Datenmodellierung, Analyse und vor allem Methoden- und Algorithmenkritik wesentliche Aufgabe (Rehbein 2015, Schulz 2018). Mittlerweile stehen ausgereifte kommerzielle und OpenSource-Programme bereit, um mittels qualitativer und/oder quantitativer Datenanalyse ganz verschiedene methodische Verfahren anzuwenden. Allerdings ist der zeitliche Umfang des Geschichtsstudiums durch die Einführung der Bachelor- und Masterstudiengänge heute streng limitiert. Viele Studierende verlassen nach dem Bachelorstudium die Universitäten und nehmen eine Arbeit auf. Innerfachlich ist daher der Anteil von historischer Fachkompetenz und digitalem methodischen Wissen bei der Ausbildung des Nachwuchses zu gewichten und es müssen zwingend Vorschläge diskutiert werden, welche grundsätzlichen Bausteine Anschlussfähigkeit zu den Digital Humanties herstellen können und wie diese in den Studienkanon integriert werden können.
         Welche Grundkompetenzen der Geschichtswissenschaft sind für die Basis der Geisteswissenschaft also ausschlaggebend? Für das Fach selbst dürfte die Verbindung von fachlicher Fragestellung, ihre Übertragung in spezifische Methoden und die Modellierung der damit in Zusammenhang stehenden Daten solche Grundkompetenzen beschreiben.
         Im Rahmen des Workshops “Digitale Lehrmethoden und digitale Methoden in der Geschichtswissenschaft. Neue Ansätze für die Lehre” hat die AG Digitale Geschichtswissenschaft im Verband der Historiker und Historikerinnen Deutschlands im Jahr 2018 eine Workshopreihe begonnen, um genau solche Spannungsfelder auszuloten und Angebote für die digitale Vermittlung zwischen DH und Fachcommunities zu leisten (König 2018). Zwar gibt es Summerschools und einzelne Workshops für die Vermittlung von digitalen Methoden, diese bieten aber häufig nur schwerpunktartige Einführungen zu bestimmten Tools und Techniken. Einen breiten Überblick über die vielfältige Landschaft digitaler Methoden, ihre Einbindung in Forschung und Lehre, bzw. ihre Anwendungsmöglichkeiten und -grenzen können diese Formen der Weiterbildung hingegen nicht leisten. Systematisches Wissen steht für die Lehre auf diese Weise bisher nur selten zur Verfügung, wie auch Anreiz- und Gelegenheitsstrukturen für eine tiefergehende digitale Schulung fehlen. Das Digitale wird so schnell zur Last anstatt zur Lust, vor allem wenn hinter den Angeboten auch ausgefeilte didaktische Konzepte und Ideen stecken sollen. Hier braucht es kreative Ideen, um im wissenschaftlichen Alltag Dozierende und Studierende zu erreichen und auf diese Weise, die bereits vorhandenen digitalen Infrastrukturen mit Leben zu füllen.
         Ein solches digitales Angebot möchten wir mit unserem Vortrag vorstellen. Im Rahmen des Workshops „Methoden auf der Testbank. Drei Zugänge zur Hexenforschung im Vergleich“ haben wir anhand von zentralen Thesen zur Hexenforschung einen Korpus von Texten und Daten zusammengestellt, der sich für drei verschiedene methodische Analysen entlang der gleichen historischen Fragestellung nach der Entwicklung, Abgrenzung und Ausdifferenzierung des Zauberei- und des Hexensabbatkonzeptes im 16. Jahrhundert nutzen lässt. Der Workshop richtet sich an Dozierende, die digitalen Methoden eher abwartend gegenüberstehen. Dabei geht es gezielt um die Verbindung fachlicher, methodischer und digitaler Problemstellungen. Aufgegriffen wird daher eine wichtige These der Hexenforschung (Voltmer 2012, Behringer 1997, Dillinger 2007), die bereits eine lange fachliche Diskussion besitzt und für die durch die Lehrkonzeption  auf digitalem Weg Einsichten und Erkenntnisse formuliert werden können. Entwickelt wurde ein Materialkorpus der folgende Materialien bereithält und bis zur Tagung auch frei zur Verfügung gestellt werden soll:
         1.) Lehrkonzept und didaktische Stoffentwicklung zur Thematik der Hexenverfolgung sowie die fachliche Entwicklung und Begründung der Fragestellung. Dieser Baustein des Angebots formuliert nicht nur die Fragestellung, sondern bietet zudem eine Einbindung zentraler fachwissenschaftlicher Texte und Thesen zum Thema, um hier verschiedene Aspekte auch für eine projektorientierte Seminargestaltung zu ermöglichen. So werden etwa Vorschläge unterbreitet, welche Formen der Analyse Studierende anhand eines selbstgewählten Projektes mithilfe der methodischen Ansätze wählen können.
         2.) Textkorpora: Digitalisiert und transkribiert wurden Urgichten und Verhörprotokolle von insgesamt 52 Personen (ca. 500 Digitalisate), die in der Stadt Rostock (Mecklenburg) während des 16. Jahrhunderts aufgrund von Zauberei oder Hexerei angeklagt wurden. Der Textkorpus erlaubt aufgrund des damit eingefangenen - auch sprachwissenschaftlich oder rechtsgeschichtlich sehr interessanten - Beobachtungszeitraums weitergehende Fragestellungen und Analysen. Aufgrund der hervorragenden Quellenüberlieferung bot der Bestand bereits mehrfach Ansatzpunkte zur auszugsweisen Edition (Koppmann 1900, Krause 1915) wie auch grundlegender Erforschung (Ehlers 1986, Moeller 2007, Müller 2017). Langfristig lassen sich hier sogar Untersuchungen zur Editionspraxis verschiedener Zeiten anstellen. Zugleich werden motivgeschichtliche Analysen möglich, da der Bestand etwa in die volkskundlichen Sammlungen und Korpora des 19./20. Jahrhunderts eingegangen ist.
         Diese Texte sind in einem zweimaligen Korrekturprozess und entsprechend der DTA- Transkriptionsrichtlinien transkribiert worden. Bis zur Tagung sollen sie nach Möglichkeit in das Deutsche Textarchiv oder ein anderes Repositorium überführt werden.
         3.) Über die Transkription hinaus erfolgte eine Modellierung der Fragestellung mithilfe eines selbst zu entwickelnden Kategoriensystems, das sich auch zur Implementation (Annotationen) in die Textdateien eignet (hier können praktische Übungen zusätzlich angeboten werden). Zum anderen aber auch in Form von Datenstrukturen zur Analyse mit MAXQDA, einem Statistikprogramm (hier spezifisch SPSS, möglich ist aber auch die Nutzung von OpenSource Software wie R) sowie einem graph- bzw. netzwerkbasierten Datenmodell (hier Gephi) Auswertungen erlaubt. Wir haben uns auf die Anwendungen von Programmen konzentriert, die Lehrenden und Lernenden einen ersten, niedrigschwelligen Zugang zur Anwendung von Methoden im Vergleich ermöglichen und vor allem für das Selbststudium auch genügend ausreichend dokumentiert sind. Über einen didaktischen Aufbau von qualitativer Datenanalyse (orientiert an Kuckartz 2014 und Mayring 2015), statistischer Auswertung und netzwerkorientierter Untersuchung können Studierende hier in der Datenmodellierung vom Konkreten zum Allgemeinen, der Entwicklung von Kategoriensystemen und der Verwendung von Standards geschult werden. Denn grundlegende Probleme in der Lehre sind meist das Verständnis von Datenstrukturen, die Formen der Modellierung, die Entscheidung für eine Methode und die Interpretation von Erkenntnissen aus den erzielten Datenanalysen.
         4.) Die Datenmodellierung wird genau dokumentiert und dient Studierenden und Lehrenden damit ebenso zur Institutionalisierung von zentralen Schritten des Forschungsdatenmanagements. Anhand von eher sozialwissenschaftlichen Dokumentationspraktiken werden Richtlinien formuliert, die nicht nur das Verständnis der Daten in ihrer Überführung von der „Quelle zur Tabelle“ (Manfred Hettling) fördern, sondern ebenso ein Beispiel für die „gute wissenschaftliche Praxis“ der Dokumentation von Forschungsleistungen bieten. Die Daten werden in langfristig speicherbaren, spezifischen Datenrepositorien abgelegt. Auf einer gemeinsamen Plattform der AG Digitale Geschichte und des Historischen Datenzentrums Sachsen-Anhalts werden die einzelnen Komponenten zu einer didaktischen Einheit nachnutzbar versammelt.
         5.) Das Datenset bzw. die Lehrkonzeption ermöglicht es den Studierenden, einen Überblick über die Verwendung von drei verschiedenen methodischen Ansätze zu erlangen, die Methoden kritisch zu vergleichen und Fähigkeiten zur Operationalisierung und Implementierung von Datenmodellierungen zur Beantwortung von Fragestellungen zu erwerben. Überdies wird eine individuelle Aktivierung der Studierenden möglich, die für die didaktische Vermittlung von Fachinhalten heute eine zentrale Rolle in der Lehre spielt (Helmke 2015). Mit Hilfe von selbsterstellten und bereitgestellten Daten können geschichtswissenschaftliche Analysen durchgeführt und die Vor- und Nachteile einzelner Methoden diskutiert werden. Vor allem aber erweist sich, ob Methodenvielfalt eher zur Bestätigung von Thesen oder zu neuen Perspektiven führt. Das Werkzeug bietet damit nicht nur Möglichkeiten der Quellen-, sondern eben auch der Methodenkritik.
         Insgesamt möchten wir damit einen Baustein vorstellen, wie sich digitale Lehre und Methoden der DH unmittelbar mit fachlichen Fragestellungen und Gegenständen der Geschichtswissenschaft verzahnen und bearbeiten lassen und sich der immer wieder beklagte Graben zwischen Digital Humanties und Fachwissenschaften einebnen lässt. Gleichzeitig möchten wir Einschätzungen zum Zeitaufwand, Lehraufwand und zur Ressourcenplanung geben. Die Lehreinheit soll eher unerfahrenen Nutzerinnen und Nutzern der Methoden einen Eindruck vermitteln, was digitale Werkzeuge leisten können und wie sie sich in der alltäglichen, fachspezifischen Lehre einbetten lassen, obwohl sie natürlich klassische Werkzeuge einer allgemeinen digitalen Forschung repräsentieren.
      
      
         
            
               Bibliographie
               
                  Wolfgang Behringer:
                  Hexenverfolgung in Bayern. Volksmagie, Glaubenseifer und Staatsräson in der Frühen Neuzeit, 
                        München 1997.
                    
               
                  Johannes Dillinger:
                  Hexen und Magie. Eine historische Einführung, 
                        Frankfurt/Main 2007.
                    
               
                  Ingrid Ehlers:
                  Über den Glauben an Hexen und Zauberer und ihre Verfolgung im Rostock des 16. Jahrhunderts. 
                        Beiträge zur Geschichte der Stadt Rostock (Neue Folge) 6, 1986, S. 21-40.
                    
               
                  Andreas Fickers:
                  Der ultimative Klick? Digital Humanities, Online-Archive und die Arbeit des Historikers im digitalen Zeitalter,
                        in: Forum für Politik, Gesellschaft und Kultur in Luxemburg 337, 2014, S. 25-29,
                        
                     http://hdl.handle.net/10993/21285.
                  
               
               
                  Andreas Helmke:
                  Unterrichtsqualität und Lehrerprofessionalität: Diagnose, Evaluation und Verbesserung des Unterrichts,
                        Seelze-Velber 2015.
                    
               
                  Rüdiger Hohls:
                  Digital Humanities vs. Digital History: Differenzen und Gemeinsamkeiten, 
                        in: Videoaufzeichnungen der Ringvorlesung "Digital Humanities: Die digitale Transformation der Geisteswissenschaften, Berlin 2017, URL: http://www.bbaw.de/mediathek/archiv-2017/24-10-2017-digital-humanities.
                    
               
                  Mareike König:
                  Workshopreihe 2018: Digitale Lehrmethoden und digitale Methoden in der Geschichtswissenschaft. Neue Ansätze für die Lehre #digigw18, 
                        2018,
                        
                     https://digigw.hypotheses.org/1660.
                  
               
               
                  Karl Koppmann:
                  Aus Hexenprozessen (aus Rostocker Niedergerichtsakten von 1576-1621), 
                        in: Korrespondenzblatt des Vereins für niederdeutsche Sprachforschung 21, 1900, S. 20-29.
                    
               
                  Ludwig Krause:
                  Die Blocksbergfeste der Hexen und Zauberer nach den Rostocker Kriminalakten des 16. Jahrhunderts, 
                        in: Niedersachsen 15, 1915, S. 238-240.
                    
               
                  Udo Kuckartz:
                  Qualitative Inhaltsanalyse. Methoden, Praxis, Computerunterstützung, 
                        Weinheim und Basel 2014.
                    
               
                  Philipp, Mayring:
                  Qualitative Inhaltsanalyse. Grundlagen und Techniken, 
                        Weinheim und Basel 2015.
                    
               
                  Katrin Moeller:
                  Dass Willkür über Recht ginge. Hexenverfolgung in Mecklenburg im 16. und 17. Jahrhundert, 
                        Bielefeld 2007.
                    
               
                  Andreas Müller:
                  Die Magie der Inhaltsanalyse. Entwurf einer  Inhaltsanalyse für den Vergleich von Hexenprozessakten aus Rostock 1584  und Hainburg, 
                        Masterarbeit Universität Wien 2017, 
                        
                     https://www.historicum.net/fileadmin/sxw/Themen/Hexenforschung/
                     
                  
                  Themen_Texte/Magister/hexen_mag_mueller.pdf.
               
               
                  Malte Rehbein:
                  Digitalisierung braucht Historiker/innen, die sie beherrschen, nicht beherrscht, 
                        in: H-Soz-Kult, 27.11.2015,
                        
                     www.hsozkult.de/debate/id/diskussionen-2905
                  
                  .
               
               
                  Patrick Sahle:
                  DH studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities, 
                        Göttingen: GOEDOC, Dokumenten- und Publikationsserver der Georg-August-Universität, 2013 (DARIAH-DE working papers 1).
                    
               
                  Patrick Sahle:
                  Forschung & Karriere. Zur anhaltenden Formierung, Professionalisierung und Professoralisierung der Digital Humanities, 
                        Vortrag Berlin, in: Ringvorlesung des Interdisziplinären Forschungsverbundes Digital Humanities in Berlin, 12.12.2017.
                    
               
                  Eva Schlotheuber / Frank Bösch:
                  Quellenkritik im digitalen Zeitalter. Die Historischen Grundwissenschaften als zentrale Kompetenz der Geschichtswissenschaft und benachbarter Fächer, 
                        auf: VHD-Blog (4 S.; PDF-Version: http://www.historikerverband.de/fileadmin/user_upload/vhd_journal_2015-04_beileger.pdf), 30. Oktober 2015.
                    
               
                  Julian Schulz:
                  Auf dem Weg zu einem DH-Curriculum. Digital Humanities in den Geschichts- und Kunstwissenschaften an der LMU München, 
                        München 2018,
                        
                     https://doi.org/10.5282/ubm/epub.42419
                  
                  .
               
               
                  Rita Voltmer / Walter Rummel:
                  Hexen und Hexenverfolgung in der frühen Neuzeit, 
                        Darmstadt 2012.
                    
            
         
      
   



      
         
            Einleitung 
            
               Methoden der Digital Humanities erfreuen sich zunehmender Akzeptanz in den Geisteswissenschaften - auch in traditionellen Disziplinen wie den Geschichtswissenschaften. Die Werkzeuge der Digital Humanities können hilfreich sein, um die eigenen Daten zu strukturieren, unerwartete Zusammenhänge zu erkennen und Erkenntnisse hervorzubringen, die man ohne den Einsatz von digitalen Methoden nicht erlangt hätte. Vor allem im Bereich der historischen Netzwerkanalyse gibt es seit einigen Jahren neue interessante Entwicklungen. Während man sich anfangs noch an Werkzeugen aus der Sozialwissenschaft bediente
                
               (Jannidis / Kohle / Rehbein 2017: 14), gibt es mittlerweile Software, die nicht nur statistische Ergebnisse ausgibt, sondern auch für historische Fragestellungen geeignet ist. Die Vorteile liegen auf der Hand: einerseits kann man komplexe Beziehungen, deren Reichweite und Auswirkungen ordnen und darstellen; andererseits durch Visualisierungen die Ergebnisse einer breiten Öffentlichkeit zugänglich machen (Düring / Kerschbaumer 2016: 31). Wenn man anfangs mit dem Gedanken spielt, ob man für die Bearbeitung einer geisteswissenschaftlichen Forschungsarbeit überhaupt digitale Hilfsmittel anwenden sollte, wird man mit verschiedenen Fragen konfrontiert:
            
            
               Wie und wo fange ich an? Welches Tool ist sinnvoll? Welche Daten habe ich überhaupt und wie möchte ich diese darstellen? Welchen Mehrwert verspreche ich mir von dem Einsatz digitaler Methoden? Muss ich meine Fragstellungen anpassen um ein zufriedenstellendes Ergebnis zu erhalten? Welche Kenntnisse und Fähigkeiten muss ich mir aneignen? Lohnt sich das überhaupt? Der Posterbeitrag soll an diese Fragen anknüpfen. 
            
         
         
            Netzwerkanalyse für Historiker
            
               Die eigene Recherche hat gezeigt, dass das Angebot und die Initiativen (national wie international) von "Social Network Analysis" (kurz SNA) für Historiker auf den ersten Blick enorm zu sein scheint. Allerdings stellt sich bei näherer Betrachtung meistens heraus, dass nur einige der Tools für das eigene Thema geeignet sind. Schließlich variieren bei geschichtlichen Forschungsfragen die gewählten Zugänge sowie die Quellenlage und der Blickwinkel. Dieses Poster soll einerseits die meist verbreiteten Techniken für Historiker in der Netzwerkanalyse vorstellen; andererseits soll an konkreten Schritten der Entscheidungsprozess für ein geeignetes Tool zur Netzwerkanalyse beispielhaft am eigenen Dissertationsvorhaben nachvollzogen werden. Aufkommende Probleme und mögliche Lösungswege sollen beleuchtet werden.
            
         
         
            Anwendung in der Praxis mit „Gephi“
            
               Mein Forschungsthema befasst sich mit der Darstellung von Palästina in württembergischen Medien des 19. Jahrhunderts. Dafür werte ich politische, religiöse und pädagogische Medien aus. Eine der Hauptfragestellungen ist unter anderem welche Netzwerke gebildet wurden, in denen die Hauptakteure eine gewissen Einfluss übten und dadurch die gegenseitigen Beziehungen beider Länder prägten und förderten. Eine große Rolle spielt die Wissensverbreitung über das damals relativ unbekannte "Heilige Land": wie wurde spezielles Wissen - unter anderem Agrarmethoden, kulturelles und religiöses Leben in Palästina - weitergegeben? Neben den sozialen Beziehungen bestand auch ein reger wirtschaftlicher Austausch zwischen Palästina und Württemberg, der sich unter anderem im Handel äußerte. In diesem Zusammenhang plane ich meine Ergebnisse, die Verbindungen und Verflechtungen, sowie den Transfer zwischen beiden Länder mit der Hilfe von digitalen Tools zu visualisieren. Knotenpunkte wie Personen, Handelsbeziehungen, etc. sollen bei der Analyse im Fokus stehen und die ursprünglich aufgeworfenen Fragen ergänzend bereichern.
            
            
               Diese Forschungsarbeit und Fragestellungen bieten den Rahmen für die nähere Betrachtung der Möglichkeiten der historischen Netzwerkanalyse mit der Open Software "Gephi". Die zahlreichen Tutorials 
               
                  (https://gephi.org/users/),
               
                die aktive Nutzercommunity 
               
                  (https://gephi.wordpress.com/),
               
                die intuitive Nutzermaske und die vielfältigen Projekten, die bereits umgesetzt wurden, sind nur einige der Gründe, die für die Anwendung von „Gephi“ sprechen. Diese und andere Entscheidungskriterien, die in diesem Beispiel zur Wahl von „Gephi“ geführt haben, werden im Posterbeitrag veranschaulicht.
            
         
         
            Zweck des Posters
            
               Der Beitrag soll einen Überblick zu den existierenden Angeboten von Netzwerkanalysen geben sowie Vor-und Nachteile zur Diskussion stellen. Am Beispiel des eigenen Promotionsvorhabens werden Anregungen gegeben, Schritte erläutert und der Entscheidungsprozess begleitet. Das Ziel ist es aus der Perspektive eines Anfängers, Möglichkeiten aufzuzeigen wie man anfängliche Herausforderungen meistern kann. Der Beitrag wendet sich an Ein-und Quereinsteiger und soll durch eine konzeptionelle Gestaltung vermitteln, welches Potenzial und welcher Mehrwert in Netzwerkanalysen steckt. Damit stellt dieses Poster zu gleichen Teilen eine Absichtserklärung und einen Erfahrungsbericht dar. 
            
         
      
      
         
            
               Bibliographie
               
                  Bastian M. / Heymann S., Jacomy M. (2009):
                  Gephi: an open source software for exploring and manipulating networks,
                        International AAAI Conference on Weblogs and Social Media. 
                        https://gephi.org/publications/gephi-bastian-feb09.pdf
               
               
                  Düring, Marten. (2017):
                  Historical Network Research. Network Analysis in the Historical Disciplines.
                  
                     http://historicalnetworkresearch.org/.
                  
               
               
                  Düring, Marten / Kerschbaumer, Florian (2016):
                  Quantifizierung und Visualisierung. Anknüpfungspunkte in den Geschichtswissenschaften, 
                        in: 
                        Düring, Marten / Eumann, Ulrich / Stark, Martin / von Keyserlingk, Linda (eds.):
                  Handbuch Historische Netzwerkforschung. Grundlagen und Anwendungen. 
                        Münster: Lit Verlag 31- 44.
                    
               
                  Grandjean, M. (2015):
                  GEPHI – Introduction to Network Analysis and Visualization. 
                        http://www.martingrandjean.ch/gephi-introduction/
                    
               
                  Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (2017):
                  Digital Humanities. Eine Einführung. 
                        Stuttgart: J. B. Metzler.
                    
               
                  Milligan, I. (2015):
                  From Dataverse to Gephi: Network Analysis on our Data, A Step-by-Step Walkthrough. 
                        https://ianmilligan.ca/2015/12/11/from-dataverse-to-gephi-network-analysis-on-our-data/
                    
               
                  Scott, John. 
                  What Is Social Network Analysis? 
                        London: Bloomsbury Academic, 2013.
                    
            
         
      
   



      
         
            Einleitung
            In diesem Beitrag wird ein Verfahren vorgestellt, das Netzwerkvisualisierungen dramatischer Texte für eine spezifische Form der kommunikativen Interaktion zwischen Figuren fokussiert.
            Es wird gezeigt, inwiefern gewichtete, gerichtete und dynamische Figurennetzwerke narrative Informationsvermittlung in der Figurenrede visualisieren können und auf diesem Weg dramennarratologische Analysen bzw. Annotationen ausgewertet werden.
            Im Gegensatz zu literaturwissenschaftlichen Netzwerkanalysen, die um die automatisierte Analyse 
                    des „kompositorische[n] Grundgerüst[s]“ 
                    
                    (Trilcke 2013: 224) von großen Dramenkorpora 
                    
                    (Piper et al 2017; Trilcke et al. 2015) bemüht sind, steht in diesem Beitrag also die Visualisierung von manuellen Annotationen im Vordergrund.
                
            Darüber hinaus werden mit Rückgriff auf die ermittelten Netzwerkdaten Deutungspotenziale exemplarisch an Kleists 
                    Die Familie Schroffenstein (DFS) diskutiert.1 Das Erkenntnisinteresse zielt also auf zwei Aspekte: (1) Inwiefern lassen sich narrative Redebeiträge, die ein zentrales Element der inneren und äußeren Informationsvermittlung im Drama (Pfister 2001: 20-22) sind, durch Annotationen netzwerkgraphisch visualisieren? (2) Inwiefern stellt die literaturwissenschaftliche Netzwerkanalyse in diesem Kontext einen Mehrwert dar?
                
         
         
            Annotation narrativer Figurenrede
            Ausgangspunkt der vorgestellten Netzwerke ist eine Typologie narrativer Figurenrede bzw. von Binnenerzählungen, die zur Annotation der Dramen Heinrich von Kleists genutzt wurden.2 Dabei wurden über 800 Vorkommnisse narrativer Figurenrede in den Dramen manuell annotiert. In einem ersten Schritt unterscheidet die Typologie zwischen narrativen Äußerungen, mit denen Figuren über ihre eigene Wirklichkeit erzählen, und narrativer Figurenrede, bei der das nicht der Fall ist. Der erste Phänomentyp, die horizontalen Binnenerzählungen, können mit dem narratologischen Kategorieninventar zur Beschreibung anachronen Erzählens (Lahn & Meister 2008: 138-141) genauer beschrieben und annotiert werden:3
            
            
               
                  Binnenerzählungen
               
               
                  Horizontal
                  Analepsen
                  488
               
               
                  
                  Simullepsen
                  123
               
               
                  
                  Prolepsen
                  33
               
               
                  Vertikal
                  Pseudoanalepsen
                  33
               
               
                  
                  Pseudosimullepsen
                  6
               
               
                  
                  Pseudoprolepse
                  29
               
            
            Tabelle 1: Vorkommen narrativer Figurenrede in Kleists Dramen
            Diese manuellen Annotationen sind die Grundlage dafür, dass unterschiedliche Formen der Informationsvermittlung netzwerkgraphisch visualisiert werden können.
         
         
            Netzwerkerstellung
            Unter Rückgriff auf die Annotationen der narrativen Figurenrede und die TEI-Annotationen von Sprecherfiguren und Szenen- sowie Aktwechseln im TextGrid-Korpus wurden chronologisierte Sender-Adressaten-Kanten erstellt. Dazu wurden die vier Sprecherfiguren, die auf eine narrative Äußerung folgen oder ihr vorangehen, als Adressaten berücksichtigt, sofern keine Akt- oder Szenenwechsel zwischen narrativer Figurenrede und potenziellem Adressaten liegt und es sich um unterschiedliche Figuren handelt. Die Anzahl der erzeugten Kanten ist somit deutlich höher als die Anzahl der narrativen Äußerungen (siehe exemplarisch in Tabelle 2 die Kanten 4 und 5 sowie 8-10). 
            
               
                  Id
                  
                     Source (Sprecher)
                  
                  
                     Target (Adressat)
                  
                  
                     Label (Akt)
                  
                  
                     Timeset (Beginn & Ende)
                  
                  
                     Weight
                  
               
               
                  1
                  Rupert
                  Eustache
                  1
                  
                     ""
                  
                  1
               
               
                  2
                  Rupert
                  Jeronimus
                  1
                  
                     ""
                  
                  1
               
               
                  3
                  Rupert
                  Ottokar
                  1
                  
                     ""
                  
                  1
               
               
                  4
                  Jeronimus
                  Ottokar
                  1
                  
                     ""
                  
                  1
               
               
                  5
                  Jeronimus
                  Ottokar
                  1
                  
                     ""
                  
                  2
               
               
                  6
                  Ottokar
                  Jeronimus
                  1
                  
                     ""
                  
                  1
               
               
                  7
                  Jeronimus
                  Kirchenvogt
                  1
                  
                     ""
                  
                  1
               
               
                  8
                  Kirchenvogt
                  Jeronimus
                  1
                  
                     ""
                  
                  1
               
               
                  9
                  Kirchenvogt
                  Jeronimus
                  1
                  
                     ""
                  
                  2
               
               
                  10
                  Kirchenvogt
                  Jeronimus
                  1
                  
                     ""
                  
                  3
               
            
            Tabelle 2: Auszug aus der Kantenliste zu DFS
            Das Kantengewicht 
                (Weight) zwischen einer Sender- und einer Adressatenfigur steigt mit jeder narrativen Äußerung, die eine Senderfigur im ‚Beisein‘ derselben Adressatenfigur äußert (siehe z.B. Kante 4 und 5). 
                
            Die Figurennetzwerke, die auf dieser Grundlage erstellt werden, illustrieren, welche Figuren sich zu welchem Zeitpunkt des Dramenverlaufs narrativ äußern, welche Figuren narrative Informationen bekommen und wie häufig Figuren an narrativem Informationsaustausch beteiligt sind.
         
         
            Visualisierungs- und Analysebeispiele
            Abbildung 1 zeigt dieses Potential der Netzwerkvisualisierung exemplarisch für 
                    Die Familie Schroffenstein.4
            
            
               
               
                  Abbildung 1.
                        Narrative Informationsvermittlung in DFS
               
            
            Die Größe der Knotenbeschriftung repräsentiert hier die betweenness centrality5 der Figuren und damit den Stellenwert bzw. Einfluss der Figur auf die narrative Informationsvermittlung innerhalb des Dramas.
                
            
               
                  Label
                  
                     betweenness centrality
                  
                  
                     weighted indegree
                  
                  
                     weighted outdegree
                  
                  indegree
                  outdegree
                  degree
               
               
                  Jeronimus
                  125,45
                  99
                  82
                  25
                  28
                  53
               
               
                  Gertrude
                  98,74
                  46
                  70
                  15
                  15
                  30
               
               
                  Sylvester
                  85,37
                  107
                  31
                  29
                  14
                  43
               
               
                  Rupert
                  74,47
                  145
                  17
                  36
                  10
                  46
               
               
                  Agnes
                  45,90
                  59
                  85
                  18
                  23
                  41
               
               
                  Eustache
                  17,25
                  37
                  73
                  14
                  19
                  33
               
               
                  Ottokar
                  5,11
                  180
                  51
                  33
                  14
                  47
               
               
                  Ursula
                  3,31
                  1
                  4
                  1
                  4
                  5
               
               
                  Santing
                  2,94
                  10
                  49
                  6
                  12
                  18
               
               
                  Johann
                  2,35
                  4
                  106
                  3
                  15
                  18
               
               
                  Kirchenvogt
                  0,00
                  1
                  80
                  1
                  14
                  15
               
               
                  Ein Diener
                  0,00
                  2
                  2
                  2
                  2
                  4
               
               
                  Sylvius
                  0,00
                  7
                  3
                  4
                  2
                  6
               
               
                  Gärtner
                  0,00
                  2
                  0
                  2
                  0
                  2
               
               
                  Aldöbern
                  0,00
                  1
                  0
                  1
                  0
                  1
               
               
                  Theistiner
                  0,00
                  5
                  15
                  4
                  5
                  9
               
               
                  Der Wanderer
                  0,00
                  0
                  1
                  0
                  1
                  1
               
               
                  Zweiter Wanderer
                  0,00
                  0
                  4
                  0
                  3
                  3
               
               
                  Die Kammerzofe
                  0,00
                  2
                  13
                  2
                  6
                  8
               
               
                  Barnabe
                  0,00
                  1
                  21
                  1
                  8
                  9
               
               
                  Ein Ritter
                  0,00
                  0
                  2
                  0
                  2
                  2
               
            
            Tabelle 3: Betweenness centrality, (weighted) in- und outdegree in DFS
            Schon anhand dieses Beispiels und der netzwerkmetrischen Daten in Tabelle 3 lassen sich einige Vorzüge einer netzwerkgraphischen Annotationsauswertung zeigen:
            
               Die Reichweite einer Figurenerzählung wird im Hinblick auf den Adressatenkreis ermittelt.
               Die narrativen Funktionen der Figuren werden durch ihre Netzwerkeigenschaften erkennbar:
                        
                     
                        Brückenfiguren: hohe betweenness centrality; Verbindung getrennter Netzwerkbereiche 
                                (Trilcke 2013: 217): z.B. Jeronimus (siehe hier und nachfolgend Tabelle 2).
                            
                     
                        Botenfiguren: Geringe betweenness centrality; mind. eine narrative Äußerung: 
                                
                           
                              Botenfiguren i.e.S., die 
                                        nach dem ersten Akt erzählend in Erscheinung treten: z.B. die Wanderer, der Ritter und Barnabe.
                                    
                           
                              Expositionsfiguren, die 
                                        im ersten Akt/Dramenteil erzählend in Erscheinung treten: z.B. der Kirchenvogt.6
                           
                        
                     
                     
                        Zielfiguren: hohe betweenness centrality; hoher gewichteter Eingangsgrad; geringer gewichteter Ausgangsgrad: z.B. Sylvester und Rupert, die häufig die Adressaten, aber selten die Sprecher narrativer Figurenrede sind. (Die Handlung des hier gewählten Beispieltexts legt die These nahe, dass diese Figuren entscheidungsmächtige Figuren sind und daher zahlreiche Informationen bekommen.)
                            
                     
                        Figuren der Informationskontrolle: hohe betweenness centrality; sehr hoher Ausgangsgrad; Netzwerke mit geringer Kantendichte: z.B. Hermann in Kleists 
                                Hermannsschlacht.
                        7
                     
                  
               
               Es lassen sich Figurenpaare und Netzwerkbereiche identifizieren, zwischen denen es keinen oder nur vermittelten Informationsaustausch gibt. Hier sind natürlich Dyaden besonders interessant, bei denen die beiden Figuren eine hohe betweenness centrality aufweisen: z.B. Rupert und Sylvester.
               Die Informationsstrukturen geben Aufschluss über den allgemeinen Grad der Informiertheit der Figuren: z.B. die Kantendichte.8
               
            
            Zudem können unterschiedliche Formen der narrativen Figurenrede netzwerkgraphisch miteinander verglichen werden. Die Abbildungen 3 und 4 zeigen dies exemplarisch. In Abbildung 3 werden Figurenerzählungen visualisiert, in denen sich Figuren in Übereinstimmung mit der fiktionalen Wirklichkeit äußern. Abbildung 4 zeigt narrative Äußerungen, bei denen das Gegenteil der Fall ist. Es handelt sich also um narrative Falschaussagen. Der Vergleich ist in diesem Fall aufgrund der vorhandenen Parallelen 
                    und Unterschiede aufschlussreich. Bei beiden Netzwerken behält Jeronimus die zentrale Position im Netzwerk. Hier schlägt sich nieder, dass er für die Verbreitung von wirklichkeitsgemäßen Informationen ebenso verantwortlich ist, wie für die Verbreitung von falschen Verdächtigungen, Lügen und Vorurteilen. Agnes‘ Netzwerkposition verändert sich hingegen stark (Abb. 4). Sie ist innerhalb ihrer Familie und in der kommunikativen Interaktion mit Ottokar die zentrale Figur bei der Weitergabe falscher Informationen.
                
         
         
            Informationsvermittlung im Dramenverlauf: Dynamische Netzwerke
            Wie 
                    
                    Agarwal et al. (2012: 94) gezeigt haben, hat die Erstellung von dynamischen Netzwerken den Vorteil, die Veränderlichkeit der netzwerkmetrischen Eigenschaften einer Figur, einer Figurengruppe oder eines gesamten Netzwerks im Verlauf eines Roman- oder Dramengeschehens berücksichtigen zu können. Das bestätigen die netzwerkmetrischen Auswertungen der 
                    Familie Schroffenstein in Abbildung 2. Hier wird der gewichtete Ausgangsgrad für vier ausgewählte Figuren aktweise dokumentiert. So tritt der Kirchenvogt narrativ nur im ersten Akt in Erscheinung, was seine Funktion als Expositionsfigur unterstreicht. Auch Barnabes Funktion als Vermittlerin von Anagnorisis-Informationen zum Dramenende spiegelt sich wider. Jeronimus Bedeutung relativiert sich, weil ersichtlich wird, dass er – aufgrund seiner Ermordung am Ende des dritten Akts – nur in den ersten drei Akten als informationsvermittelnde Figur auftritt. Seine hohen Werte in Akt zwei und drei zeigen jedoch, dass er für den Handlungsverlauf entscheidende Informationen äußert. Bei Rupert bestätigt sich seine geringe narrative Aktivität (Tabelle 3) als ein relativ konstantes Verhalten. Der niedrige gewichtete Ausgangsgrad für das gesamte Drama ist also nicht darauf zurückzuführen, dass er nur in wenigen Szenen (erzählerisch) in Erscheinung tritt. 
                
            
               
               
                  Abbildung 2.
                        Gewichteter Ausgangsgrad im Dramenverlauf in DFS
               
            
         
         
            Schluss
            Solange die automatische Annotation narrativer Figurenrede nicht möglich ist, setzt das vorgestellte Verfahren einen relativ großen Annotationsaufwand voraus. Es ermöglicht somit keinen umfassenden Vergleich von Dramen, was unter anderem zur Einordnung der vorgestellten quantitativen Netzwerkanalysen wünschenswert wäre.
            In diesem Beitrag wurde jedoch exemplarisch gezeigt, inwiefern netzwerkgraphische Visualisierungen für die Auswertung narratologischer Annotationen einen analytischen Mehrwert haben können. Die formalen Annotationen können und sollen durch inhaltsbezogene Annotationen angereichert werden. Auf dieser Grundlage könnte netzwerkgraphisch der Informationsaustausch über bestimmte Themen oder Figuren visualisiert werden.
         
         
            Anhang
            
               
               
                  Abbildung 3. Narrative Informationsvermittlung (Horizontale Binnenerzählungen/Wirklichkeitserzählungen) in DFS 
            
            
               
               
                   Abbildung 4.
                        Narrative Informationsvermittlung (Pseudoanalepsen/Falschaussagen) in DFS
               
            
            
               
                  Label
                  
                     betweenness centrality
                  
                  
                     weighted indegree
                  
                  
                     weighted outdegree
                  
                  indegree
                  outdegree
               
               
                  Hermann
                  482,7
                  66
                  47
                  36
                  27
               
               
                  Thuiskomar
                  107,25
                  5
                  10
                  5
                  6
               
               
                  Thusnelda
                  69,62
                  28
                  25
                  13
                  9
               
               
                  Ventidius
                  61,95
                  7
                  27
                  5
                  14
               
               
                  Varus
                  57,8
                  10
                  9
                  6
                  8
               
               
                  Dagobert
                  27
                  3
                  3
                  2
                  3
               
               
                  Zweite Hauptmann
                  21
                  2
                  1
                  2
                  1
               
               
                  Gertrud
                  20
                  7
                  6
                  4
                  3
               
               
                  Marbod
                  16,2
                  11
                  5
                  7
                  4
               
               
                  Wolf
                  11,96
                  5
                  4
                  5
                  4
               
               
                  Aristan
                  8,5
                  1
                  5
                  1
                  4
               
               
                  Rinold
                  6
                  1
                  5
                  1
                  4
               
               
                  Erste Cherusker
                  6
                  1
                  5
                  1
                  4
               
               
                  Der zweite Cherusker
                  5,2
                  1
                  2
                  1
                  2
               
               
                  Gueltar
                  5
                  1
                  2
                  1
                  2
               
               
                  Der Mann
                  4
                  1
                  3
                  1
                  3
               
               
                  Das Volk
                  2
                  2
                  1
                  2
                  1
               
               
                  Erste Älteste
                  0,5
                  1
                  1
                  1
                  1
               
               
                  Scäpio
                  0,33
                  1
                  5
                  1
                  4
               
            
            Tabelle 4: Figuren mit höchster betweenness centrality in Kleist Hermannsschlacht
            
         
      
      
         
             Textgrundlage der Annotationen war Kleists Werkausgabe von Siegfried Streller, die durch das TextGrid Repositorium digital zur Verfügung steht: https://textgrid.de/en/digitale-bibliothek.
             Dazu wurde das Annotationstool CATMA (Meister et al. 2016) verwendet. CATMA bietet die Möglichkeit, mit selbstdefinierten literaturwissenschaftlichen Analysetaxonomien zu annotieren und ist damit für narratologische Forschungsprozesse besonders geeignet.
             Grundlegend für die Annotation ist ein Narrativitätskonzept, das berücksichtigt, dass Texte aller Gattungen narrative Elemente enthalten können, wie es u.a. Wolf (2002) beschreibt. Zu der narratologischen Terminologie vgl. Lahn/Meister 2016: 147-149.
             Alle Visualisierungen und Netzwerkanalysen wurden mit dem Tool GEPHI (Bastian et al. 2008) erstellt.
             Mit der betweenness centrality wird gemessen, für wieviele Knotenpaare ein Knoten den kürzesten Netzwerkpfad darstellt. Eine hohe betweenness centrality in Netzwerken, die Informationsflüsse abbilden, indiziert also einen großen Einfluss der Figur auf die Informationsvermittlung im Netzwerk, da sie als Brückenfigur fungiert.
             Vgl. zum Unterschied Pfister 2001: 280f.
             Kantendichte der Hermannsschlacht: 0,051; Kantendichte Die Familie Schroffenstein: 0,388. Siehe zur Hermannsschlacht Tabelle 4 im Anhang, in der sich Hermanns propagandistische „Überzeugungsarbeit“ (Müller-Salget 2009: 78) widerspiegelt.
             Hier ist zu berücksichtigen, dass die Kantendichte natürlich auch durch andere Faktoren beeinflusst wird (Trilcke 2013: 225).
         
         
            
               Bibliographie
               
                  
                  Agarwal, A. / A. Corvalan / J. Jensen / O. Rambow (2012):
                  Social Network Analysis of Alice in Wonderland, 
                        Proceedings of the Workshop on Computational Linguistics for Literature: 88–96.
                    
               
                  
                  Bastian, M. / S. Heymann / M. Jacomy (2008):
                  Gephi: An open source sofware for exploring and manipulating networks. 
                        AVI 2008 – Proceedings of the Working Conference on Advanced Visual Interfaces.
                    
               
                  
                  Lahn, Silke/ J. C. Meister (2008):
                  Einführung in die Erzähltextanalyse. Stuttgart: Verlag J.B. Metzler.
                    
               
                  Meister, J. C. / M. Petris / E. Gius / J. Jacke (2016): 
                  CATMA 5.0. software for text annotation and analysis.
                    
               
                  
                  Moretti, F. (2011):
                  Network Theory, Plot Analysis. 
                        Stanford Literary Lab Pamphlets 2.
                    
               
                  
                  Müller-Salget, Klaus (2009):
                  Die Herrmannsschlacht, 
                        in: 
                        Ingo Breuer (Hg.):
                  Kleist-Handbuch. Leben – Werk – Wirkung. 
                        Stuttgart: Verlag J.B. Metzler. S. 76-79.
                    
               
                  Piper, Andrew / Mark Algee-Hewitt / Koustuv Sinha / Derek Ruths / Hardik Vala (2017):
                  Studying Literary Characters and Character Networks.
                        Digital Humanities 2017, Conference Abstracts.
                    
               
                  
                  Pfister, Manfred (2001):
                  Das Drama. 
                        München. Wilhelm Fink Verlag.
                    
               
                  Trilcke, P. / F. Fischer / D. Kampkaspar (2015):
                  Digital Network Analysis of Dramatic Texts. 
                        Digital Humanities 2015: Book of Abstracts.
                    
               
                  
                  Trilcke, Peer (2013):
                  Social Network Analysis (SNA) als Methode einer textempirischen Literaturwissenschaft. 
                        Empirie in der Literaturwissenschaft. Hrsg. von Christoph Rauen, Katja Mellmann und Philip Ajouri. Münster: 201–247.
                    
               
                  
                  Wolf, Werner (2002):
                  Das Problem der Narrativität in Literatur, bildender Kunst und Musik: Ein Beitrag zu einer intermedialen Erzähltheorie. 
                        Erzähltheorie transgenerisch, intermedial, interdisziplinär. Hrsg. von Vera Nünning und Ansgar Nünning. Trier: 23–104.
                    
            
         
      
   



      
         
            Abstract
            Die Bedeutung von Social Media in den digitalen Geisteswissenschaften wächst. Nicht nur als Gegenstand der Analyse (z.B. in Gao et al. 2018 oder Reid 2011) sind Social Media für die Digital Humanities von Interesse, sondern auch zunehmend für die Dissemination von Forschungsergebnissen (vgl. Ross 2012). Vor allem in Blogs und Twitter wurde großes Potential für Diskussionen und die Verbreitung von Ergebnissen erkannt (vgl. Puschmann/Bastos 2015, Terras 2012). Auch in der Rezeptionsforschung der Wissenschaftskommunikation zeigt sich, dass Webmedien besonders relevant sind (vgl. Brossard 2013, 14096–14101) und dass diese darum in besonderem Maße zur 
                    „scientific literacy” beitragen könnten (vgl. Schäfer 2017, 283). Generell bietet (informelle) Wissenschaftskommunikation über Webmedien noch viel ungenutztes Potential (vgl. Schäfer 2017, 279–280, Neuberger 2014, Voigt 2012). Unser Beitrag zeigt, wie eine multimediale und multimodale webbasierte Strategie die Dissemination von Digital-Humanities-Methoden unterstützen und die Wissenschaftskommunikation des Forschungsfeldes stärken kann. Die quantitative Analyse der Erfolge dieser Strategie lässt Rückschlüsse darauf zu, welche Methode wem wie vermittelt werden sollte und bildet daher eine wichtige Basis für die Konzeption von Forschungsprojekten und der universitären Lehre.
                
         
         
            Konzeptioneller Rahmen – Multimedialität, Multimodalität und Codierungssysteme in forTEXT
            forTEXT ist ein Vermittlungsprojekt für digitale Methoden der Textanalyse, das sich vor allem an Forschende richtet, die bisher noch nicht mit digitalen Methoden arbeiten (siehe 
                    
                  https://fortext.net).
                Neben der ‘analogen’ Dissemination in Workshops und universitärer Lehre wird auch ein Schwerpunkt auf die online-Vermittlung gelegt, da an wissenschaftlichen Themen Interessierte diese Kanäle häufig als Informationsquelle nutzen (vgl. Brossard 2013, 14098). Die hier vorgestellte webbasierte Strategie als Teil des Disseminationskonzeptes in forTEXT soll darüber hinaus zur DH-Wissenschaftskommunikation beitragen und so die Sichtbarkeit des Forschungsgebiets erhöhen (zur Bedeutung der Geisteswissenschaften in der Wissenschaftskommunikation vgl. Scheu/Volpers 2017). Für die forTEXT-Disseminationsstrategie sind Multimedialität, Multimodalität und multiple Codierungen zentrale Aspekte, die wir wie folgt definieren:
                
            
               Multimedialität: Aufbereitung und/oder Nutzung unterschiedlicher medialer Kanäle. 
                    „Medium” verstehen wir wie Roesler/Stiegler (2005, 150–152) als Vermittlungssystem innerhalb eines Kommunikationsprozesses, bei dem auch das Medium selbst Teil der Vermittlung ist.
                
            
               Multimodalität: Aufbereitung und/oder Nutzung unterschiedlicher Kommunikationsmodi. Dabei verstehen wir 
                    „Modus” als Bezeichnung für eine semiotische Einheit wie z.B. Design oder Sprache (vgl. Bucher 2007, 53).
                
            
               Multiple kulturelle Codierung: Wir übernehmen hier ein semiotisches Verständnis von 
                    „Code
                    ” als Bezeichnung für ein System relevanter Informationseinheiten (vgl. Eco 1985, 58f.). Kulturelle Codes funktionieren als Bedeutungsnetz aus Referenzen auf ein kollektives Wissenskorpus (vgl. Barthes 1976, 25). Um den Begriff klar vom informationstechnologischen (Binär-)Code zu trennen, sprechen wir von Codierung oder Codierungssystem.
                
            Medien, Modi und Codes wirken auf unterschiedlichen Ebenen des Vermittlungsprozesses. Dabei sind Medien und Modi stark miteinander verbunden. Modi können aber als Varianten in andere Medien übertragen werden. Codes sind inhaltliche Elemente, weshalb sie für die Dissemination von Forschungsergebnissen zentral sind. Sie können sich auf einen Modus in einem Medium beziehen oder modi- und medienübergreifend sein: 
            
               
            
         
         
            Arbeitspraxis – die webbasierte Disseminationsstrategie
            
               Die forTEXT-Webseite als Basis medialer Vermittlung von Digital-Humanities-Inhalten
               
                  
               
               Das zentrale Vermittlungsmedium in forTEXT ist die Projektwebseite. Hier werden in Textbeiträgen sowohl Bilder als auch Videos eingebettet. Die forTEXT-Webseite bildet die Basis für die multimediale Web-Strategie, da hier grundlegende Modi und kulturelle Codierungen umgesetzt wurden, die in den sozialen Medien erweitert werden. Die primär genutzten Modi und ihre kulturellen Codierungen sind:
               
                  Design: Gedecktes Farbschema und serifenlose Schrift stehen für Schlichtheit und Sachlichkeit. Nur im Logo gibt es verspielte Elemente, die an eine Handschrift erinnern und die Verbindung von Tradition und Modernität vermitteln.
                    
               
                  Sprache: Die Beiträge erfüllen die Ansprüche wissenschaftlichen Schreibens. Die Wissenschaftlichkeit wird durch die technische Funktionalität zum Zitieren unterstützt.
                    
               
                  Stimme: Grundsätzlich ist die Webseite mehrstimmig angelegt, da hier verschiedene Autor*innen schreiben. Alle nutzen einen sachlichen Tonfall und die implizite Leserin wird stets mit formellem 
                        „Sie” angesprochen.
                    
               
                  Bildlichkeit: Die eingebetteten Bilder sind zumeist digitale Repräsentationen der eingesetzten Tools und scheinen als solche zunächst gegenstandsneutral. Allerdings sind die Bilder häufig auch Visualisierungen der in forTEXT durchgeführten Fallstudien, d.h. sie zeigen nicht nur grafische, sondern auch textliche Elemente und verweisen auf die Modellierung eines Forschungsgegenstandes, die bei der Erstellung der Grafik stattgefunden haben muss.
                    
               Die forTEXT-Webseite richtet sich in erster Linie an drei Zielgruppen, die sich für die forTEXT-Disseminationsstrategie als besonders relevant erwiesen haben:
               
                  Studierende – Lernende der DH-Methodik
                  Nachwuchswissenschaftler*innen – Umsetzende der DH-Methodik
                  Digitale Geisteswissenschaftler*innen – Lehrende der DH-Methodik
               
            
            
               Social Media in forTEXT
               Ausgehend von den Inhalten der Webseite, deren Modi und den entsprechenden Codierungen werden drei soziale Medien zur Vermittlung genutzt. Anders als die Webseite sollen die Social-Media-Kanäle jeweils primär eine Zielgruppe erreichen: YouTube vor allem Zielgruppe 1, Pinterest Zielgruppe 2 und Twitter Zielgruppe 3.
               YouTube
               Auf YouTube erstellen wir eigene Inhalte, die die Artikel der Webseite aufgreifen, weiterführen und ergänzen. Es gibt derzeit zwei Inhaltstypen; Fallstudien und Tutorials. Beide können als Open-Educational-Ressources genutzt werden. In methodischen Fallstudien wird zum Beispiel mittels NER verglichen, welche Bedeutung die Hauptfiguren in Goethes 
                        Werther und in Plenzdorfs 
                        neuem Werther haben. Wir erklären, wie die NER-Machine-Learning-Prozesse funktionieren und verlinken sowohl zur Webseite als auch zu forTEXT-Tutorials. In den forTEXT-NER-Tutorials wird in drei kleinen Einheiten die Installation, Anwendung und das Training eines eigenen NER-Modells gelehrt.
                    
               
                  
               
               Design und sprachliche Elemente der forTEXT-YouTube-Videos richten sich nach den Vorgaben der Webseite. Abbildungen der eingesetzten Tools werden ergänzt von piktografischen Animationen. Diese sind zwar schlicht, befördern jedoch Unvoreingenommenheit und Autodidaktik. Dem Vorurteil einer geringeren Technikaffinität weiblicher Menschen begegnen wir mit einem weiblichen Voice-over (vgl. Schelhowe 2000). Dadurch werden Schwellenängste abgebaut und der Eindruck vermittelt, dass Nutzer*innen und digitale Tutorin sich gemeinsam autodidaktisch an die Methoden heranwagen.
               Pinterest
               
                  
               
               Die Anpassbarkeit in Hinblick auf Design, Stimme und Bildlichkeit der Kommunikationsmodi ist bei Pinterest am geringsten. Hier werden überwiegend fremde Artikel 
                        „gepinnt”, die lediglich mit einer kurzen Beschreibung angereichert werden. Auch führt die Besonderheit von Pinterest als Chimäre zwischen sozialem Medium und Suchmaschine dazu, dass die sprachlichen Elemente nicht nur für die menschliche Wahrnehmung, sondern insbesondere technologisch eine Rolle spielen. Primär werden hier DH-Forschende angesprochen, die Pinnwände für Tools (z.B. Stanford-NER, Carto, Gephi, CATMA), einzelne Methoden (z.B. Netzwerkanalyse, Stilometrie, Topic Modeling), Diskussionen und viele andere Themen der Digital Humanities finden.
                    
               Twitter
               Bei Twitter sind Layout und Typografie der Tweets nicht veränderbar. Jedoch wird bei jedem Tweet das forTEXT-Logo angezeigt. Im Gegensatz zu Pinterest ist auf Twitter die Ausgestaltung der Stimme bedeutsam. Hier steht die Kommunikation mit der eigenen Forschungscommunity im Vordergrund. Die Beschränktheit der Tweets auf 280 Zeichen führt dazu, dass eher Fachbegriffe als Umschreibungen genutzt werden. Hashtags führen zu Themen, die für die Community bedeutsam sind und folgen einem kulturellen Sprachcode. Hier nimmt forTEXT an kollegialen Insider-Gesprächen Teil und betont die forschungsrelevante Seite des Projektes.
            
         
         
            Quantitative Analyse
            Die webbasierte forTEXT-Disseminationsstrategie wird regelmäßig quantitativ ausgewertet. Zusätzlich zur kontinuierlichen Steigerung von Aufmerksamkeit für das Projekt (quantitativ messbar durch Impressionen, Interaktionen, Betrachtungszeiten), werden auch Analysen durchgeführt, die eher konzeptionelle Aspekte der webbasierten Dissemination von Forschungsmethoden in den Fokus rücken. Auf Basis dieser Analysen entwickeln wir eigene Relevanzmetriken, die neben quantitativen auch qualitative Aspekte berücksichtigen, wie bspw. demografische Daten, die anzeigen, welche Zielgruppen über welche Medien, welche Modi und welche Codes tatsächlich erreicht werden können, aber auch Kommentare, Feedback und Interaktionen mit anderen Nutzer*innen.
            Zum jetzigen Zeitpunkt läuft die forTEXT-Social-Media-Arbeit seit drei Monaten. Auf allen medialen Kanälen zeigt sich bereits eine steigende Aufmerksamkeit, auch wenn die Zahlenwerte nach Medium stark differieren. Twitter erzielt mit durchschnittlich 20.000 Impressionen im Monat quantitativ die größte Reichweite. Auch die Interaktionsrate ist mit bis zu 7,4% relativ hoch – die Zielgruppe 3 kann hier sehr gut erreicht werden. Mit Pinterest konnten in den ersten drei Monaten durchschnittlich 1.737 monatliche Impressionen erreicht werden, wobei die einzelnen Monate mit 780 Betrachtern im ersten Monat und 9.300 Betrachtern im dritten Monat stark schwanken. Hier zeigt sich, dass die mit maschinellem Lernen verknüpfte Suchmaschine Pinterest länger braucht, um Inhalte und Interessierte zusammen zu bringen. Neue Inhalte müssen regelmäßig und vergleichsweise hochfrequent (derzeit fünf tägliche Pins) verlinkt werden, damit die Pinterest-Algorithmen ein Profil einzuordnen lernen und anderen Nutzer*innen empfehlen. Eine Einsicht aus der Analyse der Pinterest-Daten ist, dass hier insbesondere Nutzerinnen erreicht werden können. Die Vermittlung von forTEXT-Inhalten über YouTube läuft derzeit erst etwa einen Monat, sodass die Zahlenwerte (140 Impressionen im September) noch relativ gering sind. Qualitative Rückmeldung zeigt aber, dass die Videos bisher vor allem im Rahmen der DH-Lehre auf Interesse stoßen.
            Bereits zu diesem frühen Zeitpunkt zeigt die Fallstudie des forTEXT-Projektes, welche Aspekte einer multimedialen, multimodalen und multipel codierten Vermittlungsstrategie sich als produktiv erweisen. Die Vorannahme, dass auf Twitter vor allem die eigene Community erreichbar ist, hat sich bestätigt. Hingegen deutet der Gender-Gap auf Pinterest an, dass hier weniger Zielgruppe 2, sondern eher Zielgruppe 1 erreicht werden kann, da vor allem die Zielgruppe der Studierenden geisteswissenschaftlicher Fächer meist überwiegend weiblich ist. Aus Feedback zu den forTEXT-YouTube-Videos konnten wir erfahren, dass diese derzeit vor allem für Lehrende von Interesse sind. Neben den vor allem im Marketing üblichen Relevanzkriterien von Impressionen, Engagement und Interaktion (die auch für die wissenschaftliche Impactmessung fruchtbar gemacht werden können, vgl. Herb/Beucke 2013) ist für die Vermittlung von DH-Methoden daher die tatsächlich erreichte Zielgruppe und deren Nutzungsmotivation relevant. So kann forTEXT am Ende nicht nur selbst Social Media produktiv nutzen, sondern auch aufzeigen, welche Medien für welche Ziele der Vermittlung von DH-Methoden besonders bedeutend sind.
         
      
      
         
            
               Bibliographie
               
                  Barthes, Roland (1976): 
                  S/Z. Frankfurt am Main: Suhrkamp.
                    
               
                  Brossard, Dominique (2013): 
                  New media landscape and the science information consumer, 
                        in: PNAS 110 (3), 14096–14101. 
                        
                     https://www.pnas.org/content/pnas/110/Supplement_3/14096.full.pdf
                  , [Zugriff 21.12.2018]. 
                    
               
                  Bucher, Hans-Jürgen (2007): 
                  Textdesign und Multimodalität. Zur Semantik und Pragmatik medialer Gestaltungsformen, 
                        in: 
                        Roth, Kersten Sven / Spitzmüller, Jürgen (eds.):
                  Textdesign und Textwirkung in der massenmedialen Kommunikation. 
                        Konstanz: UVK, 49–76. 
                    
               
                  Eco, Umberto (1985): 
                  Einführung in die Semiotik. 
                        München: Fink.
                    
               
                  Herb, Ulrich / Beucke, Daniel (2013): 
                  Die Zukunft der Impact-Messung. Social Media, Nutzung und Zitate im World Wide Web, 
                        in: Wissenschaftsmanagement. Zeitschrift für Innovation 19 (4), 22–25. 
                        
                     https://publikationen.sulb.uni-saarland.de/bitstream/20.500.11880/23789/1/Die_Zukunft_der_Impact_Messung_fuer_Reps_fertig.pdf
                   [Zugriff: 21.12.2018].
                    
               
                  Gao, Jin / Nyhan, Julianne / Duke-Williams, Oliver / Mahony, Simon (2018): 
                  Visualising The Digital Humanities Community: A Comparison Study Between Citation Network And Social Network, 
                        in: Digital Humanities 2018. Book of Abstracts. Puentes-Bridges.
                    
               
                  Neuberger, Christian (2014): 
                  Social Media in der Wissenschaftsöffentlichkeit. Forschungsstand und Empfehlungen, 
                        in: 
                        Weingart, Peter / Schulz, Patricia (eds.): 
                  Wissen – Nachricht – Sensation. Zur Kommunikation zwischen Wissenschaft, Medien und Öffentlichkeit. 
                        Weilerswist: Velbrück, 315–368.
                    
               
                  Puschmann, Cornelius / Bastos, Marco (2015):
                  How Digital Are the Digital Humanities? An Analysis of Two Scholarly Blogging Platforms, 
                        in: PLOS ONE 10 (2): e0115035.
                        
                     https://doi.org/10.1371/journal.pone.0115035
                  
                   [Zugriff 2.10.2018].
               
               
                  Reid, Alexander (2011):
                  Social Media Assemblages in Digital Humanities: from Backchannel to Buzz, 
                        in: 
                        Wankel, Charles (ed.): 
                  Teaching Arts and Science with the New Social Media. 
                        West Yorkshire: Emerald Publishing, 321–338.
                    
               
                  Roesler, Alexander / Stiegler, Bernd (eds. 2005): 
                  Grundbegriffe der Medientheorie. 
                        Paderborn: UTB.
                    
               
                  Ross, Claire (2012): 
                  Social media for digital humanities and community engagement, 
                        in: 
                        Warwick, Claire / Terras, Melissa / Nyhann, Julianne (eds.): 
                  Digital Humanities in Practice. 
                        London: Facet Publishing.
                    
               
                  Schäfer, Mike S. (2017): 
                  Wissenschaftskommunikation online, 
                        in: 
                        Bonfadelli, Heinz et al. (eds.): 
                  Forschungsfeld Wissenschaftskommunikation. Wiesbaden: Springer. 
                        
                     https://link.springer.com/content/pdf/10.1007%2F978-3-658-12898-2.pdf
                   [Zugriff 21.12.2018].
                    
               
                  Schelhowe, Heidi (2000): 
                  Informatik, 
                        in: 
                        Braun, Christina von / Stephan, Inge (eds.): 
                  Gender-Studien. Eine Einführung. Stuttgart, Weimar: Metzler, 207–216.
                    
               
                  Scheu, Andreas M. / Volpers, Anna Maria (2017): 
                  Sozial- und Geisteswissenschaften im öffentlichen Diskurs, 
                        in: 
                        Bonfadelli, Heinz et al. (eds.): 
                  Forschungsfeld Wissenschaftskommunikation. Wiesbaden: Springer. 
                        
                     https://link.springer.com/content/pdf/10.1007%2F978-3-658-12898-2.pdf
                   [Zugriff 21.12.2018].
                    
               
                  Terras, Melissa (2012):
                  The Impact of Social Media on the Dissemination of Research: Results of an Experiment, 
                        in: Journal of Digital Humanities 1 (3).
                        
                     http://journalofdigitalhumanities.org/1-3/the-impact-of-social-media-on-the-dissemination-of-research-by-melissa-terras/
                  
                   [Zugriff 2.10.2018].
               
               
                  Voigt, Kristin (2012): 
                  Informelle Wissenschaftskommunikation und Social Media. 
                        Berlin: Frank & Timme.
                    
            
         
      
   



      
         Bevor Kulturerbe digital verfügbar und nachnutzbar ist, stehen komplexe Prozesse an, die als spezifische „invisible work“ (Star/Strauss 1999) bezeichnet werden können. Diese werden kaum außerhalb der engeren Community problematisiert: Wo finde ich in den analogen Katalogen, Zettelkästen, Findbüchern u.ä. die notwendigen Informationen zur Erfassung und Erschließung? Welche Sammlungsbestände sind besonders wichtig und deshalb zu digitalisieren? Ist eine Massen-Digitalisierung mit geringer Detailtiefe der Erfassung oder eine detaillierte Erschließung einiger Teilbestände sinnvoll? Wie können die Vorgehensweisen von unterschiedlichen Personen und Institutionen vereinheitlicht werden? Und wie können Informationen, die bereits digital vorliegen – etwa in Tabellen oder älteren Erfassungssystemen – an internationale Regelwerke und Standards zur Metadatenhaltung angepasst werden?
         Kernanliegen des vorgeschlagenen Beitrags ist es, für die Sammlungserschließung und Digitalisierung – gerade in Museen, aber auch in Archiven und Bibliotheken – Methoden, Tools und Analyseperspektiven der Digital Humanities stärker als bisher zu nutzen. Dafür werden mögliche Synergien und exemplarische Anwendungen in einer ersten Exploration aufgezeigt. Dabei wird auf eigene Projekterfahrungen zurückgegriffen, die im Kontext des Digitalisierungsvorhabens „Digitales Portal Alltagskulturen im Rheinland“ (2013-2017) gesammelt wurden. Diese sind bereichert um Perspektiven des DH-Forschungsverbundes an der Universität Hamburg, „Automatisierte Modellierung hermeneutischer Prozesse (hermA)“ (2017-2020).
         
         
            What (not) to do with 100.000 Pictures. Sammlungserschließung als Sisyphos-Arbeit?
            Im LVR-Institut für Landeskunde lagern umfangreiche Bestände, die nach Sammlungsgeschichte gegliedert und nur rudimentär erschlossen sind. Vor allem die Fotodokumentationen sind für Erschließung und Digitalisierung prädestiniert, zeigen sie doch plurale Facetten immateriellen Kulturerbes seit etwa 1900. Dazu kommen umfangreiche Materialien zu 31 schriftlichen Befragungen aus den 1970er bis 2000er Jahren sowie kleinere Einzelsammlungen. Die Fotobestände liegen in unterschiedlichen Negativformaten sowie als Dias chronologisch sortiert vor. Dazu sind Fotoabzüge auf Karteikarten geklebt, mit Metadaten versehen und thematisch abgelegt in einer über Jahrzehnte gewachsenen Systematik. Negative und Abzüge sind also unterschiedlich archiviert und nur in Einzelfällen einander zuzuordnen. Es bestehen auf Einzelplatzrechnern gepflegte Bestandslisten sowie eine Datenbank mit Teilinventarisierung. Seit 2013 werden im Rahmen der LVR-weiten Digitalisierungsstrategie Digitalisate erstellt, mit angereichert und Metadaten abgelegt, um sie mittelfristig öffentlich zugänglich zu machen. Die Systeme werden aktuell in die Datenbank digiCULT.web übertragen, wodurch mit dem LIDO-Format größere Datenmengen standardkonform publiziert werden können.
                
            Vergleichbare gewachsene Systeme gibt es in vielen Museen, Archiven und Bibliotheken. Gerade in kleineren Museen und Forschungseinrichtungen, deren Hauptaugenmerk auf der Ausstellung bzw. Forschung und weniger auf der Sammlungserschließung liegt, sind die Erfassungen unvollständig und in der Institutionsgeschichte von verschiedenen Akteuren mit spezifischen Wissenshintergründen und Systematiken erfolgt. Diese Systeme funktionieren vor allem aufgrund des Wissens von Archivar*innen und Forschenden (vgl. zu Wissenskonzepten Koch 2006): Sie wissen, welche Materialien wo liegen, welche Strukturen was auffindbar machen, was in welchen Kontexten verwendet wurde. Wenn Stellenwechsel oder Verrentungen anstehen, geht dieses Wissen verloren oder wird bruchstückhaft weitergegeben.
            Wenn analoge Bestände digitalisiert werden sollen, geht es in der Vorbereitung (z.B. der Antragsstellung für Drittmittel) um hohe technische Qualität, Fragen des Workflows sowie der Bereitstellung, einzuhaltende Standards oder eigene Präsentationsoberflächen. Die konkreten Arbeitsschritte zur Umsetzung dieser Zielvorgaben werden oft erst in der Projektrealisierung entwickelt. Gerade die Museumsdatenbanken sind geprägt von einer gewachsenen Pluralität, die in dieser Form nicht als veröffentlichungswürdig gelten und strukturelle Nachbearbeitungen erfordern.
            Für die hier exemplarisch stehenden Fotobestände des LVR-Instituts fiel die Entscheidung, sich nicht an den bestehenden (Ablage-)Systematiken zu orientieren. Neben anderen Argumenten war dabei die unvollständige und heterogene, thematisch abgelegte Erschließung per Karteikarte ausschlaggebend. Um eine Auswahl zur inhaltlichen Erschließung zu ermöglichen und analoge Vorarbeiten gering zu halten, wurden alle Negative und Dias digitalisiert (zur Problematik von Original und Kopie, die sich für Foto-Abzüge spezifisch stellt, vgl. Schönholz 2017). Im Zuge der Auftragsvergabe wurden Bestände erstmals gezählt und liegen als vollständige Digitalisate vor. Ein Meilenstein dieser Fleißarbeit waren die im Vergleich und Überblick erstmals digital verfügbaren Bildbestände.
            Doch was macht man mit 100.000 Fotos, die außer einer Dateibenennung keinerlei Informationen mit sich bringen? Hier werden die eingangs aufgeworfenen Fragen konkret. Vergleichbar mit den Diskussionen um Distant Reading (Moretti 2007 und 2016; Crane 2006) stellt sich ob der Verfügbarkeit der Digitalisate die Frage, wie diese zielführend erschlossen werden können. Wie findet man relevante Bildbestände für eine Tiefenerschließung? Wo helfen die bestehenden Metadaten zielführend?
         
         
            Maschinelle Unterstützung nutzen, aber wie?
            Für diese Arbeitsschritte sind Methoden der Digital Humanities vielversprechend. Zwar wird zunehmend die Frage nach der Nutzung von Digitalisaten als Open Data für die Wissensproduktion diskutiert, oft jedoch erst nach Abschluss der Erschließungsarbeiten. Nicht erst die publizierten Daten digitalen Kulturerbes können mit DH-Verfahren erforscht werden – diese sind bereits in der Erschließung enorm hilfreich. Zwei Verfahren aus dem Bereich des Machine Learning scheinen besonders erfolgsversprechend: maschinelle Bilderkennung sowie die Analyse bestehender Metadaten mittels Text Mining.
                
            Die großen Mengen unerschlossener Fotos können mit maschineller Bilderkennung hinsichtlich ihrer Ähnlichkeit gruppiert werden, wie es etwa PixPlot realisiert: Bildanordnungen im Vektorraum machen Schwerpunkte des Bestandes deutlich, außerdem lassen sich Subkorpora bilden, die mit Massenbearbeitung formal erschlossen werden können. In der Arbeitspraxis ist daneben das Identifizieren von Dubletten relevant: Wenn beispielsweise Abzüge des Archivs in der Vergangenheit abfotografiert wurden, existiert das Foto in unterschiedlichen Kopien im digitalisierten Bestand – manuell eine Suche nach der Nadel im Heuhaufen, automatisiert mit Bildvergleich gut zu identifizieren (vgl. die vielversprechenden Ansätze bei Schneider 2019). Auch ähnliche Aufnahmen, z.B. aus einer Bildserie, sind so zuzuordnen. Die inhaltliche Erschließung wird durch eine Ähnlichkeitssuche ebenfalls deutlich vereinfacht: Hat man etwa eine gute Aufnahme eines Gegenstandes, so lassen sich relativ eindeutig andere Abbildungen dessen im Bestand finden. Hier wäre jedoch eine menschliche Intervention (zumindest zu Beginn eines möglichen Active Learning-Verfahrens) aufgrund der feinen Unterschiede notwendig. Zudem sind zweifelsfrei die Trainingsdaten von großer Relevanz und sollten wo möglich aus bereits erschlossenen, vergleichbaren Kulturerbe-Datensätzen bestehen.
                
            Text Mining-Verfahren würden in GLAM-Datenbanken nicht nur aus dem Museumsbereich präzisiere Suchabfragen und gerade in bestehenden Erfassungen systematische Funde ermöglichen. Ansätze wie facettierte Suchmasken mit Linked Open Data, wie sie in Plattformen zur Datenpräsentation zunehmend realisiert werden, wären im Backend enorme Arbeitserleichterungen. Schon banale Automatisierungen wie Rechtschreibkorrektur und Vereinheitlichungen der Formalerschließung sind momentan in der Regel nicht vorgesehen. Sie kosten viel Zeit und Konzentration, sobald nicht simple Ersetzungen vorzunehmen sind. Gleichzeitig fehlt den Entwickler*innen der eingesetzten Datenbanken die zielgenaue Kollaboration mit entsprechender DH-Forschung zu konkreten Tools und Verfahren sowie der Testung von verschiedenen Funktionen für eine hohe Qualität der Ergebnisse, die vor der Übernahme in die Infrastruktur erfolgen muss. LOD wird zudem aktuell nur in Ausnahmen direkt in die Erfassungssysteme eingebunden – erst so könnte die Arbeit an Ontologien und konkreten Datensätzen gezielt verbunden werden. Dazu kommt die Notwendigkeit, die Erfassungen besser zu vernetzen; auch in Fällen, in denen dies erst nach der Veröffentlichung möglich oder notwendig wird. Hier sollte eine Öffnung für fortlaufende kollaborative Ergänzung und Korrektur von Daten in Verbindung mit der Erfassung von Paradaten (McIlvain 2013) geschaffen werden.
                
            Viel Zeit wird mit der Erzeugung von metacrap (Doctorow 2001) verbracht. Die messy Metadaten, die für viele Erfassungen – oft im Backend, aber auch publiziert – bestehen, sind durch Tools einfach zu identifizieren und automatisch zu beheben: Museum Analytics beispielsweise ermöglicht es, große Mengen von Museumsdaten zu analysieren, ist allerdings für publizierte Metadaten vorgesehen. Gerade in der Migration zwischen Datenbanksystemen sowie für den internen Gebrauch zwecks Qualitätskontrolle, Bestandssichtung und Entscheidung über Nachbearbeitungen vor der Veröffentlichung erlaubt dieses einen anderen Blick auf die Bestände. Das Tool Breve, das Tabellen visualisiert, könnte ergänzende Funktionen übernehmen. Die Entwicklungsvorhaben des Verbundprojektes GND4C oder des Projekts Qrator sind richtungsweisend, leider aber noch nicht verfügbar, und entsprechende Konferenzworkshops zu DH für Gedächtnisinstitutionen (Döhl/Voges 2019) erfreulich.
                
            Erste Ansätze zur Nutzung von DH-Analyseverfahren werden auch von Museumsseite diskutiert, etwa hinsichtlich Netzwerkanalyse der Sammlungsbestände (Werner 2019) oder Möglichkeiten der Visualisierung (Mayr/Windhager 2019), bilden dort jedoch (noch) die absolute Ausnahme. Dies spiegelt sich auch in den Programmen der entsprechenden Tagungen wie „Museums and the Internet“ oder denen der Fachgruppe Dokumentation des Deutschen Museumsbundes. Falls entsprechende Ansätze bereits genutzt werden, so geschieht dies wiederum weitestgehend als „invisible work“ (s.o.) ohne Darstellung in der (Forschungs)Öffentlichkeit. In weiten Teilen der entsprechenden GLAM-Community wird gerade von Museumsseite die DH noch zu wenig als möglicher Kooperationspartner wahrgenommen, um entsprechende Workflows und Implementierungen zu konzipieren.
                
            Die vorgestellten Zugänge könnten im Ergebnis der Implementierung nicht nur aufzeigen, welche Daten in einem Bestand enthalten sind, sondern auch, welche Leerstellen in der Erfassung noch geschlossen werden sollten. Von einer linearen Durchsicht, Überarbeitung und Freigabe der Datenbank-Einträge kann mit entsprechender Tool-Unterstützung – und einhergehender Interoperabilität! – zu einer gezielten Nachbearbeitung von Teilbeständen oder Vereinheitlichung einzelner Metadatenfelder übergegangen werden. So bleibt mehr Zeit für eine inhaltliche Erschließung und Analyse sowie die dringend notwendige epistemologischen Reflexionen dieser Prozesse.
         
         
            Fazit: DH-Verfahren in Sammlungsdatenbanken
            Was fehlt in der Gesamtschau momentan? Vor allem die Öffnung von Erschließungssystemen für die dargestellten Methoden sowie die Öffnung der entsprechenden Communities zueinander.
            Erforderlich ist dabei der frühzeitige Einbezug von bestehenden Tools und Analyseverfahren – lange vor der Veröffentlichung der Datensätze. Gerade in der Exploration weitestgehend nicht erfasster Bestände zur Vorbereitung der Erschließung und in der Qualitätskontrolle von Metadaten liegen große Potentiale, die noch zu wenig genutzt werden. Wenn gleichzeitig bereits tiefenerschlossene Bestände als Trainingsdaten genutzt werden, können die Ansätze auch unabhängig von der Nutzung konkreter Datenbanken gegenseitigen Mehrwert sowohl in der Methodenentwicklung als auch in der Erschließung bringen.
            Eine öffentliche Finanzierung und Weiterentwicklung von den entsprechenden Tools ist dabei dringend notwendig. Statt weiter in gewinnorientierte Software zu investieren, sollten Genossenschaften und Vereine gegründet und ausgebaut werden. Eine Unterstützung der Toolentwicklung durch entsprechend kompetente DHler*innen ist vielversprechend. So wäre etwa ein Hackathon zur Erweiterung von Erschließungssystemen eine Möglichkeit, um über das Tagesgeschäft hinausgehende Innovationen umzusetzen. Entsprechend erweiterte Datenbanken sollten viel häufiger auch in der Forschung verwendet werden, die aktuell noch viel zu oft in Form von Excel-Sheets (weiter-)arbeitet, obwohl Datenbanken mit erweiterten Funktionen existieren. Mit einfachen Import/Exportfunktionen innerhalb der Tools und Verbindungen zu Analyseverfahren könnten Forschungsumgebungen geschaffen werden, in denen kollaboratives Arbeiten gleichzeitig die Datensätze anreichert und Forschungsfragen beantwortet.
         
      
      
         
            
  Projektergebnisse unter https://alltagskulturen.lvr.de/. Das DFG-geförderte Projekt wurde durch die Autorin koordiniert, von Dagmar Hänel geleitet und maßgeblich auch durch den wissenschaftlichen Dokumentar im Projekt, Christian Baisch, vorangetrieben.

            
  Das von Gertraud Koch und Heike Zinsmeister geleitete Verbundprojekt befragt DH-Perspektiven auf Möglichkeiten zur Modellierung hermeneutischer Prozesse. Vgl.
  .

            
  Vgl. Sammlungsbeschreibungen unter
  .

            
  Faust Software, vgl.
  .

            
  Genutzt wird eine Weiterentwicklung von MediaFiler, vgl.
  .

            
  digiCULT.web als entitätsbasierte Online-Datenbank ist vorrangig für Museumsbestände entwickelt und baut auf CIDOC-CRM auf. Vgl.
  .
    Zu LIDO vgl.
  .

            
  Vgl. etwa die Schwerpunktsetzung „Open Data – now what?“ der Sharing is Caring-Konferenz 2019.
  .
  Danke an Samantha Lutz für den Hinweis.

            
  Vgl. 
  .
  Vgl. auch Leonard 2019.

            
  Vgl. etwa Suchfacetten der DDB,
  , mittlerweile auch der Europeana,
  .

            
               .

            
               .

            
  Ziel 3 des Projektes ist die „Bereitstellung von Schnittstellen und Werkzeugen zur Unterstützung nicht-bibliothekarischer Anwendungskontexte.“ Vgl.
  .
  Danke an Axel Vitzthum für den Hinweis.

            
  Vgl.
  .
  GLAM-Institutionen mit digitalem Kulturerbe sind hier ein Anwendungsfall.

            
  Bisher waren entsprechende Beiträge bei der Tagung absolute Ausnahme. Vgl. das Archiv unter
  https://mai-tagung.lvr.de/de/startseite.html.

            
  Das Archiv der Tagungsprogramme und Vortragsfolien lässt nicht auf entsprechende Diskussionen schließen. Vgl. 
  https://www.museumsdokumentation.de/?lan=de&q=Who%20is%20who/FG%20Dokumentation%20im%20DMB/Tagungsarchiv.

         
         
            
               Bibliographie
               
                  Crane, Gregory (2006): “What Do You Do with a Million Books?” In: D
                        -Lib Magazine 12/3. http://www.dlib.org/dlib/march06/crane/03crane.html.
                    
               
                  Doctorow, Cory (2001): “Metacrap. Putting the Torch to Seven Straw-Men of the Meta-Utopia.” https://people.well.com/user/doctorow/metacrap.htm.
                    
               
                  Döhl, Frédéric / Voges, Ramon (2019): „Erklärt und ausprobiert – Digital Humanities für Gedächtnisinstitutionen.“ Workshop im Rahmen der Tagung 
                        Zugang gestalten 2019. https://zugang-gestalten.org/dokumentation-2019/.
                    
               
                  Koch, Gertraud (2006): „Die Neuerfindung als Wissensgesellschaft. Inklusionen und Exklusionen eines kollektiven Selbstbildes.“ In: Hengartner, Thomas; Moser, Johannes (Hg.): 
                        Grenzen & Differenzen. Zur Macht sozialer und kultureller Grenzziehungen. Leipzig, S. 545–559.
                    
               
                  Leonard, Peter (2019): “Large images dataset overtime: PixPlot new features.” In: 
                        Culture Analytics Workshop: Time Series, Digital Humanities 2019. https://dev.clariah.nl/files/dh2019/boa/1079.html und https://github.com/CultureAnalytics/DH2019.
                    
               
                  Mayr, Eva / Windhager, Florian (2019): „Vor welchem Hintergrund und mit Bezug auf was? Zur polykontexturalen Visualisierung kultureller Sammlungen“. Vortrag im Rahmen der Tagung 
                        Objekte im Netz. Wissenschaftliche Sammlungen im digitalen Zeitalter. Folien unter http://objekte-im-netz.fau.de/projekt/sites/default/files/2019-11/Mayr%26Windhager_PolyContext.pdf.
                    
               
                  McIlvain, Eileen (2013): “Paradata. What is Paradata?” In: 
                        NSDL Documentation Wiki. https://wiki.ucar.edu/display/nsdldocs/Paradata.
                    
               
                  Moretti, Franco (2007): “Graphs, Maps, Trees. Abstract Models for Literary History.” London, New York.
                    
               
                  Moretti, Franco (2016): “Distant Reading.” Göttingen.
                    
               
                  Schneider, Stefanie (2019): „Über die Ungleichheit im Gleichen. Erkennung unterschiedlicher Reproduktionen desselben Objekts in kunsthistorischen Bildbeständen.“ In: 
                        DHd 2019. Digital Humanities im deutschsprachigen Raum 2019. Konferenzabstracts, S. 92–94. https://doi.org/10.5281/zenodo.2596095.
                    
               
                  Schönholz, Christian (2017): „Jede Kopie ein Original!. Aspekte eines kulturellen Größenverhältnisses.“ In: Koch, Gertraud (Hg.): 
                        Digitalisierung. Theorien und Konzepte für die empirische Kulturforschung. Konstanz/München 2017, S. 157–182.
                    
               
                  Star, Susan Leigh / Strauss, Anselm L. (1999): “Layers of Silence, Arenas of Voice. The Ecology of Visible and Invisible Work.” In: 
                        Computer Supported Cooperative Work 8/1-2, S. 9–30.
                    
               
                  Werner, Claus (2019): „Die Sammlung als Graph. Gephi als Tool der Sammlungsevaluation. Deutsches Bergbau-Museum Bochum.“ Vortrag im Rahmen der Tagung 
                        Museums and the Internet (Mai-Tagung) 2019. https://mai-tagung.lvr.de/media/mai_tagung/pdf/2019/MAI-2019-Werner.pdf.
                    
            
         
      
   



      
         
            Die Komponenten: DraCor und forTEXT
            
               DraCor
               Mit ELTeC (European Literary Text Collection; 
https://github.com/COST-ELTeC) und DraCor gibt es mittlerweile zwei europäische Initiativen, die eine korpusbasierte Infrastruktur für die digitalen Literaturwissenschaften aufbauen, wobei sich DraCor (Drama Corpora Project; 
https://dracor.org) der Sammlung TEI-kodierter Dramen in verschiedenen Sprachen widmet (vgl. Fischer u.a. 2019). DraCor liefert über seine API etwa Netzwerkdaten zu Dramen aus, die auf der Kookkurrenz von Sprecherïnnen basieren und es ermöglichen, die Kommunikationsstrukturen mithilfe von Network-Analysis-Metriken zu erforschen. Darüber hinaus bietet die Plattform mit ezlinavis (Easy Literary Network Analysis Visualisation) ein didaktisches Tool an, das den Einstieg in die systematische Erhebung von Netzwerkdaten erleichtert. Außerdem wurde aus DraCor heraus mit dem 
Dramenquartett (vgl. Fig. 1) ein Kartenspiel entwickelt, mit dem das Verständnis von Netzwerkmetriken ebenso wie die typologische und historische Vielfalt von Dramennetzwerken spielerisch entdeckt und erlernt werden kann (vgl. Fischer u.a. 2018 und Fischer/Schultz 2019).

               
                  
                  Figure 1: Beispielkarte aus dem Dramenquartett (Rück- und Vorderseite)
               
            
            
               forTEXT
               
                  
                  Figure 2: Die Startseite des Disseminationsportals forTEXT.net
               
               Das DFG-Projekt 
  forTEXT (https://fortext.net; vgl. Fig. 2) bietet in diesem Zusammenhang einen Methodeneintrag (vgl. Schumacher 2018b), eine (Gephi-)Lerneinheit (vgl. Schumacher 2019b), ein Fallstudien-Video, vier Tutorialvideos sowie praktische Anwendungen in der Lehre. Hierbei sind jeweils unterschiedliche Abstraktionsgrade und verschiedene Arten der Vermittlung abgedeckt: Der Methodeneintrag ist eine abstrakte, sprachlich-theoretische Beschreibung der Methode mit dem Schwerpunkt der Anschlussfähigkeit an die traditionelle Literaturwissenschaft. Die Lerneinheit ist eine konkrete Klick-für-Klick-Einführung für Autodidakten aus der Zielgruppe junger Geisteswissenschaftlerïnnen in Form einer Text-Bild-Kombination.
  
               Die Videos vermitteln die Methode über eine Text-Bild-Audio-Kombination: Das Methodenvideo bietet eine Fallstudie zum Figurennetzwerk von 
  Emilia Galotti. Es vermittelt die Methode an eine autodidaktische Zielgruppe geisteswissenschaftlicher Studentïnnen und wählt einen Einstieg über das ,traditionelle’ Thema, taucht in technisch-theoretische Hintergründe ein und schließt dann wieder an das literarische Thema an. Die textbasierte Thematik wird so auf eine Bildebene überführt, die DH-Tool-Grafiken mit strichmännchenartigen figurativen Darstellungen koppelt (vgl. Fig. 3).
  
               
                  
                  Figure 3: Vorschaubild eines forTEXT-Fallstudien-Videos
               
               Angesprochen werden hier theoretische, strukturelle und emotionale autodidaktische Vermittlungsmuster (zur Bedeutung von Emotionen für autodidaktisches Lernen vgl. Mega u.a. 2014). Auf der Tonebene ist ein erklärender Duktus vorherrschend. Die ,selfmade’-Anmutung der Videos vermittelt, dass die autodidaktische Erarbeitung der Inhalte Betrachterïnnen und Erstellerïnnen des Videos miteinander verbindet (vgl. Horstmann & Schumacher 2019).
               Die Tutorial-Reihe schließlich funktioniert ähnlich wie die Lerneinheit als Schritt-für-Schritt-Anleitung und bietet die Möglichkeit, die Arbeit mit Gephi als Screencast zu erlernen. Das Tutorial-Video zur Nutzung des DraCor-Tools ezlinavis verknüpft die praktische Erstellung von Netzwerken mit der Nutzung der Ressource TextGrid Repository (vgl. Horstmann 2018) und den Methoden Named Entity Recognition (vgl. Schumacher 2018a) und Annotation in CATMA (vgl. Jacke 2018 und Schumacher 2019a).
            
            
               Das Dramenquartett als Erweiterung des Disseminationsmodells in forTEXT
               Die Dissemination einer digitalen Methode wie der Netzwerkanalyse durch ein nicht-digitales Kartenspiel bietet Möglichkeiten, die die bisher genannten digitalen Medien nicht abdecken konnten. Die Spielerïnnen werden in einer nicht-digitalen Umgebung mit den funktional reduzierten Ergebnissen einer digitalen Analyse konfrontiert, können diese visuell und haptisch erfahren und spielerisch explorieren. Der empfohlene Spielmodus ist ,Supertrumpf’, bei dem Werte der Netzwerke verglichen werden. Die Spielregeln sind online veröffentlicht
  (https://dramenquartett.github.io/). Neben einem neuen und vor allem kompetitiven Blick auf Dramen – der die relationale Perspektive auf figürliche Kopräsenzen hervorhebt – wird zusätzlich die Neugier auf die den Netzwerken zugrunde liegende Methode geweckt, sodass in didaktisch-produktiver Hinsicht der Prozess einer Art 
  Reverse Engineering im Sinne einer Mustererkennung auf unterschiedlichen Komplexitätsstufen angestoßen werden kann. Der Weg hin zu einem Umgang mit digitalen Ressourcen und Tools wie DraCor, ezlinavis und sogar die Anwendung eines komplexeren Tools wie Gephi ist damit geebnet, die kritische Methodenreflexion kann folgen. Dieser niedrigschwellige Zugang fügt sich in das auf Zugänglichkeit und Benutzerfreundlichkeit konzentrierte Disseminationsmodell von forTEXT ein und erweitert dieses durch den zusätzlichen Abbau von Schwellenängsten oder Vorbehalten gegen digitale Methoden.
  
               Die im Folgenden vorgestellte, reflektierte und erprobte Pipeline geht von einer ersten theoretischen Annäherung durch forTEXT-Tutorials aus, auf die eine spielerische Vertiefung der spezifischen Objektkonstitution qua Netzwerkanalyse und der entsprechenden Metriken mittels des Dramenquartetts folgt. Anschließende Arbeitsphasen könnten, wie in 3. skizziert, z. B. die formalisierte Erstellung, Gestaltung und Analyse von Dramennetzwerken mittels ezlinavis und Gephi oder die konkrete Bearbeitung von literarhistorischen Forschungsfragen mittels DraCor umfassen.
            
         
         
            Epistemische und didaktische Implikationen
            
               Epistemische Dimensionen des Medienwechsels
               Der quantifizierende Zugriff auf Dramentexte kann als „radikale ,Anästhetisierung’ der Objekte” (Trilcke, im Erscheinen) beschrieben werden. Auf die qua Formalisierung erfolgende Anästhetisierung, bei der die ursprüngliche ästhetische Dimension des literarischen Kunstwerks zunächst ausgesetzt wird, folgt jedoch eine reästhetisierende Transformation im Zuge der Diagrammatisierung (vgl. ebd.). Die Dramen werden somit zunächst zwar nicht mehr primär als textuelle Artefakte wahrgenommen, dennoch aber als ästhetische Artefakte in Form ihrer netzwerkartigen Repräsentation, wodurch andere epistemische Dimensionen angesprochen und andere epistemische Praktiken vollzogen werden können (vgl. Trilcke und Fischer 2018). Dabei ist der Weg zurück zum Dramentext vom Kartenspiel über die digitale Darstellung der entsprechenden Netzwerke dadurch geebnet, dass Medien in Form „transkriptiver Bezugnahmen” (Jäger 2010, 301) generell intermedial aufeinander Bezug nehmen und Übersetzungsprozesse somit keine einseitig vorgegebene Richtung haben.
               
               Ein entscheidender Vorteil digitaler Diagramme ist die Möglichkeit der Interaktion (vgl. Horstmann, im Erscheinen): Netzwerke lassen sich je nach Wahl des Layoutalgorithmus unterschiedlich darstellen, ein semantischer Zoom ermöglicht überdies, zusätzliche Informationen des Ausgangsmaterials zu visualisieren. Dramennetzwerke in einer festgelegten (und damit nicht mehr veränderbaren) Form als Spielkarte zu drucken, bedeutet daher in erster Linie eine funktionale Reduktion. Gerade diese funktionale Reduktion eröffnet jedoch didaktische Spielräume: Das Wissen, dass die abgedruckten Netzwerke ebenfalls in digitaler Form vorhanden und dort sogar manipulierbar sind, wird im Laufe des Spielprozesses die Neugier auf diese Funktionsvielfalt steigern, sodass der Übergang in die ,digitale Arbeit’ fließend stattfinden kann und nicht mehr als etwas kategorial anderes empfunden wird. Die Interaktion zwischen Benutzerïnnen und Netzwerken als konzeptioneller Bestandteil digitaler Netzwerkdarstellungen wird übertragen auf die Interaktion zwischen den Spielerïnnen, wodurch nicht zuletzt die von Jenkins (2006, 2) sog. 
  participatory culture im nicht-digitalen Bereich eine Entsprechung erfährt.
  
            
            
               Ansprechen unterschiedlicher Lerntypen 
               Das Kartenspiel entfaltet seinen didaktischen Mehrwert auch, weil es situational gerahmt ist: Es wird in kollektiven Unterrichtsphasen eingesetzt, die darauf abzielen, sich einem abstrakten Unterrichtsgegenstand auf spielerische Weise anzunähern. Da Menschen in ihrer Rolle als 
  visual beings vor allem ihren Sehsinn als einen wichtigen Wahrnehmungskanal nutzen, um Informationen zu verstehen (vgl. Ward et al. 2010), stellen Visualisierungen bei der Präsentation von wissenschaftlichen Erkenntnissen ein wichtiges, den Verstehens- und Erinnerungsprozess begünstigendes Element dar. Der Einsatz des Kartenspiels greift darauf zurück und spricht unter den vier Lerntypen (auditiv, haptisch, kommunikativ und visuell) v. a. visuelle, aber auch kommunikative Lerntypen an, indem das Spiel die Kommunikation über Fachinhalte fokussiert und Sprache als Medium des Lernens einsetzt (vgl. Anselm und Werani 2017).
  
               Im Fokus steht der Versuch, nicht nur kumulatives bzw. assimilatives Lernen zu initiieren, wodurch v. a. begrenztes, anwendungsorientiertes Wissen oder thematisch, anwendungsorientiertes Wissen produziert werden würde (vgl. Illeris 2010). Die – von der konkreten Kenntnis des Spielprinzips ,Supertrumpf‘ unabhängige – spielerische Aktivierung unterschiedlicher Sinneskanäle und die damit einhergehende Diskussion über Fachinhalte zielt auf die Einleitung akkommodativer und transformativer Lernprozesse und darauf, über Fachwissen in relevanten Kontexten frei verfügen zu können.
            
            
               Anwendung in der universitären Lehre und Lehrerïnnenbildung
               Das im Wintersemester 2019/2020 an der Universität Hamburg durchgeführte Seminar „Digitale Literaturwissenschaft und pädagogische Praxis” hat unterschiedliche Standardverfahren und Werkzeuge erprobt, die gegenwärtig in der digitalen Literaturwissenschaft eingesetzt werden. Dieses Feld wird zunehmend auch für Lehrerïnnen insbesondere im gymnasialen Bereich relevant: Bereits die heutige Schülerïnnengeneration zählt zu den 
  digital natives, für die der Umgang mit digitalen Medien und Werkzeugen selbstverständlich ist, die aber zugleich in Schule und/oder Studium in eine vertiefte 
  data literacy eingeführt werden müssen. Der Transfer von Digital-Humanities-Methoden in den schulischen Bereich kann deshalb als wichtige Herausforderung identifiziert werden. Gleichzeitig geht es darum, das vernetzte Denken zu fördern, mithin literaturwissenschaftliche und fachdidaktische Zugänge zu 
  einem Gegenstand stark zu machen. Um in Seminaren kein starres Wissen zu produzieren, auf das die angehende Lehrkräfte in der nächsten Phase ihrer Ausbildung – dem schulischen Alltag – nicht zugreifen können, muss die Kooperation zwischen Fachdidaktik und Fachwissenschaft gefördert werden. Neben der Einarbeitung in die Methoden steht deshalb die Frage der Komplexitätsreduktion und des schulischen Anwendungsbezuges im Zentrum des Seminars, wofür das DraCor-Kartenspiel exemplarisch herangezogen und getestet wird. Die konzeptionelle Einbettung des Kartenspiels in eine didaktische Heranführung an digitale Methoden ergänzend, wurde damit sowohl in diesem als auch im Seminar „Gender modellieren – Genderrollen und -stereotype in der Literatur des 19. Jahrhunderts”, das ebenfalls im Wintersemester 19/20 an der Universität Hamburg angeboten wurde, eine praktische Anwendung durchgeführt, deren Erfolg qualitativ evaluiert wurde. Damit soll auch ein Beitrag zur Evaluation konkreter DH-Lehrformen geleistet werden.
  
            
            
               Erste Ergebnisse
               Um den Effekt des Dramenquartetts auf den Lernerfolg der Studierenden zu untersuchen, wurde eigens ein Testverfahren entwickelt, das die Wissensstände vor und nach dem Einsatz des Quartetts mess- und v. a. vergleichbar macht. Das Verfahren setzt sich aus fünf aufeinander aufbauenden Phasen zusammen:
               (1) 
                        Vorab: Gruppeneinteilung und eigenständige Vorbereitung (Gruppe 1: Methodenbeitrag/Lerneinheit, Gruppe 2: Video-Tutorials)
                    
               Vorbereitend befasst sich ein Teil der Lerngruppe mit schriftlichen forTEXT-Lernmaterialien zur digitalen Netzwerkanalyse, während der andere Teil die Video-Fallstudien und -Tutorials konsultiert.
               
               (2)
                         Praxisphase 1: Erste Umfrage
               
               Ausgangspunkt der Erhebung stellt folglich ein gruppenspezifisch relativ homogener Wissensstand dar, der grundlegende Kenntnisse über die Methode der digitalen Netzwerkanalyse beinhaltet. Um die Wissensstände beider Gruppen vor dem Einsatz des Quartetts zu erfassen, wurde eine Umfrage entworfen und zu Beginn des Seminars in Einzelarbeit mit dem Audience Response System ARSnova durchgeführt. Die Umfragen adressieren mit jeweils neun Fragen drei Anforderungsbereiche (I: Reproduktionsleistung, II: Reorganisation- und Transferleistung, III: Reflexion und Problemlösung). Den Anforderungsbereichen entsprechend beinhalten sie Single-Choice-, Multiple-Choice- sowie Freitextfragen.
               (3)
                         Praxisphase 2: Einsatz des Dramenquartetts
               
               Nach der ersten Quizphase wurde die gesamte Testgruppe in Kleingruppen eingeteilt, die im Supertrumpf-Modus das Dramenquartett spielen.
               (4)
                         Praxisphase 3: Zweite Umfrage
               
               Eine zweite Umfrage erfasst den Wissensstand beider Gruppen, nachdem sie das Dramenquartett gespielt haben.
               (5)
                         Auswertung der Umfrage: Erste Ergebnisse und Ausblick
               
               Die Auswertung des ersten Testdurchlaufs, der mit 11 Teilnehmenden durchgeführt wurde, verweist auf einen lernförderlichen Effekt des Dramenquartetts. Im Rahmen der ersten Quizrunde wurden 43% der Fragen, nach der zweiten Umfrage 52% der Fragen richtig beantwortet. Darüber hinaus verweist ein erster Blick auf die Freitextantworten darauf, dass der spielerische Zugang die intrinsische Motivation, sich über den Seminarkontext hinaus mit digitaler Netzwerkanalyse auseinanderzusetzen, steigert. Das erarbeitete Verfahren zur vergleichenden Lernstandserhebung hat sich bewährt und wird in einem weiteren Seminar eingesetzt, um den Einfluss einer spielerischen Wissensvermittlung auf Kompetenz- und Wissensstand zu untersuchen.
            
         
         
            Ausblick: zukünftig mögliche Arbeitsfelder
            Das Projekt lotet das didaktische Potenzial von Gamification-Ansätzen in den DH konzeptionell und praktisch aus, indem es das DraCor-Kartenspiel mit Tools und Tutorials in einer didaktischen ,Pipeline‘ verbindet und damit in die Disseminationsstrategie von forTEXT integriert. Der damit entwickelte Prototyp eines Konzepts, das auch fachdidaktisch Weiterentwicklungspotenzial birgt, ermöglicht diverse Adaptionen und Transformationen: in Hinblick auf die Netzwerkanalyse literarischer Texte, in Hinblick auf andere Methoden der Digital Humanities sowie in Hinblick auf das didaktische Szenario einer Verzahnung von analogen und digitalen Ansätzen.
            So ließen sich auf der Grundlage der Netzwerkdaten aus anderen DH-Projekten, etwa zu Romanen, andere generische Karten-Sets entwerfen, wobei auch die – durch ezlinavis in Kombination mit Gephi ermöglichte – kollaborative Erstellung eigener Sets denkbar ist. Diese selbstständige Erstellung von Karten-Sets würde nicht zuletzt auch den haptischen Lerntyp ansprechen. Eine Weiterentwicklung der didaktischen Engführung von Analogem und Digitalem ließe sich über eine Verzahnung des Kartenspiels mit der digital-interaktiven Repräsentation der einzelnen Dramen auf DraCor vornehmen (z. B. über QR-Codes). Unter didaktischen Gesichtspunkten bietet sich des Weiteren die Möglichkeit, kreativ-produktionsorientierte Elemente in die skizzierte Pipeline einzubauen, etwa indem die Lernenden Netzwerke ,erfinden‘, die sie zunächst händisch zeichnen und dann – den Schritt in den digitalen Raum machend – mittels ezlinavis formal erfassen müssen.
            Der im Projekt durchgeführte Testlauf soll in diesem Sinne zu einer weiteren Diskussion über didaktische Potenziale sowohl von Gamification-Ansätzen als auch der Verzahnung von analogen und digitalen Lehrmitteln anregen und damit grundsätzlich der Reflexion über didaktische Szenarien dienen, die den spielerischen, kreativen Übergang zwischen lebensweltlich vertrauten Situationen und der Abstraktion digitaler Forschungsprozesse gestalten.
         
      
      
         
            
                Vgl. 
               https://fortext.net/ressourcen/videos/fallstudien/analyse-der-figurennetzwerke-in-lessings-emilia-galotti.
    
            
                Vgl. 
               https://fortext.net/ressourcen/videos/tutorials/netzwerkanalyse-und-literaturanalyse.
    
            
                Vgl. 
               https://de.wikipedia.org/wiki/Supertrumpf.
    
            
      Zum Diagrammatikbegriff vgl. etwa Krämer 2016.
    
            
                Zu intermedialen Übersetzungsprozessen vgl. Schmid, Veits und Vorrath 2018.
            
            
                Beide Seminare richten sich ausdrücklich an Studierende ohne technische Vorkenntnisse. Es ist also davon auszugehen, dass die Personen beider Testgruppen über keinerlei Vorbildung bezüglich Methoden der digitalen Netzwerkanalyse verfügen.
            
         
         
            
               Bibliographie
               
                  Anselm, Sabine / Werani, Anke (2017): 
  Kommunikation in Lehr-Lernkontexten. Bad Heilbrunn: Klinkhardt.

               
                  Fischer, Frank / Kittel, Christopher / Milling, Carsten / Trilcke, Peer / Wolf, Jana (2018): „Dramenquartett – Eine didaktische Intervention“, in: 
  DHd 2018. Kritik der digitalen Vernunft. Konferenzabstracts, 397–398. DOI: 
  https://doi.org/10.6084/m9.figshare.5926363.v1.

               
                  Fischer, Frank / Schultz, Anika (2019): „Dramenquartett – Eine didaktische Intervention“. Unter Mitarbeit von Christopher Kittel, Carsten Milling, Peer Trilcke und Jana Wolf. 32 Blatt in Kartonbox, Farbdruck. Bern: edition taberna kritika 2019. (Spielanleitung: 
  https://dramenquartett.github.io/)

               
                  Fischer, Frank / Börner, Ingo / Göbel, Mathias / Hechtl, Angelika / Kittel, Christopher / Milling, Carsten / Trilcke, Peer (2019): „Programmable Corpora. Die digitale Literaturwissenschaft zwischen Forschung und Infrastruktur am Beispiel von DraCor“, in: Sahle, Patrick (ed.): 
  DHd 2019. Digital Humanities: multimedial & multimodal. Konferenzabstracts, 194–197.

               
                  Horstmann, Jan (2018): „TextGrid Repository“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/ressourcen/textsammlungen/textgrid-repository [letzter Zugriff 12. September 2019].

               
                  Horstmann, Jan (im Erscheinen): „Textvisualisierung: Epistemik des Bildlichen im Digitalen”, in: Huber, Martin / Krämer, Sybille / Pias, Claus (eds.): 
  Wovon sprechen wir, wenn wir von Digitalisierung sprechen? Gehalte und Revisionen zentraler Begriffe des Digitalen, CompaRe: Fachinformationsdienst Allgemeine und Vergleichende Literaturwissenschaft.

               
                  Horstmann, Jan / Schumacher, Mareike (2019): „Social Media, YouTube und Co: Multimediale, multimodale und multicodierte Dissemination von Forschungsmethoden in forTEXT“, in: Sahle, Patrick (ed.): 
  DHd 2019. Digital Humanities: multimedial & multimodal. Konferenzabstracts, 207–211. DOI: 
  10.5281/zenodo.2596095.

               
                  Illeris, Knud (2010): 
  Lernen verstehen. Bedingungen erfolgreichen Lernens. Bad Heilbrunn: Klinkhardt.

               
                  Jacke, Janina (2018): „Manuelle Annotation“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/routinen/methoden/manuelle-annotation [letzter Zugriff 12. September 2019].

               
                  Jäger, Ludwig (2010): „Intermedialität – Intramedialität – Transkriptivität: Überlegungen zu einigen Prinzipien der kulturellen Semiosis”, in: Deppermann, Arnulf / Linke, Angelika (eds.): 
  Sprache intermedial: Stimme und Schrift, Bild und Ton. Berlin, New York: de Gruyter 299–324. DOI: 
  10.1515/9783110223613.299.

               
                  Jenkins, Henry (2006): 
  Convergence Culture: Where Old and New Media Collide. New York, London: New York University Press.

               
                  Krämer, Sybille (2016): 
  Figuration, Anschauung, Erkenntnis. Grundlinien einer Diagrammatologie. Berlin: Suhrkamp.

               
                  Mega, Carolina / Ronconi, Lucia / De Beni, Rossana  (2014):
  „What makes a good student? How emotions, self-regulated learning, and motivation contribute to academic achievement”, in:
   Journal of Educational Psychology, 
                  Vol 106(1), 121–131.
               
               
                  Odebrecht, Carolin / Burnard, Lou / Navarro Colorado, Borja / Eder, Maciej / Schöch, Christof (2019): „The European Literary Text Collection (ELTeC)”, in: 
  DH 2019. Complexities. Utrecht University. [Poster.]

               
                  Schmid, Johannes C. P. / Veits, Andreas / Vorrath, Wiebke (eds. 2018): 
  Praktiken medialer Transformationen. Übersetzungen in und aus dem digitalen Raum. Bielefeld: transcript. DOI: 
  10.14361/9783839441145.

               
                  Schumacher, Mareike (2018a): „Named Entity Recognition (NER)“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/routinen/methoden/named-entity-recognition-ner [letzter Zugriff 12. September 2019].

               
                  Schumacher, Mareike (2018b): „Netzwerkanalyse“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/routinen/methoden/netzwerkanalyse [letzter Zugriff 12. September 2019].

               
                  Schumacher, Mareike (2019a): „CATMA“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/tools/tools/catma [letzter Zugriff 12. September 2019].

               
                  Schumacher, Mareike (2019b): „Netzwerkanalyse mit Gephi“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/routinen/lerneinheiten/netzwerkanalyse-mit-gephi [letzter Zugriff 12. September 2019].

               
                  Trilcke, Peer / Fischer, Frank (2018): „Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen”, in: Huber, Martin / Krämer, Sybille (eds.): 
  Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden (Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3). DOI: 
  10.17175/sb003_003.

               
                  Trilcke, Peer (im Erscheinen): „Small Worlds, Change Rates und die Netzwerkanalyse dramatischer Texte. Reflexionen aus dem Rabbit Hole”, in: Jannidis, Fotis / Winko, Simone / Rapp, Andrea / Meister, Jan Christoph / Stäcker, Thomas (eds.): 
  Digitale Literaturwissenschaft. DFG-Symposium Villa Vigoni, 2017. Berlin, New York: de Gruyter.

               
                  Ward, Matthew / Grinstein, Georges / Keim, Daniel (2010): 
  Interactive Data Visualization. Foundations, Techniques, and Applications. Wellesley: Peters.

            
         
      
   



      
         
            Einführung und Forschungsdesign
            Das Poster diskutiert die methodischen Herausforderungen und Erkenntnismöglichkeiten bei der Modellierung und Auswertung heterogener ideengeschichtlicher Netzwerkstrukturen mit den Methoden der Sozialen Netzwerkanalyse. Dies soll im Rahmen der Vorstellung eines Projekts erfolgen, bei dem mittels einer landesgeschichtlichen Perspektive die Spätaufklärung im Fürstentum Lippe untersucht wurde. Grundlage hierfür Listen der innerhalb dieses Territoriums gelesenen und gedruckten Literatur.
            Unter dem Begriff der Aufkärung wird eine Vielzahl von Diskursen zu philosophischen, politischen, religiösen und anthropologischen Ideen gefasst, die alle Lebensbereiche in den Gesellschaften des 18. Jahrhunderts betrafen. An diesen Diskursen waren unterschiedliche Akteure und soziale Gruppen beteiligt, die nicht nur verschiedene inhaltliche Schwerpunkte setzten, sondern auch variierende, zum Teil gegenläufige, Ziele verfolgten (Stollberg-Rilinger 2011: 10). Ausgehend von dieser Bandbreite und Diffusität bewegen sich nahezu alle Studien zu diesem Themenkomplex bei der Wahl ihrer Untersuchungsperspektive in der Nähe von zwei Polen:
            
               Aufklärung wird aus einer 
                        mikroskopischen Perspektive betrachtet, wobei meist ein einzelner aufgeklärter Diskurs intensiv untersucht wird.
                    
               Aufklärung wird aus einer 
                        makroskopischen Perspektive betrachtet. Hierbei wird versucht eine möglichst allgemeine, idealtypische Begriffsbestimmung von Aufklärung zu schaffen, bei der zwar der großen Vielfalt des Themas Rechnung getragen wird, Details jedoch (zwangsläufig) ausgeblendet werden.
                    
            
            Die Bedeutung dieser Perspektiven für die Aufklärungsforschung ist nicht zu bestreiten. Dennoch bleiben mit der Auslassung einer mesoskopischen Sichtweise einige Fragen unbeantwortet. Zwar wurde an vielen Stellen die interne Struktur einzelner Diskurse offen gelegt und zugleich deutlich gemacht, welche Bedeutung diese für die Epoche der Aufklärung insgesamt darstellten. Unklar bleibt jedoch häufig, wie sich die einzelnen Teildiskurse zueinander verhielten; wie sie sich gegenseitig beeinflussten und wo inhaltliche sowie personelle Überschneidungen existierten. Auf Grund des umfassenden, gesamtgesellschaftlichen Durchdringungspotentials der Aufklärung waren diese diskursiven Bezüge jedoch vorhanden (Stollberg-Rilinger 2011: 10). Größere Entwicklungsverläufe in der aufgeklärten Diskurslandschaft lassen sich durch die skizzierten Perspektiven nicht vollständig, sondern nur in Ausschnitten darstellen; ebenso ist die Identifizierung von Teildiskursen oder auch aufgeklärten Denkschulen meist nur unter Bezugnahme auf Meistererzählungen sowie zeitgenössische Deutungen möglich. Eine Adressierung dieser Problemfelder würde jedoch nicht nur unser generelles Verständnis dieser Epoche schärfen, sondern könnte auch allgemeine Erkenntnisse zur Funktionsweise großer sozialer Bewegungen liefern, die von einer ähnlich heterogenen Diskurslandschaft getragen werden.
            In der dem Poster zu Grunde liegenden Arbeit wurde die aufgeklärte Diskurslandschaft in einem Nebenland der Aufklärung – dem Fürstentum Lippe – im Zeitraum zwischen 1796 und 1820 untersucht. Auch die Aufklärungsforschung zu diesem Territorium des Alten Reiches bewegte sich vor allem in der Nähe der oben skizzierten makroskopischen (z.B. Arndt 1992) und mikroskopischen (z.B. Behrisch 2016 oder Wehrmann 1972) Dimensionen. Um die beschriebenen Anforderungen einer mesoskopischen Perspektive auf Aufklärung zu berücksichtigen wurden daher die einzelnen historischen Quellen, Personen und Ideen in ihrer singulären Bedeutung zurückgestellt und sie stattdessen in ihrer Relationalität zueinander untersucht um Rückschlüsse auf die Struktur der aufgeklärten Diskurslandschaft als Ganzes innerhalb des Untersuchungsraumes zu gewinnnen.
                
         
         
            Modellierung ideengeschichtlicher Netzwerkdaten
            Der Modellierung ebendieser Diskurslandschaft in einer Graphenstruktur lag insbesondere eine kulturwissenschaftliche Vorannahme über die betrachteten historischen Konzepte zu Grunde: Die Operationalisierung der Diskursstruktur erfolgte über ein praxeologisches Verständnis von 
                    Aufklärung. Hierbei wurden einerseits das Verfassen von Büchern und Journalartikeln als Teilhabe an aufgeklärten Diskursen sowie andererseits deren Rezeption durch Lesen dieser Beiträge innerhalb von Sozietäten als zentrale Praktiken der Aufklärung identifiziert (Vgl. für eine allgemeine Operationalisierung aufgeklärter Kommunikation auch Bödeker 1987). Diese Praktiken bieten den Rahmen für die Spielräume in denen die verschiedenen, heterogenen Diskurse der Aufklärung öffentlich ausgetragen wurden.
                
            Die wichtigsten Quellengrundlagen waren hierbei zum Einen für die innerhalb des untersuchten Fürstentums Lippe 
                    rezipierten Publikationen die Auktionslisten der lippischen Lesegesellschaft. Zum Anderen bot für die im Untersuchungsraum 
                    publizierten Druckwerke das Verzeichnis des im Untersuchungszeitraum einzigen Verlags Lippes den Beleg (Weißbrodt 1914). Diese Literaturlisten wurden als strukturierte Informationen automatisiert in Netzwerkdaten überführt.
                
            Die in den einzelnen Publikationen behandelten Themen wurden dann als Teildiskurse bzw. 
                    Diskursfelder aufgefasst, die in ihrer Gesamtheit den aufgeklärten Diskurs im Untersuchungsraum abbildeten. Ein Diskursfeld konnte hierbei einzelne Themenbereiche darstellen – etwa die Verbesserung der Situation der Bauern, aufgeklärte Pädagogik oder die Rolle des Adels in der Gesellschaft. Diese Themenfelder stehen somit auch für sich genommen als eigene, diskursanalytisch relevante ideengeschichtliche Entitäten. Ihre Modellierung als Netzwerkknoten erfolgte auf Grundlage der bisherigen Forschung zur Aufklärung. Diese nicht-automatisierte Erstellung der Diskursfelder bedingt dementsprechend besonders stark die Erkenntnismöglichkeiten des Netzwerks. Zugleich wird so bereits bei der Modellierung sichergestellt, dass die Netzwerkdaten im Rahmen der bisherigen Aufklärungsforschung kontextualisiert sind.
                
            Diskursfelder und Publikationen wurden dann als Knoten in einem bimodalen Netzwerk modelliert (Abbildung 1). Die Verknüpfungen zwischen den Publikations- und Diskursfeld-Knoten erfolgte in einem hermeneutischen-interpretativen Prozess auf Grundlage der Titel der Bücher und Journalartikel. Das Netzwerk wurde dann zu einem unimodalen Netzwerk aus Diskursfeldern transformiert um eine Auswertung vornehmen zu können (Abbildung 2). Durch die Verwendung der Publikationen als heuristische Grundlage konnten so nach der Transformation ideengeschichtliche Ähnlichkeiten und Abhängigkeiten zwischen den einzelnen Diskursfeldern abgebildet werden. Um dabei auch Veränderungen innerhalb des Untersuchungszeitraums erfassen zu können wurden die Kanten des Netzwerks mit zeitlichen Informationen versehen. Grundlage waren die Jahre, in denen die einzelnen Bücher und Journalartikel veröffentlicht wurden – in diesem Zeitraum lässt sich durch den Druck einer Publikation ihr Einfluss auf ein Diskursfeld nachweisen.
                
            
               
               Abbildung 1: Bimodales, gerichtetes Netzwerk der innerhalb Lippes erschienenen Publikationen. Diskursfelder sind rot, Publikationen grün dargestellt. Visualisierung mit dem Fruchtermann-Reingold-Algorithmus.
            
            
               
               Abbildung 2: Aus Abbildung 1 transformiertes Diskursnetzwerk der innerhalb Lippes erschienenen Publikationen. Die Dicke der Kanten beschreibt dabei die Anzahl der Publikationen, die sich ein Diskursfeld teilen. Visualisierung mit dem Davidson-Harel-Algorithmus.
            
         
         
            Netzwerkanalytische Auswertung
            Sowohl die quantitative Auswertung mit Methoden der 
                    Sozialen Netzwerkanalyse als auch die Aufbereitung der Daten erfolgte mit der Skriptsprache 
                    R; v.a. unter Zuhilfenahme des Pakets 
                    igraph. Die Verwendung von eigenem Programmcode bietet gegenüber GUI-basierten Lösungen wie 
                    Gephi oder 
                    Pajek eine leichtere Nachnutzbarkeit und damit auch eine bessere Überprüfbarkeit der Ergebnisse.
                
            Sowohl die Vielfalt der einzelnen Wissensfelder, als auch die nahezu unüberschaubare Menge historischer Quellen dieser Zeit sollten berücksichtigt werden. Die quantitative Auswertung der Relationen zwischen den Diskursfeldern mittels netzwerkanalytischer Verfahren, wie Berechnung von Betweenness-Zentralität (Jansen 2003: 134f) oder Community-Analyse (Rosvall 2019) legten unter anderem eine moderate Diversität der aufgeklärten Diskurslandschaft Lippes offen. Über den gesamten Untersuchungszeitraum ließ sich sowohl in der Publikations- als auch in der Rezeptionstätigkeit ein breites thematisches Interesse nachweisen, wie es für die Zeit der Aufklärung als charakteristisch beschrieben wird (Stollberg-Rilinger 2000). In Bezug auf die innerhalb des Fürstentums Lippe erschienen Publikationen existierte diese Vielfalt jedoch nicht durchgängig während des betrachteten Zeitraumes zwischen 1796 und 1820. Vielmehr war der innerhalb Lippes geführte Diskurs von wenigen Spezialthemen geprägt, die einen dauerhaften Bezug zu nahezu allen anderen Diskursfeldern aufwiesen. Hier dominierten insbesondere religiöse und pädagogische Themen im öffentlichen Diskurs. Volksaufklärerische Schriften mit einer großen territorialen Selbstreferenzialität nahmen ebenfalls einen wichtigen Platz im lippischen Publikationswesen ein. Politische Aktivität und Publikationswesen waren im Fürstentum weitgehend voneinander entkoppelt.
         
         
            Methodische Perspektiven
            Aus einer landesgeschichtlichen Perspektive ermöglichte die Untersuchung eine Schärfung des Verständnisses zu ideengeschichtlichen Schwerpunkten und Strukturen der Spätaufklärung im Fürstentum Lippe. Damit nimmt sie die oben skizzierte Mesoperspektive auf die Erforschung von Aufklärung als geistesgeschichtliches Phänomen ein. Hinsichtlich der 
                    Historischen Netzwerkforschung – der Anwendung der 
                    Sozialen Netzwerkanalyse in den Geschichtswissenschaften – erprobte die Untersuchung einen Ansatz zur Operationalisierung und Auswertung von Netzwerken, die nicht aus Personen, sondern aus abstrakten Ideen bestehen. Die Untersuchung nicht-personaler Netzwerke findet in den Historischen Disziplinen bislang nur selten statt (Düring 2016: 38). Daher ist es erforderlich, Möglichkeiten ebenso wie Herausforderungen bei der Quellensammlung, Datenmodellierung, Abgrenzung (Laumann 1992) und Auswertung eines Netzwerks, das aus ideengeschichtlichen Entitäten besteht, auszudifferenzieren und zu diskutieren. 
                
            Auf diese Weise sind auch unausweichlich zentrale Themen der Tagung betroffen, welche nicht nur für das Teilgebiet der 
                    Sozialen Netzwerkanalyse, sondern für die 
                    Digital Humanities im Allgemeinen relevant sind. Die Vorauswahl bestimmter Datenmodelle und Algorithmen wird bei der 
                    Sozialen Netzwerkanalyse besonders evident: Hier beruht das gesamte Forschungsdesign auf einem Denkparadigma, dem ein streng relationales Welt-, beziehungsweise Geschichtsbild, zu Grunde liegt. Erst die Wahl dieses Paradigmas bedingt den Einsatz bestimmter Methoden und computergestützter Werkzeuge. So ist auch das Thema dieses Posters ein Beispiel dafür, wie diese Vorannahmen einerseits die Selektion von Fragestellungen und bestimmten Quellentypen beeinflussen. Andererseits bietet die 
                    Soziale Netzwerkanalyse in Verbindung mit einem praxeologischen Ansatz jedoch auch eine ‚Unvoreingenommenheit’ gegenüber dem historischen Material. Serielle Textkorpora – wie in diesem Fall Publikationsverzeichnisse – können auf Grundlage formalisierender Vorüberlegungen vollständig ausgewertet werden. Dadurch kann auch einer, der manuellen Reduktion eines Korpus inhärenten, Gefahr von Meistererzählungen vorgebeugt werden.
                
         
      
      
         
            
      Eine Veröffentlichung der Forschungsergebnisse, der dabei angefallenen Daten sowie des für deren Aufbereitung und Auswertung entwickelten Programmcodes ist derzeit in Vorbereitung.
    
            
      Das Land Lippe verfügte zwar über eine aufgeklärten Ideen gegenüber sehr aufgeschlossene landesherrliche Regierung, Verlagswesen und kleine Sozietätslandschaft, besaß jedoch kein eigenes geistiges Zentrum in Form einer Universität.
    
            
      Diese sind im Landesarchiv NRW, Abteilung Ostwestfalen-Lippe vollständig erhalten.
  
            
    Wegen ihres Umfangs bieten Literaturtitel aus dem 18. Jahrhunderts sehr ausführliche Informationen zum Inhalt des jeweiligen Werkes.
  
         
         
            
               Bibliographie
               
                  Arndt, Johannes (1992): 
                        Das Fürstentum Lippe im Zeitalter der Französischen Revolution 1770 – 1820. Münster: Waxmann. 
                    
               
                  Behrisch, Lars (2016): 
                        Die Berechnung der Glückseligkeit. Statistik und Politik in Deutschland und Frankreich im späten Ancien Régime (= Beihefte der Francia 78), Ostfildern: Thorbecke.
                    
               
                  Bödeker, Hans Erich (1987): „Aufklärung als Kommunikationsprozeß“, in: 
                        Aufklärung 2: 89–111.
                    
               
                  Düring, Marten / Kerschbaumer, Florian (2016): „Quantifizierung und Visualisierung. Anknüpfungspunkte in den Geschichtswissenschaften“ in: Düring, Marten / Eumann, Ulrich / Stark, Martin / von Keyerlingk, Linda (eds.): 
                        Handbuch Historische Netzwerkforschung. Grundlagen und Anwendungen (= Schriften des Kulturwissenschaftlichen Instituts Essen (KWI) zur Methodenforschung 1), Münster: Lit Verlag 31–43. 
                    
               
                  Füssel, Marian (2015): „Praxeologische Perspektiven in der Frühneuzeitforschung“ in: Brendecke, Arndt (ed.): 
                        Praktiken der Frühen Neuzeit. Akteure, Handlungen, Artefakte (= Frühneuzeit-Impulse 3), Köln / Weimar / Wien: Böhlau 21–33. 
                    
               
                  Jansen, Dorothea (2003): 
                        Einführung in die Netzwerkanalyse. Grundlagen, Methoden und Forschungsbeispiele. Opladen: Leske + Budrich. 
                    
               
                  Landwehr, Achim (2018): 
                        Historische Diskursanalyse. Frankfurt a. Main: Campus Verlag.
                    
               
                  Laumann, Edward O. / Marsden, Peter V. / Prensky, David (1992): „The Boundary Specification Problem in Network Analysis“ in: Freeman, Linton C. / White, Douglas R. / Romney, Antone Kimball (eds.): 
                        Research Methods in Social Network Analysis. New Brunswick / New Jersey: Taylor & Francis 61–88. 
                    
               
                  Lemercier, Claire (2015a): „Formal network methods in history: why and how?“ in: Fertig, Georg (ed.): 
                        Social Networks, Political Institutions, and Rural Societies (= Rural History in Europe 11), Turnhout : Brepols 281–310. 
                    
               
                  Dies. (2015b): „Taking time seriously. How do we deal with change in historical networks?“ in: Gamper, Markus / Reschke, Linda / Düring, Marten (eds.): 
                        Knoten und Kanten III. Soziale Netzwerkanalyse in Geschichts- und Politikforschung (= Sozialtheorie), Bielefeld: transcript 183–212. 
                    
               
                  Rosvall, Martin / Bergstrom, Carl T. (2008): „Maps of random walks on complex networks reveal community structure“, in: 
                        Proceedings of the National Academy of Sciences 4 1118–1123. 
                    
               
                  Stollberg-Rilinger, Barbara (2000): 
                        Europa im Jahrhundert der Aufklärung, Stuttgart: Reclam.
                    
               
                  Dies. (2011): 
                        Die Aufklärung. Europa im 18. Jahrhundert, Stuttgart: Reclam.
                    
               
                  Trappmann, Mark / Hummell, Hans J. / Sodeur, Wolfgang (2011): 
                        Strukturanalyse sozialer Netzwerke. Konzepte, Modelle, Methoden (Studienskripte zur Soziologie). Wiesbaden: Springer VS.
                    
               
                  Wehrmann, Volker (1972): 
                        Die Aufklärung in Lippe. Ihre Bedeutung für Politik, Schule und Geistesleben (= Lippische Studien 2). Detmold: Landesverband Lippe. 
                    
               
                  Weißbrodt, Ernst (1914): 
                        Die Meyersche Buchhandlung in Lemgo und Detmold und ihre Vorläufer. Festschrift zum 250jährigen Bestehen der Firma am 12. Juni 1914. Detmold: Meyer. 
                    
            
         
      
   



        
            The purpose of this study is to examine the validity of acknowledgement networks in reconstructing academic networks and academic trends. As for the acknowledgement network, its usefulness to draw “communication networks” between researchers was suggested in a study using acknowledgements included in journal articles (Mushanokoji, 1982). Since that time, attention has been paid from various disciplines to the meaning of the act of sending acknowledgements and the relationships extracted from it, yet studies that visualize these relationships as a network map and examine their validity are still inadequate. We ran experiments reconstructing academic networks based on the acknowledgements included in doctoral theses. In the humanities community in Japan, there is still a strong tendency for the publication of a single-authored work to be evaluated as an achievement rather than a co-authored article. Under such circumstances, it is significant to demonstrate the validity of the acknowledgement network as an alternative method of visualizing academic networks to the co-authorship network.
            The source materials used for this study are 120 Islam-related doctoral theses submitted to Japanese universities from the 1950s through the 2010s, as recorded in the CiNii Dissertations database (
                https://ci.nii.ac.jp/d/). The number of Islam-related doctoral theses in Japan began to rise in the 1980s and continued climbing into the 2010s (Fig. 1). Behind this increase in Islam-related doctoral theses seen from the 1990s onward was the introduction of an innovative research platform that transcended the boundaries of the university. In the 2000s, five leading Japanese universities and institutions launched a large-scale collaborative research project with many young researchers participating in both the research and administrative aspects. Moreover, university-level Islam-related studies—previously centered on history courses—shifted to inter-disciplinary studies under this project’s “Islamic Area Studies” framework, a phenomenon reflected by increases in area studies and multi-disciplinary research (Miura, 2004).
            
            
                
            
            Fig. 1: Number of doctoral theses in Japan (by religion studied)
            To visualize these social network connections, a graph is created displaying an “acknowledgement network,” placing authors and the scholars acknowledged by those authors as nodes, with edges extending from the acknowledgee to the author (Tian et al., 2021). The area of specialization for each person was taken from records in the KAKEN database of Japanese researchers (
                https://nrid.nii.ac.jp/index/). This makes it possible to visualize trends for separate academic fields.
            
            
                
            
            Fig. 2: Acknowledgement network since the 2010s
            Figure 2 shows the resulting acknowledgement network, created using Gephi network visualization software (
                https://gephi.org/). The number in each node is ID. This visualization of the acknowledgement network based on the doctoral theses since 2010 shows the presence of area studies (in blue) in addition to the history studies (in red) that were considered to be the mainstay of academic circles. Some acknowledgees are connected to multiple authors, indicating that these researchers are considered influential in their academic circle. These acknowledgees comprise the generation of researchers that contributed to the growth of Japan’s “Islamic Area Studies” network in the 2000s and have some formal or informal teacher-student connection to the authors. From the above, we can say there is a definite validity in using an acknowledgement network to visualize academic networks and the steps in establishing and developing academic connectivities. Also, the acknowledgements in the theses, which previously cited mainly the author’s family and academic advisors, have since the 2010s grown to list the names of as many as 10 or more researchers on average. These quantifiable trends reflect not only the increased opportunities for academic interactions as well as more researchers contributing to the completion of doctoral theses but also the given author’s intention to develop ever more public associations with other researchers. This is likely indicative of their aim to enhance their academic reputation by connecting with a larger number of prominent researchers.
            
        
        
            
                
                    Bibliography
                    
                        
                     武者小路, N. (Mushanokōji, N.) (1982). 「アメリカ経済史研究者間のコミュニケーション・ネットワーク：謝辞による分析」(Communication Networks among Scholars of American Economic History: An Analysis through Acknowledgments)『図書館学会年報』(
                        Annals of Japan Society of Library Science), 28 (1): 43–45.
                    
                    
                        Miura, T. (2004). “Survey of Middle East Studies in Japan: Historical Development, Present State, and Prospectus,” 
                        Japan Association for Middle East Studies, 19 (2): 169–200.
                    
                    
                        Tian, S., Xu, X. and Li, P. (2021). “Acknowledgement network and citation count: the moderating role of collaboration network,” 
                        Scientometrics, 126: 7837–7857, 
                        
                            https://doi.org/10.1007/s11192-021-04090-y
                         (accessed 19 April 2022).
                    
                
            
        
    



        
            This Today, relations between Russia, on the one hand, and the Czech Republic and Slovakia, on the other, are rather problematic both politically and socio-culturally. Most of the disagreements arise against a backdrop of reinterpreted history, where any interpretation of domestic political events becomes an occasion for international scandals on both sides (I.M. Savelyeva, 2004; Hradilek, 2010). 
            The source for the study was an archive of video interviews from the Czech Institute for the Study of Totalitarian Regimes "Memory and History of Totalitarian Regimes". 
            In order to build network models and conduct comparative analysis between models derived by mixed methods of computational linguistics and emotional analysis of audio recordings, 45 transcribed video interviews were selected from the source database.
            The transcribed interviews were analysed in the MAXQDA 2020 PRO software. In the first step of the analysis of the interviews, codes were generated to break down the text. It is the cluster approach of breaking down the text into valences, meaning groups, that allows the linguistic analysis to be carried out so quickly and with such a high quality. From the first minutes of working with dissidents' memories, it becomes possible to study the field of historical consciousness.
            Once the coding of the corpus of texts according to the results of the frequency analysis was completed, it became possible to create matrices of the frequencies of the coded keys in the text.
            The next step in the interview process was to analyse the texts using the 'interactive word tree' method. This approach allowed us to identify "noise" and cleanse the matrix from unnecessary word combinations. Further analysis of the texts was carried out using content analysis tools: document portrait, document comparison table and code layout.
            The resulting frequency tables and matrices for each interview were converted to CSV format and exported to the Gephi database, where network models of keywords in the form of graphs were constructed.
            Speech cues are a natural way of human communication, involving both direct linguistic content (e.g., texts) and implicit paralinguistic information (e.g., speaker emotions). Although visual emotion identification is more advanced in modern science, emotional audio analysis is gaining popularity among researchers due to ever-improving algorithms and increasing accuracy.
            The construction of network models of Czechoslovak dissidents' audio interviews based on emotional analysis allows not only a new perspective on computer analysis and visualisation methods as a researcher's tool, but also a wider disclosure of information potential of the sources under study. An important feature of this approach is the exclusion of the influence of emotional information of words.
            A frequency classification table based on a two-dimensional vector model of emotion was created to conduct emotional analysis of audio interviews and subsequent construction of network models. Pyhton script was written and Librosa audio analysis package was used to investigate the acoustic features of the interviews. The audio files were analysed: fundamental frequencies (f0), spectral characteristics, chromaticity, amplitude, harmonic characteristics, MFCC, logarithmic spectra, etc. Also, to extract markers of vocal emotion from speech (Ma, Y., et al., 2019), logarithmic spectrum conversion to Mel scale followed by discrete cosine transformation was performed (Scherer, K.R., 2003).
            The data were classified based on the audio recording spectra on a logarithmic scale for nine key emotions: sadness, fear, anger, frustration, excitement, disgust, happiness, surprise and neutral calm. 
            In the study, the first time the matrices of emotional models were constructed, an accuracy of 42% was achieved. In further analysis, difficulties were found in interpreting the emotions sadness and fear in female interviewees due to the lack of an extensive sound base of emotions in Czech. The algorithm was modified to incorporate expert judgement, and the accuracy of determination increased by more than 15%. 
            The tables of emotion analysis and emotion frequency were also translated into CSV files and uploaded to the Gephi software database to build network models in the form of graphs.
            Results of comparative analysis of online models of audio-interviews and transcribed texts gave different representations of the same events, allowing the researcher, on the one hand, to expand the information potential of the source, and on the other hand, to interpret personal experiences and experiences of interviewees within a constructive group memory about the fall of the communist regime in Czechoslovakia. 
            The proposed methodology of analysis, based on the comparison of network models, allows not only a broader disclosure of the information potential of historical sources, but also provides a basis for modelling and studying the mechanisms of transmission of group memory.
        
        
            
                
                    Bibliography
                    I.M. Savelyeva, A.V. Poletaev. Social perceptions of the past: the types and mechanisms of formation. Preprint WP6/2004/07. - Moscow: State University of Higher School of Economics, 2004. 56 с.
                    Hradilek, Adam (ed.): Za vaši a naši svobodu. Torst, ÚSTR, Praha 2010.
                    Ma, Y.; Hao, Y.; Chen, M.; Chen, J.; Lu, P.; Košir, A. Audio-Visual Emotion Fusion (AVEF): A Deep Efficient Weighted Approach. Inf. Fusion 2019, 46, 184–192.
                    Scherer, K.R. Vocal Communication of Emotion: A Review of Research Paradigms. Speech Commun. 2003, 40, 227–256.
                
            
        
    



        
            
                Introduction
                The poster presents an ongoing doctoral research project which aims at exploiting the analytical power of network visualisation to investigate European Baroque drama from a quantitative perspective. By building and exploring a balanced corpus of 150 plays in five different languages, it attempts to provide an empirical verification of the traditional view on the evolution of seventeenth-century European dramatic literature.
            
            
                Related literature
                In the last two decades, network analysis of literary texts has established itself as a core methodology within the field of computational criticism and is now routinely employed to gain insights into social formations and character interactions within fictional worlds. Its strength lies in the extreme formalisation of texts, which are converted graphs made by characters (nodes) and their relations (edges); thus, it is possible to investigate large corpora and unearth formal patterns which close reading may overlook (cf. Trilcke, 2013; Trilcke and Fischer, 2018).
                While many authors in the field have focused on methodological and technical issues, trying to leverage Natural Language Processing techniques to automate phases of the network extraction process, others have exploited the epistemological potential of literary networks analysis to try to answer questions concerning characters’ features, textual topologies, and literary genres: a comprehensive overview on both approaches, with a wide bibliography, is provided by Labatut and Bost (2020). Furthermore, growing ‘programmable corpora’, with dedicated tools for network visualisation, are now available for research and teaching purposes (e.g. the Drama Corpora project by Fischer et al., 2019).
            
            
                Project overview 
                This investigation of European drama is based on a research corpus of 150 plays, which is currently being assembled and will be later merged into the Drama Corpora repository (
                    dracor.org). The corpus includes European plays in English, French, Spanish, German, and Italian, and covers the timespan from 1561 to 1710; despite its relatively small extension, its texts have been selected (or sampled from larger collections) with the explicit purpose of avoiding canonical bias and producing a composite and somehow ‘representative’ picture of the period investigated.
                
                In a first phase, all plays which are not already available in DraCor are being transcribed or annotated to meet the platform’s requirements, starting from structured or plain (.txt from OCRs) open-access textual sources and ending up with fully formatted XML-TEI files. Once established the corpus, character networks will be extracted from the texts by means of the DraCor scripts, which operate by linking characters by scene proximity, and visualised through Gephi (
                    gephi.org) or similar software. 
                
                The textual structures embodied by the graphs will then be compared and interpreted according to the essential metrics of network analysis, such as centrality and modularity. The main aim will be measuring patterns of similarity and divergence between contemporary plays from different linguistic milieus and following the progressive evolution of drama throughout the designated temporal frame. Results are expected to contribute to the larger critical discussion on the features of Baroque, with a particular focus on the verification of some well-established literary theories the next section describes.
            
            
                Research question and goals
                The project is meant to address the lack of comparative studies on European early modern drama, and to complement the few existing ones (e. g. Küpper, 2018) with a ‘quantitative formalist’ perspective. Such shortage of extended transnational analyses of the genre has often been explained with the assumption that each ‘local’ form of Baroque drama represents a highly idiosyncratic system, sharply separated from the others by linguistic and cultural boundaries. This theory has been notably supported by Franco Moretti, who has described the evolution of European theatre throughout the seventeenth century as a process of Darwinist ‘speciation’. In his view, indeed, the common heritage of classical and medieval drama progressively broke down into several national variations, such as the German Trauerspiel or the French théâtre classique, each one with his own set of distinctive stylistic and formal features (1994: 97-99). 
                Computational literary network analysis appears particularly suited to assess the validity of Moretti’s reconstruction, since it is able to investigate the formal structures of all dramatic traditions involved with equal effectiveness. Accordingly, this study could help to measure whether (and how) Baroque-era theatre has split along national and cultural lines – and, conversely, to which extent phenomena of transfer of formal elements, such as plots or characters’ roles, have nevertheless taken place. From this topological perspective, network-based evidence might thus contribute to a clearer understanding of the development of European dramatic literature during one of its defining periods.
            
        
        
            
                
                    Bibliography
                    Fischer, F., Börner, I., Göbel, M., Hechtl, A., Kittel, C., Milling, C. and Trilcke, P. (2019). Programmable Corpora: Introducing DraCor, an Infrastructure for the Research on European Drama. Proceedings of DH2019: ‘Complexities’. Utrecht: Utrecht University doi:10.5281/ZENODO.4284002.
                    Küpper, J. (2018). The Cultural Net: Early Modern Drama as a Paradigm. Berlin and Boston: De Gruyter doi:10.1515/9783110536638.
                    Labatut, V. and Bost, X. (2020). Extraction and Analysis of Fictional Character Networks: A Survey. ACM Computing Surveys, 52 (5): 1–40 doi:10.1145/3344548.
                    Moretti, F. (1994). Modern European Literature: A Geographical Sketch. New Left Review (206): 86–109.
                    Trilcke, P. (2013). Social Network Analysis (SNA) als Methode einer textempirischen Literaturwissenschaft. In Ajouri, P., Mellmann, K. and Rauen, C. (eds), Empirie in der Literaturwissenschaft. Münster: Brill | mentis, pp. 201–47 doi:10.30965/9783957439710_012.
                    Trilcke, P. and Fischer, F. (2018). Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen. In Huber, M. and Krämer, S. (eds), Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden. Wolfenbüttel: Herzog August Bibliothek doi:10.17175/sb003_003.
                
            
        
    



        
            Exactly seventy years ago in Japan, Grandmaster Daihachi Oguchi founded Osuwa Daiko, the first ever 
                kumi-daiko, or ensemble drumming, group in the world (Bender, 2012). Many of his artistic teachings, repertoire, and philosophical practices endure to this day thanks to the countless number of students he personally trained throughout his lifetime, some of whom later became leaders of their own right. One student of his, Grandmaster Seiichi Tanaka, was the first to introduce the 
                taiko, which is the Japanese word for drum, to North Americans when he founded San Francisco Taiko Dojo in 1968 (Varian, 2013). Soon after, two more taiko groups from the United States, namely Kinnara Taiko (1969) and San Jose Taiko (1973), were established largely due to the monumental efforts of 
                sansei, or third-generation Japanese Americans, who felt empowered and heard when immersed within the artform (Ahlgren, 2018). These original pioneers were inspired to invent a unique philosophy for playing and learning taiko, which would be passed down to newer generations of taiko players throughout the rest of the continent that were at first largely of Asian descent and politically active (Wong, 2019). In the present, there are now hundreds of taiko groups scattered across North America, whose members represent a wide spectrum of racial and ethnic backgrounds, and numerous artists who have taken up taiko as their full-time profession (Walker, 2016). While recent scholarly work has made considerable strides towards elucidating the role in which taiko has played in addressing identity politics (Konagaya, 2001) (Terada, 2001), challenging gender roles (Wong, 2000) (Yoon, 2009), and reclaiming Asian American cultural heritage (Izumi, 2001) (Yoon, 2001), there is still a noticeable gap in our understanding of how a community of taiko practitioners has grown and spread from a multi-dimensional point of view. Furthermore, there is currently a lack of digital resources that make the holistic legacies of taiko artists more easily accessible to general audiences. 
            
             To this end, I have adopted a set of interdisciplinary tactics to digital scholarship that begins to successfully capture the story of how the taiko artform has expanded and evolved in North America. As an independent scholar and volunteer of the Taiko Community Alliance (TCA), I primarily use open-source digital tools to further empower the greater taiko community through several aims. One aim is to develop a comprehensive time-series map that displays when and where taiko ensembles formed over its rich 53-year history. Using custom-built software that ingests archival materials, a dynamic movie that shows the locations of each group and different moments is produced. Such a resource is becoming useful for new members interested in locating and joining a nearby taiko group. Another aim further examines the impact the original ensembles had on subsequent groups that were later founded. By using previous taiko census campaigns conducted by TCA, a database that contains individual players, names of groups that each of them was a part of, and the years in which they retained some form of membership is created. With the help of open-source programs like Gephi, static and dynamic social networks show moments when edges form and break off between groups that share common members. This effort has further yielded useful quantitative analysis on identifying the degree and directionality of influence certain taiko groups have had on other ensembles, and for uncovering distinct collections of groups using standard community detection algorithms. And the final aim is to reveal significant factors that were instrumental in sustaining the taiko community over the course of decades using a mixed-methods research approach, which places both qualitative and quantitative data on near equal footing. By analyzing publicly available online materials, electronic interviews, and census data, an intriguing model of symbiosis emerges between taiko groups that reside on college campuses and others that are largely community-based. Some important takeaways include the following: (1) many founders of collegiate groups received taiko instruction in some form as young children and adolescents from older taiko players, and (2) a sizable number of collegiate players continue being involved in taiko after graduating. Such a symbiotic relationship is reasonably hypothesized to not be strictly exclusive to the taiko community and may be an appropriate model to adhere to for other communities of practice that are strategizing their own longevity.
             There are several useful applications that can branch off this work. One ongoing avenue of research concerns unearthing salient features of North American taiko’s relationship with the rest of the world, which includes Southeast Asia, Europe, South America, and Oceania, using graph-based methods. Another immediate application is building upon the digital toolset presented earlier to create a historic map of taiko groups from all around the world. And finally, there is considerable interest in cultivating a more mature understanding of how taiko communities in both Japan and North America co-evolved over time. I hope my efforts offer novel insights into the diverse range of social dynamics that are at play within the taiko network and showcase the power of digital tools in enhancing visibility for a community of artists that are continuously looking for new opportunities to share and advance the artform. In addition, I hope to (re)ignite conversations on how the field of digital studies and humanities can better preserve and amplify the legacies of underrepresented groups that rarely receive the attention they deserve. We bear the responsibility as digital scholars to preserve the past of those who have continuously been marginalized and to continuously challenge sweeping notions of what it means to be accepted as a citizen of their community.
        
        
            
                
                    Bibliography
                    
                        Ahlgren, A. K. (2018). Drumming Asian America: 
                        Taiko, Performance, and Cultural Politics, Oxford: Oxford University Press.
                    
                    
                        Bender, S. (2012). Taiko Boom: 
                        Japanese Drumming in Place and Motion, Berkeley: University of California Press.
                    
                    
                        Izumi, M. (2001). Reconsidering Ethnic Culture and Community: A Case Study on Japanese Canadian Taiko Drumming. 
                        Journal of Asian American Studies, 4: 35-56.
                    
                    
                        Konagaya, H. (2001). Taiko as Performance: Creating Japanese American Traditions, 
                        Japanese Journal of American Studies, pp. 105-124.
                    
                    
                        Terada, Y. (2001). Shifting Identities of Taiko Music in North America, 
                        Senri Ethnological Reports, 22: 37-59.
                    
                    
                        Varian, H. (2013). 
                        The Way of Taiko, Berkeley: Stone Bridge.
                    
                    
                        Walker, K. (2016). Taiko in the USA and Canada: Key Findings from the Taiko Census 2016, Technical Report.
                    
                    
                        Wong, D. (2000). Taiko and the Asian/American Body: Drums, Rising Sun, and the Question of Gender. 
                        The World of Music, 42 (3).
                    
                    
                        Wong, D. (2019). Louder and Faster: 
                        Pain, Joy, and the Body Politic in Asian American Taiko, University of California Press.
                    
                    
                        Yoon, P. J. (2009), Asian Masculinities and Parodic Possibility in Odaiko Solos and Filmic Representations. 
                        Asian Musi: Music and the Asian Diaspora, 40(1) pp. 100-130.
                    
                    
                        Yoon, P. J. (2001). She’s Really Become Japanese Now!’ Taiko Drumming and Asian American Identifications. 
                        American Music, pp. 417-438.
                    
                
            
        
    



        
            
                Abstract
                
                This paper analyzes 1206 inscribed bronzes excavated from different archaeological sites. Treating the phenomenon that bronzes with different clan signs appeared in the same tomb as a kind of co-occurrence, the co-occurrence networks of clan signs in two sequential time periods (1300 BC – 1046 BC and 1046 BC – 900 BC) are constructed to examine the interrelationship of the clans. The
                 strong heterogeneity of the networks in different periods shows the huge impact of the Zhou conquest of Shang, which is called “the Great Transformation from Shang to Zhou” by historians and archaeologists.
            
            
                Keyword Chinese bronze inscription; 
                clan sign; co-occurrence; historical network analysis
            
            
                1
                BACKGROUND
            
            One well-recognized cultural trait of ancient China was the existence of the clan system, which can be traced to the Shang dynasty and has deeply affected Chinese history for three thousand years.
            Due to a lack of historical documents, bronze inscriptions are 
                of great value to the study of clans in early China. During the late Shang dynasty (1300 BC – 1046 BC), the most common form of bronze inscriptions was a single graph functioning as a kind of emblem designating the clan name of the ancestor to whom the bronze was dedicated. After the Zhou people conquered the Shang dynasty, this cultural custom still lasted for a period of time among the Shang people in the early Western Zhou dynasty (1046 BC – 900 BC). 
            
            According to our current estimate, there are about 6800 bronzes with the so-called clan signs. 1200 of them are unearthed, while the rest are unprovenanced. A thorough investigation of these materials can increase our understanding of the clan system and fill the gaps in the early history of China.
            
                
                    
                
                
                    
                
            
            
                Figure 1. Bronzes with different clan signs unearthed from Shigushan(石鼓山) M1
            
            
                2
                RELATED WORK
            
            Scholars have been studying the clan-sign inscriptions since the Song dynasty. However, traditional humanities researchers mostly focused on the paleographical study of clan signs or the historical origin of each clan. The inter-clan associations haven’t been paid enough attention. 
             Archaeological discoveries in recent decades provided new clues. Bronze objects belonging to one clan were often found to be buried in tombs of another clan, which can be attributed to gift-giving, marriage, exchange or pillage. Bronzes with different clan signs excavated from the same tomb can reflect the interrelationships between the tomb occupant’s clan and other clans.
             Some scholars have realized the potential of such investigation (Barnard, 1986; Sun, 2017). But their attempts were all limited to case studies, not from a macro perspective. Using methods of Social Network Analysis, we can discuss the clan system and social structure of early China from a whole new perspective. 
            
                3
                DATA AND METHOD
            
            Our data were extracted from 
                A Compendium of Inscriptions and Images on Bronzes from the Shang and Zhou Period (商周青銅器銘文暨圖像集成) by manual, including 1206 unearthed bronzes with clan signs. Among them, 684 bronzes were excavated from 219 tombs of the late Shang dynasty, while 522 bronzes were excavated from 185 tombs or hoards of the Western Zhou dynasty. The collected data underwent text preprocessing including tokenization, standardization and manual proofreading. 309 clan signs were recognized on these bronzes. 
            
            The following methods were then applied:
            (1) Frequency analysis of clan signs from the two periods was conducted to show the rise or decline of different clans, since the bronzes were thought to be the symbol of wealth and status.
            (2) Treating the archaeological phenomenon that different clan signs appeared in the same tomb as a kind of co-occurrence, the co-occurrence networks of clan signs in the two periods (1300 BC – 1046 BC and 1046 BC – 900 BC) were constructed to examine the interrelationships of these clans.
            
                
            
            
                Figure 2. Clan sign frequency in the Shang and Western Zhou dynasty
            
            
                
            
            
                
                Figure 3. Co-occurrence network of clan signs in the late Shang dynasty (1300 BC – 1046 BC)
            
            
                
            
            
                Figure 4. Co-occurrence network of clan signs in the early Zhou dynasty 
                (1046 BC – 900 BC)
            
            
                
                Table 1. Clan groups in the late Shang dynasty (1300 BC – 1046 BC)
            
            
                
            
            
                Table 2. Clan groups in the early Zhou dynasty (1046 BC – 900 BC)
            
            
                
            
            
                4
                RESULTS AND DISCUSSION
            
            
                4.1 
                
                Changes of the networks over time
            
            The network in the early Western Zhou dynasty (1046 – 900 BC) is denser than the network in the late Shang dynasty (1300 – 1046 BC), although the size of nodes in the two networks are almost the same. More frequent co-occurrences of clan signs reflect more frequent associations between clans. 
            
                Table 3. Metrics of the co-occurrence networks of clan signs
            
            
                
                    
                        Period
                    
                    Clan signs
                    Edges
                    
                        Density
                    
                    
                        Average degree
                    
                    
                        Average weighted degree
                    
                    
                        Average path length
                    
                    
                        Modularity
                    
                
                
                    
                        
                        1300 – 1046 BC
                    
                    
                        137
                    
                    
                        261
                    
                    
                        0.028
                    
                    
                        3.810
                    
                    
                        7.737
                    
                    
                        3.079
                    
                    
                        0.708
                    
                
                
                    1046 – 900 BC
                    
                        130
                    
                    
                        428
                    
                    
                        0.051
                    
                    
                        6.585
                    
                    
                        13.631
                    
                    
                        2.657
                    
                    
                        0.526
                    
                
            
            
                4.2 Clan groups in different periods
            
            Using the Louvain algorithm with a resolution at 1.0, we partitioned the two networks separately into different communities, which can be seen as the clan groups. We laid out the network with 
                the Yifan Hu algorithm and used different colors for different groups (Figure 3 and 4). Clans of the same group had closer connections.
            
            Besides, we used the Kulczynski distance (Cha, 2007; Shang, 2021) to
                 measure the similarity between groups in different periods. There is no similarity score higher than 0.2, proving that the heterogeneity of the two networks is very strong.
            
            
                Table 4. Kulczynski similarity between the clan groups in different periods
            
            
                
                    
                    
                        Group 1
                    
                    
                        Group 2
                    
                    
                        Group 3
                    
                    
                        Group 4
                    
                    
                        Group 5
                    
                    
                        Group 6
                    
                    
                        Group 7
                    
                
                
                    
                        Group A
                    
                    0.042
                    0.091
                    0.091
                    0
                    0.072
                    0.099
                    0
                
                
                    
                        Group B
                    
                    0.053
                    0.111
                    0.111
                    0
                    0
                    0
                    0.126
                
                
                    
                        Group C
                    
                    0.056
                    0
                    0
                    0.085
                    0.170
                    0
                    0
                
                
                    
                        Group D
                    
                    0.060
                    0.063
                    0
                    0
                    0
                    0
                    0.133
                
                
                    
                        Group E
                    
                    0
                    0.142
                    0.071
                    0
                    0
                    0.125
                    0
                
                
                    
                        Group F
                    
                    0
                    0.159
                    0
                    0
                    0
                    0
                    0
                
                
                    
                        Group G
                    
                    0
                    0
                    0
                    0
                    0
                    0
                    0
                
            
            Cultural and political changes from the late Shang to the early Western Zhou dynasty is called “the Great Transformation from Shang to Zhou” by some scholars (Wang, 1923). With methods of network analysis, we examined this traditional topic from a new angle and displayed it in a visual way. The results showed us how deeply the Zhou conquest of Shang changed the inter-clan associations and the entire clan system of the Shang people.
             For future work, we plan to build a hyper-network with three different types of nodes (tombs, bronzes and clan-sign inscriptions) to reveal more historical details, which can not only increase our understanding of the clan system but also contribute to the study of burial custom in early China.
        
        
            
                
                    Bibliography
                    
                        Bastian, M., Heymann, S. and Jacomy, M. (2009). Gephi: An Open Source Software for Exploring and Manipulating Networks. 
                        Proceedings of the International AAAI Conference on Web and Social Media, 3(1): 361-62.
                    
                    
                        
                        Barnard, N. (1986). A New Approach to the Study of Clan-sign Inscriptions of Shang. In Chang, K. C. (ed), 
                        Studies of Shang Archaeology. New Haven: Yale University Press, pp. 141-206.
                    
                    
                        Cha, S.-H. (2007). Comprehensive Survey on Distance/Similarity Measures Between Probability Density Functions. 
                        International Journal of Mathematical Models and Methods in Applied Sciences, 1(4): 300-307.
                    
                    
                        Fei, J. C. H. and Liu, T.-J. (1982). The Growth and Decline of Chinese Family Clans. 
                        The Journal of Interdisciplinary History, 12(3): 375-408.
                    
                    
                        He, J. (2009). 
                        Studies of Clan Inscriptions on Bronzes in the Shang and Zhou Dynasty. Jinan: Qilu shushe.
                    
                    
                        Liu, J., Wang, Z. and Xin, Y. (2013). The Excavation of the Western Zhou Tombs at Shigushan in Baoji, Shaanxi. 
                        Wenwu, 2: 4-54.
                    
                    
                        Painter, D. T., Daniels, B. C. and 
                        
                        Jost, J. (2019). Network Analysis for the Digital Humanities: Principles, Problems, Extensions. 
                        Isis, 110(3): 538-54.
                    
                    
                        Rawson, J. (1985). Late Western Zhou: A Break in the Shang Bronze Tradition. 
                        Early China, 11: 289-96.
                    
                    
                        Sun, Y. (2017). 
                        Inscribed Bronzes, Gift-giving and Social Networks in the Early Western Zhou: A Case Study of the Yan Cemetery at Liulihe. In 
                        Shaughnessy, E. C. (ed), 
                        Imprints of Kinship: Studies of Recently Discovered Bronze Inscriptions from Ancient China. Hong Kong: The Chinese University Press, pp. 47-70.
                    
                    
                        Shang, W. (2021). The Aristocratic Social Network in the Eastern Jin Dynasty (317-420 C.E.) [Panel discussion]. Historical Network Research in Chinese Studies. (Online conference)
                    
                    
                        Wang, G. (1923). Institutional Change in the Yin and Zhou Dynasties. In Wang, G., 
                        Guantang Jilin. Shanghai: Wucheng Jiangshi kanben.
                    
                    
                        Wu, Z. (2012). 
                        A Compendium of Inscriptions and Images on Bronzes from the Shang and Zhou Period. Shanghai: Shanghai guji chubanshe.
                    
                
            
        
    



        
            
            Introducción
         
            La propuesta parte del convencimiento del gran potencial de la visualización de la información para el estudio de los textos literarios, apoyándose en la teoría de las redes sociales y técnicas de la visualización de los datos. Si el texto literario es una red de fenómenos discursivos, lo es por excelencia una obra dramática, una red de parlamentos e interacciones entre los personajes. Aplicando los instrumentos de análisis y representación gráfica, se pueden ilustrar las relaciones discursivas entre los personajes. El tratamiento de los datos permite visualizar y “medir” estas relaciones, así como evidenciar fenómenos difícilmente perceptibles en la lectura tradicional, creando nuevos conocimientos e interpretaciones. 
            
            Objetivo
         
            El propósito de esta presentación es mostrar ejemplos de visualización de texto y sus posibilidades interpretativas, basándose en los cuatro dramas rurales del autor granadino:
                 Bodas de sangre (1933),
                 Yerma (1934), 
                Doña Rosita la soltera (1935)
                 La casa de Bernarda Alba (1936). Se exploran las potencialidades de Gephi y RAWGraphs para sintetizar, sistematizar y representar los datos mediante diferentes grafos y otras representaciones infográficas (dendrogramas, diagramas aluviales, etc.). La exploración muestra los nuevos conocimientos que se pueden extraer a partir de transformación de los datos en imágenes, referentes a la estructura, composición y contenidos de las obras teatrales. 
            
            
            Método
         
            En el trabajo se aplican métodos de análisis cuantitativo y cualitativo. Los textos digitalizados en formato TEI, GEFX y CVS provienen de Drama Corpora Proyect (DraCor, disponible en: 
                https://dracor.org/). Para representar la red de conexiones entre los personajes se crearán los grafos, que representan la red de protagonistas y el peso cuantitativo de las intervenciones discursivas de cada una. Para su procesamiento y generación de representaciones gráficas se utiliza el software libre Gephi 0.9.2-beta (Gephi.org 2008-2017) y RAWGraphs (https://rawgraphs.io/). Se comparan los grafos generados por ambas herramientas con los creados por la API de
                 DraCor llamada Shiny DraCor (
                https://shiny.dracor.org/). Para visualizar las relaciones entre las palabras y unidades léxicas más significativas se recurre a los grafos léxicos. Para mapear las estadísticas textuales se rastrean otras 29 herramientas de representación gráfica incluidas en la interfaz de RAWGraphs. 
            
            
            Avance de conclusiones
         
            El interés principal de la propuesta radica en la exploración de las visualizaciones y detección de las singularidades de los elementos léxicos en el tejido del texto lorquiano. Los 
                resultados de análisis informático se confrontan con los enfoques críticos tradicionales. Se evalúa críticamente la eficacia de las técnicas de visualización para la interpretación de la obra literaria, su valor epistemológico, las posibilidades y limitaciones de las herramientas utilizadas (algunas de ellas automatizadas) y los retos que planean a los estudios literarios. Sin duda, es una vía que puede revelar tendencias y patrones inesperados y aportar claves interpretativas de interés. 
            
        
        
            
                
                    Bibliografía
                    Barros García, B. B. (2020). El texto literario hecho datos: F. M. Dostoievski en el marco de las Humanidades digitales y los enfoques cuantitativos. 
                        452ºF. Revista De Teoría De La Literatura Y Literatura Comparada, (23), 53–77. 
                        
                            https://doi.org/10.1344/452f.2020.23.3
                        
                    
                    Bastian, M., Heymann, S., & Jacomy, M. (2009). Gephi: An Open Source Software for Exploring and Manipulating Networks. 
                        Proceedings of the International AAAI Conference on Web and Social Media, 
                        3(1), 361-362. 
                        
                            https://ojs.aaai.org/index.php/ICWSM/article/view/13937
                        
                    
                    Burley, D., Ashburn, V. (2010). Information Visualization As A Knowledge Integration Tool. 
                        Journal of Knowledge Management Practice, 11(4), 18.
                    
                    Cherven, K. (2015). 
                        Mastering Gephi Network Visualization. Packt Publishing Ltd
                    
                    Drucker, J. (2011). Humanities Approaches to Graphical Display. 
                        Digital Humanities Quarterly, 5 (1) 
                        
                            www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html
                        
                    
                    Martínez Carro, Elena. (2018) Los personajes femeninos lorquianos desde una interpretación digital, en: Álvarez Ramos, Eva y Blasco Pascual, Javier (eds.). 
                        Humanidades Digitales. Retos, Recursos y Nuevas Propuestas, Agilice Digital, Valladolid.
                    
                    Santa María, M.T., Calvo Tello, J., Jiménez, C.M. (2021). ¿Existe correlación entre importancia y centralidad? Evaluación de personajes con redes sociales en obras teatrales de la Edad de Plata, Digital Scholarship in the Humanities, Volume 36 (1), 81–88. https://doi.org/10.1093/llc/fqaa015
                
            
        
    



        
            
                Pitchfork.com has published music reviews, news, interviews, feature stories since 1995. Growing out of 1990s zine culture, 
                Pitchfork took advantage of the affordances of the internet early and has since become the self-proclaimed “most trusted voice in music.” In that time, they have reviewed over 23,000 albums by more than 10,000 artists. When it began, 
                Pitchfork was known for its attention to alternative/punk/indie rock and, though it has increased its coverage of hip-hop, pop, and more mainstream music over the years, it remains a source of information about unconventional and emerging artists across genres.
            
            Though they often cite an extraordinary number of genres, sub-genres, and sub-sub-genres in their reporting, they stick to a very conservative list of nine official genre tags for most of their reviews.
            
                
            
            
                Figure 1: Reviews by genre in 
                Pitchfork
                . Note that one review may have several genres, each of which would be counted in this graph. The “Null” category describes a small subset of reviews that had no genre tags.
            
            Debates over the function of genre are common in music as well as literature, film, and culture studies generally (Sanneh, 2021; Lena, 2012; Brackett, 2016). For an outlet like 
                Pitchfork, they function, at least in part, to set expectations for readers. Yet, given the scale and the breadth of the types of music covered by 
                Pitchfork, these nine highly unevenly distributed tags are somewhat uninformative. While the genre tags alone may do little to set expectations, the reviews themselves do this work in at least one highly characteristic way: by comparing the artist under review to other artists who share similar qualities. Fortunately for us, 
                Pitchfork does this in an easily minable fashion by maintaining individual pages for several thousand artists, and then using links to these pages when the artists are mentioned in reviews.
            
            
                
            
            Figure 2
            Figure 2 is a screenshot from a review. Clicking on “The Meters” or “Dr. John”, indicated by red underlining, will take you to the individual pages for each respective artist. We scraped all of the reviews and relevant metadata, including the artist links. With that data set, we created networks of artists where edges are drawn between any artists who have links in the same review.
            
                
            
            
                Figure 3. Nodes are artists; edges reflect co-presence in reviews. Node size shows edge count. Color shows detected community.
            
            Tools like Gephi can depict sub-groups in a network like the one we created via community detection (calculated in Gephi using the Louvain method (Blondel et al, 2008)). However, there are important limitations to this method. Because it is non-deterministic, the precise membership of each group, and even the total number of groups, can vary each time the algorithm is run. To work around this, we introduce a method we call “metamodularity”. We simply ran Louvain community detection on the artist network 10,000 times.
                
                     We used the NetworkX and Community libraries in Python. Some artists had no links, because they were never mentioned in an article with anyone else. To avoid very small communities and improve the legibility of results, we also filtered out 34 artists not connected to the main network. This left us with 7,524 unique artists.
                 Though there are many other possibilities that would achieve similar ends, including examining the communities detected during the “passes” from which the Louvain method constructs its final groups, our approach has several key advantages: It is simple to run, easy to understand, and eschews a non-deterministic approximation in favor of data about the probability of particular groupings.
            
            Using this method, we can show how often any two artists were sorted into the same group. For instance, the jazz musician Alice Coltrane was grouped with John Coltrane 10,000 times, with Sly and the Family Stone 4,939 times, and with Guns n’ Roses one time. This gives us a more reliable and comprehensible picture of the level of connection between the artists.
            
                
            
            
                Figure 4: Showing the top artists (sorted by number of total connections with other artists) who group with Bon Iver at increasing thresholds of connectedness
            
            Using the results from this method, we examined every group of at least 25 artists at six different thresholds and renamed the groups to reflect our assessment of the underlying artistic community. For instance, we called the rightmost group in Figure 4 “00’s Indie Rock”.
            It is worth underscoring that our group names are subjective. Nonetheless, they help to show the relationship between groups at different tiers. The Sankey diagram, Figure 5, makes clear the branching of closely-knit artist communities from larger, more loosely connected groups.
            
                
            
            Figure 5
            The richness of these results points to many interesting findings about the operation of genre in 
                Pitchfork; we will mention just one here. Some of the genres suggested by Figure 5 are incredibly specific—e.g., our names for groupings of metal artists include classic, goth, punk, grüv, and crossover. One notable exception is a group of predominately African American musicians, which is remarkably large and stable up until the 9,000 threshold. The artists within it range from instrumental hip hop composer Flying Lotus to Motown legend Marvin Gaye to trap rapper Young Thug to R&B singer-songwriter Sade to funk innovators Funkadelic to crossover hip hop star Cardi B. This is a far more capacious group along aesthetic, market, and even historic grounds than we see in, e.g., the heavy metal clusters at the same metamodularity threshold. This may reflect real-world connections, since many artists in this group have worked together in various ways, perhaps more often than metal bands have. Or this may reflect a structural difference in the way that writers at 
                Pitchfork have covered Black artists relative to their reviews of white musicians, particularly in the early years of the publication. In any case, it is a noteworthy difference in the shape of genre in this corpus.
            
            This finding will be one of three concluding points in the talk. We will also discuss how, in this dataset, metamodularity based solely on artists’ connections seems to demonstrate Carolyn Miller’s description of genre as “social action” rather than some kind of top-down, taxonomic structure (Miller, 1984). We will also reflect on the potential use of metamodularity as a more broadly useful method for understanding (and depicting) network structures.
        
        
            
                
                    Bibliography
                    
                        Blondel, V., Guillaume, J., Lambiotte, R. and Lefebvre, E. (2008). Fast Unfolding of Communities in Large Networks. 
                        Journal of Statistical Mechanics: Theory and Experiment
                        . 2008: 10. 9.
                    
                    
                        Brackett, D. (2016).
                        Categorizing Sound: Genre and 20th Century Popular Music
                        . Oakland: University of California Press.
                    
                    
                        Lena, J. (2012).
                        Banding Together: How Communities Create Genres in Popular Music
                        . Princeton: Princeton University Press.
                    
                    
                        Miller, C. (1984). Genre as Social Action. 
                        Quarterly Journal of Speech
                        . Vol 70, 2; 151-76.
                    
                    
                        Sanneh, K. (2021). 
                        Major Labels: A History of Popular Music in Seven Genres
                        . New York: Penguin Press.
                    
                
            
        
    



        
            Not all Humanities have been equally touched by the digital. For textual scholarship, history and linguistics, for instance, we can have a substantial number of scholarly contributions, particularly when we include experiences embodied in projects and resources. However, comparatively speaking, digital literary criticism has had few followers. An exception are Computational Literary Studies (CLS) that apply quantitative methods to large amount of literary and bibliometric data. Linked to the methods of distant reading [Moretti, 2005], this approach enjoys great success today, while web resources like Voyant, software like Gephi, and programming environments like R, have made text mining very accessible, even for those with limited computer skills. Linked to this approach, stylometry and authorship attribution are also thriving. Particularly mediatized researches are the initiatives that led to "unmasking" the identies of Robert Galbraith, a pseudonym of J.K. Rowling, and Elena Ferrante [Joula, 2015; Tuzzi and Cortelazzo, 2018]. However, literary criticism connected to close reading seems almost absent from the DH radar. The CATMA tool, designed to define personalized tagsets for (mainly) literary analysis [Meister 2020], represents a bright exception. Meister, in fact, is one of the few scholars that has engaged with digital literary criticism and digital hermeneutics; the latter has been explored also by Van Zundert (2016) and Ramsey (2011), but from a quantitative perspective. Relatively few scholars in DH have to addressed literary criticism with qualitative approaches, which are, conversely, among the most important for non-digital literary scholars.
            The reasons for this absence are probably to be found in the controversies about the use of markup within texts that have inflamed the scholarly community since the Eighties. The act of adding explicit markers in the text has been subjected to scrutiny, as it is perceived (rightly) as a harbinger of interpretation and this fact has been (and is, to a certain extent, still) perceived as an invasion, a disfigurement of the text; Cummings (2008) gives a vivid account of the debate and reflects on how it has limited the use of TEI for literary criticism. The argument goes that once the text is marked up, it cannot be reused by others because the interpretation added by the encoder would make it unusable. According to this vision, digital texts must be made available in their most neutral and objective form, and any form of annotation, including editorial, must be avoided. Sperberg-McQueen 1991 and Cummings 2008, amongst others, have tried to address the issue, and I have argued elsewhere on the hermeneutic fallacy of the category of objectivity [Pierazzo 2015]; but these methods remain far from impacting “the Humanities at large” and in particular the literary scholars [Meister 2020]. However, in order to contextualize this debate, one should go back to when this controversy was born. The urgency of those years was to put texts online, to create literary corpora for concordances and the study of word frequencies; at the time, digital acquisition of texts, the transformation of the printed into sequences of characters to be analyzed by computers (Machine Readable Form) was mostly done by hand, with an enormous expenditure of time and energy. The emphasis was therefore on making texts available and on the need of not repeating work. Researchers did not want to work with texts full of manually added codes which then had to be removed just as manually in order to reuse the texts.
            It is worth noting, though, how this discourse hides the concept of DH as a service: the goal was thought produce resources for others to do “real” research. This argument is not only dangerous, condemning DH to a mere service, but also wrong, as text, any text, can only be the result of the dialectical compromise between the source documents that contains it and scholars that interpret it (even when they “only” transcribe it), and therefore no text can ever be considered objectively neutral [Pierazzo, 2015]. Today conditions have changed: most literary texts are digitally available in many versions, not to mention the plethora of tools and methods to “get rid of” markup, therefore the objections do not stand in the same way.
            Another obstacle for the uptake of DH in literary studies is the conviction that close reading and critical interpretation only require a reader, a text, and a (printed) essay, and therefore computers, in this context, are useful as typewriters [Kirschenbaum, 2016]. Yet, the lack of experimentation and engagement of the scholarly community in DH for literary analysis does not allow for a clear assessment of the epistemological added value of using computers for one or few texts at a time. But shouldn’t be this the moment for rethinking Digital Literary Studies? Couldn’t we at least try to use markup, ontologies and other methods to understand a text, or answer questions about interpretation?
            The paper will present some experiences at the University of Tours using TEI markup for the history of ideas, and ontologies and databases for analysis of fictional entities (people and places). We have applied these methods to works by Boccaccio, the Vite by Vasari, and to a small corpus of librettos of the 17th century. These experiments are showing promising results, not only in literary terms, but also on a largely methodological perspective, with colleagues and researchers finding themselves challenged and enticed by DH heuristics.
            Conditions are ripe for experiences and discussions in order to evaluate the impact of DH in literary studies, particularly in the light of the advancements in HTR and other types of CLS that have the potentials of bringing a large amount of unknown and understudied texts into the literary arena. This could truly change our perspectives and understandings on literature, but we need to sharpen our hermeneutical tools first.
        
        
            
                
                    Bibliography
                    
                        Cummings, J., 2008. The text encoding initiative and the study of literature. In 
                        A companion to digital literary studies (pp. 451-76), Blackwell.
                    
                    
                        Kirschenbaum, M.G., 2016. What is digital humanities and what’s it doing in English departments?. In 
                        Defining Digital Humanities (pp. 211-220). Routledge.
                    
                    
                        Juola, P., 2015. The Rowling case: A proposed standard analytic protocol for authorship questions. 
                        Digital Scholarship in the Humanities, 30(1): 100-113.
                    
                    
                        Pierazzo, E., 2015. 
                        Digital scholarly editing: Theories, models and methods. Routledge.
                    
                    
                        Ramsay, S., 2011. 
                        Reading Machines: Toward and Algorithmic Criticism. University of Illinois Press.
                    
                    
                        Sperberg-McQueen, C.M., 1991. Text in the electronic age: Texual study and textual study and text encoding, with examples from medieval texts. 
                        Literary and Linguistic Computing, 6(1): 34-46.
                    
                    
                        Tuzzi, A. and Cortelazzo, M.A., 2018. What is Elena Ferrante? A comparative analysis of a secretive bestselling Italian writer. 
                        Digital Scholarship in the Humanities, 33(3): 685-702.
                    
                    
                        Van Zundert, J.J., 2016. Screwmeneutics and hermenumericals: the computationality of hermeneutics. 
                        A companion to digital humanities. (pp. 331-347) Blackwell.
                    
                
            
        
    



        
            Memory studies are a promising research field today since on the one hand evidence is supplemented by memories and generational memory that make history more inclusive and comprehensive (Hirsch, 1993), and on the other hand, individual text analysis tools open new horizons for reaching information from oral sources so that it would be possible to reach the main goal of using Digital Humanities approaches, to absorb more information from any sources (Burdick, 2012). Since modern historical studies focus on sources of various origins (analog, digital), using digital humanities methods for analysis seems to be in good demand.
            The paper presents the accomplishment of the practical stage of a Ph.D. thesis in Soviet History that includes using tools of computer linguistic and statistics in the analysis of collective memory. Practically, this project addresses the problems of studying the history of totalitarianism in the USSR in the context of collective memory and working with oral sources. The main trends are focused on examining ethnic minorities' experiences and the reasons for silencing the past through the lens of historiographical analysis, shedding light on the serious fragmentation of historiography about the USSR concerning ethnic cleansing, due to the reasons for the classification of archives and ideological evaluations (Chang, 2019). Basically, the research focuses on the collective memory of Russian Germans of the deportations of 1941, the repressions of the 1930s, and life in concentration camps in the 1940s and 1950s.
            Predominantly, the Digital Humanities approaches in this study are used to address two main objectives:
            1. to analyse the structure and content of the memorial narrative;
            2. verification of collective trauma in intergenerational dynamics.
            This paper aims to make wider the discussion of using digital tools and descriptive statistics in historical sources and shows the unpredictable possibility of implementation of the methods in applied historical studies such as Memory Studies. Although the practical side of the contribution is to represent a methodological synthesis for verification of collective trauma through emotional analysis and network analysis of interviews.
            Talking about the used evidence, it is important to underline that the main historical sources are interviews with victims of ethnic cleansing in the USSR. Currently, the study uses more than 200 interviews taken during field research in Russia in 2018-2020 and obtained from the archive of the Memorial organization archives (historical research non-government organization, banned in Russia Federation according to federal law number 481 of 30 December 2020 on Foreign Agents, which restricts the freedoms of academics and human rights defenders.).
            There is a lot of discussion around collective trauma about how to assess its presence, transformation or absence. The emotional analysis approach to interviews will actually allow us to trace the dynamics of the respondent's emotional involvement, but network analysis, by contrast, allows us to trace the dynamics of the narrative and assess which themes become a priority for each generation and which themes remain relevant through all generations.
            During the implementation of the project, a comprehensive interview analysis technology was developed that is relevant to the purpose and main objectives of the study. To answer the question about the possibility of transmission of collective trauma in the third and fourth generation, it was necessary to verify the interviews in terms of the respondents' expressions of emotion and to identify key themes through word form frequency analysis. As a result, by matching emotional outbursts with key themes, it is possible to compare the level of engagement across generations. This technique is much in demand in the field of memory research, as it is necessary to work with verbal sources that need comparisons and verification.
            
                The technology includes three main steps.
                
                    Step one
                    Creating an archaeographic description of the audio recording, including information such as the date the file was created, recording equipment, audio format, file storage location, etc. A transcribed copy of the interview and an archaeographic description of the file are also created. To organize the case, a database was created by MySQL, which simplifies not only data storage but also their import.
                
                
                    Step Two
                    Then the audio file is analyzed by the Praat software. Work with the audio file continues only within the framework of emotional analysis, that is, a timbre analysis of the respondent’s voice is carried out and the result of the analysis is visualized (Goriss, 2020). This stage is carried out under the established methodology of criminal forensics.
                
                
                    Step Three
                    With a transcribed copy of the interview, comprehensive text analysis is performed using the MAXQDA-12 software, including frequency text analysis, concordance analysis, and semantic text analysis. The final stage of the interview analysis is the use of network text analysis to identify key semantic objects of narration in interview groups of victims and next generations (by Gephi 9.02). The strength of communication was determined by the frequency of respondents A and B (victim and representative of the next generation, respectively) mentioning a particular topic included in the thesaurus based on the results of content analysis and expert evaluation. Discourse analysis is used as well (Leontovich; 2011).
                
                
                    Current results
                    The results of this analysis were striking. Firstly, the results helped to identify the main themes that relevance remains high in the narrative of each generation (victims of repression, their children, their grandchildren). Secondly, the most relevant thematic blocks were identified as well: these are problems of starvation, problems of mortality, and problems of transportation. Thirdly, there is a trend of increasing criticism of the Stalinist regime with each new generation; while the victims cry more often during interviews, and still cannot cope with their trauma, their grandchildren often emphasise that their relatives and they were not rehabilitated, since even they had to be born and raised in remote parts of Russia. This is a consequence of deportation that even the grandchildren of victims of Stalinist repression have been affected by it. The substantial conclusion, according to the completed analysis, is that collective trauma in the Russian Germans community is shared by different generations and has been transferred through oral stories from elder family members. And the more emotionally a particular generation reacts to key plots of the narrative the more the generation is involved in the experience and the more share the collective trauma.
                    Results of the project show that the main theme of the victims of repression is hunger (~ 90%), their children are accusing the regime of a crime (~70%), and as for the grandchildren, it is a question of evaluating the actions of the regime as a whole (Iashchenko, 2020).
                    Since the analysis results approve that Russian Germans share collective trauma through four generations, and their community is not represented in public space, the narrative, at the first time, sheds light on the memory of the victims of communism in the camps. Content analysis and Network analysis showcase that the narrative includes a traumatic segment in every generation that mentions the same topics. And no matter where in the world the interview was conducted (Russia, EU, North America, South America), the same narrative of repression and the arrangement of accents in similar places can be traced.
                
            
        
        
            
                
                    Bibliography
                    
                        Burdick, P., Lunenfeld, P., Burdick, A., Drucker, J., Presner, T., Schnapp, J. (2012) Digital_Humanities. Cambridge, Mass.: MIT Press.
                    
                    
                        Chang, J.K. (2019) Ethnic Cleansing and Revisionist Russian and Soviet History. Academic Questions, 32(2), pp. 263–270.
                    
                    
                        Gorris, C., Ricci Maccarini, A., Vanoni, F., Poggioli, M., Vaschetto, R., Garzaro, M., Aluffi Valletti, P. (2020). Acoustic Analysis of Normal Voice Patterns in Italian Adults by Using Praat, Journal of Voice, Volume 34, Issue 6, pp. 961.e9-961.e18.
                    
                    
                        Hirsch, M. (1993) "Family Pictures: Maus, Mourning, and Post-Memory," Discourse: Journal for Theoretical Studies in Media and Culture: Vol. 15: Iss. 2, Article 1. Available at: https://digitalcommons.wayne.edu/discourse/vol15/iss2/1.
                    
                    
                        Iashchenko, I. (2019). Remember How: The Place of Visualization in Preserving the Memory of Repressions of the USSR Against the Volga Germans// Digital Humanities Conference-2019 (9 -12 July 2019, Utrecht)/ Utrecht University Press, 2019. 
                    
                    
                        Leontovich, O. (2011). Methods of communicative research// Леонтович О.А. Методы коммуникативных исследований. М., 2011.
                    
                    
                        Iashchenko, I. (2020). Evidence and Memory. Memories about the repressions of the 1940s against the Volga Germans on the materials of the Perm Region (Master’s thesis). Perm State University, Perm, 2020.
                    
                
            
        
    



        
            This poster comes from the AHRC-funded project 
                Dunham’s Data: Katherine Dunham and Digital Methods for Dance Historical Inquiry. The overarching project explores 
                the kinds of questions and problems that make the analysis and visualization of data meaningful for dance history, through the case study of 20th century African American choreographer Katherine Dunham, who toured the globe extensively, picking up performers and gathering culturally specific movement for her repertory as she went (Bench and Elswit 2020; Bench and Elswit 2022). Drawing on data-informed research in theatre history (Varela 2021; Miller 2016) and feminist and anti-racist approaches to data (D’Ignazio and Klein 2020; Johnson 2018), our manually-curated core project datasets represent Dunham’s everyday itinerary of over 5000 days spent between 1947-1960 on every continent but Antarctica, the almost 200 dancers, drummers, and singers who travelled with her, and the almost 200 interconnected elements of repertory that they performed. These are currently being expanded to 1938-63, covering the majority of Dunham’s stage career, and all of her domestic and international touring.
            
            Reflecting the 2022 conference theme “Responding to Asian Diversity,” we will present digital visualizations based on data collected regarding the time Dunham spent touring in the Asia-Pacific region from 1956-1958, in particular highlighting research findings related to: 1) the sites of performances and other travel to 21 cities across Australia, New Zealand, Philippines, Singapore, Malaya, Hong Kong, Korea, and Japan, 2) the performers who toured with her and those who joined her company en route (including two Australians, a New Zealander, and eight Filipinos), and 3) the existing repertory they brought with them to perform in these locations, as well as new repertory inspired by their time in the region.
            The company’s international touring brought them into contact with a broad range of rhythms, gestures, and referents, which then circulated onward as Dunham toured. During this period, the company began to perform a number based on the Maori haka, which was later combined with Baby San and Planting Rice, pieces that referenced travels to Japan, Korea, and the Philippines, to form Eastern Suite. We employ 
                spatial, network, and computational analyses as they are used in performing arts research (Bollen and Holledge 2011; Balme 2019) to better understand how Dunham’s choreography materializes the influence of the many geographic places that infused her diasporic imagination, and trace the flows of performers working together over time and space as a dynamic collective, and how their embodied knowledge supports the creation and transmission of Dunham’s repertory. The analyses and visualizations displayed on this poster use a combination of Python/Pandas, Matplotlib, Seaborn, Gephi, Leaflet, and NetworkX. Based on Dunham’s program notes for her choreography, we geolocated every repertory work (accurate to the degree Dunham described) and assigned the map a 2D color palette in such a way that repertory associated with locations of inspiration near each other will have similar colors, which we use to represent these both on the map and off as a stacked bar chart by year.  We then seek to understand the multi-directional force of inspiration by connecting timelines of Dunham’s places visited and of repertory inspired by place as a bipartite graph. This is further complicated by joining three datasets to examine the correlations of Dunham’s travel itinerary by means of a temporal punch card, the trajectories of each company member through the company, and the passports they each carried. Together, these 
                analyses make traceable potential ripples of Dunham’s influence in the many locations the company visited, including Dunham’s long term impact in the Australian entertainment landscape (Bollen 2020), and the ways in which her presence is narrated in the development of Japanese Butoh (Michio 2019).
            
            
                Because scholars generally consider Dunham’s artistic and political project to be one of tracing resonances and retentions of Africanist elements in diasporic 
                movement practices
                 throughout the Caribbean and Americas (Clark 1994; Manning 2004; Das 2017), they have not fully accounted for Asia as a site of inspiration for her choreographic work, and how her influence may have extended throughout the area as performers joined and left the company while touring, as well as the impact of her depiction of African-diasporic practices on local audiences. This poster connects with current scholarship on the African diaspora beyond the Black Atlantic (Gilroy 1993) toward various Black internationalisms that “have never been contained within the holy trinity of Europe, Africa, and the Americas” (Patterson and Kelley 2000, 32) and offers an opportunity to illuminate Dunham as part of transnational creative flows between the Asia-Pacific region and the Afro-Caribbean and Americas.
            
        
        
            
                
                    Bibliography
                    
                        Balme, C. (2019). The Globalization of Theatre 1870–1930: The Theatrical Networks of Maurice E. Bandmann
                        . Cambridge: Cambridge University Press.
                    
                    
                        Bench, H. and Elswit, K. (2020). Katherine Dunham’s Global Method and the Embodied Politics of Dance’s Everyday, Theatre Survey, 61(3): 305-30.
                    
                    
                        Bench, H. and Elswit, K. (2022). Visceral Data for Dance Histories: Katherine Dunham’s People, Places, and Pieces, TDR, 66(1): 39-62.
                    
                    
                        Bollen, J. (2020). Touring Variety in the Asia Pacific Region, 1946–1975
                        . London: Palgrave.
                    
                    
                        Bollen, J. and Holledge, J. (2011). Hidden Dramas: Cartographic Revelations in the World of Theatre Studies, The Cartographic Journal, 48(4): 226-36.
                    
                    
                        Caplan, D. (2016). Reassessing Obscurity: The Case for Big Data in Theatre History. Theatre Journal, 68 (4): 555-573.
                    
                    
                        Clark, V.
                         (1994). Performing the Memory of Difference in Afro-Caribbean Dance: Katherine Dunham’s Choreography, 1938-87. In Fabre, G and O’Meally, R. G. (eds), History and Memory in African-American Culture. New York: Oxford University Press. 188-204.
                    
                    
                        Das, J. D.
                         (2017). Katherine Dunham: Dance and the African Diaspora. New York: Oxford University Press.
                    
                    
                        D’Ignazio, C. and Klein, L. F.
                         (2020). Data Feminism. Cambridge, MA: The MIT Press.
                    
                    
                        Johnson, J. M.
                         (2018). Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads, Social Text, 36(4):57–79. 
                    
                    
                        Gilroy, P.
                         (1993). The Black Atlantic: Modernity and Double Consciousness. Cambridge, MA: Harvard University Press.
                    
                    
                        Manning, S. 
                        (2004). Modern Dance, Negro Dance: Race in Motion. Minneapolis: University of Minnesota Press.
                    
                    
                        Michio, A.
                         (2019). From Vodou to Butoh: Hijikata Tatsumi, Katherine Dunham, and the Trans-Pacific Remaking of Blackness. In Baird, B. and Candelario, R. (eds), The Routledge Companion to Butoh Performance. New York: Routledge.
                    
                    
                        Patterson, T. R. and Kelley, R. D. G.
                         (2000). Unfinished Migrations: Reflections on the African Diaspora and the Making of the Modern World, African Studies Review, 43(1): 11-4.
                    
                    
                        Varela, M. E.
                         (2021). Theater as Data: Computational Journeys into Theater Research. Ann Arbor: University of Michigan Press.
                    
                
            
        
    

        
            
                Theoretical background
                Periodization is one of the fundamental topics of literary studies. As Rene Wellek puts it in one of the most notorious and important books of the last century theory of literature: “the concept of period is certainly one of the main instruments of historical knowledge”, meaning, of course, literary-historical knowledge. 
                    (Wellek, 1956: 268). And yet is one of the most controversial and debated:
                
                It is virtually impossible to divide periods according to dates for, as [Jurij] Lotman points out, human culture is a dynamic system. Attempts to locate stages of cultural development within strict temporal boundaries contradict that dynamism. 
                    (Bassnett, 2013: 41)
                
                How it comes that we can hypostatize the dynamic nature of cultural systems, superimposing on them a scalar chronology? How can we say, as Jameson puts it, that Ulysses is something that happened in 1922 
                    (Jameson, 1971: 313)? Following Meneghelli 
                    (Meneghelli, 2013), we can individuate at least 4 critical issues, or even aporias in literary periodization:
                
                
                    Historical categories are related to cultural and social phenomena and overlap and interact in complex ways, determining 
                        anisochronies and 
                        dischronies;
                    
                    Most if not all literary-historical categories have an ontological and trans-historical status and meaning embedded in them (take for instance Romanticism or Modernism);
                    Historical categories interact with and are dependent on geospatial ones, resulting in a multiplicity of asynchronous periodizations;
                    The notion of a historical category in literature is strictly associated with the canonical corpus of texts that are considered representative of a period, and the “dialectics between these two poles, period and canon, are complex and manifold” (Meneghelli, 2013: 3).
                
                This last point is particularly relevant: literary periodization is usually the product of a process of generalization and synthesis, within a historical and social horizon, of the small-scale critical and interpretive practices that characterize the study of literary texts. Being bound to idiosyncratic hermeneutical practices and to the “epistemology of close reading”, periodization suffers from all the pitfalls and limitations of that approach. In the last two decades, the landscape of literary and cultural studies has been enriched by a methodological perspective that is based on a quantitative approach. Among the various disciplinary labels that identify this current in studies, the most common is distant reading, introduced by Franco Moretti in his work on World Literature 
                    (Moretti, 2000) and subsequently extended to denote (even retroactively) the entire tradition of quantitative literary studies 
                    (Moretti, 2013; Underwood, 2019; Piper, 2018; Jockers, 2013). In this framework literary texts are elements of a population whose synchronic and diachronic characteristics should be empirically investigated on a molar scale, adopting statistical-probabilistic and computational methods. This would require the move from 
                    interpretation to 
                    explanation as the primary methodology of scholarly inquiry in the cultural and literary domains 
                    (Ciotti, 2021). 
                
                The possible contribution of a distant reading approach to the literary periodization problem, has been previously explored by some important studies, like 
                    (Jockers, 2013: chap. 6), 
                    (Piper, 2018: chap. 4), 
                    (Underwood, 2019) for English narrative fiction, all based on a supervised classification approach; while 
                    (Jannidis and Lauer, 2014) for German literature adopted a stylometric method. This paper explores the results of a mainly explorative and unsupervised analysis of a corpus of 19
                    th and 20th century Italian narrative fiction.
                
            
            
                The corpus and the methodology
                The main research hypothesis is if adopting computational quantitative methods, it’s possible to identify time-based groupings in a set of texts distributed over a long historical period. Related to this there is the question of whether those eventual groupings are aligned with traditional historical periodization. The corpus consists of 660 Italian novels and short novels written between 1810 and 2000 of different aesthetic "levels", canonization status, genre, dimension, and authors’ gender. Admittedly, this corpus is still far from being adequate and well balanced, mainly due to the uneven chronological distribution, but it is sufficient for an exploratory inquiry.
                I wanted to test if the corpus can be clustered in a chronological sensible way using an algorithmic approach without presuming any prior categorization: this explains the preference for unsupervised methods in this research. In particular, I have adopted two different approaches, based on different assumptions, analytical techniques and features selection:
                
                    bootstrap consensus network, following (Eder, 2017) that applies phylogenetic consensus networks method to MFW based clustering;
                    lexicon-based text analysis and subsequent K-Means clustering of the results.
                
                Fort the first experiment I have adopted the popular stylometric R package 
                    Stylo
                    (Eder et al., 2016) to generate the consensus network dataset, conflating ten hierarchical clusterings generated comparing from 100 to 1000 most frequent words. The resulting output dataset is imported into the network analysis tool Gephi 
                    (Bastian et al., 2009) and each node is associated with an attribute that specifies its decade of composition. Then modularity is calculated with a resolution set at 4, resulting in 10 modules, and the final layout is generated applying the layout algorithm Force Atlas 2.
                
                The second approach is based on the text analysis of the corpus with the tool 
                    Linguistic Inquiry and Word Count (
                    LIWC) 
                    (Pennebaker et al., 2015). For each text, LIWC produces a vector containing the relative frequencies of various words-classes (E.g.: "Affect Words", "Cognitive Processes", "Perceptual Processes", "Biological Processes"....). The final output is a low dimensional document matrix, to which I have applied a K-Means clustering process, adopting the Python implementation of the algorithm in the 
                    SciKit-Learn library 
                    (Pedregosa et al., 2011). The choice of the number of clusters has been done evaluating 20 different models with 
                    Elbow method and 
                    Silhouette Score tests, which both indicated an optimal value of 4 clusters.
                
            
            
                Results and future directions
                The results of the stylometric approach represented in Fig.1 shows that the texts clustered in time sensible way are only those written in the second half of the 20
                    th century (that in the network have a blue color tone), while texts written in the first and second half of the 19
                    th century e in the first half of 20
                    th are not clearly separated, with the exception of the island in the upper right part of the graph, which is mostly composed of canonical Italian modernist texts.
                
                
                    
                    Consensus Network of the corpus: red 1810-1860; green 1860-1900; yellow 1900-1940; blue 1940-2010
                
                This overall result is confirmed by the K-Means approach. For my analysis I have produced two matrices using the LIWC dictionary for Italian 
                    (Agosti and Rellini, 2007):
                
                
                    a matrix with all LIWC lexical categories;
                    a matrix limited to the following word categories: Verbs, Pronouns, orthographic signs of represented speech, and categories related to emotional and cognitive activity.
                
                In this way I can test an additional hypothesis very common in literary-historical and critical scholarship: the linguistic sphere of the cognitive and emotional dimension is a characteristic feature of the evolution of narrative along the nineteenth and twentieth centuries, namely it’s a characteristic of the transition to the Modernism. While the K-Means clustering of the first matrix has very little relation to chronology, the second one provides clear indicators of temporal segmentation (Figure 2). Therefore, we can say that the incidence of the lexicon related to the sphere of thought/consciousness/emotivity is a signal of an evolutional pattern in the Italian novel. Anyway, also in this case the more clearly time-based cluster is that of the text written in the second half of the 20
                    th century.
                
                
                    
                    K-Means clusters in the document matrix restricted to the cognitive/emotional lexicon
                
                In conclusion, the first analysis in general confirms for Italian literature the limited role of the time dimension for clustering texts on a purely stylometric base already observed by 
                    (Jockers, 2013: chap. 6), as compared to authorship and even author gender. Instead, there is some evidence that quantitative empirical analysis partially confirms the relevance of cognitive/emotional lexicon in the evolution of literature. In this direction, I think that a fruitful development of the research will require a more effective way to identify the presence of cognitive/emotional attitudes in texts. To this end, we are training a streamlined Italian BERT language model to identify the relevant textual blocks, and the provisional results are promising.
                
            
        
        
            
                
                    Bibliography
                    
                        Agosti, A. and Rellini, A. (2007). The Italian LIWC dictionary. 
                        Austin, TX: LIWC. Net.
                    
                    
                        Bassnett, S. (2013). 
                        Translation Studies. London: Routledge.
                    
                    
                        Bastian, M., Heymann, S. and Jacomy, M. (2009). Gephi - The Open Graph Viz Platform https://gephi.org/ (accessed 26 November 2018).
                    
                    
                        Ciotti, F. (2021). Distant reading in literary studies: a methodology in quest of theory. 
                        TESTO & SENSO(23).
                    
                    
                        Eder, M. (2017). Visualization in stylometry: Cluster analysis using networks. 
                        Digital Scholarship in the Humanities, 
                        32(1): 50–64 doi:10.1093/llc/fqv061.
                    
                    
                        Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: A Package for Computational Text Analysis. 
                        The R Journal, 
                        8(1): 107–21.
                    
                    
                        Jameson, Fredric. (1971). 
                        Marxism and Form : Twentieth-Century Dialectical Theories of Literature. Princeton (N.J.): Princeton University Press.
                    
                    
                        Jannidis, F. and Lauer, G. (2014). Burrows’s Delta and Its Use in German Literary History. In Erlin, M. and Tatlock, L. (eds), 
                        Distant Readings. Topologies of German Culture in the Long Nineteenth Century. Rochester: Camden House, pp. 29–54 gerhardlauer.de/index.php/download_file/view/335/1/.
                    
                    
                        Jockers, M. L. (2013). 
                        Macroanalysis: Digital Methods and Literary History. (Topics in the Digital Humanities). University of Illinois Press 
                    
                    
                        helli, D. (2013). Periodization, Comparative Literature, and Italian Modernism. 
                        CLCWeb: Comparative Literature and Culture, 
                        15(7) doi:10.7771/1481-4374.2386. https://docs.lib.purdue.edu/clcweb/vol15/iss7/12 (accessed 8 December 2021).
                    
                    
                        Moretti, F. (2000). Conjectures on World Literature. 
                        The New Left Review http://newleftreview.org/A2094.
                    
                    
                        Moretti, F. (2013). 
                        Distant Reading. London: Verso http://www.amazon.de/Distant-Reading-Franco-Moretti/dp/1781680841.
                    
                    
                        Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., et al. (2011). Scikit-learn: Machine Learning in Python. 
                        Journal of Machine Learning Research, 
                        12: 2825–30.
                    
                    
                        Pennebaker, J. W., Boyd, R. L., Jordan, K. and Blackburn, K. (2015). The Development and Psychometric Properties of LIWC2015. University of Texas at Austin doi:10.15781/T29G6Z. http://hdl.handle.net/2152/31333 (accessed 8 December 2021).
                    
                    
                        Piper, A. (2018). 
                        Enumerations: Data and Literary Study. Chicago ; London: The University of Chicago Press.
                    
                    
                        Underwood, T. (2019). 
                        Distant Horizons: Digital Evidence and Literary Change. Chicago: The University of Chicago Press.
                    
                    
                        Wellek, R., Warren, Austin,, (1956). 
                        Theory of Literature. New York: Harcourt, Brace & World.
                    
                
            
        
    

