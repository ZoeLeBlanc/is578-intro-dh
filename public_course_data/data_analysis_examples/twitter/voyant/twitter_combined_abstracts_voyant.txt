
This poster introduces Textal, a text analysis application for iOS, and text analysis service infrastructure, which is currently in development at UCLDH and UCLCASA and will be freely available from Summer 2013. This poster will present findings evaluating the development, launch, and reception of the app, indicating how smartphone technology can increase the potential for public engagement within the Digital Humanities.
Textal (soon to be launched at www.textal.org, currently on twitter at @textal) will be a freely available smartphone application which allows users to create, share, and explore word clouds of a document, website, or tweet stream. Those in visualization and Digital Humanities have tended to sneer at the popular use of word clouds (Harris 2011, Meeks 2012), given we are used to applying robust text analysis tools (such as http://voyant-tools.org/). However, Textal turns word clouds into an intuitive, visually-oriented interface: once a Textal of a chosen text is generated, users can click on words to access underlying statistics, such as frequency and collocates, and so we believe that the pinch, stretch, and click potential in smartphones, along with our judicious design, can fix the elements of word cloud visualization which are currently held to be problematic and act as a bridge between those who have never encountered text analysis techniques, and the more detailed approaches undertaken by researchers in Digital Humanities. All Textal visualizations, including word-clouds, graphs, charts, and word lists, can be shared via social media such as Twitter and Facebook, and the resulting interface word clouds will also be available online. Textal is powered by server-side processing of linguistic data (users can submit any text material they want, by URL, or copy or paste). The resulting sever architecture will also serve as an API for those wishing to carry out on-the-fly generation of text analysis statistics, which can be used in conjunction with other web services.
We envision the Textal iPhone, and iPad, app as a fun text-analysis-in-your-pocket product, which can raise the profile of this technique. We have built Textal with the general audience in mind, to bring Digital Humanities approaches to as wide an international audience as possible (we will be translating the interface into many languages). With an increasing move towards smartphone rather than desktop technologies (Tofel 2012) there is a need to understand how mobile technologies fit within the Digital Humanities remit. We believe we are one of the first teams to build, from scratch, a stand-alone app that brings Digital Humanities techniques to a wider, mobile based, audience. (Previous apps, do exist, such as the DH2012 conference app (https://itunes.apple.com/app/dh2012/id536290090?mt=8), which is an app based version of the conference programme. Geostoryteller is a platform for history walking tours that allow smartphone users to interact with multimedia historical information as they move around a neighbourhood (http://www.geostoryteller.org/index.php, see Rabina and Cocciolo 2012.). Others have used augmented reality viewers for historical and archaeological sites (see http://www.dead-mens-eyes.org/), often built on existing commercial platforms. We don’t believe, however, that others have built smartphone apps that allow the user to do much data analysis or processing in the way we describe).
We are building Textal from the ground up using our own server infrastructure, with the app programmed in house in Objective-C. Textal will be available for iOS only, with a plan to build a stand-alone application for use with Apple laptop and desktops. Depending on reception, we may then build an app for other operating systems. Given that we own the infrastructure, we will be able to view and analyse how, why and when people are using text analysis: we will be tracking use and users, including geo-locating text analysis, to ascertain the potential audience for this type of service and to understand more about the kind of texts people want to analyse, allowing us to undertake a reception study into Textal’s uptake, which will be of great interest to the wider Digital Humanities audience.
Although the app will not be launched until Summer 2013 this is not a promissory abstract: most of the development, including both technical infrastructure, server architecture, and design-work on the app, is now complete and at time of submission we are moving into alpha-testing with a core group of users interested in text analysis. This poster will be an up-to-the-minute account of a very recent development in Digital Humanities: what ramifications do apps hold for Digital Humanities as a discipline or a field of practice? We will report using up-to-date statistics generated from Textal as a case study, and demonstrate Textal at the poster session.
References
Harris, J. (2011). Word Clouds Considered Harmful. Nieman Journalism Lab. http://www.niemanlab.org/2011/10/word-clouds-considered-harmful/ (accessed 13 October 2011.)
Meeks, E. (2012). Using Word Clouds for Topic Modeling Results. Digital Humanities Specialist bloghttps://dhs.stanford.edu/algorithmic-literacy/using-word-clouds-for-topic-modeling-results/. (accessed 15 August 2012).
Rabina, D. L., and A. Cocciolo (2012). Uncovering lost histories through GeoStoryteller: A digital GeoHumanities project. Digital Humanities 2012, Hamburg. http://www.dh2012.uni-hamburg.de/conference/programme/abstracts/uncovering-lost-histories-through-geostoryteller-a-digital-geohumanities-project/
Tofel, K. C. (2012). Uh-Oh, PC. Half of Computing Device Sales are Mobile. Gigaom blog,http://gigaom.com/mobile/uh-oh-pc-half-of-computing-device-sales-are-mobile/ (accessed 16 January 2012.)
1. Introduction and Background
a. History and Context
Social media technologies can extend and enhance scholarly conversation while challenging traditional notions of textual authority and peer review. Twitter facilitates resource and idea sharing with a speed and ease formerly only possible at conferences; Facebook allows the formation of communities of interest founded not on geography but affinity; blogs disseminate research for widespread discussion; and, most significantly, Wikipedia has become the most popular and largest single reference resource in history, with more than 14 million articles in over 250 languages produced by 1 million monthly contributors (Wikimedia Report Card, 2012). This long paper reflects on the construction of a social scholarly edition of the Devonshire Manuscript that attempted to harness emerging social media environments to produce a new type of scholarly edition, one that allows multiple stakeholders to access, contribute, and discuss its construction. [1]

In this paper we recount the incipient formation of a new type of editing community, one that we argue is defined by iterative publication of material, multiple communities of interest contributing to a single project, the use of technology to facilitate these contributions, and the growing importance of self-directed learning to scholarly editing. Our successes and, just as importantly, our moments of failure, offer insight into best practices for a type of “facilitative scholarship” that will likely become increasingly common as comfort with social media technologies grows within the academy. As outlined in a DH2012 poster session (Crompton and Siemens, 2012), we designed the public editing process for the social edition from the start to encourage communication across editorial communities while preserving the peer review process. These communities included the Electronic Textual Cultures Lab team, the project advisory board, the online Iter Community (http://www.itergateway.org/), early modern critics and scholars operating in the blogosphere, Wikibook and Wikipedia users, Tudor enthusiasts, and the general public. [] 2

b. Materials of the Project
The Devonshire Manuscript (British Library Additional MS 17,492) contains approximately 200 items (Southall, 1964: 143, Remley, 1994: 47), including poems, verse fragments, excerpts from longer works, anagrams, jottings, and doodles by a coterie of men and women centered on the court of Queen Anne Boleyn. Inscribed in over a dozen hands, the manuscript has long been valued as a source of Sir Thomas Wyatt’s poetry. In addition to 129 of his poems, the volume contains other transcribed lyrics and original work by numerous court figures, including Mary Shelton, Lady Margaret Douglas, Mary (Howard) Fitzroy, and Lord Thomas Howard (Southall, 1964: 143). These multiple contributors often comment and evaluate each other’s work through marginal notation and in-line interjection. In addition to a consideration of the volume as “a medium of social intercourse” (Love and Marotti, 2002: 63), the multi-layered and multi-authored composition of the Devonshire Manuscript make it an ideal text for experimentation in social editing.

The Social Edition of The Devonshire Manuscript project manifests Ray Siemens’ earlier argument that social media environments might enable new editing practices (Siemens et al., 2012a). In building an edition of an early modern text on the principles of open access and editorial transparency in both production and dissemination, we have integrated scholarly content into environments maintained by the social-editorial communities that have sprung up on the web; most notably, these include the Wikimedia suite of projects (Wikipedia, Wikibooks, Wikisource). We have run an experiment to see how one might build an edition which is scholarly in a traditional sense, but which extends the editorial conversation into multiple pre-existing social media platforms including blogs, wiki discussion pages, dedicated Renaissance and early modern online community spaces, Skype-enabled interviews with our advisory group, and Twitter.

2. The Complexity of the New Scholarly Editing Community
a. Iterative Publication
Perhaps more than any other editorial choice, the iterative publication of the social edition of the Devonshire Manuscript departed most clearly from traditional scholarly editing practices. We have, in effect published (or are in the perpetual process of publishing) two versions of the edition in two mediums: a fixed PDF version, distributed to the project’s advisory board, and a version housed on the publicly-editable Wikibooks. We are currently working with multiple publishing partners to produce a second online edition, an e-reader edition, and a print edition to meet the needs of a broad and varied readership. These versions were planned to productively inform and influence each other’s development, with cross-pollination of editorial input across platforms. Although they did so, each medium also engendered difficulties in communication, coordination, and expectations to be overcome or accommodated—with varying results.

b. Communities of Interest and Technologies of Communication
As outlined above, a central aim of the project was to facilitate knowledge transfer and creation between multiple editorial communities, all of whom were invested differently in the project. These ranged from individual academics giving feedback as advisors to interested members of the public in contact with project staff via Twitter. These groups adopted, considered, and, at times, rejected different types of communication technologies in fascinating ways. Wikibook discussion pages were considered by established academics to be spaces meant for peer review; wiki editors explained that they were in fact where confrontations over edits usually occurred. Wiki editors were very helpful with questions of coding and technical production of content, while other communities felt deeply uncomfortable editing posted content. Sustained discussions in the Iter Community space proved difficult, while members of the public interested in Tudor culture followed our work avidly and often interacted with us on Twitter. Bloggers focused on the early modern period helped to generate discussion and disseminate reports as our edition building progress, but chose to limit their direct involvement with producing the edition. In often surprising ways, the technologies of communication each group used came to define, in some cases, the communities of interest and their respective investments. Considered as a whole, our project suggests that social media technologies can be harnessed for productive interaction and discussion by those scholars invested in a content area or project, but that they require comprehensive oversight by dedicated staff to develop and maintain participation in knowledge construction and dissemination.

c. Self-Directed Learning
Wikimedia content is openly editable by any individual. Project staff quickly reconsidered this theoretically nonexistent barrier to entry, though, when coding of the edition began in Wikibooks. Resembling a cross between HTML, XML, and CSS, Wikitext language is idiosyncratic and required a great deal of time and experimentation on the part of project staff to use effectively. Given the central importance of lab staff to the production of this edition, we have realized that this ad-hoc program of self-directed study produced a new community: young scholars, mostly masters level and younger doctoral students, who have shown interest in digital scholarly production. In other words, those usually construed as “assisting” in large projects here took on increasingly centralized roles in coordinating community input, coding the social edition in Wikibooks, discussing the project with various communities, and writing and disseminating critical research on the project as a whole.

3. Conclusion
a. The Open Source Edition?
The basic structures of the social edition are completely open for manipulation and repurposing. The formation, maintenance, and oversight of multiple communities, however, is central the success of any such open edition. Community investment provides a foundation for a technologically facilitated, process-driven approach. As our full paper will discuss in more detail, developing such communities is often difficult, with success depending on intensive and regular engagement and oversight. It is difficult for disparate communities, even when facilitated by social media technologies, to effectively come together for intellectual production. As even the well-regarded Transcribe Bentham project has widely discussed, crowdsourcing textual transcription—much less scholarly editing and production—is fraught with difficulties we are only beginning to navigate (Cause et al., 2012a; Causer et al., 2012b). In this reconfigured landscape of scholarly production, where we are likely “witnessing the nascent stages of a new ‘social’ edition existing at the intersection of social media and digital editing” (Siemens et al., 2012a: 446), however, we are not without models: the open source community, especially those groups devoted to general tool building and knowledge construction (OpenOffice, Wikimedia, Linux, Mozilla) is a powerful articulation of possible ways the technologically facilitated social production of intellectual content may fruitfully develop—given a robust and vibrant community of interest.

b. Ways Forward
The past two years of work suggests that some blend of intensive oversight and engagement with defined communities, along with a receptivity to spontaneously formed communities of affinity—as supported by both the Transcribe Bentham project (Causer, 2012b) and our own observations—is necessary to effectively implement social scholarly production. Only by becoming effective promoters, facilitators, and instigators can digital humanists provide an effective locus around which multiple communities can cohere. Although we encountered certain difficulties in facilitating knowledge exchange among various communities, on the whole we learned how to effectively facilitate community interaction across and between mediums and communities to produce scholarly knowledge in new ways.

References
Causer, T., J. Tonra, and V. Wallace (2012a). Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham. Literary and Linguistic Computing 27(2). 119-137.
Causer, T., and V. Wallace (2012b). Building a Volunteer Community: Results and Findings from Transcribe Bentham. Digital Humanities Quarterly. 6(1). http://www.digitalhumanities.org/dhq/vol/6/2/000125/000125.html.
Crompton, C., R. Siemens, and the ETCL and INKE Research Groups. (2013) 'Vertues Noble & Excelent’? Digital Collaboration and the Social Edition. Digital Humanities Quarterly. In consideration. [internally circulated for comment and revision]
Crompton, C., and R. Siemens (2012). The Social Edition: Scholarly Editing Across Communities. In DH2012. 16-22 July 2012. Abstract available at http://www.dh2012.uni-hamburg.de/conference/programme/abstracts/the-social-edition-scholarly-editing-across-communities/.
Love, H., and A. F. Marotti (2002). Manuscript Transmission and Circulation. In Loewenstein, D., and J. Mueller. (eds).The Cambridge History of Early Modern English Literature. Cambridge: Cambridge University Press. 55-80.
Remley, P. G. (1994). Mary Shelton and Her Tudor Literary Milieu. Rethinking the Henrician Era: Essays on Early Tudor Texts and Contexts. In Herman, P. C. (ed). Urbana: University of Illinois Press. 40-77.
Siemens, R., C. Warwick, R. Cunningham, T. Dobson, A. Galey, S. Ruecker, S. Schreibman, and the INKE Team (2009). Codex Ultor: Toward a Conceptual and Theoretical Foundation for New Research on Books and Knowledge Environments. Digital Studies/Le champ numérique 1(2). http://www.digitalstudies.org/ojs/index.php/digital_studies/article/view/177/220.
Siemens, R., M. Timney, C. Leitch, C. Koolen, A. Garnett, with the ETCL, INKE, and PKP Research Groups (2012a). Toward modeling the social edition: An approach to understanding the electronic scholarly edition in the context of new and emerging social media. Literary and Linguistic Computing. 27(4). 445-461.
Siemens, R., M. Timney, C. Leitch, C. Koolen, and A. Garnett (2012b). Understanding the Electronic Scholarly Edition in the Context of New and Emerging Social Media: Selected, Annotated Bibliographies. Digital Humanities Quarterly, 6(11). http://www.digitalhumanities.org/dhq/vol/6/1/000111/000111.html.
Southall, R. (1964). The Devonshire Manuscript Collection of Early Tudor Poetry, 1532–41. RES 15, 142-50. WikiMedia Report Card. (2012). http://reportcard.wmflabs.org/.
Notes
1. For an overview of pertinent critical contexts surrounding the modeling of the social edition, see Siemens et al., “Understanding the Electronic Scholarly Edition in the Context of New and Emerging Social Media.”

2. These efforts are in keeping with the aims of the Implementing New Knowledge Environment (INKE) Project, a $2.5 million, 7-year Major Collaborative Research Initiative (MCRI) grant from the Social Sciences and Humanities Research Council (SSHRC) of Canada devoted to “exploring the future of the book from the perspective of its history.” See the INKE website http://inke.ca/ and Siemens et al., “Codex Ultor: Toward a Conceptual and Theoretical Foundation for New Research on Books and Knowledge Environments.”
Though humanities graduates have long engaged in a range of stimulating careers, little data has been collected on humanities scholars working outside the professoriate. Consequently, discussions about alternative academic careers—those within the orbit of universities and cultural heritage institutions, but off the tenure track—have been largely anecdotal. In order to ground the conversation, the Scholarly Communication Institute (SCI) initiated a study in 2012 to investigate perceptions about career preparation provided by graduate programs. The study was a directive from SCI’s ninth summer meeting in 2011, which identified graduate education reform as an area of critical importance to the current humanities landscape.

The main goal of the study, which focused primarily on the context of North American higher education, was to establish a body of data that can serve as a foundation on which to base recommendations for new and revised methodological training. The results of the study reveal clear patterns that highlight the current strengths of graduate education relative to non-professorial employment, as well as significant opportunities for improvement.

The changing nature of career paths for humanities scholars is an issue of particular concern to digital humanities practitioners, who have long been working in the kinds of hybrid roles that the term “alternative academic” has come to describe. Many of the skills implicit in digital humanities scholarship and work products—including collaboration, project management, and technological fluency—are becoming increasingly important in new models of graduate training, even among programs not specifically allied with the digital humanities.

BACKGROUND
While doctoral study is a time of intense focus, it is also deeply exploratory. This exploration traditionally takes shape through the research process, as candidates follow the winding labyrinth of a line of inquiry, its antecedents, and its significance. Universities understand and value freedom of this nature; indeed, the fundamental structure of academic employment—tenure—is built around the importance of protecting the freedom of academic inquiry.

Increasingly, though, students need space for another kind of exploration, one more directly related to their future employment opportunities. The myth of a single (academic) job market persists in graduate programs today, perpetuated by departments that measure prestige by the tenure-track placements of their graduates. However, the convergence of increased casualization of the academic work force, a period of high unemployment, and steady enrollment in graduate programs means that people with advanced humanities training increasingly seek intellectually satisfying positions outside the professoriate. Following the 2011 launch of #Alt-Academy, a collection of essays edited by Bethany Nowviskie, the neologism and Twitter hashtag #alt-ac became a widely used shorthand to describe these kinds of careers, together with the excitement and challenges that accompany them.

In addition to the rich narrative material gathered under the #alt-ac umbrella, several other studies provide groundwork for SCI’s recent work. In particular, the 2012 report by the Council of Graduate Schools and the Educational Testing Service titled “Pathways Out of Graduate School and Into Careers,” provides a valuable look at graduate education and employment in the U.S. across all disciplines. An earlier study, “Ph.D.’s—Ten Years Later” (Nerad and Cerny, 1996), explores the experiences of Ph.D. holders working in business, government, and non-profits. It provides incredibly useful context, but the data from the study no longer accurately reflects the current academic or employment environments.

While both of these studies provide useful baseline information and analysis, the disciplinary scope of each is quite broad, making it difficult to assess finer-grained issues particular to the humanities. By focusing on a narrower segment of the academic population—humanities scholars working outside the tenure track—SCI’s study can probe more deeply into issues that concern that group.

METHODS
The study consisted of two main phases: one public, one confidential. The first phase involved creating a public database of self-identified alternative academic practitioners. The database was built within the framework of the #Alt-Academy project in order to leverage the energy of existing conversations.

The second phase of the study comprised two confidential surveys. The primary survey targeted people with advanced humanities degrees who self-identify as working in alternative academic careers, while a second targeted employers that oversee employees with advanced humanities degrees. Because we were working with a somewhat nebulous population, our subsequent distribution focused on “opt-in” strategies—especially social media, listervs, and traditional media coverage. While this method has limitations, we hoped to learn something not only from the content of the responses, but from the number and type of respondents.

Data collection extended from July to October, 2012. Overall, the surveys had a very strong response, though the response rate also highlighted an important discrepancy. Nearly 800 people completed the main survey—almost four times our initial goal of 200 respondents. The employer survey, however, fell slightly short of our more modest goal of 100 respondents, totaling about 80 responses. The uneven response rate underscores the significant difference in engagement level on the part of job seekers compared to employers.

FINDINGS
Analysis is still in progress; the final report will be completed and published by August 2013, at the end of SCI’s current phase of funding. The preliminary results of the surveys strongly suggest that while humanities graduates can and do apply their knowledge and skills to wide assortment of careers, there are many ways in which graduate programs could better equip them for the paths they take. Further, many of the skills employers report as desirable for alternative academic roles—such as project management, collaboration, and communicating with varied audiences—would also enhance the research, teaching, and service of those who do pursue academic roles.

Unsurprisingly, the data shows that a large majority of students enter graduate school expecting to pursue careers as professors—a total of 74%. What is perhaps more interesting is their level of confidence: of that 74%, 80% report feeling fairly certain or completely certain that this was the career they would pursue. These expectations are not aligned with the actual career outcomes of the respondents, or with humanities graduates more broadly.

Deepening the problem, students report receiving little or no preparation for careers outside the professoriate, even though the need for information about many different careers is acute. Only 18% reported feeling satisfied or very satisfied with the preparation they received for careers other than the professoriate. The responses are rooted in perception, so there may well be resources available that students are not taking advantage of—but whatever the reason, it is clear that students do not feel that they are being adequately prepared.

NEXT STEPS
Through a series of conversations with experts in the coming year, SCI will explore strategies to better equip students for a variety of careers without sacrificing disciplinary rigor. Based on the outcomes of the meetings, SCI plans to draft recommendations encouraging humanities departments to consider evaluating and modifying required aspects of their graduate-level curricula in ways that best serve students and the health of the discipline.

One way to move toward curricular change is to encourage humanities departments to form more deliberate partnerships with the inter- and para-departmental organizations that are already engaging in this kind of work. Traditional and digital humanities centers have jump-started excellent training programs, research projects, and public-facing work, though opportunities frequently take the form of extracurricular fellowships or informal training programs (such as the Digital Humanities Summer Institute and THATCamps, which both provide short-format, non-credit training opportunities). If departments that wish to move in similar directions connect with these centers, there may be opportunities to share infrastructure (physical and digital), expertise, time, and funding.

While informal programs have been a good starting point, incorporating successful training elements into the structures and core curricula of departments is an important move, especially in terms of sustainability and increased access (for all graduate students, not only those who win competitive spots in small programs). When individuals and small centers are supported by robust partnerships with traditional academic departments, the possibility for sustainable change becomes even greater.

CONCLUSION
This study represents an important step in the path of rethinking graduate education and academic employment, and we hope it helps to lay the groundwork for further study and concrete action. By making our data publicly available, we hope that other scholars will deepen the analysis of the responses that we have received. We also hope that an increasing number of departments will accurately track—and publish—data on the career paths of their former students. Increased information and transparency are critical to fostering an academic community that recognizes the value of permeable boundaries. Finally, we hope that the humanities community will strengthen its efforts to engage with the public. If, rather than feeling constrained by the exclusivity of a tenure-track career path, students instead feel free to explore ways to apply their humanities training to a broad spectrum of paths, their work would enrich both the academic community and the broader public.

References
Council of Graduate Schools (2012). Pathways Through Graduate School and Into Careers. 19 April 2012. http://pathwaysreport.org.
Nerad, M., and J. Cerny (1999). From Rumors to Facts: Career Outcomes of English PhDs. ADE bulletin 32.7. 11. (30 July 2012). http://www.mla.org/bulletin_124043.
Nerad, M., and J. Cerny (2012). “Ph.D.’s—Ten Years Later.” (30 July 2012). http://depts.washington.edu/cirgeweb/c/research/phd-career-path-surveys/phds-ten-years-later/.
Nowviskie, Bethany, (ed). (2012). #Alt-Academy. MediaCommons (2011). (1 Oct. 2012). http://mediacommons.futureofthebook.org/alt-ac/.
Scholarly Communication Institute (2011). New-Model Scholarly Communication: Road Map for Change. July 2011. http://uvasci.org/past-institutes/new-model-scholarly-communication/sci-9-report/.
This paper describes an experimental approach to designing and teaching an introductory digital humanities course for graduate students. In 2011 Kevin Kee was asked to create and teach a class as part of a new interdisciplinary Humanities Ph.D. program. The graduate students taking the course would be largely unfamiliar with the digital humanities.

Kee began his preparation for the course by asking several questions. The first was "what should an introductory digital humanities course attempt to accomplish"? As he searched for answers, another important question emerged: "How could digital methods be used to design and deliver the course?" In this paper, Kee and Spencer Roberts, a Research Assistant who worked with Kee, describe first their method for researching and designing the course. They then sketch the structure and content of the course that resulted from their research. Finally, they provide examples of student responses to the material and methods covered in the course (including Roberts's perspective as a graduate student). The collected responses and their reflections on the process suggest particular ways in which future courses of this kind might be designed, implemented, and improved. Most importantly, they found that an effective way to design and teach an introductory digital humanities course is to think about the discipline through discussions about its topics and to think with the discipline by using digital tools and methods in the classroom.

Overviews of digital humanities course offerings have been conducted throughout the past fifteen years. In 1999, Willard McCarty and Matthew Kirschenbaum identified only fourteen institutions that offered courses in humanities computing. In 2006, Melissa Terras conducted another survey of digital humanities curriculum, and in 2011, Lisa Spiro undertook to collect and analyze syllabi from digital humanities courses. Of these previous surveys, Spiro’s was the most comprehensive; she collected over 134 syllabi from various levels of study in the digital humanities. Although Spiro’s work parallels and was helpful to that of Kee and Roberts, the latter were unaware of her project when they began, and had no way to replicate her research method. As a result, Kee and Roberts drew on the results of their own analyses while designing the course.

For Kee and Roberts' research, Roberts designed a method by which syllabi were converted into sets of data representing reading lists, assignments, assessment methods, and digital tools used. Commonly occurring items from within those sets were highlighted and identified as items deemed important by the statistical consensus of instructors represented in their sample. For example, their results showed that seven authors of digital humanities-related articles and books appeared on reading lists at a significantly higher frequency than others; the data also showed that other instructors found these authors most useful. Kee drew on these results when deciding on readings for his course list.

Topics covered in the course included text encoding and markup, distant reading, building, mapping, modelling and simulating, playing and gaming, teaching, and collaborating. Each of these was paired with a practical application, usually drawn from a modified version of William Turkel’s "Method". For example, students learned the theory of text markup and were asked to create pages on the course wiki using the basic wiki standard markup. Franco Moretti’s theory of distant reading made more sense for students once they experimented with text mining and analysis tools such as Voyant. Kee’s assessment strategy required students to use blogs and Twitter to comment on the theories and tools they encountered; Kee also encouraged them to participate in scholarly discourse that occurs on the Web. Although most of the students studied history, Kee aimed to create an environment in which the digital humanities were understood as both theory and practice that could be incorporated into any humanities discipline.

Because Kee and Roberts hoped to learn from the experiences of graduate students new to the digital humanities, Kee built feedback mechanisms into the course assignments, and asked students to reflect on the course before and after completion. Nearly all of the students were challenged by the dual responsibility of learning theory and skills simultaneously. Although some students were relieved to finish experimenting, others were pleased with their progress and the opportunities for future research. One student commented, “Not only do I now have some new tools to use while I’m doing research… I’m also more open-minded towards using them in the first place and really trying to engage with them, rather than brushing them off.” While students readily adopted some of the tools, such as Zotero and Evernote, they found more complex tools such as DevonThink or Voyant required a level of commitment and time they did not want to make. In short, these students were not willing to commit to a new, digital research method at a time when they were simultaneously taking graduate courses rooted in conventional research methods. For some students, however, patience led to late or accidental discoveries that improved their methods; in at least one case, a student who was skeptical throughout the course became an enthusiastic supporter of digital methods and now avidly attends DH conferences and events. At the conclusion of the course, most students were open to the various theories and approaches used in the digital humanities, and were enthusiastic about trying new tools and experimenting with new methods that might improve their research and scholarship.

From the outset of the project, Kee and Roberts understood that they were asking questions for which there were several feasible answers. Some graduate level digital humanities courses focus on topics within the digital humanities; others primarily train students to develop digital skills using computational tools. Kee's approach was to combine these two approaches into one course that provided opportunity for theoretical discussion while also showcasing practical applications, so that students could see the potential beneDits of digital humanities methods without having to master sophisticated tools. The research method used to build the course syllabus employed the same theories and tools that were later discussed in the course, creating an iterative loop through which student feedback and developments in the discipline can be incorporated into future versions. Already there are new tools to improve the collection and analysis of digital humanities syllabi, and new methods being explored by instructors. Through the experimental approach described in this paper, Kee and Roberts have found that thinking about and thinking with the discipline, a method that many digital scholars employ in their research, is also an effective way to design a course, and appeals to students who are new to the discipline, fostering enthusiasm for its use in their own often conventional humanities scholarship. The authors hope that this approach contributes to the growing conversation about teaching digital humanities, while also reflecting and adapting to the dynamic topics within the field.
Introduction
Research libraries and cultural heritage institutions must ensure that staff skills and core competencies keep pace with a rapidly changing research environment if they are to continue to effectively support and engage with scholars [1] . The American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities & Social Sciences (ACLS 2006) observes that: In recent practice, “digital scholarship” has meant several related things:

a) Building a digital collection of information for further study and analysis
b) Creating appropriate tools for collection-building
c) Creating appropriate tools for the analysis and study of collections
d) Using digital collections and analytical tools to generate new intellectual products
e) Creating authoring tools for these new intellectual products, either in traditional forms or in digital form
The British Library is realigning its services and structure and in 2010 the Digital Scholarship department was established with a remit to ensure the Library develops its strategy and service propositions to enable and support these digital scholarship activities. The Digital Curator team was created within it to build the staff capacity to deliver on this strategy and ensure that the entire collections workforce is fully versed in the opportunities that digital content and new technologies can offer. This paper discusses the design and implementation of our in-house Digital Scholarship Training Programme at British Library.

Training Objectives
An important first step in shaping our training initiative was the establishment of four clear objectives for what we hoped to achieve for colleagues, and by extension Library users.

1. Staff across all collection areas are familiar and conversant with the foundational concepts, methods and tools of digital scholarship.
Outside the purview of digital scholarship were courses in basic computer literacy: this training was already available to staff through Human Resources.

2. Staff are empowered to innovate.
Seb Chan(Cooper-Hewitt Museum) and Rob Stein(Dallas Museum of Art) stress that innovation can come from anywhere within an organisation and institutions should be careful to avoid erecting unintentional barriers by allotting space and resources too selectively [2] . Our programme would underscore that staff across the Library have the power to innovate, and would provide the support to do so.

3. Our internal capacity for training and skill-sharing in digital scholarship are a shared responsibility across the Library.
Within the organisation, there are areas of world-class expertise in digital content, research, and scholarship. The programme must leverage and amplify this by working with these staff to develop and deliver course modules.

4. Collaborative digital initiatives flourish across subject areas within the Library as well as externally.
The training programme would open up direct communication between colleagues across subject areas as well as digital scholars, ensuring opportunities for collaboration and improvements on service arise.

Design & Development
In April 2012 the Digital Curator team embarked on an intensive three-month survey of the current digital scholarship landscape.

Having conducted a literature survey, the team sought out scholars working at the intersection of computing and scholarship and joined them for informal chats about their research [3] . Perhaps inevitably, we were frequently drawn to activities within the field of Digital Humanities, its very existence the embodiment of trends towards more digital scholarly practice in academia [4] . We consulted the proceedings of major conferences across Europe such as Digital Humanities 2012 in Hamburg and the Digital Humanities Congress 2012 at University of Sheffield and surveyed the skills which academics were acquiring by attending pertinent training courses [5] and reviewing open syllabi [6] and course materials. [7]

By August 2012 the team had outlined the specific concepts, methods and tools which were of direct relevance to library staff. We initially considered taking an advisory approach whereby we would point staff to externally available training opportunities in the areas we had outlined, but found this would not suffice in meeting our objectives; existing courses were by-and-large written for academics or the private sector and the cost of sending a preponderance of staff on them was prohibitive.

This informed our decision to design and deliver our own curriculum in-house and we subsequently drafted individual briefs and learning outcomes for what would become our core offering of 15 one-day courses. Each of the three Digital Curators took responsibility for managing five of the courses and worked with our internal advisory board and instructors from within the Library and institutions on the leading edge of digital scholarship such as King’s College London, Open University, University College London and University of Oxford to finalise the courses.

Instructors were asked to consider the following when preparing course materials:

Content should be aimed at “intelligent novices”, that is, staff who have heard about the concepts but haven’t had the time, space or opportunity to explore them in any depth.
Focus on the wider concepts, methods and processes which tools enable rather than teaching to the tools.
Include a hands-on practical element wherever possible, preferably using British Library digital content.
Deliver from the library practitioner perspective and highlight the Library’s current work, or potential for such work. It is crucial that staff clearly connect the relevancy of this new knowledge to their role at the Library.
Deliver a one-day workshop onsite rather than online. Courses would not be held online as that could unnecessarily alienate an audience with varied technical skills. A full-day commitment would also provide necessary time and mental space away from business-as-usual activities while underscoring this development is a priority.
The Curriculum
We launched the two-year programme officially in November 2012 and the first of four planned semesters ran through the end of March 2013 with these fifteen courses:

101 What is Digital Scholarship?
102 Digital Collections at British Library
103 Digitisation at British Library
104 Communicating our collections online: Copyright considerations and Opportunities
105 Crowdsourcing in Libraries, Museums and Cultural Heritage Institutions
106 Text Encoding Initiative
107 Data Visualisation for Analysis in Scholarly Research
108 Geo-referencing and Digital Mapping
109 Information Integration: Mash-ups, API’s and Linked Data
110 Social Media: Introduction to the Library’s Social Media Policy, Twitter and Blogging
111 Working collaboratively: Using the British Library Wiki, Yammer and Google Drive
112 Presentation skills: From Powerpoint to Prezi
113 Foundations in working with Digital Objects: From Images to A/V
114 Behind the Screen: Basics of the Web HTML, CSS, XML
115 Metadata for Electronic Resources: Dublin Core, METS, MODS, XML
The content of each was carefully designed to specifically suit the Library’s point-of-view. For example, the course ‘Information Integration: Mash-ups, API’s and Linked Data’ provided a broad overview of the terms [8] , but also stressed to staff the immediate potential for these technologies in connecting our digital content with external sources. A practical hands-on exercise [9] walked them through accessing our own British National Bibliography API as a digital scholar might and highlighted its potential as a rich resource for answering complex research questions. The exercise also showcased how our data formats helped or hindered such queries, providing a useful perspective for staff who may create API’s in future.
Early Progress & Lessons Learned
A total of 86 staff members took part in the first semester, attending an average of 2.7 courses. Nine courses were led by external instructors, while the remaining six were taught by British Library staff. Feedback was captured via free text evaluation forms collected at the end of each course which have given us some good indications about what has worked and what needs addressing in the next semester.

When asked what they enjoyed most on any given course, staff consistently noted they valued time to freely brainstorm ideas with colleagues.
The inclusion of practical hands-on activities alongside lectures was also highly valued. Highly structured exercises with clear step-by-step directions were favoured over unstructured time devoted to free exploration of tools.
We set course capacities too ambitiously which made them a challenge to deliver. We reduced capacities from 30 to 15 for courses with hands-on exercises.
Courses which are tool-based, for example 110 Social Media, will be broken into discreet modules so participants need only attend the sections they require.
Curators were given first priority on all courses initially as we had considered them our target audience. We changed this policy shortly after launch and opened courses to all interested staff as there was little justification for maintaining a waiting list in light of such positive demand.
Initial take-up has benefited from a core of early adopters and new hires. As this demographic complete the courses, we will need to be more strategic and creative in marketing the programme to those less inclined.
How we capture information now will be crucial for gauging impact over the long-term. Several attendees have alerted us to project ideas they intend to take forward and we must ensure a mechanism is in place to log and monitor such activities [10] .
This short paper reports on this model and our experience with the hope that it may be useful for similar institutions.
References
The American Council on Learned Societies (ACLS) (2006). Our Cultural Commonwealth: The report of the American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities and Social Sciences. New York: ACLS. http://www.acls.org/cyberinfrastructure/(accessed 07 March 2013).
Digital.Humanities@Oxford Summer School (2012) Full Programme. http://digital.humanities.ox.ac.uk/dhoxss/2012/fullprogramme.html (accessed 07 March 2013)
Digital Humanities 2012 Hamburg (2012). Programme. http://www.dh2012.uni-hamburg.de/conference/programme/ (accessed 07 March 2013)
Digital Humanities Congress (2012). Programme and Presentations. http://www.sheffield.ac.uk/hri/dhc2012 (accessed 07 March 2013)
Edwards, C. (2013). DH Syllabi. The CUNY Digital Humanities Resource Guide blog, http://commons.gc.cuny.edu/wiki/index.php/DH_Syllabi(accessed 07 March 2013).
Research Information Network (2011). Social media: A guide for researchers. http://www.rin.ac.uk/system/files/attachments/social_media_guide_for_screen_0.pdf (accessed 07 March 2013)
Ridge, M. (2012). War, Plague and Fire’ and ‘Bootstrapping Innovation in Museums’ at ‘Museum Ideas 2012-Museums in the Era of Participatory Culture. Open Objects blog, November 03, 2012. http://openobjects.blogspot.co.uk/2012/11/war-plague-and-fire-and-bootstrapping.html (accessed 07 March 2013).
Robichaud, A., and C. Blevins (2011). Tooling up for Digital Humanities. http://toolingup.stanford.edu/ (accessed 07 March 2013)
Schreibman, S., R. Siemens, and J. Unsworth (eds.) (2004). A Companion to Digital Humanities. Oxford: Blackwell. http://www.digitalhumanities.org/companion/(accessed 07 March 2013).
Schreibman, S., and R. Siemens (eds.) (2007). A Companion to Digital Studies. Oxford: Blackwell. http://www.digitalhumanities.org/companionDLS/ (accessed 07 March 2013).
Weller, M. (2009). The Digital Scholar: How Technology is Changing Academic Practice. London: Bloomsbury Academic .doi: http://dx.doi.org/10.5040/9781849666275 (accessed 07 March 2013).
Notes
See also Martin Weller’s The Digital Scholar: How Technology is Changing Academic Practice http://dx.doi.org/10.5040/9781849666275
From their talk Bootstrapping Innovation in Museums at Museum Ideas 2012 http://openobjects.blogspot.co.uk/2012/11/war-plague-and-fire-and-bootstrapping.html
Several groups meet regularly in the immediate vicinity of British Library such as Decoding Digital Humanities London (DDHL) https://sites.google.com/site/ddhlondon/ and the Bloomsbury Digital Humanities Group.
Texts such as A Companion to Digital Humanities (2004) http://www.digitalhumanities.org/companion/ and A Companion to Digital Studies (2007) http://www.digitalhumanities.org/companionDLS/ were highly influential in the early formation of the department as well as the initial framing of the training offering.
Digital.Humanities@Oxford Summer School http://digital.humanities.ox.ac.uk/dhoxss/
A brief collection of DH-related syllabi has been helpfully collate here: http://commons.gc.cuny.edu/wiki/index.php/DH_Syllabi
Tooling Up for Digital Humanities at Stanford http://toolingup.stanford.edu/ , David Birnbaum’s http://dh.obdurodon.org/, and Research Information Network’s Social media: A guide for researchers http://www.rin.ac.uk/our-work/communicating-and-disseminating-research/social-media-guide-researchers are notable examples.
As one attendee remarked, “Great to have something often referred to demystified!”
See the full exercise here: http://www.meanboyfriend.com/overdue_ideas/2013/02/introduction-to-apis/
Phil Hatfield, Curator of Canadian & Caribbean Studies, attended several of the courses and is now formally taking forward a project to visualise a portion of his collection: “We have a large collection of Canadian photographs and associated data at the Library and I’d been considering for some time now ways in which to work with them beyond simply hosting them in a typical image gallery. The course on Data Visualisation gave me the space to play around with some of my ideas for visualisations and pointed me in the direction of free tools out there such as Google Fusion Tables. I hadn’t realised it was so easy to get started and was able to see the shape of the collection almost immediately.”
Collective Biographies of Women (CBW) is a collaborative [1] , open-access [2] literary and prosopographical project focusing on published collections of biographies of women [3] . The literary focus of this interdisciplinary project (which also centers in women's history in Britain and the U.S.) concentrates on a popular genre that has received little critical attention. We study the narrative structure of short biographies of diverse women’s lives delineated through interpretive analysis captured in a stand-aside XML mark-up using the BESS schema [4] . Having presented the theoretical and literary aspects in multiple venues [5] , in this short paper we will present emerging prosopographical interpretations of a database of over 1200 volumes, comprising more than 13,000 biographies of more than 8000 women. The women in this corpus come from all walks of life (not simply one occupation or nation, such as the women writers considered by Orlando or Brown's Women Writers Project [6] ), and range from ancient and biblical figures to living contemporaries of the biographers.

In our proposed paper we introduce the phrase, “documentary social network,” as a label for our prosopographical interpretations because we are interested in social networks as discovered in biographical documents [7] . Yet these networks do not always resemble those that can be found in an investigation of Twitter feeds or Facebook “friends” or even of archival materials such as provided by SNAC [8] . The social networks evidenced through our collection of collections are those of documentary grouping and reference. In specialized collections, Christian missionaries in Africa or nurses in World War I may have interacted in historical time and place, but assorted tables of contents commonly link some individuals who never actually shared a "live" event or a relationship or communication. Thus, the relationships are as perceived and presented through printed volumes collecting short versions of biographies.

Let us consider one Mrs. John Livingstone. She appears in only three CBW collections and by way of those collections her immediate documentary social network has 37 other women [9] . To visualize this network, Figure 1 is centered on Mrs. Livingstone with the three collections positioned on the innermost circle and the other women of those collections around the second circle. Of her 37 collection “siblings,” nine also appear in these same three collections and nowhere else in CBW, confirming some consensus on the names that represent "Notable Women of the Scottish Reformation," a specific historical episode in one location. Other members of Livingstone's documentary “siblings” range into other collections and in Figure 2 those additional collections are positioned on the third circle. In more eclectic lists, some of the Scottish heroines of religious conflict intersect with a multi-national set of women widely recognized today, including writers such as Harriet Beecher Stowe and heroines of war such as Joan of Arc or the Countess of Montfort. Mrs. John Livingstone’s 267 documentary “cousins” are displayed on the fourth circle in Figure 3 [10] .

CBW's first experiments in digital analysis of narrative structure using the BESS schema have focused on two sample archives that are also productive for this paper's claims about documentary social networks. (Each sample archive is a set of all female collective biographies that include a certain woman, as in Fig. 1.) The two sample archives are "Noble Workers," 20 books that include a biography of Sister Dora, who ran hospitals in the industrial Midlands, and "Women of the World," 14 books that include a biography of Lola Montez, a celebrity in Europe, New York, California, and Australia. These Victorian women never met and never appear in the same book. The mediated interconnections between Sister Dora and Lola Montez appear in Figure 4. Only two women, Marie Antoinette (presented as a victimized queen) and Jenny Lind (the celebrated, virtuous singer), appear with Sister Dora in one collection and with Lola Montez in another (i.e., within one degree of separation of both Dora and Lola). The multiple versions of the lives of 141 women who "network" with Sister Dora in Noble Workers collections, compared to the versions of the 133 persons linked to Lola Montez in Women of the World books, will demonstrate the utility of our approach to analyze historical social networks and nonfiction narratives of many kinds. In spite of the vast differences between two Victorian women — one a saintly nurse, the other a notorious courtesan and performer — we discover patterns among their associates and their proximity in the CBW documentary social network.

These prosopographical networks reveal unexpected affiliations among different types of women, life stories, and collections; with the CBW tools for searching and visualization, we are now able to calibrate not only frequencies and proximities of persons and publications over time, but gradations of rhetorical assessment of women's roles and deeds, according to the perspectives of these publications [11] . Such multivalent interconnections not only cross categories but also historical periods in significant ways. An example is the connection between Sister Dora and Anne Boleyn, shown in Figure 5. This diagram reveals that the middle-class, now-forgotten Englishwoman, Sister Dora, appears with her contemporary queen, Victoria (in seven books), and in two collections with the martyred queen, Jane Grey (one of the latter, a186 [12] , is a book that Victoria also shares), but not with Henry VIII's second wife [13] . At sufficient scale, and integrated with other elements in our analysis, data on frequency or "distance" in networks can lead to productive interpretations. Anne Boleyn, unlike good Lady Jane in ambition and sexual notoriety , but like her in being executed because of the politics of English monarchy, features in 24 collections, including 11 dedicated to English queens and three focused on the English Reformation; the latter three collections include Lady Jane Grey but not one of the obscure Scotswomen of Mrs. Livingstone's ilk. To put it simply, Boleyn serves English and Protestant history, but not the promotion of feminine virtue, heroism, or social service.

More complex documentary interrelationships can be seen in Figure 6, associating the Gothic novelist Ann Radcliffe (dead before Sister Dora was born) with Sister Dora through a range of widely respected eighteenth-and nineteenth-century women writers; these lists also commend Jeanne d'Albret, the French queen regnant who championed the Calvinist Huguenots, as well as Lady Russell, Mme. Roland, and Lady Jane Grey, represented as good, highly educated women who played indelible parts in the history of their countries in revolutionary times.

Our proposed paper will present the methods and implications of prosopographical analyses afforded by the CBW documentary social network. Our demonstration of documentary social networks has implications for any "personography," as well as for historical studies of women and studies of biographical narrative. We argue, in this paper, that digital exploration of the persons, narratives, collections, and documentary social networks in the CBW genre opens a prolific field of ramifying versions of female biography impossible to decipher through an approach to individuals or single full-length biography, and invisible through the customary lenses of historiography, time and period, place and nationality.


Figure 1:
Mrs. John Livingstone and her 37 documentary siblings.


Figure 2:
The 16 collections containing Mrs. John Livingstone and her documentary siblings.


Figure 3:
Mrs. John Livingstone and her 267 documentary cousins.


Figure 4:
Connections between Sister Dora and Lola Montez.


Figure 5:
Connections between Sister Dora and Anne Boleyn.


Figure 6:
Connections between Sister Dora and Ann Radcliffe.

Notes
The collaboration is among the UVa English Department (Booth and numerous graduate students) and the Institute for Advanced Technology in the Humanities (Martin, Pitti, Ross, Girard, Brandon, and Bingler), building on earlier work with the UVa Library’s Scholars’ Lab (Gilbert and others).
To access the complete bibliography and “featured subjects” pages go to http://womensbios.lib.virginia.edu/ and to access a prototype interface to the database go to http://cbw.iath.virginia.edu/cbw_db/ .
The volumes are English-language collections published primarily between 1830 and 1940 (though publications of earlier and later dates are included), each containing biographies of from three to 150 women.
Biographical Elements and Structure Schema (BESS) is designed to reveal narrative structure and other elements in the short biographies, for measurable comparison of versions of one woman's life over many collections, and for analysis of the forms within a single collection or category of collections.
These include invited keynote lectures, e.g. Feminist Narrative Theories (a Project Narrative symposium, Ohio State), Life-Writing (a conference at Huntington Library), and British Women Writers Conference, as well as papers at North American Victorian Studies Association, Narrative, and Modern Language Association conferences (2009-2012).
See Susan Brown, Clements, Grundy, et al. Orlando: Women’s Writing in the British Isles from the Beginnings to the Present. Cambridge: Cambridge University Press, 2006. http://orlando.cambridge.org/, University of Alberta www.arts.ualberta.ca/~orlando; and Julia Flanders, Encoding Names for Contextual Exploration in Digital Thematic Research Collections. NEH ODH Level II Final Report, 30 April 2010. http://www.wwp.brown.edu.
For innovative digital work with literary biographical networks, see Alan Liu, Research Oriented Social Environment (RoSE) http://rose.english.ucsb.edu/. Unlike CBW, most digital prosopography projects reconstruct data about groups of lives in eras that predate printed documentation. See John Bradley and Harold Short, “Using Formal Structures to Create Complex Relationships: The Prosopography of the Byzantine Empire—A Case Study,” in K. S. B. Keats-Rohan (ed.), Resourcing Sources Prosopographica et Genealogica, vol. 7., Oxford, 2002. Susan Brown, Alan Liu, John Bradley and others have committed to participate in a proposed NEH Level-I Startup Grant workshop to be held at University of Virginia in 2013.
The Social Networks and Archival Context Project, see: http://socialarchive.iath.virginia.edu/
Janet Fleming Livingstone or Mrs. John Livingstone surfaces in a cohort of late-seventeenth-century Scottish Protestant Dissenters; many are wives of Presbyterian ministers or widows of martyrs or exiles. The relatively coherent group includes noblewomen, but most are historically obscure (life dates or first names unknown). Two collections have versions of the same list of persons: a029, Anderson, Rev. James [of Edinburgh], The Ladies of the Covenant: Memoirs of Distinguished Scottish Female Characters, Embracing the Period of the Covenant and the Persecution (London: Blackie, 1850), with reprints through 1880; a157, Chapman, William, Notable Women of the Covenant: Their Lives and Times (London: Swan Sonnenschein, 1883). The third book including Mrs. Livingstone is a068, Beaton, Rev. Donald, Scottish Heroines of the Faith: Being Brief Sketches of Noble Women of the Reformation and Covenant Times (London and Glasgow: Catt; Adshead, 1909).
One near relation (two degrees of separation) of Livingstone is Sister Dora, who illustrates the exponential possibilities: she appears in 20 collections with 141 “siblings” and then a prodigious 3560 “cousins.”
One near relation (two degrees of separation) of Livingstone is Sister Dora, who illustrates the exponential possibilities: she appears in 20 collections with 141 “siblings” and then a prodigious 3560 “cousins.”
This is the identifying “key” for this collection in the CBW bibliography for the genre.
Queen Victoria, who boasts 60 biographical chapters in the CBW books, shares 10 books with Lady Jane Grey, but only five with Anne Boleyn. Lady Jane's 48 chapters include 12 in books that also represent Anne Boleyn.
In this paper, we introduce the concept of dyadic pulsations as a measure of sustainability in online discussion groups. Dyadic pulsations correspond to new communication exchanges occurring between two participants in a discussion group. A group that continuously integrates new participants in the on-going conversation is characterized by a steady dyadic pulsation rhythm. On the contrary, groups that either pursue close conversation or unilateral communication have no or very little dyadic pulsations. We show on two examples taken from Usenet discussion groups, that dyadic pulsations permit to anticipate future bursts in response delay time which are signs of group discussion collapses. We discuss ways of making this measure resilient to spam and other common algorithmic production that pollutes real discussions. Can a discussion group be characterized by looking solely at the interaction patterns of its participants? Symmetrically, can the patterns of interaction of a given participant identify its role in a discussion group? Can we predict from these patterns the evolution of the interaction inside a group, spotting for instance the early signs of a group decomposition process? Our research aims to establish a new mathematical approach to distinguish between different types of discussion group participants as well as between different types of discussion groups, both with their typical ways of interacting (interaction "signatures") and life cycles.

The analysis of interaction dynamics has recently received an increased focus of attention in the network science community. Mathematical methods (Newman, Barabási and Watts 2006) have been used to identify signatures characterizing the mode of exchanges of famous scholars, comparing for instance patterns in the correspondence of Charles Darwin and Albert Einstein (Oliveira and Barabási 2005). Fluctuation patterns and delays in letter responses indicate prioritization strategies that can be modeled and simulated. These methods permit also to draw comparisons with modern forms of electronic exchanges where similar patterns, corresponding to universal scaling laws (Barabási 2005; Bunde, Eichner, Havlin and Kantelhardt 2004), can be found. Interestingly, all these analyses can be conducted without considering the semantic or pragmatic nature of the exchanges.

Patterns in correspondences networks are of great interest for research in Digital Humanities. For instance, the Stanford’s Republic of Letters project (http://republicofletters.stanford.edu) use “big data” and “distant reading” approaches to offer new visualization tools and test various hypotheses about the Enlightenment. However, mathematical analysis of such networks is not yet common in the Digital Humanities community.

Drawing on research on social networks (Wasserman and Faust 2009 [1994]) and computer mediated communication (Smith and Kollock 1999; Turner, Smith, Fisher and Welser 2005; Welser, Gleave, Fisher and Smith 2007), we analyze the activity and correspondence pattern of participants in Usenet newsgroups about religion and spirituality. Our paper reports an on-going analysis of large data set that consists of more than 1.5 million unique Usenet messages. Usenet is one of the Internet's oldest discussion systems still in widespread use. Unlike more recent platforms like Facebook and Twitter, Usenet hasn't stored its messages in a single central location and offers due to its open nature easier access to its data. Furthermore, Usenet's threaded conversations are organized by topics, with the advantage of allowing comparisons between topics.

In this paper, we introduce the concept of dyadic pulsation (DP) as a complementary measure to reply time for measuring the vitality of a given group. In our representation, a pulsation corresponds to the creation of new communication dyads, i.e. the first direct communication between two users A and B. When B first replies to A, a pulsation of type A (for asymmetric) is emitted. When A replies again to a message of B, a second pulsation of type M (for mutual) is produced. A group that continuously integrates new members in the on-going conversation is characterized by a steady dyadic pulsation rhythm, mixing type A and type M pulsations. On the contrary, groups that either pursue close conversation or unilateral communication (e.g. news feeds, announces without discussion) have no or very little dyadic pulsations.

Our working hypothesis is that evolutions in the pulsation rhythms are earlier predictors of the evolution of group dynamics. Figure A shows an example of a group maintaining a good average response time for a long period followed by an apparently unanticipated explosion of response time. At some point group members simply stop to answer timely to the messages of the discussion group. Interestingly, although no anticipated sign of this evolution could be spotted in the delay time graph, the pulsation graphs shows a progressive reduction of the frequency on the creation of new communication dyads. The grey lines in the lower bar of show pulsations of type A while the black lines show pulsations of type M. Figure B shows the change in delay time after the appearance of a succession of type A and type M pulsations, indicators for the formation of new links between discussion participants.


Figure A and Figure B:

For dyadic pulsations to be a reliable predictor, it is mandatory that they are resistant to the various forms of spam common to digital communication. This is one reason for the distinction of the two types of dyadic pulsations. Indeed, dyadic pulsations of type A can result of spam bots posting messages to a group. However, it is very unlikely that any real user answers to those messages. Thus, the presence of type M pulsations guarantees that new correspondence partners entered the group studied.

We are conducting a larger study to test the relevance of this measure in the particular case of different online discussion groups related to topics about religion and spiritualty. Our hope is to validate the hypothesis that dyadic pulsations on a group level permit to distinguish between different modes in a discussion group's life cycle.

More generally, we believe that this measure can be relevant to characterize the rises and declines of activity in correspondence networks, including literary correspondence networks.

References
Barabási, A.-L. (2005). The origin of bursts and heavy tails in human dynamics. Nature, 435(7039), 207–211. doi:10.1038/nature03459.
Bunde, A., J. F. Eichner, S. Havlin, and J. W. Kantelhardt (2004). Return intervals of rare events in records with long-term persistence. Physica A 342. 308–314.
Newman, M., A.-L. Barabási, and D. J.Watts (eds.). (2006). The Structure and Dynamics of Networks. Princeton Studies in Complexity. Princeton: Princeton University Press.
Oliveira, J. G., and A.-L. Barabási (2005). Human dynamics: Darwin and Einstein correspondence patterns. Nature. 437.7063. 1251–1251. doi:10.1038/4371251a.
Smith, M. A., and P. Kollock (eds.). (1999). Communities in Cyberspace. London: Routledge.
Turner, T. C., M. A. Smith, D. Fisher, and H. T. Welser (2005). Picturing Usenet: Mapping Computer-Mediated Collective Action. Journal of Computer-Mediated Communication. 10.4. doi:10.1111/j.1083-6101.2005.tb00270.x
Wasserman, S., and K. Faust (2009). Social Network Analysis: Methods and Applications. Cambridge: Cambridge University Press.
Welser, H. T., E. Gleave, D. Fisher, and M. Smith (2007). Visualizing the Signatures of Social Roles in Online Discussion Groups. Journal of Social Structure. 8. http://www.cmu.edu/joss/content/articles/volume8/Welser/
Socratic and Platonic Political Philosophy: Practicing the Politics of Reading (forthcoming Cambridge University Press) is an enhanced digital book that attempts to use digital media technology to cultivate the political practice of collaborative reading for which it argues.

The book’s central argument is that there is an analogy between the ways Socrates practices politics with those he encounters in the dialogues and the ways Platonic writing turns us as readers toward ideals of speaking and acting capable of transforming our lives and the community in which we live. For Socrates, politics involves speaking words to individuals that will require them to turn their attention to questions of justice, beauty and the good, ideals that are at once alluring and yet always also elusive. For Plato, politics involves writing words to readers that will require us to do the same. The book traces the practices of Socratic political speaking and Platonic political writing through five dialogues: Protagoras, Gorgias, Phaedo, Apology and Phaedrus.

This short paper will begin with a very brief overview of the basic philosophical argument of the book and the prior Digital Humanities scholarship from which it emerges. We will then turn to the technologies the enhanced digital book will deploy to further cultivate a community of readers capable of performing the politics of reading and writing for which the book itself argues. In the final third of the paper, deeper philosophical questions will be raised about the nature and limitations of authorial authority and about the connection between the book’s design and the ideals of reading and writing for which it advocates.

The book begins with what I call an “Overture” designed to open a space into which the reader is invited to enter. This space is intended to extend beyond the physical book into an online digital community of dialogue both with me as the author of a reading of Plato, and also with others interested in the possibilities raised by the book. This online community already exists in the ecosystem of online digital spaces I have created over the past five years (via Twitter, Facebook, Google Plus, Flickr, YouTube, and my system of blogs called the Long Road) [1] . During this time I have sought not simply to push information to others about my scholarly work, but rather to engage in substantive digital dialogue with a wider community about issues in which we share an interest.

So a community of scholarly and educated readers has already been cultivated in the course of the writing of the book itself, because the research for it was facilitated by an ongoing public dialogue with colleagues who joined me in discussion on my podcast, the Digital Dialogue [2] . There are currently 57 episodes of the podcast [3] , eleven of which are explicitly referenced in the book itself. In producing these podcasts, I invited scholars to join me to talk about their work and in the course of our discussion, my work was enriched and I came away with new or deepened perspectives.

The publication of the enhanced digital book is designed to further cultivate and enlarge the influence of this community of readers. A book that argues for reading as a collaborative endeavor should be published in a way that performs and enables collaborative reading.

To do this, the Cambridge University Press and I are developing a dynamic enhanced digital book that will embed the audio of the eleven podcasts into the digital book itself, enabling readers to listen to the podcasts directly as they encounter them in the text. In order to cultivate a community of collaborative reading, the enhanced digital book will also enable the reader to make all highlighting and annotations public if desired. Those annotations and markings will then themselves generate a feed that interfaces with a blog plug-in like Comment Press or some other form of integration by which the annotations and highlights can appear in public in ways that are open to further response. Although readers might decide to publish the annotations to a preferred social media site, the annotations should also be accessible to a blog I manage and moderate so that I can respond to and engage with readers as they engage with the book itself.

The publication of this enhanced digital book is designed to facilitate an ongoing dialogue about the book, its ideas and the larger questions of what I call in the book the “politics of reading.” As the conversation develops, I would envision recording new episodes of the Digital Dialogue podcast with readers who have had particularly insightful annotations or comments. Those podcasts too, if desired, could be made available in the enhanced digital book.

I envision the printed version of the book as another way to give readers access to this enhanced digital version and the community of dialogue to which it is intimately connected. We hope to include QR codes or some other method of moving the reader from the physical book to the online conversation. The hope is that the enhanced digital book will interface with existing systems of curated annotations as those found on the Kindle via Amazon.com and sites like Findings.com and Apple’s iBookstore.

Finally, the question of authorial authority emerges as central to this project. On one hand, by inviting readers to become active participants in an ongoing conversation about the ideas articulated in the book, the enhanced digital book is designed to recognize and cultivate the hermeneutical imagination of its readers and open new perspectives on the text for the author. In this sense, the publication of the book is designed to open a site of ongoing scholarly dialogue in which the author is but one of multiple interlocutors. On the other hand, by affording the author ongoing opportunities to moderate and shape the discussion associated with the book, its mode of publication could be taken to further reinforce the centrality and authority of the author. A central question thus emerges: how can digital technologies be deployed that will ensure civil, deliberative dialogue without re-inscribing the hegemony of the author into the published text?

The ideals of reading for which the book argues — the importance of attentively caring for individuals as such, of cultivating ethical imagination, and of orienting our lives toward ideals of justice, beauty and the good — cannot simply be argued for, they must be put into practice. This short paper at DH2013 will be part of the larger attempt to engage an audience of interested scholars in order to further augment the community of active readers on which the success of the project ultimately depends.

Notes
See: http://www.la.psu.edu/chrislong
See: http://www.personal.psu.edu/cpl2/blogs/digitaldialogue/blog/
See: http://ets.tlt.psu.edu/wiki/Digital_Dialogue
Introduction
Rankings of academic articles and journals have been used in most disciplines, although concerns and objections about their use have been raised, particularly when they affect appointments, promotions and research grants. In addition, journal rankings may not represent real research outcomes, since low-ranking journals can still contain good work. Arts and humanities scholars have raised additional concerns about whether the various rankings accommodate differences in cultures, regions and languages. Di Leo (2010) wrote that “journal ranking is not very useful in academic philosophy and in the humanities in general” and one reason is the “high level of sub-disciplinary specialization”. Additionally, Di Leo notes there is “little accreditation and even less funding” in the humanities when compared with business and sciences.

In a Nature article entitled, “Rank injustice”, Lawrence (2002) notes that the “Impact factor causes damaging competition between journals since some of the accepted papers are chosen for their beneficial effects on the impact factor, rather than for their scientific quality”. Another concern is the effect on new fields of research. McMahon told The Chronicle of Higher Education, “Film studies and media studies — they were decimated in the metric because their journals are not as old as the literary journals. None of the film journals received a high rating, which is extraordinary” (quoted by Howard 2008).

Although the Australian government dropped rankings after complaints that they were being used “inappropriately”, it will still offer a profile of journal publications that provides an “indication of how often a journal was chosen as the forum of publication by academics in a given field” (Rowbotham 2011). Despite concerns over rankings, educators and researchers agree there should be a quality management system. By publishing their results, researchers are not just talking to themselves. Research outcomes are for public use, and others should be able to study and measure them. However, the questions are how can we measure the research efforts and their impact, and can we get an early indication of research work that is capturing the research community’s attention. A second question is whether measures appropriate for one research area also can be applied to publications in a different area. In this study, we seek initial insights on these questions by using data from a social media site to measure a real-time impact of articles in the digital humanities.

Research Community Article Rating (RCAR)
Citation analysis is a well-known metric to measure scientific impact and has helped in highlighting significant work. However, citations suffer from delays that could span months or even years. Bollen, et al., (2009) concluded that “the notion of scientific impact is a multi-dimensional construct that cannot be adequately measured by any single indicator”. Terras (2012) found that digital presence in social media helped to disseminate research articles, and “open access makes an article even more accessed”.

An alternative approach to citation analysis is to use data from online scholarly social networks (Priem and Hemminger 2010). Scholarly communities have used social reference management (SRM) systems to store, share and discover scholarly references (Farooq et al 2007). Some well-known examples are Zotero [1] , Mendeley (Henning and Reichelt, 2008) and CiteULike [2] . These SRM systems have the potential to influence and measure scientific impact (Priem et al., 2012). Alhoori and Furuta (2011) found that SRM is having a significant effect on the current activities of researchers and digital libraries. Accordingly, researchers are currently studying metrics that are based on SRM data and other social tools. For example, Altmetrics [3] was defined as “the creation and study of new metrics based on the social web for analyzing and informing scholarship”. PLOS proposed article-level metrics (ALM) [4] that are a comprehensive set of research impact indicators that include usage, citations, social bookmarking, dissemination activity, media, blog coverage, discussion activity and ratings.

Tenopir and King (2000) estimated that scientific articles published in the United States are read about 900 times each. Who are the researchers reading an article? Does knowing who these researchers are influence the article’s impact? Rudner, et al., (2002) used a readership survey to determine the researchers’ needs and interests. Eason, et al. (2000) analyzed the behavior of journal readers using logs.

There is a difference between how many times an article has been cited and how many times it has been viewed or downloaded. A citation means that an author has probably read the article, although this is not guaranteed. With respect to article views, there are several viewing scenarios such as intended clicks, unintended clicks or even a web crawler. Therefore, the number of views has hidden influential factors. To eliminate the hidden-factors effect, we selected the articles that researchers had added to an academic social media site. In this study, we ranked readers based on their education level. For example, a professor had a higher rank than a PhD student, who in turn had a higher rank than an undergraduate student.

Zotero’s readership statistics were not available to the public, and in CiteULike, the most cited articles in Literary and Linguistic Computing (LLC) were shared by few users. Therefore, we were unable to use either system’s data. Instead, we obtained our data from Mendeley, using its API [5] . We measured the research community article rating (RCAR)using the following equation:


RCAR uses the following quantities:


Citations, Readership and RCAR
We looked at seven digital humanities journals that were added to Mendeley and also mentioned in Wikipedia [6] . Of the seven journals, Google Scholar had an h5-index for only two journals: Digital Creativity (h5-index = 7) and LLC (h5-index = 13). We calculated the RCAR and compared the top-cited LLC articles with Mendeley readerships, as shown in Table 1. The number of citations were significantly higher than the number of Mendeley readerships for LLC (p-value>0.05).


Table 1:
Google citations, Mendeley readerships and RCAR for LLC

We investigated how the digital humanities discipline is different from other disciplines. We compared LLC with a journal from a different area of research, Library Trends, which had a similar h5-index. Library Trends received more citations and readerships than LLC. Three of its top articles also had more Mendeley readerships than citations, whereas LLC only had one such case. However, there was no significant difference between Library Trends citations and readerships. Next, we tested the Journal of the American Society for Information Science and Technology (JASIST) and the Journal of Librarianship and Information Science (JOLIS). We found that JASIST and JOLIS readerships of articles published in 2012 were higher than the citations with significance difference. This indicates that computer, information, and library scientists are more active in academic social media site than digital humanities researchers. By active we mean that they share and add newly published articles to their digital library.

Citations and altmetrics
In order to better understand different socially-based measures, we compared LLC articles using altmetrics and citations. We used an implementation of altmetrics called Altmetric that gave “each article a score that measures the quantity and quality of attention it has received from Twitter, Facebook, science blogs, mainstream news outlets and more sources”. We found that most of the articles that received social media attention were published during the last two years. A number of articles that were published four or more years ago were exceptions to this finding. These older articles had received at least four citations, as shown in Table 2. We also found similar correlations with articles in Digital Creativity.

Finally, we compared readerships and Altmetric. We found no significant difference between LLC citations of articles published in 2012 and readerships. However, we found a significant difference between Altmetric and citations (p<0.05) for articles that were published in 2012. This shows that the researchers who are interested in digital humanities are more active in general social media sites (e.g. Twitter, Facebook) than academic social media sites (e.g. Mendeley).


Table 2:
Altmetric score and citations to LLC articles

In this paper we describe a new multi-dimensional approach that can measure in real-time the impact of digital humanities research using academic social media site. We found that RCAR and altmetrics can quantify an early impact of articles gaining scholarly attention. In the future, we plan to conduct interviews with humanities scholars, to better understand how these observations reflect their needs and the standards in their fields.

References
Alhoori, H., and R. Furuta (2011). Understanding the Dynamic Scholarly Research Needs and Behavior as Applied to Social Reference Management. TPDL’11 Proceedings of the 15th international conference on Theory and practice of digital libraries. Berlin Heidelberg: Springer. 169–178.
Bollen, J., H. Van de Sompel, A. Hagberg, and R. Chute (2009). A principal component analysis of 39 scientific impact measures. PloS one. 4(6). e6022.
Eason, K., S. Richardson, and L. Yu (2000). Patterns of use of electronic journals. Journal of Documentation. 56(5): 477–504.
Farooq, U., Y. Song, J. M. Carroll, and C. L. Giles (2007). Social Bookmarking for Scholarly Digital Libraries. IEEE Internet Computing. 11(6): 29–35.
Henning, V., and J. Reichelt (2008). Mendeley — A Last.fm For Research? In 2008 IEEE Fourth International Conference on eScience. IEEE. 327–328.
Howard, J. (2008). New Ratings of Humanities Journals Do More Than Rank They Rankle. The Chronicle of Higher Education. http://chronicle.com/article/New-Ratings-of-Humanities/29072 (Accessed 5 March 2013).
Lawrence, P. A. (2002). Rank injustice. Nature. 415(6874): 835–6.
Di Leo, J. (2010) Against Rank. Inside Higher Ed. http://www.insidehighered.com/views/2010/06/21/dileo (Accessed 5 March 2012).
Priem, J., and B. M. Hemminger (2010). Scientometrics 2.0: Toward new metrics of scholarly impact on the social Web. First Monday. 15(7).
Priem, J., C. Parra, H. Piwowar, P. Groth, and A. Waagmeester (2012). Uncovering impacts: a case study in using altmetrics tools. Workshop on the Semantic Publishing (SePublica 2012) at the 9th Extended Semantic Web Conference.
Rowbotham, J. (2011). End of an ERA: journal rankings dropped. The Australian. http://www.theaustralian.com.au/higher-education/end-of-an-era-journal-rankings-dropped/story-e6frgcjx-1226065864847 (Accessed 5 March 2013).
Rudner, L.M., J. S. Gellmann, and M. Miller-Whitehead (2002). Who Is Reading On-line Education Journals? Why? And What Are They Reading? D-Lib Magazine. 8(12).
Tenopir, C., and D. W. King (2000). Towards Electronic Journals: Realities for Scientists, Librarians, and Publishers. Washington, D.C.: Special Libraries Association.
Terras, M. (2012). The impact of social media on the dissemination of research: results of an experiment. Journal of Digital Humanities. 1(3): 30–38.
As the customs of the Internet grow increasingly collaborative, crowdsourcing offers an appealing frame for looking at the interaction of users with online systems and each other. However, it is a broad term that fails to emphasize the use of crowds in subtler system augmentation.

This paper introduces incidental crowdsourcing (IC): an approach to user-provided item description that adopts crowdsourcing as a frame for thinking about augmentative features of system design. IC is intended to frame discussion around peripheral and non-critical system design choices.

A provisional definition of incidental crowdsourcing will be defined in this paper, and then refined based on examples seen in practice. IC will be examined from both the user and system ends, positioned within existing work, and considered in the context of its benefits and drawbacks. This approach allows us to explore the robustness and feasibility of IC, looking at the implications inherent to accepting the provisional definition.

The consequences of considering system design on a scale between IC and non-IC design choices remain to be seen. Toward this goal, the second part of this paper shows a study comparing the participation habits of users in two online systems — one that is representative of IC properties and one that is not. This study finds differences in user engagement between the two systems.

Introduction
Crowdsourcing asks a dispersed group of people to contribute toward a common task. It does not need to be the central feature of a project; it can be used for augments parts of a project. For example, Facebook [1] uses “Likes” to gauge popularity of user-generated content, while photo-sharing website Flickr [2] uses user labeling to improve their search engine.

Incidental crowdsourcing functions in in this way, capturing useful but unobtrusive user input and making sense of it in aggregate. An incidental crowdsourcing feature is supplemental to its site’s primary function. Thus, visitors to the website are gently given ways to make a contribution, but not forced into it. IC offers a way to consider the design of online systems through the lens of crowdsourcing, which offers a compelling framework for gathering abstract, perception-based, or conceptual data in a volunteer-driven and often mutually beneficial way.

The value of IC is largely in augmenting existing information, making it valuable in the digital humanities for enriching digital resources. Version 1.0 of Digital Humanities Now, for example, used implicit linking by DH scholars on Twitter to determine the quality of online information (Cohen 2009). Another system dealing with digital resources, citation manager Mendeley, takes an IC approach in improving metadata and predicting research trends (Henning et al. 2010).

Defining incidental crowdsourcing
Incidental crowdsourcing is the gathering of contributions from online groups in an unobtrusive and non-critical way.

It is unobtrusive in that it does not cause significant barriers to a user’s completion of a task. The corollary to this is that IC exists in a task-driven environment where the user has a primary objective and IC exists alongside it without causing resistance to it.

IC is also non-critical to users and systems. For users, making a contribution is not a necessary part of a their use, while systems should not rely on contributions to function, using them for value-added features but degrading gracefully when there are few or unevenly distributed contributions.

This provisional definition is expanded in the full paper by considering complementary characteristics of examples that fit the definition. This refinement expands the definition to note that contributing to IC is descriptive — producing data about existing information objects, contributions tend to toward low granularity, and systems favor choices over statements.

IC is best considered as a scale, where the IC fitness of a crowdsourcing system design element is a mixture of how well it conforms to each part of the above definition.


Table 1:
Common forms of incidental crowdsourcing and examples

Following from the provisional definition, Table 1 shows common IC actions, alongside examples of how they are implemented. The full paper outlines these actions relative to their use in digital humanities. These include:

Scoring the quality of an information object. Rating or ranking systems that conform to the definition of IC tend to be on the lower end of granularity, most often using five- or two-point scales. Unary rating mechanisms are also used, for saving or supporting information items in online systems. For example, Facebook’s “Like” buttons allow users of the social network can make an assertion on the quality of an item. Rating systems tend to skew upward (Hu et al. 2006, Banjeree and Fudenberg 2004), and single button saving features are generally positive. Implicit recommendation is another valuable indicator of support; for example, it has been used to discover notable web resources through microblogging links (Cohen 2009).

Organizing content. Curatorial features are a way for users to thematically group information objects in a way that can teach a system about the relationships between those objects. For example, newer OPAC replacements encourage IC classification and curation with patron-built book lists, ratings, and tagging (Singer 2008, Spiteri 2011). Such catalogues can be interaction points rather than simply retrieval systems, but participation is non-critical to users.

Editing content. Incidental crowdsourcing is sometimes used to switch user roles from consumer to creator. The Australian Newspaper Digitization Project implemented this approach in corrected OCR transcriptions of old newspapers (Holley 2009), offering a link to the editing interface from the newspaper reading screen.

Feedback. Simply asking users questions which they have the capacity to answer has been noted as a strong motivator for contribution (Kraut and Resnick 2012, Organisciak 2010), and feedback mechanisms often make use of this with direct questions and easy to choose answers.

User- and System-end considerations
Since IC contributions are non-critical, systems utilizing IC should degrade gracefully when there is a lack of contributions. A system dealing with IC contributions should not be dependent on large or evenly distributed data sets. For example, the transit tracking application Tiramisu (Zimmerman et al. 2011, Tomasic et al. 2011) aggregates the location of riders when it is being used, but falls back on historical information when real-time data is unavailable.

Table 2 considers common IC actions and the corresponding value to a system and its users. Notably, in the majority of cases the user’s act of contributing is one of description rather than creation. Systems primarily use IC for understanding the content within them, while users primarily contribute to fulfill personal and social needs.


Table 2

Comparison of design choice in product ratings
How would an evaluation of systems through the lens of IC look? As an example, I compared the rating patterns of two application marketplaces—Amazon Appstore [3] and Google Play 4 . From each store the lists of best-selling free and paid items were scraped and parsed, and the applications that were on the lists of both sites were matched while others were removed from the data.

The sites were chosen because they are alike in purpose, selling applications for the Android operating system, and much of the same content is represented between them. However, how each store allows users to rate their purchases differs. Google’s rating functionality is more aligned with IC, allowing users to rate an item with two clicks on one page. Meanwhile, Amazon is non-IC, asking raters to include title, reviews, and to abide by a codebook.


Figure 1

This study found that the distribution of mean ratings skewed higher for Google Play than for Amazon Appstore (Wilcoxon p<0.001, see Figure 1). The difference in rating style exists even though there is no difference between the systems in how likely an application is to be rated (T-test p=0.9873, Ho:μdiff=0). Breaking the rating distributions down by relative choice frequency (Figure 2) shows a clear pivot at four stars.


Figure 2-1


Figure 2-2 (Simplified)

The distinct pivot between the distributions suggests that an adjustment can make Google’s distribution — collecting in a more IC appropriate manner — nearly identical to Amazon’s. Thus, while the non-IC approach receives more written reviews, Google does not appear to sacrifice rating quality with easier ratings. This could make a difference when looking to measure quality of new or barely-seen items.

Conclusion
This paper introduces the concept of incidental crowdsourcing, a way to crowdsource in a way that is non-critical, descriptive, unobtrusive and peripheral. Incidental crowdsourcing matters as a way to adopt crowdsourcing practices to reflect the subjective ‘humanness’ of digital object interpretations by consumers.

References
Banerjee, A., and D. Fudenberg (2004). Word-of-mouth Learning. Games and Economic Behavior 46(1). Web. 7 Dec. 2011.
D. Cohen. (2009). Introducing Digital Humanities Now. 18 Nov. 2009.
Henning, V., J. J. Hoyt, and J. Reichelt (2010). Crowdsourcing Real-Time Research Trend Data. Raleigh, USA. Web. 1 Nov. 2012.
Holley, R. (2009). Many Hands Make Light Work: Public Collaborative OCR Text Correction in Australian Historic Newspapers. National Library of Australia. National Library of Australia Staff Papers.
Hu, N., P. A. Pavlou, and J. Zhang (2006). Can Online Word-of-mouth Communication Reveal True Product Quality? Experimental Insights, Econometric Results, and Analytical Modeling. Proceedings of the 7th ACM Conference on Electronic Commerce-2006. 324–330.
Kraut, R.E., and P. Resnick (2012). Encouraging Contribution to Online Communities. Designing From Theory: Using the Social Sciences as the Basis for Building Online Communities.
Organisciak, P. (2010). Why Bother? Examining the Motivations of Users in Large-scale Crowd-powered Online Initiatives. 31 Aug.
Singer, R. (2008). In Search Of A Really ‘Next Generation’ Catalog. Journal of Electronic Resources Librarianship. 20(3). 139–142. Web. 1 Nov. 2012.
Spiteri, L. F. (2011). Social Discovery Tools: Cataloguing Meets User Convenience. Proceedings from North American Symposium on Knowledge Organization. 3.
Tomasic, A., et al. (2011). Design Uncertainty in Crowd-Sourcing Systems.
Zimmerman, J., et al. (2011). “Field Trial of Tiramisu: Crowd-sourcing Bus Arrival Times to Spur Co-design.” Proceedings of the 2011 Annual Conference on Human Factors in Computing Systems. CHI ’11 held in Vancouver, BC. ACM. 1677–1686. Web. 4 Dec. 2011.
Notes
1. http://www.facebook.com

2. http://www.flickr.com

3. http://www.amazon.com/appstore

4. http://play.google.com
Objective
Digital Humanities (DH) has been exhaustively defined in the literature (e.g., Rockwell, 2002; Bellamy, 2012; Text Analysis Portal for Research, 2011; Fitzpatrick, 2011). Such definitions are sometimes at odds with each other and often represent differences based upon disciplinary concerns. Despite the assertion that DH is a “term of tactical convenience” (Kirschenbaum, as cited in Gold, 2012), the existence of a DH community seems to be well-established; there are a dizzying array of scholars identifying themselves as digital humanists and there are others doing work that some have categorized as DH. However, a thorough investigation and description of the communicative practices of DH is lacking. We know neither the breadth of methods used, the depth to which they are used, nor the purposes to which they are put. To this end, this paper examines informal and formal communication channels used by members of the DH community to diffuse information and build communities. These communications are negotiated at a variety of levels including students and faculty at the individual level, collaborative teams at the group level, and funding agencies and institutions at the societal level (Svensson, 2010). We analyze the data from these communications to determine how these interactions connect DH community members at the individual, group, and institutional levels and across the DH landscape and helps answer the question: How does the socio-technical ecology connect or partition the landscape of the DH community?

Background
In a discussion of how qualitative research may aid bibliometric analyses of the humanities, Sula (2012, para. 18) claims that “a fuller picture of the humanities will help to clarify the ways in which the humanities and sciences differ, beyond citation patterns and authorship practices”, calling for studies that look to both formal scholarly communication and informal communication from sources deriving from mentoring, peer-to-peer, and other relationships (built on interactions such as conference co-attendance, editorship, and contributorship to anthologies). Sula (2012) concludes by suggesting that these proposed studies based on expanded sets of communications look to apply the methods of network analysis and visualization. Our proposed study answers this call both in terms of the data used and the methods of analysis.

Previous studies outside the realm of formal scholarly communication in the DH domain have begun this expansion of information sources, examining DH Twitter communications (Ross, Terras, Warwick, & Welsh, 2011), syllabi (Terras, 2006; Spiro, 2012), journal citation analysis (View DHQ, 2012), and research centers (Zorich, 2008), exposing the diversity of scholarly communication activities in DH; however such studies have been limited for the most part to single channels of communication. In Terras (2011), an infographic quantifying DH produced by the UCL Centre for Digital Humanities displays DH’s burgeoning internationality as well as its institutionalization. Still another dimension of diversity is addressed by McPherson (2008) via Svensson (2010) — namely, a diversity of topicality, defined as foci in digital humanities upon computing, blogging, and multimodality. The current study addresses the demands of these multiple diversities to investigate divisions in the overall DH landscape, while doing so across multiple communication channels in order to discern how different dimensions of diversity and division may or may not overlap.

Methods
This work will apply multi-dimensional network analysis to data from Twitter, LLC and DHQ journals (data taken from the Web of Knowledge database), NEH grants awarded for DH-related projects, the TEI-L and Humanist listservs, DH syllabi, and a variety of other sources (blogs, centers, and projects), employing a cumulative, normalized database composed of data from these sources to paint a wider view of the connections among people, teams, institutions, and communication channels that make up the DH landscape. Our sources will be validated through consultation with prominent members of the DH community. The resulting normalized database will be rendered as a graph connecting URLs, projects, institutions, people, publications, and grants, which will then be partitioned and analyzed using standard community detection algorithms. We will then compare community overlap over different scholarly media to explore how DH practitioners organize themselves into and across communities, specifically looking at whether certain people, technologies, or publications sit at intersection points in the network, holding communities together.

Significance
This research is innovative in its combination of both formal (syllabi, journals, grant proposals, etc.) and informal (Twitter, blogs, listserv, etc.) communication channels allowing for a broader analysis of the communication network of the DH community. Previous work has focused on single source types and has marginalized community members who communicate in other ways. There is a vacuum of formal DH connectivity and this work addresses ways in which this vacuum is being filled and what that implies about the DH community. The DH community forms a network spanning across the world (Kamada, 2010) and it’s important to understand how this network is connected and how it is establishing itself in traditional academic institutions (Adams & Gunn, 2012). From a broader perspective, the methodology introduced here to study DH is generalizable to the analysis of other fields and will hence make a valuable contribution to scholarship.

Because DH community members are situated in various locales across a wide array of institutions, there are few formalized communication channels that span the DH landscape. The lack of formalized communication channels and instructional structure indicates that multi-dimensional methods are needed to fully comprehend this network; this premise informs our selection of formal and informal data sources. It is important to note that the DH community is an area of research made up of theories, methods, and people spanning multiple domains who publish across a variety of disciplines; that said, we will not be providing an exhaustive analysis of the entire landscape of DH. Examination of a large swath of this landscape allows for a wide-ranging analysis of the various channels used to keep those in the DH community informed. It is important for members of the DH community to be made aware of the various channels of communication that are being used to spread information. As Terras (2010) stated in her plenary speech to the DH2010 conference, “digital presence and digital identity is becoming more important to Digital Humanities as a discipline.” This work addresses this statement with empirical and heterogeneous evidence.

Acknowledgments
This work was funded by the Digging into Data initiative, organized by the National Endowment for the Humanities. Specific funding for this DID project comes from the National Science Foundation in the United States (Grant No. 1208804), JISC in the United Kingdom, and the Social Sciences and Humanities Research Council of Canada.

References
Adams, J. L. & D.B. Gunn (2012). Digital humanities: Where to start. College & Research Libraries News, 73(9): 536-569. http://crln.acrl.org/content/73/9/536.full
Bellamy, C. (2012). The sound of many hands clapping: Teaching the digital humanities through virtual research environments (VREs). Digital Humanities Quarterly, 6(1): http://www.digitalhumanities.org/dhq/vol/6/2/000119/000119.html
Fitzpatrick, K. (2011). The humanities, done digitally. The Chronicle of Higher Education. http://chronicle.com/article/The-Humanities-Done-Digitally/127382
Gold, M. K. (ed.) (2012). Debates in the Digital Humanities. University of Minnesota Press.
Juola, P. (2008). Killer applications in digital humanities. Literary and Linguistic Computing, 23(1): 73-83.
Kamada, H. (2010). Digital humanities: Roles for libraries. College & Research Libraries News, 71(9): 484-485. http://crln.acrl.org/content/71/9/484.full
Kirschenbaum, M. (2010). What is digital humanities and what’s it doing in English departments? ADE Bulletin, 150: 1-6.
Rockwell, G. (2002). Multimedia: Is it a discipline? The liberal and servile arts in humanities computing. Yearbook of the Seminar for Germanic Philology 4. Paderborn: Mentis Verlag. http://computerphilologie.uni-muenchen.de/jg02/rockwell.html
Ross, C., M. Terras, C. Warwick, and A. Welsh (2011). Enabled backchannel: Conference Twitter use by digital humanists. Journal of Documentation. 67(2): 214–237
Rosenbloom, P. S. (2012). Towards a conceptual framework for the digital humanities. Digital Humanities Quarterly 6(2). http://digitalhumanities.org/dhq/vol/6/2/000127/000127.html
Schriebman, S. (2012). Digital humanities: Centres and peripheries. Historical Social Research. 37(3): 46-58.
Spiro, L. (2011). Knowing and doing: Understanding the digital humanities curriculum. (Powerpoint slides) http://digitalscholarship.files.wordpress.com/2011/06/spirodheducationpresentation2011-4.pdf
Spiro, L. (2012). Models for supporting digital humanities at liberal arts colleges. (PowerPoint slides) http://digitalscholarship.files.wordpress.com/2012/05/spirodhsupportstructureswooster2.pdf
Sula, C. A. (2012). Visualizing social connections in the humanities: Beyond bibliometrics. Bulletin 38(4): Retrieved from http://www.asis.org/Bulletin/Apr-12/AprMay12_Sula.html
Svensson, P. (2010). The landscape of digital humanities. Digital Humanities Quarterly 4(1). http://digitalhumanities.org/dhq/vol/4/1/000080/000080.html
Svensson, P. (2012). Envisioning the digital humanities. Digital Humanities Quarterly 6(1). http://www.digitalhumanities.org/dhq/vol/6/1/000112/000112.html
Text Analysis Portal for Research. (2011). How do you define Humanities Computing/Digital Humanities? http://tinyurl.com/9lw97hm (accessed 7 March 2012)
Terras, M. (2010). DH2010 Plenary: Present, not voting: Digital humanities in the panopticon. http://melissaterras.blogspot.com/2010/07/dh2010-plenary-present-not-voting.html (accessed 10 July 2010).
View DHQ (Data Visualization). (2012). 2012 ACH Microgrant: Citation Network Visualization for Digital Humanities Quarterly. http://digitalliterature.net/viewDHQ/
Wagner, C. S. (2008). The topology of science in the twenty-first century. In The New Invisible College Washington, D.C.: Brookings Institution Press. 15-32.
Zoirch, D. M. (2008). A survey of digital humanities centers in the United States. Council on Library and Information Research. (CLIR) held 12 May 2007. http://www.uvasci.org/wp-content/uploads/2008/06/dhc-survey-final-rept-2008_05_22-for-distribution.pdf, accessed on October 1, 2012.
Archaeology has a long history of innovative work with information and computing technology. While there are a small number examples in the late 1950s, the most influential comes courtesy of James Deetz’s seminal work on Arikara ceramics. Carried out in the early 1960s, Deetz’s project used the IBM704 mainframe at the MIT Computation Laboratory to discover "stylistic coherence" on over two thousand rim sherds from central South Dakota Medicine Crow site. Deetz’s work was extremely important as it suggested that computers were excellent tools for statistical, typological, chronological, or stylistic analysis of large and complex sets of data (a hallmark of archaeology).

Since these early days, digital archaeology has remained intently focused on the analysis, interoperability, and preservation of digital data. By the mid-1980s, however, the personal computer had reached a point where they became effective tools for archaeological visualization and imagery. Desktop applications such as GIS, which allowed for the visualization, analysis, and modeling of socio-spatial data, and CAD, which facilitated the production of detailed and geometrically accurate archaeological maps at various scales without time-consuming redrafting, became central in the digital archaeological ecosystem.

In recent years, along with many other disciplines in the humanities and social sciences, archaeology is entering a new age in which information, computing, and communication technology is having transformative impact on all aspects of the field. The archaeological domains and activities in which digital approaches, methods, and technologies are relevant have grown well beyond the traditional trinity of data, GIS, and CAD. All aspects of research (including field and lab methods), teaching, outreach, publication, and scholarly communication are being impacted in new and unpredictable ways by “digital.” Quite frankly, gone are the days in which digital archaeological methods were siloed off from the main body of scholarly practice. In many ways, one might argue that we have entered an age in which all archaeology is digital archaeology and all archaeologists are digital archaeologists.

In is within this context that this session will highlight a series of innovative projects and practices that represent the forefront of work in digital archaeology. Special attention has been made to highlight projects which represent a variety of domains within digital archaeology including digital data, public engagement, data & topic modeling, crowdsourcing, linked-open data, and digital fieldwork records management. All papers in the session also speak to the changed and changing nature of scholarly and professional practice within archaeology, addressing new approaches to collaboration, community engagement, citizen scholarship, cyberinfrastructure, preservation & access, capacity building, and sharing. A second, but no less important, goal of this session is to challenge the rather curious separation that exists between digital archaeology and the digital humanities by clearly placing the two domains parallel to one another and recognizing the fact that they both have much to learn from one another. The ultimate goal in this regard is to foster and support fruitful discussions and collaboration between digital archaeologists and digital humanists.

Topic Modeling Time and Space: Archaeological Datasets as Discourses
Graham, Shawn|Shawn_Graham@carleton.ca|Carleton University, Canada
Topic modeling is very popular at the moment in the digital humanities. A recent tutorial on getting started with this tool explains them as tools for extracting topics or injecting semantic meaning into vocabularies: "Topic models represent a family of computer programs that extract topics from texts. A topic to the computer is a list of words that occur in statistically meaningful ways. A text can be an email, a blog post, a book chapter, a journal article, a diary entry – that is, any kind of unstructured text" (Graham, Weingart, and Milligan 2012). In that tutorial, 'unstructured' means that there is no encoding in the text by which a computer can model any of its semantic meaning.

Archaeological datasets are rich, largely unstructured bodies of text. While there are examples of archaeological datasets that are coded with semantic meaning through xml and Text Encoding Initiative practices, many of these are done after the fact of excavation or collection. In the field, things can be rather different, and this material can be considered to be 'largely unstructured' despite the use of databases, controlled vocabulary, and other means to maintain standardized descriptions of what is excavated, collected, and analyzed. This is because of the human factor. Not all archaeologists are equally skilled. Not all data gets recorded according to the standards. Where some see few differences in a particular clay fabric type, others might see many, and vice versa. Archaeological custom might call a particular vessel type a ‘casserole’, thus suggesting a particular use, only because in the 19th century when that vessel type was first encountered it reminded the archaeologist of what was in his kitchen – there is no necessary correlation between what we as archaeologists call things and what those things were originally used for. Further, once data is recorded (and the site has been destroyed through the excavation process), we tend to analyze these materials in isolation. That is, we write our analyses based on all of the examples of a particular type, rather than considering the interrelationships amongst the data found in the same context or locus. David Mimno in 2009 turned the tools of data analysis on the databases of household materials recovered and recorded room by room at Pompeii. He considered each room as a 'document' and the artefacts therein as the 'tokens' or 'words' within that document, for the purposes of topic modeling. The resulting 'topics' of this analysis are what he calls 'vocabularies' of object types which when taken together can suggest the mixture of functions particular rooms may have had in Pompeii. He writes, 'the purpose of this tool is not to show that topic modeling is the best tool for archaeological investigation, but that it is an appropriate tool that can provide a complement to human analysis....mathematically concrete in its biases'. The ‘casseroles’ of Pompeii turn out to have nothing to do with food preparation, in Mimno’s analysis.

To date, this is the only example of topic modeling applied to archaeological data. As such, it is novel in the digital humanities for applying the tools of data mining not to texts, but to things. In this paper, I explore the use of topic models on another rich archaeological dataset, the Portable Antiquities Scheme database in the UK. The Portable Antiquities Scheme is a project "to encourage the voluntary recording of archaeological objects found by members of the public in England and Wales". To date, there are over half a million unique records in the Scheme's database. I use topic modeling of this database to tease out archaeological patterns -the discourses of topic modeling, to use Ted Underwood's phrasing - over both time and space. In order to visualize these discourses, I map them both in geographic and relational space, using the network analysis program Gephi. The constellation of ideas (the resultant ‘topics’) that make up the various discourses in the data can be represented as nodes while the strengths of the associations suggested by the topic model can be represented as edges. This two-mode graph (words and ‘topics’ or ‘discourses’) can be queried for deeper structure. I look at the modularity of this graph to determine ‘communities’ of ideas or discourses. I then lay this network against real geographic space by time-slice to understand changes over time and space in the Portable Antiquities Scheme data. I agree with Mimno's suggestion that this is an appropriate tool for the digital archaeologist, but try to understand the limitations, caveats, and lessons for digital humanities more generally, from this application.

The Archaeological Resource Cataloging System (ARCS): An Open-Source Solution to Digitizing an Archaeological Archive
Frey, Jon M.|freyjona@msu.edu|Michigan State University, United States of America
Adams, Brian|adamsb@msu.edu|Michigan State University, United States of America
Schopieray, Scott|schopie1@msu.edu|Michigan State University, United States of America
Over the past few decades, archaeologists have begun to realize the benefits of providing archival records in digital form. Whether information is collected electronically or digitized from pre-existing materials, digital archaeological data should be readily accessible from anywhere in the world. These developments have increased the productivity of scholars who no longer need to visit the actual archive and eased the strain on those projects that must accommodate visiting researchers in addition to their normal daily operations. On the other hand, the creation of digital archives has drastically impacted the ways in which we interact with documents and artifacts that form the basis of archaeological research. Financially constrained projects have lagged behind their better-funded peers in the process of digitization and dissemination of electronic records. As a result, instead of providing greater access to a wider range of archaeological data, the process of archival digitization runs the risk of further privileging the evidence of those surveys and excavations with greater financial resources. In addition, many of the existing digital archaeological archives concentrate upon artifacts to the point that the archaeologist’s field journal, arguably the most important evidence in establishing context, is rarely presented in its original form. Even where diverse forms of information are provided, a digital archive often encourages the study of objects and documents in isolation from one another and without the benefit of the institutional memory that often aids in their interpretation. While a traditional archive allows an individual to conduct their research through physical interaction with a number of different archival materials at once, often in the presence of those who discovered and prepared them, many digital archives simply rely on keyword searches to generate lists of electronic records that to the untrained eye appear to be of equal value as forms of evidence.

In this paper, we present the Archaeological Resource Cataloging System (ARCS), an open-source digital asset management application created for the Ohio State University Excavations at Isthmia in order to address these issues. Developed at Michigan State University through an NEH Digital Humanities Startup Grant, ARCS utilizes a web-based interface that allows authorized users to upload and “tag” digital resources consistently according to generally accepted metadata standards that can be further refined to reflect any project’s unique terminology. These resources can then be searched, sorted into collections and connected to one another through the creation of virtual links without affecting the integrity of the original data. In this way the essential interrelatedness of the various forms of archaeological data is preserved in a flexible electronic format. In order to foster a better sense of community among researchers, each resource is also provided with a discussion tool that allows users to ask questions or identify mistakes, thereby making use of others’ knowledge and experience to cultivate the development of the dataset. In addition, because ARCS depends on the collective effort of a community of users, the system generates a permanent record of all additions and modifications of resources so that errors can be easily corrected and dependable users more clearly identified.

Perhaps most importantly, because it is an open-source application that relies on multiple users to develop and manage the digital assets of an excavation or survey, ARCS offers an affordable option for archaeological projects that lack a dedicated digital archivist or IT specialist. Digital data can be added as it is made available and, once uploaded, new resources can be linked to body of evidence that continues to grow in size and detail. In the end, ARCS not only retains the many benefits of more traditional research involving physical documents at an actual archive but in many ways also speeds and simplifies the process of archaeological investigation.

“All of Us Would Walk Together": Digital Cultural Heritage and the African American Past at Historic St. Mary's City, Maryland
Brock, Terry P.|brockter@msu.edu|Michigan State University, United States of America
In October 2012, Historic St. Mary's City (HSMC) launched a digital exhibit and social media campaign focused on the 19th-century component of their museum. HSMC, an archaeology and living history museum, has traditionally focused its 17th-century component, which was Maryland's first capital city. The digital exhibit, however, allowed the museum to begin interpreting additional centuries without disrupting the 17th-century landscape. Additionally, the digital exhibit, HSMC is able to develop an approach that focuses on public communication and engagement, allows for transparent research methods and interpretations, and provides flexibility when integrating the content into future programming and on-site exhibit. Through a combined approach of a content based digital exhibit, research blog, and social media, the digital exhibit, called "All of Us Would Walk Together", provides an example of digital archaeology that incorporates contemporary concepts of public archaeology through digital exhibitions and research methods. This paper will discuss how this has been put into action.

During the past few decades, community engagement has become a critical component of African American archaeology. Starting with the public excavations at the African American Burial Ground project in New York City, researchers have begun to incorporate local communities and descendants in the development and implementation of research projects and museum exhibits. Establishing a transparent, reciprocal, and pragmatic back-and-forth has become a valued and integral part of the research process for many archaeologists. Online tools have also been used as a means for public engagement, in particular at the Levi Jordan Plantation and Rosewood. More recently, archaeologists have adopted social media as a means for engaging communities and stakeholders, such as at the Michigan State University Campus Archaeology Program, Florida Public Archaeology Network, and at Mt. Vernon. Although each approach has highlighted different topics and methods for engagement, each has found the use of the web and digital social media to be a beneficial means of engaging the public.

At HSMC, the interpretation of the 19th century had not been the primary focus of the museum. This was particularly evident when the structures relating to the 19th century, including a manor home, its outbuildings, and a former duplex slave and tenant quarter, were physically moved to a different location in 1992 due to its conflict with the 17th-century interpretation. While the buildings continued to be used as a bed and breakfast, they were not used as an interpretive component of the museum, causing memory of the 19th century to be lost to the public. Recently, this story has begun to resurface, due to a number of factors at the museum. Included was the reacquisition of the manor home and outbuildings from the owners of the bed and breakfast, and funding opportunities to interpret the duplex quarter through a digital exhibit and a physical exhibit. In addition to interpreting the site, the goal of the exhibits was also to build a relationship with the African American community and to reinstate the 19th-century story into the public consciousness.

The digital exhibit consists of two components: a traditional exhibit space and social media. The exhibit space presents a number of webpages devoted to the interpretation of the historical and archaeological data that has already been analyzed. These pages trace the transition from slavery to freedom for the African Americans who lived on the site, and uses historical and archaeological evidence to develop the narrative. Interspersed are links to blog posts that discuss how the evidence was gathered or used by researchers to draw the conclusions. Additionally, each exhibit page has a comment field, where the public can ask questions, offer their own interpretation, or provide additional commentary. This allows for two-way communication between the public and researchers, while also allowing the public to engage in the interpretive efforts. Lastly, these pages will be linked to the physical exhibit through the use of QR codes or augmented reality to provide more flexibility to the interpretive efforts at the site of the duplex. In doing so, the exhibit becomes flexible, transparent, and engaged physical space, fitting within the parameters of an engaged cultural heritage project.

The use of social media, through a blog, Twitter, and Facebook, adds extra depth to these efforts. The blog provides the most flexibility and transparency by allowing the exhibit space to be amended, added to, or modified in a transparent way. This provides a great deal of flexibility to the site that only a digital exhibit can provide. For example, if the research results in a major change to the exhibit, a blog post can be written to discuss the change and why it happened. In the exhibit, a link to this post can be added to demonstrate that the research process is a fluid, ongoing process. The blog and Twitter are also used to highlight research at comparable sites or examining comparable themes. This ties the archaeological work conducted at this site to larger themes in the discipline, and begins to build relationships with other institutions, while providing additional resources and access to new scholarship to the public. The blog and Twitter are both instrumental in the preservation and exhibit building process, as they make it more transparent: the public can watch and participate in the decisions about what will be included in the exhibit, in addition to understanding what types of constraints are placed on the construction of an exhibit. Twitter also allows lab and fieldwork to be shared in realtime with the public. Lastly, Twitter provides access to a larger, global network, particularly an African American network, a demographic that uses Twitter more than others. Facebook, on the other hand, is being used to connect with HSMC's current online fans through its official Facebook account.

The project itself has a high set of goals, and is approaching it with a multifaceted approach. While the digital component is a crucial step, it is only part of a larger program of public engagement. For example, HSMC has been actively soliciting feedback from local community members and seeking to engage them through the formation of an advisory board. This reiterates an important tenant of engaging in digital public archaeology: that one cannot rely solely on one approach. Nonetheless, the use of the digital space does provide us with additional exhibit and interpretive space, and gives us a great deal of flexibility when dealing with the public, research, and presentation. Most importantly, the use of the digital arena allows this all to be transparent, reciprocal, and dynamic.

An Introduction to the Practices and Initial Findings of the Digital Index of North American Archaeology (DINAA)
Wells, Joshua J.|jowells@iusb.edu|Indiana University, South Bend
Anderson, David G.|dander19@utk.edu|University of Tennessee, Knoxville
Yerka, Stephen J.|syerka@utk.edu|University of Tennessee, Knoxville
Kansa, Eric C.|ekansa@ischool.berkeley.edu|University of California, Berkeley
Whitcher Kansa, Sarah|skansa@alexandriaarchive.org|Alexandria Archive Institute
Noack Myers, Kelsey|kejmyers@indiana.edu|Indiana University, Bloomington
DeMuth, R. Carl|rcdemuth@gmail.com|Indiana University, Bloomington
The Digital Index of North American Archaeology (DINAA) is a project to create interoperability models for archaeological site databases in the eastern United States, funded by the National Science Foundation (#1216810 & #1217240). The core research team consists of researchers from the Department of Anthropology and Archaeological Research Laboratory at the University of Tennessee, the Alexandria Archive Institute, and the Anthropology and Informatics programs at Indiana University. Open Context (http://opencontext.org) will be used as the primary platform for data dissemination for this project. Our aims are to work with the databases held by State Historic Preservation offices and allied federal and tribal agencies in Eastern North America, with the goal of linking data across state lines for research and management purposes. Redacted of sensitive items, such as site location, data linkages will promote extension and reuse by government personnel in state and federal agencies, and domestic and international researchers. The project will mint stable Web-URIs for each site record, and in doing so, we will help lay the foundations for future Linked Open Data applications in North American archaeology, architectural history, and historical studies.

This project repurposes government curated datasets to support innovative humanistic and social science research. Governmental archaeological site files in North America are important loci for documentary information on known archaeological sites. Their most basic function is to contain data about site types and information quality pursuant to heritage preservation legislation at the federal level, but potentially state and local levels as well. However, as a matter of practice these files, often as relational databases, contain many other data fields that describe important archaeological findings, and other data that serve environmental and bureaucratic functions for management and protection of heritage resources. The ways in which data about archaeological sites are recorded and communicated have an important origin in theoretical models about past behavior, and also have important implications on the professional comprehension of the data at large and the use of the data to rank planning and preservation priorities.

Efforts to collect and compile archaeological data have a long history, and information about archaeological sites and collections is maintained by every state and territory. Only rarely, however, have these data been compiled and examined at large geographic scales, especially those crosscutting state lines, and never to the extent and for the research and management purposes proposed in this project. Data from some 15 to 20 states (>half a million sites) east of the Mississippi will be integrated with a common ontology, based on existing standards, and adapted in collaboration with researchers and government personnel in state and federal agencies. The ontology will classify site files according to cultural affiliation and chronology as well as agency assessments of historical significance.

Linkage of site file and other datasets will facilitate studies of past human adaptation spanning large areas, and lead to greater collaboration between archaeologists and scientists in other disciplines. As examples, the linkage of archaeological data at broad new scales will permit, for the first time, the exploration of exciting new research topics, such as how the human populations in North America responded to climate change, population growth, and/or anthropogenic environmental issues over the past 13,000 years.

The availability of output online in the form of maps and data tables (at significantly reduced spatial resolution, to protect sensitive locations) will enhance public awareness, education, and appreciation for scientific research in general and archaeology in particular. The demonstration that primary archaeological data can be integrated and used to address fundamental questions at such scales will stimulate similar efforts worldwide. Finally, by creating translating routines rather than dictating procedures, this project will foster archaeological cooperation through cyberinfrastructure with a high ratio of benefits to costs.

The project helps achieve broader archaeological concerns regarding professional data management training, research ethics and outreach education. It will foster novel networking and data integration among multiple partners, as well as research and educational activities across multiple disciplines and geopolitical boundaries. Publicly accessible data products, at coarse scales to preserve site location security, will also be available for download and reuse as shapefiles, CSV data tables, and RDF and N3 triples for other Web and desktop applications, including desktop GIS investigation. The project will provide specific instructions for the open source GIS applications QGIS gvSIG, and uDIG, and the widely used proprietary application ArcGIS, in order to foster education in geographic information science within archaeology and related disciplines. The project will fund graduate and undergraduate students, and will assist their training in critical information management skills for the 21st century. The project addresses head-on a major challenge facing research communities worldwide: how to link disconnected and incompatible data systems in such a way that the combined data are useful for important scientific research.

The integration of site file data at continental scales in a new and unique informational infrastructure will allow, for the first time, the exploration of the North American archaeological record across multiple temporal periods and geographic regions. The geographic scale and extent of data integration proposed is currently unprecedented in American archaeology yet, we believe we have demonstrated that it is readily achievable. With proper attention these data have the potential for continued growth as developed by the professional archaeological community, and as the resulting datasets become more inclusive they may transform the practice of our profession.

The Portable Antiquities Scheme: a new(ish) model for recording public discovery
Pett, Daniel|DPETT@thebritishmuseum.ac.uk|The British Museum
The Portable Antiquities Scheme (PAS) was inaugurated in 1997, following the revision of the ancient law of Treasure Trove and the subsequent implementation of the Treasure Act in 1996. This project has been through several funding phases all using public money and encourages the voluntary recording of archaeological objects discovered by members of the public in England and Wales (Scotland is subject to different legislation) and items that meet the stipulations of the Treasure Act.

One of the key pillars of the PAS has been its digital presence, which has now been online in some form for over 13 years and this paper will discuss the impact that the digital arm of the project has had on a national and international audience. The PAS has been hailed by many as a model of public archaeological engagement and decried by others for allowing the mining of the archaeological record for personal gain; however this paper will show some of the PAS’ many successes. For example the PAS has had contributions from over 20,000 individuals worldwide, a new facility has been created for contributors to record their own discoveries through taxonomy driven interfaces and over 350 projects utilise these data for informing their research - for example Oxford University’s EngLaID project. The project has also absorbed and enhanced internationally renowned resources such as Oxford University’s Celtic Coin Index and Cardiff University’s Iron Age and Roman coins of Wales database to provide the largest national, single search node for the study of Roman and Celtic coinage.

The project website records a huge array of metadata about objects, that we often have but one chance to record; images, textual description, measurements, spatial data and user generated comments and audit logs. Over 820,000 objects have been recorded on the PAS database and these are made available for all to view, comment and reuse within their own research or their own websites under a Creative Commons ‘by attribution share-alike’ licence. This liberal approach to licensing content has not been seen widely in the UK and European archaeological sector and the launch of the PAS’ Staffordshire Hoard microsite in September 2009 showed how well received this approach would be. Over 250,000 unique visitors in one day used the innovative microsite to learn more about the amazing Anglo-Saxon hoard and many more viewed the images that had been disseminated via Flickr. The PAS has also utilised and archived social media platforms with varying degrees of success and is a major case study within Lorna Richardson’s forthcoming PhD and complements that of its host organisation (the British Museum.)

This paper will show how the PAS website impacts on the public with specific reference to stories of international interest – such as the Staffordshire Hoard, the Frome hoard of 52,503 Roman coins, the Crosby Garrett helmet and the Staffordshire Moorlands Patera. It will also discuss how these successes have been reached on a minimal digital budget (less than £5000 per annum) via the use of open source technology and through the buy in of its audience. The website has been internationally recognised, winning the prestigious Museums and the Web ‘Best of Web’ award for ‘research or online collection’ (recent winners include the V&A and the Metropolitan Museum of Art.)

The author will demonstrate how a variety of digital techniques that have been employed to complement and enhance data collected via our network of archaeologists, volunteers and citizen scientists; for example geo enrichment through the use of Yahoo!, Geonames, Pleiades and Google Maps, text extraction through OpenCalais and Autonomy, the implementation of a Solr based faceted multi-core search engine and also how a wide variety of application programming interfaces (api) have been employed throughout the site. It will also show how the author has been influenced by ground breaking projects such as Open Context, Pleiades and Pelagios and a variety of Museum sector projects.

The paper will also touch on the recent steps towards providing the PAS data through linked data (for example the release of over 50,000 annotations for Pelagios), towards integrating linked data into the site (for example dbpedia enriched resources) and the CIDOC-CRM mapping process. It will also discuss 3D , computed tomography and PTM/RTI imaging projects conducted by Brighton and Southampton universities that have used PAS sourced objects and hoards of coins to provide research material. If time allows, the paper will also demonstrate the success achieved through the use of Flickr as a disseminator for a wide variety of archaeological images; how the author has leveraged news sources such as the Guardian and UK parliamentary records for background debate on the portable antiquities debate (ethical and fiscal) and how it has impacted on the public psyche within the UK and further afield.

msu.seum: A Location Based Mobile Application for Exploring the Cultural Heritage and Archaeology of Michigan State University
Watrall, Ethan|watrall@msu.edu|Michigan State University, United States of America
The spaces we inhabit and interact with on a daily basis are made up of layers of cultural activity that are, quite literally, built up over time. While museum exhibits, historical and archaeological narratives, and public archaeology programs can communicate this cultural heritage, they do not generally allow for rich, place-based, and individually driven exploration by the public. In addition, museum exhibits rarely explore the binary nature of material culture and the preserved record of human activity: the presented information about material culture and the process by which scholarly research has reached those conclusions. In short, the scholarly narrative of material culture, cultural heritage, and archaeology is often hidden from public consumption.

In recent years, mobile devices as well as the development and maturation of augmented reality (broadly construed) have offe
Technological advancement has made books available not only in printed format but also in electronic format. Bangladesh is currently experiencing the exponential growth of information and entertainment being created in a digital format. The government of Bangladesh declares ‘Digital Bangladesh’ by 2020. This steps, activities and environment are gaining importance particularly among younger people in Bangladesh (Islam & Tsuji 2011). This phenomenon may change the way people perceive about reading and how printed materials are being utilized to facilitate reading. At present this paper is the first attempt to measure reading habits and attitudes in digital environment of the students in Bangladesh. This study may trigger more such research in other developing countries.
Reading culture in Bangladesh
Based on the survey conducted Bishwa Sahitya Kendra meaning 'World Literature Center' the reading interest and habits of Bangladeshis is very low (Bishwa Sahitya Kendra, 2012). In Bangladesh, primary, secondary and higher secondary level education are completely in Bengali medium and in higher education is both in Bengali and English languages. As most of the web contents in Bangladesh are in English language, it usually reduces the reading habits in online environment. The electronic media is challenging the reading habit in the society by shifting the attention to computer, mobile phone and internet. In the last few years it has grown dramatically, although obviously from a very low base. With an estimated internet user-base of 7.5 million coming into 2012, representing a 5% user penetration by population, the local internet industry has been preparing to move into the next stage of its development (Internet World Statistics, 2012). In a recent survey by the Bangladesh Bureau of Statistics (BBS), it was found that the literacy rate is only 56.8% (CIA, 2012).
Research about reading has been approached from various possible angles and from a variety of disciplinary backgrounds including literature, social science, library and information science, information systems and more recently information and communication technologies (ICT). With the growing amount of digital information available and the increasing amount of time that people spend reading electronic media, the digital environment has begun to affect people’s reading behavior. Studies on reading habits and attitude among university students has gained as much attention in recent years due to the impact of digital media made available through the internet (Liu 2005; Ramirez 2003). Several theorists in reading and literacy such as Landow (1992), Lanham (1993), O’Donnell (1998) and Murray (1997) all agree that the digital media brought through progressive development of ICT has introduced a transformative shift in reading and writing. University students have been known to be very receptive to different forms of media in their reading and writing practices. A number of scholars argue that the arrival of digital media, together with the fragmentary nature of hypertext, is threatening sustained reading (Healy 1990; Birkerts 1994). Birkerts (1994) further notes that the younger generation growing up in the digital environment lacks the ability to read deeply and to sustain a prolonged engagement in reading. Bolter (1991) states: “The shift from print to the computer does not mean the end of literacy itself, but the literacy of print, for electronic technology offers us a new kind of book and new ways to write and read”. Digital media contribute to a transformative shift in reading. They also introduce a number of powerful advantages that are traditionally absent in the printed environment, such as interactivity, non-linearity, immediacy of accessing information, and the convergence of text and images, audio and video (Landow 1992; Lanham 1993; Murray 1997; Ross 2003). Abidin, Pour-Mohammadi & Choon Lean (2011) conducted a study on Malaysian Chinese university students and it is revealed that the participants prefer the electronic media when reading for leisure but prefer the printed media to pass exams.
Methodology
In this study, the researcher intends to explore the reading attitude and habits among the Dhaka University students using a survey research method.
Online survey tool
An online survey tool Kwik Surveys was used for this research. The URL to the online questionnaire was sent mainly through different SNTs using personal messaging option and group post where the university students were connected via Facebook, Twitter, etc. Also, the URL was printed and provided to the students to respond to the questionnaire.
The questionnaire
The survey requested basic demographic data regarding age, gender and academic status and also contained items regarding the reading attitudes and habits. Students were asked to provide answers of viz, how often do you read in a week, what types of materials do they read, resources to get the reading materials, reading materials in leisure time, times spend on reading in online and so on. The survey also included 7-point Likert scale items regarding students’ attitudes towards reading, preference of reading in online and manual (1 being the lowest and 7 being the highest). The data analysis was carried out using SPSS statistical analysis software.
Data analysis techniques
In order to determine influence of students’ demographic characteristics on their opinions on reading attitudes and behaviour, Mann-Whitney and Kruskall-Wallis tests were carried out. Descriptive statistics were used to analyze demographic characteristics of the students in relation to their reading books in digital environment. A total 192 students responded to the online survey.
Objectives of the study
This study attempts to answer the following Major Research Questions (MRQs) as formulated below:
MRQ 1. What is the reading habit of the students in terms of the following?
•        (a) What type of reading material do they read?
•        (b) How much time do student spend on reading?
•        (c) Where do they get the reading material?
•        (d) When do they read?
•        (e) What do they read during leisure time?
•        (f) What percentage of time reading spent on reading electronic documents, browsing and scanning, and their overall experience with online reading?
•        (g) What are the tools they use access to electronic resources?
MRQ2. What is the students’ attitude towards reading?
•        (a) Do they love to read book?
•        (b) Do they think reading is boring?
•        (c) Do they feel pleasure to spend money for buying book?
•        (d) Which medium (printed or online) do they feel comfort for reading book?
•        (d) What is their overall reading attitudes and other relevant issues?
MRQ3. Is there any relationship between gender, age, study level with their reading habit and attitude?
Findings
This study was conducted in an attempt to enhance our understanding about reading habits and attitudes of the university students in Bangladesh focusing on a case of a public university. In this effort, students from all the faculties were chosen as the respondents. In overall analysis that include both groups, results indicates that university students spend quite a signiﬁcant amount of time reading newspapers 84.90% and second highest is the academic books 61.46%. A good number of students (50%) read also website materials. Reading has also become a major activity during their leisure time. Most of the students (57.81%) read fiction and novel in their leisure time and it is followed by newspaper 55.73%, magazine 43.75% and website materials (29.17%). The amount of time spent on reading other materials by the university students is seen as higher than their academic books and materials. This group is expected to read more due to their engagement in the academic process that requires them to read. Despite different extent in the preference among different genders and different study level, the study ﬁnds that preference for reading printed text remains strong which is 51.56%. This clearly indicates that printed media is the more used than online media due to the lack of facilities, poor online content and less availability of reading materials in online. On the other hand, students prefer reading in online environment when read short documents, casual reading and most recent information which mean score is above 4. In online environment, access to electronic resources mobile phone, laptop and desktop are the most used tools. Attitudes towards reading was high as the statements love to read books, like to read books when have free time and enjoyment from reading score was above 4. Several limitations can be found in the conduct of this study. Among them are the small sample size, the inclusion of only one university and the limited amount of variables studied. A bigger scale study need to be conducted for more reliable results, and with the inclusion of more variables such as family background, reading exposure and availability of reading materias, and variables that are related speciﬁcally with reading in the digital environment. In addition, the ﬁndings of the study should assist the university authority, especially the library and the computing department to look into service matters pertaining to accommodating the reading as well as the studying habits of the student. 24 hours of computing service may also allow students to use the internet since the day time is fully occupied with classes. This practice has been carried out in many western high academic institutions.
References
BSK (2012). Bishwa Sahitya Kendra available at: http://en.wikipedia.org/wiki/Bishwa_Sahitya_Kendra
Birkerts, S. (1994). The Gutenberg Elegies: The Fate of Reading in an Electronic Age. Boston: Faber and Faber.
Bolter, J. D. (1991). Writing Spaces: The Computer, Hypertext, and the History of Writing. Hillsdale, NJ: Laurence Erlbaum Associates.
CIA (2012). The World Factbook: Bangladesh, available at: https://www.cia.gov/library/publications/the-world-factbook/geos/bg.html.
Healy, J. M. (1990). Endangered Minds: Why Our Children Don’t Think. New York: Simon and Schuster.
Internet World Statistics (2012). Bangladesh-Internet Markets and Forecasts, available at:http://www.internetworldstats.com/asia/bd.htm
Islam, M. A., and K. Tsuji. (2011). Bridging digital divide in Bangladesh: study on community information centers, The Electronic Library. 29 (4): 506–522
Landow, G. (1992). Hypertext: The Convergence of Technology and Contemporary Critical Theory. Baltimore: Johns Hopkins University Press.
Lanham, R. (1993). The Electronic Word: Technology, Democracy, and the Arts. Chicago: University of Chicago Press.
Liu, Z. (2005). Reading behavior in the digital environment: changes in reading behavior over the past 10 years, Journal of Documentation. 61(6): 700-12.
Murray, J. H. (1997). Hamlet on the Holodeck: The Future of Narrative in Cyberspace. Boston: MIT Press.
O’Donnell, J. J. (1998). Avatars of the Word: From Papyrus to Cyberspace. Cambridge, MA: Harvard University Press.
Abidin, M. J. Z, M. Pour-Mohammadi, and O. Choon Lean (2011). The reading habits of Malaysian Chinese University Students. Journal of Studies in Education. 1(1):E9.
Ramirez, E. (2003). “The impact of the internet on the reading practices of a university community: the case of UNAM”, paper presented at the World Library and Information Congress: 69th IFLA General Conference and Council, Berlin, August 1-9.
Ross, C. S. (2003). “Reading in a digital age”, available at www.camls.org/ce/ross.pdf.
Overview
Some humanities scholars argue that the social web “poses a grave threat to the humanities because it lacks the depth, nuance and permanence that make genuine, meaningful interactions about the human condition possible.” They fear that “we risk becoming a world without the comprehensive communication tools needed to keep the humanities alive” (Adamek 2010). Even those who embrace social media all too often dismiss it as a tool that is used primarily for community engagement and self-promotion (Terras 2012). Yet the social web — which we broadly define as the array of technologies that allow individuals to post their thoughts, pictures, and comments in a public forum — when coupled with recent advances in cloud computing, data management and statistical/visual analysis offers significant potential to explore new and enduring humanities questions.
Through careful, rigorous analysis, we believe that the social web affords opportunities for the humanities to realize a contemporary, nuanced understanding of how the public believes our past informs modern society. For example, although there is a strong consensus amongst historians, the broader American public remains conflicted, divided, and confused about the causes of the Civil War. This is evident in any number of recent public polls, such as an April 2011 CNN poll which found that 42% of Americans believed that cause was something other than slavery, and the April 18, 2011 issue of Time, whose cover said “Why We’re Still Fighting the Civil War: The Endless Battle over the War’s True Cause Would Make Lincoln Weep” (CNN 2011; von Drehle 2011). Where polls provide a snapshot of Americans’ views, analysis of online discourse and social media activity affords opportunities to explore modern attitudes towards issues surrounding the Civil War.
Hence, against the backdrop of the on-going sesquicentennial commemorations, this paper examines the intersection of humanities, social sciences, social media, and computing to probe enduring questions around the legacy of the Civil War. To do so, we will illustrate how an ongoing study that uses the Civil War serves as a testbed for examining and developing techniques to conduct traditional types of humanistic inquiry in the context of the social web. Our results demonstrate how careful analysis of the online discourse that occurs across the social web enables deeper understanding of how the Lost Cause Ideology continues to recast the origins of the war and minimize the role of slavery. Simultaneously, we explore how stereotypes of Southerners continue to be propagated and used to shape a memory of the Civil War.
To conduct this study, we demonstrate how contemporary tools developed by industry may be used to analyse the social discourse around the Lost Cause and stereotypes of Southerners. Most scholars researching the social web rely primarily on the use of single keyword search and user-specified hashtags to narrow the size of their datasets to only posts that are immediately relevant to the topic under discussion (for example, Graham 2012; Ross, et al. 2010). However, these methods can be inadequate or problematic for understanding widely diffused social phenomena. For example, when attempting to study a topic such as the Civil War or especially a concept such as “the South” this methodology isn’t adequate as the majority of users do not tag their casual online conversations with these types of metadata nor do they restrict their conversations to a single platform (e.g., Twitter, Facebook etc.). To elicit a holistic view of conversations on the social web, the researcher has to employ strategies that reach across social media platforms and go beyond simple hashtag sets. To overcome these limitations, we leverage software originally developed for business analytics to aggregate content from across the social web (Radian6, 2012) — our search includes not just Twitter and Facebook, but also content from sources such as mainstream news sites, blog posts, and forum posts.
Method
Through our analysis of the social web, we highlight issues and opportunities for scholars who work at the intersection of the humanities, social science, social media, and computing. One issue that must be overcome is that, on a global scale, there are many Souths: not only must we find a way of eliminating references to countries or continents such as South Africa, South America, and the South Pole, we must also filter out conversations related to places such as the South Side of Chicago and even the television show South Park, none of which would be relevant for this research. Similarly, while most Americans would refer to the American Civil War as simply “The Civil War,” there are many similarly named historic and ongoing conflicts that we must filter from our results. The solution is to create “topic profiles” — collections of words that fall into one of three categories: words or phrases that must be present in a post to be included in the results; phrases that must be present along with words from the first category to help categorize results into different topics; and, finally, phrases that, if present in a post, will result in that post being excluded from our results. So, when conducting research into online discussions of southern gender ideals, we create a topic profile similar to the following:
•        1. Posts that CONTAIN any of the following: “the south”, “thesouth”, “southern”
•        2. AND CONTAINS any of the following: “belle”, “lady”, “gentleman”, …
•        3. But DOES NOT CONTAIN any of: “south pole”, “south park”, “south side”, “south africa”, …
By using keywords that were selected following a survey of users of the H-South discussion list, a request from leading scholars (including several former presidents of the Southern Historical Association, of the American South, surveys of a graduate and an undergraduate history class on Southern History at Clemson University, as well as a non- random sample of black and white southerners who were not academics, we have constructed topic profiles for five different themes that might be used to segment contemporary discussion of the South online: Southern Culture (which includes concepts such as honor, hospitality, and accents), Southern Food, Discussions of Gender, Religion, and Southern History. This set of keyword groups results in approximately 300,000 unique hits from the social web each month. Figure 1 illustrates the relative volume of each keyword group during September 2012 and shows how the number of hits varies for each topic across the month. Similar keyword groups have also been constructed for specific events that occurred during the Civil War, such as the attack on Fort Sumter, the Emancipation Proclamation, and the Battle of Gettysburg.
 
Figure 1:
Relative Volume of Five Topic Profiles for September 2012
Analysis
Drilling into this data reveals numerous insights into how users of the social web conceptualize the South and what topics are of interest. Figure 2(a), for example, presents a word cloud of the major terms associated with civil war commemorations, again for the month of September 2012. This word cloud immediately reveals several potential avenues for further investigation, including the centrality of Gettysburg to the online discourse surrounding the Civil War, but also suggests other useful topics to explore. Indeed, although word clouds have frequently been criticized for divorcing words from their context (Harris, 2011), we are able to maintain the connection between the words represented in a cloud and the underlying data. Therefore, if we are interested in further exploring the appearance of the word “history”, we can, as shown in Figure 2(b), drill further into the data to reveal new levels of insight to our topics. At all times, the original posts remain available both to view at an individual level to ensure the relevancy of content and to download for further textual and statistical analysis with dedicated software packages such as R and Gephi.
 
Figure 2:
(a) Word Cloud of Frequently Used Words in the Civil War Profile for September 2012; (b) The same word cloud, instead focused on the word “history”
We can also analyse demographic information, such as age, gender, and location, from the posts we have collected, allowing for a more nuanced understanding of our results. Figure 3 illustrates the location of users in the United States whose posts and tweets were collected from the Civil War topic profile over a seven-day period in mid- September 2012, while Figure 4 presents word clouds of the conversations that occurred in our Civil War topic profile on the 150th Anniversary of the Battle of Antietam by (a) people located in Maryland; (b) those in North Carolina; and (c) users aged between 36 and 45. These charts reveal not only the geographic distribution and volume of conversations, but also regional and demographic differences in the topic, tone, and scope of this online discourse.
 
Figure 3:
Geographic Distribution of Unique Hits from the Civil War Topic Profile in September 2012. A darker shade of orange indicates a greater number of posts from that state.
 
Figure 4:
Word Cloud of conversations that occurred on the 150th Anniversary of the Battle of Antietam by (a) people located in Maryland; (b) those in North Carolina; and (c) users aged between 36 and 45.
References
Adamek, D. (2010). Social Media Flaws and the Humanities. Valley Advocate, 13 December.http://www.valleyadvocate.com/blogs/home.cfm?aid=12863 (accessed 10 October 2012).
CNN Political Unit. (2011.) Civil War Still Divides Americans. CNN, 11 April.http://politicalticker.blogs.cnn.com/2011/04/12/civil-war-still-divides-americans/ (accessed 11 April 2011).
Graham, S. (2012). Mining the Open Web With Looted Heritage — Draft. Electric Archaeology, 8 June 2012.http://electricarchaeology.ca/2012/06/08/mining-the-open-web-with-looted-heritage-draft/ (accessed 15 October 2012).
Harris, J. (2011). Word Clouds Considered Harmful. Nieman Journalism Lab, 13 October.http://www.niemanlab.org/2011/10/word-clouds-considered-harmful/ (10 October 2012).
Radian6 (2012). http://www.radian6.com. accessed 10 October 2012.
Ross, C., M. Terras, C. Warwick, and A. Welsh. (2010). Pointless Babble or Enabled Backchannel: Conference Use of Twitter by Digital Humanists. in2010 Digital Humanities Conference. http://www.ucl.ac.uk/infostudies/claire-ross/Digitally_Enabled_Backchannel.pdf
Terras, M. (2012). The Impact of Social Media on the Dissemination of Research: Results of an Experiment. Journal of Digital Humanities, 1(3) http://journalofdigitalhumanities.org/1-3/the-impact-of-social-media-on-the-dissemination-of-research-by-melissa-terras/ (accessed 10 October 2012).
Von Drehle, D. (2011). 150 Years After Fort Sumter: Why We're Still Fighting the Civil War. Time, 18 April.http://www.time.com/time/magazine/article/0,9171,2063869,00.html (accessed 18 April 2011).
The Failure of the Digital Humanities
Mark Sample’s “Unseen and Unremarked On: Don DeLillo and the Failure of the Digital Humanities” argues that post-1922 literary texts are being left behind as a part of the Digital Humanities (Sample 2012). This is a direct result of the Sonny Bono, or “Mickey Mouse,” Copyright Term Extension Act, another apparent move towards perpetual copyright. These difficulties are compounded by other obstacles including closed access or disorganised archives, insufficient preservation tools for early computer usage, and authors who simply refuse to embrace the digital. Without the necessary permissions or archival material, scholars of these twentieth century scholars are becoming increasingly envious of their colleagues, who develop tools that would equally aid interpretation of these more recent authors. Mid-twentieth century literature is of particular relevance to Digital Humanities research, since many frequently cited precursors of electronic literature including Vladimir Nabokov’s Pale Fire (1962), Julio Cortázar’s Rayuela (1963), and the short stories of Jorge Luis Borges’sFicciones (originally published c.1960s), are still protected by the Sonny Bono Copyright Term Extension Act. Many of the theoretical issues that have been teased out of these texts — especially early hypertext theory (see Landow 1992; Bolter 1991; Joyce 2002) — perhaps can only truly be tested once many of these texts have been the subject of digital experimentation. This paper argues that although these projects are often not being carried out by faculty members, the need and potential uses for such tools among non-academic readers is demonstrated through the samizdat distribution of online versions and tools readily available for all those who wish to conduct a Google search. The launch of the first authorized Pynchon e-books (Flood 2012) was met with dismissive claims that better samizdat copies had been in circulation for many years beforehand. These projects are coming into fruition externally to traditional (digital) humanities departments, spreading out to computer scientists’ extracurricular projects or the work of those outside of the academy who build digital tools and resources for the love of the original literary artefact. A few examples of the diverse work being undertaken includes wikis (for authors such as Thomas Pynchon (Ware 2006) and Terry Pratchett (Anon. 2005)) databases (Finnegans WakeExtensible Elucidation Treasury (FWEET)) (Slepon 2005), interpretations of literary texts through social media on both a single platform, and a dense and complex ecosystem of literary engagement and reception (such as the recently organized group read of William Gaddis’s JR centralised around the Twitter hashtag #occupygaddis) and many other forms that demonstrate potential platforms for further research and development.
Literature Review
This study fits into a wider field of readership and reception studies, an interdisciplinary research subject, which has had some crossover within the Digital Humanities. Anouk Lang’s edited collection, From Codex to Hypertext: Reading at the Turn of the Twenty-First Century, which includes chapters on how reader recommendation systems are changing in the digital age (Wright 2012), the community of LibraryThing (Pinder 2012), and the network of reader reviews on Amazon (Finn 2012). Furthermore, the present study runs parallel to crowdsourcing in the Digital Humanities, most recently exemplified by the Transcribe Bentham Project (Causer and Wallace 2012), as many projects involve large numbers of volunteers to organize materials. Moreover, as Henry Jenkins et al. have recently suggested, the easy transmission and manipulability of media in the early twenty-first century is essential to ensure the text’s viability, and the evidence of fan communities exploring literary texts suggests a desire for these more of these platforms. (Jenkins, Ford, and Green 2013) There have also been more specific papers exploring the use of particular social network platforms for literary reception (see Schroeder and den Besten 2009; Ketzan 2012) and how the use of these tools reflect the development of underlying software through the way users build on the platform (Howison and Crowston 2011).
Do “Amateurs” Fit into the Big Tent of the Digital Humanities?
There has been a considerable debate concerning the purview of the Digital Humanities, particularly the extent to which building tools is essential to being described as Digital Humanities. (Svensson 2012) This paper asserts that the Big Tent should be widened to include a broader spectrum of scholars, amateur or professional, who engage with the transformational nature of digital tools, whether engaging with new methods of collaborating and presenting interpretive data or building databases to explore the manipulable nature of the original texts. These pockets of activity demonstrate a potential audience for these tools and push the boundaries of what counts as fair use in ways that academic institutions typically shy away from for fear of lawsuits. The deformative acts (Samuels and McGann 1999) these projects often engage in can thus reveal the ways in which these texts reflect a Digital Humanities agenda despite their marginalized status as both amateur projects and remediated texts (Bolter and Grusin 2000) still protected by copyright. Furthermore, there is evidence of the acceptance of these projects through examining the number of citations to some of the most prominent projects such as FWEET, which has been cited as both an exemplar of hypertextuality (Krapp 2005) and a reference guide for Joyce’s enigmatic text comparable to Roland McHugh’s authoritative Annotations to Finnegans Wake. (Conley 2007) Thus, we can witness how these projects engage with the academy.
Case Studies
The present study focuses on two case studies to illustrate the range of productivity that has engaged the non-Digital Humanities community for two twentieth-century authors: James Joyce and Vladimir Nabokov. These two authors represent polar opposites regarding their respective estates’ view of intellectual property rights and digital media. The Joyce estate has been involved in a couple of high profile copyright disputes leading to the dissolution of some major digital editions of Joyce’s work, most prominently, Michael Groden’s “Digital Ulysses.” On the other hand, FWEET, maintained by Raphel Slepon, a former medical researcher and programmer, runs counter to the usually aggressive policies of the Joyce estate. FWEET collates allusions from McHugh’s Annotations to Finnegans Wake (1980) and other major reference guides to Joyce’s novel, as well as material collected from a range of independent contributors, into a database which allows the user to sift through a taxonomy of references, view all the noted allusions on a line-by-line basis, or search for particular tropes. The original text is obfuscated by the database’s interface and thus the website acts as a reference guide primarily rather than a readable digital edition of the text.
Meanwhile, the Nabokov estate has occasionally granted the use of his texts for digital work despite taking an aggressive policy towards intellectual property rights in post-Soviet Russia. Two digital Nabokov projects have been sanctioned since 1967: Ted Nelson’s demonstration of Pale Fire as a hypertext in the late 1960s and Brian Boyd’s Ada Online. Alongside these official projects, there have been a plethora of hypertext experiments with the whole or parts of Pale Fire. These examples of remediation begin to explore the generative network of Nabokov’s most complex novel and demonstrate the novel’s effectiveness as a precursor of hypertext literature. Both case studies highlight how two respected authors’ works are being transformed by digital media without the intervention of digital humanists. Through careful study of the digital reception of the texts, we can not only learn how these texts are being transmitted and circulated by a popular audience, but also start to understand how these texts, currently protected by strict copyright laws, can and will be part of a wider Digital Humanities ecology.
References
Anon. (2005). Annotations — Discworld & Pratchett Wiki. http://wiki.lspace.org/mediawiki/index.php/Main_Page(accessed 30 October 2012).
Bolter, J. D. (1991). Writing Space: The Computer, Hypertext, and the History of Writing. Hillsdale: Lawrence Erlbaum Associates.
Bolter, J. D. and R. Grusin (2000). Remediation: Understanding New Media. Cambridge, MA: The MIT Press.
Causer, T., and V. Wallace (2012). Building A Volunteer Community: Results and Findings from Transcribe BenthamDigital Humanities Quarterly 6.2 http://www.digitalhumanities.org/dhq/vol/6/2/000125/000125.html.
Conley, T. (2007). Annotations to ‘Finnegans Wake’ (review). James Joyce Quarterly 1 (2). 363–366.
Finn, E. (2012). New Literary Cultures: Mapping the Digital Networks of Toni Morrison. In From Codex to Hypertext: Reading at the Turn of the Twenty-First Century, ed. Anouk Lang, 177–202. Amherst and Boston: University of Massachusetts Press.
Flood, A. (2012). Thomas Pynchon Finally Gives in to Gravity as Digital Backlist Is Published. The Guardian.http://www.guardian.co.uk/books/2012/jun/13/thomas-pynchon-digital-backlist-published (accessed 25 October 2012).
Howison, J., and K. Crowston (2011). Collaboration Through Superposition: How the IT Artifact as an Object of Collaboration Affords Technical Interdependence Without Organizational Interdependence. Institute for Software Research. Paper 491. http://repository.cmu.edu/isr/491 (accessed 30 October 2012)
Jenkins, H., S. Ford, and J. Green (2013). Spreadable Media: Creating Value and Meaning in a Networked Culture.New York and London: New York University Press.
Joyce, M. (2002). Of Two Minds: Hypertext, Pedagogy and Poetica. Ann Arbor: University of Michigan Press.
Ketzan, E. (2012). 'Literary Wikis: Crowd-sourcing the Analysis and Annotation of Pynchon, Eco and Others'. Digital Humanities 2012. held 16-22 July in Hamburg, Germany.
Krapp, P. (2005). Hypertext Avant La Lettre. In Hui Kyong Chun, W. and Keenan, T. (eds). New Media Old Media: A History and Theory Reader. 359–373. New York: Routledge.
Landow, G. P. (1992). Hypertext: The Convergence of Contemporary Critical Theory and Technology. Baltimore and London: The Johns Hopkins University Press.
McHugh, R. (1980). Annotations to Finnegans Wake. London: Routledge & Kegan Paul.
Pinder, J. (2012). Online Literary Communities: A Case Study of LibraryThing. In Lang, A. (ed). From Codex to Hypertext: Reading at the Turn of the Twenty-First Century, 68–87. Amherst: University of Massachusetts Press.
Sample, M. (2012). Unseen and Unremarked On: Don DeLillo and the Failure of the Digital Humanities. In Debates in the Digital Humanities, ed. Gold, M. K., 187–201. Minneapolis and London: University of Minnesota Press.
Samuels, L., and J. J. McGann (1999). Deformance and Interpretation. New Literary History 30(1). 25–56.
Schroeder, R., and M. den Besten (2009). Literary Sleuths Online: e-Research Collaboration on the Pynchon Wiki.http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1086671 (accessed 9 October 2012).
Slepon, R. (2005). Love’s Old Fweet Fong. FWEET. http://fweet.org/pages/fw_prlg.php (accessed 30 October 2012).
Svensson, P. (2012). Beyond the Big Tent. In Gold, M. K. Debates in the Digital Humanities. 36–49. Minneapolis and London: University of Minnesota Press.
Ware, T. (2006-). Thomas Pynchon Wiki — A Literary/Literature Wiki. http://pynchonwiki.com/ (accessed 30 October 2012).
Wright, D. (2012). Literary Taste and List Culture in a Time of ‘Endless Choice’. In Lang, A. (ed). From Codex to Hypertext: Reading at the Turn of the Twenty-First Century. 108–123. Amherst: University of Massachusetts Press.
The open-access, multimedia, peer-reviewed journal Southern Spaceswill soon begin its tenth year.1 Published by the Emory University Libraries, Southern Spaces is an online-only journal of critical regional studies that takes as its subject the real and imagined spaces and places of the US South and their global connections. From its years of encouraging digital cultural empowerment as a strategy to transform scholarly publishing, Southern Spaces offers a case study for inquiring into the effectiveness of a project intent upon increasing participation in the creation, dissemination, and curation of humanities scholarship. Given the entrenched commercial clout of conventional scholarly publishing and the slow-to-change institutional structures for tenure and promotion that still resist digital humanities scholarship, how can we assess the significance of this project of digital cultural empowerment? And how do the years of experience in publishing Southern Spaces contribute to an understanding of broader movements of cultural critique and social change?
Writing about the future of digital scholarly publishing in Educause Review Onlinein 2013, Edward Ayers points to several examples of “acceleration into a full, digital-only environment.”
"Scholars, libraries, and professional organizations in my own field of American history are sustaining innovations in online journals such as Southern Spaces and the Journal of Southern Religion and in digital meeting places such as Common-place and History News Network (HNN). These projects bridge traditional practice and digital possibilities in strategic ways. . . . Blogs and online conversations advance and deepen scholarly conversations, with their impact measured immediately in the number of downloads, views, forwards, comments, and tweets."
Ultimately, however, Ayers argues that outside of a limited number of examples, “the articles and books that scholars produce today bear little mark of the digital age in which they are created. Thus the foundation of academic life—the scholarship on which everything else is built—remains surprisingly unaltered.”2
Years after the emergence of open-access scholarly publishing platforms, the persistence of institutional inertias, rigid business models, and professional habits of judgment delay and thwart wider deployment of the rich media environment and continually expanding platforms of digital dissemination.3 Established academic print journals have little reason to change their modes of publication and few have done so.4 As a result, young scholars, especially in humanities disciplines, find themselves startled by atavistic attitudes. Consider the following excerpt from a letter I recently received from an untenured assistant professor about his journal publishing options:
"I have to share some unfortunate news with you about my contribution to Southern Spaces. I’ve recently been able to have a series of conversations with the new director of my program, who is also my advocate and advisor with regard to tenure and promotion—my review will begin next academic year. I discussed Southern Spaces with [my director] and while we both think that the online format and the multimedia potential of the format are important to the future of scholarship, her advice to me was that other members of my P/T committee have far more traditional views and would be reluctant to recognize Southern Spaces as a publication on par with a print journal. Needless to say, I don’t agree with that assessment, but I think I have to accept it."
Expressed apologetically, “needless to say” acknowledges the tempting, strong, and rising currents of digital publishing practice that are transforming scholarship by making its production more participatory and “free” (in the sense of free speech),5 and its reception widely available. Necessary to say, peer-reviewed Southern Spaces essays are used by scholars in a variety of successful professional efforts including tenure and promotion, applications for post-doc fellowships, and landing new kinds of jobs with digital libraries and regional centers.
As part of its intentionality, the collaborative process of producing an open access journal can create a training center where students acquire a range of skills—working with open source software, copyediting, map making, assisting writers and videographers in developing essays, creating technological tools, implementing layout and design—that are valuable in the new digital publishing environment. In a sense, this is a kind of “building scholarship,” in which “the interventions that occur as a result of building are as interesting as those that are typically established through writing.”6 If conceptualized broadly, open-access journal production should also involve a network of independent scholars; researchers associated with nonprofit, grassroots, and nongovernmental organizations; alt-academic writers; as well as the expected cast of college and university-based professors. Supporting this broad understanding of scholarship, Southern Spaces (and its social media tools of promotion, e.g., Facebook, Twitter, RSS) has served as an available publishing platform for community-based groups and regional writers engaged in contemporary research related to immigration, environmental destruction, and the crisis of public education. Alerting a networked, topical public about critical writing of interest can produce tens of thousands of readers, as evidenced by a Southern Spaces essay on the crisis of government in North Carolina.7
With persistent advocacy, the presence of a journal project can lead to institutional commitments for long-term archival preservation of the born-digital materials generated in the work of the journal and into mutually supportive intergroup alliances through organizations such as the recently created Library Publishing Coalition.
It is necessary to say, and to articulate in detail, the various practices of open access publishing that, taken together, comprise a strategy that extends beyond the academy into wider social networks. Drawing upon a decade of publishing practice, my proposed paper will identify and elaborate how a project such as the multimedia journal Southern Spaces works to advance cultural empowerment through digital design, creation, dissemination, and curation.
References

1. See southernspaces.org. ISSN 1551-2754.
2. Edward L. Ayers (2013), Does Digital Scholarship Have a Future? Educause Review Online, August 5 www.educause.edu/ero/article/does-digital-scholarship-have-future?utm_source=Informz&utm_medium=Email+marketing&utm_campaign=EDUCAUSE, accessed October 19, 2013.
3. On the social consequences of habits of judgment see Judith Butler, Giving an Account of Oneself (New York: Fordham University Press, 2005).
4. For an early statement on the prospects of change that electronic publishing might bring see Bill Kasdorf, “Guest Editor’s Gloss: Reflections on the Revolution,” Journal of Electronic Publishing, 3:4 (June 1998). quod.lib.umich.edu/j/jep/3336451.0003.401/--guest-editors-gloss-reflections-on-the-revolution?rgn=main;view=fulltext.
5. What is free software? GNU Operating System. www.gnu.org/philosophy/free-sw.html.
6. Stephen Ramsay and Geoffrey Rockwell (2012), Developing Things: Notes toward an Epistemology of Building in the Digital Humanities, in Matthew K. Gold, ed., Debates in the Digital Humanities (Minneapolis: University of Minnesota Press), 83.
7. Dan T. Carter, (2013). North Carolina: A State of Shock, Southern Spaces, September 24. southernspaces.org/2013/north-carolina-state-shock.
Smith, David
david.chan.smith@gmail.com
Wilfrid Laurier
Perceptions of falling enrollments and demands for closer
alignment with the labour market have placed the humanities
under pressure in North American higher education.[1] These
issues have been particularly pressing at mid-sized institutions
such as Wilfrid Laurier University in Ontario, Canada.[2]
Faculty at Laurier responded to this challenge by developing
a digital humanities program with its first course offerings
expected in 2014-2015. This paper discusses the initial design
of that program and its relationship to three major questions in
digital humanities pedagogy. First, should digital humanities
programs be structured around a common core of learning
objectives, or instead differentiate?[3] Second, what is the
relationship between digital humanities curricula and demand
in the workforce – should digital humanities programs be
designed to pursue indifferent academic knowledge or attempt
to engage more actively with vocational preparation? Finally,
should the teaching of digital humanities focus on specific skill
development, or instead cultivate “methodologies” or critical
perspectives on technology and its application?[4]
In addressing these questions, the paper adds the
experience of one institution to the ongoing conversation among
emerging programs.[5] Although it has been asked whether
only research intensive universities might have the expertise to
field digital humanities programs, at Laurier we were mindful of
the possibilities opened by a primarily pedagogical approach.
[6] To begin with, while the secondary literature’s discussion of
digital humanities research overshadows teaching, pedagogical
arguments can be crucial to attract administrative support.
[7] This is especially the case in faculties that are sensitive
to undergraduate enrollments and their volatility. Digital
humanities programs can present an attractive option for
students while demonstrating the continuing relevance of the
humanities to technological change.[8]
The Laurier curriculum is a program option that majors
or minors can include in their course of study. Structured
around a cluster of required courses, the curriculum allows
departments throughout the Faculty of Arts to add courses
as they are developed. Students use their elective courses to
design a specific pathway, including learning to program.[9]
The required courses expose students to historical analysis
and computer science. The purpose of the curriculum, broadly
conceived, is to improve students’ “digital literacy” or their
ability to find and analyze digital information, and to use digital
tools in active and creative ways.[10] Recent research has
noted that, “There is considerable evidence to support the
view that many students do not explore information in any
deep or reflective manner.”[11] Other authors have preferred
to emphasize student “multiliteracy,” arguing that literacy in
the digital age is a broad concept, and reflects fluency with
and access to a broad range of representative forms, such as
visual or audio media.[12] A narrow concentration on traditional
textual literacy, it is argued, misses the scope of literacy in
a connected, technologically saturated world.[13] Though
some commentators also worry that young people are being
transformed into passive recipients of digital media, others
argue that technology opens their creative potential in blogs
and other formats.[14] All agree, however, with the necessity
of transforming students from “consumers” of digital content
into “creators.”[15] The literature has also argued that digital
humanities can develop students’ critical skills as they engage
with complex digital information on the web and elsewhere.[16]
Mindful of the growing significance of digital literacy, the
Laurier program prompts students to realize the challenge
of using and gathering “deep data,” rather than relying on
data returned from basic Google searches. Throughout the
curriculum students use methods from history to interpret
textual information, and weigh and contextualize evidence.
This approach connects a qualitative layer to the quantitative
and analytic skills learned from computer science.[17] For
example, the program’s foundation course introduces students
to the possibilities of big data. Using existing queries and
code they investigate familiar data sources, such as Twitter
and Google. They then use knowledge from the humanities
to contextualize and shape that data. Students are asked to
consider the limitations of digital information and how to make
data meaningful: what socially significant questions might
they ask of it? During the final project students communicate
their findings using digital media. The course attempts to
demonstrate to students that the familiar digital universe they
inhabit can reveal surprising discoveries with the right tools.
At Laurier three factors shaped the development of the
program: concerns over costs, an increasing emphasis
on differentiation within the Ontario university system, and
the challenge of engaging faculty who had a pre-existing
knowledge of the subject. These pressures demanded the
program leverage existing institutional strengths.[18] For
example, without funds to support new hires, the program
Lausanne, Switzerland
359
was by necessity an interdisciplinary effort among the faculty
already working in the digital humanities. Consequently, their
knowledge directly affected what was initially possible within
the program.[19] As we developed the program we realized
that it was possible for these perceived drawbacks, such as
lack of faculty expertise concentrated in a research cluster, to
become strengths. In response, our program became not only
interdisciplinary, but a scaffold for faculty to build their expertise
and advance their knowledge through teaching.
These factors shaped the curriculum so as to differentiate
it within the Ontario system at a time when such diversity
is becoming a compelling trend in higher education. As
universities attempt to communicate their distinctiveness to
applicants, digital humanities programs can benefit by their
alignment with the institution’s academic identities.[20] In the
case of Laurier, this tilted the curriculum towards business, one
of the university’s strongest areas. Our experience suggests
explicit differentiation is not only the preferable strategy, but
also perhaps a necessity given the resource constraints and the
dynamics of higher education in North America.
The development of the Laurier program was also related
to specific data about the job-market. Whether humanities
programs should explicitly adopt a vocational orientation
has been a subject of pyretic debate.[21] A curriculum that
trains students primarily to investigate academic problems
in the humanities might be especially suitable for research
universities. At Laurier, however, we shaped the curriculum in
consultation with the Career Centre to advance our students
in post-graduate employment more directly. Like it or not,
many students and applicants are preoccupied with the job
prospects associated with their major.[22] Among employers,
we have learned, there is concern that graduates in arts will
be intimidated or flummoxed by even basic tasks using digital
tools. These misgivings might be unfounded, but the Laurier
program explicitly cultivates digital literacy to equip students for
knowledge employment in the future.
For example, the program builds on Laurier’s strength in
business administration to provide an entry point into the
burgeoning field of analytics. This focus is especially important
since the university is located in a region with large technology
and insurance sectors.[23] The curriculum exposes students
to big data problems beginning in the foundation course,
while prompting them to think about the social meaning and
application of this information by drawing on knowledge
from the humanities. A stream may be added to educate
undergraduates specifically in big data and analytics.
An emphasis on employable experience is also reflected
in the experiential and co-op learning integrated into the
curriculum.[24] Students, to give one example, can receive
course credit for work at the Laurier Centre for Military Strategic
and Disarmament Studies. They will undertake projects
such as digitizing and making the Centre’s archival holdings
publically searchable. These kinds of work opportunities, which
combine training in the humanities with digital work, are not only
pedagogically desirable, but meet our students’ demand for coop
experience in the humanities.
Among the hardest questions to answer is whether digital
humanities courses should teach a defined set of skills over
more broadly conceived methodologies. In balance is the
preference among employers that new hires should already
have a minimal level of job-related training, yet programs
focused on imparting specific skills risk narrowness.[25] Instead
we have embraced the concept of digital literacy: students
should have broad facility with digital work, and be confident
and able to self-learn or advance their training in specific areas.
[26] For example, the exposure during the foundation course
to code does not teach them coding, but rather demonstrates
how code works and its limitations. Students choosing to
specialize can take advanced programming electives. However,
all students should leave the program with at least enough
understanding to customize off-the-shelf tools.
Can the digital humanities draw attention to the vitality of
the humanities? Part of doing so may be to demonstrate that
the humanities have much to offer the digital economy. Moving
forward at Laurier, we intend to conduct a more thorough
investigation into the relationship between the digital humanities
and the contemporary workplace. By proceeding with this
research we accept that the humanities can be more explicitly
oriented to post-graduate employment and the challenges of
“knowledge work.” Though we live in a time of doubt about the
humanities, such strategies may instead reveal that this is a
moment of renewed vigor.
References
[1] Jennifer Levitz and Douglas Belkin, Humanities Fall
from Favor, The Wall Street Journal (June 6, 2013), A3; Tamar
Lewin, As Interest Fades in the Humanities, Colleges Worry,
The New York Times (October 31, 2013), A1. For an articulate
discussion of the falling numbers of humanities concentrators
at a leading research university see, David Armitage, Homi
Bhabha et al., The Teaching of the Arts and Humanities at
Harvard College: Mapping the Future, (Cambridge, MA, 2013);
and the analysis of Anthony Grafton and James Grossman,
The Humanities in Dubious Battle , The Chronicle of Higher
Education, July 1, 2013; Contrarian views are offered by
Michael Bérubé, The Humanities, Declining? Not According
to the Numbers, The Chronicle of Higher Education, July 1,
2013; and Robert Townshend, Clio's Charm Holding Fast?
Perspectives on History (October, 2012).
[2] The author’s home institution; its full-time undergraduate
population is approximately 15,400.
[3] Lisa Spiro, Opening up Digital Humanities Education, in
Digital Humanities Pedagogy, pp. 338-39.
[4] Simon Mahony and Elena Pierazzo, Teaching Skills or
Teaching Methodology? in Brett Hirsch (ed.), Digital Humanities
Pedagogy: Practices, Principles and Politics (2012), pp.
215-225; at p. 215.
[5] See, for example, the informal survey of Tanya Clement,
Multiliteracies in the Undergraduate Digital Humanities
Curriculum: Skills, Principles, and Habits of Mind, in Digital
Humanities Pedagogy, pp. 365-88, at pp. 376-384.
[6] Bryan Alexander and Rebecca Frost Davis discuss the
challenges of digital humanities education in liberal arts schools
and the resource limitations in Should Liberal Arts Campuses
Do Digital Humanities? in Matthew K. Gold, Debates in the
Digital Humanities (Minneapolis, 2012), pp. 368-89.
[7] Tanya Clement, Multiliteracies in the Undergraduate
Digital Humanities Curriculum, pp. 366, 370-71.
[8] Luke Waltzer, Digital Humanities and the ‘Ugly
Stepchildren,’ of American Higher Education, in Gold, Debates,
pp. 335-349, 341-42; Stephen Brier, Where’s the Pedagogy?
The Role of Teaching and Learning in the Digital Humanities,
in Gold, Debates, pp. 390-401, at 390-1; and Alan Liu, Digital
Humanities and Academic Change, English Language Notes
47 (2009), pp. 17-35. Recent discussion of undergraduate
pedagogy includes, T. Mills Kelly, Teaching History in the
Digital Age (Ann Arbor, 2013) and throughout Brett Hirsch
(ed.), Digital Humanities Pedagogy. Attempts have been
made to identify the learning outcomes of digital humanities
courses, see the report of Joanna Drucker and John Unsworth
on the NEH Digital Humanities Curriculum Seminar at
jefferson.village.virginia.edu/hcs/dhcs/intro_syllabus.html.
[9] The role and necessity of coding skills in digital humanities
program has been a subject of recent discussion, see
Interchange: The Promise of Digital History, The Journal
of American History 95:2 (Sept. 2008), pp. 452-487, at pp.
459-67; Tanya Clement, Multiliteracies in the Undergraduate
Digital Humanities Curriculum, p. 369; Stephen Ramsay,
Programming with Humanists: Reflections on Raising an Army
of Hacker-Scholars in the Digital Humanities, in Hirsch (ed.),
Digital Humanities Pedagogy, pp. 227-239.
[10] Tanya Clement, Multiliteracies in the Undergraduate
Digital Humanities Curriculum, p. 366. A recent report in the
United Kingdom by a panel of higher education administrators
and faculty defined “information literacy” as “activities such as
search, retrieval and critical evaluation information from a range
of sources, and also its responsible use form the point of view
of attribution.” David Melville et al., Higher Education in a Web
2.0 World (March, 2009), p. 34.
[11] Peter Williams and Ian Rowlands, The Literature on
Young People and their Information Behaviour, work package
II (October, 2007), pp. 17-20, at 19. Other writers have gone
Digital Humanities 2014
360
further, suggesting that the very process of web interaction
and the structure of its information negatively affects users,
Nicholas Carr, Is Google Making us Stupid? What the Internet
is Doing to Our Brains, The Atlantic (July/August, 2008). Daniel
Cohen and Roy Rosenzweig examine historical accuracy on
the web in Web of Lies? Historical Knowledge on the Internet,
First Monday (December, 2005). See also Mark Bauerlein,
The Dumbest Generation: How the Digital Age Stupifies Young
Americans and Jeopardizes Our Future (2008). See also the
National Endowment for the Arts, Reading at Risk: A Survey
of Literary Reading in America, Research Division Report #46,
eds. Tom Bradshaw and Bonnie Nichols (Washington, 2004).
[12] New London Group, A Pedagogy of Multiliteracies:
Designing Social Futures, Harvard Educational Review 66:1
(1996), pp. 60-92.
[13] Ibid., p. 61.
[14] Tanya Clement, Multiliteracies in the Undergraduate
Digital Humanities Curriculum, pp. 375-76.
[15] Melville et al., Higher Education in a Web 2.0 World, pp.
22-3.
[16] Tanya Clement, Multiliteracies in the Undergraduate
Digital Humanities Curriculum, p. 366.
[17] The program also prompts students to reflect on the use
of large-scale historical data, issues that have been explored
by William Thomas III, Computing and the Historical
Imagination, in Susan Schreibman, Ray Siemens et al.
(eds.), A Companion to Digital Humanities,(Oxford, 2004),
pp. 56-68, esp. p. 65 for a discussion of the future digital
needs of historians. Frederick Gibbs and Trevor Owens also
suggest that the availability of “big data” requires the historical
profession to rethink its methods and hermeneutics, see The
Hermeneutics of Data and Historical Writing .
[18] Melissa Terras, Julianne Nyhan and others have
recently compiled differing perspectives on the digital
humanities in Defining Digital Humanities: a Reader (Surrey,
UK, 2013).
[19] Undergraduate programs can be capacity building by
attracting resources to digital humanities. Spiro, Opening up
Digital Humanities Education, p. 332.
[20] James Bradshaw, Specialize or Risk Losing Funding
Ontario tells Universities and Colleges , The Globe and Mail,
September 18, 2013.
[21] In the United Kingdom, Jisc has sponsored projects
considering the linkages between digital literacy and
employment through the Developing Digital Literacies
Programme. Examples include the University of Greenwich
Digital Literacy in Higher Education Project, and the Digitally
Ready project at the University of Reading.
[22] The recent Gallup-Lumina survey reported that 90%
of respondents among the general public believed that the
“candidates college or university major” was either “somewhat”
or “very important” to hiring decisions. Only 70% of “business
leaders” who were asked responded similarly. The 2013 Lumina
Study of the American Public’s Opinion on Higher Education
and U.S. Business Leaders Poll on Higher Education (February
25, 2014), pp. 18, 29
[23] Andrew McAfee and Erik Brynjolfsson, Big Data: The
Management Revolution, Harvard Business Review (October,
2012), p. 62; Jonathan Shaw, Why ‘Big Data’ is a Big Deal:
Information Science Promises to Change the World, Harvard
Magazine (March-April, 2014), pp. 30-35; 74-75; Claire Miller,
Data Science: The Numbers of Our Lives , The New York
Times, April 11, 2013; Thomas Davenport and D.J. Patil, Data
Scientist: the Sexiest Job of the 21st Century, Harvard Business
Review (October, 2012), pp. 70-1; Alexandra Stevenson,
New Silicon Valley Fund to Back Big Data Start-Ups , New
York Times Dealbook, October 17, 2013. Claire Miller, Data
Science: The Numbers of Our Lives, The New York Times ,
April 11, 2013.
[24] 14% of business leaders surveyed in 2014 reported
that internships or practical experience would “best prepare
graduates for success in the workforce.” This was the most
popular response (excluding “Don’t know/refused”). The 2013
Lumina Study, p. 30. For discussion of such “participatory”
educational opportunities (or their lack in the humanities) see
Alexander and Frost Davis, Liberal Arts Campuses, p. 380;
Spiro, Opening Up Digital Humanities Pedagogy, p. 352; Cathy
Davidson has called for a “core curriculum to create engaged
entrepreneurs,” here.
[25] Skills may also, without use, atrophy unlike patterns of
critical thinking, see Mahony and Pierazzo, Teaching Skills or
Teaching Methodology? p. 224.
[26] Jisc, Learning Literacies in a Digital Age , September 7,
2009.
The use of social media tools within digital archaeology helps create an engaging setting for archaeological content, which integrates archaeology into a broader social context of use by connecting scholars, archaeological heritage professionals, and the wider public. Social media offers various opportunities: researchers may find it useful for discovering, using and sharing information, organizations may use it in promoting institutional agendas and communicating with wider audiences, lay people may see it as a platform for participation, and more. More broadly, social media is transforming ways in which we perceive information, with its use becoming an increasingly important skill for researchers. This paper attempts to address issues of social media usage in digital archaeology through the case study of studying Lithuanian archaeology practices on Facebook.
The study of social media use in archaeology is still new as a topic of research, because social media haven't been around long enough to develop clear patterns of use. However, particular research questions, as well as answers, are emergining (e.g., Morris, 2011; Whitcher Kansa & Deblauwe, 2011; Pett, 2012; Richardson, 2012; Sanchez, 2013); nevertheless, discussions of social media in archaeology are still more often discussed in conferences and seminars, and also on blogs, forums, etc. Authors typically acknowledge the importance of social media, and point to successful examples providing evidence that it stimulates communication between researchers, helps information sharing and reaching wider audiences, as well as fosters community engagement and social participation. However, the diversity of existing practices opens new research questions, transcend disciplinary boundaries and challenges established authority structures.
The research project presented here is a case study of archeaological communication on Facebook (currently the most popular site for digital social networking in Lithuania), based on analysis of empirical data from thirty Lithuanian Facebook groups and pages related to archaeology. The study depends on a mixed methods approach, combining digital ethnography, content analysis and social network analysis aspects. Initial analysis revealed that overall activity relies on engaged communities rather than on research institutions, or custodian archaeological organizations, considered to be directly responsible for the creation and curation of digital archaeological content. The scope of the research covers, therefore, a wider landscape of observable social media practices, by actors including not only research organizations or professional networking groups, but also semi-formal or informal groups. Its objective is to map and understand existing trends, and to provide further insights about new phenomena that emerge from these kinds of interactions.
The paper investigates Facebook profiles of individual users (archaeologists, amateurs) and organizations, specific activities they engage in such as posting, commenting, liking, sharing, etc., and the content that is shared within the network. It seeks to address questions arising from this case study, as well as develop insights for broader research issues, such as:
• Who is using social media in archaeology, and for what reason and purpose? What are the qualitative traits, and in depth profiles, of of the most effective users? What is the nature of the shift towards public archaeology and community engagement practices? What is the role of individual archaeologists in social media? Could Facebook contribute to research proper, or be used for academic purposes?
• Do social media shape and change the content itself? How do people use and make sense of these resources? What are the most common kinds of archaeological objects that people share, like and comment on Facebook., and why? How is content influenced by the complex relation between archaeological heritage and society? What is the balance between expert knowledge and amateur perspectives?
• More generally, are we fully aware of the opportunities and challenges brought by social media? What additional value does communication among individuals and institutional structures create? How does this kind of synergy improve knowledge transfer? Does it empower organizations, or people? How is communication carried out? What is the structure of interactions between users? In what way is Facebook-based activity shaped to satisfy the needs of its users?
This paper will, firstly, provide an overview of Facebook use in archaeology by focusing on three core dimensions: users, content and communication. It will then present a detailed composition of Facebook users in Lithuanian archaeology, in an attempt to understand the position of archaeological institutions and archaeologists in social media, as well as reasons for the lack of participation as the case study suggests. Furthermore, it will describe the main types and subject-matter of current digital archaeological content, and discuss how user responses and interactions could influence the way in which we conceptualise and interpret the past. Finally, the paper will present and compare different cases of archaeological Facebook use, and will examine in what manner archaeological heritage operates in digital social media, how it serves institutional and individual needs, and what criteria could enable successful communication.
References

Morris, J. Zoobook (2011). Archaeologist connecting through social media. The SAA archaeological record, Vol. 11, No. 1 (January).
Whitcher Kansa, S. & Deblauwe, Francis (2011). User-generated content in zooarchaeology: exploring the “Middle Space” of scholarly communication. In E. Kansa et al. (Ed.) Archaeology 2.0: New Tools For Communication and Collaboration. USA: Cotsen Institute of Archaeology.
Pett, D. (2012) Use of Social Media Within the British Museum and the Museum Sector. In Ch. Bonacchi (Ed.) Archaeology and Digital Communication: Towards Strategies of Public Engagement. London: Archetype Publications.
Richardson, L. (2012) Twitter and archaeology: an archaeological network in 140 characters or less. In Ch. Bonacchi (Ed.) Archaeology and Digital Communication: Towards Strategies of Public Engagement. London: Archetype Publications.
Almansa Sanchez, J. (2013) To be or not to be? Public archaeology as a tool of public opinion and the dilemma of intellectuality. Archaeological Dialogues, Vol. 20, No. 1 (June).

CUbRIK and the History of Europe App

The integration of human expertise and machine computation enables a new class of applications with significant potential for the digital humanities. So far this potential remains largely untapped due to the severe requirements of such projects: The implementation and integration of advanced algorithms requires specialized know-how and the final users from the humanities are challenged with defining unprecedented tasks for methods which haven’t emerged yet. The FP-7-funded research project CUbRIK (www.cubricproject.eu) implements and integrates research in computer science, the design of human-computation tasks, data visualization, social engineering and the humanities.
In the proposed presentation we would like to showcase one of CUbRIK’s case studies, the demo of the History of Europe application. The application introduces an effective interface to access collections of historical sources and to discover links among and entities within them. Upon completion CUbRIK will offer an innovative approach to human-enhanced time-aware multimedia search by synthesizing research in computer science, crowdsourcing and gamification. We will conclude the presentation with an outlook on the future development of the application.
Humanist-machine interaction

The History of Europe (HoE) application is based on a curated collection of more than 3000 images, representing the main events and actors in the history of the European integration. The collection is curated and hosted by the Centre Virtuel de la Connaissance sur l’Europe (CVCE). In a first step, an image indexation pipeline identifies the location of individual faces in the photographs. The location of these faces is verified by a crowd of “click-workers” with no specific training who evaluate for each recognized face if the depicted image shows a human face or not. Following the face verification process, an automatic face recognition process is triggered that associates each of the now verified faces with a list of ten possible identities. This list of candidates is then disseminated for example through Twitter to a crowd of experts that vote and comment for their preferred identity.
Besides the identities of the different persons, all information that is associated to an image, such as the time or the place where the image was taken as well as contextual information about associated historical events can be reviewed by expert users and delegated to a crowd of domain experts for review.
Data aggregation, visualisation and analysis

Building on the computed co-occurrence of persons in images a social graph is constructed that connects them with each other. Connections gain in strength the more often persons appear together in an image. Finally the result of this process is depicted in a visualization of the social graph with a set of analytical tools.
The social graph in the History of Europe App aims at representing and visualizing dependencies between historically relevant persons in the context of European integration. Thereby the weight of the (social) links between person entities relies on their co-occurrence in historic photographs as identified by the aforementioned image indexation process. The more frequently two persons appear in different photographs, the stronger the link between the corresponding entities in the graph.
Users can interact with the History of Europe social graph in different ways, e.g. a click on a node results on an ego-graph of the selected person and clicking on an edge displays documents that relate to both selected relationship. As the documents stored in the collection very often come with a date of creation, the graph can be filtered by date with the timeline, displaying only the connections of documents created within this timespan. This timeline also shows the amount of photos per date that are contained in the collection. Another filtering option is the number of connecting documents, which allows the visualization of those relationships that are only included in an interval of a minimum and maximum number of documents. This feature is useful to highlight highest co-occurrences. Finally, the number of appearances of a person in the processed collection lets us identify people who appear particularly often in any given time frame.
Crowd discussion and a new approach to the representation of truth in digital research tools

Another challenge for the HoE app and the domain of the Digital Humanities in general is the conception of truth, which differs significantly e.g. to the conceptions of truth in Computer Science. Computer Scientists can rely on a stable foundation of what is true: Any experiment can be replicated and measured precisely. In the humanities the concept of truth is far more complex: It is based on the insight, that there is no neutral or objective way to study human environments. The way, in which questions are asked, how data is selected to answer them, by what means this data is analyzed and finally the way in which the results of such analyses are communicated and received all challenge the idea of “one truth”.
In order to represent the discursive nature of truth in the humanities within HoE we make use of a community-driven tool for question answering, similar to stackoverflow.com. User have the opportunity to answer questions and thus benefit from the knowledge within the expert crowd. However, the system allows for more than one answer and offers its users the possibility to vote and answer up or down, thereby allowing more than one answer to enter in competition with each other whilst also maintaining the full spectre of the discussion.
Summary and outlook

The History of Europe application takes on the challenge to combine cutting edge research in the domains of computer science, the design of human-computation tasks, data visualization, social engineering and the humanities by identifying synergies between the disciplines’ strengths and by compensating for their weaknesses. We do this by building a pipeline which connects face recognition tools, data visualization and input from humans and creates an ongoing cycle of iteratively improved user input and machine output. The History of Europe application stands in line with a range of other online tools for historical research but introduces new social features as well as crowd sourcing from both click-workers and expert users which continuously improves the system. In the future we will expand the selection of sources to include digitized text documents as well as audio and video interviews from different archives.
1. Framework of thought and specific purposes 

The processes of assigning value to cultural objects, as well as the establishment of the canons which derive from those processes, have constituted until today one of the intellectual, ideological and political foundations of the development of Art History discipline as an institutional discourse (Halbertsma, 2007). This explains why the critical dismantling of the concept of canon as a structure of power, criterion of authority and legitimizing argument represented a significant line of inquiry for the post-structuralism and, more recently, for the postcolonial theory (Parker and Pollock, 1981; Bloom, 1994; Perry and Cunningham, 1999; Gorak, 2001; Bart, 2005; among others). Especially, it has been emphasized the need to bring out a critical awareness of the multiplicity and heterogeneity that define the processes of assigning  value and meaning to objects on the basis of the variety of cultures, genders, races and territories.  It has been stated that, in our global world it is essential to understand the concepts of canon and value in terms of plurality and difference. It has also become necessary to explore the specific idiosyncrasies of those processes as a mean to make recognizable and significant that diversity.  
Within this framework of thought, the so-called ‘digital turn’ offers to us another scenario of critical analysis to rethink these issues from the perspective of the new conditions of the digital society, which is modeled by the prevalence of the software, and it is characterized by the potentiality of interactivity, user-generated content, and – at least in theory - global access to and massive distribution of cultural images and objects.  This is the intellectual background of my proposal. 
As a response to the theme of Digital Humanities 2014 (Digital Cultural Empowerment), I propose to explore how the digital turn, which has brought a new model of society, economy, culture, and a new epistemology (i.e. new ways of production, narration, distribution and consumption of knowledge), is leading to a redefinition of the processes of assigning values -and the values themselves- that have hitherto prevailed in the comprehension of cultural objects within the Art History discipline, resulting in new forms of canonization. 
My approach is inspired to the current Digital Humanitie’s thought which proposes to rethink the circumstances and the consequences of this 'new' disciplinary field from the perspective of the cultural critique (Lothian and Phillips, 2013; Dacos, 2013; Galina, 2013; Fiormonte, 2012; Liu, 2012; McPherson, 2012; Higgin, 2010, among others). The field of Digital Humanities is becoming aware that there is a real risk of perpetuating in the digital world and in the practice of digital scholarship the same problems of marginality and subalternity that characterized our pre-digital world. In the field of Art History this trend is represented by the super-imposition of specific canons for the understanding and explanation of artistic phenomena. A critical approach to Digital Humanities requires a review of both established and new structures of power that are emerging.However, although the field of artistic culture is one of the most affected by these new processes, the critical discourse is still in its embryonic stage within the context of Digital Art History studies. In my opinion, there is an urgent need to conduct a thorough analysis from the perspective of critical theory. My scope would be to develop such a perspective, unveiling and questioning what kind of art-historical discourses and narratives, and what kind of digital artistic culture we are building on the web (Rodríguez Ortega, 2013).
Now then, we must bear in mind that the building of the digital artistic culture, and the growth of the emerging Digital Art History itself are defined by a dialectical tension between the new processes of assigning value and the maintenance of those traditional structures that had characterized the development of Art History discipline during the Twentieth century [1] (Baca, Helmreich and Rodríguez Ortega, 2013; Kohle, 2013). Examining this tension is a complex task, since these practices and criteria are simultaneously interlaced and in confrontation.  Any inquiry must be based then on a dual question: a) we must scrutiny what is really changing in the digital medium in regard to the processes of assigning value to cultural objects, and to what extent these new processes are entailing a destabilization of the traditional criteria of Art History’s institutional discourses; in short, the aim is to explore to what extent the Art History discipline and its allied institutions (Museum, Art Criticism, Market, etc.) are being put in crisis as argument of authority; b) perhaps more importantly, we must be aware that, while these changes –sometimes very visible- are taking place, the logic that governs the processes of assigning value based on institutional policies and established power structures is maintained, as well as it is preserved the canons  that characterized the critical and conceptual definition of artistic objects and images during the twentieth century –essentially, Western, white and male. 
2.  Defining hyper-canonization and de-canonization processes 

For this presentation, I will focus on two of theses processes, which are related to the conceptualization of the social web as the new laboratory of cultural production. In this scenario, new actors, hitherto completely unrelated to the traditional ecosystem of Art History (Academy - University, Museums, Critique, Market), arise and perform, fostering a paradoxical redefinition –paradoxical due to its ambivalence- of the traditional concepts of canon and value. 
  Firstly, I would like to address the process that I propose to call ‘hyper-canonization’ since this type of process superimposes and at the same time encompass the traditional ones. Therefore, as indicated above, a regime based on institutionalism and authorial power structures remains. The challenge lies, then, in determining which are such arising power structures and who has the ability to control them.  
In part we can associate this process to the rise of software oligopolies and social networks companies (Google, Facebook, Twitter, Apple, Microsoft, etc.) [2], which belong to the same Western and Anglophone economic-cultural context. They control the technological infrastructures, the algorithms for data processing and retrieving, the channels for content distribution and the social interactions platforms that are used by cultural institutions to interrelate with their audiences (see, for example, the massive presence of museums in social networks as inexcusable part of their communication policies and activities). This indisputable technological and economic supremacy can lead us to new forms of digital colonialism and new cultural monopolies. Some of them are obvious. From my point of view, one of the clearest cases is represented by the Google Art Project, whose declared objective is to become the global gate for accessing the entire collections of museums worldwide. Nevertheless, the philanthropic mission of providing a comprehensive and free access to the objects of world culture underlies the threat that the museum identity can get lost on the web. Each museum, as a differentiated institution, is defined by certain discursive strategies, intellectual positions and critical criteria. However, these signs of identity could dissolve if the collections would be seen preferably ‘through’ Google. Not surprisingly, it is frequent to find that museums’ websites use Google Art Project among their recommended and authorized information sources. Consequently, museums themselves are participating in this process of legitimating Google – the Google Cultural Institute - as a new institutional discourse. 
Others are less obvious, but equally disturbing. For example, despite all digital archives and online catalogs developed by public and private institutions, the largest digital images archive and the most accessible is – let admit it - Google Images. Google Images establishes a hierarchy of the images retrieved based on computational procedures that run according to algorithms completely unrelated to the epistemological, aesthetical, historical and/or symbolic specificities of artistic artifacts. Thus, the software, whose conceptualization has nothing to do with these specific aspects, assumes the power of the decision making when ‘ordering’ the images of our cultural heritage. 
‘De-canonization’ is the name that I propose for the second process that I want to address in this presentation. This process emerges directly from the social and distributed users’ interactions with the cultural images and objects on the web. Under my perspective, what is in crisis here is the concept of ‘canon’ itself, because of its bottom-up orientation which dissolves the idea of canon understood as the institutionalization of specific values representing the ideas and interests of those that hold a sort of privileged position of authority (intellectual, economic, political, etc.)
This process is linked to the unprecedented empowerment of social communities to interact with and give new meanings to cultural artifacts through their multiple, heterogeneous, and distributed digital activity.  It is thus set up a new scenario that unfolds outside the institutional frame, and whose processes of assigning value are governed by very different criteria [3]. Hence, social memory, subjectivity, emotionality, etc. become fundamental factors for the re-semanticization of cultural objects and for their relocation in new scales of value. This new context involves a disruption of the principle of authority in the Art History discipline and its allied institutions, which comes into confrontation with these actions in a double way: or ignoring them, or appropriating them. 
In fact, the appropriation of the logics of participation and sharing that characterize the web 2.0 is the basis of the so-called 'social museum' (Simon, 2010). Nevertheless, these actions bring about another problematic issue on which we need to reflect critically.  Certainly, the valuable social knowledge found in the users’ interactions have already been recognized by projects that advocate for a hybrid knowledge (expert plus non-expert), which may result in a new process of assigning value and in a new canonization model. See for example, Your Painting (http:/www.bbc.co.uk/yourpaintings), a project based on the social tagging of British paintings (Baca, 2013), or the History Harvest (http://historyharvest.unl.edu), an open digital archive of historical artifacts collected by various communities through the United States, which are systematized and prepared for research and interpretation by a group of scholars.  While recognizing the positive aspects of these initiatives, some questions arise   To what extent the institutions are appropriating these logics of participation and sharing in order to subsuming them as part of their institutional discourses and canons? To what extent are we facing a phenomenon of ‘domestication’ and an attempt to attract the outsiders to the 'center', establishing a sort of 'controlled' framework for their activities, such as perturbing sometimes for the institutions?
3. Open questions: What are facing?  

I will conclude with a set of open questions that underlie this approach and that should be discussed in depth in following studies: To what extent the Art History discipline is possible outside an institutional framework? Is that condition an argument to explain the need for operating an institutionalization of the digital environment, which is, by nature, open, distributed, and multiple? To what extent the discipline of Art History and its allied institutions are willing to share their position of authority, at least consciously? And to what extent they are aware that they are yielding this position to new structures of power?Recently, James Cuno (President and CEO of the Getty Trust) wondered from a postcolonial perspective: Who owns the past? (Cuno, 2013). Now, I think, it is the time to ask: Who owns the value and the canon in the digital realm? Who has the ability to assign value to cultural objects and images?  Who holds now the authority and power to establish the new canons and legitimizing discourses in the context of digital society?  
 
Notes
 
[1] We should not forget that the dialectic tensions and contradictions have been defining factors in the development of the Art History discipline since its early beginnings (Donald Preziosi. Rethinking Art History. Yale University Press, 1989). Therefore, the challenge now is to examine which are the new factors that participate in this process. 
[2] Regarding the new inclusion-exclusion regimes associated to the software oligopolies, see  Juan Martín Prada. Prácticas artísticas e Internet en la época de las redes sociales, Madrid: Akal, 2012. 
[3] As examples, see the following projects: www.Bdebarna.net; or www.cabanyalarchivovivo.es/index.html. Both initiatives are based on the appropriation by social communities, belonging to a specific territory, of the cultural heritage related to such territory, using for that digital infrastructures and strategies. The objective is to give them –both cultural heritage and territory- new meaning and value, and rethinking them from the point of view of the social memory and collective interests. 
References

Baca, Murtha, Anne Helmreich and Nuria Rodríguez Ortega (2013). Digital Art History. Special double issue of Visual Resources. An international Journal of Documentation. vol.  XXIX (1-2), march-june.
Baca, Murtha (2013). “The Public Catalogue Foundation’s Your Panting Project”, Visual Resources, XXXIX (3), 151-153
Bart, J. M. van der Aa (2005). Preserving the Heritage of Humanity? Obtaining World Heritage Status and the Impacts of Listing. Groningen: University Library Groningen.
Bloom, Harold (1994). The Western Canon: The Books and School of the Ages. New York: Harcourt Brace.
Cuno, James (2013). “Who Owns the Past? Encyclopedic Museums in the Post-Colonial Present”, in Luis Arciniega, ed., Memoria y Significado. Uso y recepción de los vestigios del pasado, Valencia: Universidad de Valencia, 215-227
Dacos, Marin (2013). “La stratégie du Sauna finlandais”, in Blogo Numericus, Mayo 2013. Disponible en: http://blog.homo-numericus.net/article11138.html [octubre 2013]. 
Fiormonte, Domenico (2012). “Towards a Cultural Critique of Digital Humanities”, Historical Social Research – Historische Sozialforschung, Special Issue, no. 141, HSR vol.37 (2), 59-76. Disponible en: http://www.cceh.uni-koeln.de/files/Fiormonte_final.pdf [octubre 2013].
Higgin, Tanner (2010). “Cultural Politics, Critique and Digital Humanities”, in Gaming the System, 25 Mayo de 2010. Disponible en http://www.tannerhiggin.com/cultural-politics-critique-and-the-digital-humanities/ [octubre 2013].
Galina, Isabel (2013). Is There Anybody Out There? Building a global Digital Humanities community. Keynote speech. Digital Humanities Annual Conference, Nebraska (USA). Disponible en: http://humanidadesdigitales.net/blog/2013/07/19/is-there-anybody-out-there-building-a-global-digital-humanities-community/ [octubre de 2013].
Gorak, Jan, ed. (2001). Canon vs Culture: Reflections on the Current Debate. New York: Garland.
Halbertsma, Marlite (2007). “The call of the canon. Why art history cannot do wituhou”. In Elizabeth Mansfield, ed., Making Art History. A changing discipline and its institutions, New York and London: Routledge, 16-30
Kohle, Hubertus (2013). Digitale Bildwissenschaft. Glückstadt: Verlag Werner Hülsbusch.
Lothian, Alexis and Amanda Phillips (2013), "Can Digital Humanities Mean Transformative Critique",  Journal of E-Media Studies 3.1. 
Liu, Alan (2012). “What is Cultural Criticism in the Digital Humanities?”, in Mathew K. Gold, ed., Debates in the Digital Humanities, University of Minnesota Press. Disponible en:  http://dhdebates.gc.cuny.edu/debates/text/20 [octubre 2013]. 
McPherson, Tara (2012). “Why are the Digital Humanities so White?”,  in Mathew K. Gold, ed., Debates in the Digital Humanities, University of Minnesota Press, 2012. Disponible en:  http://dhdebates.gc.cuny.edu/debates/text/20 [octubre 2013].
Parker, Roszika and Griselda Pollock (1981). Old Mistresses: Women, Art and Ideology. New York: Pantheon.
Perry, Gill and Colin Cunningham (1999). Academies, Museums, and Canons of Art. New Haven, CT: Yale University Press in Association with Open University.
Rheingold, Howard (2002).  Smart Mobs. The Next Social Revolution, Cambridge: Basic Books
Rodríguez Ortega, Nuria (2011). «Narrativas y discursos digitales desde la perspectiva de la museología crítica», Museo y Territorio, Fundación General de la Universidad de Málaga, n. 4, 14-29
Rodríguez Ortega, Nuria (2013). “Digital Art History: An Examination of Consciencie”, Digital Art History. Special double issue Visual Resources. An international Journal of Documentation. Edited by Murtha Baca y Anne Helmreich and Nuria Rodríguez Ortegavol. Vol. XXIX (1-2), march-june 2013, 129-133.
Simon, Nina (2010). The Participatory Museum, Museums 2.0, Santa Cruz.
A number of recent initiatives within the DH community promote the design, development, and implementation of digital tools aimed at speeding up, clarifying, or otherwise improving the research practices of humanities scholars. This year, the One Week | One Tool (OWOT) summer institute, funded by the National Endowment for the Humanities, resulted in the creation of Serendip-o-matic, a serendipity engine for digital research. This tool relies on users to feed it a selection of text or citations in order to create a list of keywords, which it then uses to find related information. The documents returned are taken from the Digital Public Library of America (DPLA), Europeana, and Flickr1. The participants of the 2013 OWOT initiative are not alone in their quest to design a digital tool geared toward enhancing the chance encounter with information, resources, ideas, research materials, and even people. Tim Sherratt, the manager of Trove at the National Library of Australia, often includes an element of chance in the tools he designs for use in the humanities. For instance, in his tool Trove News Bot, Sheratt (2013) allows users to interact with a Twitter stream by sending tweets with directions (such as #luckydip), which will return random results from the National Archives of Australia’s digital collection2. Similar tools have been developed that introduce serendipity into the collections of the DPLA and the British Library.
One motivation for the development of digital tools aimed at enhancing serendipity in digital environments comes out of the need to redesign and recreate the complexity of the research environment found in library stacks and archival collections. It is often argued that this complexity may be lost in digital environments, which are highly predictable and primarily based on keyword search. To what extent serendipity is reduced in digital search is debatable. Nonetheless, this perception of loss directly affects how scholars, and in particular humanities scholars, adopt and use digital tools. A study of historians’ research practices suggests that these scholars are skeptical of conducting their research exclusively in digital environments because they lack the ability to encounter key resources (primary and secondary materials) that could have a major impact on their research findings 3. In this study, the authors also found that historians were willing to experiment with digital tools, if these could recreate opportunities for encountering information. Hence, scholars perceive the discovery of resources, browsing, and chance encountering as central elements of their research practice that can, and need to, be supported online.
Outside of academia, a number of tools have emerged that try to introduce serendipity into the online experience. What is less clear from the literature is how to best support this process, as a wide range of approaches have been suggested ranging from interactions in social media 4, exploration in non-search related digital environments, and information search in digital environments 5. The approach most commonly taken is to introduce serendipity into the online information search experience; this is often done by introducing some element of randomness and thereby reducing the predictability of search results. An example of this approach is BananaSlug, which returns random results to a search query. Other approaches include reversing or modifying the ranking in which search results are presented online 6. This would draw attention to a different set of items because users commonly tend to investigate only the first and perhaps second pages of search results. All of these approaches aim at broadening “the search space, promoting encounters with items that might not, under existing algorithms, come to the attention of the user”. While the majority of digital tools aimed at promoting serendipity have emerged outside of the humanities, a series of tools have recently been developed with humanists in mind. These tools have garnered considerable attention in the field, but it remains unclear what element of serendipity they support. Part of the problem is the fact that the concept of serendipity is elusive 7 and difficult to pinpoint. Reducing it to the introduction of randomness, however, does not seem to be the best way to move forward, even though it is the one most commonly utilized. A second problem, and perhaps more concerning, is that scholars need to first understand that serendipity is not a one-dimensional concept but, rather, includes a number of related facets, which need to inform tool design and implementation. The present paper critically examines four DH tools that encourage serendipitous results and attempts to place these within current models of serendipity:
Serendip-o-matic (http://serendip-o-matic.com/)
Trove News Bot (https://github.com/wragge/trovenewsbot)
Mechanical Curator (http://mechanicalcurator.tumblr.com/)
DP.LA Bot (https://github.com/wragge/trovenewsbot).
As a basis for this examination, we have established the main facets of serendipity obtained from the extensive literature in Library and Information Science (LIS). Through this comparative study, we aim to accomplish two goals. First, there is a gap in understanding exactly what aspects of serendipity digital tools support. By merging the literature in LIS with tool design in DH, we hope to create greater clarity as to what aspects have been supported. Second, the results of the study will determine what future developments are needed to better support the work of humanists in digital environments.
Interviews with 20 history scholars inform the first phase of this study. These scholars indicated a desire for serendipitous encounters with material to remain a part of their research process after the integration of digital texts to their work. After discovering that historians were seeking new methods of information acquisition online, further interviews were conducted with DH scholars to see what methods they were using to browse information. The results of these two sets of qualitative data will be discussed and used to demonstrate a need for a serendipity tool within the DH community.
The second phase of this research is an in-depth exploration of the four information-discovery tools listed above. These tools will be examined in terms of Erdelez’s (2004) model of information encountering outlined below 8. After analyzing each tool carefully, follow-up interviews will be conducted with the creators of each tool to discuss their intentions for and reflections upon, the use of the tool by humanist scholars.
A wide range of models of serendipity have been developed relying on very different data sets and assumptions. Erdelez (2004) developed one of the first models and emphasized the experience of information encountering (IE), which she defined as a type of opportunistic acquisition of information. Erdelez’s (2004) utilized an experimental setting, where participants were asked to look for information related to a foreground problem and the researcher observed how they would react to information related to a background problem. As part of her model, Erdelez (2000) identified five elements:
noticing: the perception of encountered information;
stopping: the interruption of the initial information seeking activity;
examining: the assessment of usefulness of the encountered information;
capturing: the extraction and saving of the encountered information for future use;
and returning: the reconnection with the initial information seeking task.
In Erdelez’s (2004) model, a person is primarily focusing on the information needs related to a foreground problem. However, cues related to another problem, a background problem, may catch the person’s attention. If the person notices the cues and stops to examine the newly encountered information, then there is an opportunity for discovering unexpected resources. It is this process of noticing, examining, and capturing that digital tools try to emulated or support.
Each of the four tools reflects one or more aspects of the serendipitous process as outlined by Erdelez, (see Table 1).
  Noticing Stopping Examining Capturing Returning
Serendip-o-matic     ✓   ✓    
Trove News Bot   ✓   ✓   ✓   ✓  
Mechanical Curator   ✓   ✓   ✓ some  
DP.LA Bot ✓ ✓ some    
The tools listed above, with the exclusion of Serendip-o-matic, select materials randomly and then present these to followers on Twitter. Randomness, as we know, does not necessarily mean that serendipity will occur. These tools all provide links to places that users can go to receive extraneous materials in the hopes that something of interest will come their way.
Interestingly, the capturing element of these tools seems to be largely disregarded. Considering the DH community is acutely aware of the need to instantly capture digital documents and the associated metadata with citation tools (Zotero), none of the examined tools includes this element in their framework. This leads the authors to conclude that future design could focus on this element of capturing information, and could introduce a method that allows for the saving of documents so users can retrace their footsteps after returning to the initial task or foreground problem. Our critical analysis of various DH tools and how they support serendipity provides opportunity to further enhance these tools as well as a means to design additional tools that can impact the research practices of humanities scholars. 
References

1. CHMN. (2013). Serendip-o-matic: Let your sources surprise you. One Week | One Tool. Retrieved October 31, 2013, from serendip-o-matic.com/about
2. Sherratt, T. (2013). Conversations with Collections. discontents. Retrieved October 31, 2013, from discontents.com.au
3. Martin, K., & Quan-Haase, A. (2013). Are e-books substituting print books? Tradition, serendipity, and spportunity in the adoption and use of e-books for historical research and teaching. Journal of the American Society for Information Science and Technology. 64(5), 1016-1028.
4. Bogers, T., & Björneborn, L. (2013). Micro-serendipity: Meaningful coincidences in everyday life shared on Twitter. In Proceedings of iConference (pp. 196–208).
5. Quan-Haase, A., Burkell, J., & Rubin, V. L. (n.d.). The role of serendipity in digital environments. In Encyclopedia of Information Science and Technology. IGI Global.
6. Jansen, B. J., Spink, A., & Saracevic, T. (2000). Real life, real users, and real needs: A study and analysis of user queries and on the web. Information Processing & Management, 36(2), 207–227.
7. Merton, R. K. (2004). The travels and adventures of serendipity: a study in sociological semantics and the sociology of science. Princeton, N.J: Princeton University Press.
8. Erdelez, S. (2004). Investigation of information encountering in the controlled research environment. Information Processing & Management, 40(6), 1013–1025. doi:10.1016/j.ipm.2004.02.002
9. Erdelez, S. (2000). Towards understanding information encountering on the Web. In Proceedings of the 63rd annual meeting of the American Society for Information Science (pp. 363–371). Medford, N.J.: Information Today.

John Dewey writes in Schools of To-Morrow: “Unless the mass of workers are to be blind cogs and pinions in the apparatus they employ, they must have some understanding of the physical and social facts behind and ahead of the material and appliances with which they are dealing.” This remark is not unlike the image Fritz Lang depicts at the outset of the 1927 film Metropolis: slaves to a machine becoming food for the machine. The danger in fetishizing machines is that we become subject to them. But turning away in the face of the digital will lead to much the same fate. Rather, we need to handle our technologies roughly -- to think critically about our tools, how we use them, and who has access to them.

Like Digital Humanities, Digital pedagogy has been variously defined. Brian Croxall and Adeline Koh offered a very inclusive, broad-stroke definition at their MLA Digital Pedagogy Unconference, saying that “digital pedagogy is the use of electronic elements to enhance or to change the experience of education.” And Katherine D. Harris offered up the components of her digital pedagogy -- which she borrows in part from the “mainstays of Digital Humanities” -- during a NITLE seminar on the subject: “collaboration, playfulness/tinkering, focus on process, building (very broadly defined).”

Digital pedagogy is an orientation toward pedagogy that is not necessarily predicated on the use of digital tools. This is why I like Harris’s focus on process and Croxall and Koh’s use of the seemingly vague, but in fact quite lovely, phrase “electronic elements.” The phrase dissects the notion of an educational technology, turning the discussion to a consideration of the smallest possible element that might influence teaching and learning: the electrical impulse. At this level, we’re not talking about how we might use Wordpress in a composition class, or how Smart Boards failed to revolutionize K-12 education, but about how the most basic architecture of our interactions with and through machines can inspire new (digital or analog) pedagogies. Thus, Kathi Inman Berens says paradoxically that “the new learning is ancient.”

Many have argued that the digital humanities is about building stuff and sharing stuff -- that the digital humanities reframes the work we do in the humanities as less consumptive and more curatorial, less solitary and more collaborative. I would argue, though, that the humanities have always been intensely interactive, an engaged dance between the text on a page and the ideas in our brains. The humanities have also always been intensely social, a vibrant ecosystem of shared, reworked, and retold stories. The margins of books as a vast network of playgrounds.

The digital brings different playgrounds and new kinds of interaction, and we must incessantly ask questions of it, disturbing the edge upon which we find ourselves so precariously perched.And what the digital asks of us is that every assumption we have be turned on its head. The digital humanities asks us to pervert our reading practices -- to read backwards, as well as forwards, to stubbornly not read, and to rethink how we approach learning in the digital age.

In fact, the course itself is one of our central texts, a collection of stories about reading and writing, that can be actively hacked and remixed. Sean Michael Morris writes, “A course today is an act of composition,” an active present participle and not a static container. This is more and more true of courses that live online, which demand that we carefully examine the digital as a frame, while recognizing that the digital does not supersede and can never unseat the work we do in the world. Kathi Inman Berens writes, “It doesn’t matter to me if my classroom is a little rectangle in a building or a little rectangle above my keyboard. Doors are rectangles; rectangles are portals. We walk through.” This is where learning happens, at the breaking point of its various containers.

This is true just as well of the literary texts we analyze (and ask students to analyze) with digital tools. In the syllabus for a recent undergraduate seminar in the digital humanities, I pose the following questions:

How is literature and our reading of it being changed by computers? What influence does the container for a text have on its content? To what degree does immersion in a text depend upon the physicality of its interface? How are evolving technologies (like the iPad) helping to enliven (or disengage us from) the materiality of literary texts?
Literature, film, and other media are changing, and the way we interact with them is also changing. As we imagine a digital approach to the humanities, we must look back even as we look forward, considering what media has become while we simultaneously examine the hows and whys of its becoming. We used to watch films only in a darkened theater without the distraction of other external physical stimuli. Increasingly, though, we watch film on hand-held digital devices, many with touch screens that allow more and more interaction with the content. Our apparatuses for media-consumption juxtapose digital media, literature, and film: Now, we watch Ridley Scott’s Alien in a window alongside Twitter and Facebook. Film no longer exists as a medium distinct from these other media.

The same is true of new modes of reading. Digital texts invite (or allow) us to do other things with our eyes, brains, and bodies as we experience them. As I write this, I have 9 windows open on my computer, each vying for my attention. Some of these windows have several frames in further competition. Advertisements. E-mail. Documents. Widgets. Social-networking tools. Chat interfaces. Each of these layers has an effect on how I engage the digital text. In spite of all these layers, I don’t think we experience a decreased attention; rather, the digital text demands a different sort of attention. Even as my direct engagement is challenged, my brain is offered more fuel for making connections and associative leaps. A proactive approach to online and digital pedagogy asks us to put these associative leaps to  work. So, Twitter and FaceBook may be a distraction, but that distraction can be harnessed for good pedagogy. 

Social media can function as a site for democratic participation, a leveled playing field, a harbinger for another sort of attention. The keenest analysis in the digital humanities is born of distraction and revels in tangents. The holy grail of this work is not the thesis but the fissure.

Breaking Stuff as an Act of Literary Criticism
The digital humanities is about breaking stuff. Especially at the undergraduate level, this is the work of the digital humanities that most needs doing. Mark Sample proposes “what is broken and twisted is also beautiful, and a bearer of knowledge. The Deformed Humanities is an origami crane -- a piece of paper contorted into an object of startling insight and beauty.” And, by the end of a class, if it’s successful, this is what becomes of the syllabus, the texts, the assignments, and us. Sample continues, “every fact is a fad and print is a prison. Instructors are insurgents and introductions are invasions.” In this way, my digital humanities courses work to violently dismantle fact and print, instructors and introductions, and I revel together (and part and parcel) with students in both discovery and uncertainty.

The digital humanities course I teach for undergraduates has as its first assignment the breaking of something as an act of literary criticism. Specifically, I ask students to take the words of a poem by Emily Dickinson, “There’s a certain slant of light,” and rearrange them into something else. They use any or all of the words that appear in the poem as many or as few times as they want. What they build takes any shape: text, image, video, a poem, a pile, sense-making or otherwise.

This paper expands upon a brief article I wrote about this assignment, analyzing several of the resulting student works and exploring the new pedagogies that the digital humanities demand and give rise to. 

References
Dewey, John (1915). Schools of Tomorrow. Montana: Kessinger Publishing.

Dickinson, Emily (1960). “There’s a certain slant of light.” The Complete Poems. Ed. Thomas H. Johnson. Boston: Little, Brown and Company.

Harris, Katherine D. (2012). “NITLE Digital Pedagogy Seminar.” http://triproftri.wordpress.com/2012/03/27/nitle-digital-pedagogy/ (accessed March 7, 2014).

Inman Berens, Kathi (2012). “The New Learning is Ancient.” New Media Curious. http://kathiiberens.com/2012/12/03/ancient/ (accessed March 7, 2014).

Koh, Adeline and Brian Croxall (2013). “What is Digital Pedagogy?” http://www.briancroxall.net/digitalpedagogy/what-is-digital-pedagogy/ (accessed March 7, 2014).

Morris, Sean Michael (2012). “Courses, Composition, Hybridity.” http://www.seanmichaelmorris.com/courses-composition-hybridity/ (accessed March 7, 2014).

Sample, Mark (2012). “Notes Towards a Deformed Humanities.” Sample Reality. http://www.samplereality.com/2012/05/02/notes-towards-a-deformed-humanities/ (accessed March 7, 2014).
Open access refers to the free access to and reuse of scholarly works. Peter Suber, who was the principal drafter of the Budapest Open Access Initiative (February 2002), and authored the book titled Open Access (2012), defines it as academic literature that is “digital, online, free of charge, and free of most copyright and licensing restrictions.”

What proportion of peer-reviewed digital humanities research is published in open access journals? What proportion of digital humanities monographs and edited collections are available in electronic formats, and which, if any, are available in open access form? How do open access articles about the digital humanities compare in terms of citations/downloads to their toll-access counterparts? How, when, where and why do digital humanities scholars and the public engage in online attention to online academic articles about digital humanities and why does it matter? What kind of licensing and copyright agreements are digital humanities scholars subscribing to, and of those which ones allow and encourage open collaboration and reuse, including text and data mining? What is the role of blogging in the digital humanities publishing landscape?

These are the questions guiding the research project whose findings we will visualise through an infographic. It will show the findings of a comparative, quantitative bibliometric analysis of a data set of academic articles about the digital humanities published between 2010 and 2013. The infographic will visualise the conclusions of an ongoing collaborative research project whose aims are to employ journal and article-level quantitative and qualitative analysis to determine whether alt-metrics can provide a holistic image of impact on diverse audiences.

The poster will also include a visualisation of the geographical distribution of online attention to the articles published on both journals, as well as other quantitative and qualitative data. The main objective of the poster will be to provide demographic data of online activity reflecting the attention paid to digital humanities research by other researchers, the media and the general public, providing much-needed data about where academic articles on digital humanities are published, which are the business models the chosen platforms have (toll-access, open access) as well as other information as presence or absence of digital identifiers, secure archiving, etc.

Scholars in most academic fields are increasingly using online tools and environments (social media, blogs, online reference managers, etc.) to engage with scholarly literature and other events such as lectures, conferences and symposia (Nicholas and Rowlands 2011). Digital Humanities scholars are not the exception (Ross, Terras, Warwick, Welsh, 2011; Terras 2012), but there is a paucity of bibliometric research regarding the type of publications and impact of those publications that they choose to publish their research. In spite of the extensive work by the Statistical Cybermetrics Group (University of Wolverhampton), digital humanities as a specific field of academic publishing remains largely unexplored. The poster we propose seeks to make a contribution by employing alt-metrics, the quantitative and qualitative “study and use of scholarly impact measures based on activity in online tools and environments” (Adie and Roe, 2013; Cameron 2009; Cronin 2001; Priem et al 2012) to assess the publishing landscape in digital humanities. The data about research online engagement we can obtain from them is discipline-agnostic; it is the online behaviour of researchers and interactions with the outputs from different disciplines what can significantly differ.

The poster seeks to make a contribution to the debate about the role of open access and alternative metrics in contemporary research. The poster will be accompanied by an open access online resource including further analysis and the source data, encouraging fellow researchers to explore, reuse and visualise in different ways. This companion site will discuss how alt-metrics data could potentially contribute to –or eventually generate a culture towards– strengthening the evidence informing impact case studies for journals publishing digital humanities scholarship.

Many questions arise from looking at the data. How can we better understand and use it? Can we classify articles and journals by the kind of attention they get? Are there common patterns between themes? How do they compare to articles and journals in other disciplines? Can online attention metrics encourage specific types of online behaviour amongst digital humanities scholars and across disciplinary What does it mean if somebody tweets a paper -what’s the tweeter trying to do? How can the studied journals maximise the online engagement with the research they publish?

This poster and its companion online site will aim to provide some answers in order to provide recommendations and best practices that might help democratise and increase the international access to peer-reviewed digital humanities research. 

References
Adie, E., and William R. (2013) "Altmetric: Enriching Scholarly Content with Article-Level Discussion and Metrics." Learned Publishing 26, no. 1 11-17. WEB. http://figshare.com/articles/Enriching_scholarly_content_with_article_level_di scussion_and_metrics/105851. Accessed 28 October 2013.

Cameron N, and Shirley W. (2009) "Article-Level Metrics and the Evolution of Scientific Impact." PLOS Biology 7, no. 11 e1000242. WEB. http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1000242. Accessed 28 October 2013. 

Chamberlain, Sc. (2013) "Consuming Article-Level Metrics: Observations and Lessons." Information Standards Quarterly 25, no. 2 , 4-13. http://www.niso.org/publications/isq/2013/v25no2/chamberlain

Cronin, B (2001). "Bibliometrics and beyond: Some Thoughts on Web-Based Citation Analysis." Journal of Information Science 27, no. 1, 1-7.

Nicholas D, Rowlands I. (2011). Social Media use in the Research Workflow, Information Services and Use 31(1-2, 2011): 61-83

Ross, M., Terras, C. Warwick, A. Welsh, (2011) "Enabled backchannel: conference Twitter use by digital humanists", Journal of Documentation, Vol. 67 Iss: 2, pp.214 – 237

Priem, J., Groth, P. and Taraborelli, D. (2012). "The Altmetrics Collection." PLOS ONE 7, no. 11 (2012): e48753. WEB. http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.004 8753. Accessed 28 October 2013.

Statistical Cybermetrics Group. University of Wolverhampton, United Kingdom. Publications. WEB. http://cybermetrics.wlv.ac.uk/publications.htm. Accessed 28 October 2013.

Suber, P. (2012). Open Access. (Cambridge, MA: MIT Press). WEB and print. http://mitpress.mit.edu/books/open-access Accessed 28 October 2013.

Terras, M. (2012). “Infographic: Quantifying Digital Humanities”. Melissa Terras’ Blog. 20 January 2012. WEB. http://melissaterras.blogspot.co.uk/2012/01/infographic-quanitifying-digital.html. Accessed 28 October 2013.

Terras, M. (2012). “Is blogging and tweeting about research papers worth it? The Verdict.” Melisa Terras’ Blog. 3 April 2012. WEB. http://melissaterras.blogspot.co.uk/2012/04/is-blogging-and-tweeting-about-research.html. Accessed 28 October 2013.
Where exactly is the place of digital humanities to be in undergraduate education? If, indeed, 21st century universities must begin to prepare students for professional work in which digital familiarity, skills, and facility are increasingly central, where is the site of responsibility for that training? It could be disciplinary or interdisciplinary, located within the curricula and pedagogies of existing departments or relocated to the information sciences, library, or core liberal education curriculum.

Joining a chorus of scholars considering the place of undergraduate pedagogy and digital humanities, including those on DH2013's "The Future of Undergraduate Digital Humanities" panel, this presentation will detail the highly collaborative creation and facilitation of "How To Do History," a course offered by Donna Gabaccia. In its old form, the course was one of the mainstays of upper-level offerings by the University of Minnesota's History department, serving as a way to prepare students' Senior Thesis and, implicitly, prepare these students for graduate school in History; it has at least temporarily became a venue for students explorations of, and contributions to, digital history in a disciplinary context unofficially known as "How To Do (Digital) History."

The core collaboration is between the presenters, respectively, a well-established Professor of History (Gabaccia) and the Digital Humanities Specialist for the University of Minnesota Libraries (Schell). In the summer of 2013, we met to discuss not just the content of the class (readings, assignments, etc), but also what kinds of support the Library could make available to students to actually make projects (rather than, as in the past, preparing to write individual research papers). By circumscribing the options available to them (limiting them to a few of the main digital humanities tools and methods, i.e. mapping, Omeka, digital storytelling), it made both the technologies, and the projects that could be created with those technologies, much more accessible. In addition to creating public projects, the students engaged critically with reframed historiographical questions (i.e. the writing and rewriting of history through Wikipedia) and digital literacy (critically examining Twitter and other online platforms of communication and how they relate to scholarly discourse), reading, among others, Cohen (2006)1, Rosenzweig (2011)2, Kelly (2013)3, and Nawrotszi and Dougherty (2013)Nawrotszi, K. and Dougherty, J. (2013). Writing History in the Digital Age. Ann Arbor: University of Michigan Press. 4.

While collaboration was essential to the undergraduates’ creation of digital projects, graduate students also became partners in the course. Instead of Gabaccia trying to supervise the seven groups of undergraduate students, she allowed graduate students to enroll in their own section of the course. In addition to doing further readings with her regarding the intellectual lineage and stakes of digital history and digital humanities (e.g  Gold 2012)5, the graduate students served as project managers for the undergraduate groups, getting valuable experience in facilitating collaborative projects and various digital humanities tools and methods, neither of which are normally part of their History graduate work.

A further collaborative aspect to this version of "How To Do History" was extensive collaboration with the University of Minnesota Libraries. While many subject librarians will come once during the semester to introduce research methods, subject-specific database, and other source materials, Schell attended nearly every class, doing multiple presentations about specific ideas and technologies, ranging from introductions to significant digital history projects over the last 15 years to demonstrations of specific tools. In addition, he met multiple times individually with both graduate students (to help construct a blogging assignment for the undergraduate students) as well as the collaborative project groups, helping to refine the scope their projects to make them manageable as a single semester of work.

Finally, one project group worked specifically with the Upper Midwest Jewish Archives, part of the Libraries' Archives and Special Collections, creating an Omeka exhibition around previously unprocessed and undigitized materials. The group looked at the lives of two Jewish men in the early 20th century, through two World Wars, work in the printing industry, and global travel, all set against the backdrop of vicious anti-Semitism in Minneapolis, characterized at that time as the most anti-Semitic city in the United States. A second group created a web feature for the University’s James Ford Bell Library about changing cartographical representations of Scandinavia by mapmakers in the premodern world.

The lessons from “How To Do History” do not end with the completion of these collaborative, public digital history projects. Reflecting on the course after its completion, we realized that, due to the project- and group-based environment, many students considered “middle of the road” in their skills and interests developed more research and technological skills than in previous iterations of the class taught by Gabaccia. While most students are not opting for a digital Senior Thesis, instructors have relayed to us anecdotally that they see a greater preparedness and skill in terms of research in those who took the digital version of “How to Do History” than other versions of the course. Furthermore, as we noted above, the graduate students who supervised and facilitated the undergraduate projects gained valuable experience that opened new directions for their own graduate work in terms of research and instruction. Reactions at the departmental level have been mixed.  The History Department proudly featured the student work on its webpage but also continued its ongoing internal debate about the future of HIST 3959--is the course necessary? Could methodologies other than digital history be featured?  Senior members of the department also continued to express skepticism about their ability to evaluate digital work; the department has responded by offering a future department-wide workshop on that issue.

The knowledge gained from organizing and teaching the course can inform the development of similar courses in different departments (e.g., How To Do Digital Sociology, Anthropology, Ethnomusicology, to name a few). Schell’s position in the Libraries facilitates that possibility. Digital literacy is a critical element to any undergraduate education, regardless of discipline, and especially if one seeks to receive graduate training in that field. Integrating these digital humanities lessons within each discipline helps students engage more deeply in the development of critical inquiry and with the specific transformations underway in these fields. Furthermore, it creates opportunities for transdisciplinary education, as the digital tools and methodologies students used to create these projects are not just limited to digital history projects. Whether this be part of an emerging “digital humanities” cohort or a broader idea of “digital studies,” it allows for collaborative relationships where extra-disciplinary institutions, such as Libraries, are essential partners.

References
1. Cohen, D. (2006). Digital History: A Guide to Gathering, Preserving, and Presenting the Past on the Web. Philadelphia: University of Pennsylvania Press.

2. Rosenzweig, R. (2011). Clio Wired: The Future of the Past in the Digital Age. New York: Columbia University Press.

3. Kelly, T., (2013). Teaching History in the Digital Age. Ann Arbor: University of Michigan Press.Nawrotszi, K. and Dougherty.

4. Nawrotszi, K. and Dougherty, J. (2013). Writing History in the Digital Age. Ann Arbor: University of Michigan Press.

5. Gold, M., ed. (2012). Debates in Digital Humanities. Minneapolis: University of Minnesota Press.
1. Introduction
A community of Digital Humanities in Spanish and Portuguese (HD*) has been consolidating over the past few years. Due to a series of events gathering numerous colleagues and taking place in diverse latitudes, 2013 has been a turning point. A milestone was the first DíaHD, which brought together about a hundred practitioners and showed that HD scholarship is highly active and eager to build a cohesive community. The purpose of the event was “to identify and establish or improve networks and collaborative work among the community of digital humanists in Latin America, the Caribbean, and the Iberian Peninsula, as well as digital humanists in other regions of the world whose work is done in Spanish or Portuguese” (translation ours)1. Our project, MapaHD originated that day and has embraced the bilingual profile established by that event. MapaHD is an exploration of the features and intersections among those who self-identify as HD practitioners and their characteristics beyond language affiliation. In our paper, we provide insights into issues of temporal development of HD, geographic location, interdisciplinary practices and approaches, and how progressively a community of digital humanists has been taking shape. The development of MapaHD has been made public from its beginnings at mapahd.org where we have gathered visualizations and preliminary results. Simultaneously, the data collected has been used to build an interactive and exploratory map using DARIAH-DE Geo-Browser that is also available through our website.

1.1. Overview
MapaHD is a direct address to the question launched by Domenico Fiormonte, “Is there a non Anglo-American Digital Humanities, and if so, what are its characteristics?”2. In this project we have gathered and analyzed practitioners’ data that evidences not only the existence of a thriving DH community in Spanish and Portuguese languages, but more importantly what its features are. The diversity of the characteristics we have observed sheds light on the HD community’s institutional and project affiliations, area of research, geographic location, research approaches, and temporal data.

1.2. Methodology
In order to tackle these issues, we have carried out three research phases:

1) Data collection gathered entirely online through a survey, answered voluntarily by 85 participants. The survey was available during a four-month period from June 10th to October 10th, 2013. Questions included gathered data on participants’ institutional, project, and disciplinary affiliations, research approaches, location, among others. The survey was distributed through mailing lists and Twitter. Links to the projects’ survey were tweeted using hashtags used by similar events and communities such as #DíaHD, #HDH2013, #DH2013, #ThatCampBaires, #HDBr, #RedHD, #arounddh, #HumanidadesDigitales, and #dhpoco. The aim of this distribution model was to catch the attention of as many participants in as many locations as possible. This approach to data collection sought to allow anybody who identified himself/herself as a digital humanist in the two languages to self-report their characteristics, rather than send out invitations to those we might consider to fall even under a “big tent” definition of digital humanities.3

2) Using the available data, we built a graph database organized according to the semantic network schema in Fig 1. Rather than looking for person to person connections, this analysis sought to shed light on non-obvious and non-personal connections among participants. The links joining researchers and students among them are disciplines, approaches, work spaces, and geographic proximity. The data was subjected to frequency, central tendency, and network analysis. Several results emerged from the data contained in the database and are presented in the next section.


Fig. 1: Database Schema.

3) An interactive map visualization built on DARIAH-DE Geobrowser. This resource is a second contribution of this project to the field as it seeks to serve not only as a visualization of the data collected, but also as a reference tool. Finally, aside from providing a glimpse of the state of the field, MapaHD hopes to strengthen and expand the sense of community and connection among Spanish and Portuguese speaking digital humanists initiated by DíaHD.

2. Results  
2.1. Discipline Outlook
More than half of the 85 participants reported working in at least two disciplines distributed as shown in Fig 2. Literary studies was the discipline most participants reported. However, out of the 56 participants who reported working on the literary fields, 32 also reported working in other disciplines.


Fig. 2: Discipline distribution of participants.

In Fig 3 we show the most common combinations of literary studies and other disciplines. Aside from the recurrence of literary studies together with other disciplines, the only other recurrent disciplinary combination was History and Visual Art. The rest were mostly unique combinations. This information confirms the fact that, not unlike DH, HD is also “a hybrid domain, crossing disciplinary boundaries and also traditional barriers between theory and practice, technological implementation and scholarly reflection”4.


Fig. 3: Combinations between Literary Studies, the most common discipline in the database, and other fields.

Our analyses offer qualitative insights as to what different disciplines might be bringing in into the mix. For example, the recurrence of interdisciplinary work attached to a “root” field of Literary studies suggests that the field is not necessarily the gravity centre of HD as has been suggested by Azofra5 but, as observed in Fig 4, a hub where other expertises converge, shedding light upon each other. In contrast, network analysis has shown that the second and third best connected disciplines are Information Sciences and History.


Fig. 4: Network visualization showing the prominence of Literary Studies by measuring degree, and History and Information Science as the next two best connected disciplines.

The relevance of these two fields in the network resides not so much in how many participants reported them, but how variably combined they are. As a matter of fact, the cluster formed around Information Sciences brings together a few other disciplines such as Education and Computer Science. Even though they have key links joining them to the rest of the network, disciplines such as Philosophy, Film and Media Studies, and Linguistics retain a certain level of isolation. From our data it is possible to see both the exposing of disciplines to other fields of knowledge, on the one hand; and on the other, how in opening up, disciplines flood other fields too. While some disciplines, by sheer numbers seem to be exercising a larger influence, connecting fields like History and Information Sciences might be providing a common foundation of porous perspectives through which these dynamics take place.

2.2. Geographic Outlook
The geographic location of MapaHD participants (Fig 5) was a foundation for the initial concept of the project. Through the interactive map visualization, we also explored the participants distribution. In total 41 cities located in 11 countries were identified (Fig 6). As a reference, this is close to 50% of the total number of countries represented in ACH membership set at 23, as Bethany Nowviskie reported via Twitter in October 20136.


Fig. 5: Screenshot of MapaHD, built on DARIA-DE Geobrowser, showing the spread of the HD community around the world.

Interestingly, close to half of the locations are found in the UK, USA, Canada, Germany, and Italy where, though common, neither Spanish nor Portuguese are official languages. Although some of the participants’ location can be seen as a ‘diaspora’, our results have showed that this is only a small portion of them (22%) and not the sole distinctive of the analyzed group.


Fig. 6: Geographic distribution of participants.

The issue of location and problems of centrality and periphery in the context of Digital Humanities have been better expressed by Domenico Fiormonte, who stresses the fact that DH have not “succeeded in either strengthening the field of humanities or putting some balance into the power relationships between humanities and computer science” 7. We believe that, as Isabel Galina proposes, this lack of balance “can also help us think about DH from a different perspective… [and] pushes the limits of our creativity and our capacity to solve problems”8.

The clear ties with the Anglo-American branch of DH, and to a lesser extent with the continental European one, seem to imply that as an identifiable community HD is porous and prone to cross-polination in terms of approaches, academic practices, and language. The international spread of HD practitioners might be the cause behind the particular diversity in this community. Nevertheless, the HD community maintains a sense of cohesion that can be traced perhaps to the shared lack of visibility, institutional similarities, and linguistic coincidences – Rafael Alvarado’s “network of family resemblances”9. Furthermore, HD ties with other DH communities, both geographic and linguistic, have set up a communication channel through which approaches and projects may travel back and forth.

3. Conclusions
Much has been said about the characteristics of DH on a global scale and, especially, of the Anglo-American branch. MapaHD constitutes the first data-driven approach to the HD community and we provide insights into its disciplinary and geographical particularities. Not discussed in this abstract but included in the paper are the issues of collaboration and where it takes place, as well as an outlook of the connections among the research approaches undertaken, and the historical development of our participants’ trajectories.

Endnotes
*We use HD to distinguish the Spanish and Portuguese speaking branch of digital humanities from the Anglo-American one commonly referred to as DH.

References
1. “Día de las Humanidades Digitales 2013. Día HD 2013. Día de las Humanidades Digitales. 10 May 2013. Web. 25 August 2013. Par. 1 dhd2013.filos.unam.mx/acerca/

2. Fiormonte, Domenico (2012). Towards a Cultural Critique of the Digital Humanities. Historical Social Research. Historische Sozialforschung. Vol. 37. Print. p. 59.

3. Azofra, Elena. Humanidades digitales cerca del ‘Finis terrae’. MorFlog. 9 July 2013. Web. 13 Sept. 2013. http: morflog.hypotheses.org

4. Flanders, Julia, Wendell Piez, and Melissa Terras (2013). Welcome to Digital Humanities Quarterly. DHQ: Digital Humanities Quarterly:. The Alliance of Digital Humanities Organizations, 2007. Web. 24 Sept. Par. 3 www.digitalhumanities.org/dhq/vol/001/1/000007/000007.html

5. Nowviskie, Bethany (nowviskie). Our numbers have grown & diversified since this page was last updated: ach.org/membership/ -- 480 ACHers now hail from 23 countries. October 14th, 2013, 8:50 a.m. Tweet.

6. Galina, Isabel. Is There Anybody Out There? Building a Global Digital Humanities Community. Red HD. Red de Humanistas Digitales. 19 July 2013. Web. 4 Sept. 2013. Par. 16 humanidadesdigitales.net/blog/2013/07/19/is-there-anybody-out-there-building-a-global-digital-humanities-community

7. Fiormonte, Domenico (2012). Towards a Cultural Critique of the Digital Humanities. Historical Social Research. Historische Sozialforschung. Vol. 37. Print. p. 72.

8. Galina, Isabel. Is There Anybody Out There? Building a Global Digital Humanities Community. Red HD. Red de Humanistas Digitales. 19 July 2013. Web. 4 Sept. 2013. Par. 16 humanidadesdigitales.net/blog/2013/07/19/is-there-anybody-out-there-building-a-global-digital-humanities-community

9. Flanders, Julia, Wendell Piez, and Melissa Terras (2007). Welcome to Digital Humanities Quarterly. DHQ: Digital Humanities Quarterly:. The Alliance of Digital Humanities Organizations. Web. 24 Sept. 2013. Par. 3 www.digitalhumanities.org/dhq/vol/001/1/000007/000007.html
1. Projekthintergrund: Ziele, Methoden, Ressourcen
Strukturierte Sprachressourcen (annotierte Textkorpora, Baumbanken, Wortnetze) bieten neuartige und attraktive Möglichkeiten, linguistische Fragestellungen an authentischen Sprachverwendungsdaten zu untersuchen und quantitativ auszuwerten (vgl. z.B. McEnery et al. 2006, Lüdeling & Kytö 2008/2009). Infrastrukturprojekte wie CLARIN bieten flexible Werkzeuge an, um aus diesen Ressourcen Daten zu gewinnen und auszuwerten. Für sehr viele linguistische Forschungsfragen müssen die automatisch gewonnenen Ergebnisse allerdings noch weiter bearbeitet werden – gerade wenn die Anwender nicht selbst Softwarelösungen für die Datenauswertung entwickeln können, sehen sie sich mit zeitaufwändigen, manuellen Routinearbeiten konfrontiert. Im Verbundprojekt Korpus-basierte linguistische Recherche und Analyse mit Hilfe von Data-Mining (KobRA) arbeiten Partner aus Informatik, Linguistik und Sprachtechnologie gemeinsam daran, die quantitative Auswertung strukturierter Sprachdaten zu verbessern und zu beschleunigen. Dazu werden im Rahmen korpusbasierter linguistischer Studien, die mit konkreten Forschungsaktivitäten der Projektbeteiligten in Verbindung stehen, Data-Mining-Verfahren (insbesondere Lernverfahren) im Zusammenspiel mit vorhandenen Sprachressourcen erprobt und angepasst. Die Verfahren operieren auf den Suchtrefferlisten bzw. auf großen Korpora und gehen über die reine Suche hinaus, indem sie die Suchergebnisse filtern, sortieren oder strukturieren sowie ggf. die weitere Aufbereitung der Daten für eine konkrete Fragestellung erleichtern. In unserem Vortrag stellen wir den Ansatz des Projekts vor (Abschnitt 2) und berichten über erste Ergebnisse (Abschnitte 3 und 4).

2. Projektarchitektur
Die Data-Mining-Verfahren des Projekts setzen auf der Infrastruktur der Sprachtechnologie-Partner auf. Es gibt einerseits eine Schnittstelle zu den linguistischen Anwendern und andererseits eine interne Schnittstelle zwischen der Data-Mining-Komponente und der Infrastruktur. Das Schaubild in Abbildung 1 verdeutlicht diese Verzahnung.


Fig. 1: Verzahnung und Schnittstellen zwischen den Projektkomponenten

Einige der im Projekt zu entwickelnden Lernverfahren werden direkt auf den Ergebnislisten (inkl. Annotationen und Metadaten) der von der Berlin-Brandenburgischen Akademie der Wissenschaften, dem Institut für deutsche Sprache (Mannheim) und dem Seminar für Sprachwissenschaft der Universität Tübingen bereitgestellten Sprachressourcen ausgeführt. Andere Verfahren operieren an der Schnittstelle zwischen der Data-Mining-Komponente und der Korpusinfrastruktur.

Zum Einsatz kommen bislang Verfahren der Klassifikation (z.B. Stützvektormethode (SVM), vgl. Joachims 2002) und des Clusterings (v.a. Topic Models, z.B. Latent Dirichlet Allocation (LDA), vgl. Blei et al. 2003; Tomanek & Morik 2011), die die automatische Bereinigung und Disambiguierung bzw. Klassifikation von Treffern (ggf. auf Basis einer möglichst geringen Menge intellektuell analysierter Treffer, z.B. mithilfe von Active Learning, vgl. Tomanek 2010, Tomanek & Morik 2011) ermöglichen. Um die Nutzer bei der Exploration verschiedener strukturierter Datenbestände zu unterstützen, werden auch innovative Formen der Visualisierung für typische sprachbezogene Forschungsfragen erprobt.

3. Fallstudien zu den Bereichen Lexikographie und Diachronische Sprachforschung
Erste Ergebnisse zum Nutzen von Data-Mining-Verfahren für konkrete korpusbasierte Forschungsvorhaben liegen bereits vor. Im Einzelnen wurden Verfahren für folgende Vorhaben angepasst und evaluiert: 

a) Studien zu deutschen Stützverbgefügen.

b) Studien zur korpusgestützten lexikographischen Beschreibung von Wörtern mit mehreren Lesarten.

Ad a) Stützverbgefüge sind Konstruktionen aus einem prädikativen Nomen und einem semantisch blassen Stützverb wie z.B. Anwendung findenoderzur Anwendung kommen. Im Rahmen eines Forschungsprojekts der Dortmunder Projektleiterin zur diachronen Entwicklung und Textsortenspezifik von Stützverbgefügen wurden erstmals große Korpusbestände aus unterschiedlichen Textsortenbereichen untersucht (vgl. Storrer 2013a). Weil die formbasierte Suche in den Korpora bislang keine Möglichkeit bietet, automatisch zwischen Vollverb- (etw. finden) oder Stützverbverwendungen (Anwendung/Beachtungfinden) zu unterscheiden, mussten die ermittelten Suchtreffer manuell-intellektuell analysiert werden. Die dabei entstandenen annotierten Daten wurden im KobRA-Projekt genutzt, um ein automatisches Klassifikationsverfahren für Stützverben zu lernen. Bisher wurden SVM-basierte Klassifikationsverfahren (Stützvektormethode, vgl. Joachims 2002; als Merkmale wurden Kontextwörter und syntaktische Strukturen berücksichtigt) evaluiert, die aktuell auf Trefferlisten aus dem DWDS-Kernkorpus des 20. Jh. abhängig von Verb und Textsortenbereich eine Genauigkeit (Precision) von zwischen 70 und 87% sowie eine Ausbeute (Recall) von zwischen 36 und 80% erreichen. An einer Verbesserung der Ausbeute wird derzeit noch gearbeitet. Die Klassifikationsverfahren werden für den korpusgestützten Aufbau eines Wikis zu deutschen Stützverbgefügen genutzt.

Ad b) Ausgangspunkt für die Studien zur korpusgestützten lexikographischen Beschreibung von Wörtern mit mehreren Lesarten ist das Problem, dass strukturierte Sprachressourcen momentan noch nicht in semantisch disambiguierter Form vorliegen. Automatische Frequenzerhebungen beziehen sich deshalb immer nur auf Formeinheiten; für die lexikographische Arbeit ist man aber gerade auch an den Frequenzen zu einzelnen Lesarten homographer bzw. polysemer Wörter interessiert (z.B. für das Wort Leiter: Sprossenstiege, Tonfolge, Verantwortlicher/Vorstehender, Energie übertragender Stoff). Um Wörter wie Leiter adäquat beschreiben zu können, müssen korpusbasiert arbeitende Lexikographen bislang sämtliche Treffer zu einem Suchwort sichten (für Leiter: 6895 Treffer im DWDS-Kernkorpus des 20. Jh.); Werkzeuge zur automatischen Disambiguierung wären deshalb sehr hilfreich. Sie könnten auch statistische Analyse- und Visualisierungswerkzeuge verbessern (z.B. Kookkurrenzanalysen, Wortverlaufsdiagramme), die bislang ebenfalls nicht zwischen Lesarten differenzieren. Aus diesem Grund werden im KobRA-Projekt Clusteringverfahren zur Partitionierung von Suchtrefferlisten nach Lesarten eines gesuchten Wortes evaluiert und angepasst. Beim Clustering von Trefferlisten aus dem DWDS-Kernkorpus des 20. Jh. zu den Wörtern Leiter und zeitnah(zeitgenössisch, zeitkritisch vs. unverzüglich) mithilfe von LDA-Topic-Models (vgl. Blei et al. 2003) konnten bislang F1-Werte (gleich gewichtetes Mittel zwischen Genauigkeit (Precision) und Ausbeute (Recall)) zwischen 74 und 78% erreicht werden. Dabei wurde die Partitionierung zunächst lediglich auf Basis der Kontextwörter (Bags-of-Words) vorgenommen. Aktuell wird auch der Nutzen weiterer Merkmale (Wortarten, Syntax, Textsorte, Erscheinungsdatum) erprobt.

4. Fallstudien zum Bereich Varietätenlinguistik / Internetbasierte Kommunikation
Die Kommunikation auf der Grundlage internetbasierter Kommunikationstechnologien und sozialer Medien stellt ein wichtiges neues Teilgebiet der Digital Humanities dar. Bei der interpersonalen Kommunikation in Genres wie Online-Foren, Weblogs, Chats, Twitter oder sozialen Netzwerken finden sich Produkte schriftlicher Sprachverwendung, deren sprachliche Gestaltung an den Bedingungen dialogischer Kommunikation im sozialen Nähebereich orientiert ist. Typische Merkmale der interaktionsorientierten Schreibhaltung (Storrer 2013b), auf die die Orientierung an der Mündlichkeit in der schriftlichen internetbasierten Kommunikation (IBK) zurückgeführt werden kann, sind u.a. Phänomene geschriebener Umgangssprache wie etwa Verschmelzungs-/Allegroformen (haste, biste, willste, machstes, isses, aufm, aufn), Schwa-Elisionen (ich schreib, ich mach, ich sag), die Verwendung umgangssprachlicher Lexik (moin, Maloche) oder dialektal/regional gebundener Aussprachevarianten (Oida wos wüst < Alter, was willst (du); wech < weg) sowie die häufige Verwendung von Einheiten wie Interjektionen und Abtönungspartikeln. Darüber hinaus bilden sich in der schriftlichen internetbasierten Kommunikation sprachliche Mittel aus, die auf die Unterstützung der interaktiven schriftlichen Kommunikation am Nähepol optimiert sind. Typische Beispiele dafür sind Inflektive (freu, lach, grübel, wink, seufz) und Inflektivkonstruktionen (wildsei, malanmerk, bedenkenhab), Emoticons sowie die Nutzung von Verfahren der Graphemiteration (gaaaaaanz schlecht) und der Großschreibung (mathe mündlich? BRUTAL!!!) für die graphische Nachbildung stimmlicher Kommunikationssignale.

Um die Besonderheiten der Schreibformen und sprachlichen Besonderheiten in der internetbasierten Kommunikation empirisch begründbar in einen sprach- und varietätengeschichtlichen Rahmen einordnen zu können, müssen Ausgangsbedingungen geschaffen werden, die einen Vergleich von Phänomenen konzeptioneller Mündlichkeit in internetbasierter Schriftlichkeit und dem Schreibgebrauch in historischen Korpora ermöglichen. Beim Aufbau von IBK-Korpora stellen sich derzeit noch viele Herausforderungen (vgl. z.B. Beißwenger & Storrer 2008, Storrer 2013b: Abschnitt 4), weil Verfahren und Standards, die sich für die Annotation von Textkorpora bewährt haben (Annotationsstandards, Metadatenschemata, Werkzeuge und Tagsets für die linguistische Analyse), nicht ohne Anpassungen für IBK-Korpora übernommen werden können.

Im  KobRA-Projekt werden auf der Grundlage manuell annotierter Trainingsdaten Verfahren zur Klassifizierung und Disambiguierung auf die Behandlung von Phänomentypen (Verschmelzungen, Inflektive, Emoticons) trainiert, die in der Domäne typischerweise auftreten und die von Verarbeitungswerkzeugen, die auf den Umgang mit redigierten Texten trainiert sind, nicht angemessen behandelt werden können. Als Testbett für diese Verfahren dienen Daten aus verschiedenen im Aufbau befindlichen IBK-Korpora, die im Projekt zur Verfügung stehen – u.a. aus dem Wikipedia-Korpus am Institut für deutsche Sprache (Mannheim), dem Projekt „Deutsches Referenzkorpus zur internetbasierten Kommunikation“ (DeRiK, Beißwenger at al. 2013) sowie dem Dortmunder Chat-Korpus (Beißwenger 2013). Die Verfahren sollen in Arbeiten zur Anpassung von Werkzeugen für die automatische Wortartenannotation auf die Verarbeitung von IBK-Daten einfließen. Die Annotation erfolgt auf der Grundlage einer erweiterten Version des STTS-Standards für das POS-Tagging deutscher Sprachdaten, in dessen Erarbeitung die Projektbeteiligten involviert sind (Bartz et al. 2013). Sie ist abgestimmt auf Aktivitäten zur Erarbeitung eines Standards für die Strukturannotation von IBK-Korpora im Rahmen der Text Encoding Initiative (TEI).

References
Bartz, T., Beißwenger, M., Storrer A. (2013): Optimierung des Stuttgart-Tübingen-Tagset für die linguistische Annotation von Korpora zur internetbasierten Kommunikation: Phänomene, Herausforderungen, Erweiterungsvorschläge.Journal for Language Technology and Computational Linguistics (Themenheft „Das STTS-Tagset für Wortartentagging – Stand und Perspektiven“).

Beißwenger, M. (2013): Das Dortmunder Chat-Korpus. Zeitschrift für germanistische Linguistik 41/1, 161–164. (Erweiterte Fassung online: http://tinyurl.com/chatkorpus).

Beißwenger, M., Ermakova, M., Geyken, A., Lemnitzer, L., Storrer, A.(2012): A TEI Schema for the Representation of Computer-mediated Communication.Journal of the Text Encoding Initiative (jTEI), Issue 3, jtei.revues.org/476 (DOI: 10.4000/jtei.476).

Beißwenger, M., Ermakova, M., Geyken, A., Lemnitzer, L., Storrer, A.(2013): DeRiK: A German Reference Corpus of Computer-Mediated Communication. In: Literary and Linguistic Computing. tinyurl.com/derik-llc (DOI: 10.1093/llc/fqt038).

Beißwenger, M. and Storrer, A. (2008): Corpora of Computer-Mediated Communication. In Lüdeling, A. und Kytö, M. (eds), Corpus Linguistics. An International Handbook. Volume 1. Berlin, New York: de Gruyter (Handbücher zur Sprache und Kommunikationswissenschaft / Handbooks of Linguistics and Communication Science 29.1), pp. 292–308.

Blei, D. M., Ng, A. Y., Jordan, M. I. (2003): Latent dirichlet allocation. Journal of Machine Learning Research 3, pp. 993–1022.

Geyken, A. (2007): The DWDS corpus: a reference corpus for the German language of the 20th century. In Fellbaum, C. (ed), Idioms and collocations. Corpus-based linguistic and lexicographic studies. London: Continuum, pp. 23– 40.

Joachims, T. (2002): Learning to Classify Text Using Support Vector Machines. Dissertation. Dordrecht: Kluwer.

Krenn, B., Erbach, G. (1994): Idioms and support verb constructions. In Nerbonne, J., Netter, K., Pollard, C. (eds), German in Head-Driven Phrase Structure Grammar. Stanford: CSLI publications, pp. 365–395.

Langer, S. (2004): A linguistic test battery for support verb constructions. Linguisticae Investigationes 27 (2), pp. 171–184.

Lüdeling, A. and Kytö, M.(eds) (2008/9): Corpus Linguistics. An International Handbook. 2 Bände. Berlin, New York: de Gruyter.

Morik, K., Kaspari, A., Wurst, M., Skirzynski, M. (2012): Multi-objective frequent termset clustering.Knowledge and Information Systems 30 (3) (DOI:10.1007/s10115-011-0431-3), pp. 715–738. 

McEnery, T., Xiao, R., Tono, Y. (2006): Corpus-Based Language Studies. An Advanced Resource Book (Routledge Applied Linguistics). London, New York: Routledge.

Storrer, A. (2007): Corpus-based investigations on German support verb constructions. In Fellbaum, C. (ed), Idioms and collocations. Corpus-based linguistic and lexicographic studies. London: Continuum Press, pp. 164–188.

Storrer, A. (2013a): Variation im deutschen Wortschatz am Beispiel der Streckverbgefüge. In Deutsche Akademie für Sprache und Dichtung; Union der deutschen Akademien der Wissenschaften (eds), Reichtum und Armut der deutschen Sprache. Erster Bericht zur Lage der deutschen Sprache. Berlin/New York: de Gruyter, pp. 171–209.

Storrer, A. (2013b): Sprachverfall durch internetbasierte Kommunikation? Linguistische Erklärungsansätze – empirische Befunde. Sprachverfall? Dynamik – Wandel – Variation. Jahrbuch des Instituts für Deutsche Sprache 2013.

Tomanek, K. (2010): Resource-aware annotation through active learning. Dissertation, TU Dortmund.

Tomanek, K., Morik, K. (2011): Inspecting Sample Reusability for Activ Learning. JMLR Workshop and Conference Proceedings 16, pp. 169–181.

Das Verbundprojekt wird vom Bundesministerium für Bildung und Forschung (BMBF) seit Herbst 2012 im Rahmen des Programms „eHumanities“ gefördert. Informationen zu den Projektbeteiligten und den Ergebnissen unter: http://www.kobra.tu-dortmund.de.

Engl. „support verb constructions“ (vgl. u.a. Langer 2004, Krenn & Erbach 1994, Storrer 2007/2013).

Zu den Korpora im Projekt „Digitales Wörterbuch der deutschen Sprache“ (DWDS) vgl. Geyken (2007).

Relevant sind hier v.a. die Aktivitäten der Special Interest Group „Computer-Mediated Communication“ (http://www.tei-c.org/Activities/SIG/CMC/); ein erster Entwurf für ein TEI-Schema für die Annotation von Genres internetbasierter Kommunikation ist in Beißwenger et al. (2012) beschrieben.
1. Introduction - David Robey, Oxford e-Research Centre
The historical concentration on text in humanities computing and the Digital Humanities (DH) partly reflects the technologies that have been available, and partly the majority interests of humanities researchers. Yet much humanities research also depends on scholars’ visual skills, not only in the arts and archaeology, but also in disciplines whose main concern is text, for whom the physical form of texts can be as important as their content. Thus digital textual resources increasingly link to images, thereby greatly increasing their potential scholarly benefits: an outstanding example is Prue Shaw’s recent digital edition of Dante’s Divine Comedy. This panel session is concerned with what one might consider the next stage in the use of image in DH research: the technologies that are increasingly being used for image recognition, enhancement and analysis. Unlike many of the digital text-and-image archives now available, which do little more than accelerate and facilitate humanities research, these enable the production of knowledge that would simply not be accessible by non-digital means.

The panel presents a range of innovative humanities research in progress in these areas of image recognition, enhancement and analysis at Oxford University and UCL London. It includes both practical applications of the related technologies, and a more theoretical approach to the methods deployed. The latter is an area in which the humanities have traditionally been weak. Scholars have usually been reluctant to reflect in any depth on the exact nature of their methods; even during the heady days of humanities theory towards the end of the last century, the focus of interest was much more on high-level concepts, and much less on the details of methodology. Yet this kind of reflection is essential: if the most productive use is to be made of the new technologies for image analysis, and indeed all other forms of digital analysis in the humanities, and if we are to promote their use effectively, we need to be very clear exactly what they help us to do, and how this fits in with the work that scholars have traditionally done.

The panel will begin with a presentation by Charles Crowther, from the Centre for the Study of Ancient Documents at Oxford, of a range of techniques to increase the legibility of different forms of writing in the ancient world—ancient world studies being probably the field in which scholarship depends most on analysing the exact form in which text is preserved, and as a result the field in which the use these technologies is most advanced. This will be followed by an in-depth presentation and discussion by Jacob Dahl, of Oxford’s Oriental Studies Institute, of the use of one particular form of these technologies, Reflectance Transformation Imaging, to advance the study of one of the world's earliest and still undeciphered writing systems, proto-Elamite. The paper by Segolene Tarte, from the Oxford e-Research Centre, will provide the theoretical dimension by identifying the cognitive processes involved in some of research covered by the first two papers, and in other related work. Finally Julianne Nyhan, from the Department of Information Studies at UCL London, moves into a more difficult and experimental area with a review of the use of image recognition for historical research and a brief presentation of a project for the digital study of newspaper photographs in the context of the history of the First World War.

The work that the four papers deal with is highly detailed and specialist, but has potential applications far beyond the fields in which it has been carried out so far: a topic we plan to cover in the panel discussion.

*  *  *

2. Reading Ancient Writing: Technology and Scholarship - Charles Crowther,Centre for the Study of Ancient Documents, Oxford
Much of the evidence that the scholarly community in Classics has available to extend and renew its fields of investigation is fragmentary, difficult to decipher, and tantalising. The use of new technologies opens the prospect of making this evidence more easily and extensively accessible and exemplifies the contribution of DH to scholarly research in a well-defined and coherent context.

In this paper I review the effectiveness of a range of visualisation technologies deployed to increase the legibility of ancient, primarily Greek and Latin, incised and inscribed documentary texts, and consider some directions for future work. The analysis draws perspectives from work in this field at the Oxford Centre for the Study of Ancient Documents (CSAD) over the last 15 years, and presents results from recent and continuing projects undertaken with other presenters in the panel.

I consider two types of text that offer challenges to decipherment that are broadly similar but different in significant respects: wooden and metal writing tablets and inscriptions on stone.

Regular discoveries of incised wooden and metal writing tablets in excavation of Romano-British (and Northern European) sites potentially offer new categories of evidence, but their transcription and decipherment present the constant challenge of separating fine traces of writing from background features and, in many cases, from other palimpsest layers of text. The great majority of the material is relatively new (recovered since 1980) and is still in the early stages of integration into the body of research resources in Ancient History. Successive projects undertaken since 1998 at CSAD, in collaboration with colleagues in the field of medical image analysis in the Department of Engineering Science at Oxford, have resulted in the creation of new techniques for improving the visibility and legibility of writing on wooden tablets, principally by means of a stroke detection method (shadow stereo or phase congruency) and the removal of wood-grain (Brady et al. 2005).

These advances were based on digital scans of the writing tablets made with lights illuminating the surfaces from different angles, calibrated manually. Their application has already resulted in new editions of texts of writing tablets from the Roman fort at Vindolanda (Bowman and Tomlin 2005) and, most strikingly, a Roman legal document found in Frisia in 1914 (Bowman et al. 2009), which forms a case study in the paper offered by Dr. Tarte. Central to this programme of research has been the belief that in order to develop better imaging techniques we need at the same time to explore developments in the representation of semantics, in theories of reading, and in ideas about knowledge representation (Terras 2007; Tarte 2011).

Inscriptions on stone are one of the most characteristic legacies of the culture of the ancient Greco-Roman world, from the beginning of alphabetic writing in the 8th century BC to Late Antiquity. Very large quantities of inscriptions have been recorded – the total number now published exceeds 800,000 – but few have survived intact; stones are frequently broken into fragments and very many have suffered extensive surface damage from abrasion or erosion. Techniques for the decipherment of these damaged texts had not until recently advanced significantly since the beginning of epigraphic studies in the 15th century. The principal traditional means of reading letter traces, by taking paper (or latex) casts of the surface which can be manipulated more easily than the original stone, or by using solutions of charcoal and water to emphasise surface indentations, remain effective, but involve direct action upon the surface of the stone and are now permitted only under controlled conditions and in exceptional cases by museum conservators.

Because the language and formal character of inscribed documents are well understood, small improvements in reading can lead to significant advances in decipherment and interpretation. Two examples may be cited: much of the history of the 5th-century Athenian empire has seemed to turn on the interpretation of a handful of evanescent letter traces on a stele recording an alliance between Athens and Segesta (Chambers et al. 1990); a palimpsest inscription on a basalt stele recovered during the rescue exacavations at Zeugma in the Euphrates valley in 2000 has provided new insights into one of the more remarkable expressions of ruler cult in antiquity (Crowther 2013).

However techniques for recovering text, whether based upon paper casts, illumination with raking light, or, more recently, laser scanning can only improve legibility when there are some remaining topographic traces of the original inscribed text. In this section of the paper I summarise the results of experiments using the microfocus spectroscopy beamline at the Diamond Light Synchrotron in 2010 and 2011, following earlier work at the Cornell High-Energy Synchrotron Source (CHESS), which show that trace elements associated with wear of the inscribing tool and with the pigments used to paint inscribed letters can be detected with high sensitivity and spatial resolution by X-Ray Fluorescence (XRF) Imaging, even when the stone surface has worn below its original contours (Powers et al. 2005).

XRF imaging, for the moment, requires that text artefacts be brought to a synchrotron source and is ineffective where the surface of the object has been subject to intensive cleaning since Antiquity. In the great majority of cases, analysis of surviving surface traces continues to be the principal method of decipherment. Approaches to incised and inscribed texts, accordingly, converge on the need to recover and interpret surface topography as accurately as possible. The manually calibrated methods of illumination used in previous work at CSAD have now been replaced by Reflectance Transformation Imaging (RTI), whose application to cuneiform texts and seal impressions is described in the paper by Dr. Dahl.

RTI uses multiple images captured from a fixed camera position to construct a digital model of surface form and reflectance for the object studied. The resulting files enable interactive changes to lighting, image enhancements and automated identification of visual and morphological attributes. RTI has a number of specific advantages for the capture of incised and inscribed documents: non-contact acquisition of surface data, to alleviate the concerns of museum conservators; potential representation of 3D shape characteristics without data loss due to shadows and specular highlights; virtualised surface analysis under any form and distribution of lighting; the possibility of analysing surfaces remotely and ‘re-photographing’ them for dissemination. RTI representations of documentary texts are visually striking and attractive, but current fitting algorithms do not exploit the full potential of the image data captured. In the final section of the paper I report on current work undertaken at CSAD to improve the algorithms and capture processes and their application to a range of incised and inscribed documentary texts.

Selected References
A. K. Bowman, R. S. O. Tomlin, “Wooden Stilus Tablets from Roman Britain”, in Bowman and Brady 2005, 7-14

A. K. Bowman, M. Brady (eds.) Images and Artefacts of the Ancient World (British Academy Occasional Papers, Oxford 2005)

A. K. Bowman, R. S. O. Tomlin, K. A. Worp, “‘Emptio bovis Frisica’: the ‘Frisian ox sale’ reconsidered”, JRS 99, 2009, 156-70

M. Brady, X.-B. Pan, V. Schenk, M. Terras, P. Robertson, N. Molton, “Shadow Stereo, Image Filtering, and Constraint Propagation”, in Bowman and Brady 2005, 15-30

M. Chambers, R. Gallucci, P. Spanos, “Athens’ Alliance with Egesta in the Year of Antiphon”, ZPE 83, 1990, 38-63

C. Crowther, “Inscriptions on Stone”, in W. Aylward (ed.), Excavation at Zeugma. Conducted by Oxford Archaeology (Los Altos, 2013), 192-219

J. Powers, N. Dimitrova, R. Huang, D.-M. Smilgies, D. H. Bilderback, K. Clinton, and R. E. Thorne, “X-ray fluorescence recovers writing from ancient inscriptions”, Zeitschrift für Papyrologie und Epigraphik 152, 2005, 221-227

S. M. Tarte, “Papyrological Investigations: Transferring Perception and Interpretation into the Digital World”, Lit Linguist Computing 26(2), 2011, 233-247. 

M. Terras, Image to Interpretation. An Intelligent System to Aid Historians in Reading the Vindolanda Texts (Oxford 2007)

Reflectance Transformation Imaging of ancient texts - Jacob Dahl Oriental Studies Institute, Oxford
This paper explores the use of Reflectance Transformation Imaging (RTI) technology in the study and decipherment of ancient texts. RTI is “a computational photographic method that captures a subject’s surface shape and color and enables the interactive re-lighting of the subject from any direction” (definition from http://culturalheritageimaging.org/Technologies/RTI/). It is based on the work of Tom Malzbender (2001, see http://www.hpl.hp.com/research/ptm/index.html).

The camera dome used at the University of Oxford, and built by researchers at the University of Southampton (Earl et al. 2011), uses 76 daylight-LEDs, which are attached to the inside of the plexiglas dome. A high-resolution digital camera is mounted on the top of the dome, looking straight down through a hole. The object is placed on a stage in the centre lifted up to the horizon. The diameter of the camera dome is approximately 1m, allowing for the capture of objects up to 33 cm in diameter. 76 individual raw files are captured each using a different light source and therefore a different light angle. In post-processing the images are joined to create a composite image (model) (Polynomial Texture Mapping (PTM)) where the light-source can be changed by the user.

Using RTI images captured in the Louvre Museum in Paris, researchers at the university of Oxford have been able to significantly advance the study of one of the world's earliest and still undeciphered writing systems, proto-Elamite, mimicking in the classroom the work of the epigrapher in the museum. This method has proven particularly valuable when examining secondary additions to certain signs, lightly impressed signs, alterations to signs, or seal impressions.

Proto-Elamite is the conventional name give to a derived writing system emerging in Iran following the spread of the culture and technological advances of the Late Uruk period in Mesopotamia into Western Iran c. 3500 BC (Dahl 2013a). The writing system is defined by having a high number of singletons, and possibly a high number of scribal errors, perhaps resulting from never having been standardized (Dahl 2002 and Dahl 2013b). A majority of the extant proto-Elamite texts are kept in the Louvre Museum, Paris, and the National Museum of Iran, Teheran. The writing system disappeared after a short use of at most a few centuries, and writing was not used in Iran for the following five centuries or longer.

It has long been realized that deciphering early scripts involves more than merely a linguistic puzzle, that features such as seal impressions, scribal marks, etc. hold valuable information, and that the materiality of writing is therefore more important for the study of early writing then anywhere else (Damerow 2006). Subtle differences in sign forms may be the result of scribal hands, semantic variation, or simply lack of practice (André-Salvini and Dahl in press).

Traditional print-representations of early writing specimens only poorly represent the physicality of the object. Proto-Elamite and other early writing systems are often studied by very small groups of researchers at universities or research institutions across the globe. Previous generations of researchers were confined to either using hand copies of the originals, of varying quality, or consulting the originals in museums far from their home institutions, when attempting to decode the information of these documents. High-resolution, dynamic images of these text artifacts therefore have the potential of transforming the study of early writing by simulating first-hand consultation of the originals, enabling shared research, and bringing together disparate data-sets.

Over the course of the last two years c. 1100 tablets and fragments in the Louvre Museum were imaged with the camera dome (André-Salvini and Dahl 2013a). Results of research facilitated by these images is now being published. For example, the study of RTI images of two tablets in the Louvre Museum (Sb 15229 and 15456) challenged the existing view of the imagery of the seal impression found on both (a couple of humanoid figures as well as animals), and led to a strengthening of the theory that no representations of the human form was allowed in glyptic art of the proto-Elamite period (results later confirmed by collation of originals in the Museum) (Dahl 2014).

The main issue facing a wider application of RTI technology in the study of ancient writing is the size of the files (mostly 256 mb per image, six needed for a cuneiform tablet), and the lack of a suitable on-line viewer. Image size is becoming less of an issue over time, and the issue of an on-line viewer is the focus of at least one ongoing project (http://www.arts.kuleuven.be/info/ONO/Meso/cuneiformcollection and http://portablelightdome.wordpress.com). In the meantime captures taken by the camera dome can be used to produce very high quality static images by blending different views together in an image editor.

Selected References
André-Salvini and Dahl 2013: “L’écriture proto-élamite: la numérisation à l’aide du déchifrement des premières écritures”. In Grande gallerie: le journal du Louvre 2013/2, 28-29.

André-Salvini and Dahl in press: “Le système d’écriture proto-élamite : un point sur le déchiffrement des signes, leur formation et leur usage”. In La recherche au muse du Louvre 2012 (2013).

Dahl 2002: “Proto-Elamite Sign Frequencies.” In CDLB 2002:1 (2002) http://cdli.ucla.edu/pubs/cdlb/2002/cdlb2002_001.html

Dahl 2013a: “Early Writing in Iran”. In Potts, D.T., Oxford Handbook of Iranian Archaeology (Oxford University Press) (2013), 233-262.

Dahl 2013b: “Frühe Schrift im Iran”. In N. Crüsemann et al. Uruk 5000 Jahre Megacity: Begleitband zur Ausstellung "Uruk--5000 Jahre Megacity" im Pergamonmuseum--Staatliche Museen zu Berlin (2013), 202-203.

Dahl 2014: “The proto-Elamite seal MDP 16 no 198”. In CDLN 2014:1 <http://cdli.ucla.edu/Pubs/cdln/archives/000028.html>.

Damerow 2006: “The Origins of Writing as a Problem of Historical Epistemology.” In CDLJ 2006:1 (2006) <http://cdli.ucla.edu/pubs/cdlj/2006/cdlj2006_001.html>

*  *  *

Digital Images of Ancient Textual Artefacts. Connecting Computational Processing and Cognitive Processes - Ségolène Tarte, Oxford e-Research Centre
Digital image processing is an expanding domain in the DH that naturally finds its place in the overall 

knowledge and meaning creation process that is the ultimate aim of the study of ancient textual artefacts. The cognitive aspects of this intrinsically interpretative process play a major role in the endeavour and continually interact with the computational processing mobilized by image capture and processing methodologies. In this paper, I aim to present some of the cognitive aspects of the interpretation of textual artefacts that intervene in the analysis of their digital image avatars. To illustrate those cognitive processes, I will present them in the context of “naturalistic” observations  of expert papyrologists and assyriologists working on ancient textual artefacts (observations made following ethnographic methodologies) , and connect them with similar observations made in the controlled settings of laboratory experiments as reported in the cognitive sciences literature. Each example illustrates how one specific cognitive process has been aided by the use of digital image-based technology, and how they are integrated into the interpretation workflow.

It is worth specifying that the processes highlighted here are perceptual in nature, rather than conceptual; however, as they participate in the act of interpretation of textual artefacts, in the act of knowledge creation and sense-making, they definitely qualify as cognitive processes. These cognitive processes are efficient and complementary with the services that digital tools can render. My aim in identifying them is not to attempt to emulate them digitally, but rather to identify where computational processing can provide help and where the upper hand is best left to the experts.

Making the intangible tangible: Artemidorus papyrus (in collaboration with Prof. D’Alessio (KCL), and Dr Elsner (Oxford)). Here the computational processing involved infrared image capture, and, in later work, digital image alignment (of the front and back images) as well as virtual rolling of the papyrus (Tarte, 2012). The virtual rolling was made in order to evaluate the hypothesis that the papyrus fragments needed to be reordered based on traces of inks from the reverse of the papyrus that seemed to have transferred to the obverse by mirror impression while the papyrus was rolled up. Beyond providing a rigorous argument in favour of reordering, the actual process of virtually rolling the papyrus prompted a re-materialization of it. A physical avatar was produced which allowed the researchers to experience for themselves the rolling of the document and thereby ascertain that the ink transfers could have resulted from the roll, confirming the plausibility of the reordering of the fragments. This enacted approach to interpretation, enabled by the upstream image-based technologies that have been mobilized, points to what the neurosciences call embodied cognition, where “Social Meaning is primarily the object of practical concern and not of theoretical judgment… It relies on non-inferential mechanisms, which do not require the explicit use of rationality” (Gallese, 2005, p43). Through a combination of image processing output and enacted engagement with a physical avatar, aspects of the intangible papyrus have been made tangible. 

Making the inarticulate articulate: the Roman stylus wooden tablet known as the "Frisian Ox" tablet (in collaboration with Prof. Bowman (Oxford) and Prof. Terras (UCL)). Beyond image capture, here the computational processing involved removing the woodgrain and enhancing the visibility of the scratches that constitute the script (Tarte, 2011). Digital technologies were also used to produce line drawings by means of a drawing tablet that allowed for the tracing of the text over any digital image from the collection that had been captured. One of the cognitive processes that ensued has to do with visual completion. By tracing the letter shapes, experts filled in the gaps where portions of character were absent. This enacted approach to interpretation, enabled by the upstream image-based technologies that have been mobilized, points to the phenomenon the neurosciences call illusory contour: “Detection of an illusory figure shows a precedence of specific global object properties over local attributes ... it is the surface rather than the contour that guides search” (Conci et al., 2007, p1293-4). This in part explains why the expert papyrologists stipulated that the woodgrain removal algorithms applied to the images were not only not very helpful, but also possibly confusing. Through a combination of illusory contour detection and digital tracing of the letters, aspects of the inarticulate text of the tablet have been made articulate.

Making the invisible visible: proto-Elamite tablets from ancient Iran (in collaboration with Dr Dahl (Oxford) – cf paper on this panel). Here the computational processing involved the deployment of an advanced image capture technique known as Reflectance Transformation Imaging (RTI (Earl et al., 2011)). Through this technique, it is possible to interactively change the position of a unidirectional light source shone onto the artefact, thereby allowing for the accentuation of its physical geography - an enormous asset when dealing with 3D scripts such as proto-Elamite. In effect, what RTI allows is to mobilize depth perception through monocular motion parallax (Rogers and Graham, 1982): RTI supports depth perception, and results in making visible the otherwise invisible.

Making the indiscernible discernible: Selenite curse tablets from Ancient Cyprus (in collaboration with Dr Bodard (KCL) and Prof. Radaelli (Oxford)). Here the crystalline nature of the support makes the texts indiscernible. As mounting evidence in recent bodies of work in the cognitive sciences points to an action-simulation-perception framework (related to embodied cognition), where visualizing the results of an action mobilizes the pre-motor areas of the brain that correspond to the observed action (such as in reading/writing, or in painting (Taylor et al., 2012)), Scanning Electron Microscope (SEM) images, which reveal disruptions in the crystalline structure of selenite, have the potential to inform the viewer on the ductus and the dynamics of the act of writing, thereby facilitating reading by making the indiscernible dynamics of writing discernible.

In each of these examples I have selected one specific cognitive process that took place. Of course, many others occurred, and each of those cases could have been used to illustrate one or more of the other cognitive processes highlighted here. Further work will explore the interaction of the perceptual and conceptual cognitive processes mobilized in the interpretation of textual artefacts, and how to integrate them in a digitally supported workflow. 

Selected References
Conci, M., Müller, H. J., and Elliott, M. A. (2007). The contrasting impact of global and local object attributes on kanizsa figure detection. Perception & Psychophysics, 69(8):1278—1294.

D’Alessio, G. (2009). On the “Artemidorus” Papyrus. Zeitschrift für Papyrologie und Epigraphik, 171:27—43.

Earl, G., Basford, P. J., Bischoff, A. S., Bowman, A., Crowther, C., Hodgson, M., Martinez, K., Isaksen, L., Pagi, H., Piquette, K. E., and Kotoula, E. (2011). Reflectance transformation imaging systems for ancient documentary artefacts. In Electronic Visualisation and the Arts, London, UK. BCS, The Chartered Institute for IT.

Gallese, V. (2005). Embodied simulation: From neurons to phenomenal experience. Phenomenology and the Cognitive Sciences, 4:23—48. 10.1007/s11097-005-4737-z.

Rogers, B. and Graham, M. (1982). Similarities between motion parallax and stereopsis in human depth perception. Vision Research, 22(2):261 — 270.

Tarte, S. M. (2011). Papyrological Investigations: Transferring Perception and Interpretation into the Digital World. Lit Linguist Computing, 26(2):233—47.

Tarte, S. M. (2012). The digital existence of words and pictures: The case of the Artemidorus papyrus. Historia, 61(3):325—336 (+bibliog. pp 357—61; fig. pp 363—5).

Taylor, J. E. T., Witt, J. K., and Grimaldi, P. J. (2012). Uncovering the connection between artist and audience: Viewing painted brushstrokes evokes corresponding action representations in the observer. Cognition, 125(1):26 — 36. 

Facial recognition and Digital Humanities: new directions? - Julianne Nyhan, Department of Information Studies, UCL
As documentary sources photographs can offer historians new ways to uncover, interrogate, visualise and communicate about the past. In the past new insights into historical questions have been gained through unexpected and often serendipitous observations in historical photographs: for example, the seemingly accidental discovery of the young Hitler in photographs of social-democratic rallies (cf Krumeich 2001).  Recent developments in facial recognition and image analysis techniques (see, inter alia, Wang et al. 2013, Singh et al. 2013 and Vieira et al. 2013) offer historians (and researchers across the humanities and beyond) new ways to think about and analyse visual documentary evidence. The application of facial recognition techniques may well contribute to the establishment of a more systematic basis for the discovery and analysis of actors and social networks in historical photographs, and perhaps for automatically identifying ‘suspect’ photographs. Nevertheless, comparatively little research seems to be ongoing within the DH into the applications of such facial detection, recognition and visual computing techniques.

Automated facial detection has become ubiquitous in day to day life. It is used on public and private CCTV-networks for crime and terror prevention, for example, by preselecting information for screening and at border controls using biometric passports. Digital cameras and mobile phones can auto-detect faces and even smiles on faces (see Deniz, O. et al. 2008), replacing traditional passwords or providing ‘convisual’ information to photographers, like the names of the people in the viewfinder if these people have been photographed before (see, for example, Brown 2011). Social media networks like Facebook or Twitter engage in the batch-tagging of people in photographs, while popular image cataloguing software packages like Google’s Picasa allow automatic graphical indexation of large snapshot collections at home. Automated facial detection, despite still being a relatively new branch of image analysis, has quickly matured and allows the facial indexation of large graphical databases. The result is that graphical information can be searched in ways that have hitherto been impossible.

Nevertheless, within the context of DH, few applications of such techniques can be noticed in the published literature. The paper will start with a review of literature related to image analysis techniques in order to present both the technological state of the art and uses that are being made of image analysis techniques in DH. A review of the literature relating to historians’ methodological engagement with visual documentary sources will also identify existing and new research problems in historical research that facial recognition techniques could be applied to, leading to a discussion of the potentials and drawbacks that facial recognition techniques hold for this kind of research.

In the context of DH, the most substantial research to date is that of Suárez, de la Rosa Pérez and Ulloa (2013), who have applied such techniques to representations of the human face in world literature.  To do so they analysed more than 123,500 paintings from all periods of art history with a face recognition algorithm used in Facebook’s photo-tagging system and automatically identified over 26,000 faces. Using techniques like, among others, the Elastic Bunch Graph Matching (Wiskott et al. 1997), they were able to detect what they term basic features (e.g. information about the position of facial features such as eyes and nose) and extended features (e.g. gender, mood, age range). They conclude that by comparing “the basic features set using graphs and the extended features set using clustering by K-Means method (Sculley 2010) … we are at the perfect position to analyze and characterize each of the groups according to different historical perspectives and cultural questions, for instance, the distinction among styles by giving a minimum set of features that determines its membership” (p, 535) .

From the perspective of facial recognition techniques noteworthy recent developments include the 3D face recognition systems by the University of Bradford’s Centre for Visual Computing, which they state has led to the development of a prototype system that “demonstrates that 3D facial data can overcome many inherent problems in image-based (2D) face recognition. This can be accurate up to the level of differentiating between identical twins” (see University of Bradford 2013). However, for my present purpose it is mostly historical photographs that are under discussion and at the time of writing it is not clear how such a system would work with historical photographs (even if 3D models of the 2D photographs were created using technology such as freely available software platforms like 123D Catch).

To conclude the talk an overview of the initial findings of a project on First World War photographs that we have recently started will be presented. WW1 was the first major war where photography was affordable and routinely used in both official and amateur channels; thus visual documentary evidence from the period is vital to its study. During WW1 Belgian refugees arrived in Britain en masse in what transpired to be the largest ingress of refugees in British history. However, our understanding of who arrived, how they intersected with British and diasporic social networks, how long they stayed, and whether they settled or returned home is limited. One of the historical applications reported on in this paper will be the initial findings of a pilot project that is investigating accuracy rates of facial detection techniques on historical photographs of WW1 Belgian refugees.

References
Brown, Mark. 2011. “Facebook Silently Rolls Out Face Recognition Tagging to the World.”Wired, 08 June 2011 edition, sec. Technology. http://www.wired.co.uk/news/archive/2011-06/08/facebook-face-recognition.

Deniz, O., Castrillon, M, Lorenzo, J, Anton, L, and G. Bueno. (2008)  ‘Smile Detection for User Interfaces’,Advances in Visual Computing, vol. 5359/2008, pp. 602–611

Krumeich, Gerd.  "Hitler in der Menge," in: C. Dipper, A. Gestrich, L.Raphael (eds.), Krieg, Frieden und Demokratie: Festschrift für Martin Vogt zum 65. Geburtstag (Frankfurt, 2001), 137-140.

Singh, Chandan, Ekta Walia, and Neerja Mittal. 2012. “Robust Two-stage Face Recognition Approach Using Global and Local Features.”The Visual Computer 28 (11) (November 1): 1085–1098. doi:10.1007/s00371-011-0659-7.

Suárez, de la Rosa Pérez and Ulloa (2013) ‘Not Exactly Prima Facie: Understanding the Representation of the Human Through the Analysis of Faces in World Painting’ DH Book of Abstracts. 534-6. Nebraska: Centre for Research in the Humanities

University of Bradford, ‘3D Face Recognition For Security Systems’www.visual-computing.brad.ac.uk/case-studies/3d-face-recognition-security-systems (accessed 31/10/2013)

Vieira, Tiago F., Andrea Bottino, Aldo Laurentini, and Matteo De Simone. 2013. “Detecting Siblings in Image Pairs.”The Visual Computer: 1–13. doi:10.1007/s00371-013-0884-3.

Wang, Zhifei, Zhenjiang Miao, Q. M. Jonathan Wu, Yanli Wan, and Zhen Tang. 2013. “Low-resolution Face Recognition: a Review.”The Visual Computer: 1–28. doi:10.1007/s00371-013-0861-x.
In May 2013, graduate students and faculty members at Northeastern University's NULab for Texts, Maps, and Networks began work on Our Marathon: The Boston Bombing Digital Archive (www.northeastern.edu/marathon). Motivated by the interest Northeastern’s students and faculty members displayed in sharing their stories about the 2013 Boston Marathon bombings with one another, Our Marathon is an ambitious endeavor to create a central repository of stories and content related to the event and its aftermath. In the same ways that the September 11 Digital Archive and The Hurricane Digital Memory Bank utilized crowdsourcing to gather material, Our Marathon has reached out to a wide range of individuals (within and beyond the Boston community) to collect stories, photos, social media, and oral histories.

The project is invested in the role stories and community-building play in responding to traumatic events: it encourages members of the public to share the stories they may have already told about the Marathon on sites like Facebook, Twitter, and Instagram, and it provides a site for individuals to process and to reflect on the event in a variety of genres. Like its archival predecessors, Our Marathon is committed to creating a digital forum that has value to individuals in the immediate present and to researchers and other interested parties in the decades to come. Using the Omeka web-publishing platform, the archive has been attentive to the long-term preservation standards favored by archivists and university librarians, gathering and updating metadata on its items with DublinCore standards in mind. That being said, the project also seeks to convince an audience beyond these academic contexts that the work of digital humanists is also valuable to them.

The ongoing work of building the Our Marathon digital archive has raised several questions that may be of interest to other digital humanists. How do investments in academic and non-academic audiences inform a digital archive’s content, interface, and accessibility? How can digital archivists productively collaborate across disciplines at home institutions, with students and scholars at other academic institutions, with community organizations, and with media partners? What are some of the challenges of creating a digital archive about a traumatic event shortly after its occurrence and within close proximity to many of the communities and areas directly affected by that event? What are the advantages and disadvantages of crowdsourced initiatives? How can digital archives compellingly represent and catalogue material like Facebook statuses and Tweets for both researchers and the public? What steps can digital humanists take to ensure long-term preservation of their digital projects? We also encourage attendees to navigate the archive and to share their own stories with Our Marathon.
In a recent essay on the stock footage libraries amassed by Hollywood studios in the first half of the 20th century, Rick Prelinger—moving image archivist at the Internet Archive—laments that “archives often seem like a first-aid kit or a rusty tool, resources that we find reassuring but rarely use”  (Prelinger 2012). Although he doesn’t single them out by name, web archives are particularly vulnerable to this charge. User studies, access statistics, page views, and other metrics have in recent years told a consistent story: web content that has been harvested and preserved by collecting institutions, universities, and other organizations often lies fallow, and like Prelinger’s rusty tool may be notable more for its latent potential than for having served any real purpose (Hockx-Yu 2013; Kamps 2013; Huurdeman et al 2013).  While the reasons for neglect are myriad, this paper focuses on one: the lack of tools to support a wide range of interactions with the content. We describe initiatives underway at the University of Maryland to partially redress the problem and highlight the need for qualitative user studies.

The Internet Archive’s Wayback Machine is perhaps the best-known and most widely available tool to browse captured content. Both the Internet Archive’s main public site and Archive-It, its subscription-based web archiving service, replicate the experience of viewing web pages on the live web, thus reifying a “close-reading” experience. First developed in the mid-1990s, the software came of age at the same time digital humanities scholars were building the first generation of web collections aimed at providing high-resolution digital facsimiles of literary and artistic works by Blake, Rossetti, Dickinson, Whitman, and others. The emphasis on accurate rendering and display is thus a hallmark of both the Wayback Machine and many early DH projects, the latter of which likewise self-identify as “archives,” albeit archives on a dramatically smaller scale. 

Although the capabilities offered by the Internet Archive and other commercial services are significant, we believe considerable technical advances are needed if web archives are to fulfill their promise as tools of analysis as well as preservation.  Within the field of DH, the big data vistas offered by scholars such as Matt Jockers and Ted Underwood provide both inspiration and models on which to base these efforts (Jockers 2013). Unlike the boutique digitization initiatives that characterize the early wave of DH archives of the 1990s and early 2000s, which were often devoted to the works of a single author, the new macroanalytic approaches are premised on mass-digitization of print heritage. The paradigm they embody, moreover, is not digitization in the service of verisimilitude—reproductions that show exact fidelity to their originals—but rather digitization that produces terabytes’ worth of intermediary copies that can be cleaned, normalized, segmented, tokenized, mined, and visualized to yield new insights about the cultural record writ large. Such a paradigm disrupts the usual data-information-knowledge continuum by taking the unitary wholes of creative expression—the “cooked” novels or poems or historical documents in print—and temporarily degrading them to a “raw” data state so that they can be analyzed at scale to make higher-order knowledge claims. 

We believe that the technical infrastructure to support macroanalytics or “distant reading” on web archives today is inadequate. Existing tools were built before the coming of age of “big data” technologies and provide wobbly foundations on which to build analytical tools that scale to petabytes of data. As an example, the open-source Wayback Machine is implemented as a monolithic stack primarily designed to scale “up” on more powerful servers and expensive network-attached storage. Its architecture captures the ethos of “state-of-the-art” software engineering practices of the late 1990s. Not surprisingly, the field has advanced by leaps and bounds in the last decade and a half. In the 2000s, Google published a series of seminal papers describing solutions to its data management woes, which involve analyzing, indexing, and searching untold billions of web pages. Instead of scaling “up” on more powerful individual servers, the strategy entailed scaling “out” on clusters of commodity machines (Barroso et al., 2013). Before long, open-source implementations of these Google technologies were created, bringing the same massive data analytic capabilities to “the rest of us.” These systems form the foundation of what we know as “big data” today, and provide the backbone of data analytics infrastructure at Facebook, Twitter, LinkedIn, and many other organizations. Three key systems are:

The Hadoop Distributed File System (HDFS), which is a horizontally-scalable file system designed to store data on clusters of commodity severs in a fault-tolerant manner (Ghemawat et al. 2003). The largest known HDFS instance (by Facebook) holds over 100 petabytes.
Hadoop MapReduce, which is a simple yet expressive programming model for distributed computations that works in concert with data stored in HDFS (Dean and Ghemawat, 2004). MapReduce models analytical tasks in two distinct phases: a “map” phase where computations applied in parallel, followed by a “reduce” phase that aggregates partial results.
HBase, which is a distributed store for semi-structured data built on top of HDFS that allows low-latency random access to billions of records. Google’s Bigtable (Chang et al., 2006), from which HBase descended, powers Gmail, Google Maps, as well as the company’s indexing pipeline.
Modern big data technologies provide a technical path forward and an accompanying research agenda that does for web archives what macroanalytics or so-called “distant reading” has begun to do for digitized corpora in DH. As a first step in this effort, we are developing Warcbase, an open-source platform for storing, managing, and analyzing web archives built on the three technologies discussed above. The platform provides a flexible data model for organizing web content as well as metadata and extracted knowledge. We have built a prototype application that provides functionality comparable to the Wayback Machine in allowing users to browse different versions of resources in a web archive (typically as WARC or ARC files). Since Warcbase takes advantage of proven open-source technologies, we are confident of the infrastructure’s ability to scale in a seamless and cost-effective manner.

Yet Warcbase is only the beginning. We believe that our prototype—and, more generally, the technologies described above—will provide new capabilities that support innovative uses of web archives. Responsive full-text search on massive collections of web pages, one of the first items on a scholarly wishlist, is within reach: the tools exist in various open-source projects, awaiting integration. Longitudinal analyses of web pages such as tracking the frequency of person or place names become possible if we integrate off-the-shelf natural language processing tools. Yet another possibility is topic modeling on a massive scale; a separate project at the University of Maryland has built Mr.LDA, an open-source Hadoop toolkit for scalable topic modeling (Zhai et al., 2012). To provide a hint of what’s possible, we have been working with Congressional archives from the Library of Congress to explore topic modeling and large-scale visualizations of archived content, the results of which we will share during the conference presentation.

Why are large web archives so underused? It is surely not due to a lack of culturally significant material. Valuable content, ripe for exploration, ranges across topics such as electronic literature, alternate reality games, digital tools for human rights awareness, the Arab Spring uprising, and Russian parliamentary elections, to name just a few. Restrictive access regimes are partially to blame, but that alone does not provide a sufficient explanation. We believe that the issue, to a large extent, is a technological form of circular reasoning: scholars do little because the right tools don’t exist, and tool builders are hesitant to build for non-existent needs and users.  Progress is necessary to understand the essential activities, methods, and questions of researchers. Interviews with current web archive users are a start, but breakthroughs will require deep collaborations between scholars and technologists. The end goal is a comprehensive set of tools for researchers in the digital humanities and beyond to analyze and explore our digital cultural heritage. 

References
Archive-It: Web Archiving Services for Libraries and Archives. archive-it.org

Barroso, Luiz Andre, Jimmy Clidaras, and Urs Holzle. (2013). The datacenter as a computer: an introduction to the design of warehouse-scale machines (second edition). Morgan & Claypool Publishers. 

Chang, Fay, Jeffrey Dean, Sanjay Ghemawat, Wilson C. Hsieh, Deborah A. Wallach, Michael Burrows, Tushar Chandra, Andrew Fikes, and Robert Gruber. (2006). “Bigtable: A distributed storage system for structured data.”Proceedings of the 7th USENIX Symposium on Operating System Design and Implementation (OSDI).

Dean, Jeffrey and Sanjay Ghemawat. (2004). “MapReduce: Simplified data processing on large clusters.”Proceedings of the 6th USENIX Symposium on Operating System Design and Implementation (OSDI).

Ghemawat, Sanjay, Howard Gobioff, and Shun-Tak Leung. (2003). “The Google File System.”Proceedings of the 19th ACM Symposium on Operating Systems Principles (SOSP).

Hockx-Yu, Helen. (15 February 2013) “Scholarly use of web archives.” files.dnb.de/nestor/veranstaltungen/2013-02-27-scholarly-use-of-web-archives_public.pdf

Huurdeman, Hugo, et al. (2013). “Sprint methods for web archive research.”WebSci 2013 Proceedings of the 5th Annual ACM Web Science Conference:182-190.

Jockers, Matthew. (2013). Macroanalysis: digital methods and literary history. Urbana-Champaign: University of Illinois P. 

Kamps, Jaap. (1 August 2013). “When search becomes research and research becomes search.”SIGIR'13 Workshop on Exploration, Navigation and Retrieval of Information in Cultural Heritage (ENRICH). Dublin, Ireland. www.slideshare.net/jaap.kamps/sigir-workshop-enrich13

Moretti, Franco. (2013). Distant reading. London: Verso.

Moretti, Franco. (2007). Graphs, maps, trees: Abstract models for literary history. London; New York: Verso.

Prelinger, Rick. (2012). “Driving through Bunker Hill.” In Kraus, K. and Levi, A. (Eds.). Rough Cuts: Media and Design in Process. MediaCommons: The New Everyday. mediacommons.futureofthebook.org/tne/pieces/driving-through-bunker-hill

Zhai, Ke, Jordan Boyd-Graber, Nima Asadi, and Mohamad Alkhouja. (2012). “Mr. LDA: A flexible large scale topic modeling package using variational inference in MapReduce.” Proceedings of the 21th International World Wide Web Conference (WWW).

The term “distant reading” was coined by Franco Moretti in Graphs, Maps, Trees (2007) and has undergone further elaboration in his newest book (2013). See the “Works Cited” section for full bibliographic information.
How do readers use social media to express the value and the pleasure that the experience of reading holds for them? And, given the rapidity with which corpora gathered from social media are growing, what kinds of methods are most useful for analysing this kind of (big) data so as to cast light on the phenomenology of reading experiences? This paper seeks to answer these questions by presenting the findings of a project on developing methods for analysing and evaluating literary engagement in digital contexts, funded by the Arts and Humanities Research Council under the auspices of the Cultural Value Project.1 It will report on what can be learnt from the large amount of user-generated data available on microblogging services and social network sites about the value that reading brings to the lives of individuals and communities, and will offer an evaluation of the various analytical tools and methods available to scholars working on reading and reception studies who wish to include born-digital data in their research.

Work in reception studies is increasingly focusing on the ways that an understanding of the significance of individual reading experiences can be enriched by attending to occasions when readers join with others to express opinions about a text, and work together to construct its meaning. Scholars have argued that it is in fact in these acts of public negotiation of meaning – for example book group discussions – that readers can be observed doing the private cognitive work of textual engagement, as their interpretations change in the act of articulating their response in a social context.2 The fact that the rich textual data available on social media is often generated by readers in conversation with friends or acquaintances, in contexts quite different to interviews with researchers or questionnaires which might prompt a higher level of self-editing, makes it even more compelling to work with.3 The obvious advantage of working with this sort of born-digital material is that it lends itself to analysis using the growing number of tools and methods being developed within digital humanities, which have the power to integrate textual and geospatial information, and to identify lexical trends in time-stamped data. Such computational methods not only offer scholars the opportunity to analyse much larger bodies of text than is ordinarily possible for individual researchers to examine through close reading, but also to draw on, and discover patterns in, temporal and geospatial metadata.

Data for this project was gathered from two different social media platforms, the microblogging platform Twitter and the book collection website LibraryThing.4 For the Twitter data, searches were performed for literary prizes (for example Man Booker Prize and Nobel), author names (for example [Eleanor] Catton and [Alice] Munro), and hashtags commonly used to signal reading-related tweets (for example #goodreads and #mustread). For the LibraryThing data, the results of the Twitter searches were used to suggest particular books to investigate, so as to enable a comparison of the way readers discussed books on the two platforms. The numerical review scores and the text of user reviews of these books were stored in a database, along with metadata about the user. While some interesting work on literary value has already been done by scraping data from Amazon,5 LibraryThing was selected for this project as it is a platform where readers gather primarily to share information voluntarily about books in ways not (directly) linked to commercial activity. Moreover, it is also possible to link some of this information to users’ reported geographic location, something which cannot be done with Amazon data.

Various digital methods were then applied to the resulting datasets: thematic analysis using methods from corpus linguistics, analysis of trends in word usage over time using a burst detection algorithm, and geospatial analysis.

1) Thematic analysis
Analytical techniques from corpus linguistics were employed to identify patterns of unusually prominent words, phrases and grammatical constructions. The textual data gathered were tagged with the CLAWS part-of-speech tagger,6 and the concordance program AntConc7 was then used to identify the most frequent words, determine their statistical significance as compared to a reference corpus, find the terms that most commonly collocated with them, and carry out other analytical procedures. Sub-corpora were separated out by hashtag and geographical location, and analysed individually.

2) Temporal analysis
As all the Twitter data and a significant proportion of the LibraryThing data is time-stamped, it presented an opportunity to analyse trends over time, something that can be done with burst detection analysis in order to gauge how influential particular words or hashtags have been over time.8 The Sci2 tool9 was used to perform burst detection, and to visualise the results as temporal bar graphs. Terms that “burst” into prominence were then fed back into the corpus linguistic analysis, for example in order to examine the collocation patterns around them, and to attend to the context in which they initially appeared.

3) Geospatial analysis
The software package ArcGIS was used to create a GIS database including layers derived from the Twitter and LibraryThing data, to see where particular geographical patternings in the search terms and hashtags occurred. (While not all tweets or contributions to LibraryThing have georeferences attached to them, a large enough number do to make this form of analysis worthwhile.) These data were then layered against census data (such as level of educational attainment or socioeconomic status) aggregated at the output area level, in order to enable semantic patterns in the articulation of reading-related tweets and posts to be considered alongside the demographic features of the places where they were articulated.

The paper will set out the advantages offered by thematic, temporal and geospatial analyses, and suggest the components of cultural value which are best addressed by each, while also considering how these different forms of analysis may be productively combined.

References
1. www.ahrc.ac.uk/Funded-Research/Funded-themes-and-programmes/Cultural-Value-Project/

2. Daniel Allington and Bethan Benwell (2012), Reading the Reading Experience: An Ethnomethodological Approach to ‘Booktalk’, in From Codex to Hypertext: Reading at the Turn of the Twenty-first Century, ed. by Anouk Lang (Amherst, MA: University of Massachusetts Press, 2012), pp. 217–233.

3. Rhiannon Bury, Ruth Deller and Adam Greenwood (2013), From Usenet to Tumblr: The Changing Role of Social Media, Participations 10, 299–318.

4. https://twitter.com/; www.librarything.com/.

5. Ed Finn (2011), Becoming Yourself: The Afterlife of Reception, Pamphlets of the Stanford Literary Lab 3. 15 Sept 2011. litlab.stanford.edu/?page_id=255. 1 Nov 2013.

6. ucrel.lancs.ac.uk/claws/.

7. www.antlab.sci.waseda.ac.jp/software.html.

8. Jon Kleinberg b(2002), Bursty and Hierarchical Structure in Streams, Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’02 (New York: ACM, 2002), pp. 91–101.

9 sci2.cns.iu.edu.


    
        
            
                A Digital Humanities GIS Ontology: Tweetflickertubing James Joyce’s "Ulysses" (1922)
                
                    
                        Travis
                        Charles Bartlett
                    
                    Long Room Hub, Trinity College Dublin, Ireland
                    ctravis@tcd.ie
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    GIS
                    Ulysses
                    social media
                    literature
                    history.
                
                
                    databases &amp; dbms
                    geospatial analysis
                    interfaces and technology
                    historical studies
                    interface and user experience design
                    literary studies
                    data modeling and architecture including hypothesis-driven modeling
                    ontologies
                    text analysis
                    text generation
                    virtual and augmented reality
                    internet / world wide web
                    content analysis
                    bibliographic methods / textual studies
                    interdisciplinary collaboration
                    cultural studies
                    visualisation
                    social media
                    crowdsourcing
                    mobile applications and mobile design
                    maps and mapping
                    spatio-temporal modeling
                    analysis and visualisation
                    agent modeling and simulation
                    media studies
                    data mining / text mining
                    English
                
            
        
    
    
        
            This Digital Humanities GIS (DHGSI) model cross-pollinates literary and social media practices to engage in a participatory, performative, and augmented reality survey of the relations between James Joyce’s novel 
                Ulysses (1922) and digital eco-system productions of dialogical and social space (Goodchild, 2009; Sieber, Wellen, and Yuan 2011; Priem, 2011; Young and Gilmore, 2013; Graham and Zook, 2013; Lin, 2013). Joyce famously boasted that the aim of 
                Ulysses (which kaleidoscopically relates the urban journeys of student Stephen Dedalus and advertising salesman Leopold Bloom on 16 June 1904) was ‘to give a picture of Dublin so complete that if the city one day suddenly disappeared from the earth it could be reconstructed out of my book’ (Budgen, 1972, 69). The model integrates a 
                Ulysses schema outline with live geo-spatially enabled Twitter, Flickr, and YouTube posts to map the language operating in a Bloomsday-generated digital eco-system to re-create the discourse of a virtual Joycean Dublin during the annual celebration of the novel. Consequently the ‘Joycean’ neologism 
                tweetflickertubing was coined to describe the ontological shift indicated by the HGIS model’s methodology. 
            
            
                Creating the Model
            
            In 1920 Joyce drafted a schema outlining 
                Ulysses’ eighteen Homeric episodes for the Italian critic Carlo Linati, to whom he wrote, ‘in view of the enormous bulk and the more than enormous complexity of my damned monster-novel it would be better to send . . . a sort of summary-key-skeleton schema’ (Ellman, 1974, 187). The schema’s grid designates episode title, time, color, people, science/art, meaning, technic, organ, and symbols. To create the model an Excel spreadsheet (Figure 1) was populated with this data and geo-coded according to GPS-designated decimal degree locations of the 18 Homeric episode sites in Dublin. These sites were identified through the 1904 
                Thom’s Map of Dublin, Ian Gunn and Clive Hart’s 
                James Joyce’s Dublin: A Topographical Guide (2004), and GIS ‘ground-truthing’ methods, and checked against the ‘what’s here’ function of the Google Maps app. 
            
            Site centroids were approximately identified to concrete locations described in 
                Ulysses because the various characters’ movements and locations within the novel (such as the Wandering Rocks episode) occur simultaneously and at multiple sites within and beyond the geographical and temporal boundaries distinguishing each episode in the schema. The Excel database was imported into Google Fusion and visualized through its Google Maps function. The database was also converted to a CSV file and imported into the ArcGIS Online platform and integrated with a live social media map layer. 
            
            
                
            
            
                
            
            Figure 1: Fragments of the Carlo Linati schema, Google Fusion map, and Excel/CVS Geo database. 
            Surveying Bloomsday Digital Ecosystem 
            Surveys were taken on 16 June 2014 at hourly intervals, based upon the chronology from the Linati schema, and were divided into two categories: local (Dublin) and global, weighed by the indices of site and time, respectively. Several keywords—such as ‘James Joyce’, ‘Ulysses’, and ‘Bloomsday’, as well as character and episode names from the novel—were tested in the model’s Twitter, Flickr, and YouTube search engines. ‘Bloomsday’ received the highest number of hits and became the main survey keyword. 
            The local survey focused on activity around Homeric episode sites in Dublin, and found that Davy Byrne’s Pub on Duke Street and Joyce’s Martello Tower associated with the 
                Lestrygonians and 
                Telemachus episodes attracted the most posts. Tweet postings did not, however, correspond with the chronology of 
                Ulysses’ narrative outlined in the schema, illustrating that social media activity aggregated around site location rather than novelistic time. A Tweeted image (Figure 2) captures throngs of people in funny hats assembled around Byrne’s pub, and it seems that Joyce’s identification of the ‘Oesophagus’ as the body organ symbol for this episode was indeed apt. The National Library site was originally geo-coded as the centroid of the 
                Lestrygonians episode; however, survey results suggest that perhaps because of the social gravity indicated by the number of social media posts, the centroid should be re-located to the site of the pub, illustrating the iterative process integral to Neogeographical mapping practices. In the case of the global survey, Tweets blossomed across Europe, the Middle East, Asia, Australia-Pacific, North America, and Latin America during the entire chronology of the Linati schema. ‘Orphan’ Tweets (corresponding outside of the hourly periods not included in the schema) were placed either in preceding episode time slots or in the Penelope episode—whose time indices encompass ‘Infinity’ (see Bloomsday Tweet Schema in poster). 
            
            
                
            
            
                
            
            Figure 2. Live social media map integrated with the Linati schema geo-database. 
            Surveys taken before, on, and following Bloomsday 2014 illustrate that Flickr and YouTube postings with time lags, and reflecting activity over the course of a year, exhibited the most aggregated social media activity. However, over the course of the 16 June 2014 survey, it became apparent that dominant social media ecosystem activity on Bloomsday occurred in Twitter. The following verbal snapshot reflects a parsing of language activity (see Bloomsday Tweet Schema in poster) articulated in this digital eco-system: ‘People in Dublin are wearing funny hats because it is Bloomsday and elderly ladies are getting rowdy in Davy Byrne’s Pub; a wedding anniversary is observed in Glasnevin, North Dublin, while a Spanish tweeter celebrates with a Domino’s Pizza and the latest 
                X-Men film. Individuals in Dublin, Paris and Washington D.C. resolve to again attempt to read 
                Ulysses, and a tweeter in Uruguay mentions Bloomsday to her Irish boyfriend, who asks if the day has anything to do with flowers. A few literary minded types post Joycean lines from the novel, while two individuals from Dublin get suited up in Edwardian clothes to face the day; one tweeter reflecting on the day after the night before, asks if Bloomsday was a joke brought up in a drinking session. A tweeter from Mexico City advertises online translations of Joyce’s “lascivious” letters to his wife Nora Barnacle. The celebration of 
                Ulysses converges with perhaps a larger global event to provoke a Dublin tweeter to state that there is “Nothing like a combo of World Cup and bloomsday to hear people who don’t like either Joyce or football talk about both”. One wry observation from the Bronx asks if “Bloomsday is Paddy’s Day for posh people?” And two more tweets from the USA proclaim “I’m pretty sure Joyce would love hashtags”, and “To paraphrase Laurie Anderson: 
                Ulysses? Never read it”’. 
            
            A corollary can be made between Joyce’s writing technique in 
                Ulysses and the use of language in this Twitter-based digital ecosystem. Joyce’s stream-of-consciousness technique mimicked the various ways in which the human mind ‘speaks’ to itself, through complex fluid patterns, random interruptions, incomplete thoughts, half words, and tangents (Norris and Flint, 2000, 126). Tweets, limited to a certain number of characters, reflect Joyce’s technique by conveying both focused and random thoughts, and illuminate Graham and Zook’s (2013, 78) contention that the ‘digital dimensions of places are fractured along a number of axes such as location, language, and social networks with correspondingly splintered representations customized to individuals’ unique sets of abilities and backgrounds’. The DHGIS model illuminates how literary and historical tropes can aid in contextualizing and mapping social media activity through the creation of interpretive schemas to study interactions between language, behaviour, time, and place. 
            
            
                DHGIS Implications 
            
            Re-conceptualizing J. B. Harley’s observation that ‘text’ is a better metaphor for maps than the mirror of nature through the lens of the digital humanities, it can be seen that HGIS-generated ‘maps are a cultural text. By accepting their textuality we are able to embrace a number of different interpretative possibilities’ (1989, 4). The DHGIS model enabled digital inter-textual relationships between 
                Ulysses, the Linati schema, and the dialogical and social space reflected in the Bloomsday social media eco-system. Subsequently, digital humanities methodologies of 
                deformance and 
                ergodicity were applied as interpretative techniques. By translating one ontological form of discourse to another, 
                deformance applies 
                scientia to 
                poeisis and seeks to explain unitary and unique phenomena (such as the language activity in the Bloomsday social media ecosystem) rather than establish a set of general rules or laws (McGann and Samuels, 1999). 
                Ergodicity involves an interactive type of labor between the GIS practitioner/author/coder, reader/viewer, and mapping subject to create the potential multiple narrative paths composing a digital text. 
                Ergodic applications of GIS create non-linear narratives that converge and disrupt both quotidian and epochal chronologies of time and space. As a literary tool, this DHGIS model synchronizes the resulting layers of images, words, and vectors into contrapuntal, multi-dimensional digital narratives, providing the means to ‘reconnect the representational spaces of literary texts not only to material spaces they depict, but also reverse the moment’ (Staley, 2007; Thacker, 2005, 63). Lastly, DHGIS, digital, spatial, and heohumanities modelling techniques, integrated with radical statistics holds the potential to engage Qualitative &amp; Critical GIS / GIScience studies with reflexive epistemologies to address the situatedness and positionality of Big Data as it relates to sustainability initiatives, smart city planning, transport and public health, disaster preparedness, and social and regional conflict (Cope and Elwood, 2009; Aitken and Craine, 2009; Bodenhamer et al., 2010; Dear et al., 2011; Meir, 2011; Elwood et al., 2012; Kwan and Schwanen, 2012; Leventala, 2012; Kitchin, 2014; Travis, 2014). 
            
        
        
            
                
                    Bibliography
                    
                        Aitken, S. and Craine, J. (2009). Into the Image and Beyond: Affective Visual Geographies and GIScience. In Elwood, S. A. and Cope, M. (eds), 
                        Qualitative GIS: A Mixed-Methods Approach. London: Sage. 
                    
                    
                        Bodenhamer, D., Corrigan, J. and Harris, T. M. (2010). 
                        The Spatial Humanities. Indiana University Press, Bloomington. 
                    
                    
                        Budgen, F. (1972). 
                        James Joyce and the Making of ‘Ulysses’. Oxford University Press, Oxford. 
                    
                    
                        Bulson, E. (2001–2002). Joyce’s Geodesy. 
                        Journal of Modern Literature,
                        25(2): 80–96. 
                    
                    
                        Cope, M. and Elwood, S. A. (2009). 
                        Qualitative GIS: A Mixed-Methods Approach. Sage, London. 
                    
                    
                        Dear, M., Ketchum, J., Luria, S. and Richardson, D. (2011). 
                        Geohumanities: Art, History, Text at the Edge of Place. Routledge, New York. 
                    
                    
                        Ellman, R. (1974). 
                        Ulysses on the Liffey. Faber &amp; Faber, London. 
                    
                    
                        Elwood, S. Goodchild, M. and Sui, D.
                        (2012). Researching Volunteered Geographic Information: Spatial Data, Geographic Research, and New Social Practice.
                        Annals of the Association of American Geographers, 
                        10
                        (2): 1–20.
                    
                    
                        Goodchild, M. (2009). NeoGeography and the Nature of Geographic Expertise. 
                        Journal of Location Based Service,
                        3(2): 82–96.
                    
                    
                        Graham, M., and Zook, M. (2013). Augmented Realities and Uneven Geographies: Exploring the Geo-Linguistic Contours of the Web. 
                        Environment and Planning A,
                        45: 77–99. 
                    
                    
                        Gunn, I. and Hart, C. (2004). 
                        James Joyce’s Dublin: A Topographical Guide. Thames and Hudson, London. 
                    
                    
                        Harley, J. B. (1989). Deconstructing the Map. 
                        Cartographica,
                        26(2): 1–20. 
                    
                    
                        Kitchin, R. (2014). Big Data, New Epistemologies and Paradigm Shifts. 
                        Big Data and Society,
                        1: 1–12. 
                    
                    
                        Kwan, M.-P. and Schwanen, T. (2012). Critical Space-Time Geographies. In Schwanen, T. and Kwan, M.-P. (eds), 
                        Environment and Planning A, 
                        44(9): 2043–48.
                    
                    
                        Leventala, S. (2012). A New Geospatial Services Framework: How Disaster Preparedness Efforts Should Integrate Neogeography. 
                        Journal of Map &amp; Geography Libraries: Advances in Geospatial Information, Collections &amp; Archives,
                        8(2): 134–62. 
                    
                    
                        Lin, W. (2013). Situating Performative Neogeography: Tracing, Mapping, and Performing ‘Everyone’s East Lake’. 
                        Environment and Planning A,
                        45: 37–54.
                    
                    
                        McGann, J. and Samuels, L. (1999). Deformance and Interpretation. 
                        New Literary History,
                        30(1): 25–56. 
                    
                    
                        Meir, P. (2011). New Information Technologies and Their Impact on the Humanitarian Sector. 
                        International Review of the Red Cross,
                        93(884): 1239–63.
                    
                    
                        Norris, D. and Flint, C. (2000). 
                        Introducing Joyce. Icon Books, Cambridge. 
                    
                    
                        Priem, J. (2011). As Scholars Undertake a Great Migration to Online Publishing, Altmetrics Stands to Provide an Academic Measurement of Twitter and Other Online Activity. 
                        Impact of Social Science, http://blogs.lse.ac.uk/impactofsocialsciences/2011/11/21/altmetrics-twitter/. 
                    
                    
                        Sieber, R. E., Wellen, C. C. and Yuan, J. (2011). Spatial Cyberinfrastructures, Ontologies, and the Humanities. 
                        Proceedings of the National Academy of Sciences of the United States of America,
                        108(14): 5504–9. 
                    
                    
                        Staley, D. J. (2007). Finding Narratives of Time and Space. In Sinton, D. S. and Lund, J. J. (eds), 
                        Understanding Place: GIS and Mapping Across the Curriculum. Redlands: Esri Press, pp. 35–48.
                    
                    
                        Thacker, A. (2005). The Idea of a Critical Literary Geography. 
                        New Formations,
                        57(6): 56–73. 
                    
                    
                        Travis, C. (2014). Transcending the Cube: Translating GIScience Time and Space Perspectives in a Humanities GIS. Special Issue on Space-Time Research in GIScience. 
                        International Journal of Geographical Information Science, 
                        28(5): 1149–64.
                    
                    
                        Young, J. C. and Gilmore, M. P. (2013). The Spatial Politics of Affect and Emotion in Participatory GIS. 
                        Annals of the AAG, 
                        103(4): 808–23. 
                    
                
            
        
    



    
        
            
                Black Boxed Selves
                
                    
                        Gallagher
                        Rob
                    
                    King's College London, United Kingdom
                    bobcgallagher@gmail.com
                
                
                    
                        Roach
                        Rebecca
                    
                    King's College London, United Kingdom
                    rebecca.roach@kcl.ac.uk
                
                
                    
                        Apperley
                        Thomas
                    
                    University of New South Wales, Australia
                    t.apperley@unsw.edu.au
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Panel / Multiple Paper Session
                
                
                    surveillance
                    life writing
                    gaming
                    online culture
                
                
                    audio
                    video
                    multimedia
                    literary studies
                    metadata
                    philosophy
                    internet / world wide web
                    games and meaningful play
                    cultural studies
                    social media
                    media studies
                    English
                
            
        
    
    
        
            We are witnessing a rise in the politicization of absence- and presence-oriented themes such as invisibility, opacity, and anonymity, or the relationship between identification and legibility, or the tactics of nonexistence and disappearance, new struggles around prevention, the therapeutics of the body, piracy and contagion, informatic capture and . . . data mining.
            —Alexander R. Galloway, Black Box, Black Bloc
            Taking Galloway’s essay as its point of departure, this panel draws together three papers that work to unpack the idea of ‘the black-boxed self’, addressing some of the ways in which selfhood is understood, articulated, and monetized in an age when networked digital devices are transforming politics, medicine, play, and creative endeavour—not to mention academic research.
            Indeed, we propose that much of the most interesting work proceeding in the humanities today can be understood in terms of the attempt to engage, imagine, or theorize black-boxed selves of one kind or another. We might think, here, of the intentionally opaque or illegible selves addressed in Nicolas de Villiers’ elaboration (2012) of the strategies of ‘opacity’ deployed by those seeking to queer normative models of subjectivity and biography, or of anthropologist Gabriella Coleman’s work (2014) on the amorphous hacker collective Anonymous and their bids to evade online surveillance and censorship; of attempts to render one’s own body less cryptic, whether through the kinds of life-writing analysed by those working in the medical humanities or nascent practices of digital self-quantification and ‘personal informatics’ (see Whitehead, 2013; Schull, 2014); or of research into the ‘data images’ of users captured by technologies like Microsoft’s Xbox 360, a blockbusting videogame console that pioneered modes of covert user surveillance (Cybulski, 2014).
            Like these projects, our papers adopt a multidisciplinary approach, variously taking their cues from literary theory, phenomenology, the medical humanities, media archaeology, platform studies, and affect theory, and drawing too on the new tools and tactics available to scholars working on and with digital media. Between us we address a range of media and practices, from videogaming and Twitter fiction to YouTube roleplay and digital sousveillance. By addressing developments in these specific fields, our panel aims to broach broader questions regarding the ways in which emergent forms of digital black boxing are shaping identity work and self-expression today.
            References
            
                Coleman, G. (2014). 
                Hacker, Hoaxer, Whistleblower, Spy: The Many Faces of Anonymous. Verso, London.
            
            
                Cybulski, A. D. (2014). Enclosures at Play: Surveillance in the Code and Culture of Videogames. 
                Surveillance &amp; Society,
                12(3): 427–32.
            
            
                De Villiers, N. (2012). 
                Opacity and the Closet: Queer Tactics in Foucault, Barthes, and Warhol. University of Minnesota Press, Minneapolis.
            
            
                Galloway, A. R. (2011). Black Box, Black Bloc. In Noys, B. (ed.), 
                
                    Communization and Its Discontents: Contestation, Critique and Contemporary Struggles.
                London: Minor Compositions / AK Press, pp. 237–49.
            
            
                Schull, N. D. (2014). Time on Device: Slot Machine Design and the Turn Away from Risk in Gambling. Lecture at McGill University, Montreal. 10 April 2014.
            
            
                Whitehead, A. (2013). The Medical Humanities: A Literary Perspective. In Bates, V., Bleakley, A. and Goodman, S. (eds), 
                Medicine, Health and the Arts Approaches to the Medical Humanities. Hoboken, NJ: Taylor and Francis, pp. 107–27. 
            
            Paper 1
            ASMR Culture and the Reading of the Black-Boxed Self
            Gallagher, R.
            King’s College London
            Autonomous Sensory Meridian Response (ASMR) is a term used by online communities of people who claim to experience states of euphoric, even orgasmic, cranial ‘tingling’ when exposed to certain sensory stimuli. These communities have coalesced via online platforms where participants describe their ASMR experiences and swap media that ‘trigger’ them, typically videos rich in ambient sounds; soft, deliberate speech; meticulous activity, and/or stylized gestures. These sites and practices have in turn spawned a class of ‘ASMRtists’ who produce videos catering to those with the condition, their content ranging from whispered recitations of Eliot’s 
                The Waste Land to role-plays that see them posing as dentists, witches, or librarians.
            
            Perhaps inevitably, the claims made for ASMR (from inducing ecstasy to ameliorating anxiety and insomnia to reversing hair loss) and the dubiously pseudo-scientific discourse that has evolved around it have occasioned scepticism. Here, however, I want to suspend the question of whether ASMR is ‘real’, reasoning that, whether we grant the condition medical legitimacy or not, ASMR 
                culture offers a fascinating lens on the immemorial problem of opening up the ‘black box’ of the individual body to render somatic states and physiological phenomena shareable, and on the ways in which the networked black boxes inhabiting our desks, pockets, and living rooms now factor into this problem.
            
            In short, I am willing to risk ‘black boxing’ ASMR—accepting that, whatever its medical status, the concept now functions as an ‘intermediary’ in a variety of broader conceptual, technological, cultural, and corporeal networks (Latour, 2005, 39)—in order to address what the phenomenon tell us about both the construction of the self and the nature of humanistic research in the era of data mining, search engines, and social media. Galloway argues that ours has become a ‘cybernetic societ[y]’ in which black-boxed systems are not just pervasive but paradigmatic, and that, as a consequence, any account of contemporary culture and politics must address the terms upon which ideas and entities are allowed to appear and disappear; in response, I consider the conditions of ASMR’s appearance, paying particular attention to the fact that the phenomenon is essentially coeval (as term, concept, and claimable identity, if not as experience) with the host of developments connoted by the term ‘Web 2.0’. With the move from a text-centric to a multimodal web, the proliferation of webcams and microphones, and the emergence of blogging platforms, social networks, and content aggregators has come the ability, if not the imperative, to connect, express, sharem and generally ‘Broadcast [One]self’. By demonstrating that the social web can also be a platform for 
                diagnosing oneself, ASMR raises questions about the culture of the Web, and about how we ‘read’ cultural phenomena that seem to resist traditional scholarly approaches, exemplifying an era in which ‘unveil[ing] and decod[ing]’ may have run their course as critical strategies (Galloway, 2011, 243).
            
            My paper responds by identifying three modes of reading that, although I elaborate them in relation to ASMR, might equally serve to illuminate other online cultures. The first involves reading contextually—meaning, in the case of ASMR, drawing on media archaeology and the cultural history of nosology. The former insists that ‘the history of the media is not the product of a predictable and necessary advance from primitive to complex apparatus’ and that ‘instead of looking for obligatory trends, master media, or imperative vanishing points, one should be able to discover individual variations’ (Zielinski, 2006, 7); the latter, meanwhile, teaches us that conditions like green sickness, neurasthenia, and spermathorrea did not have to be real to have very real effects and authorising contexts (e.g., Rosenman, 2003), just as various conditions currently considered medically legitimate were once dismissed or relegated to the status of psychosomatic disorder (Fletcher, 2004). Both point to the importance of considering how online ASMR culture evokes (and sometimes invokes) other media and moments, not to erase differences but to bring specificities into keener focus.
            The second mode of reading involves attending to the terms upon which media texts are instrumentalized. This means reading cultural artefacts in relation to the strategies by way of which they are integrated into and made to facilitate web users’ everyday activities. ASMR video archives tend to tag material as either ‘intentional’ (that is, created for the express purpose of eliciting ASMR) or ‘unintentional’ (material that happens to be conducive to the desired state); ASMRtists’ works can, in turn, elicit suspicion, hilarity, curiosity, and distaste when experienced out of context. If this raises issues of ‘context collapse’ (boyd and Marwick, 2011) indicative of the way media become unmoored from their original circumstances and purposes online, it also raises question of cultural value; from an ASMR perspective, QVC presenters’ demonstrations of kitchenware, fan-made videogame ‘Let’s Play’ videos, and readings of modernist poetry can prove equally valuable means of accomplishing the ‘affective labour’ of modulating moods and physiological states (Andrejevic, 2011, 89). In this respect, ASMR culture asks us to think about the fate and function of cultural artefacts on the Web. Primed by Galloway, we might characterize its approach to cultural artefacts as cybernetic rather than hermeneutic: media become ‘inputs’ judged in terms of their ability to elicit particular affective ‘outputs’ rather than texts to be decoded. Networked digital devices, meanwhile, become tools for modulating and maintaining moods and atmospheres, creating ‘bubbles’ of pleasure, comfort, and belonging within environments otherwise experienced as empty or oppressive. Swapping media, testimonies, theories, and tips, ASMR ‘sufferers’ prompt us to consider the emergence of what Lauren Berlant (2011) calls ‘genres of the present’—discursive formats and cultural practices that enable their exponents to retain a sense of continuity and community in a sociocultural landscape increasingly characterised by anxiety, precarity, and flux. 
            Finally, I propose reading cultural projects in terms of the use they make of the Web’s affordances. From this perspective, we can see ASMR culture as the product of an Internet seeking to transcend text, and it was surely Web 2.0’s move to foster the creation and dissemination of audiovisual content that allowed the varieties of acoustic (rather than, say, haptic or olfactory) experience that ‘sufferers’ recount to come to prominence. At the same time, however, ASMR culture points to the continuing privilege afforded language on a web where even nonverbal content is found and filed by way of text (the gradual rise of image- and speech-based search technologies notwithstanding). Looking back at the blogs and forums across which a discourse of ASMR developed, we can see how the project of forging a common vocabulary was both enabled and constrained by the structures, features, and implicit logics of different online platforms. Blogs’ ‘tag clouds’, for example, now operate as records of participatory etiology at work, showing how certain terms and themes ballooned as consensus built behind them while others were doomed to remain outliers. We see, too, how the ASMR community continues to pursue both personal experiences and public recognition of the condition by making imaginative use of familiar tools. Thus, Reddit’s ‘upvote’ system, conventionally used as an all-purpose measure of appreciation or interest, is, on the r/asmr ‘subreddit’, recast as a means of gauging the efficacy of different media as triggers and so refining the definition of the condition. By introducing new usage conventions, the ASMR community has refunctioned the website’s interface as a tool of definition and quantification. Turning the same black box to innovative new ends, this strategy offers a microcosmic hint as to how other black-boxed systems might be made to yield new and strange cultural outputs.
            References
            
                Andrejevic, M. (2011). Social Network Exploitation. In Papacharissi, Z. (ed.), 
                A Networked Self : Identity, Community and Culture on Social Network Sites. London: Routledge, pp. 82–101.
            
            
                Berlant, L. (2011). 
                Cruel Optimism. Duke University Press, Durham, NC.
            
            
                boyd, D. and Marwick, A. E. (2011). I Tweet Honestly, I Tweet Passionately: Twitter Users, Context Collapse, and the Imagined Audience. 
                New Media &amp; Society,
                13: 114–33.
            
            
                Fletcher, C. (2004). Dystopothesia: Emplacing Environmental Sensitivities. In Howes, D. (ed.), 
                Empire of the Senses: The Sensual Culture Reader. Oxford: Berg, pp. 380–98.
            
            
                Galloway, A. R. (2011). Black Box, Black Bloc. In Noys, B. (ed.), 
                
                    Communization and Its Discontents: Contestation, Critique and Contemporary Struggles.
                 London: Minor Compositions / AK Press, pp. 237–49.
            
            
                Latour, B. (2005). 
                Reassembling the Social: An Introduction to Actor-Network-Theory. Oxford University Press, Oxford.
            
            
                Rosenman, E. B. (2003). 
                Unauthorized Pleasures: Accounts of Victorian Erotic Experience. Cornell University Press, Ithaca, NY.
            
            
                Zielinski, S. (2006). 
                Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. MIT Press, Cambridge, MA.
            
            Paper 2
            Literature, E-Health, and the Black-Boxed Self
            Roach, R.
            King’s College London
            It is thus today no longer a question simply of the enemy’s black box, but the black boxing of the self, of any node contained in a network of interaction. The enemy’s machine is not simply a device in a German airplane, it is ourselves: a call center employee, a card reader at a security checkpoint, a piece of software, a genetic sequence, a hospital patient. The black box is no longer a cipher waiting to be unveiled and decoded, it is a function defined exclusively through its inputs and outputs. 
            —Galloway, 2011, 243
            Alexander R. Galloway’s mention of a hospital patient falls at the end of a list, a seemingly casual example among many of the black boxing of the self in contemporary society. It is, however, the patient who forms the basis of this talk, the central figure in an argument that considers the import of Galloway’s notion for the medical humanities today. As e-health becomes a central and mushrooming area for health care, medical research, and patient experience, it is as well that the medical humanities reflect on their own involvement with the digital. Galloway’s description of the ‘black-boxed self’ seems to me a useful place to begin teasing out some of the issues around representations of pain, illness, and the body in an online setting. Explicitly interdisciplinary in its discussion of research in e-health, the medical humanities, life writing, literary studies, and the digital humanities, I use two case studies as a means of focusing my discussion.
            The first, Jennifer Egan’s 2012 tweeted story ‘Black Box’, explores, against the explicit backdrop of government spying, the fusing of technologies of surveillance with the human body. In this piece of serialized digital literature, the female speaker’s body has become the black-box recorder in a theatre of war, complete with ‘universal data port’ and devices implanted in the ear and eye, under the hairline, and between the toes. Fusing body and technology, Egan’s story also emphasizes the ‘Disassociation Technique’, a process for dividing the self from bodily experience and pain. Black boxed though the body may be, Egan seems to suggest the possibility of a self outside of and distinct from the confines of that box. 
            Illuminating contemporary anxieties around surveillance, digital technologies, and the human body, Egan’s story posits a troubling relationship between aesthetics and the biodigital. Technical advances here become a nightmare perpetuation of female objectification, and even the form of the fiction, a series of tweets purportedly offering the narrator’s stream of consciousness, seems perilously close to the serialized outputs of a sousveillance device. While Egan’s story offers an important example of the ambivalence often characterising literary responses to digital advances and the black-boxed self, the denunciation of such technologies and associated conceptualisations is often much less straightforward within a medical setting. 
            Take the example that forms my second case study: epilepsy, and its online, social media, and app provision. The most common serious brain condition worldwide, epilepsy affects 50 million people, with around 80% of patients living in developing countries (WHO, 2012). A chronic condition identified by a tendency to experience seizures, people with epilepsy regularly experience the black boxing of themselves. The event that not only identifies their condition but also frequently stigmatises them in society is (in most forms of the condition) inaccessible to the individual. 
            In addition, while earlier medical approaches emphasized the diagnosis and location of seizure origin in the brain, this has been largely superseded by a black-boxed approach. Managing symptoms, with continued self-monitoring by patients, is now the focus of much epilepsy medicine. This turn has led, not only to research attention being focused on how patients with epilepsy access information, advice, and support via the Web (known to improve well-being) but also to the development of numerous apps designed to aid self-management (Hoch et al., 1999; Shegog et al., 2013; Walker et al., 2012; Koenig et al., 2007; Wicks et al., 2012). These include those that track medications and symptoms, to those that monitor heart rate and breathing, alerting carers if unusual activity suggests that a seizure might be occurring. 
            On the one hand, then, the monitored subject becomes the quintessential black box, reduced to a series of inputs and outputs, with any sense of interiority rendered problematically disrupted or inaccessible. On the other hand, such technologically aided self-management can markedly improve the patients’ experience of their condition, reducing hospital admissions and some of the dangers associated with (clonic-tonic) seizures. Furthermore, this sousveillance offers the subject a form of technologized narrative that might capture those events lost to consciousness. Can, in fact, the black boxing of the self offer the subject a new and potentially useful form of life writing? 
            In reflecting on these issues, I turn to the medical humanities, itself part of a response to the behaviourist understanding of the self (that also models the black box) so dominant in medicine at mid-century. Over the last 30 years, scholars such as Arthur Frank (2013), Arthur Kleinman (1988), Rita Charon (2006), and Kathryn Montgomery (2006) have developed the field of so-called illness narratives. Attending to the ways in which individuals experience, understand, and represent their illnesses, this scholarship has had a significant impact not only on scholars interested in thinking about relations between literature and medicine but also with medicine itself and the generalized move towards patient-centred care. Attempting to respond to and resist the conceptualization of the patient as a series of symptoms or outputs, the behaviourist black box is (theoretically at least) repudiated in favour of narrative-sensitive medicine that is conscious of the individual experience (Vaccarella 2011).
            Yet in the increasingly technologized world of medicine, within the burgeoning field of e-health, how precisely such illness narratives might operate in an online setting becomes a complex question. What, for example, does it mean to tell an illness narrative via sousveillance? Drawing on semi-structured interviews conducted for a qualitative study into epilepsy patients’ self-representation online and on recent work in digital life writing and the medical humanities (O’Riordan, 2011; Waldby, 2000; McLellan, 1997; Arthur, 2009; Zuern, 2003; McNeil, 2012), this paper will reflect on the particularities of writing (epilepsy) lives online. 
            References
            
                Arthur, P. L. (2009). Digital Biography: Capturing Lives Online. 
                A/B,
                24(1): 74–92.
            
            
                Charon, R. (2006). 
                Narrative Medicine: Honoring the Stories of Illness. Oxford University Press, New York.
            
            
                Egan, J. (2012). Black Box. Tweeted 23 May 2012+, @NYerFiction. Also available at New Yorker Online, 4 June 2012, http://www.newyorker.com/magazine/2012/06/04/black-box-2.
            
            
                Frank, A. W. (2013). 
                The Wounded Storyteller: Body, Illness, and Ethics. 2nd University of Chicago Press, Chicago.
            
            
                Galloway, A. R. (2011). Black Box, Black Bloc. In Noys, B. (ed.), 
                Communization and Its Discontents: Contestaton, Critique, and Contemporary Struggles. New York: Minor Compositions, pp. 237–49. 
            
            
                Hoch, D. B., Norris, D., Lester, J. E. and Marcus, A. D. (1999). Information Exchange in an Epilepsy Forum on the World Wide Web. 
                Seizure: The Journal of the British Epilepsy Association,
                8(1): 30–34.
            
            
                Kleinman, A. (1988). 
                The Illness Narratives: Suffering, Healing, and the Human Condition. Basic Books, New York.
            
            
                Koenig, S. A., Dobson, P., Longin, E., Michalec, D., Gerstner, T. and Taylor, J. (2007). Internet-Based Knowledge Management Database for Children and Adults with Epilepsy: A Possible Model Project for Evidence-Based Medicine in the Future. 
                Seizure: The Journal of the British Epilepsy Association,
                16(8): 703–8.
            
            
                McLellan, F. (1997). ‘A Whole Other Story’: The Electronic Narrative of Illness. 
                Literature and Medicine,
                16(1): 88–107.
            
            
                McNeil, K., Brna, P. M. and Gordon, K. E. (2012). Epilepsy in the Twitter Era: A Need to Re-Tweet the Way We Think about Seizures. 
                Epilepsy and Behavior,
                23(2): 127–30.
            
            
                Montgomery, K. (2006). 
                How Doctors Think: Clinical Judgment and the Practice of Medicine. Oxford University Press, Oxford.
            
            
                O’Riordan, K. (2011). Writing Biodigital Life: Personal Genomes and Digital Media. 
                Biography: An Interdisciplinary Quarterly,
                34(1): 119–31.
            
            
                Shegog, R., Bamps, Y., Patel, A., Kakacek, J., Escoffery, C., Johnson, E. K. and Ilozumba, U. O. (2013). Managing Epilepsy Well: Emerging E-Tools for Epilepsy Self-Management. 
                Epilepsy and Behavior,
                29(1): 133–40.
            
            
                Vaccarella, M. (2011). Narrative Epileptology. 
                Lancet,
                377: 460–61.
            
            
                Waldby, C. (2000). Virtual Anatomy: From the Body in the Text to the Body on the Screen. 
                Journal of Medical Humanities,
                21(2): 85–108.
            
            
                Walker, E. R., Bamps, Y., Burdett, A., Rothkopf, J. and Dilorio, C. (2012). Social Support for Self-Management Behaviors among People with Epilepsy: A Content Analysis of the WebEase Program. 
                Epilepsy and Behavior,
                23(3): 285–90.
            
            
                WHO. (2012). Epilepsy
                . Factsheet No. 999, http://www.who.int/mediacentre/factsheets/fs999/en/ (accessed 1 September 2014).
            
            
                Wicks, P., Kleininger, D. L., Massagli, M. P., de la Loge, C., Brownstein, C., Isojärvi, J. and Heywood, J. (2012). Perceived Benefits of Sharing Health Data between People with Epilepsy on an Online Platform. 
                Epilepsy and Behavior,
                23(1): 16–23.
            
            
                Zuern, J. (ed). (2003). Online Lives. Special issue of 
                Biography,
                26(1): v–xxv.
            
            Paper 3
            Gestural Excess, Gaming Style, and New Regimes of Surveillance
            Apperley, T.
            University of New South Wales
            Motion gaming was popularized through the release of the Wii in 2006. The wireless Wiimote operated through a combination of a motion sensor and an infrared pointer, as well as the more traditional buttons, directional pad, and trigger. Similar devices were incorporated into the other major consoles: Sony released the PlayStation Move, a motion-sensing controller for the PlayStation 3, in 2010; the add-on Kinect was also released in 2010 for the Microsoft Xbox 360. The Kinect introduced the ‘natural’ user interface to commercial gaming. It is a ‘controller-less’ device that uses a 3D camera advanced enough to allow motion capture and facial recognition. The technologies combine to create a computer interface that can be used intuitively, through a combination of motions, movements, and gestures that are recognised by the Kinect. This device allows people to play games—and use other media—on the Xbox 360 and Windows PC through movement and voice alone, making it the contemporary gaming technology that is most intimately linked to the body. The product is incredibly popular; in the 60 days following its release the Kinect sold 8 million units, becoming the world’s ‘fastest selling consumer electronics device’ (Moses, 2011). 
            These platforms make use of ‘gestural excess’ (Freeman et al., 2012; Simon, 2009), the labour that is produced by the body at the location of play, but is not registered by the digital game as an input. In this gestural excess there is a nascent, but precarious, sense of 
                style that formerly dissipated into everyday life, but is now indexed by popular visual culture. Gestural excess includes movements made unconsciously and those that are made deliberately as an act of ‘style’. Gesture and gestural excess in gaming have existed since the technologies’ inception—and indeed some gestures were possibly inherited from technological precursors like pinball (Huhtamo, 2005) and video art (Wilson, 2004; 2008). However, subsequent to the mass popularity of the Wii, the body of the gamer and gestural excess are increasingly understood by the digital games industry—and in the popular imagination—as an integral part of gaming. New forms of control in gaming that are based on ‘natural’ user interfaces using motion sensing, touch, and voice seem to offer a new potency for gesture. Buttons, for example, rarely mimic ‘natural’ actions or gestures (Parker, 2008; Shinkle, 2008, 908), and it is the limited recognition of human input by systems for which the player essentially remains a black box that establishes the possibility of gestural excess; it doesn’t matter how hard or fast that the button is hit. The ‘natural’ user interface, which operates through an ‘intuitive’ recognition of gesture, signals the possibility for the 
                recuperation of gestural excess. This possibility is considerably enhanced by the shift toward motion-sensing gaming, which has vastly increased the capacities for gesture to be accurately tracked, recorded, and incorporated into play. 
            
            However, while motion gaming offers new possibilities for expression, it also is subject to new regimes of surveillance. This shift is largely centred around gestural excess, the movements and gestures made during play that have no effect on the algorithmic or representational processes of the game, but in advertising materials are ‘represented as the very source of fun and pleasure in the game’ (Simon, 2009, 11). The significance of gestural excess is that it is not registered by the gaming technology; it is in no way programmed or coded (Simon, 2009, 12–13). Yet once gestural excess and the body are acknowledged as significant sites, and rendered spectacular and public, they are vulnerable to new forms of surveillance (Berenhausen, 2007; Millington, 2009), not just by members of the public, but through the ever-increasing accuracy of the black-boxed technologies of gaming themselves.
            Gestural excess opens new possibilities for the body to be incorporated into game play. Gestural excess always operates outside, but in relation to, computational processes, and as these processes become more efficient, and domestic motion-sensing technologies like the Xbox Kinect become more ubiquitous, the operational spaces of gestural excess will become increasingly limited. This is because the smooth operation and everyday implementation of the ‘natural’ user interface that uses the body as a controller 
                requires that gestural excess is understood as a glitch. Thus, the growing significance of motion-sensing controllers suggests both an 
                opening and immanent foreclosure for bodily expression in gaming through gestural excess.
            
            References
            
                Behrenhausen, B. G. (2007) Toward a (Kin)aesthetic of Video Gaming: The Case of Dance Dance Revolution. 
                Games and Culture,
                2(4): 335–54.
            
            
                Freeman, D., Hilliges, O., Sellen, A., O’Hara, K., Izadi, S. &amp; Wood, K. (2012). The Role of Physical Controllers in Motion Video Gaming. In 
                Conference Proceedings of DIS 2012—In the Wild, Newcastle, UK, 23–25 June 2012, http://research.microsoft.com/pubs/168081/msrc_props.pdf.
            
            
                Huhtamo, E. (2005). Slots of Fun, Slots of Trouble: An Archaeology of Arcade Gaming. In Raessens, J. and Goldstein, J. H. (eds), 
                The Handbook of Computer Game Studies. Cambridge, MA: MIT Press, 3–25.
            
            
                Millington, B. (2009). Wii Has Never Been Modern: ‘Active’ Video Games and the ‘Conduct of Conduct’. 
                New Media &amp; Society,
                11(4): 621–40.
            
            
                Moses, A.
                (2011). Microsoft Kinect World Record: Selling Faster Than iPhone, iPad,
                
                     
                
                Sydney Morning Herald
                
                    , 11 March, 
                
                http://www.smh.com.au/digitallife/games/microsoft-kinect-world-record-selling-faster-than-iphone-ipad-20110310-1bokr.html.
            
            
                Parker, J. R. (2008). Buttons, Simplicity and Natural Interfaces. 
                Loading . . . ,
                2(2), http://journals.sfu.ca/loading/index.php/loading/article/view/33/30.
            
            
                Shinkle, E.
                 (2008). Video Games, Emotions, and the Six Senses.
                
                     
                
                Media, Culture &amp; Society, 
                30
                (6): 907–15.
            
            
                Simon, B. (2009). Wii Are Out of Control: Bodies, Game Screens and the Production of Gestural Excess, 
                Loading . . . ,
                3(4): 1–17, http://journals.sfu.ca/loading/index.php/loading/article/view/65/59.
            
            
                Wilson, J. (2004). Games, Video Art, Abstraction and the Problem of Attention. 
                Convergence,
                10(3): 84–101.
            
            
                Wilson, J.
                (2008). ‘Participation TV’: Videogame Archaeology and New Media Art. In
                
                     
                    Swalwell, M. and Wilson, J. (eds), 
                
                The Pleasures of Computer Gaming: Essays on Cultural History, Theory and Aesthetics
                . Jefferson, NC: McFarland, pp. 94–117.
            
        
    



    
        
            
                Collaborative, Speculative Technologically-Enhanced Mobile Libraries, Or How Davidson College Students Learned to Stop Worrying and Love The Library
                
                    
                        Christian-Lamb
                        Caitlin
                    
                    Davidson College, North Carolina, United States of America
                    cachristianlamb@davidson.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Poster
                
                
                    mobile libraries
                    community-building
                    digital humanities pedagogy
                    collaborative projects
                    GLAM
                
                
                    archives
                    repositories
                    sustainability and preservation
                    teaching and pedagogy
                    GLAM: galleries
                    libraries
                    archives
                    museums
                    digital humanities - pedagogy and curriculum
                    mobile applications and mobile design
                    English
                
            
        
    
    
        
            This poster will discuss a collaborative assignment given to Dr. Mark Sample’s DIG 350–History and Future of the Book class, in its first iteration of being taught. The course was offered through the Digital Studies Department in fall 2014 at Davidson College, a small, private liberal arts college in North Carolina in the United States. 
            As part of their coursework, the 10 enrolled undergraduate students were asked to come up with speculative designs of a ‘technologically enhanced mobile library’, with assistance from the college’s associate archivist, Caitlin Christian-Lamb. This project was born out of collaboration between Sample and Christian-Lamb, after discussing a shared interest in experimental literature, mobile libraries, literacy, and community-building. The assignment was worth 10% of each student’s grade, and when planned by Sample and Christian-Lamb, included a series of discussions on libraries, archives, and mobile capabilities to set the table for students’ free-form exploration of possible designs. The concept of the technologically enhanced mobile library as described to the class was a combination community bookshelf, literacy outreach module, and experimental book-based playground of sorts, with many of the core elements and purposes left to students to define and expand upon. The example mobile library design given was of a bicycle with bookshelves attached, tracked by GPS and with its changing collections monitored by weekly photographs uploaded to a website or Twitter account. A possible research use would be to gather statistics on literacy and reading rates of local schools, and then place the mobile library near school buildings, advertise its services, and reevaluate whether this had an effect on the amount that students read, or increased levels of literacy.
            However, prior to the first discussion, out of curiosity Christian-Lamb asked the students to take the Pew Research Internet Project’s Library User Quiz, and was surprised that find that, although the majority of the students fell into the Pew-defined ‘Library Lovers’ and ‘Information Omnivores’ groups, the group data indicated that a smaller percentage than the general public felt that libraries promoted literacy or that a local public library closure would have a major impact on the community as a whole. Based on these results, the first project discussion instead veered in the direction of libraries as public intellectual commons, the role of librarians and archivists, and what values library services are designed to fill. Subsequent class discussions elaborated ideas of libraries and archives as social justice centers; idealized possible libraries; libraries as playful, creative spaces (and the relationship of libraries to makerspaces); and Little Free Libraries and other DIY collections.
            Possible outputs for the assignment included designs of a technologically enhanced mobile library; opinion papers or blogs on whether these mobile libraries should exist and what their functions should be, or on the place of mobile community-aimed technologies at an academic, nonpublic library setting; models or interactive websites; games based on the topic of mobile libraries; or any other free-form product approved by the instructors. In addition to illustrating the student outputs and future research possibilities, this poster will share the resources curated for class discussion on libraries, archives, makerspaces, and their community values, as well as demonstrating one example of an experimental pedagogy and the ways in which open, synergistic exploration of preconceived notions on behalf of the students and instructors resulted in a better product than the original assignment.
        
    



    
        
            
                KinDigi Social: A Mobile-centered Social Annotation Platform for the Kindai Digital Library
                
                    
                        Hashimoto
                        Yuta
                    
                    Kyoto University, Japan
                    yhashimoto1984@gmail.com
                
                
                    
                        Araki
                        Yasuyuki
                    
                    National Diet Library, Japan
                    ya-araki@ndl.go.jp
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Short Paper
                
                
                    social annotation
                    digital collections
                    online collaboration
                    open annotation
                
                
                    historical studies
                    interface and user experience design
                    software design and development
                    programming
                    social media
                    crowdsourcing
                    semantic web
                    linking and annotation
                    English
                
            
        
    
    
        
            The Kindai Digital Library (KDL) is an online collection of out-of-copyright books published in Japan in the 19th and 20th centuries run by the National Diet Library (NDL) of Japan.
                1 It offers full access to images scanned from books previously only available in the NDL. As of November 2014, the total number of volumes in the collection has grown to around 360,000. Although digitized text of the scans is not available due to the technical difficulties of applying OCR to old Japanese type fonts, the collection has been a precious resource for studies of the history of modern Japan since its start in 2003.
            
            KinDigi Social is an online platform for social annotation of digitized books in KDL; it offers iOS and Android clients to browse books in the KDL collection and provides a set of features to create and share annotations and tags, allowing users to exchange and discuss historical knowledge and ideas concerning the books. The project is one of the experimental projects supported by the NDL Lab, an R&amp;D section of NDL,
                2 and is going to be available at http://kindigi-social.org/ by the spring of 2015. In this paper, we will discuss the aims, methods, and implementation of the KinDigi Social platform.
            
            Aim of the Project
            The past several years saw growing interest in the use of social media for humanities studies in the digital humanities world. Possibilities of online collaboration for research in the humanities have been discussed on various occasions and put into practice in a number of research projects (Siemens et al, 2012; Barr and Tonra, 2014). Our research attempt is to apply the methodologies and techniques suggested by the results of those studies to the KDL. The need for such an attempt becomes clear when we see the numerous references and comments on books in the KDL made by professional and amateur historians not only in their academic papers but also in their blogs and social media posts (search ‘kindai.ndl.go.jp’ on Twitter, for instance). Although this sort of user-generated content is often quite valuable in that it can provide better understandings of the historical context of books, it has been left scattered over the Web and has not been archived for later references, which has resulted in a great loss of intellectual resources. Thus KinDigi Social aims to be a platform that aggregates and accumulates such annotations, and enables interdisciplinary discussion and collaboration among scholars for the purpose of further knowledge creation.
            Methods
            KinDigi Social allows its users to create annotations on both image region parts of a book and on the text itself. Each annotation accepts replies from other users so that idea-exchanging and discussion can take place. It is also possible to label each annotation with Twitter-like hashtags. On the other hand, the mere capacity to create annotations is not enough to build a sustainable social medium that will keep driving engagement of users and that can endure long-term use as an archive of historical knowledge; there are difficulties concerning user interfaces, information sharing, archiving and reuse of user-generated content, among other issues. Therefore we took the following three additional measures:
             • 
                Mobile first development: Since currently the KDL doesn’t offer a user interface optimized for mobile devices, its users need to keep their eyes on PC displays to read the books held in the KDL. The loss of productivity derived from the limitations of this user interface cannot and should not be ignored. Although a desktop client is also planned for KinDigi Social, we have especially focused on building a rich mobile client that enables users to access the KDL almost anywhere using their iPads or other mobile devices, and provides them the same user experience gained from the use of mobile readers such as the Amazon Kindle (see Figure 1). One large problem with displaying scanned book images on mobile devices is their size; the images from the KDL have margins surrounding the actual books. For this we developed a lightweight image analysis library that automatically detects edges in a book image and removes unnecessary margins from it (see Figure 2). The code is open-source and available on Github.
                3
            
             • 
                Support for real-time collaboration: As the recent success of social media shows, real-time communication between users is indispensable for their continual engagement. In order to support this kind of collaboration, KinDigi Social implements the following Twitter-like system: user activities, such as the creation of an annotation, will immediately prompt a notification sent to the user’s ‘followers’ through their social feeds, for real-time responses and information sharing (see Figure 3). Likewise it is possible to ‘watch’ any book in KDL just like a GitHub repository so that the user will get a notification when an annotation is created on a book. This system also contributes to forming loosely bound clusters of users. For those who do not want to make their annotations public, support for private annotations is also planned.
            
             • 
                Data modeling following the Open Annotation specification: As a description model of user-generated annotations, KinDigi Social implements the Open Annotation Data Model, an RDF-based framework designed for modeling annotations on web content whose specification has been developed by a W3C community group (Hunter, 2010).
                4 It provides controlled and machine-friendly vocabulary for modeling not only annotations but also replies from other users, folksonomy tags, and links to other resources. The adoption of the Open Annotation framework makes it possible to store and serialize user-derived content in an interoperable and reusable way for their long-term preservation and for secondary uses such as text analysis.
            
            
                
            
            Figure 1. The iOS client of KinDigi Social running on an iPad.
            
                
            
            
                
            
            Figure 2. An example of the automatic detection of book edges.
            
                
            
            
                
            
            Figure 3. An example of social feed of user activities.
            
                
            
            
                
            
            Figure 4. System architecture of KinDigi Social platform.
            Implementation
            As shown in Figure 4, KinDigi Social is a server-client system. The web server is written in Ruby on Rails and is hosted on a physical server in the NDL Lab where it retrieves scanned images and the bibliographic metadata of books in the KDL through the APIs provided by the NDL. It also exchanges JSON messages with its iOS and Android clients through its REST API. The mobile clients are built with HTML5 using the Apache Cordova framework,
                5 so that a single source code will generate distributions for different platforms. 
            
            Annotation data generated by users is at first stored in a relational database (PostgreSQL), and is regularly converted and dumped into an RDF database and made public through its SPARQL endpoint. In this way annotation data can be easily retrieved and processed by standard semantic web technologies. We chose a relational database as a primary method of data storage purely for performance and maintenance reasons.
            The user authentication is done via OAuth 2.0 protocol; users need to use their Facebook or Twitter account to log in to KinDigi Social (Authentication via OpenID is also planned). This implementation is intended to make it easy to connect with other social media platforms and to prevent vandalism by anonymous users. 
            Conclusion and Future Directions
            Our basic standpoint is that every historical digital collection should behave like a living organism; it should continue to grow, absorbing its users’ knowledge and ideas and connecting to other historical resources. The online collaboration of scholars enhanced by real-time and mobile computing, and the standardized archive system built with Open Annotation model will contribute to the transformation of currently static digital collections into such dynamic organisms.
            Although our project is still in its very early stages and we first need to evaluate how KinDigi Social can contribute to humanities research based on digital resources, a possible extension of our project would be to include support for multiple digital archives, and collections other than the KDL. A prime candidate is the digital archive administered by the Japan Center for Asian Historical Records (JACAR),
                6 which holds the official documents of the Japanese Cabinet dating from the period prior to World War II. If it were possible to search the annotations created in both the KDL and JACAR seamlessly with a single query, that would greatly benefit studies of the history of modern politics in Japan, and we therefore hope to offer this functionality with our project.
            
            Notes
            1. Kindai Digital Library. http://kindai.ndl.go.jp/?__lang=en.
            2. NDL Lab, http://lab.kn.ndl.go.jp/cms/.
            3. Kindai-cropper (a github repository). https://github.com/yuta1984/kindai-cropper.
            4. Open Annotation Data Model, http://www.openannotation.org/spec/core/.
            5. Apache Cordova, http://cordova.apache.org/.
            6. Japan Center for Asian Historical Records, http://www.jacar.go.jp/english/index.html.
        
        
            
                
                    Bibliography
                    
                        Barr, R. A. and Tonra, J. (2014). Crowdsourcing Annotation and the Social Edition: Ossian Online. 
                        DH2014.
                    
                    
                        Hunter, J., et al. (2010). The Open Annotation Collaboration: A Data Model to Support Sharing and Interoperability of Scholarly Annotations. 
                        Digital Humanities 2010: Conference Abstracts, pp. 175–78.
                    
                    
                        Siemens, R., et al. (2012). Toward Modeling the Social Edition: An Approach to Understanding the Electronic Scholarly Edition in the Context of New and Emerging Social Media. 
                        Literary and Linguistic Computing,
                        27(4): 445–61. 
                    
                
            
        
    



    
        
            
                Patterns of Novelty in Literary Data
                
                    
                        Higgins
                        Devin Cook
                    
                    Michigan State University, United States of America
                    higgi135@mail.lib.msu.edu
                
                
                    
                        Padilla
                        Thomas George
                    
                    Michigan State University, United States of America
                    tpadilla@mail.lib.msu.edu
                
                
                    
                        Hintze
                        Arend
                    
                    Michigan State University, United States of America
                    ahintze@me.com
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Poster
                
                
                    Information Theory
                    Novelty
                    Literary Studies
                
                
                    literary studies
                    text analysis
                    interdisciplinary collaboration
                    visualisation
                    data mining / text mining
                    English
                
            
        
    
    
        
            In addition to forming a piece of the lasting and living embodiment of the cultural heritage of humanity, literature also constitutes a form of data. The features of this data are precisely what define the ‘literary’ as such. In order to ‘understand the structural continuity of the step from information to literature and back again . . . [and] to grasp the 
                nonuniqueness of literature in an absolute structural sense’, that is, to specify a difference of 
                degree rather than 
                kind between literature and other forms of data, it is necessary to isolate and define the features of the literary band of the data spectrum with nuance at a granular level.
                1
            
            To isolate and define features of literary data, the authors have employed several information-theoretical techniques to analyze literary text and find distinguishing patterns. An algorithm developed to study the information novelty in DNA sequences has been applied to strings of arbitrary text. Previously used to quantify information generated by Twitter users on a daily basis, the algorithm has been adapted here to measure information novelty patterns across fictional texts.
            
                
            
            Figure 1. Novelty pattern expressed in 375 texts.
            
                
            
            Figure 2. Novelty pattern in 
                Moby-Dick.
            
            The graphs above (Figures 1 and 2) measure the proportion of novelty (
                y-axis) over intervals of 10,000 characters (
                x-axis) within each text, moving from beginning to end. Novelty is determined by the percentage of 
                n-length character segments that have not previously appeared in a given text. This measurement stands distinct from a measure of lexical diversity wherein a count is given for unique words that occur in a text. The novelty measure accounts for the totality of combinations of characters in a given text rather than counting unique words. In the case of the graphs above, where 
                n=5, novelty declines over the duration of texts according to a pattern of exponential decay. Fitting curves to novelty patterns allows us to make quantitative and comparative claims about patterns of information. The r-squared value in Figure 2 indicates that nearly 80% of the novelty data is explained by the exponential function used to describe the curve (in red).
            
            
                
            
            Figure 3. Novelty in 
                A Portrait of the Artist as a Young Man.
            
            Yet other texts resist curve-fitting, displaying information patterns that are highly variable and erratic. Figure 3 represents the novelty pattern for Joyce’s 
                A Portrait of the Artist as a Young Man, in which the exponential function can only account for approximately 38% of the recorded variation—wild swings in the data that are unexpected, perhaps, in one of Joyce’s less experimental works. (The r-squared value for 
                Ulysses was 52%.)
            
            The novelty measure is not only useful when looking at patterns over individual works but as a way of assessing linguistic ingenuity, or fluctuating historical trends in literary authorship, by studying the works over time of a single author, or of many authors across historical epochs. Figure 4 depicts novelty across three novels by Virginia Woolf (in chronological order), in which spikes of novelty are visible at the start of each new work (at approximately the 75 and 175 points along the 
                x-axis).
            
            
                
            
            Figure 4. Novelty across three novels by Virginia Woolf.
            The significance or not of ‘novelty’ in regard to literary studies is a question for debate that our poster will address. Recent work on patterns of information has shown that the concept of novelty (describing a formation that is new only from a particular perspective) is strongly linked to the concept of innovation (describing one that is new to all perspectives) (Tria et al., 2014). Tying novelty to innovation allows us to go further in building arguments about the role that novelty measurement could play in building an image of the particular form of data known as literature.
            Our poster will present visualizations of key findings as we continue to investigate literary data, via an algorithm designed to detect patterns of novelty. The poster would also work well as a live demonstration, during which texts could be fed to the algorithm ‘live’ as the audience circulates and poses questions.
            Note
            1. Terence Turner, quoted in Hayot (2014).
        
        
            
                
                    Bibliography
                    
                        Hayot, E. (2014). What Is Data in Literary Studies?
                    
                    http://erichayot.org/ephemera/mla-what-is-data-in-literary-studies/.
                    
                        Tria, F., et al. (2014). The Dynamics of Correlated Novelties. 
                        Nature, http://www.nature.com/srep/2014/140731/srep05890/full/srep05890.html.
                    
                
            
        
    



    
        
            
                Research Through Design and Digital Humanities in Practice: What, How and Who in an Archive Research Project.
                
                    
                        Schofield
                        Tom William
                    
                    Culture Lab, Newcastle University, United Kingdom
                    tom.schofield@ncl.ac.uk
                
                
                    
                        Kirk
                        David
                    
                    Culture Lab, Newcastle University, United Kingdom
                    david.kirk@ncl.ac.uk
                
                
                    
                        Whitelaw
                        Mitchell
                    
                    Faculty of Arts &amp; Design, University of Canberra
                    mitchell.whitelaw@canberra.edu.au
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    Design
                    Methodology
                    Practice-Based
                    Artefacts
                
                
                    archives
                    repositories
                    sustainability and preservation
                    digital humanities - nature and significance
                    interface and user experience design
                    user studies / user needs
                    GLAM: galleries
                    libraries
                    archives
                    museums
                    visualisation
                    English
                
            
        
    
    
        
            This paper highlights shared methods, questions, and challenges between research through design (RTD) and digital humanities (DH) through the discussion of an archival research project. In DH, debates continue (Gold, 2012) regarding the impact of digital technologies on epistemology, methodology, and our professional identities as researchers, scholars, academics, and teachers. Our reading of this debate is that there is a tripartite relationship at work between the kind of work we should call DH (and aspire to produce); the nature of knowledge, research, and scholarship (particularly with reference to the role of artefacts produced); and issues of disciplinary orientation or professional identity. We could phrase these as the what, how, and who of DH and, of course, RTD. The discussion of our project is in no sense intended to provide an exclusive answer to those questions, but to give one snapshot of what DH and RTD look like when they come together. 
            RTD emphasises artefact-led, practice-based research with an emphasis on developing design methods, conceptual frameworks and theories, as well as products (Gaver, 2012). Its speculative orientation and emphasis on making with digital materials aligns it well with some currents of thought in DH (see, e.g., Drucker, 2009). Indeed, Joanna Drucker has pointed to a shared ground with design, generally noting that ‘all forms of design share a propositional orientation that is well-suited to the challenges that come with designing new structures, for design asks, “What if?”’ (Burdick et al., 2012). At the same time she warns that ‘the cultural authority of digital technology is still claimed by the fields that design the platforms and protocols on which we work. These are largely fields in which quantitative, engineering, and computational sensibilities prevail’. We wish to qualify Drucker’s claim by noting a series of more detailed concordances with RTD of a particular propositional, speculative, and explorative orientation. We note that despite Drucker’s valid concerns for the positivist outlook of some design research contexts, within the field of human-computer interaction (despite its background and reputation for some as computing engineering’s heartland), and interaction design, a vibrant dialogue is being sustained concerning the role of designed artefacts in knowledge production (Bowers, 2012), their responsiveness to the values of users (see, e.g., Vines et al., 2013), and the state of critique and criticality (Bardzell and Bardzel, 2013), all of which are directly relevant to DH. In the following paragraphs we will describe aspects of an ongoing project, positioned as DH with roots in RTD methodology. Through it we will position our work in relation to the tripartite relationship we identified between what digital humanities research looks like, what role its artefacts play, and what kind of people do it. 
            Our project is, typically for digital humanities research, based around an archive. Bloodaxe Books is a small but internationally significant publisher of contemporary poetry, whose archive, consisting mostly of edited manuscripts, was purchased by Newcastle University, UK, in 2013. Our role within a research project as artists and Interaction Designers was to create exploratory and provocative interactions with the archive both online and in physical space. This is the ‘what’ of our project. 
            The interfaces we designed for the archive respond to its formal and textual specificities, and this flexible and responsive mode of engagement is typical of RTD processes. We think that this kind of approach is well aligned with a view of the role of DH as one that expands the ground of research processes as well as simply augmenting their methods with new tools and techniques. In our project we began with loose research questions (as described above) that were refined alongside the project. Our treatment of the archival material was to be informed by the experience of 30 project participants, each of whom was conducting personal, creative research in the archive. Inspired by previous research through design work, we conceived of a ‘cultural probes’ (Gaver et al., 2004) activity to gain insight into the way the archive was being used by our project participants and to uncover some of the things they found interesting about the materials themselves, the better to inform our ‘what’. Our activity used a bookmark-like insert completed and left by participants in the archive boxes themselves to act as a conversation ‘backchannel’ for participants. A trial of this activity revealed a number of interesting features of the archive that focused our interests and informed future designs. Particularly our later work with archival Marginalia and our related interest in the temporality of the archive was significantly informed by this process. When Drucker asks, ‘Have the humanities had any impact on the digital environment?’, one answer is in exactly this kind of enquiry, which has at least a 10-year history in design research. The probing activity described above was user-focused, not in the sense of a formal ‘requirements’ analysis but as a kind of critically sensitizing activity. For instance, in the final interface three particular areas of our design responded to questions and issues raised by the participants and our literary research colleagues: ‘Shapes’
                1 allows users of our interface to sketch—with the mouse—the shape of a poem and receive matches from the archive. ‘Data’ mashes our own metadata with British Library and Wikipedia data to explore wider the context of the archive.
                2 ‘Words’
                3 uses text mining techniques and word distancing to produce network graphs of correspondences between documents and relationships to themes (e.g., flowers, death).
            
            In practice-based research processes, an inevitable question arises concerning the status of the artefact as a bearer or disseminator of knowledge, and researchers in (Ramsay and Rockwell, 2012) and out (Ingold, 2013) of DH have noted the problems this causes for the place of such work within academia. In other words, our ‘how’ is about how we treat the things we make, how we evaluate their contribution and ensure that they are productive. Our perspective, as writers of both code and research papers, is that there is knowledge in both objects and commentary but that the relationship between them should be negotiated honestly and delicately. In our project we adopted a fast prototyping cycle in which we made a series of early visualisations and web interfaces to the digitized material publically available and invited criticism and feedback. We also made a feature of both our ongoing design work and the cataloguing and digitization taking place in the library by producing a Twitter feed of computationally extracted archival marginalia connected live to the work of the archivist and digital assistant and a drawing robot, the Marginalia Machine, which publically re-draws these same editorial notes. 
            
                
            
            Figure 1. Screen grab from the @BloodaxeArchive Twitter bot.
            These artefacts acted as kinds of a ‘boundary object’ (Star and Griesemer, 1989), around which we and our colleagues in the library and the English Department could discuss future iterations of design work. Their production afforded what we might call the ‘unimagined interactions’ characteristic of this kind of speculative prototyping. In our project, as is common in RTD, the artefacts were a part of a methodology for integrating facets of our research, working with our colleagues and participants and of course improving future designs. Despite this pivotal function for the ‘things’ (Brown, 2001) of our research, we recognise the value of reflection, in written or other forms, for future work. John Bowers proposes the ‘annotated portfolio’ (Bowers, 2012) as one approach to communicating the value of our work to ourselves and others. We note that however much artefacts articulate a position, their situatedness in a world of stuff means that they will always be poly-vocal. This is both their strength (when we wish to explore) and their weakness (when we want to be specific). 
            
                
            
            Figure 2. Detail of the Marginalia Machine.
            Finally we observe that in the picture of the digital humanities researcher as one who can ‘research, write, shoot, edit, code, model, design, net-work, and dialogue with users’ (Burdick et al., 2012), RTD shares a problem educating, finding, and hiring such people. We further note that the role of a designer, even one with such competencies, in a DH project remains problematic for a number of reasons. If we wish to pursue successful interdisciplinary research, then we must decide to what degree our research questions will be common with our collaborators or distinct. One model for the latter sees a humanities researcher provide a problem for a computer scientist to work with—for instance, the development of a corpus analysis tool. The computer scientist produces technical innovation while the humanist integrates the tool into the research. In our project, however, the designer, coming from a humanities background himself, brought his own questions to bear on the material, which were informed by art history, philosophy, and the cultural history of design. A challenge for such designers is how to meaningfully collaborate with other humanities researchers without, first, denigrating their own research questions and competencies to a second-tier status, and second, ignoring the nuance and depth of good humanities research. Strategies for the successful avoidance of these two pitfalls, we feel, should be the focus of future discussion of this relationship. 
            Notes
            1. http://bloodaxe.ncl.ac.uk/explore/#/shapes.
            2. http://bloodaxe.ncl.ac.uk/explore/#/data.
            3. http://bloodaxe.ncl.ac.uk/explore/#/words.
        
        
            
                
                    Bibliography
                    
                        Bardzell, J. and Bardzel, S. (2013). What Is Critical about Critical Design? In 
                        Proceedings of CHI 2013, Paris, 27 April–2 May 2013, ACM Press, pp. 3297–306.
                    
                    
                        Bowers, J. (2012). The Logic of Annotated Portfolios: Communicating the Value of ‘Research Through Design’. In 
                        Designing Interactive Systems. ACM Press, pp .68–77.
                    
                    
                        Brown, B. (2001). Thing Theory. 
                        Critical Inquiry,
                        28(1): 1–22. 
                    
                    
                        Burdick, A., Drucker, J., Lunenfeld, P., Presner, T. and Schnapp, J. (2012). 
                        Digital_Humanities. MIT Press, Cambridge, MA.
                    
                    
                        Drucker, J. (2009). 
                        SpecLab. University of Chicago Press, Chicago, doi:10.7208/chicago/9780226165097.001.0001.
                    
                    
                        Gaver, W. (2012). What Should We Expect from Research through Design? In 
                        Proceedings of CHI 2012, Austin, TX, 5–10 May 2012. ACM Press, pp. 937–46.
                    
                    
                        Gaver, W., Boucher, A., Pennington, S. and Walker, B. (2004). Cultural Probes and the Value of Uncertainty. 
                        Interactions, 
                        11(5): 53–56. 
                    
                    
                        Gold, K. (2012). 
                        Debates in the Digital Humanities. University of Minnesota Press, http://dhdebates.gc.cuny.edu/debates.
                    
                    
                        Ingold, T. (2013). 
                        Making: Anthropology, Archaeology, Art and Architecture. Routledge: London.
                    
                    
                        Ramsay, S. and Rockwell, G. (2012). 
                        Developing Things: Notes toward an Epistemology of Building in the Digital Humanities. 
                        Debates in the Digital Humanities. http://dhdebates.gc.cuny.edu/debates/text/11.
                    
                    
                        Star, S. L. and Griesemer, J. R. (1989). Institutional Ecology, ‘Translations’ and Boundary Objects: Amateurs and Professionals in Berkeley’s Museum of Vertebrate Zoology, 1907–39. 
                        Social Studies of Science,
                        19(3): 387–420, doi:10.1177/030631289019003001.
                    
                    
                        Vines, J
                        ., Clarke, R., Wright, P., McCarthy, J. and Olivier, P. (2013). Configuring Participation: On How We Involve Users in Design. 
                        Proceedings of the 2013 ACM SIGCHI Conference on Human Factors in Computing Systems, Paris, 27 April–2 May 2013, pp. 429–38.
                    
                
            
        
    



    
        
            
                Social Media Data: Twitter Scraping on NeCTAR
                
                    
                        Hutchinson
                        Jonathon
                    
                    The University of Sydney
                    jonathon.hutchinson@sydney.edu.au
                
                
                    
                        Hammond
                        Jeremy
                    
                    Intersect, Australia
                    jeremy@intersect.org.au
                
                
                    
                        Martin
                        Fiona
                    
                    The University of Sydney
                    fiona.martin@sydney.edu.au
                
                
                    
                        Yazbek
                        Daniel
                    
                    Intersect, Australia
                    daniel@intersect.org.au
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    big data
                    Social Media Network Analysis
                    cloud compute
                    twitter
                    online communities
                
                
                    corpora and corpus activities
                    internet / world wide web
                    digital humanities - facilities
                    interdisciplinary collaboration
                    visualisation
                    social media
                    networks
                    relationships
                    graphs
                    media studies
                    data mining / text mining
                    English
                
            
        
    
    
        
            The rapid growth in social media communications research in the last five years has seen the assembly of complexly connected datasets of a scale and scope previously rare in humanities scholarship, highlighting the sociocultural intricacies of follower networks (see Weller et al., 2013). Big data inquiry of social media has also been driven by the emergence of publicly accessible, integrated aggregation, indexing, query, and visualisation approaches (Hansen et al., 2011; Burnap et al., 2013). Indeed, the big data moment has challenged humanities researchers to develop innovative methodologies to extrapolate new social knowledge, and brought humanists and computer scientists together in the pursuit of robust eResearch infrastructure and workflows to support these objectives.
            This paper explores a novel social media network analysis (SMNA) research methodology, which resulted in the development of a Twitter visualisation tool for the NeCTAR research cloud. It explores the workflow issues of using large-scale eResearch infrastructure for digital humanities research, and discusses the results of the research program on social media network mapping. In doing so, the authors demonstrate the complexities of interdisciplinary humanities and computer science research collaboration, while revealing new insights made possible through eResearch partnerships.
            SMNA as a methodology requires the development of an integrated workflow using National eResearch Collaboration, Tools and Resources (NeCTAR) as the computing service to host both a Twitter scraper and a network visualisation tool. We used the native Twitter application programming interface (API) to maximise flexibility, minimise costs, and reduce reliance on commercial third-party data providers. The workflow included scripting an automated ‘clean-up’ phase so that hundreds of thousands of raw tweets could be gathered and indexed in a useful format. Analysing the characteristics of social networks across large datasets is computationally taxing, and to solve this problem we set up an interactive visualisation program, Gephi, in the NeCTAR cloud environment. 
            During the discovery and implementation phases of the project we made the following key observations. In Australia, there has been significant investment in eResearch infrastructure, including large-scale storage, data discovery, and high-performance and cloud computer systems. However, these services often have imposing barriers to entry for humanities researchers, who either lack the technological skills, desire, or established collaborative networks to apply these methodologies to their field of research (Meyer and Dutton, 2009). Moreover, eResearch assemblages are generally constructed by computer or physical scientists, who may be unfamiliar with the philosophies and theoretical perspectives of social scientists, and so it is not always clear that these technologies can immediately address the humanities’ ‘big data’ problems (Meade et al., 2013). However, in constructing the NeCTAR-based Twitter research infrastructure and pursuing an extended collaborative approach, the yield from the original research corpus provided more unique and novel results than either discipline could deliver alone. Traditionally, computer science researchers have sought interdisciplinary collaborations in the natural sciences and engineering to find appropriate research problems that balanced scale, complexity, and solvability. The materialisation of big data as a research concern within the humanities now gives software engineers new real-world applications of their techniques, a process that is key to the computer science discipline’s evolution (Hopcroft et al., 2011).
            The research presented in this paper demonstrates that early methodological discussions between humanists and computational specialists strengthened the research design, producing collaboratively designed research questions, while avoiding the high knowledge barriers to lay eResearch entry (Goggin et al., 2014). For example, in this research context, which drew on the expertise of University of Sydney media researchers and NSW Intersect’s computer scientists, a series of new research questions emerged while interrogating the initial social mapping results. The development of a novel computational research workflow emerged from the humanities scholars’ interest in understanding the languages, norms, or rules of communication activities within the Twitter social media platform. Following that research focus, the computer scientists explored a deeper understanding of the relationships between individual users, and the positive and negative communicative sentiment expressed in users’ interactions. The collaboration required significant mutual investment in exploring each research contributor’s agencies and abilities, and ongoing calibration of the research design.
            The resulting workflow and research tool have enabled a rigorous interrogation of the communication activities of complex follower networks within the Twitter platform, and the evolution of a methodology that addresses ethical concerns about big data research raised by boyd and Crawford (2012) along with Ess (2014). It will also underpin the development of an automated research tool that can be rolled out across a number of humanities research projects, and within virtual lab environments.
        
        
            
                
                    Bibliography
                    
                        boyd, d. and Crawford, K. (2012). Critical Questions for Big Data: Provocations for a Cultural, Technological, and Scholarly Phenomenon. 
                        Information, Communication, &amp; Society, 
                        15(5): 662–79.
                    
                    
                        Bradley, J. (2009). What the Developer Saw: An Outsider’s Ciew of Annotation, Interpretation and Scholarship. 
                        New Paths For Computing Humanists,
                        1(1).
                    
                    
                        Burnap, P., Rana, O. and Avis, N.
                         (2013). Making Sense of Self-Reported Socially Significant Data Using Computational Methods.
                        International Journal of Social Research Methodology, Computational Social Science: Research Strategies, Design and Methods,
                        16
                        (2).
                    
                    
                        Ess, C. (2014). At the Intersections between Internet Studies and Philosophy: ‘Who Am I Online?’ 
                        Philosophy &amp; Technology, 
                        25(3): 275–84.
                    
                    
                        Goggin, G., Dwyer, T., Martin, F. and Hutchinson, J. (2014). Finding Mobile Internet Policy Actors in Big Data: Methodological Concerns in Social Network Analysis. Paper presented at 
                        Australasian Association of the Digital Humanities, Expanding Horizons, Perth, Australia, 18–21 March 2014.
                    
                    
                        Hansen, D. L., Shneiderman, B. and Smith, M. A. (2011). 
                        Analyzing Social Media Networks with NodeXL: Insights from a Connected World. Morgan Kaufmann.
                    
                    
                        Hopcroft, J. E., Soundarajan, S. and Wang, L. (2011). The Future of Computer Science.
                         International Journal of Software Informatics, 
                        5(4): 549–65, http://www.ijsi.org/1673-7288/5/i110.htm.
                    
                    
                        Meade, B., Manos, S., Sinnott, R., Fluke, C., van der Knijff, D. and Tseng, A. (2013). Research Cloud Data Communities. 
                        THETA: The Higher Education Technology Agenda 2013, Hobart, Tasmania, 7–10 April 2013, http://eprints.utas.edu.au/16326/1/THETA_2013_Meade_16326.pdf.
                    
                    
                        Meyer, E. T. and Dutton, W. H. (2009). Top‐Down e‐Infrastructure Meets Bottom‐Up Research Innovation: The Social Shaping of e‐Research. 
                        Prometheus,
                        27(3).
                    
                    
                        Weller, K., Bruns, A., Burgess, J., Mahrt, M. and Puschmann, C. (2013). 
                        Twitter and Society. Peter Lang Publishing, New York. 
                    
                
            
        
    



    
        
            
                "Sonic Materialization of Linguistic Data" (working title)
                
                    
                        Paraskevoudi
                        Nadia
                    
                    Sonic Linguistics, Greece
                    nadiaparask@gmail.com
                
                
                    
                        Alexandropoulos
                        Timos
                    
                    Sonic Linguistics, Greece
                    timosalexandropoulos@gmail.com
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Poster
                
                
                    sonification
                    social media
                    twitter
                    prosodic features
                    stress
                
                
                    corpora and corpus activities
                    audio
                    video
                    multimedia
                    prosodic studies
                    software design and development
                    text analysis
                    interdisciplinary collaboration
                    linguistics
                    programming
                    creative and performing arts
                    including writing
                    social media
                    data mining / text mining
                    English
                
            
        
    
    
        
            
                The Problem of Sonification
            
            Kramer Gregory (1994), in his book 
                Auditory Display: Sonification, Audification, and Auditory Interfaces, defines sonification as ‘use of non-speech audio to convey information or perceptualize data’. 
            
            In our digital age we can store, edit, and examine almost all qualities and quantities as data. Sound itself can be considered as a pure stream of information able to be modulated, transformed, and analyzed in a lot of different ways. 
            The success of sonification occurs when the sound reveals one or more qualities of data or data reveals one or more qualities of sound. Thus, this kind of materialization of data is an interdisciplinary act that involves the proper analysis of data as well as the structure of sound. 
            While technology provides us with a wide variety of tools, the core of the problem still exists. As this kind of interdisciplinary knowledge is hard to be combined, there aren’t enough available tools that help artists to escape from an arbitrary mapping of data to sound qualities. This leads to arbitrary results both for the artist and the listener as the sonification process doesn’t take advantage of either the auditory perception properties or sound’s advantages in temporal, amplitude, and frequency resolution. As a result, in most cases, sonification fails its purpose, which ‘is to encode and convey information about an entire data set or relevant aspects of the data set’ (Hermann et al., 2011). 
            
                Sound and Linguistics
            
            Sonic Materialization of Linguistic Data is a series of works and a research project aiming to provide sound artists with the tools for the proper linguistic analysis of the mined data. 
            In our age of constant connectivity, social media—and especially the text-based Twitter platform—can be considered as a monitor corpus that evolves perpetually and is in a process of constant change. In order to create new structures and transform this chaotic stream of data into new material—in our case, sound—it needs to be organized according to its different kind of properties, namely here, its linguistic aspects
                . With our Sonic Materialization of Linguistic Data work, we provide different software modules that can perform real-time linguistic analysis of data and output the result for sonification purposes. 
            
            Our software consists of different kind of modules from which the user can choose only one or a combination of more. Here we present the 
                Stress Module. The program enables the user to aggregate data from different hashtag [#] feeds on Twitter in real time. The incoming data is being processed according to their linguistic features and in particular stress. The algorithm performs a series of tasks and extracts the stressed syllables of the aforementioned data. The output is a phonetic transcription code that represents each phoneme of the input Twitter feed. The encoded outputted list of data also includes suggestions for the sonic mapping that occurs from data’s linguistic features and the sound’s nature. For instance, the strong syllables are a numerical output that represents a longer sound event (time envelope), whereas the weaker syllables are a numerical representation of a briefer sound event. Similar kinds of optional mapping can also affect other sound features, such as pitch, timbre, ADSR envelopes, modulation, etc. 
            
            Stress, which can be considered as a prosodic feature, manifests itself in the speech stream in several ways. Stress patterns seem to be highly language dependent, considering that there is a dichotomy between stress-timed and syllable-timed languages. In stress-timed languages, primary stress occurs at regular intervals, regardless of the number of unstressed syllables in between, whereas in syllable-timed languages syllables tend to be equal in duration and therefore are inclined to follow each other at regular intervals of time. According to Halliday, ‘Salient syllables occur in stress-timed languages at regular intervals’ (1985, 272). Strong syllables bear primary or secondary stress and contain full vowels, whereas weak syllables are unstressed and contain short, central vowels. 
            Particularly in English, which is a stress language, speech rhythm has a characteristic pattern that is expressed in the opposition of strong versus weak syllables. Stressed syllables in English are louder, but they also tend to be longer and have a higher pitch. Despite the fact that stress can be also influenced by pragmatic factors such as emphasis, our project aims to capture the natural stress pattern of English in order to extract meaning from sound patterns, too, as they will be delineated by the phonetic structure of natural language. 
            
                Presentation
            
            For the presentation of the project we are proposing a poster with the description of how exactly the software works and what its aim is. We also would like to include a pair of headphones and a small screen (or projector) in order to have the data analysis and the sonification process in real time for the audience to experience. 
        
        
            
                
                    Bibliography
                    
                        Halliday, M. A. K. (1985). 
                        An Introduction to Functional Grammar. Arnold, London. 
                    
                    
                        Hermann, T., Hunt, A. and Neuhoff, J. G.
                         (eds). (2011). 
                        The Sonification Handbook
                        . Logos Verlag Berlin, Berlin.
                    
                    
                        Kramer, G. (1994). Auditory Display: Sonification, Audification, and Auditory Interfaces. 
                        Santa Fe Institute Studies in the Sciences of Complexity, Proceedings Vol. XVIII. Addison Wesley, Reading, MA. 
                    
                
            
        
    



    
        
            
                What Do You Do With A Million Readers?
                
                    
                        Bandari
                        Roja
                    
                    Twitter Inc., United States of America
                    roja.bandari@gmail.com
                
                
                    
                        Tangherlini
                        Timothy Roland
                    
                    UCLA, United States of America
                    tango@humnet.ucla.edu
                
                
                    
                        Roychowdhury
                        Vwani
                    
                    UCLA, United States of America
                    vwani_rc@yahoo.com
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Short Paper
                
                
                    reader response
                    plot
                    machine learning
                    crowd sourcing
                
                
                    literary studies
                    natural language processing
                    text analysis
                    content analysis
                    crowdsourcing
                    data mining / text mining
                    English
                
            
        
    
    
        
            In 2006, Gregory Crane posed the provocative question, ‘What do you do with a million books?’, a question that captured the increasing anxiety in the humanities that accompanied the rapid digitization of many library collections, and the distribution of more and more books in digital form (Crane, 2006). Far more books than could ever possibly be read by a single person were now machine actionable, requiring a response from the intellectual community. In the ensuing years, that response has emerged from the digital humanities community, and DH projects have focused on a broad range of approaches to the study of literature at scale, operationalizing the ‘distant reading’ that Franco Moretti had already proposed in his well-known article, ‘Conjectures on World Literature’ (Moretti, 2000; 2013). Yet what has been overlooked in many of these studies is that, alongside the explosion in the digitization of world literature, there has been an equally large explosion of readers commenting on books in online forums and other easily accessed electronic venues. The analysis of these responses allows for the consideration of reader response at scale. The goal of our work is to provide a preliminary answer to the question, ‘What can one do with a million readers?’ At the very least, we want to know what types of information one can extract automatically from thousands of reader posts about a particular work of literary fiction, and what this can tell us about how people (or classes of people) read.
            The target data for our study are reader reviews of sixteen works of fiction, five of which we focus on in this presentation (
                The Hobbit; 
                Gone with the Wind; 
                The Life of Pi; 
                Frankenstein; 
                Of Mice and Men). The works were chosen from the list of the most frequently rated books on the Goodreads site (number of ratings &gt; 500,000). From these books, we downloaded the maximum allowed 3,000 reviews. The sixteen works we ultimately chose were selected on the basis of the broad disparity in their narrative structures, number of characters, and character relationships. The initial goal was to develop a review-based summary of the novel in the form of a character graph (Figure 1), with dramatis personae and pair-wise connections between them based on actions, events, or other relationships. These graphical representations can be seen as an abstraction of what the reviewers on Goodreads collectively imagine who the main characters and what the main events to be, and the relationships between these characters and events.
            
            The reviews were harvested using a crawler specifically designed for this project. Readily available information on the reviewer was retained as metadata for use in addressing second-order questions related to classes of reviewers (e.g., gender, age, frequency of reviewing). To evaluate reader reviews at scale, we devised two metrics related to plot: completeness and accuracy. We used SparkNotes as a basis for ‘gold standard’ summaries, a choice motivated by SparkNotes’ high degree of completeness and accuracy for target features (entities and relationships), and the brevity of the summaries. 
            We devised several progressively more difficult challenges to test the type of information we could derive from the approximately 48,000 reader reviews we crawled from Goodreads. First, without any training data, could we successfully discover the main dramatis personae in each novel? Second, could we automatically discover the action-based relationships between characters as represented by the reviewers? Third, could we discover the ‘events’ in which dramatis personae played a role? And fourth, could we develop a visualization that captured these relationships in a clear and engaging fashion that also represented the varying degree or strength of relationships between these entities? 
            
                To achieve reproducible results, we devised a simple workflow that can be applied to other similar sites. This workflow consists of a preprocessing step, a statistical entity ranking step that surfaces the main dramatis personae in any given work, a pair-wise relationship discovery step, and a visualization step. As with most blogs and crowdsourced data sources, the Goodreads reviews are ‘noisy’. Consequently, preprocessing focused on reducing noise. We also ran language detection on the reviews to eliminate non-English reviews, and a stemmer to aggregate inflected words.
            
            In our efforts to discover the dramatis personae for any target work, we experimented with three main approaches. LDA proved to be successful at separating the works; we intend to explore this approach more thoroughly in future work for classifying reviews that are not preclassified as they are on Goodreads. Traditional NER (named entity recognition) approaches proved to be less accurate, given the broad variance in orthography that typify these reviews. Ultimately, the most successful approach was based on a statistical ranking of tokens between the review corpus and the individual subcorpus of target work reviews.
            The next challenge was to discover the relationships between dramatis personae as represented in the reviews. Here, we focused on the verbs, extracted using POS tagging, between two entities, after discovering all sentences with pairs of entities. In future work, we hope to refine these pair-wise relationships by collapsing verbs into a series of higher-level representations of the entity-relationship space. Even without this processing, the current approach discovers important relationships. For example, by looking at the reviews for 
                The Hobbit, the relationship between ‘Tolkien’ and ‘Hobbit’ is dominated by the verb ‘write’ with a directional relationship in which ‘Tolkien’ inhabits the subject position and ‘Hobbit’ the object position. Similarly, ‘Bilbo’ has a relationship with ‘ring’ characterized by ‘find’, and a relationship with ‘adventure’ of ‘go on’, while the dragon ‘Smaug’ has a relationship with ‘treasure’ of ‘guard’. Interestingly, the reader reviews also generated a relationship between ‘Bilbo’ and ‘Smaug’ characterized by ‘kill’, an inaccurate depiction of the actual events (Bard the Bowman slays Smaug). Our automatic method is able to build a surprisingly large number of relationships between discovered entities without any training data, nor any preexisting lists of entities or relationships.
            
            For evaluation, using only the SparkNotes plot summaries, experts created a list of nouns including (1) names, (2) locations, (3) objects, and (4) concepts that are explicitly mentioned in the summary; these were considered the true dramatis personae. Completeness was quantified by computing the proportion of this list also produced by the algorithm. We measured both Precision (proportion of main characters produced by the algorithm) and the False Detection Rate (proportion of produced characters not in the relevant set). Similarly, using only words explicitly found in the SparkNotes plot summary, experts derived relationships based on verbs that connected pairs of characters mentioned above. We again measured the Precision and the False Detection Rate (FDR) for these relationships. As an example, applying this to 
                The Hobbit reviews produced a measure of Completeness (or precision) for dramatis personae of 0.6 and Accuracy (False Detection Rate) of 0.13. 
            
            The final challenge was to present these relationships in a visually engaging manner (see Figures 1 and 2). We have developed directional, multicolored graphs that represent the strength (or confidence) of a relationship by an edge of varying width. These graphs are easily compared with the ‘gold standard’ ground truth graphs, and they provide us with a visual representation of our Completeness and Accuracy measures. What is immediately clear is that reader reviews have significantly lower Completeness than a resource dedicated to providing comprehensive summaries, while the Accuracy of described relationships is good (Bilbo’s dragon-slaying feats notwithstanding). The comparison raises intriguing issues about memory—for example, why is it that certain events disappear from the user-driven graphs, while others become accentuated?
            Other graphs are generated for classes of reviewers: e.g., female reviewers vs male reviewers of 
                The Hobbit, which allow for a different type of comparison. Here the question is on which aspects of a story different types of reviewers tend to comment. Additional refinements could include metrics that reveal the number of reviews that mention particular entities or particular relationships. Currently, a missing component is a dynamic representation of reviewers’ concepts of plot (dynamics), which we are reserving for future work. 
            
            The approach we describe here is widely applicable to other crowdsourced response sites. Of particular interest are movie review sites such as Rotten Tomatoes that, much like Goodreads, allow viewers to present their own reviews of popular films. An intriguing aspect of many of these review sites is the propensity of reviewers to provide ‘plot summaries’ as opposed to critical engagements of more sophisticated thematic analysis. While this may drive many literary scholars toward the brink of insanity, it does allow us to consider questions regarding the popular engagement with literature and other forms of artistic production. Given the responses that people do post, can we use the scale of these sites to derive insight into how people (or groups of people) not only read but also remember? It is our contention that what people remember, and what people forget (or choose to leave out), can be very telling indicators of popular engagement with art. 
            
                
            
            
                Figure 1. Character/relationship graph of 
                The Hobbit.
            
            
                
            
            
                Figure 2. Character/relationship graph of 
                Frankenstein
                .
            
        
        
            
                
                    Bibliography
                    
                        Crane, G. (2006). What Do You Do with a Million Books? 
                        D-Lib Magazine,
                        12(3): 1.
                    
                    
                        Moretti, F. (2000). Conjectures on World Literature. 
                        New Left Review, 
                        1 
                        (January–February): 54–68.
                    
                    
                        Moretti, F. (2013).
                         Operationalizing: or, the Function of Measurement in Modern Literary Theory. 
                        New Left Review,
                        84
                        (November–December): 103–19.
                    
                
            
        
    



    
        
            
                You can leave your hat on-line: Multiple Context-Dependent Identities on Social Networking Sites
                
                    
                        Angelopoulos
                        Spyros
                    
                    Horizon Digital Economy Research Institute, University of Nottingham, United Kingdom
                    Spyros.Angelopoulos@nottingham.ac.uk
                
                
                    
                        Brown
                        Michael
                    
                    Horizon Digital Economy Research Institute, University of Nottingham, United Kingdom
                    Michael.Brown@nottingham.ac.uk
                
                
                    
                        Price
                        Dominic
                    
                    Horizon Digital Economy Research Institute, University of Nottingham, United Kingdom
                    Dominic.Price@nottingham.ac.uk
                
                
                    
                        Mortier
                        Richard
                    
                    Horizon Digital Economy Research Institute, University of Nottingham, United Kingdom
                    Richard.Mortier@nottingham.ac.uk
                
                
                    
                        McAuley
                        Derek
                    
                    Horizon Digital Economy Research Institute, University of Nottingham, United Kingdom
                    Derek.Mcauley@nottingham.ac.uk
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    Computer-Mediated Communication
                    Multiple Identities
                    Empirical Methods
                    Quantitative Prototyping User-Centered Design / Human-Centered Design
                
                
                    software design and development
                    user studies / user needs
                    internet / world wide web
                    social media
                    English
                
            
        
    
    
        
            In this paper we seek to elucidate understandings of the self-management of multiple context-dependent identities of Social Networking Sites (SNS) users. Online social networking is now in its second decade and has become a central activity to a large proportion of the global population. This shift is not surprising, as humans are social animals with a need to connect and communicate with each other. SNS augment our existing offline networks, allowing us to keep in touch with people over great distances, share our experiences and associated content, organize our social lives, and discover new contacts beyond physical reach. As previous researchers have noted, many users maintain multiple online identities through which they actively manage their social interactions on the various SNS (Golbeck and Rothstein, 2008). 
            Most SNS, however, suffer from a common problem that prevents them from capturing the true richness of our offline social networks. As social beings, we tend to participate in different, overlapping social groups, and we adjust our identities to match the contexts, as well as our use to match the constraints imposed by the various SNS. The reasons why people choose to explicitly manage the overlap among social networks, even keeping some networks completely distinct from others, are commonplace and usually not clandestine—for example, teenagers wishing to discuss sensitive health matters in online forums (van der Velden and El Emam, 2013), employees complaining about treatment at work (O’Brien, 2014), or those engaged in political commentary in uncomfortable or dangerous situations (Attia et al., 2011). 
            The self-management of multiple context-dependent identities represents a topic that deserves greater attention within digital humanities and needs to be further explored and elucidated, since it incorporates the entanglement of online and offline interactivity within and around computer mediated environments (Angelopoulos and Merali, 2013) which can have significant implications for the overall sociability of SNS users (Angelopoulos and Merali, 2015). The self-management of multiple context-dependent identities implies a process in which the users control how the other users with whom they are socially connected perceive them (Baumeister and Leary, 1995; Leary et al., 1995). The extant computer-mediated communication literature highlights the needs of a diverse range of groups to incorporate and maintain multiple identities on a plethora of media, such as hobbyists like cigar smokers (Angelopoulos and Merali, 2013; 2015) and bodybuilders (Ploderer et al., 2008), as well as professionals who need to separate their professional from personal lives (Peluchette et al., 2013). Although the need for users’ multiple context-dependent identities to be further explored and elucidated is highlighted in the literature (Karl and Peluchette, 2011; Peluchette et al., 2013; Talamo and Ligorio, 2001), to date there are very few studies exploring the issue directly (Talamo and Ligorio, 2001). 
            We adopt a quantitative approach and explore the concept through a survey. We designed the questionnaire based on extensive literature analysis on online/offline identities as well as the environment analysis and the survey planning (Duffy et al., 2000; Karl and Peluchette, 2011; Kodjamanis and Angelopoulos, 2013; Koh and Kim, 2003; Kuhn and McPartland, 1954; Peluchette et al., 2013), and our findings are drawn from a sample of 
                n=272 participants and guided by a previous pilot study of 60 participants. 
            
            Our findings demonstrate that, compared to other SNS, use of multiple accounts is more common within Facebook and Twitter, and considerably less common within LinkedIn. This finding is surprising given the fact that both Facebook and Twitter provide the users with features to manage multiple groups of contacts within a single account, unlike LinkedIn. 
            Moreover, our findings reveal that behavior between the use of different SNS suggest that different SNS are used to manage different aspects of people’s lives despite the fact that most SNS provide tools for this, yet such tools to manage groups of contacts within SNS are rarely used, as reported by the participants of our study. Such a finding reveals that the tools that are already available and provided by the SNS for the management of multiple context-dependent identities are insufficient and neglected by the users, and thus there is a need for better tools to be implemented by either the SNS or by third parties that take into account the real needs of the users. Exploring the differences between users of multiple accounts and users of single accounts has revealed that those who use more than one account on one or more SNS generally share more personal information with those accounts and tend to engage in more audience management behavior both online and offline. These findings suggest that users of multiple accounts use both control of information and targeted sharing of personal information to manage their identities, highlighting the need for networking activities that can support both behaviors. The combination of these two tendencies also highlights the importance of security and privacy between SNS for these individuals as they reveal a lot of personal information online but want strict control of what information is revealed to whom (boyd, 2010; Edwards and McAuley, 2013; Livingstone, 2008). Practically, the findings of our survey demonstrate the need of users for better tools for the self-management of multiple context-dependent identities, as the current tools provided especially from Facebook and Twitter are insufficient. 
            The self-management of multiple context-dependent identities is still an issue to be pursued by the organizations behind SNS platforms, and there is a profound need for better tools to be implemented either by them or by third parties. We call, thus, for future research to focus on applied approaches trying to solve this real and practical problem. Our findings are drawn from a sample, which, although adequate for the needs of the study, remains too small to enable us to reflect on a larger scale. Whilst the following step in our future research plans is to expand our scope and explore the concept further, we suggest that future studies should explore the issue on online communities in general, as well as on online communities of hard-to-reach populations (Angelopoulos and Merali, 2013; 2015; Ploderer et al., 2008).
        
        
            
                
                    Bibliography
                    
                        Angelopoulos, S. and Merali, Y. (2013). Offline Interactions on Online Communities: The Entanglement of Social and Material. 29th 
                        European Group for Organizational Studies, Montreal, Canada, 4–6 July 2013. 
                    
                    
                        Angelopoulos, S. and Merali, Y. (2015). Bridging the Divide between Virtual and Embodied Spaces: Exploring the Effect of Offline Interactions on the Sociability of Participants of Topic-Specific Online Communities. 48th 
                        Hawaii International Conference on System Studies, Kauai, HI, USA, 5–8 January 2015. 
                    
                    
                        Attia, A. M., Aziz, N., Friedman, B. and Elhusseiny, M. F. (2011). Commentary: The Impact of Social Networking Tools on Political Change in Egypt’s ‘Revolution 2.0’. 
                        Electronic Commerce Research and Applications,
                        10(4): 369–74. 
                    
                    
                        Baumeister, R. F. and Leary, M. R. (1995). The Need to Belong: Desire for Interpersonal Attachments as a Fundamental Human Motivation. 
                        Psychological Bulletin,
                        117(3): 497. 
                    
                    
                        boyd, d. (2010). Social Network Sites as Networked Publics: Affordances, Dynamics, and Implications. In Papacharissi, Z. (ed.), 
                        Networked Self: Identity, Community, and Culture on Social Network Sites. New York: Routledge, pp. 39–58. 
                    
                    
                        Duffy, M. K., Shaw, J. D. and Stark, E. M. (2000). Performance and Satisfaction in Conﬂicted Interdependent Groups: When and How Does Self-Esteem Make a Difference? 
                        Academy of Management Journal,
                        43(4): 722–83. 
                    
                    
                        Edwards, L. and McAuley, D. (2013). What’s in a Name? Real Name Policies and Social Networks. 
                        Proceedings of 1st International Workshop on Internet Science and Web Science Synergies, Paris, 1 May 2013. 
                    
                    
                        Golbeck, J. and Rothstein, M. (2008). Linking Social Networks on the Web with FOAF: A Semantic Web Case Study. 
                        AAAI,
                        8: 1138–43. 
                    
                    
                        Karl, K. A. and Peluchette, J. V. E. (2011). Friending Professors, Parents and Bosses: A Facebook Connection Conundrum. 
                        Journal of Education for Business,
                        86(4): 214–22. 
                    
                    
                        Kodjamanis, A. and Angelopoulos, S. (2013). Consumer Perception and Attitude Towards Advertising on Social Networking Sites: The Case of Facebook. 
                        International Conference of Communication, Media Studies and Design, Famagusta, North Cyprus, 2–4 May 2013. 
                    
                    
                        Koh, J. and Kim. Y. G. (2003). Sense of Virtual Community: Determinants and the Moderating Role of the Virtual Community Origin. 
                        International Journal of Electronic Commerce,
                        8(2): 75–93. 
                    
                    
                        Kuhn, M. H. and McPartland, T. S. (1954). An Empirical Investigation of Self-Attitudes. 
                        American Sociological Review,
                        19(1): 68–76. 
                    
                    
                        Leary, M. R., Tambor, E. S., Terdal, S. K. and Downs, D. L. (1995). Self-Esteem as an Interpersonal Monitor: The Sociometer Hypothesis. 
                        Journal of Personality and Social Psychology,
                        68(3): 518. 
                    
                    
                        Livingstone, S. (2008). Taking Risky Opportunities in Youthful Content Creation: Teenagers' Use of Social Networking Sites for Intimacy, Privacy and Self-Expression. 
                        New Media &amp; Society,
                        10(3): 393–411. 
                    
                    
                        O’Brien, C. N. (2014). The Top Ten NLRB Cases on Facebook Firings and Employer Social Media Policies. 
                        Oregon Law Review,
                        92(2). 
                    
                    
                        Peluchette, J. V. E., Karl, K. and Fertig, J. (2013). A Facebook ‘Friend’ Request from the Boss: Too Close for Comfort? 
                        Business Horizons,
                        56: 291–300. 
                    
                    
                        Ploderer, B., Howard, S., Thomas, P. and Reitberger, W. (2008). ‘Hey World, Take a Look at Me!’: Appreciating the Human Body on Social Network Sites. In 
                        Persuasive Technology. Berlin: Springer, pp. 245–48. 
                    
                    
                        Talamo, A. and Ligorio, B. (2001). Strategic Identities in Cyberspace. 
                        CyberPsychology and Behavior, 
                        4(1): 109–22. 
                    
                    
                        van der Velden, M. and El Emam, K. (2013). ‘Not All My Friends Need to Know’: A Qualitative Study of Teenage Patients, Privacy, and Social Media. 
                        Journal of the American Medical Informatics Association, 
                        20(1): 16–24. 
                    
                
            
        
    


        
            Since 2014, some of the countries that were formerly belligerent of the Great War – most particularly France and UK – have organised a series of commemorations of the First World War, known as the ‘Centenaire’ (France) or the ‘Centenary’ (UK). We can assume that there is a strong link – that cannot let a historian indifferent – between those commemorations, collective memory and historical studies.
            Though studies about collective memory are numerous since the famous works of the French sociologist Maurice Halbwachs (Halbwachs, 1950), few of them are examining how collective memories are being expressed – maybe even transformed – on social networks on-line.
            In the case of the Centenary of the First World War, a set of questions can be asked: What is the on-line echo of the commemoration of the centenary of the 1st World War? What is the behaviour of Memorial/Heritage Institutions about the 1st World War on Twitter? How do they transmit information about the Centenary? Is there an influence of the English predominance on Twitter about the Centenary on how non-english-speaking twitter accounts are considering the 1st World War? Are there specific subjects that are discussed on-line? Which ‘temporalities’ are present in tweets when Twitter users speak about the Great War on-line?
            Though we are not yet able to respond to all those questions, we’ll use our database of tweets in order to answer them at least partially.
            Indeed, since the 1
                st April 2014, around 1.5 millions of tweets containing a hashtag (keyword) linked to the 1st World War were written by over 350 000 Twitter accounts in several languages (mainly English and French). Twitter is a good field to analyse relationships between history and collective memory, memorial institutions and citizens, historians and a wide non-academic audience. We started to explore this database (which is still expanding): we intend to show how a historian can collect, analyse and interpret those tweets, using Digital Humanities methodologies and software in order to answer questions about collective memory of the First World War online.
            
            
                Tools and Methodologies 
                We are using 140dev, a PHP open source script within a LAMP environment to collect tweets through the Twitter streaming API
                    
                        
                            
                                http://140dev.com/ (accessed 4 March 2016).
                            
                        
                    . The tweets are then stored in a MySQL database. Diverse information (tweets and their metadata, hashtags, user information, mentions, retweets) about those tweets can easily be extracted through SQL queries. Those queries can also be used to extract different kind of relations: between tweets, between Twitter users or even between hashtags (
                    ie if a Twitter user mentioned or retweeted another twitter user, if two users wrote the same hashtags, etc). Concerning privacy, we respect the Twitter API Terms.
                
                To analyse tweets, we are using mainly two sets of methodologies/software: social network analysis and network visualisations (with Gephi: mention, retweets or hashtags are considered a link); text analysis through the theory of the 
                    mondes lexicaux (Reinert, 1993) as it is implemented in the IRaMuTeQ software (Ratinaud and Dejean, 2009)
                    
                        
                            
                                http://www.iramuteq.org/ (accessed 4 March 2016) - Interface de R pour les Analyses Multidimensionnelles de Textes et de Questionnaires. IRaMuTeQ is a free software based on python and R. It is available in French, English, German and Spanish (interface and analyses).
                            
                        
                    . The combination of both tools and methodologies has been described by (Smyrnaios and Ratinaud, 2014). IRaMuTeQ, thanks to time-stamped metadata, can also help us working on temporalities. Indeed, clusters that are defined by this software can be projected in time: we can know, day-by-day, the most used kind of tweets.
                    
                        
                             IRaMuTeQ works in dividing the corpus in small segments of text (around 40 words). In our case each segment is a tweet and each tweet is also a text.
                        
                     It helped us, for instance, finding that French fallen soldiers are not described with the same words the 11
                    th of November in comparison to the rest of the year.
                
                The methodologies and tools that remain to be found for this research concern temporalities – even if IRaMuTeQ has helped us answer some question on time. There are several temporalities that are expressed in this corpus: the constant feed of information that is the nature of Twitter; the temporality of each twitter user; the temporality of the Centenary (which is different from one country to the other, and from the Great War temporality); and the temporality of the War itself.
            
            
                First results
                
                    Language
                    English is overwhelmingly present in this corpus. Around 10% only of the collected tweets are not in English. Among those 10%, French is largely in majority and German almost absent, even though German hashtags are collected. The fact that Twitter is an English-based social network does not explain fully this disequilibrium between English and other languages. The Memorial institutions' communication policies on Twitter are better factors to explain it. 
                    The decentralized communication policy of British memorial institutions (the BBC and all its Twitter accounts or the Imperial War Museum for instance) is obviously more efficient than the French centralized communication policy of the 
                        Mission du centenaire. French WW1-related museums do not have Twitter accounts or do have one but do not follow twitter implicit rules such as the use of a general hashtag like #ww1 or the French #pgm.
                    
                
                
                    British and French are not commemorating WW1 the same way
                    The most striking difference between the French corpus and the English one is the fact that both linguistic areas do not commemorate the Great War the same way. There are two major differences between both countries:
                    
                        French are mainly remembering the soldiers (
                            Poilus). British citizens are remembering soldiers, but also battles.
                        
                        The French are focusing on the end of the war, the Armistice, on the 11th November. The British are focusing on the way they entered the war.
                    
                
                
                    English public history and French history amateurs
                    Thanks to the Network visualisations, this corpus also helps understand how public history is present in Britain, in contrary to France where it just begins to appear. The presence of amateurs of history in the French corpus also shows that French historians are not on twitter, in contrary to amateurs who, next to the 
                        Mission du Centenaire, are structuring discussions about the First World War on Twitter.
                    
                
            
            
                Conclusion
                
                    Comparing multilingual corpora
                    To compare our two main corpora (the French one and the English one) that can be extracted from the database, we had to use the two main pieces of software the same way on both corpora and then to ‘humanly’ compare the results. We could not find any tools able to compare two corpora that are in different languages.
                
                
                    Distant reading / Close reading
                    This research project shows that, for historians, it is still important to keep a direct link with each single primary source, as some information can be learned from the interpretation of single tweets. Though methods used in this research are dealing with Franco Moretti’s notion of 
                        distant reading (Moretti, 2007), it proved strategic to be able to go back to every single tweet. The software used, if metadata are kept all along their use, allow this.
                    
                
                
                    Twitter and the rest of the web
                    Why Twitter? The fact that the Twitter API, though sometimes very unstable, is very convenient to use is one of the criteria of this choice. Is it really pertinent in terms of research? Shouldn't we have broader sources? How to extrapolate the project's results to other on-line social networks? Last but not least, the difficulty to anticipate hashtags to be collected might introduce biases in our research.
                
                
                    Future of this research project
                    The question of ‘temporalities’ and their imbrications (the temporality of Twitter / the temporality of users / the temporality of the commemorations / the temporality of the First World War itself) should be the next step of this research. But, as it will require the use of Named Entity Recognition, extending our research to places will be possible as well.
                
            
        
        
            
                
                    Bibliography
                    
                        Halbwachs, M. (1950). 
                        La mémoire collective, Paris: Albin Michel.
                    
                    
                        Moretti, F. (2007). 
                        Graphs, Maps, Trees: Abstract Models for Literary History. London: Verso.
                    
                    
                        Ratinaud, P. and Dejean, S. (2009). IRaMuTeQ: Implémentation de la methode ALCESTE d’analyse de texte dans un logiciel libre [Implementation of the ALCESTE method of text analysis in an open-source software]. 
                        Presentation. Available at: http://repere.no-ip.org/Members/pratinaud/mes-documents/articles-et-presentations/presentation_mashs2009.pdf (accessed 4 March 2016).
                    
                    
                        Reinert, M. (1993). Les ‘mondes lexicaux’ et leur ‘logique’ à travers l’analyse statistique d’un corpus de récits de cauchemars. 
                        Language et Société, 66(1): 5–39. doi:10.3406/lsoc.1993.2632
                    
                    
                        Smyrnaios, N. and Ratinaud, P. (2014). Comment articuler analyse des réseaux et des discours sur Twitter. 
                        tic and société, 7(2). doi:10.4000/ticetsociete.1578
                    
                
            
        
    


        
            
                Introduction
                Big Data is reshaping the historical profession in ways we are only now beginning to grasp. The growth of digital sources since the advent of the World Wide Web in 1990-91 presents new opportunities for social and cultural historians. Large web archives contain billions of webpages, from personal homepages to professional or academic websites, and now make it possible for us to develop large-scale reconstructions of the recent web. Yet the sheer number of these sources presents significant challenges: if the norm until the digital era was to have human information vanish, “now expectations have inverted. Everything may be recorded and preserved, at least potentially” (Gleick, 2012).
                While the Internet Archive makes archived web content available to the general public and mainstream scholarly community through its “Wayback Machine,” (at http://archive.org/web) which allows visitors to enter a Uniform Resource Locator (URL) to visit archived web versions of a particular page, this system is limited: not only do visitors need to know the URL in the first place, but they are limited to individual readings of single webpages.
                By unlocking the Wayback Machine’s underlying system of specialized files, primarily ISO-standardized WebARChive (WARC) files, we can develop new ways to systematically track, visualize, and analyze change occurring over time within web archives. Warcbase, an open-source platform for managing web archives built on Hadoop and HBase, provides a flexible data model for storing and managing raw content as well as metadata and extracted knowledge. Tight integration with Hadoop provides powerful tools for analytics and data processing. Using a case study of one collection, this paper introduces the work that we have been doing to facilitate web archive access with warcbase. We have growing documentation at http://docs.warcbase.org.
            
            
                Project Rationale and Case Study
                In 1996, the Internet Archive launched a complementary research services company, Archive-It, which offers subscription-based web archiving to collecting institutions. 
                The University of Toronto Library (UTL) began collecting a quarterly crawl in 2005 of Canadian political parties and political interest groups (the collections were separate in 2005, merging in 2006) (University of Toronto, 2015). The collection itself has a murky history: UTL had been part of a broader project that would have collected political websites. It fell through, but UTL opted to carry out their crawl on their own and the librarian was responsible for selecting the seed list herself (faculty and other librarians did not respond for calls for engagement). While formal political parties are robustly covered, the “political interest groups” collection was a bit more nebulous: sites were discovered through keyword searches, and some were excluded due to robots.txt exclusion requests. Beyond this brief sketch, we have little information about the decisions made in 2005 to create this collection. This lack of documentation is a shortcoming of this collection model, as if a historian was to use this material in a peer-reviewed paper, questions would be raised about its representativeness.
                If a user wants to use the Canadian Political Parties and Interest Groups Collection (CPP) through Archive-It today, they visit the collection page at https://archive-it.org/collections/227 and enter full-text search queries. In August 2015, our group also launched http://webarchives.ca, based on the British Library’s SHINE front end for web archives; this was a way to facilitate a different form of more casual user access, aimed at the general public (we discuss this in a separate paper).
                The Archive-It portal is limited. There are no readily-available metrics of how many pages have been collected, how they break down by domain and date, and the portal undoubtedly provides skewed results unless the search phrase is dramatically narrowed down.
                Consider the search for “Stephen Harper,” Canada’s Prime Minister between 2006 and 2015 in Figure 1.
                
                    
                    Figure 1: Archive-It Search Portal
                
                The results are decent: Harper’s Facebook page from 2009, a Twitter snapshot from 2010, and some long-form journalism articles and opposition press releases. But amidst the 1,178,351 results, there is no indication as to how the ranking took place, what facets are available, and how things may have changed over the last ten years of the crawl.
                The data is there, but the problem is access.
            
            
                Warcbase: A Platform for Web Archive Analysis
                Warcbase is a web archive platform, not a single program. Its capabilities comprise two main categories:
                
                    Analysis of web archives using the Pig or Spark programming languages, and assorted helper scripts and utilities
                    Web archive database management, with support for the HBase distributed data store, and OpenWayback integration providing a friendly web interface to view stored websites
                
                One can take advantage of the analysis tools (1) without bothering with the database management aspect of Warcbase – in fact, most digital humanities researchers will probably find the former more useful. This paper focuses on the former capabilities, showing how we can use the warcbase platform to carry out text and network analyses.
            
            
                Using Warcbase on Web Archival Collections: Text Analysis
                We have begun to document all warcbase commands on a GitHub wiki, found at 
                    https://github.com/lintool/warcbase/wiki. We begin with installation instructions, and then provide simple scripts written in Apache Spark to run the commands.
                While possible to generate a plain text version of the entire collection, a more fruitful approach has been to generate date-ordered text for particular domains. If a researcher is interested in say, the Green Party of Canada’s evolution between 2005 and 2015, they can extract the plain text for greenparty.ca by running the following script:
                
                    
                
                All they would need to change would be the path/to/input to the directory with their web archive files, the path/to/output for where they want to save the resulting plain-text files, and the greenparty.ca value to whatever domain they might be interested in researching.
                They then receive a date-ordered output of all plain text for that domain (as per the extractCrawldateDomainUrlBody command). It can then be sorted and used in other research avenues. For example, this plain text could be loaded into a text analysis suite such as http://voyant-tools.org/ or other digital humanities environments.
                We have also been experimenting with other visualizations based on the extracted plain text. Computationally intensive textual analysis can be carried out using warcbase itself. Using the Stanford NER package in parallel, we have a script that extracts entities, counts them, and then visualizes them using D3.js to help see overall changes in a web archival collection. Figure 2 below shows the output of the NER visualizer.
                
                    
                    Figure 2: Named Entity Visualization within Warcbase
                
                Finally, another text approach is topic modelling (Blei et al., 2003). LDA works by finding topics in unstructured text. To visualize topic models, we elected to use the Termite Data Server, which is a visual analysis tool for exploring the output of statistical topic models (“uwdata/termite-data-server,” n.d.). As Figure 3 demonstrates, the visualization allows you to get a top-down view at the topics found in a web archive.
                
                    
                    Figure 3: Termite Topic Model
                
                Warcbase presents versatile opportunities to extract plain text and move it into other environments for analysis. Unlike the keyword-based Archive-It portal, we now have data that can be inquired in many fruitful ways.
            
            
                Using Warcbase on Web Archival Collections: Hyperlink Analysis
                Warcbase can also extract hyperlinks. While text can be very important, these sorts of metadata can often be more important: allowing us to see changes in how groups link to each other, what articles and issues were important, and how relationships changed over time.
                Consider Figure 4, which visualizes the links stemming from and between the websites of Canada’s three main political parties.
                
                    
                    Figure 4: Three major political parties in Canada
                
                Above, we can see which pages only link to the left-leaning New Democratic Party (ndp.ca), those that link only to the centrist Liberals (liberal.ca) in the top, and those that only connect to and from the right-wing Conservative Party at right. We can use it to find further information, such as in Figure 5.
                
                    
                    Figure 5: NDP attack
                
                The above links are from the 2006 Canadian federal election. The Liberal Party was then in power and was under attack by both the opposition parties. In particular, the left-leaning NDP linked hundreds of times to their ideologically close cousins, the centrist Liberals, as part of their electoral attacks, ignoring the right-leaning Conservative Party in the process. Link metadata illuminates more than a close reading of an individual website would. It contextualizes and tells stories itself.
                While we have traditionally used Gephi to do analysis, importing material into Gephi from warcbase required many manual steps as documented at https://github.com/lintool/warcbase/wiki/Gephi:-Converting-Site-Link-Structure-into-Dynamic-Visualization. We have been prototyping a link analysis visualization in D3.js, which can run in browser (Figure 6).
                
                    
                    Figure 6: Link Visualization
                
            
            
                Conclusions
                With the increasingly widespread availability of large web archives, historians and Internet scholars are now in a position to find new ways to track, explore, and visualize changes that have taken place within the first two decades of the Web. Warcbase will allow them to do so. This project is among the first attempts to harness data in ways that will enable present and future historians to usefully access, interpret, and curate the masses of born-digital primary sources that document our recent past.
            
        
        
            
                
                    Bibliography
                    
                        Brügger, N. (2008). The Archived Website and Website Philology: A New Type of Historical Document, 
                        Nordicom Review,  29(2): 155–75.
                    
                    
                        Gleick, J. (2012). 
                        The Information: A History, a Theory, a Flood. London: Vintage.
                    
                    
                        University of Toronto. (2015). Archive-It - Canadian Political Parties and Political Interest Groups [WWW Document]. 
                        
                            https://archive-it.org/collections/227
                        .
                    
                    
                        uwdata/termite-data-server. GitHub. 
                        https://github.com/uwdata/termite-data-server.
                    
                    
                        Blei, D. M., Ng, A. Y., Jordan, Michael I. (2003). Latent Dirichlet Allocation. 
                        J. Mach. Learn. Res, 3: 993–1022.
                    
                    
                        Brügger, N. and Finnemann, N. O. (2013). The Web and Digital Humanities: Theoretical and Methodological Concerns. 
                        Journal of Broadcasting & Electronic Media, 57(1): 66–80.
                    
                    
                        Lin, J., Gholami, M. and Rao, J. (2014). Infrastructure for Supporting Exploration and Discovery in Web Archives. 
                        Proceedings of the 23rd International Conference on World Wide Web. doi:10.1145/2567948.2579045.
                    
                
            
        
    


        
            
                Panel Topic
                Folkloric communication is a specific mode of social interaction that uses existing knowledge and creatively adapts it to particular situations, audience, and intent. It always includes some repetition and some innovation or, in other words, as Michel de Certeau (1984) expresses it in his study of everyday practices, the main attribute of cultural transmission is the changing nature of everything that is being passed on.
                The questions of authorship, stylistics and variation in time and space have been discussed and problematized in folklore studies for over a hundred years. Digital era has brought along mass digitization of cultural heritage documents and the compilation of folklore databases and text corpora (see e.g. Schmitt, 2014). However, the use of computational methods in researching folklore and its specifics has thus far been modest. The intention of this panel is to discuss the possibilities that computational methods have to offer in revealing the inherent qualities of folkloric communication in various text collections and research corpora. The participants of the panel deal with different source data including archival records and publications as well as contemporary social media. They discuss different aspects of folkloric communication and variation, contributing together to a general insight into these processes.
            
            
                The Panel: speakers and contributions
                In this panel, the participants will introduce their projects, data, methods and research results to contextualize and elicit a discussion on the state-of-the-art in digital folkloristics, and on possible ways forward.
                
                    Mari Sarv and 
                    Risto Järv (Estonian Literary Museum) analyse the essence of 
                    folkloric variation relying on the text corpora from the collections of Estonian Folklore Archives. The Estonian Runic Songs’ database (1996-2015) contains ca. 100,000 poetic texts, and the database of folk tales consists of 11,000 tales (both together with metadata). Previous studies of folksongs have shown that the statistical analysis of poetical features of songs as well as their content allows us to locate their geographical origin (i.e. tracing the belonging of songs to the tradition of a local community) quite precisely. At the same time there are clear differences in the geographical distribution of linguistic-poetical features, especially compared to the elements of content. The stylistic analysis of folklore texts enables us to find out how much the personal style of performer is revealed in folkloric recordings of traditional plots (‘types’ in the folkloristic discourse). The potential of computational methods to tackle the dichotomy of stability and variation in the folkloric communication poses a most intriguing challenge.
                
                
                    Kati Kallio (Finnish Literary Society) will discuss 
                    the characteristics of oral poetry, including complex patterns of variation and tricky definitions of authorship, and concentrate on the challenges posed by of the SKVR-corpus of Finnic oral poetry (see also Digital Archive of Finnish Folk Tunes). The corpus represents various languages and dialects, orthographies, personal writing styles and traces of different modes of performance, but is held together by similar poetic registers. For this kind of specific poetic register in several related small languages and across a wide variety of genres, no ready-made tools for computational linguistic analysis exists. On the other hand, the corpus already includes a detailed thematic index, and the researchers have applied various manual methods to the corpus for hundred years. What kinds of new questions could be answered with computational methods? The author will present some test analyses and discuss the future possibilities of digital folkloristics on oral poetry.
                
                
                    Greta Franzini and 
                    Emily Franzini (Göttingen Centre for Digital Humanities) will elaborate on two research projects focusing on the computational interrogation of folktale collections and corpora. Using the Brothers Grimm’s Kinder- und Hausmärchen as a case study and base reference, one project addresses the popularization of fairy tale motifs triggered by the Brothers, and seeks to algorithmically crawl web corpora to study the global network of motifs. Motifs form the significant set of “key words” of a tale. In order to systematically crawl for parallel texts, we prepare a digital, machine-readable and -citable index in different languages. TRACER as a text reuse framework is used to check if a document contains similar keywords or describes the same tale. It implements a seven-layer approach combining segmentation, preprocessing, featuring, selection, linking, scoring, and postprocessing steps. The other project examines the textual evolution of the Kinder- und Hausmärchen, starting with the first edition published in 1812 to the seventh and last in 1857. The number of fairy tales grew with every edition, and the numerous changes the Brothers made over the decades in terms of both style and content were symptomatic of societal interest and development. These seven editions represent an ideal testbed not only to computationally verify existing research about this progression but also to identify and distinguish the authorial and stylistic fingerprints of Jacob and Wilhelm Grimm. The discussion will demonstrate the possibilities afforded by the Digital Humanities to conduct 
                    web-scale and 
                    Big Data research. 
                
                
                    Liisi Laineste (Estonian Literary Museum) is applying the methodologies of Digital Humanities to the folkloric aspect of 
                    social media content in combination with theories of global information flow, participatory journalism and humour theory. Internet has become central in contemporary cultural communication – most of online communication can be treated as folklore, in which shared norms and values are constructed through cultural artifacts. Forums, commentary boards of news sites, blogs, Twitter and other social media applications have become commonplace during the last fifteen years. In the Internet, news, ideas and opinions travel fast. Social media spreads folklore in unforeseen volumes across national and cultural boundaries. Above all, the focus is on producing and consuming texts, images and multimedia. The presentation will attempt to trace such cultural texts as they move across and between cultures. Besides, as social media is often perceived as a catalyst and accelerator of public discussion and citizen movements, cultural texts as valuable agents in citizen engagement will be discussed in the light of the 2015 refugee crisis in Europe.
                
            
        
        
            
                
                    Bibliography
                    
                        De Certeau, M. (1984). 
                        The Practice of Everyday Life. Berkeley: University of California Press.
                    
                    
                        Digital Archive of Finnish Folk Tunes, http://esavelmat.jyu.fi/index_en.html (accessed 27 October 2015).
                    
                    
                        
                            Estonian Runic Songs’ database (1996-2015), Estonian Folklore Archives. Estonian Literary Museum, 1996-2015, http://www.folklore.ee/regilaul/andmebaas/?ln=en (accessed 1 November 2015).
                        
                    
                    
                        
                            Schmitt, Christoph (Ed.) (2014). 
                            Corpora ethnographica online. Strategies to digitize ethnographical collections and their presentation on the Internet. Waxmann Verlag GmbH. (Rostocker Studien zur Volkskunde und Kulturgeschichte; 5).
                        
                    
                    
                        SKVR-tietokanta – kalevalaisten runojen verkkopalvelu. Suomalaisen Kirjallisuuden Seura, http://skvr.fi. (accessed 27 October 2015).
                    
                
            
        
    


        
            New York City English (NYCE) has long been a stigmatized variety of English. In his seminal research on language use in the New York City dialect, the sociolinguist William Labov referred to New York City as “a great sink of negative prestige” (Labov, 1966)—a characterization that reflected the negative view of NYCE speech shared by non-New Yorkers and New Yorkers alike. Decades later, Preston (2003) elicited extremely low ratings of the New York City dialect on scales of both “correctness” and “pleasantness” by participants from across the US. While these studies present strong evidence of the prevalence of negative language attitudes toward NYCE speech, a more complete picture of linguistic ideology would include what speakers say about NYCE when they are not participating in an academic study. This project seeks to accomplish just that, by examining linguistic ideology with respect to NYCE as espoused by users of the social networking service, Twitter.
            Twitter has been recognized as an important resource for humanists and social scientists alike. Scholars have collected and analyzed Twitter messages (tweets) in order to investigate numerous textual and linguistic phenomena such as 
                lexical variation (differences in use of synonymous words and phrases, such as 
                pop vs. 
                soda vs. 
                coke). Russ (2012) in particular (see also Bamman, 2011) illustrates the utility of Twitter for examining regionally defined lexical variation through comparison of the geographic distribution of word choices in 
                geotagged tweets (with GPS coordinates from which they originated) to more traditionally collected dialectology data. All related research has focused on differences in production. However, I argue that Twitter represents an untapped resource for the investigation of 
                perceptions of language use, particularly language attitudes toward regional dialects and differences in their phonetic features (which can be identified by non-standard orthography). Using Twitter solves a primary quandary for language attitude researchers—how to acquire naturally occurring data given the fact that participation in research decreases naturalness.
            
            Tweets containing attitudes and ideology were collected using a range of strategies, including text mining for words—and, crucially, spellings—that reference individual features. To do this, however, it is necessary to determine which features get noted and then which lexical items—and which spellings—are used to signal them. For instance, 
                cawfee (also, 
                cawffee) is a common orthographic representation of the word “coffee” as pronounced with a raised-THOUGHT vowel, one of the signature dialect features of NYCE. Widely used spellings that reflect 
                r-vocalization, another key feature of the NYCE dialect, include 
                New Yawk and 
                fuhgeddaboudit. In addition to collecting tweets containing orthographic representations of nonstandard features, Twitter search parameters included over 20 terms related to possible names for the dialect itself (e.g., 
                New York accent, Manhattan dialect, Brooklynese). These were included in part to determine the extent to which the general public perceives a distinction among speakers from the five boroughs (a distinction which has not been borne out by linguistic analysis).
            
            Repeated automated text mining of Twitter using a Python script to interact with the Twitter API yielded 6,384 tweets that match the aforementioned criteria. Elimination of retweets that did not introduce additional linguistic content and inspection to ensure the tweets reference NYCE produced a final corpus of 1,773 tweets. Relative frequencies of the borough-specific and pan-regional terms in the 1,315 tweets that explicitly reference NYCE by some name reveal that Twitter users most frequently refer to NYCE as the 
                New York accent (N=805; 61.2%), though 
                Brooklyn accent (N=359; 27.4%) accounts for more than a quarter, with 
                Bronx accent (N=54), 
                Queens accent (N=29, tied with 
                Brooklynese—the most frequent 
                -ese moniker), and 
                Staten Island accent (N=10) being used much less often. Whether 
                New York accents and 
                Brooklyn accents are perceived as linguistically or socially distinct, or two names for the same dialect region, will be explored in the paper.
            
            All tweets were manually coded to determine their sentiment with respect to NYCE.
            
                POSITIVE: 
                    I swear girls from New York accent sound so sexy 
                
                NEUTRAL: 
                    GAWGEOUS idea she said in her New Yawk accent 
                
                NEGATIVE: 
                    If you have a Brooklyn accent I automatically want to punch you. 
                
            
            Almost half of these tweets are neutral in sentiment (N=584, 44.4%); 378 were positive (28.7%) and 200 negative (15.2%). However, 154 tweets were classified as UNCLEAR (8.7%)—many are ambiguous as to whether they evaluate an imitation of an accent or the accent itself, such as when describing an actor’s performance (which is common among these types of tweets):
            
                UNCLEAR: 
                    his New York accent is so bad /: 
                
            
            Examples such as these pose significant obstacles to automated sentiment analysis—which has been extended to Twitter data (see for instance Pak and Paroubek, 2010)—particularly of language attitudes. Automatic methods would simply code the tweet as negative without recognizing the need to differentiate its underlying meaning. It is noteworthy, however, that even if every UNCLEAR tweet is actually expressing negative sentiment, there would 
                still be a greater number of tweets with positive opinions of NYCE speech than negative ones. Furthermore, when Twitter users reference a specific NYCE feature, their evaluation of it is more likely to be positive, regardless of whether they use standard (N=79) or nonstandard (N=568) orthography to represent the feature.
            
            These findings portray a broader range of reactions to NYCE than the language attitudes speakers have presented when engaged in academic research. The paper will include discussion of both negative and positive language attitudes that Twitter users espouse concerning the dialect features associated with NYCE. For instance, any tweets with positive sentiment will be examined to determine if they represent instances of “covert prestige” (Labov 1966), whereby speakers use stigmatized varieties for in-group identification and solidarity. Additional discussion will focus on which regional features evoke the most meta-commentary. Furthermore, I will explore the extent to which Twitter users draw (additional) attention to non-standard forms they employ through capitalization (
                NEW YAWK), hashtags (
                #newyawk), and other orthographic means.
            
        
        
            
                
                    Bibliography
                    
                        Bamman, D. (2011). Lexicalist. http://www.lexicalist.com/ (accessed 30 August 2015). 
                    
                    
                        Labov, W. (1966). 
                        The Social Stratification of English in New York City. Washington, D. C.: Center for Applied Linguistics.
                    
                    
                        Pak, A. and Paroubek, P. (2010). Twitter as a Corpus for Sentiment Analysis and Opinion Mining. 
                        Proceedings of Language Resource and Evaluation Conference (LREC), Valletta, Malta. 
                    
                    
                        Preston, D. (2003). Language with an attitude. In J. K. Chambers, Peter Trudgill and Natalie Schilling-Estes (eds),  
                        The Handbook of Language Variation and Change. Oxford: Wiley- Blackwell, pp. 40-66. 
                    
                    
                        Russ, B. (2012). 
                        Examining large-scale regional variation through online geotagged corpora. Presented at the 2012 American Dialect Society Annual Meeting.
                    
                
            
        
    


        
            For many at this conference, stylometry and authorship attribution need little introduction; the determination of who wrote a document by looking at the writing style is an important problem that has received much research attention. Research has begun to converge on standard methods and procedures (Juola, 2015) and the results are increasingly acceptable in courts of law (Juola, 2013). 
            The most standard experiment looks something like this: collect a training set (aka "known documents," KD) representative of the documents to be analyzed (the testing set, aka "questioned documents," QD) and extract features from these documents such as word choice (Burrows, 1989; Binongo, 2003) or character n-grams (Stamatatos, 2013). On the basis of these features, the QD can be classified -- for example, if Hamilton uses the word "while" and Madison uses the word "whilst" (Mosteller and Wallace, 1963) a QD that doesn't use "while" is probably Madisonian. 
            ... unless it's not in English at all, in which case, neither word is likely to appear. The need for the KD to represent the QD fairly closely is one of the major limitations on the use of this experimental methodology. By contrast, the authorial mind remains the same irrespective of the language of writing. In this paper, we report on new methods based on cross-linguistic cognitive traits that enables documents in Spanish to be attributed based on the English writings of the authors and vice versa. Specifically, using a custom corpus scraped from Twitter, we identify a number of features related to the complexity of language and expression, and a number of features related to participation to Twitter-specific social conventions. 
            We first identified (by manual inspection) a set of 14 user names that could be confirmed to have published tweets in both English and Spanish. Once our user list had been collected, we scraped the Twitter history of each user to collect between 90 and 1800 messages ("tweets") from each user and used the detectlanguage.com server to identify automatically the language of each tweet.
            A key problem is feature identification, as most features (e.g. function words or character n-grams) are not cross-linguistic. For this work, we have identified some potentially universal features. One of the most long-standing (de Morgan, 1851) features proposed for authorship analysis is complexity of expression, as measured variously by word length, distribution of words, type/token ratio, and so forth. We used thirteen different measures of complexity that have been proposed (largely in the quantitative linguistics literature) to create a multivariate measure of complexity that persists across languages. Similarly, we identified three specific social conventions (the use of @mentions, #hashtags, and embedded hyperlinks, all measured as percentage of occurrence) that people may or may not participate in. Our working hypothesis is that people will use language in a way that they feel comfortable with, irrespective of the actual language. Hence, people who use @mentions in English will also do so in Spanish. Similarly, people who send long tweets in English also do so in Spanish, people who use big words in English also do so in Spanish, people who use a varied vocabulary in English also do so in Spanish, and of course vice versa. 
            We were able to show, first, that the proposed regularities do, in fact, hold across languages, as measured by cross-linguistic inter-writer correlations. (Thus, we also showed that our working hypothesis is confirmed, at least for these traits). Second, we showed via cluster analysis that these measures are partially independent from each other, and thus they afford a basis for a stylistic vector space. (Juola and Mikros, under review). This potentially enables ordinary classification methods to apply. The results reported here show that, in fact, they do. 
            To do this, we apply normal classification technology (support vector machines using a polynomial kernel) to the vector space thus constructed. We first broke each individual collection into 200 word sections (thus conjoining multiple tweets). Each section was measured using each complexity feature and then raw values were normalized using z-scores [thus a completely average score would be zero, while a score at the 97th percentile would be approximately 2.0; this is similar to Burrows' Delta (Burrows, 1989)]. For our first experiment, the English sections were used to create a stylometric vector space, then the Spanish sections were (individually) embedded in this space and classified via SVMs. For our second experiment, the languages were reversed, classifying English sections based on Spanish stylometric space. Since SVM with polynomial kernel is a three parameter model, we optimized the classifier's performance using a grid-search parameter tuning and comparing 3 different values for each of the three parameters (totaling 3^3 models). The classifier's performance was evaluated using a 10-fold cross-validation scheme and the best single language model was used for predicting the authorship of the texts written in the other language from the same authors.
            This resulted in 2652 attempts to predict authorship of individual 200 word sections in Spanish, and another 1922 attempts in English, classified across fourteen potential authors. Baseline (chance) accuracy is therefore 1/14 or 0.0714 [7.14%].
            Using the English data to establish the stylometric space and the Spanish samples to be attributed yielded an accuracy of 0.095, a result above baseline but not significantly so. By contrast, embedding English data into a Spanish space yielded an accuracy of 0.1603, more than double the baseline. This result clearly establishes the feasibility of cross-linguistic authorship attribution, at least at the proof of concept level. Experiments are continuing, both to establish clearer statistical results, and also to evaluate the additional effectiveness of the Twitter-specific social conventions as features.
            We believe this result to be the first recorded instance of using training data from one language to attribute test data from another language using a formal, statistical attribution procedure. This is a very difficult dataset using an extremely small set of predictive variables, and the samples (200 words) are very small (Eder, 2013). In light of these issues, the relatively low (in absolute terms) accuracy may still represent a major step forward.
            Like many research projects, these results pose as many questions as they answer. Why is English->Spanish easier than Spanish->English? What other types of language-independent feature sets could be developed, and how would performance compare? Do these results generalize to different language pairs, or to different genres than social media and Twitter in particular? What additional work will be necessary to turn this into a practical and useful tool? Can this generalize to other authorial analysis applications such as profiling (of personality or other attributes)? 
            Further research will obviously be required to address these and other issues. In particular, this study is obviously only a preliminary study. More language pairs are necessary (but finding active bilinguals on Twitter is difficult). Studies of other genres than tweets would be informative, but again corpus collection is problematic. We acknowledge that the current accuracy is not high enough to be useful. For the present, however, the simple fact that cross-linguistic authorship attribution can be done and has been done, remains an important new development in the digital humanities. 
        
        
            
                
                    Bibliography
                    
                        Binongo, J. N. G. (2003). Who wrote the 15th book of Oz? An application of multivariate analysis to authorship attribution. 
                        Chance, 16(2): 9–17.
                    
                    
                        Burrows, J. F. (1989).`An ocean where each kind...': Statistical analysis and some majordeterminants of literary style. 
                        Computers and the Humanities, 23(4-5): 309-21.
                    
                    
                        Eder, M. (2013). Does size matter? Authorship attribution, short samples, big problem.
                        Digital Scholarship in the Humanities, 30(2): 167–82.
                    
                    
                        De Morgan, A. (1851). Letter to Rev. Heald 18/08/ 1851. In Elizabeth, S. and Morgan, D. (eds), 
                        Memoirs of Augustus de Morgan by His Wife Sophia Elizabeth de Morgan with Selections from His Letters. Cambridge: Cambridge University Press.
                    
                    
                        Juola, P. (2013). Stylometry and immigration: A case study. 
                        Journal of Law and Policy, 21(2): 287–98.
                    
                    
                        Juola, P. (2015). The Rowling case: A proposed standard analytic protocol for authorship questions. 
                        Digital Scholarship in the Humanities.
                    
                    
                        Juola, P. and Mikros, G. (under review). Cross-Linguistic Stylometric Features: A Preliminary Investigation. Ms. Submitted to 
                        JADT 2016. 
                    
                    
                        Mosteller, F. and Wallace, D. L. (1964). 
                        Inference and Disputed Authorship: The Federalist. Reading, MA: Addison-Wesley.
                    
                    
                        Stamatatos, E. (2013). On the robustness of authorship attribution based on character n-gram features. 
                        Journal of Law and Policy, 21(2): 420-40.
                    
                
            
        
    


        
            
                Introduction
                Sentiment analysis and opinion mining methods are established for automatically summarizing information shared by users in product reviews or in social media platforms like Twitter, Facebook or more specific fora (Liu 2015). These approaches can be categorized into coarse-grained and fine-grained methods: The first focus on assigning a polarity (positive, negative, neutral) and optionally an intensity to a text snippet (Täckström and McDonald 2011; Pang and Lee 2004). The latter additionally aim at detecting the opinion holder (for instance a specific person mentioned in a news article) and the target (for instance a specific aspect of a product in a review) (Hu and Liu 2004; Popescu and Etzioni 2005; Jakob and Gurevych 2010).
                Transferring such methods to the analysis of literature leads to at least two questions: Firstly, are polarities for this domain as helpful as for the analysis of reviews? Secondly, how can such methods from sentiment analysis be improved, and what can they contribute to literature analysis?
                Regarding the first aspect, resources to measure the occurrence of words which are associated with different emotions have been developed for English but, to the best of our knowledge, not for German (Mohammad et al. 2015). Secondly, it should be noted that research in German sentiment analysis is still comparably limited (counter examples are Ruppenhofer et al. 2014; Klinger and Cimiano 2015; Remus, Quasthoff, and Heyer 2010). In addition, sentiment analysis has mainly focused on the Web, like social media, and product reviews. However, the analysis of emotions and sentiment in literature has been proven to be of interest and value (Mellmann 2007; Winko 2003). A prerequisite for a quantitative approach is that emotions are (at least to some extend) a surface phenomenon (Hillebrandt 2011, p. 154), i.e., that words carry information such that it is possible to infer “private states” of specific emotions (Wiebe, Wilson, and Cardie 2005).
                Our two main contributions are: (a) We make German dictionaries of words associated with seven fundamental emotions publicly available, and (b) perform a case study on Kafka’s “Amerika” and “Das Schloss” regarding emotion analysis to support literature studies with a focus on complex non-factual phenomena and the analysis of personality traits. All resources and software used in this paper are made publicly available at 
                    http://www.romanklinger.de/emotion/.
                
            
            
                Materials and Methods
                The goal of this work is to detect different emotions represented in literary texts. Psychological research offers different models to categorize emotions. The most common ones include Plutchik’s wheel of emotions (Plutchik 2001) and Ekman’s definition of fundamental emotions (Ekman 1999). A discussion of relevant context is offered by Russell (Russell 1991). We opt for roughly following the structure of Ekman’s definition of emotions and focus on 
                    anger (Wut), 
                    disgust (Ekel), 
                    fear (Angst), 
                    enjoyment (Glück), 
                    sadness (Trauer), and 
                    surprise (Überraschung) and 
                    contempt (Verachtung).
                
                To track emotions over the whole text, we assign an emotion score es(
                    e,
                    t
                    ab) to a subset of consecutive tokens 
                    t
                    ab from textual position 
                    a to position 
                    b as
                
                
                    
                
                where 
                    D
                    e is a dictionary containing words expressing the specific emotion 
                    e and 1
                    t
                    ∈
                    D is 1 if and only if t
                    i∈
                    D
                    e and 0 otherwise. This score corresponds to the number of tokens which are in a window and in the respective emotion dictionary, normalized by the dictionary size.
                
                To track the development of the emotions over the whole text, we apply a sliding window approach which is parameterized by window size w such that 
                    b = a + w − 1 (which can be interpreted as a smoothing parameter). To allow for a character oriented analysis, we assign an emotion score as in the sliding window, but for windows around each mention of such character in the text, with an additional normalization based on number of character mentions. Each token and dictionary entry is normalized by mapping to lower-case and stemming with the Porter stemmer (Porter 2001).
                
                As a resource for the emotion dictionaries, two authors of this paper manually selected words from different sentiment polarity, subjectivity, and emotion resources in German and English (translated to German) into the emotion categories (Waltinger 2010a; Waltinger 2010b; Remus, Quasthoff, and Heyer 2010; Mohammad and Turney 2013). We semi-automatically enriched this resource with synonyms (Naber 2005; Wermke, Kunkel-Razum, and Scholze-Stubenrecht 2010).
            
            
                Experiments and Conclusions
                As an estimate for the difficulty of emotion assignments, we performed an annotation experiment of 300 words (stratified sample from all emotions in the dictionary mentioned above) with fluent speakers of German. In 85 % of all words two out of three annotators agree on the same emotion, however, only in 46 % of of all words, three annotators agree on the associated emotion.
                As a use-case, we apply our methods to Franz Kafka’s “Der Verschollene” (“Amerika”) and “Das Schloß”. Especially the latter is interesting as a comprehensive emotion-focused manual analysis is available (Hillebrandt 2011). It is narrated in third person and interesting from an emotion analysis point of view, as attribution of specific emotions to the protagonist is difficult (Hillebrandt 2011, p. 165).
                The development of emotions in Figures 1 and 2 visualize the outcomes of our analyzes. In “Das Schloss”, the strong increase of surprise towards the end is striking (most indicative words are “neu”, “schnell”, “plötzlich”, “ungeduldig”). Another example for an eye-catching peak of fear is shortly after start of chapter 3 (“ängstlich”, “Gefahr”, “unruhig”, “Gewalt”). In “Amerika”, one striking characteristic is the decrease of enjoyment after a peak in chapter 4 (“gut”, “Mutter”, “glücklich”) followed by disgust in chapter 5 (“unerträglich”, “Elend”, “schrecklich”, “beschmutzt”). Emotions for each mention of a selection of characters in “Amerika” and “Das Schloss” are shown in Figures 3 and 4.
            
            
                Acknowledgements
                This project has been partially funded by the project CRETA (Zentrum für reflektierte Textanalyse, 
                    http://www.creta.uni-stuttgart.de/), funded by the German Ministry of Education and Research.
                
                
                    
                
                
                    
                
                
                    
                
                
                    
                
            
        
        
            
                
                    Bibliography
                    
                        Ekman, P. (1999). “Basic Emotions”. In: 
                        Handbook of Cognition and Emotion. Ed. by M Dalgleish T; Power. Sussex, UK: John Wiley & Sons.
                    
                    
                        Hillebrandt, C. (2011). 
                        Das emotionale Wirkungspotenzial von Erzähltexten. Deutsche Literatur - Studien und Quellen. Berlin, Germany: Akademie Verlag.
                    
                    
                        Hu, M. and Liu, B. (2004). “Mining and summarizing customer reviews”. In: 
                        Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. Seattle, WA, USA: ACM, pp. 168–77.
                    
                    
                        Jakob, N. and Gurevych, I. (2010). “Extracting opinion targets in a single- and cross-domain setting with conditional random fields”. In: 
                        Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Cambridge, Massachusetts: Association for Computational Linguistics, pp. 1035–45.
                    
                    
                        Klinger, R. and Cimiano, P. (2015). “Instance Selection Improves Cross-Lingual Model Training for Fine-Grained Sentiment Analysis”. In: 
                        Proceedings of the Nineteenth Conference on Computational Natural Language Learning. Beijing, China: Association for Computational Linguistics, pp. 153–63.
                    
                    
                        Liu, B. (2015). 
                        Sentiment Analysis. Cambridge University Press.
                    
                    
                        Mellmann, K. (2007). 
                        Emotionalisierung – Von der Nebenstundenpoesie zum Buch als Freund. Vol. 4. Poetogenesis - Studien zur empirischen Anthropologie der Literatur. Münster, Germany: Mentis Verlag.
                    
                    
                        Mohammad, S. M. and Turney, P. D. (2013). “Crowdsourcing a Word-Emotion Association Lexicon”. In: Computational Intelligence, 29(3): 436–65.
                    
                    
                        Mohammad, S. M., Zhu, X., Kiritchenko, S. and Martin, J. (2015). “Sentiment, emotion, purpose, and style in electoral tweets”. In: 
                        Information Processing & Management, 51(4): 480–99.
                    
                    
                        Naber, D. (2005). 
                        OpenThesaurus: ein offenes deutsches Wortnetz. http://danielnaber.de/ publications/gldv-openthesaurus.pdf (visited on 02/17/2015).
                    
                    
                        Pang, B. and Lee, L. (2004). “A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts”. In: 
                        Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Main Volume. Barcelona, Spain, pp. 271–78.
                    
                    
                        Plutchik, R. (2001). “The Nature of Emotions”. In: 
                        American Scientist,  89: 344–50.
                    
                    
                        Popescu, A.-M. and Etzioni, O. (2005). “Extracting Product Features and Opinions from Reviews”. In: 
                        Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing. Vancouver, British Columbia, Canada: Associ- ation for Computational Linguistics, pp. 339–46.
                    
                    
                        Porter, M. F. (2001). 
                        Snowball: A language for stemming algorithms. http://snowball. tartarus.org/texts/introduction.html.
                    
                    
                        Remus, R., Quasthoff, U. and Heyer, G. (2010). “SentiWS – a Publicly Available German- language Resource for Sentiment Analysis”. In: 
                        Proceedings of the 7th International Language Resources and Evaluation (LREC’10), pp. 1168–71.
                    
                    
                        Ruppenhofer, J., Klinger, R., Struß, J. M., Sonntag, J. and Wiegand, M. (2014). “IGGSA Shared Tasks on German Sentiment Analysis”. In
                        : Workshop Proceedings of the 12th Edition of the KONVENS Conference. Ed. by G. Faaß and J. Ruppenhofer. Hildesheim, Germany: Uni- versity of Hildesheim.
                    
                    
                        Russell, J. A. (1991). “In Defense of a Prototype Approach to Emotion Concepts”. In: 
                        Journal of Personality and Social Psychology, 60(1): 37–47.
                    
                    
                        Täckström, O. and McDonald, R. (2011). “Semi-supervised latent variable models for sentence- level sentiment analysis”. In: 
                        Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Portland, Oregon, USA: Association for Computational Linguistics, pp. 569–74.
                    
                    
                        Waltinger, U. (2010a). “GERMANPOLARITYCLUES: A Lexical Resource for German Sentiment Analysis”. In: 
                        Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC). Valletta, Malta.
                    
                    
                        Waltinger, U. (2010b). “Sentiment Analysis Reloaded: A Comparative Study On Sentiment Polarity Identi- fication Combining Machine Learning And Subjectivity Features”. In: 
                        Proceedings of the 6th International Conference on Web Information Systems and Technologies (WEBIST ’10). Valencia, Spain.
                    
                    
                        Wermke, M., Kunkel-Razum, K. and Scholze-Stubenrecht, W., (eds). (2010). 
                        Duden – Das Synonymwörterbuch. Mannheim, Zürich: Dudenverlag.
                    
                    
                        Wiebe, J., Wilson, T. and Cardie, C. (2005). “Annotating Expressions of Opinions and Emotions in Language”. In: 
                        Language Resources and Evaluation, 39(2): 165–210.
                    
                    
                        Winko, S. (2003). 
                        Kodierte Gefühle: Zu einer Poetik der Emotionen in lyrischen und poetologischen Texten um 1900. Erich Schmidt Verlag.
                    
                
            
        
    

        
        
            Summary
            This poster will seek to illustrate the enhanced communications presence of the European Association for Digital Humanities (EADH) since 2014, and measure the impact of its action in building a community of geographically-dispersed members. It will outline the strategies undertaken by the EADH in furthering its communications initiative, for which a number of Communications Fellows have been activated on the organisation's communications policy in an effort to promote the work of digital scholars across the European region. Accounts of this activity will be further contextualised by theoretical discussion on the evolution of social media and Web-based communications across academia.
            
            
                Introduction
                This poster will seek to illustrate the enhanced communications presence of the European Association for Digital Humanities (EADH) since 2014, and measure the impact of its action in building a community of geographically-dispersed members. The poster will outline the strategies undertaken by the EADH in furthering its communications initiative over the last 24 months, during which a number of EADH Communications Fellows have been collaborating on the organisation's communications policy in an effort to promote, more broadly, the work of digital scholars across the European region. Accounts of this activity will be further contextualised by theoretical discussion on the evolution of social media and Web-based communications across academia.
            
            
                Communications Fellows
                As noted, in an effort to engage further with the international Digital Humanities community in respect to news, events, and opportunities, the EADH has motivated a diverse group of four junior scholars tasked with strengthening the Association's public interactions. The self-directed goals of the Fellows are as follows:
                
                    Publish news, announcements and CfPs relevant to the European community of digital scholars
                    Document projects undertaken in Europe in recent times and feature them in a slider to promote access and collaboration between members
                    Ensure quality of language, as well as accuracy and detail of information, across all official EADH correspondence
                    Increase community engagement through social media, particularly Facebook, Twitter, and LinkedIn
                    Enhance the profile of relevant scholars and scholarship from across Europe’s various Digital Humanities projects, centres, and initiatives
                    Curate and disseminate information across a variety of European languages, enhancing the cultural diversity of the organisation's communications 
                
            
            
                Social Media
                The social media revolution that has spread into the academy in recent years has shifted scholarly communications towards participatory technologies. Collectively, these social technologies are now dominating the ways in which users interact across the Web. In an effort to foster a network of researchers which embraces diversity, one must take advantage of the networks of exchange facilitated by these platforms. Initially, the focus of the fellows was oriented towards social media, Facebook, Twitter, and LinkedIn. These channels have proved essential in the collection and distribution of relevant information, and have contributed to the advancement of Europe's community of Digital Humanities scholars. Currently, the EADH engages with 2373 followers on Twitter, 491 users on Facebook, and 85 users on Linkedin. In the context of this networking activity, the EADH collaborates with associate organisations, AIUCD, Dhd, Nordic DH, and DH Benelux as well as with the Alliance of Digital Humanities Organizations (ADHO) to increase the synergies and the visibility of its members.
            
            
                Discussion
                This poster will illustrate the presence and performance of the EADH on social media, provoking discussion with attending researchers on their personalised use of social media and their future aspirations in regards to the employment of social media in promoting Digital Humanities research across regional and global contexts. Furthermore, the poster will present metrics on the success of this particular communications drive, as well as detail various strategic decisions, such as how to structure a Facebook group for a scholarly community, and the benefits and drawbacks of each.
            
        
    


        
            How is cultural authority visualized in social media through the publication, revision, and erasure of citations? This poster highlights the social dimensions of Wikipedia’s creation, revision, and dissemination. Many of us may be familiar with the mechanics of knowledge production on Wikipedia: the "citation needed" requests that follow unattributed information on Wikipedia pages, the lengthy revision histories available to interested readers, the use of bots as well as human editors, the privileging of public domain images (among other dimensions). Attention will be paid to the interfaces of the Wikipedia and its articles: their performative dimensions as sites of cultural authority, the edit histories of articles and the forms of gatekeeping revealed in patterns of page revisions, and the visualization tools created by the Wikipedia community to visualize patterns in composition and citation. More broadly, I want to situate the investments in particular patterns and performances of curation and citation present in Wikipedia within the larger context of modes of cultural production found on the web, focusing particular attention on forms of erasure and practices that, for various reasons, disavow or ignore investments in citation and attribution: screencapping, the creation of image macros, and other transformative uses of cultural objects on Twitter, Tumblr, and other platforms.
            I use Wikipedia pages related to poet John Ashbery to highlight the ways ideas of literature circulate in one of the world’s largest and most accessible encyclopedic resources. I am interested in Ashbery because of his literary stature in the eyes of segments of North American and global audiences. Additionally, Ashbery creates particular challenges for scholars (and other readers) interested in periodization and reception history; he has ties to several literary coteries across time and space (the New York School, L=A=N=G=U=A=G=E poetry), his work has been praised by scholars, journalists, and MTV (among other audiences), andhe continues (as of this writing) to publish new material that unsettles the work being done to cement his legacy. The various editorial and citational practices involved in creating and revising that legacy on Wikipedia can tell us much about the impact of academic and scholarly works, the privileging of certain modes of reading and cultural analysis, and the various investments in certain ideas about the value of poetry that circulate on the web. Wikipedia is an extremely visible and malleable public space where competing claims about aesthetics collide and re-collide.
        
    


        
            
                Introduction
                The study of an individuals’ personality traits is a new line of research that emerged only recently, primarily through the investigation of dialogue systems and weblogs (Gill et al., 2012; Konstantopoulos, 2010; Mairesse and Walker, 2006; Mairesse and Walker, 2007). This paper proposes a novel application for personality classification by leveraging on cognitive computing research and by exploiting the poetic production of theatrical plays. More specifically, this research is circumscribed to the analysis of Shakespeare’s tragedies, which offer a rich spectrum of characters for a detailed and in-depth study. This research does not aim at introducing the innovative technological aspects of personality classification in cognitive computing but rather at employing such technology in the study of English literature, and analyse some implications that arise from the results.
                Before presenting the core of this research, we outline the psychological theory of the “Big Five,” its implementation as well as its extension in IBM Watson. Successively, in Section 2, we introduce the data used, the method applied and the results obtained in our analysis. In Section 3, we discuss some useful applications and extensions for literature scholars. Finally, we conclude by presenting some considerations for future research.
            
            
                    IBM Watson and the Big Five personality insight
                    Cognitive computing originated in the early 60s, however, it has been improved dramatically in recent years, achieving significant success through the launch of IBM Watson in 2010. IBM Watson simulates human cognitive systems by implementing advanced natural language processing, information retrieval, knowledge representation, automated reasoning, and machine learning technologies to the field of open domain question answering (Ferrucci et al., 2010; Ferrucci, 2012). Among human cognitive activities, one of the most employed is the capability to understand and forecast other people’s personalities. As described in the Big Five theory (Norman, 1963) formulated by scholars in psychology, each individual presents a different aptitude in identifying characteristic patterns of thinking, feeling and behaving in others. The Big Five theory is the main theory on which IBM Watson has been built; it distinguishes between five broad dimensions underlying an individual’s personality, namely openness, conscientiousness, extroversion, agreeableness, and neuroticism.
                    
                        Openness mirrors the level of scholarly interest, imagination and an inclination for oddities. 
                        Conscientiousness is the propensity to be reliable, to show self-control and act obediently. 
                        Extroversion comprises vitality, positive feelings, confidence, amiability and the propensity to look for incitement in the organization of others. 
                        Agreeableness is the inclination to be merciful and agreeable as opposed to suspicious and adversarial towards others. 
                        Neuroticism is the propensity to encounter obnoxious feelings, for example, outrage, uneasiness, wretchedness, and powerlessness.
                    
                    Next to the Big Five personalities, IBM Watson takes into account the concept of “Needs,” which are described by the literature as universal needs shared by all human beings (Ford, 2005; Kotler and Armstrong, 2010). Along with Big Five and Needs, IBM Watson takes into account the psychological concepts of Values which are defined as “desirable, trans-situational goals, varying in importance, that serve as guiding principles in people’s lives” (Schwartz, 2006). As mentioned on the IBM Watson website
                        .
                            accessed on October 25 2015
                         “Schwartz summarizes five features that are common to all values: (1) values are beliefs; (2) values are a motivational construct; (3) values transcend specific actions and situations; (4) values guide the selection or evaluation of actions, policies, people, and events; and (5) values vary by relative importance and can be ranked accordingly.”
                    
            
            
                Method and Results
                Our objective consists in comparing the personality of the main characters of three Shakespeare’s tragedies in a positive versus negative sentimental context. To establish these contexts we use the work of Nalisnik et al. (Nalisnick and Baird, 2013) on sentiment analysis to divide the characters of the play into two groups. The first group is composed of those characters towards which the main character expresses mainly positive sentiments; the second group comprises those characters towards which the main character expresses mainly negative sentiments. In practice, we extracted each instance of continuous speech from the plays
                    . XML versions provided by Jon Bosan: 
                        accessed on October 15 2015
                    . The groups divisions take in account the sentiment valence and the minima of sings that the IBM Personality Insights needs to produce significant results and then assumed that each speech act by one speaker was directed towards the character that spoke immediately before him. We used this assumption to replicate Nalisnik’s data but, as he pointed out in his paper, “This assumption does not always hold; it is not uncommon to find a scene in which two characters are expressing feelings about someone off-stage.”. We retrieved the sentiment valences for each main character
                    . The tables provided by Nalisnik in 
                        accessed on October 15 2015
                    , reported in the following tables:
                
                
                    
                        
                            
                                
                                    Hamlet’s Sentiment 
                                
                                
                                    Valence Sum
                                
                                
                                    Guildenstern 
                                    31 
                                
                                
                                    Polonius 
                                    25 
                                
                                
                                    Gertrude 
                                    24 
                                
                                
                                    Horatio 
                                    12 
                                
                                
                                    Ghost 
                                    8 
                                
                                
                                    Marcellus 
                                    7 
                                
                                
                                    Osric 
                                    7 
                                
                                
                                    Bernardo 
                                    2 
                                
                                
                                    Laertes 
                                    -10 
                                
                                
                                    Phelia 
                                    -5 
                                
                                
                                    Rosencrantz 
                                    -12 
                                
                                
                                    Claudius 
                                    -27 
                                
                            
                        
                        
                            
                                
                                    Othello’s Sentiment
                                
                                
                                    Valence Sum
                                
                                
                                    Iago 
                                    71 
                                
                                
                                    Cassio 
                                    38 
                                
                                
                                    Brabantio 
                                    27 
                                
                                
                                    Duke of Venice 
                                    24 
                                
                                
                                    Montano 
                                    7 
                                
                                
                                    Desdemona 
                                    -1 
                                
                                
                                    Lodovico 
                                    -4 
                                
                                
                                    Emilia 
                                    -10
                                
                            
                        
                        
                            
                            >Table 1: Tables representing the sentiment valence sum, and the used groups for each main character. Positive scores stand for a character’s positive attitude towards others and negative scores stand for a negative attitude
                                
                                    Macbeth’s Sentiment
                                
                                
                                    Valence Sum
                                
                                
                                    Murderer 1 
                                    22 
                                
                                
                                    Banquo 
                                    16 
                                
                                
                                    Duncan 
                                    8 
                                
                                
                                    Angus: 
                                    7 
                                
                                
                                    Macduff 
                                    5 
                                
                                
                                    Which 3 
                                    5 
                                
                                
                                    Which 1 
                                    1 
                                
                                
                                    Young Siward 
                                    -4 
                                
                                
                                    Lennox 
                                    -11 
                                
                                
                                    Seyton 
                                    -20 
                                
                                
                                    Lady Macbeth 
                                    -39 
                                
                            
                        
                    
                
                By assembling all continuous speech sequences directed to the characters in the different groups, we created two text groups: the first group contains all lines of the main character towards the others when expressing positive sentiments, the second group includes those speech sequences characterized by a negative sentimental connotation. We performed this task for three main characters, more specifically Hamlet, Othello and Macbeth. In Table , the positive and negative word counts for these text assemblies are plotted.
                
                Table 2: Word counts per characters of positive and negative sentimental context
                    
                        
                        Positive 
                        Negative
                    
                    
                        Hamlet 
                        6655 
                        3455 
                    
                    
                        Othello 
                        3424 
                        2407 
                    
                    
                        Macbeth 
                        1293 
                        1852 
                    
                
                Finally, we used IBM’s personality insight service to create personality profiles for each of the main characters based on the positive and negative texts. Figures 1, 2 and 3 represent the distribution of needs and values with respect to the positive or negative sentimental valence. Points on line represent independence with respect to the latter. For needs and values falling below this line, the distribution is characteristic for the expression of negative sentiments. We also created personality graphs with respect to the groups. Although they are calculated as percent of a given facet (as indicated by the x axis), the facets vary between low and high values. Thus, the bars in the graph begin at 50% and can either be low (to the left) or high. A score of 50% signifies that the facet is balanced. 
                
                    
                        
                        Figure 1: Hamlet’s scores for Needs, Values and the Big Five categories
                    
                
                
                    
                        
                        Figure 2: Othello's scores for Needs, Values and the Big Five categories
                    
                
                
                    
                        
                        Figure 3: MacBeth's scores for Needs, Values and the Big Five categories
                    
                
            
            
                Some Insights and Discussion
                A quantitative approach to character clearly generates a comparatively large amount of linguistic data. The question is, how are we to use this data? What can such an approach offer Shakespeare studies and the humanities more broadly? How does the data we have generated contribute to the critical conversation in a sub-field like character criticism (Hazlitt, 1845; Bradley, 1992; Desmet, 1992; Yachnin and Slights, 2009) which has such a long and distinguished history. There are two answers to this question. First, our analysis shows in a more concrete and detailed way than ever before, the close relationship between character and language, something easy to forget in the context of a representational practice like theatre which is so dependent on non-linguistic features, such as gesture, costume, and stage properties. Playgoers, however, do not just see character; they also hear it. And in the modern humanities classroom, they read it. Accordingly, words play a significant role is crafting what we would now call the “personalities” of Shakespeare’s stage. Our approach offers a new means of isolating and analyzing these verbal features of character. The second way in which our work contributes to Shakespeare studies has to do with something our data does not tell us. To understand what we mean by this, consider for a moment why it is that the words associated with some characters generate a personality profile that anyone familiar with the plays knows does not quite fit? Why, for example, does Hamlet have a verbal data set that makes him seem much more serious and honor-driven than he actually is It is because the technology we are using is not capable of accounting for context and therefore cannot detect things like irony and wordplay, two things that are extremely important components of the way language articulates character and personality. While this may be viewed as a methodological weakness from the perspective of information technology, it is a great strength from the perspective of the humanities. For it is precisely at those moments that the data fails to deliver coherent results that we are forced to ask compelling questions about language and art: why do words that seem to mean one thing have the opposite effect on stage? What is the relationship between the literal and implied meanings of words in the constitution of dramatic character? In the end, this facet of our contribution to Shakespeare studies illustrates something important not just about our project specifically, but also about the digital humanities generally: the value of applying computational technology to literary texts lies not in the promise of “better data” or irrefutable “facts,” but rather in the way such technology returns us again and again to the fundamental humanist questions that help us understand how literature and art work
                    . This idea has been advanced influentially in the work of Jonathan Hope and Michael Witmore. See their blog, Wine Dark Sea 
                        http://winedarksea.org.
                    .
                
            
            
                Future Research
                In this paper we introduced novel techniques based on cognitive computing to get an understanding of characters in Shakespeare’s plays. This research explores newway of computer-assisted methods for the investigation of literature. Nonetheless, some technical issues need to be overcome in order to improve the quality of this new methodology. First, an improved methodology to outline the sentiment polarity needs to be developed. Second, the low representativity of IBM’s personality model needs to be enhanced in order to catch literary phenomena in sources such as Twitter, Wikipedia, and other such corpora. Thus, recreating a personality insight service based on a literary work corpus would surly enhance the results of our method.
            
        
        
            
                
                    Bibliography
                    
                        Bradley, A. C. (1992). 
                        Shakespearean Tragedy: Lectures on Hamlet, Othello, King Lear, Macbeth. 3rd ed. New York: St. Martinis Press.
                    
                    
                        Desmet, C. (1992). 
                        Reading Shakespeare’s Characters: Rhetoric, Ethics, and Identity. Amherst: Univ of Massachusetts Press.
                    
                    
                        Ferrucci, D. A. (2012). Introduction to ‘This is Watson’. 
                        IBM Journal of Research and Development, 
                        56(3): 1 doi:10.1147/JRD.2012.2184356.
                    
                    
                        Ferrucci, D. A., Brown, E. W., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A., Lally, A., et al. (2010). Building Watson: An Overview of the DeepQA Project. 
                        AI Magazine, 
                        31(3): 59–79.
                    
                    
                        Ford, J. K. (2005). 
                        Brands Laid Bare: Using Market Research for Evidence-Based Brand Management. Wiley https://books.google.it/books?id=0Q3OClLh6UsC.
                    
                    
                        Gill, A., Brockmann, C. and Oberlander, J. (2012). Perceptions of Alignment and Personality in Generated Dialogue. 
                        INLG 2012 Proceedings of the Seventh International Natural Language Generation Conference. Utica, IL: Association for Computational Linguistics, pp. 40–48 http://www.aclweb.org/anthology/W12-1508.
                    
                    
                        Hazlitt, W. (1845). 
                        Characters of Shakespeare’s Plays. Boston: Wiley and Putnam.
                    
                    
                        Konstantopoulos, S. (2010). An Embodied Dialogue System with Personality and Emotions. 
                        Proceedings of the 2010 Workshop on Companionable Dialogue Systems. Uppsala, Sweden: Association for Computational Linguistics, pp. 31–36 http://www.aclweb.org/anthology/W10-2706.
                    
                    
                        Kotler, P. and Armstrong, G. M. (2010). 
                        Principles of Marketing. (PRINCIPLES OF MARKETING). Prentice Hall https://books.google.it/books?id=5HkrAQAAMAAJ.
                    
                    
                        Mairesse, F. and Walker, M. (2006). Automatic Recognition of Personality in Conversation. 
                        Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers. New York City, USA: Association for Computational Linguistics, pp. 85–88 http://www.aclweb.org/anthology/N/N06/N06-2022.
                    
                    
                        Mairesse, F. and Walker, M. (2007). PERSONAGE: Personality Generation for Dialogue. 
                        Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Prague, Czech Republic: Association for Computational Linguistics, pp. 496–503 http://www.aclweb.org/anthology/P07-1063.
                    
                    
                        Nalisnick, E. T. and Baird, H. S. (2013). Character-to-character sentiment analysis in Shakespeare’s Plays.
                    
                    
                        Norman, W. T. (1963). Toward an adequate taxonomy of personality attributes: replicated factors structure in peer nomination personality ratings. 
                        J Abnorm Soc Psychol, 
                        66: 574–83.
                    
                    
                        Schwartz, S. H. (2006). Basic human values: Theory, measurement, and applications. 
                        Revue Française de Sociologie, 
                        47(4): 249–88.
                    
                    
                        Yachnin, P. and Slights, J. (2009). 
                        Shakespeare and Character: Theory, History, Performance and Theatrical Persons. Basingstoke: Palgrave Macmillan.
                    
                
            
        
    


        
            During the last ten years the use of web archives as primary sources for historical research has attracted the attention of researchers in several different fields. From the web archiving community (Foot et al., 2003; Hockx-Yu, 2014) to Internet studies scholars (Brügger, 2009; Ankerson, 2012), from web scientists (Huurdeman et al., 2013; Hale et al., 2014) to STS researchers (Rogers, 2013; Schafer, 2013), several case studies have been presented. All these different projects have been designed to highlight the potential of born digital sources to offer a new and more complete perspective on our recent past. More recently, traditionally trained and digital historians have been directly engaged in the debate, raising both methodological and theoretical questions (Milligan, 2012; Webster, 2015). However, within Digital Humanities, the web as a source and as an object of study has not been widely debated.
            For these reasons, the proposed panel brings together different researchers whose work focuses on the use of born digital materials as historical sources and who have developed, employed, criticized or refused the use of computational/quantitative methods in order to extract useful pieces of information from them. 
            The purpose of this panel is twofold. First, we aim to discuss how the toolkit available to new generations of historians will necessarily have to combine a vast series of new skills: from accessing and dealing with web archive objects to analyzing networks of hyperlinks, from employing text mining approaches to re-discussing the reliability of a primary source when it is born digital. 
            Second, our intention is to discuss with the community how this new field of study could be recognized as a relevant example of a digital humanities practice. While the number of born digital sources is increasing rapidly, a broad discussion within the Digital Humanities community is needed, in order to reflect on how to prepare the first generation of digital historians that will work with materials that do not have an analogue counterpart, of which web archives are just one.
            
                Anat Ben-David is a lecturer in the department of Sociology, Political Science and Communication, the Open University of Israel. Her research focuses on Internet geopolitics, web historiography and digital methods for Web research. Ben-David's contribution presents her recent work on the reconstruction of portions of the Web's deleted pasts. In particular, her presentation argues that the use of the Web as a primary source for studying the history of nations is conditioned by the structural ties between sovereignty and the Internet Protocol, and by a temporal proximity between live and archived websites. The argument is illustrated with an archival reconstruction of the history of the top-level domain of former Yugoslavia, .yu, which operated on the Web since 1989 and was discontinued in 2010. The archival reconstruction of a portion of the Web's deleted past serves to assess and conceptualise the Web’s limits as an appropriate source for telling its own history.
            
            
                Niels Brügger is Professor and head of the Centre for Internet Studies as well as of the internet research infrastructure NetLab within the Danish Digital Humanities Lab, Aarhus University, Denmark. His research interests are web historiography, web archiving, and media theory. Within these fields he has published monographs, edited books, and book chapters at international publishers, as well as articles in international peer reviewed journals. He has participated in a number of large research projects, in Denmark and in the UK.  Niels Brügger is cofounder and now coordinator of RESAW, a Research Infrastructure for the Study of Archived Web Materials (
                resaw.eu). 
            
            
                Meghan Dougherty is an assistant professor of digital communication at Loyola University Chicago. Her research focuses on methodological challenges in answering questions about how we move through networks of digital culture to create knowledge, form memory, and make history. The questions that guide her inquiry explore how knowledge production — ranging from identity formation and group membership to scholarly knowledge — is shaped by digital communication infrastructure, and vice versa. Dougherty’s contribution to the panel stems from her current book project under contract at University of Toronto Press, 
                Virtual Digs: Excavating, preserving, archiving, and curating the Web in which she describes a common field of Web Archaeology drawn together from experiments in Web archiving, digital preservation, and curating that are commonly found in Internet research, digital humanities, and information science.  She argues that the nature of scholarly evidence and interpretation must be reconsidered in the new media ecology.
            
            
                Ian Milligan, an assistant professor of digital and Canadian history at the University of Waterloo (Canada, Ontario) will talk on a specific web archiving project that he has been involved with. His contribution, "WebArchives.ca: Enabling Access to Canadian Political Party Web Archives," explores the development, deployment, and reception of 
                http://webarchives.ca. He argues that since 1996, we have been collecting web archives – now we need to put them to good use. However, accessing and making sense of results requires computational skills. Milligan's case study, a 2005-2015 assemblage of political parties and political interest groups within Canada, should have arguably have been used far more than it had been given pivotal shifts in the Canadian political milieu during the period it studies. These collections were underused, however, due to problems of access restrictions and a lack of technical knowledge. His contribution to this roundtable will then quickly explore various access methods, from content analysis to metadata parsing, using his case study as a reference point throughout.
            
            
                Federico Nanni is a PhD student in Science, Technology and Society at the Centre for the History of Universities and Science of the University of Bologna and a visiting researcher at the Data and Web Science Group of the University of Mannheim. His research is focused on understanding how to combine methodologies from different fields of study in order to face both the scarcity and the abundance of born digital sources related to the recent history of Italian universities. In particular, he employed oral histories and traditional hermeneutic practices for reconstructing the evolution of the University of Bologna website, which was excluded from the Wayback Machine of the Internet Archive. Later, he applied text mining methods in order to study the collected data. He finally focused his attention on comparing the changes in academic input and output of this institution during the last two decades by analysing the descriptions of the courses presented on the website and the dissertations abstracts available in the digital library.
            
            
                Jane Winters is Professor of Digital History at the Institute of Historical Research, University of London. Her research interests include the ways in which researchers in the humanities can work with born digital big data, including the archived web. She was Principal Investigator of a big data project funded by the Arts and Humanities Research Council, ‘Big UK Domain Data for the Arts and Humanities’, which sought to develop a theoretical and methodological framework for the study of web archives over time. Drawing on the lessons of this project she will discuss the challenges faced by researchers wishing to work with the archive(s) of UK web space, and suggest ways in which archiving institutions and researchers can collaborate to overcome them. She will address the fractured and diverse nature of the available web archives - from the open and comprehensive UK Government Web Archive to the dataset derived from the Internet Archive for 1996-2013, from a focused institutional collection such as the Parliamentary Web Archive to the ongoing domain crawl undertaken by the British Library since 2013 - and consider the barriers for researchers in the arts and humanities who choose to use this material, whether as a key source for study or simply as one primary source among many.
            
            Together, these six papers all point towards the growing significance of web archives within contemporary historical practice. In order to encourage the discussion, each speaker will briefly introduce his or her work (5 min), then another speaker will address a general comment (5 min) and following there will be an open discussion (5 min).
                
                     A similar format has been already successfully employed at the conference “Web Archives as Scholarly Sources: Issues, Practices and Perspectives”, Aarhus, 2015.
                
            
            The building blocks of the field are in place: what is needed is a discussion within the historical community, but also towards the broader field of digital humanities practice. Our discussion will look to find commonalities and similarities both within our work, but also within the broader DH 2016 conference.
        
        
            
                
                    Bibliography
                    
                        Ankerson, M. S. (2012). Writing web histories with an eye on the analog past. New Media and Society,
                        14(3): 384-400.
                    
                    
                        Brügger, N. (2009). Website history and the website as an object of study. New Media and Society,
                        11(1-2): 115-32.
                    
                    
                        Hale, S. A., Yasseri, T., Cowls, J., Meyer, E. T., Schroeder, R. and Margetts, H. (2014). Mapping the UK webspace: Fifteen years of british universities on the web.
                        Proceedings of the 2014 ACM conference on Web science. ACM.
                    
                    
                        Foot, K., Schneider, S. M., Dougherty, M., Xenos, M. and Larsen, E. (2003). Analyzing linking practices: Candidate sites in the 2002 US electoral Web sphere.
                        Journal of Computer-Mediated Communication,
                        8(4).
                    
                    
                        Hockx-Yu, H. (2014). Access and scholarly use of web archives.
                        Alexandria: The Journal of National and International Library and Information Issues,
                        25(1-2): 113-27.
                    
                    
                        Huurdeman, H. C., Ben-David, A. and Sammar, T. (2013). Sprint methods for web archive research.
                        Proceedings of the 5th Annual ACM Web Science Conference, ACM, pp. 182-90.
                    
                    
                        Milligan, I. (2012). Mining the “Internet Graveyard”: Rethinking the Historians’ Toolkit.
                        Journal of the Canadian Historical Association/Revue de la Société historique du Canada,
                        23(2): 21-64.
                    
                    
                        Schafer, V. and Tuy, B. (2013).
                        Dans les coulisses de l’internet: RENATER, 20 ans de Technologie, d'Enseignement et de Recherche. Armand Colin.
                    
                    
                        Rogers, R. (2013).
                        Digital methods. MIT press.
                    
                    
                        Webster, P. (2015). Will historians of the future be able to study Twitter? http://peterwebster.me/2015/03/06/future-historians-and-twitter/
                    
                
            
        
    


        
            When an earthquake struck Nepal in 2015, the band One Direction sent a tweet encouraging their fans to donate to relief efforts. This one tweet was retweeted a few times, but quickly lost in a flood of other tweets about One Direction’s tour. Simultaneously, an Indian Hindu extremist politician flooded his Twitter stream with rumors that Christian missionaries were coercing conversions from Nepalis in exchange for humanitarian aid. Additionally, an Indian religious group mixed substantial numbers of tweets about a movie they had released with tweets about their relief efforts in Nepal. These are just a few of the users who engaged with the disaster from a distance: they had different motives for tweeting about the disaster, and different levels of engagement with it. We call these users “onlookers:” they tweet about a disaster, but are not directly affected by it.
            This paper analyzes onlooker behavior: it argues that onlookers who will send a few tweets can be predicted by their interests, while onlookers who will tweet heavily about it have few, if any, shared interests. We show that onlookers who primarily tweet about entertainment topics and news topics are likely to mention the disaster, yet send few tweets about it. On the other hand, onlookers who tweet substantially about a disaster after it happens are difficult to identify before the disaster occurs because they do not share common interests aside from the disaster.
            
                Background
                Natural disasters often attract significant attention on Twitter, both by those affected and those who are distant. A substantial amount of research has explored how social media causes users to engage with political, social, and humanitarian problems; however, opinions on social media’s effectiveness—whether it causes users to donate money, stay informed, or participate in campaigns—are mixed. Some argue that displaying concern in social media is more about acquiring social capital than effecting change (Shulman, 2009; Gladwell, 2011; Morozov, 2012; Morozov, 2014), while a Pew Research Center survey finds that social media does create change (Raine et al., 2016). Additionally, many have argued that social media was important though not essential to protests in Egypt (Mazaid, 2011; Tufekci and Wilson, 2012) and other nations (Raine et al., 2016; Shirky, 2011). One analysis found that charities’ use of social media does not increase donations (Malcolm, 2016), while another finds that certain tweeting strategies do (Gasso Climent, 2015) although tweets may not raise awareness about the charity’s causes (Bravo and Hoffman-Goetz, 2015). Where all these studies concur is that social media enable a substantial amount of discourse about crises. The question we explore is how to predict how much attention Twitter users pay to crises: social media presents the opportunity for a user to send a single retweet about a disaster—as many One Direction fans did—or to sustain interest by following other users and sending many tweets about the event over a period of time.
                Additionally, there is little question that social media is useful for those directly affected by disasters. A substantial amount of research finds that social media helps first responders (Regalado et al., 2015; Dugdale et al., 2012; Omilion-Hodges and McClain, 2016; Burns, 2015; Xiao et al., 2015; Kaewkitipong et al., 2016; Meng et al., 2015; Madianou, 2015; Palen, 2008). In fact, specialized algorithms have been developed for that purpose (Pohl et al., 2013a; Pohl et al., 2013b; Platt et al., 2011). Little work examines users who tweet about disasters at a distance, however, despite the large numbers of such users. We examine these onlookers because they produce a large amount of the tweets about humanitarian crises.
            
            
                Method
                We use quantitative text analysis to identify tweets about the earthquake, to cluster onlookers based on shared interests, and to derive a correlation between onlookers’ interests and the number of tweets they sent about the earthquake. This section outlines our methods.
                To attain a broad sample of onlookers, we gathered a dataset of over 5 million tweets sent by around 15,000 users in the three weeks following the Nepal earthquake. We harvested the data from the Twitter REST API by searching for any tweets that mentioned the word “Nepal” from April 24 to May 8. We randomly selected 15,000 users from this set and harvested all of their tweets sent between April 24, 2014 and May 5, 2015. We attempted to capture only English-speaking users to increase the likelihood that we would capture users not directly affected by the earthquake, but we still captured some users who tweeted in multiple languages. This left us with roughly 11,000 onlookers.
                To determine how many tweets a user had sent about the earthquake, we trained a Naive Bayesian classifier using MALLET (McCallum, 2002) on a set of 100 onlookers’ tweets (totaling about 30,000 tweets), marking them as either quake-related or not. We applied the trained classifier to the remainder of the dataset to count each user’s quake-related tweets. Spot checking showed this technique had acceptable accuracy.
                To find shared interests, we used Latent Dirichlet Allocation (Blei et al., 2003), treating all of a user’s tweets as one document. We ran LDA with MALLET with various numbers of topics, and settled on 80. These topics represented a broad span of themes: greetings, news, entertainment, technology, plus four topics directly related to the earthquake. We then looked for connections between onlookers by building an edge list of shared topics, creating a weighted edge between two onlookers if over 25% of both onlookers’ tweets consisted of a shared topic. The edge weight was the product of their affinities to that topic. Using Gephi (Bastian et al., 2009), we then ran a weighted Louvian modularity algorithm (Blondel et al., 2008) over this onlooker graph to generate communities of users.
            
            
                Analysis
                This experiment resulted in 21 communities of onlookers being identified. The communities were labelled using the strongest topics in each. 
                
                    
                        ID
                        Label
                        Average Number of Tweets
                        Users
                        Average Quake-Related Tweets
                    
                    
                        0
                        Foreign language (Spanish)
                        419
                        882
                        9
                    
                    
                        1
                        Japanese Music
                        403
                        135
                        5
                    
                    
                        2
                        Greetings
                        326
                        199
                        5
                    
                    
                        3
                        Portuguese/Fifth Harmony
                        710
                        658
                        7
                    
                    
                        4
                        News Media
                        977
                        11
                        1
                    
                    
                        5
                        News Media
                        652
                        1312
                        22
                    
                    
                        6
                        News/Politics
                        600
                        1236
                        26
                    
                    
                        7
                        Indonesia
                        386
                        416
                        8
                    
                    
                        8
                        Foreign language (unknown)
                        495
                        312
                        5
                    
                    
                        9
                        Unclassified
                        372
                        589
                        25
                    
                    
                        10
                        Dera Sacha Sauda
                        1732
                        54
                        205
                    
                    
                        11
                        News about Russia
                        780
                        30
                        18
                    
                    
                        12
                        Shopping
                        1153
                        226
                        11
                    
                    
                        13
                        Greetings
                        476
                        1010
                        11
                    
                    
                        14
                        Greetings
                        439
                        1347
                        5
                    
                    
                        15
                        Science and animals
                        521
                        108
                        13
                    
                    
                        16
                        One Direction
                        388
                        1085
                        10
                    
                    
                        17
                        Foreign Language (Italian)
                        553
                        47
                        14
                    
                    
                        18
                        TV/Music
                        598
                        722
                        5
                    
                    
                        19
                        Music Videos
                        649
                        14
                        30
                    
                    
                        20
                        Nepal
                        393
                        748
                        125
                    
                
                After pruning out the foreign language communities in the dataset and some that were difficult to classify (Communities 0, 7-9, and 17), we can further group these onlookers into three broad interest groups: Casual Users (Communities 1-3, 12-14, 18, and 19), News and Pundits (4-6, and 13), and Engaged Users (20). We divided these subgroups based on the topics that were strongest in each, but these subgroups correlated with the number of quake-related tweets that each sent. They are described in the table below.
                
                    
                        Category
                        Proportion
                        Quake-Related Tweets/Week
                        Topic Affinities
                    
                    
                        Casual Onlookers
                        46%
                        3
                        Entertainment, greetings
                    
                    
                        News Onlookers
                        25%
                        6
                        News and politics
                    
                    
                        Engaged Onlookers
                        12%
                        10
                        Nepal
                    
                
                
                    Casual Onlookers. Onlookers in these communities showed high activity but low engagement with the disaster, sending an average total of three quake-related tweets a week. Their primary topics of discussion were entertainment, or greetings and positive sentiments. This is the largest group.
                
                
                    News Onlookers. These accounts are either the accounts of professional news outlets or amateur pundits. We find low average quake-related tweets in this group as well: users sent an average of six relevant tweets per week. News outlets generally moved from one topic to another quickly, and pundits only sustained interest in the topics that appealed to them.
                
                
                    Engaged Onlookers. This group sent the most quake-related tweets of all users; the strongest LDA topics in this group were two “Nepal earthquake” topics. However, users in this community had few other topics in common with each other.
                
                This breakdown suggests a model for predicting the number of tweets onlookers send about events. There will be roughly three categories of onlookers: Casual Onlookers, News Onlookers, and Engaged Onlookers. Casual Onlookers will consist of roughly 50% of onlookers, and will send only a few tweets over the first three weeks. Membership in this group is predicted by an interest in entertainment topics. The number of News Onlookers will be half the size of the Casual Onlookers, but they will be roughly twice as engaged. An onlookers’s affinity to this group will be predicted by a general interest in news. Finally, the Engaged Onlookers will send 10-20 times as many tweets as the Casual Onlookers, and will comprise slightly over 10% of onlookers. This group sends the most tweets about an event, but membership in this group cannot be predicted from their preexisting interests.
            
            
                Conclusion
                We find that it is easy to predict shallow engagement with a disaster on Twitter, but difficult to anticipate sustained interest. Onlookers who tweet about entertainment are likely to pass on at least a few messages about donating money because entertainers are likely to post these messages, and fans are likely to retweet them. On the other hand, onlookers who tweet more about an event are likely to have preexisting interests that intersect with a particular aspect of the disaster, but the relevant interests are hard to predict because doing so would require knowing the nature of the disaster ahead of time. For example, to know the Hindu extremist would tweet about rumors of coerced conversions in Nepal, we would have to predict a crisis that would produce such rumors. 
                Additionally, we acknowledge that our model is derived from a single case study. As such, we treat it as provisional pending further experiments. We hope to confirm this model with future work.
            
        
        
            
                
                    Bibliography
                    
                        Bastian, M., Heymann, S., Jacomy, M. et al. (2009). Gephi: an open source software for exploring and manipulating networks. http://www.aaai.org/ocs/index.php/ICWSM/09/paper/viewFile/154./1009 (accessed 22 February 2016).
                    
                    
                        Blei, D. M., Ng, A. Y. and Jordan, M. I. (2003). Latent dirichlet allocation. 
                        The Journal of Machine Learning Research, 
                        3: 993–1022 (accessed 30 June 2014).
                    
                    
                        Blondel, V. D., Guillaume, J.-L., Lambiotte, R. and Lefebvre, E. (2008). Fast unfolding of communities in large networks. 
                        Journal of Statistical Mechanics: Theory and Experiment, 
                        2008(10). doi:10.1088/1742-5468/2008/10/P10008 (accessed 17 October 2013).
                    
                    
                        Bravo, C. A. and Hoffman-Goetz, L. (2015). Tweeting About Prostate and Testicular Cancers: What Are Individuals Saying in Their Discussions About the 2013 Movember Canada Campaign? 
                        Journal of Cancer Education. 1(8). doi:10.1007/s13187-015-0838-8 (accessed 20 February 2016).
                    
                    
                        Burns, R. (2015). Digital Humanitarianism and the Geospatial Web: Emerging Modes of Mapping and the Transformation of Humanitarian Practices Thesis https://digital.lib.washington.edu:443/researchworks/handle/1773/33947 (accessed 20 February 2016).
                    
                    
                        Dugdale, J., Walle, B. Van de and Koeppinghoff, C. (2012). Social media and SMS in the haiti earthquake. ACM Press, pp. 713. doi:10.1145/2187980.2188189. http://dl.acm.org/citation.cfm?doid=2187980.2188189 (accessed 20 February 2016).
                    
                    
                        Gasso Climent, C. (2015). Twitter as a social marketing tool: modifying tweeting behavior in order to encourage donations. Info:eu-repo/semantics/bachelorThesis http://essay.utwente.nl/68039/ (accessed 20 February 2016).
                    
                    
                        Gladwell, M. (2011). 
                        Outliers: The Story of Success. Reprint edition. Back Bay Books.
                    
                    
                        Kaewkitipong, L., Chen, C. C. and Ractham, P. (2016). A community-based approach to sharing knowledge before, during, and after crisis events: A case study from Thailand. 
                        Computers in Human Behavior, 
                        54: 653–66 doi:10.1016/j.chb.2015.07.063 (accessed 20 February 2016).
                    
                    
                        Madianou, M. (2015). Digital Inequality and Second-Order Disasters: Social Media in the Typhoon Haiyan Recovery. 
                        Social Media + Society, 
                        1(2). doi:10.1177/2056305115603386 (accessed 20 February 2016).
                    
                    
                        Malcolm, K. (2016). How Social Media Affects the Annual Fund Revenues of Nonprofit Organizations. 
                        Walden Dissertations and Doctoral Studies http://scholarworks.waldenu.edu/dissertations/2005.
                    
                    
                        Mazaid, P. N. H. and A. D. and D. F. and M. H. and W. M. and M. (2011). Opening Closed Regimes: What Was the Role of Social Media During the Arab Spring?. http://ictlogy.net/bibliography/reports/projects.php?idp=2170 (accessed 15 May 2014).
                    
                    
                        McCallum, A. K. (2002). 
                        MALLET: A Machine Learning for Language Toolkit. Amherst, MA: UMass Amherst http://mallet.cs.umass.edu.
                    
                    
                        Meng, Q., Zhang, N., Zhao, X., Li, F. and Guan, X. (2015). The governance strategies for public emergencies on social media and their effects: a case study based on the microblog data. 
                        Electronic Markets, 
                        26(1): 15–29 doi:10.1007/s12525-015-0202-1 (accessed 20 February 2016).
                    
                    
                        Morozov, E. (2012). 
                        The Net Delusion: The Dark Side of Internet Freedom. Reprint edition. New York: PublicAffairs.
                    
                    
                        Morozov, E. (2014). 
                        To Save Everything, Click Here: The Folly of Technological Solutionism. First Trade Paper Edition edition. New York: PublicAffairs.
                    
                    
                        Omilion-Hodges, L. M. and McClain, K. L. (2016). University use of social media and the crisis lifecycle: Organizational messages, first information responders’ reactions, reframed messages and dissemination patterns. 
                        Computers in Human Behavior, 
                        54: 630–38 doi:10.1016/j.chb.2015.06.002 (accessed 20 February 2016).
                    
                    
                        Palen, L. (2008). Online social media in crisis events. 
                        Educause Quarterly, 
                        31(3): 76–78 (accessed 20 February 2016).
                    
                    
                        Platt, A., Hood, C. and Citrin, L. (2011). From Earthquakes to‘# morecowbell’: Identifying Sub-topics in Social Network Communications. 
                        Privacy, Security, Risk and Trust (passat), 2011 Ieee Third International Conference on and 2011 Ieee Third International Conference on Social Computing (socialcom). IEEE, pp. 541–44 http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6113164 (accessed 17 May 2014).
                    
                    
                        Pohl, D., Bouchachia, A. and Hellwagner, H. (2013a). Supporting Crisis Management via Detection of Sub-Events in Social Networks. 
                        International Journal of Information Systems for Crisis Response and Management (IJISCRAM), 
                        5(3): 20–36 (accessed 17 May 2014).
                    
                    
                        Pohl, D., Bouchachia, A. and Hellwagner, H. (2013b). Social media for crisis management: clustering approaches for sub-event detection. 
                        Multimedia Tools and Applications, pp. 1–32 (accessed 17 May 2014).
                    
                    
                        Raine, L., Purcell, K. and Smith, A. (2016). The Social Side of the Internet | Pew Research Center 
                        Pew Research Center: Numbers, Facts and Trends Shaping Your World http://www.pewinternet.org/2011/01/18/the-social-side-of-the-internet/ (accessed 21 February 2016).
                    
                    
                        Regalado, R. V. J., McHale, K., Dela Cruz, B., Garcia, J. P. F., Ma, C., Kalaw, D. F. and Lu, V. E. (2015). FILIET: An Information Extraction System for Filipino Disaster-Related Tweets. Manila, Philippines: De la Salle University. http://www.dlsu.edu.ph/conferences/dlsu_research_congress/2015/proceedings/SEE/010-HCT_Regalado_RJ.pdf (accessed 20 February 2016).
                    
                    
                        Shirky, C. (2011). Political Power of Social Media - Technology, the Public Sphere Sphere, and Political Change, The. 
                        Foreign Affairs, 
                        90: 28.
                    
                    
                        Shulman, S. W. (2009). The Case Against Mass E-mails: Perverse Incentives and Low Quality Public Participation in U.S. Federal Rulemaking. 
                        Policy & Internet, 
                        1(1): 23–53 doi:10.2202/1944-2866.1010 (accessed 21 February 2016).
                    
                    
                        Tufekci, Z. and Wilson, C. (2012). Social Media and the Decision to Participate in Political Protest: Observations From Tahrir Square. 
                        Journal of Communication, 
                        62(2): 363–79 doi:10.1111/j.1460-2466.2012.01629.x (accessed 15 May 2014).
                    
                    
                        Xiao, Y., Huang, Q. and Wu, K. (2015). Understanding social media data for disaster management. 
                        Natural Hazards, 
                        79(3): 1663–79 doi:10.1007/s11069-015-1918-0 (accessed 20 February 2016).
                    
                
            
        
    


        
            As a community of practice, digital humanists deal with data and metadata not as static artifacts, but rather as complex, multi-dimensional and multi-layered datasets that can be analyzed, annotated and manipulated in order to produce new knowledge. One of the most important challenges facing DH today is how to consolidate and repurpose available tools; how to create reusable but flexible workflows; and, ultimately, how to integrate and disseminate knowledge, instead of merely capturing it and encapsulating it. This technical and intellectual shift can be seen as the “infrastructural turn” in digital humanities (Tasovac et al. 2015).  
            Application Programming Interfaces (APIs) have the potential to be powerful, practical building blocks of digital humanities infrastructures.
                
                    
                        Infrastructures are installations and services that function as “mediating interfaces” or “structures ‘in between’ that allow things, people and signs to travel across space by means of more or less standardized paths and protocols for conversion or translation” (Badenoch and Fickers 2010, 11). Digital research infrastructures are no different: they are a mediating set of technologies for research and resource discovery, collaboration, sharing and dissemination of scholarly output. 
                    
                 On the technical level, they let heterogeneous agents dynamically access and reuse the same sets of data and standardized workflows. On the social level, they help overcome the problem of “shy data”, i.e. data you can “meet in public places but you can’t take home with you”  (Cooper 2010). Some 10 years ago, Dan Cohen started the conversation about APIs in DH by pointing out that, despite their potential, Andreas few humanities projects — in contrast to those in the sciences and commercial realms — were developing APIs for their resources and tools (Cohen 2005). In the decade since, API development in the digital humanities has certainly increased: today, both large-scale, national and international initiatives, such as HathiTrust, DPLA or Europeana, as well as individual projects, such as Canonical Text Services (CTS), Open Siddur, Folger Digital Texts, correspSearch etc., are focusing their attention and resources on developing APIs. It is now time to reflect on this development: have standards or best-practices evolved?  What workflows are most effective and efficient for creating APIs? What are the challenges or stumbling blocks for creating or using APIs? Are APIs being used by DH researchers? What is the future of API development and use in the humanities community? 
            
            This panel will cover both the theory and practice of APIs in the digital humanities today. It will bring together researchers working on major European and North American projects, who will discuss APIs from the perspectives of design, implementation, and use, as well as technical and social challenges. Each group will have 10 minutes for their statement, and 40 minutes will remain for group discussion and questions from the audience. One of the panel members will serve as the moderator. All speakers have confirmed their intention to participate in the panel. 
            
                Toma Tasovac (Belgrade Centre for Digital Humanities) will discuss an API-centric approach to designing and implementing digital editions. Starting with the notion of text-as-service and textual resources as dynamic components in a virtual knowledge space, Tasovac will show how two recent projects — 
                 Raskovnik: A Serbian Dictionary Platform and 
                Izdanak: A Platform for Digital Editions of Serbian Texts — were implemented using API-focused data modeling at the core of the project design process. The API-first approach to creating TEI-encoded digital editions оffers tangible interfaces to textual data that can be used in tailor-made workflows by humanities researchers and other users, well-suited to distant reading techniques, statistical analysis and computer-assisted semantic annotation. The “infrastructural turn” in Digital Humanities does not only have practical implications for the way we build tools and create resources, but also has theoretical ramifications for the way we distinguish highly from loosely structured data: if text is not an object, but a service; and not a static entity, but an interactive method with clearly and uniquely addressable components, a formal distinction between a dictionary and, say, a novel or a poem, is more difficult to maintain. 
            
            
                Clifford Wulfman and 
                Natalia Ermolaev (Center for Digital Humanities, Princeton) will discuss the design and implementation of Blue Mountain Springs, the API for the Blue Mountain Project’s collection of historic avant-garde periodicals.
                
                    
                         http://bluemountain.princeton.edu
                    
                  By modeling magazine data using the FRBRoo ontology and its periodical-oriented extension PRESSoo (PRESSoo, 2014), this RESTful API exposes the Blue Mountain resource in a variety of data formats (structured metadata, full-text, image, linked data). The authors will provide several examples of how Blue Mountain Springs has been used by researchers, drawing especially from the results of the hackathon they will host at Princeton in February 2016, which will bring together approximately twenty periodical studies scholars, technologists, and librarians to work with the API. Creating APIs is part of a trend in DH to move into a post-digital-library phase, when the traditional library functions of discovery and access are no longer sufficient to support research in the humanities. This trend also suggests that DH researchers must reconceptualize their own engagement with material, to think less in terms of monographs and more in terms of resources, and consequently to promulgate their work not as web sites but as web services. 
            
            
                Thibault Clérice (University of Leipzig) will discuss the design of the Canonical Text Services (CTS) and its URN scheme, which make the traditional citation system used by classicists machine-actionable (Blackwell and Smith 2014)
                
                     Traditional scholars have been citing texts the same way for centuries : for example, “Hom. Il. 1.1”, which corresponds to Homer’s Iliad, Book 1 Line 1. However, although the passage identifier does not change from one scholar to another in most cases, the abbreviation used for the author and the work title will diverge among authors, countries, and publications. 
                . The Homer Multitext (HMT) implementation of CTS requires textual data to be extracted out of its original digital representation into RDF triples in order to be served. The Perseus Digital Library (PDL) implementation, on the other hand, uses extended transformations to slice XML files into multiple records, each representing a passage at a certain level. While relational and RDF database approaches have had some success in scalability and speed (Tiepmar 2015), they also have to deal with maintenance and evolution capacity. There is a real need for this type of DH projects to scale not only in terms of data retrieval speeds, but also in terms of allowing  researchers to correct and enhance their data. In addition, projects need to be able to propose other narratives: sliced data doesn’t easily provide access to the full data model. Clérice will discuss why and how, using both a native XML-based system such as eXist and a Python-based implementation, one can achieve scalability while guaranteeing maintenance and evolution.
            
            
                Adrien Barbaresi from the Austrian Academy of Sciences (ICLTT) will discuss the use of APIs in building resources for linguistic studies. The first case deals with lesser-known social networks (Barbaresi 2013) while the second tackles the role of the Twitter API in building the ICLTT’s "tweets made in Austria" corpus
                
                     http://www.oeaw.ac.at/icltt/node/193
                . For computational linguists, short messages published on social networks constitute a "frontier" area due to their dissimilarity with existing corpora (Lui & Baldwin 2014), most notably with reference corpora of written language. Since data are mainly accessed and collected through APIs and not in the form of web pages, Barbaresi argues that social networks are a frontier area for (web) corpus construction. He will point out the challenges of using Twitter’s API, for example how to reveal the implicit decisions and methodology used by API designers, as well as concrete implementation issues, such as the assessment and optimization of data returned by the API. Free APIs may come at no cost, but they also offer no guarantee, so that the use of commercial APIs for research purposes has to be seen with a critical eye in order to turn a data collection process into a proper corpus.
            
            Finally, 
                Jennifer Edmond and 
                Vicky Garnett (Trinity College Dublin), will provide reflections on the place of APIs within European research infrastructures for the humanities. Their contribution to the panel builds on their recent study on the Europeana Cloud project, which found that while access to data is a real and growing area of interest, very few humanities researchers seem to actively and directly use APIs.
                
                     This is a result of the confluence of several factors: that the research data most humanists want to access is seldom available via an API; that many sources that do offer relevant data lack the structure or metadata to make the API useful; and that humanistic researcher generally lack the skill set to experiment directly with APIs.  At the same time, however, one of the key desires expressed by researchers in the current landscape is the federation of high-quality data (Edmond and Garnett 2015). 
                 They will describe two initiatives, one technical, one social, aiming to better harness the potential of the API to meet researcher’s implicit needs. The first is the Collaborative European Digital Archival Research Infrastructure (CENDARI) project, whose platform is structured around an internal API that will allow multiple data sources (local repository, triple store, metasearch engine) to be aligned, enhanced and then served out to a number of environments and tools, including the project’s native note-taking environment. The second example is the genesis and development of the concept of the ‘inside-out’ archive. This framework, which has arisen out of a collaborative venture between several European humanities research infrastructure projects, seeks to encourage collection holding institutions to look beyond their own digitization programs and platforms and recognize the rising importance of machines-as-users (requiring specific access points and formats) rather than the somewhat outdated model of individual institutional web presence serving individual human resource seekers.
            
            The five speakers on this panel will address some of the most pressing issues related to the ongoing development and future of APIs on the DH research infrastructure landscape. The discussion will cover both micro- and macro levels, ranging from methodological implications and technical scalability to the ways in which API-based data access to collections challenges traditional norms of institutional identity and independence. As such, the panel will offer a timely platform for a multifaceted debate on the potentials and pitfalls of building and using APIs in the digital humanities. 
        
        
            
                
                    Bibliography
                    
                        Badenoch, A. and Fickers A. (2010). Europe Materializing? Toward a Transnational History of European Infrastructures. In Badenoch, A. and Fickers A. (eds.), Materializing Europe: Transnational Infrastructures and the Project of Europe, 1-26. Basingstoke, Hampshire; New York: Palgrave Macmillan.
                    
                    
                        Barbaresi, A. (2013). Crawling microblogging services to gather language-classified URLs. Workflow and case study. In Proceedings of the 51th Annual Meeting of the ACL, Student Research Workshop, pages 9-15.
                    
                    
                        Blackwell, C. and Smith N. (2014). 
                        Canonical Text Services Protocol Specification. http://folio.furman.edu/projects/citedocs/cts/. Accessed October 23, 2015.
                    
                    
                        Cohen, D. Do APIs Have a Place in the Digital Humanities. http://www.dancohen.org/blog/posts/do_apis_have_a_place_in_the_digital_humanities. Accessed October 24, 2015.
                    
                    
                        Cohen, D. (2006). “From Babel to Knowledge: Data Mining Large Digital Collections.” 
                        D-Lib Magazine 12, no. 3.
                    
                    
                        Cooper, D. (2010). When Nice People Won’t Share: Shy Data, Web APIs, and Beyond, 
                        Second International Conference on Global Interoperability for Language Resources (ICGL 2010), n. pag.
                    
                    
                        Edmond, J, Bulatovic N. and O’Connor, A. “The Taste of ‘Data Soup’ and the Creation of a Pipeline for Transnational Historical Research.” Journal of the Japanese Association for Digital Humanities 1, no. 1 (2015): 107–22.
                    
                    
                        Edmond, J. and Garnett V. (2015). APIs and Researchers: The Emperor’s New Clothes, International Journal of Digital Curation, 10(1): 287-97.
                    
                    
                        LeBeuf, Patrick (ed.). PRESSoo. Extension of CIDOC CRM and FRBROO for the modelling of bibliographic information pertaining to continuing resources. Version 0.5, 
                        http://www.ifla.org/files/assets/cataloguing/frbr/pressoo_v0.5.pdf. Accessed, November 1, 2015. 
                    
                    
                        Murdock, J. and Allen C. (2011). InPhO for All: Why APIs Matter, Journal of the Chicago Colloquium on Digital Humanities and Computer Science 1(3): 
                        http://www.jamram.net/docs/jdhcs11-paper.pdf. Accessed, October 23, 2015. 
                    
                    
                        Tasovac, T. Rudan S. and Rudan S. (2015). Developing Morpho-SLaWS: An API for the Morphosyntactic Annotation of the Serbian Language. Systems and Frameworks for Computational Morphology, 137-47. Heidelberg: Springer.
                        
                        Tiepmar, J. (2015). Release of the MySQL based implementation of the CTS protocol. In Bański, P., Biber H., Breiteneder E., Kupietz M., Lüngen H. and Witt A. (eds.), Proceedings of the 3rd Workshop on Challenges in the Management of Large Corpora (CMLC-3), 35-43. Mannheim: Institut für Deutsche Sprache.
                    
                
            
        
    


        
            The Web is now deeply integrated into contemporary culture, and scholars interested in current phenomenon cannot afford to ignore it, however, collecting data from the web is not easy. The web is based on a mix of continually changing technical standards which make creating an archival copy of a web site for scholarly reference very difficult. Without such a copy there is no way for future scholars to question the interpretations we make today or reinterpret the phenomenon in light of new evidence tomorrow.
            Researchers and organizations, such as the Internet Archive, are attempting to preserve portions of the web for future retrieval, but much of the web disappears quickly. A 2014 study of web links in scholarly papers found that 1 out 5 scholarly papers contains links to web URLs which no longer function or no longer exist (Klein et al., 2014). The need for humanists to recognize the value of web archives to historical research is especially acute. Researchers cannot engage recent cultural histories and ignore the culture of the web (Milligan, 2012).
            The challenge of web archiving is especially acute for rapidly changing stories which track specific events. Discussions about controversial topics, such as GamerGate,
                
                     This paper is the result of a larger project investigating the discourse surrounding GamerGate, an internet controversy about feminism and gaming, which grew dramatically in 2014. The paper presents some of the methods used by our research group to study GamerGate. For a brief non-academic explanation of GamerGate see Hathaway, 2014
                 take place across multiple websites, use multiple forms of media, and occur in very different discourse communities. Underneath the different social worlds gathered around online communities there is an incredibly diverse set of technological platforms which require customized strategies for tracking and collecting data. This paper will:
            
            
                Describe the key challenges for researchers wishing to collect just-in-time archives of web based cultural phenomenon.
                Put current challenges in an historical context of differing goals between web developers and web archivists.
                Propose some social and technical solutions to improve the situation, and
                Introduce a set of tools to help researchers engaged in these areas.
            
            
                Challenges
                Dynamic changes in online content present one of the unique challenges for gathering contemporary web discourse. Most internet users are familiar with the constantly updating nature of Twitter and other social media platforms. Social media platforms present a challenge for web archivists because of their technological structure and commercial ownership. The speed of updates on social media requires specialized tools to download, especially in large quantities. A researcher needs deep technological knowledge of these tools and the application programming interface (API) provided by the website in order to build a reliable and useful corpus. On the legal side, the terms of service affect the types of information that can be gathered by researchers and how that data can be analyzed or shared with other researchers.
                Other commercial sites, such as news media web sites, often host comment threads where internet users can post their opinions on the topics covered in the main story. It is relatively trivial to download the main content of a news story posted on the web but collecting the comment stream may present a challenge because the comments may be hosted by another website service or may be displayed dynamically as a user scrolls further through a web page. In such cases the default web archiving tools may not be sufficient. Web discussion forums present yet another technical challenge.
                Researchers often frame their questions about web phenomena by describing a topic that they wish to study. But the architecture of the web is built around the key idea of a web site, a particular set of files which may include many different types of media including text, images, and video, and is hosted by a particular business, institution, or individual. These web sites are identified by the Uniform Resource Locators people type into their browsers in order to navigate to a web page. 
                The tools used to archive the web are built on this technical background for dealing with URLs, APIs, REST, RSS, and other interfaces which human beings do not usually interact with. In the language of web archive software the unit of research is the seed, or base URL, from which data can be harvested. For the researcher the unit of work is the topic. Negotiating between these two conceptions of how online research should work is a major social challenge for any type of internet research. Researchers and web users just want to see the content, but automating the collection of that content means reproducing a complicated software experience which has gradually been built by web developers and web browsers over the past 25 years.
                Humanities researchers have traditionally relied on stable or slowly changing content. Efforts by humanities scholars have been made to adapt to the changes in digital content represented by the web. Some universities have set up web labs for collecting and analyzing web data. One key task of these labs has been building subcollections from the overall web in order to further the study of particular topics (Arms et al., 2009). One of the key insights from our work is the need to continue building strong collaborations between multiple fields. Libraries and the Internet Archive need input from digital humanists in order to understand their research questions and digital humanists need to understand the technological challenges of web archiving in order to collectively design systems which will help future researchers. The web, however, is constantly changing at multiple levels, ranging from the technology used to deliver content, the processes of creating content, commenting on content, and the distribution of information. Archiving the web for humanities research calls for changing the conceptual image of stable sources, collaborating with new communities, and adopting new technologies.
            
            
                Solutions
                The implication of the technological treadmill described above is that it becomes more and more difficult for a single researcher to adequately collect the web. There are two potential solutions to this problem: technological and social.
                Computer scientists are working to build better web archive software which can integrate with social media in order to reduce the amount of administrative overhead needed to collect information on particular topics.
                    
                         Some of these research groups are located at the 
                            Center for the Study of Digital Libraries at Texas A and M; 
                            Web Science and Digital Libraries Research Group at Old Dominion; and the 
                            Digital Library Research Laboratory at Virginia Tech.
                        
                     These tools will automate the selection of web sites to be archived, removing some of the human intervention needed to curate web materials. But simplifying the data gathering process today may make future explanations of the context of a collection more technical. For now researchers are dependent upon a mix of tools, often customized for specific uses, and mixing open source and commercial software.
                
                In our research project we used a combination of open source tools, subscription services, and customized API calls. For gathering data from Twitter we used a program called twarc.
                    
                         Github repository at https://github.com/edsu/twarc
                     Customizations were made to improve the performance of the tool for our uses, which was tracking specific hashtags. The Twitter scraper was initially installed on a laptop belonging to a member of the research team, but when the number of tweets became too large for a laptop the program was moved to a cloud server provided by Compute Canada. The data from Twitter was stored in JSON and then transformed using standard libraries into files which could be analyzed for most frequent Twitter posters, most frequent URLs, and most frequent hashtags. Data from web pages was collected using the Archive-IT subscription service
                    
                         Web site https://archive-it.org/
                     provided by the Internet Archive and the wget
                    
                         Web site https://www.gnu.org/software/wget/
                     command line tool. Some specific websites, such as 4chan and 8chan, required the development of custom API interfaces to download material from relevant chat boards. Additional programs to download comments from YouTube are currently being tested. We plan to document our recipes for using these different tools on methodi.ca,
                    
                         Web site http://methodi.ca/
                     the methods commons for text analysis.
                
                Archiving the web involves many different institutions and disciplines. The largest players are the Internet Archive and various national libraries; the Internet Archive operates as a non-profit and has the most comprehensive collection of digital materials from the web. Unfortunately, the Internet Archive collections are not primarily built for researcher access and can be especially difficult to work with if you are investigating topics which cover multiple URLs or lengthy time periods. Any research project using their collections requires significant human labor. Libraries and museums can step in to fill some of the gaps by using services such as Archive-IT, which provides more curatorial control over the collection development process and also has a more robust search interface. In order to improve these tools, humanists will need to build connections with other disciplines, such as information and library science, computer science, and archival studies. Only by working together and extending our disciplinary horizons can we build the collections which current and future digital humanists can use to study our current era.
                One final social issue of importance are the legal and ethical implications of gathering large amounts of data from the web. We will not discuss these issues in great depth in this paper but they do need to be acknowledged because they constrain some of the actions which can be taken in web data gathering. In our project on GamerGate we have looked closely at the ethical implications of sharing data gathered from social media. The dataset we shared online includes an appendix on ethical issues related to data sharing.
                    
                         Rockwell, G., Suomela, T., 2015, "Gamergate Reactions", http://dx.doi.org/10.7939/DVN/10253 V5 [Version]
                    
                
            
        
        
            
                
                    Bibliography
                    
                        Arms, W.Y., Calimlim, M. and Walle, L. (2009). EScience in Practice: Lessons from the Cornell Web Lab. 
                        D-Lib Magazine, 15(5/6). doi:10.1045/may2009-arms
                    
                    
                        Hathaway, J. (2014). What is GamerGate, and Why? An Explainer for Non-Geeks. 
                        Gawker. http://gawker.com/what-is-gamergate-and-why-an-explainer-for-non-geeks-1642909080
                    
                    
                        Klein, M., Van de Sompel, H., Sanderson, R., Shankar, H., Balakireva, L., Zhou, K. and Tobin, R. (2014). Scholarly Context Not Found: One in Five Articles Suffers from Reference Rot. 
                        PLoS ONE, 9(12): doi:10.1371/journal.pone.0115253
                    
                    
                        Milligan, I. (2012). Mining the “Internet Graveyard”: Rethinking the Historians’ Toolkit. 
                        Journal of the Canadian Historical Association, 23(2): 21-64. doi:10.7202/1015788ar
                    
                
            
        
    


        
            The nineteenth century provides a perfect setting for a digital humanities class as a result of the similarities between the industrial and digital revolutions and between the proliferation of print and periodicals and the rise of blogs and Twitter. Arthur Conan Doyle’s Sherlock Holmes stories are likewise the perfect subject matter: most of Holmes’s cases revolve around the technology of the day, from cabinet photographs in “A Scandal in Bohemia” to the typewriter in “A Case of Identity.” These connections enable students to learn about an earlier time period and literature while also historicizing their own technological moment. The resurgence in popularity of Holmes adaptations in recent years—Sherlock (BBC), Elementary, and Mr. Holmes, to name just a few—emphasizes these connections and also brings students into the classroom. 
            “Digital Tools for the 21st Century: Sherlock Holmes’s London” (taught from the Fall of 2014 through the Fall of 2015) is an introductory digital humanities class that uses Holmes stories as a corpus on which to practice a wide variety of basic digital humanities methodologies and tools, including visualizations, digital archives and editions, mapping, and distant reading. “Digital Tools” unites theory and practice with a tripartite structure for each unit, which I have dubbed “Read, Play, Build.” First, students read articles from books and blog posts about the pros and cons of each methodological approach. They then examine current projects to discuss the ways that each enhances scholarly fields and poses new research questions. Each unit concludes with an in-class lab component, in which they build a small project using a well-known tool. For example, in the archives and editions unit, students examine Jerome McGann’s “Radiant Textuality,” discuss the importance of preservation, access, and challenging the canon, examine The Rossetti Archive (and many others), and use the tool Juxta Editions to create their own, fully transcribed digital edition of a Sherlock Holmes story of their choice, using page images of the original printing in The Strand Magazine to learn about XML, basic bibliography, and best editorial practices. Likewise, in the unit on distant reading and topic modeling, they read Franco Moretti’s “The Slaughterhouse of Literature” and Ted Underwood’s blog posts on topic modeling, examine Mining the Dispatch to see that methodology in action, and topic model all 56 Sherlock Holmes short stories and analyze the trends in a blog post. 
            Although this class uses Holmes stories as base texts, it also situates these stories in their historical and cultural context by examining Victorian digital humanities projects from fields other than English. Students explore the Proceedings of the Old Bailey, a searchable archive of all court records in the Old Bailey from 1674-1913, to learn about crime, class, and the legal system. They also examine the David Livingstone 1871 Field Diary to learn about empire and Charles Booth’s poverty maps from 1898-1899 from the Charles Booth Online Archive to learn about socioeconomic conditions in London and to compare Conan Doyle’s fictional London to the actual city. 
            This paper will present the class in greater depth and will provide examples of the digital projects students collaboratively created—from contributing to the marginalia project Book Traces to making digital narrative maps of Holmes stories with Mapbox—in order to provide new models for student scholarship and their role in the future of the English departments and the humanities. It will also discuss the importance of the Holmes stories as the corpus for the class, as the character of Holmes himself provides a useful model for a digital humanist, especially when students may be unused to thinking about data in a humanities context, as he combines data science with humanities skills of close reading, archiving, and a love of literature and music.
            Teaching Holmes with digital tools lets students build on the traditional humanities skills of close reading, understanding patterns, and using archives, and augments that scholarly toolkit by guiding students to a better understanding of rhetorical patterns and spatial significance, while also teaching them about collaboration, interdisciplinarity, and public humanities. In accordance with its public humanities focus, the course’s materials, including the syllabus and assignments, are publicly available on the class’s website (https://hawksites.newpaltz.edu/dhm293/). By melding research with project-based learning, this course enables students to engage with research and Victorian history more actively than is common at the introductory level.
        
    


        
          
          Digital Scholarship Training Programme 2012-2015
            Research libraries and cultural heritage institutions must be able to adapt to a changing research landscape and invest in the development of staff skills and core competencies to match if they are to continue to effectively support and engage with modern scholars (Adams, 2013). The Digital Research team at British Library, which includes the BL Labs project and the Digital Curator team, engages with those operating at the intersection of academic research, cultural heritage and technology to enable innovative use of our digital collections, and creates opportunities for library staff to develop skills necessary to support emerging areas of modern scholarship, particularly the Digital Humanities (DH).
            This paper presents the final report on the pilot Digital Scholarship Training Programme delivered to staff at British Library between 2012-2015. It provides an evaluation of the skill-building initiative, incorporating preliminary findings from research into the current trends and international developments in the field of DH that will inform the next phase of staff training.
            In 2012, the Digital Curator team embarked on a plan to design and deliver a bespoke training programme for staff (McGregor et. al, 2013). Four objectives were set to guide and ultimately measure the success of the programme:
            
                Staff across all collection areas are familiar and conversant with the foundational concepts, methods and tools of digital scholarship.
                Staff are empowered to innovate.
                Collaborative digital initiatives flourish across subject areas within the Library as well as externally.
                Our internal capacity for training and skill-sharing in digital scholarship are a shared responsibility across the Library.
            
            In consultation with experts from within the Library and institutions on the leading edge of digital scholarship we designed and delivered in-house a catalogue of 19 one day courses suited to building the digital skills of information professionals in the research library and cultural heritage sector. Though much of the programme was rooted in and inspired by the field of digital humanities, the wider umbrella of Digital Scholarship was retained to future-proof the programme and to envelope developments in related fields like social and computer sciences. The following titles represent a cross-section of courses created:
            
                Behind the Screen: The Basics of the Web
                Crowdsourcing in Cultural Heritage (Ridge, 2015)
                Georeferencing and Digital Mapping
                Social Media: An Introduction to Blogging and Twitter
                Working with Digital Objects: From Images to A/V
                Information Integration: Mash-ups, APIs and the Semantic Web (Stephens, 2014a)
                Cleaning up Data (Stephens, 2014b)
            
            The four-member Digital Curator team oversaw the running of 88 hands-on courses (or roughly 30 a year) between November 2012 and September 2015. Courses were delivered by a mix of internal and external trainers. Over 400 individual staff members came through the programme, on average attending two or more courses each.
            Throughout the pilot we collected feedback formally via post-course evaluation forms, and informally through avenues such as a weekly Digital Research Clinic, and personal conversations that arose in the course of our daily work. Colleagues were asked to provide comments to help us improve the course, including what they enjoyed most out of the day, what they anticipated using in their work, and what was not clearly articulated. 
            Hands-on practical exercises were cited most often as the most enjoyable element, though not to the exclusion of the lecture and discussion time which participants felt provided necessary context. While many attendees cited specific technologies such as Open Refine as something they anticipated using in their work, they also tended to comment that having the technology underpinning innovative digital research projects demystified was helpful inspiration for future projects. Topics which could have been more clearly articulated centred on a lack of clarity on practical steps for turning aspiration into application. 
            Looking specifically at the objectives set, there is ample evidence to support the continuation of the training programme, such as the incorporation of the programme in staff induction for established Library projects such as Qatar Digital Library. As staff across all collection areas have become more familiar and conversant with the foundational concepts, methods and tools of digital scholarship, we have witnessed its profile increase across the Library. For instance three new PhD placements were offered this year specifically within the digital research domain (Sheperd, 2016) for the first time.
            Staff have felt empowered to innovate, and collaborative digital initiatives have flourished. A particularly cogent example is that of curator Dr. Sandra Tuppen, who attended one of our courses on cleaning up data and went on to secure a £79,000 grant towards a research project which enriched and cleaned British Library catalogue data so that it could be successfully aligned with other printed music datasets in support of a big data approach to the history of music (Tuppen, 2014). 
            Our internal capacity for training and skill-sharing in digital scholarship has become a shared responsibility across the Library, with internal course instructors now outnumbering external instructors. With the increasing number of Library staff being involved in projects and other programmes that include digital research activities and methodologies, we have been able to integrate more in-house expertise to the courses offered. Additionally the Digital Curator team has prioritised our own upskilling through a monthly informal “Hack & Yack” where we work through online tutorials with a view towards incorporating them into training, as well as more formal courses such as Train the Trainer aimed at enhancing our teaching strategies, combining theory and practical methodologies in the planning and delivery of the courses. 
          
          
          Looking to the future 
            We will continue the Digital Scholarship Training Programme and for 2016/2017 will maintain 7 of the 19 courses in their current form:
            1. 101 This is Digital Scholarship
            2. 103 Digitisation at the British Library
            3. 105 Crowdsourcing in Libraries, Museums and Cultural Heritage Institutions 
            4. 108 Geo-referencing and Digital Mapping
            5. 109 Information Integration: Mash-ups, API’s and Linked Data
            6. 116 Metadata for Electronic Resources
            7. 118 Cleaning up Data
            Focusing on delivering this smaller core of courses will free up resource to improve upon how we:
            
                Reach staff who are keen and could most make use of the information but have not yet engaged
                Providing guidance and support to staff who are looking to implement what they have learned
                Addressing more explicitly the challenges and opportunities for working with complex collection materials, such as with non-Western materials 
            
            We aim to provide a more diverse training offering to ensure that there are sufficient opportunities for staff in a variety of roles at the Library to engage with digital research. Often colleagues have said they would like to attend a course, but either their workload is such that they feel they cannot spare a full day for it or they work a rota, as is the case with our colleagues who staff reading rooms. Finding creative ways to articulate more clearly and succinctly the practical value of time spent on a course, for example through shorter more frequent taster sessions explaining how a digital tool or method might help solve a specific problem, may help to reach those who have yet to engage. We are also working in partnership with reading room staff on rota to develop new approaches for conveying the training (perhaps through short informational videos).
            On the opposite end of the spectrum is the need to support increasing numbers of library staff who have engaged with the programme and are now looking to implement what they have learned. In an ideal world we could offer 'just in time' training to colleagues at the point of their immediate need. However, as Miriam Posner (2012) and others have discussed, each question a colleague asks may bring with it a hidden overhead of time taken to respond well. In some cases we may seek to hire existing trainers to support specific project needs but more practically, we will look to better promote and leverage our weekly Digital Research Clinic, a drop-in service for staff to get guidance on any aspect of digital research. A collection of practical 'Getting Started' guides will be further developed and shared via an internal wiki. Additionally, Digital Curators sit on key infrastructure development projects so as to directly inform the development of these in support of digital research. 
            Finally, digital scholarship is a complex and global affair, as evidenced by the rapid expansion of DH centres around the world. Our courses to date have tended to deal with relatively simple forms of digitised material such as digitised printed English language books. However for colleagues working with non-Western texts for instance, knowledge of cutting edge developments in transcription and Optical Character Recognition would be highly beneficial in helping ensure these materials can be leveraged by digital scholars. Our collections are as global and diverse as the DH communities interests worldwide, and future staff training provision must more accurately address the complexities and opportunities of working with our vast non-Western materials.
          
        
        
            
                
                    Bibliography
                    
                        McGregor, N. and Farquhar, A. (2013). The Digital Scholarship Training Programme at British Library, 
                        Abstracts | Digital Humanities 2013. http://dh2013.unl.edu/abstracts/ab-264.html (accessed 16 February 2016).
                    
                    
                        Posner, M. (2012). What are some challenges to doing DH in the library? 
                        Miriam Posner’s Blog. http://miriamposner.com/blog/what-are-some-challenges-to-doing-dh-in-the-library/ (accessed 29 February 2016).
                    
                    
                        Ridge, M. (2015). Resources for “Crowdsourcing in Libraries, Museums and Cultural Heritage Institutions”, 
                        Mia Ridge’s Blog. 
                        http://www.miaridge.com/resources-for-crowdsourcing-in-libraries-museums-and-cultural-heritage-institutions/ (accessed 16 February 2016).
                    
                    
                        Shepard, J. (2016). PhD placements in Digital Scholarship British Library, 
                        Digital Scholarship. http://britishlibrary.typepad.co.uk/digital-scholarship/2016/02/phd-placements-in-digital-scholarship.html (accessed 16 February 2016).
                    
                    
                        Stephens, O. (2014). Information Integration: Mash-ups, APIs and the Semantic Web | Overdue Ideas, 
                        Information Integration: Mash-Ups, APIs and the Semantic Web. http://www.meanboyfriend.com/overdue_ideas/2014/10/information-integration-mash-ups-apis-and-the-semantic-web/ (accessed 16 February 2016).
                    
                    
                        Stephens, O. (2014). Working with Data using OpenRefine | Overdue Ideas, 
                        Working with Data Using OpenRefine. http://www.meanboyfriend.com/overdue_ideas/2014/11/working-with-data-using-openrefine/ (accessed 16 February 2016). 
                    
                    
                        Tuppen, S. (2014). A Big Data History of Music, 
                        British Library Music Blog. http://britishlibrary.typepad.co.uk/music/2014/04/a-big-data-history-of-music.html (accessed 3 March 2016).
                    
                
            
        
    


        
            Understanding users’ online behaviour is of growing interest to academic researchers in a variety of fields. Traditionally, in the marketing domain commercial research companies map consumer behaviour to understand when and where customers decide to buy products. For this purpose, web metrics of individual websites serve as detailed source of information on when, how and at which section a user enters a website. Recently this type of data is also being used by cultural heritage institutes to understand the interest of their visitors (De Haan and Adolfsen, 2008), to track where their digital content is being reused (Navarrete Hernández, 2014) or to understand the query’s users perform in search systems by analysing the log files (Batista and Silva, 2002; Huurnink, 2010). In this type of research, the website is the central research object providing traces that Menchen-Trevino (2013) calls ‘Horizontal Data sets’. These contain data that are ‘organized around a specific type of trace, for example search terms, web browsing log files, tweets, hashtags, likes or friend and follower ties’ (Menchen-Trevino, 2013: 331). An advantage of using this type of data is that they are not obtrusive to the respondents since they are created automatically as users are surfing the web. However, this also leads to an ethical disadvantage since users are not aware that their online behaviour is being examined, nor could they give their consent to have their data being analysed. While Horizontal Data Sets are organized around one type of trace, Vertical Data Sets are organized around research participants that deliberately ‘give permission for researchers to collect their digital traces’ (Menchen-Trevino, 2013: 331). 
            
            Since mid-1990s, commercial research agencies have started to collect these types of vertical data by building tools and panels of respondents whose online behaviour is monitored 24/7 to provide data on usage across media and purchase behaviour (Coffey, 2001; Napoli, 2010; Taneja and Mamoria, 2012). Similar to television viewing rates, these lists are mainly created to gain more insight in the background of website visitors in order to provide potential advertisers with information on how to reach their online target audience in the best possible manner. Obviously these commercial research data contain very rich information, also for academics who are interested in collecting real-world Web use data. However, apart from lists of the most popular domains that are published as open data by companies such as Alexa and Similarweb
                
                    
                        http://www.alexa.com/topsites , 
                        http://www.similarweb.com/global
                    
                , data containing information about visits to each individual page and information about the background of the panel is not available. Main arguments of commercial agencies to not collaborate with scholars is to ensure the confidentiality of their respondents’ identity and to prevent scholars to gain insight into the techniques applied by the companies. 
            
            Nevertheless, researchers in a variety of disciplines are interested in tracking online behaviour in a real-world situation. Especially in the communications science realm, scholars experimented with several techniques of tracking people’s online behaviour (Ebersole, 2000; Tewksbury, 2003; Findahl et al., 2013; Findahl, 2009; Munson et al., 2013; Damme et al., 2015; Menchen-Trevino and Karr, 2012). Striking about these pioneering monitoring studies is its multi-method approach. By default each does not only monitor web use but also compares its outcome with either survey, diary or interview data. By triangulating the results, these researchers try to overcome the critique on classic studies on media consumption that often deploy either surveys or diaries registering self-reported media behaviour (e.g. Van Cauwenberge, d’Haenens and Beentjes, 2010; Schrøder and Kobbernagel, 2010; Taneja et al., 2012; Reuters Institute for the Study of Journalism, 2015). These methods strongly rely on the memory of the participants while several scholars found respondents often overestimate their media use (Ebersole, 2000; Prior, 2009; Robinson, 1985). Furthermore, since filling in diaries and surveys on news consumption is very labour-intensive, its outcomes mainly focus on 
                when media have been consumed or on 
                which devices, while it remains unclear 
                what has been consumed. One way to gain insight in the consumed news content is focus on metrics of individual news organisations (Batista and Silva, 2002; Boczkowski et al., 2011; Lee et al., 2012; Usher, 2013) or the most clicked items (e.g. Boczkowski et al., 2011; Karlsson and Clerwall, 2013; Lee et al., 2012; Nederlandse Nieuwsmonitor, 2013). However, given the focus of these studies on individual websites or most-clicked articles it remains unknown which genres of news websites constitute users’ 24/7 news menu. Do they e.g. visit news about sports mainly in the morning and news on politics mainly during the evening? Taneja at el. (2012) tried to overcome this problem by literally following 495 users throughout an entire day. However, even with this labour-intensive fieldwork it proved not to be possible to incorporate the genres of consumed news items. 
            
            Web monitoring tools such as the above mentioned examples, now offer a less labour-intensive and more precise way of registering digitally consumed news items. By deploying these techniques, we could overcome the knowledge gap of the 24/7 news consumption menu. Therefore, we created 
                the Newstracker, a custom built system that collects web activities of specified and authenticated users, cleans the data by removing non-relevant data, extracts the associated content and stores this as a new dataset to be used for analysis. While most existing online tracking studies mainly report the visited websites, our set-up goes two steps further. We did not only monitor the website titles but also the actual visited URLs and crawled all textual and visual contents of the visited websites. Since one of the problems when monitoring a person’s online behaviour is the magnitude of the data that is being collected (Batista and Silva, 2002; Manovich, 2012; Vicente-Marino, 2013: 43), we deployed automated content analyses techniques (Atteveldt, 2008; et al., 2012) to detect the topics that are being discussed in the news items. This enabled us to calculate the topical online news consumption during the day.
            
            In this paper we will describe the set-up of ‘The Newstracker’ in a study on the online news consumption of a group of young Dutch news users and its applicability for other types of Digital Humanities research such as user studies focussing on formulating requirements based on existing user behaviour. We will demonstrate the workflow of the Newstracker and how we designed the data collection and pre-processing phase (see figure 1).
            
                
                Figure 1. Workflow of the Newstracker application, illustrating the two main phases: Data Collection and Pre-processing. The latter consists of three stages: cleaning, content extraction and merging
            
            By reflecting on the technical, methodological and analytical challenges we encountered, we will illustrate the potential of online monitoring tools such as the Newstracker. We will end our paper with discussing its limitations by stressing the need for a multimethod study design when aiming not only to register but also to understand online user behaviour.
        
        
            
                
                    Bibliography
                    
                        Batista, P. and Silva, M. (2002). Mining Web Access Logs of an On-line Newspape. Proceedings of the 2nd International Conference on Adaptive Hypermedia and Adaptive Web Based Systems http://xldb.di.fc.ul.pt/xldb/publications/rpec02.pdf.
                    
                    
                        Bhulai, S., Kampstra, P., Kooiman, L., Koole, G. and Kok, B. (2012). Trend visualization on Twitter: What’s hot and what’s not?. IARIA. (Data Analytics). Barcelona, pp. 43–48.
                    
                    
                        Boczkowski, P. J., Mitchelstein, E. and Walter, M. (2011). Convergence Across Divergence: Understanding the Gap in the Online News Choices of Journalists and Consumers in Western Europe and Latin America. Communication Research, 
                        38(3): 376–96 doi:10.1177/0093650210384989.
                    
                    
                        Coffey, S. (2001). Internet Audience Measurement: A Practicioner’s View. Journal of Interactive Advertising, 
                        1(2): 10–17.
                    
                    
                        Damme, K. V., Courtois, C., Verbrugge, K. and Marez, L. D. (2015). What’s APPening to news? A mixed-method audience-centred study on mobile news consumption. Mobile Media & Communication, 
                        3(2): 196–213 doi:10.1177/2050157914557691.
                    
                    
                        De Haan, J. and Adolfsen, A. (2008). De Virtuele Cultuurbezoeker. Den Haag: Sociaal en Cultureel Planbureau http://www.scp.nl/dsresource?objectid=19697&.
                    
                    
                        Ebersole, S. (2000). Uses and Gratifications of the Web among Students. Journal of Computer-Mediated Communication, 
                        6(1): 0–0 doi:10.1111/j.1083-6101.2000.tb00111.x.
                    
                    
                        Findahl, O. (2009). The Swedes and the Internet 2009. Gävle: World Internet Institute.
                    
                    
                        Findahl, O., Lagerstedt, C. and Aurelius, A. (2013). Triangulation as a way to validate and deepen the knowledge about user behavior. A comparison between questionnaires, diaries and traffic measurement. Audience Research Methodologies: Between Innovation and Consolidation. New York, pp. 54–69.
                    
                    
                        Huurnink, B. (2010). Search in Audiovisual Boradcast Archives Amsterdam: University of Amsterdam http://dare.uva.nl/document/2/83234.
                    
                    
                        Karlsson, M. and Clerwall, C. (2013). Negotiating Professional News Judgment and “Clicks”. Nordicom Review, 
                        34(2): 65–76 doi:10.2478/nor-2013-0054.
                    
                    
                        Kleinneijenhuis, J. and Atteveldt, W. van (2006). Geautomatiseerde inhoudsanalyse, met de berichtgeving over het EU-referendum als voorbeeld. Inhoudsanalyse: Theorie En Praktijk. Kluwer, pp. 227–50.
                    
                    
                        Lee, A. M., Lewis, S. C. and Powers, M. (2012). Audience Clicks and News Placement: A Study of Time-Lagged Influence in Online Journalism. Communication Research: 0093650212467031 doi:10.1177/0093650212467031.
                    
                    
                        Manovich, L. (2012). How to Follow Software Users? http://lab.softwarestudies.com/2012/04/new-article-lev-manovich-how-to-follow.html (accessed 28 January 2014).
                    
                    
                        Menchen-Trevino, E. (2013). Collecting vertical trace data: Big possibilities and big challenges for multi-method research. Policy & Internet, 
                        5(3): 328–39 doi:10.1002/1944-2866.POI336.
                    
                    
                        Menchen-Trevino, E. Tracing our every move: Big data and multi-method research The Policy and Internet Blog http://blogs.oii.ox.ac.uk/policy/tracing-our-every-move-big-data-and-multi-method-research/ (accessed 30 April 2015).
                    
                    
                        Menchen-Trevino, E. and Karr, C. (2012). Researching Real-World Web Use with Roxy: Collecting Observational Web Data with Informed Consent. Journal of Information Technology & Politics, 
                        9(3): 254–68 doi:10.1080/19331681.2012.664966.
                    
                    
                        Munson, S. A., Lee, S. Y. and Resnick, P. (2013). Encouraging Reading of Diverse Political Viewpoints with a Browser Widget. Seventh International AAAI Conference on Weblogs and Social Media http://www.aaai.org/ocs/index.php/ICWSM/ICWSM13/paper/view/6119 (accessed 25 September 2015).
                    
                    
                        Napoli, P. M. (2010). Audience Evolution: New Technologies and the Transformation of Media Audiences. New York: Columbia University Press http://cup.columbia.edu/book/audience-evolution/9780231150347.
                    
                    
                        Navarrete Hernández, T. (2014). A history of digitization: Dutch museums Amsterdam: University of Amsterdam http://dare.uva.nl/record/1/433221 (accessed 25 September 2015).
                    
                    
                        Nederlandse Nieuwsmonitor (2013). Seksmoord Op Horrorvakantie: De Invloed van Bezoekersgedrag Op Krantenwebsites Op de Nieuwsselectie van Dagbladen En Hun Websites. Amsterdam: Nederlandse Nieuwsmonitor http://www.nieuwsmonitor.net/d/244/Seksmoord_op_Horrorvakantie_pdf.
                    
                    
                        Prior, M. (2009). The Immensely Inflated News Audience: Assessing Bias in Self-Reported News Exposure. Public Opinion Quarterly, 
                        73(1): 130–43 doi:10.1093/poq/nfp002.
                    
                    
                        Reuters Institute for the Study of Journalism (2015). Digital News Report 2015. Oxford http://www.digitalnewsreport.org/.
                    
                    
                        Robinson, J. P. (1985). The validity and reliability of diaries versus alternative time use measures. Time, Goods, and Well-Being.
                    
                    
                        Schrøder, K. C. and Kobbernagel, C. (2010). Towards a typology of cross-media news consumption: a qualitative-quantitative synthesis. Northern Lights: Film and Media Studies Yearbook, 
                        8(1): 115–37 doi:10.1386/nl.8.115_1.
                    
                    
                        Taneja, H. and Mamoria, U. (2012). Measuring Media Use Across Platforms: Evolving Audience Information Systems. International Journal on Media Management, 
                        14(2): 121–40 doi:10.1080/14241277.2011.648468.
                    
                    
                        Taneja, H., Webster, J. G., Malthouse, E. C. and Ksiazek, T. B. (2012). Media consumption across platforms: Identifying user-defined repertoires. New Media & Society, 
                        14(6): 951–68 doi:10.1177/1461444811436146.
                    
                    
                        Tewksbury, D. (2003). What Do Americans Really Want to Know? Tracking the Behavior of News Readers on the Internet. Journal of Communication, 
                        53(4): 694–710 doi:10.1111/j.1460-2466.2003.tb02918.x.
                    
                    
                        Usher, N. (2013). Al Jazeera English Online. Understanding Web metrics and news production when a quantified audience is not a commodified audience. Digital Journalism, 
                        1(3): 335–51 doi:10.1080/21670811.2013.801690.
                    
                    
                        Van Cauwenberge, A., Haenens, L. S. J. d’ and Beentjes, J. W. J. (2010). Emerging consumption patterns among young people of traditional and internet news platforms in the Low Countries. Observatorio, 
                        4(3): 335–52.
                    
                    
                        Vicente-Marino, M. (2013). Audience research methods. Facing the challenges of transforming audiences. Audience Research Methodologies: Between Innovation and Consolidation. New York, pp. 37–53.
                    
                
            
        
    


        
            Aufgrund der technischen Entwicklungen in den letzten Jahrzehnten sind GeisteswissenschaftlerInnen in ihrem Forschungsalltag vor neue Herausforderungen gestellt. Während es für den wissenschaftlichen Nachwuchs in Österreich eine Reihe von curricularen Angeboten sowie Summer Schools im Bereich der Digitalen Geisteswissenschaften gibt
                
                    Vgl. Liste von DH-Studiengängen im deutschsprachigen Raum verfügbar unter 
                        http://www.cceh.uni-koeln.de/Dokumente/BroschuereWeb.pdf [letzter Zugriff 29.10.2015]; Sahle, Patrick (2013): "DH Studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities". DARIAH-DE Working Papers Nr. 1. Göttingen: DARIAH-DE. URN: urn:nbn:de:gbv:7-dariah-2013-1-5, Anhang BA- und MA-Studiengänge, 30-31 [letzter Zugriff 29.10.2015]; Dariah.eu Digital Humanities Course Registry
                         
                        https://dh-registry.de.dariah.eu/ [letzter Zugriff 29.10.2015]; Tabellarischer Vergleich der Studiengänge. In: Thaller, Manfred (Eds.), (2015). Digital Humanities als Beruf. Fortschritte auf dem Weg zu einem Curriculum. Akten der Dhd Arbeitsgruppe „Referenzcurriculum Digital Humanities“ vorgelegt auf der Jahrestagung 2015, Graz 24.-27. Februar 2015, 123-125.
                    
                , sind erfahrene Forschende meist auf sich gestellt, wenn es darum geht, sich forschungsrelevante ICT-Kompetenzen für individuelle Fragestellungen anzueignen: Gemäß der von DARIAH
                
                    Digital Research Infrastructure for the Arts and Humanities, 
                        http://dariah.eu/ [letzter Zugriff 29.10.2015]
                    
                 initiierten Umfrage (Schneider / Scholger in Druck) im Jahr 2014/2015 gaben mehr als 50 Prozent der befragten Forschenden in Österreich an, dass Weiterbildungsangebote zum Thema DH-Methoden und Werkzeuge und wie diese ihre eigene Forschung verbessern könnten und tendenziell wichtig oder sehr wichtig für ihre Arbeit wären.
            
            Das Austrian Centre of Digital Humanities (ACDH) an der Österreichischen Akademie der Wissenschaften sieht es als eine seiner Aufgaben, DH-Inhalte und -kompetenzen zu vermitteln und die Forschenden dabei zu unterstützen, das Potenzial digitaler Methoden und Werkzeuge für ihre konkreten Forschungsprojekte zu nützen. Aus diesem Grund werden am ACDH unterschiedliche Formate der Wissensweitergabe erprobt und evaluiert. In unserem Kurzvortrag stellen wir eines dieser Formate, die ACDH Tool Gallery
                
                    
                        https://acdh.oeaw.ac.at/acdh/en/acdh-tool-gallery [letzter Zugriff 29.10.2015]
                    
                , als Fallstudie vor und berichten über deren Konzeption und Etablierung als außeruniversitäres Weiterbildungsangebot. 
            
            Anders als in herkömmlichen Vortragsreihen, in denen praktischen Aspekten weniger Bedeutung zugemessen wird, stellt die ACDH Tool Gallery das Experimentieren mit eigenen Daten in den Mittelpunkt. Das Konzept der ACDH Tool Gallery besteht darin, zwei Gruppen von Expertinnen und Experten zusammenzubringen: einerseits jene, die sich mit der Entwicklung von Tools beschäftigen und diese bereitstellen, und andererseits jene, die in ihrer geisteswissenschaftlichen Fachdisziplin ausgewiesen sind, und diese Tools in Zukunft verwenden möchten. Bei der Auswahl der Kurzreferate am Vormittag wird diesem Konzept insofern Rechnung getragen, als dass sowohl IT-ExpertInnen als auch GeisteswissenschaftlerInnen, die diese Tools bereits für ihre Forschung einsetzen, als Vortragende eingeladen werden. Die eintägigen Veranstaltungen sind jeweils einem Themenkomplex gewidmet: Die Tools, die am Vormittag in Einführungsvorträgen und Präsentationen vorgestellt werden, können nachmittags von den TeilnehmerInnen erprobt werden, indem diese Schritt für Schritt von der Installation bis zur Anwendung begleitet werden. Idealerweise hat jeder der TeilnehmerInnen ein eigenes, individuelles Set an Daten am Laptop vorbereitet, an dem die Tools getestet werden. Mit der Teilnahme an der Veranstaltung kann eine Kursbestätigung erworben werden. 
            Die ACDH Tool Gallery versteht sich als Angebot für EinsteigerInnen im Bereich der Digital Humanities und vermittelt einen Überblick sowohl zu bewährten als auch zu neuesten Tools, die zur Verfügung gestellt werden. Im Vordergrund steht der Austausch zwischen AnbieterInnen und potentiellen AnwenderInnen: Gemeinsam kann eine Einschätzung darüber erfolgen, ob und inwieweit ein jeweiliges Tool zur Beantwortung individueller Forschungsanliegen geeignet ist. Ausreichend Zeit zur (spontanen) Diskussion ist die Voraussetzung zum Gelingen dieses innovativen Formats. 
            Die Zielgruppe sind ForscherInnen aller geisteswissenschaftlichen Disziplinen sowie zum Teil MitarbeiterInnen von wissenschaftlichen Einrichtungen wie Bibliotheken und Archiven. Die ACDH Tool Gallery ist ein unentgeltliches Angebot, das sich nicht nur an ForscherInnen der Österreichischen Akademie der Wissenschaften richtet, sondern allen Interessierten offensteht. Damit dieses Format gelingt, braucht es intensive Vorbereitung auf allen Seiten: 1. liegt es an den TeilnehmerInnen zu überlegen, an welchen Daten sie das jeweilige Tool zu testen beabsichtigen; 2. leisten die Tool ExpertInnenen Vorarbeit, indem sie einen Zugang zu ihren Tools schaffen; und 3. gibt das ACDH die konzeptionellen und organisatorischen Rahmenbedingungen für dieses Format vor. Da es sich um ein neues Angebot handelt, wird die ACDH Tool Gallery intensiv über die ACDH-Website, die dha-Website, die ÖAW-Website, diverse Mailinglisten und Social Media (Facebook and Twitter) beworben. 
            Die ACDH Tool Gallery wird dreimal im Jahr angeboten und hat bislang bereits zu den Themen “Automated Recognition and Transcription of Handwritten Documents”, “Basic Text Enrichment - TreeTagger for DH-Application” und “Semantic Web Tools” stattgefunden. In einem Bewertungsbogen wurden die TeilnehmerInnen jeweils nach der Veranstaltung befragt, wie sie dieses neue Format einschätzen, mit welchem Vorwissen sie daran teilgenommen haben, was sich verbessern ließe und welche weiteren Tools relevant für ihre Forschungen wären, um künftig im Rahmen der Tool Gallery vorgestellt zu werden. Im Fragebogen wurden außerdem anonyme personenbezogene Daten (z.B. Alter, Geschlecht, Beruf, Herkunftsinstitution) erfasst. 
            Die Ergebnisse dieser Fallstudie werden im Kurzvortrag präsentiert. Vor dem Hintergrund dieser Erfahrungen möchten die Autorinnen diskutieren, was daraus abgeleitet werden kann und Empfehlungen geben, wie ein attraktives, außeruniversitäres Weiterbildungsangebot, das den disziplinären Anforderungen
                
                    Vgl. die Empfehlungen zur Implementierung eines österreichweiten Schulungsprogrammes zum Thema Forschungsdaten speziell im Bereich der Geisteswissenschaften bei Bauer, Bruno; Ferus, Andreas; Gorraiz, Juan; Gründhammer, Veronika; Gumpenberger, Christian; Maly, Nikolaus; Mühlegger, Johannes Michael; Preza, José Luis; Sánchez Solís, Barbara; Schmidt, Nora; Steineder, Christian (2015): Forschende und ihre Daten. Ergebnisse einer österreichweiten Befragung – Report 2015. Version 1.2, S. 70. DOI: 10.5281/zenodo.32043. Online auch unter: http://phaidra.univie.ac.at/o:407513
                 unterschiedlicher Forschenden gerecht wird, gestaltet werden könnte. 
            
        
        
            
                
                    Bibliographie
                    
                        Bauer, B., Ferus, A., Gorraiz, J., Gründhammer, V., Gumpenberger, Ch., Maly, N., Mühlegger, Johannes M. Preza, J. L. Sánchez Solís, B., Schmidt, N., Steineder, Ch. (2015).
                        Forschende und ihre Daten. Ergebnisse einer österreichweiten Befragung – Report 2015. Version 1.2, DOI: 10.5281/zenodo.32043. Online auch unter: http://phaidra.univie.ac.at/o:407513 (letzter Zugriff 29.10.2015).
                    
                    
                        Sahle, P. (2013). DH Studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities. DARIAH-DE Working Papers Nr. 1. Göttingen: DARIAH-DE. URN: urn:nbn:de:gbv:7-dariah-2013-1-5, Anhang BA- und MA-Studiengänge, pp. 30-31 (letzter Zugriff 29.10.2015).
                    
                    
                        Schneider, G., Scholger, W. (2015). In: Dallas and N. Chatzidiakou (Eds.), 
                        DARIAH survey on scholarly practices and needs of European humanities researchers in the digital environment 2014-15, Technical report, Athens: Digital Curation Unit. Forthcoming, in Druck: Austria.
                    
                
            
        
    

        
            There is an abundance of biographical information online that begs to be analyzed with computational methods. Resources like Wiki- and DBpedia, Biographical Dictionaries, Historical Databases, Newsfeeds, Facebook and Twitter all provide information on individual’s lives. ‘Biographical data’ is of particular interest to computer scientists, because it can be well structured and all people share common attributes such as place of birth, place of residence, parents, et cetera. 
            The analysis of `biographical data' with new techniques is a topic that is finding strong interest in research groups all over the world, demonstrated most recently by the first conference on Biographical Data, organized in Amsterdam in 2015. This conference brought researchers from various domains together including historians, librarians, computer scientists, data scientists, and computational linguists.
            The purpose of this workshop is to take a next step in strengthening the community working with digital biographical data by exploring possibilities of turning shared interest into new international collaborations. A central theme in this next step will be connecting and linking data. 
            A strong international community working on biographical data and aiming for shared data representations can directly support other domains in the digital humanities: easily accessible background information on people involved in historical or contemporary events, on artists, researchers or groups of people with common professions can provide background information and therefore be of interest to digital humanities researchers working with topics beyond research on biographical data.
            This workshop brings together researchers from various domains working on biographical data. In addition to sharing latest progress, it has the specific aim of initiating efforts to share (knowledge about) data and data models. The workshop directly contributes to the efforts of the DARIAH workgroup on biographical data and aims to involve new researchers in this collaboration.  A call for organization will go out for the Biographical Data in a Digital World Conference in 2017 (2015 conference: http://ceur-ws.org/Vol-1399/).
            The workshop consist of two main components: 1) a poster session where researchers can share their latest work on biographical data and computational analysis and 2) dedicated sessions about data and data models.
            For our poster session, we explicitly invite researchers to the workshop who work with biographical data for historical research or data analysis (e.g. computational linguists, visualization experts) and are thus already very familiar with models for biographical data, but are not necessarily involved in designing them. This perspective is of great value during discussions on sharing and modeling data and can provide insights into what kind of data models are practical to work with or which links between various datasets are most valuable for research. These insights can in turn help to identify logical and practical first steps towards increasing international collaboration
            Descriptions on data models and data samples will be distributed to participants in advance and studied by a panel. The panel will present their findings and support the discussion on sharing data. Direct involvement in several projects and strong relations with other international partners guarantees an interesting set of data and data models.
        
    


        
            
                
            
            
                When discussing the web, the two layers of online experience we most often talk about are interactions and their underlying technology. We unpack the conversations and transactions happening online (anti-feminist rhetoric, how Twitter differs from Medium), and we discuss how algorithms shape those experiences (Facebook’s decisions about what is in your feed, A/B testing). But at an intermediary level, an entire corpus is being written in Javascript variables and HTML comments
                , standardized but hidden files, and even the structure of websites themselves. This workshop will investigate these intermediary layers from a critical, exploratory point of view.
            
            
                This workshop will use a hackathon-like methodology of play and exploration to maximize breadth and depth – the goal is creative and critical investigation rather than technical. After some short, fairly low-tech tutorials to introduce tools and methodologies, participants will spend the remainder of the workshop digging across the web and presenting their findings. All levels of technical expertise will be encouraged, as will ad hoc collaboration. Those with prior experience using the tools introduced are encouraged to dig deeper.
            
            
                
                    Workshop Leader
                    Jeff Thompson an artist, teacher, hacker, and writer whose work investigates the poetics of technology, how its functionality can provide access points for meaningful exploration, the agency of increasingly self-aware systems, and the physicalization of otherwise invisible processes. He is currently Assistant Professor and Program Director of Visual Arts and Technology at Stevens Institute of Technology.
                
            
            
                Questions about this workshop: jeff.thompson@stevens.edu
            
            
                
                    Tech 
                    requirements
                    Participants should bring:
                
            
            
                
                    A laptop with WiFi access
                
                
                    An up-to-date version of Firefox installed (mozilla.org/firefox)
                
                
                    Sublime Text text editor (free version is fine, sublimetext.com)
                
                
                    If possible, please install wget (gnu.org/software/wget)
                
                
                    Other text-mining/web tools of your choice
                
            
        
    


        
            Gephi is a free and open source network analysis software used, among other things, in social network analysis. This workshop is intended for beginners as well as confirmed users. First, we offer an introduction to the basics of Gephi, then we explore through practice the question of visual network interpretation. We provide a dataset of both Twitter hashtags and Twitter followers graphs on various topics related to the DH community.
            
                Why network interpretation matters
                Reading a network visualization can sometimes be harder than simply producing it. Once the graph has been produced, what are we supposed to look at? Nowadays, it is common to learn how to use social network analysis software such as Gephi via online tutorials, but it is often difficult to learn how to interpret the results. Based on the experience of members of the software development team and Gephi power users, we offer this workshop to help users interpret their results.
                Network visualizations are exploratory rather than explanatory. As a scholar, it is important to leverage network visualization in order to find interesting insights inside your data. Exploring a network requires mobilizing external knowledge on data’s context. Exploration is about generating, and not validating, hypotheses. Networks do not carry a single, clear message, and it is as fruitful for analysis as it is bad for communication. Dispelling this misunderstanding is very important if you want to fully benefit from a tool like Gephi.
                The idea that a tool can analyze things for you is another misunderstanding we can help tackle. Gephi allows you to explore multiple facets of your data, but the interpretation remains to be done by the user him/herself. Users have to spend time with their data, and a workshop is a good place to introduce this data-care principle.
                Once you know what to look for in a network, you will capable of finding insight but you still have to excavate some evidence. Network metrics are more capable of doing so than the visualization itself. In this workshop we will also learn to match visual features with metrics so that you can provide evidence for what you have seen. For instance, observed clusters are proven to be modularity clusters in the sense of Newman (Noack, 2009).
            
            
                Workshop schedule
                
                    Part 1: Visual network analysis in a nutshell
                    We start the workshop with a presentation about why and how we visualize networks (Jacomy et al., 2014) and how we interpret them (Venturini et al., 2015) through a Exploratory Data Analysis method (Tukey, 1977)
                
                
                    Part 2: Gephi practice
                    
                            
                                
                                Figure 1: Participants will learn how to produce a readable Gephi map
                            
                    
                    In this part we explain Gephi through examples. We manipulate Gephi on screen while participants execute the same operations on their computers, using the provided datasetsAckland, R. 2013. “Web social science: Concepts, data and tools for social scientists in the digital age.” SageAckland, R. 2013. “Web social science: Concepts, data and tools for social scientists in the digital age.” Sage (Grandjean, 2015). The complete chain of usage will be addressed by illustrating Gephi features from basics (software installation, layout, data table…) to advanced (computing statistics, filtering, exporting…).
                
                
                    Part 3: Guided practice
                    Each group makes a visualization and wraps it up in a few slides using screenshots (Girard et al., 2015). The trainers provide practical help to participants.
                
                
                    Part 4: Collective discussion
                    Each group presents its findings, and we leverage these live examples to discuss the interpretation process through networks and notably its robustness compared to the visualisation choices.
                    This workshop is supported by DIME-WEB part of DIME-SHS research equipment financed by the EQUIPEX program (ANR-10-EQPX-19-01).
                
            
        
        
            
                
                    Bibliography
                    
                        Girard, P., Jacomy, M. and Plique, G. (2015). Manylines, a graph web publication platform with storytelling features Paper presented at the graph dev room, FOSDEM, Bruxelles, Belgique. https://archive.fosdem.org/2015/schedule/event/graph_manylines/ (accessed 14 March 2016).
                    
                    
                        Grandjean, M. (2015). GEPHI – Introduction to Network Analysis and Visualization, 
                        Martin Grandjean. http://www.martingrandjean.ch/gephi-introduction/ (accessed 14 March 2016).
                    
                    
                        Jacomy, M., et al. (2014). ForceAtlas2, a Continuous Graph Layout Algorithm for Handy Network Visualization Designed for the Gephi Software. (Ed.) Muldoon, M. R., 
                        PLoS ONE, 
                        9(6): e98679 doi:10.1371/journal.pone.0098679.
                    
                    
                        Noack, A. (2009). Modularity clustering is force-directed layout. 
                        Physical Review E, 
                        79(2): 026102 doi:10.1103/PhysRevE.79.026102.
                    
                    
                        Tukey, J. W. (1977). 
                        Exploratory Data Analysis. Addison-Wesley Publishing Company.
                    
                    
                        Venturini, T., Jacomy, M. and Pereira, D. (2015). Visual Network Analysis: the Example of the Rio+20 Online Debate. Working paper. http://www.medialab.sciences-po.fr/wp-content/uploads/2015/06/VisualNetwork_Paper-10.pdf.
                    
                
            
        
    





Inspired by DH: The Day of Archaeology


Richardson
,
Lorna-Jane

UCL Centre for Digital Humanities, United Kingdom
l.richardson@ucl.ac.uk




University of Nebraska-Lincoln

Center for Digital Research in the Humanities

319 Love Library
University of Nebraska–Lincoln
Lincoln, NE 68588-4100
cdrh@unlnotes.unl.edu


Lincoln, Nebraska

University of Nebraska-Lincoln
Lincoln, NE 68588-4100







The Day of Digital Humanities has become part of the Digital Humanities landscape: Inspired by the project, the Day of Archaeology (DoA) was established by a group of professional digital archaeologists and PhD students in 2011. With over 500 participants, the web project aims to collate archaeological experiences and connect archaeologists across the world, using a variety of digital technologies. The participants record and share their Day on the Wordpress-based DoA website:
www.dayofarchaeology.com
, alongside photo-sharing sites, Facebook, YouTube and Twitter. This paper will explore how the Day of Digital Humanities model has translated into a more defined discipline; what the DoA has learned from Day of Digital Humanities; the difficulties and benefits of the web-based model when applied to archaeology; how social media has been used; and what impact the DoA has had, and will have as it develops, for participants, the archaeology community and the wider public.



No source: created in electronic format.




SP01





Paper


Short Paper


archaeology
Day of Digital Humanities
online community
social media


archaeology
audio
video
multimedia
knowledge representation
internet / world wide web
social media
crowdsourcing


withdrawn






Erin Pedigo
Initial encoding







Inspired by DH: The Day of Archaeology


The Day of Digital Humanities has become part of the Digital Humanities landscape: Inspired by the success of the Day of Digital Humanities project, the Day of Archaeology (DoA) was established as a voluntary project by a group of professional digital archaeologists and PhD students in 2011. The project is organised and run for free, and server space and staff time is donated and voluntary. The aim of the DoA was to utilise digital and participatory technologies, using a simple Wordpress-based platform, that would enable even the least digitally-minded archaeologist to share their work within the archaeology community and with the wider public. The web project aims to collate archaeological experiences and connect archaeologists across the world, using a variety of digital technologies. The participants record and share their Day on the Wordpress-based DoA website:
www.dayofarchaeology.com
, alongside photo-sharing sites, Facebook, YouTube and Twitter.


The Day of Archaeology
www.dayofarchaeology.com
has subsequently developed into an annual Public Archaeology event, which offers a unique insight into the working day of archaeologists worldwide. Participant-archaeologists have come from Asia, North and South America, Europe, Australia and Africa. They have written about their day from excavations, offices, museums, community projects, the tourist industry, local government and voluntary groups. The project participants aim to answer a simple question in their contribution; “what do archaeologists do?”. Participants have contributed blog posts, films, photos, Tweets, Facebook pages, archive 'bingo' and 'ask an archaeologist'. The first event was held on the July 29th 2011, where some 500 people working, studying or volunteering in archaeology projects around the world contributed blog posts describing their day. The published posts and text are not scripted by the organisers, and only minimally edited to avoid defamation or incorrect information being shared.


The resulting website presents a behind-the-scenes view of archaeology that incorporates not only the exciting discoveries often showcased in Public Archaeology, but also everyday details of archaeological work in the real world. This project aims to move the public understanding of archaeology away from the 'Indiana Jones' model of excitement and object-oriented discovery, to one that can appreciate the painstaking and vital work undertaken by professionals and volunteers to protect, preserve and interpret our shared pasts.


This paper will explore how the Day of Digital Humanities model has translated into a more defined discipline; what the DoA has learned from Day of Digital Humanities; it will presents details of the project, how it was organised and who participated; the difficulties and benefits of the web-based model when applied to archaeology; how social media has been used; critical reflections on how the project has engaged with different audiences and what impact the DoA has had, and will have as it develops, for participants, the archaeology community and the wider public.





Introduction
The Russian flu 1889-1893 epidemic reached Europe from the East in November and December of 1889 and spread over the whole globe in the space of a few months. It was one of the first epidemics of influenza that occurred during the period of the rapid development of bacteriology. In addition, it was the first ever epidemic that was publicly and intensively narrated in the developing daily press, especially those

published in German located in Germany and Austria

(Miroslawska et al., 2013). However, as stated in (Valtat et al., 2011), very limited information about the epidemiology of this influenza has been found, which was based on materials published in English. While a large amount of news about the flu was published in German, it is hard to find a study on the epidemic based on German documents. These motivate our goal in this work, which is to build a framework from German materials to support research community getting more insights into the disease. Our framework consists of different components including data collection and cleaning, corpus creation, and associated tools for analysis. The framework is pictorially shown in Figure 1.


Figure 1. Russian flu exploration framework

Related work
There is limited information about the epidemiology of the Russian flu epidemic 1889-1893. In (Miroslawska et al., 2013), the authors conducted an analysis to examine the impact of the epidemic in 14 cities in Europe. Their results showed that the epidemic spread quickly from Saint Petersburg, Russia to other parts of Europe with a speed of around 400 km/week and reached the American continent only 70 days after the original peak in Saint Petersburg. In addition, some detailed information about case fatality ratio and the median basic reproduction was given also. However, their work was based on reports of only two local daily newspapers in Poznan, which implies some uncertainty due to the lack of data coverage. Val-leron et al., 2010 presented a case study on the trans-missibility and geographic spread of the Russian flu. A similar approach was followed by Valtat et al., 2011 to examine the age distribution of the affected people and the mortality rate of this flu event. In a recent study, Ewing et al., 2016 collected contemporary reports and explored a digital humanities approach to interpret information dissemination regarding this particular epidemic. The limitations common to all of these studies are the heterogeneity and lack of coverage of data used.

Table 1: Keywords used to collect high recall collection of newspaper issues containing stories about influenza epidemic


Data preparation

ID

Keyword

Variation

1

Influenza

Influenza Jnsolvenza

2

Epidemie

3

Influenza-Epidemie

Influenzaepidemie

4

Grippe

5

erkrankt

ertrankt

6

Pathologie

Data collection

Data used in this work was collected from the Austrian Newspapers Online (ANNO) repository. ANNO contains almost all issues from many newspapers in Austria and Germany during the time the Russian flu epidemic took place. The data are accessible in both scanned PDF and OCR formats. These are appropriate for our goal in terms of extracting Russian flu related stories from noisy OCR text and checking against the scanned PDF content for validity. To establish the data collection, the keywords listed in Table 1 (along with some misspelt variations of keywords, which were included due to OCR misrecognition) were used to search the ANNO repository The search query was constrained by the time interval from 1889 to 1893. After preprocessing the search results we obtained 4,806 issues, which become the candidates to extract stories about the Russian flu.

Noise reduction

Due to the low quality of the scanned images of newspaper issues, a lot of noise is present in the corresponding OCR texts. The word error rate (WER) computed on sample texts is around 18.9%. Our goals here were to remove noise and correct misrecognized words as much as possible but at the same time manage keep the language as it was so that the derived corpus pertains its historical perspective. It is noted that modern German is rather different in writing and usage of many words due to the language's evolution. To cope with these issues, we adopted a snapshot of the Google-2-gram dataset for German from 1885 to 1895. The dataset was used to train our bigram-based model for word segmentation and spell checking. After running the model, the word error rate was reduced to 5.5%.

Text block classification

A difficult challenge for the task of extracting complete stories is that recognized OCR text blocks are very often not aligned in the same order as they were in the original image of an issue. Our approach was to automatically pre-classify OCR text blocks to identify the ones that are more likely part of some flu-related stories. Then we developed a tool to effectively help annotators extract complete Russian flu stories. For this, we adopted the KL-divergence based technique developed in (Schneider, 2004) to build a classifier. The model was trained with 245 OCR text paragraphs and obtained recall of 81.5% and precision of 68.6%. Basically, the output of the classifier can be used to help annotators start working on an issue by looking at suggested text blocks first, from which they can then select paragraphs that are part of the same story.

Extraction tool

After completing the high-recall automatic pre-extraction, we implemented a Web-based tool for annotators to help build our corpus collaboratively. The main GUI of our tool is shown in Figure 2. After having annotators work through the whole collection, we obtained a corpus of 639 news articles about Russian flu from 42 newspapers, identified with 85.7% agreement between annotators.


Figure 2. Main GUI of our tool for Russian flu story extraction

Geo and temporal information extraction

Given that location and time are helpful features for exploring the development of the epidemic, we extracted and normalized geographic names and temporal expressions occurring in the corpus. For geographic names, the Geodict tool created by Pete Warden (2011) was adapted to work with country and city names in German. HeidelTime (Strotgen and Gertz, 2013) was used for temporal information extraction and normalization.

Indexing and search engine

We created a search engine on the corpus to support research community in searching for information. The searching GUI is shown in Figure 3.


Searching results:

There are 77 stories found!

1    Extracted from newspaper issue: APR 10-12-1889 by annotator: admin

Telegramme der.presse". Prag, 9. December. In Folge eines speciellenFalles, daß einGemeinde-Amt die Annahme einer in ungarischer Sprache abgefaßten Eingabeverweigerte,gabder Statthalter von Mähren einen Erlaß hinaus, wonach sämmt sämmtliche liehe sämmtliche autonomen Behörden die Eing...

2    Extracted from newspaper issue: APR 12-12-1889 by annotator: admin

Lieber Me Infiueiya. Wien, 11. December. Ein hiesiger Arzt, der an derjnfluenza" erkranktwar, aber nach kaum viertägiger Krankheit vollkommen genesenist, beschreibt uns den Verlauf der Krankheit, den er an sich selbst beobachtet hat, wie folgt: Bevor die Krankheit zum Ausbruch gekommen war, ist ein...

Figure 3. Russian flu story searching module

Exploration tools and sample results
We provided associated tools along with the corpus. The corpus timeline provides statistics on the number of stories in the corpus across time and news outlet. In addition, it provides interactive visualization. As an example shown in Figure 4, during the peak time in late December 1889 and January 1890, extensive news about the influenza was published.


Figure 4. Press attention on the flu and topic changes over time

Newspapers were trying to narrate the outbreak as fast as possible. Words that appear significantly in the stories include influenza, epidemic, krankheit (disease) and erkrankt (sick). A short time after this peak period, fewer reports were published about the outbreak of the flu and communities started discussing the treatment more. Names of doctors appearing in the news (e.g., Leyden, Proust) together with words describing symptoms such as fieber (fever), kopfschmerzen (headache), appetitlosigkeit (anorexia). Furthermore, by exploring word collocations one can find even more interesting information. Figure 5 shows a frequent pattern of word collocation describing the influenza. The words heute (today) and gestern (yesterday) indicate that news about the flu is updated every day; and the word janner (January) implies that the flu outbreaks happened during winter.


Figure 5: A frequent pattern of word collocation extracted from our corpus

Figure 6 presents the co-occurrences of three words infuenza, erkrankt, and london over time. It shows a similar trending pattern of the words infuenza and erkrankt being used to note about the flu. In addition, one can observe that the peak time of the flu in London was from late December 1889 to early January 1890 as indicated in (Honigsbaum, 2010; Goff, 2011). This suggests that the temporal distribution of terms can give us more insights into the evolution of the epidemic.


Figure 6: Similar trending pattern of the two words influenza and erkrankt, and the peak time of the flu in London

The framework also supports studying the evolution of the flu over time and geographic regions. We employed the method introduced in (Abdelhaq et al., 2013) for localized event detection. Figure 7 shows three snapshots describing the development of the epidemic over cities in Europe during the peak time from late November 1889 to January 1890.

Summary


We have introduced a framework for research communities to explore the historical Russian flu 1889-


1893 from German newspapers. We developed a tool for collaborative annotators to help build our corpus. We further presented some interesting insights that we achieved from analyzing articles in the corpus. By making the corpus and associated tools available, we provide useful contributions to the community in support of conducting studies on influenza epidemics and evaluating temporal IR models.


Figure 7. Evolution of the Russian flu over geographic regions during its peak time

Acknowledgements
This research is supported by the German Research Foundation (DFG) for the project “Tracking the Russian Flu in U.S. and German Medical and Popular Reports, 1889-1893” on Grant No. NE 638/13-1. We also thank you the Austrian National Library for supporting us in collecting data.

Bibliography
Abdelhaq, H., Sengstock, C., Gertz, M..(2013). “EvenTweet:

Online Localized Event Detection from Twitter.” Proc.

VLDB Endowment Journal, 6(12):1326-4.

Aramaki, F., Maskawa, S., Morita, M..(2011). “Twitter

Catches the Flu: Detecting Influenza Epidemics Using

Twitter.” In proceedings of the Conference on Empirical

Methods in Natural Language Processing. pp. 1568-9.

Austrian National Library. (2011) Austrian Newspapers

Online. Repository online at http://anno.onb.ac.at

Ewing, E.T., Kimmerly, V. and Ewing-Nelson, S. (2016).

“Look Out for ‘La Grippe': Using Digital Humanities Tools to Interpret Information Dissemination during the Russian Flu, 1889—90.” Medical history,60(1):129-3.

Honigsbaum, M.(2010). “The Great Dread: Cultural and

Psychological Impacts and Responses to the Russian Influenza in the United Kingdom 1889-1893”.Social history of medicine,23: 299-21.

Kempinska-Mirostawska,    B., and WoYniak-Kosek,

A.(2013). “The influenza epidemic of 1889-90 in selected European cities - a picture based on the reports of two Poznan daily newspapers from the second half of the nineteenth century.” Med Science Monitor,19:1131-11.

Le Goff, J.M.(2011). “Diffusion of influenza during the winter of 1889-1890 in Switzerland.” Jenus, 67(2): 77-23.

Paul, M.J. and Dredze, M.. (2011). “You Are What You Tweet: Analyzing Twitter for Public Health.” In proceedings of the Fifth International Conference on Weblogs and Social Media. pp. 265-8.

Schneider, K.-M.(2004). “A New Feature Selection Score for Multinomial Naive Bayes Text Classification Based on KL-divergence.” Proceedings of the ACL 2004 on Interactive Poster and Demonstration Sessions. pp. 186-4.

Strotgen, J. and Gertz, M..(2013). “Multilingual and crossdomain temporal tagging.” Language Resources and Evaluation, 47(2): 269-30.

Valleron, A.J., Cori, A., Valtat, S., Meurisse, S., Fabrice Carrat, F., and Boelle, P.Y.. (2010). “Transmissibility and geographic spread of the 1889 influenza pandemic.” In proceedings of the National Academy of Sciences of the United States of America (PNAS). pp. 8778-4.

Valtat, S., Cori, A., Carrat, F., and Valleron, A.-J. (2011). “Age distribution of cases and deaths during the 1889 influenza pandemic.”Vaccine, 29(2): B6-B10.

Warden, P. (2010) GeoDict. Accessible via Github at https://github.com/petewarden/geodict
Introduction

With many ongoing debates (Gold, 2012) and “unwritten” histories (Nyhan and Flinn, 2016), the research practice of the Digital Humanities (DH) has been around for 70 years. Many works have been trying to draw general conclusions of the disciplinary structure (McCarty, 2003; Gold, 2012; Terras et al., 2013; Schreibman et al., 2016; Nyhan and Flinn, 2016), and have pointed to the potential usefulness to analyse the discipline from statistical aspects. The usefulness focuses on describing the intellectual structure, scholarly interactions and disciplinary development of DH. Some studies have dedicated their attention to these matters (Grandjean, 2016; Nyhan and Duke-Williams, 2014; Quan-Haase et al., 2015; Wang and Inaba, 2009), or have focussed on one of these topics (Sugimoto et al., 2013), but few of them have engaged either with the bibliometric network method, or with the latest large-scale scholarly datasets to study the DH community as a whole.

Therefore, to fill this gap, based on a provisional dataset that has been compiled from core DH journals, this study performs an exclusive all-author co-citation analysis (ACA) with the 200 most cited scholars by fractional citation count to map and demonstrate the intellectual structure and to identify the most influential scholar groups and topics within DH.

To the best of our knowledge, this study is the first to apply bibliometric methods to visualise DH knowledge structure and the scholar clusters. This research output will make a valuable contribution to the current discussions and debates about DH knowledge structure and wider scholarly networks.

Methodology

With ACA as the main methodology, the research contains four steps, and each with a different methodology: building a DH citation index according to the publications of these journals; selecting authors as the core objects for citation analysis; assigning scholars to different distance-based clusters by calculating the author co-citation matrix to similarity matrix (Waltman and van Eck, 2013); finally, visualising the DH citation network which aims to show the scholar clusters, and the knowledge structure and diffusion of DH.

The three DH core journals that our dataset has been constructed from are: “Computers and the Humanities” (CHum), “Digital Humanities Quarterly” (DHQ), “Literary and Linguistic Computing” (LLC) (now “Digital Scholarship in the Humanities”) (DSH). The bibliographies as well as the metadata of all their publications (including the reviews and editorials etc.) published until June 2016 have been collected. It should be noted that none of these journals spanned the whole period selected (19662016): CHum, the first DH journal started in 1996, and ceased publication in 2004; LLC/DSH began in 1986; DHQ began in 2007. Figure 1 shows the total publications each year from 1966 until June 2016 for these journals.


Figure 1. In total, 3,068 journal articles: CHum (1,195 articles with 26,033 citations), LLC/DSH (1,633 articles with 28,501 citations), and DHQ (240 articles with 4,289 citations)

Author co-citation analysis (ACA) can reveal the intellectual structure of a field from its academic publications by calculating the frequencies with which two authors are cited together. That is to say, if an article cites at least one article of author A, and at least one of author B that is different from the one of A, the co-citation count increases by 1. The more co-citations two authors receive, the more likely their publications and researches are related (Bellardo, 1980). Therefore, the clusters of related authors indicate the networks of research topics, or influential focuses within a discipline.

The initial findings with the top cited 200 authors displayed on the maps (see the provisional maps in

Figure 2 and Figure 3) have provisionally revealed

five sub-fields within DH.

,«owsky. d.


Figure 2. The provisional ACA network map in DH, data from journals CHum, LLC/DSH, and DHQ, 1966-2016, created using VOSviewer


Figure 3. The provisional ACA density map in DH, data from journals CHum, LLC/DSH, and DHQ, 1966-2016, created using VOSviewer

Both of the maps (Figure 2 and Figure 3) are

distance-based. Each node on the map represents an author, and the distance between two authors is their relations (the closer the distance, the stronger the connection). Authors are distributed quite unevenly, and this makes it easy to identify clusters of related nodes. The size of the node represents the citation count this author received, and the higher the citation count is, the bigger the node. On the density map, the density value depends on the size, number and distance of the nodes around it, so the higher the density value, the colour is more red than blue.

Both maps have revealed the general structure of the scholarly communication between DH scholars via publications. Horizontally across the centre of the map, there is a loosely connected circle of five DH scholar clusters: centre (focused on “Leech, G”), top (focused on “Miller, G”), bottom (focused on “Nerbonne, J”), left (focused on “Holmes, D.I”), and right (focused on “McCarty, W”). The clusters distribution on the density map reveals that there is a clear separation between top, centre, right clusters to left and bottom clusters. Especially the right cluster (focused on “McCarty, W”) and the left cluster (focused on “Holmes, D.I”) turn out to be denser than other clusters. This shows that these two clusters are more significant and have more citation influence. According to the provisional analysis, these five clusters appear to be associated with five different DH research topics: English study at the centre; general historical literacy and information science on the right; language modelling and natural language processing at the top; statistics and text analysis on the left; computational linguistics particularly on Dutch and German speaking at the bottom. These five clusters, however, are also grouped into two different bigger groups. The English study, language modelling, and general historical literacy seem to be in one group which is more related, while the statistics and Dutch-German linguistics are also very closely related to each other.

Limitations and Future study

This research is part of the first author's ongoing PhD study, funded by UCL ORS scholarship and based at the UCL Centre for Digital Humanities. The doctoral reseach maps DH intellectual, social and environmental structures using the Invisible College model (Zuccala, 2006).

There are some limitations that need to be noted, such as the citation lag time. In order to build up a citation record for co-citation, it takes around five to eight years (Hopcroft et al., 2004). This could explain that certain recognisable authors might not appear on the maps yet. Also, because the co-citation method studies the knowledge base as its subject, the map emphases more on authors published some time ago, which might not include the “new comers”.

In the future work, the ACA study will be extended to include more citation data. The ACA study will be divided into discreet periods to construct maps of different DH development stages. Given that different journals have different topical foci, the research will also analyse individual journal to discover its attribute.

Bibliography

Bellardo, T. (1980). “The use of co-citations to study

science.” Library Research, 2(3): 231-237.

Gold, M. K. (2012). Debates in the Digital Humanities. University of Minnesota Press.

Grandjean, M. (2016). “A social network analysis of Twitter: Mapping the digital humanities community.”

Cogent Arts and Humanities, 3(1): 1171458.

Hopcroft, J., Khan, O., Kulis, B., Selman, B., (2004). “Tracking evolving communities in large linked networks.” Proceedings of the National Academy of

Sciences, 101: 5249-5253.

McCarty, W. (2003). “Humanities Computing.” In

Encyclopedia of Library and Information Science. New

York: Marcel Dekker.

Wang, X., and Inaba, M. (2009). “Analyzing Structures and Evolution of Digital Humanities Based on

Correspondence Analysis and Co-word Analysis.” Art

Research, 9: 123-134.

Zuccala, A., (2006). Modeling the Invisible College. Journal of the Association for Information Science and Technology. 57: 152-168.

Nyhan, J., and Flinn, A. (2016). Computation and the Humanities: Towards an Oral History of Digital Humanities. Cham: Springer International Publishing.

Nyhan, J., and Duke-Williams, O. (2014). “Joint and multi-authored publication patterns in the Digital

Humanities.” Literary and Linguistic Computing, 29 (3): 387-399.

Quan-Haase, A., Martin, K., and McCay-Peet, L. (2015). “Networks of Digital Humanities Scholars: The Informational and Social Uses and Gratifications of

Twitter.” Big Data and Society, 2(1).

Schreibman, S., Siemens, R. G., and Unsworth, J. (Eds.). (2016). A New Companion to Digital Humanities. Chichester, West Sussex, UK: John Wiley

and Sons Inc.

Sugimoto, C. R., Thelwall, M., Larivière, V., Ding, Y., and Milojevic, S. (2013). Mapping Digital Humanities. Retrieved from http://did.ils.indiana.edu/dh/

Terras, M., Nyhan, J., and Vanhoutte, E. (Eds.). (2013). Defining Digital Humanities: A Reader. Farnham, Surrey, England : Burlington, VT: Ashgate Publishing Limited ; Ashgate Publishing Company.

Waltman, L., and van Eck, N. J. (2013). “A smart local moving algorithm for large-scale modularity-based community detection.” The European Physical Journal B, 86(11).
Introduction

In March of 2016, the failure of Microsoft's prototype chatbot, Tay, was not just a technological failure. It was a disciplinary failure. It was a failure of an industry leader to adopt a critical perspective when building systems in a complex cultural and social environment. Tay, which stands for “thinking about you,” was the name given to an artificial intelligence chatbot for Twitter that was quickly corrupted by users and began spewing racist, sexist, and homophobic slurs. Pundits quickly leapt to conclusions about the political beliefs of internet users, but these same pundits failed to understand that this hacking of Tay was in fact a critique of chatbots in the real world. Users of Twitter were exposing a fundamental error made by the Microsoft development team. Because the system learned direct-ly from user input without editorial control or content awareness, Tay was quickly trained to repeat slurs by users eager to embarrass Microsoft.

This moment in technological development makes for an interesting anecdote, but it also represents the moment that chatbots entered the public consciousness and became nothing less than the future direction of a unified interface for the whole of the web. Of course, chatbots captured imaginations in the 90s as well. Systems like Cleverbot, Jabberwacky, and Splotchy were fascinating to play with, but they had no real application. Today, text based AI has been identified as the the successor to keyword search. No longer will we plug in keywords into Google, comb through lists of text, and depend on search engine optimization (SEO) to deliver the best content. Search will be around for a long time, but in the near future much more content will be delivered through text based messenger services and voice controlled systems. We've seen the early stages of this change in products like Amazon's Alexa, Apple's Siri, Google Now, and Microsoft's Cortana. There are now bots embedded within common platforms like Slack, Skype, and Facebook

Messager. We are now approaching a world that Apple envisioned in 1987 with a mockup system called the “Knowledge Navigator” that sought to give users an interactive and intelligent tool to access, synthesize, present, and share information seamlessly.

Humanities in the Loop

We are likely decades away from a true “knowledge navigator,” but the second generation of these chatbots are now in development. The company that developed Siri for Apple is now in the final stages of development on a system called Viv (Matney). Viv is the first viable company to produce a unified interface for text and speech based AI assistants. Facebook is testing project M within its messenger app to allow users to issue commands, access services, and make purchases through text input (Hempel). The remarka-ble thing about M is that Facebook has built a system with “humans in the loop.” This means that when a service is accessed, perhaps by purchasing movie tick-ets, a human will fine tune the AI generated results for each transaction. There is currently an understanding within the machine learning community that human assisted training of these systems produces more accurate results but will also train more robust systems going forward (Biewald, Bridgwater). The current need for human in the loop systems means that we are at a crucial moment for humanists to lend their experience and critical abilities to the development and training of AI systems. In the field of machine learning, training a system to answer humanities based problems will show how these systems succeed or fail, but they will also demonstrate the value of the humanities in a digital world. If the purpose of the humanities is to better understand what it is to be human, training AI to answer philosophical, historical, or cultural questions will help us understand our experiences as we become more accustomed to intelligent systems in our lives. Grappling with AI, whether it is in a mundane consumer exchange or in matters of grave ethical importance, is rapidly becoming a practical problem in our lives.

With humanists in the loop, we will better understand the social and cultural contexts in which these systems appear and avoid the regrettable failure of systems like Tay in the future. We are currently on the cusp of a revolution in the applicability of natural language understanding, artificial intelligence, and conversation based interfaces design. These technologies will have ranging consequences socially, culturally, and economically in the coming decade, but these technologies are also deeply connected to the social and cultural contexts in which they appear. My goal is to train machines to be humanists. It is the literary crit-ic's ability to close read complex philosophical, histori-cal, and artistic meaning that these systems lack. It is the ability of the historian to contextualize political and technological change within the breadth of human progress. It is the dramatists ability to understand performance and dialogue that will animate our conversations with computers. The digital humanities are well situated to make the most of NLP techniques and find culturally significant training sets.


Figure 1: Faulknerbot interface with basic query and response

Method: Conversational Data Retrieval

Biographical and archival material has been used to train a system to allow a conversation with the famed American author William Faulkner. I will pre-sent a system trained with nearly all the interviews that Faulkner has given. Author interviews are an ex-cellent training set because the questions asked by the interview anticipate user interests and model a con-versa-tional style of response. The interviews collected during Faulkner's visit to the University of Virginia were instrumental in building this tool. The applica-tions for such a system are numerous. A conversation with Faulkner might benefit a creative writing student in the midst of writer's block. A chatbot offers a more inviting interface for a general public. Most important-ly, Faulknerbot will represent a novel form of content discovery for student researchers. Once a user has developed a chat history worth exploring, Faulknerbot's responses link to original archival materials for research purposes.

Current systems have come a long way from the toy-like chatbots that populated the web in the late 90s. After a pre-processing stage using word2vec, which vectorizes the bag of words, this model uses Tensor Flow to generate two complementary neural networks that encodes and decodes inputs and responses. This model has only recently been made accessible to non-computer science researchers recently by Google open sourcing Tensor Flow. is not based on the retrieval based model using a rule based expressions, with a heuristic to determine intent and draw from a predefined response. This is not a simplistic tree model based on nested “if/then” statements. Instead this uses a generative model. This generative model uses sequence to sequence learning with neural networks (https://arxiv.org/abs/1409.3215). This model links words statistically to determine “flows” of meaning through a word vector. Geoff Hinton calls this a “thought vector.” In other words, this is an end-to-end model that remains open. Rather than a retrieval method, which limits the scope of the conversation, this system dynamically learns and allows for a retention of what has been said. The generative model allows for this context based discussion without resorting to an enormous conversation log. In Tensor Flow, this operates on a Long Short Term Memory (LSTM) network. As I've said, the sequence to sequence model is based on two neural nets. One is an encoder, which encodes input data from the user. The decoder model determines the reply by generating the output, which need not echo the size of the vector. This thought vector generalizes input and links to a target response. This is not a “feed forward” neural net. It is a recurrent neural net that continually retrains on the training data, which is often the marker of a true “deep learning” system. This model makes no assumption about purpose or predetermined output. It simply reinforces relationships between thought vectors over time. There is a deeply emotional resonance that is carried through conversation. The blurring of lines between social media, search, and messaging will result in a seamless and unified interface for digital technology. Driven by the mobile space's demand for streamlined UI design, we will become more reliant on assistive technologies that can anticipate, learn, and adapt to user input.

Conclusion

It is important for the humanities to anticipate this new cultural space. When the Google autocomplete system was introduced to search, there were many cultural commentators decrying the loss of independ-ent thought and the potential for entrenching damag-ing stereotypes (Postcolonial). The loss of critical awareness and even just the ability to spell. Technolo-gy that offends our sense of what it is to be essentially human is usually the next important media type. Chat-ting with machines tends to cross such lines. There are practical uses for remedial education and composition studies. A functioning Teaching Assistant Bot capable of answering questions about deadlines, assignments, and course policy would be welcome by most educators. Indeed, an AI TA has been developed recently, but it is unclear if this system can be trained on any course material or was custom built for this class (Maderer). Generalizing these systems is a difficult task, to be sure. The newly open sourced Tensor Flow machine learning library can answer questions derived from a training set of just over a million words. When we consider the limits of machine learning in intelligent assistants, scholarly communication through chat interfaces is certainly the next logical step. However these systems require humans in the loop. They require-thoughtful and critical reflection. They require an attention to depth and nuanced meaning. They require a humanist in the loop.

Bibliography

Alphabet. (2011) Google Code Archive. Web. <https://code.google.com/archive/p/word2vec/>.

Apple (2016) “Knowledge Navigator.” 28 October. <https://www.youtube.com/watch?v=HGYFEI6uLy0>.

Bridgwater, A. (2016) “Machine Learning Needs A Human-In-The-Loop.”    7 March.

<http://www.forbes.com/sites/adrianbridgwa-

ter/2016/03/07/machine-learning-needs-a-human-in-

the-loop/#2175c4ba6590>.

Biewald, L. “Why human-in-the-loop computing is the future of machine learning.” 13 Nov. 2015.<http://www.computerworld.com/arti-

cle/3004013/robotics/why-human-in-the-loop-compu-

ting-is-the-future-of-machine-learning.html>.

Carpenter, R. Cleverbot. 28 Oct. 2016. <http://www.clever-bot.com/>.

Carpenter, R. Jabberwacky. 28 Oct. 2016. <http://www.jab-berwacky.com/>.

Railton, S. et al. (n.d.)Faulkner at Virginia. <http://faulk-ner.lib.virginia.edu/>.

Hempel, J. “Facebook Launches M, Its Bold Answer to Siri and Cortana.” 26 Aug. 2015. <https://www.wired.com/2015/08/facebook-launches-m-new-kind-virtual-assistant/>.

Hunt, E. (2016). “Tay, Microsoft's AI chatbot, gets a crash course in racism from Twitter.” 24 March .Web. <https://www.theguardian.com/technol-

ogy/2016/mar/24/tay-microsofts-ai-chatbot-gets-a-

crash-course-in-racism-from-twitter>.

Maderer, J. (2016) “Artificial Intelligence Course Creates AI Teaching Assistant.” 9 May 2016. <http://www.news.gatech.edu/2016/05/09/artificial-intelligence-course-creates-ai-

teaching-assistant>.

Matney, L. (2016). “Siri-creator shows off first public demo of Viv, ‘the intelligent interface for everything'” TechCrunch. 9 May.<https://techcrunch.com/2016/05/09/siri-crea-

tor-shows-off-first-public-demo-of-viv-the-intelligent-

interface-for-everything/>.

Microsoft. (2016) “TayTweets.” Twitter. 28 October. <https://twitter.com/tayandyou>.

Postcolonial Digital Humanities. (2013) “Google's Auto-com-pletion: Algorithms, Stereotypes and Accountability.” 19 November. <http://dhpoco.org/blog/2013/11/19/googles-auto-

completion-algorithms-stereotypes-and-accountabil-

ity/>.

Splotchy (2016). Algebra.com 28 Oct. <https://www.alge-bra.com/cgi-bin/chat.mpl>.
Introduction
Humanities scholarship is becoming increasingly collaborative, participatory, and public facing. As humanists take up digital tools to conduct and share research, larger teams are needed to complete ever more complex computational tasks. When blending these heterogeneous teams--which may include faculty, librarians, staff, undergraduates, graduate students, postdocs, and community contributors--humanists have an ethical responsibility to offer a fair and transparent accounting of research activities. Tracing the evolution of research contributions is necessary for a range of issues facing digital scholarship such as authorship allocation, promotion and tenure, and reports to funders. The allocation of credit and authorship is an increasingly thorny issue for teams with a range of possible roles and a variety of research outputs and media types. There is, however, a large amount of data being generated by these teams that is capable of describing and measuring the contributions made on a variety of platforms and by multiple team member and community partners.

Despite our inheritance of social and collaborative tools, many of these systems elide the nuance and process of humanities based research. Knowledge creation is not merely a function of how much code is produced. New knowledge is often the result of a key insight made by a team of students, staff, and faculty. These insights are generated in a complex and overlapping system of mentorship, service, teaching, learning, and authorship that are deeply dependent, social, and human. With a system that is possible of visualizing the history of a digital project over the course of years, which may see the ranks of team members change over time, primary investigators and project funders will be better able to address often thorny and ethically charged issues relating to student assessment, mentorship, authorship, promotion, and tenure. Credit, promotion, funding, and credentialing are more complex topics than ever, yet many individuals and institutions rely on simple, outdated structures to assess the value of insights made by networked teams.

Social Knowledge Creation
The Penn State Digital Humanities Lab (Penn State Behrend) in partnership with the Teaching and Learning with Technology group (Penn State University Park) has developed a prototype of an ongoing project entitled the Social Knowledge Timeline (sktimeline.net). By linking together popular collaboration tools, the SKTimeline stores, analyzes, and communicates user data in three distinct areas of social knowledge creation:

•    Collaboration Platforms: Many scholars are turning to collaboration platforms like Slack, Yammer, and Basecamp to organize teams and foster communication within teams. These systems use an interface similar to a social media feed to pool project member input into a single narrative and eliminate the need for email. These systems help share documents and support conversations that may lead to drafting manuscripts on Google Drive and other services. These platforms offer a rich, conversational natural language data set that describes how team members mentor and support each other over time.

•    Version Control Systems: Github and Bitbucket are two of the most common version control platforms. These tools help facilitate large programming and encoding projects by allowing multiple coders to work simultaneously. When a team member “commits” code, a commit message describes the nature of the contribution as well as the date and time. This message will offer a highly granular view of coding projects as they unfold. Similarly, by including a feed from Google Drive’s own version control system, document authorship may be traced with similar precision.

•    Social Media: Platforms like Twitter, LinkedIn, and Facebook have proven to be fast paced and engaging areas for social and cultural exchange. Twitter has long been a particularly important site for digital humanists. The SKTimeline draws together multiple hashtags and user handles to frame preserve and contextualize this often ephemeral site of both popular and scholarly debate. Hashtags associated with digital projects, conferences, publications, and even course work can be analyzed and set in real time with other platforms.

Credit allocation in large teams is dependent on our ability to describe, quantify, and visualize our activities. By analyzing the rich natural language conversations generated by teams, the SKTimeline solves these ethical and institutional problems. The appearance of “Collaborators’ Bill of Rights” for digital humanities projects in 2011 is symptom of a need for greater clarity in heterogeneous collaborative teams (Clement et al 2011). The Modern Language Association’s “Guidelines for Evaluating Work in Digital Humanities and Digital Media” are similarly responding to appropriate credit allocation for researchers. There is a need for a more formalized and automated system of data collection and analysis for collaborative researchers across the university.

Machine Learning Contributor Taxonomies
The Taxonomy of Digital Research Activities in the Humanities (TaDiRAH) is used to quantify and describe user contributions. Machine learning systems like Google’s Cloud Platform is used to conduct language analysis, and translation, image recognition, sentiment analysis, and keyword extraction. Custom machine learning systems has also been layered on to these services using the Tensor Flow library to learn the project specific phrasing for contributions. Additional text analysis will be conducted using standard tools like the Natural Language Toolkit (NLTK) to link to TaDiRAH’s defined contributions. This project will reshape authorship and credit allocation in the humanities and beyond, but it will also be a perfect test bed for an emerging set of artificial intelligence tools that are now finding common application throughout society. In this way, the SKTimeline is representative of a broader cultural trend toward AI systems in aiding research.


Figure 1. The Social Knowledge Timeline displaying Slack channels posts and Twitter hashtags chronologically. Images associated with posts are used for backgrounds on the timeline

Conclusion
Undergraduate course projects, ongoing faculty research with graduate researchers, digital humanities labs, and library based digital research projects are just some of the contexts this round of user testing will examine. The data collected on participating teams against interview and form based user surveys. This kind of socially oriented knowledge creation emerges from a community of practice that moves fluidly between curricular experiences and co-curricular research experiences often hosted in DH labs, libraries, and centers. The SKTimeline seeks to solve a critical problem within scholarly communication in a digital context. The SKTimeline offers a means to capture complex narratives that constitute the organic and nuanced unfolding of humanities research.

Bibliography
Alphabet. (n.d.) Google Cloud Platform.

<https://cloud.google.com/products/machine-

learning/>

Alphabet.    (n.d.)    Tensor    Flow.

<https://www.tensorflow.org/>.

Borek,    L.    (n.d.)    TaDiRAH.

<https://github.com/dhtaxonomy/TaDiRAH>.

Clement et al. Eds. (2011) Off the Tracks: Laying New Lines for Digital Humanities Scholars.

Media Commons Press,    (2011).    Web.

<http://mcpress.media-commons.org/offthetracks/>.

Committee on Information Technology (2012)

“Guidelines for Evaluating Work in Digital Humanities and Digital Media.” Modern Language Association. Web.

<https://www.mla.org/About-

Us/Governance/Committees/Committee-

Listings/Professional-Issues/Committee-on-

Information-Technology/Guidelines-for-Evaluating-

Work-in-Digital-Humanities-and-Digital-Media>.

Di Pressi et al. (2015). “A Student Collaborators’ Bill of Rights.” UCLA Digital    Humanities.    Web.

<http://www.cdh.ucla.edu/news-events/a-student-collaborators-bill-of-rights/>.

Python Software Foundation. (n.d.) Python Language Reference, version 3.6. Available at <http://www.python.org>.

Ronacher, A. (n.d.) Flask. <http://flask.pocoo.org/>.
Introduction

Many leading universities in the United States have recognized the profoundly transformative effect that DH has had on research and teaching and have established lab-based research programs or institutional centers for interdisciplinary collaboration on digital projects in the humanities. The University of Virginia led with the Institute for Advanced Technology in the Humanities, which now supports more than forty DH research projects. The Stanford Humanities Laboratory, established in 2001, is a collaborative research environment for crossdisciplinary and multi-institutional, technologically transformative projects. Duke, a founding member of HASTAC (Humanities, Arts, Science, and Technology Advanced Collaboratory), adopted a similar model within its John Hope Franklin Humanities Institute. Other universities such as Harvard, Brown, Dartmouth, Berkeley, Princeton, and the University of Michigan have begun aggressively to hire in the field and to design multidisciplinary DH programs. Moreover, University of Pennsylvania and Yale have attracted large donations from alumnae for DH institutes, while state institutions like Maryland, Nebraska, and UCLA have garnered millions of dollars in external funds to support their field-leading digital scholarship centers.

Digital Humanities scholarship is, by necessity, collaborative and interdisciplinary. DH approaches to the creation and dissemination of scholarship investigate the information life cycle from digitization to preservation including multimediality, design elements, and computational reasoning and implementation through perspectives developed in the context of deep understandings of the key issues at stake in the humanities. DH approaches to teaching typically involve an emphasis on experiential learning and research by creatively expanding modes of access and networks of participation through the methods that students employ. This multidisciplinary work in research and teaching is not limited to conventional humanities departments, but, rather, emerges in every humanistic field, in arts and architecture, information studies, film and media studies, archaeology, geography, ethnic studies, and the social sciences. Because of this essentially multidisciplinary nature of DH work, some of the universities listed above have begun to move away from more traditional models of DH where DH is housed in the library or where DH is primarily an endeavor of the English or History Department. Northeastern University's center for Digital Humanities and Computational Social Science (NULab) and King's College DH Department, for example, bridge the humanities and computational, qualitative social sciences while Bard College's Experimental Humanities Concentration and Initiative is focused primarily on the arts.

Even with all this funding and enthusiasm, however, it remains surprisingly challenging to design curricula in DH on the undergraduate and graduate level. These challenges are a direct result of the fact that attempts to develop new paradigms and models for "training" students break (or stretch) molds for learning and teaching that have calcified over decades into departmental silos and administrative policies concerning credits and teaching loads. These challenges include the difficulty of organizing teaching arrangements that include faculty from multiple disciplines, supporting cluster hires versus supporting "reskilling" faculty to teach new methods from different perspectives, and designing single courses with heavy loads of material including perspectives on the world (such as cultural critique) alongside advanced topics in, for example, statistical models, computational approaches, and data visualization.

This panel will include representatives from five programs who bring perspectives from both private and public, large research universities and small liberal arts colleges in the United States. The panelists have worked within the context of long-standing digital humanities centers as well as within initiatives just now developing without these historical, infrastructural legacies. Each panelist will speak for twelve to fifteen minutes on the below topics, leaving time for questions from and conversations with the audience.

Tanya Clement

For many years, Digital Humanities at the University of Texas at Austin has happened on a project-level basis, without the support of a DH center, in American Studies, Anthropology, Classics, English, French and Italian, History, Information Studies, and in Portuguese and Spanish, among other departments (some projects can be viewed at the UT DH page)

At UT, we are experiencing a scenario that is familiar at universities and colleges that are supported by instantiated DH centers and initiatives. The existing, funded projects at UT do not offer enough project-based, training opportunities for a wide range of students. Project-based work with digital information technologies is often heralded as the site of work that defines DH (Burdick, et. al, 2012; Drucker,

2012; Hayles, 2012; Svensson, 2009, 2012). It is also the site of experiential learning and project-based research in DH on the undergraduate and graduate level, a trend that is noticeable in many of the lessons and curricula collected in the recent Modern Language Association publication, Digital Pedagogy in the Humanities: Concepts, Models, and Experiments. Even with this fabulous array of DH classroom-based assignments, the students who are best trained in DH -- those who on the graduate level are best prepared for the job market in private and public industry as well as the academy -- remain those students who participate in the kinds of long-term, interdisciplinary-team-led projects that have become the mainstay of DH. These kinds of projects, however, usually only have the funding to train a handful of lucky students. At UT, we are working to consider a different paradigm for teaching and training students that is not dependent, primarily, on the soft-money world that remains the status quo for most underfunded DH projects.

To better understand our proposed methods of training, it is important to describe the UT landscape.

The University of Texas at Austin is a large research university with over 3,000 teaching faculty and over 50,000 students. It is rich in multicultural, multimedia, obscure and popular special collections of interest to

humanities scholars. Some examples include the

Gloria Anzaldua archives, the Guatemalan National Police Historical Archive (AHPN), and the Radio Venceremos collection of digital audio recordings of guerrilla radio from the civil war in El Salvador housed in the Benson Latin American Collection. The papers of Carson McCullers, David Foster Wallace, and Gabriel Garcia Marquez as well as many collections of medieval and early modern manuscripts including one of five copies of the Gutenberg Bible in the United States are at the Harry Ransom Center, which also holds a robust collection of film (including the archive of Robert Dinero), photographs (such as the Magnum Photos' New York bureau collection, dating from 1929 to 2004), and authors' recordings including tapes of Anne Sexton's therapy sessions and Spalding Gray's performances alongside the center's other robust collections of twentieth century writers and performing artists. At the Briscoe Center for American History there is the Texas Poster Art Collection which documents the visuals behind the pivotal early music careers (1960s to 1970s) of iconic music legends such as Willie Nelson and Townes Van Zandt as well as over 2000 reels of tape that include the field recordings of folklorists John A. Lomax, William A. Owens, Americo Paredes, and John Henry Faulk.

The list of amazing collections at UT goes on, but in terms of DH, it doesn't matter. Most of these materials, like most of the materials (no matter if they are paper, reels, tape, or photographs) in special collections at many institutions are almost completely inaccessible to DH methods of presentation and inquiry - even when they are digitized. These materials are either under copyright; in non-text formats such as an image, audio, or video file; undescribed (and therefore, in many cases, undiscoverable); or unstructured; all of which make them unsuitable for most DH tools. Instead, UT scholars and teachers, like scholars and teachers everywhere, typically use the same collections that others use: what is freely available online - a trend that systematically limits the kinds of questions we can ask in DH scholarship and teaching.

At UT, we are trying to address these issues in our plans to create an undergraduate and graduate curriculum in DH by considering three primary questions that plague DH curriculum development everywhere:

1. How do we train a wide range of students on the undergraduate and graduate levels across a wide range of DH issues including, but not limited to, the creation of digital collections and archives; the analysis of digital materials; and the use of digital technologies to write, publish, and consume scholarship when our faculty are siloed from each other not only by administrative departments but by their, sometimes, many decades of differing experiences and training?

2. How do we teach students to consider multimedia and multiculturalism when using DH methods when the primary materials to which we have “data ready” access are text-based, in English, and unstructured?

3. How do we create an interdisciplinary framework that is productive and innovative as well as sustainable?

Building on the amazing work done by DH scholars in praxis-based training programs in libraries such as the Scholars Lab at the University of Virginia, Columbia's Developing Librarian Project, and Indiana University's Research Now: Cross Training for Digital Scholarship initiative, the DH@UT initiative is imagining a sustainable model for training students that is experiential, collaborative, and interdisciplinary. Collections are at the heart of humanities research. The work to make such collections “data ready” for DH scholarship coincides with deep expertise in the humanities as it relates to the organization, preservation, curation, analysis, visualization, and communication of digital works in the humanities. Pairing students with projects for making the collections at UT more accessible offers a unique opportunity to train students to generate a more diverse range of data-ready collections and to immerse them in questions surrounding critical information infrastructure studies (Clement, 2015; Liu 2016; Verhoeven 2016) that has engaged staff, undergraduates, graduates, and faculty at the heart of DH research.

Questions remain, however, about how such a program could be implemented. Faculty from a variety of disciplines will have to commit to using UT collections that perhaps do not fit exactly into their research objectives. The time commitments of library and archives staff, who are already often overwhelmed and under resourced, will need to be committed to the goals of the program; and, policies concerning how and when archival materials can cross the transom of the brick-and-mortar collection building (both physically and virtually due to copyright and privacy restrictions) will have to be reconsidered. There is reason to believe that faculty, used to “stretching” their area of expertise to teach classes, would be willing to commit their considerable effort in teaching classes in these new directions. There is equal reason to believe that staff members, committed to the goals of the institution to make their collections more accessible will also be in favor of such a program. Yet, in order to make this program sustainable, both faculty and staff will need support from upper administration. Commitment at the university level for innovative teaching and research remains imperative for changing the praxis of pedagogy. This talk will discuss our progress in these endeavors.

Alison Booth

Technological literacy is a stated educational goal for all students at the University of Virginia, and various existing groups in the Library and schools such as Arts and Sciences support teaching with technology and short assignments in courses (maps; e-portfolios, etc.; Learning Design Technology). But many would agree that digital humanities is something distinct from teaching with technology; there is more to be gained from student participation in open-ended projects (exceeding the timeframe of a single assignment) that reflect upon their tools and methods as well as on the specific data of a discipline. Considerable international discussion of DH pedagogy has advanced the field (Digital Pedagogy), and several textbooks serve courses that introduce students to DH (e.g. Gold and Klein). Certificates in DH (e.g. at Northeastern University and University of Maryland) provide models, as do departments of Digital Humanities (as at King's College, London). CenterNet presents a directory of DH centers on every continent, some like UCLA's Center for Digital Humanities offering a program with an undergraduate minor and graduate certificate.

What are the best models? How should we build an infrastructure for DH education at the University of Virginia, based on what we already have? The University of Virginia has longstanding centers practicing DH: the Institute for Advanced Technology in the Humanities, SHANTI, Scholars' Lab. Advanced research projects and courses or workshops in methods and tools are supported as well in Research Data Services, Data Sciences Institute, Makerspaces in arts, architecture, and engineering, among others. And yet the curricular offerings in DH have remained dispersed among a few academic departments and the Scholars' Lab's Praxis Program and fellowships, along with some digital fellowships for undergraduates working in art and archeology. I suggest a model: the Pedagogical Pyramid, which can be multiplied within one or many institutions in communities of interaction.

Is a Pedagogical Pyramid a menacing structure, an image of hierarchy and exploitative labor? Instead, it is intended as a metaphor and visualization (equilateral triangle in multiple dimensions) of a graduated structure of collaborations, potentially across institutions as well as UVA schools, on advanced research in the humanities, arts, and social sciences: more undergraduates, fewer graduate students, and fewer faculty. The numbers, limited only by practicable scales of collaboration, are flexible, but based on likely proportion of an institution's personnel in these ranks. The paid internships, fellowships, and mentorships proposed in this structure would be available in smaller numbers for team members who are at later career stages, but there will be no idle supervisory roles for those figuratively at the top. Participants would develop a charter in keeping with the Collaborators' Bill of Rights. With sufficient resources for faculty as well as students (stipends, wages, facilitators and spaces), we would run concurrent Pyramids.

We have developed parts of such a vision. IATH supports two residential fellows per year for two years each, with some course release and research funds. The Scholars' Lab under Bethany Nowviskie created a custom-built, annual cohort of six doctoral fellows who collaborate on a project. Praxis flourished under Purdom Lindblad and current members of Scholars' Lab, and has generated a Praxis Network connecting various institutions. Each year in addition there are 23 dissertation fellows in the Scholars' Lab, and we have had graduate fellows jointly in Data Sciences and Scholars' Lab. Undergraduates and graduates are employed in the Scholars' Lab Makerspace, and any students or faculty may work on any projects in that innovative research laboratory. We collaborate with a liberal arts college in Virginia, Washington & Lee, that adopted the Praxis model in its library-centered DH Studio; UVA and W&L hold exchanges among both institutions' Library staff, UVA's Praxis and DH fellows, and W&L's undergraduate students. Scholars' Lab has also led two summer programs for 4-6 Leadership Alliance Mellon Initiative (LAMI) undergraduate students from HCBUs and Puerto Rico, introducing DH and other research methods in preparation for graduate school applications. Collective Biographies of Women, an IATH and Scholars' Lab project, each year trains small groups of graduate research assistants (primarily MA) and a few paid undergraduates; LAMI students each summer have added research on African American and Latino cohorts.

Which brings us to planning the future of a DH community at UVA. Recent developments have focused on coordinating the community in a DH@UVA website and the DH@UVA 2016 conference, at which the open discussion came to a consensus on a certificate or program in DH. We do not want to build a DH department, as that may be less adaptable for future technological and curricular change.

Desirables: early introduction to digital research methods in interdisciplinary courses, workshops, and bootcamps suitable for undergraduates, graduate students, or both; engagement of undergraduates (for credit or pay) in faculty or graduate-student research projects in humanities, arts, and social sciences, beyond the classroom assignment; opportunities for graduate students to mentor and teach these undergraduates; paid graduate interns and project managers mentored by Library-affiliated faculty and staff who can work with faculty projects from many departments; different models of support for faculty digital scholarship beyond the IATH fellowships; expansion beyond the six Praxis students and 2-3 DH dissertation fellows per year that we currently support in Scholars' Lab.

Challenges for building such alliterative pyramids: persuading more faculty to participate and encourage their students in these opportunities; luring the CS faculty and students, the Data Sciences Institute, and the Library's Research Data Services group to collaborate with humanistic computational research, increasing broad interdisciplinarity; coordinating with the Graduate School and other schools to fund graduate fellowships, encourage dissertation advisors' participation, and monitor time-to-degree; securing additional resources for wages and faculty stipends as well as graduate fellowships and teaching release; negotiation with Directors of Undergraduate Studies and Graduate Studies as well as the Graduate School regarding requirements, credit courses and a certificate or program, which would have to be maneuvered through the channels of educational policy at university and state levels. Curriculum would have to be designed that allows discipline-specific units to be added to shared units on generally applicable tools, methods, and issues. The structures would have to ensure that all contributors are compensated and acknowledged, including in presentations and publications. UVA already has ample evidence of the enhancement of research and the advantages in students' learning and placement that come from the existing activities in DH groups. We look forward to giving these aims more substance and structure.

Ryan Cordell

During MLA 2013, Natalia Cecire wryly observed on Twitter, “1. DHers usually don't see dh as panacea. 2. Admins often do. 3. DHers often need for admins to have this erroneous belief.” Our experiences building a graduate DH curriculum at Northeastern in many ways illustrate this rhetorical tension. We benefit from substantial administrative support for curricular ingenuity while struggling to reconcile that support with increasing disquiet in the departments that must underwrite any substantive changes we seek to make.

We are enormously fortunate at Northeastern and the NULab. Our administration has funded several years of cluster hires, which have allowed us to bring DH faculty into the English; History; and Cultures, Societies and Global Studies departments, as well as DH faculty and staff in the library. Over the course of four years we have founded a new center, integrated introductory and advanced DH courses into our curriculum, launched a DH graduate certificate program, and trained many students through work on locally- and grant-funded projects. This in turn has led to an increased number of students applying to our graduate programs seeking DH training.

This rapid, whole-cloth invention of a DH program, however, has been attended by pressures, fissures, and tensions with existing programs. For example, NULab faculty are proud of the robust coursework required for the DH certificate program: the equivalent of 3 courses out of the 10 required in our English MA program or 14 required in our English Ph.D., plus the development of a small scale DH project. Our students take not only an introductory DH course, but also advanced methods courses (data modeling, text analysis, etc.) that prepare them to integrate DH methods into their theses and compete for DH positions after graduation. Within English, however, completing this requirement requires students to decide their path almost immediately upon admission, and the decision to pursue the certificate dictates very particular paths through the larger Ph.D. program. While our DH faculty are a larger group than at most institutions, even so we cannot practically mount more than two courses per year: an introductory course each fall and an advanced course each spring. These advanced courses rotate among NULab faculty and thus have very distinct foci. Thus students' options for completing coursework remain relatively constrained over two years of full time coursework in ways that sometimes mitigate against the particular training individual's need. A student primarily interested in digital archive creation, for instance, might by necessity take their advanced course in Humanities Data Analysis rather than Data Modeling; while the latter would be more appropriate to their interests it can only be offered every three years or so, when a particular faculty member is on rotation for the advanced seminar.

These pressures are compounded for MA students in English or Public History; in the latter case we find there is really only one viable set of courses that can result in both a DH certificate and Public History credential within the timeline of the program. Due to these challenges, NULab faculty are currently reevaluating how to align our high expectations for DH training with the practical realities of a certificate program, which must exists alongside and in harmony with the primary curricular structures of humanities departments.

In addition to pressures on students, the popularity of the DH certificate among its first two cohorts of students has led to growing worry among departmental faculty that DH is driving down enrollments outside certificate program courses. We might be tempted toward market explanations (“we cannot dictate which courses students are interested in” or, less generously, “if our colleagues made their courses more enticing”) but these are neither sufficient nor reflective. The NULab has created a certification that students perceive as necessary in a competitive job market, despite ambiguity about the role of DH in securing jobs (Risam 2013). Thus we have institutionalized a hierarchy of graduate course offerings that does privilege DH courses over others in the curriculum, in ways that partially reflect students' interests but partly reflect their anxieties. Moreover, the administration's vision for graduate education in the future clearly emphasizes digital humanities in ways that worry even NULab faculty. We cannot, in other words, entirely dismiss our colleagues' worries about how digital humanities, which belongs to no department in particular, has shifted the character and priorities of graduate programs in the particular departments of English and History.

In my presentation, then, I will think through what constitutes a successful DH graduate curriculum in an institutional culture of abundant top-down support and atrophying bottom-up enthusiasm. Can we structure robust DH training in ways that integrates with rather than competing with departmental training? Can a DH program be partner rather than usurper?

Miriam Posner

UCLA's Digital Humanities graduate certificate, founded in 2011, now enrolls 24 Ph.D. and master's degree students from across the university. Initially, the certificate was conceived as a means of providing an imprimatur for work that was already taking place at the graduate level. UCLA's humanities graduate students were already apprenticing on a wide range of faculty-led digital humanities projects, such as HyperCities and RomeLab, and developed a great deal of discipline-specific expertise through these experiences. The university, moreover, has a strong community of faculty DH practitioners and an established tradition of integrating graduate students into projects as collaborators. But as formal DH curricula grew at other institutions, UCLA graduate students and faculty began to feel that a formal credential might be useful to graduates as they entered the job market.

Even as students clamored for it, the introduction of an official graduate certificate has also had the effect of surfacing some challenges and dilemmas for graduate education in the digital humanities: how much of the graduate curriculum should be formal, and how much should come in the form of project work; how to provide the time- and resource-intensive instruction graduate students require; how to help students balance traditional dissertation work with digital work; how to accommodate the very distinct needs of Ph.D. students and professional master's degree students; and how to prepare graduate students for an unpredictable job market. Alexander

Reid identified many of these dilemmas in a 2012

essay for Debates in the Digital Humanities, observing

an uptick in digital humanities activity and arguing

that we would soon witness a widespread shift in the education of scholars, toward an understanding of digital literacy as fundamental to graduate training

(Reid 2012).

From the vantage of 2016, the picture seems less clear. Digital humanities continues to thrive as a field, but we have yet to see the searching, widespread reevaluation of graduate education that some observers expected. While a number of standout programs, such as the City University of New York's

Graduate Center, the University of Virginia's Scholars' Lab, and the University of Victoria's Electronic Textual Cultures Laboratory, have seemed to forecast change on a larger scale, most humanities graduate programs still deal only gingerly, if at all, with digital technology.

The example of UCLA might help to illuminate some reasons for this very piecemeal rate of change. UCLA's graduate students, like most graduate students, are under enormous pressure and feel pulled in multiple directions by an erratic and whimsical job market. Assailed by advice to publish in top journals on the one hand, and to develop digital skills on the other, they often come to the DH program ready to perform a cost-benefit calculation about how this training will position them on the job market -- not exactly the spirit of embracing failure and creative experimentation that many DH experts advise (Ramsay 2010, Drucker 2009, Sample 2012). Devising a curriculum that makes sense for them in this climate, then, is constantly demanding and resource-intensive. Among the questions UCLA faculty has faced:

•    Should a digital humanities program for graduate students emphasize collaborative scholarship, as many practitioners advise, or

should students' DH work advance the

individual dissertation?

•    What is the program's responsibility toward preserving and archiving student digital work, and particularly digital dissertations?

•    If a graduate DH program remains interdisciplinary, how can it assemble and retain the necessary core faculty to staff the program?

•    How can an interdisciplinary program retain a “center of gravity” sufficient to enable graduate students to feel as though they are part of a community?

•    How can a graduate DH program communicate its value to students' advisers, many of whom do not engage in digital work themselves?

•    Given the highly individualized nature of dissertation-level work, how can graduate DH programs provide sufficient resources (staff time and technical assets) to help students advance their research meaningfully?

•    How should a graduate program in DH balance the distinct needs of professional master's degree students (in UCLA's case,

MLIS students) with Ph.D. students?

In this presentation, I will discuss the ways in which we at UCLA have attempted to develop a graduate curriculum that makes sense for a program that faces challenges familiar to most universities: lack of resources, little centralized support, and overtaxed faculty. I will also raise some questions about the sustainability of a digital humanities graduate curriculum without answering some searching and difficult questions about what a graduate program should be and do. Finally, I will propose some infrastructural and institutional solutions to help address some of the most pressing needs of graduate DH programs.

Maria Sachiko Cecire

At Bard College, we don't have a formal Digital Humanities center. Instead, we have an interdisciplinary curricular initiative and hub for faculty collaboration that we call Experimental Humanities (EH). DH scholarship at its most visible typically creates and employs digital tools to pursue project-based humanities research. While EH does some of this, our program was designed to align with the mission of our undergraduate-focused institution, and to be flexible enough to bring together faculty from diverse intellectual and personal backgrounds. We say that Experimental Humanities is Bard's liberal arts-driven answer to the Digital Humanities: it uses a network of courses and faculty-identified research clusters to variously interrogate how technology mediates what it means to be human. EH engages with media and technology forms from across historical periods, including our own, and combines experimental research methods with critical thinking about way media and technology function as a part of cultural, social, and political inquiry. We encourage the reconsideration of older media in light of today's technologies, and look ahead to the developments on the horizon.

The decision to name our program Experimental Humanities instead of Digital Humanities was grounded in the unique character and history of Bard College, which has developed an international reputation for its commitment to the arts, humanities, and the notion that access to a liberal arts education should be a fundamental human right. Although our primary campus in the Hudson Valley of New York serves just over 2000 students, Bard has a much wider reach that includes degree-granting programs in state prisons, early college programs at high schools that serve low-income youth in cities around the U.S., and in international university partnerships that bring liberal arts curricula to countries such as Kyrgyzstan, Palestine, and Russia. In this context of passionate liberal arts advocacy, we wanted a title that would leave room for DH but not exclude non-digital forms of artistic and scholarly production. As Wendy Chun has suggested, terminology that draws hard lines between “old” and “new” technologies runs the risk of excluding (or at least seeming to exclude) the lessons, theories, and knowledge of the past from the practice and study of “new” media. The notion of the “experimental” embraces both digitality and previous moments of technological change, invokes the practices of both the sciences and the arts, and has the kind of hands-on and countercultural associations that align with the Bard ethos.

We saw the rise of DH as an exciting opportunity to establish a program dedicated to reconsidering the methods and subjects of humanistic study in the light of changing material conditions. This is an ongoing project, and one that also allows us to continuously reevaluate our pedagogical approaches: rethinking which tools and methods we use in the classroom and encouraging in-class reflection with our students about the relationships between who we are, what we study, and how we study it. For instance, when faced with N. Katherine Hayles's work on hyper and deep attention, students may put forward arguments for practicing deep attention in their coursework or advocate for a pedagogy that introduces more multimediated and hands-on content, thereby creating the opportunity for jointly designed assignments. Finally, in keeping with the concerns raised by #transformdh (see, for instance, Moya Z. Bailey's essay “All the Digital Humanists Are White, All the Nerds Are Men, but Some of Us Are Brave”) we wanted to keep cultural critique and questions of inclusion central to what we do as humanities scholars.

With these ideals in mind, we set out to design a program that would be critical, inclusive, undergraduate-focused, and also able to participate in wider DH networks. In their essay about whether or how small liberal arts colleges might “do” DH, Bryan Alexander and Rebecca Frost Davis outline several challenges to establishing DH programs and centers at institutions like Bard. They note a lack of infrastructure to support major research projects, the difficulty of pulling together the human resources to do work that requires a wide range of skills, our limited access to graduate students that can sustain long-term research, and the pedagogical focus at SLACs. However, they argue that models that include curricular elements and partner with existing campus resources like library and IT can still be successful, developing proficiency in select project areas and sending students on to DH graduate programs.

Experimental Humanities does work closely with Library/IT and encourage faculty projects through training opportunities, the guidance of a Digital Projects Coordinator with a PhD in the humanities, and the support of a student Media Corps. But while several successful DH programs at other small liberal arts colleges have grown out of the library, like Occidental's Center for Digital Liberal Arts, or out of faculty research, as with Hamilton's Digital Humanities Initiative, EH was from its first imaginings a primarily curricular initiative, built on the three pillars of history, theory, and practice. All EH students take the core courses “A History of Experimentation” and “Introduction to Media,” which EH faculty rotate teaching, and at least one practice-based course beyond the college arts requirement (this may include Computer Science or one of the visual, written, or performing arts). Students also take at least two more courses from the wide offering of EH-listed courses designed by faculty according to their research interests, and which are available each semester in fields from Music and Anthropology to Medieval Studies and Theater. These classes present the hands-on projects and research that they do at collective Share Events each semester.

At an administrative level, EH is a concentration (like a minor; Bard loves to have its own terminology for everything), which means that our students pair their coursework with a foundation in a major program of study, and that EH faculty also belong to a home program. All Bard students do yearlong senior projects, which allow our students to bring what they have learned in the concentration into conversation with their major discipline in a capstone project. We have seen senior projects that use topic modeling to analyze slave narratives, develop gaming apps with the potential to treat psychopathy, push the boundaries of traditional interview-based ethnography to consider the social implications of conversing via text message, delve into the history of the book and other media forms, and lead to immersive art installations in both digital and analog formats. Our students do not necessarily go into DH programs (though some do), but rather become curators, teachers, librarians, programmers, artists, and work for non-profits.

This kind of breadth makes Experimental Humanities sustainable even on a small campus, providing a hub for a range of interests and methods.

Meanwhile, our regular course and event offerings give the program continuity and create opportunities for faculty and students to meet during the semester as a self-identifying community. We have also worked to bolster faculty research in recent years, beginning with the launch of our faculty-led, topic-based clusters in 2014. The clusters have been very successful in bringing faculty together to share and further their own research across disciplinary boundaries, and have yielded a number of new courses and given rise to a form of experimental symposia that bring together scholars, artists, practitioners, students, and community members around cluster topics such as “Sound” and “Surveillance.” The clusters have become a model in the college for how to foster interdisciplinary collaboration that encourages both new research and student engagement. Other research models in EH include faculty-led humanities labs around individual projects, intensive student sessions with historical societies to create digital repositories and interfaces for the public to access its local history, and courses with embedded digital projects that allow faculty to work with undergraduates to build up layers of data each time that they teach the course.

EH is now in its fifth year, and coming to the end of a three-year grant from the Mellon Foundation. In my presentation I will discuss several of the challenges that we face at this crucial stage of transition. These include the ongoing struggle to define what we do and why it's useful to students, parents, and future employers, given our capacious title and mission; how to practically negotiate the need for disciplinary foundations in our students' and faculty's home programs and the invitation to experiment in EH courses and projects; how to move off of a major grant to become sustainable within the existing college structure (and thereby resist the kind of dependence on external grants that is contributing to the neoliberalization of the humanities, as outlined in the widely circulated “Dark Side of the Digital Humanities” papers); and how to better integrate the wider Bard network of underserved high school and undergraduate students both in the US and around the world into the work that we do.

Bibliography

Alexander, B., and Davis, R. F. (2012). “Should Liberal Arts

Campuses Do Digital Humanities? Process and Products

in the Small College World,” in Debates in the Digital

Humanities, ed. Matthew K. Gold http://dhdebates.gc.cuny.edu/debates/text/89

Bailey, M. Z. (2011) “All the Digital Humanists Are White, All the Nerds Are Men, but Some of Us Are Brave,” Journal

of Digital Humanities 1.1

http://journalofdigitalhumanities.org/1-1/all-the-

digital-humanists-are-white-all-the-nerds-are-men-but-

some-of-us-are-brave-by-moya-z-bailey/

Burdick, A., Drucker, J., Presner, T., Schnapp, J., and Lunenfeld, P. (2012) Digital_Humanities. Cambridge, MA: The MIT Press.

centerNet (n.d.). “centerNet: An international network of digital    humanities    centers”.    Web.

http://dhcenternet.org/centers

Chun, W. H. K. (2006) “Introduction: Did Somebody Say New Media?” in New Media, Old Media: A History and Theory Reader, ed. Wendy Chun and Thomas Keenan (New York: Routledge), 1-10.

Chun, W. H. K., Grusin, R., Jagoda, P., and Raley, R. (2016)“The Dark Side of the Digital Humanities,” in Debates in the Digital Humanities, ed. Matthew K. Gold .

http://dhdebates.gc.cuny.edu/debates/text/89

Clement, T. (2015) “The Information Science Question in

DH Feminism.” Digital Humanities Quarterly, vol. 9, no. 2.

Drucker, J. (2012) “Humanistic Theory and Digital Scholarship.” In Debates in the Digital Humanities, edited by Matthew K. Gold, Minneapolis: University Of Minnesota Press, pp. 85 - 95.

Drucker, J. (2009). SpecLab: Digital Aesthetics and Projects in Speculative Computing. Chicago: University of Chicago Press.

Gold, M. and Klein, L., eds. (2016). Debates in Digital Humanities. Minneapolis: University of Minnesota Press.

Hayles, N. K. (2012) How We Think: Digital Media and Contemporary    Technogenesis.    Chicago; London:

University Of Chicago Press.

Hayles, N.K. (2007). “Hyper and Deep Attention: The Generational Divide in Cognitive Modes,” MLA Profession

(2007): 187-199.

Liu, A. (2016) “Drafts for Against the Cultural Singularity” Alan Liu. 2 May 2016.

Modern Language Association. (n.d.) “Digital Pedagogy in the Humanities: Concepts, Models, and Experiments”.

MLA    Commons,.    Web.

https://digitalpedagogy.commons.mla.org/

Off the Tracks (2011). Collaborators' Bill of Rights. Media Commons Press. Web.    http: //mcpress.media-

commons.org/

Ramsay, S. (2010). “The Hermeneutics of Screwing Around; or What You Do with a Million Books.” (2010). Unpublished presentation delivered at Brown University, Providence, RI 17 (2010): n. Pag. Web. 17 Oct.

2012.

Rector and Visitors of the University of Virginia. (2017).

DH@UVA. Web. http://dh.virginia.edu

Rector and Visitors of the University of Virginia. (2008)

“Sciences, Humanities and Arts Networked Technology Initiatives,” SHANTI at the University of Virginia. . Web. http://shanti.virginia.edu/

Reid, A. (2012).“Graduate Education and the Ethics of the Digital Humanities.” Debates in the Digital Humanities. Ann Arbor: University of Michigan, Print.

Risam, R. (2013) “Where Have All the DH Jobs Gone?” Roopika Risam 15 Sept. 2013. Web. 1 Nov. 2016.

Sample, M. (2012) . “Notes towards a Deformed Humanities.” samplereality. N.p., 2 May 2012. Web. 29 Oct. 2016.

Svensson, P. (2012) “Envisioning the Digital Humanities” Digital Humanities Quarterly, vol. 6, no. 1.

Svensson, P. (2009) “Humanities Computing as Digital Humanities” Digital Humanities Quarterly, vol. 3, no. 3.

The Leadership Alliance (2016). “Mellon Initiative” . The Leadership    Alliance.    Web.

http://www.theleadershipalliance.org/programs/sum

mer-research/mellon-initiative

UCLA Center for Digital Humanities. (n.d.) “UCLA Digital Humanities”. Univeristy of Claifornia Los Angeles. Web. http://www.cdh.ucla.edu/

University of Virginia. (n.d.) Institute for Advanced Technology in the Humanities. Web. http://iath.virginia.edu/

University of Virginia College and Graduate School of Arts and Sciences (n.d.) “Learning Design Technology, Arts and Sciences”. University of Virginia.. Web.

http://learningdesign.as.virginia.edu/

University of Virginia Library. (n.d.) Scholars' Lab,. Web.

http://scholarslab.org/

Verhoeven, D. (2016) “‘As Luck Would Have It.” Feminist Media Histories, vol. 2., no. 1. pp. 7-28.

Washington and Lee Digital Humanities Action Team

(n.d.). “DH @ W&L”. Washington and Lee University. Web. http://digitalhumanities.wlu.edu/
The Lizzie Bennet Diaries is a digital update of Jane Austen's novel, Pride and Prejudice, in which Austen's narrative is reimagined for the twenty-first century through its distribution across multiple media platforms. The Lizzie Bennet Diaries (hereafter referred to as LBD) is centred on a series of YouTube video diaries by Lizzie Bennet and includes four complementary YouTube channels, thirteen interconnected Twitter feeds, Tumblr posts, Facebook profiles, and numerous social media interactions and ‘conversations' between the narrative's characters and its readers.

This poster expands upon existing research conducted as part of a larger, ongoing PhD research project. It will present a visual overview of LBD and the specific modes of participation available to readers during the narrative's initial release in 2012 and 2013 (an important distinction, since the narrative was originally released as a serial story). Drawing from a sample of LBD reader comments on YouTube and Twitter (using MaxQDA to qualitatively code the comments), the poster will explore how readers participated in the LBD narrative and how their participation may have influenced or affected their reading habits. In addition, Aarseth's cybertext theory and McGann's radial reading theory will provide a foundation for discussing LBD's participatory elements in a theoretical context, with an emphasis on discussing how digital media have invited us to revisit and rework those theories.

According to Aarseth, cybertexts such as LBD invite and encourage readers to make deliberate and intentional choices as they navigate through the text, its multiple entry points, and its various narrative paths (1997). As a result, readers actively participate in shaping their individual reading experience. Thus, LBD reader must decide which elements of the narrative to consume, and in which order: does she choose to follow Lizzie on Twitter, but not Darcy? Does she watch Lydia's YouTube videos, or reblog Jane's fashion posts on Tumblr? Each choice leads the reader in a slightly different direction in the narrative, and provides additional information that offers context and meaning to the core series of YouTube videos. The choices made by readers are also indicative of McGann's theory of radial reading, where readers seek out additional information not immediately available in the core text (McGann 1991). The various modes of participation available to LBD readers, as well as the degree of interactivity implied by cybertext theory and radial reading, gives readers an avenue for active engagement with the narrative and helps reinforce the belief that their contributions to the narrative matter. This belief, what Professor Stephen Coleman calls “the feeling of being counted, or the affective character of an experience that renders it fulfilling for individuals” (2013), serves to strengthen a reader's engagement with the text.

Using Aarseth and McGann's theories as a foundation, the poster will consider how LBD's modes of participation might strengthen a reader's engagement with and immersion in the text, and whether immersive digital narratives like LBD can encourage engagement with traditional close reading techniques by prompting individuals to read (or re-read) Austen's original Pride and Prejudice novel. As Frank Rose, Henry Jenkins and others have pointed out, the Internet has changed readers' expectations from stories and narratives, to the point that many readers of digital narratives now expect some level of participation or interactivity. This poster (and the larger research project) is focused on exploring those participatory elements and their connection to overall digital reading practices.

Bibliography

Aarseth, E. (1997). Cybertext. Baltimore: Johns Hopkins University Press.

Coleman, S. (2013). How voters feel. Cambridge:

Cambridge University Press.

Dowling, D. (2014). “Escaping the Shallows: Deep

Reading's Revival in the Digital Age.” Digital Humanities Quarterly, [online] 8(2). Available at:

http://www.digitAlhumanities.org/dhQ/volZ8Z2/

OOO18O/QOO180htmi [Accessed: 27 Oct. 2016].

Jenkins, H. (1992). Textual Poachers. New York:

Routiedge.

McGann, J. (1991). “How To ReaD a BooK” in The Textual Condition. Princeton: Princeton University Press.

Pemberley Digital. (2016). The Lizzie Bennet Diaries. [online]    Available    aT:

Http://www.pEMBERLEYDiGiTAL.coM/THE-LizziE-

BenneT-Diaries/ [AccesseD: 27 OcT. 2016].
As a social media platform, Twitter (twitter.com) offers opportunities to examine public reactions in what could be described as the world's largest village square. Twitter provides an opportunity not only to participate in public discourse, but even to shape it. However, to fully understand this discourse, it is helpful to understand the people behind the discourse. Stylometric authorship profiling (Argamon, et al, 2009) provides an example of this. People are free to post their opinions, but by analyzing the writing style of the individual tweets, we can infer other attributes of the individual authors.

In this study, we infer personality categories using the well-known Myers-Briggs Type Inventory (MBTI) to analyze whether or not there are any differences between the supporters (and detractors) of the major party candidates, Hillary Clinton (D) and Donald Trump (R). The MBTI categorizes people along four major axes, as encoded in a four-character summary (for example, ENFJ: Extroverted, iNtuitive, Feeling, Judging). We have shown (Gray and Juola, 2011; Juola et al., 2013) that personality can be inferred with high accuracy from writing, and further that MBTI personality types can be gleaned specifically from Twitter feeds. Using The EthosIO system developed by Juola & Associates (www.ethosio.com), we have applied this (Juola, Vinsick, and Ryan, 2016) to large-scale analyses of the demographics of personality on Twitter, finding substantial differences between the accepted distribution of personality in the general US population and between the distribution of personality among active Twitter participants. For example, introverts make up approximately 50% of the general US population, but nearly 80% of active Twitter users. Similarly, nearly 3/5 of the general US population are "sensing" (S) [as opposed to "intuitive" (N)], but half or fewer Twitter users are. Two specific subgroups, INFP and INFJ, are vastly overrepresented on Twitter, being only about 5% of the overall US population, but 30% or more of the samples gleaned from Twitter.

We extend this to analyzing personality differences between politically disparate groups of Twitter participants. As with (Juola, Vinsick, and Ryan, 2016), we harvested a large group of user names from the Twitter sample feed, selecting users whose public tweets included one of several political hashtags. Based on the hashtags seen, we divided participants into four groups: anti-Clinton (identified by one or more of

'#NeverHillary', '#CrookedHillary', '#WhichHillary', '#DraftOurDaughters', '#hillary4prison', '#hil-lary4prison2016', '#StopHillary', '#CrimeWithHer', or '#Killary'); pro-Clinton ('#ImWithHer', '#Clinton', '#ClintonKaine16', '#ClintonKaine2016', '#Hil-laryClinton'], '#ClintonKaine', '#WhyIWantHillary', '#HillarysArmy', or '#Hillary2016'); anti-Trump ('#NeverTrump', '#dumptrump', '#trumptaxreturns', '#dontvotefortrump', '#dumpthetrump', '#boy-cotttrump', '#trumpsexism'); and pro-Trump ('#Im-WithYou', '#TrumpTrain', '#MakeAmericaGreatA-gain', '#TrumpPence16', '#TrumpPence2016', '#Trump', '#AltRight', '#VoteTrump', '#TeamTrump') This gave us approximately 600 user names for each of the four groups in our preliminary dataset. These user names were submitted to the EthosIO personality analyzer to produce distributional data for each subgroups.

We therefore had in our preliminary corpus 651 pro-Clinton subjects, 587 pro-Trump subjects, 635 anti-Clinton subjects and 639 anti-Trump subjects, for a total of 2512 total user names, divided across 16 MBTI categories (details in full paper). As expected from previous work, the overall statistics do not match US demographics; for example, types ISFP and INFP are strongly overrepresented in all samples, as are introverts in general. Our interest, however, is in whether or not political differences also show up as personality differences as well. In plainer language, does the average Clinton supporter have a different personality than the average Trump supporter?

We tested this hypothesis with a variety of chi-squared tests (df=15 throughout). At the most basic

level, we found extremely significant differences (p ~

10A-9) between Clinton supporters and Trump supporters. We also found significant differences (p ~ 10A-12) between "Democrats" (either pro-Clinton or anti-Trump) and "Republicans" (pro-Trump and antiClinton). Examining cells in detail suggest that ISFJ and ISFP are both overrepresented among Democrats while ESTJs and INFJs are overrepresented among Republicans.

By contrast, there was no significant difference (p > 0.10) between "Anti" and "Pro" subjects, despite the possible participation, for example, of third party supporters who are opposed to both Trump and Clinton. Similarly, we found no significant difference between pro-Trump and anti-Clinton subjects, or between antiTrump and pro-Clinton subjects, suggesting that other factors than personality are affecting whether a person chooses to self-express in favor of a particular candidate or in opposition to that candidate's rival(s).

In this study, there are a number of potential confounding factors, the effects of which have not yet been assessed. The first is simply the presence of active attempts to manipulate the dialogue, for example, through the use of automated 'bots' (Kollanyi, Howard, and Wolley, 2016), or simply through the use of "sock puppets," multiple identities in an attempt to create a an appearance of consensus and of larger margins. A second factor is the issue of overlapping categories. Approximately 20% of our preliminary "anti-Trump" sample also self-identified as "pro-Clinton," and similarly, approximately 20% of the anti-Clinton sample self-identified as pro-Trump. More counterintuitively, approximately 5% of the anti-Clinton sample also identified as anti-Trump, and approximately 5% of the pro-Clinton sample was also pro-Trump. This may be due to a third confounding factor, the inability of simple keyword spotting to identify the use of irony (for example, in posting a link to an article highly critical of Trump's campaign and using the '#Trump' hashtag to draw attention to it). As we continue this analysis, based in part on data to be collected during the final and most intense week of the campaign, we will address these issues (and discuss our methods of address in the final paper).

We have therefore shown, using text analysis of Twitter on a moderately large scale, that there are significant differences between the types of people who self-identify as supporters (and opponents) of one of the major candidates in the 2016 US presidential election. We have also shown that there does not appear to be significant personality-related differences between whether one supports one's chosen candidate or opposes the other one. We have also confirmed the previous results (Juola, Vinsick, and Ryan, 2016) about the general distribution of personality types on Twitter, and hasten to point out that the differences we have identified are still relatively minor and that the overall distribution of personality types in both camps are broadly similar to the distribution of personality types on Twitter in general. However, our results show, first, that, in keeping with prior work, inferring personality type via Twitter is practical and useful. Second, they show that personality may play a factor in the selection of one's chosen candidate.

Finally, the question of "who are Trump's voters?" against "who are Clinton's voters?" will no doubt interest historians for decades. Our results provide some insight into possible psychological motivations in addition to the more traditional social, political, and economics reasons, and may therefore enrich future discussion and scholarship.

Nota bene
This version of the paper was written approximately one week before the actual 2016 election and will be updated as appropriate.
Resumen
En las universidades iberoamericanas las Humanidades Digitales están construyendo un campo

emergente con diversas iniciativas a nivel institucional. Las instituciones de educación superior poseen características diversas que las distinguen por sus recursos humanos y financieros, sus infraestructuras físicas, tecnológicas y administrativas, sus modelos educativos y sus programas de estudio. Esta heterogeneidad genera condiciones diversas para la consolidación del campo. Sin embargo, aunque los contextos varíen, nos encontramos con elementos comunes en esas estructuras institucionales que obstaculizan el desarrollo de las Humanidades Digitales y, a la vez, pliegues o espacios liminales que posibilitan su avance. Este panel tiene el propósito de a) presentar un marco de referencia general sobre las HD en Iberoamérica, b) realizar un análisis de los desafíos como campo y un balance de los factores determinantes para el avance de las HD en español, c) identificar las estrategias que han sido aplicadas para la expansión de las HD en la región y, d) proponer acciones en red y mecanismos dentro de las universidades que permitan la consolidación del campo, a través de la práctica docente, la investigación, el desarrollo de capacidades, la formación de recursos humanos y la vinculación social de las Humanidades Digitales en el marco de los principios de una cultura digital crítica y libre. Descripción

En Iberoamérica existen instituciones de educación superior con características diversas que las distinguen por sus recursos humanos y financieros, sus infraestructuras físicas, tecnológicas y administrativas, sus modelos educativos y sus programas de estudio. Esta multiplicidad de contextos genera dinámicas y espacios diferenciados para la consolidación de las Humanidades Digitales en la región. Dentro de este contexto institucional variado, las HD en español están construyendo un campo emergente que posee características que lo distinguen de las experiencias en otras regiones y otras lenguas. Sin embargo, al abrir un línea de trabajo nos encontramos con múltiples elementos de esas estructuras institucionales que obstaculizan su avance. La formación de profesionales y el desarrollo de proyectos digitales se enfrentan a una serie de problemáticas dentro de las que se encuentran decisiones administrativas, el desconocimiento sobre el campo, las carencias en infraestructura o de mecanismos para que las universidades incluyan nuevas temáticas en el currículo a nivel de licenciatura, posgrado o como líneas de investigación.

Sostenemos que las instituciones cuentan con una serie de elementos estructurales que, a su vez, forman parte de un sistema de educación pero también de producción de conocimiento, por ello enumeramos aquí algunos de los elementos externos e internos del contexto en el que buscamos consolidar las HD en Iberoamérica con el fin de presentar un diagnóstico y una propuesta que permita a las instituciones tomar decisiones estratégicas si buscan transitar a un nuevo paradigma.

•    Administración: las instituciones cuentan con un aparato administrativo que, en ocasiones, representa un obstáculo para el desarrollo de nuevas propuestas. El planteamiento de nuevos contenidos y conocimientos debe sujetarse a la estructura administrativa, lo cual puede permitir o impedir el desarrollo de las HD.

•    Evaluación de la producción académica: aunque no es un problema nuevo, la presencia de medios digitales complejiza la valorización de los productos intelectuales, sólo tangibles en productos ya validados, como los libros o los artículos, sin embargo, hoy es posible desarrollar productos en soportes y formatos diferentes para los cuales no existe aún un mecanismo de validación o el que existe no ha sido adaptado para estas nuevas formas de producir conocimiento.

•    Modelo educativo, planes de estudio y propuesta pedagógica: los distintos modelos educativos, planes de estudio o propuestas pedagógicas, sea cual sea el programa o nivel académico, obedecen a la división tajante del conocimiento en disciplinas, ello impide en la mayoría de los casos, impartir asignaturas que integren nuevas visiones pedagógicas, formas de trabajo, objetos de estudio, teorías y métodos de investigación o incluso romper con la compartimentación disciplinaria y la ampliación de entornos de aprendizaje

•    Comunidad y cultura digital: la formación de profesionales depende no sólo del interés de los docentes, sino también de los propios estudiantes. Ambos grupos reconocen la necesidad de adquirir nuevos conocimientos y competencias digitales pero en muchas ocasiones únicamente desde un enfoque instrumental, lo cual no implica una reflexión sobre el impacto de medios digitales y de internet en el desarrollo de sus disciplinas.

•    Infraestructura tecnológica: el acceso en términos de conectividad, hardware y software.

•    Transdisciplinar:    resultado de la

fragmentación y especialización disciplinar dentro de las estructuras universitarias, el trabajo colaborativo interdisciplinar y transdisciplinar es aceptado con cierta reticencia, dado que su desarrollo debe apegarse a las estructuras académicas y administrativas ya establecidas.

•    Financiamiento: la investigación requiere de recursos económicos para la realización de proyectos pero también para la formación de los humanistas. Los aspectos mencionados se encuentran dentro de un contexto más amplio que reafirma la división estricta entre disciplinas, lo cual incide en el financiamiento para proyectos digitales desde la perspectiva de las HD. El acceso a recursos internacionales es limitado puesto que requiere de redes de colaboración y capacidades con las que no necesariamente cuentan los investigadores. Políticas públicas.

A pesar de que esta estructura institucional no siempre es favorable para el avance de las HD, se han desarrollado iniciativas en las universidades que van desde la organización de eventos de HD, la publicación académica de revistas especializadas, la creación de plataformas, la vinculación con otras iniciativas en el marco del movimiento de acceso abierto (MOOC, bases de datos, bibliotecas digitales, Wikipedia); hasta la implementación de laboratorios como parte de la estrategia para abrir o consolidar la práctica de las HD. El panel sobre Humanidades Digitales en Iberoamérica esbozará un estado de la cuestión y, a través de las diversas experiencias institucionales de los panelistas, buscará identificar los principales desafíos, prácticas exitosas y propuestas para generar mecanismos dentro de las universidades que permitan avanzar en el desarrollo del campo, la práctica docente, la investigación, el desarrollo de capacidades, la formación de recursos humanos y la vinculación social de las Humanidades en el marco de una cultura digital crítica y libre.

Introducción al panel
El panel muestra diversos intereses, aunque con un objetivo común: conocer las estrategias que han sido aplicadas para la expansión de las HD en Iberoamérica. Aunque existen algunos mapas y registro de proyectos, nuestro interés es entrar a profundidad en el análisis de algunos factores que consideramos determinantes para el avance de las HD en español. Como punto de partida, el panel presenta un marco de referencia general sobre las HD en Iberoamérica como introducción a los casos y cierra con un balance de la situación y los desafíos como campo con el propósito de identificar continuidades y fortalecernos a través de estrategias compartidas.

El panel aborda estudios de casos relevantes que reflejan la labor de las instituciones Iberoamericanas en las se han desarrollado proyectos e iniciativas para extender las HD, que han generado ya redes de colaboración y un sentido de pertenencia y reconocimiento mutuo. Este panel como resultado de esas redes de cooperación interinstitucional e internacional busca consolidar una apuesta conjunta, generar un espacio de discusión abierto y dar visibilidad al trabajo heterogéneo que se realiza en Iberoamérica. Si bien no se mapean todos los casos de HD en Iberoamérica, hay una voluntad explícita de incorporar, reconocer y rescatar las iniciativas de la región. Esperamos que a partir de la oportunidad de una discusión pública, podamos, en un futuro próximo, integrar académicos y experiencias de otras instituciones y países.

Desde Iberoamérica la comprensión de las HD es más amplia que en el mundo anglosajón, lo que muestra también un desafío en términos académicos, administrativos y de práctica de las HD. Consideramos relevante destacar nuestros contextos específicos de acción para luego identificar las estrategias más efectivas para avanzar en el campo, no únicamente a través de proyectos, sino a través de la formación de estudiantes, el desarrollo de competencias entre los investigadores y el incentivo de formación a futuro de una infraestructura colaborativa. Sostenemos que es necesario conocer el estado de la cuestión desde los aspectos más macro hasta los más micro. La inclusión de temas sobre tecnología, educación, cultura digital y transformación institucional en el panel responde a la necesidad de presentar la pluralidad de estrategias, proyectos e iniciativas que buscan ampliar las posibilidades de institucionalización de las HD, generando condiciones básicas para avanzar en la reflexión, el planteamiento de nuevos proyectos y la consolidación del campo en la región.

Las HD en los programas de Licenciatura de Humanidades. Estado de la cuestión
Adriana Álvarez Sánchez
Miriam Peña Pimentel
La división del conocimiento en Áreas ha permitido un alto grado de especialización disciplinaria, sin embargo, la inclusión de nuevos objetos de estudio -incluidos los digitales - requiere aproximarse y, en ocasiones, transitar hacia otras áreas del conocimiento, para ello es necesario que los estudiantes en formación conozcan no sólo los principios teóricos y técnicas de otras disciplinas, sino que lleven a la práctica esa interdisciplinariedad. Ello podría conseguirse si durante la licenciatura (bachelor) cuentan con, al menos, una experiencia de trabajo colaborativo con estudiantes de otras áreas. La inter y transdisciplina son consideradas características inherentes de las HD; por ello, la réplica de esta forma de trabajo podría ser incorporada a los planes de estudio actuales. El uso de recursos y herramientas digitales facilitan la colaboración entre disciplinas; sin embargo los planes de estudio actuales no consideran esta vinculación como un requisito, por el contrario, la tendencia en el “entrenamiento” de los estudiantes, tiende hacia el individualismo y la hiper-especialización; la única posibilidad de conexión con otras áreas del conocimiento (humanístico, social o científico) es la posibilidad de cursar materias optativas en otras facultades o de otras disciplinas -siempre y cuando se demuestre cumplir con los prerequisitos de cada materia, condición que se cumple con dificultad entre más “alejadas” estén las disciplinas-; la gestión o desarrollo de proyectos en pequeña o gran escala, no se enfocan en entablar procesos de investigación colaborativos, las estructuras jerárquicas se mantienen y la cadena de aprendizaje mantiene el status quo; perpetuando el modelo “tradicional” de trabajo. Es importante señalar que esta propuesta no ataca o desprestigia la estructura ni las formas tradicionales de la academia, por el contrario, busca ofrecer una serie de recomendaciones que en convivencia enriquezcan la experiencia universitaria en los diferentes niveles de acción: estudiantado, profesorado, investigación y administración.

En esta ponencia nos centraremos en dos disciplinas concretas: Letras Hispánicas e Historia.

La Licenciatura en Filología Hispánica tiene diferentes connotaciones dependiendo de la Universidad que imparta el programa de estudios (Lengua y Literatura Hispánicas, Letras Hispánicas, Literatura

Latinoamericana, etc.); el nombre de la carrera denota la tendencia a la que se enfoca cada licenciatura, desde el formato más general (lingüística y literatura), hasta el de enfoques delimitados (literatura latinoamericana); sin embargo, para fines prácticos, los egresados de cualquier modalidad de esta licenciatura, cuentan con una instrucción similar a lo largo de 8 semestres. A pesar de las diferencias en los planes de estudio que aquí se presentarán, en su mayoría coinciden con una falta evidente de apertura hacia la inclusión de propuestas “alternativas”; en el caso de la UNAM, por ejemplo, el plan de estudios de la Licenciatura en Lengua y Literatura Hispánicas es de 1999; razón por la cual se entiende innecesario incluir una línea pedagógica hacia las Humanidades Digitales (o hacia la literatura electrónica o las “nuevas tecnologías”); sin embargo, en planes más contemporáneos (2012 en el caso de la Universidad Iberoamericana), encontramos faltas similares que, a juicio de las autoras de esta propuesta, de incorporarse a las prácticas pedagógicas podrían dar parte a una serie de programas interdisciplinares con tendencia a las Humanidades Digitales y la Pedagogía Digital; tendencias que consideramos necesarias para la innovación académica en las regiones periféricas.

La Licenciatura en Historia de la Facultad de Filosofía y Letras, de la Universidad Nacional Autónoma de México (UNAM) es un programa centrado en la historiografía como eje rector de la formación de un historiador al que se busca formar para la investigación, la docencia y la difusión. No obstante, que el programa fue creado a finales de los noventa, en su contenido únicamente se menciona en una ocasión Internet como parte de los métodos de búsqueda bibliográfica (Plan de Estudios de la Licenciatura en Historia, 1999, Vol. I, p.67).

Para realizar el diagnóstico planteado en el panel, se llevará a cabo una revisión crítica de los planes de estudio de estas Licenciaturas que se ofertan en la ciudad de México: Puesto que ambas licenciaturas tienen presencia en la mayoría de las universidades capitalinas, se decidió revisar los planes de estudios de las mismas instituciones: la Facultad de Filosofía y Letras (Escolarizado), UNAM; Universidad Autónoma Metropolitana, Iztapalapa, y Universidad Iberoamericana, esta última de carácter privado. El objetivo, además de comparar los programas, es identificar “áreas de oportunidad” para plantear una propuesta para impulsar la interdisciplinariedad y el trabajo colaborativo a través de medios digitales, lo cual implicará considerar temas como el acceso, la infraestructura, etc., es decir, el contexto institucional de cada caso. Se considerarán las experiencias institucionales que la autora de la ponencia, en colaboración con la Dra. Miriam Peña Pimentel, han tenido como profesoras del Seminario Taller Especializado Humanidades Digitales (2014-2015) e Historia, e Historia Digital (2016) dentro de la Licenciatura en Historia, UNAM; el Seminario de Humanidades Digitales (2013-2016) y la Optativa: Humanidades Digitales en los estudios literarios (2016-2 y 2017-1); además de esfuerzos semejantes llevados a cabo en otras instituciones mexicanas, sin dejar de lado el contexto mundial. Todo ello con la intención de presentar un diagnóstico conjunto que permita crear un modelo para introducir las HD a nivel licenciatura en México.

La evaluación del conocimiento en la sociedad digital. Progresos para una alternativa en Humanidades Digitales
Esteban Romero Frías
Weller (2011), autor del libro The Digital Scholar, emplea el término Digital Scholarship para referirse al conjunto de actividades académicas facilitadas por las nuevas tecnologías. Un conjunto de cambios profundos que, para este autor, son producto de la convergencia de tres elementos: lo digital, lo conectado en red y lo abierto. Las transformaciones digitales producidas en el espacio de las Humanidades, denominadas Humanidades Digitales, representan un caso de especial interés para observar el desarrollo de estas prácticas, en muchas ocasiones conflictivas. Goodfellow, en un artículo de 2013 titulado “Scholarly, digital, open: an impossible triangle?“, señalaba justamente que lo abierto, lo académico y lo digital, constituyen categorías difícilmente compatibles tal y como se consideran en la actualidad. Ciertamente, en el ámbito de las prácticas digitales vinculadas al conocimiento, el conjunto de categorías que hay que tener en cuenta es muy diverso, existiendo un continuo en una pluralidad de dimensiones en las que podemos situar los distintos casos con los que nos enfrentamos. Algunas de estas categorías son, entre otras:    lo académico/no académico, lo

digital/analógico, lo abierto-cerrado. Junto a esto surgen en el ámbito de la sociedad digital nuevas formas de generación y difusión del conocimiento que no son reconocidas por los sistemas de evaluación tradicionales centrados en el impacto bibliométrico. Más allá aún, la apertura que generan los medios sociales hace preciso medir otros impactos de la vida académica aparte del vinculado a la investigación. Esta problemática se agrava particularmente cuando nos referimos a las Humanidades Digitales, herederas de modelos tradicionales asentados durante muchos años. Para un efectivo desarrollo y consolidación de las Humanidades Digitales es preciso identificar y evaluar los nuevos artefactos digitales académicos que en muchos casos constituyen el principal resultado de un proyecto o de la carrera de un académico.

El proyecto Knowmetrics - evaluación del conocimiento en la sociedad digital, financiado en julio de 2016 por la Fundación BBVA en la línea de Humanidades Digitales, desenvuelve precisamente su trabajo en dos líneas. En la primera, con una perspectiva micro, se centra en la identificación de investigadores en Ciencias Sociales y Humanidades Digitales, en la elaboración de una taxonomía de artefactos digitales académicos y de indicadores para evaluarlos y en la propuesta de informe integrado de los impactos de la vida académica para humanistas digitales, incluyendo diversas dimensiones, desde la investigación a la implicación social. En la segunda línea, con una visión macro, trabaja en la evaluación del impacto digital del conocimiento en la universidades a través de diversas aproximaciones: a través de un índice Knowmetrics que se apoye en el trabajo anterior, a través de altmetrics y de medios propios de Internet, como es el caso de Twitter.

La aportación en la mesa redonda que se propone permitirá dar a conocer los avances efectuados por dicho proyecto cuando se encuentre en el ecuador de su ejecución en 2017.

Repensar la producción de conocimiento, las instituciones y las humanidades digitales
Virginia Brussa
Paola Ricaute
Enedina Ortega
En nuestro actual contexto, son frecuentes los cuestionamientos con respecto a la labor de las universidades, el sentido de la producción académica y su vinculación con los complejos problemas que aquejan a la sociedad. De acuerdo con Heleta (2016) los académicos no se encuentran perfilando los debates públicos. Menciona que anualmente se publica un millón y medio de artículos en revistas académicas, que son en su mayoría ignorados por la comunidad científica. En el caso específico de las humanidades, menciona que el 82% de los artículos no se cita ni una vez. Ello enfatiza una problemática que va más allá del acceso, también denota la necesidad de acción y agenda amplia de lo “abierto”.

En una entrevista realizada a Saskia Sassen (Torres, 2016) la socióloga destaca que el mundo académico no está respondiendo a las particularidades del momento. Los académicos, menciona Sassen, se “instalan” en zonas de confort: su carrera académica, sus publicaciones, el uso de categorías dominantes, y no arriesgan, ni en la comprensión profunda y problematización de los conceptos para abordar fenómenos contemporáneos, ni en abrir nuevas fronteras de investigación. Kathleen Fitzpatrick (2011) utiliza el concepto de obsolescencia como una categoría pertinente para dar cuenta de una serie de condiciones culturales asociadas con el sistema actual de producción y difusión del conocimiento en horizonte tecnocultural: la revisión por pares, las nociones de autoría, la categoría del texto y el papel de la universidad.

Esta situación pone de relieve al menos tres problemáticas que nos interesan: por una parte, la necesidad de reconfigurar el papel (y la estructura administrativa) de las instituciones educativas para que den respuestas a los problemas sociales desde nuevas aproximaciones y espacios de “encuentro” ; por otra, transformar los sistemas de producción de conocimiento; y por último, reflexionar sobre el lugar de las humanidades ( específicamente digitales) en este escenario. En este último punto, enfatizar el estado de situación de modalidades de Labs y la creación de herramientas digitales en el seno de nuestros espacios de HD.

Esta propuesta tiene como propósito ofrecer un panorama sobre a) un modelo de transformación institucional; b) una estrategia que permite reconceptualizar la producción de conocimiento desde la academia; y c) una apuesta para promover visiones alternativas sobre el trabajo de los académicos en las humanidades (digitales).

Si buscamos el desarrollo y la consolidación de las HD como campo en Iberoamérica, es necesario fortalecer espacios que fomenten modalidades democratizadoras y colaborativas en la producción de conocimiento, frente a la institucionalidad, estructura y formas pedagógicas propuestas en el seno del ámbito académico actual. Sostenemos la caducidad y el agotamiento de las instituciones sociales (universidad, partidos políticos, dependencias gubernamentales) concebidas como sistemas cerrados y autocontenidos, desconectados de la ciudadanía. Apelamos a la conformación de extituciones: formas organizacionales abiertas a la participación ciudadana como un aspecto fundacional de su constitución y operación.

La universidad debe reconfigurarse bajo este modelo para tener cabida en las nuevas dinámicas sociales. En este sentido, planteamos que las Humanidades Digitales no deberían estar exentas de estos debates y desafíos si se encuentran inscritas en los valores de colaboración, apertura e interdisciplinariedad. Manifestamos la necesidad de una opción crítica y pública en las humanidades que se hacen desde el sur global. Por esta razón, consideramos pertinente repensar los modelos triple hélice que consolidan los lazos entre universidad, gobierno y sociedad civil.

Frente a ello, nuestra propuesta se basa en presentar un planteamiento sobre la vocación y orientación de la humanidades en nuestra región, que contempla una serie de propuestas relativas a nuevas visiones, temáticas, procesos y proyectos. En relación con el impulso de los debates sobre la universidad de hoy, es menester hacer mención a la creación de nuevos espacios, metodologías y modalidades de producción del conocimiento, a la conformación de otras redes con comunidades y organizaciones, a la incorporación de esquemas flexibles y alternativos de colaboración, a las formas de mediación y las conversaciones en distintos niveles de lo “abierto”: institucional (gobiernos, universidades, ONG), temáticos (gobiernos abiertos, datos abiertos, ciencia abierta, ciencia ciudadana ), procesos y herramientas. Consideramos que desde las universidades es importante rescatar la vocación de experimentación y la generación de conocimiento en abierto para transformar la cultura académica, el entorno y las comunidades. Las humanidades tienen una oportunidad invaluable de incidencia en este proceso. Proponemos por tanto, líneas de acción que podrán aportar a dichos debates y problemáticas que enfrenta la universidad, así como el impacto que ello conlleva para la materialización de las Humanidades Digitales críticas y públicas en el ámbito regional.

La reconfiguración social de aprender en red
Cristóbal Suárez-Guerero
Paola Ricaurte Quijano
La educación de hoy se piensa y hace en internet. No obstante, el perfil y el papel de los agentes educativos en los entornos de aprendizaje en red es algo que cuesta definir no solo por la multiplicidad de agentes que hay, sino porque el paradigma dominante de “experiencia de aprendizaje” está sujeta a la relación unidireccional de profesor-estudiantes. Esta matriz de relación educativa entra en crisis cuando hablamos de interacción educativa en internet. Por tanto, más allá de las herramientas tecnológicas es preciso preguntarse ¿qué implica hablar de lo social cuando nos encontramos aprendiendo en internet? Para dar respuesta y entender la importancia de lo social en el aprendizaje hay que apelar a una pregunta invisible en el modelo educativo centrado en el docente: “¿con quién aprender?”

Como entorno de aprendizaje, internet es un entorno abierto -demasiado abierto para muchos- de aprendizaje donde su principal valor -junto al acceso a recursos- es la posibilidad de construir redes entre sujetos para aprender y trabajar con metas compartidas. Si hablamos de aprendizaje, la estructura de estas redes puede facilitar o potenciar los procesos de aprendizaje y la generación de conocimiento. No obstante, internet como entorno de interacción social no es uniforme. Hay que tener en cuenta que este entorno social en red, que configura internet, coexisten distintas formas de comunicación, las tradicionales, pero también formas emergentes e inéditas de comunicación y coordinación humana. Este panorama educativo en red implica, por lo menos, tres retos sociales para repensar con quien aprender: 1. Entender internet como ambiente social de aprendizaje más que como un entorno tecnológico. 2. Reconocer qué nuevo rol de aprendizaje se cumple dentro de la estructura social en red, y 3. identificar las transformaciones digitales de las organizaciones educativas en torno a la cultura educativa.

Para mostrar la reconfiguración social potencial se presentan una serie de casos que pueden dar una idea de la amplitud de perfiles y experiencias que ahora existen en la red. No obstante esta selección es parcial, y queda por desarrollar una serie patrones recurrentes en el trabajo en red y una serie de perfiles que abarcan, como se señala en el Peeragogy Handbook (Rheingold et al. 2015) roles potenciales en el proceso de aprendizaje entre pares tanto en los espacios digitales como físicos: el co-líder, co-director del equipo, editor, autor, procesador de contenido, revisor, presentador, comunicador, diseñador, curador, creativo, traductor, estratega, gerente de proyecto, coordinador, asistente, participante, mediador, moderador, facilitador, etc. A pesar de la variedad de nombres que pueda recibir “¿con quién aprender?” hay que reconocer que estamos frente a nuevos roles educativos. Todos estos roles tienen un denominador común: existen porque las condiciones de aprendizaje se han abierto más allá de la docencia y se emplea la red para crear otras formas de relación.

Por tanto, la aportación en la mesa redonda consiste en enfocar internet como un entorno educativo en red donde, además de la enseñanza, caben otros flujos de comunicación y formas de aprendizaje. Esto empuja a redefinir nuestra otredad al momento de pensar la pedagogía y el aprendizaje en distintas disciplinas y un desafío para la práctica de las Humanidades Digitales.

La urgencia de políticas de incorporación de TIC pertinentes en el área de humanidades: estudio de caso, Universidad Veracruzana
Ana Teresa Morales Rodríguez
Alberto Ramírez Martinell
La Sociedad de la Información y el Conocimiento

(SIC), es un paradigma postindustrial que asume como materia prima la información, donde se asevera que la productividad está basada en la generación de conocimiento (Castells, 2001). En esto, las TIC son un elemento fundamental que provoca alteraciones, rupturas y rompe esquemas en diversos ámbitos (Brunner, 2005). Cambian los modos de producción (Gibbons, 2001), las formas de socializar son distintas y para cada disciplina hay transformaciones específicas (Becher 2001; Trowler, 2012). Lo que hace necesario que los futuros profesionales sean capaces de incorporar las TIC en sus campos de conocimiento.

Es por eso que las Instituciones de Educación

Superior (IES) se han ocupado en incorporarlas en el contexto universitario y ponerlas a disposición de la comunidad universitaria. Entonces, se invierte en tecnología y proyectos de incorporación de TIC para que los egresados sepan usarlas. Pero ¿qué tan pertinentes y efectivos son estos proyectos y políticas?. En el caso de la Universidad Veracruzana (UV), los esfuerzos por incorporar las TIC, se hacen visibles en los planes de desarrollo que los rectores han planteado desde finales de los años 90 (PETIC, 2012), a partir de los cuales se han dictado las estrategias y líneas de acción para la incorporación de las TIC, entre las que destacan: la dotación de infraestructura (equipamiento, conectividad y servicios tecnológicos) para cada una de sus entidades académicas (PETIC, 2012) (las cuales se encuentran distribuidas en cinco regiones geográficas distintas); la implementación de programas de formación en TIC para el profesorado; la puesta en marcha de proyectos como AULA, en el que se capacita a profesores universitarios para el uso de TIC tanto para el diseño de sus clases como para su ejecución; el establecimiento de un marco común de computación para todas las licenciaturas de la universidad; la ampliación de la oferta de servicios de TIC institucionales como iTunes UV, la biblioteca virtual, el sistema de información distribuida Eminus creado al interior de la universidad, entre otros. Dado este contexto, es importante reflexionar que la incorporación de las TIC, implica no solo la dotación de infraestructura, sino que es necesario contar con habilidades digitales para usarlos recursos ya que se desconoce si se está garantizando que los estudiantes de las distintas disciplinas sean capaces de usar las TIC en sus actividades profesionales. Al analizar las políticas de incorporación de las TIC hemos observado que éstas han sido implementadas de manera homogénea (Morales, Ramírez y Excelente, 2016), aún cuando el sistema superior está fragmentado de acuerdo a las diversas disciplinas que convergen al interior.

Implementar políticas de incorporación de TIC de manera homogénea, no permite considerar las necesidades propias de cada disciplina, lo que evita una incorporación pertinente y por lo tanto ineficaz. Es por eso que en el proyecto de Brecha Digital, se ha diseñado una metodología para el análisis de los saberes digitales que requieren los egresados de distintas disciplinas (Casillas y Ramírez 2015). En esta se realizan discusiones colegiadas con las comunidades de académicos de determinada disciplina y se consensan cuáles son los saberes informáticos, informacionales, los requerimientos de comunicación, los elementos de ciudadanía digital y el software especializado, propio de cada disciplina. Esta información sirve como insumo para la actualización de programas de estudio de las carreras en cuestión, y de esta manera la incorporación se plantea que los expertos en cada disciplina (los profesores) puedan definir los saberes digitales mínimos que un egresado debe tener; esos que le permitan apropiarse de tecnologías que puedan usar en apoyo a su disciplina. Así mismo se han llevado a cabo análisis desde diferentes aristas, en los que encontramos que en el área de humanidades hay rechazo al uso de las TIC por parte de algunos profesores, la visión acerca de las posibilidades que éstas les brindan, aún son limitadas. Las pocas innovaciones que hay en el área de humanidades digitales, son escasas y provienen de profesores protecnológicos (Darín, 2015), y a pesar de que éstas resultan enriquecedoras, es necesario y urgente diseñar estrategias que permitan a la universidad encaminar a las carreras de humanidades hacia el uso de las TIC y dejar de tener solo casos de éxito aislados. Es preciso que el análisis y discusión de

cuáles son los saberes digitales que deben desarrollarse en los estudiantes de manera diferida en su formación profesional, se plantee a nivel institucional, ya que urge desarrollar en los estudiantes, habilidades que les permitan adaptarse a los cambios tecnológicos, pues de no hacerlo estarían dejando ir una de las grandes oportunidades que ofrece el mundo moderno y en la pasividad se quedarán en el papel de espectadores. Es necesario poder dar a los estudiantes de humanidades (y de todas las áreas), un capital tecnológico que pueda darles ventajas competitivas, hacerlos parte de los cambios y reconfiguraciones actuales, que sean dinámicos, y a su vez congruentes con las necesidades sociales.
Description of Session Topic
How can DH regional communities best be cultivated and sustained? This panel explores US-based digital humanities collectives that foster active communities of practice. Regional consortia are a growing phenomenon in US digital humanities, offering opportunities for loosely organized groups of DHers to share workshops, events, datasets, conferences, and news with one another. Such arrangements have the benefits of addressing institutional barriers to DH work and enabling sharing that can help address, at least in a small way, funding and infrastructural inequities that can make it hard for newcomers to begin DH work. According to John Theibault, regional consortia “can be distinguished from, on the one hand, state and national digital humanities groups that organize conferences and edit journals and require paid membership, as well as digital humanities centers located in a single institution or formally constituted groups with explicit criteria for admission, and, on the other hand, not visibly organized interactions in active digital humanities regions, even if those interactions are frequent in practice” (2016). Theibault identified 11 self-organized regional consortia not affiliated with a larger organization such as EADH, mostly in the US. Such consortia can serve an important need in connecting researchers from a range of institutions and building community more expansive than a campus and more localized than a national or international scholarly organization. As Rebecca Frost Davis and Bryan Alexander have pointed out, “largescale multi-institutional projects aimed at building resources and pooling expertise . . . [can be] constructed to match the needs of both small liberal arts colleges and large research institutions.” The building out of regional consortia, then, can help broaden the impact and audience for digital humanities work while simultaneously addressing infrastructural needs and allowing institutions to share complementary strengths.

This panel will consider how a range of regional DH groups have organized their communities, many of them through the free WordPress-based platform Commons In A Box, to build active and lasting communities of DH practice. Presentations will cover the contours and plans of specific regional communities as well as the guidance such examples may offer to academic communities just beginning to organize themselves. Attention will be paid to shared commonalities across regional organizations as well as distinctive areas of focus. Included in the panel will be a discussion of software tools that communities can use to build regional DH organizations, with a particular focus on the Commons In A Box software used by many of the groups represented on the panel. The panel will also delve into the challenges of organizing and sustaining a regional consortium, including rewarding the volunteer labor that such organizations depend upon, raising awareness of the consortium, and developing an appropriate governance model.

Though this panel focuses on US-based regional organizations, it is hoped that the panel will stir conversations on an international level about shared models of community sustenance and infrastructure. Out of this exchange, the panel hopes to build an informal network of regional DH consortia that can serve as the basis for the ongoing development of DH communities.

CBOX Development and NYCDH
Matthew K. Gold
This presentation will discuss the design, implementation, and future plans for Commons In A Box (CBOX), with a particular focus on how it is being used in the NYCDH community to foster community among New York City DH researchers and practitioners. In NYCDH, a CBOX site has been used to create groups around discrete topics such as “Digital Art History,” “Librarians In DH,” and the “Digital Antiquity Working Group.” Each of these groups were started by members of the site and are used, along with larger public groups such as “Announcements,” to foster connections across a range of NYC institutions. In recent years, the site has facilitated the planning and implementation of an annual conference that includes a week of workshops on DH subjects offered across the NYC area by a range of institutions, and an annual prize for the best graduate student DH project. The site includes a shared calendar that is populated by members and member institutions, so that it provides a quick sense of DH events in the region.

Commons In A Box is a free software project supported by a grant from the Alfred P. Sloan Foundation and maintained by the Graduate Center of the City University of New York. Commons In A Box has two elements: a plugin manager and a default theme. The plugin manager enforces version dependencies between a collection of WordPress plugins, ensuring that all such plugins work seamlessly and without conflict—a technical innovation for the WordPress plugin ecosystem. For the end-user, the experience of installing Commons In A Box is extremely smooth; upon activating CBOX, the plugin manager automatically performs what would otherwise be dozens of manual steps, including plugin installation, activation, and configuration. The CBOX theme, which is based on the design of the CUNY Academic Commons, can also be installed; when activated, it creates a Commons space that features image sliders, community elements such as lists of members and recently updated blogs in the sidebar, and group-related functionality such as “reply-byemail” group forum functionality that has been crucial to community building on the CUNY Academic Commons. The result, and the key technical innovation of the CBOX project, is the transformation of a complicated set-up and customization procedure into an easy, streamlined installation process. CBOX is currently used by a number of organizations, including the Modern Language Association, NYCDH, Texas DH, Virginia DH, and Florida DH, to build Commons spaces for DH communities.

Under the aegis of an NEH Implementation Grant, Commons In A Box - OpenLab is a new version of CBOX dedicated specifically to teaching and learning, called CBOX-OL. In this presentation, we will describe how integrating a suite of teaching-centered digital tools for content sharing and annotation into the core code of CBOX and bundling it with a core set of DH pedagogical tools ensuring that CBOX-OL communities meet accessibility standards. We will also partner with OER initiatives at partner institutions to create ways of sharing open-access humanities content within and across Commons environments that promote and reinforce sound citation practices. Finally, we have secured a commitment from Reclaim Hosting, a popular hosting service for educational institutions and digital humanities projects, to integrate CBOX-OL into its suite of easily installable software packages. Included in the new version of CBOX will be a set of teaching-related plugins as well as a suite of DH-related WordPress tools in the areas of scholarly communication, such as Braille, which translates English text into Braille; Anthologize, which enables bloggers to create eBooks from their posts; DiRT Tools, which helps community members identify the tools from the DiRT Directory they use; and PressForward, which can be used by teachers to collect content, and then to assign the selection, evaluation, and re-publication of that content with an introduction as an activity to encourage students to connect their classroom discussion with ongoing contemporary debates. All of these plugins will benefit the digital humanities humanities via their potential to enable open learning and exchange, emphasizing scholarship as an open, process-based activity, rather than a closed, solitary one.

Commons spaces don’t build themselves; rather, they are cultivated and energized by participation and leadership from members. Technical development lays the foundation on which such work can happen, but the growth, evolution, and sustainability of such platforms require vision, commitment, and resourcefulness from members of the communities where they launch. This presentation will close with a consideration of the ongoing personal, institutional, individual, consortial, and infrastructural support needed to foster and maintain flourishing communities of active practitioners.

The CUNY Academic Commons
Micki Kaufman and Lisa Rhody
The CUNY Academic Commons is an academic network built at the City University of New York (CUNY) by and for faculty, graduate students, administrators, staff, postdocs, and alumni across the 24-campus system. The site serves to foster community and promote scholarship across our twenty-four campuses. With over 7,293 members and well into its seventh year, the Commons continues to evolve as a vibrant space where members connect, create, collaborate, and explore.

As the site has grown and matured, user outreach has become an increasing focus. In addition to webinars and one-to-one engagement with users and administrators of Commons groups, the team has most recently established Commons Faculty Fellowships to assist faculty in setting up Commons sites and customizing the platform to provide the right teaching tool for class needs. In conjunction with CUNY campus Teaching and Learning Centers, these outreach efforts are helping to promote the benefits of the Commons as a teaching tool across the 24 campuses.

A significant focus of the Commons development has been on personalization of the user experience. With the addition of attractive, customizable public profiles, members can now create beautiful online portfolios for their work. The high-impact header section provides an elegant profile synopsis and collapses as one scrolls down the page. Social media icons and website links help connect user profiles to a range of services on the web. Our sophisticated profile builder makes it easy to create an online CV using freeform and specialized widgets designed to highlight positions, education, publications, and interests, and the new RSS feed widget helps members include excerpts of recent posts. Likewise, the addition of Quick Links gives user profiles, blogs and groups customizable shortened links, easy to remember and ideal for business cards and CVs. Using the cuny.is/ URL shortener helps Commons users to personalize their site and to more effectively communicate their relationship to CUNY.

The Commons allows users to create and join as many groups and websites as they want. This allows users to administrate departments, teach classes, discuss and share resources on a selected topic, and connect with people sharing common interests on the Commons. Features of the Commons include the ability to have groups be public, private, or hidden and can include a host of functions including discussion forums, file storage and document collaboration, and rich email integration that allows a more email-driven, ‘listserv-like’ interaction with the Commons. Sites created by Commons users take many forms including personal blogs, research projects, department, class, event or conference sites, journals, reviews and news commentaries, and photo blogs, and are configurable using hundreds of themes and plugins for a wide range of visual presentations. Users also have free access to the Commons community’s WordPress Help group to get help building their sites and managing accessibility.

The Commons also now includes Social Paper, a networked writing environment that enables students to compose and share all forms of their written work across classes, disciplines, semesters, and publics. Likewise, students can browse and comment on the papers of their peers. Unlike many learning management systems or course blogs, Social Paper gives students full control over the sharing settings of each individual piece of writing. Students may choose to share a paper with a professor, a class, a writing group, the public at large, or alternately, keep it private as part of their personal, in-progress, reflective writing portfolio. Additionally, while composing, students can post comments on their writing with questions mentioning other users in order to solicit peer feedback or interest. By giving students a centralized space to manage the totality of their writing, students can easily change privacy settings as they mature as writers and thinkers, develop audiences for their growing body of work, and reflectively build off prior writing.

As the Commons continues to grow, and as its development team continues to release updated versions of Commons In A Box, the site will endeavor to continue serving the needs of faculty, students, administrators and alumni across the 24 CUNY campuses -- and in the process exemplifying how an academic social network can be sustained across a university system with multiple campuses.

Texas Digital Humanities Consortium
Lisa Spiro
Even as digital humanists at Texas colleges and universities are creating significant digital humanities projects, we face common challenges, such as finding collaborators, learning new skills and developing educational programs. In the fall of 2013, Lisa Spiro (Rice), Cameron Buckner (University of Houston), and Laura Mandell (Texas A&M) discussed establishing a statewide digital humanities consortium to connect digital humanists across the state and foster collaborations. The University of Houston hosted the first Texas Digital Humanities Consortium (TxDHC) conference, co-sponsored by Rice and Texas A&M, in April of 2014. At the conference, we held an open business meeting to plan the TxDHC, which attracted 19 participants from colleges and universities across the state. We discussed common needs, including to support faculty, graduate, and undergraduate training in DH; to build community so that members are aware of projects, opportunities and potential collaborators across the state; and to gain access to infrastructure (such as Omeka) for projects (Spiro 2014). To meet these needs, we planned to develop a website, hold an annual peer-reviewed conference, and provide informal opportunities to interact, such as by publicizing visiting speakers at our home institutions. We also explored creating internship opportunities for graduate students and advocating for DH. Rather than establishing formal structures, we decided to operate as a “coalition of the willing,” with decision-making by consensus. To gather additional input, TxDHC used an online survey, which had 14 respondents between April and September of 2014. Respondents ranked the following as the leading “high” priorities: foster networking (92.9%); identify researchers in Texas with common interests (71.4%); and facilitate collaborative research (64.3%).

Informed by the input received from the meeting and survey, Spiro set up a founding steering committee (SC) for TxDHC, inviting representatives from Texas universities and colleges to serve. Current SC members include Spiro (chair), Jennifer Hecker (University of Texas), Laura Mandell, Rafia Mirza (UT-Arlington), Laurel Stvan (UT-Arlington), Toniesha Taylor (Prairie View A&M), Andrew Torget (University of North Texas), and Dillon Wackerman (SMU); representatives from Southwestern University and University of Houston have also served on the committee.The Steering Committee meets via video conferencing several times each year to discuss the ongoing development of consortium and events such as conferences and webinars; we also communicate fairly regularly through email. Our mission is to “ promote digital research in the humanities disciplines and facilitate interaction amongst researchers working in the digital humanities both within the state, nationally, and internationally” by connecting people, facilitating training and knowledge sharing, and raising the visibility of DH work. Membership in TxDHC is open to anyone who sets up a profile on the organization’s website. Currently there are 58 “active” members and 116 members signed up for announcements.

This presentation will explore TxDHC’s history, initiatives, challenges and future plans. TxDHC’s core activities focus on building community across the state, including through:

•    Our website. Texas A&M’s Initiative for Digital Humanities, Media, and Culture (thanks particularly to the work of former staff member Matthew Christy) installed and hosts the TxDHC’s website ( http://www.txdhc.org/ ), using Commons in a Box to encourage collaboration. The website includes member profiles, groups focused on topics such as Training and Metadata Standards, and a calendar. We have also implemented the DiRT Tools plugin to identify tools used by TxDHC members ( http://www.txdhc.org/tool/ ).

•    State-wide    conferences. TxDHC has

sponsored two multi-day conferences: its inaugural conference at the University of Houston in 2014 and its second conference at UT Arlington in 2015. While these conferences boasted strong programs, recruiting Texas institutions to host the event has proven challenging, especially since TxDHC lacks resources beyond endorsing the conference, publicizing it, and enlisting steering committee members to assist with it. In 2016, TxDHC shifted to a strategy of partnering with other Texas digital humanities/ digital library conferences to hold post-conference events, avoiding duplication of efforts, making it more convenient for people to attend both events, and taking advantage of cross-publicity. Recently we organized    a    one-day    hybrid

unconference/mini-conference following the Texas Conference on Digital Libraries in Austin and a THATCamp following Digital Frontiers in Houston. The mini-conference combined the best of THATCamps and more formal conferences by giving speakers fifteen to twenty minutes to set the context, then devoting the rest of the hour to discussion. While attendance at THATCamp Digital Frontiers was fairly small, the event connected participants from Rice, the University of Houston, UT Arlington, UT Austin, and local museums and explored topics such as creating an introduction to DH course and reviving DH projects.

•    Webinars: TxDHC runs occasional web-based workshops on topics such as OpenRefine, DPLA,

and using tools like Omeka or Hypothes.is in the classroom. To encourage interaction and build community, we employ a video conferencing platform so that participants can see each other’s faces, and we try to set aside time at the end of each event for information sharing.

TxDHC faces several challenges:

•    Raising awareness of its existence. At the

recent Digital Frontiers conference, it became clear that many attendees didn’t know about TxDHC. The organization is primarily publicized through its events, website, word of mouth, and Twitter, but more outreach to DH groups across the state is needed.

•    Accomplishing its vision with few resources.

Lacking a budget or staff, TxDHC depends on the time and commitment of its hard-working volunteers, particularly its steering committee members. Of course, these volunteers face competing demands on their time, so consortium work is fit in as it can be.

•    Keeping the website updated and encouraging people to make full use of it. Ideally, members of TxDHC would add events to the calendar and participate more actively in online groups, but those hopes haven’t yet materialized. Focused outreach may increase participation.

•    Developing a governance model. Initially we operated without clearly defined roles, which allowed for flexibility but also meant that members didn’t necessarily get recognized for their contributions and weren’t tied to particular responsibilities. Moreover, we would like to develop a more coherent and transparent method for bringing new members onto the steering committee and rolling senior members off. In December of 2015, members of the steering committee came together for a daylong retreat at Rice University, where we discussed the need to spread out the work and ensure that all are recognized for their contributions and sketched out more defined roles. We are working on bylaws to formalize the organization’s operations and are in the process of implementing plans made at the retreat. At the same time, many SC members are sympathetic to the lightweight organizational approach taken by NYCDH, which several Steering Committee members learned about during a recent presentation by Alex Gil and Kimon Keramidas at Digital Frontiers 2016. Focusing on partnerships and fostering communication across Texas DH organizations seems like a sound strategy for a small, allvolunteer organization like TxDHC.

• Dealing with geographical distance: Since Texas is the second largest state in the US, it can be difficult to connect people across such a significant distance. To cope with this distance, we have organized state-wide events and online gatherings, but we also hope to encourage members to use the website to promote events and activities within a particular city or region within Texas.

By working together, regional consortia can learn from each other, explore developing common infrastructure (as we have already benefited from CUNY’s work on Commons in a Box), and facilitate broader collaborations around research and teaching. Florida Digital Humanities Consortium

Scot French
My presentation will explore the opportunities and challenges involved in creating a geographically extended, self-governing regional consortium (FLDH/Florida Digital Humanities Consortium) that depends, for its intellectual labor and technology support infrastructure, on academic institutional sponsorship while promoting free, open access to academic networks and DH resources.

Founded at THATCamp Florida 2014, FLDH’s mission “is to provide a platform for studying and discussing digital tools, methods, and pedagogies as well as for educating teachers, faculty, and the public about the multiple, interdisciplinary ways humanities research and computing impact our world. It meets annually to identify issues of interest and to set goals for future collaboration and digital humanities research.” At present the group has 12 institutional members, ranging from large research institutions to small liberal arts colleges: University of Florida (Gainesville), University of Central Florida (Orlando), Florida State University (Tallahassee), University of South Florida (Tampa), University of Miami, Florida International University (Miami), Rollins College (Winter Park), New College of Florida (Sarasota), Florida Southern College (Lakeland), Eckerd College (St. Petersburg), and the Florida Humanities Council. Inspired by NYCDH and the CUNY Academic Commons, group organizers adopted the Commons In a Box (CBOX) academic commons social networking platform as a virtual space in which to foster community and share resources.

An internal grant from the University of Central Florida funded an organizational meeting in Orlando at which representatives of participating schools and the Florida Humanities Council established an Executive Council with two representatives from each institution. A five-member elected Steering Committee drafted a mission statement and set of by-laws, established short- and long-term goals, and began planning for two major initiatives: Hosting HASTAC17 in Orlando (scheduled for November 2017) and submitting a proposal to host a southeast regional preconference workshop on visualization tools through the National Endowment for the Humanities’ Institutes for Advanced Topics in the Humanities. Working on these initiatives has placed new demands on FLDH’s leadership structure, and prompted a shift from volunteer initiative to a more active chair and working group/subcommittee organizational framework.

To date, all of our discussions about how best to build and organize our statewide/regional consortium have been internal to the group. We seek to expand the conversation to representatives of similar groups, particularly those using the CBOX platform developed by Matthew K. Gold and his project team at CUNY. Out of this exchange we hope to build an informal network of regional DH consortia for purposes of information sharing that can be expanded and formalized as needed.

This paper/presentation will raise issues of general interest to the DH community and of particular concern to members of the FLDH Executive Council (which I chair) and Steering Committee (on which I serve as a founding member).

•    Leadership/Self-Governance    Structures.
What sorts of leadership/self-governance structures do regional DH consortia employ to ensure    broad-based    institutional

representation, inclusion of diverse views, and transparency in setting group agendas and policies?

•    Academic Commons Activity. What strategies or best practices might regional consortia adopt to build community and generate sustained activity on CBOX or other academic commons platforms? How can participating institutions most effectively contribute to the development of CBOX or similar platforms, with an eye

toward enhanced functionality and customization?

•    Staffing/Program Support. Is it feasible for participating institutions to “share” staff in support of mutually beneficial program and projects? What roles might digital humanities center staff, graduate research assistants, or postdoctoral research/ teaching fellows play in fostering a regionwide DH community and providing training for interested faculty and students? Is state, national, international, or foundation funding available in support of such regional collaborations?

•    Service/”Invisible Work.” To what extent, if at all, is the volunteer work of consortia officers (such as FLDH Executive Council/Steering Committee members) recognized as valued service within their respective institutions? Could regional consortia have a role in documenting and validating the contributions of active members seeking promotion and tenure or other paths to career advancement?

•    Geographic Distance/Virtual vs. Face-to-Face Meetings. Are webinars and Google Hangouts an adequate substitute for face-to-face meetings and workshops? How might regional consortia spanning large geographic areas (such as Florida and Texas) create more funded opportunities for travel to consortia-sponsored conferences/meetings? What options are available both within and across member institutions for fundraising in support of travel?

As chair of the FLDH Executive Council and a member of the Steering Committee I will conduct an informal member survey to solicit other potential topics for discussion during this session. Ideas generated by this panel will be shared with the Executive Council and serve as the basis for discussion and action at an FLDH organizational meeting during the Orlando HASTAC conference in November 2017.

SD|DH: Building and Strengthening DH Teaching and Learning through a Regional Network
Erin Glass and Jessica Pressman
How can DH regional networks work to spread resources to underserved student populations, foster digital literacy and confidence in educators without DH institutional support, and strengthen local, humanistic forms of social advocacy? This presentation will focus on the development and future plans of the San Diego Digital Humanities (SD|DH) regional network, a collective comprised of faculty (and some staff and graduate students) from seven different local higher education institutions ranging from a R1 university to community colleges. While San Diego is not prominently known for humanities research—let alone DH—we recognize that its location on the border of Mexico, and the diverse student body of its institutions, make it an exceptionally rich site for socially-engaged, participatory forms of DH activity. By pooling resources, expertise, experience, perspectives, and moral support, SD|DH embraces the power of diversity as our chief value.

SD|DH has been working together for the last three years to share experiences and resources, plan events, apply for funding, and seek collegial support for our endeavors. We work from different institutional situations and relationships to digital humanities: a campus pursuing a grass-roots and ground-up approach to digital humanities research and teaching but lacking funding for institutional infrastructure, a campus administration wanting to implement digital humanities from a top-down structure, and a campus successfully implementing digital humanities projects within a specific department but lacking the leadership to build out from there. We have been successful in different ways at different campuses, but we now have a full-blown DH initiative at SDSU and an emerging program at USD, and we continue to use our individual campus efforts to bolster the region

Our first goal in forming SD|DH was to assess the barriers to implementing DH across a wide spectrum of institutions and diverse student populations, and develop work-around strategies for implementing DH by drawing upon the resources of multiple institutions within a single region. From the start, these efforts have been voluntary, but we were able to expand our efforts and visibility with the generous support of a National Endowment for the Humanities Level I Digital Start Up Grant for our project “Building and Strengthening Digital Humanities through a Regional Network.” This grant supported a series of workshops that focused on distributing DH to institutions and student populations usually left out of DH. It also helped encourage educators to explore new DH techniques for teaching within the safety net of a supportive community. Educators engaged in a wide range of DH methods such as text analysis and game design as well as DH-inspired techniques to creative re-imagine uses of everyday software and tools for pedagogical purposes. We met several times throughout the year to discuss challenges and successes, share expertise, and brainstorm new possible projects.

In this presentation, we will discuss our methods for developing and facilitating these workshops and pedagogical engagements as well as some of the relevant issues concerning coordination, labor, administrative support, technical resources, and transportation. We will also share lessons learned throughout our experience thus far and suggest protocols that could potentially be re-used and repurposed by other institutions. Finally, we will argue for the need to collectively and creatively communicate the value of multi-institutional collaboration to administrators in order to increase administrators’ receptivity to these efforts going forward.

In addition to discussing projects already carried out, we would like to share our plans for moving forward. As part of deepening and expanding our community’s ties, SD|DH is working towards launching a multi-institutional digital commons powered by Commons in a Box (CBOX) hosted at UCSD. While some SD|DH campuses could ostensibly run and host their own CBOX we have decided to experiment with creating a single commons for our community so that all participating SD|DH institutions can take advantage of the many affordances of CBOX. We are planning to use this Commons in multiple ways. First, we will use it to facilitate communications for the SD|DH group as a list serv, events calendar, member directory, and showcase of SD|DH projects. However, we will also invite all students, faculty, and staff of SD|DH institutions to use the Commons for networking across institutions, building websites related to research and teaching (for any discipline), and creating interest groups that cut across disciplines and institutions. Finally, we also hope to implement Social Paper, a collaborative writing tool developed at The CUNY Graduate Center for CBOX through the generous support of a NEH Digital Start Up Grant. We are currently discussing the possibility of designing multi-institutional synchronous courses that will engage students in thinking through the role of collaboration in their respective disciplines by collaborating with students outside of their institutions through Social Paper.

Bibliography
Alexander, B. and Frost Davis, R. (2012) "Should Liberal

Arts Campuses Do Digital Humanities? Process and

Products in the Small College World." Debates in the Digital Humanities, Ed. Matthew K. Gold. Minneapolis: University of Minnesota Press.

Spiro, L. (2014) “Creating the Texas Digital Humanities Consortium.” Digital Scholarship in the Humanities. April 23,    2014.

https://digitalscholarship.wordpress.com/2014/04/23 /creating-the-texas-digital-humanities-conso rtium/.

Theibault, J.    (2016) “Regional Digital Humanities

Consortia: An Emerging Formalization of Informal Network Ties? [Poster].” In Digital Humanities 2016: Conference Abstracts, 902-3. Krakow: Jagiellonian University    & Pedagogical University. h

ttp://dh2016.adho.org/abstracts/176.
Introduction

Spam, or unsolicited commercial communication, has evolved from telemarketing schemes to a highly sophisticated and profitable black-market business. Although many users are aware that email spam is prominent, they are less aware of blog spam (Thomason, 2007). Blog spam, also known as forum spam, is spam that is posted to a public or outward facing website. Blog spam can be to accomplish many tasks that email spam is used for, such as posting links to a malicious executable.

Blog spam can also serve some unique purposes. First, blog spam can influence purchasing decisions by featuring illegitimate advertisements or reviews. Second, blog spam can include content with target keywords designed to change the way a search engine identifies pages (Geerthik, 2013). Lastly, blog spam can contain link spam, which spams a URL on a victim page to increase the inserted URLs search engine ranking. Overall, blog spam weakens search engines' model of the Internet popularity distribution. Much academic and industrial effort has been spent to detect, filter, and deter spam (Dinh, 2013), (Spirin and Han, 2012).

Less effort has been placed in understanding the underlying distribution mechanisms of spambots and botnets. One foundational study in characterizing blog spam (Niu et al., 2007) provided a quantitative analysis of blog spam in 2007. This study showed that blogs in 2007 included incredible amounts of spam but does not try to identify linked behavior that would imply botnet behavior. A later study on blog spam (Stringhini, 2015) explores using IPs and usernames to detect botnets but does not characterize the behavior of these botnets. In 2011, a research team (Stone-Gross et al., 2011) infiltrated a botnet, which allowed for observations of the logistics around botnet spam campaigns. Overall, our understanding of blog spam generated by botnets is still limited.

Related Work

Various projects have attempted to identify the mechanics, characteristics, and behavior of botnets that control spam. In one important study (Shin et al., 2011), researchers fully evaluated how one of the most popular spam automation programs, XRumer, operates. Another study explored the behavior of botnets across multiple spam campaigns (Thonnard and Dacier, 2011). Others (Pitsillidis et al., 2012) examined the impact that spam datasets had on characterization results. (Lumezanu et al., 2012) explored the similarities between email spam and blog spam on Twitter. They show that over 50% of spam links from emails also appeared on Twitter.


Figure 1: Browser rendering of the ggjx honeypot

The underground ecosystem build around the botnet community has been explored (Stone-Gross et al., 2011). In a surprising result, over 95% of pharmaceuticals advertised in spam were handled by a small group of banks (Levchenko et al., 2011). Our work is similar in that we are trying to characterize the botnet ecosystem, focusing on the distribution and classification of certain spam producing botnets.

Experimental Design

In order to classify linguistic similarity and differences in botnets, we implement 3 honeypots to gather samples of blog spam. We configure our honeypots identically using the Drupal content management systems (CMS) as shown in Figure 1. Our honeypots are identical except for the content of their first post and their domain name. Ggjx.org is fashion themed, npcagent.com is sports themed, and gjams.com is pharmaceutical themed. We combine the data collected from Drupal with the Apache server logs (Apache, 2016) to allow for content analysis of data collected over 42 days. To allow botnets time to discover the honeypots, we activate the honeypots at least 6-weeks before data collection.

We generate three tables of content for each honeypot (Bevans and Khosmood, 2016). In the user table, we record the information the spambot enters while registering and user login statistics that we summarize in Table 1. This includes the user id, username, password, date of registration, registration IP, and number of logins. In the content table, we record the content of spam posts and comments which we summarize in Table 2. This includes the blog node id, the author's unique id, the date posted, the number of hits, type of post, title of the post, text of the post, links in the post, language of the post, and a taxonomy of the post from IBM's Alchemy API.

Honey pot

Quantity

Mean Logins/User

# of Countries

ggjx

62992

1.066

83

gjams

28230

1.102

40

npcagent

34332

1.05

53

Table 1: User table characteristics for three honeypots

Honeypot

Quantity

Avg. Hits

Avg. Links

English Posts

ggjx

2279

28.237

2.356

1962

gjams

2225

18.178

0.311

2137

npcagent

1430

29.043

1.823

1409

Table 2: Characteristics for the content tables

Honeypot

ggjx

gjams

npcagent

# Of Entities

3430

1790

1566

# of Users

62992

28230

34332

Mean Users/Entity

18.365

15.771

21.923

Max Users/Entity

37589

14249

23577

cr of Users/Entity

666.128

359.619

611.157

# of IPs

5291

3092

2120

Mean IPs/Entity

1.543

1.727

1.354

Maximum IPs/entity

118

135

60

<r of IP Quantity

4.277

5.551

2.406

Mean Posts/Entity

.664

1.243

.907

Max Posts/Entity

163

484

664

a of Posts/Entity

5.319

14.448

17.256

% of Entities Who Posted

15.2

12.4

13.5

Table 3: Characteristics of entities

Lastly, in the access table, we include data and meta-data from the Apache logs. This includes the user id, the access IP, the URL, the HTTP request type, the node ID, and an action keyword describing the type of access.

Our honeypots received a total of 1.1 million requests for ggjx, 481 thousand requests for gjams, and 591 thousand requests for npcagent.

Entity Reduction

It is widely accepted that spambot networks, or botnets, are responsible for most spam. Therefore, we algorithmically reduce spam instances into unique entities representing botnets. For each entity, we define 4 attributes: entity id, associated IPs, usernames, and associated user ids. To construct entities we scan through the users and assign each one to an entity as follows.

1. For a user, if an entity exists which contains its username or IP, the user is added to the entity.

2. If more than one entity matches the above criteria, all matching entities are merged.

3. If no entity matches the above criteria, a new entity is created.

We summarize the entity characteristics in Table 3. The maximum number of users in one entity is almost 38 thousand for ggjx with over 100 unique IP addresses. These results confirm what is expected - the vast majority of bots interacting with our honeypots are part of large botnets. This also allows us to perform content analysis exploring what linguistic qualities differentiate botnets.

Feature

Description

Indicates

Effective

Bag or Words

Set of words with count

Lexical content

Yes

Alchemy

Document taxonomy

Taxonomy

Yes

Link

URL core domain names

URL similarity

Variable

Vocab

Vocab complexity

Vocabulary complexity

No

Part-of-speech

A BoW of parts-of-speech

Simple syntax

No

Table 4: NLP feature sets we consider for our content analysis and their effectiveness at differentiating botnets

Content Analysis

To better understand botnets, we use natural language processing (Collobert and Weston, 2008) for analyzing the linguistic content of entities. For our analysis, we consider various feature sets as proxies for linguistic characteristics as summarized in Table 4. We use a Maximum Entropy classifier (Mega M, 2016) to test which features differentiate botnets. In order to test a feature, we train the classifier with 70% of the posts, randomly selected, from the N largest entities and test it with the remaining 30% of the posts. Our final results are the average of three runs.

The first feature set we test is Bag Of Words (BoW) which models the lexical content of posts. Put simply, each word in a document is put into a ‘bag' and the syntactic structure is discarded. For implementation details, see our technical report (Bevans, 2016). In Figure 2, we show our analysis of the BoW feature set.

When considering the top 5 contributing entities, the classification accuracy is less than 95% which implies that the lexical content of botnets varies greatly. The second feature we consider is the taxonomy provided by IBM Watson's AlchemyAPI. Alchemy's output is a list of taxonomy labels and associated confidences. For the purpose of our analysis, we discard any low or non-confident labels. In Figure 3, we show our analysis of the Alchemy Taxonomy feature set which highlights the accuracy of Alchemy's taxonomy. We note that the Alchemy Taxonomy feature set is dramatically smaller in size than the BoW feature set while still providing high performance. This indicates a full lexical analysis is not necessary but a taxonomic approach is sufficient. Our third feature is based on the links in the posts. To create the feature, we parse each post for any HTTP links and strip the link to its core domain name.

The classifier with the link feature set had varied results, as shown in Table 5, where it was reliable in differentiating ggjx entities but less reliable for the other two honeypots. These results correlate with link scarcity from Table 2.


We test the normalized vocabulary size of a post as a feature. We derive this from the number of unique words divided by the total number of words in the post. As shown in Table 5, the vocabulary size does not differentiate botnets.

We also form a feature set based on the part-of-speech (PoS) makeup of a post using the Stanford PoS Tagger. The Stanford PoS tagger returns a pair for each word in the text, the original word and corresponding PoS. We create a BoW from this response that creates an abstract representation of the document's syntax. As shown in Table 5, the PoS does not differentiate botnets.

Feature Set

Database

Accuracy (10 entities)

Accuracy (60 entities)

BoW

BoW

BoW

ggjx

gjams

npcagent

93%

92%

93%

71%

78%

83%

Alchemy

Alchemy

Alchemy

ggjx

gjams

npcagent

87%

91%

91%

80%

84%

82%

Link

Link

Link

ggjx

gjams

npcagent

89%

53%

72%

84%

37%

61%

PoS

PoS

PoS

ggjx

gjams

npcagent

32%

53%

70%

16%

39%

60%

Vocab

Vocab

Vocab

ggjx

gjams

npcagent

32%

50%

74%

17%

36%

60%

Table 5: Accuracies for various features when identifying 10 and 60 entities using the maximum entropy classifier

Conclusions

In this paper, we examine interesting characteristics of spam-generating botnets and release a novel corpus to the community. We find that hundreds of thousands of fake users are created by a small set of botnets and much fewer numbers of them actually post spam. The spam that is posted is highly correlated by subject language to the point where botnets labeled

by their network behavior are to a large degree re-discoverable using content classification (Figure 3).

While link and vocabulary analysis can be good differentiators of these botnets, it is the content labeling (provided by Alchemy) that is the best indicator. Our experiment only spans 42 days, thus it's possible the subject specialization is a feature of the campaign rather than the botnet itself.

Bibliography

Apache virtual    host.    (2016).

http://httpd.apache.org/docs/current/vhosts Accessed: 2016-08-10.

Bevans, B., and Khosmood, F. (2016). Forum Spam Corpus. http://users.csc.calpoly.edu/~foaad/bfbevans Accessed: 2017-04-01.

Bevans, B. (2016). “Categorizing Forum Spam.” Master's Theses at Cal Poly Digital Commons. http: //digitalcom-mons.calpoly.edu/theses/1623 Accessed: 2017-04-01.

Collobert, R., and Weston, J. (2008). “A unified architecture for natural language processing: Deep neural networks with multitask learning.” Proceedings of the 25th International Conference on Machine Learning, ACM: 160-67.

Dinh, S. et al. (2015). “Spam campaign detection, analysis, and investigation.” Digital Investigation, (12) S12-S21.

Geerthik, S. (2013). “Survey on internet spam: Classification and analysis.” International Journal of Computer Technology and Applications, 4(3): 384.

Levchenko, K. et al. (2011). “Click trajectories: End-to-end analysis of the spam value chain.” Symposium on Security and Privacy, IEEE. 431-446.

Lumezanu, C. and Feamster, N. (2012). “Observing common spam in twitter and email.” Proceedings of the 2012 ACM conference on Internet measurement, ACM. 461466.

Mega M. (2016). “Mega model optimization package.” https://www.umiacs.umd.edu/~hal/megam/, Accessed: 2016-08-10.

Shin, Y., Gupta, M., and Myers, S. A. (2011). “The nuts and bolts of a forum spam automator.” LEET.

Spirin, N., and Han, J. (2012). “Survey on web spam detection: Principles and algorithms.”

ACM SIGKDD Explorations Newsletter, 13(2): 50-64.

Stone-Gross, B., et al. “The underground economy of spam: A botmaster's perspective of coordinating large-scale spam campaigns.” LEET, 11: 4.

Stringhini, G. (2015). “Evilcohort: Detecting communities of malicious accounts on online services.” 24th USENIX Security Symposium (USENIX Security 15), 563-578.

Thomason, A. (2007). “Blog spam: A review.” CEAS, Citeseer.

Thonnard O. and Dacier, M. (2011). “A strategic analysis of spam botnets operations.” Proceedings of the 8th Annual

Collaboration, Electronic messaging, Anti-Abuse and Spam Conference, ACM, 162-171.

Niu, Y. et al. (2007). “A quantitative study of forum spamming using context-based analysis.” NDSS.

Pitsillidis, A. et al. (2012). “Taster's choice: A comparative analysis of spam feeds.”

Proceedings of the 2012 ACM conference on Internet measurement, ACM. 427-440.
Introduction

When an earthquake struck Nepal in 2015, the band One Direction sent tweets encouraging their fans to donate to relief efforts, while an Indian activist tweeted accusations of Christian missionaries trading conversions for aid. While Twitter users were quick to bring their own agendas to the Nepal earthquake, does the same hold true for earthquakes in other parts of the world? A series of earthquakes that struck Kumamoto, Japan, and then Muisne, Ecuador in 2016 attracted a substantial amount of Twitter attention as well, yet as far as we are aware, the One Direction fans and the Indian activist made no comment. These users are onlookers to all three earthquakes: in other words, they are not directly affected by these events, but they tweet about them.

This paper explores onlookers’ responses across three different earthquakes: the 2015 Nepal earthquake, and the nearly-simultaneous earthquakes in Kumamoto and Ecuador in 2016, which we treat as a single event. This present work expands on our previous conclusion that onlookers tend to bring their own agendas to disasters. This paper shows that users who tweeted about the Kumamoto and Ecuador earthquakes were generally more interested in the earthquake or the affected areas than their own agendas, as their interest in the earthquake could not be predicted by interests in other topics.

Background

A substantial amount of research has explored how social media causes users to engage with political, social, and humanitarian problems; however, opinions on social media’s effectiveness—whether it causes users to donate money or participate in campaigns— are mixed. Some argue that displaying concern in social media is more about acquiring social capital than effecting change (Shulman; Gladwell; Morozov, The Net Delusion; Morozov, To Save Everything, Click Here), while a Pew Research Center survey finds that social media does create change (Raine, Purcell, and Smith). One analysis found that charities’ use of social media does not increase donations (Malcolm), while another finds that certain tweeting strategies do (Gasso Climent) although tweets may not raise awareness about the charity’s causes (Bravo and Hoffman-Goetz). All these studies concur that social media enable substantial discourse about crises. The question we explore here is how much of this conversation is predicted by a user’s preexisting interests, and how this varies even among the same type of event in different areas.

Methodology

We followed a similar data collection process for both Nepal and the Kumamoto and Ecuadorean earthquakes: we sampled data from Twitter’s REST API to attain a broad sample of onlookers. For Nepal, we had gathered a dataset of tweets sent during the three weeks following the Nepal earthquake by searching for any tweets that mentioned the word “Nepal” from April 24, 2015 to May 8, 2015. We then randomly selected 15,000 users from this set and harvested all tweets they sent between April 24, 2014 and May 8, 2015. We attempted to capture only English-speaking users to increase the likelihood that we would capture users not directly affected by the earthquake, but we still found some users who tweeted in multiple languages. This left roughly 11,000 onlookers for Nepal. For Kumamoto and Ecuador, we gathered a dataset of tweets sent in the two weeks following the Kumamoto earthquake that mentioned “Kumamoto” or “earthquake.” We randomly selected 30,000 users and harvested every tweet they sent between March 16 and May 16, 2016. We collected more users, but fewer tweets for each user, than we did in the Nepal dataset so as to look for users who displayed a broader set of interests. This left around 25,000 onlookers in Kumamoto and Ecuador. We were able to filter out non-English tweets much more effectively in the latter dataset than the Nepal dataset.

For the tweets for each event, we made a bipartite graph of users to words, and performed community detection using a method proposed by Okamoto and Qiu (2015)    [2], which allows for overlapping

communities. Okamoto and Qui’s method takes a single parameter, alpha, which controls the resolution of community detection: the smaller its magnitude, the larger the number of detected communities. We set alpha to 0.001 in both cases. The output of this method was a list of each node (users and words), and a percentage ranking rating its affinity with each community. We used these results to generate a list of top words in each community, which told us what users who tweeted about that community were interested in. From this process, a number of topics emerged, which we labelled manually according to our interpretations of the top words in each.

Since this method also gave us a ranking for users’ affinities to each community, it allowed us to examine the influence of other topics on a user’s likelihood to tweet about either event. We wanted to examine how much a user’s propensity to tweet about other topics predicted the probability that he or she would tweet about topics related to the earthquake. We ran multivariate linear regressions on each topic in the dataset using the Python sklearn module (Pedregosa et al.). We ran one regression for each topic, in which we treated a user’s propensity to tweet about the topic under consideration as a dependent variable predicted by his or her propensity to tweet about other topics.

Results

Our analysis demonstrated a certain predictive power for some topics in each dataset. Applying this process to the Nepal tweets produced 17 topics about a variety of concerns, from entertainment to world events. Table 1 shows these topics. Two of them, topics 5 and 15, treat the earthquake directly.

A correlation exists between tweeting about entertainment topics and tweeting about the earthquake. Tweeting about topic 15 predicts that a user will tweet about topic 2, which is about pop music: the top words include “fifth,” “harmony,” “video,” and “Justin.” This correlation is the strongest in the dataset; few other topics show nearly as much correlation. Consequently, we observe a degree of correlation between tweeting about entertainment topics and tweeting about the disaster in Nepal. While the more targeted topics, like the One Direction topic, do not show much correlation with other topics, the more general entertainment topic does.

Title

One Direction

Fifth Harmony

Earthquake

Earthquake

0 One Direction

1.000

0.117

-0.004

0.000

1

Weather

0.005

0.008

0.007

0.001

2

Pop Music

0.099

1.000

-0.003

0.004

3

India

-0.028

-0014

0.135

-0.001

4

England News

0.005

0.023

0.003

0.001

5

Earthquake

-0.007

-0.007

1.000

0.006

6 Good Feelings

-0.012

0.028

0.050

0.000

7 Good Feelings

0.182

0.115

0.004

0.004

8

Pera Sacha Sauda

0.015

0.031

0.097

0.002

9

China

-0.025

-0015

0.181

-0.001

10

Twitter

0.001

0.003

0.001

0.001

11

Emotions

0.174

0.223

-0007

0.000

12

Shopping

-0015

-0.006

-0.001

0.001

13

US Politics

-0.025

-0.018

0.095

0.000

14

Technology

-0017

0.005

0.034

0.000

15

Nepal

-0025

7 0Q4

1 493

1.000

16

US News

-0013

0.017

0.049

0.000

Table 1: Topics for Nepal, showing probability of tweeting about one topic (X-axis) given likelihood of tweeting about other topics (Y-axis)

In summary, we observe correlation between tweeting about entertainment topics and tweeting about the Nepal earthquake. Those who bring other agendas such as an interest in a particular musical group to the disaster tend to tweet mostly about those topics.

Does the same hold true for Kumamoto and Ecuador? Table 2 shows a few topics from the Kumamoto and Ecuador earthquakes. Our analysis demonstrates that an onlooker’s propensity to tweet about some topics could be predicted by interest in others. For example, a user who tweeted about news topics, such as U.S. politics (specifically, topic 43) or Asian news (topic 4), was likely to tweet about Nigerian politics (topic 2). Likewise, a user who tweeted about Japanese Entertainment (37) was also likely to tweet about other entertainment topics.

Topics as Independent    Nigerian Politics Entertainment Kumamoto 1 Kumamoto 2

Variables

0 Good Feelings

-0.01335

-0.01

0.00070

0.00086

1 US Politics

-0.01958

-002

0.00069

0.00063

2 Nigerian Politics

1.00000

-002

0.00068

0.000o8

3 Middle East

0.01191

-0.01

0.00082

0.00082

4 Asian News

551858

2.72

0.00419

0.00707

5 Roberta Lange

0.90105

0.43

0.00101

0.00138

6 Kumamoto 1

3.31768

1.92

1.00000

0.00578

7 MSG

-0,01637

-0.02

0.00046

0.00047

8 Anime/Good Feelings

-001770

-002

0.00040

0.00037

9 Good Feelings

0.26131

0.26

0.00114

0.00281

10 Music

0.01621

0.04

0.00046

0.00074

11 BTS

-0.01426

-0.01

0.00059

0.00065

12 MSG

-0.01521

-0.02

0.00047

0.00059

13 Good Feelings

-0.00662

-0.01

0.00084

0.00056

14 Social Media

-0.01055

-001

0.00036

0.00043

15 Good Feelings

0.96011

1

0.00201

0.00269

16 Good Feelings

-001305

-001

0.00056

0.00062

17    US Politics

18    Entertainment

-0.02949

-0.02242

-0.03

-0.02

0.00063

0.00042

0.00059

0.00046

19    Entertainment

20    Prince’s Death

-002122

-0.02317

-002

-0.02

0.00041

O.OOO58

0.00040

0.00080

21 Entertainment

-0.02253

-002

0.00051

0.00075

22 Kumamoto 2

4.43153

1.81

0.00775

1.00000

23 Good Feelings

-0.00948

-001

0.00032

0.0111)41

24 Team Seymour

-0.01254

-0.02

0.00045

0.00045

25 Soccer

0.01116

0.03

0.00054

0.00050

26 Entertainment

-0.01708

-0.02

0.00037

0.00028

27 Tobacco

-001941

-0.02

O.OOO55

0.00046

28 Good Feelings

-001928

-0.02

0.00044

0.00061

29 Captain America

14.44602

27.88

0.00722

0.00669

30 Pom

-001421

-001

0.00027

0.00034

31 Emotions

-0.01924

-0.02

0.00037

0.00042

32 MSG

-001707

-0.02

0.00045

0.00054

33    Help

34    Disasters

35    Emotions

-0.01220

-0.02593

-0.01023

-0.02

-002

-001

0.00053

0.00146

0.00044

0.00071

0.00293

0.00060

36 Entertainment

-0.03060

I]

0.00063

0.00044

37 Japan and Entertainment

4.87650

4.88

0.00512

0.00865

38 Indian Politics

0.06220

0.01

0.00088

0.00061

39 Good Feelings

0.01971

0.06

0.00063

0.00060

40 Good Feelings

-0.02143

-002

0.00061

0.00049

41 US Politics

-0.01536

-0.02

0.00068

0.00063

42 US Politics

-0.02893

-0.03

0.00057

0.00049

43 US Politics

1529698

17.34

0.00777

0.02044

Table 2: Topics for Kumamoto and Ecuador, showing probability of tweeting about one topic (X-axis) given likelihood of tweeting about other topics (Y-axis) (truncated for space)

On the other hand, no such correlation was observed in the opposite direction: no topic predicted a user’s tendency to tweet about topics 6 and 22, the earthquake topics. All coefficients in those regressions were under 0.01. The two topics that focus on Kumamoto are relatively closed: users who tweet most about the Kumamoto earthquake tweet about little else during this period.

Our interpretation is that users who tweeted about Kumamoto or Ecuador were specifically interested in earthquakes, Japanese culture, or the affected regions. The majority of users who tweeted about the Kumamoto and Ecuador earthquake topics were interested in specialized topics relevant to the events: they were not, for example, One Direction fans. We therefore conclude that while some users tweeting about Kumamoto and Ecuador were motivated by general interests in news or entertainment, they were a much smaller group than in the Nepal dataset. Conclusions

We find that while users often brought their own agendas to tweeting about Nepal, fewer did so when tweeting about Ecuador and Japan. Users who tweeted about Kumamoto and Ecuador tended to focus on topics related to the earthquakes, and less on issues that the earthquakes might demonstrate.

Our future work will test these conclusions with other earthquakes. In particular, we will examine the 2011 Tohoku Earthquake which raised serious political issues. Additionally, in our present work, we treat the Kumamoto and Ecuador earthquakes as a single event because distinct “Kumamoto” and “Ecuador” topics did not emerge from our text mining, which itself is suggestive of how Twitter users understood them. In our future work, we will probe more deeply for differences between the two earthquakes.

Bibliography

Bravo, C. A., and Hoffman-Goetz, L. (2015) “Tweeting

About Prostate and Testicular Cancers: What Are

Individuals Saying in Their Discussions About the 2013

Movember Canada Campaign?” Journal of Cancer

Education. 1-8. link.springer.com. Web. 20 Feb. 2016.

Gasso Climent, C. (2015) “Twitter as a Social Marketing

Tool: Modifying Tweeting Behavior in Order to

Encourage    Donations.”    info:eu-

repo/semantics/bachelorThesis. N.p., 27 Aug. 2015. Web. 20 Feb. 2016.

Gladwell, M. (2011) Outliers: The Story of Success. Reprint edition. Back Bay Books. Print.

Malcolm, K. (2016). “How Social Media Affects the Annual

Fund Revenues of Nonprofit Organizations.” Walden

Dissertations and Doctoral Studies (2016): n. pag. Web.

Morozov, E. (2012) The Net Delusion: The Dark Side of

Internet Freedom. Reprint edition. New York:

PublicAffairs. Print.

Morozov, E. (2014) To Save Everything, Click Here: The Folly of Technological Solutionism. First Trade Paper Edition edition. New York: PublicAffairs. Print.

Okamoto, H., and Qiu, X.-L. (2015) “Modular Decomposition of Markov Chain: Detecting Overlapping and Hierarchically Organized Communities in Networks.” Abstracts of NetSci-X. Rio de Janeiro, Brasil: N.p. Print.

Pedregosa, F. et al. (2011) “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825-2830. Print.

Raine, L., Purcell, K., and Smith, A. (2016) “The Social Side of the Internet | Pew Research Center.” Pew Research Center: Numbers, Facts and Trends Shaping Your World. N.p., 1 Mar.Web. 21 Feb. 2016.

Shulman, S. W. (2009) “The Case Against Mass E-Mails: Perverse Incentives and Low Quality Public Participation in U.S. Federal Rulemaking.” Policy & Internet 1.1: 23-53. Wiley Online Library. Web. 21 Feb. 2016.
It can be challenging to be a solo digital humanist, given the range of skills required, deficits in training, and the need for software, hardware and technical support. As E.E. Snyder points out, researchers may be using digital methods but not consider themselves “digital humanists,” and thus exist outside of virtual networks like Twitter and conferences like Digital Humanities (2012). Even as colleges and universities recognize the potential of digital humanities, they often struggle to support and sustain DH projects. Digital humanities centers, like many cutting-edge, interdisciplinary academic programs and centers, are fragile, subject to changing priorities and budget cuts (Sample 2010). Moreover, DH centers may not necessarily be able to support the range of researchers’ interests and needs. In any case, many digital humanists work at institutions without a DH center, so they often lack institutional support and immediate colleagues whom they can learn from and with. Sample describes the common situation of the DH scholars who lack centers: “We’ll never be able to turn to colleagues who routinely navigate grant applications and budget deadlines... We’ll never have an institutional advocate on campus who can speak with a single voice to administrators, to students, to donors, to publishers, to communities about the value of the digital humanities” (Sample 2010). The isolation among many digital humanists also means that effort is duplicated, as, for example, faculty at multiple institutions are developing DH educational materials similar to those being created elsewhere.

Networks provide a potential solution to isolation by linking people with shared research interests and enabling them to exchange ideas and expertise. Nancy Maron suggests that a campus-based network model--whether with equal partners or “a strong central hub, like a library or a DH center, with many spokes” -- may be a preferred organizational model for DH, as it can balance experimentation and sustainability and combine units’ strengths (Maron 2015). Beyond the campus, as Sample argues, researchers can build their own communities that transcend institutions and are more agile and resilient than formal organizations: “Stop forming committees and begin creating coalitions. Seek affinities over affiliations, networks over institutes” (2010). Networks can cultivate collective expertise and facilitate acting on common interests without getting caught up in bureaucracy or being limited by long-term obligations.

Indeed, digital humanists are participating in a range of networks, from global to regional to university-based. Many digital humanists are connected around the world through Twitter’s virtual network. At the country or regional level, organizations such as Red de Humanidades Digitales (RedHD), NYCDH, and the Texas Digital Humanities Consortium (authors Spiro and Taylor are part of the steering committee of the TDHC) provide online platforms for researchers to discover each other and share information, as well as organize training and events. Within universities or university systems, digital humanities networks such as the Oxford DH Network and the University of Wisconsin-Madison's Digital Humanities Research Network coordinate events and build community among local digital humanists.

As powerful as these networks are, most do not provide funding for collaboration across institutions on research projects, nor do they organize common work on curriculum. Enter Resilient Networks to Support Inclusive Digital Humanities, a collaboration among George Washington University (GW), Rice University, Davidson College, and Prairie View A&M University funded by the Andrew W. Mellon Foundation in the spring of 2016. This network aims to advance digital scholarship by sponsoring collaborative projects among faculty, librarians and students and by developing openly available educational modules that can be used to form a DH curriculum. It brings together private research universities in Washington DC (GW) and Texas (Rice), a public historically black university in Texas (Prairie View A&M), and a private liberal arts college in North Carolina (Davidson College). The co PIs are a member of the library staff and a faculty member from each institution who, together with the project director, oversee the program through bi-weekly online meetings and occasional face-to-face meetings. An Advisory Committee provides strategic guidance for the project.

Between the spring of 2016 and the spring of 2018, Resilient Networks will create:

•    A set of openly licensed, adaptable educational modules on digital humanities that can be used in different contexts, such as workshops and semester-long courses. Planned modules include introduction to digital humanities, data in the humanities, the ethos of digital humanities, and framing projects for the public, as well as electives on topics such as text mining and database design and development.

•    Cross-institutional projects in which a faculty member, librarian and students collaborate on digital humanities research.

To facilitate the cross-institutional projects, Resilient Networks is awarding faculty $5000 jump start packages as seed funding; one is being granted at each institution during year 1, and three during year 2.

•    A group of librarians, faculty, and students knowledgeable about digital humanities methods and collaborative approaches. In August 2016, a small group of faculty and librarians gathered at GW for a training workshop on DH project development and humanities approaches to data facilitated by Trevor Munoz. In addition, the Network sponsored a THATCamp at the Digital Frontiers conference hosted at Rice in September 2016 and a THATCamp at George Washington University in March 2017. The network will further support training by organizing THATCamps and providing funding for members to attend intensive DH workshops.

•    Intra- and inter-institutional relationships that will facilitate ongoing DH collaborations. Many networks depend on strong personal relationships. Through collaborative work on research, curriculum and training, the Resilient Networks will develop such relationships, laying the foundation for ongoing collaborations.

There are challenges in establishing the Network that are to be expected with a cross-institutional collaboration, including setting common goals, maintaining strong communication, negotiating different academic calendars and bureaucratic systems, and accomplishing tasks in the face of competing responsibilities. In addition, the sheer diversity of digital humanities methods makes it difficult to build a coherent community. On the upside, however, by working within the existing institutional structures at each university rather than creating a separate organizational unit, the work will more likely be sustainable in the long term and better serve the needs of the researchers at each institution. As Snyder cautions, “Decentralised networks that lack both institutional support and dedicated time spent in creating resources will face serious barriers; if there is no position that has explicit responsibility for developing the network, the network may fall by the wayside in the pressure of more urgent responsibilities” (2012). To mitigate this risk, Resilient Networks hired a Digital Humanities Project Director to organize program activities and manage day-to-day operations. Establishing inter-institutional and crossinstitutional ties will leverage already-established organizational structures rather than creating new ones. We expect the network to scale to include more institutions, which will expand available expertise. We will be conducting assessments to evaluate the various aspects of the resilient network model to determine its effectiveness in meeting our overall objectives.

In this short paper, we will discuss the network model for digital humanities research and education, results from the first year of “Resilient Networks,” and future plans.

Bibliography

Maron, N. (2015). “The Digital Humanities Are Alive and

Well and Blooming: Now What?” EDUCAUSE Review,

August    17.

http://er.educause.edu/articles/2015/8/the-digital-humanities-are-alive-and-well-and-blooming-now-what.

Sample, M. (2010). “On the Death of the Digital Humanities Center.”    Samplereality.    March    26.

http://www.samplereality.com/2010/03/26/on-the-death-of-the-digital-humanities-center/.

Snyder, E. E. (2012). “A Framework for Supporting the Digital Humanities: An Alternative to the DH Centre.” In Proceedings of the Digital Humanities Congress 2012. Studies in the Digital Humanities. Sheffield: HRI Online Publications.

https://www.hrionline.ac.uk/openbook/chapter/dhc2

012-snyder.
Abstract

Broad access to online digital humanities projects raises questions about who our audiences are and how we respond to their engagement with our work. Using Project Vox as a case study, we highlight motivations and implications for providing audiences more access to decisions that shape the evolution of a project. In doing so, we hope to encourage critical discussion about how we conceive, assess, and engage the diverse audiences of digital scholarship.

Digital humanities practitioners may be familiar with Bertolt Brecht's proposal that radio ought to become "the finest possible communication apparatus in public life"-- a medium that "bring[s] the listener into a relationship instead of isolating him."(Brecht 1964) This thought is often cited as a visionary evocation of the internet, with all its potential for many-to-many communication and meaningful audience response. Yet even as Web 2.0 has realized this kind of open communication space, digital scholarly resources often have not tried to foster engagement with a broad audience. There are some obvious reasons for this tendency; esoteric subject matter comes to mind, as do limitations of time and resources. But audience diversity is generally taken to be the desirable and nearly inevitable outcome of "the democratization of access . . . that Benjamin recognized as a feature of mechanical reproduction," (Unsworth 1996) so it seems incumbent upon project creators to think critically about how to engage a diverse audience -- especially when the perspectives of that audience may suggest new avenues of research, project development, or dissemination.

Can a project carry out its intellectual mission while also adapting to the needs of an unexpected but engaged audience? Underlying that question is an even simpler one: why respond to and involve anyone other than one's intended audience? These questions are not new (Jewell 2009, see also Gibbs & Owens, 2012 and Robertson, 2012), but our answers to them become more vital amid greater access to, and more affordances to engage with, humanities scholarship. The Project Vox team is contending with such questions as we plan our next phase of development.

Project Vox is a digital resource aimed at transforming the canon of modern philosophy, beginning with

how philosophy is taught to undergraduates. To that end, Project Vox provides resources for teaching and research focused on women philosophers traditionally excluded from the western philosophical canon. The content and mission of the site imply a fairly narrow audience-- instructors of undergraduate history of philosophy courses -- and the Project Vox team designed and built the site with those readers in mind. Yet something unexpected happened when the site launched in March 2015: by academic standards, Project Vox went viral. Through wide sharing on social media and articles in scholarly and popular media (for a complete listing of citations and links to press coverage, see the Project Vox publicity page) it enjoyed a kind of publicity and uptake uncommon for a digital humanities venture, let alone a niche project aimed at a single discipline. A substantial part of Project Vox's Twitter mentions could be traced back to a post on Feministing (which also accounted for significant traffic to the site, as captured through Google Analytics, Dusenbery 2015). Popular and scholarly media coverage included the Washington Post, the Times Higher Education Supplement, the London Times, and The Atlantic. Beyond the head-spinning gratification of positive public attention, the fact of this broader and unexpected audience invited us to think more critically about whether and how to accommodate them.

Naturally, it's easiest to ignore an emergent audience and continue to speak directly to an imagined group, regardless of the degree to which that group actually exists as a proportion of a site's readers. Andrew Jewell advocates this plan when he considers the audience of the Willa Cather Archive:

“How shall our editing practices change to address new audience needs?” I have considered this question for some time and with some seriousness, and my current response is this: How the hell should I know? . . . [M]y response to the diversified audience has been not to change my practices at all, but to continue to make content additions that address the needs of the audience I know and, frankly, care about the most: other Cather scholars and teachers. For me, being a central resource to the most informed audience is a sign of great success . (Jewell 2009)

But it is also possible to adapt, and despite his stay-the-course rhetoric, Jewell acknowledges that his "understanding of audience diversity has inspired certain projects within the Cather Archive that address that diversity while also being useful for Cather scholars." (Jewell 2009). Finding exactly that kind of balance has become an unexpected but important part of our work on Project Vox.

In short, we want to adapt Project Vox to serve a broader, engaged community and support our core mission. This goal entails not only making Project Vox available as an open scholarly resource but also providing our audiences more access to and involvement in decisions that shape the evolution of the project. To cite one concrete example, a better understanding of our audience -- for instance, those who discovered our site through the Feministing tweets and posts -- may help determine the degree to which we acknowledge the English writer Mary Astell as an early feminist as well as a philosopher. In making such decisions, we want to clearly understand all of our audiences, their interests in our project, and how our mutual interests could be better aligned to improve and grow the site. Put another way, we want to cultivate the kind of audience-creator dynamic that Andrew Jewell cites as the impetus for creating a bibliography of Cather translations. This dynamic is also embodied in earlier instances, such as the Blake Archive for Humans website and Jon Saklofske's re-imagining of the Blake Archive via his NewRadial software. Both are concrete examples of how users of an openly accessible scholarly resource can have different ideas about the content and its use. Such phenomena force us to recognize divergences between intended and actual audiences and to consider whether we will respond (see Kirschenbaum, 2004; and Saklofske, 2012).

In this talk, we will describe the data and methods Project Vox has used to identify and engage its audience (including Google Analytics data, targeted Qual-trics surveys, and collaboration with user experience professionals). This work is currently underway, and by summer 2017 we expect to share how these results are shaping the development of Project Vox. We will also be able to report on the work of our outreach and assessment coordinator, who is instrumental in engaging our audience and assessing the site's impact. (The very fact of having an outreach and assessment coordinator suggests some of the resource implications for prioritizing audience engagement, and we look forward to discussing the decisions we have made and continue to weigh.)

The questions of audience engagement are persistent and thorny, and they have been under-examined in DH scholarship. As Michael J. Kramer has recently noted, "[o]ne shift in the digital humanities that needs more attention is the changing, often contested, understanding of audience . . . How do makers wish to treat their audiences?. . . Should they be reconstituted as fellow makers? As active respondents?" (Kramer 2016). Project Vox is a public-facing DH project working to engage a broad user base without abandoning its scholarly rationale. By analyzing what we know about our audiences and sharing how we act on that information, we hope to provide a report of general interest to the DH community. And in striving to be transparent and methodical, we hope that our critical, sustained selfassessment can help to elucidate some useful approaches to engaging diverse audiences, which we consider an inherently worthwhile goal of digital scholarship.

Bibliography

Brecht, B. (1964). Brecht on Theatre: The Development of an

Aesthetic. (J. Willett, Ed. & Trans.). New York: Hill and

Wang.

Dusenbery, M. (2015, March 10). New website aims to transform the philosophy canon by highlighting women. Retrieved November 1, 2016, from http://femi-nisting.com/2015/03/10/new-website-aims-to-trans-form-the-philosophy-canon-by-highlighti ng-women/.

Gibbs, F., & Owens, T. (2012). Building Better Digital Humanities Tools: Toward broader audiences and user-centered designs. Digital Humanities Quarterly, 6(2). Retrieved from http://www.digitalhumani-ties.org/dhq/vol/6/2/000136/000136.html

Jewell, A. (2009). New Engagements with Documentary Editions: Audiences, Formats, Contexts. Library Conference Presentations and Speeches, 56. Retrieved from http://digitalcommons.unl.edu/library_talks/56.

Kirschenbaum, M. (2004). “So the Colors Cover the Wires”: Interface, Aesthetics, and Usability. In S.

Schreibman, R. G. Siemens, & J. Unsworth (Eds.), A companion to digital humanities. Malden, MA: Blackwell

Pub. Retrieved from http://www.digitalhumani-ties.org/companion/view?docId=black-

well/9781405103213/9781405103213^

ml&chunk.id=ss1-5-4&toc.depth=1&toc.id=ss1-5-

4&brand=default;

Kramer, M. J. (2016, June 12). The Digital Humanities Reader. Retrieved from http://www.michael-jkramer.net/cr/the-digital-humanities-reader/.

Robertson, S. (2012). The Differences between Digital Humanities and Digital History. In M. K. Gold (Ed.), Debates in the digital humanities. Minneapolis: Univ Of Minnesota Press. Retrieved from http://dhdebates.gc.cuny.edu/de-bates/text/76.

Saklofske, J. (2012). NewRadial: Revisualizing the Blake Archive. Poetess Archive Journal, 2(1). Retrieved from https://journals.tdl.org/paj/index.php/paj/arti-cle/view/8).

Unsworth, J. (1996). Electronic scholarship, or, scholarly publishing and the public. Journal of Scholarly Publishing, 28(1), 3-12.
Queer Infrastructures: Digital Intimacies, Spaces, Affordances and Collaboration
This panel centers queer epistemological formations as they structure the study of digital phenomena. Each of the papers mobilizes the methodological foundations of what Kara Keeling has called a “Queer OS” (2014) -- or an operating system in which “the historical, sociocultural, conceptual phenomena that currently shape our realities in deep and profound ways, such as race, gender, class, citizenship, and ability (to name those among the most active in the United States today), [are understood as] mutually constitutive with sexuality and with media and information technologies, thereby making it impossible to think any of them in isolation” (152). The panelists address core questions including how digital infrastructures mirror and/or model queer and feminist approaches to scholarship, social and artistic practice; what are some possible aesthetic/political retoolings for digital technologies originally designed to surveille and/or maximize female bodies as reproductive vessels; how do social media platforms shift the organizing of live performance cultures?;

what are some of the genealogies for contemporary queer digital culture and design?; how can we mobilize theories of interiors and architecture to help us think about the design of online spaces?; what can we learn from long histories of queer social, erotic, political and artistic life that can help us to build online platforms that resist calls to full public access?

These four papers contribute to the larger conversation at the nexus of Digital Humanities, Digital Media Studies, and Theories and Practices of Transgender, Feminist and Queer Cultural Production. Here we bring together the practice of performance art that merges digital technological invention to the long history of feminist body art, and the often invasive politics of digital surveillance and control of women's bodies, along with the introduction of “Transmedial Drag” as a set of methods for analyzing the digital transfer and potentiality of live performance art, analog queer networks and spaces. We bring this together with a discussion of the development of digital and analog infrastructures for minoritarian spaces and scenes in Mexico City and a material analysis of many of the tropo-logical framings of digital culture--screens, codes and filters--which have been centrally important to the field of feminist design and architectural studies.

Transmedial Drag: Transgender, Feminist and Queer Performance Art in the Age of Digital Reproduction
T.L. Cowan
One of the central foci of Digital Humanities scholarship has been the production of online repositories of previously inaccessible archival materials. Often the project of producing an online repository is generated by the assumption that open access to materials that were once held behind the walls of institutional protocols or hidden in personal collections, is a good thing both for the research community that may want access and for the materials themselves and the lives and events that they assemble. However, as scholars invested in the project of decolonization--including Mukurtu and Local Context co-founders Jane Anderson and Kim Christen--have show us, making openly accessible all of the archival materials collected about a particular community, will very likely violate the cultural protocols of that community. Indeed, the entitlement required to imagine that all materials about a particular community should be made accessible to people who are not part of that community is a perspective with long colonial and imperialist genealogies.

In this paper, I present a range of speculative-pragmatic methods that my collaborators and I have used

towards the development of The Cabaret Commons: An Online Archive and Anecdotal Encyclopedia for Trans- Feminist and Queer Artists, Activists and Audiences. Over the course of the past 5 years of community-engaged research, we have worked both to assemble materials from live performance scenes in Mexico City, New York City, Montreal and Toronto to make publicly accessible for researchers and artists alike, and to design the Cabaret Commons as an online platform structured by community protocols, filters, ethics, and aesthetics of the stages and performance spaces that structure the social-cultural scenes that produce these materials.

After years of working with the idea of the Cabaret Commons, I named our method “Transmedial Drag,” and have elaborated how we work in the mode of transmedial drag along with co-investigator Jasmine Rault, and collaborators Dayna McLeod, Carina Guzman and Robyn Overstreet as well as many performance artists with whose work we are in conversation.

“Transmedial drag” is the method of study, knowledge production and citational practice involved in moving across media/mediums (for example, from live performance, to video documentation, to digital archive to online platform), which creates a sort of pastiche of the ‘original,' denaturalizing its status as ‘originary' and teaching us something new about the excesses and limitations of each media form. Riffing on Judith Butler's game-changing theorization of drag as a performance that, “[i]n imitating gender ...reveals the imitative structure of gender itself--as well as its contingency” (Gender Trouble 138). Or, thinking through Benjamin's anxieties about transmedial transfer in the early 20th Century, it's what we might call the work of

queer performance art in the age of digital reproduction...! Thus we practice a process-heavy collaborative protocol that seeks to “simulate,” not just the “aura” of live performance, but is also structured by locally-spe-cific, community-based social and political ethics as manifested in minoritized cabaret scenes. Transmedial drag compels us to reckon with the cultural and ethical limitations of digital accessibility in the face of technical possibility (cf. Robertson 2016).

The decolonizing, community-collaboration and accountability methods for the digital transmission of traditional Indigenous knowledges, practiced for example by the projects Mukurtu and Local Contexts, provide a generative model for negotiating this paradox of intimate privacies and networked cultural histories and memories. Indeed, Indigenous studies has pointed out the colonial and Western-expansionist logics and impulses underlying the push to open-access and digitization, and have developed networked information management and archiving systems that follow indigenous cultural protocols, prioritizing privacy and multi-tiered user-generated-access levels.

What we're calling transmedial drag refers to the troubling excess that remains in even the most ‘successful' transition from one medium or digital form to another - those elements that are incompatible to the form/medium as it currently exists and compel us to ask whether the form/medium exists because of these constitutive exclusions, or on the condition that certain elements remain excessive/external. Indeed, what remains excessive, incompatible and bit of a drag on the seductive techno-cultural possibilities of openness, are things like labour, consent, privacy, the agency and will of research-collaborators. For those of us trained in gender, sexuality, critical race, post and decolonial research, it becomes clear that our scholarly fields and practices of digital media and digital humanities can be sustained by reproducing a familiar set of constitutive exclusions -- those skills, values, techniques and priorities cultivated by and for the survivance of women, people of colour, indigenous people, trans people, queers, etc.

Transmedial drag is indebted to Elizabeth Freeman's work on ‘temporal drag' and Heather Love's on “feeling backward” -- or the ways that queerness is often experienced, expressed or interpreted as an attachment to the past, as a stall in personal or social development, a drag on cultural-temporal progress narratives. We are also indebted to Kadji Amin's work on transgender temporalities, which allow us to understand the ways that “transgender experiences are constituted by yet exceed normative temporalities” and the necessity of incorporating asynchronic temporalities “in order to do justice to the complex ways in which people inhabit gender variance” (220).

With Love, Freeman and Amin, our digital research-creation project is oriented to those people, scenes, cultures, affects and knowledges cast as a drag on the forward-momentum- digital-entitlement fantasies of unfettered access to everything, everyone, everywhere -- fantasies that resonate so strongly with the gender, sexual and racial histories of colonial-moder-

nity that we know too well. Thus in true queer epistemological form, “transmedial drag” is both a method of study modeled on queer performance and theories of performativity (i.e. Butler) and a conceptual framework through which to make sense of elements of cultural life and work that are generally unintelligible (nonsense).

Like the deep cultural understandings, consultation, collaboration and re-engineering that led to the development of Mukurtu and Local Contexts, we imagine culturally-sensitive and specific software that prioritizes highly nuanced cultural logics, protocols and specialized knowledges in the development of computational logics, protocols and specialized tools, towards what Kara Keeling calls a “Queer OS/operating system”. Keeling's QUEER OS is an operating system in which the “historical, sociocultural, conceptual phenomena that currently shape our realities in deep and profound ways, such as race, gender, class, citizenship, and ability (to name those among the most active in the United States today), [are understood as] mutually constitutive with sexuality and with media and information technologies, thereby making it impossible to think any of them in isolation” (152).

Indeed, transmedial drag denaturalizes the constitutivweorks for unobstructed open access - and these conti-exclusions of most of our media and information technolnou- ities need to be understood within a context of mod-gies; lets us both recognize their limits and imagine their neecr-n-colonial regimes of gender, sexuality and race. My essary transformations; and puts into relief some of the dpresentation focuses on early twentieth century namics of cabaret that continue to sustain trans- feminist anwdomen's queer interventions into modernist architec-queer scenes and lives even within media environments thftfral and design ideals as a genealogy to contextualize, are designed around their impossibility and imperceptibilitbye. tter understand and support recent innovations in In this moment when new digital archives of mi- decolonizing, queer, feminist and anti-racist designs


nority and subcultural scenes seem compelled to reproduce perilous Web 2.0 logics of unbridled open-access -- based in modern-colonial entitlement fantasies of absolute accessibility -- the Cabaret Commons is using trans feminist and queer relational epistemologies towards the decolonizing project of ethical collective platform protocols for sensitive cultural memory, heritage projects and accountable digital design. This is part of a larger epistemological-transformational project to decolonize both trans- feminist and queer practices, and the dominant digital cultural practices that assumes that all networks are networked publics leading us to theorize what we're calling networked privates.

The hope that we have for the Cabaret Commons is that it wmillodern production and regulation of sexuality, race bring the activated characteristics of cabaret performance, asn d gender. Moreover, from around 1900 to 1935,

well as other grassroots and politically-engaged live perfomr-odernist architecture was increasingly committed to

mance (like street performance, marches and protest artscommunicative clarity - or immersive living spaces of what Mexican artist/activist Jesusa Rodriguez calls “matotal communication - and several sexually dissident cabaret”) along with their translocal trans feminist and queer


scenes, ethics, politics, social and sexual lives to bear on digital archiving infrastructures.

Screens, Codes, Filters: sapphic modernist design genealogies for queer digital culture
Jasmine Rault
What can early twentieth century modernist architecture and design teach us about contemporary decolonizing, feminist, queer and anti-racist practices and protocols for networked digital architecture? Euro-Atlantic modernist architecture was driven by ideals of internationally standardized open communication - beyond the mass rail, steamboat and automobile distribution of low-cost print media and photographic reproductions towards material changes like the open floor plan, strip windows and glass walls, unimpeded visual access to interior and exterior space, eliminating walls and structural as well as decorative or symbolic obstructions to complete open access. There are some striking ideological continuities between these modernist architectural ideals (and aesthetics) and contemporary Euro-Atlantic values of unbridled digitization, designing global information net-

for networked digital practices, archives and spaces. I

draw from my research on Eileen Gray - which explores some of the intersections between histories of communication, European architectural modernity and sapphic modernity, or the cultural history of female sexual dissidence - and my collaborative research on “the Cabaret Commons,” designing a networked digital archive for trans feminist and queer performance artists, activists and audiences.

My research on Gray, and her contemporaries in queer interior designs, shows that modernist architecture was invested in creating not only new buildings and living spaces, but new bodies and subjects. As such, modernist architecture was contributing to the

female artists, writers, interior designers and architects, like Eileen Gray, worked to interrupt this clarity. Indeed, women in the US, Canada, Western Europe and the UK worked on domestic interior design to work out new possibilities for gender and non-heterosexualities.

trans-feminist and queer archives and social memory.

We are currently working with the data set of the digitlaicls' and we are working on collective action towards,

(digitized) archive of the Meow Mix Cabaret, a show anwdhat we might have learned from Gray and her condance party for “bent girls and their buddies” which ran tienmporaries: techniques of mediated privacy. Montreal from 1997-2012. Technically, we have been granted

permission by the copyright holders to make this archiQueering the Vaginal Canal: Make Art Here public through an online platform. However, through interviews with individual performers who have appeared on the Dayna McLe°(J

Meow Mix stage over the years, we've learned that a majority My presentation will examine how feminist per-of the artists are not interested in reproducing or getting ifno-rmance artists have employed explicit perfor-volved in the distribution of a full-run, open-access digital amr-ance-based practices in their work, specifically chive of their materials - for many reasons: including low obry using their vaginal cavity as a site of art produc-degraded quality of the video and images; the unpotil-on. I will compare these works, practices, and ished/amateur aesthetic of their work at an earlier stage mofethodologies with my own, specifically citing their careers; the fact that they did the performance for theUterine Concert Hall, a vaginal media work that friends or for a particular event and do not want broad circfue-atures my body as a concert venue. Equipped lation that will leave a digital trace; gender, sex, sexualitwy,i th a 54 kHz internal speaker (Babypod), my vag-body shape and size transitions; nudity; and the potentianlal canal acts as the stage with my cervix as the hazards of being associated with trans- feminist and queperoscenium, for the audience of my uterus. A live scenes. However, many artists are interested in some of theDirJ pumps sound directly into me via 6-foot cable work being available online for some people. And thus bthyat reaches from their booth. My vaginal canal is working through what we are calling a speculative/pragmatihce scene of the performance and the instrument of method of transmedial drag, we are designing an online spaictes production. I can feel the DJ's varying frequen-


I focus on the creative extent to which these possibilities depended on codes, screens, filters - as interpretive and communicative strategies, but also as design materials. That is, the aesthetic, social and cultural phenomena of female gender and sexual queerness was enabled at the start of the 20c through complex claims to privacy. My presentation draws connections to my current book project, co-written with T.L. Cowan, provisionally entitled, Checking In: Feminist Labour in Networked Publics, which takes up the ways that similar claims to mediated privacy are emerging as central concerns in the design of very different spaces - online digital designs for networked in which this kind of mediation will be possible.

In this presentation, I suggest that understanding

the architectural innovations of Gray and her contemporaries, who built on the premise of mediated privacy, on filtered access, offers us a unique genealogy to studying and designing digital architecture that respond to similar needs and desires for online spaces through which minoritized subjects push back against the dominant pressure for full publicity, for the full availability and open access to our online selves, socialities and intimacies. The metaphor of architecture is

often used in reference to online built environments but rarely is the metaphor pushed to a point of usefulness. Danah Boyd has argued that “what it means to be public or private is quickly changing before our eyes and we lack the language, social norms, and structures to handle it” (2007). Indeed, queers, people of color, indigenous people, trans folks, and disabled people are hacking popular social media corporate sites like Facebook, Twitter, Instagram, Snapchat, etc. to build differentiating filters in order to choose which selves to “out” in which contexts, and organizations like the Feminist Technology Network (FemTechNet), the Center for Solutions to Online Violence (CSOV), Fem-Bot, Crash Override (Zoe Quinn & Anita Sarkesian) are working not only to build these infrastructures but also to communicate the differential consequences of total publicity, open access, or the digital ‘open plan.' That is, we can see the development of networked ‘privates' rather than the fetishization of ‘networked pub-

cies, pitch shifts, and throbbing tones while external concertgoers are invited to eavesdrop via stethoscope, on the faint echoes of the recital through the very flesh of my body. Like showing up to a concert and listening from outside, this piece purposefully excludes external listeners while engaging with explicit performance-based production practices, and feminist art practices of intimacy.

In The Explicit Body in Performance, Rebecca Schneider identifies Annie Sprinkle's cervix,

viewed similarly through a medical device (a spec

ulum) in Public Cervix Announcement (1990-1992),

as a “theoretical third eye,” a counter-gaze that

looks back at the viewer who is gazing at Sprinkle's

cervix (55). In Uterine Concert Hall, the viewer's

gaze and concentrated attention is focused on me

and on the message my body embodies for their

speculation: the spectator sees, hears, and touches

my body while I absorb both the concert through

my vagina and their expectations. My “theoretical

third eye” listens. Schneider also notes that “any

body bearing female markings is automatically

shadowed by the history of that body's significa

tion” (20), and the invasive imagined construction

of that body. For an external audience to Uterine

Concert Hall, my uterus-as-intended-audience -

cervix-as-proscenium - vagina-as-stage, all become

a singular blank screen of possibilities for viewers

to project their fantasies onto: to imagine its ap

pearance and construction, and imagine what

sounds might emit from my flesh. How might my

body distort or otherwise enhance the anticipated

DJ set? (How) does sound transform the body (my

body)? By the time the viewer arrives at my side

where they are handed a stethoscope to eavesdrop

into/onto/through my flesh, their anticipation and

fantasy is confronted by the very real presence of

the mise en scene. Here they are featured in the

role of pseudo-physician, which is complicated by

issues of consent, interior/exterior bodily access,

and their demands of the body (my body) as sound

medium.

This outsider, exclusionary status that the viewer is assigned also comes with a controlling, medicalized, obstetrical gaze. In Canada and the United States, we employ much monitoring of women's bodies through our expectations of how women look, behave, act, and feel. These expectations are pathologized, reinforced, and legitimized through medicalized surveillance and control. Invasive and non-invasive examinations and procedures like ultrasounds, transvaginal probes, and visualizing and monitoring technologies are normalized for bodies marked female who are evaluated by a normalized and medicalized gaze in relationship to potentially housing or not housing a fetus (Balsamo). “Protection of the fetus is often offered as a com-monsensical, and, hence, ideological rationale for intervention into a woman's pregnancy, either through the actual application of invasive technologies or through the exercise of technologies of social monitoring and surveillance” (Clarke and Olesen). Uterine

Concert Hall is not a place for babies, fetuses, or heteronormative determinism. However, it is a site that questions these cultural assumptions of women's bodies, of what we expect from bodies marked female, and why we think we have the right to make any kinds of demands on these bodies in the first place. This project does this by using digital technologies and affordances to interrupt their intended functions (i.e. playing music for a uterus-bond fetus from an adjacent vaginal canal) that contribute to the medicalized surveillance and control culture of women's bodies.

In this panel presentation, I am also interested in discussing Uterine Concert Hall in its queering of the uterus as a viable physical space, the performing body, and the artist-audience exchange. Further, I am interested in putting Uterine Concert Hall in conversation with the works and practices of artists who have similarly used vaginal and uterine space as physical space as the site of their works' production, like Annie Sprinkle's Public Cervix Announcement (1990-1992), Carolee Schneemann's Interior Scroll (1975), and Casey Jenkins' Vaginal Knitting (2013).

“How many lesbians does it take to flyer for a party?”. The impact of shifting digital landscapes on queer women's nightlife organizing in Mexico City since 2005.

Carina Guzman
In this paper, I open an inquiry on how the evolution of the digital landscape has brought shifts in the strategies of queer women's nightlife organizing in Mexico City since 2005, and what the possible implications of these shifts are.

Following Cowan and Rault's concept of transmedial drag, I ask if and how social media platforms and media such as memes, used as organizing/politi-cal tools, can possibly take the shape of/mimic/reflect the queer world-making of the social scenes they are being used to create, especially when contrasted with the resources they have replaced such as e-mails or flyers, which necessarily imply physical contact between organizers and the queer crowd. I pay special attention to independently organized events held outside the circuit of commercial gay nightlife venues; dance parties and cabaret revues in adapted spaces such as azoteas and casas de cultura. This leads me to consider the intersection of labour, material culture, digital culture, and the sexual politics of place-making. At the same time, I consider the shift within digital culture that has happened with the emergence of social media as an organizing tool, and memes as political statements.

In my doctoral research in the Communication Studies program at Concordia University I have established, partially through an autoethnographic narrative, that independent and capitalist-alternative lesbian and queer women's nightlife organizing in Mexico City is a political project that responds to issues of economic gender disparity and spatial justice.

The small lesbian collective Meras efímeras was formed in 2005 by a group of friends I was a part of to organize nightlife alternatives for queer women in Mexico City. At the time, the main nighttime recreational events for queer women were “ladies' nights” at gay men's bars and clubs. Though inexpensive and conveniently located in the city's gay ghetto, these were not especially well organized; patrons could often find poor service, strip shows that made some feel uncomfortable and unclean facilities. It was evident that they were held to create a niche clientele on slow weeknights.

Meras efímeras contended that within the capitalist logic of commercial nightlife, a queer women's crowd could not “compete” with an audience of gay men, making it virtually impossible to find women-oriented or women-welcoming queer events on the weekend. We, thus, understood that independently organizing nightlife implied taking a political stand on the economic disparity between men and women, and the issue of spatial justice for queer women in the urban nightscape. This also implied we could not expect to be paid for our work. And, as we were also unable to pay rent at a club, we had to physically adapt spaces or negotiate agreements at other types of venues outside of the gay ghetto.

Today's use of memes to make political statements, as well as the use of platforms such as Facebook event pages, Twitter and Instagram as an event organizing tools contrasts sharply with what it was like to call on a queer crowd for a political/social event around 2005. Our main digital resource was a Yahoo! Groups mailing list to which we would manually add e-mails requested at the door of our events. We also advertised in the local LGBT free weeklies. But, the first point of contact were flyers we'd drop off at businesses or hand-out outside “ladies' nights” events.

In this paper I establish that the simultaneous use of paper-based and e-mail group based strategies Meras efímeras employed around 2005 constitute a presocial media-as-we-know-and-use-it-today organizing landscape that straddled material and digital culture. While this strategy was partially material and partially digital, it necessarily implied exchanges made physically; handing out of the flyer on the street, requesting and giving an e-mail at the door of an event. So, this paper also asks what are the stakes in queer women's nightlife organizing, as a political project, in the transfer from organizing strategies that implied a physical presence to current social media resources. Moreover, I ask if resources such as memes, Facebook event pages, Tweets or Instagram are capable of an effective transmedial drag; can they properly represent queer women's nightlife political world-views?

Bibliography
Amin, K. (2014). “Temporality.” Transgender Studies Quarterly 1 (1-2): 219-22.

Balsamo, A. M. (1995) Technologies of the Gendered Body: Reading Cyborg Women. Durham NC: Duke University Press.

Butler, J. (1998). Gender Trouble: Feminism and the Subversion of Identity. 1st ed. New York: Routledge.

Clarke, A. E., Olesen, V. (2013) Revisioning Women, Health and Healing: Feminist, Cultural and Technoscience Perspectives. London and New York: Routledge.

Freeman, E. (2010). Time Binds: Queer Temporalities, Queer Histories. Durham N.C.: Duke University Press Books.

Keeling, K. (2014). “Queer OS.” Cinema Journal 53 (2): 15257.

Love, H. (2007). Feeling Backward: Loss and the Politics of Queer History. Cambridge, Mass: Harvard University Press.

Robertson, T. (2016). “Digitization: Just Because You Can, Doesn't Mean You Should.” Tara Robertson. March 20. http://tararobertson.ca/2016/oob/.

Schneider, R. (1997) The Explicit Body in Performance. London and New York: Routledge.
Natural language processing (NLP) is a research area that stands at the intersection of linguistics and computer science; its focus is the development of automatic methods that can reason about the internal structure of text. This includes part-of-speech tagging (which, for a sentence like John ate the apple, infers that John is a noun, and ate a verb), syntactic parsing (which infers that John is the syntactic subject of ate, and the apple its direct object), and named entity recognition (which infers that John is a Person, and that apple is not, for example, an Organization of the same name). Beyond these core tasks, NLP also encompasses sentiment analysis, named entity linking, information extraction, and machine translation (among many other applications).

Over the past few years, NLP has become an increasingly important element in computational research in the humanities. Automatic part-of-speech taggers have been used to filter input in topic models (Jockers, 2013) and explore poetic enjambment (Houston, 2014). Syntactic parsers have been used to help select relevant context for concordances (Benner, 2014). Named entity recognizers have been used to map the attention given to various cities in American fiction (Wilkens, 2013) and to map toponyms in Joyce's Ulysses (Derven et al., 2014) and Pelagios texts (Simon et al., 2014). The sequence tagging models behind many part-of-speech taggers have also been used for identifying genres in books (Underwood et al., 2013).

There is a substantial gap, however, between the quality of the NLP used by researchers in the humanities and the state of the art. Research in natural language processing has overwhelmingly focused much of its attention on English, and specifically on the domain of news (simply as a function of the availability of training data). The Penn Treebank (Marcus et al., 1993)—containing morphosyntactic annotations of the Wall Street Journal—has driven automatic parsing performance in English above 92% (Andor et al., 2016); part-of-speech tagging on this same data now yields accuracies over 97% (S0gaard, 2011). While a handful of other high-resource languages (German, French, Spanish, Japanese) have attained comparable performance on similar data (Hajic et al., 2009), many languages simply have too few resources (or none whatsoever) to train robust automatic tools. Even within English, out-of-domain performance of many NLP tasks—in which, for example, a syntactic parser trained on the Wall Street Journal is used to automatically label the syntax for Paradise Lost—is bleak. Figure 1 illustrates one sentence from Paradise Lost automatically tagged and parsed using a tool trained on the Wall Street Journal. Since this model is trained on newswire, it expects newswire as its input; errors in the part-of-speech assignment snowball to bigger errors in syntax.


Figure 1: Parsers and part-of-speech taggers trained on the

WSJ expect newswire syntax. Automatically parsed syntactic dependency graph with part-of-speech tags for Long is the way and hard, that out of Hell leads up to light. Errors in part-of-speech tags and dependency arcs are shown in red. Part-of-speech errors snowball into major

syntactic errors.

Table 1 provides a summary of recent research that has investigated the disparity between training data and test data for several NLP tasks (including part-of-speech tagging, syntactic parsing and named entity recognition). While many of these tools are trained on the same fixed corpora (comprised primarily of newswire), they suffer a dramatic drop in performance when used to analyze texts that come from a substantially different domain. Without any form of adaptation (such as normalizing spelling across time spans), the performance of an out-of-the-box part-of-speech tagger can, at worse, be half that of its performance on contemporary newswire. On average, differences in style amount to a drop in performance of approximately 10-20 absolute percentage points across tasks. These are substantial losses.

Citation

Task

In domain

Accuracy

Out domain

Accuracy

RäüSQRetal. (2007)

POS

English news

97.0%

Shakespeare

81.9%

§^giWftetal.(2011)

POS

German news

97.0%

Early Modem German

69.6%

Moon and Baidridge, (2007)

POS

WSJ

97.3%

Middle English

56.2%

gfiSUagSidSMiand

Xaa&9dß(2008)

POS

Italian news

97.0%

Dante

75.0%

l&xsw&et al. (2013b)

POS

WSJ

97.3%

Twitter

73.7%

Yang and Eisenstein (2016)

POS

WSJ

Early Modem English

74.3%

Südsa(2oon

PS

parsing

WSJ

86.3 F

Brown corpus

80.6 F

Lease and CMroM (2005)

PS

parsing

WSJ

89.5 F

GENIA medical texts

76.3 F

ßjHgaetal. (2013)

Dep.

parsing

WSJ

88.2%

Patent data

79.6%

ggjggetal. (2014)

Dep.

parsing

WSJ

86.9%

Broadcast news

79.4%

Magazines

77.1%

Broadcast

conversation

73.4%

QSi«W&et al. (2013a)

NER

CoNLL 2003

89.0 F

Twitter

41.0 F

Figure 2: Out-of-domain performance for several NLP tasks, including POS tagging, phrase structure (PS) parsing, dependency parsing and named entity recognition. Accuracies are reported in percentages; phrase structure parsing and NER are reported in F1 measure.

While many techniques are currently under development in the NLP community for domain adaptation (Blitzer et al., 2006; Chelba and Acero, 2006; Daumé III, 2009; Glorot et al., 2011; Yang and Eisenstein, 2014), including leveraging fortuitous data (Plank, 2016), they often require specialized expertise that can be a bottleneck for researchers in the humanities. The simplest and most empowering solution is often to create in-domain data and train NLP methods on it directly; in-domain data can substantially increase performance, almost to levels approaching state-of-the-art on newswire. When adding training data of Early Modern German and adding spelling normalization, Scheible et al. (2011) increase POS tagging accuracy on Early Modern German texts from 69.6% to 91.0%; when Moon and Baldridge (2007) train a POS tagger on Middle English texts, this pushes their accuracy from 56.2% to 93.7%; when Derczynski et al. (2013b) train a POS tagger directly on Twitter data, this increases accuracy from 73.7% to 88.4%. In-domain data is astoundingly helpful for many NLP tasks, from part-of-speech tagging and syntactic parsing to temporal tagging (Strotgen and Gertz, 2012).

The difficulty, of course, is that training data is expensive to create at scale since it relies on human judgments; and the cost of this data scales with the complexity of the task, so that morphosyntactic or semantic annotations (which require a holistic understanding of an entire sentence) are often prohibitive. Few projects achieve this scale for domains in the humanities, but when they do, they have real impact - these include WordHoard, which contains part-of-speech annotations for Shakespeare, Chaucer and Spenser (Mueller, 2015); the Penn and York parsed corpora of historical English (Taylor and Kroch, 2000; Kroch et al., 2004; Taylor et al., 2006); the Perseus Greek and Latin treebanks (Bamman and Crane, 2011), which contain morphosyntactic annotations for classical Greek and Latin works; the Index Thomisticus (Passarotti, 2007), which contains morphosyntactic annotations for the works of Thomas Aquinas; the PROIEL treebank (Haug and J0hndal, 2008), which contains similar annotations for several translations of the Bible (Greek, Latin, Gothic, Armenian and Church Slavonic); the Tycho Brahe Parsed Corpus of Historical Portuguese (Galves and Faria, 2010); the Icelandic Parsed Historical Corpus (Rognvaldsson et al., 2012), and Twitter, annotated for part-of-speech (Gimpel et al., 2011) and dependency syntax (Kong et al., 2014).

The availability of these annotated corpora means that we have the ability to train NLP tools for some dialects, domains and genres in Ancient Greek, Latin, Early Modern English, historical Portuguese, and a few other languages; this doesn't help the scholar working on John Milton, Virginia Woolf, Miguel Cervantes, or the countless other authors and genres in the long tail of underserved domains that researchers are increasingly finding high-quality NLP useful to help analyze. In this talk, I'll argue for an alternative: an open repository of linguistic annotations that scholars can use to train statistical models for processing natural language in a variety of domains, leveraging information from complementary sources (such as the works of Shakespeare) to perform well on a target domain of interest (such as the works of Christopher Marlowe). What this repository critically relies on is the expertise of the individuals who simultaneously are the consumers of NLP for their long-tail domain and are in the uniquely best position to create linguistic data to support their own work—and in doing so, can help develop an ecosystem that can support the work of others.

Citation

Task

In domain

Accuracy

Out domain

Accuracy

RäüSQRetal. (2007)

POS

English news

97.0%

Shakespeare

81.9%

§^giWftetal.(2011)

POS

German news

97.0%

Early Modem German

69.6%

Moon and Baidridge, (2007)

POS

WSJ

97.3%

Middle English

56.2%

gfiSUagSidSMiand

Xaa&9dß(2008)

POS

Italian news

97.0%

Dante

75.0%

l&xsw&et al. (2013b)

POS

WSJ

97.3%

Twitter

73.7%

Yang and Eisenstein (2016)

POS

WSJ

Early Modem English

74.3%

Südsa(2oon

PS

parsing

WSJ

86.3 F

Brown corpus

80.6 F

Lease and CMroM (2005)

PS

parsing

WSJ

89.5 F

GENIA medical texts

76.3 F

ßjHgaetal. (2013)

Dep.

parsing

WSJ

88.2%

Patent data

79.6%

ggjggetal. (2014)

Dep.

parsing

WSJ

86.9%

Broadcast news

79.4%

Magazines

77.1%

Broadcast

conversation

73.4%

QSi«W&et al. (2013a)

NER

CoNLL 2003

89.0 F

Twitter

41.0 F

Figure 1. Out-of-domain performance for several NLP tasks, including POS tagging, phrase structure (PS) parsing, dependency parsing and named entity recognition. Accuracies are reported in percentages; phrase structure parsing and NER are reported in F1 measure.

Acknowledgements
Many thanks to the anonymous reviewers for helpful feedback. This work is supported by a grant by the Digital Humanities at Berkeley initiative.

Bibliography
Andor, D., Alberti, C., Weiss, D., Severyn, A., Presta, A., Ganchev, K., Petrov, S., and Collins, M. (2016). Globally normalized transition-based neural networks. In

Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2442-2452, Berlin, Germany, August 2016. Association for Computational Linguistics.

Bamman, D., and Crane, G. (2011) The ancient Greek and Latin dependency treebanks. In Language Technology for Cultural Heritage, pages 79-98. Springer, 2011.

Benne, D. C. (2014). Marrying the benefits of print and digital: Algorithmically selecting context for a key word. In Digital Humanities 2014, 2014.

Blitzer, J., McDonald, R., and Pereira, F. (2006) Domain adaptation with structural correspondence learning. In

Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP '06, pages

120-128, Stroudsburg, PA, USA, 2006. Association for Computational Linguistics.

Burga, A., Codina, J., Ferraro, G., Saggion, H., and Wanner, L. (2013). The challenge of syntactic dependency parsing adaptation for the patent domain. In ESSLLI-13 Workshop on Extrinsic Parse Improvement, 2013.

Chelba, C., and Acero, A. (2006). Adaptation of maximum entropy capitalizer: Little data can help a lot. Computer Speech & Language, 20 (4): 382-399, 2006.

Daume, H. (2009) Frustratingly easy domain adaptation.

arXiv preprint arXiv:0907.1815.

Derczynski, L., Maynard, D., Aswani, N., and Bontcheva,

K. (2013a) Microblog-genre noise and impact on semantic annotation accuracy. In Proceedings of the 24th ACM Conference on Hypertext and Social Media, pages 21-30. ACM, 2013.

Derczynski, L., Ritter, A., Clark, S., and Bontcheva, K.

Twitter part-of-speech tagging for all: Overcoming sparse and noisy data. In RANLP, pages 198-206, 2013b.

Derven, C., Teehan, A., and Keating, J. (2014). Mapping and unmapping Joyce: Geoparsing wandering rocks. In Digital Humanities 2014, 2014.

Galves, C., and Faria, P. (2010). Tycho Brahe Parsed Corpus of Historical Portuguese. http://www.tycho.iel.uni-camp.br/corpus/en/index.html .

Gildea, D. (2001) Corpus variation and parser performance. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 167202, 2001.

Gimpel, K., Schneider, N., O'Connor, B., Das, D., Mills, D., Eisenstein, J., Heilman, M., Yogatama, D., Flanigan, J.,

and Smith, N. A. (2011) Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, companion volume, Portland, OR, June 2011.

Glorot, X., Bordes, A., and Bengio, Y. (2011). Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 513-520, 2011.

Hajic, J., Ciaramita, M., Johansson, R., Kawahara, D., Marti, M. A., Marquez, L., Meyers, A., Nivre, J., Pado,
S., Stepanek, J., et al. (2009) The CoNLL-2009 shared

task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference

on Computational Natural Language Learning: Shared

Task, pages 1-18. Association for Computational Linguistics, 2009.

Haug, D. TT, and J0hndal, M. (2008) Creating a parallel treebank of the old indo-european bible translations. In

Proceedings of the Language Technology for Cultural Heritage Data Workshop (LaTeCH 2008), Marrakech, Morocco, 1st June 2008, pages 27-34, 2008.

Houston, N. (2014) Enjambment and the poetic line: Towards a computational poetics. In Digital Humanities 2014, 2014.

Jockers, M. (2013) “Secret” recipe for topic modeling themes. http://www.matthewjock-ers.net/2013/04/12/secret-recipe-for-topic-modeling-themes/, April 2013.

Kong, L., Schneider, N., Swayamdipta, S., Bhatia, A.,

Dyer, C., and Smith, N. A (2014). A dependency parser for tweets. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing

(EMNLP), pages 1001-1012, Doha, Qatar, October 2014. Association for Computational Linguistics.

Kroch, A., Santorini, B., and Delfs, L. (2004). Penn-Helsinki parsed corpus of Early Modern English. Philadelphia: Department of Linguistics, University of Pennsylvania, 2004.

Lease, M., and Charniak, E. (2005). Parsing biomedical literature. In Natural Language Processing-IJCNLP 2005, pages 58-69. Springer.

Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A.
(1993). Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19 (2): 313-330, 1993.

Moon, T., and Baldridge, J. (2007). Part-of-speech tagging for Middle English through alignment and projection of parallel diachronic texts. In EMNLP-CoNLL, pages 390399, 2007.

Mueller, M. (2015). Wordhoard. http://wordhoard.north-western.edu/, Accessed 2015.

Passarotti. M. (2006), Verso il Lessico Tomistico Bicul-turale. La treebank dell'Index Thomisticus. In Petrilli Raffaella and Femia Diego, editors, Il filo del discorso. In-trecci testuali, articolazioni linguistiche, composizioni logiche. Atti del XIII Congresso Nazionale della Societa di Filosofia del Linguaggio, Viterbo, Settembre 2006, pages 187-205. Roma, Aracne Editrice, Pubblicazioni della Societa di Filosofía del Linguaggio, 2007.

Pekar, V., Yu, J., El-karef, M., and Bohnet, B. (2014)Ex-

ploring options for fast domain adaptation of dependency parsers. SPMRL-SANCL 2014, page 54.

Pennacchiotti, M., and Zanzotto, F. M. (2008). Natural language processing across time: An empirical investigation on italian. In Advances in natural language processing, pages 371-382. Springer.

Plank, B. (2016). What to do about non-standard (or noncanonical) language in NLP. In KONVENZ, 2016.

Rayson, P., Archer, D., Baron, A., Culpeper, J., and Smith,

N. (2007). Tagging the bard: Evaluating the accuracy of a modern pos tagger on early modern english corpora.

In Proceedings of Corpus Linguistics (CL2007).

Rognvaldsson, E., Ingason, A. K., Sigursson, E. F., and
Wallenberg, J. (2012). The Icelandic Parsed Historical Corpus (IcePaHC). In LREC, pages 1977-1984, 2012.

Scheible, S., Whitt, R. J., Durrell, M., and Bennett, P.

(2011). Evaluating an ‘off-the-shelf' POS-tagger on early modern German text. In Proceedings of the 5th ACL-HLT

workshop on language technology for cultural heritage, social sciences, and humanities, pages 19-23. Association for Computational Linguistics, 2011.

Simon, R., Barker, E. T. E., de Soto, P., and Isaksen, L.
(2014). Pelagios 3: Towards the semi- automatic annotation of toponyms in early geospatial documents. In

Digital Humanities 2014.

S0gaard, A. (2011). Semisupervised condensed nearest neighbor for part-of-speech tagging. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 48-52. Association for Computational Linguistics, 2011.

Strotgen, J., and Gertz, M. (2012). Temporal tagging on different domains: Challenges, strategies, and gold standards. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Mae-gaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12), Istanbul, Turkey, may 2012. European Language Resources Association (ELRA). ISBN 978-29517408-7-7.

Taylor, A., and Kroch, A.S. (2000) The Penn-Helsinki Parsed Corpus of Middle English. University of Pennsylvania, 2000.

Taylor, A., Nurmi, A., Warner, A., Pintzuk, S., and Neva-lainen, T. (2006). Parsed Corpus of Early English Correspondence. Oxford Text Archive.

Underwood, T., Black, M. L., Auvil, L., and Capitanu, B.

(2013). Mapping mutable genres in structurally complex volumes. In Big Data, 2013 IEEE International Conference on, pages 95-103. IEEE, 2013.

Wilkens, M. (2013). The geographic imagination of Civil War-era American fiction. American Literary History, 25 (4): 803-840, 2013. 10.1093/alh/ajt045.

Yang, Y., and Eisenstein, J. (2014). Fast easy unsupervised domain adaptation with marginalized structured dropout. Proceedings of the Association for Computational Linguistics (ACL), Baltimore, MD, 2014.

Yang, Y., and Eisenstein, J. (2016). Part-of-speech tagging for historical English. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1318-1328, San Diego, California, June 2016. Association for Computational Linguistics.
Digital Humanities (hereafter - DH) recently became almost the most dynamically developing direction in the Humanities despite the Humanities crisis and scaling-down of Humanities studies in universities and academic centers all over the world. It is not only the transformation in Humanities that leads to this. Significant improvements in algorithms and computational tools that are being used for complex data, as well as social connections via new understandings of language, culture, and history also influence these phenomena. These up-to-date technologies, in addition to appealing to the natural sciences and mathematical knowledge, enhance the status of DH among the scientific community and broaden the opportunities for its rapid development.

At the same time, we still witness heated debates about the opportunities and perspectives of DH as a research field (Gold and Klein, 2016). The papers published in 2015-2016 emphasize the DH research priorities such as data analysis and data visualization, studying the creative process of software developers and so on.

Striving for a clear picture of DH development, one should study how dynamic the development of the DH infrastructure and its information environment are. That is the objective of the present study.

To collect the data for the study we performed the search queries in the Internet engines with the key word “digital humanities” in 6 European and 4 Eastern languages using some automated frameworks. This linguistic diversity the result of preliminary research that has shown that many DH units have their web sites in a national language alone. We processed 4000 search results via content analysis with the help of «Statistica v6.1.Ru» specialized software, Microsoft Access and MySQL databases, and ArcGIS. As a result, we have defined 430 science and education organizations these results lead to. These organizations/ units are active in various areas in Humanities and declare themselves as those related to DH (or we have defined them as such having analyzed their websites).

This DH information environment study is based on the analysis of the types of information interactions that appear in DH and form its information environment.

An analysis of the verified empirical data allowed the authors to define the organizational forms of the DH science and education units, the main research directions, and localize these units connecting them to

the map of the world. The map (open for update) has been designed via an open online tool Google My Maps (Department of Informatics Problems of the Humanities, 2017)

Before designing the interactive map, we developed the DH centre database including such fields as title, address, abstract, and information about the unit head, website, contacts, and date of foundation. ArcGIS allows us to automatically process the data on the map according to the database fields and visualize the search results, providing an understandable picture of the digital humanities across the globe.

The results of the preliminary study performed in 2015 have been published earlier (Mozhaeva et al, 2016). In the present paper we focused on DH units rather than on papers, extended the data and updated it (as of August 2016). We have extended the number of resources, languages and DH units, finished an interactive map of DH centres and a catalogue of open access software for DH studies that are going to simplify the search for new partners, developing network projects.

Those 430 units in 42 countries that we have found are spread over the world (Figure 1):

• in Europe: 174 units, mostly in Great Britain (27), France (25), Germany (21) and Russia

(17);

• in Asia: 65 units, mostly in Japan (33) and China (16);

• in North America: 9 units, including 7 in Brazil and 2 in Argentina;

• in Pacific basin countries and Australia: 17 units (11 in Australia and 6 in New Zealand);

• in Africa: 5 units (Republic of South Africa).


Figure 1. Interactive map of DH centres (screenshot)

The mostly widespread unit types are centres (139 units that is 32%), laboratories (67 units i.e. 16%), institutes (59 units, 14%) and university units (departments, subdivisions, faculties, schools) (57 units, 13%). There are 14 “groups”, 13 “initiatives” and 13 ‘societies” that give 3% of the general number of units that have been found.

The dynamics of creating the DH units shows that about 20% of them have been created in the 1960-80's at the major universities as centres for the Humanities research that were positioned as DH units in 19902000. About 25% of the studied units have been created in the 1990’s, 39% - in the 2000’s, 16% - in the 2010's. A steady increase in the number of DH units is observed since the mid 2000’s. In the 2010’s there is active institutionalization of DH: permanent organizations, educational and research units (centre, laboratory, institution, department, chair, school) at major universities, research institutions, etc. are being established. Among the studied 430 DH units such units amount to more than 75%. At the same time, the number of temporary units and teams (projects, groups, initiatives, etc.) created to solve specific problems is being reduced.

There is a growing attention to the educational activities in the field of DH (masters and postgraduate programs, short-term training and courses), which is typical for 26% of the studied DH units. The applied

developments are becoming more and more important:

•    22% of the studied units develop and introduce new digital tools, methods and models;

•    27% of units provide a variety of digital resources, services and platforms, mobile applications, multimedia systems, 3D-models,

GIS objects, etc.;

•    7% of units develop online tools for learning.

The analysis of the main directions of scientific and educational activities in the field of DH found that in the number of the DH units the focus is not only on the use of digital tools, but also on the study of the results of their application, the impact on the transformation of learning processes in the field of the Humanities and social sciences.

We spotted researchers’ consolidation in the framework of this research field, as well as the development of common principles, methods and scientific digital tools. We revealed 28 network associations (association, network, platform, consortium, alliance) that are both national and international, mostly continental and transcontinental (Mozhaeva et al, 2016).

The development of DH network infrastructure goes along with the extension of information interactions among specialists from diverse DH units worldwide. The infrastructure of DH centres reflects the units of the DH information environment. The increasing trend of centres comes with the trend for extension of the information environment and of those information interactions that build it.

Another trend in DH is intercultural information interactions that might become real during face-to-face meeting at the international conferences, workshops, seminars and so on, or continue virtually via the Internet technologies video presentations, webinars, forums, social networks, blogs and others. It is worth mentioning that virtual communication is very common for DH (it is possible via virtual research environments, communities, networks and associations, specialized online resources, services and platforms, Skype, blogs, forums, publications and discussions in social networks). Studying that type of interactions allows us to model the DH information environment and forecast the perspectives of this multidisciplinary field.

An example of today’s indicator of the intense and effective information interactions in DH environment is for instance a growth in number of blogs, e-journals and pages devoted to the DH, as well as number of subscribers to these editions. The social networks analysis (Twitter, Facebook, LinkedIn, Instagram, YouTube, VK) shows that the most popular and ever-growing platform for DH is Twitter. The number of subscribers

to @DHQuarterly and @DHNow on Twitter is 7,034 and 23,6 thousand people accordingly in August 2016.

The study that we performed allowed us to see the scale of the extension of digital humanities, visualize and localize 430 DH units, classify the major directions of their activity. The interactive map of DH centres extends the opportunities for research communication, facilitates setting the conditions for integrative processes in the development of Digital Humanities, as well as launching multidisciplinary and international research and educational projects. We consider the growing number of DH centres, as well as the qualitative change in their activity and expansion of the information environment, as evidence of the dynamic development of this field.

Acknowledgements
This work is supported by the grant of the Russian Humanitarian Science Foundation No. 14-03-00659 «The humanities in the digital age: from branch informatics to digital humanities».

Bibliography
Gold, M. K. and Klein, L. F., eds. (2016). Debates in the Digital Humanities. Published by the University of Minnesota Press. 2016. 632 p. [Electronic resource] // URL: http://dhdebates.gc.cuny.edu/ (access date: 07.10.2016).

Prototype of the interactive map of the Digital Humanities centers [Electronic resource] Department of Informatics Problems of the Humanities, Tomsk State University // URL: http://huminf.tsu.ru/nir/dh/map.htm (access date: 07.10.2016).

Mozhaeva G.V., Mozhaeva Renha P.N., Zakharova U.S,

(2016).. Information Environment of Digital Humanities: Analysis of Information Interactions // Journal of Siberian Federal University. Humanities & Social Sciences 7

(2016 9) - P. 1572 - 1585.

DigitalHumQuarterly [Electronic resource] // Twitter. 2016. URL: https://twitter.com/dhquarterly (access date: 07.08.2016).

DigitalHumanitiesNow [Electronic resource] // Twitter. 2016. URL: https://twitter.com/dhnow (
Introduction

Twitter data have been growingly used as a source for scholarly studies in various disciplines in recent years (Williams et al., 2013). The value of such borndigital data as primary source materials for future researches in history is already being acknowledged (Webster, 2015, Steinhauer, 2015). But, at least for now, historians seem rather reluctant to make use of them, although some recent works deal with the perception and memory of the past on Twitter (Clavert, 2016, Turgeon, 2014) or propose both documentation and analysis of present time events (Ruest and Milligan, 2016).

One possible reason of this reluctance could be the attachment of historians to traditional archival collections, e.g. those organized by professional archivists. But the creation of archives for social network sites data is not yet systematic and still in the beginning. As for the global Twitter archive of the Library of Congress, it is unknown when it will be functional (Zimmer, 2015). As it is possible for one to retrieve Twitter datasets, a second reason of this reluctance could be the need for acquaintance with basic methods and tools for gathering, understanding and analyzing born-digital data. However, not all historians are trained to digital humanities and quantitative methods that provide for such skills. Last but not least, the main reason could be the relation historians have with

time. It has always been difficult to define the moving

frontier between the present time and the recent past in contemporary history (Bedarida, 2003). As instant ephemera data that belong in the very present time, Twitter data precisely underline the difficulty for historians to define their own territory in these temporalities.

Nonetheless, for historians concerned with contemporary historical events - historical in the sense of a conjuncture that reveals a before and an after (Le Goff, 1999) - Twitter provides an original documentation. This documentation is generated in real time; organized around folksonomies - the hashtags - that reveal a direct perception from below; but also in close relation with media coverage. Since the creation of Twitter, a series of hashtagged global events (#IranE-lection, #15M, #Occupy, the 2011 Arab revolutions under various hashtags, the 2015 terrorist attacks in Paris...) update the concept of the monster-event (Nora, 1972) in that they are produced, lived, transmitted and shared in real-time around the globe - or at least in its connected parts. In spite of the known biases, mainly the fact that it is mainly used by relatively young and highly educated adults (Pew Research, 2016), Twitter offers an original kind of non-institutional primary sources (tweets) that can be complementary to the traditional ones the historians use.

This paper is a tentative to document and to provide a first analysis, based on Twitter primary sources, of the Greek referendum of 2015. This event has already obtained a distinctive status in the “before” and the “after” the current crisis marks in Greece's posttransition to democracy history (after 1974) (Avgher-idis et al., 2015), although time and future historians will definitely tell. The paper considers the transnational phase of this event, which followed the Greek vote in favour of the “no” to further austerity measures, included negotiations in the instances of the EU and ended with the agreement of the Greek government to conclude a third harsh austerity programme. Our main research hypothesis is that the imbrication of different hashtags reveals different temporalities that allow researchers to construct regimes of historicity of an event.

Event background

In the aftermath of the 2008 financial crisis, Greece, an EU and Eurozone member, began going through a severe debt crisis that revealed the structural weaknesses of the European monetary union and soon expanded to other weak members (Portugal, Ireland, Cyprus and Spain). Since 2010, the crisis has been managed through the setup of a European financial assistance mechanism in exchange for national programmes of structural reforms and budgetary cuts. Two such programmes were applied to Greece in 2010 and in 2012, plus a debt restructuring, that were monitored by the European Commission, the European Central Bank and the International Monetary Fund (Papaconstantinou, 2016; Zettelmeyer, 2013). The ongoing crisis provoked profound social and political transformations in the country that brought to power a coalition government led by the radical Left party of Syriza in January 2015. Syriza won the election with the promise to put an end to austerity politics. The party emerged in the context of the post-2008 crisis that shook the countries of Southern Europe and the 2011 Indignant movements, just like Podemos in Spain. Thus, the referendum of July 2015 was far from being significant only in the context of the Greek crisis as an effort of the new government to ameliorate the terms of the Greek programmes, as it put at stake different visions for the EU and its crisis management politics.

Data collection and analysis: method and tools

Tweets using the hashtag #greferendum were collected with NodeXL, an add-in to MS Excel (Smith et al., 2009). The collect was setup once daily from July, 6 to July, 16 2015. The size of the gathered sample was determined by the capacities of the tool, that can collect a maximum of around 20,000 tweets at once. A total of 204,714 tweets were collected of which 139,945 are retweets (68,36 %), 8, 686 responses (4,24 %), 56,086 mere tweets (27,39 %). Minor collects were also launched for other related hashtags (mainly #thisisacoup). Hashtag data were treated with Open-Refine and further explored with R software.

Statistical analysis of textual data (tweets) was made with TXM-Textometry software (Heiden, 2010). The corresponding dataset had been previously encoded following the TEI P5/XML standard with use of the OxGarage service.

Social network analysis and visualizations were

made with Gephi software (Bastian et al., 2009).

Data analysis: first findings

Hashtags

The first part of the research focused on reading the hashtags of the dataset. The hashtag #greferen-dum was used with a variety of hashtags, a total of some 12,000 words (all languages and variants included). A first study focused on the hashtags with a frequency over 99, which gave a total of 158 words. After an elementary typology was established, it was possible to distinguish: geographic names, names of persons, institutions, common names, neologisms that came out of contractions (such as “greferendum”), short phrases that had the function of commentary.

The use of hashtags varied between tag and commentary, or included both functions at once (Bruns and

Burgess, 2011). The big majority of hashtags are in English (112 out of 158). However, in the thirty most frequent hashtags of the dataset, it is possible to find

words in Spanish, Italian, French, and German. By consequence, the linguistic communities that participated in the global interactions were the ones that were the most concerned by the crisis. As for the Greek language, it is not entirely absent as such, but it is mainly present in its greeklish form: Greek words used as hashtags but written in Latin alphabet.

A close reading of the thirty most frequent hashtags with parallel consideration of the associations of words (coocurrencies) shows the tractations that followed the Greek referendum were basically perceived as an intergovernmental affair with the EU actors occupying a secondary position.

An interesting case is the emergence of the hashtag #thisisacoup as an act of solidarity of Spanish militants of Barcelona en Comu towards the Greek government during the Eurogroup and the Euro Summit negotiations (12-13 July). The corresponding dataset is more oriented to the expression of personal opinion than the dissemination of information with hashtags in the form of phrases that function more as commentary than tags (such as #yovoycongrecia).

Domains

The most tweeted domains were twitter.com (7,352 tweets) and theguardian.com (7,217 tweets). In general, it is possible to distinguish two main tendencies. First, the dissemination of information in social media (Twitter, YouTube, Instagram, Facebook ). Second, the dissemination of authoritative information (international media, specialized independent blogs, personal blogs).

Communities detection

The network of the #greferendum corpus is composed by 103,733 nodes and 204,713 relations. After nodes with a degree higher than 10 were isolated (around 4% of the total), 326 communities were detected with Gephi (Louvain method). These communities need to be further explored, however the first findings for the most important of them show that affinities developed around sources of information (media), political and/or intellectual personalities, professional communities, and also linguistic communities. Conclusion

The network and the detected communities seem to have been structured around the dissemination of information but also political affinities and/or mili-tantism. However, further exploration is necessary in order to better understand the network structure.

A quantitative analysis of the tweets, with emphasis on the associations between the hashtags, indicate coexistence of different temporalities within the temporality of the 2015 Greek referendum that are principally related to the Eurozone crisis, the associated national sub-crisis, and post-2008 anti-austerity movements. In this sense, Twitter primary sources offer insights from a transnational scale.

Bibliography

Avgheridis, M., Gazi, E. and Kornetis, K. (eds) (2015).

M£TanoAÎT£uan. H EÀÀafta oto p.£Taixp.io Svo auiivoiv

[Metapolitefsi. Greece Between Two Centuries]. Athens:

Themelio, pp. 15-18 and 335-66.

Bastian, M., Heymann, S., Jacomy, M. (2009). “Gephi: An

Open Source Software for Exploring and Manipulating

Networks.” International AAAI Conference on Weblogs and Social Media. San Jose, California: Association for the Advancement of Artificial Intelligence, pp. 361-62

Bédarida, F. (2003). Histoire, critique et responsabilité.

Brussels: Complexe, p. 64

Bruns, A. and Burgess, J. (2011). “The Use of Twitter

Hashtags in the Formation of Ad Hoc Publics”. Proceedings of the 6th European Consortium for Political Research

(ECPR) General Conference. Reykjavik: University of Iceland. Available at: http://eprints.qut.edu.au/46515/

Clavert, F. (2016). “#ww1. The Great War on Twitter.” Digital Humanities 2016: Conference Abstracts. Krakow: Jagi-ellonian University & Pedagogical University, pp. 46162. Available at: http://dh2016.adho.org/abstracts/378

Hanneman, R. A. and Riddle, M. (2005). Introduction to social network methods. Riverside, California: University of California, 2005

Heiden, S. (2010). “The TXM Platform: Building OpenSource Textual Analysis Software Compatible with the TEI Encoding Scheme.” 24th Pacific Asia Conference on Language, Information and Computation. Sendai: Institute for Digital Enhancement of Cognitive

Development, Waseda University, pp.389-398. Available at: https://halshs.archives-ouvertes.fr/halshs-

00549764/document

Le Goff, J. (1999). “Les « retours » dans l'historiographie française actuelle”. Les Cahiers du Centre de Recherches

Historiques,    22. Available at: http://ccrh.re-

vues.org/2322 ; DOI:10.4000/ccrh.2322

Nora, P. (1972). "L'événement monstre". Communications, 18: 162-72

Papaconstantinou, G. (2016). Game Over: The Inside Story of the Greek Crisis. Middleton, Delaware: CreateSpace

Pew Research Center (2016). “Social Media Update 2016”. Available    at:    http: //www.pewinter-

net.org/2016/11/11/social-media-update-2016/

Ruest, N. and Milligan, I. (2016). “An Open Source Strategy for Documenting Events: The Case Study of the 42nd Canadian Federal Election on Twitter.” Code4Lib Journal, 32. Available at http://iournal.code4lib.org/arti-cles/11358

Smith, M., Shneiderman, B., Milic-Frayling, N., Rodrigues, E.M., Barash, V., Dunne, C., Capone, T., Perer, A. and Gleave, E. (2009). “Analyzing (Social Media) Networks with NodeXL.” Proceedings of the Fourth International Conference on Communities and Technologies. New York: ACM, pp. 255-64

Steinhauer, J. (2015). “Preserving Social Media for Future Historians.” Insights. Scholarly Work at the John W. Kluge Center.    Available    at:

https://blogs.loc.gov/kluge/2015/07/preserving-so-cial-media-for-future-historians/

Turgeon, A. (2014). “Comment travailler la mémoire sur Twitter”. Études canadiennes / Canadian Studies, 76. Available at: http://eccs.revues.org/216

Webster, P. (2015), “Will Historians of the Future Be Able to Study Twitter ?”. Webstory, Peter Webster's Blog. Available at: https://peterwebster.me/2015/03/06/fu-ture-historians-and-twitter/

Williams, Sh.-A., Terras, M. and Warwick, C. (2013). “What people study when they study Twitter: Classifying Twitter related academic papers.” Journal of Documentation, 69 (3): 384-410

Zettelmeyer, J. , Trebesch, Ch. and Gulati, M. ( 2013) “The Greek debt restructuring: an autopsy”. Economic Policy, 28 (75): 513-63.

Zimmer, M. (2015). “The Twitter Archive at the Library of Congress: Challenges for information practice and information policy." First Monday 20 (7). Available at: http://firstmonday.org/ois/index.php/fm/arti-cle/view/5619/4653
Introduction
La rigueur de la critique des sources historiques est-elle soluble dans l'immédiateté et la course à l'audience des médias sociaux ? Depuis plusieurs années, les projets de médiation culturelle se multiplient, en particulier sur Twitter où la brièveté des messages se prête bien à une nouvelle forme de storytelling historique et dont le public est particulièrement curieux et avide de retweeter l'une ou l'autre photographie d'ar-chive documentant la grande (et plus rarement la petite) histoire de l'humanité ou de participer à la commémoration d’un événement (Clavert et al. 2015). Mais les institutions patrimoniales et les universités ne sont pas les seules à mettre ainsi en valeur leurs collections et leurs compétences, elles sont aujourd’hui largement dépassées par des comptes créés de toutes pièces pour partager automatiquement des images célèbres ou des citations flatteuses à des millions d’abonnés, sans contextualisation ni précautions de véracité, de licence ou de liens vers la source originale. Alors que certaines de ces initiatives sont le fait de passionnés, qui s’attachent à un événement qui les touchent personnellement et ont à coeur de le faire vivre à leurs abonnés, nombre de pages Facebooks/Twitter sont en fait produites en série par des individus qui, conjointement à des comptes partageant les plus belles photos de paysages du monde, d’animaux ou de personnalités célèbres, construisent des audiences de dizaines de millions d’internautes à des fins publicitaires.

Nous proposons une analyse d’un certain nombre de comptes Twitter présentant des caractéristiques très différentes. On verra que la montée en puissance des comptes commerciaux est basée sur des stratégies relativement simples, de l’achat de followers à l’autoréférencement. Dans un second temps, nous établirons une typologie des usages des documents historiques à des fins de communication sur les médias sociaux. Ce classement, qui se veut également un outil pour les institutions qui seraient tentées de mettre au point leur propre projet de médiation culturelle sur ces plateformes, montrera en particulier que la palette des usages ne se restreint pas forcément au partage de ressources mais tient parfois beaucoup plus du récit et de la mise en scène.


Figure 1. Exemples de comptes Twitter à caractère historique.

Prendre la mesure d'un phénomène
Un compte Twitter comme @HistoryInPics génère

quotidiennement plus de 10,000 retweets, alors que

les social media editors de la Bibliothèque nationale de France (@GallicaBnF) dépassent occasionnellement la centaine d'interactions, malgré leurs contenus originaux et souvent présentés avec humour. Mais plus d'abonnés ne veut pas toujours dire plus d'engagement de ceux-ci, surtout quand une majorité d'entre eux sont inactifs : c'est pourquoi nous analyserons en détail les publications d'une dizaine de comptes pendant un mois. La quantification de ces audiences (fig. 2) est un passage obligé pour comprendre l'ampleur du phénomène et tenter de comprendre la nature de ces publics. Fondamentalement, si ces démarches -même commerciales - ont un tel succès, c’est qu’il est temps que les historiens se saisissent de la question.

On verra en particulier que les audiences des grands comptes peu scrupuleux se recoupent largement, un phénomène qu’on expliquera en détail par l’analyse d’un échantillon de plusieurs dizaines de milliers d’abonnés. On s’attachera également à dresser, pour les comptes d’institutions patrimoniales, une statistique des contenus les plus susceptibles de créer un fort engagement. Encore une fois, il s’agit d’adopter une posture d’observation, de décoder des pratiques numériques souvent en décalage avec les idées reçues qui circulent dans les milieux académiques et de questionner la relation ambigüe entre recherche historique et valorisation de celle-ci.

Engagement


Figure 2 : Analyse d'un mois de tweets de quatre comptes Twitter partageant des contenus historiques de manière très différente (Grandjean 2014). Chaque point est un tweet dont

le succès est représenté sur une échelle logarithmique de retweets+favoris. Les boîtes de droite résument la dispersion du résultat (50% des tweets de chaque compte obtient un “engagement” qui tient dans la zone définie par la boîte correspondante).

Décrire les usages
Certains mettent valeur leurs collections, d'autres en font des exercices (Steffen et Nunes Coelho 2014), cherchent le buzz pour lancer une start-up en communication numérique, luttent pour qu'une cause historique soit reconnue ou mettent en scène leur grand-

père au travers de son journal de guerre, mais tous ont

en commun l'utilisation de documents d'archives -souvent photographiques - dans leurs micro-messages.

Dans cette recherche, nous ferons la différence entre les communications basées sur la disponibilité d'archives et celles qui, pour illustrer un récit, font usage de documents d'archive (fig. 3). Paradoxalement, c'est dans la première catégorie que l'on trouve les usages les plus « purs » comme les usages les plus critiquables : d'une part des institutions d'archive qui partagent des pièces de leurs collections ou des chercheurs qui consultent des documents et tweetent les « perles » qu'ils rencontrent et d'autre part des comptes semi-automatisés qui partagent des photographies issues de grandes banques d'images, du portrait de président américain à la couverture d'album des Beatles. Il s'agit toujours ici d'un processus ar-chive^communication. À l'inverse, la démarche com-munication^archive est mise en pratique lorsque l'internaute souhaite mettre en scène un événement, un récit historique ou une thématique et que celui-ci va piocher dans des illustrations ou des documents pour étayer son propos.


Figure 3 : Typologie des usages de Twitter pour la communication de documents historiques.

Perspectives
Les médias sociaux favorisent-ils une démocratisation et une réappropriation de l'histoire par son public ou sont-ils au contraire un vecteur d'une histoire spectacle décontextualisée ? S'il apparaît de manière évidente que la réponse se situe entre ces deux extrêmes, ou du moins qu'elle varie selon les usages précis, rappelons que nous proposons ici une plongée dans une réalité, quotidienne pour certain et étrangère pour d'autres. Cet état des lieux, en deux parties, quantitative et typologique, doit servir de base à une réflexion plus large sur le rôle des historiens et des institutions patrimoniales dans une société numérique où l'on ne peut lutter contre la dissémination d'images mal référencées et de contenus instrumentalisés. Cerner ces phénomènes, c'est aussi préparer la réplique, rigoureuse, critique et pourquoi pas créative.

Bibliography
Butticaz E. (2013). Twitter, cette machine à remonter le temps, Le Temps, https://www.letemps.ch/no-sec-tion/2013/11/22/twitter-cette-machine-remonter-

temps (accessed 1st October 2016).

Clavert F., Majerus B. et Beaupré N. (2015). Twitter, the Centenary of the First World War and the Historian. Twitter for Research 2015, Lyon.

Grandjean M. (2014). Source criticism in 140 characters: rewriting history on social networks, International Federation for Public History Conference, Amsterdam.

Steffen M. et Nunes Coelho P. (2014). Tweeting during World War II, http://h-europe.uni.lu/?p=2037 (accessed 1st October 2016).

Varin V. (2014). Tweeps Discover the Past, Perspectives on

History, https://www.historians.org/publications-and-directories/perspectives-on-history/april-2014/tweeps-discover-the-past (accessed 1st October 2016).
Over the past year and a half, the Cyberinfrastructure for Digital Humanities (CyberDH) Group at Indiana University has been developing an open instructional workflow for text analysis that aims to build algorithmic understanding and basic coding skills before scaling up analyses (Gniady et al., 2017). We have chosen to bootstrap in R, a high level and high productivity language, with methods that are open, repeatable, and sustainable. The aim is to provide code templates that can be adapted, remixed, and scaled to fit a wide range of text analysis tasks. This poster presents our approach to teaching computational text analysis and a representative hypothetical case study in which two different users are able to start with the same corpus and adapt code to achieve very different end results in a way not currently possible with black box tools.

This paradigm is fundamentally different from that currently practiced by many in the digital humanities. Black-boxed tools with GUIs that hide computation are very popular for introducing new practitioners of text analysis in the digital humanities to basic algorithms and outputs. In 2012, AntConc was downloaded 120,000 times by users in 80 different countries (Anthony, 2014). Voyant 1.0 had 113 sites linking to it actively in 2012 (Sinclair and Rockwell, 2013) and the week Voyant 2.0 was released the server went down multiple times from excess traffic (@VoyantTools, 2016). However, one of its default corpora is the Shakespearean dramas, with speaker names and stage directions. ((Sinclair and Rockwell, 2016). The inclusion of speaker names skews all algorithms related to frequency counts of characters (e.g. word clouds), which a new user may not even think to take into account. Using AntConc's concordance tool with a Shakespearean corpora including speaker names gives an idea of when a character speaks and when a character is mentioned, but this conflation might not jump out at a new user. If anything, we suggest learning about algorithms first and then moving up to black-box tools when one has the means to critique them.

Having looked at popular “plug-and-play” tools for corpora visualization, it becomes evident that even simple visualizations can lead to inaccurate results if the user is not thinking through how a corpus is being processed to produce a result. We believe that if the user understands how the algorithm is generating visualizations, they can contribute more meaningfully to critiques of sophisticated algorithms when partnered with programmers or even go on to bootstrap themselves with awareness of their domain's particular caveats. Thus, we advocate teaching humanists the basics of coding to create conversant programmers similar to the methodology behind Matthew Jockers' Text Analysis with R for Students of Literature, but with a slightly slower ramp up. To this end we have a three-step process of introducing R: web-deployed Shiny apps, highly marked up RNotebooks, and lightly commented RScripts, both in “regular” and higher performance versions. All are available for download on Github (with associated sample data from Shakespeare and Twitter) (CyberDH Team, 2017). We hope that this simpler bootstrapping method that mixes code and explanation, pedagogy and self-driven inquiry, will be of use to those looking to onramp new practitioners who may go on to partner with programmers if needed or to remix available code to look at their own knowledge domain.

Bibliography

Anthony, L. (2016). Antconc 3.4.4. Software.

http://www.laurenceanthony.net/soft-

ware/antconc/.

Gniady, T. Thomas, G. and Kloster, D. (2017). Text Analysis Github Repository. https://github.com/cyberdh/Text-Analysis.

Jockers, M. (2014). Text Analysis with R for Students of Literature. New York: Springer International Publishing.

Sinclair, S. and Rockwell, G. (2013). “Voyant Notebooks: Literate Programming, Programming Literacy.” Digital

Humanities 2014: Conference Abstracts. Nebraska-Lin-

coin: http://dh2013.unl.edu/abstracts/ab-295.html.

Sinclair, S. and Rockwell, G. (2016). Voyant Tools. http://voyant-tools.org/.

@VoyantTools. Twitter. 8 April 2016.
This poster focuses on OBSERVATOR!O2016, a web-based platform for collecting, structuring and visualizing the online response to Rio-2016 from content shared on Twitter. This project was developed at the Vision and Computer Graphics Laboratory and is based on two cross related research lines. First, the conception and design of the OBSERVATORJO2016 website, which provides a space to explore comments and images about the Olympics through structured visualizations. Second, the ongoing deployment of research regarding the application of a digital method to explore large sets of images. The goal is to highlight how we use Deep Learning applications - such as automatic image classification - and visualization techniques to enhance discoverability and expression of subject features within an extensive image collection.

Method: Deep Learning and mosaic visualization

OBSERVATOR!O2016 collected around 1 million tweets from April 18th to August 25th, 2016. In other to gather different perspectives on the debate about the Olympics we created seven Twitter search queries. The data was presented in eight interactive visualizations:


digital images inaugurates new avenues for researchers interested in the human creative practice. In this sense, the investigation of Lev Manovich on digital methods to study visual culture is quite relevant. With this in mind, we explored Rio-2016 images through Deep Learning approaches. As the investigation is currently ongoing, we will report the research process of a single task, which resulted in the Torch Mosaic visualization.

During the pre-Olympics, it became evident that many of the images gathered by our query scripts were related to the Olympic torch relay and depicted the iconic object. Part of these images were accompanied by texts that mentioned the torch, but not all. In addition, some tweets mentioning the torch relay incorporated images that didn't depict the object. In other words, text analysis alone was not sufficient to detect a set of images containing the torch.

Thus, we referred to a Deep Learning approach to recognize the Olympic torch in our database. The field of visual pattern recognition has been recently improved by the efficient performance of Convolutional Neural Networks (CNN). In 2012, the work of Krizhev-sky et al. on training a deep convolutional neural network to classify the 1.2 million images in the ImageNet LSVRC-2010 contest into 1000 different classes had a substantial impact on the computer vision community.

More recently, and thanks to Google, computer vision tasks such as image classification have become more accessible and applicable. That's because the company released last year their open source software library TensorFlow. This library runs code for image classification on Inception-v3 CNN model, which can be retrained on a distinct image classification task (this quality is referred to as transfer learning). By creating a set of training images, it is possible to update the parameters of the model and use it to recognize a new image category. That said, we retrained the network by showing it a sample of 100 manually labeled images containing the torch. Finally, the retrained network ran over our database and returned a set of images with their corresponding confidence score for the new category.

Approximately 180,000 of the collected tweets included unique images. The production of large sets of


R!02016

DEEP LABELS


Figure 2. Confidence score over 83%


•-

T

il 1

¡jííi J

?? A *

?

p r

W'

Ï

r»

z ¿A

s y

y

i

« -

r.

j

£

’•j    •    *7^'*

í-

»w*'"

,y '

ia

&

7

Figure 5. Tile images


Until June 25th, around 1500 images with over 85% confidence score for the Olympic torch category had been classified by our network. We used them to create a mosaic visualization that can be zoomed and panned. The mosaic idea is that, given an image (target image), another image (mosaic) is automatically build up from several smaller images (tile images). To implement the mosaic, we used a web-based viewer for high-resolution zoomable images called OpenSeadragon.


Figure 3. The target image


Figure 4. The mosaic

The organization nature of the mosaic visualization is mainly aesthetic. Nevertheless, zooming and panning the mosaic allows the user to explore a wide variety of views, and to discover image details and surprises such as the spoof picture of Fofao, a Brazilian fictional character, carrying the torch.

For DH2017 poster session, we expect to present the results for another subject feature - the sporting disciplines - and visualisation technique - the videosphere - we are working on at the moment. In addition, we plan to discuss with participants some possible scenarios in which Deep Learning models could be applied to help image collections become more discoverable and expressive.

Bibliography

Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012).

"Imagenet classification with deep convolutional neural networks". Advances in neural information processing systems, pp. 1097-1105.

Manovich, L (2012). "How to Compare One Million Images?”. In Berry, D. (ed), Understanding Digital Humanities. Palgrave, pp. 249-278.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Angue-lov, D., ... & Rabinovich, A. (2015). "Going deeper with convolutions". Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1-9.
Over the last five years, research computing has undergone a shift from focusing on running infrastructure for highly technical researchers, primarily in the sciences, to supporting medium- to large-scale computational needs across a wide range of disciplines, where practitioners fall across the spectrum of technical proficiency.

This shift in approach has opened up new opportunities, both for scholars whose research questions are facilitated by computationally intensive algorithms (e.g. photogrammetry, natural language processing, OCR at scale), and for humanists in support positions that require translation between technical and non-technical audiences. This panel will discuss opportunities for research and career development through the perspectives of research IT staff whose backgrounds in both humanistic inquiry and computational and digital methodologies enable them to engage in outreach to, training of, and support for humanities researchers. It will also provide practical suggestions for humanists who could benefit from research computing resources but find it difficult to navigate the expectations of research IT organizations.

The communication, writing, and teaching skills cultivated through an advanced degree in the humanities align with employment trends in research IT groups. While some familiarity and comfort with computation is generally a requirement for working in research IT, advanced programming or system administration skills are crucial for a minority of positions in these groups. As the mandate of research computing groups has expanded, it has become increasingly clear that successful research computing programs require documentation that is comprehensible to a non-technical audience, hands-on workshops for non-specialists, and staff capable of understanding the fundamentals of researchers’ work and identifying effective approaches to meeting their computation needs. To that end, in 2014 the US National Science Foundation funded the Advanced Cyberinfrastructure - Research and Education Facilitators (ACI-REF) program, which developed a cohort of computation “facilitators” across a number of universities who could "maximize the impact of investment in research computing" by “assisting] researchers in leveraging ACI resources for themselves” and sharing solutions among participating institutions (NSF 2014). This approach to providing support has been highly influential among campus research IT organizations, and related workshops such as the ACI-REF virtual residency (featuring plenaries such as “Effective Communication: How to talk to researchers about their research” and “Writing Grant Proposals”) have drawn large crowds (Neeman et al. 2016).

Similarly, the Extreme Science and Engineering Discovery Environment (XSEDE), funded by the National Science Foundation, has begun specifically reaching out to disciplines that have been typically under-represented in the high performance computing arena, including those in the humanities. XSEDE has a program called Extended Collaborative Support Services (ECSS) that partners humanities scholars and others from under-represented areas with computing professionals who can help them achieve their desired research objectives (Wilkins-Diehr et al. 2016). This has been both necessary and fruitful particularly in areas that are focused on data analytics (such as video analysis, image analysis, network analysis, etc.), as opposed to the more traditional simulation and modeling done by scientists and engineers. XSEDE’s Novel and Innovative Projects group includes a specialist in digital humanities, as well as specialists in pertinent related areas such as big data analytics.

Along the same lines, in January 2014, Compute Canada hired a full-time Digital Humanities coordinator to lead a national team supporting researchers wanting to engage with national advanced research computing (ARC) resources. Compute Canada, a national non-profit organization incorporated in 2012, plans and oversees panCanadian ARC resources used for big data analysis, visualization, data storage, software, portals and platforms for research computing serving Canadian academic and research institutes. The national support team for Compute Canada now consists of the full-time DH coordinator, and a large geographically dispersed team that meets bi-weekly to coordinate national initiatives like training at the Digital Humanities Summer Institute and national competitions such as the recent partnership between Compute Canada and the Canadian Social Sciences and Humanities Research Council. The DH team also meets locally with humanities researchers at their own institutions to help them leverage national infrastructure, and shares experiences and advice back to the national team to help in developing tools, services, and training opportunities that will benefit the national DH community (Simpson 2015).

One of the interesting ramifications of these changes in IT staffing trends is the addition of humanists to IT-based groups in high performance computing, cyberinfrastructure, visualization, and data architectures--humanists who are called upon to maintain both a deep understanding of computational systems, as well as track the needs of scholars in the humanities, which tend to be quite different from researchers in the “hard” or social sciences. Often, they are the lone humanist in a group dominated by engineers and computer scientists. There is a clear need and desire to expand the use of computational resources into "non-traditional" (i.e., non-hard-sciences) disciplines, both to justify an institutionwide investment in research computing, and as a way of building institutional capacity for supporting digital humanities scholarship (as described in the forthcoming 2017 ECAR/CNI white paper on institutional digital humanities support). Nonetheless, institutions are struggling with translating their services into comprehensible and relevant offerings for humanities researchers. Finding effective means of supporting these researchers within the traditional model of the research IT group has also been a challenge, given the ways that it differs from library-based support models with which scholars are more familiar. Panelists will reflect on projects they have worked on that successfully bridged the humanities-research computing divide, to the benefit of both groups.

As one example, at Indiana University, a workflow for teaching text analysis with R has been developed that uses web-based Shiny scripts to introduce an algorithm, highly annotated RNotebooks explaining each line of code, lightly annotated RScripts allowing for remixing and adaptation, and, finally, RScripts to leverage multicore environments. The scripts use data both from literature and Twitter, and these tutorials consistently draw the highest attendance in the series DH for Humanists, held throughout the semester. Individual research projects using more sophisticated algorithms such as NER and LDA have also grown out of this project.

As another example, at the University of Chicago, the recently-developed Visual Text Explorer provides a new type of framework for reading texts along with a range of user-customizable analytics, allowing for simultaneous close and distant reading. Other humanities research computing projects include web-based data-driven animated interactive mapping systems, tools for comparative sequence analysis across literary corpora, and automated aggregators of ranges of specific secondary data sources to inform the reading of specific texts/types of texts, all of which require no knowledge of computer programming by the user but nonetheless leverage research computing resources.

At UC Berkeley, 3D modeling work by Near Eastern Studies scholars that uses photogrammetry software running on the high-performance compute cluster has been helpful for testing new Graphics Processing Unit (GPU) nodes in the cluster.

Finally, panelists will provide recommendations for how humanities scholars can translate their research projects in ways that will make them more comprehensible and compelling for institutional research IT groups that may not have a humanist on their support staff.

Panel participants
Quinn Dombrowski is the Digital Humanities Coordinator in Research IT at UC Berkeley. Research IT supports research data management, museum informatics, and computationally intensive research across all domains. She is the author of Drupal for Humanists and has an MA in Slavic linguistics and an MLIS from the University of Illinois.

Tassie Gniady is the manager of the Cyberinfrastructure for Digital Humanities Group at Indiana University. The CyberDH group focuses on workflows for text analysis and photogrammetry. She also teaches an Introduction to Digital Humanities course in the Information and Library Science School. Tassie has a Ph.D in early modern literature from UC-Santa Barbara and an MIS from Indiana University.

Megan Meredith-Lobay is the Digital Humanities and Social Sciences Scientific Analyst for the University of British Columbia's Advanced Research Computing Department. She is also part of WestGrid and Compute Canada, the Canadian HPC national infrastructure platform. Megan has a PhD from the University of Cambridge Department of Archaeology in which she explored the early Christian archaeology of Argyll, Scotland using GIS and early online archaeological databases.

force". In XSEDE16 Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale. ACM: New York. DOI: 10.1145/2949550.2949584

Simpson, J. (2015). “Building Support for Digital Humanities”. Compute Canada blog. https://www.com-putecanada.ca/blog/building-support-for-digital-hu-

manities/

Wilkins-Diehr, N., et al. (2016)“An Overview of the XSEDE Extended Collaborative Support Program” in High Performance Computer Applications, ed. Isidoro Gitler and Jaime Klapp. Springer Link:    2016.

http://link.springer.com/chapter/10.1007%2F978-3-

319-32243-8 1

Lisa M. Snyde is the director of Campus Research Initiatives for UCLA's Office of Information Technology, and manager of the GIS, Visualization, and 3D Modeling group for the Institute for Digital Research and Education. She has a Ph.D. in Architecture and teaches Virtual Reality and 3D Modeling in UCLA's digital humanities program.

Jeffrey Tharsen is Computational Scientist for the Digital Humanities at the University of Chicago where he is the lead technical domain expert for digital and computational approaches to humanistic inquiry. Jeffrey has a Ph.D. from the University of Chicago's East Asian Languages & Civilizations department, specializing in the fields of premodern Chinese philology, phonology, poetics and paleography.

Bibliography
National Science Foundation. (2014) “Advanced Cyber-

infrastructre - Research and Educational Facilitation: Campus-Based Computational Research Support.” https://www.nsf.gov/awardsearch/show-

Award?AWD ID=1341935

Neeman, H., et al. (2016) "The Advanced Cyberinfrastructure Research and Education Facilitators Virtual Residency: Toward a National Cyberinfrastructure Work-
This study examines the increasing utilisation of the Internet and related digital media technologies such as Facebook, Twitter, and YouTube for political activities and civic engagement in Nigeria. It focuses specifically on the ways in which political actors and citizens in Nigeria draw on the existing multilingual resources within the Nigerian sociolinguistic ecosystem in their web-based political interactions.

The study also discusses how these stakeholders use a combination of mixed codes in online interactions with political issues and political stakeholders to reshape the political space and promote participatory democracy.

Theoretical insights that underpin the study adopt approaches from computer-mediated discourse analysis and pragmatics. The data set consists of a range of media multilingual-based online political posts between 2011 and 2015 elicited from our corpus construction project, CONNMDE (Corpus of Nigeria New Media Discourse in English). The methodological

procedures also utilise some digital humanities software packages and concordances such as Sketch Engine and AntConc3.4 for data harvesting and to complement the qualitative and quantitative analyses

(Baker, et al, 2008:275).

Generally speaking, the study reports some findings on the ongoing digital humanities project focusing on new media corpus construction. It then isolates and identifies clear instances of deliberate deployment of multilingual resources influenced by and rooted in ethnolinguistic identities as communication inclusion strategy. It equally confirms that Nigerian politicians use political websites with multilingual texts to encode acts of political persuasion and convey pragmatic meanings.
The Digital Humanities Summer Institute, or DHSI, provides “an ideal environment for discussing and learning about new computing technologies and how they are influencing teaching, research, dissemination, creation, and preservation in different disciplines, via a community-based approach” (DHSI). What began in 2001 as a small event at Malaspina University-College is now an annual 2-week affair at the University of Victoria which offers dozens of sessions and attracts a global audience. The growth of DHSI over the past 16 years mirrors the larger development of digital humanities as an academic (inter)discipline that has shifted from a niche endeavor for computational linguists and technology early-adopters to both a mainstream methodological approach and a distributed community of practitioners.

DHConnections examines DHSI attendance data. This website functions both as a tool for research about DHSI (and thus the history of disciplinary training in the digital humanities), and as a platform for DHSI alumni to connect with other researchers with similar interests, or to reconnect with contacts from previous DHSI sessions. In this way, DHConnections intersects with the “Collaborators” component of center-

Net's excellent DH Commons website, but is more explicitly focused on the distributed community of DHSI

alumni.

The DHSI organizational team maintains an archive of past DHSI sessions and participants. With the permission of DHSI and the Electronic Textual Cultures Laboratory at University of Victoria, I scraped this collection and cleaned it with OpenRefine to remove typographical errors, standardize attendee and organization names, and validate the data. There is no participant data (only instructors) for 2001-2003, but since 2004 there is an accurate list matching every attendee to the session he or she attended. This information is further broken out by role - student, instructor, speaker, or staff, with numerous specific subcategories of each. From 2006 onward, attendee institutional and organizational affiliations are also included. I also built a controlled vocabulary of DH topics and manually added topics to each session based on the session title and abstract when available. Together, these components form a temporal, topical, spatial, and biographical dataset which captures the attendance data for 2,678 individuals who collectively attended 254 sessions across 4,932 instances.

DHConnections allows users to access and interpret this dataset. Researchers can access the raw data via a JSON endpoint, but DHConnections also features numerous interactive interfaces which provide intuitive ways to understand the growth of DHSI through what Joanna Drucker calls “visual epistemologies ... ways of knowing that are presented and processed visually” (2014, 8). These include a searchable table of all participants; charts of the growth of DHSI over time by country, and by total institutional attendance; a facetable map of participants' institutional affiliations; a graph which examines the popularity of DHSI session topics and when they were introduced (figure 1); and a searchable network graph of participants, linked by session attendance, scaled by frequency, and color-coded by participant role. These and other visualizations provide information about the growth of an international DH and DHSI constituency, identify major contributors and organizations within the field, and


Figure 1: Topics assigned to DHSI sessions, colored by year introduced and sized by frequency


DHConnections also helps users connect with fellow DHSI alumni and find potential collaborators. For example, users interested in organizing a maker fair could query the database for DHSI alumni within 200 miles who attended a session between 2010 and 2016

and are interested in the topics of “3D printing,” “augmented / virtual reality,” “physical computing,” or “maker culture and praxis.” While “interests” are currently established by proxy via a link to attended session topics, the accuracy of such a search will increase over time because users can claim their profile on the site via an opt-in process.

Once a user claims his or her profile, he or she can edit their institutional affiliations; research interests; projects, papers, or personal websites; and/or contact information (Twitter / email). Users are also able to quickly opt-out of DHConnections and anonymize their attendance. DHConnections is designed for eventual expansion based on the availability of additional data sources such as lists of members or participants in THATCamps, HASTAC, HILT, European Summer University, DH@Madrid, DHOxSS, and conferences such as ADHO.

This poster presentation will allow ADHO attendees to test DHConnections. As the creator and developer of the project, I will be able to answer any questions about DHConnections and the underlying dataset, and look forward to talking to users and gathering feedback to help improve the platform.

Bibliography

DHSI. (2017). Digital Humanities Summer Institute Homepage. http://dhsi.org/ (accessed March 30 2017).

Drucker, J. (2014). Graphesis: Visual Forms of Knowledge Production. Cambridge, MA: Harvard University Press.
As a professor whose appointment encompasses both electronic literature and printed book publishing, I think a lot about interactive reading and reward systems. Reward systems are fundamental to videogames, where the consequences of reader/gamer choice materialize the stakes of interpretation (Juul, 2005). Computational reader response critics like Bell, Ensslin, Bouchardon and Saemmer frame their discussions of reading as medium-specific and, as I have argued, device specific (Berens). This emergent subfield is “literary interface criticism” (Pold), informed by comparative textual studies (Hayles and Pressman), performance studies (Fletcher), and interface criticism (Galloway; Critical Code Studies Working Group; Chun).

The book publishing world is only now, in the age of mobile-first web access, beginning to reckon with the implications of readers' daily exposure to pervasive, quotidian experiences of interactive reading and gaming environments in video games and social media. These entail nonhuman disruptions in the value chain of traditional industrial book production and distribution. John Maxwell observes that “large swaths of DH [digital humanities] practice overlap or are adjacent to practices in [book] publishing (e.g. markup, database design, user experience design, editing), yet publishing studies and the digital humanities often appear to run at right angles to one another. There is surely an opportunity for complementary work here.” My object in this talk is to articulate points of contact between DH and book publishing. Literary interface criticism offers a window onto interactivity and reward systems that extend how we understand the actions of reading: physical, cognitive, social. A book's medium-specific affordances (random-access device; portable; cheap; no Digital Rights Management) are attractive. But how might books take advantage of digital interactivity to provide more rewarding interaction than the “reflowable content” of an e-reader platform?

“Playable books”—the subject of my monograph-in-progress—are part of the “complementary” space Maxwell identifies between book publishing and DH. A playable book is a story object that can be held in human hands, requires physical interaction between human and computer to render, and outputs a story experience that can be “bound” or is otherwise finite. A printed book with interactive elements is playable (see, for example, Tyehimba Jess's Olio and Zachary Thomas Dodson's Bats of the Republic); a novel displayed on an e-reader is not; an improvised, participatory story in social media is not. (My monograph situates playable books in both literary games and fanfiction archives and databases: such situation is beyond the scope of this short paper-see Note).

In this paper I compare reading's reward structures in one bestseller and a popular, socially dynamic e-literature. I suggest how physically playful digital interactivity could inform mainstream book production and marketing. Selp-Helf, a 2015 New York Times bestselling book published by Gallery Books (an imprint owned by Simon and Schuster) is a successful YA [young adult] title. It is designed for hands-on, playful interaction; and its author, YouTube sensation Miranda Sings [Colleen Ballinger] sparked such a successful pre-sale campaign that her book débuted at #1 on the Publisher's Weekly Nonfiction Hardcover list, and #1 on the New York Times' Advice, How-To and Miscellaneous list, where it remained for eleven weeks. But Selp-Helf is just one piece of a successful transmedia campaign spanning a Netflix series Haters Back Off, a fifty-seven city comedy tour, and several musical albums. Interactivity, in this case, is spread among various media. Book publishing gets a small slice of the pie.

How might mass market book publishing increase its relevance in the contemporary media ecology? I present a short reading of Ink After Print (2012), an interactive story machine installed in public spaces such as a rock festival and public libraries, as a way of suggesting what next-generation story interactivity could look like for book publishers who currently funnel their social activity around book into social media campaigns they don't own, because they are hosted in platforms that dictate the terms of interactivity and serve their own agendas. A book like Selp-Helf has potential to benefit from reading rewards that are materialized in the book interface itself rather than a paratextual social media campaign.

Rationale & Analysis
Ink After Print is a full-body, playable literary interface. Exhibited at rock festivals, public libraries and train stations (in a French copy of the Danish original), Ink brings full-body haptics to the unbound book in ways that resonate with the embodied online social marketing of YA [young adult] titles. Both Ink and Selp-Helf ask the reader to do real things in the world, and leave traces of those activities in the literary interface, whether it's navigating through the sea of words in “Ink” and printing the results, or posting photos of oneself when meeting the YA author, or dressing up as a character (in this case, teen girls dressing as Miranda Sings.)

In both cases, writer/readers or “w/readers” (Landau, 1999) are having authentic experiences with literary interfaces. “Spreadable media” (Jenkins, Ford and Green, 2013) is a byproduct that empowers Ballinger's fans to use her book as a springboard to articulate their own perspectives on identity and gender. Use of digital skills is the precondition for fan interaction. Jenkins reminds us that audiences are individuals, “produced through measurement and surveillance, usually unaware of how the traces they leave can be calibrated by the media industries.” Publics are collectives that “actively direct attention onto the messages they value” (166). An entire subculture of book fans—often of young adult literature—is using books as totems around which to build worlds made by and through participatory media; Selp-Helf is one strong example. Selp-Helf and other YA books like it are centerpieces of book-specific media microecologies with particularized rules of conduct, aesthetics, and dynamic interaction. “Playability” focuses through the book, but exceeds the bounded dimensions of the book itself.

As more book marketing focuses on live events captured for and refracted through social media, this paper proposes that book interactivity should do more to engage the actual practice of reading to draw audiences into memorable relationships with the works. Analysis will focus on how the physical aspects of unbound book reading disclose new quantitative and qualitative shifts in mass market book reading practices.

Book publishers, loathe to develop content that they can't expressly monetize, run cheap social media campaigns in platforms they don't own like YouTube, Twitter, Instagram and Snapchat. That's where book publishers should look to literary experimental pieces like Ink for how to create “eventness” (pace Bahktin) around distribution and play beyond social media.

This would involve investment in digital-first book design and possibly a reading apparatus that could be physically moved location to location. Such techniques could scale, having a few select interactive reading “shows” that are captured in and for the social media audiences. Book publishers have built the expectation among YA readers that social media is their gathering space. Following the example of Ink After Print, publishers could offer actual, embodied, interactive reading experiences.

Ink After Print provides a rich context for readers to experiment with their affective experience of boundedness. The mechanics of Ink gameplay are sufficiently challenging that readers might feel a sense of reward in assembling a poem using the hand-held “books”; in this sense, the printed receipt is token of achievement. But it is also a highly portable object and a potential gift: to the ephemeral community of others playing Ink After Print, where you can share your poems with others who have played; and to the virtual community where Ink “receipts” are stored in the blog. When I curated a media arts show and exhibited Ink, I observed readers also folding their receipts into small objects that they then shared with others. The un-trackability of what people do with their Ink “receipts” stands in stark contrast to the databased traces of participation left by fans of Selp-Helf. While Ink does output to a blog, its outputs focus on the words themselves, not the user identity. In this sense, Ink resists the types of identity quantification that feeds and funds corporate sponsorship of social media platforms.

Note
[1] Working groups of note in this space are: the

Games and Literary Theory group founded by Espen

Aarseth, and the Critical Code Studies Working

Group. Books of note: Astrid Ensslin's Literary Gaming; Timothy Welsh's Mixed Realism: Videogames and the Violence of Fiction; Anasta-sia Salter, What Is Your Quest? From Adventure Games to Interactive Books; the excellent collec-tion Analyzing Digital Fiction, edited by Alice Bell and Astrid Ensslin. Alice De Kosnik's Rogue Ar-chives, and Amy Earhart's Traces of the Old, Uses of the New discuss the effect of fan archives and tribute sites that (as Earhart shows) are subject to abandonment, decay and obsolescence.

Bibliography
Aarseth, E. J. (1997). Cybertext: Perspectives on Ergodic Literature. Baltimore: Johns Hopkins UP.

Berens, K.I. (2015). “Touch and Decay: Porting Tomasula's TOC to iOS,” in The Art and Science of Steve Tomasula's New Media Fiction. New York: Bloomsbury.

Bouchardon, S. (2013). “Figures of Gestural Manipulation in Digital Texts” in Analyzing Digital Fiction, ed. Alice Bell, Astrid Ensslin and Hans Kristian Rustad. New York and London: Routledge Press.

Bouchardon, S.. (2016). “Toward a Tension-Based Definition of Digital Literature.” Journal of Creative Writing Studies, Vol. 2, Issue 1.

Chun, W.H.K. (2016). Updating to Remain the Same: Habitual New Media. Cambridge: The M.I.T. Press.

Critical Code Studies Working Group.    (2014).

http://wg14.criticalcodestudies.com/ (Accessed 30 October 2016.)

De Kosnik, A. (2016). Rogue Archives: Digital Cultural Memory and Media Fandom. Cambridge: The M.I.T. Press.

Earhart, A. (2015). Traces of the Old, Uses of the New: The Emergence of Digital Literary Studies. Ann Arbor: University of Michigan Press.

Ensslin, A. (2014). Literary Gaming. Cambridge: The MIT Press.

Fletcher, J. “Introduction.” Performance Research Journal. Vol. 18, No.5. Special Issue: On Writing and Digital Media. DOI: 10.1080/13528165.2013.867168

Galloway, A. (2012). The Interface Effect. New York: John Wiley and Sons.

Juul, J. (2005). Half-Real: Video Games Between Real Rules and Fictional Worlds. Cambridge: The M.I.T. Press.

Hayles, N. K. (1999). How We Became Posthuman: Virtual Bodies in Cybernetics, Literature, and Informatics. Chicago: University of Chicago Press.

Hayles, N. K. (2004). “Print is Flat, Code is Deep: The Importance of Media-Specific Analysis.” Poetics Today. DOI: 10.1215/03335372-25-1-67. Accessed 30 October 2016.

Hayles, N.K. and Pressman, J. (2014) Comparative Textual Media. Minneapolis: University of Minnesota Press.

Hunicke, R., LeBlanc, M. and Zubek, R. “MDA: A Formal Approach to Game Design and Game Research.” http://www.cs.northwestern.edu/~hunicke/MDA.pdf (accessed 5 March 2016).

Jenkins, H., Ford, S., Green, J. (2013). Spreadable Media: Creating Value and Meaning in a Networked Culture. New York: New York University Press.

Maxwell, J. “Publishing Education in the 21st Century and the Role of the University.” Journal of Electronic Publishing, Vol. 17, Issue 2. Spring 2014.

Pold, S. (2016). “Ink After Print: Literary Interface Criticism.” Journal of Electronic Publishing: Vol. 19, Issue 2:

Disrupting the Humanities: Towards Posthumanities.

DOI: http://dx.doi.org/10.3998/3336451.0019.207

Pold, S., Anderson, C.U.    (2012). Ink After Print.

http://www.inkafterprint.dk/

Pold, S., Anderson, C.U. (2014). “Post-digital Books and Disruptive Literary Machines: Digital Literature Beyond the Gutenberg and Google Galaxies.” Formules/Revue Des Creations Formelles.

Pressman, J. (2009) “The Aesthetic of Bookishness in Twenty-First Century Literature.” Michigan Quarterly Review Vol. XLVIII, Issue 4: “Bookishness: The New Fate of Reading in the Digital Age.”

Saemmer, A. (2013). “Hyperfiction as a Medium for Drifting Times: a Close Reading of the German Hyperfiction Zeit für die Bombe” in Analyzing Digital Fiction, ed. Alice Bell, Astrid Ensslin and Hans Kristian Rustad. New York and London: Routledge Press.

Salter, A. (2014). What Is Your Quest? From Adventure Games to Interactive Books. Des Moines: University of Iowa Press.

Sings, M. (2015). Selp-Helf. New York: Gallery Books.

Tender Claws [Samantha Gorman and Danny Canni-zarro]. (2014, 2016). Pry. Self-published; distributed through Apple App Store.

Tosca, S. (2013). “Amnesia: The Dark Descent” in Analyzing Digital Fiction, ed. Alice Bell, Astrid Ensslin and Hans Kristian Rustad. New York and London: Routledge Press.

Welsh, T. (2016). Mixed Realism: Videogames and the Violence of Fiction. Minneapolis: University of Minnesota Press.
La culture du réseau sans fil et du partage instantané a profondément remodelé notre conception du monde, ainsi que nos pratiques d'écriture, de lecture et de spectature. L'objectif de cette table ronde est d'examiner quelques projets récents qui cherchent à exploiter et à comprendre ces nouveaux dispositifs.

La question que nous nous posons est avant tout

celle des usages des technologies mobiles numériques dans le cadre des pratiques artistiques et litteraires actuelles. Comment les créatrices et les créateurs se servent-ils des medias et dispositifs numeriques? De la meme façon, comment nous servons-nous des dispositifs numeriques comme outils de façonnement d’un discours critique sur les arts et les litteratures, de recontextualisation, de mise en scene ? Comment la cohabitation entre les œuvres et les discours critiques peut-elle donner lieu au deploiement d’un imaginaire de la mobilite et de la création numerique? Il ne suffit pas qu'il y ait des technologies pour que des pratiques naissent, encore faut-il des environnements de recherche, de connaissance et de collaboration qui favorisent leur reconnaissance et leur diffusion.

Il s'agira de considérer la mobilité comme condition et facteur de création des œuvres ou comme modalité de réception. Les intervenant-e-s aborderont en ce sens les expressions de cette mobilité sous trois angles complémentaires, à savoir les dispositifs mobiles, les plateformes (Twitter, Periscope, Facebook, Facebook Live, Instagram, Snapchat, Youtube, etc.) et leurs usages et détournements.

Nous interrogerons par exemple la façon dont les plateformes mobiles se posent comme lieux d'exposition. Les internautes deviennent commissaires en ligne, agençant photographies et textes selon des critères de cohérence qui leur sont propres. Instagram ou Tumblr, en tant que plateformes centrées sur l'image, s'imposent comme des espaces parallèles à l'espace muséal où le travail d'association et de rapprochement de l'internaute se fait valoir. Les plateformes mobiles sont également le lieu d'exposition de soi par excellence. Les internautes sont invités à performer une identité projetée et régentée, encore une fois, par une ligne directrice qui révèle des appartenances et des positionnements. Les artistes de groupes marginalisés saisissent notamment cette opportunité pour bâtir des communautés en ligne liées par un discours commun. Ces considérations seront l'occasion d'explorer deux pans de la pratique qui se recoupent par un même principe, soit la mise en scène ou l'exposition en contexte numérique.

Ces questions sont étroitement liées aux enjeux politiques et identitaires en ligne, soulevés par plusieurs créateurs et créatrices. Critiques, les œuvres tendent alors à employer différentes stratégies esthétiques pour rendre compte des rapports de pouvoir, souvent invisibles, qui régissent pourtant le réseau. Nous commenterons à cet effet les œuvres qui détournent les flux du Big Data pour créer des espaces-autres, que ce soit par l'entremise du court-circuit, du piratage, de l'infiltration ou de l'appropriation.

Enfin, plusieurs thèmes et problématiques recoupent l'ensemble des présentations brièvement déclinées ci-dessus, soit l'introduction d'un nouveau rapport au temps et à l'espace, l'adaptation de nos modes de lecture et de spectature et la constitution de communautés de créateurs-rices et de chercheurs-ses.

Thèmes abordes:

1) Plateformes mobiles comme lieux d'exposition

2) Pratiques littéraires d'écriture et de lecture en contexte numérique

3) Parcours littéraires et imaginaires urbains

4) Détournements du big data

5) Représentation des communautés margi

nalisées en ligne
Introduction
This half-day tutorial introduces digital humanists at any level of experience to XQuery, a mature, high-level programming language used in many DH projects because it is purpose-built for analyzing, manipulating, and publishing data stored in the XML-based data formats that many DH projects use, e.g., TEI, EAD,

MODS, METS. Prominent XQuery-based projects inelude Carl Maria von Weber Gesamtausgabe, Foreign Relations of the United States and Syriaca.org.

Led by two experts who each have a decade of experience using and teaching XQuery and who have coauthored XQuery for Humanists (forthcoming, Texas A&M University Press), this half-day tutorial introduces the key concepts underlying the XQuery language and the kinds of analysis that it makes possible. The focus will be on exploring TEI-encoded editions with simple XQuery expressions.

Using a free and easy to install XQuery learning environment, participants (who must bring their own laptops) will gain hands-on experience writing queries against open datasets, including a TEI-encoded documentary edition, Foreign Relations of the United States. Participants will gain a basic foundation in the language and be introduced to community resources for further study.

This half-day workshop will cover the basics of XQuery, providing participants with sufficient hands-on experience to start exploring their own scholarly editions and metadata with XQuery. We presuppose that participants will have come with a basic understanding of XML and TEI.

I. Introduction: XQuery for the Digital Humanities

II. Setting up an XQuery environment

III.    Finding data with XPath

IV.    Writing FLWOR expressions

V.    Exploring XQuery Full-Text

Each section (except the introduction) will include hands-on exercises.

Target audience:
Students, scholars, and practitioners who use or are interested in using digital methods in their humanities work in academic departments, libraries, "alt-ac" fields, or their private capacity; no previous programming experience required; some experience XML or an XML-based format (TEI, EAD, MODS, METS) useful but not required. Participants will work with a common dataset provided by the tutorial leaders, but they may bring their own datasets for practice during the lab and consultation period.

Tutorial leaders
Clifford B. Anderson is Associate University Librarian for Research and Learning at Vanderbilt University in Nashville, Tennessee. He has a M.Div. from Harvard Divinity School and a Th.M. and Ph.D. from Princeton Theological Seminary. He also holds a M.S. in Library and Information Science from the Pratt Institute in New York City. Cliff started working with XQuery in 2006 before the first official version of the language was released. In 2014, he served as the project leader of the NEH-funded XQuery Summer Institute at Vanderbilt University. He has also taught sessions on XQuery for iterations of Laura Mandel's Programming for Humanists course at Texas A&M and leads the weekly XQuery working group at Vanderbilt University for digital humanists

Joseph C. Wicentowski is the Digital History Advisor in the Office of the Historian at the U.S. Department of State. He received his Ph.D. from Harvard University in modern East Asian history. He started using XQuery in 2007 to analyze and publish the Office of the Historian's TEI-encoded publications and datasets. For more on the project, see Wicentowski (2011). All code and data from the project are freely available on GitHub. He recognized XQuery's potential to empower students, scholars, and practitioners to take control of their own data and build their own applications. But he knew that without resources geared toward people with a humanities background, others would struggle

as he first did. He began writing about XQuery in various digital humanities forums, contributing to the XQuery Wikibook online textbook, and giving workshops at the TEI@Oxford and Digital Humanities@Ox-ford Summer School programs. Joe regularly speaks and writes in the fields of history, documentary editing, and open government. He also actively participates in TEI, XQuery, and digital humanities communities, and fosters discussion about XQuery on Twitter at

@XQuery.

Bibliography
Wicentowski, J. (2011) "history.state.gov: A case study of Digital Humanities in Government," Journal of the Chicago Colloquium on Digital Humanities and Computer Science, vol. 1 no. 3 , https://let-terpress.uchicago.edu/index.php/jdhcs/arti-

cle/view/80.
Introduction
This half-day tutorial introduces digital humanists at any level of experience to XQuery, a mature, high-level programming language used in many DH projects because it is purpose-built for analyzing, manipulating, and publishing data stored in the XML-based data formats that many DH projects use, e.g., TEI, EAD,

MODS, METS. Prominent XQuery-based projects inelude Carl Maria von Weber Gesamtausgabe, Foreign Relations of the United States and Syriaca.org.

Led by two experts who each have a decade of experience using and teaching XQuery and who have coauthored XQuery for Humanists (forthcoming, Texas A&M University Press), this half-day tutorial introduces the key concepts underlying the XQuery language and the kinds of analysis that it makes possible. The focus will be on exploring TEI-encoded editions with simple XQuery expressions.

Using a free and easy to install XQuery learning environment, participants (who must bring their own laptops) will gain hands-on experience writing queries against open datasets, including a TEI-encoded documentary edition, Foreign Relations of the United States. Participants will gain a basic foundation in the language and be introduced to community resources for further study.

This half-day workshop will cover the basics of XQuery, providing participants with sufficient hands-on experience to start exploring their own scholarly editions and metadata with XQuery. We presuppose that participants will have come with a basic understanding of XML and TEI.

I. Introduction: XQuery for the Digital Humanities

II. Setting up an XQuery environment

III.    Finding data with XPath

IV.    Writing FLWOR expressions

V.    Exploring XQuery Full-Text

Each section (except the introduction) will include hands-on exercises.

Target audience:
Students, scholars, and practitioners who use or are interested in using digital methods in their humanities work in academic departments, libraries, "alt-ac" fields, or their private capacity; no previous programming experience required; some experience XML or an XML-based format (TEI, EAD, MODS, METS) useful but not required. Participants will work with a common dataset provided by the tutorial leaders, but they may bring their own datasets for practice during the lab and consultation period.

Tutorial leaders
Clifford B. Anderson is Associate University Librarian for Research and Learning at Vanderbilt University in Nashville, Tennessee. He has a M.Div. from Harvard Divinity School and a Th.M. and Ph.D. from Princeton Theological Seminary. He also holds a M.S. in Library and Information Science from the Pratt Institute in New York City. Cliff started working with XQuery in 2006 before the first official version of the language was released. In 2014, he served as the project leader of the NEH-funded XQuery Summer Institute at Vanderbilt University. He has also taught sessions on XQuery for iterations of Laura Mandel's Programming for Humanists course at Texas A&M and leads the weekly XQuery working group at Vanderbilt University for digital humanists

Joseph C. Wicentowski is the Digital History Advisor in the Office of the Historian at the U.S. Department of State. He received his Ph.D. from Harvard University in modern East Asian history. He started using XQuery in 2007 to analyze and publish the Office of the Historian's TEI-encoded publications and datasets. For more on the project, see Wicentowski (2011). All code and data from the project are freely available on GitHub. He recognized XQuery's potential to empower students, scholars, and practitioners to take control of their own data and build their own applications. But he knew that without resources geared toward people with a humanities background, others would struggle

as he first did. He began writing about XQuery in various digital humanities forums, contributing to the XQuery Wikibook online textbook, and giving workshops at the TEI@Oxford and Digital Humanities@Ox-ford Summer School programs. Joe regularly speaks and writes in the fields of history, documentary editing, and open government. He also actively participates in TEI, XQuery, and digital humanities communities, and fosters discussion about XQuery on Twitter at

@XQuery.

Bibliography
Wicentowski, J. (2011) "history.state.gov: A case study of Digital Humanities in Government," Journal of the Chicago Colloquium on Digital Humanities and Computer Science, vol. 1 no. 3 , https://let-terpress.uchicago.edu/index.php/jdhcs/arti-

cle/view/80.
Orange for Text Analytics
In recent years, the digital humanities community has been introduced to many powerful tools for text analysis, but few of these tools combine powerful data mining and machine learning algorithms within a simple and capable user interface. For flexible and creative analysis, researchers need a tool that focuses on intuition, visualizations and interactivity.

This workshop will introduce participants to Orange, a visual programming environment for data mining, suitable for both beginners and experts. Particular emphasis will be placed on its Text add-on, which offers components for text mining, visualization and deep- learning-based embedding.

This is a hands-on workshop, where the participants will actively construct analytical workflows and go through case studies with the help of the instructors. They will learn how to manage textual data, preprocess it, use machine learning, data projection and visualisation techniques to expose hidden patterns and evaluate the resulting models. At the end of the workshop, the participants will know how to use visual programming to seamlessly construct powerful data analysis workflows, which can be applied to a wide range of challenges in digital humanities.

Structure of the Workshop

Part 1: Visual programming, workflows, data input and preprocessing

First, we will show the basics of Orange: how to load the data, inspect and visualize it. Participants will be introduced to several options for data import, from standard Corpus to Twitter, Guardian and Text Import. Once the corpus is loaded, we will preprocess it and display the result in a word cloud. A particular emphasis will be on the use of custom preprocessing techniques and how to successfully apply them to the corpus. The results of each technique will be observed in an interactive word cloud and concordances.


Figure 1: Preprocessing results displayed in a word cloud

Part 2: Machine learning and deep-learning-based embedding for predictive analysis

Next, we will use Twitter data to construct an author prediction pipeline and test some classifiers. We will fetch author Timelines from Twitter and observe the retrieved corpus. This time we will introduce a pre-trained tweet tokenizer and pass the preprocessed corpus through a bag of words. We will discuss bag of words parameters and how to best prepare the data for further analysis. The results of using different parameters will be observed in a data table to understand the underlying data structures. For comparison, we will use deep-learning-based embedding to derive vector representation of tweets and in this way enable machine learning.

We will explain how we can use machine learning in text mining and introduce a number of techniques for predictive analysis. We will use cross-validation to test the constructed bag of words models and compare classification scores for each algorithm. We will discuss the quality of constructed models and what scores are usually the best for observing model quality. Additionally, we will inspect misclassified tweets in a confusion matrix and even further in Corpus Viewer, to leverage the possibilities of a close(r) reading.

Part 3: Data clustering, sentiment analysis, image and geo analytics

In the third part, we will work on geomapping and image analytics. We will transform textual and visual data into feature vectors and plot these data onto a world map to discover interesting relations.

We will discuss how to acquire geolocated data from Twitter and why this is useful. Next, we will use geotagged Twitter data and apply a pre-trained sentiment analysis model to acquire sentiment orientation. We will map the sentiment-tagged tweets and explore how to use sentiment together with geomapping.

Finally, the participants will be introduced to image analytics for humanities research. We will explain why and how to transform raw images into multidimensional vectors and how to work with the new data. We will cluster Instagram images into groups and explore how to map image-containing tweets on a world map. Do images correspond to geolocation? We will see.


Figure 2: Images from social media are embedded with ImageNet embedding, clustered with Hierarchical Clustering and displayed on a map by their geolocation.

        
            Abstract 
            In this workshop for non - coders, participants will be guided through two tasks: the first task will guide participants in creating an application to tap into Twitter’s API, in our case to get Twitter data. The second task will guide participants in the use of a Google spreadsheet to capture streaming (live) data from Twitter in order to archive it, download it and perform text analysis, data visualization and other studies. This workshop will include a brief introduction contextualizing social media data collection good practices including user data privacy issues. 
            
                Keywords: Archiving, Data Collection, Social Media, Twitter, Text Analysis 
            
            Rationale 
            Twitter data can be very valuable for researchers of perhaps all disciplines, not just DH. Given the difficulties to properly collect and analyse Twitter data as viewable from most Twitter Web and mobile clients (as most people use Twitter) and the very limited short - span of search results, there is the danger of losing huge amounts of valuable historical material. 
            Tweets are like butterflies – one can only really look at them for long if one pins them down out of their natural environment. The reason why we have access to Twitter in any form is because of Twitter’s API, which stands for Application Programming Interface. Free access to historic Twitter search results is limited to the last 7 days. This is due to several reasons, including the incredible amount of data that is requested from Twitter’s API, and – this is an educated guess – not disconnected from the fact that Twitter’s business model relies on its data being a commodity that can be resold for research. Twitter’s data is stored and managed by Twitter’s enterprise API platform. 
            For the researcher interested in researching Twitter data, this means that harvesting needs to be do ne not only through automated means but in real time. It also puts scholars without the required coding and data mining skills at a disadvantage. As a researcher, this basically means that there is no way to do proper research of Twitter data without understanding how it works at API level, and this means understanding the limitations and possibilities this imposes on researchers. 
            What’s a n individual researcher without access to pay corporate access to do? The whole butterfly colony cannot be captured with the nets most of us have available. At small scale, however, and collecting in a timely fashion, it is still possible to capture interesting and more - or - less complete specimens using fairly simply, non - coding required methods. (The Library of Congress h s now 12 years’ worth of text - only Tweets. However, as before, the Library of Congress Twitter collection will remain embargoed and there was no projected timetable for providing public access as of 26 December 2017). 
            Most researchers out there are likely not to benefit from access to huge Twitter data dumps. For researchers without much resources that are trying to do the talk whilst doing the walk, and conduct research 
                on Twitter and 
                about Twitter, this workshop and tutorial will guide participants into creating a Twitter application in order to tap into the Twitter API, followed 
            
            by the setting up of a Twitter Google Archiving Spreadsheet. Once a trial archive or dataset has been collected, we will attempt text analysis and basic visualisations using Excel and Voyant Tools. This workshop will include a brief introduction contextualizing social media data collection good practices including user data privacy and research ethics issues. 
            Workshop Requirements 
            • Room with projector and screen 
            • Wifi access 
            • Power plugs for participants to charge devices if required 
            Participants Requirements 
            • Interest in collecting small Twitter datasets and basic Text Analysis 
            • Wifi - enabled Laptop with Excel or similar spreadsheet software 
            • Twitter account, and the login credentials to access it (username and password) 
            
                Tools We’ll Use 
            
            • TAGS 
            https://tags.hawksey.info/ 
            • Voyant Tools 
            https://voyant - tools.org/ 
            El taller se puede dar también en español o bilingüe inglés - español. 
        
        
            
                
                    Bibliography
                    For complete references please follow links in the referenced outputs below and in the body of the text above. 
                    Priego, E. 2018. #rfringe17: Top 230 Terms in Tweetage. 
                    
                        https://epriego.blog/2017/08/05/rfringe17-top 230-terms-in-tweetage/
                    
                    [Accessed 30 January 2018] 
                    Priego, E., 2016. Bar Chart: Number of #DH2016 Tweets in Archive per Conference Day (Sunday 10 to Friday 15 July 2016 GMT). Available from: 
                    
                        https://figshare.com/articles/Bar_Chart_Number_of_DH2016_Tweets_in_Archive_per_Conf erence_Day_Sunday_10_to_Friday_15_July_2016_GMT_/3490001/1 [Accessed 31 Jan 2018]. 
                    
                    Priego, E. 2016. “Stronger In”: Looking Into a Sample Archive of 1,005 StrongerIn Tweets. 
                    
                        https://epriego.blog/2016/06/21/stronger-in-looking-into-a-sample-archive-of-1005- strongerin-tweets/ [Accessed 30 January 2018] 
                    
                    Priego, E. and Zarate, C., 2014. #MLA14 Twitter Archive, 9 - 12 January 2014. Available from: 
                    
                        https://figshare.com/aticles_MLA14_Twitter_Archive_9_12_January_2014/924801/1
                    
                    [Accessed 31 Jan 2018]. 
                    Priego, E. 2014. Some Thoughts on Why You Would Like to Archive and Share [Small] Twitter Data Sets. Available from:
                    
                        https://epriego.blog/2014/05/28/some-thoughts-why-you-would-like-to-archive-and-share-twitter-small-data / [Accessed 30 January 2018] 
                    
                    Priego, E. 2014. Publicly available data from Twitter is public evidence and does not necessarily constitute an “ethical dilemma”. London School of Economics Impact Blog. Available from:
                    
                        http://blogs.lse.ac.uk/impactofsoc ialsciences/2014/05/28/twitter-as-public-evidence/ [Accessed 30 January 2018] 
                    
                
            
        
    

        
            
                Resumen
                El fenómeno de la biopirateria es una problemática que afecta principalmente a las comunidades indigenas y sus conocimientos tradicionales. De esta manera, las medicinas tradicionales son afectadas por la practica de la biopirateria. Este estudio exploratorio propone desarrollar una cartografía sobre la biopirateria de las medicinas tradicionales en la red social Twitter utilizando los métodos digitales. De esta manera, se presentan una serie de gráficos donde se describe el fenómeno y se identifican los diversos productos que son mencionados en Twitter. Concluimos que los métodos digitales nos permiten interpretar el fenómeno de la biopirateria de las Medicinas tradicionales en Twitter e identificar los productos que son mencionados.
                
                    Palabras claves: Biopirateria, medicinas tradicionales, análisis de redes sociales, métodos digitales, cartografía de internet, humanidades digitales.
                
            
            
                Introducción
                Se considera que el término biopirateria es un concepto contemporáneo (RAFI, 1994)⁠, pero también se encuentra que es una práctica antigua, con orígenes colonialistas (Boumediene, 2016; Shiva, 1997). Aubertin y Moretti mencionan sobre el término biopirateria.
                La Coalición contra la biopirateria (etcGroup) define la biopirateria como la apropiación general mediante los derechos intelectuales de recursos genéticos, de conocimientos locales y de culturas tradicionales pertenecientes a campesinos o comunidades indigenas quienes han desarrollado y mejorado el uso de recursos naturales. La biopirateria incluye la bioprospeccion, las patentes sobre genes y moléculas y la comercialización de los conocimientos culturales (2013, p. 91).
                Al mismo tiempo que surge el término biopirateria, emerge un movimiento que busca la legitimación de los conocimientos locales e indigenas. De hecho, la principal problemática de la biopirateria se encuentra en la falta de legitimación de los conocimientos tradicionales por parte de gobiernos e instituciones. Como consecuencia, la problemática se extiende a la apropiación de conocimientos y recursos biológicos por parte de empresas e instituciones con la ayuda de otras instituciones que gestionan los derechos de autor y la propiedad intelectual (OMPI). Se encuentran diversos ejemplos, el caso del Neem y la Curcuma en la India, la Ayahuasca, la Quinoa, la Maca y la Mayacoba en America Latina y la Hoodia y la Rooibos en Africa (Aubertin et al., 2007; Delgado, 2002; Dumesnil, 2012; IEPI, 2016)⁠.
                El caso de la biopirateria de los conocimientos alrededor de las medicinas tradicionales ha sido ignorado por gobiernos, instituciones y empresas por mucho tiempo, los casos de biopirateria como muestra el etcGroup (RAFI, 1994)⁠ son variados a lo largo del tiempo. En la actualidad por fin se encuentran discusiones sobre el tema en instituciones como la la Organización Mundial de la Propiedad Intelectual (OMPI)
                     http://www.wipo.int/meetings/en/details.jsp?meeting_id=42302 y por otro lado en el Convenio sobre la Diversidad Biológica
                     https://absch.cbd.int/es/ y el protocolo de Nagoya (CBD, 2012)⁠. De la misma forma, es importante un rol mas activo de la UNESCO en la protección de los conocimientos locales y las medicinas tradicionales como Patrimonio Cultural Inmaterial (Unesco 2003).
                
            
            
                Cartografía de internet
                La cartografía de Internet permite analizar los perfiles, los comportamientos y asimismo analizar las relaciones entre los actores, las comunidades y las tendencias (Bastard, et al., 2017; Diminescu, 2012; Severo &amp; Venturini, 2016)⁠. Nos interesa el análisis y la cartografía de la red social Twitter, ya que es una red abierta donde se encuentran conversaciones sobre fenómenos sociales y políticos.
                El objetivo de esta investigación exploratoria es analizar los textos de las publicaciones en Twitter y desarrollar una cartografía de los productos relacionados con las medicinas tradicionales que son mencionados en las publicaciones. Por tanto, mediante el uso de los métodos digitales se desarrolla una cartografía del fenómeno de la biopirateria en Twitter.
            
            
                Métodos digitales 
                Los llamados métodos digitales son una serie de métodos, técnicas y herramientas que permiten realizar estudios sobre la redes sociales e Internet en general (Diminescu, 2012; Rieder, 2013; Rogers, 2013; Severo and Venturini, 2015).
                El método se desarrolló en cuatro etapas: la extracción, el análisis y la clasificación, la cartografía y finalmente la interpretación.
                1. En la primer etapa, la extracción de datos en Twitter se realizó mediante la herramienta TAGS
                     https://tags.hawksey.info/ y la opción de búsqueda: biopiracy OR biopiraterie OR biopirateria OR bio-pirateria OR bio-piracy OR bio-piraterie. 
                
                2. En la segunda etapa, con la herramienta OPEN REFINE
                     http://openrefine.org/ se desarrolló el análisis, la clasificación y la limpieza general de la base de datos. Con el objetivo de analizar todos los tweets registrados se decidió desarrollar la cartografía a partir de la relación entre los usuarios (@) y los tweets. Para el tratamiento y el análisis de los tweets se realizó un agrupamiento de los textos, por lo cual se agrupo en un mismo tipo de mensaje una publicación normal y un RT, así como otras con ciertas variaciones en el texto (Fig. 2). Asimismo se analizó cada texto para encontrar pistas sobre los productos de las MT y se creo una variable en la BD con el nombre del producto.
                
                
                    
                        
                    
                    Fig. 1. Ejemplos de agrupamiento de los textos de las publicaciones
                
                3. En Gephi1 se desarrolló la cartografía utilizando el algoritmo “Force Atlas” para visualizar la red espacialmente y se aplicaron los valores “in-degre” y “out-degre” para identificar las nodos con mayor valor. 
                4. En la etapa de la interpretación, se realizó el diseño visual de la cartografía utilizando también otras herramientas de visualización de datos.
            
            
                Análisis de la biopirateria en Twitter 
                El resultado de la muestra de datos extraídos de Twitter entre las fechas 06/02/2017 – 06/10/2017 consistió en 3,995 publicaciones (Tweets) de los cuales se encontraron 494 repetidos y 1 con error, por lo cual se realizó el análisis con una muestra total de 3,500 Tweets. Asimismo se encontraron 581 tweets sin hashtags ni marcas de usuarios mencionados (@usuario).
                
                    
                        
                    
                    Fig. 2. Cartografía de usuarios por idioma
                
                En la Fig. 2 se presenta una red donde se aplicó la variable “Out degree”, y se observa el contexto general de las conversaciones sobre la biopirateria según el idioma seleccionado por los usuarios. Esto no quiere decir que los usuarios publiquen únicamente en la lengua en la que se registraron en Twitter. El idioma mayoritario es el español, pero los usuarios mas activos se observan en la región en francés. La relación con las publicaciones se observa en la Fig. 3, donde se presentan los 20 tweets mas difundidos.
                
                    
                        
                    
                    Fig. 3. Cartografía de los tweets mas difundidos
                
                En la figura 3 se presenta una red donde se aplicó la variable “In degree” para darle mas relevancia a los tweets. Los tweets 559 y 161 son difundidos principalmente por bots y hablan sobre la implementación del protocolo de Nagoya en Ecuador (24, 45, 62). El Tweet mas difundido en ingles (67) menciona el movimiento contra la #LeyDeBiodiversidad en México que se relaciona con la ratificación del protocolo de Nagoya (27, 40, 42, 60, 62, 79). Se observa en este caso una disputa entre los ciudadanos que están en contra y los bots del gobierno. Los tweets 26 y 59 mencionan un articulo publicado en la revista @NatureNews sobre los riesgos del protocolo de Nagoya para el desarrollo de la vacuna contra la influenza (flu). El tweet 28 menciona a Vandana Shiva quién es una luchadora relevante contra la biopirateria. Se observa asimismo dos tweets que mencionan situaciones particulares sobre la biopirateria en Indonesia y en Haiti (29, 60). Finalmente, se observan tres tweets importantes los cuales mencionan algunos productos de las medicinas tradicionales como el Curare (35), la Estevia (60) y la Sacha inchi, Tara, Aguaje y Huanarpo macho (24).
                A continuación se presenta la cartografía con los 16 productos encontrados que se relacionan con las medicinas tradicionales.
                
                    
                        
                    
                    Fig. 4. Cartografía de las plantas y productos de las medicinas tradicionales
                
                Observamos las diversas plantas y productos mencionados en Twitter, de los cuales la Stevia , la Sacha Inchi, el Aguaje, el Huanarpo macho, la Tara, la Maca, el Curare, la Quassia, el Maqui, la Hoja de coca y el Kambô son de origen latinoamericano. Después se observa el Kakadu plum de origen australiano, luego el Neem y el Turmeric de origen indio y entre los menos mencionados la Ixempra y el Rooibos de origen africano. Entre todos ellos la Ixempra no se encuentra su uso en las medicinas tradicionales, pero se menciona como un caso de biopirateria.
            
            
                Conclusiones
                Esta investigación exploratoria nos permitió analizar el contexto de la biopirateria en Twitter e identificar los diferentes productos que son mencionados. La cartografía presentada es en sí misma una interpretación y por lo tanto es una visión particular de la biopirateria. 
                Twitter es una red social abierta en la cual los usuarios participan libremente, pero se decidió no mostrar sus nombres en la cartografía para mantener en respeto su privacidad. 
                Este estudio exploratorio nos permite tener una base de conocimiento para seguir una investigación más específica sobre cada uno de los productos encontrados.
            
        
        
            
                
                    Bibliografía
                    Aubertin, C. and Moretti, C. (2013) La biopiraterie, entre illégalité et illégitimité. In: Les Marchés de la Biodiversité. Institut de recherche pour le développement, p. 269.
                    Aubertin, C., Pinton, F. and Boisvert, V. (2007) Les marchés de la biodiversité. Recherche. IRD Editio. Paris: Institut de Recherche pour le Développement.
                    Bastard I., Cardon, D., Charbey, R., et al. (2017) Facebook, pour quoi faire ? Sociologie 8(1): 57–82. DOI: 10.3917/socio.081.0057.
                    Boumediene, S. (2016) La colonisation du savoir : une histoire des plantes médicinales du ‘Nouveau Monde’ (1492-1750). Vaulx-en-Velin: Les éditions des mondes à faire.
                    CBD. (2012) Protocole de Nagoya sur l’accès aux ressources génétiques et le partage juste et équitable des avantages découlant de leur utilisation relatif à la Convention sur la diversité biologique. Convention sur la diversité biologique Nations Unies. Montreal. Available at: http://www.cbd.int/abs/doc/protocol/nagoya-protocol-fr.pdf.
                    Delgado, G. C. (2002) Biopi®acy and Intellectual Property as the Basis for Biotechnological Development : The Case of Mexico. International Journal of Politics, Culture and Society 16(2): 297–318.
                    Diminescu, D. (2012) Introduction: Digital Methods for the Exploration, Analysis and Mapping of e-Diasporas. Social Science Information 51(4): 451–458. DOI: 10.1177/0539018412456918.
                    Dumesnil, C. (2012) Les savoirs traditionnels médicinaux pillés par le droit des brevets ? Revue internationale de droit économique XXVI(3): 321–343. DOI: 10.3917/ride.257.0321.
                    IEPI. (2016) Primer informe sobre biopiratería en el ecuador. Quito.
                    RAFI. (1994) COPs... and Robbers... Transfer-Sourcing Indigenous Knowledge. Pirating Medicinal Plants. Occasional Paper Series 1(4): 20. Available at: http://www.etcgroup.org/content/volume-1-4-pirating-medicinal-plants.
                    Rieder, B. (2013) Studying Facebook via Data Extraction: The Netvizz Application. In: Proceedings of WebSci ’13, the 5th Annual ACM Web Science Conference, 2013, pp. 346–355. DOI: 10.1145/2464464.2464475.
                    Rogers, R. (2013) Digital Methods. Massachusetts: MIT Press.
                    Severo, M. and Venturini, T. (2015) Intangible cultural heritage webs : Comparing national networks with digital methods. New Media and Society 18(8): 1616–1635. DOI: 10.1177/1461444814567981.
                    Severo, M. and Venturini, T. (2016) Enjeux topologiques et topographiques de la cartographie du web. Reseaux 1(195): 85–105. DOI: 10.3917/res.195.0085.
                    Shiva, V. (1997) Biopiracy : the plunder of nature and knowledge. South end Press.
                    Unesco. (2003) Convention pour la Sauvegarde du Patrimoine Culturel Immateriel. Paris.
                
            
        
    

        
            37% of the United States population is non-white, but 90% of the books published for children during the last twenty-one years contain no multicultural content. This discrepancy has been called “The Diversity Gap” (Erlick, 2015) and, more starkly, the “Apartheid of Children’s Literature” (Myers, 2014). Based on data gathered since 1985 by the Cooperative Children’s Book Center, the representation gap has barely shifted over thirty years, with books by and about non-white people hovering between 10-14% of total children’s book production (CCBC 2017). Panels and initiatives about diversity in book publishing have not actually produced more books by and about non-white people. Book discoverability is thus a significant challenge to parents, librarians, and teachers seeking picture books depicting the lives of non-white children. 
            As it’s currently practiced in the North American book industry, “diversity” usually tallies “how many” rather than delving into the lived experience of non-white people. The Diverse BookFinder [DBF], a database and metadata project sponsored by Bates College and funded by the Institute for Museum and Library Services [IMLS], asks: how can metadata help to tackle these entangled problems? Can we build a network of information about children’s picture books that trains users to search for and discover books using complex concepts related to their own communities rather than race or ethnicity as the sole marker?
            Our long paper:
            
                Surveys the problem of whiteness as the de facto point-of-view in children’s books, and the populist social media movements that resist this phenomenon;
                Examines the limitations of current metadata in k-3 books about non-white children; 
                Presents Diverse BookFinder as a strategic disruption of current metadata practices; 
                Conveys the pedagogical value of Diverse BookFinder in academic and public settings.
            
            Readers have created online, massively participatory movements to prompt the predominantly white book publishing industry to publish more books by and about non-white people (Low, “Diversity Baseline Survey,” 2016). #WeNeedDiverseBooks, #1000BlackGirlBooks, and #OwnVoices originated in Twitter hashtags then converted their social media capital (likes, shares, reposts, followers) into an array of recommendation services: published anthologies, book finder apps, a short story contest, even a granting agency. Populist interventions are welcome and useful, but they are insufficient to remedy the problem of classifying existing books using metadata that reinscribe white privilege. 
            One intervention in these human and machinic systems is a human-curated and -coded catalog, Diverse BookFinder (
                
                    https://diversebookfinder.org/
                ). Metadata and recommendation systems are not neutral. They operationalize cultural assumptions that the creators may not have intended or even be aware of. Critical code studies, the scholarship of platforms and software that examines computer source code hermeneutically, has charted useful ground in exploring how metaphors of containment and layers, for example, rationalize logics of racial exclusion (McPherson 2012). Many metadata schemas for books relate back to the physical structures libraries and classrooms use to organize books for readers. These systems create fixed and singular ways of relating items that construct contextualized exclusion (Drabinski 2013). The common cataloging systems used in the United States, including the Library of Congress and Dewey Decimal System, are centered in whiteness and maleness and reinforce the otherness of diverse titles. The separation of topics on women and gender (including queerness) from broader topics such as literature or history, for example, reinforce the notion that women and queers secondarily contribute to history and literature. This segregation repeats in the various forms of metadata where difference is replicated and continually defined by whiteness. 
            
            Exclusion is not specific to physical location like a library. Algorithmic “overfitting” is the phenomenon where recommendations are culled from a narrow spectrum of a user’s interests. Overfitting “can occur when a user is trying to be helpful by providing explicit feedback only about the content s/he strongly likes. This leads to the creation of a very specific model that knows the exact user preferences but is unable to detect any other types of interesting items since the user has not shown any interest in it” (Kunaver &amp; Poztlz, 2017, 156). Under this system, the typical person would have difficulty in finding diverse titles online if those books did not already match to their past search behavior. Inconsistencies in the application of metadata compound this problem. 
            Metadata sometimes contain errors that hide or misrepresent the books, or don’t classify the types of information that would be most relevant to communities seeking books about non-white children. Such books are “mirrors and windows,” that reflect back or “mirror” one’s own lived experience in the faces, bodies, customs and cultural milieu depicted in the book, and open “windows” onto new cultures different from one’s own (Bishop 1990). Such books develop myriad literacies beyond reading comprehension, including conflict resolution, tolerance for the unfamiliar, and awareness of cultures beyond one’s own. When used in the classroom as an intervention toward intergroup contact, diverse picture books can foster intercultural understanding among children (Aronson, et al. 2016). Unfortunately, existing metadata does not account for the intricacies of diverse titles and so these books remain difficult to comprehensively identify or locate. Hand-coding is a remedy to discoverability problem.
            Without controlled language, books are simply not findable. There is no eschewing metadata; there is only writing better metadata, and theorizing the best practices that writers of descriptive metadata should follow in order not to reinscribe racist stereotypes and cultural marginalization. The purpose of this vocabulary is not to undo prior standards--which are each problematic in many ways-- but to contribute to the larger representation of diverse books and fill in the information holes. Systematic SEO work is underway to add language as it is used by the communities represented; for example, a user may enter “
                Boricua” into DBF and yield results about Puerto Rican characters. The goal is to write metadata that reflects the lived experience of people the books depict. 
            
            When books are entered into the Diverse BookFinder, they go through a multi-step, hand coding process to compile the metadata commonly missing from other sources. Book characters are coded for racial and/or ethnic identity, gender, setting, with additional tags such as tribal nation, immigration status, or religion where applicable. Most books fall into one of nine categories that capture the message conveyed by these books. The categories are: Beautiful Life, Oppression, Cross-group, Biography, Race/Culture Concepts, Folklore, Incidental [ensemble or background characters of color], and Informational [factual content unrelated to race or culture]. These categories arose from an application of grounded theory, and created by a rigorous analysis of commonalities in picture book stories. This analysis shows that the concept of Beautiful Life, stories about a particular racial or cultural group experience, dominates diverse book publishing, but such a message is commonly unavailable in existing metadata outside of DBF. African Americans are most likely to be depicted in situations of oppression; Native Americans are disproportionately represented in “folklore,” and Hispanic and Latinx people are underrepresented generally in picture books. DBF has engaged students at all levels, at several institutions in thinking about representation and participating in research to better understand the role of picture books in children's development. 
            It’s pedagogically valuable to give students a direct search experience of how imprecise book metadata impedes book discoverability. In a lab exercise designed by Bell and implemented by Inman Berens, master’s students retrieved book metadata across three venues for two books in the DBF database. Those venues were: publisher website, retailer, library catalog. Students discovered significant errors in metadata, and notable variability from venue to venue. Ensuing class discussion allowed students to trace the interoperation of human classification errors and legacy systems such as Library of Congress Subject Headings with machinic processes. The students then reviewed a Library of Congress copyright form submitted to the LoC by our student-run trade press (Ooligan Press), and discovered ambiguities in the Library form’s language that prompted misclassification of our press’s just-released young adult novel. This exercise drove home that automated processes are framed by human judgment.
            The Diverse BookFinder is unique precisely for the level of human labor that goes into the data entry and book coding process. The inconsistencies and inaccuracies in book metadata and the additional information added to each book’s metadata could not be done by machine. This process serves to bridge the gaps in metadata, help users identify many more diverse titles than the average search, and provides new insights into what stories dominate in picture books. As public scholarship, this project seeks to move the diverse books discussion beyond a focus simply on the lack of numbers to also consider content and impact by translating research findings so that they are accessible and useful.
        
        
            
                
                    Bibliography
                    Aronson, K. M., Stefanile C., Matera C., Nerini A., Grisolaghi J., Romani G., ...Brown R. (2016). Telling tales in school: extended contact interventions in the classroom. 
                        Journal of Applied Social Psychology, 46, 229–241. doi: 10.1111/jasp.12358
                    
                    Bishop, R.S. (1990). Windows and mirrors: children’s books and parallel cultures. In M. Arwell and A. Klein (Eds.), 
                        California State University San Bernardino Reading Conference: 14th Annual Conference Proceedings (pp.11-20). San Bernadino, CA: CSUSB Reading Conference. Retrieved from https://files.eric.ed.gov/fulltext/ED337744.pdf#page=11
                    
                    Cooperative Children’s Book Center (CCBC). (2017). Children's Books By and About People of Color Published in the United States. Retrieved from (
                        
                            http://ccbc.education.wisc.edu/books/pcstats.asp
                        ).
                    
                    Drabinski, E. (2013). Queering the catalog: queer theory and the politics of correction. 
                        Library Quarterly: Information, Community, Policy, 83(2), 94-111. doi.org/10.1086/669547
                    
                    Erlick, H. (2015, March 5). The diversity gap in children’s publishing, 2015 (blog post). 
                        Lee and Low Books: The Open Book Blog. Retrived from 
                        
                            http://blog.leeandlow.com/2015/03/05/the-diversity-gap-in-childrens-publishing-2015/
                        
                    
                    
                        Jackson, Chris. (2017). Diversity in Book Publishing Doesn’t Exist -- But Here’s How It Can (blog post). Retrieved from 
                        
                            http://lithub.com/diversity-in-publishing-doesnt-exist-but-heres-how-it-can/
                        
                        .
                    
                    Kunaver &amp; Poztlz (2017). Diversity in recommender systems -- A survey. 
                        Knowledge-Based Systems 123 (154-162). 
                        
                            http://dx.doi.org/10.1016/j.knosys.2017.02.009
                        
                    
                    Low, Jason T.(2016, January 26). Where is the Diversity in Publishing? The 2015 Diversity Baseline Survey Results (blog post). 
                        Lee and Low Books: The Open Book Blog. Retrieved from http://blog.leeandlow.com/2016/01/26/where-is-the-diversity-in-publishing-the-2015-diversity-baseline-survey-results/
                    
                    McPherson, Tara. (2012). Why Are the Digital Humanities So White? or Thinking the Histories of Race and Computation. 
                        Debates in Digital Humanities, ed. Matthew K. Gold. Retrieved from http://dhdebates.gc.cuny.edu/debates/text/29
                    
                    Myers, Christopher. (2014, March 15). The Apartheid of Children’s Literature. 
                        New York Times. 
                        
                            https://www.nytimes.com/2014/03/16/opinion/sunday/the-apartheid-of-childrens-literature.html?_r=0
                        
                    
                
            
        
    

        
            El 3 de mayo de 2017, el cuerpo sin vida de una mujer de 22 años fue encontrado dentro de un campus universitario de la capital mexicana. En su difusión por Twitter de los avances del caso, la Procuraduría General Judicial de la Ciudad de México difundió detalles como el nombre de la joven (Lesvy Berlín) y realizó afirmaciones con respecto a que ella vivía con su pareja, que habría interrumpido sus estudios dos años atrás, que antes de hacerlo no habría aprobado varias asignaturas y que de forma inmediatamente previa a su fallecimiento la mujer y su pareja habrían consumido bebidas alcohólicas y drogas en las instalaciones de la universidad. En Twitter, el recibimiento de la difusión del caso inmediatamente se transformó en una recriminación ciudadana contra la Procuraduría General Judicial, al compartirse generalizadamente la impresión de que la forma en que la instancia presentó a Lesvy Berlín generaba un discurso justificatorio de su muerte y que, en última instancia, hacía recaer en ella la responsabilidad de haber perdido la vida. La crítica social ante este discurso se canalizó hacia el hashtag #SiMeMatan, al cual las y los usuarios de redes sociales le acompañaron de descripciones de sus propias vidas que, en el hipotético caso de que se les encontrase sin vida, las instituciones de procuración de justicia potencialmente podrían difundir para crear una impresión de lo “justificado” de su fallecimiento.
            Por una parte, el surgimiento y difusión en torno a #SiMeMatan puede estudiarse como una muestra de ciberactivismo o activismo
                
                     “El activismo es una categoría de acción para participar en la política que es ‘públicamente declarada y una abierta contribución a la vida política’” (Yeatman, 1998, p. 33, citado por Demetrious, 2013, p. 34). Esta definición de activismo presenta la limitación de haber sido generada desde una perspectiva según la cual la acción colectiva tiene como su objetivo fundamentalmente a los planteamientos políticos, sin tomar en cuenta motivos como el reconocimiento de identidades marginalizadas, la lucha por el medio ambiente o cualquiera de las causas asociadas a los nuevos movimientos sociales de los contextos postmaterialistas. Pese a lo anterior, de Yeatman y Demetrious se retoma la vocación del concepto “activismo” para enunciar la acción participatoria de un campo de la vida social (
                        à la Melucci: fuere para modificarlo o conservarlo).
                    
                 que tiene lugar exclusivamente en Internet y/o que se sirve de la red para reforzar acciones que tendrán lugar en entornos no virtuales (McCaughey y Ayers, 2003). En segundo término, es posible analizar la manera en que esta acción (virtual) de participación en la vida social plasmó en la forma de textos a comportamientos, valores y actitudes percibidos como disruptivos de los cánones de normalidad vigentes en la sociedad mexicana contemporánea. Así, cada vez que las y los usuarios emplearon el hashtag #SiMeMatan acompañado de una descripción de las “disrupciones” por las que las autoridades podrían culparles de su propia muerte, los ciberactivistas contribuyeron al registro de dos conjuntos de discursos sociales interdependientes: el de las “faltas” a la normalidad y, en una imagen invertida por el espejo, el discurso de los comportamientos, valores y actitudes colocados en una posición de hegemonía por su conformación con las normas sociales más ampliamente aceptadas. El análisis discursivo del uso de #SiMeMatan perite el rastreo de “grandes temas” o ejes en torno a los cuales se evidenciaron las tensiones de la polaridad 
                aceptable/no aceptable percibida por los usuarios de redes sociales. Fundamentalmente, se trató de los terrenos en disputa del cuerpo, el uso del tiempo libre, los vínculos entre varones y mujeres, la orientación sexual, así como el dualismo autonomía/conformismo. Al condensar las características movilizadas en los discursos de lo desafiante
                
                     Aquello por lo cual las autoridades construirían discursos de ruptura con la normalidad en caso de perder la vida, de acuerdo con los usuarios que utilizaron el hashtag #SiMeMatan.
                 y lo desafiado en ocasión de la muerte de Lesvy Berlín, es posible apreciar que la imagen a trazos amplios de “la normalidad” identificada por los ciberactivistas del caso coincide con una descripción de heteropatriarcalidad, en tanto que elementos situados por fuera del despliegue modesto/tradicionalista del cuerpo femenino y las preferencias heterosexuales fueron señalados críticamente como motivos latentes de “merecimiento de consecuencias negativas” (desde la discriminación a la pérdida de la vida). Sin embargo, en este caso de ciberactivismo también puede analizarse la manera en que los usuarios que emplearon el hashtag #SiMeMatan integraron a la religión como parte del canon de heteropatriarcalidad que hicieron objeto de sus críticas: mensajes como “#SiMeMatan será porque no creía en Dios, usaba falda y fumaba”, “#SiMeMatan será porque no era católica, era lesbiana y salía a bailar”, “#SiMeMatan será porque no iba a misa y decía groserías”, “#SiMeMatan será porque no me confesaba y no tenía relaciones estables” o “#SiMeMatan será porque era atea, me emborrachaba y salía sola con hombres”, son algunos de los ejemplos que permiten conformar como objeto de estudio una percepción social según la cual el desafío a la “normalidad religiosa” se concibe como parte integral del conjunto de desafíos a la heteropatriarcalidad vigente en la sociedad mexicana.
            
            La presente discusión tiene como objetivo analizar el procesamiento ciberactivista de la religión como parte del canon heteropatriarcal en México en ocasión de la muerte de Lesvy Berlín y el surgimiento del hashtag #SiMeMatan en la Ciudad de México a inicios de 2017. Este objeto de estudio haya su justificación en la forma en que el caso contribuye al mapeo de las percepciones sociales vigentes en torno a la “normalidad y el desafío de la normalidad” en los comportamientos, valores y actitudes de varones y mujeres jóvenes del México contemporáneo. De igual manera, este análisis brinda información sobre la manera en que la no-conformidad con un canon religioso está entrelazado dentro del discurso social de no-conformidad con otros cánones en materia de sexualidad y de expectativas en torno al comportamiento femenino. Finalmente, el caso también brinda oportunidad para debatir la categoría de ciberactivismo cuando se le aplica a reacciones coyunturales de los usuarios de redes sociales. El análisis se construye a partir del análisis discursivo de cien comentarios marcados con el hashtag #SiMeMatan, seleccionados por muestreo teórico, difundidos por la red social Twitter durante el día 3 de mayo de 2017. En un primer momento se contextualiza la dinámica con la que se desenvolvió el tratamiento oficial en redes de la muerte de Lesvy Berlín y al que se dio respuesta con el hashtag #SiMeMatan. A continuación, se discute por qué los comentarios que se sumaron a esta expresión en Twitter pueden calificarse como ciberactivismo en particular y una acción colectiva en términos generales. Para ello, se recurre a la propuesta teórica de Snow (1986) en lo que respecta a los procesos de vinculación de los intereses, valores y creencias de los individuos como pilar de la acción colectiva. A partir de esas bases, se usa la metodología de análisis del discurso para procesar las evidencias de percepciones de normalidad y no-normalidad movilizados en las respuestas de #SiMeMatan en Twitter. De ese modo, se muestra la conformación del canon de normalidad en torno al cuerpo, la modestia tradicionalista y la heterosexualidad criticado por los ciberactivistas del caso y se brindan evidencias de la integración de una idea de “normalidad religiosa” dentro de ese conjunto de valores hegemónicos criticados.
        
        
            
                
                    Bibliografía
                    Checa Godoy, A. (2008). 
                        Historia de la comunicación: de la crónica a la disciplina científica. La Coruña, España: Netbiblio.
                    
                    Demetrious, K. (2013). 
                        Public Relations, Activism, and Social Change: Speaking Up. Nueva York, Estados Unidos: Routledge.
                    
                    McAdam, D. (1999). Oportunidades políticas: Orígenes terminológicos, problemas actuales y futuras líneas de investigación. En D. McAdam 
                        et al. (Eds.). 
                        Movimientos sociales: perspectivas comparadas (pp. 49-70). Madrid, España: Istmo.
                    
                    McCarthy, J. D., y Zald, M. N. (1977). Resource Mobilization and Social Movements: A Partial Theory. 
                        American Journal of Sociology, 82, 1212-1242.
                    
                    McCaughey, M. y Ayers, M. (2003). 
                        Cyberactivism: Online Activism in Theory and Practice. Nueva York, Estados Unidos: Routledge.
                    
                    Melucci, A. (2003 [1996]). 
                        Challenging codes. Collective action in the information age. Nueva York, Estados Unidos: Cambridge University Press.
                    
                    Snow, D. A., Rochford, E., B. Jr., Worden, S., K., y Benford, R. D. (1986). Frame Alignment Processes, Micromobilization, and Movement Participation. 
                        American Sociological Review, 51, 464-481.
                    
                    Turner, E. (2013). New Movements, Digital Revolution, and Social Movement Theory.
                         
                        Peace Review,
                         
                        25(3), 376-383.
                    
                
            
        
    

        
            
                Introduction
                An understanding of the intellectual and social structures of Digital Humanities (DH) has been sought by many scholars; some have pointed to the potential usefulness of quantitative methods in such analyses (McCarty, 2003; Terras et al., 2013). A few existing studies have applied quantitative methodologies to analyse publication, conference and social media data (e.g. Nyhan and Duke-Williams, 2014; Weingart, 2016; Grandjean, 2016). This study not only incorporates such approaches but extends them by integrating new analysis and visualisation methods into the wider study of DH’s intellectual and social structures.
                In this paper, we will introduce research on the citation and social network of DH that is giving rise to new understandings of the field’s community structure; scholarly interactions; disciplinary development; and formal/informal communication channels. The citation network of Author Co-Citation Analysis (ACA) comprises 22,321 cited authors across 52,823 cited references from the three core DH journals, while the social network of Twitter Co-Retweet Analysis comprises 3,160 Twitter users and 5,929,609 tweets. To the best of our knowledge, this study is the first to combine bibliometric and social network methods to visualise and compare the DH communities and to uncover their histories. This research contributes to ongoing discussions and debates about the DH knowledge and community structures (Gold and Klein, 2016).
            
            
                Data Analysis
                
                    Citation network
                    The Author Co-Citation network study was presented at DH2017; this paper extends this earlier analysis. For reasons of clarity, we here give a brief overview of this research. The network has been constructed by collecting the citation data of three core DH journals up to December 2016: 
                        Computers and the Humanities (
                        CHum), 
                        Digital Scholarship in the Humanities (
                        DSH) and 
                        Digital Humanities Quarterly (
                        DHQ). 2,582 articles with 52,823 cited references were collected (see Figure 1).
                    
                    
                        
                    
                    Figure 1. number of articles collected each year (1966-2016)
                    By using 
                        fractional non-self-citation count and 
                        exclusive co-citation count (Zhao and Strotmann, 2008), the weights of nodes and links respectively were calculated for visualisation using the software VOSviewer 1.6.7 (van Eck and Waltman, 2010). An author name disambiguation method (Strotmann et al., 2009) was used, and 22,321 unique cited authors identified. Where possible, other pertinent information was collected (i.e. author full names, gender, country of affiliation, etc.). After counting the occurrences of two authors being cited together, ACA shows the DH intellectual structure and influential topics and scholar groups (Figure 2).
                    
                    
                        
                    
                    Figure 2. DH Author Co-Citation network
                
                
                    Twitter network
                    Given DH’s early adoption and active use of Twitter (Ross et al., 2011), previous studies have explored the field’s scholarly communications and community on Twitter (e.g. Quan-Haase et al., 2015; Grandjean, 2016). This study introduces a new approach (
                        co-retweet) to visualise the DH social network.
                    
                    We have selected all the Twitter users that are followed by the Alliance of Digital Humanities Organisations (ADHO) and its member organisations’ (see 
                        http://adho.org/) Twitter accounts. As dynamic and interdisciplinary as the DH community, it is often difficult and subjective to select the users by their account descriptions. In contrast, the following relationships by the DH organisations indicate more genuine and representative identification. A total of 3,160 unique users have been collected along with all the 6 million tweets from 21/03/2006 when Twitter was created up to 22:00 (GMT) on 5/11/2017 (see Figure 3).
                    
                    
                        
                    
                    Figure 3. number of tweeting users collected each year
                    Similar to the citation network method, the Twitter user Co-Retweet network has been constructed by calculating the number of non-self-retweets the user received (
                        non-self-retweet count), and the number of same tweets that two users both retweeted (
                        co-retweet count) for the weights of the nodes and links respectively. The network resulting visualisations are shown on Figure 4.
                    
                    
                        
                    
                    Figure 4. DH Twitter Co-Retweet network
                
            
            
                Results and comparison
                In the citation network, the authors identified distinct topic-based clusters of researchers with backgrounds in information studies and historical literature; in linguistics; in statistical text analysis; in early concordance projects; and biotech influenced text analysis. In contrast, the co-retweet network exhibits grouping based on language and region, with clusters related to scholars in North America; in Australia; in the UK; and clusters with Francophonic, Germanophonic and Hispanophonic backgrounds.
                The Twitter clusters are connected closely whereas clusters on the citation network are more loosely linked. This makes sense, as topics of study are generally more specific and less likely to change, whereas users on social media probably share a wider range of interests. The citation network is based on formal communications and it would take years to get sufficient citations to form links between two scholars. The Twitter network, however, is constructed by more informal interactions between users, and once two users retweeted the same tweet, they immediately build a link on the network.
                By visualising both networks during different time periods, this study will also present the topics, disciplines, countries that are involved, and how the networks have been developed and formed over time.
                As shown in Figure 5, we divided the 51 years (1966-2016) of historical citation data into five different periods (Hockey, 2004) for visualisation. The citation clusters experienced isolation (1966-1970); connection (1971-1985); consolidation (1986-1990); sub-fields development (1991-2005); and new specialties expansion (2006-2016). Over time, the most cited topics moved from concordance construction, to computational linguistics, then to information and historical literature studies.
                
                    
                
                Figure 5. DH Author Co-Citation networks in five periods
                As shown in Figure 6, DH Twitter users started to have co-retweet connections in 2009; and then they experienced the beginning of connection (2010); multi-region connection (2010); Anglophonic cluster to centre (2011); Francophonic cluster to develop (2012); North America and UK to separate (2013); Germanophonic to come out (2014); Australian cluster to show (2015); Hispanophonic cluster to emerge (2016); Density continue to move to North America cluster (2017). Over time, the network visualisations show that the density is moving from European clusters towards the North American cluster.
                
                    
                
                Figure 6. DH Twitter Co-Retweet networks in different years
            
            
                Discussion and conclusion
                This study is not only the first to contribute to the DH history and community studies by visualising and comparing bibliometric and social networks, but also introduces new network approaches (
                    co-retweet) to study communications on social media that could support wider social network and data visualisation studies.
                
                As we will discuss, network studies offer powerful but partial ways of studying the aspects of communities that are amenable to quantitative methods. We do not present the visualisations included in this paper as normative representations of the DH “community” or “communities”. Nevertheless, when used with caution, network studies can shed new light on important aspects of the historical formation of DH. 
                There are methodological limitations exist. For example, because the research subjects (cited authors and retweeting users) are not the same group of people (although with much overlap), obvious differences are expected. Besides, the citation lag time has been considered. Other practical methods to identify and study the DH Twitter communities can also be applied.
            
        
        
            
                
                    Bibliography
                    Davis, L.S., Johns, S.A., Aggarwal, J.K. (1979). Texture Analysis Using Generalized Co-Occurrence Matrices. 
                        IEEE Transactions on Pattern Analysis and Machine Intelligence PAMI-1, 251–259. https://doi.org/10.1109/TPAMI.1979.4766921
                    
                    Gold, M.K. and Klein, L.F. (2016). 
                        Debates in the digital humanities: 2016. University of Minnesota Press, Minneapolis London.
                    
                    Grandjean, M. (2016). A social network analysis of Twitter: Mapping the digital humanities community. 
                        Cogent Arts &amp; Humanities,3:1171458. https://doi.org/10.1080/23311983.2016.1171458
                    
                    Hockey, S. (2004). The History of Humanities Computing, in: Schreibman, S., Siemens, R., Unsworth, J. (2004). 
                        A Companion to Digital Humanities. Blackwell Publishing Ltd, Malden, MA, USA, pp. 1–19.
                    
                    McCarty, W. (2003). Humanities Computing, in: 
                        Encyclopedia of Library and Information Science. Marcel Dekker, New York.
                    
                    Nyhan, J. and Duke-Williams, O. (2014). Joint and multi-authored publication patterns in the Digital Humanities. 
                        Literary and Linguistic Computing,29:387–399. https://doi.org/10.1093/llc/fqu018
                    
                    Quan-Haase, A., Martin, K., McCay-Peet, L. (2015). Networks of Digital Humanities Scholars: The Informational and Social Uses and Gratifications of Twitter. 
                        Big Data &amp; Society,2. https://doi.org/10.1177/2053951715589417
                    
                    Ross, C., Terras, M., Warwick, C., Welsh, A. (2011). Enabled Backchannel: Conference Twitter Use by Digital Humanists. 
                        Journal of Documentation,67:214–237. https://doi.org/10.1108/00220411111109449
                    
                    Strotmann, A., Zhao, D., Bubela, T., (2009). Author name disambiguation for collaboration network analysis and visualization. 
                        Proceedings of the American Society for Information Science and Technology,46:1–20. https://doi.org/10.1002/meet.2009.1450460218
                    
                    Terras, M.M., Nyhan, J., Vanhoutte, E. (2013). 
                        Defining Digital Humanities: A Reader. Ashgate Publishing Company, Farnham, Surrey, England: Burlington, VT.
                    
                    van Eck, N.J. and Waltman, L. (2010). Software survey: VOSviewer, a computer program for bibliometric mapping. 
                        Scientometrics,84:523–538. https://doi.org/10.1007/s11192-009-0146-3
                    
                    Weingart, S.B., (2016). dh quantified: A Review of Quantitative Analyses of the Digital Humanities. 
                        the scottbot irregular: data are everywhen.
                    
                    Zhao, D. and Strotmann, A. (2008). All-author vs. first-author co-citation analysis of the Information Science field using Scopus. 
                        Proceedings of the American Society for Information Science and Technology, 44:1–12. https://doi.org/10.1002/meet.1450440262
                    
                
            
        
    

        
            My research is multi- and interdisciplinary focusing on electronic literature and cybercultures in/of Latin America. My latest articles and book manuscript explore the divide and convergence in literature and technology. This project lends itself well to the application of those theories and the evaluation of how they can best be implemented in classroom practices and complemented with co-curricular modules. I will therefore present my research findings on the use of Digital Humanities components specifically for the teaching of Latin American Studies. The presentation would thus serve as a report of: 1) initial research findings and best practices found at other institutions; 2) work accomplished at the DHSI 2018 Workshop (Victoria, Canada) “Critical Pedagogy and Digital Praxis in the Humanities”; 3) feedback gained from presentation at the DHSI 2018 Conference &amp; Colloquium; and, 4) samples of syllabi to foster a lively discussion on the application of such a course with co-curricular components for Latin American Studies programs.
            The goal of this project is to do a detailed study of program and curriculum design at other institutions on the use of DH modules specifically for Latin America/US Latino culture with a focus on pedagogical methodologies that engage critically about the problems that DH platforms do and do not resolve in Latin American Studies. The course design and the co-curricular components complement and intersect each other. This project will facilitate the assessment of various curriculums and specialized courses for the digital humanities and would ultimately lead to develop a course for all students interested in DH Latin American Studies.
                
                     Students in this course would include (but not limited to) those in the Latin American Studies Minor Program, International Business, International Studies (BAIS and GPIS), Humanities, Political Science, Spanish majors and Minors, World Cultural Studies majors and minors.
                 Course components will include developing language proficiency, learning and using DH tools, and analyzing the effectiveness and drawbacks of such technologies specifically to Latin American Studies.
            
            The interactive, systematic, and innovative features of digital humanities have been proven to advance language learning both in and outside of the classroom. Through exploring different forms of digital humanities, including multi-media, online archives, as well as existing web tools like Google Earth and Twitter, instructors and scholars of foreign languages not only facilitate collective and immersive language learning, but also broaden and deepen students’ exposure and knowledge of foreign culture. These projects break the traditional geographical and cultural boundaries in learning a foreign culture and/or language. Therefore, it is essential for instructors to reflect on how best to incorporate digital humanities in language/culture learning, and to determine to what extent digital learning complements and even replaces traditional ways of teaching and learning. 
            Students will be encouraged to adapt these new tools of analysis to their own future career objectives. The field of Digital Humanities is collaborative and very interdisciplinary as it produces new scales of analysis with varying modules (texts, maps, audio-mapping and networks) which may include experiments across modalities with: distant reading alongside close reading techniques, programming language, audio creation, geotagging, speech recognition encoding documents in TEI (Text Encoding Initiative
                
                     Text Encoding Initiative Markup Language at the University of Virginia, 
                        https://dh.virginia.edu/tool/text-encoding-initiative-markup-language-tei (for my future reference)
                    
                ), learning the basics of computational text analysis, programming chatbots using the Python programming language, etc. The course will also note the drawbacks or pitfalls of the use of technology.
            
            However, the skills needed in DH have less to do with a particular hardware or commercial software and more about engaging in digital literacy (train interpretative methods necessary for critical analysis), and showcase how digital humanities is valuable to better understand Latin America’s transformations in the production, circulation and reception as well as its impact on culture, politics, history, literature, music, etc. The course will encourage students to develop more analytical projects from the use of such modalities. The focus will also be to analyze and address 
                why this method of learning is complementary or even superior to traditional methods, specifically addressing the impact and implications that technology involved on ideologies, ethics and ideas. For example, a more involved topic would approach the idea of “mapping” as interpretation of geospatial data in GIS, georectify historical maps in 
                Map Warper, manage digital archival objects in 
                Omeka, and use 
                Neatline to build “deep maps” of particular neighborhoods or landmarks in a city, layering historical photographs, maps, geospatial data, literary texts, and other elements to build analysis about their city. 
            
            Additionally, the course will attempt to link to public libraries (Slover in Norfolk), museums (Chrysler, Mariners, Living Museum), research centers, community groups (Norfolk Chamber of Commerce, Hispanic Chamber of Commerce, Hispanic Community Dialogue) or other campus-level initiatives (ODU’s Institute of Humanities “Mapping Lambert’s Point Project,” for instance). The goal is to build projects that make use of the University and community’s collections. These public projects can energize students to work that much harder, as they can create materials with a chance of life beyond the classroom itself. The course will draw on resources from, participate in and continue their learning with the Regional, National and International Network
                
                     To be featured in the Latin American Studies Program website
                 aimed to promote digital humanities initiatives to Old Dominion University faculty and to learn from and collaborate with external groups.
                
                     I already have established contacts and am in current collaborations with: Centro de Cultura Electrónica in Mexico City; the project Cultura Digital Chile (Universidad Diego Portales, Chile); the Latin American and Digital Humanities/Cybercultures at University of Georgia; the Digital Latin American Cultures Network: Researching the Cultural Dimensions of New Media in the United Kingdom; I am also a board member of the organization Lit-e-Lat: Red de Literatura Electrónica.
                 This network would be dedicated to exploring, analyzing, and sharing the cultural and visual modalities of digital humanities in the research and teaching of Latin America. The network would engage in these discussions through symposia for faculty and students with guest speakers or virtual conferences, virtual exhibitions, and online or hybrid workshops.
                
                     For example, “Tecnoestética y sensorium contemporáneo: arte, literatura, diseño y tecnología” in September 2017 in Córdoba Argentina; 
                 The network and initiatives that I foresee fostering and/or facilitating may include: 
            
            
                K-12 Service Education: Working with the College of Education and the Licensure Students in the World Languages and Cultures Department to: Expand on its longstanding educational outreach commitments with K-12 educators and students at the local and state level; and, serve as a resource to K-12 educators working to meet Virginia Performance Standards as they relate to Latin American content in the social, natural, and life sciences by 
                Language Without Borders Initiative: Create the next generation of global professionals through innovative language education, with Superior level proficiency in Spanish and overseas internship experience.
                DH and Latin/o American Cybercultures Initiative: Exposure to the digital culture of Latin America through seminars, symposia, courses, exhibitions, and workshops.
            
        
    

        
            Can 
                design represent a culture/nation? Can the tools of digital design be used in collaboration with industrial and interior design to establish an interactive communication with culture? While design and 
                designwork were seen as essential symbols of nation-based identity construction in most of the 20
                th century, today, the notion of design deliberately shies away from exposing its cultural/national implications because of global aspirations. Today’s world, dominated by multinational corporations, with its imposition on self-centered identities seemingly curtains the close connection/flirtation of design to its cultural roots. The project that is developed as a collaborative design task at School of Art + Design at New Jersey Institute of Technology (NJIT) aims to question and build on the assumption that suggests a connection between design and culture/nation, with the emphasis on the fact that 
                nation is also a social construction (Anderson, 1983). 
            
            This poster visualizes the results of the collaborative design project that I taught at NJIT in Fall 2016 and again in 2017. Throughout the semester students from different design fields were expected to work as a group on the design of a pavilion for the culture/nation of their selection that together with other teams formed an imaginary exposition center. Instead of superficial identifications, systematic research process and critical design concepts based on intellectual analysis of the findings determined a basis for the design project. By both researching and producing, teams aimed to create a digital tool that would be developed to investigate whether 
                designwork can represent a culture/nation, subculture or simply a cultural issue. Three teams consist of three students from three different design fields worked on their pavilions that are imagined as interactive tools. These tools incorporating data processing software, motion capture, virtual and augmented reality establish vivid, interactive communication with the user. In doing so, instead of creating informative two-dimensional representations, projects aimed to involve users to explore their contribution to the dynamics of a culture. In other words, instead of imposing a 
                meaning, pavilions ask users to build new meanings via their interactions both with the pavilion and with other users.
            
            The poster documents three different design processes each of which produced its own interactive digital tools to communicate culture. One team envisioned a mobile pavilion for Burlesque culture that offered users to design their own shows. Augmented reality helped users/performers select and 
                put-on a stage costume digitally. With a digital control panel performers were given a chance to adjust atmospheric effects such as light level and color, while physicality of the setting was conceived through a meticulous analysis of the Burlesque culture, such as heavily ornate historic furniture, wallpapers, textile, and decoration. 
            
             Second team created a digital crafting tool to educate visitors about Japanese Temari balls, which are toy balls made from embroidery may be used in handball games. Team tackled weaving as a craft with the question how and why weaving can be utilized as data analysis with an emphasis on its fabrication processes by using Japanese Temari balls as a case study. The pavilion encouraged visitors not only to learn about Temari tradition, but also share their experience with other users, who do not necessarily speak the same language or come from similar cultural backgrounds by transforming Temari making into a cultural activity that is virtually organized around a 
                ball game / spectacle. 
            
            
                
                Burlesque Pavilion by Hideyoshi Azama, Emily Gutierrez, Tulio Squarcio (left); Temari Pavilion by Danielle Archibold, Wuraola Ogunnowo, Florencia Pozo (middle); Pavilion Anahita by Negaar Amirihormozaki, Albeirys Francisco-Parra, Nazifa Hamidullah (right)
            
            The third team designed a pavilion that aimed to create a community by gathering people both physically in the space of the pavilion and virtually through social platforms such as Facebook, Twitter, Snapchat, and Instagram. The team problematized Iran’s mandatory hijab law by connecting the issue to sexism in different parts of the world that creating a network on women’s rights issues. Hijab’s ban in some countries and its enforced use in others were carefully examined to generate a digital forum for different opinions on this specific issue.
            This research was conducted to investigate culture’s changing perceptions. Rather than attempting to redefine a preconceived notion of culture by simply incorporating modern technologies, digital tools, and social media, it aimed to reveal new interactive networks that culture forms with other notions and omit others when conventional relations needed replacement; for example, a new interconnectedness instead of nationality. Finally, this project highlighted areas that were defined by the conventional cultural tools and perceptions that are still relevant.
        
        
            
                
                    Bibliography
                    
                        Anderson, B. (1983). 
                        Imagined Communities: Reflections on the Origin and Spread of Nationalism.  London and New York: Verso.
                    
                
            
        
    

        
            Over a year before Donald Trump took the presidency in 2016, a group of self-identified Cuban brujas, latina practitioners of witchcraft and/or indigenous rituals, led by Yeni Sleidi released an online video titled “Brujas Hex Trump.” From this platform on YouTube, the video called for fellow witches in both the digital and physical realms to intervene in the presidential candidate’s campaign through a type of activism not previously considered political — ritual, witchcraft, hexing. Since this initial call via YouTube, monthly hexes have continued among Brooklyn-based latinas, organized via social media.
            The organization of social justice activism through interpersonal networks on Facebook, Twitter, Instagram, and other platforms is not an unusual phenomenon for marginalized communities, evidenced by such movements as #BlackLivesMatter and #MeToo. However, the model of integration between online and offline practices demonstrated in social media witchcraft or brujería communities is worthy of note, as a reclamation of the female-identified body and indigeneity in this current political climate. Witchcraft in its traditional forms would seem to be the antithesis of digital media due to its emphasis on materiality, embodied presence, and physically-enacted rituals. However, these networked communities of digital brujas transcend this divide, as a politicized tool for empowerment, and decolonization of history and the female body. 
            Groups like Brujas Hex Trump capitalize on the ability of personal practices to have overarching political impacts, while an organization called Witch Cabinet creates workshops and digital courses for femme- and queer-identified people to learn self-care through magic ritual, and social media astrologer Danielle Ayoka gives horoscopes and personalized tarot readings via Twitter and Instagram. The intersection of the embodied rituals of witchcraft and the digital space of social media appear to be irreconcilable, and for this reason, digital expressions of witchcraft and magic are widely considered to be cheap, commercialized, or inconsequential. However, we will examine these points of apparent conflict, between medium and message that occur in these examples of witchcraft, in order to demonstrate a method for seeing the social media space as a mediator, and not an obstacle, for these practices.
            Using a theoretical frame based on Chela Sandoval’s work on dissident coalition building, Chon Noriega’s understanding of museological power structures, and the investments of black digital studies in a radical black archival practice, we build from embodied theories of ethnic studies and art ecosystems to find a method for considering race and ritual in the digital sphere. Undertaken primarily by women of color, digital witchcraft is successful at translating online presence into embodied action in ways that perhaps offer strategies for other social, institutional, and cultural communities and their activism. By studying the ways in which these digital witchcraft communities make use of social media platforms in order to bridge these divides, sometimes by using them against their designed purposes, the potential of digital activism, and its implications for studies of chicana and black feminism, indigenous studies, and other branches of ethnic studies, digital or otherwise, can be considered. 
        
    

        
            Resumen
            El desarrollo de productos dentro las Humanidades Digitales podría verse como la diferencia entre círculo y circunferencia. La circunferencia es lo que rodea al círculo, y el círculo es todo lo que contiene la circunferencia. En este orden de ideas, el objetivo de este panel es plantear las diferentes posiciones desde lo que rodea al producto como: la estrategia digital, el diseño emocional, las herramientas para producirlas, y cómo esto ha pasado de la teoría a la práctica.
            Con ocasión de la Conferencia de Humanidades Digitales de la ADHO, queremos proponer esta conversación desde algunos escenarios y casos de estudio colombianos para integrarnos en las discusiones regionales e internacionales sobre lo que significa para nosotros estudiar las Humanidades Digitales desde América Latina; y, además, porque el espíritu del libro encaja muy bien dentro de la temática de la Conferencia. 
            Esta conversación nos permitirá plantear una postura desde el Grupo de Investigación sobre lo que entendemos por Humanidades Digitales en Colombia a partir de un contexto de producción “circunferencia”. Reconocemos que desde el mundo hispanohablante ha habido un trabajo arduo por alcanzar un consenso, pero la diversidad de prácticas ha señalado particularidades que hacen que continuamente se revalúen las tentativas definiciones. 
            Adicionalmente, consideramos que la conformación del grupo “De Punto a Pixel” y su participación en el fortalecimiento de las Humanidades Digitales ha tenido una historia particular, desde que surgió la idea de conformarlo en la Biblioteca Nacional de Colombia. Relatar ese proceso nos permitirá contrastar nuestra experiencia con la latinoamericana y la de otras latitudes.
            Estrategia digital
            Natalia Restrepo Saldarriaga
            Ministerio de Cultura de Colombia
            “... para que la ciencia avance, no basta concebir ideas fructíferas, elaborar nuevos experimentos, formular nuevos problemas o establecer nuevos métodos. Las innovaciones deben ser efectivamente comunicadas a otros. A fin de cuentas, esto es lo que entendemos por contribución a la ciencia: es algo que se da al fondo común del conocimiento”.
            Robert K. Merton, La sociología de la ciencia.
            El fin de toda producción científica es que sea conocida por otros. No estamos eximidos de ello los humanistas y mucho menos cuando se tiene intenciones de exponer un acervo, un resultado de investigación o proponer una forma novedosa de hacer algo en el sector de las humanidades. A pesar de que lo anterior es claro para los humanistas, es evidente que hoy en día los proyectos propios de las humanidades digitales no tienen estrategias de divulgación y circulación que garanticen que los públicos interesados estén enterados de las nuevas producciones.
            El espacio digital propicia escenarios en donde las audiencias no solo consumen información, sino que también la producen y la reconstruyen, lo que abre un amplio espectro para nuevos procesos de innovación en la sociedad. La idea de divulgar las humanidades a través de lo digital necesita ir acompañada de estrategias de construcción colectiva de conocimiento; bien sea durante o posteriormente, esa participación crea nuevas relaciones y significados a partir del reconocimiento y apropiación de los contenidos científicos por parte de la comunidad. La misma Internet hoy facilita estas dinámicas de colaboración a través de los medios sociales como herramientas para que cualquiera pueda crear, compartir, publicar o reconstruir un contenido. Sin embargo, los humanistas digitales están haciendo poco uso de estas herramientas. Romero-Frías (2014) pone de manifiesto el escaso uso que los humanistas digitales le están dando a los medios sociales. En su investigación encontró que “la presencia en redes sociales es moderada (Twitter , 50 %; Facebook , 30,8 %)... evidenciando cómo aún hay camino por recorrer en las humanidades digitales para asumir las ideas de las culturas digitales…”.
            Esta propuesta de presentación para el panel de nuestro grupo de investigación tomará como caso de estudio la Estrategia Digital de Apoyo a la Formación Musical ‘Viajeros del Pentagrama’ del Ministerio de Cultura de Colombia, la Fundación Nacional Batuta y la OEI, como un proyecto cultural que desde su gestación se ha construido en torno a dos objetivos. El primero, y oficial, es que todos los niños del país adquieran habilidades y competencias musicales antes de terminar su educación primaria. El segundo es que cada contenido de la Estrategia sea al mismo tiempo un insumo para campañas de divulgación y apropiación de la misma. Esto significa que los productos realizados, como videos, podcast o infografías, fueron pensados en términos académicos y culturales y también en términos de divulgación de la estrategia.
            Esta presentación muestra al humanista digital una propuesta de generación de estrategias digitales de divulgación y apropiación proyectadas desde el origen del proyecto, de manera que se optimicen recursos produciendo contenidos y formatos que cumplan al mismo tiempo con la misión de exponer el tema y de ser usados en pro de la divulgación del proyecto. Además de que puedan hacer uso de herramientas digitales interactivas, como los medios sociales, que permiten a las audiencias interactuar y apropiar el contenido.
            Diseño emocional
            Tata Méndez Mahecha
            Biblioteca Nacional de Colombia
            El objetivo de esta presentación es evidenciar la importancia de adaptar metodologías como el 
                Design Thinking a los proyectos de Humanidades Digitales y reflexionar sobre el hecho de generar nuestras propias formas de empatizar digitalmente, para lograr la comprensión profunda del usuario y sus necesidades desde una perspectiva creativa que genere desarrollos digitales con un componente emocional que origine una relación cercana entre nuestros usuarios y el acervo patrimonial.
            
            Pensar empáticamente desde la creación de producto es ponernos en el lugar del otro, es anteponer las necesidades de las personas, entender su mundo y comprender lo que sienten y, al mismo tiempo, equilibrar los requerimientos que se tengan como entidad, organización o institución. Si reflexionamos sobre esto en nuestro diario vivir podríamos decir que nos sentimos más vinculados a aquellos productos que nos son más cercanos, por tanto customizar logrará la gran diferencia para generar un compromiso o involucración respecto al producto.
            Como Biblioteca Nacional de Colombia y colectividad cultural, exigirnos apropiar y transformar metodologías como el 
                Design thinking para cautivar a nuestros usuarios, nos permitirá generar argumentos y contextos humanos tan fuertes desde las disciplinas involucradas que, a la hora de crear desarrollos digitales, podremos deshacernos de todo lo que no es esencial y lograr un compromiso emocional humanizando el resultado, en nuestro caso el patrimonio de la nación.
            
            Las herramientas como facilitadoras en las HD
            Duván Barrera
            Biblioteca Nacional de Colombia 
            La profundización sobre las herramientas en las HD es el de la optimización de contenidos y productos digitales para mejorar la experiencia de usuario (UX), hace una década probablemente el aspecto visual no era tan atractivo como lo es ahora y hace veinte años era tan escaso como precario, en los años recientes se ha hablado bastante sobre mejorar la experiencia de usuario y esta suele ser la hoja de ruta para una gran cantidad de proyectos y productos digitales que emergen y proliferan cada día más. Pero no es solo hablar del aspecto visual ya que una experiencia de usuario destacada está dada en buena medida por la arquitectura propuesta para su contenido y la metodología que se usó desde su planificación, no es solo la usabilidad y navegabilidad, es un compendio de elementos que en conjunto nos dan un resultado para ser publicado como sitio web, app o 
                software.
            
            Afortunadamente en la actualidad existen una gran cantidad de herramientas que facilitan tanto a humanistas como a personas con conocimiento netamente técnico a poner en marcha sus proyectos digitales, abarcaremos con ejemplos actuales las ventajas que nos proporcionan algunas de dichas herramientas y su rol en el diseño de una experiencia de usuario óptima.
            Más que una guía definitiva sobre las herramientas para usar o no usar este capítulo, se piensa como una guía práctica para aportar con ejemplos algunas herramientas que ayuden en la conceptualización, ejecución y puesta en marcha de proyectos de Humanidades Digitales que encuentran un bache en los aspectos técnicos inherentes al producto o contenido final.
            Lenguajes de marcado para las humanidades digitales. 
            Camilo Martínez
            Universidad de los Andes
            Los lenguajes de marcado son herramientas útiles para el trabajo con contenido textual que debe ser manipulado tanto por máquinas cómo por seres humanos, sin embargo el problema de los lenguajes de marcado más populares, como XML y sus derivados, tiene que ver con que el uso de etiquetas de apertura y cierre y atributos tiende a crear archivos con gran extensión, que son confusos de leer por seres humanos. La reciente introducción de lenguajes de marcado livianos como Markdown y YAML está reduciendo la complejidad del trabajo con texto en el contexto de las Humanidades Digitales. Estas herramientas permiten escribir y leer textos estructurados más fácilmente y pero al mismo tiempo pueden ser procesados automáticamente por sistemas digitales. Esta característica hace de estos lenguajes una serie de herramientas una eficiente a la hora de hacer trabajo investigativo en el campo de las Humanidades Digitales. Sin embargo, más allá de la optimización los procesos de trabajo con corpus textuales, estos nuevos lenguajes de marcado pueden facilitar prácticas colaborativas de anotación y estructuración de textos. Esta posibilidad permite pensar en formas de abrir los procesos de investigación y construcción de conocimiento propios de las Humanidades Digitales a la participación de comunidades no especializadas. En este ensayo se analizarán los lenguajes de marcado livianos más populares del momento y algunas herramientas con las que es posible crear contenido, asignar metadatos o estructurar un texto semánticamente para su posterior procesamiento por sistemas digitales. Finalmente, se plantearán posibles usos de estos lenguajes, evidenciando su potencial para la práctica abierta y colaborativa de las humanidades digitales
            La conservación y la restauración
            Ponentes: María Helena Vargas y Sara del Mar Castiblanco. 
            Biblioteca Nacional de Colombia.
            Para el campo de la conservación y restauración del patrimonio bibliográfico y documental, la nueva era digital ha traído consigo retos y tareas importantes en cuanto la implementación de nuevas metodologías, procedimientos y técnicas, tanto para el análisis de la materialidad de los soportes físicos nacientes, como para la preservación a largo plazo de los soportes y la producción de contenidos en formatos digitales. Tal como lo plantea Lafuente (2014) no se trata de que figuras como la del conservador, curador, el bibliotecario o el mismo archivista ya no se necesiten. El asunto es que tanto, bibliotecas, museos, casas de cultura y centros culturales en general, tienen que reinventarse en un nuevo contexto en el que el acceso a la información no sólo es fácil y económico, sino que implica prácticas informales, tecnologías distribuidas y procesos deslocalizados.
            Hace ya varios años, la conservación y restauración se encuentra en una encrucijada conceptual. Lo que hoy en día conocemos como patrimonio, organizado y almacenado en bibliotecas, museos, e instituciones culturales, no siempre refleja lo que diferentes grupos sociales consideran como propio y cada vez es mayor la necesidad de que estas instituciones se acerquen a las personas con nuevas formas de presentar la información y el conocimiento. Insurralde (2010) plantea que el objeto restaurable ya no es sólo un objeto histórico o artístico que vale por sí mismo, sino necesariamente un objeto que adquiere valor por los significados que los sujetos vierten sobre este. Vale la pena preguntarse frente a esta afirmación ¿qué objetos se valoran o se valorarán ahora en la era digital?¿Cómo aprovechar las humanidades digitales para generar o reactivar diferentes significados sobre los objetos?
            Tanto la encrucijada conceptual como este último cuestionamiento llevan a pensar que, si las humanidades digitales son un campo de trabajo en donde las áreas de conocimiento tienen nuevos horizontes donde poder evolucionar de formas inesperadas y donde se pueden desarrollar, gestionando su información de forma más compleja, necesariamente los profesionales en conservación y restauración deben dirigir su mirada hacia esa dirección.
            Partiendo de autores como Arsenio Sánchez, Javier Tacón, Luis Crespo y Alberto Campagnolo, entre otros teóricos de la conservación del patrimonio en bibliotecas, pero también de autores como Charles Faulhaber, Piscitelli, Antonio Lafuente, Gimena del Río, Helena Blanco, entre otros, esta propuesta de presentación para el panel de nuestro grupo de investigación abordará las diferentes relaciones entre la conservación y restauración del patrimonio bibliográfico y documental como disciplina y su desarrollo necesario en el campo de las humanidades digitales.
            La propuesta buscará resaltar el gran potencial y variedad de posibilidades y perspectivas que se están desarrollando con el tránsito lento pero constante en este sentido. Este es sólo un paso más para afianzar la reunión de estas dos áreas que requiere también, integrarse con otras áreas de conocimiento —que se entiende desde la conservación y restauración, deben tener presencia pero que aún las vemos escasamente articuladas— como por ejemplo las ciencias de la documentación, ciencias de la información y la bibliotecología, la bibliografía, la historia y las ciencias y técnicas historiográficas, la lingüística, entre otras (Vargas, 2017).
            Siendo la conservación y restauración de bienes bibliográficos y documentales una disciplina incipiente en Colombia que ha tenido muy pocos espacios en nuestro país para desarrollarse, evaluarse, validarse, etc., se busca además resaltar el trabajo del grupo de conservación de la Biblioteca Nacional en la mencionada articulación entre disciplinas
            La edición digital
            Ponente: Javier Beltrán
            Biblioteca Nacional de Colombia
            
                Cómo guardar versus 
                guardar como: la producción de contenidos digitales para la preservación bibliográfica y documental
            
            Más allá del trasnochado debate del supuesto enfrentamiento entre el libro impreso y el libro electrónico, de la aparente novedad de los formatos de las publicaciones digitales y de la utópica innovación de las plataformas que las contienen, existe una realidad incuestionable en el mundo de hoy, hiperconectado e hipercomunicado: la digitalización y la producción de contenidos nacidos digitales puede salvar la memoria de la humanidad. Sí, 
                salvar, 
                memoria y 
                humanidad, en el sentido más literal de esas palabras.
            
            Es muy posible que los bits acaben por imponerse definitivamente sobre el papel en un futuro todavía indetermidado, más posible aún que los formatos electrónicos y los análogos convivan como complementos los unos de los otros en un futuro cercano; pero lo que resulta verdaderamente imposible es que los bits, los metadatos, la arquitectura de la información, la web semántica, los html5, los ePub2 o los ePub3 reemplacen la impronta que la historia y la memoria colectiva de la humanidad han dejado en millones de folios y superficies de papel que hoy empiezan a desvanecerse por no ser preservados, o por no estar en la lista de espera o de priorización de lo que debe ser digitalizado.
            Y hoy no hay mejor lugar para constatar esa cruda realidad que una Biblioteca Nacional latinoamericana: ante la escasez presupuestal para adquirir una tecnología que garantice la total y perfecta salvaguarda del patrimonio documental y bibliográfico de una nación, una institución con esa misión se ve en la situación paradójica de poner en cola el patrimonio que tal vez se pueda digitalizar y preservar en un futuro (o no), y simultáneamente producir contenidos digitales que satisfagan la demanda de millones de usuarios que los piden o necesitan en todos los rincones de una complicada geografía, mientras los formatos que produjo hace menos de diez años ya entraron también en la obsolescencia informática y tecnológica y pasan a una cola de espera, más atrás de la otra cola de espera.
            ¿Aliarse lo público con lo privado para salvar la memoria de la humanidad? ¿Dejar de innovar en la producción de contenidos para reforzar la preservación y la restauración digital? ¿Diseñar una política digital responsable que pueda garantizar la preservación democrática de los documentos en papel y al mismo tiempo la creación y gestión de los contenidos digitales? 
            Hoy más que nunca se necesita una ingeniería de la edición, la mezcla idónea de humanismo, ciencia y arte para salvar nuestra memoria, escribirla, reescribirla y hacerla circular.
            Existimos en la sociedad de la información en donde la tecnología moldea las nuevas formas de conocimiento, cultura y sociedad (Manuel Castells, 1996). Esto propone un reto para la construcción de proyectos digitales cohesivos, responsables y relevantes en el mundo de las Humanidades Digitales.
            En este orden de ideas, desde el diseño como una disciplina y una herramienta amplia y ramificada que propone y participa activamente en los procesos de estructuración y creación de proyectos en Humanidades Digitales, particularmente desde su capacidad para reunir y entrelazar varias facetas, derivando en soluciones creativas o diferentes, por medio metodologías articuladas como 
                Design Synthesis que es el proceso de manipular, organizar y filtrar datos de un contexto para producir soluciones o conocimiento por medio de varios métodos (Jon Kolko, 2010), al igual que el 
                Design Thinking que manifiesta el diseño desde un centro humano (IDEO, 2016). 
            
            No basta únicamente con ser un experto en esta área ni tener un equipo de trabajo para desarrollar proyectos multidisciplinares, es necesario crear puentes y conexiones para producir desde 
                la emoción, por esto es vital empatizar humana, emocional, conceptual y digitalmente con cada etapa del proyecto: “La empatía digital es un proceso en el cual una persona puede analizar &gt; reflexionar &gt; proyectar &gt; predecir &gt; sentir mediante la comunicación con lo digital” (Friesem, 2105). 
            
            La experiencia de crear productos digitales en la Biblioteca Nacional de Colombia hace parte de la transformación en la forma en como se hacen y se muestran los productos digitales para un contexto latinoamericano y específicamente colombiano y particularmente desde uno de nuestros proyectos llamado Piedra y Cielo, un movimiento poético alternativo de finales de los años 30, integrado por Integrado por Jorge Rojas, Carlos Martín, Arturo Camacho Ramírez, Eduardo Carranza, Tomás Vargas Osorio, Gerardo Valencia y Darío Samper, en el que deciden publicar “su entrañable verdad”, la poesía en sí misma sin mensajes políticos ni segundas intenciones.
            Varias cosas fueron transversales en la construcción de Piedra y cielo, como la conformación de un equipo multidisciplinar compuesto por un coordinador, una investigadora, cuatro editores, una diseñadora, dos ingenieros y una estratega digital que empatizó con los contenidos y con la relevancia del proyecto. Así mismo, representa una forma de trabajo no piramidal, podríamos decir que 
                circular en la distribución y liderazgo de tareas.
            
        
        
            
                
                    Bibliografía
                    Lange , Josua (2015). Rise of the Digitized Public Intellectual: Death of the Professor in the Network Neutral Internet Age. DOI 10.1007/s10780-014-9225-3.
                    Meza, Aurelio. “Decolonizar las humanidades digitales: cómo diseñar un repositorio digital de sur a norte”. 
                        Intervenciones en estudios culturales volumen 4, 2017: 109-131. 
                        
                            https://intervencioneseecc.files.wordpress.com/2017/07/n4_art07_meza.pdf
                        .
                    
                    Pons, Anaclet. El desorden digital: guía para historiadores y humanistas, Siglo XXI de España Editores, S.A., 2014.
                    Sánchez Hernampérez, Arsenio. “Paradigmas Conceptuales En Conservación”. CoOL Documents. 23 de noviembre del 2008. 15 de enero del 2017.
                        
                            http://www.cool.conservation-us.org/byauth/hernampez/canarias.html
                        
                    
                    Vargas M., Maria (2017). Las reparaciones ‘de época’ en libros medievales. Tesis. Lleida, España.
                    El paradigma digital y sostenible del libro, Manuel Gil y Joaquín Rodríguez, Trama editorial, Madrid, 2011.
                    Manual de edición. Guía para estos tiempos convulsos. Manuel Gil y Martín Gómez, Cerlalc, 2016.
                
            
        
    

        
            
                Multiple Paper Session: “Networks of Communication and Collaboration in Latin America”
            
            
                Panel Overview:
            
            Drawing on this year’s conference theme of “bridges/puentes,” this panel examines the ways in which networks emerge among individuals working and operating in Latin America and beyond during the twentieth and twenty-first centuries. We use digital tools to explore how artists and intellectuals connected and collaborated across countries in the early part of the twentieth century. We assess the linguistic and cultural dimensions of web readership and its communities. We investigate alternative digital distribution methods for contemporary Mexican poetry in the twenty-first century. We analyze how the visibility of (digital) narratives surrounding sexual violence in Latin America creates a unique space for necessary dialogues. We look to the particular expressions of disappearance, mortality and even spirituality in Latin American Post Internet culture. And we study how collaborative practices in digital literary creation alter the various ways in which we produce and consume texts.
            We, thus, consider not only how networks are formed 
                within Latin America, but also the ways in which these links and connections 
                extend to other regions of the world. The networks we analyze range from the literary and social, to the economic and political. How, for instance, are contemporary print forms the product of their settings, their individual publics, and their social networks? Are artistic networks conceived and maintained differently prior to the digital age? How might contemporary hashtag projects in the region expand the notion of the trans-Hispanic web? How do certain social media platforms alter our conception of self, nation, and world through their unique development of networks? How do cultural and artistic narratives eliminate social hierarchies and reveal networks of social justice? Viewed together as a collective whole (or a network of their own, perhaps), these projects explore what it means to be connected across geographies, cultures and time.
            
            Paper #1
            
                Title: Global Networks of Cultural Production
            
            
                Abstract:
            
            Victoria Ocampo, her world-renowned journal 
                Sur, and her publishing house of the same name, all loom large over Latin American cultural production in the twentieth century. While much has been written about this Argentine socialite and her impressive literary enterprises, a great deal of work still remains to be done with regard to the extent of her global reach. In an effort to address these issues, I am engineering a digital project, “Global Networks of Cultural Production,” that details a complex web of connected intellectuals, both inside and outside of Latin America, through their correspondence, translations, prologues, and edited editions. In this presentation I will describe the central cruxes of my digital project as well as provide an initial demonstration of the database I am creating. The first layer, “The 
                Sur Enterprise,” presents users with the option to navigate among three modules: People, 
                Sur, and Editorial Sur. Within each module, users can interact with data pertaining to Ocampo’s networks. For instance, in the “People” module, users can explore the occupation(s), birthplace, death place, and sex for each person that is linked to Ocampo’s literary network (and pinpoint overlaps among individuals), while the “
                Sur” module allows users to interact with contributions to the literary journal 
                Sur (grouped by genre, author, and issue). The second layer, “Visual Essays,” provides a series of network analysis visualizations that demonstrate the spatial and temporal impact of Ocampo’s efforts on the Latin American, European, and Asian populaces. Critical essays that narrate the significance of the queried data and its visual iteration accompany all of these visualizations. Each of these layers is fueled by a relational database that holds up the established links with an archive of metadata gleaned from a variety of documents, including correspondence, contracts, and even physically published books and magazines. All of these dimensions work together to digitally model Victoria Ocampo’s work in creating networks, literary circles, and literary canons.
            
            Paper #2
            
                Title: The Digital Readership Networks of the Trans-Hispanic Web
            
            
                Abstract:
            
            Despite the position of Spanish as the fifth most prominent language in overall web content, scholars are only beginning to explore the nuances of the trans-Hispanic web (2015). Drawing on case studies from Spain, Argentina, Chile, Peru and Mexico, my research assesses the web as a linguistic and cultural territory that can be mapped using digital tools and methods. “The Digital Geographies of the Hispanic World” is the first comprehensive geographical study of the web as an arena for reading and engaging with literary content. This is a project with two intersecting goals: 1) to map the readership of web-based content related to Latin American literature through a series of Spanish-language websites, identifying the networks they establish; 2) to determine if digital literary production conforms to a broader post-national aesthetic observed in print literature. Indeed, in the twenty-first century, digital content comes to life as it intersects with web analytics, aiding scholars in grasping the cultural and linguistic configurations that emerge around web content. Given the new possibilities of data analysis of this rich content, scholars are beginning to realize that how readers engage with web-based literary content often has more to do with language communities than the IP addresses or national contexts from which literary content arises. This presentation will explore some of the most recent data collected on web readership and network analysis of a series of leading literary websites from the Hispanic world.
            Paper #3
            
                Title: Post-Print Culture and Publishing Networks in Contemporary Mexican Poetry
            
            
                Abstract:
            
            In the last fifteen years, independent publishing houses have been the central space that has defined the most relevant literary themes and forms of contemporary Mexican poetry. These publishers have changed the inertia of the literary field through strategies that produce an aggregate value to their books, based on symbolic frameworks and alternative distribution practices. These unique methods of book circulation enable experimental poetics to find an auspicious space in independent publishing houses. The publishers in question use the academic prestige of experimental poetics, while speculative poetics nourish intellectual distinction when published by independent firms.
            Nevertheless, despite the efforts to distribute books, independent publishers do not have the economic resources for national or international shipping. As a response to those problems, in recent years web platforms and collaboration networks have appeared, allowing the free circulation of books in PDF format. The existence of both types of distribution poses questions regarding the social forms of circulation for contemporary Mexican poetry, particularly in terms of how literary forms establish a dialogue or refuse to deal with those alternative practices of distribution and distinction. To answer these questions, I propose “post-print” as a concept that can be broad enough to explain the relationship between print publishing and digital distribution, as well as the use of consent and collaboration in the reproduction of experimental literary forms.
            Paper #4
            
                Title: 
                Nuestro Primer Acoso: Digital Networks and Collective Action against Sexual Violence
            
            Abstract:
            In the spring of 2016, new digital activist networks emerged to address gendered and sexual violence in Latin America.  Of the hashtags generated by these movements, few gained the public recognition of #MiPrimerAcoso (or “My First Harassment” or “My First Abuse”), a hashtag that encouraged individuals to tweet their first experiences of sexual violence. When evaluating #MiPrimerAcoso’s popularity, it is necessary to contextualize the concrete metrics of #MiPrimerAcoso within the intangible, affective dimensions that characterize the streams of discourse that grew out of the hashtag: the networks of #MiPrimerAcoso formed on the basis of shared experiences and shared public feelings. This analysis seeks to surpass traditional metrics of Twitter engagement and delve deeper into the kinds of connections that users form with each other within the intangible streams of discourse generated by the hashtag. For example, what makes retweeting a news story about #MiPrimerAcoso different from retweeting another user’s story of sexual violence? The quantitative dimensions of #MiPrimerAcoso's digital proliferation – the prevalence of retweets, for example, or the use of other hashtags to link formerly disparate currents of digital conversation – are explored alongside a critical analysis of the discursive conditions generated under the hashtag’s narrative premise. In examining this dialogue, this project illustrates the networks of affect that stitched recollections of trauma into a political outcry. 
            Paper #5
            
                Title: Critical Networks: Latin American Death, Remembrance, and Recovery in the Post-Internet
            
            
                Abstract:
            
            The faceless of Latin America, the 
                desaparecidos (disappeared), historically total in the tens of thousands within multiple nations, with extreme numbers in Chile and Argentina, due primarily to military dictatorships. Thus, a history of disappearance and loss have become embedded into the national psyche of many in Latin America, leading to the advent of “truth commissions” during the “memory boom” of the 1970s and 1980s. Today, Latin America, one of the fastest growing Internet populations in the world, now finds itself rapidly joining a globalized electronic culture. Arguably, the network leads to monoculture, and a commoditization of the individual, a result of the electronic Culture Industry. Therefore, today, Latin America may find itself disappearing digitally.
            
             
            With digital remembrance and disillusionment in mind, this paper investigates the particular expressions of disappearance, mortality and even spirituality in Latin American Post Internet culture through the work of Brazilian artist Eva Rocha, primarily, as well as Teresa Margolles of Mexico, both of whose work is devoted to the outcast, the dead, and the forgotten. Through an analysis of these critical works, one may find a foil to the new electronic colonialism of the global digital networ
            Paper #6
            
                Title: Collaborative Practices in Digital Literary Creation
            
            
                Abstract:
            
            ​In Latin America, digital literature is a relatively new phenomenon. In the analysis of Latin American digital texts, I have considered both their material composition as well as aspects of authorship and reception practices. Materiality here refers to the technologies that have been used by the author in the production of the digital text. Depending on the technology used in digital narratives, we find texts that range from simple productions—like hypertext based productions—to more complex texts that include music, images, moving text, and also make use of many different software. Thus, the effects produced in readers can be aesthetically varied, and are determined by the technologies used to create the literary works in question. Although it is true that the specificity of the medium is a main component in the study of digital literature, the sole attention to the material elements of the texts is not enough to grasp some features that are unique to these productions, especially in a region where the introduction and uses of new technologies are strongly related to politics.
             
            In this presentation, I examine how both the production and the reception of literature have been affected by digital technology, with special emphasis on issues related to Latin American digital literature. I will analyze Jaime Alejandro Rodriguez's 
                Narratopedia, Doménico Chiappe's 
                La Huella de Cosmos, and Leonardo Valencia's and Eugenio Tiselli's 
                El Libro Flotante in order to highlight collective practices of creation involved in digital productions. Through a discussion of these issues, I offer an overview of ongoing changes wrought by digital technology in contemporary Latin American cultural production.
            
        
    

        
            Cuando hablamos de pensar filosóficamente las humanidades digitales, nos referimos a la producción de un tipo de aproximación crítica que revisa la configuración tecnológica, así como la producción discursiva que la acompaña. Dicha configuración en algún momento se pensó gozaba de cierta neutralidad, pero el análisis de la tecnología ha conducido, cada vez más, a reconocer que la producción de estas herramientas tiene supuestos teóricos que, a partir de un examen filosófico, permiten encontrar parte de sus problemas y posibilidades. Por otra parte, si los discursos se reconocen técnicos pueden contribuir a fortalecer la tradición de investigación dirigida por la lectura y escritura de textos, a partir de las posibilidades de visualización, producción de datos, acumulación y procesamiento de información. Pensar filosóficamente las humanidades digitales se trata no sólo de una oportunidad de hacer consciente el carácter técnico del trabajo académico en las humanidades, sino de mostrar un desplazamiento en la producción del saber humanístico, y el sentido que éste tiene, a partir de la intervención tecnológica.
            
                 Nuestra aproximación busca hacer un examen crítico de estas prácticas, como ya lo hacen algunos estudios de teoría de las humanidades digitales, las ontologías tecnológicas, los estudios de retórica computacional y otros campos. Este análisis, por otro lado, nos permite reflexionar sobre las técnicas de producción del pensamiento filosófico. A partir de este examen queremos mostrar las dificultades prácticas y discursivas que surgen dentro de las humanidades digitales, que requieren pensarse para delinear sus discursos y mejorar sus prácticas. 
            
            
                Palabras clave: Filosofía, Tecnología, Humanidades Digitales, Crítica
            
            
                Francisco Barrón: 
                    Distant reading
                     y el ejercicio tecnológico de la filosofía
                
            
            Embistamos al ejercicio de la filosofía, tal como lo conocemos y llevamos a cabo aún el día de hoy, con un ejercicio de cuestionamiento tecnológico de las condiciones de su producción. Las Humanidades Digitales no sólo son la oportunidad de discutir los procedimientos académicos de producción de saber humanístico, permiten formular la pregunta sobre una modificación tecnológica del ejercicio del pensamiento.
            El actual ejercicio de la filosofía comienza en unas épocas tecnológicas bien determinadas y anteriores a la nuestra. Las técnicas y tecnologías del ejercicio de la filosofía pertenecen a formas de producción artesanales. Sin embargo, vivimos la experiencia del malfuncionamiento, la alteración de la figura, las técnicas y las funciones de lo que llamamos el ejercicio del pensamiento. No es para nadie una noticia que el mecanismo político-académico moderno del pensamiento, su sentido, junto con sus prácticas, sus instituciones y las figuras que la encarnaban, parece ya no funcionar adecuadamente para ciertos acontecimientos contemporáneos. Sería arduo hacer el recuento de las condiciones que han hecho eso. La tecnología parece tener algo que ver también con ello, junto con políticas de muerte, transformación de las formas de escritura, caídas de instituciones de distribución del saber, alteración de los modos de reproducción de la subsistencia, y muchos más acontecimientos a enumerar.
            El ejercicio de la filosofía, tal como se nos ha heredado en una tradición ilustrada-romántica, europea, parece ya no funcionar para nosotros aquí y ahora. Lo tecnológico, sobre todo su avatar digital contemporáneo, tiene efectos radicales en la producción y organización del trabajo intelectual. Hasta el día de hoy, en el ejercicio de la filosofía se reniega de su carácter técnico. Pero, quizás más nos valdría aventurarnos en la experiencia tecnológica del ejercicio de la filosofía. Más valdría hacer experimentos con tecnologías y metodologías de las Humanidades Digitales para saber, al menos, si se puede ejercer el pensamiento filosófico en estas condiciones digitales de producción de saber. 
            Franco Moretti (
                Literatura vista desde lejos, 2005 y 
                Lectura distante, 2013) ha discutido, al analizar enormes conjuntos de datos para estudiar la literatura, supuestos teológicos de los actos de lectura y de la crítica literaria. Lo ha hecho así para producir saber y crítica de la literatura usando tecnologías computacionales. Quizás sería posible discutir si la 
                distant reading nos permitiría ejercer tecnológicamente el pensamiento filosófico.
            
            
                Palabras clave: Filosofía, Tecnología, 
                Distant reading, Pensamiento
            
            
                Ana María Guzmán Olmos: 
                    Frankenstein y Cthulhu. Crítica en las (in)humanidades digitales 
                
            
            En 
                Where Is Cultural Criticism in the Digital Humanities? (2012) Alan Liu cuestionó el potencial crítico de las humanidades digitales y su perspectiva a futuro como disciplina. El texto se sitúa dentro de los clásicos que cuestionan la idea de que los procesos de digitalización del contenido humanístico son suficientes para generar saber, un saber mediado por lo digital. Sirviéndose del binomio que compone el concepto “humanidades digitales”, en su diagnóstico Liu hace énfasis en la crítica como centro de la producción humanística. Según la tesis de Liu, si hay algún potencial a futuro en las humanidades digitales, este debería ser buscado en la noción de crítica; en mi presentación voy a discutir esta idea. Voy a señalar que el modelo de la crítica está vinculado a un mecanismo de representación que nubla la posibilidad de pensar la diferencia. La crítica, según el modelo presentado por Liu, invisibiliza la diferencia.
            
            Para discutir el modelo crítico del proyecto humanístico seguiré el texto 
                Loving the Alien (2017)
                , en el cual Lisa Blackman se sirve de la categoría 
                alien como herramienta para pensar aquello que no es comprensible dentro del campo de lo humano. Lo humano se construye como una categoría excluyente constituida mediante marcas de género, raza o modelos de corporalidad; este mecanismo produce un límite donde el exterior queda atravesado por condiciones de vulnerabilidad y explotación. Mediante las figuras de lo inhumano Blackman discute la idea de una política no centrada en el cuerpo. Siguiendo a Blackman voy a explorar el concepto del “inhumanismo de lo humano y el humanismo de lo inhumano” (Blackman, 2017), esto en el campo abierto por las humanidades digitales. 
            
            Mi tesis es que la noción de crítica en las humanidades, sean estas digitales o no, depende de hacer visible la multiplicidad en un cuadro homogéneo que permite a los sujetos reflexionar sobre la unidad; este es el modelo de la representación. Dicho modelo puede ser observado en diversas metodologías usadas en las humanidades digitales, las cuales voy a ejemplificar con el método de visualización de datos. Frente a la visualización de datos, que traduce el lenguaje computacional en lenguaje “humano”, me interesa pensar, siguiendo a Luciana Parisi (
                Reprogramming Decisionism, 2017), el razonamiento de las máquinas que se elabora en procesos como la recombinación algorítmica autónoma, o el aprendizaje que hacen los algoritmos de otros algoritmos, y que dan lugar a espacios de indeterminación. Estos factores del razonamiento maquínico permiten elaborar un concepto de diferencia que depende de la repetición y el 
                performance algorítmico, y no de la unificación. El potencial de las humanidades digitales estaría, en este sentido, ligado a su condición inhumana. 
            
            
                Palabras clave: humanismo, representación, diferencia, lenguaje computacional, visualización de datos, recombinación algorítmica 
            
            
                Sandra Reyes Alvarez. 
                    Filosofar la tecnología: un camino educativo
                
            
            Los enfoques educativos actuales han enfatizado la importancia de la educación digital. Incluso es uno de los ejes de dichos enfoques, se considera pieza clave dentro de las competencias a desarrollar en los estudiantes y se califica como necesaria para “los retos del siglo XXI”. Voy a referirme, concretamente, al Nuevo Modelo Educativo mexicano dentro de la Educación Media Superior, partiendo de mi experiencia docente en el área de las Humanidades en las asignaturas de filosofía.
            Cuando se habla de educación digital se parte, en primer lugar, de una definición parcial de lo que se concibe como tecnología. En segundo lugar, se considera una brecha generacional que produce un déficit entre maestros y alumnos respecto a dicha tecnología, generalmente estas consideraciones suelen desprestigiar la figura del docente y exaltar la eficacia que implica la intervención tecnológica en los procesos de enseñanza-aprendizaje. Estos aspectos manifiestan la ausencia de una reflexión sobre lo tecnológico en relación con los enfoques psicopedagógicos que regulan la educación general y particularmente digital. 
            Lo que me interesa señalar es cómo a partir de una reflexión filosófica es posible mostrar las deficiencias y problemáticas que se derivan de la implementación de una educación digital que no contempla las prácticas tecnológicas de profesores y alumnos, y los discursos bajo los cuales éstas se han interpretado y bajo los que se gestionan. Me propongo elaborar una crítica filosófica al respecto, con la finalidad de esbozar una propuesta que encamine la implementación de dicha educación dentro de un contexto lo más aproximado a la realidad del bachillerato mexicano, específicamente en el ámbito de las humanidades y desde un pensamiento filosófico que reflexiona continuamente sobre la relación entre su saber -incluidas sus formas de producción- y lo tecnológico y cómo esto desplaza el sentido tradicional de la enseñanza filosófica actual, sin descalificar los procesos tradicionales ni la importancia de la enseñanza de la filosofía, pero mostrando cómo al desplazarse se generan nuevas formas, no sólo de enseñanza filosófica, sino de producción filosófica que intervienen el ámbito educativo y la profesionalización del filósofo como maestro que reflexiona en la producción de su saber, y de las maneras en que transmite y gestiona el mismo, ante la intervención de la tecnología. 
            Pienso que, frente a la importancia que se imprime a la educación en general, y particularmente a la digital, es necesario analizar los discursos y prácticas que sostienen su relevancia, así como los factores y actores que intervienen en su aplicación con la finalidad de trazar posibles caminos que consideren las variables que pudieran presentarse, así como los contextos y espacios concretos en donde se lleva a cabo la misma. Me interesa, por último, señalar cómo las dificultades de implementación del Nuevo Modelo Educativo intervienen en la aplicación concreta de una educación tecnológica que va desde la infraestructura que la hace posible, hasta el modo de concebir dicha educación.
            
                Palabras clave: educación, tecnología, enseñanza, filosofía, producción, humanidades, digital, bachillerato, profesor. 
            
            
                Elena León Magaña:
                     Ejercer maquínicamente la Filosofía. Pensamiento técnico vs Pensamiento filosófico
                
            
            ¿Puede un robot desarrollar pensamiento filosófico? El Seminario Tecnologías Filosóficas se planteó el problema del pensamiento maquínico en octubre del 2015. Durante el tiempo de vida del seminario hemos discutido la modificación tecnológica de la experiencia y de la práctica de pensamiento; en este sentido, hemos intentado desarrollar una genealogía de la relación entre el pensamiento filosófico y la tecnología. Derivado de ello hemos encontrado discursos que problematizan la articulación tecnología/experiencia. En este sentido, la idea de desarrollar una “herramienta digital” que ponga a prueba los discursos que enuncian la modificación tecnológica de la experiencia en la que se relaciona, tanto la práctica del pensamiento filosófico, como de la tecnología. 
            Dicho lo anterior, algunas de las preguntas que fueron surgiendo durante la planeación y problematización de dicha herramienta fueron: ¿Sería posible determinar una tecnología filosófica? ¿Determinarla fuera de esos mitos metafísicos de la herramienta, de un sujeto que la usa, de la creación del pensamiento o de la inspiración genial? ¿Sería posible determinar elementos y procesos del ejercicio del pensamiento filosófico? Después de un largo camino, donde, por ejemplo, se planteó el uso de un 
                bot basado en el pensamiento del filósofo Gilbert Simondon, que interactuase en Twitter con otros bots a partir del procesamiento en una herramienta de 
                Machine Learning; o bien, un 
                bot de Twitter que fuese capaz de discernir entre una postura negativa, positiva o neutra respecto de lo tecnológico, evaluando los 
                tweets. 
            
            Finalmente, se desarrolló un 
                bot filosófico en dos etapas: 
            
            
                Un 
                    chatbot basado en el Modelo oculto de Markov para la toma de decisiones; es decir, basado en el contenido proporcionado: una curaduría de citas sobre textos de los filósofos: Gilbert Simondon, Walter Benjamin, Unabomber, Tiqqun, Jean Louis Deottè, Günther Anders y Michael Foucault. Con base en estas citas el bot decidirá entre copiar el pensamiento para interactuar con 
                    tweets con las palabras clave seleccionadas, o bien, si a partir de la información proporcionada, desarrollará nuevos 
                    tweets. Las palabras que serán elegidas para iniciar la interacción son: tecnología, experiencia, técnica, filosofía, máquina, maquinaria, maquinal, maquinizado (a), artificial, artificialidad, artificio, herramienta, herramientas, automático, automatización, autómata, automatismo, función, funcionamiento, funcionalización, funcional, político (a), politización, políticas, revolución, revolucionario, aparato, aparatos, dispositivo, tecnológico, usuario (a), usuarios (as), tecnologizado, cibernético (a). 
                
                
                    Deep learning; en esta etapa se utilizó una plataforma 
                    deep learning. A partir de ella el robot aprenderá y desarrollará nuevo pensamiento basado en sus interacciones en Twitter. La apuesta es desarrollar una discusión filosófica a partir de los resultados de la interacción para determinar si se ha 
                    innovado pensamiento filosófico, y si el producto de las interacciones del 
                    bot puede ser considerado pensamiento filosófico. 
                
            
            
                Palabras clave: filosofía, bot, twitter, pensamiento técnico, pensamiento filosófico, inteligencia artificial, máquinas, tecnología, pensamiento, máquina, maquínico. 
            
            
                Efrén Marat Ocampo Gutiérrez de Velasco: 
                    Filosofía y Tecnología: Proyectos y Teoría en el Seminario de Tecnologías Filosóficas 
                
            
            La relación entre filosofía y tecnología permite un examen crítico sobre cómo se afectan ambos. Por una parte, la filosofía que reconoce sus medios tecnológicos permite explorar su producción en términos ontotécnicos y éticos. En el caso de la tecnología se puede explorar como ésta también pertenece a una forma de pensamiento y no implica una suspensión reflexiva ante su hacer como lo ha discutido Simondon. Pensar filosofía y tecnología aporta a las humanidades digitales una perspectiva crítica para el trabajo en estos campos.  
            En específico se busca revisar los problemas teóricos entre filosofía y tecnología que ha abordado el Seminario de Tecnologías Filosóficas. Esta agrupación ha optado por revisar la aproximación teórica encontrada en los procesos de producción de sus proyectos. Entre estos se encuentran el análisis de la reconsideración de la biblioteca pública en la Biblioteca Vasconcelos de la Ciudad de México, el análisis de la base de datos de las tesis de filosofía producidas en la Universidad Nacional Autónoma de México, la discusión de textos sobre filosofía de la tecnología y filosofía de los medios y la producción de un 
                bot a partir de contenido filosófico. Esta muestra de proyectos ha ayudado a producir un nexo de problemas teóricos sobre la práctica de las humanidades digitales.
            
            Las discusiones llevadas a cabo han permitido observar de qué forma la tecnología de la producción teórica, y la configuración de sus prácticas, conducen a una discusión sobre cómo se posibilitan, a partir de la investigación de las humanidades, las herramientas tecnológicas. Algunas de las aproximaciones que se han explorado son:
            
                La modificación política del espacio público a partir de la digitalización de los archivos de las bibliotecas públicas señalando a la especialidad de las prácticas de las humanidades digitales. La noción de experiencia en términos del uso tecnológico.
                La transformación de la práctica discursiva a partir de los mecanismos de automatización y programación de las humanidades. Incluida la pregunta sobre la diferenciación de su moralización, su carga de opinión y su relación con la producción de datos. 
                Los cambios en la producción de las prácticas y de la comprensión de su desarrollo. El problema de las nociones de lectura, escritura y visualización en términos digitales como una cuestión política, ontológica y ética. 
                La revisión del lugar de la teoría por su mediación tecnológica. ¿Qué hace que se necesiten mantener diferenciadas? ¿Por qué el pensamiento ha encontrado en su consideración técnica una forma de replantearse? 
            
            
                 De esta forma, se piensa que reconociendo estas dinámicas se puede hacer una referencia a su impacto en estudio filosófico de las humanidades digitales. La filosofía en las humanidades digitales todavía tiene un papel a ser considerado como un desarrollo sobre la revisión crítica de prácticas cada vez más sofisticadas técnicamente, que revisando sus producciones entienden sus efectos.  
            
            
                Palabras clave: filosofía, discursividad tecnológica, teoría crítica, biblioteca pública, pensamiento maquínico, tecnología. 
            
            
                Ethel Zaira Rueda Hernández: 
                    Los laboratorios de humanidades: tecnologías del error
                
            
            Las humanidades actuales se hacen en el laboratorio. La proliferación de espacios de investigación humanística que se llaman a sí mismos, o se asumen de una u otra manera, como laboratorios, o como espacios de experimentación tecnológica y científica, indica la institucionalización de un mecanismo que hasta hace poco parecía vetado a las disciplinas que se ubican fuera del espectro de las llamadas “ciencias duras”. Si bien el laboratorio no es un dispositivo nuevo como modo de producción de saberes, sí tiene, en el ámbito de las humanidades, un halo de novedad que lo vuelve una opción atractiva y popular, frente a los ya consabidos formatos de seminarios, talleres, mesas de debate o discusión, etc.
            ¿Qué es, qué hace un laboratorio de humanidades? ¿Por qué se asocia esta figura, y la de la experimentación humanística, con lo digital? Para Bruno Latour el laboratorio es una máquina privilegiada para cometer errores. Esto es importante porque son justo esos mecanismos de aceleración de los procesos de ensayo y error los que permiten obtener el poder de modificar el mundo. Si a las humanidades les interesa tener alguna incidencia en lo real, parece que apropiarse de la maquinaria del laboratorio es una estrategia que, al menos a los ojos de Latour, podría resultar conveniente.
            ¿Qué es “cometer un error” para las humanidades? ¿Qué implica que el laboratorio se esté convirtiendo en una de las figuras dominantes de los campos de investigación humanísticos? ¿Funcionan los laboratorios de humanidades de manera similar o análoga a los laboratorios científicos? ¿Cuál es la relación con el discurso científico que se hace posible a partir de la ampliación del concepto de laboratorio? Si las tecnologías digitales abren la posibilidad al quehacer humanístico de tomar la forma de los tubos de ensayo ¿qué ensayamos?, ¿qué es este ensayar, este equivocarse sistemáticamente, este acumular errores, del que las humanidades digitales y las artes parecen tan interesadas en apoderarse? 
            ¿Dónde radica el poder de un error? ¿En recordarlo para no cometerlo de nuevo? ¿En señalarlo para marcar un límite? ¿En incorporarlo a una base de datos que mapea todas las combinaciones posibles de lo real? Se trata de pensar el laboratorio como dispositivo, más allá de las disputas sobre el valor de la ciencia frente a las artes liberales, se trata de pensar nuestras tecnologías digitales experimentales más allá de la perversa ingenuidad de la tecnofilia y la tecnofobia, se trata de preguntarnos por el funcionamiento del laboratorio de humanidades en tanto que máquina de producción de conocimiento, de abrir la caja negra y adentrarnos en el entramado de cables, circuitos y piezas que constituyen las entrañas de nuestro labor cotidiana.
            
                Palabras clave: Laboratorio, Humanidades, Dispositivo, Error, Producción de conocimiento
            
        
        
            
                
                    Bibliografía
                    
                        Arendt, H. (2016).
                         La condición humana. Barcelona: Paidos.  
                    
                    
                        Barrón, F. (2013). Individuación tecnológica. Apuntes para pensar la educación tecnológica. 
                        Blog de la Red de Humanidades Digitales. [en línea]. 16 de febrero del 2013. Disponible en: 
                        
                            http://humanidadesdigitales.net/blog/2013/02/16/individuacion-tecnologica-apuntes-para-pensar-la-educacion-tecnologica/
                         [26/04/18].
                    
                    
                        Bawa.Cavia. (2018). “The Inclosure of Reason” en 
                        Techosphere Magazine. Dossier 
                        Human; 2016, visitado el 1 de febrero de 2018, &lt;
                        
                            https://technosphere-magazine.hkw.de/article1/6aefb210-0ee6-11e7-a253-d9923802c14e
                         &gt;. 
                    
                    
                        Blackman, L. (2016). 
                        Loving the alien. A Post-Post Human Manifesto. Miami:Institute of Contemporary Art Miami: Fall Semester.
                    
                    
                        Blatt, B. (2017). 
                        Nabokov’s Favorite Word Is Mauve. 
                        What the Numbers Reveal About the Classics, Bestsellers, and Our Own Writing. New York: Simon and Schuster.
                    
                    
                        Brussa, V. (2017). Otros laboratorios: discutiendo la extitución y democratización tecnocultural en los laboratorios de Humanidades Digitales iberoamericanos. 
                        Revista Virtualis, 7(13).
                    
                    
                        Ernst, W. (2013). 
                        Digital memory and the archive. Minnesota:Minnesota University Press. 
                    
                    
                        Estalella, A., Lafuente, A. y Rocha, J. (2013). Laboratorios de procomún: experimentación, recursividad y activismo. 
                        Revista Teknocultura, 10(1): 21-48. 
                        Gutierrez, E. (2013). Buenas prácticas en la docencia digital. 
                        Blog de la Red de Humanidades Digitales. [en línea]. 20 de noviembre del 2013. Disponible en: 
                        
                            http://humanidadesdigitales.net/blog/2013/11/20/buenas-practicas-en-la-docencia-digital/
                         [26/04/18].
                    
                    
                        Gutierrez, E. (2012). La educación permanente y las nuevas tecnologías de la información y la comunicación. 
                        Blog de la Red de Humanidades Digitales. [en línea]. Disponible en: 
                        
                            http://humanidadesdigitales.net/blog/2012/12/02/la-educacion-permanente-y-las-nuevas-tecnologias-de-la-informacion-y-la-comunicacion/
                         [26/04/18].
                    
                    
                        Haraway, D. (2016). 
                        Staying with the trouble. Estados Unidos: Duke University Press.
                    
                    
                        Herbón, D. (2014). Reflexiones contra el ciberfetichismo académico. 
                        Blog de la Red de Humanidades Digitales. [en línea]. 7 de mayo del 2014. Disponible en: 
                        
                            http://humanidadesdigitales.net/blog/2014/05/07/ciberfetichismo-academico/
                         [26/04/18].
                    
                    
                        Ihde D. (2009). 
                        Postfenomenología y tecnociencia. España: Sello Arsgames. 
                    
                    
                        Instituto de Investigaciones sobre la Universidad y la Educación. [IISUE UNAM oficial]. (2017/03/29). ¿El Nuevo Modelo Educativo promueve el uso de las TIC?. [Archivo de video]. Disponible en: 
                        
                            https://youtu.be/hltn0oXFhaM
                         [Recuperado 26/04/18]
                    
                    
                        Kittler, F. (2014). 
                        The truth of the technological world. Stanford: Stanford University Press. 
                    
                    
                        Krzywkowski, I. (2010). 
                        Machines à écrire : Littérature et technologies du xixe au xxie siècle. UGA Éditions.
                    
                    
                        Laruelle, F. (1994). 
                        The concept of ‘first technology’: a ‘unified theory’ of technics and technology. trans. Nadita Biswas Mellamphy. 
                    
                    
                        Latour, B. (2003). The World Wide Lab. Wired. 
                        
                            http://www.wired.com/2003/06/research-spc/
                         Accesado el 1 de febrero de 2018.
                    
                    
                        Latour, B. y Woolgar, S. (1995).
                         La vida en el laboratorio. Madrid: Alianza.
                    
                    
                        Liu, A. (2012): “Where is Cultural Criticism in the Digital Humanities?” en 
                        Debates in the Digital Humanities. University of Minnesota Press: visitado el 1 de febrero de 2018, &lt;
                        
                            http://dhdebates.gc.cuny.edu/debates/text/20
                         &gt;.
                    
                    
                        Liu, A. (2018). 
                        Digital Humanities Diversity as Technical Problem. 
                        Alan Liu; 15 de enero 2018.
                        
                            doi:10.5072/FK2222ZR81
                        , visitado el 1 de febrero de 2018
                    
                    &lt;
                        
                            http://liu.english.ucsb.edu/digital-humanities-diversity-as-technical-problem/
                         &gt;.
                    
                    
                        Mirowski, P. (2017). Against citizen science. 
                        
                            https://aeon.co/essays/is-grassroots-citizen-science-a-front-for-big-business
                         Accesado el 1 de febrero de 2018.
                    
                    
                        Moretti, F. (2007). 
                        La literatura vista desde lejos. Barcelona: Marbot Ediciones. 
                    
                    
                        Moretti, F. (2016). 
                        Lectura distante, Buenos Aires: Fondo de Cultura Económica.
                    
                    
                        Negrestani, Reza (2014). “The labor of the inhuman (Parte I y II)” en 
                        E-Flux. Journal #52; febrero 2014, visitado el 1 de febrero de 2018, &lt;
                        
                            http://www.e-flux.com/journal/52/59920/the-labor-of-the-inhuman-part-i-human/
                        &gt;.
                    
                    
                        Parisi, L. (2017). “Reprograming Decisionism” en 
                        E-flux. Journal #85; octubre 2017, Accessed on 20th december; 2017, visitado el 1 de febrero de 2018,
                    
                    &lt; 
                        
                            http://www.e-flux.com/journal/85/155472/reprogramming-decisionism/
                         &gt;.
                    
                    
                        Parisi, L. (2013), 
                        Contagious architecture : computation, aesthetics, and space. Cambridge, Mass: MIT Press.
                    
                    
                        Piscitelli, A. (2005). Tecnologías Educativas. Una letanía sin ton ni son. 
                        Scielo. Revista de estudios sociales. [en línea]. N° 22. Disponible en: 
                        
                            http://www.scielo.org.co/scielo.php?pid=S0123-885X2005000300012&amp;script=sci_arttext&amp;tlng=en
                         [Recuperado 26/04/18].
                    
                    
                        Power, N. (2017) “Inhumanism, Reason, Blackness, Feminism” en 
                        Glass - Bead. Site 1: Logic gate, The Politics of the Artifactual Mind; 2017, visitado el 1 de febrero de 2018,&lt;
                        
                            http://www.glass-bead.org/article/inhumanism-reason-blackness-feminism/?lang=enview
                         &gt;. 
                    
                    
                        Ramírez, A. (2016). La (
                        presencia) ausencia de las TIC en el Modelo Educativo 2016. 
                        Blog de la Red de Humanidades Digitales. [en línea]. 27 de septiembre del 2016. Disponible en: 
                        
                            http://humanidadesdigitales.net/blog/2016/09/27/ticmodelo2016/
                         [26/04/18].
                    
                    
                        Ramírez, A. [armartinell]. (2016/09/24). Presencia de las TIC en el Modelo Educativo de la SEP 2016. [Archivo de video]. Disponible en: 
                        
                            https://youtu.be/109Eeg1fXWU
                         [Recuperado 26/04/18].
                    
                    
                        Rangel, D. (2018). Educación futura. (n.d.) [online] Available from: 
                        
                            http://www.educacionfutura.org/lo-que-el-modelo-educativo-no-contemplo-la-realidad-social/?platform=hootsuite
                         [Recuperado 26/04/18].
                    
                    
                        Scolari, C. Alfabetismo transmedia: un programa de investigación. 
                        Blog de Carlos Scolari. [en línea]. 26 de septiembre del 2014. Disponible en: 
                        
                            https://hipermediaciones.com/2014/09/26/transalfabetismos/
                         [26/04/18]. 
                    
                    
                        Secretaría de Educación Pública. (2017). Modelo Educativo Para la Educación Obligatoria. Disponible en: 
                        
                            https://www.gob.mx/cms/uploads/attachment/file/198738/Modelo_Educativo_para_la_Educacio_n_Obligatoria.pdf
                         [Recuperado 26/04/18].
                    
                    
                        Serra, A. (2013). Tres problemas sobre los laboratorios ciudadanos. Una mirada desde Europa. 
                        Revista Iberoamericana de Ciencia, Tecnología y Sociedad. 8(23): 283-298.
                    
                    
                        Shaviro, S. (2012) 
                        Without Criteria. Massachusetts: MIT Press.
                    
                    
                        Simondon, G. (2007). 
                        El modo de existencia de los objetos técnicos. Buenos Aires: Prometeo. 
                    
                    
                        Simondon, G. (2015). 
                        La individuación a la luz de las nociones de forma y de información. Buenos Aires: Cactus.
                    
                    
                        The Point (2014). The New Humanities. 
                        Revista The Point. 
                        
                            https://thepointmag.com/2014/criticism/the-new-humanities
                         Accesado el 1 de febrero de 2018.
                    
                
            
        
    

        
            On September 20, 2017, Hurricane Maria made landfall in Puerto Rico. As the strongest hurricane to hit the island since 1928, the storm has caused significant damage—especially to infrastructure including roads, dams, communications networks, the electrical grid and the water supply. With much of the island still without power, and with limited aid coming from the United States, Puerto Rico is being left to deal with a humanitarian crisis on its own. The slow nature of the United States’ response, coupled with Donald Trump’s barrage of tweets, highlight the ways in which colonial narratives are feeding into disaster response efforts. For example, when San Juan Mayor Carmen Yulin Cruz requested an increase in federal aid, Trump replied, “Such poor leadership ability by the Mayor of San Juan, and others in Puerto Rico, who are not able to get their workers to help. They want everything to be done for them when it should be a community effort” (@realDonaldTrump). He later went on to claim that Puerto Rico’s need for aid was hurting the federal budget and to claim that Hurricane Maria was not “a real catastrophe” for the island (“Trump compares Puerto Rico to Katrina”). Trump’s victim-blaming behavior highlights both his lack of empathy for the citizens of Puerto Rico and the racial prejudice that undergirds the U.S. colonial enterprise. Although rarely so blatant, such behavior is not new; rather, the United States has an ongoing legacy of racialized disaster relief that is grounded in its colonial endeavors, particularly in Puerto Rico. 
            According to 
                El Nuevo Día, the most widely-circulated newspaper in Puerto Rico, “El huracán María no superó a San Felipe II según un informe preliminar”, or “Hurricane Maria did not surpass the strength of the San Felipe II Hurricane” (Ortega Marrero). Nevertheless, the two storms bear striking similarities. Both hit the island of Puerto Rico as category 5 hurricane, both crossed the island from the southeast corner and moved through the center of the island to the northwest corner, and both had significant long-term effects on the island. 
            
            While coverage of the 1928 storm’s devastation in Florida is prominently displayed in novels and journalistic reports, coverage of the damage in Puerto Rico is almost non-existent in the mainland United States. I argue that the vulnerabilities created by the hurricane of 1928 were pivotal to the United States colonial agenda in Puerto Rico, resulting in a land grab by corporations and government entities that would impede the island's agricultural industry and economy for decades. This is made evident by the fact that the U.S. downplayed effects of the storm, the U.S. implemented policies to hurt small farmers &amp; agricultural workers, and the U.S. denied that their actions caused economic and environmental harm to Puerto Rican citizens.
            To make these connections clearer and to bring the stories of the storm’s underrepresented victims back into our cultural memory, I have launched a digital work called the 
                Hurricane Memorial project. This site includes my preliminary research, visualizations of my findings, and interviews with survivors and their family members. 
            
            As Florida and the Caribbean start to recover from Hurricane Maria, it is important to note that those living in economically disadvantaged communities will suffer the greatest from the storm’s damage—just as they did in 1928. Aid quickly was rushed to Florida, while the federal government is “killing [Puerto Rico] with inefficiency” (“I Am Mad As Hell”). Such a response demonstrates the ways in which United States’ racialized response to natural disasters is deeply rooted in its colonial enterprise. Failing to address these issues risks reinforcing harmful colonial narratives and causing irreparable harm to communities throughout the Caribbean and the world.
        
        
            
                
                    Bibliography
                    @realDonaldTrump. “Such poor leadership ability by the Mayor of San Juan, and others in Puerto Rico, who are not able to get their workers to help. They want everything to be done for them when it should be a community effort.” Twitter, 30 September 2017, 5:26 A.M. https://twitter.com/realDonaldTrump/status/914089003745468417
                    Hurston, Zora Neale. 
                        Their Eyes Were Watching God. 1937. HarperPerennial, 2006.
                    
                    “‘I Am Mad As Hell’: San Juan Mayor Carmen Yulin Cruz Criticizes Maria Response.” 
                        YouTube, uploaded by NBC Nightly News, 29 September 2017. https://www.youtube.com/watch?v=41hl5RwfOVc
                    
                    Ortega Marrero, Melisa. “El huracán María no superó a San Felipe II según un informe preliminar.” 
                        El Nuevo Dia [Guaynabo, Puerto Rico], 29 September 2017.
                    
                    Sharp, Deborah. “Storm's path remains scarred after 75 years.” 
                        USA Today, 4 September 2003. 
                    
                    Sterghos Brochu, Nicole. “Florida's Forgotten Storm: the Hurricane of 1928.” 
                        South Florida Sun-Sentinel, 2003.
                    
                    “Trump compares Puerto Rico to Katrina, ‘a real catastrophe.” 
                        YouTube, uploaded by USA Today, 3 October 2017. https://www.youtube.com/watch?v=J18rugiTxoU
                    
                
            
        
    

        
            
                Introduction
                Clarifying the genesis of a passed down text is of outmost importance for many scholarly disciplines within the humanities such as history, literary studies, and Bible studies. Often, historical text sources have been copied over and over for hundreds or even thousands of years, thus being subjected to paraphrasing and other kinds of modifications, repeatedly. Despite the significance of source criticism for the humanities as a whole, algorithmic support in this matter is still limited. While current approaches are able to tell 
                    if and 
                    how frequent a text has been modified—to the best of our knowledge—there has been no work on determining the 
                    degree of paraphrastic modification. To a human reader, the introduction of, say, spelling variations is indubitably a minor modification compared to substituting entire words. Yet, how can the different “degrees” of alterations, which are intuitively clear to scholars, be captured in an algorithmic way? 
                
                To this end, we present a first approach for designing a metric for paraphrastic modification in text (henceforth paraphrasticality). Based on an English Bible corpus (three literal Hebrew and Greek translations and three standard translations) we measure the frequency of different classes of textual variations between each pair of Bibles. We then use the probability of these variations in a machine learning experiment to derive weights for these classes of modifications. Ultimately, this allows us to define a metric for paraphrasticality which we validated with promising results.
            
            
                Related work
                Measuring the 
                    similarity or 
                    distance between two spans of text is relevant to many areas in and related to natural language processing (NLP). One of the earliest approaches is Levenshtein’s (Jurafsky and Martin, 2009) edit distance which is based on character-level removal, insertion, and replacement operations. BLEU (Papineni, 2002) is the most common evaluation metric in machine translation, capturing the difference between gold and automatic translations based on (word-level) n-gram overlap. In 
                    stylometry, different kinds of delta metrics are used to compute the difference between the writing style of authors or texts (Jannidis et al., 2015). These are typically based on the frequency distribution of the most frequent words. These first three approaches have in common that they rely on surface features (token and character-level) alone and do not incorporate semantic proximity. In contrast to that, computing the 
                    semantic similarity between two sentences is a popular task within NLP (Xu et al., 2015). However, approaches in this field are typically not indented for manual inspection and are thus less suited for applications in the humanities. Lastly, Moritz et al. (2016) quantify modification operations on a parallel Bible corpus yet do not present a way to aggregate these counts into a distance metric. In contrast to these related contribution, here, we aim to develop a metric which is both semantically informed as well as human interpretable.
                
            
            
                Data 
                We use a parallel corpus of the Old Testaments of six English Bible translations
                    
                         Note that our approach is not limited to applications on historical text and that our choice of textual material is based on technical reasons only. In fact, any paraphrastic, parallel corpus would work equally well for our proposed method.
                     from the 19
                    th century, half of them being literal translations that closely follow the primary source texts’ language and syntax while the other half are standard translations (see Table 1).
                
                
                    
                
                
                    Table 1: Bible editions used for investigation. Sources: bst: 
                    https://www.biblestudytools.com/; mys: 
                    https://www.mysword.info; ptp: Parallel Text Project (Mayer and Cysouw, 2014)
                
                
                    Literal translations: Robert Young, the translator of 
                    YLT, created a highly literal translation of the original Hebrew and Greek texts. Thus, Young tried to be as consistent as possible in representing Greek tenses with English ones, e.g., he used present tense where other translations used past tense (see Young, 
                    1898a; Young, 1898b) as in: ‘In the beginning of God's preparing the heavens and the earth —’ (Genesis 1:1). 
                    SLT: Upon publication, Julia Smith’s Bible translation was considered the only one directly translating the historical source texts to contemporary English. She aimed at complete literalness and tried to translate each original word with the same English word, consistently, and tended to translate the Hebrew imperfect to English future tense (Malone, 2010). 
                    LXXE by Sir Lancelot Charles Lee Brenton is an English translation from the Codex Vaticanus version of the Greek Old Testament, which itself is a translation of the Hebrew Old Testament (Roger, 1958).
                
                
                    Standard translations: WBT by Noah Webster is a revision of the King James Bible mainly eliminating archaic words and simplifying Grammar (Marlowe,
                    
                        2005
                    ). 
                    ERV is today’s only officially authorized revised version of the King James Bible in Britain (no author
                    , 1989). The most recent edition in our study is 
                    DBY, Darby’s translation of the Bible. The Old Testament was published by his students in 
                    1890 and is based on Darby’s German and French versions (Marlowe, 2017). 
                
            
            
                Methods
                
                    Preprocessing and alignment: We use MorphAdorner (Burns, 2013), a specialized toolkit for early modern and modern English, to tokenize and lemmatize the Bibles. After removing punctuation and verse identifiers, we pair up our six Bibles in every possible combination (15 in total). Since the different Bible versions are inherently aligned on the verse-level (by their verse identifier), our next step builds up a statistical alignment at the token level for each pair of bibles using the Berkeley Word Aligner (De Nero and Klein, 2007), a tool originally designed for machine translation. 
                
                
                    Counting modification operations: Building on these word-aligned pairs of Bibles, we can describe the divergence between a pair of verses in terms of the 
                    modification operations—such as replacing a word by its synonym—which would be necessary to convert one version into another. We automatically apply and count the modification classes introduced by Moritz et al. (2016) for each verse and Bible pair (see Table 2). Synonyms, hypernyms, hyponyms and co-hyponyms, are identified based on BabelNet (Navigli and Ponzetto, 2012).
                
                
                    
                
                
                    Table 2: Operations used as features together with normalized estimated weights (coefficients) of the fitted model
                
                
                    Weight identification: By counting modification operations, we gain a fine-grained description of the 
                    exact differences between two spans of text. However, to construct a metric, we had to find a way to condense these modification frequencies down to a single number. For that we exploit the fact that we deal with two classes of Bible translations, literal and standard ones. Thus, to estimate a human judgment of deviation, we assume that standard translations are more homogenous to each other than literal translations (since the latter demand for more creative language use; see Section 3). Hence, we can train a classifier to distinguish whether a pair of Bible verses is from the same class (both Bibles being standard or literal translations, respectively) or from different classes. For this task, we train a maximum entropy classifier
                    
                         Using the scikit-learn.org implementation. Training for this binary classification task was done using 10-fold cross-validation achieving an accuracy of .68.
                     where we use the relative frequencies of the modification operations as features. Now, the key part of our contribution is that we can exploit the coefficients of our fitted model as the first ever presented empirical estimate of the relative importance of these modification operations for paraphrasticality.
                
            
            
                Results 
                
                    Feature weights: Table 2 lists the final, normalized (summing up to 1) feature weights of our fitted model. Lemmatization, hyponym and synonym relations turn out to be especially important for the classification task.
                
                
                    Metric: Based on these coefficients, we define the paraphrasticality metric 
                    par between two word-aligned text spans 
                    a and 
                    b as 
                
                
                    
                
                where 𝑛 is the total number of features (or classes of operations), 𝜃
                    𝑖 is the absolute weight for feature 𝑖 determined via the classifcation experiment and 𝒳
                    𝑖
                    a,b is the relative frequency of the respective operation. In order to gain face validity for this newly defined metric, we compute the paraphrasticality score for each one of the 15 Bible pairs in our corpus (as average of their verse paraphrasticality). The results are presented in Table 3.
                
                
                    
                
                
                    Table 3: Deviation between each pair of Bibles in terms of our newly developed paraphrasticality metric; higher values indicate higher distance 
                
                
                    
                
                
                    Table 4: Top 3 most frequent operations (without fallback) per Bible pair
                
                
                    Qualitative validation: We can identify three regions in the plot. The upper left triangle shows that our standard translations do not differ much from each other (as expected), especially since WBT and ERV are revisions of the same Bible. The 3x3 rectangle in the upper right corner represents pairs of one literal and one standard translation, respectively. We can see that the distance between those is about 0.3 thus displaying increasing paraphrasticality compared to pairs of 
                    only standard translations. The highest deviation however is between the literal translations by Smith (SLT) and Young (YLT) compared to the English Septuagint (LXXE). This can be explained by the choice of vocabulary by each translator and by the purpose they follow in their translations. For example, SLT and LXXE use “firmament” when YLT uses “expanse”, SLT and YLT use “rule” when LXXE uses “regulating”. We thus conclude that our metric yields valid and—perhaps even more important for applications in the humanities—interpretable results.
                
                Our approach also enables to judge distance on a fine-grained level based on pure operation counts. In Table 4 we show the top 3 operations for each Bible pair. As we can see, most of the top 3 operations per Bible pair relate to semantic relations between the aligned word pairs (matching lemma, synonymy, or co-hyponomy) underscoring the advantage that our metric has as opposed to more surface feature-dependent approaches (to textual similarity) such as Levenshtein distance or delta measures.
            
            
                Conclusion
                We presented the first study on designing a metric for paraphrasticality. Different from existing approaches on measuring distance or similarity between texts, we describe paraphrasticality as frequency of specific modification operations for which we tried to find empirically adequate weights via a machine learning experiment. As demonstrated, our approach is specifically useful for applications in the humanities as operation frequencies, and feature weights, as well as paraphrasticality scores are open to manual inspection. A more comprehensive comparison against existing similarity metrics and a human judgment is left for future work.
            
        
        
            
                
                    Bibliography
                    
                        Burns, P. R. (2013). Morphadorner v2: A java library for the morphological adornment of English language texts. 
                        Northwestern University, Evanston, IL, no page numbers.
                    
                    
                        De Nero, J. and Klein, D. (2007). Tailoring word alignments to syntactic machine translation. 
                        Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Prague, pp. 17–24.
                    
                    
                        Jannidis, F., Pielström, S., Schöch, C. Vitt, T. (2015). Improving Burrows’ Delta—An empirical evaluation of text distance measures. 
                        Digital Humanities Conference 2015. Sydney, no page numbers.
                    
                    
                        Jurafsky, D. and Martin, J. H. (2009). 
                        Speech and language processing: An introduction to natural language processing, speech recognition, and computational linguistics. Englewood Cliffs: Prentice-Hall.
                    
                    
                        Malone, D. (2010). 
                        Julia Smith bible translation (1876), https://recollections.wheaton.edu/2010/12/julia-smith-bible-translation-1876/ 
                        
                            (accessed November
                            2017
                            ).
                        
                    
                    
                        Marlowe, M
                        
                            .
                             (2005). 
                        Webster’s Revision of the KJV (1833)
                        
                            , 
                        
                        http://www.bible-researcher.com/webster.html
                        
                            (accessed November 2017
                            ).
                        
                    
                    
                        Marlowe, M. (2017). John Nelson Darby’s Version, 
                        http://www.bible-researcher.com/darby.html
                        
                            (accessed November 2017
                            ).
                        
                    
                    
                        Mayer, T. and Cysouw, M. (2014). Creating a massively parallel bible corpus. 
                        Proceedings of the Ninth International Conference on Language Resources and Evaluation. Reykjavik, pp. 3158–61.
                    
                    
                        Moritz, M., Wiederhold, A., Pavlek, B., Bizzoni, Y. and Büchler, M. (2016). Non-literal text reuse in historical texts: An approach to identify reuse transformations and its application to bible reuse. 
                        Empirical Methods in Natural Language Processing. Austin, pp. 1849–59.
                    
                    
                        Navigli, R. and Ponzetto, S. P. (2012). Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. 
                        Artificial Intelligence, 193(2012): 217–50.
                    
                    
                        Papineni, K., Roukos, S., Ward, T. and Zhu, W. J. (2002). BLEU: a method for automatic evaluation of machine translation. 
                        Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Philadelphia, pp. 311–18.
                    
                    
                        
                            Roger, N. (1958, 1959). New Testament Use of the Old Testament. In 
                        Henry, C. F.H. (ed.), 
                        Revelation and the Bible. Contemporary Evangelical Thought. Grand Rapids: Baker, 1958. London: The Tyndale Press, 1959, pp. 137–51. 
                    
                    
                        Xu, W., Callison-Burch, C. and Dolan, B. (2015). SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter (PIT). 
                        SemEval@ NAACL-HLT. Denver
                        , pp. 1–11.
                    
                    
                        
                            Young, R.
                             (1898a). Young's Translation: Publisher's Note and Preface, 
                        
                        http://www.ccel.org/bible/ylt/ylt.htm
                        
                            (accessed November 2017).
                        
                    
                    
                        
                            Young, R.
                             (1898b). 
                        Young's Literal Translation, 
                        http://www.bible-researcher.com/young.html
                        
                            (accessed November 2017).
                        
                    
                    
                        
                            No Author. (
                        1989). The Holy Bible. 
                        Revised Version. London: 
                        Cambridge University Press. Synopsis.
                    
                
            
        
    

        
            Background
            The infrastructures that we use to navigate the world often become invisible as they become indispensable (Bowker and Star, 2000). However, critical examination of information systems is necessary to understand their implicit biases, and the ways that they invite some types of engagement and restrict others. Structures of power continue to be replicated in the ways that technologies are deployed in our lives (Noble, 2016; Tufekci, 2016), and the inability to access and assess the standards which make digital communication possible risks the uncritical perpetuation of those power structures (Drabinski, 2013). The moments of rupture, when an established system takes on a new facet with unintended consequences, can be an important moment of visibility, where we are able to reveal its ideological foundations, and the ways that its users adapt their own behaviors to it, or push back against its uncomfortable constraints (Raley, 2006; Marino, 2007). The introduction of emojis to the Unicode Standard, and their widespread adoption over the decade from 2006-2017 is one such moment of transition.
             Scholars of standards and standardization argue that the input of users is necessary for a standard to meet the needs of those users (Foray, 1994), and while the process of adding content to the Unicode Standard remains rigid, the unicode.org website provides an explicit record of the development and evolution of the face that Unicode presents to its users, and is able to be read as a text which reveals the contemporary state of Unicode and the cultural ideologies which shape it.
            Methodology
            While major language- and script-based additions are made with each update to the Unicode Standard, my analysis focuses on changes to the unicode.org website, and its role as an intermediary document between the Consortium, the Standard itself, and everyday users. The introduction of emojis in various updates to the Standard has resulted in changes to the content and structure of the unicode.org website that reflect an increased engagement with end users, which I argue is the result of increased semantic value of emoji characters for the user
                
                     A notable exception to this semantic shift is written Chinese, which is already a semantic-character-based language, as opposed to syllable- or alphabet-based, as are the rest of the world’s major languages. Thomas S. Mullaney gives a thorough historical analysis of the implication of this on text-encoding technologies in 
                        The Chinese Typewriter (MIT Press, 2017). 
                    
                , as compared to an individual character in a language's written script. It is my intention, through this analysis, to describe the types of changes that happen to the governing body and public documents of Unicode as major changes happen to the Standard itself.
            
            A timeline was created of the dates of major updates to the Unicode Standard since its introduction in 1991, using the official release dates for updates to the Unicode Standard as maintained by the Unicode Consortium. I cross-reference this document with the rollout of each new version by the major platforms
                
                     https://unicode.org/emoji/format.html#col-vendor lists the major “vendors” of emojis, or platforms with proprietary visual displays of emojis. These vendors are Apple, Google, Twitter, Facebook, Facebook Messenger, Windows, and Samsung.
                , with a particular emphasis on updates featuring new emoji characters, beginning with Unicode 6.0 in 2010
                
                     While the first major batch of emojis were incorporated into Unicode in 2010, and the first official “Emoji 1.0” release was in 2015, work has been done within the standard since late 2006 to consider the addition and management of emoji-like characters within Unicode— hence the specific 2006-2017 emphasis of this research. (https://www.unicode.org/reports/tr51/#Introduction)
                . 
            
            With this timeline in mind, I scraped the unicode.org domain using Python and the Beautiful Soup
                
                     https://www.crummy.com/software/BeautifulSoup/
                 library to collect the URLS of all the unique pages under the parent domain, as well as a table of links between those pages. This serves as a source-target list for the creation of a network visualization of the unicode.org domain, using the network visualization software Gephi.
                
                     http://gephi.io
                 This process is repeated using archived versions of the unicode.org site, available from the Internet Archive’s Wayback Machine
                
                     https://web.archive.org/
                , resulting in several structural snapshots of the unicode.org website over time, which can then be overlaid and compared to one another to note particular areas of change within the site.
            
            Additionally, using points of change within the site structure as a guide, I also collect and code page content data to reflect the type of changes made to those pages during each major update. This coding is done on two axes: The first labels each change as being content- or structure-based (eg. adding text or links to a page, respectively), and the second designates which aspect of the Standard and/or Consortium is being addressed by the change. Examples of this second type of labelling would be “Emoji,” “Membership,” “Meta-Documentation,” or “Language Scripts.” This coding is done in two phases— an initial survey of this data in order to formally create labelling categories, and then a closer examination of the updates to apply those labels.
            Discussion and next steps
            This research project addresses issues of digital infrastructure from a unique angle: one that considers the socially-constructed nature of technology, as well as the meta-narrative of maintenance and upkeep of a system that has become crucial to our ability to communicate in a digital world. Through analysis of the secondary documents relating to the Unicode Standard, it is possible to gain invaluable insights into the ways that knowledge is organized collectively and continuously, as well as the embedded values that shape who can access and influence that knowledge.
            
                This case study will provide a foundation for more expansive examination of systems of digital infrastructure. It is a beginning point both for further analysis of the adoption and adaptation of Unicode (and emojis in particular), but also as a framework for examining other forms of scaffolding which uphold the content of digital spaces. 
            
        
        
            
                
                    Bibliography
                    
                        Bowker, G. C., and Star
                        , S. L. (2000). 
                        Sorting things out: Classification and its consequences
                        . Cambridge: MIT Press.
                    
                    
                        Drabinski, E. (2013). Queering the catalog: queer theory and the politics of correction. 
                        The Library Quarterly 83(2): 94-111. doi:10.1086/669547
                    
                    
                        Foray, D
                        . (1994). Users, standards and the economics of coalitions and committees. 
                        Information Economics and Policy
                        , 
                        6
                        (3): 269-293.
                    
                    
                        Marino, M. C. (2007, December 4). Critical code studies. 
                        Electronic Book Review. Retrieved from http://electronicbookreview.com/thread/electropoetics/codology
                    
                    
                        Noble, S.U. (2016). A future for intersectional black feminist technology studies. 
                        The Scholar &amp; Feminist Online. 13.3 - 14.1. Retrieved from: http://sfonline.barnard.edu/traversing-technologies/safiya-umoja-noble-a-future-for-intersectional-black-feminist-technology-studies/0/ 
                    
                    
                        Raley, R. (2006). Code.surface || Code.depth, 
                        Dichtung Digital. Retreived from http://www.dichtung-digital.org/2006/01/Raley/index.htm
                    
                    
                        Tufekci, Z. (2016, June). 
                        Machine intelligence makes human morals more important. [Video file]. Retrieved from https://www.ted.com/talks/zeynep_tufekci_machine_intelligence_makes_human_morals_more_important
                    
                
            
        
    

        
            Introduction
            The spring of 2016 has become known as the “Primavera Violeta” (“Purple Spring”), a period that saw the emergence of new digital activist networks tackling gendered and sexual violence in Latin America. Of the hashtags generated by these movements, few gained the public recognition and “celebrity status” of #MiPrimerAcoso (“My First Harassment” or “My First Abuse”), a hashtag that asked users to publically share their first experiences of sexual violence. On April 23, 2016, women in Mexico and across Latin America shared their stories via their personal Twitter accounts in response to a request tweeted by journalist Catalina Ruiz Navarro of the pop-feminism collective (e)stereotipas: “¿Cuándo y cómo fue tu primer acoso? Hoy a partir de las 2pmMX usando el hashtag #MiPrimerAcoso. Todas tenemos una historia, ¡levanta la voz!”
                 (When and how did your first acoso  happen? Today from 2pm on, use the hashtag #MiPrimerAcoso. We all have a story, raise your voice!)
            
            
                
                Figure 1: A typical #MiPrimerAcoso tweet. In English: I was eleven years old and a man passed on a bicycle and grabbed my breast. A woman in the street blamed me for wearing that blouse. 
            
            After its initial launch, #MiPrimerAcoso spread rapidly throughout Mexico and quickly became a trending topic across Latin America. This analysis investigates the ways that Twitter users– activists, laypersons, public figures– use hashtags to talk about trauma, paying special attention to the ways that quantifiable modes of Twitter engagement point to more complex affective experiences. 
            Methods
            This project undertakes both qualitative and quantitative analyses of tweets posted using #MiPrimerAcoso in order to examine the key actors, contexts, and conditions that emerged from the hashtag’s narrative premise. For the initial assessments, this analysis uses the #MiPrimerAcoso corpus collected and published by media company Lo Que Sigue. To provide a point of comparison, this project also analyzes a collection of tweets posted using another Primavera Violeta hashtag– #VivasNosQueremos (“We Want to Live”)– whose corpus was collected and published by Lo Que Sigue at the same time as the #MiPrimerAcoso corpus. 
            
                Affective (and Effective) Tweeting
            
            Hashtag dialogues serve to construct and re-construct bridges between different streams of dialogue within movements, between movement collaborators and stakeholders, and between activists, political powers, and the general public. To illustrate some of the preliminary findings of this exploration, I evaluate the prevalence of retweets and multiple-hashtag use (or “co-tagging”) in the #MiPrimerAcoso corpus and another corpus published by #LoQueSigue of tweets posted using #VivasNosQueremos. Throughout this paper, I call upon Papacharissi’s (2015) work on the affective properties of Twitter dialogues to further illustrate the forms of personal and political affect that drove the trans-national trajectory of #MiPrimerAcoso. 
            
                
            
            Although #MiPrimerAcoso is entangled with other Twitter dialogues on gender violence, it “stands alone” more often than one of its closest peers, and is less frequently retweeted and co-tagged. Here, I find that these concrete metrics summarize diverse modes of engagement: retweeting another user’s personal story of violence is necessarily a different act than retweeting a popular news story about the hashtag. However, these metrics do demonstrate the ways in which use characteristics reflect the discursive mandate of a hashtag. Engagement with #MiPrimerAcoso might include reading, listening, creating original content, rebroadcasting, or responding to the content of other users within the affective public generated by the hashtag. This diverse set of practices allows Twitter users to “tune into an issue or a particular problem of the times but also to affectively attune with it, that is, to develop a sense for their own place within this particular structure of feeling” (Papacharissi 118). 
                The Twitter users who tweeted their experiences of violence undertook a delegated task of content creation in response to the prompts posted by Ruíz Navarro. This guiding of the discussion allowed Twitter users to act and to 
                feel using a pre-constructed response frame. By asking users to share the how and when of their first acoso, users tasked with personifying the political and 
                making it about themselves. By focusing on a tweet structure that outlines an individually expressive personal action frame through the medium of shared experience, #MiPrimerAcoso allows its users to make "small and fitful contributions" (Bennett and Segerberg 2011) to a cause while feeling a profound sense of identification with the movement. 
            
            If we want to understand what it is people want from digital activism, #MiPrimerAcoso offers captivating insights regarding our need to see ourselves within online political movements. The secret of #MiPrimerAcoso’s handling of collective and individual resonance lies in its personalization and generalizability: although the hashtag calls on a specific category of experience, it is sufficiently broad that many interpretations of 
                acoso fit the bill, and many users were able to affiliate with the hashtag without necessarily sharing a personal story of sexual violence. As Papacharissi (2015) notes, the use of hashtags as “open” signifiers allows various publics to affiliate with a movement and “fill in” the open hashtag with their own desired meanings. Women were able to link their own experiences of sexual violence to the individual narratives that had already been shared using the hashtag #MiPrimerAcoso. What, then, of those who did not contribute their original narratives to the library of primer acosos, but instead chose to respond or rebroadcast existing #MiPrimerAcoso content? In responding to a tweet, users may amplify, stifle, or otherwise alter the public life of the digital 
                acoso. Although Papacharissi and others have linked the act of retweeting to the expression of solidarity with a movement, this conclusion may prove reductive in the context of #MiPrimerAcoso. However, solidarity does not adequately summarize the act of rebroadcasting another person’s 
                acoso: it is an expansion of the tweet’s intangible audience of ethical witnesses to the tweeted acoso, a “re-telling” of scene of violence. Like any other hashtag, #MiPrimerAcoso needed to meet specific communicative and technical (in the case of Twitter) requirements in order to maximize its “reach” and extend beyond the core audience of (e)stereotipas. Referring to the act of retweeting, Papacharissi argues that refrains reinforce affect (Papacharissi 2015). By posting tweets tagged #MiPrimerAcoso, users spread the affective and contextual implications of the hashtag to their own Twitter audiences: those in digital “earshot” of their tweets. Similarly, the authors of original #MiPrimerAcoso tweets were also invited to act as amplifiers of the larger movement by adding their story to a collaborative, polyvocal narrative of lived violence. 
            
            Conclusions 
             In our study of digital movements, the use of the hashtag is the tip of the iceberg in comparison to the forms of knowledge, feeling, and understanding that emerge from these affective discourses. The results of this research have also suggested that conventional Twitter analysis methods may not adequately assess the affective clout of digital dialogues. For this reason, this analysis has strived to use the concrete metrics of the #MiPrimerAcoso data as guide to direct a “closer” reading of the narrative attributes of the tweets. When examining Twitter data, we must strive to expand the possibilities behind a simple, quantifiable act such as a retweet, and understand the hashtag as a point of contact between the user and digito-phenomenological processes of which we are largely unaware. Of course, there are key characteristics of the hashtag itself that are crucial to our understanding: its connectivity, for example, or its capacity to understand individual content as part of a larger dialogue. The hashtag is a departure point: an entity that gives rise to visible manifestations of trauma, digital acts of vulnerability and moments of personal catharsis, responses of support, condemnation, or indifference. 
            We should consider the tweet, then, as the execution of a series of digital actions, but also as the manifestation of a confluence of contacts between the ontological and phenomenological worlds of Twitter. To better assess these intangible qualities of Twitter data, we can listen to the testimonies of #MiPrimerAcoso authors, and pay attention to the strategies they employ to construct the 
                acoso in relation to their present selves, the ways in which they reflect on the act of tweeting the 
                acoso in front of an intangible digital audience. Here, I want to emphasize the diversity of experiences that users bring to the discursive space of Twitter, and the need to pay attention to the varied motivations that drive Twitter users to participate in social campaigns. These experiences do not easily reduce themselves to quantitative metrics, but we can search for their traces in the textual manifestations of our digital activity: the stories we tell, the words we use, the affective investments that we make as observers and participants.
            
        
        
            
                
                    Bibliography
                    Bennett, W. Lance, and Alexandra Segerberg. "The logic of connective action: Digital media and the personalization of contentious politics." Information, Communication &amp; Society 15.5 (2012): 739-768.
                    Gerbaudo, P. (2014) The persistence of collectivity in digital protest,
                    
                        Information, Communication &amp; Society, 17:2, 264-268, DOI: 10.1080/1369118X.2013.868504
                    
                    Lo Que Sigue TV (2016). Tuits de #MiPrimerAcoso disponible en “table_5d787653”. Database available on Carto. https://lqs.carto.com/tables/table_5d787653/public
                    Papacharissi, Z. (2015) 
                        Affective publics: Sentiment, technology, and politics. Oxford University Press.
                    
                
            
        
    

        
            The question of when “digital humanities” will drop the “digital” modifier and become “humanities” has special resonance for adjunct instructors. Digital humanities senior scholars might bridge the gap between tenured working conditions and adjunct working condition when crafting field infrastructures: not just because adjuncts merit both employment protections and what I call “microbenefactions” (more on that below), but because adjuncts are the invisible mass of humanities faculty buttressing every kind of institution, from community college to elite research-1 university. Adjuncts shoulder the humanities enterprise, teaching the general education classes that free researchers to pursue critical questions that advance the field. 
            This talk examines the infrastructural causes of DH adjunct invisibility and proposes two remedies: to motivate DH adjunct self-identification by convening DH adjunct-specific prizes and bursaries; and to invite senior DH faculty to perform “microbenefactions” that cost little effort and give adjuncts access to prize-worthy work opportunities or other benefits, such as renewable funding. 
            When “digital” humanities becomes just humanities, what’s to stop “adjunctification” from converting DH tenure lines into part-time or other tenure-ineligible work, as has happened pervasively in other sub-specialties? In 2012, Stephen Ramsay problematized DH as “the hot thing.” It's a skepticism shared by many in the field, including panelists of the DH 2017 Conference panel “Challenges for New Infrastructures and Paradigms in DH Curricular Program Development,” which openly wondered whether graduate students were well served by DH certificate programs.
                
                    
                         See the DH 2017 panel abstract here: 
                            https://dh2017.adho.org/abstracts/176/176.pdf. Ryan Cordell pointedly observes in published version of his DH 2017 talk that “completing the hours required for our robust [DH graduate] certificate program requires students to decide their path almost immediately upon admission, and the decision to pursue the certificate dictates very particular routes through the larger Ph.D. program.” See Cordell’s “Abundance and Usurpation While Building a DH Curriculum” posted to his blog: 
                            http://ryancordell.org/research/abundance/
                        
                    
                 Miriam Posner notes that DH’s “sexiness” today obscures the “widespread understaffing” of many DH initiatives.”
                
                    
                         Miriam Posner, “Money and Time,” http://miriamposner.com/blog/money-and-time/
                    
                 This is an analog to adjunctification, the “shortsighted” boom/bust cycles of “soft” money quickly depleted which then require maintenance with a precarious budget. Amy Earhart has documented the unsustainability of early DH passion projects, websites whose hand-built archives rusticate when the faculty author retires or moves institutions.
                
                    
                         Earhart, 
                            Traces of the Old, Uses of the New. 
                        
                    
                 Startups are sexy, but maintenance is not. When today’s senior DH faculty retire in ten or twenty years, what infrastructures of care will be in place to stop those vacated tenure lines from being converted to part-time positions? The gender politics of “sexy,” “hot” DH cast a pall over the field when one factors in that the majority of adjuncts are women. “As a woman of color,” Liana M. Silva wonders, “I am especially interested to know what the women in contingent ranks look like. According to the Education Department’s 2009 report, 51.6 percent of contingent faculty are women. The same report says 81.9 percent of contingent faculty are white. To what extent is contingent labor a problem for white women? Or, from another angle, to what extent is this a white labor issue, where class is meant to trump race?”
                
                    
                         Liana M. Silva: 
                            https://chroniclevitae.com/news/1017-how-many-women-are-adjuncts-out-there; National Center for Education Statistics 2009 report to which Silva refers: 
                            https://nces.ed.gov/pubs2011/2011150.pdf See also: “Women as Contingent Faculty: The Glass Wall,” published by the American Association of University Professors 
                            http://archive.aacu.org/ocww/volume37_3/feature.cfm?section=1 and New Faculty Majority’s “Women and Contingency” project: 
                            http://www.newfacultymajority.info/women-and-contingency-project/
                        
                    
                 These questions about race, gender and contingent labor are digital humanities questions.
            
            Awarding DH Adjuncts
            In its mentoring, promotion, and awards structures, the humanities professoriate is legacy-bound, oriented to a tenure system that pertains to only one quarter of people working in the field.
                
                    
                         “Adjunctification” is well documented by adjunct advocacy organization like New Faculty Majority and Adjunct Nation; professional groups such the AAUP and the Modern Language Association (2014); intra-university studies such as George Mason’s, which surveyed 240 GMU adjuncts and “has been hailed as the most comprehensive study of a university’s contingent faculty working conditions to date” (2014); trade journals like 
                            Inside Higher Education and 
                            The Chronicle of Higher Education; and the popular press. I am struck by 
                            The Atlantic Monthly’s occasional series (2013-present) that features titles like “There’s No Excuse for How Universities Treat Adjuncts” and “The Cost of an Adjunct.” See also Kathi Inman Berens and Laura E. Sanders, “DH and Adjuncts: Putting the Human Back in the Humanities.”
                        
                    
                 If, as James F. English contends in 
                The Economy of Prestige, the key indicator of any contemporary cultural phenomenon entering the mainstream is the creation of a prize (2), then perhaps it is time for digital humanists to create criteria of DH excellence specific to DH adjunct working conditions because adjuncting is the instructional mainstream. Doing so would motivate adjunct DHers to identify their work as DH and contribute recognizably toward DH research and pedagogy field development. Lack of access to an adjunct-specific DH prize reinforces adjunct invisibility, making it highly unlikely that even very good research will attain the recognition necessary to vault the scholar out of adjuncting. Most of the seven DH adjuncts I interviewed don’t necessarily identify themselves as “digital humanists” because they are not hired specifically to teach DH, though their methods are consistent with DH pedagogy practices.
                
                    
                         A note about method. My university’s Human Subjects Research Review Committee determined an IRB was not required for me to conduct informational interviews with adjuncts. I used a common set of questions with each adjunct. The conversations veered to the specifics of their own particular cases. 
                    
                 “Imposter syndrome” is intensified by employment insecurity and DH definitional heterogeneity.
                
                    
                         The authors of the “Alternate Histories of DH” panel note in their abstract: “Matthew Kirschenbaum’s identification of the digital humanities in 2014 as a ‘discursive construction’ that ignores the ‘actually existing projects’ of the field set the stage for scholars to rethink how the digital humanities conceptualizes its work and its history (‘What Is’ 48). More recently, in the introduction to 
                            Debates in the Digital Humanities 2016, Matthew Gold and Lauren Klein use the scholarship of Rosalind Krauss who, in 1979, described art history as emerging as ‘only one term on the periphery of a field in which there are other, differently structured possibilities.’”
                        
                    
                
            
            How to give adjuncts access to prize-worthy work opportunities? Senior scholars are key. In my talk, I will discuss microbenefactions senior scholars gave me when I adjuncted (2011-2014). Those invitations gave me access to nationally-visible projects and let me train myself in techniques that are now a core part of my tenure track job. 
            “Microbenefactions” is a term I invented to signify the opposite of microaggressions. They are small actions that shift the balance of power, the order of operations, that give adjuncts access to prestige or information otherwise inaccessible to them. Note that I use the singular here: “an” adjunct. These acts of inclusion are do-able as a one-off or in the course of a given term, not the Herculean efforts of adjunct advocates such as New Faculty Majority President Maria Maisto, Adjunct Nation, and the PrecariCorps collective who publish PrecariTales, 300-500 word anonymously authored adjunct stories.
                
                    
                        
                            https://precaricorps.org/about/true-stories/ The pinned story at time of writing details an adjunct who’s taught at the same university for ten years and has been hired to revise materials for a large enrollment course. One chair made sure she got paid the first lump sum; the replacement chair didn’t with the second, and she’s still waiting with “no recourse except to wait.” The Twitter hashtags #AdjunctLife and #RealAcademicBios also gather adjunct (but don’t curate) stories.
                        
                    
                
            
            Unlike state-mandated employment protections, microbenefactions are individual and hyperlocal. They layer adjuncting’s transactional dyad with the more branching, collegial conceptualization of value typical of tenure-track employment. This is human-centered DH infrastructure. We acknowledge that humans are not widgets, and that DH teaching is not a dissemination of knowledge. The medium is the message. If the medium is adjuncting, then the message our students imbibe is that learning is transactional. Microbenefactions disrupt neoliberal infrastructure that shrinks learning and collegiality to transactions.
            What is a microbenefaction? It’s action by a tenured or tenure-track scholar who 
            
                writes funding for adjunct salary into grant proposals
                advises and mentors adjuncts
                seeks input from adjuncts about student-centered pedagogy
                aids adjuncts in finding university resources or paid extra work
                invites adjuncts to meetings
                co-authors with adjuncts
                doesn’t eliminate adjunct applications when deciding awards and honors
                authorizes support for adjunct professional development, such as conference travel
                pays to license adjunct-authored course materials after the adjunct leaves the institution
                writes letters of recommendation for adjuncts
            
            Microbenefactions enact DH’s ethical ambit, which the Global Outlook::Digital Humanities special interest group articulates as a recognition “that excellent work is being done around the world,”
                
                    
                         Global Outlook::Digital Humanities is a special interest group of the Alliance of Digital Humanities Organization. See http://www.globaloutlookdh.org/
                    
                 even in elite first-world institutions that rely on adjunct labor but largely eliminate that labor from tenure and promotion consideration. 
            
        
        
            
                
                    Bibliography
                    Berens, Kathi Inman. “Judy Malloy’s Seat at the (Database) Table: A Feminist Reception History of Early Electronic Literature Hypertext.” 
                        
                            Literary and Linguistic Computing
                        
                        , Volume 29, Issue 3, 1 September 2014, pages 340–348, 
                        
                            https://doi.org/10.1093/llc/fqu037
                        .
                    
                    ______. “Want to Save the Humanities? Pay Adjuncts to Learn Digital Tools” in 
                        Disrupting the Humanities: Digital Edition, 05 January 2015, http://www.disruptingdh.com/want-to-save-the-humanities-pay-adjuncts-to-learn-digital-tools/. Accessed 27 November 2017.
                    
                    _____. “Sharing Precarity: Adjuncts, Global Digital Humanities, and Care,” in 
                        Debates in Digital Humanities 2017, eds. Lauren F. Klein and Matthew K. Gold. Minneapolis: University of Minnesota Press. In press.
                    
                    Berens, Kathi Inman and Laura E. Sanders. “Putting the Human Back in the Humanities: Adjuncts and Digital Humanities” in 
                        Disrupting Digital Humanities: Print Edition, eds. Dorothy Kim and Jesse Stommel. New York: Punctum Press. 2017.
                    
                    Bessette, Lee Skallerup. 
                        Adjunct Run. 
                        https://adjunctrun.readywriting.org/ Accessed 27 November 2017. 
                    
                    Bretz, Andrew. “The New Itinerancy: Digital Pedagogy and the Adjunct Instructor in the Modern Academy.” 
                        Digital Humanities Quarterly Vol. 11, No. 3 (2017). http://www.digitalhumanities.org/dhq/vol/11/3/000304/000304.html
                    
                    Clement, Tanya [panel chair], Alison Booth, Ryan Cordell, Miriam Posner, Maria Sachiko Cecire. “Challenges for New Infrastructures and Paradigms in DH Curricular Program Development,” panel at the 2017 Digital Humanities Conference in Montréal, Québec, Canada August 8-11, 2017. 
                        https://dh2017.adho.org/abstracts/176/176.pdf.
                    
                    Cordell, Ryan. “Abundance and Usurpation While Building a DH Curriculum.” 
                        http://ryancordell.org/research/abundance/ 23 August 2017.
                    
                    Davis, Rebecca Frost, Matthew K. Gold, Katherine D. Harris and Jentery Sayers, eds. 
                        Digital Pedagogy in the Humanities, Digital Edition (peer editing version) 
                        https://digitalpedagogy.mla.hcommons.org/. Accessed 27 November 2017.
                    
                    English, James F. 
                        The Economy of Prestige: Prizes, Awards, and the Circulation of Cultural Value. Cambridge: Harvard University Press, 2008.
                    
                    Finley, Ashley. “Women as Contingent Faculty: The Glass Wall.” 
                        On Campus With Women featured article of the Association of American Colleges and Universities. Vol. 37, No.3 (Winter 2009). 
                        http://archive.aacu.org/ocww/volume37_3/feature.cfm?section=1 Accessed 27 November 2017.
                    
                    Gonzales, Andrea and Sophie Houser. 
                        Tampon Run. 
                        http://tamponrun.com/ Accessed 27 November 2017.
                    
                    Higgen, Parker. Tweet dated 6 January 2015. 
                        https://twitter.com/xor/status/552456370629672960. Accessed 27 November 2017.
                    
                    Honn, Joshua. “Never Neutral: Critical Approaches to Digital Tools Culture in the Humanities.” 
                        https://figshare.com/articles/Never_Neutral_Critical_Approaches_to_Digital_Tools_Culture_in_the_Humanities/1101385 Accessed 21 November 2017.
                    
                    Jacobs, Ken, Ian Perry, and Jenifer MacGillvary. “The High Public Cost of Low Wages: Poverty-Level Wages Cost U.S. Taxpayers $152.8 Billion Each Year in Public Support for Working Families.” UC Berkeley Center for Labor Research and Education. April 13, 2015 Report. 
                        http://laborcenter.berkeley.edu/the-high-public-cost-of-low-wages/
                    
                    Jasnik, Scott. “Humanities Job Woes.” 
                        Insider Higher Ed.  January 4, 2016.
                    
                    
                        https://www.insidehighered.com/news/2016/01/04/job-market-tight-many-humanities-fields-healthy-economics Accessed 27 November 2017.
                    
                    Knapp, Laura G., Janice E. Kelly-Reid and Scott A. Ginder. “Employees in Postsecondary Institutions, Fall 2009, and Salaries of Full-Time Instructional Staff, 2009-10,” a Report published by the U.S. Department of Education. November 2010. https://nces.ed.gov/pubs2011/2011150.pdf Accessed 27 November 2017.
                    Koseff
                        , Alexei. “Part-time community college instructors to get job protections” [
                        sic]. 
                        Sacramento Bee. 30 September 2016. 
                        http://www.sacbee.com/news/politics-government/capitol-alert/article105301086.html Accessed 27 November 2017.
                    
                    Losh, Elizabeth, ed. 
                        MOOCs and Their Afterlives: Experiments in Scale and Access in Higher Education.  Chicago: University of Chicago, 2017.
                    
                    Manyika, James, Susan Lund, Jacques Bughin, Kelsey Robinson, Jan Mischke, and Deepa Mahajan. “Independent Work: Choice, necessity, and the Gig Economy.” 
                        McKinsey Global Institute. October 2016. 
                        https://www.mckinsey.com/global-themes/employment-and-growth/independent-work-choice-necessity-and-the-gig-economy. 
                    
                    McGrail, Anne. “Whole Game: Digital Humanities at Community Colleges.” 
                        Debates in Digital Humanities 2016, eds. Lauren F. Klein and Matthew K. Gold. Minneapolis: University of Minnesota Press, 2016. 
                        http://dhdebates.gc.cuny.edu/debates/text/53
                    
                    McPherson, Tara. “Theory/Practice: Lessons Learned from Feminist Film Studies,” on the panel “Alternate Histories of the Digital Humanities: a Short Paper Panel Proposal,” convened by Roger Whitson and featuring Whitson, Amy Earhart, Steven Jones and Padmini Ray Murray, at the Digital Humanities 2017 conference July 8-11, 2017 in Montréal, Québec, Canada. 
                        https://dh2017.adho.org/abstracts/115/115.pdf Accessed 27 November 2018.
                    
                    Molloy College DH Adjunct Job Advertisement. 
                        https://main.hercjobs.org/jobs/10389448/new-media-and-digital-humanities-adjunct. Accessed 18 November 2017. [The link will expire; see screenshot in Appendix.] 
                    
                    Nazer, Daniel and Elliot Harmon. Electronic Frontier Foundation, “Stupid Patent of the Month: Elsevier Patents Online Peer Review.” August 31, 2016. 
                        https://www.eff.org/deeplinks/2016/08/stupid-patent-month-elsevier-patents-online-peer-review
                    
                    New Faculty Majority “Women and Contingency Project.” 
                        http://www.newfacultymajority.info/women-and-contingency-project/ Accessed 27 November 2017.
                    
                    Pierazzo, Elena. “The Disciplinary Impact of the Digital: DH and 'The Others'.” Keynote at the Digital Humanities Summer Institute 2017, Victoria, B.C., Canada 16 June 2017. Abstract viewable here: 
                        http://dhsi.org/schedule.php
                    
                    Precaricorps “True Stories.” https://precaricorps.org/about/true-stories/
                         Accessed 27 November 2017.
                    
                    Ramsay, Stephen. “The Hot Thing.” (2012) 
                        https://github.com/sramsay/sramsay.github.com/blob/master/_posts/2012-04-09-hot-thing.markdown. Accessed 27 November 2017.
                    
                    Risam, Roopika and Susan Edwards. “Micro DH: Digital Humanities at Small Scale.” Conference talk at Digital Humanities 2017 Conference in Montréal, Québec, Canada August 8-11, 2017. Abstract viewable here: https://dh2017.adho.org/abstracts/196/196.pdf
                    Silva, Liana. “How Many Women Are Adjuncts Out There?” 
                        Chronicle of Higher Education. 27 May 2015. 
                        https://chroniclevitae.com/news/1017-how-many-women-are-adjuncts-out-there Accessed 27 November 2017.
                    
                    Stanley, Sara Catherine. “Why DH?” (2017) 
                        http://scatherinestanley.us/2017/06/why-is-dh Accessed 21 November 2017.
                    
                    Varner, Stuart. “Digital Humanities or Just Humanities?” 
                        https://stewartvarner.com/2013/11/digital-humanities-or-just-humanities/ Accessed 21 November 2017.
                    
                    Warford, Erin. “StoryTelling with Digital Maps,” a workshop at the 2017 Summer Digital Humanities Workshop Series at Canisius College. https://blogs.canisius.edu/digital-humanities/gis2017/ Accessed 27 November 2017.
                
            
        
    

        
            In the wake of Michael Brown’s murder in Ferguson, Missouri, on August 9, 2014, and the non-indictment of police officer Darren Wilson on November 25, 2014, backlashing protests and riots took to the streets of Ferguson and to other major American cities across the country. They also took to the Twittersphere. A national conversation about police brutality and the American criminal justice system exploded on Twitter during this time period, eventually elevating the hashtag #Ferguson, tweeted over 27 million times, to the most frequent in Twitter’s ten-year history, and the hashtag #BlackLivesMatter, tweeted over 12 million times, to third place (Sichynsky, 2016). First coined by Alicia Garza, Patrisse Cullors, and Opal Tometi in July 2013, the hashtag #BlackLivesMatter became a banner for a national protest movement and an index for conversations about the systematic devaluing and elimination of black life. Over the last five years, literary scholars and historians have noted that, within this massive social media movement, the novelist, essayist, and civil rights literary icon James Baldwin seemed to be often and increasingly invoked (Maxwell, 2016). The perceived frequency of Baldwin-related tweets has been pointed to by many as evidence of the Harlem-born author’s 21
                st-century resurrection and recent political resonance (Glaude Jr., 2016; Robinson, 2017). Because tweets can be digitally archived and made computationally tractable, they can be collected, measured, and analyzed at scale, and they can offer a picture of Baldwin’s social media reception that goes beyond perception and anecdotal evidence. This talk will share work-in-progress from my project 
                Tweets of a Native Son (
                http://www.tweetsofanativeson.com/), which brings large-scale social media data and computational methods to bear on Baldwin’s 21
                st-century remediation, recirculation, and reimagination. This talk will discuss the methods and progress made in the project thus far, argue that social media analysis might usefully contribute to a growing body of computationally-assisted scholarship focused on readership, reception, and textual circulation, and finally gesture to how such an approach might change our understanding of how texts are shared between communities of people, namely through its emphasis on networks.
            
            Methods, Analysis, Initial Findings
            First I “hydrated,” that is, retrieved the full JSON information for, an archive of over 32 million tweets that were sent between June 1, 2014 and May 31, 2015 and that mentioned Ferguson, Black Lives Matter, and 20 other black individuals who were killed by the police during this time period, which was first purchased from Twitter and shared by Deen Freelon, Charlton McIlwain, and Meredith D. Clark (Freelon, McIlwain, Clark, 2016). I next searched for all the tweets that mentioned “James Baldwin” by his first and last names using the Python and command-line tool “twarc” and the command-line JSON processor “jq,” which returned 7,326 tweets and retweets. By using twarc utilities, a k-means clustering algorithm, and manual tagging, I then identified the most retweeted tweets in the archive and the text that appeared most often across all tweets in the archive, which revealed that the most frequent appeal to Baldwin during this time period was through quotation and overwhelmingly through the quotation of Baldwin’s 1960s-era essays, radio interviews, and television appearances. 
            By studying the text of the most retweeted and most frequently cited tweets, and by tracing tweeted Baldwin quotations back to their literary and historical origins, my project argues that Baldwin’s appeal as a #BlackLivesMatter muse comes, at least in part, from the remediation of much of his non-fictional work into YouTube videos and free online essays; from his aphorisms with deep roots in African American written and oral traditions; and from his sympathetic proximity to but never full embrace of black radicalism. Another goal of 
                Tweets of a Native Son, however, is to let others explore, hypothesize, and learn about Baldwin’s #BlackLivesMatter-related social media reception through a series of interactive data visualizations on the project’s website. These interactive visualizations are meant to provide a perspective on Baldwin’s living legacy, a refracted vision of Baldwin’s life and career through those who actively called upon him in a moment of political and emotional urgency, a means by which others can come to their own conclusions about Baldwin’s resurrection.
            
            DH Reception Studies and Networked Reading
            
                Tweets of a Native Son most broadly hopes to join and affirm recent digital humanities work that is trained on readership, reception, and textual circulation, such as Lincoln Mullen’s 
                American’s Public Bible and Ryan Cordell and David Smith’s 
                Viral Texts, and to amplify Katherine Bode’s call that the digital humanities better attend to and account for the ways in which literary texts “circulated and generated meaning together at particular times and places” (Mullen, 2016; Cordell and Smith, 2017; Bode, 2017). Like the 19
                th-century newspaper archives used by Mullen, Cordell, and Smith, social media archives offer a window into how texts travel, how texts are used and changed by individuals, and what these texts mean in context. Social media archives additionally offer massive amounts of (relatively) clean, recent data. Though of course with these advantages, they also present more ethical challenges, since this data is often tied to corporations and produced by still-living human beings whose consent, possible harm, and creative attribution must always be considered.
            
            Finally, however, I believe that social media data might help us better theorize and make visible the networked structures of readership, reception, and textual circulation, because social media data, such as Twitter data, is often inherently networked in structure, recording retweets, replies, follower communities, hashtag communities, and more. This networked structure emphasizes the way that texts are not only engaged with by individuals but are shared between individuals, taking on social and communal meanings. For the particular case of Baldwin and #BlackLivesMatter in 2014-2015, the quotations of Baldwin’s words were often recirculated as coalition- and community-building material, helping to forge connections between individuals across space, time, and American history. During the future stages of this project, I hope to employ network science and network visualization to better understand Baldwin’s significance within #BlackLivesMatter. 
        
        
            
                
                    Bibliography
                    
                        Bode, K. (2017). The Equivalence of “Close” and “Distant” Reading; or, Toward a New Object for Data-Rich Literary History. 
                        Modern Language Quarterly, 78(1): 94. 
                    
                    
                        Cordell, R. and Smith, D. (2017). Viral Texts: Mapping Networks of Reprinting in 19th-Century Newspapers and Magazines, 
                        http://viraltexts.org.
                    
                    
                        Freelon, D., McIlwain, C. D., and Clark, M. D. (2016). Beyond the hashtags: #Ferguson, #Blacklivesmatter, and the online struggle for offline justice. Center for Media and Social Impact. 
                    
                    
                        Glaude Jr., E. S. (2016). James Baldwin and the Trap of Our History. 
                        Time. 
                    
                    
                        Maxwell, W. J. (2016). Born-Again, Seen-Again James Baldwin: Post-Postracial Criticism and the Literary History of Black Lives Matter. 
                        American Literary History, 28(4).
                    
                    
                        Mullen, L. (2016). America’s Public Bible: Biblical Quotations in U.S. Newspapers, 
                        http://americaspublicbible.org.
                    
                    
                        Robinson, Z. (2017). Ventriloquizing Black Feeling, Re-Voicing Black Life: Speaking Baldwin on the Internet. 
                        Communities in Conversation: Digital Baldwin, Rhodes College.
                    
                    
                        Sichynsky, T. (2016). These 10 Twitter Hashtags Changed the Way We Talk about Social Issues. 
                        The Washington Post. 
                    
                
            
        
    

        
            
                Introducción
                
                    Debido a la generalización del uso de cámaras fotográficas en teléfonos móviles y a la posibilidad de su inmediata publicación en redes sociales, la fotografía compartida ha devenido una parte fundamental de la comunicación on-line. Sin embargo, ha sido menos estudiada que los objetos textuales publicados en algunas plataformas sociales, por ejemplo en Twitter. (Highfield y Leaver, 2016). Recientemente la investigación académica comienza a analizar el contenido visual generado por los usuarios. Estas temáticas y nuevos modos de producción de información son crecientemente abordados en análisis científicos y críticos con metodologías innovadoras, como la analítica cultural (Manovich, 2009) los métodos digitales (Rogers, 2009) y la visualización de información (Niederer y Taudin Chabot, 2015).
                
                Nos proponemos investigar empíricamente en el modo en que las ciudades iberoamericanas son representadas en Instagram. A tal fin hemos recolectado un conjunto de fotografías etiquetadas como #buenosaires, #cdmx (México) y #madrid, publicadas en la mencionada plataforma durante la primera semana de octubre de 2016. El estudio profundiza en las formas en que las tres ciudades son representadas desde el punto de vista de los usuarios de la plataforma, describe las especificidades de cada una, e identifica el uso social de las etiquetas o hashtags de gran porte, donde se publican miles de fotos diariamente. 
                Empleamos una aproximación metodológica distante (Moretti, 2007, 2015) que considera tanto la dimensión digital de esas interacciones como una interpretación crítica que pueda identificar el papel que los objetos digitales juegan en la producción de la cultura contemporánea. Emplea una metodología de investigación mixta que combina el análisis cuantitativo, el empleo de software de procesamiento de datos textuales y numéricos y la interpretación de resultados desde una perspectiva sociocultural.
            
            
                La aproximación distante como método digital
                Interrogamos el corpus a través de un conjunto de técnicas que denominamos genéricamente aproximación distante. En el campo de los estudios literarios cuantitativos Moretti (2007, 2015) distingue dos tipos de lecturas: la distante y la cercana. La primera es de tipo exploratoria, opera con la masa, la generalidad y los hechos comunes. El crítico identifica en esa masa patrones de regularidad y frecuencia, grandes agrupamientos o clústers, ciclos temporales, y estructuras reticulares (Manovich, 2009). Por otro lado, la aproximación cercana usa técnicas procedentes de diferentes corrientes interpretativas a fines de atribuir un sentido a la producción analizada, y establecer relaciones con la cultura en la que estas manifestaciones tienen lugar. Las teorías interpretativas otorgan a las producciones simbólicas y de registro un lugar fundamental para la comprensión de las culturas. 
                La aproximación distante propone interrogar los datos y metadatos desde diferentes técnicas basadas en software. Aplicamos el análisis de contenido (Rose, 2016) y la analítica visual (Thomas y Cook, 2005 Manovich, 2011b) al corpus fotográfico con el fin de identificar temáticas recurrentes y patrones estéticos. Empleamos la analítica textual (Moreno y Redondo, 2016) para identificar las palabras frecuentes en las descripciones o Captions. El análisis de redes (Venturini, Jacomy y Carvalho, 2015) nos fue útil para establecer conexiones y clúster
                    s o agrupamientos entre etiquetas co-ocurrentes. Finalmente estudiamos las reacciones en relación al consumo activo de las fotografías una vez publicadas en la plataforma, también denominado 
                    engagement (Turner, 2014 y Rogers, 2016). 
                
            
            
                Hallazgos empíricos y discusión metodológica
                En el corpus estudiado se evidencia la recurrencia de elementos temáticos, estéticos y textuales. Las palabras frecuentes evidencian una práctica homogénea, cuyo significado se fija en pocas redes semánticas asociadas a la fotografía, el viaje, la arquitectura, el consumo. Los patrones textuales demuestran diferentes maneras de representar y experimentar las ciudades. En 
                     #madrid las fotografías de personas en el ámbito urbano cobran mayor importancia que en otras etiquetas, en consecuencia merecen ser estudiadas en profundidad. El uso publicitario y la promoción del consumo también son recurrentes. En #buenosaires lo son la experiencia de práctica fotográfica y los estilos de vida, así como en #cdmx el patrón dominante es el de la estilización del entorno urbano. Las recurrencias pueden interpretarse en varias direcciones: en relación a las características propias de los objetos representados, debido a la homogeneidad de imaginarios sociales, o también como emergencia de nuevos géneros narrativos asociados a la fotografía compartida. La co-ocurrencia de etiquetas esboza redes de intereses, temáticas y comunidades acordes a la definición de la fotografía compartida como elemento de exhibición y mensaje comunicativo de intercambio efímero
                    . El análisis de de reacciones evidencia las diferencias culturales y comunicativas entre el acto de fotografiar y el de mirar una fotografía. En #cdmx se destaca la alta cantidad de fotografías de amaneceres y atardeceres, pero es la temática urbana y arquitectónica la que recibe mayores reacciones.
                
                Las tres ciudades se distinguen por el uso publicitario de la imagen generado por los propios usuarios. Los que reciben mayores reacciones por otro lado también hacen un uso económico del hashtag pues su fin es el de la autopromoción. Además desarrollan estrategias para lograr la visibilidad de sus fotos publicando en múltiples hashtags y deslocalizando los territorios representados a partir del etiquetado. Al menos una parte de ellos concibe su práctica como parte de una comunidad, evidenciada por la aparición recurrentes de hashtags asociados a la publicación en Instagram: “instagrammers”, “mextagram” y otras similares. Podemos suponer entonces que Instagram instaura una suerte de “economía de la visibilidad”, donde las reacciones son la moneda con la que se paga la creatividad vernácula.
                La alta homogeneidad y recurrencia sugieren la emergencia de codificaciones semióticas propias de la plataforma y demuestra la existencia de una gramática de acción (Agre, 1994), donde las acciones aisladas adquieren sentido cuando se las analiza colectivamente. Estos elementos pueden indicar el surgimiento de la fotografía compartida no sólo como práctica cultural sino como género discursivo con sus propias temáticas, estéticas y prácticas. Las producciones recurrentes de los usuarios pueden considerarse codificaciones que se siguen para ser parte de diversas comunidades de práctica materializadas en el uso de hashtags. 
                A partir del estudio realizado podemos observar que la aproximación distante resulta efectiva para abrir la caja negra de los medios sociales y mapear los principales temas y patrones estéticos de la fotografía compartida, aunque este abordaje debe en un futuro someterse a mayor investigación empírica y profundización epistemológica sobre varios de sus componentes. Entre los puntos que requieren mayor investigación se encuentran las técnicas de investigación de datos, la comprensión del modo en que funciona el software que se emplea en el procesamiento de los datos y metadatos, y la determinación de la importancia del volumen de datos que se producen en las redes para la investigación social.
                El estudio de las mediaciones en Latinoamérica siempre ha relacionado la práctica cultural con las estructuras sociales, identificando en los consumos culturales o bien prácticas de subordinación, o bien de resistencia (Martín Barbero, 2001). En este trabajo la fotografía compartida sobre ciudades emerge como una práctica donde la búsqueda de visibilidad y el uso de autopromoción resultan evidentes. Será entonces necesario plantear su función social en el contexto de una economía global de intercambios simbólicos, línea que deberá ser profundizada tanto teórica como empíricamente.
            
        
        
            
                
                    Bibliografía
                    
                        Highfield, T. y Leaver, T. (2016
                        ). Instagrammatics and digital methods: Studying visual social media, from selfies and GIFs to memes and emoji. 
                        Communication Research and Practice. 2: 47-62.
                    
                    
                        Manovich, L. (2009). Cultural Analytics: Visualizing Patterns in the era of more media. Recuperado el 14 de mayo de 2017, a partir de http://www.manovich.net
                    
                    
                        Manovich, L. (2011b). What is visualization? 
                        Visual Studies, 26(1): 36–49.
                    
                    
                        Martín Barbero, Jesús. (2001).
                        De los medios a las mediaciones: comunicación, cultura y hegemonía. Naucalpan, Mexico: G. Gili.
                    
                    
                        Moreno, A., Redondo, T. (2016). Text Analytics: the convergence of Big Data and Artificial Intelligence. 
                        International Journal of Interactive Multimedia and Artificial Intelligence, 3 (Special Issue on Big Data and AI): 57–64. 
                    
                    
                        Moretti, F. (2007).
                        La literatura vista desde lejos. Barcelona: Marbot.
                    
                    
                        Moretti, F. (2015).
                        Distant reading. London: Verso.
                    
                    
                        Niederer, S., y Taudin Chabot, R. (2015). Deconstructing the cloud: Responses to Big Data phenomena from social sciences, humanities and the arts. 
                        Big Data and Society, 2(2). 
                    
                    
                        Rogers, R. (2009). The End of Virtual. Digital Methods. Recuperado a partir de http://www.govcom.org/rogers_oratie.pdf
                    
                    
                        Rose, G. (2016).
                        Visual methodologies: an introduction to researching with visual materials. London: Sage
                    
                    
                        Thomas K., y Cook, K., eds. (2005). Illuminating the Path: Research and Development Agenda for Visual Analytics. IEEE-Press, recuperado de 
                        http://vis.pnnl.gov/pdf/RD_Agenda_VisualAnalytics.pdf
                    
                    
                        Turner, P. (2014). The figure and ground of engagement. 
                        AI and Society, 29(1): 33–43. 
                    
                    
                        Venturini, T., Jacomy, M., y Carvalho P. D. (2015). Visual Network Analysis. Recuperado a partir de http://www.tommasoventurini.it/wp/wp-content/uploads/2014/08/Venturini-Jacomy_Visual-Network-Analysis_WorkingPaper.pdf
                    
                
            
        
    


    
        
            
                Visualizing Data for Digital humanities Producing Semantic Maps with Information extracted from Corpora and other Media
                
                    
                        Poibeau
                        Thierry
                    
                    LATTICE-CNRS, France
                    thierry.poibeau@ens.fr
                
                
                    
                        Terras
                        Melissa
                    
                    DH, UCL, UK
                    melissaterras@gmail.com
                
                
                    
                        Ruiz
                        Pablo
                    
                    LATTICE-CNRS, France
                    pablo.ruiz.fabo@ens.fr
                
                
                    
                        Gray
                        Steven
                    
                    DH, UCL, UK
                    steven.gray@ucl.ac.uk
                
                
                    
                        Roe
                        Glenn
                    
                    ANU College of Arts and Social Sciences
                    glenn.roe@anu.edu.au
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Pre-Conference Workshop and Tutorial (Round 2)
                
                
                    Text mining; Visualization
                
                
                    data modeling and architecture including hypothesis-driven modeling
                    multilingual / multicultural approaches
                    natural language processing
                    ontologies
                    digitisation
                    resource creation
                    and discovery
                    scholarly editing
                    semantic analysis
                    text analysis
                    linguistics
                    visualisation
                    social media
                    semantic web
                    linking and annotation
                    data mining / text mining
                    English
                
            
        
    
    
        
            
                Brief Description of the Content and Its Relevance to the Digital Humanities Community 
            
            The goal of this proposal is to explore efficient methods to extract information from texts and produce meaningful and readable representations. The three-hour session (half a day) will include formal presentations, demonstrations, and discussion with the audience. It will thus be halfway between a workshop and a tutorial (see the structure of the session in the next section). 
            It is well known that we are now facing an information deluge, and experts in different domains, especially social sciences and literary studies, have long acknowledged that available texts—and more generally the mass of data available through different media—now constitute one of the important sources of knowledge. However, computers are unable to directly access information encoded through texts: this information must first be extracted, normalized, and structured in order to be usable. Moreover, meaningful representations are needed in order for people to understand it. This process is not trivial, and more and more groups have to face this dilemma: information is here, available on the Web or in more remote databases, but its manipulation is hard since it requires a complex process that most of the time is out of the hands of social scientists and literary studies experts. See, for example, this quotation (from the Médialab in Paris) that is typical of the current situation: 
            Qualitative researchers [. . .] arrive at the médialab bringing rich data and longing to explore them. Their problem is that qualitative data cannot be easily fed into network analysis tools. Quantitative data can have many different forms (from a video recording to the very memory of the researcher), but they are often stored in a textual format (i.e. interview transcriptions, field notes or archive documents . . .). The question therefore becomes: how can texts be explored quali-quantitatively? Or, more pragmatically, how can texts be turned into networks? (Venturini et Guido, 2012)
            The goal of this workshop is to practically address the question. It will include three presentations detailing some challenges and solutions. A large part of the workshop will be devoted to the presentation of practical tools and to discussion with the audience.
            
                Structure of the Workshop 
            
            The workshop will be chaired by Thierry Poibeau, Melissa Terras, and Isabelle Tellier. It will consist of talks by Pablo Ruiz, Steven Gray, and Glenn Roe. 
            Pablo Ruiz will address the concept of entity linking. In natural language processing, entity linking is the task of determining the identity of entities mentioned in text. It includes named entity recognition (NER) and the identification of their reference (via links to DBPedia or other structured databases). The bases of entity linking will be presented along with the demonstration of a platform combining different entity linking systems so as to obtain an efficient and robust coverage across text genres and application domains. Lastly, the production of maps from the result of the entity linking analysis will be presented. 
            Steven Gray will address the collection and visualisation of real-time online datasets. By utilising the Big Data Toolkit (a toolkit that specialises in mining social media data), the session will focus on the collection of data from various APIs (Application Programming Interfaces) and the mapping of the data by using Google Maps’ online mapping APIs. The session will conclude by focussing on visualising this real-time textual data for the Web, building an interactive discovery tool for geospatial social media data. 
            Glenn Roe will address the identification and visualisation of text reuse in unstructured corpora. Identifying text reuse is a specific case of the more general problem of sequence alignment—that is, the task of identifying regions of similarity shared by two strings or sequences, often thought of as the longest common substring problem. This technique is widely applied in the field of bioinformatics, where it is used to identify repeated genetic sequences. This talk will outline several different approaches to sequence alignment techniques for humanities research, as well as two recent projects aimed at visualising the output of alignment comparisons between texts and the alignment process itself using visual analytics. 
            The structure of the workshop will be as follows: each talk will consist of 20 minutes for presentation, 15 minutes for demonstration, and 15 minutes for discussion. A final slot (30 minutes) will be devoted to a general discussion on the topic with the audience. 
            
                Workshop Leaders 
            
            Thierry Poibeau 
            Laboratoire LATTICE 
            Ecole Normale Supérieure &amp; CNRS 
            1, rue Maurice Arnoux 92120 Montrouge FRANCE 
            Mail: thierry.poibeau@ens.fr 
            Web: http://www.lattice.cnrs.fr/Thierry-Poibeau 
            
                Thierry Poibeau is a director of research at CNRS and the head of the CNRS Lattice laboratory at Ecole Normale Supérieure. He is also an Affiliated Lecturer at the Department of Theoretical and Applied Linguistics (DTAL) of the University of Cambridge. His main interests are natural language processing (NLP), and its application to digital humanities. He is a recognized expert in information extraction, question answering, semantic zoning, knowledge acquisition from text, and named entity tagging. He is the author of three international patents and has published two books and more than 80 papers in journal and conferences. 
            
            Melissa Terras
            Director, UCL Centre for Digital Humanities 
            Vice Dean of Research (Projects), UCL Faculty of Arts and Humanities 
            Professor of Digital Humanities 
            Department of Information Studies 
            Foster Court 
            University College London 
            Gower Street 
            WC1E 6BT 
            
                Melissa Terras is director of the UCL Centre for Digital Humanities, professor of digital humanities in UCL’s Department of Information Studies, and vice dean of research (Projects) in UCL’s Faculty of Arts and Humanities. With a background in classical art history, English literature, and computing science, her doctorate (engineering, University of Oxford) examined how to use advanced information engineering technologies to interpret and read Roman texts. Publications include 
                Image to Interpretation: Intelligent Systems to Aid Historians in the Reading of the Vindolanda Texts (2006, Oxford University Press) and 
                Digital Images for the Information Professional (2008, Ashgate), and she has co-edited various volumes, such as 
                Digital Humanities in Practice (Facet, 2012) and 
                Defining Digital Humanities: A Reader (Ashgate, 2013). She is currently serving on the Board of Curators of the University of Oxford Libraries, and the Board of the National Library of Scotland. Her research focuses on the use of computational techniques to enable research in the arts and humanities that would otherwise be impossible. You can generally find her on twitter @melissaterras. 
            
            Isabelle Tellier 
            Laboratoire LATTICE 
            Ecole Normale Supérieure &amp; CNRS 
            1, rue Maurice Arnoux 92120 Montrouge FRANCE 
            Mail: isabelle.tellier@univ-paris3.fr 
            Web: http://www.lattice.cnrs.fr/sites/itellier/ 
            
                Isabelle Tellier is a professor of natural language processing and digital humanities at Université Paris 3 Sorbonne Nouvelle. She has published more than 50 papers in international venues in computational linguistics. Her main research topic is developing advanced machine learning techniques adapted to Natural Language Processing issues. She is also an expert in the application of these techniques to domain-specific corpora, especially in the digital humanities field. 
            
            
                Workshop Main Speakers 
            
            
                Pablo Ruiz is a research associate in digital humanities at LATTICE, a research laboratory of Ecole Normale Supérieure in Paris. He has designed and developed several applications in the domain of natural language processing, including, for example, a prototype for the lexical normalization of Spanish microblog (tweets), and a sequence alignment engine used in automatic subtitling. He currently works on text analytics technologies for digital humanities, especially entity recognition and linking, so as to extract structured and meaningful pieces of information from unstructured texts. 
            
            
                Steven Gray is a teaching fellow in Big Data Analytics and the UCL Centre for Advanced Spatial Analysis. With over 10 years’ professional software engineering under his belt, and a background in computing science and human computer interaction, he has built multiple award-winning systems, and his work has been featured in various worldwide media outlets (CNN, BBC, etc.). In recent years he has specialised in building mobile applications and systems that open up the world of data visualisation, mining, and analysis to the masses. His current research focuses on distributed high-performance computing, machine learning, and analysing large datasets in real-time while visualising the results. He currently serves as a Google Developer Expert for Google Maps where he evangelises best practises using cloud computing and online mapping platforms. You can regularly find him either on Twitter, @frogo, or on Google Plus as +StevenGray. 
            
            
                Glenn Roe is lecturer in digital humanities at the Australian National University. In 2011–2013 he held a Mellon Post-Doctoral Fellowship in digital humanities at the University of Oxford. Prior to that, he spent eight years as a senior project manager for the University of Chicago’s ARTFL Project (American and French Research on the Treasury of the French Language), one of the older and better-known North American research and development centres for computational text analysis. Glenn’s research agenda is primarily located at the intersection of new computational approaches with traditional literary and historical research questions. Glenn has presented and published widely on a variety of scholarly subjects, from French literary and intellectual history, to the design and use of new digital methodologies for literary research, and the implications of large-scale digital collections on humanities scholarship. 
            
            
                Target Audience 
            
            The target audience is large. Practitioners in social sciences are of course the key target, but experts in literary studies, information technology, and library management are concerned as well. In brief, the workshop will be of interest for anybody concerned with the extraction of information from large corpora and producing relevant maps to visualize key information. 
            
                Special Requirements for Technical Support 
            
            None, except a conference room projector. 
        
    

Digital humanists broadly agree that tool directories are a good and valuable thing, worth building and maintaining, but there is no sustainability model. This forum aims to move beyond platitudes and interrogate the value of directories and possible models for sustaining them. For whom are tool directories valuable, and in what context? Tool directories require ongoing attention in order to remain relevant -- and more technically sophisticated directories face infrastructure maintenance costs as long as the directory remains online and functional. Some directories have adopted a crowdsourcing model to address content updates, translation, and other necessary functions once grant funding runs out. As unpaid labor increasingly becomes an area of attention and concern for digital humanities, one is left to ask, how ethical are directories, particularly when this volunteer labor is at particular risk of being lost through fragile infrastructure? At a certain point, time and funding are a zero-sum game: are directories actually worth it?This forum is organized by individuals representing a range of DH directories, spanning from 2002 (Geoffrey Rockwell’s TAPoR) to 2020 (Frank Fischer and Laure Barbot representing the Marketplace developed within the SSHOC project), along with the defunct DiRT (Lisa Spiro and Quinn Dombrowski). We will share in advance a brief white paper with case studies and provocative questions, and elicit discussion via Twitter and a mailing list. For the forum itself, we will briefly introduce the context for the discussion to accommodate participants who have not read the white paper (15 minutes). We will then organize the forum attendees into breakout discussion groups, each focused on 1–2 agreed-upon questions (15 minutes). These breakout discussions will last 30 minutes, and each group will have one participant report back to the larger group, which will lead into a general discussion (20 minutes). We will close the forum with a short synthesis and ask participants to suggest next steps (10 minutes).The forum will help the organizers grapple with the difficult decisions that fall out from the “directory paradox”, where the DH community’s praise of directories is wildly incommensurate with the interest or resources available for sustaining them. Similarly, we hope it will help others running directories at various scales (including in forms such as Libguides or lists on GitHub pages) return to their projects with a clearer sense of what they’re doing, for whom, why, and for how long. Questions: How should we review tools? Validity of tools for designers’ stated purposeUsability in classroom Applicability to certain research questions Who are directories useful for? And to do what? Are directories the most cost-efficient way to do these things? Who should be responsible for creating and maintaining directories? What would a directory look like that you personally would be willing and able to contribute to? Does that even exist? How can they be sustained when the grant runs out? How can students use them? Do we need standardized ontologies? How can we involve volunteers? What are the different models for tool directories? Why shouldn’t we just let Google do it?
The advent of social media is changing the dynamics of political communication, engagements and election campaigns across the world. These platforms extend opportunities for political actors especially political parties, governmenta and citizens to engage directly to build mutually beneficial relationships through engagements ans interactions. Using interviews and content analysis, this article explores “Twittering” in political communication and engagements; how political parties use social media especially during their 2018 intra-party elections in Ghana. It examines how the New Patriotic Party and National Democratic Congress in Ghana use Twitter in intra-party elections. The article gathers data from political parties and their Twitter pages to theorize the political appropriation of social media in Ghana.The findings show that the political parties minimally appropriate Twitter; as they occasionally used it for public information purposes than interactive engagements and mutually beneficial relationships via two-way symmetrical communication. There were insignificant amount of tweet made by the political parties during their intra-party elections in 2018, which indicate that political use of social media in Ghana is still infantile. Therefore, political parties need to improve their use of Twitter and other social media platforms to take maxim advantage of the interactive features and relationship building potentials to garner support from the public and votes in elections. In this regard, political parties require enlightenEd communication strategies and professional political public relations tact for improved use to mature in social media communication.
The Spanish republican exile was the result of the Republican defeat in 1939 by the Francoist army, led by the general Francisco Franco. Nearly half a million-people had to go into mass exile during the months of January and February, through the French border crossings. Many other exiles did so, months later, from Alicante to the North African coasts. These places of destination were, in most cases, places of passage to successive destination countries in Europe and, especially, in Latin America. The international nature of this historical event means that there is currently a large number of personal files scattered in different places around the world. In order to recover these stories, the e-xiliad@s project was conceived in 2009, with a Digital Humanities and Public History perspective: www.exiliadosrepublicanos.info. It is an crowdsourcing project that, through a multilanguage digital platform, retrieves unpublished documents about the anonymous exiled. From the research point of view, the privileged target audience is composed by relatives and friends of the exiles and those interested in the subject. This initiative funded twice (2009 and 2011) by the General Directorate of Migrations of the Spanish Ministry of Employment and Social Security, uses a methodology created ad hoc to obtaining data based on public participation from citizen science. That is, the content is generated on-line by the public at an international level and coordinated by a scientific specialist. For almost a decade, this crowdsourcing project has been developing an online public engagement strategy for public participation based on open data, supported by a custom digital platform and its digital social networks, with more than 1.500 followers. At this stage, the project recovered around five hundred unpublished archives among photographs, memories, official documents, letters and interviews, that comes associated with about two hundred completed exile records. The vast majority of these data are public, thanks to the informed consent of the author.Figure 1. Examples of documents and photos provided by relatives of exiles registered on the project platform and collaborators.At technological level, e-xiliad@s has been built using Drupal 6 LTS (Long Term Support) with a MySQL database. The technological solution answer the need of the internal survey form that has been created using a specific data model connected with a strategic communication plan, that serves to get data online via user’ confidence with the project and to stimulate his/her family memoir.Figure 2. Detail of the internal form to be filledAs lesson learned, the need to maintain a technology that is getting older without official support (Drupal); the management of a project social network community continuously growing; and the spamming problem due to the project popularity within the republican exile community (more than 11.000 spamming users) are questions that we are trying to solve. Despite this, E-xiliad@s acts as a digital identity place for those connected to this topic and as a space that informs society, with scientific rigor, from the field of Digital Public History. Currently there are six large online social network communities that move much of the information on the subject of Republican exile and the Spanish Civil War, among which is the e-xiliad@s project with its social networks (Facebook: @exiliados.republicanos and Twitter: @exiliadas. They play a significant role at the national and international level regarding the recovery of historical memory, due to the quantity and quality of the information it offers. Graph analysis that represents the first and second level connections of the social networks of the E-xiliad@s project, which is represented in the community in green.
On March 8 2019, Spanish-speaking Twitter communities erupted with a polyvocal outcry against gender violence, while simultaneous offline protests took place in cities across the globe. This paper presents the results of a multi-modal spatial and thematic analysis of regional appropriations of the hashtag #8M (8 March) to illustrate the carrefours/intersections of trans-Hispanic networks of feminist solidarity online. Through spatial, network, and word frequency analyses of #8M, we examine the hashtag’s intersections with existing transnational and regionally-specific feminist Twitter dialogues, including the #NiUnaMenos movement launched to combat femicide in Argentina and the hashtag #Cuéntalo used to protest the lenient sentencing of a group of men who raped a woman in Pamplona, Spain. By combining quantitative and geospatial analyses using Twitter Archiving Google Sheets, Voyant-Tools, and Carto with targeted close readings of tweets containing #8M, this paper traces the regional variations in a large, multinational online Spanish-language conversation about gender violence.
Project Twitter Literature (TwitLit), seeks to address a growing gap in the literary-historical record by establishing a consistent, rigorous, and ethical method for scraping and cleaning up Twitter data for the use of humanities scholars. In particular, my project explores the growing community of amateur writers who are using Twitter as a means of publication and dissemination for their literary output. There are three parts to my project: the research findings related to the global literary community on Twitter, the tools and resources developed as part of the project and made openly available to other scholars, and partnership with a university library to ensure the long-term preservation of the collected data.The data that I have collected shows that social media is altering literary practices by providing a space for amateur writers to publish, disseminate, and receive feedback from a global community of writers. Preliminary figures put the number of active Anglophone writers using Twitter as a publication platform for their literary output at over 1 million users per year since 2015, and writers working in non-English languages on Twitter raise these numbers even higher. This practice is changing how literature is produced, published, and shared. Readerships too are changing, for rather than being tied to print subscriptions or access to physical books, audiences of social media literature are based on online communities and tied to the costs of physical devices and internet access.My presentation will showcase these research findings in order to highlight the importance and necessity of social media archival work. In so doing, I will discuss how I collected the data using a Python script (co-developed by myself and several other scholars), challenges of cleaning up and visualizing this data (using ArcGIS and tools developed by Documenting the Now), and ethical best-practices for using social media data in research. Information relating to this process – including detailed instructions, the Python scripts used to collect Twitter data, and a list of resources – are free and openly accessible on the project’s website (www.twit-lit.com) and GitHub repository (https://github.com/TwitLit/TwitLitSource). Other scholars are invited to use these scripts and other resources to collect their own social media data.Additionally, my project has attempted to plan for the long-term preservation of the over eight million tweets that I have collected. This preservation has been made difficult by Twitter’s strict Developer Policy and Agreement, which prevents individuals from keeping or disseminating large data sets for more than 30 days. The only exception to this policy is made on behalf of academic institutions, which may store Twitter data for unlimited amounts of time on behalf of academic research. The Project TwitLit project thus presents best-practices for establishing a working relationship with university libraries for storing and disseminating Twitter data in a way that is both in accord with Twitter’s legal restrictions and responsive to the needs of scholars. In short, Project TwitLit provides a case-study of a growing community on Twitter while simultaneously developing a set of tools and guidelines for other scholars seeking to engage in similar work. In December 2017, the Library of Congress, which began archiving Twitter in 2010, announced that it would no longer collect all Tweets; instead, Tweets produced after December 2017 would only be collected on a selective basis. There are no other ongoing, systematic efforts to collect and preserve this digital material. See Library of Congress, “Update on the Twitter Archive at the Library of Congress” (December 2017). For more information related to the challenges of collecting and storing Twitter data, please see Christian Howard, “Studying and Preserving the Global Networks of Twitter Literature,” in Post-45.
This paper presents the prototype of a platform aimed at curating simple open bricks for historical knowledge, shaped as short texts. These historical bricks, named “Pulses,” are meant to be based on no prefixed ontology, but collectively negotiated conventions and reformatting by bots. The Pulses curation platform that can be interpreted as hybridisation between some characteristics of Twitter and Wikipedia, organises a collective process of “smooth semantization” in which pulses are progressively rewritten to become more machine readable. The article reports on the principle and implementation of this system conducted in the last two years.
This study takes an intersectional and community-informed approach to characterizing and mining the Twitter accounts of thousands of users that self-identify openly in their Twitter bios as members of the Bisexual+ community. Twitter data was chosen due to the documented hesitations many Bi+ community members have about reporting their identities when completing official health documents. Accounts were assessed by multiple coders for inclusion in the dataset, in order to best represent the great diversity of the community while removing fake, automated and inactive accounts. The presenters share both the nuanced classification guidelines used for this project, and the process of developing guidelines that are responsive to community rhetoric and the intricacies of mining social media -- real, messy "data." The presenters also share on topic modelling methods and challenges, the nuances of natural language processing with tweets and the results found in the health topics Bi+ users share about online.
The International Organization for Migration (IOM) recently reported that the world has more migrants than ever before “both numerically and proportionally” and that the number of environmental migrants alone could reach 1 billion by 2050 (2017, p. 2; 2014). The IOM has also argued that understanding how people think and feel about migration is essential to the development of policy that supports the safe passage and successful integration of migrants into their new communities (2017). The United States receives more migrants per year than any other country (United Nations, 2016). It is also a country where migration is a prominent topic in social, political and media discourse. This poster reports the findings of an exploratory study whose principal research questions are: 1) What do Twitter users in the United States talk about in their migrant- and migration-related tweets, 2) how do authors feel when they tweet about these subjects, and 3) how do their sentiments and topics of interest compare to Twitter users residing outside the United States? To approach these questions, we apply sentiment analysis, topic modeling and time series analysis to 111,785 English language tweets containing the term “migrant” or “migration” collected from the Twitter API between January 21 and March 4, 2019. Our quantitative and computational methodology relied on descriptive statistics, the Python programming language, and associated libraries, such as the Natural Language Toolkit (NLTK) for text processing, Pandas for data manipulation, VADER for sentiment analysis, Gensim for topic modeling and Seaborn for data visualization. The visualized data shows that authors in the United States focused more on politics whereas authors in other countries focused more on humanitarian issues. Tweets from U.S. authors were more negative than those authored by residents of other countries, except when the topics of the tweets involved children. A time series plot revealed three sentiment spikes, one positive and two negative, over the course of the collection period. The negative spikes were partially explained by looking at word frequencies, visualized as word clouds, alongside news headlines for the dates in question. The positive spike was more difficult to explain because of the limitations of sentiment analysis and the negative nature of news reporting. Our findings cannot be generalized because the Twitter Search API does not generate a representative sample (González-Bailón, 2014). Future work could involve an improved sampling method that would permit the use of inferential statistical methods. The addition of multiple languages to the tweet dataset or social network data to the analysis may yield interesting new findings.
#DecolonizingDigital is a project that runs out of the Indigenous Life Promotion & Social Action Network Labs at the Ontario Institute for Studies in Education. The project investigates how Indigenous people are using Twitter for cultural revitalization, well-being, and community connection. This panel offers three papers with findings for stage one of the project. The first paper discusses how Twitter-based Indigenous language revitalization movements enable Indigenous language learners to live their languages in culturally grounded and place-based ways, and present Indigenous languages not as dying but, rather, as vibrant, active, and living. The second paper discusses Twitter as an Indigenous community shaped by beaders who are connecting and sustaining culture, love, identity, healing, and Indigenous trade systems through beading in Twitter’s ecosystem. The third paper discusses how We Matter, an Indigenous youth-led organization, is challenging colonial theorizations of mental health and re-articulating Indigenous health through the lived experiences and knowledges of Indigenous youth.
Sociocultural trends from social media platforms such as Twitter or Instagram have become an important part of knowledge discovery. The `trend' construct is however ambiguous and its estimation from unstructured sociocultural data complicated by several methodological issues. This paper presents an approach to trend estimation that combines (`intersects') domain knowledge of social media with advances in information theory and dynamical systems. In particular, we show how *trend reservoirs* (i.e., signals that display trend potential) can be identified by their relationship between novel and resonant behavior, and their minimal persistence.This approach contrasts with trend estimation that relies on linear or polynomial techniques to study point-like novelty behavior in social media, and it completes approaches that rely on smooth functions of time.
Since Twitter's creation in 2006, the social media platform has become a high-profile conduit for backchannel conversations and the public dissemination of information during academic conferences and events. While some tweets simply vanish into the internet void, others provoke lively conversation within the conference community or are retweeted to signal boost them into adjacent or similar communities.  The networks formed by these conference tweets—a 21st-century Republic of Letters written 140 or 280 characters at a time—can be put in conversation with one another to reveal interdisciplinary patterns of associations among digital humanists.
Oupoco (L’ouvroir de poésie combinatoire) is a project taking inspiration from Raymond Queneau's book Cent mille mille milliards de poèmes, published in 1961. Queneau’s book is a collection of ten sonnets which verses can be freely recombined to form new poems. The book can be seen as composed of ten sheets, each separated into fourteen horizontal bands, each band carrying a verse on its front. The reader can choose, for each verse, one of the ten versions proposed by Queneau. The ten versions of each verse have the same scansion and rhyme, which ensures that each sonnet thus assembled is regular in shape [Queneau, 1961].It would be tempting to develop a computer-based version of Queneau’s work, but Queneau’s book is still under copyright, and it is by definition limited to its ten original sonnets. To overcome this problem, we developed the Oupoco project, aiming at proposing a sonnet generator based on the recombination of a large collection of 19th century French sonnets. The challenge is thus more complex than the one proposed originally by Queneau since our sonnets do not have the same scansion and rhyme. From this point of view, even if the project is intended to generate new sonnets, it is largely based on the development of analysis tools able to identify the scansion, the rhyme and the structure of the original sonnets. It is thus very different from the numerous projects dedicated to the pure generation of poetry, being with symbolic [Gervás, P., 2013] or neural methods [Ghazvininejad et al., 2017] [Van De Cruys, 2019] (among many others).Oupoco is currently based on a collection of 788 sonnets from 16 authors from the 19th century, and this database is regularly expanding. Each sonnet is encoded in a XML format along with related metadata, and a TEI version of the database is available. The project requires to get access to a formal representation of rhymes [Beaudouin, 2002]. In order to do this, the first step is to get a phonetic transcription of the last word of each verse, but this is not enough: for example, aimé and aimée have the same phonetic transcription, but do not rhyme, according to French rhyming rules (feminine and masculine do not rhyme according to the classical rules of French poetry); there are also cases where the phonetic transcription diverges but words actually rhyme (for example with sounds like [e] and [e]). A series of rules had thus to be defined to get a proper analysis of rhyme from the phonetic transcription of the last word of each verse. The generator uses this analysis to produce random sonnets, with different possible structures, respecting the rules of French versification (the code and the resources used, especially the sonnet database, are open source and freely available for research, see: https://github.com/clement-plancq/oupoco-api).A series of “side products” have been produced from the project, including:a website (https://oupoco.org/):Figure 1: Web page introducing the Oupoco websiteFigure 2: A generated poem, along with the constraint panel (on the left), allowing the user to select the authors and the structure of the poem he wants to generate. Placing the mouse on a given verse gives access to the exact reference of that verse.a bot posting on Twitter (https://twitter.com/oupoco_bot): Figure 3: The Oupoco bot on Twitter, generating a quatrain every 6 hours. and a “poetry box” (la boîte à poésie), a portable version of the original idea that can be demonstrated in public events (based on Raspberry Pi components). Through these devices our goal is to reach a wider audience and engage people to reconnect with poetry.Figure 4: « La Boîte à poésie », a portable version of the project, conceived Atelier Raffard-Roussel (Paris). This portable device allows one to demonstrate the system in various venues; the electric power required to generate a poem can be produced manually, thanks to the crank on the side.The main interest of the project is to present French poetry through a new and original setting. With our system, poetry is not any more just a literary genre [Derrida and Ronnel, 1980], but a dynamic object that can be manipulated and experienced. For lots of people, poetry is seen at best as something related to school years, at worse as something boring and uninteresting from the past. Our new setting, in itself, makes it possible to show that playing with poetry can be fun. Our setting puts in perspective the notion of text coherence [Reinhart, 1980], since the result of the generator can be more or less satisfactory from a semantic point of view.This has two consequences. The first one is related to interpretation: because the machine produces structurally impeccable sonnets, the experiencer is unconsciously encouraged to find coherence in them, simply because we are used to coherence in our everyday life and because incoherence is bewildering. The second consequence is a frequent need for the experiencer to go back to the original poem, to see where from a given verse originates (tooltips always allows the experiencer to go back to the original sonnet where a verse has been extracted). The project is thus not just a sacrilege game over venerated texts, but a way to make people experience and rediscover poetry.
In August of 2014 a disgruntled ex-boyfriend of a female game designer posted a vicious diatribe on the net accusing her and the gaming media of corruption. This launched a culture war in the gaming community that played out on Twitter and elsewhere under the rubric #Gamergate. This paper will present key results from the study of the over 11 million tweets gathered.

        
            Linguistic elements are known to be powerful signals for social categories such as class, race, education, political affiliation, and gender (Lakoff 1973; Lanehart 2015; Zappavigna 2012). Significant research has been conducted within the field of digital humanities to explore the ways in which language function to form communities across large corpora (Gavin 2018; Hoyt 2018; Orlikowski 2018). The vast majority of work on linguistic signalling in the digital humanities has focused on the analysis of print culture due to the availability of large textual datasets and readably available methods. Spoken language, however, is known to vary considerably within communities, even when they share a common written language and dialect (Cutler, 1997). Phonetic features such as tone, rhythm, and phoneme variation all serve to signal social identity. Methods for collecting and studying such variation offer, therefore, important insights into linguistic signaling that fail to be recognized by the study of text-only corpora. 
            In this poster, we present a general pipeline for the construction, alignment, and analysis of spoken linguistic data. Our pipeline uses a combination of open-source tools in the R programming language and will be made available as an open-source toolkit through GitHub. The goal in our alignment workflow is to produce a single table collectively representing each of the elements collected in the multiple annotations. As the smallest unit of analysis, we chosen to align the corpus at the phoneme level. Other larger linguistic units—such as syllables and words—and metadata are simply duplicated across the relative phonemes (see Figure 1). Unique identifiers for each unit are also included (these are not show in the figure only due to space), allowing for reconstruction of the original annotations. Once the data was collected as a single table, we were able to compute new lemmatised word forms, part of speech tags, dependency relationships, and named entities. 
            As a way of illustrating how this linguistic data pipeline is able to produce new scholarship, the poster focuses on an application to a corpus of spoken British English curated by the French-led Aix-MARSEC project (Auran. Bouzon, Hirst, 2004). The dataset provides features for analysing vowels, pitch, rhythm, phonosyntax, for prediction of phrase breaks for text-to-speech systems. It has even been used as a baseline for psycholinguistic experiments. In our analysis, we suggest that we can contribute to a finer-grained analysis of cultural and situational factors on the prosodic hierarchy by taking into account the original annotations of the corpus and adding new layers. We synthesize the earlier stages of the corpus, from the Spoken English Corpus to the Aix-MARSEC speech database.
            Our poster lays out two experiments: the analysis of major and minor boundaries acknowledged in the corpus on the basis of a multidimensional analysis of the different subgenres of the corpus and of its prosodic and syntactic annotation when analysing the final nuclear contours of the prosodic units. Results of the distribution of the main intonation values (Major versus Minor) across the final tonal segment types in final positions according to discourse genres are shown in Figure 2. As, Brierly and Atwell explain, prosodic parsing can be based on the speaker’s desire to highlight specific aspects of the syntax producing a break after the item she wishes to highlight as in ‘...The idea that it’s important | for developing countries to become self-sufficient | in food | is widely | and uncritically accepted | not just in Brussels; | but from the orthodox economic standpoint | it’s without foundation...’ whereas the syntactic model would predict a break after idea and before to (2004). ’Highlighting’ as a strategy means emphasizing the role of adjectives in final position of the intonation unit. The relative proportion of adjectives in final position of minor units should be monitored in relation to this ’highlighting’ strategy. The dominance of Higher (H) pitch targets (see Left) for minor units confirms our previous observation as does the clustering of Same and Bottom for major units (consistent with finality). Our 3-gram analysis of final pitch targets in intonation units reveal phonosyntactic patterns. Considering the number of S-S-S sequences, a 4-gram analysis might be more relevant for the definition of the span of the pitch targets that characterize these units. Figure 3 illustrates a clear gender difference in tonetic stress marks and intonation unit. An overall patterning of major units with tonetic stress markings suggesting finality (falls) whereas rises co-occur with minor units, marking continuity.
            
                
                
            
            
                Figure 1. Example of the input data (top) and aligned corpus (bottom) using our alignment pipeline.
            
            
                
            
            
                
            
            
                Figure 2. Distribution of the main intonation values in final positions according to discourse genres.
            
            
                Figure 3.  Distribution of tonetic stress marks by gender and intonation unit.
            
        
        
            
                
                    Bibliography
                    Auran, Cyril, Caroline Bouzon, and Daniel Hirst. "The AixMARSEC project: an evolutive database of spoken English." In In Bel, B. &amp; Marlien, I.(eds) Proceedings of the Second International Conference on Speech Prosody. 2004.
                    Beliao, Julie. "Characterizing speech genres through the relation between prosody and macrosyntax." In Student Sessions at the European Summer School in Logic, Language and Information, pp. 1-18. Springer, Berlin, Heidelberg, 2013.
                    Brierley, Claire, and Eric Atwell. "Prosodic phrase break prediction: problems in the evaluation of models against a gold standard." TAL Journal: Traitement Automatique des Langues 48, no. 1 (2007): 187-206.
                    Cutler, Anne, Delphine Dahan, and Wilma Van Donselaar. "Prosody in the comprehension of spoken language: A literature review." Language and speech 40, no. 2 (1997): 141-201.
                    Degaetano-Ortlieb, Stefania and Elke Teich. Using relative entropy for detection and analysis of periods of diachronic linguistic change. Santa Fe, New Mexico: Association for Computational Linguistics, 2018, pp. 22–33. 
                    Gavin, Michael and Eric Gidal. “Scotland’s Poetics of Space: An Experiment in Geospatial Semantics”. In: Cultural Analytics (2018). 
                    Hoyt Long, Anatoly Detwyler, and Yuancheng Zhu. “Self-Repetition and East Asian Literary Modernity, 1900-1930”. In: Cultural Analytics (2018). 
                    Lakoff, Robin. "Language and Woman's Place." Language in society 2, no. 1 (1973): 45-79.
                    Lanehart, Sonja, ed. The Oxford Handbook of African American Language. Oxford University Press, 2015
                    Orlikowski, Matthias, Matthias Hartung, and Philipp Cimiano. "Learning diachronic analogies to analyze concept change." In Proceedings of the Second Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, pp. 1-11. 2018.
                    Zappavigna, Michele. Discourse of Twitter and Social Media. Bloomsbury, 2012.
                
            
        
    

        
            
                With ca. 17 million visitors annually to culture and heritage sites in Glasgow and Edinburgh alone, tourism is a major driver of the Scottish economy - it is worth some £6bn annually, ca. 5% of Scottish GDP; and supports 196,000 jobs
                Scottish Government20171(Scottish Government, 2017)1112Scottish Government,Introduction to TourismStatistics topics2018October 222017October 13websitehttps://www.gov.scot/Topics/Statistics/Browse/Tourism?&gt;
                . The centrality of cutting-edge immersive experiences for tourism, the heritage industry and audience development has been increasingly evident in recent years, with the development of
                
                    Ars Electronica Linz
                
                 (1996); the
                
                    Robert Burns Birthplace Museum
                
                 (2009); 
                
                    The Battle of Bannockburn
                
                 (2013), and other venues.
            
            
                Immersive experiences describe all forms of perceptual and interactive use of technologies and physical spaces in order to create a 
                hybrid reality
                , in which visitors feel “part of the experience as a whole, encompassing all spheres of attention” – immersion can be Sensory (audio-visual, olfactory, haptic elements), Challenge-based (interaction) and/or Imaginative (narrative and interpretation) 
                Ermi20052(Ermi &amp; Mäyrä, 2005)2247Ermi, LauraMäyrä, FransFundamental Components of the Gameplay Experience: Analysing Immersion2005 DiGRA International Conference: Changing Views: Worlds in Play2005Vancouver, CanadaPaperhttp://www.digra.org/digital-library/publications/fundamental-components-of-the-gameplay-experience-analysing-immersion/?&gt;
                . 
            
            
                For tourism and cultural heritage, immersion represents a pathway towards a 
                mixed-mode experience economy
                , which reflects the nuances of differing experience dimensions embodied by different elements of a site
                Suntikul20163(Suntikul &amp; Jachna, 2016)3317Suntikul, WantaneeJachna, TimothyThe co-creation/place attachment nexusTourism ManagementTourism Management276-28652C2016?&gt;
                . In this mixed-mode experience economy, visitor engagement combines activities across the “Realms of an Experience”.
            
            
                Despite the opportunities afforded by immersive experiences – and relevant investment in such experiences in Scotland – there has been a lack of substantive evidence (from scholarship and practice) to evaluate current approaches and guide future developments:
            
            
                How successful are the approaches to immersive technologies at major heritage sites in Scotland, in terms of outcomes against business plan expectations and in terms of visitor response? What kinds of future development are supported by existing evidence?
            
            
                
                    The Scottish Heritage Partnership
                
                
                    These questions are the remit of the
                    
                        Scottish Heritage Partnership
                    
                    project. The Scottish Heritage partnership is a 2018-19 Engineering and Physical Sciences Research Council/Arts and Humanities Research Council funded project at the University of Glasgow, aiming to address the existing practice and future potential of immersive experiences and technologies in the culture and heritage industry in Scotland. 
                
                
                    The team is led by Professor Murray Pittock, with Professor Lorna Hughes and Dr Maria Economou as the Co-investigators and Dr Agiatis Benardou and Dr Leo Konstantelos as Research Associates. The National Trust for Scotland, Glasgow Museums and The National Library of Scotland are key partners in the project and so is our industry partner, Soluis Heritage. 
                
                
                    The objectives of the project are to:
                
                
                    Build on and expand existing partnerships to explore the efficacy of immersive experiences at major heritage sites.
                
                
                    Build a decision-making tool and gather evidence for policy development.
                
                
                    Explore how we can best harness and shape cutting-edge digital technology and develop effective, meaningful content into leading edge inclusive and impactful immersive experiences.
                
                
                    Outline the kind of social/group experiences facilitated or limited by immersive technology, and study how these affect the visitor experience overall.
                
                
                    Examine the consequences of service-wide adoption of immersive technology in Scotland's leading heritage and collections resource provider, Glasgow Life.
                
                
                    Produce a website, a digital decision-making tool, a policy paper and a risk assessment together with:
                
                
                    An evidence-based market model for use with Scottish Government, VisitScotland, local tourist authorities and nongovernmental agencies
                
                
                    A route to developing suitable immersive technologies which can be scaled/developed to meet the criteria identified under (a), in the process benefiting our digital partner, Soluis.
                
                
                    Support National Trust for Scotland, National Library of Scotland and Glasgow Museums' strategic use of their collections in interpretation and exhibitions development.
                
            
            
                
                    Our methodology
                
                
                    With support from our partners at Glasgow Museums, the National Trust for Scotland, and the National Library of Scotland, we have designed, collected and processed a substantial corpus of empirical evidence. To date, hundreds of questionnaires have been completed by visitors at six major heritage sites across Scotland, including the Battle of Bannockburn; Culloden Battlefield; the Robert Burns Birthplace Museum; the Riverside Museum; the Kelvingrove Art Gallery and Museum; and the NLS at Kelvin Hall. We have conducted observations of visitor experience at Riverside and RBBM, harvested and processed visitor comments from online websites, and we are using secondary analysis of existing visitor experience data to answer the project’s research questions.
                
                
                    Development of an evidence-based, decision-making model was completed. Formulated as a policy and risk assessment document, the model is meant to help heritage institutions identify the kinds of future immersive experiences that are supported by our evidence; as well as assess how to develop effective, meaningful content into leading edge inclusive and impactful immersive experiences.
                
                
                    Our digital partner, Soluis Heritage, developed a visualisation of both the model and project findings, as a decision-making tool to illustrate the wider implications for policy and good practice, making the project’s findings accessible and clearly showing the underlying data and empirical evidence used. This output was made freely available online, as a resource illustrating the creative and critical processes, and key decision points, of developing immersive technologies in a cultural heritage environment.
                
            
            
                
                    What have we found:
                
                
                    Our findings provide valuable pointers and evidence on how cultural heritage institutions in Scotland (and beyond) can prepare for dynamic change in the immersive experiences economy. Based on visitor feedback and other data sources, we have investigated the components of perceived value in a digital experience; the audiences most interested in immersive technologies; as well as characteristics of good immersive design and content. 
                
                
                    A summary of our findings was presented in a decision-making visualisation, while the entire corpus of evidence informed the development of a policy document for decision-making in incorporating immersive experiences in the cultural heritage sector.
                
                
                    To stay up-to-date with our work, follow us on Twitter
                    
                        @scotmimmersives
                    
                    ; and visit our
                    
                        project website
                    
                    .
                
            
        
        
            
                
                    Bibliography
                    
                        Ermi, L.
                        , &amp; Mäyrä, F. (2005). 
                        Fundamental Components of the Gameplay Experience: Analysing Immersion
                        . Paper presented at the 2005 DiGRA International Conference: Changing Views: Worlds in Play, Vancouver, Canada. Paper retrieved from http://www.digra.org/digital-library/publications/fundamental-components-of-the-gameplay-experience-analysing-immersion/
                    
                    
                        Perry, S.
                        , Roussou, M., Economou, M. , Pujol-Tost, L. and Young, H. (2018) Moving Beyond the Virtual Museum: Engaging Visitors Emotionally. In: 23rd International Conference on Virtual Systems and Multimedia (VSMM 2017), Dublin, Ireland, 31 Oct - 04 Nov 2017.
                    
                    
                        Liarokapis, F.
                        , Pujol-Tost, L., Killintzis, V., Sylaiou, S., Mania, K. and Paliokas, I. (2017) Exploring the educational impact of diverse technologies in online virtual museums. International Journal of Arts and Technology, 10(1), p. 58.
                    
                    
                        Economou, M.
                          and Pujol-Tost, L. (2011) Evaluating the use of virtual reality and multimedia applications for presenting the past. In: Styliaras, G., Koukopoulos, D. and Lazarinis, F. (eds.) Handbook of Research on Technologies and Cultural Heritage. IGI Global, pp. 223-239. 
                    
                    
                        Pittock, M
                        . and Mackay, P. (2011) Beyond text: Burns, Byron and their material cultural afterlife. Byron Journal, 39(2), pp. 149-162.
                    
                
            
        
    

        
            The idea that Digital Humanities practitioners might provide a translational capacity within and between the arts, humanities, information and computer science, easing collaboration between these disciplines and enhancing shared results, is not a new one: in fact, there is a long tradition of conceptualising at least some digital humanists as “intermediaries,” (Edmond 2005) “translators” (Siemens et. al., 2011) or “hybrid people” (Liu et al 2007, Lutz et al 2008 cited in Siemens et. al., 2011). As the long-predicted mainstreaming of digital humanities and digital methods into arts and humanities research advances, we might expect this transformation of the digital humanities from a disruptive to a supportive force to continue. Furthermore, while some within the academy certainly view the potential industrial relevance of the digital humanities with suspicion (Allington et. al., 2016), there are also many voices from industry itself calling for the development of a more humanistic, critical dimension in the work of the ICT industry (Hern, 2018; Madsbjerg, 2017; Hartley, 2017; Copenhagen Letter, 2017; Centre for Humane Technology, 2018).
            While it may therefore seem timely to explore, as Liu (2012, 2016) has called for, how the digital humanities might deliver a linchpin set of critical competencies for and reflections on the techno-social interface, how this cultural intervention into technology development might resonate with of the core tenets of DH remains unclear. This paper will introduce such a frame of reference by exploring the implications for digital humanities to be found in a corpus of 38 linked interviews about big data research. The project that developed this material, an EU-funded collaboration known as Knowledge Complexity, or KPLEX for short (
                
                    www.kplex-project.eu
                ), explored in depth the perspectives of and attitudes toward big data found among computer scientists, collections holding institutions, and an interdisciplinary research community reaching from philosophy to fMRI studies (emotion research). The project originally focussed on understanding unconscious bias in such research, but they also expose the depth of the misalignment between approaches to how knowledge is generated and validated across contributing disciplines. 
            
            The data the project produced therefore offers much food for thought to those of us who identify as digital humanists, as it points toward a number of key barriers commonly faced and ideally negotiated within our hybrid research space. When viewed from the perspective of the KPLEX project’s data, six distinct points of ‘aporia’ arise, places where the interviewees explicitly or tacitly exposed gulfs in epistemic culture that are clearly at the heart of the tensions between disciplines as they seek to collaborate. These gulfs in goals and understanding echo the work of digital humanists, but also expand upon and throw into relief the underlying tensions in their research. While none of these findings presents, strictly speaking, an insoluble problem, the KPLEX interviews clearly illustrate the embeddedness of these challenges in the foundations of the contributing disciplines. This entanglement with professional identities and values raises them above the level of mere barriers, to a status where a more fundamental reconsideration of the scholarship produced within such collaborations may be required. In these fundamentals we may find future avenues for DH to grow in its own right, but also to expand and reconsider its potential impact. This paper will focus its exposition on the nature of and evidence for these gaps given in the interviews, which can be briefly described as follows:
            
                Language matters. In particular the interviews with computer scientists showed a resistance to discussing what certain key terms might mean or imply, a lack of precision that would draw criticism in a purely humanities context. This impulse weakens the potential for self-reflection in computer science but also greatly impedes successful interdisciplinary work, which may progress for extended periods on a falsely constructed sense of common understanding. While this obscurity had already been observed by Borgman (2015), the KPLEX project results provide not only empirical evidence of the phenomenon, but also of its eventual negative consequences. 
            
            
                Context matters. Datafication implies decontextualization, and this data/context-tradeoff is only rarely reflected in data-driven methodologies (for a notable exception see Nelson, 2017). But in humanistic enterprises context is indispensable: for an historian, for example, provenance is an all-important facet in the understanding of any source. But that which is a potentially harmful data ‘modification’ for one community is a neutral, or in fact positive, process of data ‘cleansing’ for another. 
            
            
                Tools and standards are 
                pharmaka
                , giving much but taking as well. In particular, information scientists can see how the availability of certain dominant tools (like keyword searches and metadata standards) are both liberating and limiting in equal measure. Data and metadata standards can be perceived by humanists as handcuffs, limiting possible iterative adaptation of parameters, but the resulting variability and complexity stand in opposition to interoperability, aggregation, and scaling (Saklofske et al., 2015).
            
            
                Data without theory is as problematic as theories without evidence. A popular notion has been proposed that big data may have delivered us to the ‘end of theory’ (Anderson, 2008), but researchers actively working at the edges of big data can see clearly that this is not the case. That said, the lack of a critical frame merely pushes much of the transparency around complex phenomena into a black box with an authority based on a potentially flawed algorithm. 
            
            
                The power structures of technology inhibit accommodation of analogue or hybrid narratives. Much of the humanistic source landscape is still measured in kilometres of shelving rather than terabytes of data. Because of this, digital humanities practices must be well-adapted to resisting the Matthew Effect (Merton, 1968), by which research becomes concentrated on the limited, potentially flawed data - this is not always the case outside of the humanities, however. Moreover, the struggle between ‘archival thinking’ and ‘computational thinking’ evidenced in the interviews and the conceit of routinisation raises questions of who will control cultural heritage knowledge in the future.
            
            
                Humanistic competences are not taught in conjunction with digital approaches. Critical, speculative, and hermeneutic thinking - the hallmarks of the humanities - are not taught alongside empirical methodologies, and critical approaches are not systematically implemented in computational studies -- Jonathon Morgan’s analysis of the Alt-Right Movement on Twitter (2016), and the Digital Humanities’ Now ‘Editor’s Choice’ Project ‘Torn Apart / Separados’ (2018) are two rare and enlightening exceptions. 
            
            The paper will conclude with a series of reflections on how digital humanities researchers could move within their disciplines and beyond to become uniquely able to negotiate some of these critical conversations. It will also address crucial points DH can share with all interdisciplinary collaboration, such as shared data formats and structuring approaches, how misconceptions are surfaced and resolved, the place of self-reflection and methodological discussions, and the incommensurability of research questions and methodologies. In conclusion, it will offer recommendations for how each of the six aporias might be met and used to create a stronger digital humanities community and culture, fulfilling its potential as both a disruptive and productive force.
        
        
            
                
                    Bibliography
                    
                        Allington, D., Brouillette S. and Golumbia D. (2016). Neoliberal Tools (and Archives): A Political History of Digital Humanities. 
                        LA Review of Books, May 1st, 2016. 
                        
                            https://www.lareviewofbooks.org/article/neoliberal-tools-archives-political-history-digital-humanities/
                         (accessed 11 April 2019).
                    
                    
                        Ahmed, M. et al. (2018). Editors’ Choice: Torn Apart / Separados. 
                        Digital Humanities Now.
                    
                    
                        
                            http://digitalhumanitiesnow.org/2018/06/editors-choice-torn-apart-separados/
                         (accessed 11 April 2019).
                    
                    
                        Anderson, C. (2008). The End of Theory: The Data Deluge Makes the Scientific Method Obsolete. 
                        Wired, June 23rd, 2008. 
                        
                            https://www.wired.com/2008/06/pb-theory/
                         (accessed 11 April 2019).
                    
                    
                        Borgman, C. L. (2015). 
                        Big Data, Little Data, No Data. Scholarship in the Networked World. Cambridge, MA: The MIT Press.
                    
                    
                        The Centre for Humane Technology. http://humanetech.com/.
                    
                    
                        The Copenhagen Letter. https://copenhagenletter.org/.
                    
                    
                        Edmond, J. (2005). The Role of the Professional Intermediary in Expanding the Humanities Computing Base. 
                        Literary and Linguistic Computing, Vol 20, No.3, pp.367-380.
                    
                    
                        Hartley, S. (2017). 
                        The Fuzzy and the Techie: Why the Liberal Arts Will Rule the Digital World. New York: Houghton Mifflin.
                    
                    
                        Hern, A. (2018). Tech suffers from lack of humanities, says Mozilla head. 
                        The Guardian, October 12th, 2018. 
                        
                            https://www.theguardian.com/technology/2018/oct/12/tech-humanities-misinformation-philosophy-psychology-graduates-mozilla-head-mitchell-baker?CMP=share_btn_tw
                         (accessed 11 April 2019).
                    
                    
                        Knowledge Complexity (KPLEX).
                        
                            www.kplex-project.eu
                        . (accessed 11 April 2019).
                    
                    
                        Liu, A. (2012). Where is Cultural Criticism in the Digital Humanities? 
                        
                            http://dhdebates.gc.cuny.edu/debates/text/20
                         (accessed 11 April 2019).
                    
                    
                        Liu, A. (2016). Drafts for Against the Cultural Singularity (book in progress). 
                        
                            http://liu.english.ucsb.edu/drafts-for-against-the-cultural-singularity/
                         (accessed 11 April 2019).
                    
                    
                        Madsbjerg, C. (2017). 
                        Sensemaking. What Makes Human Intelligence Essential in the Age of the Algorithm. London: Little Brown.
                    
                    
                        Merton, R. K. (1968). The Matthew Effect in Science. 
                        Science. 159 (3810): 56–63. doi:10.1126/science.159.3810.56
                    
                    
                        Morgan, J. (2016). The Radical Right and the Threat of Violence. 
                        Medium.com
                        
                            https://medium.com/@jonathonmorgan/the-radical-right-and-the-threat-of-violence-f66288ac8c4
                         (accessed 11 April 2019).
                    
                    
                        Nelson, L. K. (2017). Computational Grounded Theory: A Methodological Framework. 
                        Sociological Methods &amp; Research. DOI: 
                        
                            https://doi.org/10.1177/0049124117729703
                         (accessed 11 April 2019).
                    
                    
                        Saklofske, J., &amp; Research Team. (2015). NewRadial: Challenging scales and standards of humanities scholarship through new knowledge environment prototypes. 
                        Digital Studies/le Champ Numérique. DOI: 
                        
                            https://doi.org/10.16995/dscn.24
                         (accessed 11 April 2019).
                    
                    
                        Siemens, L, Cunningham, R., Duff W. and Warwick C. (2011). “A tale of two cities: implications of the similarities and differences in collaborative approaches within the digital libraries and digital humanities communities.” 
                        Literary and Linguistic Computing, Vol. 26, No. 3, 335-348.
                    
                
            
        
    

        
            As an interactive, pluri-directional and multimodal realm, the cybersphere is characterized by the incessant production and sharing of information content, with an ever-growing number of bottom-up discourse formations and disseminations (KhosraviNik and Unger 2016). One of the most significant and complex drawbacks of this unprecedented proliferation of user-generated content, and the so-called democratization of access to symbolic recourses, is the acutely increasing incidence of online hate or cyberhate. Hostility is a complex social, cultural and psychological phenomenon: motives behind people’s hate are various, different and often obscure, and the fluid and widely unregulated nature of the cybersphere seems to have added to further complicate an already thorny matter.
            One of the key scholarly assumptions on the issue is that social media affordances seem to act as a force multiplier, both in terms of sheer quantity and vitriolic quality of interactions. Both social psychologists and criminologists have attempted at sketching haters’ underlying motivations and strategies by drawing on psychological theories and research. These studies have provided useful insights into how some features inherent to Computer Mediated Communication - e.g. perceived anonymity (Joinson 2003) and physical separation (Weisband and Atwater 1999) - contribute to trigger social practices online - e.g. dishinibition and de-individuation (Thurlow et al. 2009), polarization (Wallace 2016) and mob dynamics (Citron 2009). As a result, the recognition of strong psychological features in antisocial behaviours like hate speech is basically entrenched in the differences between face-to-face communication and online interactions. One of the dangers of relying on these scholarly interpretations is the relatively straightforward establishment of a cause-effect relation between the affordances of the participatory web and practices of hostility online, highlighting the role of the digital medium and downplaying socio-political structures and power hierarchies. 
            This paper advocates a Social Media Critical Discourse Studies (SM-CDS) approach to online hostility. As “a socially committed, problem-oriented, textually based, critical analysis of discourse (manifested in communicative content/practices)” (KhosraviNik 2017: 586), SM-CDS deals with discourse as its central object of analysis: it is not only interested in what happens in the media per se as a closed loop but also in how it may shape and influence the social and political sphere of our life worlds and vice versa. Such an approach would deliberately steer away from media determinist accounts as well as from universalist understandings of social media effect: communication is to be regarded as a human endeavour, irrespective of the sophistication of the medium used. Despite difficulties in demographic and geographic accounting of online communities, macro-contextual aspects, including materialities of sociocultural categorizations (class, ethnicity, gender, age, dis/ability, agency, cultural capitals, as well as cultural positioning, stereotypes, power structures, histories etc.) are to be carefully taken into account and not to be distilled “into a bland cybernetic metaphor” (Couldry 2012: 117). 
            In particular, this paper focuses on gender as a source of hate in its own right which has not received sufficient institutional and academic attention. While the dangers and risks of the digital world are well acknowledged, we still lack a clear grasp of what it actually entails being a woman navigating the cyberspace, and which specific threats and troubles this journey can bring about. In approaching online gender-based hostility, we would always start from the assumption that any online form of gendered violence replicates and extends the gender and power relations that pre-exist digital communications technologies and vice versa. Digital misogyny is to be regarded as a purposeful discursive strategy to maintain a gendered asymmetry of power by threatening, discrediting and ultimately silencing women in a way that it has historically regimented (Butler 2009). The domain of online misogyny as a digital discursive practice would be, therefore, conceptualized at the intersection of digital media scholarship, discourse theorization and critical feminist explication, with audacious interdisciplinarity (KhosraviNik and Esposito 2018) and substantial intersectionality (Lykke 2010) representing the epistemic way forward.
            This paper presents a number of epistemological considerations in relation to digital media, discourses of hostility and critique, grounded in the results of a multi-lingual pilot study conducted in the context of a project funded by the European Commission (H2020-MSCA-IF-2017). The study investigates phenomena of online misogyny (such as gender-based hate speech, rape threats and image-based sexual harassment), against highly visible, political and institutional female figures in Europe. More specifically, it maps the multimodal discursive strategies of online hate against women in the public sphere by collecting and analysing a corpus of user-generated comments on Social Networking Sites (namely, Youtube and Twitter) from three different linguistic landscapes and political cultures in Europe, namely Italy, Spain, and the U.K.
            In order to locate abundant relevant foci of data, the preliminary phase has been characterized by a digital ethnographic stance (see Androutsopoulos 2008). Criteria of inclusion encompassed: degree of digital presence, critical ‘moments’ or events of particular relevance or visibility, as well as overall number of views, likes and comments, (regarded as indices of audience attention and likely to yield a high occurrence of polarized content. The selection of case studies has been also guided by the typology of online sexual harassment by Powell and Henry (2017), to include instantiations of: a) gender-based hate speech; b) rape threats; and c) image-based harassment. 
            The multimodal nature of data has called for an integrated methodology, encompassing: 1) Corpus Linguistics tools (Baker and Egbert 2016), for a quantitative identification of linguistic patterns (e.g. key-keywords, collocations and semantic prosody); 2) Critical Discourse Analysis, for a close qualitative and critical analysis mapping the vast number of discursive strategies and rhetorical devices through which online misogyny is realized, in four different heuristic levels of context (Reisigl and Wodak 2001). 3) Visual Content Analysis, for a multimodal analysis of the videos containing image-based harassment by means of the four-layered framework proposed by Rodriguez and Dimitrova (2011).
            Emerging results in all the three linguistic and socio-political contexts in exam are showing:
            1) From a more ‘micro’ linguistic perspective, the different degrees of formulaicity and creativity of the multimodal discursive strategies of online hate. This is in line with the already demonstrated algebraic and tediously predictable quality (see Jane 2017) of digital misogyny, but also highlights digital creativity as an integral part of mob dynamics and mentality online, often resulting in an ever-escalating competition to produce the most abusive content. 
            2) From a more ‘macro’ social perspective, the profoundly intersectional nature of online gender-based hostility. In particular, the analysis points toward the interaction of mutual and intertwined factors both triggering and stoking hate such as: class, race, gender identity or behaviour, age as well as feminist activism.
            These results contribute to a more in-depth understanding of gender-based hostility against women in politics as an extremely multi-faceted and multi-layered phenomenon, where gender is not the only factor at play. They also call for the further integration and development of the concept of Digital Intersectionality (Noble and Tynes 2016), which would allow to further question the organization of social relations embedded in digital technologies and foster a clearer understanding of how power relations are organized through them.
        
        
            
                
                    Bibliography
                    Androutsopoulos, J. (2008). Potentials and Limitations of Discourse-Centred Online Ethnography. 
                        Language@Internet 5(8). https://www.languageatinternet.org/articles/2008/1610 (Accessed 22 April 2019).
                    
                    Baker, P. and Egbert, J. (Eds.) (2016). 
                        Triangulating Methodological Approaches in Corpus-Linguistic Research. London: Routledge.
                    
                    Butler, J. (2009). Performativity, precarity and sexual politics. 
                        Revista de Antropología Iberoamericana 4(3): i–xiii.
                    
                    Citron, K. (2009). Cyber civil rights. 
                        Boston University Law Review 89: 61–125.
                    
                    Couldry, N. (2012). 
                        Media, Society, World: Social Theory and Digital Media. Cambridge: Polity.
                    
                    Jane, E. A. (2017). 
                        Misogyny Online: A Short (and Brutish) History. London: SAGE.
                    
                    Joinson, A. (2003). 
                        Understanding the Psychology of Internet Behaviour: Virtual Worlds, Real Lives. New York: Palgrave Macmillan.
                    
                    KhosraviNik, M. (2017). Social Media Critical Discourse Studies (SM
                        ‐CDS). In Flowerdew, J. and Richardson, J. (eds.), 
                        Handbook of Critical Discourse Analysis. London: Routledge, pp. 583–596.
                    
                    KhosraviNik, M. and Esposito, E. (2018). Online hate, digital discourse and critique: Exploring digitally-mediated discursive practices of gender-based hostility. 
                        Lodz Papers in Pragmatics 14 (1): 45-68.
                    
                    KhosraviNik, M. and Unger, J. (2016). Critical discourse studies and social media: Power, resistance and critique in changing media ecologies. In Wodak, R. and Meyer, M. (eds.), 
                        Methods of Critical Discourse Analysis, 3
                        rd edn. London: Sage, pp. 205–234.
                    
                    Lykke, N. (2010). 
                        Feminist Studies: A Guide to Intersectional Theory, Methodology and Writing. New York: Routledge.
                    
                    Noble, S. U., and Tynes, B. M. (2016). 
                        The Intersectional Internet: Race, Sex, Class, and Culture Online. New York: Peter Lang.
                    
                    Powell, A. and Henry, N. (2017). 
                        Sexual Violence in a Digital Age. Palgrave Macmillan.
                    
                    Reisigl, M. and Wodak, R. (2001). 
                        Discourse and Discrimination: Rhetoric of Racism and Anti-Semitism. London: Routledge.
                    
                    Rodriguez, L. and Dimitrova, D. (2011). The Levels of Visual Framing. 
                        Journal of Visual Literacy 30(1): 48–65.
                    
                    Thurlow, C., Lengel, L. and Tomic, A. (2009
                        ). Computer Mediated Communication: Social Interaction and the Internet. London: Sage.
                    
                    Wallace, P. (2016). The Psychology of the Internet. New York: Cambridge University Press.
                    Weisband, S. and Atwater, L. (1999). Evaluating Self and Others in Electronic and Face-to-Face Groups. Journal of Applied Psychology 84: 632– 639.
                
            
        
    

        
            The European Commission notes that, in order to solve Europe’s economic and societal challenges, innovation in science and technology, pursued through Research Infrastructures (RIs), is vital (European Commission).  Efficient RIs enable the greatest discoveries in science and technology, attract researchers from around the world and build bridges between research communities. They allow the training of researchers and facilitate innovation and knowledge-sharing.  One such research infrastructure in particular, DARIAH (Digital Research Infrastructure for the Arts and Humanities), was established as a European Research Infrastructure Consortium (ERIC) in August 2014. Currently, DARIAH has 17 Member states and many cooperating partners across 11 non-member countries. As a pan-European network, DARIAH aims to enhance and support digitally-enabled research and teaching across the arts and humanities.  And yet, despite this wide interdisciplinary and international reach, RIs such as DARIAH remain a distant concept to many of the researchers who could directly benefit from them.
            The Community Engagement Working Group within DARIAH has, since Nov 2017, been investigating the often complex reasons for a lack of engagement with RIs among researchers as part of our wider research into Research Communities.  Over the course of our exploration around these themes, we have conducted a webinar, an online survey, interviews with researchers at different stages of careers, and a roundtable session at a discipline-specific regional conference.  Many of our respondents to these various methods of data gathering have reported that they are aware of RIs, such as DARIAH, but for various reasons they did not choose to engage with them. We analysed their responses by both career level, and by some of the disciplinary groups to see if further patterns emerged.
            Researchers experience different pressures and issues at all stages in their career. However, we have chosen to look at Early Career Researchers (ECRs) here as the pressures on ECRs are well known, often taking on a sort of ‘apprenticeship’ role where not only the direction of the research is somewhat predetermined, but also membership of organisations and indeed RIs is decided by the more senior members in a team.  An ECR does have 
                some autonomy in terms of how they network, how they communicate with others in their field, and of course which teams or researchers they choose to work with in the first place.  
            
            To find out how RIs could communicate more directly with ECRs, we focus here on the results of our research from that group in particular.  We found differences in how ECRs communicate with others in their field compared with mid-career and senior researchers, specifically that ECRs favoured more face-to-face approaches in networking and communication (through conferences and workshops) over social media such as Facebook, Twitter or LinkedIn, as was widely used by the mid-career and senior researchers. 
            When we looked at specific reasons for not engaging with RIs, the responses from the ECRs either indicated a lack of awareness about them, or focussed much more on time constraints due to competing priorities such as getting publications accepted and completed, or finishing their PhD.  Insecurity in their current employment also created anxiety that meant they were unable to form a ‘long-term view’ beyond trying to find the next contract, and thus unable to spend the perceived time and effort it would take to learn how to work with an RI. Responses from more senior researchers tended to err on the side of exceptionalism, with many taking the view that, while they were aware of RIs, their own area of research was specialised, and therefore not likely to benefit from an interdisciplinary RI.
            Steps that RIs have taken to try to reach potential members at all levels in their career have so far included creating national contact points to act as advocates, and by establishing some manner of training; either as occasional Summer or Winter Schools, or through online training resources (RITrain
                
                    
                        http://ritrain.eu/ (retrieved 27th Nov 2018)
                    
                , the PARTHENOS cluster-project
                
                    
                        http://training.parthenos-project.eu/, part of the PARTHENOS project. (retrieved 27th Nov 2018)
                    
                , and CESSDA
                
                    
                        https://www.cessda.eu/Temp-archive/Training (retrieved 27th Nov 2018)
                    
                 are three such examples).  However information from and about these initiatives is often disseminated via social media, so despite these interventions there are still gaps between provision to enable engagement, and the modes of communicating these provisions.  
            
            This poster will present these results in more detail, and offer recommendations for how RIs might integrate the needs of this specific research community into their wider communications practices.
        
        
            
                
                    Bibliography
                    
                        European Commission European Research Infrastructures Text 
                        European Commission - European Commission https://ec.europa.eu/info/research-and-innovation/strategy/european-research-infrastructures_en (accessed 27th Nov 2018).
                    
                
            
        
    

        
            
                1 Introduction
                After the East Japan Great Earthquake on 11 March, 2011, rumors about an explosion at a petrochemical complex owned by Cosmo Oil spread rapidly on twitter. Stories of oil tanks exploding and releasing harmful substances into the air caused widespread panic until official government news releases corrected the misinformation the following day. This story demonstrates the importance of fake news detection: while an enormous real disaster (the earthquake) was unfolding, rumors of imaginary disasters spread misinformation and diffused attention on social media to imaginary dangers. 
                The story of the fake Cosmo Oil fire provides an example of a situation in which fake news detection is important, and also provides a test case for studying the characteristics of fake news on twitter. We propose a method for fake news detection based on topic diversity. Our method computes topic diversity by a micro-clustering approach that makes clusters smaller than those produced by conventional clustering methods. We begin by extracting micro-clusters, small sets of keywords that represent topics, using a data polishing algorithm (Uno 2015; Uno 2017) from tweets about the event. Next, we analyze clusters over time using visualization methods to understand how these topics change. We observe that a diversity measure for clusters, a measure of both the number of clusters and the number of words, in one cluster, shows topic transition. This observation has implications in the measure of the truthfulness of a story. Users tweeting about real news stories often show sudden changes in opinions, which causes a drastic increase in the diversity of the opinions expressed. Even if the number of tweets on the topic does not increase, topic diversity rises. On the other hand, users tweeting fake news stories are often not thinking critically about a topic, so there is no change on the number of clusters even as the number of tweets increases.
                
                    
                        
                        Figure 1: Outline of Our Method
                    
                
            
            
                2 Topic Extraction using Micro-Clustering
                Figure 1 illustrates our method. First, we build a graph of tweets: each tweet is a node, and an edge between two nodes represents two tweets whose Jaccard similarity is greater than 0.3. This is our Input Data. Then, cliques are extracted by micro-clustering using 
                    Data Polishing (Uno 2017).
                
                Micro-clusters represent small topics within a larger set of tweets. Micro-clusters are groups of records that have high similarity—in our case, tweets that include similar sets of words. To create micro-clusters of similar tweets, we perform maximal clique enumeration (MCE). A maximal clique is a clique included in no other clique within the graph. A maximal clique is not necessarily the largest clique in a graph, so the size of maximal cliques in the same graph can vary significantly. 
                Because there are usually a huge number of maximal cliques in a graph, MCE is a computationally intractable problem. Data polishing reduces the complexity of MCE. It makes an edge between pairs of nodes if they seem to belong to the same cluster, and removes edges between nodes that do not seem to belong to the same cluster. It clarifies the graph's cluster structures, and thus makes MCE far simpler. 
                We eliminate edges using the following procedure. If nodes 
                    u and 
                    v are in the same clique of size 
                    k, 
                    u and 
                    v have at least 
                    k − 2 common neighbors. Thus, we have 
                    |N
                    u
                     ∩ N
                    v
                    | ≥ k, so 
                    u and 
                    v are in a clique of size at least 
                    k. If 
                    u and 
                    v are in a sufficiently large pseudo-clique, they belong to a pseudo-clique with a high probability of being semantically meaningful; otherwise, they do not. To compute these nodes’ similarity, we compute the Jaccard coefficient, 
                    J, of their neighbor sets. We set the threshold 
                    s’ and consider each pair of nodes in the graph. If 
                    J(N
                    u
                    , N
                    v
                    ) &gt; s’, we add an edge between 
                    u and 
                    v. Conversely, if 
                    J(N
                    u
                    , N
                    v
                    ) 
                    &lt;
                     s’, we remove any edges between them.
                
                Micro-clustering produces a set of topics, each made up of one or more clusters. Next, topic transitions are analyzed by calculating the diversity of clusters that constitute the corresponding topic. Micro-clusters are groups of similar or related records. 
                In a graph, micro-clusters correspond to dense subgraphs, and non-edges in the dense subgraphs are ambiguities. We also consider edges not included in any cluster to be ambiguities. Our data polishing approach for micro-clustering consists of adding edges for these non-edges, and removing these edges from the graph. 
            
            
                3 Target Data
                Our target data set is the over 200 million tweets sent around the time of the Great East Japan Earthquake on March 11, 2011. We obtained this dataset from the social media monitoring company Hotto link Inc. (Hottolink). Hotto link tracked users who used one of 43 hashtags (including #jishin, #nhk, and #prayforjapan) or one of 21 keywords related to the disaster. Later, they captured all tweets sent by these users between March 9th (2 days prior to the earthquake) and March 29th. This dataset offers a significant document of users’ responses to a crisis, but its size presents a challenge.
                We show our experimental result for tweets from 00:00 on March 11 to 24:00 on March 15, a total of 120 hours. We began by creating the sequence of tweet-word count matrices for our dataset for every 30 minutes, that means we had 240 slots. For example, the matrix for 30 minutes on March 11 before 14:30 (before the earthquake), contains 60,000-80,000 tweets. On the other hand, the matrix for 30 minutes on March 11 after 15:00 (after the earthquake) contains 300,000-500,000 tweets. The number of tweets increased dramatically after the earthquake. The size of each matrix after 15:00, March 11 is around 15MB and they were all quite sparse. 
            
            
                4 Target Topic
                We selected the fake news about the petrochemical complex explosion that happened just after the earthquake. The story can be divided into four stages:
                
                    Fact: around 15:00 on March 11, just after the earthquake, the Cosmo Petrochemical Complex in Chiba caught fire.
                    Fake: Around 19:00 on March 11, the following fake stories appeared as tweets and were retweeted frequently:
                        
                            “Radiation and harmful chemicals are leaking into the air from the petrochemical complex. Be careful!” 
                            “Don’t go out! Because the rain includes radioactivity and harmful materials by the petrochemical complex explosion.” 
                        
                    
                    Correction: Around 15:00 on March 12 (the day after the earthquake), the company’s website and the local government’s twitter account explained that there had been no explosion. 
                    Convergence: At night on March 12, the topic was converged. 
                
                The fake news about the oil tank emitting harmful substances scared users as it spread. Finally, the government released a report correcting the misinformation and the fake news disappeared from Twitter. To evaluate the progress of the target topic, we investigated micro-clusters with the phrase “Cosmo Oil” over time. We examined the target topic transition and the diversity of clusters in each time period to show our method’s effectiveness. 
                
                    
                        
                        Figure 2: # of Tweets vs # of Micro-Clusters that include the word ”cosmo oil” (base 10 log-log plot)
                    
                
            
            
                
                    
                        
                        Figure 3: Topic diversity for all tweets
                    5 Fake News Detection
                
                The progress of the fake news is shown in Figure 2. The graph shows the relationship between the number of tweets and the number of micro-clusters. Each circle on the graph shows one half-hour time period. 
                Fake news stories show low topic diversity: Figures 2 and 3 illustrate the difference between a real story and a fake story. Both graphs plot the correlation between the number of micro-clusters that contain a phrase and the number of tweets that contain a phrase over each half-hour period. Figure 2 shows the topic diversity of a fake news story, tweets that contain “Cosmo Oil;” in contrast, Figure 3 plots all tweets from our data set during the same time. We use Figure 3 as an index of a known real story, the Great East Japan Earthquake, in contrast to the false rumor of the Cosmo Oil explosion. 
                For a real news story, the relationship between tweet count and micro-cluster count is linear. Figure 3 shows the nearly linear relationship observed on the log-log plot for a real news story, which implies a power law relationship between the number of tweets and the number of micro-clusters. We can make the hypothesis that the line is the upper bound of topic diversity: that is, when a topic emerges independently, the total number of topics is equal to this upper bound. 
                However, for a fake news story, the micro-cluster count is much lower relative to the tweet count in many time periods, and so the relationship is non-linear. In some time periods of Figure 2, highlighted in red, the number of micro-clusters is lower than the expected number. Figure 2 does not show a similar correlation between the number of micro-clusters and the number of tweets that contain the phrase “Cosmo Oil.” We suggest that a fake news story is more likely to have a lower topic diversity because there are fewer facts to report in such a story, and, as such, the story is likely to change little over time.
                
                    
                        
                        Figure 4: Dynamics of the fake topic transition about ”cosmo oil” (base 10 log-log plot)
                    
                
                We evaluate the progress of the fake news over time (Figure 4). In Figure 4(a), first, the “fact” topic occurred. Then the “fake” topic appeared as rumors of the explosion spread, shown in Figure 4 (b). Compared to the “fact” and the “correction” periods, the “fake” period shows low diversity: the measurements. Figure 4(c) shows the “correction” period, when the government corrected the fake story and the correction overtook the spread of the rumors. At that time, the number of tweets and the number of clusters grew together, and so the diversity increased. Finally, In Figure 4 (d), the convergence happened: the Cosmo Oil story started to disappear, while gthe diversity stayed high, but gradually the number of tweets decreased. The progress shows the dynamics of the topic transition and a kind of topic life cycle. 
                Through the experiment, we confirmed that our method can extract quality micro-clusters by data polishing. In addition, we realized that micro-clusters can show dynamics of the topic transition using real tweets. 
            
            
                6 Conclusion
                This paper proposed a fake news detection method based on micro-clustering using data polishing. It showed that fake news stories follow a certain lifecycle. Furthermore, it suggests that topic diversity measures can help to detect fake news before an official correction is issued, as in the case of the Cosmo Oil story.
            
            
                Acknowledgements
                This work was partially supported by JST CREST JPMJCR1401, JSPS KAKENHI 19H01133 and 19K12125, 18K1143, and 17H00762.
            
        
    

        
            
                As the digital humanities have rapidly gained prominence and attention over the last decade, learning has shifted from individual experiences at training environments such as THATCamps and Institutes to more formal institutional instruction. This means that the number of people 
                teaching
                 digital humanities (DH) has had to increase. Who are these teachers? Where do they teach? Who are they teaching? What support do they have from their institutions? These questions are some that we hope to answer through a survey of those teaching DH—in any capacity. 
            
            In this poster, we will present the work that we have done to develop a survey of those teaching digital humanities throughout the world. First, we will discuss the development of the survey, including the process of securing (cross-)institutional review board approval and eliciting feedback from the broader DH community. Second, we will outline the methodology we have employed in developing the survey in order to best ascertain how and who these teachers are. Among these will be reports we receive from external reviewers of our survey from colleagues at institutions throughout the world. Third, we will begin in real time the data collection at the conference. At our poster, we plan to provide individuals with devices with which they can take the survey, thereby making this poster presentation truly interactive. We will also have print materials with links to the survey that we will distribute at the poster session and throughout the conference. Insofar as our research team is comprised of individuals from one continent, we are especially excited about the opportunity to reach a global audience in Utrecht so that our data can be as representative as possible. 
            Some of what we hope to glean from the survey includes:
            
                Basic demographics (age, gender, race/ethnicity) of those teaching DH
                Information about the environments in which this teaching occurs; for example, whether individuals are teaching in conventional courses, one-off workshops, week-long training institutes
                Information about teacher’s institutional homes (if any) and their employment status within those institutions
                Information about teachers’ own training in DH and how that informs their approaches to teaching (see Jakacki [2016])
                
                    Information about the level of course being taught (introductory, methodological, survey, disciplinary or interdisciplinary, graduate/undergraduate) and its sophistication (how “DHy” must a course be in order for it to satisfy curricular requirements?)
                
                
                    Information about how instructors think about student participation in research-based learning (see Jenstad &amp; Takeda [2017] and Keralis &amp; Andrews [2018])
                
            
            
                Even as we write a phrase such as “basic demographic information,” we must acknowledge that individuals’ demographics are 
                never
                 “basic.” But as the world has moved from seeing categories such as gender as non-binary, so too has the concept of teaching within the digital humanities expanded to include many different types of instructors. In particular, we see this as one of the first ‘tangible’ opportunities to engage in serious dialogue about the complexities of DH pedagogy in interdisciplinary,  interinstitutional, and international contexts. Our goal is to reveal the complexities of these teaching experiences and challenge our colleagues to develop best pedagogical practices.
            
            
                Data collection, which will begin in Utrecht, will continue for 6 weeks after the conference, with notifications being sent out via Humanist, DHSI, Digital Library Federation, HASTAC, and other appropriate listservs, as well as social media, including Facebook and Twitter, and direct invitations to individuals the authors know teach digital humanities. The data will then be collected and analyzed prior to publication in a forthcoming volume on digital humanities pedagogy. This survey will certainly not be the first conducted in the field of digital humanities. As such, we will be building on the work of Nowviskie and Porter (2010), who surveyed those caring for “end of life” DH projects, similarly beginning at the 2010 DH Conference in London; Sula, Hackney, and Cunningham (2017), who examined the range of DH programs, including minors and majors; and Rasmussen, Croxall, and Otis (2017), who conducted oral interviews with those teaching DH in libraries and concluded that a broad survey of DH pedagogues was needed (p. 85).  
            
        
        
            
                
                    Bibliography
                    
                        Jakacki, D. (2016). How We Teach? Digital Humanities Pedagogy in an Imperfect World. Canadian Society for Digital Humanities/Société canadienne des humanités numériques 2016. Calgary, Canada. Retrieved from 
                        
                            http://dianejakacki.net/how-we-teach-digital-humanities-pedagogy-in-an-imperfect-world/
                        
                         (26 November 2018).
                    
                    
                        Jenstad, J., &amp; Takeda, J. (2017). Making the RA Matter: Pedagogy, Interface, and Practice. In J. Sayers (Ed.) 
                        Making Things and Drawing Boundaries
                        . University of Minnesota Press.
                    
                    
                        Keralis, S. B., &amp; Andrews, P. (2018). Labor. In R. Frost Davis, M. K. Gold, K. D. Harris, &amp; J. Sayers (Eds.), 
                        Digital Pedagogy in the Humanities: Concepts, Models, and Experiments
                        . Modern Language Association. Retrieved from 
                        
                            https://github.com/curateteaching/digitalpedagogy/blob/master/keywords/labor.md
                        
                         (26 November 2018).
                    
                    
                        Nowviskie, B., &amp; Porter, D. (2010). The Graceful Degradation Survey: Managing Digital Humanities Projects Through Times of Transition and Decline. 
                        Digital Humanities 2010: Conference Abstracts
                        . Retrieved from 
                        
                            http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/html/ab-722.html
                        
                         (26 November 2018).
                    
                    
                        Rasmussen, H., Croxall, B., &amp; Otis, J. (2017). Exploring How and Why Digital Humanities is Taught in Libraries. In J. R. Eyre, J. C. Malachlan, &amp; C. Williford (Eds.), 
                        A Splendid Torch: Learning and Teaching in Today’s Academic Libraries
                        , CLIR Report 174, pp. 69 - 88. Retrieved from 
                        
                            https://clir.wordpress.clir.org/wp-content/uploads/sites/6/2017/10/pub174.pdf
                        
                         (26 November 2018). 
                    
                    Sula, C. A., Hackney, S. E., &amp; Cunningham, P. (2017). A Survey of DH Programs. Innovations in Digital Humanities Pedagogy: Local, National, and International Training. Digital Humanities 2017. Montreal, Canada.
                
            
        
    

        
            The field of African language technology (De Pauw et al. 2011; Ndinga-Koumba-Binza and Bosch 2012; Amadou Dia 2014) has seen a rapid development in recent years, and several digital humanities projects and hubs have been established across the continent. Language documentation projects have focused on several endangered and minority languages, producing large digital corpora and data sets for African under-described languages. What is the place of African languages in the African digital landscape and what is the state-of-the-art of African digital scholarship? What are the challenges and the solutions for a DH approach in the field of African languages and linguistics? What are the good practices for building African-based repositories, language infrastructures and other digital capabilities? What is a sustainable model for the engagement of wider audiences and for digital capacity building in Africa? How can we address ethical issues and the “tension” between the trend towards open access and the need to protect privacy and property rights of community speakers and researchers?
            The panel brings together scholars from different backgrounds (computational linguistics, natural language processing, language documentation and description) to answer these and other related questions and to share the experiences of scholars who are directly involved in DH research in Africa and in the management of African-based digital archives, repositories and infrastructures. In this way we hope to have a first bird’s eye-view of DH research for African languages, which will allow a critical discussion of the nature and future of the field.
            Papers:
            Tunde Ope-Davies (Opeibi), Digital Humanities, University of Lagos
            Reframing community building and civic engagement in L2 public sphere: A study of new media multilingualism in Nigeria democracy.
            One striking effect of the ongoing digital revolution is the evolving reconfiguration of the public sphere in most socio-political jurisdictions. From Europe to the Americas, and from Asia to Africa, social media is revolutionizing communications and social networking activities, redefining the mechanics of our daily interactions in private and public spaces. 
            In the last one decade and more, reforms in most Telecommunication sectors and increase in internet penetration have positively impacted communication practices in Africa. In some of these democratic contexts, the phenomenal growth in computer-mediated communication has made the practice of democracy more participatory; creating a more virile public sphere. Citizens’ online activities have expanded due to rapid growth in internet penetration and the proliferation of social media platforms now accessible through various handheld devices, laptops, and more recently affordable smartphones. 
            This present study addresses gaps in online multilingual political discourse studies in Africa, using Nigeria as a case study. First, it examines the use of social media within the Nigerian socio-political context. Second, it discusses the extent to which social media platforms have provided tools and possibilities for participatory and inclusive democratic process. 
            Pivotally, the study focuses on how the emergence of new media multilingual mechanism reshapes online political conversation and social engagement in Nigeria. It studies how the use of English and some local languages (Yoruba, Igbo, Pidgin) in online posts help to facilitate community building, social mobilization, social networking, and foster civic engagement. Also, it discusses how these new technologies absorb/adopt some offline linguistic behaviour to inspire online social networking and participation in public conversation on current and topical political issues.
            The data was drawn from the repository of the Corpus of Nigeria New media Discourse in English (CONNMDE), an ongoing Digital Humanities project at the University of Lagos. In eliciting the data deposited in the corpus, we utilized web-based corpus tools and applications to harvest relevant posts and chats from the websites, Twitter handles and Facebook pages of key political parties and political actors in Nigeria between July 2014 and October 2018 (the first phase of the project). Additional data was elicited from the online portals of three major national newspapers in Nigeria (Punch, Vanguard and the Nation).
            Relying on theoretical insights from Computer Mediated Discourse Analysis (CMDA) and Speech Accommodation Theory (SAT), the study considers language choices mediated by new technologies as strong motivation for innovative discourse strategies being deployed in this context. 
            Among others, some of our findings suggest that new media platforms now accommodate and instantiate some offline 
                socio-linguistic behaviour found in second language English-speaking contexts in Africa. For instance, some features of Speech Accommodation strategies (e.g. Giles &amp; Powesland, 1975) common in African socio-cultural contexts, and online socio-pragmatic discourse cues, now constitute a key component of communication strategies adopted in building online community and promoting civic engagement within the public sphere. 
            
            
                Emmanuel Ngué Um (University of Ngaoundéré, Cameroon)
            
            The Asynchronous relationship between Digital Archives and the discipline of Linguistics in Africa
            
                The dominant trends in present day Linguistics in Africa are both theoretical and applied. Theoretical research is geared towards either eliciting structural features of envisaged linguistic systems or the testing of grammatical theories which require little or no data for their operationalization. Applied linguistics is mostly concerned with language teaching and resorts to naturalistic language data only for the sake of illustration. Running contrary to this trend, Digital scholarship and language archiving operate on infrastructures that provide a integrated, data-conducive environment for the assembly, organization, processing and publication of research. As it is, therefore, current practices in Linguistics in Africa do not seem to be in vital need of Digital Archives. This presentation does not focus as such on the structural discrepancy between language archives and the discipline of linguistics in Africa. Instead I will attempt to interrogate the relevance of a century of academic endeavor (i.e. in the discipline of linguistics) with regard to the complexity of its object, namely Africa’s extensive multilingualism. One crucial question which needs to be asked, at this juncture, is to know the ultimate end of current linguistic research. What do linguists in Africa aim at when they set out to analyse aspects of the linguistic reality? What is the significance of the information that each piecemeal research produces? Are linguists in Africa working together towards achieving common goals or, has the discipline of linguistics become a routine in the academia which serves as justification for the continuity of an institutional business? Answers to the above questions may neither be obvious nor straightforward. However, most stake-holders will admit that our discipline, in Africa, does not pursue coherent objectives from one university to another. It could have been hoped that the spur of language documentation into Africa's linguistics over the past fifteen years and the lobby thereto of funding bodies such as DOBES and ELDP could have regenerated the linguistics scholarship in the continent. The increasing amount of data which has been harnessed from documentary projects and which are accessible via well-established and archives such as LAT (MPI-Nijmegen) and ELAR do not seem to have substantially impacted the scientific agenda of the discipline of linguistics in Africa so far. I will content in this presentation that, embracing digital archives and digital scholarship in the discipline of linguistics in Africa is tantamount to shifting from the current ad hoc, individualistic research paradigm, to communal scholarship. This entails definition of common research objectives and methodologies, from data collection and organization to data dissemination. This also entails mutualization of the scientific information and cross-verification of structural analyses from one named language data set to another, which would aim at pursuing a general understanding of the underlying experiences which justify the fact that, under apparent linguistic variation, there seems to be an abstract cultural reality which no individual grammar of a named language could help to uncover.
            
            Moses E. Ekpenyong
            Department of Computer Science, University of Uyo, Nigeria
            Intelligent Humanities: Towards High Performance Applications for African Languages
            Today’s society is witnessing the production of extreme large data sets (Bag Data) that has challenged traditional processing and storage methods. This sudden state of data explosion has indeed introduced major changes into existing data management processes, demanding a robust and sustainable solution, to efficiently process, analyze, store, share, and disseminate data into the future.
            From experience, the processing mechanism can be structured following the standard data mining pipeline (digitization -&gt; transcription -&gt; pattern recognition -&gt; simulation and inference -&gt; preservation -&gt; curation), and the degree of difficulty scales with the complexity and volume of data. Hence, massive digital objects in the humanities (e.g., large-scale corpus, images, unprocessed artifacts, audio, and videos) require suitable methods to guarantee useful extractions, for meaningful interpretations. In this contribution, we examine the role computer algorithms play in mining, shaping and representing data in the humanities. We propose an intelligent framework that maintains consistent applications that go beyond traditional methodologies, to reveal inherent patterns, trends, associations, and especially relates to human behavior and interaction – the Big Data experience. 
            Linking the past and the future constitute our Digital Humanities efforts in the University of Uyo, Nigeria – a product of an interdisciplinary cooperation between the Computer Science and Linguistics and Nigerian Languages departments. The first stage of the proposed framework is predicated on cross-domain metadata, and metadata mapping – which pose the problem of connecting existing metadata to embedded links. The key challenge however, is the metadata mapping – as heterogeneity of inputs complicates one-to-one mapping, and harmonization of the metadata and ontologies appear intractable. As such, experience and best practices are mandatory when transforming and consolidating formats into internal knowledge representation, for clustering and reasoning. Learning from existing data constitutes the second phase of our framework that drives the intelligence (imposes cluster patterns, and reasoning), required to enhance the classification process – for accurate prediction and visualization of the data sets. This at the end maximizes the use of the digital resources. Further, dissemination of the resources can be achieved through a Web Management Interface (WMI), after proper ethical and copyright procedure has been followed.
             An implementation of the proposed framework to salvage a critically endangered language, ‘Medefaidrin’ is demonstrated in this paper. A multilingual application that embeds intelligent techniques for the analysis and visualization of prosodic features of selected West African languages: Medefaidrin (Artificial, Nigeria), Ibibio (New Benue Congo, Nigeria), Igbo (Benue Congo, Nigeria), Yoruba (Niger Congo, Nigeria), and Hausa (Afro-Asiatic, Nigeria), is developed. The speech corpus adopted is the Ibadan 400 words – a list of basic (English) lexical items of any language, translated and recorded in the various languages. The developed application is Web-based and enables useful pattern discovery that reveals the dynamic nature of these languages, as well as aid the pronunciation and simulation of sentences – a necessary Computer Aided Learning (CAL) tool.
            Juan Steyn, South African Centre for Digital Language Resources &amp; Digital humanities Association of Southern Africa
            Building Sustainable Digital Infrastructures
            
                The South African Centre for Digital Language Resources (
                
                    SADiLaR
                
                ) is a new research infrastructure (RI) set up by the Department of Science and Technology (DST) forming part of the new South African Research Infrastructure Roadmap (
                
                    SARIR
                
                )
            
            The centre, which is currently still in its incubation phase runs two main programmes. A digitisation programme and a Digital Humanities (DH) programme. 
            
                The digitisation programme focusses on the creation text, audio and multimodal datasets as well as the development of NLP tools and software for the 11 official languages of South Africa. 
                The DH programme focusses on enabling and promoting the use of digital data and new methodological approaches within the broad Humanities and Social Sciences.
            
            The establishment of the 
                SARIR programme, which aims to provide "a high-level strategic and systemic intervention", provides South African researchers with a unique opportunity to develop and foster new research fields and possibilities. 
            
            This paper will share some of the challenges experienced during the current incubation period as well lessons learned by specifically reflecting on:
            
                What to keep in mind when building a new language related RI.
                Providing access to resources and tools through a RI is not enough. Why catalysing and sustaining contextualised capacity development is very important.
                
                    Where to find collaborators and training partners. The value of engaging and working with the international 
                    
                        Software, Data and Library Carpentry
                    
                     community.
                
                Successes in promoting DH approaches in African language teaching, learning and research contexts for faculty and students.
                Stories on re-defining DH from a Southern African perspective.
            
            Setting up a RI is only part of the process. The end goal is to ensure that the wider research community is able to not only access resources and tools but enabled to critically apply new skills to address the challenges of tomorrow.
            
                Felix Ameka, Leiden University
            
            Ethical issues in digital data collection and exploration
            All forms of research in the language sciences critically depend on data collection from language users. Increasingly and universally data collection methods and processing are digital. Moreover, there is the growing realisation that to understand language practices we cannot ignore the visual mode of language. This can only be captured through digital video. Furthermore, researchers are increasingly under pressure to make their data openly accessible. However, such digital data carries representative of data providers. How can we ethically go about these? In this talk, I will raise the ethical challenges involved in collecting, processing, curating, storing and exploring different forms of data. How can we carry out these activities paying attention to the principles of justice, and of beneficence/maleficence, i.e. practices should not be harmful to research participants
        
        
            
                
                    Bibliography
                    
                        Amadou Dia Ibrahima. (2014). 
                    
                    
                        De Pauw, G., de Schryver, GM., Pretorius, L. et al. (2011). Introduction to the special issue on African Language Technology. In 
                        Language Resources and Evaluation 45:263. 
                        https://doi-org.ezproxy.leidenuniv.nl:2443/10.1007/s10579-011-9157-9
                    
                    
                        DHASA (2017). Abstracts. 
                        http://dh2017.digitalhumanities.org.za/abstracts/
                    
                    
                        DHASA (2017)
                        . The Southern African context current activities and projects.
                        http://digitalhumanities.org.za/index.php/activities
                    
                    
                        DST (2016). South African Research Infrastructure Roadmap. 
                        http://www.dst.gov.za/images/Attachments/Department_of_Science_and_Technology_SARIR_2016.pdf
                    
                    
                        Giles, H. and Powesland, P. F. (1975). 
                        Speech style and social evaluation. New York: Academic Press. 
                    
                    
                        Ndimele, Ozo-mekuri. (2016). 
                        ICT, globalisation &amp; the study of languages &amp; linguistics in Africa. Port Harcourt: M &amp; J Grand Orbit Communications. 
                    
                    
                        Ndinga-Koumba-Binza, S. and Sonja E. Bosch. (2012). 
                        Language Science and Language technology in Africa: Festschrift for Justus C. Roux. Stellenbosch: SUN MeDIA
                    
                
            
        
    

        
            'Viral news' is often associated with twenty-first century social media, but early modern news behaved in a similar way. News spread faster than would be expected in either a linear or diffuse model, much like the spread of information on Twitter or the global spread of contagious disease. Recent scholarship has shown that early modern news operated as a network (Raymond, 2016: 115). Crucially, it is theorised that to operate in this way, the network could be said to adhere to a 'Barabási-Albert' model: a model for reconstructing networks using preferential attachment, which explains why in many real-world networks the vast majority of connections go to a small number of entities (Barabási and Albert, 1999: 509). Network analysis has been used by early modern scholars to examine correspondence networks between individuals, with recent work showing how network analysis can uncover both broad and specific patterns of communication. (Ahnert and Ahnert, 2019: 28). This poster shows that the same methods can be used to examine an early modern network where the nodes are cities rather than individuals. 
            This poster uses data from seventeenth news sources to examine one aspect of this network: modularity. Modularity is the tendency for a network to divide into individual components: it is a measure of the extent to which a network is a series of cliques or, on the other hand, a closely connected whole (Blondel et. al., 2008: 2). Modularity measures and 'community detection' algorithms can score any network on its degree of heterogeneity and then suggest the mostly likely divisions in that network. Understanding these divisions is key to understanding the impact and spread of early modern information. 
            Between 1641 and 1649 London saw the development of a relatively free printed news industry. This included regular news from abroad: news mostly from Europe but occasionally further afield. It has been shown that this news moved in individual 'chunks', organised on the level of the paragraph rather than issue or even article (Slauter, 2012: 253). The regular, paragraphed structure of these early 'newsbooks' means that structured data can be collected from the texts, either manually or extracted automatically. Utilising a manually-collected dataset of paragraphs of news, it is possible to analyse the underlying network of cities behind the information found in the newsbooks.
            The poster will demonstrate how the use of such a dataset with network analysis has led to the discovery of communities of news and information, specifically using network modularity and community detection to suggest the extent to which Europe could be divided into individual clusters of cities closely linked by the sharing of information, and how this can be used to understand the viral nature of early modern news.
            The network analysis shows that, at least from the perspective of London, Europe's news network was highly modular. It can be divided into eight distinct, closely-connected communities, with twenty-eight key cities holding together the entire networked system. This modularity explains how Europe's news system was highly viral and efficient. The communities found share different properties which are explored through this poster: sometimes regional and in close proximity to each other, but always joined by lines of communication rather than politics, language or religious confession. They come together to make a sketch of Europe's communities of information.
            The poster presents maps, network diagrams and sample data to illustrate the suggested communities of information in Europe, as found from London newsbook data. It suggests reasons for the individual communities, and outlines the extent to which the communities can be seen to follow linguistic, confessional or political lines, or whether they deviate from these traditional divisions. The poster explores the extent to which understanding the network as a series of closely-knit communities can explain the reasons for the viral nature of news transmission, even in an early modern setting.
        
        
            
                
                    Bibliography
                    
                        Ahnert, R. and Ahnert, S. (2019). Metadata, Surveillance and the Tudor State. 
                        History Workshop Journal, 87: 27-51.
                    
                    
                        Barabási, A. and Albert, R. (1999). Emergence of Scaling in Random Networks. 
                        Science, 286 (5439): pp. 509-512.
                    
                    
                        Blondel, V., Guillaume, J., Lambiotte, R. and Etienne Lefebvre. (2008). Fast unfolding of communities in large networks. 
                        Journal of Statistical Mechanics: Theory and Experiment, 10: 1-12.
                    
                    
                        Raymond, Joad. (2016). News Networks: Putting the ‘News’ and ‘Networks’ back in. In Moxham, N. and Raymond, J. (eds), 
                        News Networks in Early Modern Europe. Brill, pp. 102 – 129.
                    
                    
                        Slauter, Will (2012). The Paragraph as Information Technology: How News Traveled in the Eighteenth Century World. 
                        Annales. Histoire, Sciences Sociales, 67(2): 253 – 278.
                    
                
            
        
    

        
            
                In the last years, word embeddings have become important resources to deal with many Natural Language Processing (NLP) tasks (Collobert et al., 2011; Maas et al., 2011; Lample et al., 2016). Several pre-trained word vectors have been released generated with different algorithms but all based on a huge amount of contemporary texts, mainly news and Wikipedia pages but also Twitter posts and crawled web pages. 
            
            
                
                The interest towards this type of distributional approach has recently emerged also in the Digital Humanities community as proved by the organization of dedicated workshops (e.g., 
                
                    http://dariah-tda.github.io/meeting/activity/workshop/2017/12/20/CfP-Workshop-on-Embeddings.html
                
                ) and the publication of scientific articles on vectors built from historical or literary texts for tracking semantic shifts (Hamilton et al., 2016; Wohlgenannt et al., 2018; Leavy et al., 2018).
            
            
                This submission aims at expanding current research on historical word embeddings by presenting a set of English vectors pre-trained on a sub-part of the Corpus of Historical American English (COHA) (Davies, 2012) with three different algorithms. The subset of COHA we have chosen includes 36,856 texts of all the four available genres (fiction, newspaper, magazine, non-fiction) published between 1860 and 1939 for a total of more than 198 million words. We chose this specific time frame because we have a collection of travel writings of the same period of publication on which we planned to perform several NLP tasks as the one presented in the “Application” section below. In particular, this collection (
                
                    https://sites.google.com/view/travelwritingsonitaly
                
                ) contains both travel reports and guides published in a period of radical transformation in travel habits thanks to several technological, economic and sociological factors that led to the decline of the Grand Tour and the emergence of leisure-oriented travels. As for the applied models, we used the GloVe, fastText and Levy &amp; Goldberg's approaches (Pennington et al., 2014; Levy &amp; Goldberg, 2014; Grave et al., 2018). By adopting these three models, we cover different types of word representation: GloVe is based on linear bag-of-words contexts, fastText on a bag of character n-grams, and Levy &amp; Goldberg’s model on dependency parse-trees.
            
            
                Before applying these models, we lower-cased all the texts; tokenisation and dependency parsing (required by the Levy &amp; Goldberg approach) were then performed with Stanford CoreNLP (Manning et al., 2014). The training was done by considering all words appearing at least 10 times in the COHA sub-corpus and a context window size of 10. In the first phase, words are mapped to their frequency count, then a context vocabulary is created taking into consideration the context window. Our pre-trained word embeddings (called 
                HistoGlove
                , 
                HistoFast
                 and 
                HistoLevy
                ) have 300 dimensions and are publicly available online (
                
                    http://dh.fbk.eu/technologies/histo
                
                ). 
            
            
                Table 1. Examples of the most 7 similar words as induced by different embeddings
                
                    
                    HistoFast
                    HistoGlove
                    HistoLevy
                    Word2Vec
                
                
                    gay
                    merry, gayest, joyous, gaiety, gayly, gayety, light-hearted
                    
                        merry, bright, joyous, cheerful, brillant, happy, flowers
                    
                    merry, gorgeous, joyous, rosy, lively, bright, cheerful
                    
                        lesbian, bisexual, lgbt, lesbians, women, sexual, gays, homoxesual
                    
                
                
                    dancing
                    
                        dance, danced, dances, dancers, dancer, walzing, dancin
                    
                    dance, playing, danced, singing, music, dances, dancers
                    singing, bathing, skating, swimming, feasting, wrestling, chattering
                    dance, singing, dances, songs, dancers, ballroom, featuring
                
                
                    woman
                    
                        girl, madwoman, lady, irishwoman, husband, maid, she
                    
                    girl, man, women, wife, she, husband, mother
                    
                        girl, man, damsel, gentlewoman, englishwoman, youngster, creature
                    
                    man, person, girl, child, women, children, men
                
            
            
                Table 1 shows the top 7 similar words, in terms of cosine similarity, of a given set of target words (i.e., “gay”, “dancing”, and “woman”) as found in our three historical word embeddings and in Word2Vec trained on contemporary data (Mikolov et al. 2013). Among the words reported in Table 1, the main meaning shift is observed for “gay”, for which the reference to homosexuality is not present in the historical vectors. As for “dancing”, it is worth noticing that historical vectors brings out typical terms of the considered period (
                walzing
                ) and that the dependency-based approach induce similarities having the same syntactic role (that is, other gerunds, i.e. 
                singing, bathing, skating, swimming, feasting, wrestling, chattering
                ): instead of finding words having high domain similarity, Levy &amp; Goldberg model finds words with high functional similarity (Turney, 2012), thus words behaving like the target word. Terms rarely used in contemporary texts are detected for the target word “woman” as well (see the visualization of the corresponding embeddings in HistoGlove in Figure 1): e.g. 
                madwoman, damsel, gentlewoman
                . Social roles such as 
                maid
                 and 
                wife
                 does not appear in the list of the most similar words in Word2Vec, replaced by the neutral term 
                person
                .
            
            
                
                 Visualization of the embeddings of “woman”. Image created with the Embedding Projector 
                    https://projector.tensorflow.org/
                
            
            
                
                Application
            
            
                
                 Place names automatically detected and then visualized using Carto, https://carto.com/
            
            
                Our embeddings can be useful resources for the development of NLP tools aiming at processing historical texts with neural architectures (Sprugnoli and Tonelli, 2019). For example, we applied them to the recognition of place names of different types (e.g. “Vesuvius”, “Venice”, “Forum Romanum”) in English historical travel writings on Italy (Sprugnoli, 2018). The deep learning architecture we adopted (Reimers and Gurevych, 2017), using a small set of in-domain training data (100,000 tokens), the HistoGlove embeddings and no feature engineering, outperformed both the CoreNLP CRF (
                Conditional random fields)
                 model retrained with the same dataset and the same neural architecture employing bigger vector spaces pre-trained on contemporary texts. Our best model achieves a precision of 86.4, a recall of 88.5 and an F-measure of 87.5. Figure 2 displays the place names, related to the center of Florence, automatically detected in the tenth chapter of “Florence and Northern Tuscany with Genoa” (Hutton, 1908).
                
            
        
        
            
                
                    Bibliography
                    Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K. and Kuksa, P. (2011). Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug), pp.2493-2537.
                    Maas, A.L., Daly, R.E., Pham, P.T., Huang, D., Ng, A.Y. and Potts, C. (2011). Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1 (pp. 142-150). Association for Computational Linguistics.
                    Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. and Dyer, C. (2016). Neural Architectures for Named Entity Recognition. In Proceedings of NAACL-HLT (pp. 260-270).
                    Pennington, J., Socher, R. and Manning, C. (2014). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).
                    Grave, E., Bojanowski, P., Gupta, P., Joulin, A. and Mikolov, T. (2018). Learning word vectors for 157 languages. arXiv preprint arXiv:1802.06893.
                    Hamilton, W.L., Leskovec, J. and Jurafsky, D., (2016). Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (Vol. 1, pp. 1489-1501).
                    Wohlgenannt, G., Chernyak, E., Ilvovsky, D., Barinova, A. and Mouromtsev, D. (2018). Relation Extraction Datasets in the Digital Humanities Domain and their Evaluation with Word Embeddings. In Proceedings of CICLING 2018, Hanoi, Vietnam.
                    Leavy, S., Wade, K., Meaney, G. and Greene, D. (2018). Navigating Literary Text with Word Embeddings and Semantic Lexicons. In Proceedings of the Workshop on Computational Methods in the Humanities (COMHUM 2018).
                    Davies, M. (2012). Expanding horizons in historical linguistics with the 400-million word Corpus of Historical American English. Corpora, 7(2), pp.121-157.
                    Levy, O. and Goldberg, Y. (2014). Dependency-based word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (Vol. 2, pp. 302-308).
                    Manning, Christopher D., Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. (2014). The Stanford CoreNLP Natural Language Processing Toolkit In 
                        Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60.
                    
                    Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In 
                        Advances in neural information processing systems (pp. 3111-3119).
                    
                    Peter D. Turney. (2012). Domain and function: A dual-space model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533–585.
                    Reimers, N., &amp; Gurevych, I. (2017). Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging. In 
                        Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 338-348).
                    
                    Hutton, E. (1908). 
                        Florence and Northern Tuscany with Genoa. Methuen.
                    
                    Sprugnoli, R. and Tonelli, S. (2019). Novel Event Detection and Classification for Historical Texts. Computational Linguistics, Vol. 45, No. 2, June 2019, pp.1-38. 
                    Sprugnoli, Rachele. (2018). Arretium or Arezzo? A Neural Approach to the Identification of Place Names in Historical Texts. In 
                        Proceedings of the Fifth Italian Conference on Computational Linguistics (CLiC-it 2018), Torino, Italy, December 10-11, 2018.
                    
                
            
        
    

        
            A paramount challenge in present-day humanities knowledge production is to communicate research results in ways that aligns with increasingly digital research workflows. The OpenMethods metablog aims to explore and deliver a solution for this need in a Digital Humanities (henceforth DH) context. It provides a platform to bring together all formats of openly available digital publications. To this end the OpenMethods metablog provides a convenient and easy way for DH experts from around the globe to select, propose, curate, and highlight online published content. Suitable online content may be proposed by Community Volunteers. The OpenMethods platform is intentionally interdisciplinary and multilingual to facilitate a timely disclosure and spread of knowledge and to raise peer recognition for the related research results. The group of DH experts, known as the OpenMethods Editorial Team, currently comprises 23 editors from 11 countries. 
            The platform has been developed in partnership with the DARIAH community since it is an offspring of the DARIAH “Humanities at Scale” project (Engelhardt et al. 2017). 
            OpenMethods offers a collaborative model of open scholarly communication that is growing out of community practices and needs in DH (e.g. Spiro 2012). OpenMethods goes beyond traditional paper-based communication practices in several ways, as explained below.
            1. OpenMethods is dedicated to the critical discussion of DH tools and methods. Digital tools and methods are genuine research outputs whose academic recognition is still lagging behind monographs and journal articles that serve as the highest value currency in current academia (Schreibman, Mandell, and Olsen (2011)). In addition to spreading the knowledge and raising peer recognition for existing digital resources, a core mission of the project is also to facilitate the culture of reuse of these materials. 
            2. OpenMethods is inclusive with a variety of content types like blog posts, videos, preprints, podcasts, etc. These are becoming increasingly important aspects of scholarly work (Dacos and Mounier (2010)) as they are not only accelerating discussions within and outside of academia but are also flexible enough to follow the dynamic and multimodal nature of DH methodology discourse.
            3. One of the key aim of the platform is helping researchers in navigating through the rich and dynamically evolving DH landscape to find the most relevant tools and methods for their research. This is achieved via a novel form of expert community review. As an enrichment of the preselected valuable Open Access publications, successful nominations are categorised with tags based on TaDiRAH and a brief introduction in English is added to each post in which one of our Editors explains the relevance of the republished content to the DH practices. The multilingual character of the platform allows for the representation of multiple languages and cultures in the DH discourse yet currently this possibility is not exploited to its full extent.
            4. The platform not only propagates the culture of reuse but has been built on reused tools itself. It is based on a WordPress CMS and the PressForward plugin. It enables us to create a simple workflow for our experts: almost all steps of their work (content nomination, discussion, review, publishing, keeping track of published content) can be undertaken within this plugin. Besides, we are constantly seeking ways to put DH tools in service of more effective content discovery and enrichment. For this purpose, we have created plugins to achieve interoperability with the entity recognition NERD service and the research discovery platform ISIDORE.
            Our goal is to reach and engage the widest possible range of DH communities, from scholars taking the first steps towards going digital to DH experts who are shaping specific research areas as representatives for particular methods.
            In achieving its goals OpenMethods faces many difficulties. In our poster we wish to highlight a number of these challenges:
            
                Reaching a critical number of readership as a result of focused outreach strategy (like via our Twitter channel, presenting the platform on DH conferences).
                Finding solutions for long-term incentivisation of the editors, ensuring that editors are recognised for their contributions, and sustaining a viable pool of reviewers.
                Establishing bidirectional exchange between traditional journal publishing and novel components in scholarly communication such as blogging.
            
            Our poster presentation will allow us to solicit the widest possible feedback from DH communities. To this end it will not only explain the aims and strategies of OpenMethods, but will also include an interactive demo. We also wish to encourage the conference attendees to join and expand the OpenMethods network, explore its potentials for advancing their own research methods and participate in the development of the platform.
        
        
            
                
                    Bibliography
                    
                        Dacos, M., and Mounier, P. (2010). 
                        Les carnets de recherche en ligne, espace d’une conversation scientifique décentrée. Albin Michel.
                    
                    
                        Schreibman, S., Mandell,L., and Olsen, S. (2011). Introduction. 
                        Profession
                        (79): pp.123–35. DOI: 10.1632/prof.2011.2011.1.123.
                    
                    
                        Spiro, L. (2012). „This Is Why We Fight“: Defining the Values of the Digital Humanities. In Gold, Matthew K.(eds), 
                        Debates in the Digital Humanities. Minneapolis 2012, pp. 16-35.
                    
                
            
        
    

        
            
                Introduction
                With the rise of the Internet, it has been pointed out that the Babylonian Talmud, with layers of texts commenting and referring to other texts, was an ancient hyper-text (Jerry Waxman, 1993; David Porush, 1998) Now, with the rise of social media, we can start to think of the Talmud as an ancient social network. True, Abaye and Rava did not use Facebook or Twitter. Rava cites Rav Nachman rather than retweeting him or sharing his status update. Although Abaye didn’t “friend” Rava, they studied under a shared teacher, Rabba, both headed the academy at Pumpedita, and they frequently take opposing positions and argue with one another.
                We set out to create a social network graph of the Babylonian Talmud. Using Named Entity Recognition of a statement-aligned Aramaic / English bitext, we found nodes (rabbis) and edges (scholastic interactions, such as citation). We induced formal scholastic relationships from these interactions, and propagated biographical information across the graph. We highlighted rabbis in the Talmudic text, using color to indicate scholastic generation, and displayed relevant subgraphs to show how the participants on a folio relate to one another both locally and globally.
                This biographical and scholastic information can aid a student of Talmud in understanding the dynamics of the discourse and why specific rabbis say what they say. For instance, the modern Revadim approach to Talmud (Hayman 2004) study makes great use of which generation a Tanna or Amora lived, which academy he belonged to, and so on. 
            
            
                Related Work
                The study of social network graphs of scholastic relationships has occurred in other, non-Talmudic, contexts. See for example Kofia et al. (2015).
                For the Babylonian Talmud, many printed works and a few existing digital resources can prove useful to identifying rabbis and their interactions. Satlow (2017) created a spreadsheet with approximately 5000 rabbinic names listed by Aaron Hyman in 
                    Toldot Tanaʾim ṿe-Amoraʾim (1910), as an intended first step to performing social network analysis of rabbinic literature. Parker (2009) assembled a database of approximately 250 Tannaim and Amoraim, their scholastic generations, and teacher / student relationships between them. Sefaria is an open and free online library of rabbinic literature. Among the items in their database is a digital version of the Koren Noé Talmud, with the Aramaic and the English translation aligned statement by statement or paragraph by paragraph, and with linked commentary. 
                
            
            
                Our Approach
                We performed Named Entity Recognition on the statement-aligned bitext corpus to mark up the scholars and their interactions, in both the Hebrew and English text. Such NER is challenging on purely Hebrew text (due to such ambiguities as where a name begins and ends), but we exploited the aligned text and a transliteration model to attain fairly accurate results. We used customized fuzzy string matching to match these names to those in the Parker database, when the names are spelled differently or have a patronymic. 
                 The Talmud consists of 5,894 folio pages, of which we have processed 4965 (84%), to recognize approximately 145,000 named entities and 12,000 interactions. Testing on a small tractate consisting of 59 folio pages, for just the English named entities, we achieve 81% precision, 94% recall, and an F1 score of 87%. For pairs of (English, Hebrew) in the literal text, performance is slightly lower (76% precision, 89% recall, 82% F1). 
                We built two graphs. The first was based on Parker’s data. The second graph was populated from the NER- extracted names and interactions. Rabbis are represented as nodes, and weighted edges exist for each type of interaction (such as inquires, cites, speaks_to, and so on). Where possible, we transfered generational information from the Parker data, which sometimes required disambiguation based on context. For instance, “Rabbi Eleazar” would refer to Rabbi Eleazar ben Shamua in a Mishna or 
                    brayta but usually Rabbi Eleazar ben Pedat, an Amora, otherwise.
                
                Because many rabbis are not encoded in Parker’s database, our graph includes nodes not marked for scholastic generation. We are exploring a few approaches to bootstrap off our initial generational knowledge, using our detected relationships. As an example, if known scholar A, of generation 3, cites unknown scholar B, then we can assume that B is 
                    probably of generation 1 or 2, that is, a near but previous generation. If known scholar A speaks to unknown scholar B, and B replies, they are probably within a generation of one another. By considering these constraints and iterating, we can propogate generational knowledge across the graph.
                
                We have begun to summarize interactions as scholastic relationships (e.g., if A often cites/inquires of B, then A is likely B’s student). Other identifiable relationships are primary teacher, colleague, and frequent disputant. 
            
            
                Results and Conclusions
                The current release version of the system is available at 
                    www.mivami.org. Here, we include a few figures to illustrate the system output, drawn from Menachot 2b.
                
                
                    
                
                
                    Figure 1 is a single Talmudic statement. The literal translation is bolded while the gloss text is not. Rabba is colored red and Abaye green, matching their Amoraic generation as given in a legend (A3 and A4). Rabbi Shimon is colored as a 5th generation Tanna. Hover text presents patronymic and generation. 
                
                
                    
                
                
                    Figure 2 is the teacher/student relationship graph of that statement, again color coded. Circles with outlines are rabbis who appear on the page. Circles without outlines are shared teachers. Thus, we see that Abaye and Rava are contemporaries (A4) and were both students of Rav Yosef and Rav Nahman. 
                
                
                    
                
                
                    Figure 3 displays local interactions; the bidirectional arrow shows Rabba and Abaye speaking to one another.
                
                
                    
                
                Finally, 
                    Figure 4 shows the rabbis local to the page, as they interact across the entire Talmud. Rava both speaks to and cites Rabba, making it apparent that they interacted face-to-face with Rabba as teacher.
                
                Together, these graphs provide valuable insight into the meaning of the texts and dynamics of Talmudic discussions by highlighting relevant biographical information that is not readily accessible on the page of the Talmud. 
            
        
        
            
                
                    Bibliography
                    Pinchas Hayman, Uncovering the Layers: The Revadim Method, 
                        Jewish Educational Leadership, volume 3, 2004
                    
                    Kofia et al, Social Network: a Cytoscape app for visualizing co-authorship networks, F1000 Research, v.4; 2015
                    Joshua Parker, 
                        
                            http://www.joshua-parker.net/sages/
                        
                    
                    David Porush, Talmud and Hypertext, 1998, 
                        
                            http://kairos.technorhetoric.net/3.1/coverweb/porush/contra4.html
                        
                    
                    Michael Satlow, 
                        http://mlsatlow.com/2017/08/08/naming-rabbis-a-digital-list/
                    
                    Jerry Waxman, Hypertextual Systems: Antecedents and Implications, AOJS Conference, Spring Glenn N.Y., August 1993
                
            
        
    

        
            This short paper will examine notions of, and practices in relation to, digital data in museums. On the one hand, it will discuss how digital data are conceptualized, identified, and considered in museums; on the other hand, it will explore how they are produced, collected, curated, shared, and preserved within the heritage sector.
            Digital data have now a liminal position in museums, where they are increasingly being recognized as part of 21
                st century heritage. Despite the UNESCO 
                Charter on the Preservation of Digital Heritage dating to 2003, and a CIDOC Digital Preservation Working Group active since 2006, museums have begun to recognise the value of preserving digital data as sources and representations of contemporary heritage only in recent years. For example, MoMA announced its first collection of videogames in 2012; at the Museum of London, in 2017, the installation 
                Pulse tracked social media and displayed live what Londoners were tweeting; in 2018, a major exhibition on videogames was held also at the Victoria and Albert Museum in London; and, outside of the museum sector, the Daily Show programme (on Comedy Central) organised the pop up 
                The Donald J. Trump Presidential Twitter Library, which has now visited a few North American cities and it is the subject of a virtual museum. However, this recent attention for digital data as potential museum objects implies also a reconsideration of their position within museum taxonomies, where in the past they have mostly been treated as auxiliary information and interpretative support.
            
            In addition, user generated content related to the museum and its collections, and generated by visitors during museum visits and by online audiences on social media, will be considered also for its potential in expanding object biographies (Kopytoff, 1986; Hill, 2012). Previous research has focused on participatory practices and digital engagement (Simon, 2010; Adair et al., 2011; Giaccardi, 2012; Kidd, 2014) and the evaluation of digital programmes (Villaespesa, 2016), although there are not yet clear frameworks and methodologies for harvesting, managing, and using this data (Marstine, 2011; Kidd and Cardiff, 2017). Less work has been done on the exploration of objects online biographies, as emerging from the museum digitisation practices and its collection management systems, and continuing through social media photos and digital engagement material, and this paper will highlight some of the emerging challenges.
            While the inclusion of digital data in collections might prompt a redefinition of the values and position of digital heritage in a museum and might provoke new questions on the digital lives of museum objects, this data causes also a series of curatorial and methodological challenges. Firstly, this data will come in different forms, each one presenting different technical challenges in relation to their collection, archiving, and representation. Secondly, it might even be part of assemblages of digital and analogue items which, together, represent the heritage of 21st century events (e.g. a political protest leaves behind social media posts as well as placards). The acquisition of both physical and digital material within a museum system poses a series of additional challenges to its ontologies and vocabularies, which are not prepared to acquire born digital materials in the same digital infrastructure as that of more ‘traditional’ museum objects.
            Throughout the discussion, the paper will observe how museum approaches to digital collecting and to digital preservation diverge or complement existing practices in parallel fields. The idea that history and heritage are now increasingly shared and produced online, and thus we ought to preserve digital outputs and research the circulation of news, opinions, and debates in the digital sphere has a longer story in the field of digital humanities, and web archiving in particular (Rosenzweig, 2004; Graham et al., 2015; Brügger, 2013, 2017; Giaccardi and Plate, 2016; Winters, 2017). Hence, the paper will observe how discussions and projects in the digital humanities field could be productively inspire new forms of curating in museums, in order to improve practices in relation to the acquisition, recording, management, and preservation of this data. Similarly, the field of digital ethnography (Hine, 2008; Kozinets, 2011; Pink et al., 2016) and research in social sciences have emphasised the values of collecting, analysing, and eventually preserving our online lives. For the purposes of this paper, it is particularly the ethical discussions around the collection of contemporary data which are of interest.
            In conclusion, the paper will focus on the one hand on questions on the collection, management, and use of digital data, which are increasingly crucial for future museum curating practices. On the other hand, the paper will discuss how the repositioning of digital data as heritage raises new questions in relation to the materiality and authenticity of the ‘digital’, to the politics and impact of co-production and knowledge creation online, to the management of digitised and born-digital content, and to the ethics of collecting digital data and its consequent implications.
        
        
            
                
                    Bibliography
                    
                        Adair, B., Filene, B. and Koloski, L. (2011). Letting Go? Sharing Historical Authority in a User generated World. Philadelphia: The Pew Center for Arts and Heritage.
                    
                    
                        Brügger, N. (2013). Web historiography and Internet Studies: Challenges and perspectives. 
                        New Media &amp; Society, 15(5): 752 – 764.
                    
                    
                        Brügger, N. and Schroeder, R. (2017). The Web as History: Using Web Archives to Understand the Past and the Present. London: UCL Press.
                    
                    
                        Giaccardi, E. (2012). Heritage and Social Media: Understanding Heritage in a Participatory Culture. Oxford and New York: Routledge.
                    
                    
                        Giaccardi, E. and Plate, L. (2016). How memory comes to matter: From social media to the internet of things. In Muntean, L. Plate, L. and Smelik A. (eds), 
                        Materializing Memory in Art and Popular Culture. Taylor and Francis Group, pp. 65-88.
                    
                    
                        Graham, S., Milligan, I. and Weingart, S. (2015). Exploring Big Historical Data: The Historian’s Macroscope. London: Imperial College Press.
                    
                    
                        Hill, K. (2012). Museums and biographies: stories, objects, identities. Woodbridge: Boydell.
                    
                    
                        Hine, C. (2008). Internet Research as Emergent Practice. In Hesse, Biber S.N. and Leavy, P. (eds), 
                        Handbook of Emergent Methods. New York: Guilford Press, pp.525 – 542.
                    
                    
                        Kidd, J. (2014), Museums in the New Mediascape: Transmedia, Participation, Ethics. London: Routledge.
                    
                    
                        Kidd, J. and Cardiff, R. (2017). ‘A space of negotiation’: Visitor Generated Content and Ethics at Tate. 
                        Museums &amp; Society, 15(1): 43-55.
                    
                    
                        Kopytoff, I. (1986). The cultural biography of things: commoditization as process. In Appadurai, A. (ed), 
                        The Social Life of Things: Commodities in Cultural Perspective. Cambridge: Cambridge University Press, pp. 64-91.
                    
                    
                        Kozinets, R. V. (2010). Netnography: doing ethnographic research online. London: SAGE.
                    
                    
                        Marstine, J. (ed) (2011). Routledge Companion to Museum Ethics: Redefining Ethics for the Twenty- First Century Museum. London and New York: Routledge.
                    
                    
                        Pink, S., Horst, H., Postill, J., Hjorth, L., Lewis, T. and Tacchi, J. (2016). Digital Ethnography: principles and practice. London: SAGE.
                    
                    
                        Rosenzweig, R. (2004). How will the net’s history be written? Historians and the internet. In Nissenbaum, H. and Price, M. E. (eds), 
                        Academy &amp; the Internet. New York: Peter Lang, pp. 1–34.
                    
                    
                        Simon, N. (2010). The Participatory Museum. Santa Cruz: Museum 2.0.
                    
                    
                        Villaespesa, E. (2016). Measuring Social Media Success: The Value of the Balanced Scorecard as a Tool for Evaluation and Strategic Management in Museums. University of Leicester: PhD thesis.
                    
                    
                        Winters, J. (2017). Breaking in to the mainstream: demonstrating the value of internet (and web) histories. 
                        Internet Histories, 1(1-2): 173 – 179.
                    
                
            
        
    

      
         Annotation natürlicher Sprachdaten aus sozialen Medien zur
        Erforschung zeitgenössischer Szenen, zur Sprach- und Trendanalyse und zur
        Weiterentwicklung von Sprachtechnologien gewinnt mit der zunehmenden Verfügbarkeit
        großer Datenbestände weiter an Bedeutung (Farzindar / Inkpen 2015). Zeitgenössische
        Kommunikation in sozialen Medien verfügt über inhaltliche und strukturelle
        Besonderheiten und ist von umgangssprachlicher Ausdrucksform geprägt. Beiträge, die
        im Kontext internetbasierter Diskussionskulturen in Foren entstehen, stellen eine
        wichtige Forschungsquelle dar. Diese nutzergenerierten Texte, in Form von semi- oder
        unstrukturierten Kommentaren, repräsentieren Meinungen und Bewertungen einer
        Gemeinschaft zu einem Thema, Produkt oder Werk und beziehen sich in der Regel auf
        inhaltliche, technische oder ästhetische Aspekte. Die Autoren verwenden dabei
        Sprachmittel wie Metaphern, Analogien, Ambiguität, Humor und Ironie sowie
        metalinguistische bildhafte Mittel wie Emoticons oder andere graphische Zeichen
        (Reyes et al. 2012).
         Vor diesem Hintergrund adressiert dieses Projekt Herausforderungen, die bei der linguistischen und statistischen Verarbeitung von realen web-basierten Daten entstehen. Es wird ein Ansatz semi-automatischer Annotation zur Extraktion von Begriffen für die ontologiebasierte Beschreibung von computergenerierten audiovisuellen Kunstwerken einer digitalen Kunstszene präsentiert. Forschungsgegenstand ist die Diskussionskultur der Demoszene, einer spezialisierten Computerkunstszene. Bisher sind die zahlreichen Beiträge der Gemeinschaft, die sich auf ästhetische und technische Aspekte der Kunstwerke beziehen, nicht erschlossen. Bei diesen Beiträgen handelt es sich um informelle, emotionale, kurze und unstrukturierte Kommentartexte. Das verwendete Vokabular ist mehrsprachig und beinhaltet fachspezifische Terminologien, exklusive Neologismen und einen eigenen szenespezifischen orthographischen Stil. Diese Beiträge bieten detaillierte Einblicke in die Charakteristika der Werke, weshalb ihre Erschließung deren Verständnis fördert und eine gezielte Recherche einzelner Werke ermöglicht. Das Projekt befasst sich mit der Fragestellung, in wieweit sich aktuelle Verfahren der natürlichen Sprachverarbeitung (NLP), die auf grammatikalisch korrekte Schriftformen optimiert und auf Zeitungskorpora trainiert sind, anwenden lassen. Somit leistet das präsentierte Projekt einen Beitrag im Bereich der Entwicklung von Ansätzen zur Aufbereitung großer textbasierter Datenbestände sowie der Erforschung des Sprachgebrauchs zeitgenössischer digitaler Kunstszenen, aber auch hinsichtlich Nutzung semantischer Technologien.
         Die Anwendung von NLP-Verfahren für textbasierte Kommunikation
          in soziale Medien bedarf einiger Anpassungen an die sprachlichen Besonderheiten
          (Maynard 2012). Die Nutzung standardisierter Techniken ist bisher nur wenig
          erfolgversprechend (Gimpel 2011; Finin 2010). Bestehende Frameworks, wie das Natural
          Language Toolkit (NLTK, vgl. Bird et al. 2015), bieten die Möglichkeit der
          Implementierung eines individuellen NLP-Prozesses, bei dem verschiedene
          Verarbeitungsschritte modular integriert und miteinander kombiniert werden können.
          Für das vorliegende Projekt wurde eine Pipeline konzipiert und implementiert, die
          die Generierung von Annotationsebenen, begonnen mit der Tokenisierung und
          Part-of-Speech Tagging bis hin zur Extraktion von relevanten werkbeschreibenden
          Begriffen umfasst. Zur Evaluation des entwickelten Ansatzes wird ein regelbasiertes
          überwachtes Experiment mit einer definierten Teilmenge von 1255 Kommentaren
          durchgeführt. Es lässt sich feststellen, dass Emoticons und Partikeln falsch
          verarbeitet werden. Darüber hinaus werden auch Nomen, Verben und Adjektive,
          insbesondere Gerundien häufig falsch annotiert. Das Experiment zeigt, dass die
          konzipierte Pipeline für das vorliegende Kommentarkorpus iterativ optimiert werden
          muss. Der generierte Index werkbeschreibender Terminologie wird ferner für die
          Erweiterung einer domainspezifischen Ontologie zur Unterstützung semantischer
          Annotation verwendet. Hierfür wird ein Ansatz für das Lernen von Ontologien aus
          Texten verfolgt, wobei die ermittelten Begriffe als Kandidaten für Instanzen
          beschrieben werden. Als Referenzontologie wird eine auf CIDOC CRM-basierte Adaption
          verwendet (Hastik et al. 2013).
         Dieses Projekt präsentiert einen innovativen Ansatz, um mit NLTK Kommentartexte aus Onlineforen der Demoszene zu annotieren. Das Standard-Tagset muss jedoch angepasst werden. Die Erweiterung der CIDOC CRM-basierten Ontologie auf Basis des generierten Indexes ermöglicht die semantische Beschreibung der Werke.
      
      
         
            
               Bibliographie
               
                  Bird, Steven / Klein, Ewan / Loper, Edward (2015):
              Natural Language Processing with Python. NLTK
              Book http://www.nltk.org/book/
              [letzter Zugriff 15. Februar 2016].
               
                  Farzindar, Atefeh / Inkpen, Diana (2015): Natural Language Processing for Social Media. San
              Francisco: Morgan & Claypool.
               
                  Finin, Tim / Murnane, Will / Karandikar, Anand / Keller,
                Nicholas / Martineau, Justin (2010): "Annotating Named Entities in
                Twitter Data with Crowdsourcing", in: Proceedings of the
                NAACL HLT 80–88. 
               
                  Gimpel, Kevin / Schneider, Nathan / O'Connor, Brendan /
                  Dipanjan, Das / Mills, Daniel / Eisenstein, Jacob / Heilman, Michael /
                  Yogatama, Dani / Flanigan, Jeffrey / Smith, Noah A. (2011):
                  "Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments",
                  in: Proceedings of the 49th Annual Meeting of the
                  Association for Computational Linguistics 42-47. 
               
                  Hastik, Canan / Steinmetz, Arnd / Thull, Bernhard
                  (2013): "Ontology based Framework for Real-Time Audiovisual Art", in: IFLA World Library and Information Congress. 79th
                  IFLA General Conference and Assembly: Audiovisual and Multimedia with
                  Cataloguing http://library.ifla.org/87/1/124-hastik-en.pdf [letzter Zugriff
                  15. Februar 2016]. 
               
                  Maynard, Diana / Bontcheva, Kalina / Rout, Dominic
                  (2012): "Challenges in Developing Opinion Mining Tools for Social Media",
                  in: Proceedings of @NLP can u tag #usergeneratedcontent?!
                  Workshop at International Conference on Language Resources and
                  Evaluation (LREC 2012) 8.
               
                  Reyes, Antonio / Rosso, Paolo / Buscaldi, Davide
                  (2012): "From Humor Recognition to Irony Detection: The Figurative Language
                  of Social Media", in: Data Knowledge Engineering.
                  Applications of Natural Language to Information Systems 74: 1-12. 
            
         
      
   



      
         
            Projekt
            Das DFG-Projekt Future Publications in den Humanities (Fu-PusH)
        untersuchte die Potenziale des digitalen Publizierens in den
        Geisteswissenschaften und erarbeitete anhand von Szenarien Handlungsempfehlungen
        für akademische Infrastruktureinrichtungen wie insbesondere
        Universitätsbibliotheken und Rechenzentren, um Publikationsprozesse zu
        unterstützen und dabei den funktionalen Anforderungen unterschiedlicher
        geisteswissenschaftlicher Fachrichtungen gerecht zu werden.
            Auf dem Poster werden zum einen die Ergebnisse der Studie beschrieben und zum anderen ein speziell in diesem Projekt entwickeltes Recherche-Tool zur Auswertung qualitativer Interviews (Statement Finder) vorgestellt, das als niedrigschwelliges Open-Source-Tool der Community zur Verfügung gestellt wird. Abschließend werden eine Reihe von Handlungsempfehlungen formuliert für die an digitalen Publikationsprozessen beteiligten Akteursgruppen, namentlich für geisteswissenschaftliche Fachgemeinschaften darunter insbesondere die Digital-Humanities-Community, für Infrastruktureinrichtungen wie Bibliotheken und Rechenzentren, für Wissenschaftsverlage sowie für Förderinstitutionen und für die Wissenschaftspolitik.
         
         
            Ergebnisse
            Die Ergebnisse des Fu-PusH-Projektes zeigen sehr deutlich die Unterschiede im Forschungs- und Publikationsverhalten sowohl zwischen den Geisteswissenschaften und den Naturwissenschaften als auch innerhalb des disziplinären Spektrums der Geisteswissenschaften selbst. Dies betrifft insbesondere die Zurückhaltung gegenüber der Nutzung digitaler Publikationsmedien, auch angesichts ihrer anerkannten Potenziale.
            Das
          Publikationsverhalten in den Geisteswissenschaften orientiert sich nach wie vor weitgehend an traditionellen Formen aus der Printkultur wie Monografien, Sammelbandbeiträge, Zeitschriftenaufsätze sowie Rezensionen. Wo digital publiziert wird, folgt man den etablierten Modellen der Verlagspublikation in einem dem Printparadigma möglichst ähnlichen Format. Hier sind auch perspektivisch nur geringe oder selektive Änderungen und Optimierungen zu erwarten. Die Einbindung von multimedialen Erweiterungen wird auf der Materialebene durch urheberrechtliche Bedingungen und auf der technischen Ebene durch den Mangel an Standards und niedrigschwelligen Lösungen eingeschränkt. Bisher lässt sich am ehesten die Form des Bloggens als dauerhafte zusätzliche Variante für die wissenschaftliche Kommunikation bestimmen.
        
            Das Publizieren nach dem
          Open-Access-Prinzip scheint in den Geisteswissenschaften geringer ausgeprägt als in den Naturwissenschaften. Dafür lassen sich mehrere Gründe identifizieren. Zum einen fehlen an vielen Stellen bislang fachwissenschaftlich etablierte Infrastrukturen. Zum anderen genießen rein digitale Publikationen nach wie vor keinen guten Ruf, was sich beispielsweise auf die Kreditierung des Forschungsoutputs auswirkt. Schließlich wirkt im Vergleich zu naturwissenschaftlichen Publikationen der Zugangsdruck zu Neuerscheinungen an vielen Stellen durch längere Forschungszeiträume und geringere Kosten weniger stark.
        
            Auffällig ist, dass sich stärker in Schnittstellen mit Naturwissenschaften
          befindliche und internationalisierte Disziplinen (z. B. Sprachwissenschaften,
          Archäologie) deutlich aktiver in dieser Richtung entwickeln, als die vorwiegend
          hermeneutisch-interpretativ arbeitenden Fächer. Eine Nutzung von frei
          zugänglichen Materialien erfolgt dagegen fächerübergreifend. Es existiert beim
          Open Access also eine Diskrepanz zwischen Publikations- und Rezeptionsverhalten. 
            In einigen Bereichen vor allem unter dem Einfluss der
            Digital Humanities finden sich jedoch auch stärker digital orientierte Entwicklungen. Eine Erklärung lautet, dass viele Formen dieser Wissenschaft überhaupt erst durch digitale Technologien realisierbar werden. Dort wo größere Datenmengen flexibel verarbeitet werden müssen, etwa in der Editionswissenschaft oder der Computerlinguistik, finden sich bereits stärker etablierte Formen der digitalen Forschung und des digitalen Publizierens, die – sehr selektiv – auch auf anderen Fachbereiche inspirierend einwirken. Eine Zwischenform zwischen Publikation und Forschung, der vergleichsweise viel Potential zuerkannt wird, ist das digitale
            Annotieren. Damit zusammenhängend wird das größte Zukunftspotenzial des digitalen Publizierens im Bereich der
            digitalen Editionen gesehen, die häufig zugleich als mögliche Hybridausgaben zur differenzierten Rezeption wie auch als digitales Forschungsdatum zur weiteren Verarbeitung gesehen werden.
          
            Die Nutzung von
            Social-Media-Anwendungen scheint sich in vielen Bereichen der Geisteswissenschaften weitgehend auf die Vernetzung durch soziale Wissenschaftsnetzwerke oder Kurznachrichtendienste (Twitter) zu beschränken. Mit Hypotheses.org etabliert sich allerdings nach und nach eine Blogplattform, die sich durchaus ein gewisses Renommee aufbaut. Das ist insofern der relevante Schritt, weil eine zentrale Hürde bei der Nutzung solcher Medien die bisher fehlende Kreditierbarkeit für wissenschaftliche Karrieren darstellt. Zudem existiert die Sorge, dass frei auf solchen Wegen zum Beispiel vor einer “ordentlichen” Publikation publizierte Ergebnisse von Anderen übernommen und verwertet werden.
          
            Bei vielen Aspekten vernetzter und digitaler Forschung bzw. des interaktiven Publizierens zeigt sich, wie sehr wissenschaftskulturelle Aspekte der Nutzung bestimmter technologischer Formen entgegenstehen. Das betrifft insbesondere den Aspekt der Kollaboration, der Voraussetzung für den sinnvollen Einsatz
            virtueller Forschungsumgebungen ist. Hier findet sich nur eine geringe Nutzungsbereitschaft. Es ist zu vermuten, dass sowohl wissenschaftskulturelle Gepflogenheiten als auch eine vergleichsweise komplexe Nutzbarkeit die Akzeptanz und Nutzung solcher Angebote bremsen. Zweckmäßiger erscheinen hier einfache, modularisierte und miteinander verknüpfbare Lösungen.
          
            Herausforderungen werden generell bei Fragen der technischen
            Standardisierung zur Gewährleistung von
            Interoperabilität deutlich. Dies betrifft sowohl die Werkzeuge als auch die digitalen Forschungsdaten. Zudem zeigen sich wahrgenommene Risiken, die generell von Technologien im Kontext der Digital Humanities ausgehen. Zum einen liegen bisher kaum Erfahrungswerte vor, mit denen sich eine tatsächliche Relevanzbewertung von Informationsinfrastrukturen bzw. Publikationsszenarien vornehmen lässt. Zum anderen besteht die Gefahr, dass neue technische Dispositive bestimmte Forschungs- und Erkenntnispraxen begünstigen und dafür andere weniger angemessen berücksichtigen.
          
         
      
      
         
            
               Bibliographie
               
                  Universitätsbibliothek der Humboldt-Universität zu
              Berlin (o. J.): Fu-PusH. Future Publications
              in the Humanities https://www2.hu-berlin.de/fupush/ [letzter Zugriff 06. Januar
              2016].
            
         
      
   



      
         
            Einleitung
            Die Datenverarbeitung innerhalb der Geisteswissenschaften ist sehr eng mit den
          gegenwärtigen technologischen Entwicklungen verbunden und dementsprechend auch
          stark davon abhängig. Ein sehr gutes Beispiel dafür ist das Gebiet der
          Dialektologie / Dialektometrie. Klassische Dialektometrie ist eine
          Forschungsrichtung innerhalb der Linguistik, die sich mit der Erforschung
          möglichst hochrangiger Ordnungsstrukturen in sprachgeographischen Netzen
          beschäftigt. Diese Aufgabe wurde bislang hauptsächlich durch die Analyse
          gesprochener Sprache (z. B. akustische Aufnahmen) oder der sogenannten
          Fragebögen (z. B. gezielt abgefragte, schriftliche Daten) bewältigt. Ein
          Nachteil dieser ist allerdings, dass die erhobenen Daten stark beeinflusst oder
          nicht schriftlich sind. Durch die gegenwärtigen Entwicklungen in der
          Informationstechnologie sind Sammlungen von neuartigen Dialektdaten erreichbar
          (die ohne äußeren Einfluss, gesammelt wurden und darüber hinaus in schriftlicher
          Form als Datensatz vorhanden sind), womit in der Dialektometrie neue Wege
          gegangen werden können. Ein Beispiel dafür sind neue Medien, wie z. B.
          Wikipedia, Twitter, digitale Zeitschriften, etc., in denen außerdem
          Veränderungen in der Gesellschaft schnell abgebildet werden.
             Allein in Wikipedia ist eine große Anzahl an Dialekten vertreten, wie zum
            Beispiel die italienischen Dialekte Lombardisch (31.986 Artikel)
            1, Sizilianisch (25.273 Artikel), Neapolitanisch (14.346 Artikel) etc., die
            fortlaufend mit neuen Artikeln erweitert werden, die nicht nur von einem,
            sondern von mehreren Autoren editiert werden. Aus diesen Artikeln kann eine
            bisher nicht vorhandene Art Korpus erstellt werden, dessen Untersuchung die
            Beantwortung völlig neuer Fragestellungen möglich werden lässt. 
            Die Größe dieser neuen Korpora ermöglicht nicht nur neuartige Fragestellungen in der Dialektometrie, sondern auch einen zeitgenössischen und automatisierten Vergleich für die Analyse von Dialekten und ihren linguistischen Eigenschaften (basiert auf statistische Ansätze). Für solche Verfahren ist allerdings nicht nur die vorhandene Datenmenge wichtig, sondern auch die leichte Erreichbarkeit von qualitativen Annotationen und Analysetools. Diese wurden bislang hauptsächlich für die Standardsprachen entwickelt, für Dialekte existieren diese bis jetzt nur in wenigen Ausnahmefällen.
            Ein solches Analysetool für die Standardsprache Italienisch ist AnIta (Tamburini
              / Melandri 2012), ein morphologisches Finite-State-Analysetool, welches bisher
              nur für das Italienische verwendet werden kann. In AnIta können aber auch viele
              empirische Belege für Dialekte integriert werden, sodass die maschinelle
              Bearbeitung vieler italienischer Dialekte möglich wird. Die neuen
              Dialektwikipedias ermöglichen auch einen halb automatisierten Ansatz dafür.
         
         
            
                SiMoN
              
            
               Überblick
               In unserer Softwaredemonstration möchten wir eine vorläufige Erweiterung von AnIta vorstellen, die mit vielen regelmäßigen Verbparadigmen des sizilianischen Dialekts erweitert wurde - SiMoN (Sizilianische Morphologie für NLP-Anwendungen). Die Version der Softwaredemonstration ist schon
                  online erreichbar. Aus Einträgen der sizilianischen Wikipedia wurden Verblemmata (368 sizilianische Lemmata) für das Lexikon von AnIta automatisch extrahiert anhand von dem Auftreten regulären sizilianischen Verbendungen und einer Liste von Verben im Italienischen. Da sich die Verben des Sizilianischen in nur zwei Typen aufteilen (statt wie im Italienischen in drei), sind nur Verbeinträge mit Endungen auf
                  -ari und auf
                  -iri vorhanden. Die gesamte Zahl, der durch Flexionsparadigmen erfassten Verbformen beläuft sich auf ca. 24.700. Damit bietet SiMoN einen ersten Grundstock für die Entwicklung einer computergestützten, sizilianischen Morphologie.
                
            
            
               Dokumentierte Paradigmen
               Der Fokus der zu untersuchenden Paradigmen liegt in dieser Arbeit auf den Konjugationsmustern regelmäßiger Verben. Das vorderste Ziel ist es hier, eine Grundlage für die Verbanalyse für Sizilianisch zu schaffen. Im Gegensatz zum Italienischen gibt es für einige Verben eine große Zahl an Wahlmöglichkeiten für Endungen konjugierter Formen, die regional unterschiedlich verbreitet und gleichermaßen gültig sind. Bonner und Cipolla (2001) dokumentieren für die regelmäßigen Verben einiger Zeiten und Modi alternative Formen, die wir verfolgen. Diese Alternativformen gehören alle zum selben Paradigma. Daher gibt es im jeweiligen Lexikon der beiden Verbtypen in SiMoN teilweise mehrfache Einträge zur Konjugation der ersten, zweiten oder dritten Person. Eine vorläufige Analyse des gewonnenen Wikipedia-Korpus zeigte ebenfalls, dass die verschiedenen Varianten der Verben in der Praxis verwendet werden. Stammveränderungen in der sizilianischen Verbgrammatik existieren ebenfalls, diese Fälle werden allerdings mit SiMoN im Moment noch nicht abgedeckt.
               
                  
                  
                     Tabelle 1: Die regelmäßigen Konjugationsformen, die in
                SiMoN integriert wurden.
               
               In Tabelle 1 sind die regelmäßigen Konjugationsformen (die in SiMoN vorhanden
                  sind) am Beispiel der sizilianischen Verben parrari
                  (Deutsch - reden) und battiri (Deutsch - schlagen)
                  aufgeführt. Die Formen beider Verbtypen in den Flexionskategorien Indikativ,
                  Imperativ und Subjunktiv, sowie Konditional und Gerundium sind jeweils
                  vorhanden. Die Paradigmen der unregelmäßigen Hilfsverben essiri (Deutsch - sein) und aviri (Deutsch
                  - haben) sowie das sehr häufig verwendete fari
                  (Deutsch - machen) wurden ebenfalls in SiMoN in die Liste der Lemmata
                  aufgenommen, um Partizipkonstruktionen u. ä. zu erkennen. 
            
         
         
            Ausblick
            Unserer Ziel ist vorerst anhand den Texten der Wikipedia für Standard Italienisch und alle andere Dialektwikipedias weiterhin automatisch dialektspezifische Verben zu extrahieren und damit SiMoN zu erweitern. Damit können zusätzliche Dialekte auch behandelt und entwickelt werden. SiMoN würde dann eine automatisierte morphologische Analyse für reguläre italienische Dialektparadigmen ermöglichen, was wir bis jetzt nur für Sizilianisch anbieten können. Weiterhin ist es geplant auch irreguläre Dialektparadigmen manuell zu integrieren.
         
      
      
         
            Die Zahlen sind von Wikipedia
                  entnommen worden (Stand: August 2015). 
         
         
            
               Bibliographie
               
                  Bonner, J. K. "Kirk" / Cipolla, Gaetano (2001): Introduction to Sicilian Grammar. Brooklyn, NY:
                      Legas. 
               
                  Tamburini, Fabio / Melandri, Matias (2012): „AnIta: A
                      Powerful Morphological Analyser for Italian“, in: Proceedings of the Eight International Conference on Language Resources
                      and Evaluation (LREC’12), Istanbul, Turkey 941-947.
            
         
      
   



      
         
            Zusammenfassung der Sektion
            Die Vermessung der Welt mittels digitaler Medien hat längst begonnen. Von der Durchdringung der Gesellschaft zeugen nicht nur Street View und weltweit verfügbare Satellitenaufnahmen, sondern auch Twitter und Facebook und nicht zuletzt die Auswirkungen auf die Wissenschaftskulturen, insbesondere auf die der Geisteswissenschaften. Hierfür hat sich der Begriff der Digital Humanities etabliert, der schillernd und komplex zugleich ist. Während zunächst historisches Material und Artefakte digitalisiert wurden, rückte in den letzten Jahren vor allem die Annotation von Digitalisaten und die Anlage und Aufbereitung von Datenbanken ins Zentrum des Interesses. Derzeit fächert sich das Spektrum der Digital Humanities weiter auf. 
            Anne Burdick, Johanna Drucker und andere (Burdick et al. 2012) weisen in ihrem
          intensiv rezipierten und vielfach zitierten Buch zum Konzept der Digital
          Humanities darauf hin, dass die Möglichkeiten und Chancen der Digital Humanities
          quasi einer Erweiterung der Geisteswissenschaften gleichkommen, die sowohl
          Werte, interpretative Praxis und Strategien der Bedeutung als auch die
          Ambiguitäten der menschlichen Existenz betreffen. Innerhalb der Sektion soll der
          Blick insbesondere auf zwei auch von Burdick und Drucker thematisierte Aspekte
          gelenkt werden: Zum einen ermöglicht die Erweiterung der Digital Humanities neue
          Wege der transmedialen Erforschung durch interdisziplinäre Kooperationen. Zum
          anderen darf nicht nur die Anwendung von digitalen Werkzeugen und Datenbanken im
          Fokus stehen, sondern auch Konzeption, Entwicklung und Nutzung können und müssen
          neue Wege beschreiten. 
            Innerhalb dieser Sektion soll die kooperative, interdisziplinäre und interfakultäre Konzeption und Entwicklung einer mobilen Anwendung exemplarisch diese neuen Wege und Prinzipien als eine wichtige Option kulturwissenschaftlicher und informatischer Verschränkung vorgestellt werden. 
            An der Universität Paderborn hat sich vor zwei Jahren eine Forschergruppe
            innerhalb des akademischen Mittelbaus gebildet, die ein auf mehrere Semester
            angelegtes interdisziplinäres und interfakultäres Forschungs- und Lehrprojekt
            entwickelt hat. An diesem Projekt, das im September 2015 mit dem Forschungspreis
            der Universität Paderborn ausgezeichnet wurde, sind derzeit die Fächer
            germanistische Mediävistik und Linguistik, Geschichte, Informatik und
            Kunstgeschichte beteiligt. Die ‚Historisches Paderborn‘-App (kurz HiP-App) zeigt
            die neuen Verknüpfungen, die durch das Konzept der Digital Humanities in den
            letzten Jahren proklamiert wurden, geradezu idealtypisch auf: Denn in der
            HiP-App ist die Informatik nicht lediglich Dienstleister für die
            Kulturwissenschaften und sind die Kulturwissenschaften nicht ausschließlich
            Content-Lieferanten für die Informatik. Vielmehr widmet sich das Projekt
            übergreifenden Forschungsfragen und ermöglicht darüber hinaus ebenso
            Individualforschung. In unserem Projekt fokussieren wir Fragestellungen, die
            sich aus der Konzeption, der Entwicklung und der Nutzung von digitalen
            Anwendungen (Apps) für mobile Endgeräte wie Smartphones oder Tablets
            ergeben.
            Innerhalb dieser Sektion liegt der Fokus auf drei essenziellen Schwerpunkten des Projekts: (I.) auf dem Potenzial kooperativer Exploration und Konzeption mobiler Anwendungen für die Vermittlung wissenschaftlicher Inhalte in einem außeruniversitären Kontext, (II.) auf multimodaler Kommunikation und Raumwahrnehmung und (III.) auf der evolutiven Software-Entwicklung unter Berücksichtigung einer mensch-zentrierten Entwicklung, die in der interdisziplinären Kooperation zwischen Kulturwissenschaften und Informatik innerhalb unseres Projektes verwirklicht werden kann.
         
         
            Ins Leben gerückt. Zum Potential mobiler Anwendungen für die Vermittlung vormoderner Artefakte 
            
               Markus Greulich, Nicola Karthaus
              und Ariane Schmidt (Universität Paderborn)
            
            Insbesondere die historischen Geisteswissenschaften, im ganz besonderen Maße die
                mediävistischen Fächer, gelten manchem als längst überholte
                Wissenschaftsdisziplinen, aus denen weder ein Wert für die Gegenwart noch für
                die Zukunft zu erwarten ist. ‚Das‘ Mittelalter gilt als gut erforscht, die
                Einträge in online-Ressourcen vermitteln ein abgeschlossenes Bild: Wir wissen,
                wie unsere Vergangenheit war. Doch immer wieder gibt es Irritationen: Da
                geistern Pergamentfragmente durch die Tagesschau, widmen sich Regisseur_innen
                mit großem Erfolg den Lebensgeschichten von Kaiser_innnen und Heiligen, zeigen
                internationale Serien, dass die Päpste des Mittelalters und der Renaissance ein
                nur begrenzt katholisches Leben pflegten und immer wieder geraten wertvolle
                Handschriften und Kunstobjekte des Mittelalters in den Fokus der Öffentlichkeit.
                So wie unsere Gegenwart nie als geschlossenes Bild vor uns stehen kann, so
                verändert sich auch ‚unser‘ Blick auf ‚das ‘ Mittelalter. Diesen Blick zu
                schärfen, vormoderne Artefakte lesbar zu machen und ihre eigene Geschichte als
                Teil ‚unserer‘ Geschichte, als Teil der Gegenwart erfahrbar zu machen – dies
                soll das interdisziplinäre Forschungs- und Lehrprojekt ‚Historisches
                Paderborn‘-App, kurz HiP-App, leisten. 
            Die HiP-App ist eine Anwendung für mobile Endgeräte, die
                auf ansprechende Weise detaillierte und wissenschaftlich sinnvoll aufbereitete
                Materialien zur selbstständigen historischen Erkundung der Stadt Paderborn
                anbietet. Sie wird derzeit von Mittelbau-Vertreter_innen der Universität
                Paderborn aus den Bereichen Informatik, germanistische Mediävistik und
                Linguistik, Geschichte und Kunstgeschichte entwickelt. Die HiP-App ist zugleich Forschungsgegenstand und -infrastruktur, die
                durch ihren grundlegend evolutiven Charakter vielfältige Forschungsfragen
                erzeugt (Burdick et al. 2012). Existierende Denkmäler dienen als Ausgangspunkt,
                um germanistische, historische und kunsthistorische Inhalte zu erläutern. Das
                interaktive Front-End der HiP-App, d. h. die für den
                Benutzer sichtbare Oberfläche, wird u. a. durch aktuellste Präsentationsformen
                historischer Artefakte im Bereich der Augmented Reality, d. h. der virtuell
                erweiterten Realität, gestaltet. Hierbei können Kunstwerke und andere Objekte
                nicht nur schriftlich oder mündlich erläutert, sondern auch singuläre Details
                hervorgehoben oder verlorene Sinnzusammenhänge visualisiert werden. Sogar der
                ursprüngliche Kontext eines Werkes kann so rekonstruiert werden. Auch ist es
                möglich, kunsthistorische Vergleiche zu ziehen, verwandte Werke zu zeigen,
                zusätzliche Materialien anzubieten und eine kulturhistorische Kontextualisierung
                vorzunehmen. Die neuen technischen Möglichkeiten der medialen Aufbereitung von
                Kulturgeschichte sind verbunden mit neuen sozialen Praktiken des Wahrnehmens an
                der Schnittstelle zwischen physischem und digitalem Raum (Buschauer / Willis
                2013; Schüttpelz 2006). Die so gestalteten historischen Aussagen werden im
                Hinblick auf eine für die Gegenwartsorientierung relevante (stadt)geschichtliche
                Sinnbildung (Rüsen 1996) motiviert. Somit soll die Neugier der Nutzer_innen
                geweckt und durch eine veränderte Wahrnehmung des urbanen Umfelds eine
                weiterführende A useinandersetzung mit dem Paderborner Kulturraum gefördert
                werden. 
            Zentrales gemeinsames Forschungsanliegen ist es, am Beispiel der HiP-App
                  Methoden, Prozesse und Analysen im Bereich der Digital Humanities zu entwickeln.
                  Forschungsgegenstand und Grundlage hierfür bilden Genese, Entwicklung, Betrieb
                  und Pflege der HiP-App, aber auch die kritische Reflexion
                  der Kopplung physischer und digitaler Räume mithilfe mobiler Apps, die von der
                  menschlichen Körperorientierung ausgehend den städtischen Raum diachron lesbar
                  machen. So sollen durch Analyse der Dateneingabe etwa auch Aspekte der
                  Software-Usability und der Entwicklung multimodaler Kommunikationsformate in den
                  Blick genommen werden. Bezogen auf die Front-End-Entwicklung stehen verschiedene
                  Formen des epistemologischen Präsentmachens durch Visualisierung, auditive
                  Aufbereitung und weitere empirische Anknüpfung durch materielle Spurensuche auf
                  dem Prüfstand (Kesselheim 2010). Die interdisziplinäre Kooperation lässt
                  gleichzeitig Methoden der verschiedenen Disziplinen produktiv zusammenwirken.
                  Entwicklung und Betrieb von Front- und Back-End finden aktuell in Form agiler
                  Softwareentwicklung statt, in die die beteiligten Akteure aus den
                  Kulturwissenschaften (insbesondere auch die Studierenden) fest eingebunden sind.
                  Weitere Schwerpunkte liegen auf der leichten Nutzbarkeit von Technologien,
                  beispielsweise einer einfachen Pflege der eingestellten Daten innerhalb eines
                  Web-Back-Ends sowie einer flankierenden kulturwissenschaftlichen Reflexion der
                  raumgenerierenden Potenziale neuer mobiler Systeme. Die Motivation des Projekts
                  liegt somit auch darin, die aus der Informatik heraus entwickelten neuen Formen
                  der Überblendung und Verkopplung physischer, kartographierter und medialer Räume
                  in ihren produktiven Momenten der Verräumlichung (Habscheid / Reuther 2013) und
                  der historischen Narrativierung kulturwissenschaftlich und experimentell
                  (Back-End-Editieren, Nutzerverhalten) zu begleiten. 
            Im Rahmen der ersten Projektphase werden derzeit drei historische Stadtrundgänge zum Heiligen Liborius, zu Kaiser Karl dem Großen und zu Bischof Meinwerk von Paderborn entwickelt. Diese historischen Persönlichkeiten sind für die Genese und Entwicklung der Stadt Paderborn im Frühmittelalter von besonderer Bedeutung gewesen. Sie sind auch heute noch in vielfältiger Weise im Stadtbild präsent. Unter anderem wird dies in einem weiteren Rundgang thematisiert, der sich historischen Orts- und Straßennamen widmet, denn auch in der Namensgebung von Orten und Straßen artikuliert sich das kulturelle Gedächtnis. Ein zentrales Anliegen unseres Projekts ist es, den Stadtraum historisch erfahrbar zu machen. Ziel ist es, unter anderem die Ungleichzeitigkeit des Gleichzeitigen herauszuarbeiten: Die in der heutigen Präsenz als in sich geschlossene Einheit sichtbaren Objekte sollen als historisch gewachsen erfahren werden, ihre einzelnen Elemente als aus unterschiedlichen
                    Epochen stammend. 
            Ein gutes Beispiel hierfür ist der Paderborner Dom (einführend: Quednau 2011),
                      der sowohl von vielen Ortsansässigen als auch von Touristen mit seinen
                      zahlreichen Artefakten und religiösen Objekten als gegebenes Bauwerk
                      wahrgenommen wird. Dass dieses Bauwerk aber keinesfalls statisch, sondern
                      vielmehr historisch gewachsen ist (u. a. Lobbedey 1986) und seit der
                      Grundsteinlegung unter Karl dem Großen im Jahre 777 im Laufe der Jahrhunderte
                      vielfältige bauliche Veränderungen, Erweiterungen und Überformungen erfahren
                      hat, ist nur wenigen bewusst. Hier setzt die HiP-App an.
                      Mit ihrer Hilfe ist es möglich, den Dom in seiner ganzen historischen Dimension
                      und Vielschichtigkeit für den Betrachter sichtbar und erfahrbar zu machen. Als
                      ein ganz konkretes Beispiel für die Ungleichzeitigkeit des Gleichzeitigkeit
                      bietet sich das Paradiesportal des Paderborner Doms an: 
            Ursprünglich als romanische Vorhalle konzipiert, wurde das Paradies im Zuge eines Umbaus des Westquerhauses mit einem Figurenportal ausgestattet. Dieses wies zunächst die Struktur eines rein ornamentalen Portals auf, dem in der Konzeptionsphase Sandsteinfiguren hinzugefügt wurden. Die monumentalen gotischen Gewändefiguren entsprechen dem zeitgenössischen Geschmack des 13. Jahrhunderts und orientierten sich an Skulpturen der französischen Gotik, beispielsweise denen der Kathedralen von Paris und Reims (Sauerländer 1971). Auf diese Weise 'modernisierte' also die Paderborner Dombauhütte das Hauptportal des Sakralbaus bereits im Hochmittelalter, und zwar in Anlehnung an die damals aktuelle und hochwertige Bauskulptur des französischen Königreichs. Die beiden romanischen Holzskulpturen des Portals, die den hl. Kilian und den hl. Liborius darstellen, stammen hingegen aus dem 12. Jahrhundert, wurden jedoch erst deutlich später, nämlich 1815, an den Portaltüre
                        n angebracht. Diese Veränderung und Ausgestaltung des Domportals eignet sich in geradezu idealer Weise, um den Nutzer_innen mit Hilfe von Augmented Reality innerhalb des Front-Ends die historische Dimension nicht nur des Portals, sondern exemplarisch auch des gesamten Doms vor Augen zu führen. 
            Für interessierte Nutzer_innen hält die HiP-App aber auch
                        vertiefende Informationsangebote bereit: So z. B. erläuternde Texte zu einzelnen
                        Skulpturen des Portals, die etwa typische, in der App farbig hervorgehobene
                        Attribute der Heiligen erklären, oder aber kulturhistorische Einordnungen mit
                        Blick auf die Nutzung (Tack 1958) oder die religiöse Praxis (Bawden 2014). Auch
                        können Brücken und Verbindungen zwischen verschiedenen Fachdisziplinen
                        geschlagen werden: So illustriert die zentrale Marienfigur des Trumeaus die
                        hochmittelalterliche Marienverehrung anhand des Melker Marienlieds, eines wenig
                        bekannten Textes aus dem 12. Jahrhundert. Die Nutzer_innen erfahren hier nicht
                        nur Wissenswertes zum Text, sondern sehen mit Hilfe der App auch - vielleicht
                        erstmals - eine mittelalterliche Handschrift. Dass Maria im Melker Marienlied
                        als Himmelspforte (porte des paradyses) bezeichnet wird, schlägt dabei eine
                        Brücke zur Portalsymbolik und gibt - über die Einzelbetrac htung des Portals
                        hinaus - einen Einblick in die Kulturgeschichte und spezieller noch in die
                        Liturgie. War die Nutzung des Portals - u. a. als erzbischöflicher Zugang und
                        als Gerichtsort - bereits zur Bauzeit vielschichtig, so erfuhr sie im Laufe der
                        nachfolgenden Jahrhunderte noch weitere Veränderungen und Ergänzungen. So bildet
                        das Domportal etwa noch heute den monumentalen Rahmen für die feierliche
                        Liborius-Prozession: Sie erinnert alljährlich an die Translation der Gebeine des
                        Heiligen aus dem damals westfränkischen, heute französischen Le Mans und an ihre
                        Ankunft in Paderborn im Jahre 836. Darüber hinaus werden in der App regionale
                        und überregionale Vergleichsobjekte (Sauerländer 1971; Lobbedey 1999)
                        vorgestellt, die eine kunsthistorische Einordnung in die Portalskulptur geben.
                        Damit macht die App eben gerade nicht nur die konkrete Stadt- und Baugeschichte
                        Paderborns anschaulich erlebbar, sondern greift darüber hinaus. Es geht gerade
                        auch darum, den Nutzer_innen der stadtgeschichtlichen App ein exemplarisches
                        Wissen zu vermitteln, durch das Sehgewohnheiten verändert und historische
                        Artefakte selbständig les- und erfahrbar werden.
         
         
            Digitalisierung von geschichtlichem Wissen im Raum und raumgebundener Erinnerungskultur – am Beispiel von Straßennamen 
            
               Kristina Stog (Paderborn)/ Nicole
                          M. Wilk (Paderborn) 
            
            
               Idee: Vervielfältigung der Lesarten durch neue Visualisierungsmethoden
               Wissensarten sind an Darstellungsformate gebunden. Informationen über das
                              Medienmaterial, seine Gestaltung, Beschaffenheit und Platzierung gehen bei
                              der Verarbeitung von Daten im Zuge der Digitalisierung größtenteils verloren
                              oder werden isoliert vom Textkorpus z. B. in Form von Metadaten gespeichert.
                              Dieser Verlust wird in den interpretierenden Disziplinen oft in Kauf
                              genommen, da quantitativ motivierte Fragestellungen bereits an diese
                              Reduktionssituation angepasst sind. Doch es setzt sich gleichzeitig in
                              linguistischen und (sozial)semiotischen Forschungskontexten die Erkenntnis
                              durch, dass Wissen immer situiertes Wissen in materiellen und
                              institutionellen Umgebungen ist (Fix 2008), und dass mit Blick auf
                              multimodale Gebrauchsmuster die Wahl der semiotischen Ressource (O'Halloran
                              2004) und nicht zuletzt die Raumbasiertheit von Kommunikation semantische
                              und Diskurs strukturierende Effekte haben können (Habscheid / Reuther 2013). 
               Kaum ein „Text“ ist so eng mit seinem Ort verknüpft wie ein aufgedruckter
                                „Name“. Am Beispiel der Namen für Straßen, Gebäude und Plätze zeigen wir in
                                unserem Beitrag Digitalisierungsmöglichkeiten auf, die die Verknüpftheit von
                                stadtgeschichtlichem Wissen mit Orten auf (technisch) verschiedene Weise
                                modellieren. Hierfür wird mit dem Smartphone ein mobiles Instrument gewählt,
                                das über eine App Schnittstellen zwischen materiellem und digitalem Raum
                                erzeugt (Weber 2012), um Stadtgeschichte in unterschiedlichen Deutungsrahmen
                                (visuell, auditiv) verfügbar zu machen. Der Raum erweist sich dabei als
                                interaktive Ressource (Hausendorf / Mondada / Schmitt 2012), auf die die
                                kulturellen Sinnangebote ausgerichtet sind und die sie selbst als solche
                                hervorbringen (reflexiver Raumbegriff). 
               Mit den neuen technologischen Verfahren erschöpft sich die interaktive Dimension nicht in der Aufmerksamkeitslenkung durch schriftbasierte oder bildliche Information, mithilfe standortbezogener Informationen durch Location-based Services kann Nutzer_innen vielmehr an konkreten Orten durch eine mittelalterliche Geräuschkulisse oder eine visuelle Anreicherung des Stadtbilds ein Einstieg in historische Szenarien geboten werden. 
            
            
               Hintergrund: Straßennamen als Kondensate kulturellen Wissens und der Inanspruchnahme von Geschichte
               Namen von Straßen, Plätzen und anderen Örtlichkeiten (Toponyme) dienen der
                                  Orientierung. Sie erschließen den Raum und strukturieren ihn physisch und
                                  historisch, zugleich können Toponyme als Verweise auf die Geschichte sowie
                                  auf das Geschichtsbewusstsein einer Stadt gelesen werden. Anders als
                                  Denkmäler, die aufgrund ihrer erinnernden Funktion bewusst aufgesucht
                                  werden, stellen sie – in ihrer Sekundärfunktion – „Medien kultureller
                                  Erinnerung“ (Pöppinghege 2005: 10) dar, die von den Rezipient_innen im
                                  urbanen Raum täglich genutzt werden. Vor allem die frühen Orts- und
                                  Straßennamen, die aufgrund von gemeinsamen Gewohnheiten, Bedürfnissen oder
                                  Wahrnehmungen in der Interaktion (bereits im Mittelalter) gewachsen sind
                                  (Fuchshuber-Weiß 1996) geben Hinweise auf Vergangenes, im heutigen Stadtbild
                                  möglicherweise nicht mehr Sichtbares: Geographische bzw. topographische
                                  Merkmale oder Besonderheiten des Ortes, nennenswerte Gebäude in der
                                  Umgebung, eine bestimmte Nutzung des Bezugsbereiches (wie etwa dort
                                  angesiedeltes Gewerbe) oder soziokulturelle Tatbestände vor Ort
                                  (Fuchshuber-Weiß 1996). 
               Namen stellen dabei keinen schlichten Spiegel tatsächlicher Gegebenheiten dar, sondern geben Einblick in die kollektiven Sicht- und Vorstellungsweisen ihrer Nutzer und bilden „in ihrer auswählenden und akzentuierenden Thematisierung des Stadtraumes Dokumente einer Mentalitätsgeschichte des Sehens“ (Glasner 1999: 320). In dieser Hinsicht gibt auch die heutige Benennungspraxis Aufschluss über das Geschichtsbewusstsein einer Stadt: So spiegelt sich etwa in der Vergabe von Namen, die sich auf Historisches „vor Ort“ beziehen, auch das „kultur- und alltagsgeschichtliche Verständnis“ (Pöppinghege 2005: 10) einer Stadt. Lokalen Ereignissen, Personen, aber auch Intentionen oder Vorstellungen, mit denen sich eine Stadt identifiziert, werden in Form von Straßen(-namen) begehbare „Zeichen gesetzt“. 
               In der Erarbeitung toponymischen Wissens ergibt sich eine Herausforderung daraus, dass verschiedene Wissenssorten zusammenkommen: Legendenbildungen, Volksetymologien, Geschichtswissen der Historiker und ein zeitabhängiges Geschichtsbewusstsein, das teilweise mit einer intensiven Geschichte der Umbenennung einhergeht. Beobachtungen zur thematischen Verarbeitung von Namensgebung und Namensgeschichte in bestehenden Apps zur Stadtgeschichte belegen das allgemeine Bedürfnis nach „Lesbarkeit“ von urbanen Räumen: Namen werden als Spuren geschichtlicher Zusammenhänge aufgeschlossen. Doch wie werden durch sie Diskurse räumlich materialisiert? Wie können Wissenssorten auch durch mediale Operationen reflektiert werden? Das Beispiel der in Entwicklung befindlichen 'Historisches Paderborn'-App soll aufzeigen, wie die doppelte Situiertheit des stadtgeschichtlichen Wissens in einem metakommunikativen und in einem räumlichen Sinne durch Visualisierungstechniken darg
                                    estellt werden kann. 
            
            
               Ausgangslage: Typonyme in bisherigen Kommunikationsangeboten und Apps zur Stadtgeschichte
               Um in mobilen Geräten Geschichtliches auf neue experimentelle Weise darzustellen, müssen zunächst die traditionellen Repräsentationen geschichtlichen Wissens im Stadtraum hinsichtlich ihrer raumstiftenden Qualitäten erschlossen werden. Straßen- und Gebäudenamen haben trotz des in ihnen sedimentierten impliziten Wissens über Ereignisse der Stadtgeschichte für viele Bewohner und Stadtbesucher einen primär pragmatischen Sinn und dienen der räumlichen Orientierung. Selbst Legenden und Volksetymologien, die sich um die Namen im Stadtraum ranken, drohen verloren zu gehen. Auf diese Situation reagieren ortsfeste und ortsgebundene digitale Kommunikationsangebote zur Stadtgeschichte, die den urbanen Raum als Medium des kollektiven Gedächtnisses und mit ihm eine völlig neue städtische Erzählkultur etablieren. In einer medienlinguistischen Studie zur Musterhaftigkeit ortsfester Kommunikationsangebote zur Stadtgeschichte konnten zwei wesentliche Tendenzen in
                                      der Entwicklung internetbasierter Formate und Stadtgeschichts-Apps festgestellt werden: der Ausbau einer dialogischen Sequenzierung stadtgeschichtlichen Wissens und die Narrativisierung urbaner Erzählsequenzen („Histörchen“). Ohne Angabe von Quellen und ohne Verweise auf die Deutungsvielfalt der historischen Dokumente werden Brauchtümer und Motive sprachlich so dargelegt als seien die Namen Repräsentationen einer zu allen Zeiten und eindeutig herauszulesenden historischen Faktizität (vgl. Wilk 2015). Der Einsatz multimodaler Darstellungsformen verspricht hier Möglichkeiten, Namen und Namenswandel exemplarisch in einem Spurkonzept zu modellieren (vgl. Müller 2012), das Typonyme im linguistischen Sinn weniger als objektive Zeugen eines geschichtlichen Geschehens aufschließt als vielmehr anhand der Namensgebung den Kampf um historische Lesarten und ihre Orientierung für die Zukunft verdeutlichen. Aufgabe der Medienlinguistik ist es dabei, anhand konkreter Textentwicklunge
                                      n zu beschreiben, wie unter Nutzung verschiedener Daten aus den Visualisierungen eines durch historische Szenarien erweiterten Stadtraums unterschiedliche historische Interpretationen hervorgehen. 
            
            
               Das Beispiel Paderborn – Motiviertheit und sozialer Sinn hinter den Spuren
               Wie Namen als Spuren von Vergangenem im heutigen Stadtbild gelesen werden
                                        können, lässt sich anhand einiger Straßennamen in der Paderborner Innenstadt
                                        beispielhaft zeigen: Sie können Hinweise auf das historische Stadtbild
                                        geben, wie etwa der Name Grube, der als einer der ältesten Straßennamen in
                                        der Altstadt auf die heute nicht mehr sichtbare Grube, die nach Auffüllung
                                        eines Steinbruchs südlich der Domburg im 12. Jahrhundert zu sehen war (vgl.
                                        Liedtke 1999: 101), verweist. Namen wie Im Düstern oder Krummer Ellenbogen
                                        geben darüber hinaus Einblicke in die Wahrnehmung des städtischen Raums aus
                                        der Perspektive ihrer Nutzer_innen. Neben Anwohnergruppen (Weberberg)
                                        spiegeln sich in Namen bestimmte Nutzungsweisen von Straßen (wie etwa Kühe
                                        durch die Kuhgasse zur Tränke an die Pader zu treiben (vgl. Liedtke 1999)).
                                        Auch Spuren des Niederdeutschen, das als gesprochene Alltagssprache in
                                        Paderborn kaum noch existiert, finden sich in Namen wie Abtsbrede (Brede
                                        bezeichnet einen breiten Acker) oder Börnepader (börnen: tränken). 
               Nach Nübling (2012: 244) lassen sich aus diesen primären Straßennamen, die in engem Zusammenhang mit den Straßen, die sie bezeichnen, entstanden sind, „Topographie und Sozialgeschichte einer Stadt hervorragend rekonstruieren“. In einer App zum Historischen Paderborn bilden sie nicht nur einen Anknüpfungspunkt für die Auseinandersetzung mit dem historischen Raum, sondern auch mit den kollektiven Sicht- und Vorstellungsweisen ihrer Nutzer_innen. Dies gilt auch für die sekundären Straßennamen, die administrativ vergeben werden: Die Wahl der regionalen und überregionalen Personen und Ereignisse, nach denen eine Stadt ihre Straßen benennt, gibt Aufschluss über ihr (Stadt-)Geschichtsbewusstsein (vgl. Pöppinghege 2007). So deutet etwa die Benennung der Straßen eines Viertels in Paderborn, in dem neben der Karlsstraße und dem Karlsplatz auch Albin-, Gerold-, Einhard- und Widukindstraße auf historische Zusammenhänge und Personen im Umfeld Karls des
                                          Großen verweisen, auf die Bedeutung hin, die dieser im Selbstverständnis der Stadt einnimmt. 
               Politische und gesellschaftliche Umbrüche werden vor allem in der Auseinandersetzung mit den Umbenennungen von Straßen oder Plätzen sichtbar. In ihnen spiegeln sich die Vorstellungen, Ideen und Ideologien, zu deren Verbreitung Toponyme seit dem 18. Jahrhundert genutzt werden (vgl. Fuchshuber-Weiß 1994: 1472). Deutlich wird dies am Beispiel des Le-Mans-Walls in Paderborn, der bis 1938 als Wilhelmstraße, in der Zeit des Nationalsozialismus als Horst-Wessel-Wall und nach 1945 erneut als Wilhelmstraße bezeichnet wurde, bis 1967 mit der Städtepartnerschaft die Umbenennung nach der französischen Stadt Le Mans folgte (vgl. Liedtke 1999: 151). Mit der Benennung, die sich nun zugleich an mittelalterlichen Ereignissen orientierte, wurde der Straße – als Weg, über den 836 der Zug mit den Reliquien des Hl. Liborius von Le Mans in Richtung Dom geführt haben soll – somit auch eine größere Bedeutung innerhalb der Stadtgeschichte Paderborns zugesprochen. 
               In der Hip-App lässt sich die Geschichte dieser
                                          Umbenennung und der Vereinnahmung historischer Persönlichkeiten für die
                                          städtische Identität multiperspektivisch visualisieren, so dass anschaulich
                                          wird, wie zu verschiedenen Zeitpunkten Historisches in städtischen
                                          Strukturen repräsentiert (worden) ist. Diese Repräsentationen der
                                          historischen Traditionen schließen zudem die Konsequenzen für das
                                          (Geschichts-)Bild der gegenwärtigen Stadt und der Stadt der Zukunft auf. 
               Hierbei sollen epistemischen Effekte, d.h. insbesondere
                                            komplexitätsreduzierende Wissenseffekte verschiedener Darstellungsweisen
                                            (Karten, archäologische Modelle, Einblendungen) exemplarisch erfasst werden.
                                            Diese variieren mit der Auswahl der Kategorien, der Relationierung markanter
                                            Ereignisse und nicht zuletzt der Herstellung von Bezügen zum materiellen
                                            Raum. Die Differenzierung von Erzählzeit und erzählter Zeit reflektiert
                                            dabei die Variabilität historischer Sinnstiftung: So können historische
                                            Figuren (Könige, Ritter, Pilger etc.), die z. B. mit Pferdegetrappel an
                                            ausgewählten Stellen des Stadtrundgangs die Wege der Nutzer_innen kreuzen
                                            und damit historische Situationen auf dem Hellweg simulieren, eine
                                            mittelalterliche Vergangenheit einerseits behaupten. Andererseits lassen sie
                                            sich anschließen an eine stadttypische Rezeptionsgeschichte
                                            mittelalterlicher Quellen und Schriften. In der (zusätzlichen) Darstellung
                                            gewandelter städtischer Identitätsdiskurse z. B. über kartografierte
                                            Wissensbezüge (Bezug zum Mittelalter, zur niederdeutschen Varietät, zur
                                            Geografie etc.) lässt sich ein jeweils zeitabhängiges Geschichtsbewusstsein
                                            veranschaulichen.
            
         
         
            Auf dem Weg zu einer experimentellen und evidenzbasierten Softwareentwicklung in den Digital Humanities
            
               Björn Senft, Simon Oberthür – SICP
                                            – Software Innovation Campus Paderborn, Universität Paderborn
            
            Das interdisziplinäre Projekt ‚Historisches Paderborn'-App, kurz HiP-App, ist ein gutes Beispiel für die sich verändernden
                                              Anforderungen an den Software-Entwicklungsprozess im DH-Kontext bzw. im Kontext
                                              des digitalen Wandels (Digital Transformation). Klassische Entwicklungsmodelle
                                              wie das Wasserfallmodell wurden für Situationen entworfen, in denen
                                              Funktionsumfang und Aufbau der Software zu Beginn der Entwicklung relativ genau
                                              festgelegt werden können. 
            Für die HiP-App ist dies jedoch aus mehreren Gründen nicht
                                              möglich. Aufgrund der Verwendung neuer Technologien in der App und der Raum- und
                                              Zeitgebundenheit der Inhalte ergeben sich vollkommen neue Wege, mit Wissen
                                              umzugehen. Das Dilemma ist nicht gerade selten: Informatiker verfügen über das
                                              Wissen um moderne Technologien, Kulturwissenschaftler verfügen über das Wissen
                                              der zu vermittelnden Inhalte. Ein sinnvoller und innovativer Einsatz neuer
                                              Technologien kann aber nur in enger Verzahnung mit konkreten Inhalten erfolgen.
                                              Oft kann auch eine Verzahnung vorab nicht hinreichend beurteilt werden, sondern
                                              muss beispielsweise experimentell bewertet werden. Die sinnvolle Anwendung neuer
                                              Technologien ist deshalb ein Forschungsdesiderat und muss durch geeignete
                                              Entwicklungsmethoden und -abläufe unterstützt werden. 
            Weil in unserem Kontext die Anforderungen vor der Implementierung nicht genau
                                                spezifiziert werden können, muss erforscht werden, welche Faktoren zielführend
                                                für die Lösung der zu bewältigenden Entwicklungsaufgabe sind. Da die Informatik
                                                mit dieser Situation relativ häufig konfrontiert ist, wurden hierfür Methoden
                                                wie etwa die agile Entwicklung gebildet. Das ist allerdings nur ein möglicher
                                                Ansatz zur Lösung dieses Problems, da die Methoden nur einen lockeren Rahmen
                                                bieten und nicht genauer darauf eingehen, wie mit der Software experimentiert
                                                werden kann, geschweige denn, wie eine systematische Extraktion und Evaluation
                                                in diesem Kontext aussehen könnte. Sinnvoll wäre beispielsweise ein
                                                Softwareleitstand, der Experimente mit verschiedenen Nutzergruppen (Betatester,
                                                Endnutzer, Experten, etc.) bzw. Ebenen (Simulation, Menschen, etc.) ermöglicht.
                                                Erschwerend kommt dabei hinzu, dass Prozesse verschiedener Domänen
                                                (Kulturwissenschaften, kulturelle Institutionen, etc.) in den eigentlichen
                                                Softwareentwicklungsprozess integriert werden müssen.
            
               Lösungsansatz
               In unserem Projekt entwickeln wir ein mensch-zentriertes Prozessmodell (siehe
                                                    Abbildung 1), dass das DevOps-Prinzip (Hüttermann 2012; Sharma / Coyne 2015)
                                                    mit dem Führungskreislauf des St. Galler Management Modells (Ulrich / Krieg
                                                    1974) kombiniert, um so die vielschichtige Verzahnung von Technologien,
                                                    Inhalten, Domänen und Akteuren (Informatiker, Kulturwissenschaftler,
                                                    Usability-Experten, Nutzer, etc.) zu gewährleisten. Die Grundidee hinter
                                                    diesem Modell ist die Aufteilung in generellere Phasen, um so
                                                    Verzahnungspunkte für die unterschiedlichen Prozesse zu definieren. In der
                                                    Ermittlungsphase werden mit Methoden der Informatik und der
                                                    Kulturwissenschaft Daten aus der Realität (Verwendung des Prototyps,
                                                    Interviews, Unit-Tests, ‚ausgelieferte‘ Software, etc.) ermittelt, um so ein
                                                    Lagebild zu erstellen, das die Grundlage für die Weiterarbeit in der
                                                    Strategie- und Analysephase bildet. In dieser kommen die unterschiedlichen
                                                    Akteure zus ammen und analysieren gemeinsam die ermittelten Daten und
                                                    beraten über die zukünftige Strategie, die in der Realisierungsphase
                                                    (Implementierung, Entwickeln eines konkreten Interviews, etc.) umgesetzt und
                                                    anschließend erneut in der Realität eingesetzt und evaluiert wird. Wichtig
                                                    ist dabei - da es kein definiertes Ende gibt -, die einzelnen Phasen
                                                    kontinuierlich und iterativ zu durchlaufen, da wir nach Drucker et al.
                                                    (2012) davon ausgehen, dass sich die Software, aufgrund der hieraus
                                                    entstehenden neuen Erkenntnisse, ständig weiterentwickeln wird: 
                „Digital Humanities work embraces the iterative, in which experiments are
                                                      run over time and become objects open to constant revision. Critical design
                                                      discourse is moving away from a strict problem-solving approach that seeks
                                                      to find a final answer: Each new design opens up new problems
                                                      and—productively—creates new questions.“ (Drucker et al. 2012: 22).
               
                  
                  
                     Abb. 1: Verwendetes Prozessmodell der HiP-App-Entwicklung im Digital Humanities Kontext
               
            
            
               Erfahrungen und Erkenntnisse
               Der bisher entwickelte Teil des Lösungsansatzes basiert auf den Erfahrungen,
                                                        die bislang bei der Entwicklung der HiP-App gemacht
                                                        wurden. Eine studentische Projektgruppe der Informatik entwickelte in
                                                        stetiger Rückkopplung mit den Kulturwissenschaften das Backend zum
                                                        Einpflegen der Daten. Die Studierenden entwickeln die Software nach Ansätzen
                                                        der agilen Softwareentwicklung (Beck et al. 2001) und nach dem
                                                        DevOps-Prinzip (Hüttermann 2012; Sharma / Coyne 2015), die für uns
                                                        Schlüsselfaktoren sind, um einen kontinuierlichen mensch-zentrierten
                                                        Softwareentwicklungsprozess (Mayhew / Follansbee 2012) mit explorativen
                                                        Möglichkeiten zu erreichen.
               Dass eine enge Kooperation von Informatik und Kulturwissenschaften im Sinne der Digital Humanities notwendig ist, hat sich bereits in den ersten Arbeitsphasen des Projekts bestätigt. Wie bereits erwähnt, können sinnvolle Anwendungen nur in enger Verzahnung von Inhalten und Technologien entstehen. Informatik und Kulturwissenschaften müssen deshalb interagierend Daten auswerten und die Strategie anpassen. Diese Erkenntnis ist eine Quintessenz aus unserer Projekterfahrung. 
               Den involvierten Kulturwissenschaftlern fehlten anfangs Bewusstsein und
                                                          Wissen über die notwendige Spezifizität, über den Realisierungsaufwand und
                                                          auch über die Nachhaltigkeit der Softwareentwicklung, die für eine
                                                          zielgerichtete Entwicklung qualitativ hochwertiger Software jedoch
                                                          essenziell sind. Die beteiligten Informatiker verloren sich dagegen allzu
                                                          schnell in technologischen Herausforderungen anstatt die Nutzeranforderungen
                                                          zu fokussieren. Es bedarf deshalb eines gemeinsamen Verständnisses und einer
                                                          gemeinsamen Strategie, welche Merkmale einer Anwendung welche Priorität
                                                          haben und wann diese implementiert werden sollen oder aber mit Hilfe anderer
                                                          Frameworks zu realisieren sind. Um wichtige technologische Entscheidungen
                                                          treffen zu können, müssen sich die Anforderungen herauskristallisieren, die
                                                          sich im Detail aus den Prozessen und Inhalten ergeben. Daher erscheint uns
                                                          ein Verhältnis Dienstleister (Informatik) und Content-Lieferant
                                                          (Kulturwissenschaften) für die Entwicklung von Software im Kontext von
                                                          Digital Humanities wenig sinnvoll, ja sogar kontraproduktiv. 
               Festzuhalten ist, dass unser Prozessmodell möglichst kurze Durchläufe erlaubt, um so frühzeitig neue Erkenntnisse zu gewinnen, die dann zeitnah in die weitere Softwareentwicklung einfließen können. Die kurzen Wiederholungen im Prozessmodell helfen, nicht-verbalisierbares Nutzerwissen verfügbar zu machen. Solches Wissen ist beispielsweise grundlegend, um die Lösungen bestmöglich auf die Bedürfnisse der Nutzer auszurichten.
               Unser Lösungsansatz ermöglicht ein Experimentieren, das nicht nur auf Softwareprototypen bezogen ist. Beim gemeinsamen „Design Thinking“ (Uebernickel et al. 2015) von Informatikern und Kulturwissenschaftlern haben wir die Erfahrung gemacht, dass erst und vor allem das häufige Evaluieren und Experimentieren dabei hilft, sich von technologischen und organisatorischen Restriktionen zu lösen und stattdessen sinnvolle Anwendungen zu identifizieren. Es müssen zudem technologische Konzepte entwickelt werden, die sowohl ein Experimentieren mit verschiedenen Varianten als auch eine Evolution von Software-Architektur und -Design und ein Reagieren auf fehlerhaften Code (Resilience) erlauben.
            
            
               Abgrenzung vom aktuellen Stand der Forschung und Technik
               Bisherige Ansätze in der Softwaretechnik sind vor allem mit Blick auf das
                                                            Extrahieren und Experimentieren unzureichend. Ein klassisches
                                                            Entwicklungsmodell in der Informatik ist das Wasserfallmodell (Royce 1970),
                                                            in dem bestimmte Phasen wie Anforderungserhebung, Systementwurf und
                                                            Implementierung lediglich einmal durchlaufen werden. Dieses Modell ist vor
                                                            allem für solche Entwicklungen geeignet, die bereits existente Prozesse
                                                            digitalisieren sollen. Wenn jedoch neue digitale Prozesse entwickelt werden
                                                            sollen, wirkt sich bei diesem Modell nachteilig aus, dass Fehlentwicklungen
                                                            erst am Ende des Prozesses sichtbar werden, also erst dann, wenn die
                                                            Software als Ganzes bereits fertig ist. Um schneller auf sich ändernde
                                                            Anforderungen reagieren zu können, wurden deshalb agile Methoden entwickelt,
                                                            deren Rahmen mithilfe des agilen Manifest (Beck et al. 2001) definiert
                                                            werden. Zur Grundidee der agilen Softwareentwicklung gehören kurze, feste
                                                            Iterationen mit de m Ziel, möglichst frühzeitig lauffähige Produktinkremente
                                                            auszuliefern. So können öfter Rückmeldungen vom Kunden eingeholt und
                                                            Fehlentwicklungen frühzeitig erkannt werden. Vor allem aber ist dieser
                                                            Prozess auch transparenter für den Kunden, da er regelmäßig Fortschritte
                                                            sieht. Da dieser Ansatz davon ausgeht, dass man zwangsläufig ‚scheitern‘
                                                            wird, soll das Scheitern im Kleinen stattfinden, um so Potenzierungseffekte
                                                            zu minimieren. Die agilen Methoden haben jedoch den Nachteil, dass sie
                                                            lediglich einen Rahmen bilden und keine Spezifika bieten, wie z. B. konkret
                                                            experimentiert werden kann oder soll. Als Anforderungen werden im agilen
                                                            Ansatz Scrum User Stories verwendet, die aus Nutzersicht die gewünschten
                                                            Funktionalitäten beschreiben. Um eine gute Produktqualität zu erreichen,
                                                            muss Scrum (Sutherland / Schwaber 2007) mit klassischen Ansätzen kombiniert
                                                            werden. 
               Im Gegensatz zu den hier erläuterten Ansätzen bietet das von uns vorgestellte
                                                              Modell durch den ständig wiederkehrenden Dialog der domänenübergreifenden
                                                              Akteure sowie das Experimentieren die Möglichkeit, die Entwicklung neuer
                                                              Methoden und Werkzeuge systematisch zu unterstützen. Gestützt von Prozessen
                                                              können so neue Technologien experimentell auf ihre Anwendbarkeit untersucht
                                                              werden. Ein Aspekt, der sich aktuell in der Projektarbeit der HiP-App bereits bestätigt hat.
            
            
               Übergeordnete offene Fragestellungen
               In ihren Anfängen zeichneten sich die Digital Humanities hauptsächlich durch die Übertragung bewährter Konzepte der Informatik aus. Es ist aber zu fragen, ob die Informatik nicht stärker von den Kulturwissenschaften lernen kann? Wäre es für die Informatik beispielsweise nicht hilfreich, verstärkt auch soziologische Methoden (qualitative Methoden wie Experteninterviews, quantitative Verfahren, etc.) für den Prozess der Anforderungserhebung und des Experimentierens zu adaptieren? Wie aber könnte das in der Softwareentwicklung praktikabel und systematisch angewandt werden? Diese offenen Fragestellungen gilt es weiterhin im Auge zu behalten.
            
         
      
      
         
            
               Bibliographie
               
                  Bawden, Tina (2014): Die Schwelle
                                                              im Mittelalter (= Sensus 4). Köln / Weimar / Wien:
                                                              Böhlau-Verlag.
               
                  Beck, Kent et al. (2001): Manifesto
                                                              for agile software development
                  http://agilemanifesto.org
                                                              [letzter Zugriff 20. Januar 2016].
               
                  Burdick, Anne / Drucker, Johanna / Lunenfeld, Peter /
                                                                Presner, Todd / Schnapp, Jeffrey (2012): Digital_Humanities. Cambridge: The MIT Press / Massachusetts
                                                                Institute of Technology.
               
                  Buschauer, Regine / Willis, Katharine S. (2013): Locative Media. Medialität und Räumlichkeit.
                                                                Multidisziplinäre Perspektiven zur Verortung der Medien. Bielefeld:
                                                                Transcript.
               
                  Fix, Ulla (2008): "Nichtsprachliches als Textfaktor.
                                                                Medialität, Materialität, Lokalität", in:  Zeitschrift für
                                                                Germanistische Linguistik 36, 3: 343–354. 
               
                  Fuchshuber-Weiß, Elisabeth (1996): "Straßennamen:
                                                                deutsch / Street Names: German / Noms de rues: domaine allemand", in:
                                                                Eichler, Ernst / Hilty, Gerold / Löffler, Heinrich / Steger, Hugo / Zgusta,
                                                                Ladislav (eds.): Namenforschung. Ein internationales
                                                                Handbuch zur Onomastik / Name Studies. An
                                                                International Handbook of Onomastics / Les noms
                                                                propres. Manuel international d'onomastique (= Handbücher zur
                                                                Sprach- und Kommunikationswissenschaft 11,2). Berlin: De Gruyter 1465-1468. 
               
                  Glasner, Peter (1999): "Ein sprachhistorischer
                                                                Beitrag zur Semiotik der Stadt: das Pilotprojekt 'Kölner Straßennamen'", in:
                                                                Muttersprache 109: 316-330. 
               
                  Habscheid, Stephan / Reuther, Nadine (2013):
                                                                "Performatisierung und Verräumlichung von Diskursen. Zur soziomateriellen
                                                                Herstellung von ‚Sicherheit‘ an öffentlichen Orten", in: Felder, Ekkehard
                                                                (ed.): Faktizitätsherstellung in Diskursen. Die Macht
                                                                des Deklarativen (= Sprache und Wissen 13). Berlin / New York: de Gruyter
                                                                127–145.
               
                  Hausendorf, Heiko / Mondada, Lorenza / Schmitt, Reinhold
                                                                  (eds.) (2012): Raum als interaktive Ressource
                                                                (= Studien zur Deutschen Sprache 62). Tübingen: Narr. 
               
                  Hüttermann, Michael (2012): DevOps
                                                                for Developers. New York: Apress. 
               
                  Kesselheim, Wolfgang (2010): "'Zeigen, erzählen und
                                                                dazu gehen': Die Stadtführung als raumbasierte kommunikative Gattung", in:
                                                                Costa, Marcella / Müller-Jacquier, Bernd (eds.): Deutschland als fremde Kultur. Vermittlungsverfahren in
                                                                Touristenführungen. München: Iudicum 244–271. 
               
                  Liedkte, Gerhard (1999): Abbestraße
                                                                bis Zwetschenweg. Straßennamen in Paderborn. Paderborn: H&S
                                                                Verlag. 
               
                  Lobbedey, Uwe (1986): Die
                                                                Ausgrabungen im Dom zu Paderborn 1978/80 und 1983 (= Denkmalpflege
                                                                und Forschung in Westfalen 11). Bonn. Habelt.
               
                  Lobbedey, Uwe (1999): Romanik in
                                                                Westfalen. Würzburg: Zodiaque-Echter.
               
                  Mayhew, Deborah J. / Follansbee, Todd J. (2012):
                                                                "User Experience Requirements Analysis within the Usability Engineering
                                                                Lifecycle", in: Jacko, Julie A. (ed.): The
                                                                Human-Computer-Interaction-Handbook. Boca Raton, FL, USA: CRC Press
                                                                945–953.
               
                  Müller, Marcus (2012): "Geschichte als Spur im Text",
                                                                in: Jacko, Julie A. / Bär, Jochen A. (eds.): Geschichte
                                                                der Sprache – Sprache der Geschichte. Probleme und Perspektiven der
                                                                historischen Sprachwissenschaft des Deutschen. Berlin 159–179.
               
                  Nübling, Damaris (2012): Namen. Eine Einführung in die Onomastik. Tübingen: Narr. 
               
                  O'Halloran, Kay L. (ed.) (2004): Multi-modal Discourse Analysis. Systemic Functional Perspectives.
                                                                London / New York. 
               
                  Pöppinghege, Rainer (2005): Geschichte mit Füßen getreten: Straßennamen und Gedächtniskultur in
                                                                Deutschland (= Paderborner Universitätsreden 94). Paderborn:
                                                                Universitätsverlag Paderborn. 
               
                  Pöppinghege, Rainer (2007): Wege
                                                                des Erinnerns. Was Straßennamen über das deutsche
                                                                Geschichtsbewusstsein aussagen. Münster: Agenda. 
               
                  Quednau, Ursula (2011): Handbuch
                                                                der deutschen Kunstdenkmäler. Nordrhein-Westfalen II. Westfalen.
                                                                Berlin / München: Deutscher Kunstverlag.
               
                  Royce, Winston W. (1970): "Managing the development
                                                                of large software systems", in: Proceedings of IEEE
                                                                WESCON 26, 8: 328-388.
               
                  Rüsen, Jörn (1996): "Historische Sinnbildung durch
                                                                Erzählen. Eine Argumentationsskizze zum narrativistischen Paradigma der
                                                                Geschichtswissenschaft und der Geschichtsdidaktik im Blick auf
                                                                nicht-narrative Faktoren", in: Internationale
                                                                Schulbuchforschung 18: 501–543.
               
                  Sauerländer Willibald (1971): "Die
                                                                kunstgeschichtliche Stellung der Figurenportale des 13. Jahrhunderts in
                                                                Westfalen", in: Westfalen 49: 1-76. 
               
                  Schüttpelz, Erhard (2006): "Die
                                                                medienanthropologische Kehre der Kulturtechniken", in: Engell, Lorenz /
                                                                Siegert, Bernhard / Vogl, Joseph (eds.): Kulturgeschichte
                                                                als Mediengeschichte (oder vice versa?). Weimar: Universitätsverlag
                                                                Weimar 87–110. 
               
                  Sharma, Sanjeev / Coyne, Bernie (2015): DevOps for Dummies. 2nd IBM Limited Edition. Hoboken:
                                                                John Wiley & Sons, Inc. 
               
                  Sutherland, Jeff / Schwaber, Ken (2007): The Scrum Papers. Nuts, Bolts, and Origins of an
                                                                Agile Method. Boston: Scrum, Inc.
               
                  Tack, Wilhelm (1958): "Die Paradies-Vorhalle des
                                                                Paderborner Domes und die Wallfahrt nach Santiago de Compostela", in: Alte und Neue Kunst im Erzbistum Paderborn 8:
                                                                27-62.
               
                  Uebernickel, Falk / Brenner, Walter / Pukall, Britta /
                                                                  Naef, Therese / Schindlholzer, Bernhard (2015): Design Thinking – Das Handbuch. Frankfurt am Main: Frankfurter
                                                                  Allgemeine Buch.
               
                  Ulrich, Hans / Krieg, Walter (³1974): St. Galler Management-Modell. Bern: Haupt.
               
                  Weber, Heike (2012): "Urbanisierung und Umwelt: Ein
                                                                  Plädoyer für den Blick auf Materialitäten, Ressourcen und urbane
                                                                  ,Metabolismen'", in: IMS. Informationen zur modernen
                                                                  Stadtgeschichte 2: 28-35.
               
                  Wilk, Nicole M. (2015): "'Gebäude erzählen
                                                                  Geschichte(n)'. Medienlinguistische und diskursgrammatische Untersuchung zur
                                                                  multimodalen Herstellung historischer Stadt-Räume durch Schilder, Pulte,
                                                                  Stelen, Mobile Tagging und Apps", in: Networks. Die
                                                                  Online-Schriftenreihe des Projekts mediensprache.net 72: http://www.mediensprache.net/networx/networx-72.pdf [letzter
                                                                  Zugriff 20. Januar 2016].
            
         
      
   



      
         Public History (vgl. einführend Zündorf 2010) ist im deutschsprachigen Raum ein noch junges Feld, die erste Professur wurde erst Ende 2012 in Heidelberg eingerichtet. Die Disziplin ist zurückzuführen auf die doppelte Erkenntnis, dass die Mehrheit der Fachstudierenden nicht in der Geschichtswissenschaft wird arbeiten können (und dementsprechend zielgerichtet in Vermittlungskompetenzen aller Art geschult werden muss) und dass die meisten HistorikerInnen sich zwar über mangelnde Aufmerksamkeit für ihr Fach nicht beklagen können, demgegenüber aber kaum wissenschaftlich valide Werkzeuge für den Umgang mit der Öffentlichkeit entwickelt wurden.
         Paradoxerweise scheint die Public History trotz ihres modernen Selbstanspruchs den Fehler der herkömmlichen Geschichtswissenschaft zu wiederholen: Die Digitalisierung ihrer Arbeit bleibt weit hinter den technischen Möglichkeiten zurück und beschränkt sich größtenteils auf die Erleichterungen einer erweiterten Schreibmaschine. Doch Öffentlichkeiten, die sie schon ihrem Namen nach im Blick hat, migrieren zusehends in den digitalen Raum der sozialen Netzwerke und sollten genau dort angesprochen werden.
         Eine Möglichkeit, die digitale Teilöffentlichkeit zu erreichen, bietet das soziale Netzwerk Twitter. Seit ungefähr sechs Jahren werden dort historische Ereignisse in je maximal 140 Zeichen zeitgenau nacherzählt, was unter den Bezeichnungen „Re-Entweetment“ oder auch “Twhistory” bekannt geworden ist. Dieses Potential des Medium wurde bislang fast ausschließlich von Laien genutzt, so über die Accounts 
                @TitanicRealTime und das MDR-Projekt 
                @9Nov89live, das über einen Tag eine fiktive Geschichte des Mauerfalls zeichnete. In jüngerer Zeit wird es aber zunehmend auch von einer geringen Zahl von (Public) Historians aktiv angeboten, beispielsweise für 
                @NRWHistory und das Zweitweltkriegsprojekt 
                @DigitalPast, zu dem parallel das Sachbuch “Als der Krieg nach Hause kam” (Hoffmann 2015) veröffentlicht wurde. Wahrscheinlich besser als jede andere Medienform bietet Twhistory die Möglichkeit der Erzählung in Echtzeit als nicht-textlichem Inhalt, über den Geschichte lebendig gemacht und vorhandenes historisches Interesse (re-)aktiviert werden kann.
            
         Insbesondere die Zeichenbegrenzung ist für das Re-Entweetment Chance und Risiko zugleich: die Einstiegsschwelle ist im Vergleich zu herkömmlichen Darreichungsformen (Buch, Museum) äußerst gering, zugleich besteht die Gefahr der Simplifizierung sowie der Falschdarstellung von Geschichte als Aneinanderkettung von Einzelereignissen. Trotz der mittlerweile international steigenden Projektzahl hat sich noch keine Best Practice ergeben, um diesen Risiken zu begegnen. Dadurch ist auch die Zahl der digitalen Tools für diesen Bereich noch sehr klein, die Liste der Desiderate an die Digital Humanities aber lang und äußerst divers. Beispielsweise sind für die Planung, die Sammlung, die Gesamtschau und die Quellenreferenzierung von Inhalten Datenbanken oder zumindest tabellarische Aufstellungen notwendig, für die noch keine Möglichkeit bestand, die aggregierten Inhalte auch automatisch mit der Twitter-Plattform zu verknüpfen.
         Dies hat sich mit der Bereitstellung der Software autoChirp geändert, die an der Kölner Informationsverarbeitung entwickelt wurde, um die Umsetzung entsprechender Twhistory-Projekte zu unterstützen. Zum einen vereinfacht autoChirp die Arbeit für die ErstellerInnen von Twitter-Timelines historischer Ereignisse, indem es eine Schnittstelle zum automatischen Upload von tabellarischen Sammlungen unterschiedlichen Formats anbietet. Dabei können neben dem gewünschten Datum, der genauen Uhrzeit und den Tweet-Text auch Bilder und Geolocations für den Tweet angegeben werden (vgl. Abb. 1). Auch können ganze Gruppen von Tweets per Mausklick auf eine neue Referenzzeit geschedulet werden.
         
            
         
         Abb. 1: Sceenshot des autoChirp-Web-Clients, mit dem eine Reihe von Tweets automatisch aus einer Tabelle geschedulet wurde. Das Web-Application-Frontend interagiert mit einer redundant angelegten Datenbank, um die Sicherung der in den verschiedenen Projekten generierten Tweets auch jenseits der Twitter-Plattform nachhaltig zu gewährleisten. 
         Die autoChirp-App wird zur Zeit mindestens von den Twitter-Projekten @DigitalPast (
                http://digitalpast.de/), @NRWHistory (
                http://nrwhistory.de/)und 
                @goals_from_past genutzt und dabei unter anderem auch in der Lehre eingesetzt. Dabei stehen die EntwicklerInnen im engen Austausch mit den AnwenderInnen, um das Potential für Weiterentwicklungen abzuwägen. Aktuell wird die Integration von autoChirp in das Tiwoli-Projekt (vgl.Fischer & Strötgen 2015) realisiert, was zeigt, dass nicht nur historische, sondern auch literaturwissenschaftliche Vorhaben von einer Unterstützung im Zugang zur Twitter-Plattform profitieren können. 
            
         
            Für einen niederschwelligen Einstieg läuft eine Instanz von autoChirp als Web-Application zur freien Nutzung unter 
                
               https://autochirp.spinfo.uni-koeln.de/
            . Dort finden sich auch ausführliche Tutorials zur Benutzung. Für Weiterentwicklungen steht der dokumentierte Code im Github-Verzeichnis 
                
               https://github.com/spinfo/autoChirp
             zur Verfügung.
            
      
      
         
            
               Bibliographie
               
                  Fischer, Frank / Strötgen, Jannik (2015):
                        „Wann ﬁndet die deutsche Literatur statt? Zur Untersuchung von Zeitausdrücken in großen Korpora“,
                        in: 
                        DHd 2015: Von Daten zu Erkenntnissen.
                    
               
                  Hoffmann, Moritz (2015): 
                        Als der Krieg nach Hause kam.
                        Berlin: Ullstein.
                    
               
                  Strötgen, Jannik / Gertz, Michael (2012):
                        „Temporal Tagging on Different Domains: Challenges, Strategies, and Gold Standards“,
                        in 
                        Proceedings of LREC 2012 3746–3753. 
                    
               
                  Zündorf, Irmgard (2010):
                        „Zeitgeschichte und Public History, Version: 1.0“,
                        in: 
                        Docupedia-Zeitgeschichte, 11.2.2010 
                        http://docupedia.de/docupedia/index.php?title=Public_History&oldid=68731 [letzter Zugriff 24. August 2016].
                    
            
         
      
   



      
         
            Einleitung
            Im vorliegenden Abstract stellen wir eine Methode sowie erste Ergebnisse der Analyse von Entitäten-Assoziationen realer Leserinnen und Leser vor.
            Literaturwissenschaftliche Rezeptions-, Lese- und Lesertheorien gehen seit ihren hermeneutischen und wirkungsästhetischen Anfängen (Schleiermacher 1838, insb. 309f.; Iser 1976) von professionellen (Dijkstra 1994), informierten (Fish 1970, 86), Modell- (Eco 1979) oder sogar idealen (Schmid 2005) Lesern aus (vgl. Willand, 2014). Diesen wird die Kompetenz zugeschrieben, idealerweise sämtliche Textmerkmale referentialisieren zu können, wobei je nach literaturtheoretischer Provenienz unterschiedliche Kontexte die Grundlage der Zuschreibungen an den Text bilden. Dazu gehören u.a. Informationen über den Autor oder über die sozialhistorischen Bedingungen der Textproduktion, über die Rezeptionsbedingungen, über Vorgänger- oder zeitgenössische Texte oder über Wissen aus dem Bereich der Literaturwissenschaftlerin bzw. des Lesers selbst. 
            An bestimmte Wissensbestände dieser 
                    realen Leserinnen und Leser literarischer Texte können wir uns durch eine computergestützte empirische Analyse von Rezeptionszeugnissen aus sozialen Medien annähern. Konkret ist unser Ziel die Rekonstruktion und Analyse der von literarischen Texten ausgelösten Assoziationen. Dabei beschränken wir uns auf die Assoziationen, die reale oder fiktive Entitäten betreffen, also etwa Personen des öffentlichen Lebens oder Figuren aus fiktionalen Werken.
                
            Die Plattform Goodreads bietet Leserinnen und Lesern die Möglichkeit des freien schriftlichen Austauschs über literarische Texte in einer großen Community. 55 Mio. Mitglieder haben bis 2017 über 50 Mio. Reviews geschrieben, wobei die Besprechungen die Inhalte der Bücher selbst und nicht - wie etwa bei Verkaufsplattformen wie Amazon - die Distribution, den Preis o.ä. fokussieren (Piper et al. 2015). 
         
         
            Verarbeitung
            Als Grundlage unserer Analysen  wurden die Reviews in einer lokalen Datenbank gespeichert. 
            Die Datenbank enthält 1,3 Millionen englischsprachige Reviews zu 5.481 besprochenen Texten. Die Reviews umfassen insgesamt etwa 150 Mio. Tokens, d.h. uns steht eine große Datenmenge zur Extraktion der Entitäten zur Verfügung. In einem ersten Schritten wurden die Reviews bereinigt: HTML-Tags wurden entfernt und Wiederholungen von mehr als dreimal dem gleichen Zeichen oder Wort auf drei reduziert. 
            Zur Extraktion der Entitäten aus den Reviews haben wir den Stanford Named Entity Recognizer (Finkel et al., 2005) verwendet. Der Tagger klassifiziert die gefundenen Entitäten in mehrere Klassen. Für uns ist die Klasse „PERSON“ relevant, da diese alle gefundenen Entitäten von Personen enthält.
            Im nächsten Schritt disambiguieren wir die extrahierten Entitäten, da z.B. ein Name wie “Harry” auf viele mögliche Träger des Namens verweisen kann. Mit Hilfe von UKB (Agirre et al., 2009) und UKB-wiki (Agirre et al., 2015) können den Entitäten Wikipedia-Seiten zugeordnet werden, welche die möglichen Entitäten repräsentieren. Für diese Disambiguierung verwendet UKB den PageRank-Algorithmus (Page et al. 1999), der Dokumente nach ihrem Verlinkungsgrad bewertet. Sobald Namen wie „Ron“ und „Dumbledore“ im selben Kontext erwähnt werden, wird die Wahrscheinlichkeit größer, dass mit “Harry” 
                    Harry Potter, mit “Ron” 
                    Ron Weasly und mit “Dumbledore” 
                    Albus Dumbledore aus der Romanreihe 
                    Harry Potter gemeint sind, weil diese Entitäten aus dem selben Kontext kommen und dies in der Wissensbasis Wikipedia durch Verlinkungen explizit ablesbar und quantifizierbar ist.
                
            UKB-wiki stellt einen herunterladbaren Graphen zur Verfügung, der Wikipedia-Seiten und Links auf andere Wikipedia-Seiten repräsentiert. In einem mitgelieferten Wörterbuch sind Entitäten mit allen möglichen Entitäten (Verweise auf Wikipedia Seiten) aufgeführt. 
            Die auf diese Weise gewonnen Wikipedia-Einträge wurden anschließend hinsichtlich des ontologischen Status der referenzialisierten Entität kategorisiert, also ob es sich um eine reale Person oder fiktionale Figur handelt. Dazu wurde die Wissensbasis DBpedia
                     verwendet, die die Daten aus Wikipedia strukturiert und maschinenlesbar kodiert. Da die Disambiguierung Wikipedia-Einträge liefert, können wir anhand dieser die auf den zugehörigen DBpedia-Eintrag zugreifen. Über DBpedia lassen sich neben ontologischen Kategorien  auch andere Eigenschaften extrahieren, die für eine Analyse ggf. interessant sind, etwa Geschlecht oder Relationen zu anderen Figuren.
                
            Die extrahierten Daten werden zunächst als Tabelle gespeichert und erlauben somit eine flexible weitergehende Verarbeitung, etwa in einem Netzwerk. Eine Zeile der Tabelle besteht aus dem Werktitel, der disambiguierten Entität (Verweis auf Wikipedia Seite), einer Liste der extrahierten Entitäten aus den Reviews, einer Liste von Review-IDs, um nachvollziehen zu können in welchen Reviews der Name erwähnt wird, der Anzahl der Erwähnungen und der Angabe ob es sich um eine Figur handelt oder nicht.
            
               
                  Titel
                  Disambiguierte Entität (Verweis auf Wikipedia Seite)
                  Extrahierte Entität
                  Review IDs
                  Anzahl der Erwähnungen
                  Handelt es sich um eine fiktionale Figur?
               
               
                  The Hound of the Baskervilles
                  Agatha_
                            Christie
                  christie, agatha_
                            christie, agatha_
                            christy
                  4707841
                            20, …, 1886
                            08568
                  20
                  False
               
               
                  The Hound of the Baskervilles
                  Spock
                  spock
                  429714
                            73
                  1
                  True
               
               
                  The Hound of the Baskervilles
                  Robert_Downey,_Jr.
                  robert_downey_jr, robert_downey
                  107754
                            3609, …, 125
                            0976986
                  18
                  False
               
               
                  The Hound of the Baskervilles
                  Ann_Radcliffe
                  ann_radcliffe
                  435380
                            655
                  1
                  False
               
            
            
               Tabelle 1: Auszug aus den extrahierten Daten. Die extrahierten Entitäten stammen aus den Reviews zu
                     The Hound of the Baskervilles.
                
            
               Zwischenergebnisse
               Um ein exemplarisches Resultat zu präsentieren, haben wir Reviews zu “The Hound of the Baskervilles” (deutsch: “Der Hund von Baskerville”) analysiert. Unter den häufig erwähnten Entitäten finden sich erwartungsgemäß Sherlock Holmes, Dr. Watson, sowie der Autor Arthur Conan Doyle. Weitere häufig erwähnte Figuren aus der fiktionalen Welt des Sherlock Holmes' sind James Mortimer und Charles Baskerville. Aber auch Professor Moriarty wird häufig erwähnt, obwohl er in diesem Buch der Sherlock-Reihe gar nicht auftaucht. Das System erzeugt jedoch auch Fehler. Beispielsweise wird der Antagonist Stapleton zwar sehr oft erwähnt, da zu ihm aber kein eigener Wikipedia-Eintrag existiert, wird er fälschlicherweise mit dem Footballspieler Frank Stapleton verknüpft. Henry Baskerville, der Sohn von Charles und Erbe des Anwesens, wird im Buch fast durchgehend als Sir Henry bezeichnet, und kommt mit diesem Namen ebenfalls häufig in den Reviews vor. Da auch für ihn kein eigener Wikipedia-Eintrag existiert und der Name Henry extrem mehrdeutig ist, werden eine Reihe klar falscher Entitäten verknüpft: Henry II. von Frankreich; Henry County (Alabama); oder Henry I. von England.
               Bemerkenswert sind insbesondere jedoch die referenzialisierten extra-textuellen Entitäten, also diejenigen, die nicht aus der fiktionalen Welt Sherlocks stammen. Es finden sich etwa 
                        Hercule Poirot und 
                        Agatha Christie unter den erwähnten Entitäten, was als klares Zeichen dafür gesehen werden kann, dass die Leserinnen und Leser den Text vor dem Hintergrund eines starken Gattungsbewusstseins rezipieren. Dafür spricht auch, dass mit 
                        Benedict Cumberbatch, 
                        Robert Downey, Jr., 
                        John Barrymore und 
                        Jeremy Brett gerade die Schauspieler unter den assoziierten Referenzen vertreten sind, die in einer der vielen Verfilmungen die Rolle des Sherlock Holmes verkörpert haben.
                    
            
            
               Fehleranalyse
               Das am häufigsten auftretende Problem ist das Fehlen eines Wikipedia-Eintrages für eine Figur. In der englischsprachigen Wikipedia sind fiktionale Figuren zwar nicht per se davon ausgeschlossen -- Richtschnur hier ist deren “Notability”. Viele Figuren sind jedoch nur auf den Einträgen des entsprechenden Werks erwähnt. Da der Algorithmus nicht in der Lage ist, 
                        keinen Eintrag zu liefern, wird in solchen Fällen eben ein anderer Eintrag verwendet, auch wenn dieser relativ weit entfernt sein mag. Eine technische Lösung wäre sicher, nur ab einem gewissen Schwellwert eine Disambiguierung vorzunehmen, und die nicht-disambiguierten Einträge zumindest als solche erkennen zu lassen. Eine andere Möglichkeit läge in der (zusätzlichen) Verwendung von Literaturlexika, die (womöglich) eine größere Abdeckung zu fiktionalen Figuren aufweisen. Beide Optionen werden wir in zukünftigen Arbeiten genauer untersuchen. 
                    
               Da es sich bei den Reviews letztlich um Inhalte aus einem sozialen Medium handelt, kommt es auch vor, dass Namen falsch geschrieben werden oder gar der gesamte Text schriftsprachliche Konventionen übergeht. Prima vista sind diese Fälle im Vergleich zu Buchrezensionen zwar häufig anzutreffen, wir können das Problem aber umgehen, indem wir nur diejenigen Erwähnungen berücksichtigen, die mehr als einmal vorkommen. Festzuhalten bleibt aber ebenfalls, dass die Texte im Vergleich zu z.B. Twitter-Daten deutlich sauberer sind.
               Eine weitere mögliche (jedoch noch nicht tatsächlich beobachtete) Fehlerquelle liegt in der Natur des PageRank-Algorithmus: Wenn eine Figur in einem Werk existiert, ein Leser oder eine Leserin jedoch explizit z.B. eine Person des öffentlichen Lebens mit dem gleichen Namen erwähnt, wird der Algorithmus diese Erwähnung eher der Figur zuschlagen, da diese dichter mit anderen Figuren verknüpft ist. 
            
         
         
            Auswertung als Netzwerk
            Die oben extrahierten Daten erlauben Auswertungen auf vielfältige Weise. Exemplarisch konzentrieren wir uns hier auf eine Form, in der von Lesern zugeschriebene Gemeinsamkeiten zwischen literarischen Texten untersucht werden. Die Texte und die ihnen zugeschriebenen Assoziationen werden dabei als Knoten in einem Netzwerk repräsentiert. Ein Text ist also mit allen ihm zugeschriebenen Assoziationen verbunden, wobei das Gewicht der Kante die Anzahl der Reviews angibt, in denen eine bestimmte Assoziation auftaucht. 
            Durch diesen Aufbau ergeben sich Kerneigenschaften des Netzwerkes, die bei der Analyse zu beachten sind: Ein Teil der erwähnten Entitäten sind 
                    intratextuelle Referenzen, d.h. Figuren aus dem jeweiligen Text selbst (Veldhues, 1995). Auch wenn diese keine 
                    intertextuellen Assoziationen und damit nur sekundäres Extraktionsziel sind, behandeln wir sie als gleichwertige Assoziationen
                    . 
                
            Figuren, die in mehr als einem Werk auftauchen (z.B. 
                    Sherlock Holmes oder 
                    Harry Potter) bilden eine hoch gewichtete Verbindung zwischen den Texten einer literarischen Reihe, wobei Reihen durch die von ihnen geteilte fiktionale Welt markiert sind. Als gemeinsamer Assoziationsraum sind sie aufgrund der hohen Gewichtung auch angemessen im Netzwerk repräsentiert.
                
            Durch die gemeinsame Darstellung der Werke und assoziierten Entitäten ergeben sich -- bei Auswahl eines geeigneten Layout-Algorithmus z.B. in Gephi
                     -- eng zusammenhängende Gruppen von Werken. Das hier exemplarisch angeführte Resultat eines engen Zusammenhangs repräsentiert jedoch nicht bestimmte Texteigenschaften selbst, sondern lediglich von Leserinnen und Lesern gemeinsam gemachte Zuschreibungen an diese Texte.
                
            Das hier beschriebene Netzwerk wird im Zuge der Konferenz frei zugänglich gemacht.
            
               
                  
                  Abbildung 1: Assoziationen zu Conan Doyles The Hound of the Baskervilles, extrahiert aus den Reviews von Benutzern. Die Abbildung zeigt zur Illustration sämtliche assoziierte Entitäten, unabhängig von der Häufigkeit.
               
            
         
         
            Nächste Schritte
            Durch den Zugriff auf bisher undenkbar große Rezeptionsdatenmengen erhält die empirische Leseforschung einen sie fundamental erweiternden Impetus, war sie methodisch betrachtet bisher überwiegend auf Fragebögen
                     und peripheriephysiologische Messungen angewiesen, jüngst gestützt durch bildgebende Verfahren. Computerlinguistische Methoden der Sprach- und Korpusverarbeitung versprechen nicht nur die Analyse unlesbarer Mengen an Rezeptionszeugnissen, sondern auch eine Modellierung leserattribuierter Kontexte literarischer Texte und somit einen ersten Einblick in die bisher unbeantwortete Frage, mit welchem Vorwissen echte Leser eigentlich lesen.
                
            In diesem Sinne präsentiert das eingereichte Paper erste, jedoch bereits substanzielle Ergebnisse. 
            Die nächsten Schritte leiten sich direkt aus der oben diskutierten Fehleranalyse ab. Zum einen soll die Wissensbasis um fiktionale Figuren aus den Werken erweitert werden (was z.B. über 
                    named entity recognition über den Volltexten machbar wäre). Zum anderen soll der Algorithmus in die Lage versetzt werden bestimmte (fehlerhafte) Zuweisungen zurückzuweisen, etwa mit einem zu definierenden 
                    threshold.
                
         
      
      
         
            http://wiki.dbpedia.org/
            Das Filtern von innertextuellen Figuren ist technisch möglich (Beck, 2017), aber zeitaufwändig und für die hier vorgestellte Nutzung als Explorationswerkzeug letztlich unnötig.
            https://gephi.org
            Groeben 1979; Baurmann 1981; Funke 2003; Christmann u. Schreier 2003; Wübben 2009 u.v.m.
         
         
            
               Bibliographie
               
                  Agirre, Eneko / Soroa, Aitor (2009): “Personalizing PageRank for Word Sense Disambiguation”, in: Proceedings of the 12th conference of the European chapter of the Association for Computational Linguistics (EACL-2009). Athens, Greece.
                    
               
                  Agirre, Eneko / Barrena, Ander / Soroa, Aitor (2015): Studying the Wikipedia Hyperlink Graph for Relatedness and Disambiguation. http://arxiv.org/abs/1503.01655
                    
               
                  Beck, Jens (2017): How do People Read Literature? - Detection and Identification of Names in Book Reviews. Bachelor’s thesis, Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart.
                    
               
                  Baurmann, Jürgen (1981). „Textrezeption empirisch. Wege zu einem ziel, behelfsbrücken oder holzwege?". Rezeptionspragmatik. Beiträge zur Praxis des Lesens. Uni-Taschenbücher. Band 1026. Hrsg. v. Gerhard Köpf, 201–218. München.
                    
               
                  Christmann, Ursula / Margrit Schreier (2003). „Kognitionspsychologie der Textverarbeitung und Konsequenzen für die Bedeutungskonstitution literarischer Texte". Regeln der Bedeutung. Zur Theorie der Bedeutung literarischer Texte. Revisionen. Hrsg. v. Fotis Jannidis, Gerhard Lauer, Matías Martínez & Simone Winko, 246–284. Berlin.
                    
               
                  Dijkstra, Katinka (1994): Leseentscheidung und Lektürewahl. Empirische Untersuchungen über Einflussfaktoren auf das Leseverhalten. Berlin.
                    
               
                  Dimitrov, Stefan / Zamal, Faiyaz / Piper, Andrew / Ruths, Derek (2015): “Goodreads vs Amazon: The Effect Of Decoupling Book Reviewing And Book Selling", in: International Conference on Web and Social Media (ICWSM-14).
                    
               
                  Eco, Umberto (1979): The Role of the Reader. Explorations in the Semiotics of Texts. Bloomington, IN.
                    
               
                  Finkel, Jenny Rose / Grenager, Trond / Manning, Christopher (2005): “Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling”, in: 
                        Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370.
                    
               
                  Fish, Stanley E. (1970): „Literature in the Reader: Affective Stylistics“, in: 
                        New Literary History 1(2): 123–162.
                    
               
                  Funke, Mandy (2003). „Das Abenteuer der Fragebögen. Aspekte zur empirischen Wirkungsforschung in der DDR". Wissenschaft und Systemveränderung. Rezeptionsforschung in Ost und West – Eine konvergente Entwicklung? Euphorion. Band 44. Hrsg. v. Wolfgang Adam, Holger Dainat & Gunther Schandera, 119–126. Heidelberg.
                    
               
                  Groeben, Norbert (1979). „Zur Relevanz empirischer Konkretisationserhebungen für die Literaturwissenschaft". Empirie in Literatur- und Kunstwissenschaft. Grundfragen der Literaturwissenschaft. Hrsg. v. Siegfried J. Schmidt, 43–82. München.
                    
               
                  Iser, Wolfgang (1976). Der Akt des Lesens. Theorie ästhetischer Wirkung. Band 636. München.
                    
               
                  Page, Lawrence / Brin, Sergey / Motwani, Rajeev / Winograd, Terry (1999): “The PageRank Citation Ranking: Bringing Order to the Web”
                        , technical Report. Stanford InfoLab.
                    
               
                  Schleiermacher, Friedrich (1838): Hermeneutik und Kritik mit besonderer Beziehung auf das Neue Testament. Aus Schleiermachers handschriftlichem Nachlasse und nachgeschriebenen Vorlesungen herausgegeben von Friedrich Lücke. In: Friedrich Schleiermacher’s sämmtliche Werke. Berlin: Reimer.
                    
               
                  Schmid, Wolf (2005): Elemente der Narratologie. Narratologia. Band 8. Berlin.
                    
               
                  Veldhues, Christoph (1995): "Gleich- und Gegenüberstellung".Intratextuelle und intertextuelle Bedeutung in der Literatur. Zeitschrift für französische Sprache und Literatur 40/3 (1995), 243-267.
                    
               
                  Willand, Marcus (2014): Lesermodelle und Lesertheorien. Historische und systematische Perspektiven. Narratologia. Band 41. Berlin.
                    
               
                  Wübben, Yvonne (2009). „Lesen als Mentalisieren? Neuere kognitionswissenschaftliche Ansätze in der Leseforschung". Literatur und Kognition. Bestandsaufnahmen und Perspektiven eines Arbeitsfeldes. Poetogenesis. Band 6. Hrsg. v. Martin Huber & Simone Winko, 29–44. Paderborn.
                    
            
         
      
   



      
         Alle, die mit maschinellen Methoden Sprache analysieren, erleben momentan einen tiefgreifenden methodologischen Wandel. 
                Einerseits erfreut man sich vielleicht als Geisteswissenschaftler/in am immer stärkeren Interesse der Ingenieurtechniken und der Informatik für Sprache. Dies kann durchaus als Erfolg der Linguistik betrachtet werden, zurückgehend auf den Linguistic Turn, der viele andere Disziplinen schon seit Jahrzehnten beeinflusst. Unternehmen interessieren sich für ihre Reputation im massenmedialen Diskurs oder sind der Überzeugung, ihr in unzähligen Dokumenten versprachlichtes Wissen besser verwalten zu können, wenn sie es nach sprachlichen Kriterien neu ordnen. Das Geschäftsmodell von Internetunternehmen basiert ganz erheblich darauf, sprachliche Kommunikation maschinell zu verarbeiten um daraus Wissen aufzubauen und Vorhersagen über das Handeln von Kunden zu machen. Auch in der Politik ist die Analyse von Sprachgebrauch ein wichtiger Faktor, um Wahlkämpfe zu gewinnen.
            
         Andererseits beschert einen dieses Interesse eine Vielzahl von neuen Methoden für die maschinelle Analyse von Text, die auch für geisteswissenschaftliche Fragestellungen interessant sind. Die Digital Humanities sind ein Beispiel für eine Disziplin, die sich den Experimenten mit diesen Methoden verschrieben hat. Auch die Korpuslinguistik profitiert maßgeblich von diesen neuen Methoden.
         Aktuell erfahren in der Computerlinguistik und generell im Data Mining neuronale Netze großen Zuspruch, die den Prozess des maschinellen Lernens nach dem Modell des menschlichen Gehirns gestalten. Solche Systeme, „Deep Learning“-Systeme genannt, sind in der Lage, Muster in den Daten zu erkennen, ohne dass vorher explizit die Eigenschaften festgelegt werden, die getestet werden sollen. Zudem findet das Lernen auf mehreren verborgenen Ebenen statt, so dass das Lernen nicht beobachtet und damit auch die Frage, welche Eigenschaften nun welchen Einfluss auf das gelernte Modell haben, kaum beantwortet werden kann.
         In der Computerlinguistik wurden bereits für viele Probleme Deep-Learning-Algorithmen eingesetzt, meist mit Erfolg. Erfolg bedeutet, dass die statistischen Modelle besser den Goldstandard voraussagen können, aber nicht, dass das grundlegende linguistische Problem (z.B.: Sentiment-Analyse: wie werden Gefühle und Meinungen ausgedrückt; Textklassifikation: wie drückt sich Stil, Autorschaft, Textsorte, Thema etc. aus) besser gelöst wäre.
         Überall wo Sprachgebrauch quantitativ und maschinell analysiert wird, gibt es einen starken Trend, möglichst ohne linguistischen Kategorien und Theorien auszukommen und Black-Box-Systeme zu verwenden. Das ist nachvollziehbar, da es in den meisten Fällen darum geht, ein System zu bauen, das eine klar definierte Aufgabe sehr gut lösen kann. Obwohl diese Ansätze natürlich auch für die Korpuslinguistik interessant sind, genügen sie linguistischen Forschungsinteressen eigentlich nicht, da sie keinen Beitrag dazu leisten, sprachliche Phänomene zu verstehen und erklären zu können.
         Viel dramatischer ist jedoch, dass die Linguistik offensichtlich nicht in der Lage ist, einen nützlichen Beitrag zur Lösung der Probleme der maschinellen Textanalyse zu leisten. Die Linguistik scheint für die quantitative Analyse von Text weitgehend bedeutungslos zu werden.
         Um der Bedeutungslosigkeit zu entgehen, muss die Linguistik ein kritisches Verhältnis zur Forschungslogik in den ingenieurstechnischen Disziplinen pflegen und auf zwei Prinzipien bestehen: 1) Mehr linguistische Theorie. 2) Ergebnisse von quantitativen Analysen müssen gedeutet werden.
         Zu 1): Nicht nur für die Linguistik, sondern für alle geistes- und sozialwissenschaftlichen Disziplinen gilt: Eine theoretische Fundierung der Analysekategorien ist essentiell. Dafür werden valide Analysekategorien benötigt, die deutbar sind. Dieses Prinzip richtet sich jedoch keinesfalls gegen datengeleitete Verfahren, im Gegenteil: Sie sind es, die die theoretischen Modelle herausfordern und schärfen können. Aber das Ziel aller Analysen muss darin liegen, ein Puzzleteil zu einem besseren Verständnis sprachlicher Strukturen, von Sprachgebrauch oder gesellschaftlichen und kulturellen Bedeutungen von Sprache zu führen. Wir benötigen White-Box-, nicht Black-Box-Systeme.
         Das Problem der fehlenden Validität zeigt sich z.B. im Feld der sog. „Authorship Attribution“, also der Zuordnung eines Textes X zu einem Autor A, B, C, …. Um dies zu tun, stehen Texte zur Verfügung, von denen die Autorschaft bekannt ist. Die Frage ist dann also, ob über die sprachlichen Merkmale des Textes X automatisch bestimmt werden kann, wer der Autor/die Autorin (aus der Menge der möglichen Autoren/innen) von Text X ist. Genauer lautet die Frage aber, ob und wie sich persönlicher Schreibstil sprachlich niederschlägt.
         Besonders erfolgreich für diese Aufgabe sind Methoden maschinellen Lernens, die das Problem als Klassifikationsaufgabe auffassen und anhand von Trainingskorpora typische sprachliche Merkmale der Texte der jeweiligen Autor/innen lernen. Dabei zeigt sich, dass „low-level features like character n-grams are very successful for representing texts for stylistic purposes” (Stamatatos, 2009, S. 24). 
                
                Das bedeutet, solche Modelle, die auf der Distribution von Buchstaben-N-Grammen beruhen, sind, gemessen an einem Goldstandard, am erfolgreichsten. Allein: Solche Modelle lassen sich nicht linguistisch deuten, da völlig unklar ist, was sie eigentlich messen. Ist es Stil, Thema, Textsorte, …? Es handelt sich also weder um eine valide, noch um eine deutbare Kategorie (insbesondere, wenn das statistische Modell nicht einsehbar ist). Für spezifische Aufgaben der Autorschaftsattribution mag das ausreichend sein, aber bereits für forensische Anwendungen, beispielsweise vor Gericht, ist eine solche Modellierung fragwürdig und gefährlich. Und für eine linguistische Deutung des Phänomens Autorschaftsstil ist sie gänzlich unbrauchbar.
         
         Die Kritik geht jedoch nicht nur in Richtung des Textminings und der Computerlinguistik, manchmal nicht-valide Kategorien einzusetzen (was zudem oft für die dortigen Zwecke auch sinnvoll ist), sondern auch in die Richtung der Linguistik: Die Computer- und die Korpuslinguistik zeigen beide gleichermaßen, wie wichtig es ist, auch abstrakte Kategorien so zu definieren versuchen, dass überhaupt eine Chance besteht, sie für eine quantitative Analyse operationalisierbar zu machen. Wenn eine linguistische Kategorie so vage ist, dass sich selbst (geschulte) Menschen uneinig darüber sind, wenn sie an authentischem Sprachegebrauch angewendet werden, scheitert die quantitativ-maschinelle Lösung unweigerlich.
         2) Die Ergebnisse von quantitativen Analysen sind nicht Antworten auf Fragestellungen, sondern neue Daten, die vor einem geistes- und sozialwissenschaftlichen Hintergrund genauso hermeneutisch gedeutet werden müssen, wie einzelne Texte. Das ist vielleicht das größte Missverständnis, wenn Textminer und Computerlinguistinnen mit Korpuslinguistinnen zusammenarbeiten: Erstere wollen, dass ein Werkzeug ein Ergebnis hervorbringt, das an einem Goldstandard evaluiert werden kann. Das Ergebnis ist dann im Einzelfall richtig oder falsch und in der Gesamtheit genügend präzise oder nicht. Das Ergebnis ist dann auch im Idealfall die Lösung der Forschungsfrage. Bei den meisten geistes- und sozialwissenschaftlichen Fragestellungen beginnt auf der Grundlage dieser Ergebnisse jedoch ein Interpretationsprozess, um (meist in Kombination mit weiteren Analysen) eine plausible Deutung zu ermöglichen – eine vorläufige Deutung. Die Stärke der Geistes- und Sozialwissenschaften liegt dabei ja gerade darin, dass in ihrer Methodologie ein Zweifeln inhärent ist, mit dem die „gegenwärtig besiegelten Bedeutungen jeweils eingeklammert oder angezweifelt [werden], um zu prüfen, inwiefern sich nach rationalem Ermessen nicht bessere Lösungen, überlegenere Interpretationen oder zustimmungsfähigere Regelungen finden lassen“ (Honneth, 2016, S. 312). 
                
            
         Neben der Suche nach validen Analysekategorien und dem Hochhalten geisteswissenschaftlicher Prinzipien der Deutung sehe ich einen weiteren Aspekt, der helfen sollte, der Korpuslinguistik eine deutliche linguistische Prägung zu verleihen. Es ist der Versuch, korpuslinguistisches Arbeiten als „diagrammatisches Operieren“ aufzufassen. Mit dem Diagramm-Begriff folge ich Krämer (2016), 
                
                die deutlich macht, dass Diagramme als Formen der Visualisierung von Daten „Denkzeuge“ sind, mit denen operiert wird: Ich kann Daten in einem Diagramm darstellen (auf einer Karte, in einem Netzwerkgraph, einem Punkteplot, …) und danach damit operieren, um neue Erkenntnisse daraus zu ziehen. Wenn man einem breiten Diagramm-Begriff folgt, wird deutlich, dass auch Listen, Tabellen und dergleichen diagrammatischen Charakter haben (Siegel, 2009; Steinseifer, 2013). 
                
                Dies sind nun aber Formen, die in der Korpuslinguistik zentral sind: Die Keyword in Context-Liste (zurückgehend etwa auf Zettelkästen im 16. Jahrhundert) etwa kann als Keimzelle eines völlig neuen Textverständnisses angesehen werden, mit dem die Einheit des Textes zerstört wird, um eine neue Sicht auf Textdaten zu gewinnen. Viele weitere Formen der Anordnung von Textdaten spielen ebenfalls wichtige Rollen, entscheidend etwa die Überführung von Textdaten in den Vektorraum, in dem operiert werden kann (z.B. in Form geometrischer Operationen – Lagen von Vektoren und ihren Winkeln zueinander). Aber auch die Erfindung der Partiturdarstellung bei Gesprächstrankripten, mit der überhaupt erst eine moderne Gesprächslinguistik möglich wurde, zeigt die Kraft von diagrammatischen Umformungen, um Daten neu lesbar zu machen. Hinter diesen diagrammatischen Umformungen stecken diagrammatische Grundfiguren (Bubenhofer im Druck b), die in den Geisteswissenschaften generell wirkmächtig sind.
            
         Ich meine, es lohnt sich, korpuslinguistisches Arbeiten unter diagrammatischer Perspektive zu reflektieren, um die Mechanismen und Möglichkeiten der Gegenstandskonstitution besser zu verstehen. Die Digitalität der Daten und Methoden erlaubt dabei neue Transformationen und macht Daten, egal welcher Modalität, miteinander verrechenbar. Aber die diagrammatischen Grundfiguren führen zu unterschiedlichen Gegenständen: Repräsentiert in einem Vektorraum geben die gleichen Daten einen völlig anderen Gegenstand ab als dargestellt in einer Keyword in Context-Liste. Und es müsste vordringliches Ziel sein, noch ganz andere Formen der diagrammatischen Darstellung von Text zu finden, um damit andere Gegenstandskonstitutionen und Fragestellungen zu ermöglichen. Die algorithmische Repräsentation der Daten folgt dabei ebenfalls den diagrammatischen Transformationen (Beispiel Vektorraum) und kann daher nicht unabhängig davon gedacht werden. Für eine hermeneutische Deutung brauchbare Analysekategorien zu erarbeiten, bedeutet deshalb auch, die damit verbundenen diagrammatischen Operationen zu reflektieren. Dafür nötig sind semiotische und natürlich auch wissenschaftstheoretische Überlegungen, die für alle Disziplinen, die mit maschineller Textanalyse befasst sind, relevant sein müssten.
      
      
         
             Dieses „extended Abstract“ ist eine verkürzte und angepasste Fassung des stärker linguistisch ausgerichteten Beitrages von Bubenhofer (im Druck a).
             Vgl. für eine aktuelle linguistisch motivierte Diskussion von stilometrischen Messmethoden für die Autorschaftsattribution Büttner et al. (2017). 
         
         
            
               Bibliographie
               
                  Bubenhofer, Noah (im Druck a): Wenn „Linguistik“ in „Korpuslinguistik“ bedeutungslos wird. Vier Thesen zur Zukunft der Korpuslinguistik. In: Osnabrücker Beiträge zur Sprachtheorie (OBST).
               
                  Bubenhofer, Noah (im Druck b): Visual Linguistics: Plädoyer für ein neues Forschungsfeld. In: Bubenhofer, Noah / Kupietz, Marc (Hg.): Visualisierung sprachlicher Daten. Heidelberg: HeiUP.
               
                  Büttner, Andreas / Dimpel, Friedrich Michael / Evert, Stefan / Jannidis, Fotis / Pielström, Steffen / Proisl, Thomas / Reger, Isabella / Schöch, Christof / Vitt, Thorsten (2017): »Delta« in der stilometrischen Autorschaftsattribution. In: Zeitschrift für digitale Geisteswissenschaften. text/html Format. DOI: 10.17175/2017_006.
               
                  Honneth, Axel (2016): Denaturierung der Lebenswelt. Vom dreifachen Nutzen der Geisteswissenschaften. In: Panteos, A./Rojek, T. (Hrsg.): 
                        Texte zur Theorie der Geisteswissenschaften, 
                        Reclams Universal-Bibliothek. Stuttgart : Reclam, S. 283–315
                    
               
                  Krämer, Sybille (2016): 
                        Figuration, Anschauung, Erkenntnis: Grundlinien einer Diagrammatologie. Frankfurt/Main: Suhrkamp Verlag.
                    
               
                  Nakov, Preslav/Ritter, Alan/Rosenthal, Sara/Stoyanov, Veselin/Sebastiani, Fabrizio (2016): SemEval-2016 Task 4: Sentiment Analysis in Twitter. In: 
                        Proceedings of the 10th International Workshop on Semantic Evaluation, 
                        SemEval ’16. San Diego, California : Association for Computational Linguistics.
                    
               
                  Siegel, Steffen (2009): 
                        Tabula: Figuren der Ordnung um 1600. Berlin / Boston : Akademie-Verlag.
                    
               
                  Stamatatos, Efstathios (2009): A Survey of Modern Authorship Attribution Methods. In: 
                        J. Am. Soc. Inf. Sci. Technol. Bd. 60, Nr. 3, S. 538–556
                    
               
                  Steinseifer, Martin (2013): Texte sehen – Diagrammatologische Impulse für die Textlinguistik. In: 
                        Zeitschrift für germanistische Linguistik Bd. 41, Nr. 1, S. 8–39
                    
            
         
      
   



      
         
            Hintergrund und Zielsetzung
            Am 13. Oktober 2016 gab die Schwedische Akademie bekannt, dass sie den Nobelpreis in Literatur an Bob Dylan „für seine poetischen Neuschöpfungen in der großen amerikanischen Songtradition“ verleihen werde. 
                    
                    Dass Dylan als Musiker und Songwriter den Literaturnobelpreis erhielt wurde mitunter sehr kontrovers diskutiert. Auf Kritik stieß z.B. die unzulässige Herauslösung von Dylans Texten aus der Musik und die Deutung seiner Lieder als Gedichte. 
                    
                    Unbestritten ist nichtsdestotrotz Bob Dylans Rolle als einer der einflussreichsten Musiker des 20. Jahrhunderts. 
                    
                
            Die (welt-)politischen Entwicklungen, die das Schaffen Dylans inspirierten, sind im Kontext seines Wirkens umfassend diskutiert worden, unter anderem in „Bob Dylan und die sechziger Jahre: Aufbruch und Abkehr“, 
                   
                    und liefern noch immer Diskussionsstoff, wie etwa eine in jüngerer Zeit erschienene Arbeit von Taylor & Israelson (2015) 
                    
                   über Dylans politische Einflüsse zeigt. Erfolgte die Beschäftigung mit Dylans Werk bislang allenfalls episodisch, so muss eine systematische Analyse des Gesamtwerks als Desiderat gelten, welches in gewisser Weise bereits von Bob Dylan selbst formuliert wurde:
                
            All these people who say whatever it is I’m supposed to be doing – that’s all gonna pass, because, obviously, I’m not gonna be around forever. That day’s gonna come when there aren’t gonna be any more records, and then people won’t be able to say ‘Well, this one’s not as good as the last one.’ 
                    They’re gonna have to look at it all (eigene Hervorhebung). And I don’t know what the picture will be, what people’s judgement will be at that time. I can’t help you in that area. 
                    – Bob Dylan 
                
            
            Dieser Beitrag erprobt, inwiefern mithilfe digitaler Methoden im Sinne des 
                    Distant Reading-Paradigmas ein neuer Zugang zu Dylans Gesamtwerk geschaffen werden kann. Bezugnehmend auf das Konferenzmotto einer „Kritik der Digitalen Vernunft“ soll untersucht werden, wo die Grenzen und Möglichkeiten eines solchen digitalen Analyseansatzes liegen, indem überprüft wird, ob sich bestehende qualitative Einteilungen von Dylans Werk in unterschiedliche Schaffensperioden auch anhand statistisch signifikanter Wörter (Rayson, Berridge & Francis 2004) und N-Gramme (Evert 2005) belegen lassen. 
                
         
         
            Stand der Forschung
            Bereits vor der Auszeichnung Dylans mit dem Nobelpreis in Literatur, waren seine Texte Gegenstand wissenschaftlicher Betrachtungen im Sinne des 
                    Close Reading (vgl. etwa Brown 2014). Brown unterteilt Dylans Werk in einzelne Phasen und verknüpft diese jeweils mit allgemeinen, zeitgeschichtlichen Ereignissen sowie biographischen Meilensteinen des Künstlers. Taylor & Israelson (2015) gehen einen ähnlichen Weg, versuchen jedoch Dylans Werk abseits verbreiteter politischer Einordnungen zu betrachten. Etwas anders ausgerichtet ist die Arbeit von Wissolik et al. (1994): Hierbei handelt es sich um eine Art Wörterbuch, in dem Namen und Gegenstände, die in Dylans Texten auftauchen, erläutert werden. 
                
            Eine umfassende Untersuchung Dylans Werks mithilfe computerbasierter Methoden fand sich bis zum Abfassungszeitpunkt des vorliegenden Texts nicht. Allerdings sind quantitative Verfahren zur stilistischen und inhaltlichen Analyse von Liedtexten in den Digital Humanities durchaus verbreitet. So beschreiben etwa, 
                    
                    wie mithilfe von N-Gramm-Modellen ein Liedtext-Korpus anhand der Merkmale Textlänge, Textstruktur, Wortschatz und Semantik analysiert werden kann, um eine automatisierte Genrezuordnung vornehmen zu können. Daneben existieren stilometrische Untersuchungen von Liedern oder Gedichten, die sich mit der Berechnung von autoren- und genrespezifischen Merkmalen befassen, wie z.B. Suzuki & Hosoya (2014), die japanische Pop-Songs analysieren.
                
         
         
            Forschungsmethodik
            
               Bezugsrahmen der Analyse: Schaffensphasen Bob Dylans
               Den analytischen Bezugsrahmen dieser Studie stellt die phasenweise Einteilung von Dylans Schaffen nach Brown (2014)
                         
                        dar. 
                         
                        Brown unterscheidet dabei neun unterschiedliche Phasen, die mit „Becoming Bob Dylan“ (1960-1964) beginnen und vorläufig mit „Bob Dylan Revisited“ (2000-2012) enden. Diese Stilphasen umfassen z.B. Dylans Hinwendung zum Christentum oder seine elektronische „Folk Rock“-Phase (vgl. Brown, 2014).
                    
            
            
               Korpus und Datenaufbereitung
               In dieser Arbeit wurde ein Korpus bestehend aus 452 Liedtexten mit einem Umfang von 133.045 Tokens untersucht, die Bob Dylan zwischen den Jahren 1962 und 2016 auf Studio-Alben veröffentlicht hat. Die Liedtexte und Metainformationen wie etwa Titel, Album und Jahr stammen von der Plattform 
                        LyricsWikia
                  . Da es sich bei 
                        LyricsWikia um ein community-gestütztes Projekt handelt, erfolgte vorab ein stichprobenartiger Abgleich einzelner Lieder mit den offiziellen Texten nach, 
                        
                        wobei keinerlei Abweichungen festgestellt werden konnten. 
                    
               Das Korpus wurde weiterhin mit Methoden der Computerlinguistik aufbereitet, insbesondere unter Verwendung des 
                        Python Natural Language Toolkits (NLTK). Die Verarbeitung des Korpus umfasst die grundlegende Lemmatisierung mit dem 
                        WordNet-Lemmatizer (Teil des NLTK) und eine Stoppwortbereinigung (NLTK-Stoppwortliste für Englisch mit eigener Erweiterung) sowie die Wortartenannotation mithilfe des 
                        Stanford Log-linear Part-of-Speech-Taggers.
                        
                        Da Dylan in seinen Texten häufig umgangssprachliche Formulierungen wie etwa verkürzte Gerundformen (bspw. „savin“, „swimmin“) verwendet, wurde für den POS-Tagger ein Modell verwendet, welches auf der Grundlage von Twitter-Texten trainiert wurde und gute Ergebnisse für Texte mit nicht-standardisiertem Vokabular und Slang liefert. 
                        
                    
            
            
               Korpusvergleich – Assoziationsmaße und Referenzkorpus
               
                  Assoziationsmaße
                  Ein etabliertes Verfahren, um aus einem Korpus spezifische Wörter zu extrahieren, ist ein direkter Korpusvergleich mit dem 
                            Log-Likelihood-Test, 
                            
                            der sich zum Vergleich von Korpora unterschiedlicher Größe besonders eignet (Rayson, Berridge, & Francis, 2004). Damit können Wörter, die im untersuchten Korpus mit einem signifikanten Frequenzunterschied zum Referenzkorpus auftreten, als Schlagworte betrachtet werden. Dies kann besonders aussagekräftige Ergebnisse liefern, wenn zusätzlich eine POS-Filterung erfolgt, womit sich beispielsweise signifikante Nomen oder Verben eines Korpus berechnen lassen. Darüber hinaus wurde eine Berechnung von N-Grammen in Form von Bi- und Trigrammen umgesetzt. Die berechneten N-Gramme lassen sich in der Web-App unter der Wahl eines Assoziationsmaßes, wie dem 
                            Chi Quadrat-Test, dem Jaccard-Test, dem Poisson-Stirling-Test, dem Likelihood Ratio-Test sowie dem Pointwise Mutual Information-Test anzeigen. Dabei liefert jedes Verfahren zur N-Gramm-Berechnung eigene spezifische Ergebnisse. Dieser Freiraum wird ganz bewusst erhalten, um die verschiedenen Facetten eines Texts, die ein Assoziationsmaße jeweils anzeigt, für die spätere Datenanalyse nutzen zu können.
                        
               
               
                  Referenzkorpus 
                  Als Referenzkorpus dient das mündliche Subkorpus des 
                            Open American National Corpus (OANC; American National Corpus Project),
                            
                            welches insgesamt 3.862.172 Tokens umfasst. Das Korpus enthält viele Belege aus der mündlichen Kommunikation 
                             
                            und eignet sich dadurch in besonderer Weise als Vergleichskorpus für Dylans Texte, die wie bereits beschrieben einen hohen Anteil umgangssprachlicher Formulierungen und Slang-Ausdrücke enthalten.
                        
                  Beim Korpusvergleich kann entweder das gesamte Dylan-Korpus mit dem Referenzkorpus verglichen werden, oder mit den jeweiligen Dylan-Subkorpora, also bspw. all seinen Texten aus den 1970er-Jahren oder aus der ersten Schaffensperiode „Becoming Bob Dylan“ (1960-1964). Ein Vergleich der einzelnen Dylan-Subkorpora zum Gesamtwerk ist ebenso möglich. Letztere Option wird z.B. genutzt, um anhand jeweils signifikanter Wörter die einzelnen Schaffensperioden nach Brown (2014) zu überprüfen und damit die grundsätzliche Eignung solch quantitativer Verfahren zur Identifikation thematischer Verschiebungen zu untersuchen. Die Ergebnisse dieses Korpusvergleichs sind, zusammen mit allen anderen Ergebnissen der angewandten Analyseverfahren, in einer interaktiven Webanwendung über unterschiedliche Visualisierungen (Balkendiagramm, 
                            treemap, wordcloud, Tabelle) für weitere Interpretationen zugänglich. Wie schon bei den Assoziationsmaßen, so gilt auch hier, dass jede Visualisierungsform eine bestimmte Perspektive auf die Berechnungsergebnisse eröffnet.
                        
               
            
         
         
            Ergebnisse
            Im direkten Vergleich des gesamten Dylan-Korpus (1962-2016) mit dem OANC-Referenzkorpus treten einige interessante, signifikant-häufige Wörter im Werk Dylans hervor. Die von Bob Dylan verwendeten Adjektive erzeugen in der Gesamtschau tendenziell eher eine bedrückende Stimmung (blind, weary, lonely, drunken, scared, restless, ragged). Bei den Substantiven mischen sich unter viele Personen- und Ortsnamen auch religiöse Begriffe (soul, heaven, devil, eden, prayer, paradise). Viele der übrigen Begriffe sind erwartungsgemäß typisch für Folk-Musik (levee, rooster, train), was sich wiederum durch die Wahl des Referenzkorpus, das verschiedenartige mündliche Textquellen enthält, erklären lässt (Rayson, Berridge, & Francis, 2004: 8).
                
            Die Analyse signifikant-häufiger Wörter für die einzelnen Schaffensphasen Dylans liefert Ergebnisse mit hoher Aussagekraft. So fällt etwa für die Phase „The Changing of the Guard" (1978-1981), in der sich Dylan dem Christentum hinwendet, auf, dass das Vokabular tatsächlich viele christliche Motive aufweist (lord, Jesus, devil, altar, faith, confession, grace, power, serve). Insgesamt nimmt der Anteil an „düsterem“ Vokabular in dieser Phase ab, verschwindet jedoch nicht komplett (bspw. 
                    shot, destruction). Der Anteil an hoffnungsvollen Wörtern nimmt hingegen zu (bspw. 
                    beginning, ready, arise, wake, thank). Bei den übrigen Schaffensphasen fallen die Ergebnisse jedoch mitunter wesentlich weniger deutlich aus.
                
            Ein differenziertes Bild ergibt sich für die N-Gramm-Analyse, was einerseits der Vielfalt an verfügbaren Methoden zur Berechnung 
                     
                    und andererseits den unterschiedlichen N-Gramm-Längen geschuldet ist. Die Ergebnisse für Bigramme mit Hilfe des 
                    Pointwise-Mutual-Information-Tests (PMI) erschienen dabei am geeignetsten, um die thematischen Schwerpunkte von Dylans Schaffensphasen nach 
                     
                    nachzuvollziehen. So findet das PMI-Verfahren im Subkorpus der Phase „The Changing of the Guard“ (1978-1981) Bigramme wie 
                    close prayer, name lucifer, jesus good, jesus bone oder arise upon, die eindeutig religiöse Bezüge in Dylans Texten dieser Phase veranschaulichen. Generell fällt jedoch die Dominanz von Refrain-Versen in den Liedern bedeutend ins Gewicht (z.B. 
                    knock heaven door), was die Qualität der Ergebnisse insbesondere bei den Trigrammen beeinflusst. 
                
         
         
            Diskussion
            Im Sinne einer Kritik der Digitalen Vernunft bleibt demnach festzuhalten, dass sich Methoden der computergestützten Textanalyse und des statistischen Korpusvergleichs grundsätzlich dafür eignen, einen inhaltlichen Gesamtüberblick zu einem Liedtext-Korpus zu erhalten. Es können damit diachrone Entwicklungen des Wortschatzes und Verlagerungen thematischer Schwerpunkte als grobe Tendenzen aufgezeigt werden, um das Bild des Gesamtwerks zu ergänzen. Ein solcher Ansatz eignet sich demnach gut für die initiale Thesengenerierung und kann in gewisser Weise die Funktion eines Empfehlungs- bzw. Hinweissystems für erklärungsbedürftige Stellen
                     in den Geisteswissenschaften übernehmen.
                
            Die Identifikation konkreter Schaffensperioden, ausschließlich auf Basis signifikant häufiger Wörter ist aber – zumindest für das Werk Dylans – nicht ohne Weiteres erfassbar. Bei den N-Grammen zeigt sich, dass im Falle von Dylans Texten methodenübergreifend und mit zunehmender N-Gramm-Länge meist keine brauchbaren Ergebnisse erzielt werden konnten. Dies ist ein Hinweis darauf, dass die hier präsentierten Analysemethoden, die für andere Textsorten wie bspw. Parlamentsprotokolle bereits erfolgreich eingesetzt werden konnten (vgl. Sippl et al. 2016), auf Liedtexte nur eingeschränkt anwendbar sind. Ein möglicher Kritikpunkt am hier beschriebenen Vorgehen mag zudem das verwendete OANC-Referenzkorpus sein, welches trotz hoher Anteile mündlicher Kommunikation doch nur beschränkt vergleichbar mit der Textsorte „Liedtext“ ist. Für künftige Vergleichsstudien böte sich ggf. ein Vergleich mehrerer unterschiedlicher Künstler und deren Liedtexte an, also bspw. Bob Dylan vs. Johnny Cash.
         
      
      
         
             http://lyrics.wikia.com, alle Hyperlinks dieses Dokuments wurden zuletzt abgerufen am 10.01.2018
             Verfügbar unter http://www.nltk.org/
             Filterung von Stoppwörtern, wie „hey“, „ah“, „yeah“, und Verkürzungen, wie „‘ve“, „‘s“ etc.
             https://www.colin-sippl.de/dylan (Klick auf den Analyse-Button rechts oben)
             Diesen Gedanken äußerte Hubertus Kohle auf der #DigiCampus-Tagung im Juni 2017 in München, vgl. https://twitter.com/8urghardt/status/876725916487036928.
         
         
            
               Bibliographie
               
                  American National Corpus Project (2015a): 
                        American National Corpus. Frequency Data. http://www.anc.org/data/anc-second-release/frequency-data/ [Letzter Zugriff 10. März 2017].
                    
               
                  American National Corpus Project (2015b): 
                        The Open American National Corpus (OANC). http://www.anc.org/ [Letzter Zugriff 10. März 2017].
                    
               
                  Brown, Donald (2014): 
                        Bob Dylan: American troubadour. Lanham, Md. [u.a.]: Rowman & Littlefield.
                    
               
                  Cott, Jonathan (2006): 
                        Bob Dylan, the essential interviews. New York: Wenner Books.
                    
               
                  Derczynski, Leon et al. (2013): "Twitter part-of-speech tagging for all: Overcoming sparse and noisy data", in: 
                        Proceedings of the Recent Advances in Natural Language Processing September, 198–206. http://www.derczynski.com/sheffield/papers/twitter_pos.pdf [Letzter Zugriff 9. März 2017].
                    
               
                  Dylan, Bob (2016): 
                        The lyrics: 1961-2012. New York: Simon & Schuster.
                    
               
                  Evert, Stefan (2005): "The Statistics of Word Cooccurrences, Word Pairs and Collocations", in: 
                        Unpublished doctoral dissertation Institut fur maschinelle Sprachverarbeitung Universitat Stuttgart 98: August 2004, 353. http://en.scientificcommons.org/19948039 [Letzter Zugriff 3. März 2017].
                    
               
                  Fell, Michael / Sporleder, Caroline (2014): "Lyrics-based Analysis and Classification of Music", in: 
                        International Conference on Computational Linguistics 25: 23–29, 620–631.
                    
               
                  Geisel, Sieglinde (2016): 
                        Bob Dylan - Literaturnobelpreisträger wider Willen. Deutschlandradio Kultur. http://www.deutschlandradiokultur.de/bob-dylan-literaturnobelpreistraeger-wider-willen.1005.de.html?dram:article_id=373494 [Letzter Zugriff 7. März 2017].
                    
               
                  Rayson, Paul / Garside, Roger (2000): "Comparing corpora using frequency profiling", in: 
                        Proceedings of the workshop on Comparing Corpora 1–6.
                    
               
                  Rayson, Paul / Berridge, Damon / Francis, Brian (2004): "Extending the Cochran rule for the comparison of word frequencies between corpora", in: 
                        JADT 2004: 7es Journées internationales d’Analyse statistique des Données Textuelles: 1–12.
                    
               
                  Schmidt, Mathias R. (1983): 
                        Bob Dylan und die sechziger Jahre: Aufbruch und Abkehr. Frankfurt am Main: Fischer Taschenbuch Verlag.
                    
               
                  Sippl, Colin / Burghardt, Manuel / Wolff, Christian / Mielke, Bettina (2016): Korpusbasierte Analyse österreichischer Parlamentsreden. In: 
                        Netzwerke: Tagungsband des 19. Int. Rechtsinformatik Symposions IRIS 2016: 25.- 7. Feb. 2016, Univ. Salzburg, S. 139-148.
               
               
                  Suzuki, Takafumi / Hosoya, Mai (2014): "Computational Stylistic Analysis of Popular Songs of Japanese Female Singer-songwriters", in: 
                        Digital Humanities Quarterly 8: 1, .
                    
               
                  Svenska Akademien (2016): 
                        Der Nobelpreis in Literatur des Jahres 2016.
                    
               
                  Taylor, Jeff / Israelson, Chad (2015): 
                        The Political World of Bob Dylan: Freedom and Justice, Power and Sin. New York: Palgrave Macmillan.
                    
               
                  Toutanova, Kristina / Klein, Dan / Manning, Christopher D (2003): "Feature-rich part-of-speech tagging with a cyclic dependency network", in: 
                        Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL ’03), 252–259. http://nlp.stanford.edu/~manning/papers/tagging.pdf [Letzter Zugriff 3. März 2017].
                    
               
                  Wissolik, Richard David / McGrath, Scott. / Colaianne, A. J. (1994): 
                        Bob Dylan’s words: a critical dictionary and commentary. Greensburg, PA: Eadmer Press.
                    
            
         
      
   



      
         
            Digital Medievalist is an international web community for medievalists working with digital media. Established in 2003 by a group of volunteers
                 and before the arrival of Facebook and Twitter
                , the goal of 
                Digital Medievalist would also become that of the popular social networks: to connect people around the world providing them with an exchange platform. But where Facebook and Twitter were driven by relationships, 
                Digital Medievalist was driven by interest. 
            
         The very first Digital Humanities disciplinary-focus community of practice, 
                Digital Medievalist sought to meet the increasingly sophisticated demands faced by creators of digital projects working with medieval content. Among its initial community-building activities, 
                Digital Medievalist started an homonymous open access scholarly journal, which is still active today, and commissioned the publication of short tutorials on its website to guide interested scholars through the basics of text encoding, web development and manuscript digitisation, to mention but a few. The benefits brought by 
                Digital Medievalist became so evident that other, similar web communities began to emerge, such as the 
                Digital Classicist
             (Mahony, 2017) and 
                Digital Victorianist.
            
         Over time, and as new technologies rapidly developed, 
                Digital Medievalist’s didactic component was superseded by free online courses, moodles and web tutorials, shifting its focus toward the dissemination of scholarly research to the widest possible audience. The 
                Digital Medievalist community has continued to gather importance since its founding and today serves a number of disciplinary fields, including Digital Humanities, Medieval Studies and Auxiliary Sciences, Cultural Heritage, Archaeology, Literary Studies, History, Linguistics, and Museum and Archival Science.
            
         Membership to 
                Digital Medievalist is open to anyone with an interest in its subject matter, regardless of skill or previous experience in Digital Humanities or medieval studies. Participants range from novices contemplating their first project to many of the pioneers in the field. The entire 
                Digital Medievalist community counts over 1,500 members worldwide. 
            
         The current activities and assets of 
                Digital Medievalist include: 
            
         
            The Digital Medievalist mailing list: 1,272 (as of Sept. 14th 2017) list members use this platform to ask for advice, discuss problems, and share any kind of information related to the field of medieval studies. The list’s collegial atmosphere encourages a variety of conversations.
                
            The Digital Medievalist journal: The community’s online, open access, refereed journal publishes original research and scholarship, notes on technological topics (standards, tools, software, etc.), commentary pieces discussing developments in the field, bibliographic and review articles, and project reports. The journal is funded in part through grants provided by the University of Lethbridge School of Graduate Studies and recently joined the Open Library of Humanities (OLH), a non-profit organisation dedicated to publishing open access scholarship with no author-facing article processing charges. Funded by an international consortium of libraries OLH has built a sustainable business model in order to make scholarly publishing fairer, more accessible, and rigorously preserved for the digital future.
                
            The Digital Medievalist website: The community’s online presence provides comprehensive information about the organisation including membership, structure and bylaws. It also provides announcements and an up-to-date list of recent and upcoming conferences, colloquia, workshops and training events relevant to (digital) medieval studies. The website also invites members to write blog-posts for several thematic series.
                
            The Digital Medievalist Facebook group with over 1,600 members and a Twitter presence to widen the scope and impact of scholarly communication, and to disseminate best practice, data and knowledge pertaining to digital medieval studies (Ross, 2012; Terras, 2012).
                
         
         In 2017, Digital Medievalist joined the European Alliance for Social Sciences and Humanities (EASSH) as a learned society in order to increase the visibility of the 
                Digital Medievalist community. 
            
         DHd 2018 provides the ideal venue to expose 
                Digital Medievalist to a large German-speaking community of scholars. The 
                Digital Medievalist website averages 2,760 views per day from Germany alone; the 
                Digital Medievalist conference representatives are eager to speak to practitioners in Germany to better understand how 
                Digital Medievalist is meeting their needs and how it can improve. Additionally, we think that 
                Digital Medievalist can still serve as an example for community building. Its history and current state demonstrates how the interest in digital methods intersects with the use of digital communication tools, and is thus maybe an archetypical example of Digital Humanities.
            
         The poster will outline the aforementioned activities and will serve as a conversation starter to establish connections with relevant initiatives, start reflection on the role of this and similar activities, collect feedback and continue fostering as wide a geographical coverage as possible. 
      
      
         
             Daniel Paul O'Donnell, Peter Baker, James Cummings, Martin Foys, Murray McGillivray, Dot Porter, Roberto Rosselli Del Turco, and Elizabeth Solopova. 
                        Digital Medievalist was founded with direct financial support from the Faculty of Arts and Science at the University of Lethbridge, the Curriculum Redevelopment Centre (now the Teaching Centre) at the University of Lethbridge, the
                        Image, Text, Sound, and Technology (ITST) programme of the Social Sciences and Humanities Research Council of Canada (SSHRC).
                    
             Facebook was founded in 2004 and Twitter in 2006.
             For a discussion on the relationship between 
                        Digital Medievalist and 
                        Digital Classicist, see Bodard and O’Donnell (2008).
                    
         
         
            
               Bibliography
               
                  Bodard, G., O’Donnell, D. (2008) ‘We are all together: On publishing a Digital Classicist issue of the Digital Medievalist journal’, 
                        Digital Medievalist, 4. DOI: http://doi.org/10.16995/dm.18
                    
               
                  Digital Medievalist website: 
                        https://digitalmedievalist.wordpress.com/
               
                  Digital Medievalist journal: 
                        https://journal.digitalmedievalist.org/
               
                  Digital Medievalist mailing list: 
                        https://digitalmedievalist.wordpress.com/mailing-list/
               
                  Digital Medievalist on Facebook: 
                        https://www.facebook.com/groups/49320313760/
               
                  Digital Medievalist on Twitter: 
                        https://twitter.com/digitalmedieval
               
                  European Alliance for Social Sciences and Humanities: http://www.eassh.eu/
               
                  Mahony, S. (2017) ‘The Digital Classicist: Building a Digital Humanities Community’, 
                        Digital Humanities Quarterly, 11(3). At: http://www.digitalhumanities.org/dhq/vol/11/3/000335/000335.html
                    
               
                  Open Library of Humanities: https://www.openlibhums.org/
               
                  Open Scholarly Communities on the Web, ISCH COST Action A32. At: http://www.cost.eu/COST_Actions/isch/A32 
               
                  Ross, C. (2012) ‘Social media for digital humanities and community engagement’, In C. Warwick, M. Terras and J. Nyhan (eds.) 
                        Digital Humanities in Practice. Facet Publishing, pp. 23-46.
                    
               
                  Terras, M. (2012) ‘The Impact of Social Media on the Dissemination of Research: Results of an Experiment’, 
                        Journal of Digital Humanities, 1(3). At: http://journalofdigitalhumanities.org/1-3/the-impact-of-social-media-on-the-dissemination-of-research-by-melissa-terras/
                    
            
         
      
   



      
         
            Einleitung
            Twitter ist ein soziales Netzwerk bzw. ein Mikroblogging-Dienst, welches das Senden von Textnachrichten („Tweets“) ermöglicht. Es ist das derzeit am schnellsten wachsende soziale Netzwerk, worin die Twitter-Benutzer untereinander stark vernetzt sind und ihre eigenen Meinungen sowie Gefühle zu aktuellen Themen ausdrücken. Die textuellen und sprachlichen Inhalte der einzelnen Tweets sind nicht normativ, da die enthaltenen Informationen von umgangssprachlichen Ausdrücken, Abkürzungen, Emoticons und von Grammatikfehlern durchsetzt sind, sodass kein standardisiertes / automatisiertes Auswertungsverfahren zur Sentiment Analyse angewendet werden kann. Den Tweets können zudem Anhänge wie Bilder, Videos oder Hyperlinks beigefügt werden. Memes sind beliebte Vertreter solcher Bildanhänge in Tweets: 
                    „I define an Internet meme as: (a) a group of digital items sharing common characteristics of content, form, and/or stance, which (b) were created with awareness of each other, and (c) were circulated, imitated, and/or transformed via the Internet by many users.” (Shifman L., 2013, S. 41) Auf Basis dieser Definition von L. Shifman zählen somit auch Videos zur Gattung der Memes. Memes haben die Eigenschaft Bild und Text zu kombinieren, dadurch eine polysemische Nachricht zu generieren, welche die Rezipienten anspricht und Emotionen auslösen. 
                    „Ein Internet-Meme ist die humoristische/sarkastische Reaktion der Internetgemeinde auf ein (mediales) Ereignis.“ (Marx und Weidacher, 2014, S. 143) Zudem sind Memes in politischem Kontext aktuell wenig empirisch erforscht 
                    (Shifman L., 2013, S. 119). Gerade aus diesen Gründen wurden Memes, speziell Memes in politischem Kontext, als Untersuchungsgegenstand für diese Sentiment Analyse gewählt.
                
            Die vorliegende Analyse basiert auf zwei Memes: Das erste Meme steht im Kontext zur Wahl des US-Präsidenten inkl. des Vizepräsidenten vom 8. November 2016, das Zweite im Zusammenhang der Ersten 100 Amtstage von Präsident Donald John Trump. Parallel zur Präsidentschaftswahl verbreitete sich bereits zwei Tage nach dem Wahlstichtag ein statisches Bild der Simpsons, worin schon im Jahr 2000 der Wahlausgang mit der geografischen Karte der Wahlergebnisse prophezeit wurde 
                    (siehe Abbildung 1). Als Reaktion auf die Ersten 100 Tage von D. Trump im Amt wurde am 26. April 2017 ein Video von den Simpsons 
                    (siehe Abbildung 2) auf YouTube veröffentlicht, welches genau diesen Zeitraum seiner Amtshandlungen parodiert. Bereits einen Tag danach setzte die Diskussion zum Video auf Twitter ein. Die vorliegende Arbeit analysiert die aus diesen beiden Memes entstandenen emotionalen Diskussionen und Reaktionen auf die beiden politischen Ereignisse via Twitter.
                
            
               
               Abbildung 1: Simpson Prediction
               
            
            
               
               Abbildung 2: Donald Trump's First 100 Days In Office | Season 28 | THE SIMPSONS
               
            
         
         
            Forschungsstand
            Dass die Meinungen anderer Mitmenschen unsere eigenen Entscheidungen beeinflussen, ist schon lange aus der Psychologie bekannt 
                    (Friedkin, 1990). Auch die Sammlung und Auswertung von Meinungen wird schon lange betrieben. Mit dem Aufkommen des Web 2.0 ergeben sich viel bessere Möglichkeiten, große Datenmengen gezielt auf Meinungen hin zu analysieren. Die Sentiment Analyse zielt auf die Ergründung der Haltung, Stimmung, Meinung und die generelle Einstellung von Personen in Bezug auf ein speziell ausgewähltes Produkt, andere Personen, Dienstleistungen oder aktuellen Themen ab. Dieses Forschungsgebiet fällt in den Bereich der 
                    Computerlinguistik bzw. der 
                    linguistischen Datenverarbeitung (Natural Language Processing), welche Untergebiete des 
                    Text Minings sind. Die Sentiment Analyse ist als Klassifikationsproblem von Texten und dessen Polaritätserkennung zu verstehen. Für die Polaritätserkennung müssen Indikatoren im Text identifiziert werden, welche Rückschlüsse auf das sogenannte Sentiment zulassen. Dabei handelt es sich um sprachspezifische Ausdrücke, die aufgrund ihrer Wortbedeutung bereits positiv, neutral oder negativ vorbelegt sind. Diese Stimmungsinformation lässt sich aus sogenannten Sentimentlexika (bzw. Sentiment-Wörterbüchern) der jeweiligen Sprachen entnehmen. Hier werden stimmungstragende Ausdrücke - häufig Adjektive - als solche gekennzeichnet. Meist wird von deren kontextunabhängigen Polaritätsausprägung ausgegangen, die binär (positiv/negativ bzw. +/-) oder verhältnisskaliert (wie beispielsweise beim Wörterbuch 
                    „SentiWordNet“) kodiert wird. Die Vorgehensweise bei einer Sentiment Analyse kann entweder 
                    Lexikon basiert, anhand eines Wörterbuchs erfolgen oder 
                    lernbasiert, wo man sich auf die Algorithmen aus dem Fachgebiet des maschinellen Lernens stützt. Bei der Durchführung der Sentiment Analyse untergliedert man die drei Ebenen (
                    Level): 
                    Dokumenten-Ebene, 
                    Satz-Ebene und die 
                    Aspekt-Ebene. Bei der 
                    Dokumenten-Ebene wird der gesamte Inhalt eines Dokuments in die Analyse mit einbezogen um eine generelle Stimmung zu deuten. Genauere Ergebnisse liefert die Betrachtung der 
                    Satz-Ebene, da hier die ausgedrückte Meinung für jeden einzelnen Satz berechnet wird. Die genausten Ergebnisse erhält man bei der Analyse der 
                    Aspekt-Ebene, da hier zu jedem Aspekt einer bestimmten Entität, die Stimmung des Meinungsvertreters zu einem bestimmten Zeitpunkt betrachtet wird.
                
            Nach der Definition von 
                    Liu Bing besteht eine Meinung aus einem 5-Tupel: 
                
            
               opinion = (e,a,s,h,t)
            
            Wobei 
                    e eine Entität (Objekt), 
                    a einen Aspekt (Feature) der Entität, 
                    s die subjektiv positive, negative oder neutrale Stimmung (Sentiment) des Aspekts der Entität, 
                    h den Meinungsvertreter (Opinion holder) und 
                    t den Zeitpunkt der Meinungsäußerung darstellt.
                
         
         
            Forschungsdesign und Methode 
            Aufbauend auf dem Forschungsstand untersucht diese Arbeit die inhaltlichen Diskussionen zu beiden eingangs erwähnten Memes auf Twitter hinsichtlich deren Emotionen und stellt diese in einem direkten Vergleich gegenüber. 
                    „Communication messages such as tweets, emails, and digital images are by definition memes, because they are replicable transmitters of cultural meanings.” (Spitzberg B. H., 2014, S. 312) In dieser Arbeit findet ein 
                    Lexikon basierter Ansatz für die Sentiment Analyse der Tweets Anwendung, da im Gegensatz zu den 
                    lernbasierten Methoden des maschinellen Lernens, die Lexikon basierten Verfahren für Bereiche eingesetzt werden können, für die keine Trainingsdaten existieren (Kennedy und Inkpen, 2006). Da für die vorliegende Sentiment Analyse lediglich ein relativ kleiner Untersuchungskorpus von 167 Tweets zur Verfügung steht, kommt die Anwendung der lernbasierten Methoden nicht in Frage. Des Weiteren können bei den Lexikon basierten Methoden kontextbedingte Ambivalenzen und andere sprachliche Konstrukte leichter berücksichtigt werden, da linguistische Aspekte eines Textes in Betracht gezogen werden können (Brooke et al., 2009). Dies ist vor allem zur Analyse von Mikroblogging-Einträgen geeignet, weil die Texte sehr kurz gehalten sind (Twitter erlaubt maximal 140 Zeichen pro Tweet). Grundsätzlich sind allerdings die Methoden des maschinellen Lernens bei Sentiment Analysen im Hinblick auf die Genauigkeit und Präzision der Klassifizierung meist effektiver als die Lexikon basierten Ansätze (Kennedy und Inkpen, 2006).
                
            Es existieren zahlreiche Wörterbücher, wie beispielsweise 
                    „MPQA Subjectivity Lexicon“, „Bing Liu and Minqing Hu Sentiment Lexicon“, „SentiWordNet“, „VADER Sentiment Lexicon“, „SenticNet“, „LIWC“, „Harvard General Inquirer“, „ANEW“ usw. um nur einige Beispiele zu nennen. Die meisten dieser beispielhaft genannten Wörterbücher liefern allerdings nur die tendenziellen Stimmungen 
                    positiv, 
                    negativ oder 
                    neutral. Das hier verwendete Wörterbuch 
                    „SentiWordNet 3.0“ liefert vertiefend die einzelnen Gewichte der Stimmungen („Score“) zu einzelnen Wörtern.
                
            Das Ziel dieser Sentiment Analyse ist die 
                    Bestimmung der emotionalen Färbungen der einzelnen Tweets und in weiterer Folge die 
                    Ermittlung der generellen Stimmungshaltung der Diskussionen bezüglich des Statischen im Vergleich zum bewegten Bild. Die Datenerhebung der einzelnen Tweet-Ströme (Tweets + Retweets) erfolgt über die Twitter API mit Hilfe des Web-Tools 
                    „FollowTheHashtag“
               . Die Datensätze werden direkt in Microsoft Excel exportiert und dort weiterverarbeitet. Zur Grundgesamtheit gehören alle Tweets (exkl. Retweets) die im Zuge der US-Präsidentschaftswahl oder im Zuge der Ersten 100 Tage nach Amtsantritt von D. Trump via Twitter weltweit abgesetzt wurden und beide Hashtags 
                    „#thesimpsons“ und 
                    „#trump“ beinhalten. Retweets werden bei der vorliegenden Sentiment Analyse nicht berücksichtigt, da diese keine neuen Aussagen, Meinungen oder Emotionen enthalten, sondern lediglich eine intendierte Wiederholung eines vorangegangenen Tweets darstellen.
                
            Die Analyseeinheit ist der jeweils im Tweet enthaltene Text 
                    („Tweet Content“). Der textuelle Inhalt eines Tweets kann Hashtags (#), Taggings (@) oder (Medien-) Links enthalten. Alle Texte, deren Aussagen nicht in Relation mit den beiden Memes stehen, werden als Spam klassifiziert und aus dem Datensatz bereinigt. Die beiden Memes an sich werden nicht untersucht, sondern stellen nur den Auslöser der Diskussion dar.
                
            Die Untersuchungszeiträume betragen jeweils sieben Tage ab dem Stichtag des Absetzens des ersten Tweets zu einem der beiden definierten Memes. Der Datensatz des statischen Bildes beläuft sich somit auf den Untersuchungszeitraum vom 
                    10.11.2016 bis zum 
                    16.11.2016 und beinhaltet 
                    N = 94 Tweets (exkl. Retweets). Der Datensatz des Videos beläuft sich auf den Untersuchungszeitraum vom 
                    27.04.2017 bis zum 
                    04.05.2017 und beinhaltet 
                    N = 73 Tweets (exkl. Retweets). Von der Kombination des Lexikon basierten Ansatzes mit lernbasierten Methoden wird aufgrund der geringen Datenmenge 
                    (N = 167 relevante Tweets) abgesehen.
                
            Nach der Datenerhebung und –bereinigung folgt manuell der Prozess der 
            Textnormalisierung nach dem Konzept von 
                    Tajinder Singh and Madhu Kumari, gefolgt von der manuellen Vorverarbeitung inkl. Satztypen Erkennung der textuellen Einheiten nach dem Konzept von 
                    Lei Zhang et al. Die einzelnen Tweet Contents werden hinsichtlich ihrer Satz- und Wortebene unterteilt, wobei für jede textuelle Einheit die 
                    positive, 
                    negative oder 
                    neutrale Stimmung aus einem Wörterbuch entnommen wird. Die einzelnen Tweets werden nach den Satztypen 
                    deklarativ, 
                    imperativ und 
                    interrogativ kategorisiert. Interrogativsätze fließen nicht in die Auswertung ein, da dieser Satztyp keine informativen Meinungen, sondern lediglich Fragestellungen zum Thema oder zu vorausgehenden Tweets ausdrückt.
                
            Danach folgt die Betrachtung der Wortebene, wobei nun alle Wörter mit emotionaler Stimmung aus den textuellen Einheiten extrahiert werden. Die Gewichtung der Stimmung jedes dieser Wörter wird mit Hilfe des Wörterbuchs 
                    „SentiWordNet 3.0“ bestimmt. Für die einzelnen Abfragen aus dem Wörterbuch wird der freie Programmcode von 
                    Petter Törnberg adaptiert. Nach der Abfrage aller Gewichte erfolgt die Auswertung der Daten. Hierzu wird der 
                    „Score“ je Tweet (bestehend aus einzelnen bzw. mehreren Sätzen) durch die Summe der einzelnen Gewichte der Stimmungen der Wörter der textuellen Einheiten eines Tweets ermittelt. 
                
            
               score = score(pos(Wort)) + score(neg(Wort))
            
            Wobei 
                    score(pos(Wort)) die Summe der Scores aller positiven Gewichte der relevanten Wörter eines Tweets enthält. 
                
            
               Score(neg(Wort)) stellt analog die Summe aller negativen Gewichte dar. Durch obige Formel wird der Score des Tweets berechnet. 
                
            Punktationen, wie beispielsweise ;-), Smileys, Emoticons oder ähnliche nicht textuelle Ausdrucksformen von Stimmungen werden in der Sentiment Analyse nicht berücksichtigt.
         
         
            Ergebnisse 
            Die Ähnlichkeiten und Unterschiede der emotionalen Diskussionen beider Memes konnten ermittelt und beschrieben werden. Dabei stellte sich insbesondere heraus, dass sich die emotionalen Richtungen der Diskussionen bezüglich des statischen Bilds im Vergleich zum Video erheblich voneinander unterscheiden. Die Auswertung der Häufigkeiten der emotionalen Färbungen der Tweets zum statischen Bild 
                    (N = 86) unterteilt sich in 
                    19 positive, 
                    51 neutrale und 
                    16 negative Äußerungen. Die Ergebnisse für das Video 
                    (N = 70) beinhalten 
                    18 positiv, 
                    36 neutral und 
                    16 negativ gestimmte Tweets.
                
            Vergleicht man rein die Häufigkeiten des Auftretens der einzelnen Stimmungen, sieht man, dass die Verteilung fast ähnlich ist. Betrachtet man die Scores für das statische Bild 
                    (score = 1,339) bzw. das Video 
                    (score = -.153), sprich die Summe aller positiven und negativen Gewichte der Wörter aller Tweets je Datensatz, so zeigt sich, dass trotz ähnlicher Häufigkeitsverteilungen die finale Stimmung für das statische 
                    Bild in Summe positiv gehalten ist. Beim 
                    Video fällt die Stimmung negativ aus. Ob bzw. in wie weit beispielsweise das Unterhaltungserlebnis oder die (ironischen) Inhalte des Videos im Gegensatz zum statischen Bild einen Einfluss auf die Emotionalität der Twitter-Nutzer beim Verfassen der einzelnen Texte der Tweets hat, wird im Rahmen dieser Forschungsarbeit nicht betrachtet.
                
            Die Forschungsarbeit dient als Anwendungsbeispiel für eine Social Media Sentiment Analyse auf Twitter Daten und bildet einen thematisch übergreifenden Forschungsansatz für die Disziplinen der Informatik, der traditionellen Geisteswissenschaften und der Digital Humanities aufgrund der Kombination von 
                    „User-Generated-Content“ in sozialen Netzwerken, über eine Programmierung bis hin zum Untersuchungsgegenstand der Memes, welcher wiederum typisch für die Geistes- und Sprachwissenschaften ist.
                
         
         
            Schlussfolgerungen
            
               Die Plattform Twitter ist ein stark genutztes soziales Netzwerk bzw. ein Mikroblogging-Dienst, wodurch dessen Nutzer ihre eigenen Meinungen sowie Gefühle zu Themen aller Art ausdrücken. Die Twitter-Benutzer kommentierten das Thema der US-Wahl und die ersten 100 Amtstage von D. Trump. Die Auslöser dieser Diskussionen stellten die beiden eingangs gezeigten Memes dar inkl. ihrer transportierten Nachrichten, welche durch das Zusammenspiel von Text und Bild vermittelt werden. Memes können durchaus Emotionen in der Internetgemeinde erzeugen - unter anderem auch in politischen Kontexten – ansonsten hätten sich diese beiden Memes nicht binnen kürzester Zeit via Twitter verbreiten können.
               Bezüglich der inhaltlichen Ausgestaltung der einzelnen Tweet Contents kann man sagen, dass nahezu alle Tweets Abkürzungen und/oder Emoticons beinhalteten. Die Polaritäten der Emoticons wurden allerdings nicht berücksichtigt, da hier auf keinen entsprechenden wissenschaftlichen Ansatz zurückgegriffen werden konnte.
               Für manuell durchgeführte Sentiment Analysen auf kleinem Untersuchungskorpus eignen sich Lexikon basierte Ansätze hervorragend. Ob eine Kombination mit Methoden des maschinellen Lernens herangezogen wird, hängt vom Untersuchungsgegenstand und von der Größe des Untersuchungskorpus ab.
               Grundsätzlich existieren zahlreiche Wörterbücher für Sentiment Analysen. Allerdings liefern die meisten Wörterbücher lediglich die tendenziellen Wortbedeutungen 
                        positiv, 
                        negativ oder 
                        neutral. Das Wörterbuch 
                        „SentiWordNet“ hingegen skaliert die stimmungstragenden Wörter auf dem Intervall [-1, 1] und verleiht den Wörtern spezifische Gewichte, wobei „-1“ negativ und „1“ positiv bedeuten. Die verhältnisskalierten Stimmungsinformationen zu den einzelnen Wörtern konnten größtenteils über dieses Wörterbuch bestimmt werden, allerdings gab es einige Wörter die selbst dieses Wörterbuch nicht beinhaltete. Das Wörterbuch „
                        SentiWordNet“ ist somit kein vollständiges Sentimentlexikon.
                    
               Bei der reinen Auswertung der Häufigkeiten der emotionalen Färbungen der Tweets zeigt sich, dass die Verteilung fast ident ist zwischen dem statischen Bild 
                        (N = 86, davon 
                        19 positiv, 
                        51 neutral und 
                        16 negativ) und dem Video 
                        (N = 70, davon 18 positiv, 
                        36 neutral und 
                        16 negativ). Deshalb wurden zusätzlich die verhältnisskalierten Stimmungsinformationen erhoben und einzelnen Gewichte betrachtet. Hierdurch lässt sich zeigen, dass die finale Stimmung für das statische 
                        Bild in Summe positiv (score = 1,339) gehalten ist. Beim 
                        Video fällt die Stimmung negativ (score = -.153) aus. Die Deutung/Interpretation der Ergebnisse einer Sentiment Analyse ist somit stark von der verwendeten Methode und dessen Ansatz abhängig.
                    
            
         
      
      
         
            Quelle Abbildung 1: 
                 https://img.buzzfeed.com/buzzfeed-static/static/201611/9/16/asset/buzzfeed-prod-fastlane02/sub-buzz18441-1478727536-5.png?downsize=715:*&outputformat=auto&output-quality=auto (zuletzt aufgerufen: 30.06.2017) 
             
            Quelle Abbildung 2: 
                 https://www.youtube.com/watch?v=Qo3fT0xPeHs (zuletzt aufgerufen: 30.06.2017)
             
            
               http://www.followthehashtag.com/ (zuletzt aufgerufen: 26.07.2017)
             
            Programmcode verfügbar unter GNU General Public License. Quelle: 
                 https://github.com/mserrate/twitter-streamingapp/blob/master/twitter-stormtopology/src/main/java/analysis/SentiWordNet.java (zuletzt aufgerufen: 30.06.2017) 
             
         
         
            
               Bibliographie
               
                  Baccianella, S., Esuli, A., & Sebastiani, F. (2010). 
                        SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. LREC, Vol. 10, S. 2200-2204. 
                    
               
                  Brooke, J.; Tofiloski, M.; Taboada, M.: Crosslinguistic sentiment analysis: From english to spanish. In: Proceedings of the 7th International Conference on Recent Advances in Natural Language Processing, Borovets, Bulgaria, 2009, S. 50–54.
               
                  Friedkin, N.E. & Johnsen, E.C. (1990) 
                        Social influence and opinions. J. Math. Soc. 15. pp. 193 – 206. 
                    
               
                  Kennedy, A.; Inkpen, D.: Sentiment Classification of Movie and Product Reviews Using Contextual Valence Shifters. In: Computational Intelligence 22 (2006), Nr. 2, S. 110–125.
               
                  Liu, B. (2010):
                        „Sentiment Analysis: A Multifaceted Problem“. IEEE Intelligent Systems, S. 76-80. 
                    
               
                  Marx, K. & Weidacher, G. (2014). 
                        Internetlinguistik. Ein Lehr- und Arbeitsbuch. Tübingen: Narr. 
                    
               
                  Shifman, L. (2013). 
                        Memes in Digital Culture. MA: MIT Press. 
                    
               
                  Singh, T. und Kumari, M. (2016). 
                        Role of Text Pre-processing in Twitter Sentiment Analysis. Procedia Computer Science, 89, 549-554. 
                    
               
                  Spitzberg, B. H. (2014). 
                        Toward A Model of Meme Diffusion (M3D). Communication Theory 24. S. 311–339. 
                    
               
                  Wilson, T., Wiebe, J., & Hoffmann, P. (2005). 
                        Recognizing contextual polarity in phrase-level sentiment analysis. In: Proceedings of the conference on human language technology and empirical methods in natural language processing. S. 347–354. 
                    
               
                  Zhang, L., et al. (2011). Combining Lexicon-based and Learning-based Methods for Twitter Sentiment Analysis. HP Laboratories, Technical Report HPL-2011-89. 
            
         
      
   



      
         
            Projektvorstellung
            Die Methoden der Digital Humanities sind ebenso zahlreich wie die Fachbereiche, in denen sie zur Anwendung kommen. Ob Archäologie oder Literaturwissenschaft, Soziologie oder Geschichte: Unser Podcast will über Theorien und praktische Anwendungsbereiche der Digital Humanities informieren und diskutieren; er will neugierig machen und die Angst vor der Anwendung neuer, noch ungewohnter Forschungsmethoden nehmen. 
            Die Rezeptionsform „Podcast“ soll Studierende auf eine noch weitgehend ungewohnte Weise ansprechen: Wir möchten sie in ihrem Alltag erreichen, sie zur Entdeckung neuer Forschungsmethoden anregen und gleichzeitig den kreativen Umgang mit ihrem je eignen Forschungsthema fördern.
            Umgekehrt soll die Aufmerksamkeit für bereits existierende DH-Projekte und -Studiengänge erhöht werden, um den Forschungsnachwuchs neugierig zu machen und DH-Vorhaben mehr Reichweite zu ermöglichen. Dabei soll der Eindruck, dass die DH in weiten Teilen eine algorithmen- und werkzeuggetriebene Wissenschaft sei, dem kritischen Anspruch der Geisteswissenschaften gegenübergestellt werden. Wir führen u. a. Interviews mit Wissenschaftler/-innen und Praktikern und bereiten Forschungsdiskurse auf.
         
         
            Themen
            Wir möchten einen umfassenden Einblick in die Methoden und Anwendungsbereiche der Digital Humanities geben und die Forschungswerkstätten beleuchten. In Form von Interviews werden in den ersten Folgen Mitarbeitende verschiedener Universitäten und Institute interviewt und zu ihren aktuellen Forschungsprojekten befragt. Weitere Kooperationen sind in Planung. Doch auch "Praktiker" wie Editionswissenschaftler, Bibliothekare, Data Scientists etc. sollen nach ihren berufspraktischen Erfahrungen gefragt werden. Wie können hier digitale Methoden über die universitäre Forschungspraxis hinaus zum Einsatz kommen? Ziel ist zudem, laut über mögliche Kritikpunkte nachzudenken: Welche Potenziale haben digitale Methoden, welche Defizite des Analogen können damit ausgeglichen werden? Welche Gefahren ergeben sich? Geht der Methode überhaupt eine Theorie voraus?
            Ergänzend zum Podcast bauen wir ein Glossar in Form des sogenannten "Wissensblogs" auf: Parallel zu den Audio-Beiträgen informieren wir hier ganz grundlegend über konkrete Werkzeuge und Methoden. Die hier gelieferten Informationen werden in den einzelnen thematisch passenden Folgen ggf. aufgegriffen und dem Publikum vorgestellt. Hier geben wir dem wissenschaftlichen Nachwuchs Tools an die Hand, die fachübergreifend nützlich sein können. So beleuchten wir zum Beispiel Fragen der Lizenzierung von Online-Veröffentlichungen, digitale Präsentationsmöglichkeiten, einheitliche Daten-Referenzsysteme etc.
            Eine weitere Informationsquelle soll die Rubrik "Empfehlungen" darstellen, die ebenfalls über die Webseite zu erreichen ist. Hier werden hilfreiche Bücher und andere Publikationen rezensiert.
            Der Podcast wird kostenlos über die üblichen Kanäle wie iTunes etc. und unsere Website verbreitet.
            Das Poster soll eine Kostprobe geben und vor allem Lust aufs Zuhören machen. Ziel ist es, Aufmerksamkeit zu schaffen und ggf. neue Interviewpartner zu finden. Zudem sind kurze Hörproben geplant.
            
               
            
         
         
            Key Facts
            
               Dauer pro Folge: 45 Minuten
               6 Folgen pro Jahr
               Arbeitsaufwand pro Folge: ca. 25 Stunden
               Webauftritt: 
                        http://www.digitale-wissenschaft.de
               
               Twitter: 
                        @DigiWissen
               
               Verwendete Technik/Software: Blue Yeti Podcasting Mikrofon, Ultraschall
               Reichweite über: iTunes, Webseite, Twitter, Facebook, DH-Liste
               Finanzierung: Crowdfunding via Patreon
            
            
               Aufbau
               
                  Dialog zwischen Jens-Martin Loebel und Carolin Hahn
                  30 Minuten Interview mit einer dritten Person (je nach Rubrik)
                  15 Minuten Vor- und Nachbesprechung
                  Einführung in ein zum Thema passendes Wissensgebiet im Bereich Digital Humanities, das im Blog ausführlicher nachgelesen werden kann
               
            
            
               Zielgruppe
               Als Informationsportal adressieren wir explizit Studierende und interessierte Laien. 
            
            
               Relevante Informationen auf dem Poster
               
                  Key Facts, Beteiligte, Konzept, Distibutionskanäle und Zielgruppe
                  Rubriken und Programm 2018
                  Audio-Effekt: Einbau einer dezenten Tonspur im Poster per Lautsprecher, Hörproben zum Mitnehmen
               
            
         
      
      
         
            
               Bibliographie
               
                  Geoghegan, Michael / Dan Klass (2007): Podcast Solutions. The Complete Guide to Audio and Video Podcasting. New York: Springer.
                    
               
                  Hagedorn, Brigitte (2006): Podcasting: Konzept | Produktion | Vermarktung. Köln: mitp Verlags GmbH.
                    
               
                  Podcast der Helmholtz-Gemeinschaft: https://resonator-podcast.de/ [letzter Zugriff 15. September 2017].
                    
               
                  Podcast der Universität Wien: http://medienportal.univie.ac.at/uniview/podcast-audimax/ [letzter Zugriff 15. September 2017].
                    
               
                  Physik-Podcast im ORF: http://www.physikalischesoiree.at/ [letzter Zugriff 15. September 2017].
                    
            
         
      
   



      
         
            Einleitung
            In diesem Beitrag wollen wir ein Vorhaben zur Diskussion stellen, das an zwei zentralen Herausforderungen in den Digital Humanities ansetzt: Der Erstellung adäquater Annotationsrichtlinien für geisteswissenschaftlich relevante textuelle Konzepte und der Schnittstelle in der Kooperation zwischen beteiligten Wissenschaftlerinnen und Wissenschaftlern aus Geisteswissenschaft und Informatik. Für DH-Projekte sind Kooperationen unerlässlich, wenn fortgeschrittene Techniken zur Textanalyse eingesetzt werden und/oder es um eine Zusammenführung von Konzepten oder Zugangsweisen geht, die bereits intradisziplinär als komplex gelten. Dabei wird ein signifikanter Anteil der Projektlaufzeit auf die Entwicklung einer “gemeinsamen Sprache” und die Identifikation der exakten, gemeinsamen wissenschaftlichen Fragestellung verwendet. Dies ist zweifellos ein produktiver Prozess, dessen erfolgreiche Durchführung allerdings voraussetzt, dass auf beiden Seiten Forscherinnen und Forscher beteiligt sind, die sich auf das interdisziplinäre Vorgehen voll einlassen und auch den nötigen Zeitaufwand tragen. 
            Methodisch-technisch ist ein substanzielles Nadelöhr bei der Entwicklung automatischer Werkzeuge das Fehlen von annotierten Goldstandards, an/auf denen Werkzeuge trainiert, verglichen und feinjustiert werden können. Das Fehlen der Goldstandards ist jedoch eigentlich ein nachgelagertes Problem, wie sich z.B. in narratologisch orientierten Projekten zeigt (heureCLÉA: Bögel et al., 2015; Propp annotation: Fisseni et al., 2014): Die Umsetzung narratologischer Theorien als Annotationen ist alles andere als trivial, da narratologische Konzepte nicht im Hinblick auf Annotation entwickelt wurden. Leerstellen in den Definitionen müssen gefüllt, Voraussetzungen geklärt und Unterkategorien geklärt werden. Die Annotation solcher Kategorien ist also kein reiner Umsetzungs- oder Implementierungsprozess, sondern einer bei dem sich tiefe, konzeptionelle Fragen stellen. Als Ergebnis solcher Prozesse stehen dann Annotationsrichtlinien, die die Brücke zwischen Theorie und Praxis schlagen. Erst wenn Annotationsrichtlinien für ein Phänomen (oder eine Gruppe von Phänomenen) etabliert sind, können größere Annotationsprojekte mit Aussicht auf Erfolg durchgeführt werden.
            Das von uns vorgeschlagene Vorgehen erlaubt den Beteiligten Forscherinnen und Forschern ihre Expertise einzubringen, ohne in einem gemeinsamen Projektkontext zu arbeiten. Die Schnittstelle zwischen D und H wird hierbei von annotierten Daten und Annotationsrichtlinien gebildet, wobei die Richtlinien ohne Kompromisse bezüglich möglicher Automatisierungen erstellt werden. Das Vorhaben gibt somit auch narratologisch/literaturwissenschaftlich anspruchsvoller Konzeptentwicklung und damit Theoriebildung einen Rahmen. Verfügbare annotierte Daten wiederum erlauben Informatikerinnen und Informatikern ohne Expertise in narratologischen Fragen die Entwicklung von Werkzeugen für komplexe technische Probleme.
         
         
            Ein 
                    shared task zur Erstellung von Annotationsrichtlinien
                
            Shared Tasks sind in der Computerlinguistik weit verbreitet und haben für viele Bereiche gezeigt, dass sie ein geeignetes Instrument sind, Forschungsbemühungen verschiedener Gruppen zum gleichen Thema zu bündeln und zu verstärken. In einem 
                    shared task versuchen verschiedene Arbeitsgruppen mit verschiedenen Methoden dieselbe, klar definierte Aufgabe zu lösen, z.B. Word Sense Disambiguation (z.B. Mihalcea et al., 2004), Sentiment Analysis (z.B. Nakov et al., 2013) oder Named Entity Recognition (z.B. Sang and/De Meulder 2003). Auch wenn bisweilen im Rahmen von NLP-shared tasks Annotationsstandards neu entwickelt werden, liegt der Fokus hier auf der Verbesserung der Vorhersagequalität automatischer Systeme. Damit in einem solchen Vorgehen literaturwissenschaftlich relevante und interessante Konzepte und Phänomene bearbeitet werden, muss literaturwissenschaftliche Expertise bei der Erstellung der Annotationsrichtlinien einfließen.
                
            Als Rahmen für die Entwicklung von Annotationsrichtlinien organisieren wir einen shared task der sich genau auf dieses Ziel konzentriert (Phase 1: Erstellung von Guidelines). Sind die Richtlinien etabliert, kann anschließend ein großes Korpus annotiert werden, das wiederum in einem NLP-shared task eingesetzt werden kann, um Verfahren zu erproben, die die annotierten Phänomene automatisch finden (Phase 2: Automatisierung).
            Als Phänomen haben wir uns dabei auf Erzählebenen in englischen und deutschen Texten festgelegt, da diese für zahlreiche, komplexere literaturwissenschaftliche Fragestellungen hilfreich sind, ohne selbst (für einen ersten 
                    shared task) zu komplex zu sein. Zudem sind sie als Phänomen omnipräsent: Praktisch jeder narrative Text enthält mehr als eine Erzählebene, und sie sind auch in nicht-textuellen Medien wie z.B. Filmen verbreitet. Die Existenz verschiedener Theorien zur Analyse von Erzählebenen in literarischen Texten belegt, dass es dabei auch konzeptionellen, theoretischen Entwicklungsbedarf gibt. Erzählebenen bilden darüber hinaus eine wichtige Segmentierungsstufe für die weitere automatische semantische Verarbeitung von Texten: z.B. sollte Koreferenzresolution von der vorher erfolgten Erkennung von Erzählebenen profitieren,da Koreferenzketten in heterodiegetischen eingebetteten Erzählungen nicht ebenenübergreifend sein sollten.
                
            Während Details zum Gesamtaufbau des Shared Tasks bereits in einem anderen Artikel beschrieben wurden (Reiter et al., 2017), fokussieren wir uns in diesem Beitrag auf die genauere Beschreibung der ersten Phase des 
                    shared tasks. 
                
         
         
            Geplanter Ablauf
            
               Erstellung von Annotationsrichtlinien 
               (bis Mitte Juni 2018)
               Im ersten Schritt wird allen Teilnehmerinnen und Teilnehmern ein 
                        development corpus bestehend aus ca. 20 Texten zugänglich gemacht. Die Texte liegen auf deutsch und englisch vor und decken verschiedene Genres und Epochen ab. Die Texte enthalten verschiedene Arten von Erzählebenen, gemäß eines etwas vagen Vorverständnisses.
                    
               Die Texte können und sollen von den Teilnehmerinnen und Teilnehmern benutzt werden, um Richtlinien für die Annotation von Erzählebenen zu entwickeln und zu testen. Ob die Texte in einer oder in beiden Sprachen verwendet werden, ist dabei den Teilnehmerinnen und Teilnehmern überlassen. Sie sollten dabei das Ziel verfolgen, eine möglichst breite Anwendbarkeit der Richtlinien sicherzustellen (auch jenseits des 
                        development corpus). Außerdem sollen die Richtlinien vollständig und selbsterklärend sein, so dass kein Expertenwissen zur Anwendung vorausgesetzt wird. Um mehrsprachige Anwendung zu ermöglichen, sollen die Richtlinien auf Englisch formuliert sein, sie dürfen aber sprachspezifische Beispiele enthalten.
                    
               Wie genau die Gruppen dabei vorgehen, bleibt ihnen überlassen. In vergangenen Annotationsprojekten (mit und ohne Bezug zu Literaturwissenschaft bzw. literarischen Texten) hat sich aber ein iterativer Prozess als fruchtbar erwiesen. Sobald eine erste Version der Richtlinien erstellt wurde, werden sie auf neuen Texten getestet, um ihre Definitionslücken oder Vagheiten zu identifizieren. Aus dem Schließen der Lücken ergibt sich dann eine weitere Version der Richtlinien, die wiederum auf Texten getestet werden können.
            
            
               Anwendung eigener Guidelines
               (bis Ende Juni 2018)
               Im zweiten Schritt sollen die Arbeitsgruppen ihre eigenen Richtlinien auf neuen Texten testen. Nach dem Einreichen ihrer Richtlinien erhalten die Teilnehmerinnen und Teilnehmer hierzu sechs neue literarische Texte, die vom Organisationsteams des 
                        shared tasks ausgesucht wurden. Die Annotation dieser Texte muss dabei in einem Web-basierten, frei zugänglichen, von den Organisatoren bereitgestellten Annotationstool durchgeführt werden, um die automatisierte Auswertung der Annotationen und ihren Vergleich zu ermöglichen.
                    
            
            
               Anwendung von Guidelines anderer Teilnehmer
               (bis Mitte Juli 2018)
               Im dritten Schritt erhält jede teilnehmende Gruppe Richtlinien anderer Gruppen, auf deren Basis Erzählebenen in den sechs Texten erneut annotiert werden, wobei alle Richtlinien von uns zuvor anonymisiert werden. Zusätzlich wird auch eine vom Organisationsteam betreute Gruppe von studentischen Hilfskräften alle eingereichten Annotationsrichtlinien auf den sechs Texten anwenden.
            
            
               Evaluation aller vorgeschlagener Guidelines
               (August/September 2018)
               Im letzten Schritt der ersten Phase des 
                        shared tasks werden alle eingereichten Annotationsrichtlinien verglichen und evaluiert. Dafür treffen sich die Teilnehmerinnen und Teilnehmer zu einem Workshop, auf dem sie ihre eigenen Richtlinien vorstellen und gemeinsam Qualität und Komplexität bewertet werden. Das Ziel des Workshops ist außerdem, basierend auf der Diskussion und den Informationen bezüglich der Inter-Annotator-Agreements im Plenum und möglichst konsensual die Annotationsrichtlinien zu bestimmen, die dann in der zweiten Phase des 
                        shared tasks verwendet werden. Auf deren Basis werden dann Methoden und Systeme entwickelt, die automatisch Erzählebenen in Texten identifizieren können.
                    
               Zur vergleichenden Evaluation von Annotationsrichtlinien sind bisher Ansätze aus der Computer- und Korpuslinguistik zur quantitativen Messung des Inter-Annotator-Agreement (IAA) bekannt (vgl. Artstein, 2017), die im Bereich der Digital Humanities angewendet wurden und werden. Da es aber bei der Erstellung von Annotationsrichtlinien für narratologische Phänomene eben nicht 
                        nur um die Umsetzung und Erklärung einer klar spezifizierten Theorie geht, sondern eben 
                        auch um die (Weiter-)Entwicklung narratologischer Konzepte, bedarf es eines weitergehenden Blickes. Dabei sollen drei Aspekte Berücksichtigung finden: Die 
                        Anwendbarkeit von Annotationsrichtlinien kann durch quantitatives IAA gemessen werden. Hier stellen sich durch möglicherweise unterschiedliche theoretische Zugänge vor allem Fragen der Vergleichbarkeit. Der Aspekt der begrifflichen 
                        Abdeckung bezieht sich darauf, welche (bekannten) narratologischen Ebenenkonzeptionen  in der konkreten Ausgestaltung vollständig oder teilweise enthalten sind. Dies wird sich nur durch qualitative Analyse und wissenschaftliche Diskussion basierend auf theoretischen Vorstudien klären lassen, für die der Workshop einen Rahmen bieten soll. Die 
                        Nützlichkeit von Annotationsrichtlinien kann bei narrativen Ebenen dahingehend bewertet werden, ob sie interpretativ wertvolle Beschreibungen erlauben. Leitgedanke ist hier, dass narratologische Annotationen eine deskriptive Basis für literaturwissenschaftliche Interpretationen liefern sollen. Unterschiedlichen Annotationsrichtlinien zu folgen hieße also zu unterschiedlichen Text-Deskriptionen zu kommen, die wiederum unterschiedliche Interpretationen zulassen.
                    
            
         
         
            Conclusions
            Im Rahmen des Vortrags wollen wir insbesondere zwei der o.g. Aspekte in den Fokus rücken und diskutieren: Die iterative Entwicklung von Annotationsrichtlinien als verteiltes, kollaboratives Projekt sowie die Evaluation und Vergleichbarkeit von Annotationsrichtlinien für literarische Phänomene.
         
      
      
         
            
               Bibliographie
               
                  Artstein, Ron (2017): “Inter-annotator Agreement”, in: Ide, Nancy / Pustejovsky James (eds.): 
                        Handbook of Linguistic Annotation. Dordrecht: Springer. DOI 10.1007/978-94-024-0881-2.
                    
               
                  Bögel, Thomas / Gertz, Michael / Gius, Evelyn / Jacke, Janina / Meister,  Jan Christoph / Petris, Marco / Strötgen, Jannik (2015): “Collaborative text annotation meets machine learning: heurecléa, a digital heuristic of narrative”, in: DHCommons 1.
                    
               
                  Fisseni, Bernhard / Kurji, Aadil / Löwe, Benedikt (2014): “Annotating with Propp’s morphology of the folktale: Reproducibility and trainability”, in: 
                        Literary and Linguistic Computing 29(4):488–510, 1093/llc/fqu050
                    
               
                  Mihalcea, Rada /  Chklovski, Timothy / Kilgarriff, Adam (2004): “The Senseval-3 English Lexical Sample Task”. In 
                        Proceedings of SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain.
                    
               
                  Nakov, Preslav / Rosenthal, Sara / Kozareva, Zornitsa / Stoyanov, Veselin / Ritter, Alan / Wilson, Theresa (2013): “SemEval-2013 Task 2: Sentiment Analysis in Twitter”. In 
                        Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval ‘13, Atlanta, Georgia, USA.
                    
               
                  Reiter, Nils / Gius, Evelyn / Strötgen, Jannik / Willand, Marcus (2017): “A Shared Task for a Shared Goal - Systematic Annotation of Literary Texts
                        ”. In Digital Humanities 2017: Conference Abstracts, Montreal, Canada.
                    
               
                  
                  Sang, Erik F. Tjong Kim / de Meulder, Fien (2003): “Introduction to the CoNLL-2003 Shared Task: Language-independent Named Entity Recognition”, in 
                        Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Volume 4 (CONLL '03).
                    
            
         
      
   



      
         
            Thematik und Ziel
            
               „Der Schlaf der Vernunft gebiert Monster“, wusste Francisco de Goya. Ein 
                    Schlaf der Quellenkritik auch. Deshalb ist eine dem Digitalen angepasste, auf Daten erweiterte quellenkrititische Methodik üblich in den Digital Humanities und verwandten Fächern. Akzeptanz für eine kontextuell orientierte Quellenkritik im Digitalen ist auch im erweiterten Diskurs detektierbar, wenn auf einer abstrakteren Ebene für kulturkritische Perspektiven mit verstärkt ganzheitlichen Sichtweisen plädiert wird (Liu 2012, Presner 2015). In diesem Zusammenhang steht das Ziel des Vortrags, der sich in zwei Blöcke gliedert: Einer Analyse von Quellenspezifika im Digitalen folgt, vergleichend und übertragend, die Skizze eines digital-quellenkritischen Leitfadens, der Kriterien der 
                    exakt historischen Methode auf Born Digital spiegelt. Damit nimmt das transdisziplinäre Experiment methodisch Anleihe an der volkskundlichen 
                    Münchner Schule, die wiederum auf “Klassiker” der Quellenkritik zurückgreift, etwa Johann Gustav Droysen. Diese dezidiert 
                    historische Sichtweise wird eingenommen, da auch digitale Quellen historisch bedingt sind und ihre Deutung - im Sinne einer Ganzheitlichkeit - dem Rechnung tragen sollte. Es erscheint sinnvoll, eine Systematik anzuwenden, die hilft, Kontexte entsprechend zu identifizieren und transparent in hermeneutische Prozesse miteinzubeziehen. 
                
         
         
            Stand
            Die historische Dimension des Digitalen (das Internet: Brügger 2017; Born Digital, Webseiten: z. B. Nanni 2017; Twitter: Sternfeld 2014) ist ebenso Gegenstand in den Digital Humanities wie die kulturwissenschaftliche (Klawitter et al 2012). Quellenkritikim Digitalenpräzisieren Handbücher(Crompton et al 2016; Griffin, Hayler 2016), Angebote wie „compas. Strukturiertes Forschen im Web“ (infoclio.ch, Baumann/Hügi 2017) führen niederschwellig ein in„Quellenkritik bei Quellen aus dem Internet“. Zwei Beiträge seien hier herausgestellt: Eva Pfanzelter (2010)vergleicht explizit historische Quellenkritik („innere/äußere Kritik“, Pfanzelter 2010: 43) mit Quellenkritik im Digitalen und beleuchtet den daraus notwendig resultierenden „kritischen Umgang mit digitalen Ressourcen“. Peter Haber Peter Haber (2011)rekurriert in „Digital Past“ explizit auf Droysens Methodik. 
         
         
            Spezifika
            Vorangestellt sei ein Diktum von Alan Liu, 
                    „(…) the virtual is indeed fully material“ (Liu 2014: 276). Dem wird zugestimmt 
                    und aufgezeigt, dass das Ungreifbare Folgen hat für die Einordnung von Inhalt und Kontext: Digitale Quellen weisen spezifische Eigenschaften auf, die sich auf Autorschaft, Stoffliches und Zeitliches beziehen. Diese Kriterien werden anhand digitaler Quellenarten herausgearbeitet und in Bezug auf kritische Methodik betrachtet, um dann die methodische Übertragung zu zeichnen.
                
            
               Daten und Autorschaft
               Konventionell bezieht sich Autorschaft im Digitalen auf sekundäre oder primäre Quellen, bei denen Publizierende konservativ verzeichnet sind, sowie auf Schwarmprodukte mit fließenden Autorschaften, Schichtungen und Intentionen, deren Identifikation synoptische Auswertungsprozesse verlangt. 
                        Da Autorschaft und Intentionalität als kritisches Moment eng hängen zusammenhängen, ist die Frage der Autorschaft im gegebenen Kontext zu erweitern auf Daten: Datenkritik. Das „Ethos der Statistik“, das sich auf Erfassungsparameter und Algorithmen genauso wie auf Fragestellungen und Operationalisierungen bezieht, ist im quellenkritischen Sinn zu erweitern auf hermeneutische Interpretationen. Am Beispiel von Malte Rehbeins (2017) Kritik des Projekts 
                        Charting Culture wird der Wert dieser kritischen Verschränkung deutlich. 
                    
            
            
               Digitalisat und Stofflichkeit 
               Digitalisate bedürfen als vom Analogen ins Digitale transformierte Quellen besonderer Kritik, sowohl bezüglich des Objekts als auch der Metadaten. Im Analogen bildet die Dualität von Medium und Text Information aus, bei Daten als Träger von Information fallen Medium und Botschaft im McLuhanschen Sinne zusammen. Deshalb wohnt digitalen Repräsentationen immer ein Informationsverlust inne, dem Erfassung und Modellierung lediglich entgegenwirken. So teilt ein digitales Faksimile mehr über die 
                        kritische Physis der analogen Quelle mit (z. B. Alterung) als es die Homogenität eines OCR-prozessierten Textes vermag (immanente Schriftinformationen). 
                    
            
            
               Born Digital und Zeitlichkeit
               Born Digital hat keine Rückbindung an Greifbares und ist selbst potentiell ungreifbar. Ihrem Wesen nach sind diese Quellen fluid: Zum einen unterliegen sie ständigen Alternierungsprozessen, die der Rezipient bestenfalls passiv zur Kenntnis nehmen kann. Folgen für das Erfassen und Tradieren, die Domäne der Webarchivierung, sind Selektion, motiviert durch permanente „Vervielfältigung“ der sich im Turnus oder unregelmäßig verändernden Quellen, daraus resultierende Lücken sowie Probleme bei Datenspeicherung bzw. -vorhaltung. Zum anderen oszilliert Born Digital zwischen Ewigem Leben (vgl. Recht auf Vergessen) und spontanem Verschwinden. Diese Eigenschaften haben in Summe Konsequenzen für Korpusvalidität, Datierungen bzw. Ordnungen (Zeugenschaften), die Kritik von Inhalten sowie für die potentielle geschichtliche Dimension der Quellenart als solcher. Aufgrund der besonderen Bedeutung als „Quelle der Zukunft“ und ihrer komplexen Beschaffenheit stellt Born Digital eine besondere Herausforderung dar. 
            
         
         
            Methode
            Das präzise Sondieren dynamischer kulturhistorischer Phänomene ist sowohl den Digital Humanities als auch der Volkskunde eigen, in der der hier diskutierte methodische Bezugspunkt Mitte der 1950er Jahre gesetzt wurde. Hans Moser und Karl-Sigismund Kramer initiierten die als 
                    Münchner Schule bezeichnete Perspektive. Sie trug zu einer Neuaufstellung nach der NS-Zeit bei, in der etliche Fach-Akteure die Blut-und-Boden Ideologie mitgestaltet hatten. Anstelle der Suche nach Absolutem im (germanischen) Vergangenem („Ursprungsforschung“) trat die 
                    exakt historische Methode als „exakte Geschichtsschreibung der Volkskultur“ mit definierten Quellen, Räumen und Zeiten. 
                    Damals teils polarisierend, forderte Hermann Bausinger (
                    Tübinger Schule) zeitnah eine Orientierung am Aktuellen, der „technischen Welt“ – in Kombination führten u. a. diese beiden Ansätze zu einer Art vektorialen Denkens in der Volkskunde: Heutige Phänomene methodisch 
                    historisch zu lesen.
                
         
         
            Prozess
            Die Historische Quellenkritik staffelt sich zuerst in „äußere“und „innere Kritik“. Der„äußeren Kritik“(vgl. zum Begriff: Pfanzelter 2010: 43) zuzuordnen sind Aspekte der Multimodalität - das Zusammenspiel von Text, Bild, Audiovisuellem, Interaktion - was imFolgenden nicht dezidiert vertieftwird; derBlick geht vielmehr 
                    exakt historisch von Außen nach Innen. Karl-Sigismund Kramer formuliert 1968 modellhaft Kriterien der Quellenkritik, die Übertragung folgt dieser Systematik. 
                
            Der Quellenkritik vorgelagert ist eine Material-Kritik zur Unterscheidung „objektiven oder subjektiven Zeugniswerts“ bzw. von „Mischlagen“, was an der individuellen Quelle zu beurteilen ist. Übertragen auf Born Digital, erscheinen komplexere Formen wie Blogs und Foren, die ausgeprägt durch „Mischlagen“ charakterisiert sind, probat: „Objektiv“ bezieht sich auf inhaltlich definierte Themenkomplexe, „subjektiv“ auf eine erste Grobordnung nach Tendenzen. 
            Es folgen die drei Stufen der Quellenkritik (nach Droysen; vgl.: Haber):
            
               „1. Kritik der Echtheit“; dies verlangt den kritischen Abgleich von Traditionen bezüglich falscher Sachverhalte. Z. B.: Das Erzeugen einer Authentizitäts-Anmutung, die Optimierungsprozessen geschuldet ist und von einem spezialisierten Microtask-Markt mitgetragen wird.
               „2. Kritik des Früheren und Späteren“; das Prüfen zeitlicher Schichtung hat bei der Dynamik der gegebenen Quellen zufolge, dasslineare Vergleiche nur anhand systematisch eingehegter Archivierung möglich sind – diese Stufe der Kritik verweist auf die Notwendigkeit einer solchen zur Herstellung der Arbeitsbasis.
               „3. Kritik des Richtigen, d. h. die Frage nach dem Grad der Verzeichnung [eines objektiven Verhalts, d. Verf.], die (...) besonders durch subjektive und tendenziöse Verfärbung eingetreten sein kann“; aufbauend auf der bereits erfolgten Material-Kritik werden Themenkomplexe weiter aufgesplittet und granularer „objektiv“ kategorisiert. Dieses Extrapolieren von Konnotationen benötigt eine intermediale, dezidiert historische Lesart. 
                        
               
            
            Auf dieser Basis kann die Interpretation in vier Stufen vorgenommen werden:
            
               „1. Pragmatische Interpretation, d. h. die Herstellung des sachlichen Zusammenhanges innerhalb des Forschungsgegenstandes (ob Einzelerscheinung oder Gesamtaspekt), wie er sich aus dem kritisch geordneten Material ergibt.“ Hier wird Verlinkung im Kontext der Korpusvalidität angesprochen – wie ist „Gesamtheit“ im Terrain von Born Digital bewertbar? 
               „2. Interpretation der Bedingungen, (…) Umwelteinflüsse im engeren Umkreis der lokalen, wirtschaftlichen, rechtlichen, sozialen, technischen und allgemein geistigen Bedingungen, die auf das Werden der Erscheinung eingewirkt haben und ihre Funktionen bestimmen“. Das Beispiel Fake News verweist auf Fragen, die hier Relevanz haben.
               „3. Psychologische Interpretation, d. h. Versuch der schärferen Erkenntnis der seelischen Konstitution der Umwelt, in der die Erscheinung beheimatet ist, und des Willens und der Gefühle der aktiv oder passiv beteiligten Personen und Gruppen.“ Bezugsrahmen für Subjektives, das psychosozial eingeordnet und kontextuell decodiert wird, ist hier Twitter.
               „4. Interpretation nach den bewegenden sittlichen und politischen Mächten, (...) überindividuelle und [auf] den engeren Umkreis übergreifenden Impulse, die auf das Volksleben einwirken, es bewegen und gestalten“: Hier erfolgt die kulturkritische Einbettung in größere gesamtgesellschaftliche bzw. gobale Kontexte und theoretische wie empirische Metaperspektiven.
            
         
         
            Fazit
            Das transdisziplinäre Experiment versteht sich als „synkretistischer“ Versuch, eine tradierte Denkschule auf den Raum des Digitalen zu projizieren. Die Übertragung gibt Impulse für methodische Vertiefungen (konzeptionelle Verdichtung, Use Cases) und für Adaptionen in der Lehre (geisteswissenschaftliche Grundlagen und Analytik, Kritikfähigkeit).
         
      
      
         
            
               Bibliographie
               
                  Baumann, Jan / Hügi, Jasmin (2017): „compas. Strukturiertes Forschen im Web. Ein Projekt von infoclio.ch.“; ebd.:„2. 5. 2. Quellenkritik bei Quellen aus dem Internet“. 25.04.2017 http://www.compas.infoclio.ch/de/kompas/2-5-2-quellenkritik-bei-quellen-aus-dem-internet/164 [letzter Zugriff 11. September2017].
               
                  Brückner, Wolfgang (1985): „Hans Mosers Bedeutung für die Volkskunde“, in: Moser, Hans (1985): Volksbräuche im geschichtlichen Wandel. Ergebnisse aus fünfzig Jahren volkskundlicher Quellenforschung. Berlin-München: Deutscher Kunstverlag X-XI
                    
               
                  Crompton, Constance / Lane, Richard J. / Siemens, Ray(eds.) (2016) : „Doing Digital Humanities. Practice, Training, Research“. London: Routledge 
               
                  Nanni, Federico (2017): „Reconstructing a website’s lost past. Methodological issues concerning the history of Unibo.it.“, in: 
                        Digital Humanities Quarterly 2017 Volume 11 Number 2
                    
               
                  Gerndt, Helge (ed.) (1987): Volkskunde und Nationalsozialismus. Referate und Diskussionen einer Tagung der Deutschen Gesellschaft für Volkskunde, München 23. bis 25. Oktober 1986. München: 
                        Münchner Vereinigung für Volkskunde, 1989² 
                    
               
                  Griffin, Gabriele / Hayler, Matt (2016): Research methods for reading digital data in the digital humanities. Edinburgh: Edinburgh University Press
                    
               
                  Haber, Peter (2011): Digital Past. Geschichtswissenschaft im digitalen Zeitalter. München: Oldenbourg Verlag 104-112
                    
               
                  Kaschuba, Wolfgang (1999/2003): Einführung in die Europäische Ethnologie. München: Beck 2006³ 83-85
               
                  Klawitter, Jana / Lobin, Henning / Schmidt Torben(2012): „Kulturwissenschaftliche Forschung – Einflüsse von Digitalisierung und Internet“, in: Diess. (eds.): Kulturwissenschaften digital. Neue Forschungsfragen und Methoden. Frankfurt am Main: Campus Verlag 9-29
                    
               
                  Köstlin, Konrad: „Historische Methode und regionale Kultur“ in: Ders.(ed.) (1987): Historische Methode und regionale Kultur. Karl-S. Kramer zum 70. Geburtstag. Regensburger Schriften zur Volkskunde, B. 4. Berlin-Vilseck: Tesdorpf Verlag 7-23
                    
               
                  Kramer, Karl-Sigismund(1968): „Zur Erforschung der historischen Volkskultur“, in: 
                        Rheinisches Jahrbuch für Volkskunde, 1968, 19. Jahrgang. Bonn: Ferdinand Dümmler Verlag 7-41
                    
               
                  Liu, Alan (2012): „Where Is Cultural Criticism in the Digital Humanities?“, in: Gold, Matthew K. (ed.): Debates in the Digital Humanities. Minneapolis-London: University of Minnesota Press 490-509. http://dhdebates.gc.cuny.edu/debates/text/20 [letzter Zugriff 11. September 2017].
                    
               
                  Liu, Alan (2014): „The Big Bang of Online Reading“, in: Arthur, Paul Longley / Bode, Katherine (eds.): Advancing Digital Humanities. Research, Methods, Theories. Basingstoke: Palgrave Macmillan 275-290 
                    
               
                  Mittler, Elmar (2012): „Wissenschaftliche Forschung und Publikation im Netz. Neue Herausforderungen für Forscher, Bibliotheken und Verlage“, in: Füssel, Stephan (ed.): Medienkonvergenz – Transdisziplinär. Media Convergence, Band 1. Berlin-Boston: Walter de Gruyter Verlag 31-80 
                    
               
                  Pfanzelter, Eva (2010): „Von der Quellenkritik zum kritischen Umgang mit digitalen Ressourcen“, in: Gasteiner, Martin / Haber, Peter (eds.): Digitale Arbeitstechniken für die Geistes- und Kulturwissenschaften. Wien: UTB 39-49
                         
               
               
                  Presner, Todd (2015): „Critical Theory and the Magle of Digital Humanities“, in: Svensson, Patrik (ed.):Between humanities and the digital. Cambridge, Mass: MIT Press 55-67
                    
               
                  Rehbein, >Malte (2015): Forum: „Digitalisierung braucht Historiker/innen, die sie beherrschen, nicht beherrscht“, in: H-Soz-Kult, 27.11.2015 http://www.hsozkult.de/debate/id/diskussionen-2905 [letzter Zugriff 11. September 2017].
                    
               
                  Rehbein, Malte (2017): „Geschichtsforschung im digitalen Raum. Über die Notwendigkeit der Digital Humanities als historische Grund- und Transferwissenschaftwissenschaft“, in: Herbers, Klaus / Trenkle, Viktoria (eds.): Papstgeschichte des hohen Mittelalters: Digitale und hilfswissenschaftliche Zugangsweisen zu einer Kulturgeschichte Europas (im Druck).
                    
               
                  Schaller, Martin (2015): „Arbeiten mit digitalisierten Quellen. Herausforderungen und Chancen“, in: Schmale, Wolfgang (ed.): Digital humanities. Praktiken der Digitalisierung, der Dissemination und der Selbstreflexivität. Stuttgart: Steiner 15-30 
                    
               
                  Schich, Maximilian / Song, Chaoming / Ahn, Yong Yeol /Mirsky, Alexander / Martino, Mauro / Albert Barabási, Albert László / Helbing, Dirk (2014): „A network framework of cultural history“, in: 
                        Science 345 (6196). DOI: 10.1126/science.1240064. 558–562
                    
               
                  Sternfeld, Joshua (2014): „Historical Understanding in the Quantum Age“, in: 
                        Journal of Digital Humanites Vol. 3, No. 2 Summer 2014
                    
               
                  Weber, Matthew S. (2017): „The tumultuous history of news on the web“, in: Brügger, Niels / Schroeder, Ralph (eds.): The Web as History. Using Web Archives to Understand the Past and the Present. London: UCL Press 83-100
                    
            
         
      
   



      
         
            Sentiment Analyse und Dramenanalyse
            Sentiment Analyse (SA) beschreibt eine Reihe von computergestützten Methoden zur Prädiktion der Polarität eines Texts, versucht also vereinfacht gesagt automatisiert herauszufinden, ob ein Text ein positives oder negatives Gefühl ausdrückt (Liu 2016). Darüber hinaus werden teilweise auch komplexere emotionale Kategorien (wie z.B. Zorn und Freude) betrachtet (Mohammad & Turney 2010). Zentrale Anwendungsfelder der SA sind bislang vor allem die Analyse von Online-Reviews (McGlohan, Glance & Reiter 2010) und Social Media-Daten (Kouloumpis, Wilson & Moore 2011). 
            Zur Analyse von literarischen Texten mittels SA-Techniken finden sich bislang nur wenige Studien, z.B. zu Märchen (Alm, Roth & Sproat 2005) und Romanen (Kakkonen & Kakkonen 2011; Elsner 2012; Jannidis et al. 2016). Auf größeren Textkorpora wurde getestet, inwiefern SA-Werte eines Textes und Emotionskurven von Texten zur Genreklassifikation verwendet werden können (Kim, Padó & Klinger 2017) und wie begriffsgeschichtliche Bedeutungsverschiebungen in literarischen Texten mithilfe von erweiterten SA-Methoden erforscht werden können (Buechel, Hellrich & Hahn 2017). In Dramentexten hat man bisher die Verteilung von emotionalen Kategorien (Mohammad 2011) oder die Entwicklung von Figurenbeziehungen (Nalisnick & Baird 2013) in Shakespeare-Dramen untersucht. Auch der vorliegende Beitrag beschäftigt sich mit dem Einsatz von SA im Bereich der Dramenanalyse. Es werden erstmals systematisch verschiedene Methoden der SA für Dramen getestet und evaluiert. Zudem wird exploriert, inwiefern bisher in der Literaturwissenschaft erforschte Aspekte von Dramen mithilfe der SA erfasst werden und inwiefern die SA auch für die Gewinnung neuer literaturwissenschaftlicher Erkenntnisse eingesetzt werden kann.
            Das im Rahmen dieser Studie verwendete Lessing-Korpus umfasst ein mit Strukturinformationen annotiertes Dramenkorpus mit 11 Dramen, bestehend aus insgesamt 8224 Einzelrepliken. Sämtliche Dramen wurden über die Plattform 
                    TextGrid
                bezogen, so dass alle im Rahmen dieses Beitrags entwickelten Tools auch auf andere 
                    TextGrid-Dramen anwendbar sind. Mit dem am besten evaluierten SA-Verfahren wurde eine webbasierte Anwendung zur Analyse und Visualisierung von Sentiment-Verteilungen und -Verläufen implementiert.
                
         
         
            Evaluation unterschiedlicher SA-Verfahren
            
               Lexikonsbasierte SA
               Innerhalb der SA unterscheidet man zwei wesentliche Ansätze: (1) die Nutzung maschinellen Lernens und (2) die Verwendung lexikonbasierter Verfahren. Für das erstgenannte Vorgehen ist typischerweise ein mit Sentiment-Informationen annotiertes Trainingskorpus notwendig (D‘Andrea et al. 2015), welches für die Dramenanalyse bislang nicht vorliegt. Aus diesem Grund werden in der vorliegenden Arbeit lexikonbasierte Verfahren eingesetzt. Ein Sentiment-Lexikon ist dabei eine Wortliste, in der für jedes Wort Sentiment-Informationen angegeben sind (Liu 2016: 10), also z.B. ob es positiv oder negativ konnotiert ist und in welchem Ausmaß (Polaritätsstärke). Ein derartiges Wort nennt man auch 
                        sentiment bearing word (SBW; Liu 2016: 189).
                    
            
            
               SA-Parameter
               Folgende SA-Optionen wurden in unterschiedlichen Kombinationen systematisch evaluiert: 
               
                  i) Lexika – Es wurden fünf zentrale Sentiment-Lexika für den deutschsprachigen Bereich herangezogen: 
                        SentiWortschatz (SentiWS; Remus, Quasthoff & Heyer 2010), die 
                        Berlin Affective Word List – Reloaded (Bawl-R; Vo et al. 2009), die deutsche Version des 
                        NRC Emotion-Association Lexicon (NRC, Mohammad & Turney 2010), ein Lexikon von Clematide & Klenner (2010; im folgenden CK genannt) und das 
                        German Polarity Clues (GPC; Waltinger 2010). SentiWS, Bawl-R und CK enthalten Polaritäten und Polaritätsstärken, das NRC und GPC nur Polaritätsangaben. Das NRC enthält des Weiteren Annotationen zu acht unterschiedlichen Emotionen (Zorn, Furcht, Erwartung, Freude, Vertrauen, Ekel, Traurigkeit, Überraschung).
                    
               
                  ii) Historisch-linguistische Varianten – Über ein Tool des Deutschen Text-Archivs von Jurish (2011) wurde die Option der Lexikon-Erweiterung mit historischen linguistischen Varianten der Originalwörter untersucht.
                    
               
                  iii) Stoppwortlisten – Analog zu Saif et al. (2014) wurde der Einfluss der Verwendung von insgesamt drei unterschiedlichen Stoppwortlisten auf die Qualität der SA untersucht. Grund hierfür ist, dass durch verschiedene Kombination der Verfahren Sentiment-tragende Stoppwörter entstehen. Neben herkömmlichen Stoppwörtern wurden dabei auch Listen mit hochfrequenten Wörtern des Korpus untersucht. Dadurch wird der Einfluss von Wörtern analysiert, die zwar als sentiment-tragend in SA-Lexika ausgezeichnet werden, aber aufgrund der häufigen Nutzung im Korpus ein ungleichmäßiges Sentiment-Gewicht erzeugen (z.B. Herr, Fräulein).
                    
               
                  iv) Lemmatisierung – Eine weitere untersuchte Verarbeitungsform für die SA ist die Lemmatisierung. Als Lemmatisierer werden der 
                        Pattern-Lemmatisierer (De Smedt & Daelemans 2012) der Python-Bibliothek 
                        textblob und der Python-Wrapper des 
                        treetagger-Tools (Schmid 1995) evaluiert. Viele SA-Lexika enthalten lediglich Grundformen. Aufgrund der Probleme und Schwierigkeiten der Lemmatisierung im Deutschen (Eger, Gleim & Mehler 2016) soll vergleichend untersucht werden, welcher Lemmatisierer die besten Ergebnisse in Kombination mit Lexika erzielt. Ferner enthalten einige SA-Lexika manuell angegebene flektierte Wortformen. Es wird somit auch die automatische Lemmatisierung mit der manuellen Erweiterung verglichen.
                    
            
            
               SA-Metriken
               Alle nachfolgenden Berechnungen wurden bezüglich aller kombinatorischen Möglichkeiten der soeben beschriebenen SA-Parameter durchgeführt. Dabei werden die jeweiligen SA-Metriken nach Term-Zähl-Methodik (Kennedy & Inkpen 2006) berechnet, d.h. ein Text wird hinsichtlich vorhandener SBWs untersucht, positive und negative Wörter ausgezählt und für einen Polaritätswert die positive von der negativen Zahl subtrahiert. SA-Metriken wurden auf folgenden Ebenen über die jeweils zugehörigen Texte kalkuliert: Drama, Akte, Szenen, Repliken sowie Sprecher und Sprecherbeziehungen pro Drama, Akt, Szene und Replik. Die Beziehungen zwischen den Figuren wurden nach einer Heuristik von Nalisnick & Baird (2013) berechnet. 
            
            
               Erstellung des Gold Standards
               Zur systematischen Evaluation der Prädiktionsleistung der verschiedenen SA-Ansätze wurde ein Evaluationskorpus bestehend aus 200 Repliken erstellt. Bei der Auswahl der Repliken wurde darauf geachtet, dass die dramenspezifische Verteilung berücksichtigt wird, längere Dramen sind also mit mehr Repliken vertreten. Ferner wurden nur solche Repliken aufgenommen, die mindestens 19 Wörter umfassen. Diese Länge entspricht etwa -25% des Mittelwerts des Gesamtkorpus und vermeidet damit die Selektion von zu kurzen Repliken. Es wurde insgesamt auf eine gleichmäßige Längenverteilung geachtet.
               Die Repliken wurden von insgesamt fünf Personen (4 weiblich, 1 männlich; alle jeweils mit Deutsch als Muttersprache) jeweils unabhängig voneinander bezüglich deren Polaritätswirkung bewertet. Die Polarität jeder Replik wurde jeweils sechswertig (sehr negativ, negativ, neutral, gemischt, positiv, sehr positiv) und binär (positiv, negativ) bewertet. Die Annotationen wurden bezüglich des Übereinstimmungsgrades analysiert. Dazu wurden das Übereinstimmungsmaß Fleiss‘ Kappa (Fleiss 1971) sowie der Durchschnittswert der prozentualen Übereinstimmung aller Annotatoren und Annotatorinnen berechnet (vgl. Tabelle 1).
               
                  
                  
                     Tabelle 1. 
                            Annotator agreement.
                        
               
               Man erkennt eine geringe Übereinstimmung für die Bewertungsskala mit sechsstufiger Polarität und eine moderate Übereinstimmung für die binäre Variante. Die Ergebnisse verhalten sich konform zu verwandten Studien bei der Interpretation literarischer Texte (Alm & Sproat 2005). Als finale Annotation für eine Replik wird die binäre Polarität gewählt, die die Mehrheit der Annotatoren und Annotatorinnen ausgewählt haben (Endresultat: 139 negativ, 61 positiv).
            
            
               Evaluationsmaße 
               Als Evaluationsmaße wurden Genauigkeit (accuracy), Recall, Precision und F-Werte (Gonçalves et al. 2013) herangezogen. Abb. 1 zeigt einen Ausschnitt aus den je fünf besten Kombinationen pro Lexikon, geordnet nach Genauigkeit.
                        
               
               
                  
                  
                     Abbildung 1: Ausschnitt aus der detaillierten Ergebnistabelle zur Evaluation der SA-Kombinationsmöglichkeiten.
                        
               
            
            
               Ergebnisse der Evaluation
               Nachfolgend erfolgt eine überblicksartige Zusammenstellung einiger zentraler Ergebnisse aus der Evaluation:
               
                  Eine explizite Lemmatisierung führt zu einer verbesserten Leistung. Beide Lemmatisierer erzielen dabei meist ähnliche Ergebnisse. Die Lexikonerweiterung durch historische Varianten macht die explizite Lemmatisierung jedoch weitestgehend unnötig, da hierbei auch eine grundlegende Lemmatisierung inkludiert ist. 
                  Es zeigt sich eine konsistente Verbesserung durch die Lexikonerweiterung mittels der Wort-Varianten aus dem Tool von Jurish (2011). 
                  Stoppwortlisten haben nur auf vereinzelte Lexika (GPC, CK) einen merklich positiven Einfluss. 
                  Lexika mit Polaritätsstärken sind meist besser als reine Term-Zähl-Verfahren desselben Lexikons. 
                  Das Lexikon, dass die höchsten Genauigkeiten für die SA erzielt, ist SentiWS 
                  Die beste Leistung (unter Analyse aller Metriken) erzielt das erweiterte SentiWS mit den Polaritätsstärken, lemmatisiert mittels Pattern-Lemmatisierer und ohne Stoppwortliste (Genauigkeit = 0,67; F-Wert = 0,64). Die Erkennungsrate ist besser als die random baseline von 0,576 aber schlechter als viele Erkennungsraten auf anderen Anwendungsgebieten der SA (Vinodhini & Chandrasekran 2012). 
               
               Aufgrund der Tatsache, dass hier ein verhältnismäßig simpler SA-Ansatz gewählt wurde und bereits menschliche Annotatoren und Annotatorinnen Schwierigkeiten mit der Polaritätsbestimmung haben, sind die Ergebnisse insgesamt durchaus positiv zu bewerten.
            
         
         
            Online-Tool
            Abschließend wurde auf Basis des besten SA-Ansatzes ein Web-Tool für die SA bei Dramen entwickelt. Dieses bietet interaktive Visualisierungen der Sentiment-Verteilungen und -Verläufe für alle berechneten Ebenen. Neben den SentiWS-Metriken wurden auch die Emotionskategorien des NRC integriert. Über das Tool kann man erste Fallstudien auf Dramen-, Akt-, Szenen-, Repliken-, Sprecher- und Sprecherbeziehungsebene durchführen. Die SA-Komponente ist online verfügbar.
                    
            
            Trotz der historischen Differenz stimmen die Ergebnisse der automatischen SA tendenziell mit dem überein, was man in der Dramengeschichte über Bewertungen von Figuren und deren Verhalten weiß. Zusätzlich ist aber ein wichtiger heuristischer Mehrwert zu beobachten: eine Analyse allein auf der Basis von Sentiment-Zuschreibungen führt dazu, dass man das Augenmerk gezielt auf Fakten des Textes richtet, die bisher nicht berücksichtigt wurden. 
            Im Folgenden einige Beispiele für die Bestätigung bekannter Ergebnisse und für Entscheidungen von Analysefragen: 
            
               Fallstudie: Minna von Barnhelm
               Die Analyse von Minna von Barnhelm zeigt, dass die negativen emotionalen Bewertungen insgesamt gegenüber den positiven deutlich überwiegen (vgl. Abb. 2). Dieser Befund bestätigt die bekannte Erkenntnis, dass Lessing das Schema des rührenden Lustspiels verwendet hat. Während die Komik im Stück eher das Ergebnis von Schlussprozessen ist, geht es auf der wörtlichen Ebene überwiegend um ernste Vorwürfe und drohenden Identitäts- und Beziehungsverlust.
               
                  
                  
                     Abbildung 2: Polaritätsverteilung im Drama – 
                            Minna von Barnhelm
                  
               
               Es ist verschiedentlich behauptet worden (Saße 1993), Minna und nicht Tellheim sei die lächerliche Figur des Stücks. Die Sympathielenkung auf der wörtlichen Ebene des Textes, die in der unten stehenden Sentimentverteilung pro Akt abgebildet ist, kann dazu herangezogen werden, diese Frage negativ zu bescheiden (vgl. Abb. 3). Es ist eine auffällige Abweichung der Polarität im zweiten Akt erkennbar. In diesem Akt tritt Minna von Barnhelm zum ersten Mal auf, Tellheim jedoch nicht.
               
                  
                  
                     Abbildung 3: Polaritätsverlauf pro Akt – 
                            Minna von Barnhelm
                  
               
            
            
               Fallstudie: Emilia Galotti
               Die letzte Visualisierung kann genutzt werden die Frage zu diskutieren, warum Emilia in Lessings Drama „Emilia Galotti“ sterben muss (vgl. Abb. 4). Auffällig ist hier die starke negative Bewertung Emilias im zweiten Akt. Entgegen bisheriger Interpretationen, in denen nur die Intrige des Prinzen und Marinelli dafür verantwortlich gemacht werden, dass Emilia um ihre Tugend fürchten und ihren Vater dazu bringen muss, sie umzubringen, wird dadurch die Abwertung allein durch die Avancen des Prinzen sichtbar, die später sowohl Emilias als auch für Odoardos Einschätzung der Ehrbarkeit Emilias in ihrem zukünftigen Leben bestimmen.
               
                  
                  
                     Abbildung 4: Polaritätsverlauf von Sprechern pro Akt – 
                            Emilia Galotti
                  
               
            
            
               Fazit 
               Insgesamt sind die ersten Analyse-Ergebnisse über das Web-Tool sehr vielversprechend. Dabei ist zu bedenken, dass über die Verwendung von SA-Lexika ein sehr einfacher SA-Ansatz gewählt wurde. Über ML- oder Hybrid-Ansätze können Besonderheiten der poetischen und veralteten Sprache möglicherweise besser beachtet werden. Ferner ist fraglich, ob eine Reduktion auf das sonst in der SA übliche binäre System positiv/negativ ausreichend ist für komplexe Interpretationen von Emotionen in Dramen.
               Durch Optimierung des SA-Verfahrens, Ausbau der Funktionen im Front-End und Erweiterung des Tools mit zusätzlichen Dramen sollen künftig Möglichkeiten und Nutzen der SA in der Dramenanalyse weiter exploriert werden.
            
         
      
      
         
             https://textgridrep.org/repository.html; Hinweis: alle im Beitrag erwähnte URLs wurden zuletzt am 12.1.2018 überprüft
             Die vollständige Tabelle ist online verfügbar unter https://drive.google.com/open?id=1cvyqiiLJ03XT1VNaWgSDoajeTE3wgeqxxr2PXp-VM4w
             http://lauchblatt.github.io/QuantitativeDramenanalyseDH2015/FrontEnd/sa_selection.html
         
         
            
               Bibliographie
               
                  Alm, Cecilia Ovesdotter / Sproat, Richard (2005): "Emotional sequencing and development in fairy tales.", in:
                         International Conference on Affective Computing and Intelligent Interaction 668-674.
                    
               
                  Alm, Cecilia Ovesdotter / Roth, Dan / Sproat, Richard (2005): "Emotions from text: machine learning for text-based emotion prediction.", in: 
                        Proceedings of the conference on human language technology and empirical methods in natural language processing 579-586.
                    
               
                  Buechel, Sven / Hellrich, Johannes / Hahn, Udo (2017): “The Course of Emotion in Three Centuries of German Text – A Methodological Framework.”, in: 
                        Digital Humanities 2017 176-179.
                    
               
                  Clematide, Simon / Klenner, Manfred (2010): "Evaluation and extension of a polarity lexicon for German.", in: 
                        Proceedings of the First Workshop on Computational Approaches to Subjectivity and Sentiment Analysis 7-13.
                    
               
                  D’Andrea, Alessia et al. (2015): "Approaches, tools and applications for sentiment analysis implementation.", in 
                        International Journal of Computer Applications 125.3: 26-33.
                    
               
                  De Smedt, Tom / Daelemans, Walter (2012): "Pattern for python.", in: 
                        Journal of Machine Learning Research 13: 2063-2067.
                    
               
                  Eger, Steffen / Gleim, Rüdiger / Mehler, Alexander. (2016). “Lemmatization and Morphological Tagging in German and Latin: A Comparison and a Survey of the State-of-the-art.”, in: 
                        LREC 1507–1513.
                    
               
                  Elsner, Micha (2012): "Character-based kernels for novelistic plot structure.", in: 
                        Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics 634-644.
                    
               
                  Fleiss, Joseph L. (1971): "Measuring nominal scale agreement among many raters.", in: 
                        Psychological bulletin 76.5: 378-382.
                    
               
                  Gonçalves, Pollyanna, et al. (2013): "Comparing and combining sentiment analysis methods.", in: 
                        Proceedings of the first ACM conference on Online social networks 27-33.
                    
               
                  Jannidis, Fotis, et al. (2016): "Analyzing Features for the Detection of Happy Endings in German Novels.", in: 
                        arXiv preprint arXiv:1611.09028
                    
               
                  Jurish, Bryan (2011): 
                        Finite-state canonicalization techniques for historical German. Diss. Universitätsbibliothek der Universität Potsdam.
                    
               
                  Kakkonen, Tuomo / Kakkonen, Gordana Galić (2011): "SentiProfiler: creating comparable visual profiles of sentimental content in texts.", in: 
                        Language Technologies for Digital Humanities and Cultural Heritage 62-67.
                    
               
                  Kennedy, Alistair / Inkpen, Diana (2006): "Sentiment classification of movie reviews using contextual valence shifters.", in: 
                        Computational intelligence 22.2: 110-125.
                    
               
                  Kim, Evgeny / Padó, Sebastian / Klinger, Roman (2017): “Investigating the relationship between Literary Genres and Emotional Plot Development.”, in: 
                        Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature 17–26.
                    
               
                  Kouloumpis, Efthymios / Wilson, Theresa / Moore, Johanna D.  (2011): "Twitter sentiment analysis: The good the bad and the omg!.", in: In 
                        Proceedings of the Fifth International Conference on Weblogs and Social Media 538-54.
                    
               
                  Liu, Bing (2016): 
                        Sentiment analysis: Mining opinions, sentiments, and emotions. New York: Cambridge University Press.
                    
               
                  McGlohon, Mary / Glance, Natalie S. / Reiter, Zach (2010) "Star Quality: Aggregating Reviews to Rank Products and Merchants.", in: 
                        Proceedings of the International Conference on Weblogs and Social Media (ICWSM-2010) 114-121.
                    
               
                  Mohammad, Saif (2011): "From once upon a time to happily ever after: Tracking emotions in novels and fairy tales.", in: 
                        Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities 105-114.
                    
               
                  Mohammad, Saif M. / Turney, Peter D. (2010): "Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon.", in: 
                        Proceedings of the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text 26-34.
                    
               
                  Nalisnick, Eric T. / Baird, Henry S. (2013): "Character-to-character sentiment analysis in shakespeare’s plays.“, in:
                         Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics 479–483.
                    
               
                  Remus, Robert / Quasthoff, Uwe / Gerhard, Heyer (2010): "SentiWS-A Publicly Available German-language Resource for Sentiment Analysis.", in: 
                        LREC 1168-1171.
                    
               
                  Saif, Hassan, et al. (2014): "On stopwords, filtering and data sparsity for sentiment analysis of twitter.", in: 
                        Proc. 9th Language Resources and Evaluation Conference (LREC) 810-817.
                    
               
                  Saße, Günter (1993): 
                        Liebe und Ehe: oder, wie sich die Spontaneität des Herzens zu den Normen der Gesellschaft verhält. Lessings Minna von Barnhelm. Tübingen: Niemeyer.
                    
               
                  Schmid, Helmut (1995): "Improvements in part-of-speech tagging with an application to German.", in: 
                        Proceedings of the acl sigdat-workshop.
                    
               
                  Vinodhini, G. / Chandrasekaran, R. M. (2012): "Sentiment analysis and opinion mining: a survey.", in: 
                        International Journal of Advanced Research in Computer Science and Software Engineering 2.6: 282-292.
                    
               
                  Võ, Melissa LH, et al. (2009): "The Berlin affective word list reloaded (BAWL-R) ", in: 
                        Behavior research methods 41.2: 534-538.
                    
               
                  Waltinger, Ulli (2010): "Sentiment Analysis Reloaded-A Comparative Study on Sentiment Polarity Identification Combining Machine Learning and Subjectivity Features.", in: 
                        Proceedings of the 6th International Conference on Web Information Systems and Technologies (WEBIST '10).
                    
            
         
      
   



      
         „Als Grundwissenschaft erwirken die Digital Humanities das so elementar wichtige 
                Nutzenkönnen digitaler Methoden und Daten, wie die Paläographie uns das 
                Lesenkönnen unserer Quellen sicherstellt“. (Rehbein 2015) Wie Malte Rehbein hier andeutet, scheint es hinsichtlich des Stellenwertes, der den Historischen Hilfs- oder Grundwissenschaften (HGW) wie auch den Digital Humanities (DH) eigentlich beigemessen werden sollte, durchaus Ähnlichkeiten zu geben. „Sollte“! Denn sowohl bei den HGW als auch bei den DH ist ihr Status als eigenständiger wissenschaftlicher Zweig nicht gänzlich unumstritten. Beide werden aktuell häufig als reine Zulieferer-Wissenschaften oder Dienstleister gegenüber der „richtigen“ Forschung wahrgenommen und ihr eigener wissenschaftlicher Wert in Zweifel gezogen.
            
         Die zum Kanon der traditionellen HGW gehörenden, teils sehr unterschiedlichen Teildisziplinen – neben der bereits genannten Paläographie zählen unter anderem auch Kodikologie, Epigraphik, Heraldik, Sphragistik oder Diplomatik dazu – arbeiten allesamt quellennah und betreiben damit wertvolle Grundlagenforschung. Ob der Breite dieses Kanons fällt es nicht ganz leicht, die HGW in Patrick Sahles „3-Sphären-Modell zur Kartierung der Digital Humanities als Schnittmenge, Brücke und eigenständigem Bereich zwischen (ausgewählten) traditionellen Disziplinen“ (Sahle 2015) zu verorten. In vielen Aspekten scheinen sie den DH im Hinblick auf Interdisziplinarität, Methoden, Stellenwert etc. jedoch sogar näher zu stehen als die Geschichtswissenschaft, unter die sie im Allgemeinen subsumiert werden. Ja, es gab sogar eine Phase, in der die Historische Fachinformatik als neue Teildisziplin der HGW galt.
                
         
         Während Professuren mit einer DH-Ausrichtung oder Denomination auf dem Vormarsch zu sein scheinen – in seinem Beitrag „Zur Professoralisierung der Digital Humanities“ zählt Sahle mittlerweile 53 Ausschreibungen (Stand: Januar 2018) im deutschsprachigen Raum mit allerdings äußerst diversen Ausrichtungen (Sahle 2016) – ist in den letzten Jahren die Zahl der Universitätsstandorte, die HGW im Programm haben, zunehmend kleiner geworden, so dass diese heute mit zu den strukturprekären Disziplinen gehören.
                 (Arbeitsstelle Kleine Fächer) Diese Situation war Ende 2015 Anlass für die Formulierung des Positionspapieres „Quellenkritik im digitalen Zeitalter. Die Historischen Grundwissenschaften als zentrale Kompetenz der Geschichtswissenschaft und benachbarter Fächer“ von Eva Schlotheuber und Frank Bösch (Schlotheuber / Bösch 2015), welches eine breite Diskussion auf „H-Soz-Kult“ in Gang setzte, bei der (teils beiläufig) auch immer wieder das Verhältnis von HGW und DH thematisiert wurde. Trotz aller Differenzen, die bei diesem – mitunter durchaus kontrovers geführten – Austausch zutage kamen, herrschte hinsichtlich eines Aspektes mehrheitlich Einigkeit: Der Wegfall von Professuren, Studiengängen und Lehrveranstaltungen, die das notwendige methodische, grundwissenschaftliche Rüstzeug an heutige und künftige Generationen von Studierenden weitergeben, resultiert in einem Mangel an entsprechenden Fachkompetenzen. In einer Zeit, in der im Zuge zunehmender Digitalisierung historische Quellen in großer Zahl allgemein und jederzeit verfügbar geworden sind, führt dies zu der grotesken Situation, dass das Auffinden von Quellen und der Zugriff auf sie heute zwar deutlich einfacher geworden ist, die Mittel, mit diesen adäquat umzugehen, vielen Personen aber nicht mehr (oder noch nicht) zur Verfügung stehen. Dass der Zugang zu Datenbanken, die beispielsweise bei der Datierung, Verortung oder Einordnung von Einbänden, Wasserzeichen, Initialen etc. helfen, die Arbeit der Grundwissenschaftler|innen und auch anderer Forschenden heute erleichtert und ökonomisiert, wird von den Nutzer|inne|n niemand bestreiten. Das Gros des heutigen in den HGW beschäftigten Lehrpersonals ist allerdings selbst oft nicht ausreichend geschult, um die notwendigen Kenntnisse im Umgang mit diesen Ressourcen an die Studierenden zu vermitteln. 
            
         Dennoch fordern aktuelle Ausschreibungen von Bewerber|innen häufig ausgeprägte grundwissenschaftliche Kompetenzen und gleichzeitig Kenntnisse im Bereich der DH. Die wenigsten Absolvent|inn|en deutscher Hochschulen können diesem Profil heute wirklich gerecht werden. Personen, die im Rahmen ihres Studiums noch eine tiefergehende Ausbildung im erstgenannten Bereich genossen haben, stehen oft vor der Problematik, dass sie sich ihr Wissen im Bereich der DH mühevoll im Selbststudium oder in den zahlreich angebotenen Summer Schools erarbeiten müssen. 
         Neben diesen praktischen Problemen der Zugänglichkeit zu entsprechenden Weiterbildungsangeboten, differieren aber auch die grundlegenden Auffassungen darüber, welche (technischen) Kompetenzen überhaupt notwendig sind, um das „digital“ Dargebotene hinsichtlich seiner Wissenschaftlichkeit und Vollständigkeit hinreichend bewerten zu können.
                 Auch herrscht weiterhin Uneinigkeit darüber, wie diese Kompetenzen (an Studierende und Lehrende) überhaupt vermittelt werden können. Sind die Grundwissenschaften in der Pflicht, ihre Vermittlungskonzepte auf die veränderte Situation anzupassen? (Vogeler 2015) Definitiv! „[S]ind digitale Techniken und Methoden nicht nur eine Chance, sondern vielleicht auch die einzige Möglichkeit für eine sinnvolle Weiterentwicklung der Hilfswissenschaften“? (Hiltmann 2015) Wahrscheinlich ja! Doch wie kann diese Weiterentwicklung ganz konkret aussehen? (Wie) Können die hergebrachten grundwissenschaftlichen Kompetenzen erhalten, und dabei gleichzeitig neue Kompetenzen aufgebaut werden, die für das heutige und zukünftige wissenschaftliche Arbeiten benötigt werden? Wie müsste eine Neuausrichtung grundwissenschaftlicher Curricula, die stärker digitale Methoden in die Lehre integrieren, aussehen? Welche Umsetzungsversuche gibt es hier bereits? Welche Kompetenzen werden gebraucht und wo kann Kompetenzaufbau (auch für Graduierte) stattfinden? Welche Maßnahmen sind hierfür notwendig? Welche konkreten Maßnahmen wurden in den vergangen zwei bis drei Jahren vielleicht auch schon in Gang gesetzt, um den Bereich der HGW zu erhalten bzw. zu neuem Leben zu erwecken?
                 Wie kann das unzweifelhaft vorhandene Potenzial bestmöglich genutzt werden, um die grundwissenschaftliche Forschung zu befördern? Aber auch: Wo liegen vielleicht grundlegende Probleme in der Kollaboration von HGW und DH?
            
         Bei der Aushandlung des Stellenwertes bzw. der Verortung der DH ging es bisher meist um das Verhältnis zwischen angewandter Informatik und traditionellen Geisteswissenschaften. Der Dialog zwischen HGW und DHs erscheint aber besonders für einen fruchtbaren Austausch geeignet, da beide „Disziplinen“ teils mit sehr ähnlichen Problemen zu kämpfen haben und aus sich heraus schon interdisziplinär arbeiten. Andererseits stellen aber die DH gerade aus Sicht einiger Grundwissenschaftler|innen vermehrt ein Feindbild dar, da sie vermeintlich die Existenz des eigenen Faches bedroht sehen und/oder sich den neuen Herausforderungen nicht gewachsen fühlen. 
         Das Panel, welches diesmal Vertreter|inn|en der traditionellen HGW 
                UND Digital Humanists als dezidierte „Grenzgänger“ an einen Tisch bringt, soll also zum einen dazu dienen, Vorurteile abzubauen und die bereits begonnenen Diskussionen am Leben zu halten, zu konkretisieren und weiterzuführen, zum anderen auch Gelegenheit bieten, beispielsweise Projekte mit grundwissenschaftlicher Ausrichtung sichtbar zu machen und damit gleichzeitig deren Ansätze, Methoden und Umsetzung zur allgemeinen Diskussion zu stellen. 
            
         Die folgenden Personen (in alphabetischer Reihenfolge) haben ihre Teilnahme am Panel zugesagt:
         Jun.-Prof. Dr. Étienne Doublier, Juniorprofessur für Historische Hilfswissenschaften (Bergische Universität Wuppertal)
         Jun.-Prof. Dr. Torsten Hiltmann, Juniorprofessur für die Geschichte des Hoch- und 
                Spätmittelalters / Historische Hilfswissenschaften (Westfälische Wilhelms-Universität Münster)
            
         Prof. Dr. Andrea Stieldorf, Institut für Geschichtswissenschaft, Abteilung für Historische Hilfswissenschaften und Archivkunde, Rheinische Friedrich-Wilhelms-Universität Bonn
         Prof. Dr. Georg Vogeler, Zentrum für Informationsmodellierung in den Geisteswissenschaften, Karl-Franzens-Universität Graz
            
         Der geplante Ablauf ist wie folgt: 
         
            Nach einer knappen Einleitung in die Thematik des Panels und der Motivation zu dessen Organisation, erhalten die Diskutant|inn|en zunächst Gelegenheit zu einer individuellen Stellungnahme. Der Verlauf der anschließenden Diskussion wird nicht – wie sonst üblich – durch vorgegebene Fragen seitens der Moderatorin vorgegeben, sondern „von außen“ bestimmt, so dass dieser für alle Beteiligten nicht vorhersehbar ist. Mit „von außen“ ist hier zum einen das Plenum der Anwesenden gemeint, unten denen sich hoffentlich auch zahlreiche Vertreter der Studierendenschaft und des wissenschaftlichen Nachwuchses befinden, zum anderen aber gerade auch Personen, die selbst nicht an der Veranstaltung teilnehmen können. Hierzu wurde Ende 2017 unter anderem über Twitter ein Aufruf gestartet, (unter 
            
               #
               dhdp3a
            ) Fragen und Diskussionsthemen einzureichen, die dann vor Ort besprochen werden können. Dieses Vorgehen soll sicherstellen, dass 1) in der Diskussion tatsächlich jene Themen aufgegriffen werden, die von allgemeinem Interesse sind, 2) nicht bereits vielfach geführte Debatten lediglich repliziert werden, und 3) 
                ein wirklicher Dialog zwischen „Betroffenen“ und anderen Interessierten stattfinden kann, da diesmal nicht exklusiv auf professoraler Ebene diskutiert wird.
         
      
      
         
             http://www.hgw.geschichte.uni-muenchen.de/ueber_uns/faecher/fachinformatik/index.html
             Die innerdeutsche Verteilung gestaltet sich entsprechend recht übersichtlich: 
                
                  Ruprecht-Karls-Universität Heidelberg (Früheres Mittelalter und historische Grundwissenschaften) 
                  Eberhard-Karls-Universität Tübingen (Geschichtliche Landeskunde und Historische Hilfswissenschaften)
                  Otto-Friedrich-Universität Bamberg (Historische Grundwissenschaften)
                  Friedrich-Alexander-Universität Erlangen-Nürnberg (Mittelalterliche Geschichte und Historische Hilfswissenschaften)
                  Ludwig-Maximilians-Universität München (Historische Grundwissenschaften und Historische Medienkunde)
                  Universität Passau (Mittelalterliche Geschichte und historische Hilfswissenschaften)
                  Universität Regensburg (Historische Hilfswissenschaften)
                  Julius-Maximilians-Universität Würzburg (Mittelalterliche Geschichte und historische Hilfswissenschaften)
                  Christian-Albrechts-Universität zu Kiel (Mittelalterliche Geschichte und historische Hilfswissenschaften)
                  Ruhr-Universität Bochum (Historische Hilfswissenschaften)
                  Rheinische Friedrich-Wilhelms-Universität Bonn (Historische Hilfswissenschaften und Archivkunde)
                  Universität zu Köln (Historische Hilfswissenschaften)
               
            
             Das Gleiche gilt auch für die schon zahlreich verfügbaren Tools zu deren vereinfachten Be- oder Verarbeitung.
             Exemplarisch können hier – neben der noch aktuellen Ausschreibung einer (wenn auch befristeten) W2 Professur für „Historische Grundwissenschaften unter besonderer Berücksichtigung der Digital Humanities“ – auch der Zusammenschluss historisch arbeitender Wissenschaftler|inne|n zur „Arbeitsgemeinschaft Historische Grundwissenschaften“ (AHiG, 
                        https://www.ahigw.de/) genannt werden, sowie die Gründung des „Netzwerk Historische Grundwissenschaften“ (NHG, 
                        https://www.ahigw.de/nachwuchsnetzwerk/). Bei letzterem handelt es sich um einen Zusammenschluss von Nachwuchswissenschaftler|inn|en verschiedener Disziplinen und Qualifikationsstufen, die neben der Organisation einer jährlichen Konferenz mit grundwissenschaftlicher Ausrichtung, auch auf anderen Wegen versuchen, sich produktiv in die aktuellen Diskussionen einzuschalten und Entwicklungen voran zu treiben – so z.B. auch mit der Organisation dieses Panels. 
                    
         
         
            
               Bibliographie
               Arbeitsstelle Kleine Fächer, Fachstandort der Historischen Hilfswissenschaften. URL: 
                        http://www.kleinefaecher.de/historische-hilfswissenschaften/ [letzter Zugriff 24.09.2017]
                    
               
                  Hiltmann, Torsten (2015): „Hilfswissenschaften in Zeiten der Digitalisierung“, in: H-Soz-Kult, 14.12.2015. URL: 
                        www.hsozkult.de/debate/id/diskussionen-2936 [letzter Zugriff 24.09.2017]
                    
               
                  Rehbein, Malte (2015): „Digitalisierung braucht Historiker/innen, die sie beherrschen, nicht beherrscht“, in: H-Soz-Kult, 27.11.2015. URL: 
                        www.hsozkult.de/debate/id/diskussionen-2905 [letzter Zugriff 24.09.2017]
                    
               
                  Sahle, Patrick (2015): „Digital Humanities? Gibt’s doch gar nicht! in: Grenzen und Möglichkeiten der Digital Humanities (Sonderband der Zeitschrift für digitale Geisteswissenschaften 1). DOI: 
                        http://dx.doi.org/10.17175/sb001_004 [letzter Zugriff 13.01.2017]
                    
               
                  Sahle, Patrick (2016): „Zur Professoralisierung der Digital Humanities“, in: DHd-Blog, 23. März 2016. URL: 
                        http://dhd-blog.org/?p=6174 [letzter Zugriff 24.09.2017]
                    
               
                  Vogeler, Georg (2015): „Digitale Quellenkritik in der Forschungspraxis“, in: H-Soz-Kult, 28.11.2015. URL: 
                        www.hsozkult.de/debate/id/diskussionen-2893 [letzter Zugriff 24.09.2017]
                    
               
                  Schlotheuber, Eva / Bösch, Frank (2015): „Quellenkritik im digitalen Zeitalter. Die Historischen Grundwissenschaften als zentrale Kompetenz der Geschichtswissenschaft und benachbarter Fächer“, in: H-Soz-Kult, 15.11.2015. URL: 
                        www.hsozkult.de/text/id/texte-2890 [letzter Zugriff 24.09.2017]
                    
            
         
      
   



      
         Soziale Medien spielen weltweit im politischen Meinungsbildungsprozess eine immer wichtigere Rolle. Sowohl Onlineangebote von Zeitungen als auch Twitteraccounts von Organisationen oder Personen können quasi in Echtzeit über aktuelle Geschehnisse informieren.
                Die Kommentar- und Replyfunktionen bieten zudem einen digitalen Ort für den öffentlichen Austausch. Damit können auch ‚normale’ Nutzer ganz gezielt Nachrichten wie auch persönliche Kommentare oder Gerüchte in einem Ausmaß verbreiten wie es im vor-digitalen Zeitalter kaum möglich war. Das Entstehen eines vollkommen neuen digitalen Kommunikationsraumes, der sowohl Grenzen überschreitet als auch potenziell neue Grenzen schafft („Echokammern“) kann zum einen positiv im Sinne einer Demokratisierung öffentlicher Meinungsbildung gewertet werden (Mossberger et al. 2007), birgt aber auch Risiken (Mancini, 2013, Sarcinelli, 2014).
            
         Die Analyse der Rolle von Sozialen Medien im öffentlichen Meinungsbildungsprozess ist inzwischen ein aktives Forschungsfeld (z.B. Törnberg & Törnberg, 2016, Eilders, 2013). Für eine umfassende Auswertung des Datenmaterials sind jedoch mächtige Analyseverfahren notwendig (Sentimentanalyse, Netzwerkanalyse, Diskursanalyse, Bot-Erkennung etc.), die zur Zeit noch nicht in adäquatem Maß zur Verfügung stehen (s. Mohammad et al., 2015). So ist zum Beispiel die Sentimentanalyse relativ gut erforscht, beschränkt sich aber in der Regel auf das Englische sowie auf die Textsorte „Rezension“. In drei Pilotstudien haben wir untersucht, mit welchem Aufwand sich Methoden der Sentimentanalyse und Bot-Erkennung auf neue Sprachen anpassen lassen und wo mögliche Grenzen dieser Verfahren liegen. Im weiteren Projektverlauf, soll ergründet werden, inwieweit sich Prozesse der öffentlichen Meinungsbildung in sozialen Medien mit den zur Zeit zur Verfügung stehenden Verfahren nachvollziehen lassen. Die Pilotstudien untersuchen den öffentlichen Diskurs im Kontext von Wahlen und decken zwei verschiedene Sprachen (französisch, deutsch) und Textsorten (Kommentare auf Nachrichtenseiten, Tweets) ab und variieren hinsichtlich der ausgewerteten Datenmenge und Herangehensweise (gemischt qualitativ-quantitative stilistische Auswertung vs. primär quantitative Polaritätsanalyse vs. Bot-Erkennung).
         
            Pilotstudie 1: Sentimentanalyse zur Bundestagswahl
            In der ersten Studie stehen Tweets zur Bundestagswahl (BTW) im Vordergrund, für die ein Sentimenttagger entwickelt wurde, um das in den Tweets ausgedrückte Sentiment im Hinblick auf Themen, Parteien und Personen zu analysieren. Zwischen Mai und September 2017 wurden mehr als 5.4 Millionen Tweets gesammelt und nach Schlagworten und Hashtags zur BTW und zu den Parteien gefiltert. 600 Tweets wurden von 3 AnnotatorInnen manuell als `positiv', `negativ', 'neutral' oder 'irrelevant' (kein Bezug zur Bundestagswahl) klassifiziert. Um die Anforderungen eines (bisher nicht verfügbaren) Twitter-Sentimenttaggers für das Deutsche im Hinblick auf die Fragestellung zu ermitteln, wurden in einer Vorstudie 100 Tweets von fünf AnnotatorInnen auf ihr Sentiment hin untersucht. Dabei wurde deutlich, dass das Sentiment zum Teil aus dem Kontext inferiert werden muss (z.B. angehängte Bilder), was besonders die automatische Analyse erschwert. Zudem werden oft mehrere Sentiments ausgedrückt werden (s. Mohammad, 2016). Das Inter-Annotator-Agreement für alle 3 AnnotatorInnen lag bei 61% (94,5% bei Übereinstimmung von 2 AnnotatorInnen). Eine Sichtung der annotierten Daten zeigt, dass Tweets zur BTW überwiegend sentiment-behaftet sind (91%) und negatives Sentiment vorherrscht (81% neg., 10% pos, 9% neut). Für den Sentimenttagger wurden verschiedene überwachte maschinelle Lernverfahren auf den annotierten Daten trainiert und getestet (10-fache Kreuzvalidierung). Verwendet wurden dabei Unigramme sowie weitere Informationen wie das Vorkommen von Emoticons oder bestimmten Satzzeichen. Der beste Klassifikator (Naive Bayes) erreichte 71% F-Score (69% Prec., 75% Rec.). Besonders indikativ für negatives Sentiment in unserem Datenset sind die Unigramme „Schulz“ und „Merkel“, während positives Sentiment durch „!“ angezeigt wird. Hier ist sicher eine weitergehende Analyse notwendig.
         
         
            Pilotstudie 2: Französische Präsidentschaftswahl
            In einer gemischt qualitativ-quantitativen Analyse der französischen Präsidentschaftswahl wurden Kommentare unter Onlineartikeln der französischen Tageszeitung
                Le Monde
                 im Zeitraum zwischen dem ersten (23.04.2017) und dem zweiten (07.05.2017) Wahltag manuell und automatisch analysiert. Hierfür wurden zunächst themenähnliche Artikel ausgewählt, die sich mit den zwei Präsidentschaftskandidaten der zweiten Wahlrunde befassten. Für die weitere Analyse wurden sechs repräsentative Onlineartikel und die zugehörigen Userkommentare ausgewählt. Diese ausgewählten Textdateien wurden mithilfe der Topic Modelling Software MALLET
                 auf ihre Hauptthemen hin analysiert. Als Trainingsdaten für das Topic Modelling wurden ähnliche Onlineartikel anderer Zeitungen und Wahlprogramme der zwei Hauptparteien der zweiten Wahlrunde verwendet. Da es auch für das Französische keinen frei verfügbaren Sentimenttagger gibt, wurde ein regelbasiertes System entwickelt, dass auf einer Kombination von verschiedenen Sentimentlexika beruht, u.a. auch multi-linguale Ressourcen (NRC Emotion Lexicon, Mohammad & Turney, 2013), die auf der einen Seite die Datengrundlage vergrößern, auf der anderen Seite aber auch potenziell Fehler (z.B. fehlerhafte Übersetzungen, Lesartenambiguitäten) beitragen.
                
         
         
            Pilotstudie 3: Erkennung von Social Bots
            In der dritten Studie geht es um die Unterscheidung von menschlichen und künstlichen Akteuren. Um die potenziellen Auswirkungen von Social Bots auf die politische Meinungsbildung zu quantifizieren ist es notwendig, künstliche Agenten automatisiert erkennen zu können. Bisherige Verfahren (z.B. Varol et al., 2017) können simple Bot-Accounts zuverlässig erkennen, scheitern aber an fortgeschrittenen Bots, die sich nicht mehr offensichtlich von echten Menschen unterscheiden. In einem weiterentwickelten Verfahren analysieren wir speziell Accounts dieser Art. Als Datengrundlage hierfür wird der MIB-Datensatz verwendet (Cresci et al., 2017). Zur Bot-Erkennung wird mit überwachten machinellen Lernverfahren experimentiert. 
         
      
      
         
            
               Bibliographie
               
                  Cresci, Stefano / Di Pietro, Roberto / Petrocchi, Marinella / Spognardi, Angelo / Tesconi, Maurizio 
                        (2017): „The Paradigm-Shift of Social Spambots: Evidence, Theories, and Tools for the Arms Race.“ in:
                        
                     Proceedings of the 26th International Conference on World Wide Web Companion
                  
                   (WWW '17 Companion).
               
               
                  Eilders, Christiane
                         (2013): „Öffentliche Meinungsbildung in Online-Umgebungen. Zur Zentralität der normativen Perspektive in der politischen Kommunikationsforschung.“ In: Karmasin, M. et al. (Eds.):
                        Normativität in der Kommunikationswissenschaft, Wiesbaden: Springer, 329-351.
                    
               
                  Mancini, Paolo
                         (2013): "Media Fragmentation, Party System, and Democracy."
                        The International Journal of Press/Politics 18 (1): 43-60.
                    
               
                  Mohammad, Saif
                         (2016): „A Practical Guide to Sentiment Annotation: Challenges and Solutions.“ in:
                        Proceedings of the NAACL 2016 Workshop on Computational Approaches to Subjectivity, Sentiment, and Social Media (WASSA), June 2014, San Diego, California.
                    
               
                  Mohammad, Saif / Kiritchenko, Svetlana / Zhu, Xiaodan / Martinet, Joel
                         (2015): „Sentiment, Emotion, Purpose, and Style in Electoral Tweets“, in: 
                        Information Processing & Management, 51:4, 480–499.
                    
               
                  Mohammad, Saif / Turney, Peter
                        (2013): Crowdsourcing a Word-Emotion Association Lexicon, Computational Intelligence, 29 (3), 436-465, 2013.
                    
               
                  Mossberger, Karen / Tolbert, Caroline J. / McNeal, Ramona S.
                         (2007): 
                        Digital Citizenship. The Internet, Society, and Participation, Cambridge MA/London: MIT Press.
                    
               
                  Sarcinelli, Ulrich (2014): „Von der Bewirtschaftung der Aufmerksamkeit zur simulativen Demokratie?“ in:
                        Zeitschrift für Politikwissenschaft 24 (3), 329 - 339.
               
               
                  Schmid, Helmut
                   (1994): „Probabilistic Part-of-Speech Tagging Using Decision Trees.“
                         in: Proceedings of International Conference on New Methods in Language Processing, Manchester, UK. 
               
               
                  Törnberg, Anton / Törnberg, Petter
                         (2016): „Combining CDA and topic modeling: Analyzing discursive connections between Islamophobia and anti-feminism on an online forum“, in:
                         Discourse and Society  27:4, 401-422.
                    
               
                  Varol, Onur / Ferrara, Emilio / Davis, Clayton A. / Menczer, Filippo / Flammini, Alessandro
                        (2017): „Online Human-Bot Interactions: Detection, Estimation, and Characterization.“ in:
                         Proceedings of ICWSM'17
               
            
         
      
   



      
         
            Einleitung: Digitales Publizieren in den Geisteswissenschaften
            
               Mit der zunehmend selbstverständlichen Nutzung digitaler Ressourcen und der Etablierung der Digital Humanities rückt auch die Frage nach Formen des digitalen Publizierens in der Wissenschaft ins Blickfeld: Während zunehmend digitale Methoden der Erfassung, Erschließung und Analyse zur Anwendung kommen, bleiben jedoch die Publikationswege häufig noch traditionell und analog geprägt. Dabei bieten digitale Veröffentlichungsformen Potenziale für offene und innovative wissenschaftliche Erkenntnisprozesse sowie eine direktere Wissenschaftskommunikation. Die zunehmende Etablierung von (Open)-Peer-Review-Verfahren wirkt gegen das Vorurteil der vermeintlich geringeren Qualität von digitalen Publikationen; auch wissen die Wissenschaftlerinnen die freie und mobile Verfügbarkeit von digitalen Publikationen zunehmend zu schätzen.
            
            
               Die sich im Wandel befindenden medialen Bedingungen wirken direkt auf die Akteurinnen im (digitalen) Publikationsprozess ein (DHd-Arbeitsgruppe 2016). Die Rolle und das Zusammenspiel von Urheberinnen, Autorinnen, Verlag und Rezipientinnen werden daher grundlegend in Frage gestellt (Fitzpatrick 2011: 50). Gleichfalls unterliegt die wissenschaftliche Publikation selbst einem Prozess der Neudefinition: Traditionelle Formen wie Monographie oder Zeitschriftenartikel verlieren ihren Ausschließlichkeitsanspruch, da zunehmend digitale Präsentationsformen im wissenschaftlichen Diskurs als vollwertige wissenschaftliche Publikationen angesehen werden (Kohle 2017: 199). Digitale Publikationen interagieren weit mehr als ihre analogen Vorbilder mit anderen mediale Formen, sei es durch die Einbettung von multimedialen Inhalten (Maciocci 2017), Social Media und Forschungsdaten oder durch Verweise auf andere online verfügbaren Ressourcen im Sinne von Linked Open Data (W3C 2017). Die Integration von crossmedialen Inhalten ist technisch bereits möglich, es fehlen allerdings noch Anwendungskonzepte und Best Practice Beispiele. 
            
            
               Die oftmals ungefilterte Offenheit digitaler Medien wirft jedoch auch kritische Fragen der Qualitätssicherung auf, da nicht alle aus dem Kontext der gedruckten Publikation gewohnte Mechanismen greifen (Herb 2012). Dennoch sind Vorteile und Mehrwert des Digitalen evident: Digitale Texte sind leicht aufzufinden, durchsuchbar und im Idealfall schrankenlos kopierbar. Sie begünstigen damit die breite Distribution und Rezeption sowie die Nachnutzung durch digitale (z B. analytische) Verfahren. Anders als im Druck erschienene Publikationen können digitale Publikationen fortgeschrieben werden, ohne ihre Referenzierbarkeit verlieren zu müssen (durch Versionierung). Sie lassen sich mit anderen Texten verknüpfen (Hypertext) und können auf der Basis geeigneter Vokabulare bzw. Ontologien in eine maschinell auswertbare semantische Beziehung mit anderen Dokumenten und Gegenständen treten (Semantic Web). Die bei digitalen Dokumenten favorisierte Trennung von Struktur- und Layoutschicht ermöglicht es, Texte nicht mehr einem starren Präsentationsregime zu unterwerfen, sondern nach Wünschen der BenutzerInnen neue Ansichten oder überhaupt Präsentationsformen jenseits traditioneller Textbegriffe zu generieren. Die kollaborative Text- und Datenpublikation wird im digitalen Raum begünstigt, zieht aber auch Probleme bezüglich der Autorinnenschaft und der Differenzierung der Rollen im digitalen Publikationsprozess nach sich (SoSciSo Redaktion: 2017). Abschließend gilt es, die Schlüsselfunktion von Open Access (OA) und freien Lizenzmodellen (z.B. nach Creative Commons) in digitalen Publikationsprozessen zu betonen: Sie schaffen die Voraussetzungen für ungehindertes Forschen und werden damit zu zentralen Bedingungen wissenschaftlichen Publizierens.
            
         
         
            
               Veranstaltungsformat Barcamp
            
            
               Mit dem Format eines halbtägigen Barcamps möchte die DHd-AG »Digitales Publizieren« der interessierten Community die Möglichkeit bieten, die soeben skizzierten Themen und Fragen, aber auch andere Aspekte rund um das digitale Publizieren gemeinsam zu diskutieren und sich dazu auszutauschen (Dogunke 2018). Das Format bedingt, dass das Programm maßgeblich von den Teilnehmerinnen gestaltet wird und sowohl dynamisch als auch interaktiv entwickelt werden kann. Das Barcamp möchte Expertinnen und interessierte Wissenschaftlerinnen aus unterschiedlichen Disziplinen zusammenbringen und wird ausreichend Raum bieten, sich in ausgewählte Bereiche der Thematik zu vertiefen, aber auch grundlegende Fragen zu thematisieren. Ziel ist es, gleichermaßen die Ansprüche einer Informationsveranstaltung mit impulsgebenden Statements zu kombinieren. 
            
            
               Es bestehen zwei Möglichkeiten, Themen für die Veranstaltung zu benennen: Zum einen wird im Vorfeld der Tagung DHd2019 eine Umfrage über den DHd-Blog, Twitter und Mailinglisten stattfinden. Hier entscheidet die Quantität der Nennung einzelner Themen über ihre Annahme. Ähnlich gelagerte Themen werden dabei zusammengefasst bzw. gruppiert. Am Barcamp Interessierte haben dabei auch die Möglichkeit, eine Gestaltungsform für den genannten Vorschlag zu nennen und ihre Rolle zu definieren (s.u.). Spontan können zum anderen aber auch Themen direkt innerhalb des Workshops platziert werden. Die endgültige Tagungsordnung für das Barcamp wird gemeinsam mit dem Plenum zu Beginn des Workshops festgelegt. Die DHd-AG »Digitales Publizieren« möchte die Ergebnisses des Barcamps erstens zur Überarbeitung des Arbeitspapieres »Digitales Publizieren« nutzen und damit einen Beitrag zur Klärung des aktuellen Selbstverständnisses in der Gemeinschaft leisten. Zweitens soll die Veranstaltung der weiteren Vernetzung der Interessierten innerhalb der Community dienen. Drittens soll aber auch das gewählte Format auf seine Eignung geprüft werden, die Kommunikation zwischen der AG und der Community aktiver zu gestalten, woraus sich bei positivem Befund auch weitere Veranstaltungen ergeben könnten.
            
         
         
            Potentielle Themen und Fragen
            
               Um eine Vorstellung von potentiellen Themen und der inhaltlichen Gestaltung der Veranstaltung zu bekommen, seien im Folgenden einige Aspekte und zentrale Fragen zum digitalen Publizieren genannt, welche die Verfasserinnen der Einreichung auf der Grundlage eigener Erfahrung und der aktuellen Forschung im Rahmen der AG Digitales Publizieren identifiziert haben:
            
            
               Aktuelle und zukünftige Publikationsformate: Welche Rolle wird PDF als Publikationsformat in Zukunft haben? Werden Beiträge direkt in XML verfasst werden können?
               
                  Data Publications als Publikationsformat
                  : Wie und in welcher Form können (Forschungs)daten publiziert werden? Welche Formate existieren bereits und gibt es Best Practice Beispiele? Wie können Datenpublikationen als wissenschaftliches Publikationsformat etabliert werden?
               
               
                  (darauf aufbauend): 
                  Was zählt eigentlich als digitale Publikation und welche Abgrenzungen zu anderen Publikationsformen sind notwendig?
                   Welche technischen und inhaltlichen Kriterien müssen beispielsweise Blogbeiträge erfüllen, um als wissenschaftliche Publikation zu gelten? 
               
               
                  Kollaboratives Schreiben
                  : Wie kann die Rolle der beteiligten Personen kenntlich gemacht werden und welche Rolle gibt es außer der der Autorinnen bei einer Publikation?
               
               
                  Infrastrukturen für digitale Publikationen
                  : Welche Repositorien und Publikationsumgebungen existieren und sind für Forscherinnen im deutschsprachigen Raum zugänglich? Welche Standards haben sich etabliert?
               
               
                  Wie kann die Qualität von digitalen Publikationen gemessen werden?
                   Welche Bedeutung könnte der Impactfaktor in Zukunft haben? Wie kann die Zitationshäufigkeit von digitalen Publikationsformen gesteigert werden?
               
               
                  Welche Bedeutung kommt dem traditionellen Intermediären im digitalen Publikationszyklus zu
                  ? Sind Bibliotheken die neuen Verlage? Ist das hybride Publizieren nur eine Übergangserscheinung oder ein langfristiges Erfolgsmodell?
               
               
                  Wie ist der aktuelle Stand bei den Lizenzen und Rechten im Kontext vom digitalen Publizieren?
                   Wie stark hat sich Open Access wirklich durchgesetzt?
               
               Bedarf es genuiner Gutachterkulturen für digitale Publikationen?
               Wie gestalten sich digitale Publikationsworkflows?
               Hat beim digitalen Publizieren die wissenschaftliche Kommunikation einen direkteren Einfluss auf die Publikation?
               
                  Warum hat sich bisher trotz der stetig wachsenden Bedeutung von Forschungsdaten das Modell der
                  enhanced publication
                  noch nicht durchgesetzt und welche Chancen bestehen für dieses Format (Degwitz 2015: 52)?
               
               
                  Welche Rolle kommt im Sinne des Titels der
                   Tagung cross- bzw. intermedialen Inhalten 
                  bei digitalen Publikationen zu
                  ?
               
            
         
         
            Durchführung
            
               Wer ein Thema vorschlägt, hat gleichzeitig die Möglichkeit, auch ein Durchführungsformat zu wählen. Die unterschiedlichen Formate werden mit der Umfrage zusammen vorgeschlagen. Der Grund für diese flexible und Teilnehmerinnen-gesteuerte Auswahl des Barcamps ist es, dass einige der oben genannten Themen sich eher für ein Expertengespräch eignen, während andere eher in einer gemeinsamen Diskussion thematisiert werden könnten oder Gegenstand eines Impulsreferats sein könnten. Es soll daher weder bei den Inhalten noch bei den Formaten fest Vorgaben geben.
            
            
               Die ein Thema vorschlagenden Personen können selber angeben, ob sie a) sich für das Thema grundsätzlich interessieren oder sich b) als ExpertIn für das Thema im Rahmen des Workshops zur Verfügung stellen. Zusätzlich werden die Organisatorinnen im Vorfeld des Workshops Expertinnen zu den einzelnen Themen einladen, bzw. Themen als gemeinsame Diskussionen mit dem Plenum planen und vorbereiten. Die Moderation und Durchführung der Veranstaltung wird von Mitgliedern der AG bedient.  
            
            Folgende Formate von circa jeweils 30 Minuten Dauer sind denkbar:
            
               
                  Expertinnenformat: Eine Expertin bzw. ein Experte hält ein impulsgebendes Referat, danach findet eine moderierte Diskussion statt.
               
               Thementische (abhängig vom Raum): Es gibt unterschiedliche Thementische, an denen Expertinnen Rede und Antwort stehen.
               Diskussionen: Mehrere Expertinnen diskutieren zu einem Thema, danach folgt eine Diskussion mit dem Plenum.
               Gruppenformat: Kleinere Gruppen diskutieren gemeinsam ausgewählte Themen und präsentieren die Ergebnisse danach dem Plenum.
            
            
               Vor allem der letzte Punkt scheint für das Tagungsformat gut geeignet zu sein, da dadurch alle Beteiligten involviert werden. Für die Durchführung dieser unterschiedlichen Formate wäre ein gut unterteilbarer Raum ebenso sinnvoll wie der Einsatz von Moderationsmaterialen (Flipcharts etc.). Eine laufende Dokumentation der Ergebnisse des Barcamps wird während des Workshops über ein Etherpad erfolgen. Des Weiteren sollen zentrale Ergebnisse in die neue Version des Workingspapers “Digitales Publizieren” integriert werden.Abhängig vom Verlauf des Barcamps können weitere Formate, wie z. B. ein Blogbeitrag, möglich sein.
            
         
         
            Organisatorisches
            Das Barcamp wird von der DHd-AG Digitales Publizieren veranstaltet. Die Planung und Durchführung wird organisiert von:
            Katrin Neumann, (Max-Weber-Stiftung), 
            
               neumann@maxweberstiftung.de, Forschungsinteressen:
            Digitales Publizieren, Publikationsplattformen, Wissenschaftliches Bloggen
            Melanie Seltmann, (Universität Wien), 
            
               melanie.seltmann@univie.ac.at, Forschungsinteressen:
            Digitales Publizieren, Natural Language Processing, Citizen Science
            Walter Scholger, (Universität Graz), 
            
               walter.scholger@uni-graz.at, Forschungsinteressen:
            Digitales Publizieren, Digitale Editionen, Open Access und Lizenzen
            Timo Steyer, (Forschungsverbund Marbach Weimar Wolfenbüttel/Herzog August Bibliothek Wolfenbüttel),
            
               steyer@hab.de, Forschungsinteressen:
            Digitales Publizieren, Digitale Editionen, Metadaten und Datemmodellierung
            
               Die Teilnehmerzahl ist auf 40 Personen begrenzt. Der Workshop sollte eine Dauer von einem halben Tag haben. Benötigt werden ein Beamer, Moderationsmaterial und eine Raumgröße, welche die Bildung mehrere Arbeitsgruppen ermöglicht.
            
         
      
      
         
            
               Bibliographie
               
                  Degkwitz, Andreas (2015): “Enhanced Publications Exploit the Potential of Digital Media”, in: Evolving Genres of ETDs for Knowledge Discovery. Proceedings of ETD 2015 18th International Symposium on Electronic Theses and Dissertations 51-59.
                    
               
                  DHd-Arbeitsgruppe (2016): "Digitales Publizieren", in: DHd-Arbeitsgruppe (eds.): Working Paper "Digitales Publizieren"
                        http://diglib.hab.de/ejournals/ed000008/startx.htm [letzter Zugriff: 21.09.2018]
                    
               
                  Dogunke, Swantje / Steyer, Timo / Mayer, Corinna (2018): "Barcamp Data and Demons: von Bestands- und Forschungsdaten zu Services. Treffen sich ein Bibliothekar, eine Archäologin, ein Informatiker, …", in: LIBREAS. Library Ideas 33
                        https://libreas.eu/ausgabe33/dogunke/ [letzter Zugriff: 21.09.2018].
                    
               
                  Fitzpatrick, Kathleen (2011): Planned Obsolescence Publishing, Technology, and the Future of the Academy. New York: New York Univ. Press.
                    
               
                  Herb, Ulrich (2012): "Offenheit und wissenschaftliche Werke: Open Access, Open Review, Open Metrics, Open Science & Open Knowledge”, in: Herb, Ulrich (eds): Open Initiatives: Offenheit in der digitalen Welt und Wissenschaft. Saarbrücken Universaar 11-44.
                    
               
                  Kohle, Hubertus (2017): “Digitales Publizieren” in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): Digital Humanities. Eine Einführung. Stuttgart: Metzler Verlag 199-205.
                    
               
                  Maciocci, Giuliano (2017): "Designing Progressive Enhancement Into The Academic Manuscript: Considering a design strategy to accommodate interactive research articles", in: Blogpost auf eLife Sciences
                        https://elifesciences.org/labs/e5737fd5/designing-progressive-enhancement-into-the-academic-manuscript [letzter Zugriff: 21.09.2018].
                    
               
                  Penfold, Naomi (2017): "Reproducible Document Stack – supporting the next-generation research article", in: Blogpost auf eLife Sciences
                        https://elifesciences.org/labs/7dbeb390/reproducible-document-stack-supporting-the-next-generation-research-article [letzter Zugriff: 21.09.2018].
                    
               
                  SoSciSo Redaktion (2017): "Kollaboratives Schreiben mit webbasierten Programmen", in: Blogpost auf Social Science Software 
                        https://www.sosciso.de/de/2017/kollaboratives-schreiben/ [letzter Zugriff: 21.09.2018].>
                    
               
                  W3C (2017): "W3C Data Activity. Building the Web of Data" 
                  https://www.w3.org/2013/data/> [letzter Zugriff: 21.09.2018].
                    
            
         
      
   



      
         Computer und Internet haben die Art und Weise, wie Forscher kommunizieren und zusammenarbeiten, grundlegend verändert. Ab Anfang der 90er Jahre konnten Wissenschaftlerinnen und Wissenschaftler über verschiedene Orte und Zeitzonen hinweg kollaborativ an Text, Bild, Audio, Video und Code arbeiten. Während E-Mail, Newsgroups und Online-Chats eine many-to-many-Kommunikation im virtuellen Raum ermöglichten, kamen die wichtigsten aktuellen Entwicklungen in der wissenschaftlichen Online-Kommunikation durch soziale Medien: mit Microblogging, Blogs, Wikis und Social Network Sites (SNS) wie Facebook, Academia.edu, ResearchGate und anderen. Durch sie wurden die Hindernisse für die Veröffentlichung und Kommunikation im Internet deutlich reduziert. Produktionsprozesse, die bisher professionelles Wissen, Ausrüstung und Kapital erforderten, können nun von einfachen Personen mit Computer- und Internetzugang durchgeführt werden. In der Folge wurde das Ökosystem der wissenschaftlichen Kommunikation breiter, schneller, interaktiver, dynamischer, multimodaler und zunehmend vernetzter (König 2015).
         Als öffentlich geführte wissenschaftliche Notizbücher eignen sich insbesondere Wissenschaftsblogs zur selbstkritischen Reflektion des eigenen Forschungsprozesses wie auch zur Dokumentation desselben. Nicht nur Nachwuchswissenschaftlerinnen und Nachwuchswissenschaftlern bietet Bloggen die Möglichkeit, bereits in einem frühen Stadium auf ihr Projekt aufmerksam zu machen, mit erfahrenen Wissenschaftlerinnen und Wissenschaftlern in Austausch zu treten, sich zu vernetzen, Schreiben zu üben und Gedanken im Schreibprozess zu ordnen. Wissenschaftsblogs haben ein hohes Potential für die schnelle Verbreitung und Diskussion aktueller Forschungsinhalte und nutzen die Möglichkeiten des Web 2.0 für eine direkte und interaktive Publikation, bei der multimediale Inhalte 
                wie Bilder, Grafiken, Animationen und Verlinkungen ohne Mehrkosten eingebunden werden können. Wissenschaftsblogs werden zwar zumeist für die eigene Fachcommunity geschrieben, sie sind jedoch offen einsehbar und werden ebenso von Journalisten und von der breiten Öffentlichkeit wahrgenommen.
            
         Wissenschaftsblogs bieten Einblicke in die Werkstatt von Forschenden und zeigen Forschung im Entstehen (Mounier 2013). Gerade in den Digital Humanities sind Blogs und Twitter die wichtigsten Medien für die Diskussion neuer Forschungsansätze und Methoden (Ullyot 2012). Blogs dokumentieren den Forschungsprozess und damit die Phase vor der abschließenden Projektveröffentlichung. Damit ersetzen sie bisherige Praktiken und Formate der Kommunikation und Publikation zumeist nicht – auch wenn sie es theoretisch könnten –, sondern ergänzen diese und stellen in ihrer Ausprägung etwas Neues dar: ein eigenes Format, das Kennzeichen aus der analogen (mündlich wie schriftlichen) und der digitalen Wissenschaftskommunikation als „missing link“ mischt und um neue Merkmale ergänzt. Wenn das Medium die Botschaft ist (Marshall McLuhan), dann zeigen bloggende Forscherinnen und Forscher, wie sie sich Wissenschaft vorstellen: offen, vernetzt, horizontal, direkt, schnell, vielseitig, multimedial… und mit der akzeptierten Möglichkeit, sich zu irren (König 2015). Forschende schreiben in Blogs über einzelne Aspekte ihres Themas, über Publikationen, die sie gelesen haben, über Vorträge und Veranstaltungen, die sie besucht oder über Begegnungen, die sie inspiriert haben. Blogbeiträge handeln von einem konkreten Ereignis oder Gegenstand oder entwickeln theoretische und methodische Überlegungen. Zumeist zeigt ein Wissenschaftsblog die subjektive Lebenswelt der Forschenden und macht somit ganz generell die Subjektivität der Wissenschaft und des wissenschaftlichen Tuns deutlich. 
         Mit de.hypotheses.org wurde Anfang 2012 eine Plattform für geistes- und sozialwissenschaftliche Blogs geschaffen, in deren Umfeld seither eine stetig wachsende deutschsprachige Community als Teil eines europäischen Netzwerks entstanden ist. Mittlerweile sind dort über 500 deutschsprachige Blogs aus allen geisteswissenschaftlichen Disziplinen vereint. Die Blogplattform trägt zur Sichtbarkeit und zur Vernetzung der Bloggenden bei und ist eine zentrale Anlaufstelle, bei der die Blogs langzeitarchiviert werden, eine ISSN verliehen bekommen und die Blogbeiträge mit Permalinks ausgestattet sind. Für die Startseite des Portals werden von einer Redaktion und vom Community Management die aktuell besten Beiträge ausgewählt und kuratiert, die darüber eine erhöhte Sichtbarkeit erhalten. 
         Die bei de.hypotheses vorhandenen unterschiedlichen Blogtypen belegen die große Vielfalt der geisteswissenschaftlichen Blogosphäre. Es gibt Blogs von Forschergruppen und zu Forschungsprojekten, thematische Gemeinschaftsblogs, Blogs zu Quellen und Methoden, Blogs von Instituten und wissenschaftlichen Einrichtungen wie Archive und Bibliotheken, Seminar- und Tagungsblogs, Blogs, die eine Zeitschrift oder eine Publikation begleiten, Blogs für Lehre und Didaktik, Fotoblogs, Blogs zu einer wissenschaftlichen Debatte etc. (König 2013).
         Der auf einen halben Tag angelegte Workshop knüpft direkt an den medientheoretischen und –praktischen Teil des Tagungsthemas an und richtet sich an DH-Forschende, die bisher noch nicht bloggen und ein eigenes Wissenschaftsblog anlegen möchten – ob als Einzel- oder als Gemeinschaftsblog, ob begleitend zur Lehre oder zu einem Forschungsprojekt – und dafür ein Konzept entwickeln und grundlegende inhaltliche und technische Überlegungen anstellen und praktisch einüben möchten. 
         Im Rahmen des Workshops wird zum einen die theoretische und konzeptionelle Seite des wissenschaftlichen Bloggens als eigenes multimediales Medium besprochen, zum anderen ein praktischer Teil angeboten. Zunächst werden einleitend verschiedene aktuelle Praktiken des Wissenschaftsbloggens, der besondere Schreibstil und die Interaktion mit der Leserschaft thematisiert und Elemente für die Strategiebildung für ein eigenes Wissenschaftsblog erläutert (darunter: was bloggen? wie bloggen? wie viel Zeit investieren? für welches Publikum? alleine oder kollaborativ bloggen? wie Themen für das Wissenschaftsblog finden? (Scherz 2013). Es werden best practice Beispiele aus verschiedenen geisteswissenschaftlichen Disziplinen und aus den DH vorgestellt. Gegenstand der Diskussion sind darüber hinaus rechtliche Fragen (vom Einbinden fremder Inhalte wie Bilder und Videos und dem Lizenzieren eigener Inhalte, über die Bestimmungen der DSGVO bis hin zum „Eigenplagiat“ bei Promovierenden usw.) sowie die Frage nach dem „return of investment“ des Bloggens, das durchaus zeitintensiv sein kann und aufgrund der zumeist mangelnden offiziellen Anerkennung überlegt erfolgen sollte. Thematisiert wird außerdem der Umgang mit Kommentaren im Blog sowie die Frage, was Promovierende über ihre Dissertationen bloggen können und was nicht. 
         In einem Hands-on-Teil – der etwa drei Viertel der Zeit des Workshops einnehmen wird – werden anschließend Schritt für Schritt die einzelnen Aspekte der Blogpraxis vorgestellt und vorgeführt. Die Teilnehmerinnen und Teilnehmer vollziehen die einzelnen Schritte an eigens eingerichteten Wordpress-Schulungsblogs nach, von der Gestaltung und Einrichtung des Blogs, der Formulierung einer guten Überschrift, einer sinnvollen Navigation und Kategorienbildung bis hin zum Einbetten von Videos, und wenden damit das Gelernte sofort an. Sie lernen darüber die Grundlagen moderner CMS-Systeme kennen. Während des Workshops werden parallel zum praktischen Teil Tipps gegeben für die Anfangsphase eines wissenschaftlichen Blogs und für Themen wie Suchmaschinenoptimierung, Menüführung und graphische Gestaltung. 
         Inhalte und Übungen des Praxisteils sind: Erstellen einer Menüleiste, öffentliche Autorennamen einstellen und Profil ausfüllen, Rechteverwaltung bei mehreren Nutzerinnen und Nutzern, Titel und Untertitel ändern, Design-Theme auswählen (welche sind für welche Form des Bloggens geeignet?), Bild in die Kopfzeile einfügen, Nennung des Urhebers und Lizenz, eigenen Artikel erstellen, Überschrift auswählen, Zitat einfügen, Fußnoten, Verlinkung einfügen, Weiterlesen-Button, Bildrechte, Bilder und Lizenzen einfügen (Grundlagen CC-Lizenzen), Kategorien, Schlagwörter zuweisen, Seite anlegen, Menü anlegen, Meta, Text, Bild und Link, RSS-Feed, Verknüpfung zu Twitter, Videos einbinden, Statistiken lesen, Reichweite vergrößern, rechtliche Bestimmungen der DSGVO, Umgang mit Kommentaren.
         Die Anzahl der Teilnehmenden ist auf 25 begrenzt. Der Workshopraum sollte über ein W-Lan und einen Beamer verfügen. Eine weitere technische Ausstattung wird nicht benötigt. Besondere technische Vorkenntnisse sind für die Teilnahme nicht erforderlich. Ein eigenes Laptop oder ein anderes Endgerät muss selbst mitgebracht werden. 
         Workshopleiterinnen: Dr. Mareike König: Sie ist Projektleiterin der deutschsprachigen Plattform für geisteswissenschaftliche Blogs de.hypotheses und leitet dort die Redaktion. Ihre Forschungsinteressen beziehen sich auf Wissenschaftskommunikation im Web 2.0 und hier speziell auf das Wissenschaftsbloggen als neue Form des wissenschaftlichen Schreibens als Herausforderung für unsere Wissenschaftskultur. 
         Kontakt: Dr. Mareike König, DHIP, 8, rue du Parc Royal, 75003 Paris, mkoenig@dhi-paris.fr
         Ulla Menke: Sie ist Community Managerin der Blogplattform de.hypotheses seit 2016 und arbeitet in der Max Weber Stiftung. Als Community Managerin kümmert sie sich um die rund 350 Wissenschaftsblogs, die es auf der Plattform de.hypotheses derzeit gibt und steht den Bloggenden seit 2016 mit Tipps und Hilfe bei Fragen rund um Technik, SEO, Blognavigation und Bloggestaltung zur Seite. 
         Kontakt: Ulla Menke, Max Weber Stiftung, Rheinallee 8, Bad Godesberg, menke@maxweberstiftung.de
      
      
         
            
               Bibliographie
               
                  König, Mareike (2013):
                  Die Entdeckung der Vielfalt: Geschichtsblogs auf der internationalen Plattform hypotheses.org,
                        in: 
                        Haber, Peter / Pfanzelter, Eva (eds.): 
                        Historyblogosphere. Bloggen in den Geschichtswissenschaften,
                        München: Oldenbourg 181–197.
                    
               
                  König, Mareike (2015):
                  Herausforderung für unsere Wissenschaftskultur: Weblogs in den Geisteswissenschaften
                        in: 
                        Schmale, Wolfgang (ed.):
                  Digital Humanities. Praktiken der Digitalisierung, der Dissemination und der Selbstreflexivität,
                        Stuttgart: Steiner 57-74.
                    
               
                  Scherz, Sabine (2013a):
                  Warum sollte ich als Wissenschaftler/in bloggen? 
                        in Redaktionsblog, 21.5.2013, 
                        https://redaktionsblog.hypotheses.org/1209.
                    
               
                  Scherz, Sabine (2013b):
                  Mein erster wissenschaftlicher Blogartikel – was schreibe ich bloß?
                        in Redaktionsblog, 24.5.2013, 
                        https://redaktionsblog.hypotheses.org/1214. 
                    
               
                  Scherz, Sabine (2013c):
                  Wie finde ich Themen für mein Wissenschaftsblog?
                        in Redaktionsblog, 28.5.2013, 
                        https://redaktionsblog.hypotheses.org/1217. 
                    
               
                  Scherz, Sabine (2013d):
                  Texte für das Wissenschaftsblog schreiben, wie?
                        in Redaktionsblog, 5.6.2013, 
                        https://redaktionsblog.hypotheses.org/1220. 
                    
               
                  Mounier, Pierre (2013):
                  Die Werkstatt öffnen: Geschichtsschreibung in Blogs und sozialen Medien,
                        in:
                        Haber, Peter / Pfanzelter, Eva (eds.):
                  Historyblogosphere. Bloggen in den Geschichtswissenschaften,
                        München: Oldenbourg 51-59.
                    
               
                  Ullyot, Michael (2012):
                  On Blogging in the Digital Humanities,
                        in: Ullyot, http://ullyot.ucalgaryblogs.ca/2012/02/24/on-blogging-in-the-digital-humanities/.
                    
            
         
      
   



      
         Die automatische Analyse von Tweets mit politischem Inhalt kann Sozial- und Politikwissenschaftlern Aufschluss über die Prozesse politischer Meinungsbildung geben. Beiträge in den sozialen Medien spiegeln oft Tendenzen in der Zufriedenheit mit politischen Parteien wider und helfen umstrittene oder viel diskutierte Themen zu identifizieren. Auch das Kommunikationsverhalten verschiedener Gruppen lässt sich aus ihrer Interaktion bei Twitter analysieren (z.B. unterschiedliche Dominanz von Echokammereffekten (Colleoni et al., 2014) oder Verbreitung von Gerüchten und Fake News (Ma et al., 2018)).
         Eine wichtige Rolle spielt dabei die Sentimentanalyse, die es erlaubt zu identifzieren, ob ein Akteur Zustimmung oder Ablehnung zu einem bestimmten Inhalt signalisiert. Für Textdaten lässt sich das Sentiment meist recht gut bestimmen. Tweets sind aufgrund ihrer Kürze jedoch zum einen oft schwer recht kryptisch, zum andern enthalten sie häufig weitere Materialien, insbesondere Bilder, die nennenswert zur Aussage beitragen. Z.B. ist der Text „und wieder ein neuer Morgen“ neutral formuliert, gewinnt aber eine negative Bedeutung, wenn er um eine Bild bereichert wird, das eine lange Autoschlange zeigt. Ebenso kann es sein, dass ein relativ neutrales Bild lediglich über den Text ein positives oder negatives Sentiment zugewiesen bekommt.
         Die meisten bisher existierenden Sentimentanalyseverfahren beschränken sich auf die Verarbeitung entweder von Text- oder von Bilddaten. Modelle, die beide Modalitäten
         berücksichtigen, sind noch vergleichsweise selten, können jedoch eine signifikant höhere Genauigkeit bei der Sentiment-Vorhersage erreichen als solche, die dies nicht tun (You et al. 2016). Fast alle multimodalen Verfahren nutzen eine Deep-Learning-Architektur. Solche Verfahren sind herkömmlichen Lernverfahren zwar oft überlegen, sie sind aber aufgrund der Vielzahl der möglichen Architekturen auch relativ schwer zu optimieren. Das Ziel dieser Arbeit ist es, verschiedene multimodale Sentimentanalyseverfahren und -architekturen systematisch zu vergleichen und auf ihre Vor- und Nachteile hin zu untersuchen.
         Das grundsätzliche Schema der Modelle orientiert sich am "Latent Multimodal Mixing" (Bruni et al. 2014); hierbei werden zunächst Text- und Bild-Features extrahiert, als Vektoren kodiert
          und anschließend in einem dritten Schritt auf einen gemeinsamen (multimodalen) Vektorraum abgebildet (Fusion). Aus diesen Vektoren kann dann mit Methoden des maschinellen Lernens das Sentiment berechnet werden. Innerhalb dieses Schemas können beliebige und auch neuartige Kombinationen verschiedener Methoden zur Feature-Extraktion und Fusion verwendet werden. Hierfür gibt es unter anderem folgende Möglichkeiten:
         
            Texte können mit dem Doc2Vec-Verfahren auf einen latenten Vektorraum abgebildet werden. Dieses Verfahren erzielte in der Vergangenheit bereits gute Ergebnisse bei der Sentimentanalyse. (Le et al. 2014)
            Basierend auf einem existierenden Word-Embedding-Modell (z.B. GloVe (Pennington et al. 2014)) können die Word-Embeddings aller Wörter eines Textes auf verschiedene Arten zu einem Text-Embedding aggregiert werden (z.B. gewichteter Mittelwert, elementweises Minimum/Maximum). (De Boom et al. 2016)
            Für die Extraktion visueller Features können bereits existierende Deep Learning-Modelle zur Bild-Klassifikation in leicht modifizierter Form wiederverwendet werden. (Campos et al. 2017)
            Aus dem Farbhistogramm eines Bildes können können statistische Features erster Ordnung berechnet werden.
            Der Fusionsschritt besteht aus einer einfachen Verkettung der Text- und Bild-Vektoren. Zusätzlich kann auch eine affine Projektion auf einen latenten multimodalen Vektorraum gelernt werden. (Chen et al. 2017)
         
         Die Datengrundlage für das Training der Modelle bilden manuell annotierte multimodale Tweets u.a. aus dem Photo Tweet Sentiment Benchmark (Borth et al. 2013), sowie das Columbia MVSO Image Sentiment Dataset (Dalmia et al. 2016). Aufgrund der unterschiedlichen Größe der Datensätze wird ein Transfer Learning-Ansatz verfolgt: Die Modelle werden zunächst auf den MVSO-Daten vortrainiert und anschließend mithilfe der Twitter-Daten feinadjustiert.
         Erste Testergebnisse bestätigen, dass Modelle, die Text- und Bild-Features fusionieren, eine höhere Genauigkeit erreichen können als unimodale Modelle. Allerdings haben die bisher getesteten Modelle derzeit noch Schwierigkeiten damit, negatives Sentiment korrekt zu klassifizieren.
      
      
         
            
               Bibliographie
               
                  You, Quanzeng / Luo, Jiebu / Jin, Hailin / Yang, Jianchao (2016):
                  Cross-modality Consistent Regression for Joint Visual-Textual Sentiment Analysis of Social Multimedia, 
                        in: The Ninth International Conference on Web Search and Data Mining, February 2016, San Francisco, CA, USA.
                    
               
                  Bruni, Elia / Tran, Nam Khanh / Baroni, Marco (2014):
                  Multimodal distributional semantics, 
                        in: Journal of Artificial Intelligence Research 49, 1 (Januar 2014), 1-47
                    
               
                  Colleoni, Elanor/ Rozza, Alessandro / Arvidsson, Adam (2014):
                  Echo Chamber or Public Sphere? Predicting Political Orientation and Measuring Political Homophily in Twitter Using Big Data, 
                        in: Journal of Communicatio 64 (2014) 317–332
                    
               
                  De Boom, Cedric / Van Canneyt, Steven / Demeester, Thomas / Dhoedt, Bart (2016):
                  Representation learning for very short texts using weighted word embedding aggregation, 
                        in: Pattern Recognition Letters 80, C (September 2016), 150-156.
                    
               
                  Campos, Victor / Jou, Brendan / Giró-i-Nieto, Xavier (2017):
                  From Pixels to Sentiment: Fine-uning CNNs for Visual Sentiment Prediction, 
                        in: Image and Vision Computing 65 (September 2017), 15-22
                    
               
                  Chen, Xingyue / Wang, Yunhong / Liu, Qingjie (2017):
                  Visual and Textual Sentiment Analysis Using Deep Fusion Convolutional Neural Networks, 
                        in: IEEE International Conference on Image Processing (ICIP), Beijing, China
                    
               
                  Dalmia, Vaidehi / Liu, Hongyi / Chang, Shih-Fu (2016):
                  Columbia MVSO Image Sentiment Dataset, 
                        in: CoRR, abs/1611.04455
                    
               
                  Borth, Damian / Ji, Rongrong / Chen, Tao / Breuel, Thomas / Chang, Shih-Fu (2013):
                  Large-scale Visual Sentiment Ontology and Detectors Using Adjective Noun Pairs, 
                        in: The 21st ACM International Conference on Multimedia, October 2013, Barcelona, Spain
                    
               
                  Le, Quoc / Mikolov, Tomas (2014):
                  Distributed Representations of Sentences and Documents, 
                        in: The 31st International Conference on Machine Learning, June 2014, Beijing, China
                    
               
                  Ma, Jing / Gao, Wei / Wong, Kam-Fai (2018)
                  Rumor Detection on Twitter with Tree-Structured Recursive Neural Networks, 
                        in: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 1980–1989, Melbourne, Australia, July 15 - 20, 2018
                    
               
                  Pennington, Jeffrey / Socher, Richard / Manning, Christopher (2014):
                  GloVe: Global Vectors for Word Representation, 
                        in: Empirical Methods in Natural Language Processing (EMNLP), 1532-1543
                    
            
         
      
   



      
         Die einfache Zugänglichkeit von Wissenschaftsblogs ab der Jahrtausendwende ermöglicht es Forschenden, selbst zu entscheiden wann, wo und was sie veröffentlichen wollen. Diese selbstbestimmte Übernahme eines wissenschaftlichen Publikationsraums ist ein spektakulärer Schritt, ähnlich wie die Erfindung von "Essays" durch Montaigne im 16. Jahrhundert oder die Entstehung der "Gelehrtenrepublik" ab der Mitte des 17. Jahrhunderts (König 2015: 58). Wissenschaftliche Blogs sind Orte, in denen aus laufenden Forschungsprojekten kommuniziert und mit der Fachcommunity diskutiert werden kann. Sie ermöglichen Einblicke in das Labor oder die Werkstatt der Forschenden und zeigen damit "Wissenschaft im Entstehen" (Mounier 2013). Der wissenschaftliche Austausch über Blogartikel, Kommentare und Links ist interaktiv, schnell und direkt, in einer einzigartigen Weise, die in anderen Publikationsformaten wie Mailinglisten oder Zeitschriften nicht möglich ist oder nicht praktiziert wird. Blogs können als das fehlende Bindeglied zwischen mündlicher Kommunikation auf Konferenzen oder in Universitätsseminaren und schriftlicher Kommunikation in traditionellen Artikeln oder Rezensionen angesehen werden. Sie ermöglichen es Forschenden, eine direkte Verbindung zugleich zu ihren Peers und zu ihren Studierenden herzustellen und darüber hinaus mit Journalisten und der interessierten Öffentlichkeit in Kontakt zu treten. 
         Seit 2012 sorgt das Blogportal für die Geistes- und Sozialwissenschaften de.hypotheses.org für eine florierende Blogpraxis im deutschsprachigen Raum. Das Portal ist Teil der europäischen Plattform hypotheses und bietet als zentraler Einstiegsort kostenlose und werbefreie Blogs an für Forschende, die technische Updates, Hosting und Sicherheitsfragen nicht selbst übernehmen können oder wollen und Teil einer Community sein möchten. Community Management und Redaktion bewerben die besten aktuellen Beiträge in den sozialen Medien und auf der Startseite der Plattform. Sie bieten außerdem technischen und graphischen Support und beantragen bei den Nationalbibliotheken die Zuteilung einer ISSN für die Blogs. Die Blogbeiträge sind mit Permalinks versehen und die Inhalte der Plattform werden von BnF und DNB archiviert. Derzeit sind auf der deutschsprachigen Seite rund 350 Wissenschaftsblogs vereint.
         Obwohl es sich um ein relativ neues Phänomen handelt, ist die Forschung zur wissenschaftlichen Nutzung von sozialen Medien in den letzten Jahren stark gewachsen (für einen umfassenden Überblick siehe Sugimoto et al. 2017). Die empirische Forschung zur Nutzung von sozialen Medien erfolgt durch Fragebögen, qualitative Interviews und teilnehmende Beobachtungsstudien. Diese Studien untersuchen Praktiken von innen heraus und fragen Forschende nach ihren Methoden, Vorlieben oder Widerständen in Bezug auf die wissenschaftliche Nutzung von sozialen Medien (siehe z.B. Ponte und Simon (2011), hauptsächlich für Großbritannien; Bader, Fritz und Gloning (2012) und Pscheida et al. (2013) für Deutschland). Andere Forschungsbereiche untersuchen digitale Praktiken und Online-Communities von außen, z.B. über die Analyse von Inhalten und Sprache oder über die Analyse von Netzwerken anhand von Links und Kommentaren. Beide Arten von Studien zeigen zumeist eine Vielzahl von unterschiedlichen Nutzungen, Zwecken und Motiven der sozialen Medien, je nach Plattform, akademischem Rang, Status, Geschlecht und Alter der Forschenden sowie unterschiedlich nach Disziplinen, Ländern und geografischen Regionen. 
         Statistiken und Beobachtungen der geisteswissenschaftlichen Blogs geben einen Einblick in die Motivationen der Bloggenden, in ihre Blogpraktiken wie auch in ihre Kommunikation über Kommentare und Verlinkungen. Abgesehen von den beiden erwähnten bereits gealterteten Umfragen allgemein zu sozialen Medien in der Wissenschaft (Bader, Fritz und Gloning, 2012, sowie Pscheida et al. 2013), gab es anders als im angelsächsischen Raum (Jarreau 2015) im deutschsprachigen Raum noch keine spezifische Befragung geisteswissenschaftlicher Bloggenden im größeren Ausmaß. Diese Lücke wird durch eine Umfrage geschlossen, die im Herbst 2018 bei den rund 350 Blogs von de.hypotheses sowie auch darüber hinaus durchgeführt wird und deren Ergebnisse der Vortrag vorstellen möchte. 
         Die Umfrage zielt in erster Linie darauf, Gründe für das Wissenschaftsbloggen sowie konkrete Praktiken des geisteswissenschaftlichen Bloggens abzufragen und darüber mögliche Änderungen im Publikations- und Kommunikationsverhalten von Geisteswissenschaftlerinnen und -wissenschaftlern empirisch gestützt zu ermitteln. Als empirische Studie mit medientheoretischem Bezug knüpft der Vortrag damit direkt an das Tagungsthema an.
         Die Hauptfragebereiche der Umfrage beziehen sich auf Motivationen für das Bloggen, auf Inhalte, Zeitaufwand, Publikum und formale Gestaltung sowie auf den „Erfolg“ der Blogs in Bezug auf Kommentare, Rückmeldungen und Zugriffsstatistiken. Das Abfragen von Personendaten soll ermöglichen, diese Antworten mit akademischem Rang, Alter und Geschlecht der Bloggenden zurückzukoppeln und darüber etwa gender- und statusspezifische Praktiken ausmachen zu können. Dagegen geht es nicht um die Abfrage der Zufriedenheit der Bloggenden mit der Plattform de.hypotheses selbst. Folgende Themenblöcke werden u.a. angesprochen:
         Die Nutzung wissenschaftlicher Blogs ist je nach Strategie und Zielen der Forschenden sehr unterschiedlich. In der Kategorie "Über das Blog" auf den Wissenschaftsblogs von de.hypotheses bekommt man dazu einen Einblick. Bloggende Forschende nennen als erste Motivation den Wunsch, ihr Forschungsthema zu diskutieren, ihre Online-Reputation zu verbessern, sich in der Wissenschaft zu positionieren und Netzwerke zu pflegen (König 2015: 59). Darüber hinaus wollen die Forschenden das Schreiben üben oder ihren Schreibstil verbessern und sich kreativ ausdrücken. Andere Untersuchungen deuten darauf hin, dass Bloggen Forschenden das Gefühl vermittelt, in ihrer Arbeit mit anderen verbunden zu sein (Mewburn und Thomson 2013: 1107). Blogs können als Dokumentation für Forschungsprojekte dienen, als eine Art digitaler Zettelkasten, der über Schlagwörter und Kategorien strukturiert und öffentlich zugänglich ist. Diese Angaben zur Motivation und Gründe des Bloggens werden in der Umfrage abgefragt, wobei es zugleich auch darum gehen wird, ob diese Ziele subjektiv nach Empfinden der Einzelnen erreicht werden.
         Ein weiterer Fragenblock der Umfrage behandelt die internen Abläufe bei der Veröffentlichung auf Wissenschaftsblogs. Einige Blogs funktionieren ähnlich wie Zeitschriften: Sie haben eine Redaktion, die Autorinnen und Autoren einlädt, Artikel redaktionell bearbeitet und sicherstellt, dass Blogbeiträge in traditionellen Bibliographien und Bibliothekskatalogen katalogisiert werden. Aber auch in Einzelblogs publizieren Autorinnen und Autoren oftmals erst, nachdem die Beiträge von einer anderen Person gegengelesen worden sind. Dies schließt an die Beobachtung an, dass Forschende ihre Blogs als Orte der Selbstpublikation nicht leichtsinnig befüllen, sondern sich viele Gedanken machen, was sie wann, wie und in welcher Form publizieren. Vielen Forschenden fällt es schwer, unfertige oder aufkommende Ideen zu veröffentlichen. Sie haben Angst davor, sich zu irren und wollen vermeintliche Sackgassen nicht öffentlich machen. Die Angst vor Plagiaten hindert sie ebenso daran, über aktuelle Erkenntnisse und aktuelle Projekte zu bloggen. Welche strategischen und konzeptionellen Grundideen geisteswissenschaftliche Bloggende verfolgen soll ebenso wie die Organisation der redaktionellen Zwischenschritte vor der Veröffentlichung durch die Umfrage deutlich werden.
         Es gibt eine große Vielfalt an Inhalten, die in wissenschaftlichen Blogs veröffentlicht werden. Einige Bloggende schreiben grundsätzlich nur über ihr Forschungsthema. Andere diskutieren wissenschaftliche Arbeitspraktiken, geben Karriereberatung oder nutzen ihr Blog zur Begleitung der Lehre. Häufig wird in wissenschaftlichen Blogs die akademische Kultur allgemein kritisiert (Mewburn und Thomson 2013: 1110). Alles in allem lassen Blogs den Forschenden als hybride Person erscheinen und zeigen, dass die akademischen Interessen von Wissenschaftlerinnen und Wissenschaftlern viel breiter sind als die in klassischen Medien veröffentlichte Forschung das widerspiegelt (Mewburn und Thomson 2013: 1114). Darüber hinaus unterstreicht der unterschiedliche und informelle Stil der Blogartikel die Vielfalt des wissenschaftlichen Schreibens und die Vielfalt der Perspektiven. In wissenschaftlichen Blogs ist es möglich, in der ersten Person Singular zu schreiben, engagiert, witzig, kreativ und essayistisch zu schreiben, Smileys oder Strikes zu verwenden, Code, Bilder und Videos einzubetten und damit multimedial zu publizieren (König 2015: 64-65). Die Umfrage soll Aufschluss geben, ob und in welchem Umfang die bloggenden Geisteswissenschaftlerinnen und Geisteswissenschaftler von den stilistischen Freiheiten des Genres profitieren oder ob sie sich freiwillig an traditionellere Formate und Publikationsrhythmen anpassen.
         Einige explorative Studien deuten darauf hin, dass sich Blogs weder bei der Sprachwahl noch bei der Themenwahl an ein Laienpublikum wenden (Mahrt und Puschmann 2014: 4; Mewburn und Thomson 2013: 1113). In der Umfrage wird gezielt bei der deutschsprachigen Community von hypotheses abgefragt, an welches Publikum sich die Forschenden in der Regel wenden, ob der Transfer von Forschungsergebnissen in die Öffentlichkeit zu den Zielen gehört und ob sich die Bloggenden sprachlich auf ein Laienpublikum einstellen oder ob sie auf Medienaufmerksamkeit zielen. Es gibt Rückmeldung von Bloggenden, wonach Blogbeiträge als Vorstufe für Peer Review-Artikel gesehen werden und Forschende aufgrund von Blogbeiträgen aufgefordert worden sind, diese zu vollständigen Artikeln auszubauen. Wie verbreitet dieses Phänomen ist, soll die Umfrage empirisch zeigen.
         Bloggen wird als eine hochgradig interaktive Praxis angesehen (Mahrt und Puschmann 2014: 6), obwohl Kommentare zu geistes- und sozialwissenschaftlichen Blogs knapper geworden sind, u.a. weil die Diskussion von Blogartikeln auf Twitter, Facebook oder andere soziale Medien verschoben wurde (König 2015: 72). Auf dem Blog-Hub SciLogs mit mehrheitlich Bloggenden aus den Naturwissenschaften erhalten Artikel durchschnittlich fünf Kommentare. Blogging-Stars wie der österreichische Astronom Florian Freistätter wiederum erhalten regelmäßig zwischen 50 und 100 Kommentare pro Artikel (Lobin 2017: 226). Die aktuellen Statistiken für die Plattform de.hypotheses liegen zwar vor, bei den Bloggenden wird aber nachgefragt, wie sie mit den Kommentaren umgehen, ob sie selbst welche schreiben, ob die Inhalte der Kommentare überwiegend positiv, neutral oder negativ sind, welche andere Form von Rückmeldungen sie für ihre Blogbeiträge erhalten und wie wichtig ihnen diese für das Einschätzen des eigenen Erfolgs sind.
      
      
         
            
               Bibliographie
               
                  Bader, Anita / Fritz, Gerd / Gloning, Thomas (2012): Digitale Wissenschaftskommunikation 2010-2011. Eine Online-Befragung. Gießen, 
                        
                     http://geb.uni-giessen.de/geb/volltexte/2012/8539/
                  . 
                    
               
                  Jarreau, Page (2015):
                  All the Science That Is Fit to Blog. An Analysis of Science Blogging Practices. 
                        LSU Doctoral Dissertations 1051, 
                        https://digitalcommons.lsu.edu/cgi/viewcontent.cgi?article=2050&context=gradschool_dissertations.
                    
               
                  König, Mareike (2013):
                  Die Entdeckung der Vielfalt: Geschichtsblogs auf der internationalen Plattform hypotheses.org, 
                        in: 
                        Peter Haber / Eva Pfanzelter (eds.):
                  Historyblogosphere. Bloggen in den Geschichtswissenschaften. 
                        München: Oldenbourg 181–197. 
                    
               
                  König, Mareike (2015):
                  Herausforderung für unsere Wissenschaftskultur: Weblogs in den Geisteswissenschaften, 
                        in: 
                        Wolfgang Schmale (ed.):
                  Digital Humanities. Praktiken der Digitalisierung, der Dissemination und der Selbstreflexivität. 
                        Stuttgart: Steiner 57-74.
                    
               
                  Lobin, Henning (2017):
                  Aktuelle und künftige technische Rahmenbedingungen digitaler Medien für die Wissenschaftskommunikation, 
                        in: 
                        Peter Weingart / Holger Wormer / Andreas Wenninger / Reinhard F. Hüttl (eds.):
                  Perspektiven der Wissenschaftskommunikation im digitalen Zeitalter. 
                        Weilerswist: Velbrück 223-258.
                    
               
                  Mahrt, Merja / Cornelius Puschmann (2014):
                  Science Blogging: an exploratory study of motives, styles, and audience reactions, 
                        in: Journal of Science Communication 13/3: A05. 
                        https://jcom.sissa.it/archive/13/03/JCOM_1303_2014_A05
                  (accessed August 31, 2018).
                    
               
                  Mewburn, Inger / Pat Thomson (2013):
                  Why Do Academics Blog? An Analysis of Audiences, Purposes and Challenges, 
                        in: Studies in Higher Education 38/8: 1105–1119.
                    
               
                  Mounier, Pierre (2013):
                  Die Werkstatt öffnen: Geschichtsschreibung in Blogs und sozialen Medien, 
                        in: 
                        Peter Haber / Eva Pfanzelter (eds.):
                  Historyblogosphere. Bloggen in den Geschichtswissenschaften. 
                        München: Oldenbourg 51-59.
                    
               
                  Ponte, Diego / Simon, Judith (2011):
                  Scholarly Communication 2.0: Exploring Researchers' Opinions on Web 2.0 for Scientific Knowledge Creation, 
                        Evaluation and Dissemination, Serials Review 37(3): 149-156.
                    
               
                  Pscheida, Daniela / Albrecht, Steffen / Herbst, Sabrina / Minet, Claudia / Köhler, Thomas (2013):
                  Nutzung von Social Media und onlinebasierten Anwendungen in der Wissenschaft. Erste Ergebnisse des Science 2.0-Survey 2013 des Leibniz-Forschungsverbunds “Science 2.0”, 
                        https://d-nb.info/1069096679/34.
                    
               
                  Sugimoto, Cassidy R. / Sam Work / Vincent Larivière / Stefanie Haustein (2017):
                  Scholarly Use of Social Media and Altmetrics: a Review of the Literature, 
                        in: Journal of the Association for Information Science and Technology, 68/9: 2037–2062.
                    
            
         
      
   



      
         
            Abstract
            Die Bedeutung von Social Media in den digitalen Geisteswissenschaften wächst. Nicht nur als Gegenstand der Analyse (z.B. in Gao et al. 2018 oder Reid 2011) sind Social Media für die Digital Humanities von Interesse, sondern auch zunehmend für die Dissemination von Forschungsergebnissen (vgl. Ross 2012). Vor allem in Blogs und Twitter wurde großes Potential für Diskussionen und die Verbreitung von Ergebnissen erkannt (vgl. Puschmann/Bastos 2015, Terras 2012). Auch in der Rezeptionsforschung der Wissenschaftskommunikation zeigt sich, dass Webmedien besonders relevant sind (vgl. Brossard 2013, 14096–14101) und dass diese darum in besonderem Maße zur 
                    „scientific literacy” beitragen könnten (vgl. Schäfer 2017, 283). Generell bietet (informelle) Wissenschaftskommunikation über Webmedien noch viel ungenutztes Potential (vgl. Schäfer 2017, 279–280, Neuberger 2014, Voigt 2012). Unser Beitrag zeigt, wie eine multimediale und multimodale webbasierte Strategie die Dissemination von Digital-Humanities-Methoden unterstützen und die Wissenschaftskommunikation des Forschungsfeldes stärken kann. Die quantitative Analyse der Erfolge dieser Strategie lässt Rückschlüsse darauf zu, welche Methode wem wie vermittelt werden sollte und bildet daher eine wichtige Basis für die Konzeption von Forschungsprojekten und der universitären Lehre.
                
         
         
            Konzeptioneller Rahmen – Multimedialität, Multimodalität und Codierungssysteme in forTEXT
            forTEXT ist ein Vermittlungsprojekt für digitale Methoden der Textanalyse, das sich vor allem an Forschende richtet, die bisher noch nicht mit digitalen Methoden arbeiten (siehe 
                    
                  https://fortext.net).
                Neben der ‘analogen’ Dissemination in Workshops und universitärer Lehre wird auch ein Schwerpunkt auf die online-Vermittlung gelegt, da an wissenschaftlichen Themen Interessierte diese Kanäle häufig als Informationsquelle nutzen (vgl. Brossard 2013, 14098). Die hier vorgestellte webbasierte Strategie als Teil des Disseminationskonzeptes in forTEXT soll darüber hinaus zur DH-Wissenschaftskommunikation beitragen und so die Sichtbarkeit des Forschungsgebiets erhöhen (zur Bedeutung der Geisteswissenschaften in der Wissenschaftskommunikation vgl. Scheu/Volpers 2017). Für die forTEXT-Disseminationsstrategie sind Multimedialität, Multimodalität und multiple Codierungen zentrale Aspekte, die wir wie folgt definieren:
                
            
               Multimedialität: Aufbereitung und/oder Nutzung unterschiedlicher medialer Kanäle. 
                    „Medium” verstehen wir wie Roesler/Stiegler (2005, 150–152) als Vermittlungssystem innerhalb eines Kommunikationsprozesses, bei dem auch das Medium selbst Teil der Vermittlung ist.
                
            
               Multimodalität: Aufbereitung und/oder Nutzung unterschiedlicher Kommunikationsmodi. Dabei verstehen wir 
                    „Modus” als Bezeichnung für eine semiotische Einheit wie z.B. Design oder Sprache (vgl. Bucher 2007, 53).
                
            
               Multiple kulturelle Codierung: Wir übernehmen hier ein semiotisches Verständnis von 
                    „Code
                    ” als Bezeichnung für ein System relevanter Informationseinheiten (vgl. Eco 1985, 58f.). Kulturelle Codes funktionieren als Bedeutungsnetz aus Referenzen auf ein kollektives Wissenskorpus (vgl. Barthes 1976, 25). Um den Begriff klar vom informationstechnologischen (Binär-)Code zu trennen, sprechen wir von Codierung oder Codierungssystem.
                
            Medien, Modi und Codes wirken auf unterschiedlichen Ebenen des Vermittlungsprozesses. Dabei sind Medien und Modi stark miteinander verbunden. Modi können aber als Varianten in andere Medien übertragen werden. Codes sind inhaltliche Elemente, weshalb sie für die Dissemination von Forschungsergebnissen zentral sind. Sie können sich auf einen Modus in einem Medium beziehen oder modi- und medienübergreifend sein: 
            
               
            
         
         
            Arbeitspraxis – die webbasierte Disseminationsstrategie
            
               Die forTEXT-Webseite als Basis medialer Vermittlung von Digital-Humanities-Inhalten
               
                  
               
               Das zentrale Vermittlungsmedium in forTEXT ist die Projektwebseite. Hier werden in Textbeiträgen sowohl Bilder als auch Videos eingebettet. Die forTEXT-Webseite bildet die Basis für die multimediale Web-Strategie, da hier grundlegende Modi und kulturelle Codierungen umgesetzt wurden, die in den sozialen Medien erweitert werden. Die primär genutzten Modi und ihre kulturellen Codierungen sind:
               
                  Design: Gedecktes Farbschema und serifenlose Schrift stehen für Schlichtheit und Sachlichkeit. Nur im Logo gibt es verspielte Elemente, die an eine Handschrift erinnern und die Verbindung von Tradition und Modernität vermitteln.
                    
               
                  Sprache: Die Beiträge erfüllen die Ansprüche wissenschaftlichen Schreibens. Die Wissenschaftlichkeit wird durch die technische Funktionalität zum Zitieren unterstützt.
                    
               
                  Stimme: Grundsätzlich ist die Webseite mehrstimmig angelegt, da hier verschiedene Autor*innen schreiben. Alle nutzen einen sachlichen Tonfall und die implizite Leserin wird stets mit formellem 
                        „Sie” angesprochen.
                    
               
                  Bildlichkeit: Die eingebetteten Bilder sind zumeist digitale Repräsentationen der eingesetzten Tools und scheinen als solche zunächst gegenstandsneutral. Allerdings sind die Bilder häufig auch Visualisierungen der in forTEXT durchgeführten Fallstudien, d.h. sie zeigen nicht nur grafische, sondern auch textliche Elemente und verweisen auf die Modellierung eines Forschungsgegenstandes, die bei der Erstellung der Grafik stattgefunden haben muss.
                    
               Die forTEXT-Webseite richtet sich in erster Linie an drei Zielgruppen, die sich für die forTEXT-Disseminationsstrategie als besonders relevant erwiesen haben:
               
                  Studierende – Lernende der DH-Methodik
                  Nachwuchswissenschaftler*innen – Umsetzende der DH-Methodik
                  Digitale Geisteswissenschaftler*innen – Lehrende der DH-Methodik
               
            
            
               Social Media in forTEXT
               Ausgehend von den Inhalten der Webseite, deren Modi und den entsprechenden Codierungen werden drei soziale Medien zur Vermittlung genutzt. Anders als die Webseite sollen die Social-Media-Kanäle jeweils primär eine Zielgruppe erreichen: YouTube vor allem Zielgruppe 1, Pinterest Zielgruppe 2 und Twitter Zielgruppe 3.
               YouTube
               Auf YouTube erstellen wir eigene Inhalte, die die Artikel der Webseite aufgreifen, weiterführen und ergänzen. Es gibt derzeit zwei Inhaltstypen; Fallstudien und Tutorials. Beide können als Open-Educational-Ressources genutzt werden. In methodischen Fallstudien wird zum Beispiel mittels NER verglichen, welche Bedeutung die Hauptfiguren in Goethes 
                        Werther und in Plenzdorfs 
                        neuem Werther haben. Wir erklären, wie die NER-Machine-Learning-Prozesse funktionieren und verlinken sowohl zur Webseite als auch zu forTEXT-Tutorials. In den forTEXT-NER-Tutorials wird in drei kleinen Einheiten die Installation, Anwendung und das Training eines eigenen NER-Modells gelehrt.
                    
               
                  
               
               Design und sprachliche Elemente der forTEXT-YouTube-Videos richten sich nach den Vorgaben der Webseite. Abbildungen der eingesetzten Tools werden ergänzt von piktografischen Animationen. Diese sind zwar schlicht, befördern jedoch Unvoreingenommenheit und Autodidaktik. Dem Vorurteil einer geringeren Technikaffinität weiblicher Menschen begegnen wir mit einem weiblichen Voice-over (vgl. Schelhowe 2000). Dadurch werden Schwellenängste abgebaut und der Eindruck vermittelt, dass Nutzer*innen und digitale Tutorin sich gemeinsam autodidaktisch an die Methoden heranwagen.
               Pinterest
               
                  
               
               Die Anpassbarkeit in Hinblick auf Design, Stimme und Bildlichkeit der Kommunikationsmodi ist bei Pinterest am geringsten. Hier werden überwiegend fremde Artikel 
                        „gepinnt”, die lediglich mit einer kurzen Beschreibung angereichert werden. Auch führt die Besonderheit von Pinterest als Chimäre zwischen sozialem Medium und Suchmaschine dazu, dass die sprachlichen Elemente nicht nur für die menschliche Wahrnehmung, sondern insbesondere technologisch eine Rolle spielen. Primär werden hier DH-Forschende angesprochen, die Pinnwände für Tools (z.B. Stanford-NER, Carto, Gephi, CATMA), einzelne Methoden (z.B. Netzwerkanalyse, Stilometrie, Topic Modeling), Diskussionen und viele andere Themen der Digital Humanities finden.
                    
               Twitter
               Bei Twitter sind Layout und Typografie der Tweets nicht veränderbar. Jedoch wird bei jedem Tweet das forTEXT-Logo angezeigt. Im Gegensatz zu Pinterest ist auf Twitter die Ausgestaltung der Stimme bedeutsam. Hier steht die Kommunikation mit der eigenen Forschungscommunity im Vordergrund. Die Beschränktheit der Tweets auf 280 Zeichen führt dazu, dass eher Fachbegriffe als Umschreibungen genutzt werden. Hashtags führen zu Themen, die für die Community bedeutsam sind und folgen einem kulturellen Sprachcode. Hier nimmt forTEXT an kollegialen Insider-Gesprächen Teil und betont die forschungsrelevante Seite des Projektes.
            
         
         
            Quantitative Analyse
            Die webbasierte forTEXT-Disseminationsstrategie wird regelmäßig quantitativ ausgewertet. Zusätzlich zur kontinuierlichen Steigerung von Aufmerksamkeit für das Projekt (quantitativ messbar durch Impressionen, Interaktionen, Betrachtungszeiten), werden auch Analysen durchgeführt, die eher konzeptionelle Aspekte der webbasierten Dissemination von Forschungsmethoden in den Fokus rücken. Auf Basis dieser Analysen entwickeln wir eigene Relevanzmetriken, die neben quantitativen auch qualitative Aspekte berücksichtigen, wie bspw. demografische Daten, die anzeigen, welche Zielgruppen über welche Medien, welche Modi und welche Codes tatsächlich erreicht werden können, aber auch Kommentare, Feedback und Interaktionen mit anderen Nutzer*innen.
            Zum jetzigen Zeitpunkt läuft die forTEXT-Social-Media-Arbeit seit drei Monaten. Auf allen medialen Kanälen zeigt sich bereits eine steigende Aufmerksamkeit, auch wenn die Zahlenwerte nach Medium stark differieren. Twitter erzielt mit durchschnittlich 20.000 Impressionen im Monat quantitativ die größte Reichweite. Auch die Interaktionsrate ist mit bis zu 7,4% relativ hoch – die Zielgruppe 3 kann hier sehr gut erreicht werden. Mit Pinterest konnten in den ersten drei Monaten durchschnittlich 1.737 monatliche Impressionen erreicht werden, wobei die einzelnen Monate mit 780 Betrachtern im ersten Monat und 9.300 Betrachtern im dritten Monat stark schwanken. Hier zeigt sich, dass die mit maschinellem Lernen verknüpfte Suchmaschine Pinterest länger braucht, um Inhalte und Interessierte zusammen zu bringen. Neue Inhalte müssen regelmäßig und vergleichsweise hochfrequent (derzeit fünf tägliche Pins) verlinkt werden, damit die Pinterest-Algorithmen ein Profil einzuordnen lernen und anderen Nutzer*innen empfehlen. Eine Einsicht aus der Analyse der Pinterest-Daten ist, dass hier insbesondere Nutzerinnen erreicht werden können. Die Vermittlung von forTEXT-Inhalten über YouTube läuft derzeit erst etwa einen Monat, sodass die Zahlenwerte (140 Impressionen im September) noch relativ gering sind. Qualitative Rückmeldung zeigt aber, dass die Videos bisher vor allem im Rahmen der DH-Lehre auf Interesse stoßen.
            Bereits zu diesem frühen Zeitpunkt zeigt die Fallstudie des forTEXT-Projektes, welche Aspekte einer multimedialen, multimodalen und multipel codierten Vermittlungsstrategie sich als produktiv erweisen. Die Vorannahme, dass auf Twitter vor allem die eigene Community erreichbar ist, hat sich bestätigt. Hingegen deutet der Gender-Gap auf Pinterest an, dass hier weniger Zielgruppe 2, sondern eher Zielgruppe 1 erreicht werden kann, da vor allem die Zielgruppe der Studierenden geisteswissenschaftlicher Fächer meist überwiegend weiblich ist. Aus Feedback zu den forTEXT-YouTube-Videos konnten wir erfahren, dass diese derzeit vor allem für Lehrende von Interesse sind. Neben den vor allem im Marketing üblichen Relevanzkriterien von Impressionen, Engagement und Interaktion (die auch für die wissenschaftliche Impactmessung fruchtbar gemacht werden können, vgl. Herb/Beucke 2013) ist für die Vermittlung von DH-Methoden daher die tatsächlich erreichte Zielgruppe und deren Nutzungsmotivation relevant. So kann forTEXT am Ende nicht nur selbst Social Media produktiv nutzen, sondern auch aufzeigen, welche Medien für welche Ziele der Vermittlung von DH-Methoden besonders bedeutend sind.
         
      
      
         
            
               Bibliographie
               
                  Barthes, Roland (1976): 
                  S/Z. Frankfurt am Main: Suhrkamp.
                    
               
                  Brossard, Dominique (2013): 
                  New media landscape and the science information consumer, 
                        in: PNAS 110 (3), 14096–14101. 
                        
                     https://www.pnas.org/content/pnas/110/Supplement_3/14096.full.pdf
                  , [Zugriff 21.12.2018]. 
                    
               
                  Bucher, Hans-Jürgen (2007): 
                  Textdesign und Multimodalität. Zur Semantik und Pragmatik medialer Gestaltungsformen, 
                        in: 
                        Roth, Kersten Sven / Spitzmüller, Jürgen (eds.):
                  Textdesign und Textwirkung in der massenmedialen Kommunikation. 
                        Konstanz: UVK, 49–76. 
                    
               
                  Eco, Umberto (1985): 
                  Einführung in die Semiotik. 
                        München: Fink.
                    
               
                  Herb, Ulrich / Beucke, Daniel (2013): 
                  Die Zukunft der Impact-Messung. Social Media, Nutzung und Zitate im World Wide Web, 
                        in: Wissenschaftsmanagement. Zeitschrift für Innovation 19 (4), 22–25. 
                        
                     https://publikationen.sulb.uni-saarland.de/bitstream/20.500.11880/23789/1/Die_Zukunft_der_Impact_Messung_fuer_Reps_fertig.pdf
                   [Zugriff: 21.12.2018].
                    
               
                  Gao, Jin / Nyhan, Julianne / Duke-Williams, Oliver / Mahony, Simon (2018): 
                  Visualising The Digital Humanities Community: A Comparison Study Between Citation Network And Social Network, 
                        in: Digital Humanities 2018. Book of Abstracts. Puentes-Bridges.
                    
               
                  Neuberger, Christian (2014): 
                  Social Media in der Wissenschaftsöffentlichkeit. Forschungsstand und Empfehlungen, 
                        in: 
                        Weingart, Peter / Schulz, Patricia (eds.): 
                  Wissen – Nachricht – Sensation. Zur Kommunikation zwischen Wissenschaft, Medien und Öffentlichkeit. 
                        Weilerswist: Velbrück, 315–368.
                    
               
                  Puschmann, Cornelius / Bastos, Marco (2015):
                  How Digital Are the Digital Humanities? An Analysis of Two Scholarly Blogging Platforms, 
                        in: PLOS ONE 10 (2): e0115035.
                        
                     https://doi.org/10.1371/journal.pone.0115035
                  
                   [Zugriff 2.10.2018].
               
               
                  Reid, Alexander (2011):
                  Social Media Assemblages in Digital Humanities: from Backchannel to Buzz, 
                        in: 
                        Wankel, Charles (ed.): 
                  Teaching Arts and Science with the New Social Media. 
                        West Yorkshire: Emerald Publishing, 321–338.
                    
               
                  Roesler, Alexander / Stiegler, Bernd (eds. 2005): 
                  Grundbegriffe der Medientheorie. 
                        Paderborn: UTB.
                    
               
                  Ross, Claire (2012): 
                  Social media for digital humanities and community engagement, 
                        in: 
                        Warwick, Claire / Terras, Melissa / Nyhann, Julianne (eds.): 
                  Digital Humanities in Practice. 
                        London: Facet Publishing.
                    
               
                  Schäfer, Mike S. (2017): 
                  Wissenschaftskommunikation online, 
                        in: 
                        Bonfadelli, Heinz et al. (eds.): 
                  Forschungsfeld Wissenschaftskommunikation. Wiesbaden: Springer. 
                        
                     https://link.springer.com/content/pdf/10.1007%2F978-3-658-12898-2.pdf
                   [Zugriff 21.12.2018].
                    
               
                  Schelhowe, Heidi (2000): 
                  Informatik, 
                        in: 
                        Braun, Christina von / Stephan, Inge (eds.): 
                  Gender-Studien. Eine Einführung. Stuttgart, Weimar: Metzler, 207–216.
                    
               
                  Scheu, Andreas M. / Volpers, Anna Maria (2017): 
                  Sozial- und Geisteswissenschaften im öffentlichen Diskurs, 
                        in: 
                        Bonfadelli, Heinz et al. (eds.): 
                  Forschungsfeld Wissenschaftskommunikation. Wiesbaden: Springer. 
                        
                     https://link.springer.com/content/pdf/10.1007%2F978-3-658-12898-2.pdf
                   [Zugriff 21.12.2018].
                    
               
                  Terras, Melissa (2012):
                  The Impact of Social Media on the Dissemination of Research: Results of an Experiment, 
                        in: Journal of Digital Humanities 1 (3).
                        
                     http://journalofdigitalhumanities.org/1-3/the-impact-of-social-media-on-the-dissemination-of-research-by-melissa-terras/
                  
                   [Zugriff 2.10.2018].
               
               
                  Voigt, Kristin (2012): 
                  Informelle Wissenschaftskommunikation und Social Media. 
                        Berlin: Frank & Timme.
                    
            
         
      
   



      
         In diesem Poster werden die Möglichkeiten und Grenzen der Öffnung eines Forschungsprojekts für andere Forschende, aber auch für die interessierte Öffentlichkeit dargestellt. Das Open Science-Projekt 
                Handke: in Zungen dient dabei als Beispiel, anhand dessen verschiedene Aspekte der “Öffnung” von digitaler Geisteswissenschaft diskutiert werden. Nach einer kurzen Vorstellung des Projekts widmet sich der Beitrag den darin angewandten Methoden der Offenen Wissenschaft, aber auch den Anforderungen, die diese mit sich bringen sowie der Frage danach, wie und wozu Offene Wissenschaft konsequent umgesetzt werden kann. 
            
         Seit den beginnenden 1980er Jahren haben Fremdsprachen in den Bühnentexten des österreichischen Schriftstellers Peter Handke (*1942) zunehmend an Bedeutung gewonnen. In den frühen, sprachkritischen Stücken der 1960er und 70er Jahre spielen die Sprache, ihre Gemachtheit und die Reflexion darüber die zentrale Rolle – mit einem Umschwung, der sich am “dramatischen Gedicht” Über die Dörfer (1981) festmachen lässt, werden die Bühnenarbeiten Handkes zunehmend “erzählend” (Kastberger/Pektor 2012: 5), gewinnen zunehmend an “Handlung”. Mit dieser “Wende” (Höller 2013), mit der auch der Beginn von Handkes Tätigkeit als Übersetzer einhergeht, halten auch die fremden Sprachen Einzug in die Stücke des Autors. 
         Im vorgestellten Projekt werden sämtliche fremdsprachigen Wörter und Textteile in den beinah 30 Bühnentexten Handkes erhoben und untersucht. Die Leitfragen dabei sind, ob und in welcher Weise bestimmte Sprachen für bestimmte semantische Felder und Themenbereiche eingesetzt werden, welche Sprachen vorherrschen, ob und wie sich die Wichtigkeit einzelner Sprachen im Lauf der Zeit verändert und wie die verschiedenen einfließenden Fremdsprachen miteinander in Beziehung stehen. Für die Analyse dieser Fragen werden die relevanten Textstellen in der relationalen Datenbank 
                Handke: in Zungen gesammelt, wo sie sortier-, durchsuch- und auswertbar gemacht werden. Die Datenbank und das Projekt, in dessen Rahmen sie entsteht, sind der Offenen Wissenschaft verpflichtet und dienen daher als Ausgangspunkt für den geplanten Beitrag.
            
         Die Umsetzung eines Offenen Ansatzes in Forschungsprojekten bringt eine Reihe an Themen mit sich, mit denen sich Forschende der traditionellen Geisteswissenschaften nicht vorrangig beschäftigen müssen, die aber in den Digital Humanities von zentraler Bedeutung sind. Zu diesen gehören etwa die Frage nach offener Lizenzierung von Daten, Code und Forschungsergebnissen wie Aufsätzen und Präsentationen, aber auch jene nach deren (langfristiger) Aufbewahrung und Verfügbarmachung, nach adäquater Dokumentation und nach Kommunikation und Vermittlungsarbeit. Das 
                Handke: in Zungen-Projekt eignet sich für eine Diskussion dieser verschiedenen Aspekte von Open Science deshalb in besonderer Weise, weil es unter anderem dank Unterstützung durch Wikimedia Deutschland im Rahmen des Wikimedia-Fellowship-Programms zu Freiem Wissen und Offener Wissenschaft umgesetzt wurde. Aus diesem Grund hat das Projekt neben der eigentlichen Datenbank-Web-App mehrere online-Präsenzen, die zur Öffnung des Projekts und des darin gesammelten Wissens beitragen: Zwei GitHub-Repositories machen Projekt-Informationen und -Logbuch sowie den Code der Web-App verfügbar, eine Wikiversity-Seite versammelt den Datenmanagementplan sowie alle weiteren relevanten Informationen und Berichte zum Projekt, in einer offenen Zotero-Gruppe sind die Quellenangaben der bearbeiteten Primärtexte verfügbar und auf einem Twitter-Account werden alle Interessierten über Neuigkeiten aus dem Projekt auf dem Laufenden gehalten.
            
         Die zahlreichen Kommunikations- und Distributionskanäle, die von diesem Projekt bespielt werden, werden in diesem Beitrag vorgestellt und ihre jeweils spezifischen Vor- und Nachteile diskutiert. Ebenso werden die Voraussetzungen und Rahmenbedingungen des Projekts (rechtliche Voraussetzungen, Personal- und Zeitressourcen), die seinen Grad an Öffnung beeinflusst haben, zum Thema gemacht. Ebenfalls thematisiert werden die Notwendigkeit und Rolle von Publikumsveranstaltungen in Offenen Forschungsprojekten. Diese Bereiche werden dabei den Aktionsfeldern von Open Science zugeordnet, wie sie das Open Science Network Austria OANA definiert (Open Access, Open Research Data, Open Evaluation, Citizen Science, Open Methodology).
         Es soll dabei vorgeschlagen werden, “Open Science” nicht als eine strikt definierte Methode mit einem fixen Satz an verpflichtenden Elementen der Öffnung zu verstehen. Vielmehr sollte “Open” als eine Skala gesehen werden, auf der Projekte, die offene Methoden anwenden wollen, den für sie jeweils angemessenen Platz finden müssen, der von den oben erwähnten Rahmenbedingungen mitbestimmt wird. Grundsätzlich jedoch, so das abschließende Argument dieses Beitrags, sollte sich die geisteswissenschaftliche Forschung - insbesondere die digitale - konsequent auf ihre eigene Öffnung hin orientieren. Dafür sprechen neben praktischen auch ideologische Argumente. So formulieren es auch Pomerantz und Peek in ihrem Aufsatz 
                Fifty shades of open, in dem die gesellschaftliche Bedeutung von Offener Wissenschaft thematisiert wird: “As the number of open resources of all types increases, the more open resources will be created using them and derived from them, and the more open resources there will be. This snowballing growth of openness is socially beneficial, and, we believe, will make the world a better place.” (Pomerantz/Peek 2016)
            
      
      
         
            
               Bibliographie
               Höller, Hans: Eine ungewöhnliche Klassik nach 1945. Das Werk Peter Handkes. Berlin: Suhrkamp 2013.
               Kastberger, Klaus / Pektor, Katharina: Vorwort, in: Dies. (Hg.): Die Arbeit des Zuschauers. Peter Handke und das Theater. Salzburg/Wien: Jung und Jung 2012.
               Open Science Network Austria (OANA): Über Open Science. 
                        https://www.oana.at/ueber-open-science/
               
               
                  Pomerantz, Jeffrey / Peek, Robin:
                  Fifty shades of open, 
                        in: 
                        First Monday 4/2016. 
                        http://firstmonday.org/ojs/index.php/fm/article/view/6360/5460
               
               Ressourcen zum Projekt:
               Web-App: 
                        https://handkeinzungen.acdh.oeaw.ac.at/
               
               Projekt-Logbuch: 
                        https://github.com/vanyh/handkeinzungen
               
               Github-Repository: 
                        https://github.com/vanyh/handkeinzungen-app
               
               Wikiversity-Seite: 
                        https://de.wikiversity.org/wiki/Wikiversity:Fellow-Programm_Freies_Wissen/Einreichungen/Dramatische_Sprachen:_Fremdsprachen_in_den_B%C3%BChnentexten_von_Peter_Handke
               
               Zotero-Gruppe: 
                        https://www.zotero.org/groups/1840645/peter_handke_stage_texts
               
               Twitter-Account: 
                        https://twitter.com/HandkeinZungen
               
            
         
      
   



      
         
            Traditionelle und digitale Arbeitsweisen
            Die Anwendung computergestützter Verfahren in den Geistes- und Kulturwissenschaften prägt seit geraumer Zeit die Entwicklung unterschiedlicher Fachdisziplinen (vgl. Thaller 2012). Neue Methoden bahnen sich ihren Weg in den Methodenkanon ganz unterschiedlicher Domänen (vgl. Sahle 2015). Wie aber kann man Lehrenden – mit den unterschiedlichen Ansprüchen universitär Dozierender oder Lehrender an Schulen – einen möglichst niedrigschwelligen, aber dennoch wissenschaftlich seriösen Zugang zu dem Repertoire digitaler Methoden der Texterforschung eröffnen, das zum Spektrum der Digital Humanities zählt? Wie kann man sowohl Begeisterung wie kritische Kompetenz im konkreten Umgang mit Verfahren der digitalen Textanalyse so vermitteln, dass die Alltagspraxis des Lehrens und Forschens davon profitiert? Man muss nicht immer gleich einen theoretischen „Paradigmenwechsel" ausrufen, sondern kann das „neue” Feld besser zunächst im „hands-on"-Modus erschließbar machen. Durch einen niedrigschwelligen Disseminationsansatz entsteht die Möglichkeit, dass alte Fragen und neue Methoden sinnvoll aufeinander bezogen werden können (vgl. etwa Horstmann / Kleymann 2019).
            Das im November 2017 an der Universität Hamburg gestartete DFG-Projekt forTEXT (https://fortext.net) entwickelt vor diesem Hintergrund Strategien zur Dissemination digitaler Verfahren für die Arbeit mit Texten (vgl. Horstmann / Jacke / Meister 2018). In den auf der projekteigenen Webseite als Open-Access-Publikationen bereitgestellten zitierfähigen Besprechungen von Routinen, Ressourcen und Tools werden sämtliche Phasen eines literaturwissenschaftlichen Forschungsprojekts abgedeckt. Das Projekt leistet damit die Übersetzungsarbeit zwischen literaturwissenschaftlichen Fragestellungen und technischem Know-how, die für die Vermittlung digital gestützten Arbeitens an traditionellere Geisteswissenschaftlerïnnen notwendig ist.
                
         
         
            Routinen
            In der Rubrik Routinen stellen wir einführende Einträge zu digitalen 
                    Methoden der Textdigitalisierung, -annotation, -analyse, -visualisierung, -präsentation etc. zur Verfügung, in denen neben Definition, Diskussion und technischen Hintergründen stets auch die literaturwissenschaftliche Tradition der jeweiligen Methode betont wird. In 
                    Lerneinheiten zum Selberlernen werden Nutzerïnnen schrittweise an die Umsetzung der vorgestellten Methode in Kombination mit der Anwendung eines konkreten Tools (vgl. Abschnitt 4) und ausgewählter Ressourcen (vgl. Abschnitt 3) herangeführt. Die 
                    Lehrmodule bieten ebenfalls in Verbindung mit konkreten Ressourcen und Tools die Möglichkeit, das bereitgestellte Material in die eigene universitäre Lehrveranstaltung zu integrieren. Es werden zudem Unterrichtsmaterialien für den schulischen Unterricht erarbeitet, die durch eine noch erhöhte Komplexitätsreduktion Routinen der digitalen Literaturerforschung zugänglich machen und dezidiert an fachliche und KMK-Lernziele anknüpfen.
                
         
         
            Ressourcen
            Ausgewählte und etablierte deutschsprachige 
                    Textsammlungen, die sinnvoll mit den besprochenen Routinen der digitalen Literaturwissenschaft kombiniert werden können, stellen wir nicht nur vor, sondern ordnen und bewerten diese entsprechend ihrer thematischen Schwerpunkte. Die einzelnen Einträge folgen dabei einem wiedererkennbaren Schema, sodass insgesamt eine schnelle und bedarfsgerechte Orientierung ermöglicht wird. In der Kategorie Ressourcen bieten wir außerdem Tutorial-
                    Videos, die digitale Methoden anhand ausgewählter Tools Schritt für Schritt als Screencasts erklären und Video-Fallstudien, die literaturwissenschaftliche Fragestellungen beispielhaft mithilfe digitaler Tools bearbeiten und vorstellen. Außerdem enthält die Ressourcen-Kategorie auf literaturwissenschaftlichen Theorien basierende 
                    Tagsets und ein umfangreiches 
                    Glossar mit Erläuterungen zu Standardbegriffen der DH.
                
         
         
            Tools
            Für jede vorgestellte Methode stellen wir mindestens ein Tool vor, das für die praktische Umsetzung dieser Methode eingesetzt werden kann. Die Tools werden bedarfsgerecht hinsichtlich ihrer Funktionalität, Anwendungsfreundlichkeit, Nutzerbetreuung, Datensicherheit, Nutzungsbedingungen und des Grads ihrer Etablierung im wissenschaftlichen Diskurs befragt. Die Tooleinträge folgen – wie auch die einzelnen Beitragsformate in den Kategorien Routinen und Ressourcen – einem wiedererkennbaren Schema, in dem konkrete Fragen aus Nutzerïnnenperspektive gestellt und beantwortet werden.
         
         
            CATMA 6
            Mit der Entwicklung der sechsten Version von CATMA (https://catma.de) hat forTEXT im Oktober 2019 neue Funktionen, eine projektzentrierte Arbeitsstruktur und ein vollständig überarbeitetes, intuitiver nutzbares Interface des webbasierten, kollaborativ nutzbaren Annotations- und Analysetools (derzeit weltweit gut 13.000 Accounts) zur Verfügung gestellt. Das Tool integriert sich durch seine nutzerïnnenfreundliche Zugänglichkeit und die Konzentration auf die Methode der manuellen Annotation sowie der Analyse und Visualisierung von Text- wie Annotationsdaten in das forTEXT-Disseminationmodell und orientiert sich an den Bedarfen textwissenschaftlicher Fachwissenschaften.
                
         
         
            Nicht-digitale und digitale Dissemination
            Das Projekt wird durch umfangreiche Maßnahmen der nicht-digitalen Dissemination seiner Inhalte begleitet. Einerseits bieten die Projektmitarbeiterïnnen bedarfsgerechte Workshops und Vorträge für Forschungsgruppen oder Veranstaltungsreihen an Universitäten und auf Konferenzen an. Darüber hinaus werden schulinterne Workshops durchgeführt, die auf die z. T. sehr unterschiedliche technische Infrastruktur vor Ort eingehen und sich in der inhaltlichen Ausrichtung ebenfalls eng an der spezifischen Bedarfslage der Teilnehmerïnnen orientieren.
            Die umfangreiche Social-Media-Strategie von forTEXT (vgl. Horstmann / Schumacher 2019) ist ein essentieller Teil des gesamten Dissiminationsprogramms: Auf Twitter, Youtube, Facebook und Pinterest treten wir in unterschiedlichen Modi mit diverse Zielgruppen in Kontakt und führen diese in die digitale Arbeit mit Texten ein. So tritt forTEXT nicht nur an neue Nutzerïnnengruppen heran, sondern integriert sich auch selbst im fachwissenschaftlichen/DH-Diskurs.
         
         
            Individualisiertes Empfehlungssystem
            Im Januar 2020 wird ein digitales Empfehlungssystem implementiert, das im Frage-Antwort-Schema die Projekte der Nutzerïnnen so klassifiziert, dass die automatische Generierung individualisierter Empfehlungen von Routinen, Ressourcen und Tools zur Bearbeitung der jeweiligen Fragestellung möglich sein wird. Das Empfehlungssystem wird somit dafür sorgen, dass die einzelnen Bereiche von forTEXT einerseits zusammengefasst, andererseits aber auch bedarfsorientiert und effektiv durch sie navigiert werden kann. Das System macht damit insbesondere Nutzerïnnen ohne vorherige DH-Erfahrung den Einstieg in digitale Methoden zur Unterstützung ihrer Projekte individuell möglich.
         
      
      
         
            
Von den derzeit 13.033 Accounts wurden 3030 nur einmalig benutzt und 1876 waren Guest-Accounts, sodass man von 8127 Nutzerïnnen ausgehen kann (Stand: Dez. 2019).

         
         
            
               Bibliographie
               
                  Horstmann, Jan / Jacke Janina / Meister, Jan Christoph (2018): „Digital vs. Humanities. Didaktische Aufbereitung digitaler Methoden für die klassischen Geisteswissenschaften im Projekt forTEXT“, in: 
                        Kritik der digitalen Vernunft. DHd 2018 Köln. Konferenzabstracts, 386–391. 
                        http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf [Zugriff: 26. August 2019].
                    
               
                  Horstmann, Jan / Schumacher, Mareike (2019): „Social Media, YouTube und Co: Multimediale, multimodale und multicodierte Dissemination von Forschungsmethoden in forTEXT“, in: Sahle, Patrick (ed.): 
                        DHd 2019. Digital Humanities: multimedial & multimodal. Konferenzabstracts, 207–211. DOI: 
                        
                     10.5281/zenodo.2596095
                  .
                    
               
                  Horstmann, Jan / Kleymann, Rabea (2019): „Alte Fragen, neue Methoden – Philologische und digitale Verfahren im Dialog. Ein Beitrag zum Forschungsdiskurs um Entsagung und Ironie bei Goethe“, in: 
                        Zeitschrift für digitale Geisteswissenschaften DOI: 
                        
                     10.17175/2019_007
                  .
                    
               
                  Sahle, Patrick (2015): „Digital Humanities? Gibt’s doch gar nicht!”, in: Baum, Constanze / Stäcker, Thomas (eds.): 
                        Grenzen und Möglichkeiten der Digital Humanities. Sonderband der Zeitschrift für digitale Geisteswissenschaften, 1. DOI:
                         
                  
                     10.17175/sb001_004
                  .
                    
               
                  Thaller, Manfred (2012): „Controversies around the digital humanities: an agenda”, in: 
                        Historical Social Research, 
                        37(3): 7–23.
                    
            
         
      
   



      
         
            Die Digitalisierung verändert die Bedingungen für die Produktion, Distribution und Rezeption und damit auch für die Erforschung von Literatur. In den Digital Humanities stehen dabei bislang insbesondere die neuen Möglichkeiten der digitalen Auswertung (Distant/Scalable Reading) und die Digitalisierung vorhandener Druckbestände im Zentrum der Aufmerksamkeit. Die veränderten medialen Bedingungen führen jedoch nicht nur zu einer Übersetzung von gedruckten Texten in digitale Objekte, sondern bringen selbst produktiv neue literarische Formen und Gattungen hervor, für die computergestützte Elemente konstitutiv sind. Hierzu zählen etwa literarische Hypertexte, Blog-Formate, computergestützte kollektive und kollaborative Projekte, literarische Tweets und Twitter-Bots, Texte und Textgeneratoren, die auf computerlinguistische Methoden setzen, schließlich auch frühere Formen computergestützter Literaturproduktion wie der Poesieautomat von Hans Magnus Enzensberger oder die 
                Stochastischen Texte von Theo Lutz. (Rettberg 2019, Suter 2012, Tomaszek 2011, Lutz 1959). Hinzu kommen im Bereich Literaturforschung und -archive zunehmend digitale Vor- und Nachlässe, die eine Vielzahl von unterschiedlichen Datenträgern und Datenformaten beinhalten.
            
         Das jüngst ins Leben gerufene interdisziplinäre 
                Science Data Center für Literatur (SDC4Lit) hat sich das Ziel gesetzt, die Anforderungen, die Digitale Literatur an ihre Archivierung, Erforschung und Vermittlung stellt, systematisch zu reflektieren und entsprechende Lösungen für einen nachhaltigen Datenlebenszyklus Literatur langfristig umzusetzen.
            
         Für die Archivierung, Analyse und Vermittlung von Digitaler Literatur wird eine Forschungsplattform entwickelt. Da eine solche Plattform nur in der interdisziplinären Zusammenarbeit zu bewerkstelligen ist, sind im Projekt Partner mit unterschiedlichen Expertisen in den einzelnen Teilbereichen vereint, nämlich das Deutsche Literaturarchiv Marbach, das Höchstleistungsrechenzentrum Stuttgart, sowie das Institut für Maschinelle Sprachverarbeitung und die Abteilung Digital Humanities der Universität Stuttgart.
         Die born-digital Bestände des Deutschen Literaturarchivs bestehen zum einen aus digitalen Nachlässen und zum anderen aus archivierten netzliterarischen Werken. Der umfangreichste digitale Nachlass am Deutschen Literatuarchiv ist von Friedrich Kittler und umfasst 1,5 Millionen Dateien. Zur deutschsprachigen Netzliteratur können weitaus weniger Objekte gezählt werden. Netzliteratur ist durch Verlinkungen und Multimedialität geprägt. Das erschwert die Definition von Objektgrenzen und führt zu nichtlinearen Objektstrukturen, die in der Rezeption nichtlineare Handlungen ermöglichen
         Zum einem scheinen sich diese Texte also zur Anwendung computergestützter und computerlinguistischer Methoden besonders anzubieten, da sie genuin in elektronischer Form vorliegen. Zum anderen bringt gerade diese Form für ihre Archivierung und Bereitstellung eine Reihe von besonderen Anforderungen mit sich.
         Digitale Nachlässe sind aufgrund großer Mengen an Daten ohne computergestützte Methoden kaum erschließbar und zugänglich zu machen. Um auf diese wachsende Herausforderung in Archiven und Bibliotheken einzugehen, soll der Einsatz von Methoden der Digital Humanities für die inhaltliche Erschließung textbasierter born-digital Bestände erprobt werden. Wenn digitale Nachlässe bereits obsolete Dateiformate enthalten, sind diese nicht ohne vorherige Formatmigration für aktuelle computergestützte Analysen zugänglich.
         Auch literarische Webseiten sind von den hochfrequenten Erneuerungszyklen digitaler Technik betroffen. Weiterentwicklungen der Betriebssysteme, der Browser, des HTML-Standards und gängiger Webtechnologien können zu fehlerhafter Darstellung oder fehlenden Funktionen einer Webseite führen. Um ein Werk der Netzliteratur dokumentieren zu können, sind daher neue Formen der Modellierung von Texten, die über eine bloß lineare Form hinausgehen, gefragt.
         Diese und weitere Bestände sollen mit modernen digitalen Methoden erschlossen, erforscht und vermittelt werden können. Im Zentrum stehen daher der Aufbau verteilter langzeitverfügbarer Repositories für Digitale Literatur inklusive Forschungsdaten und die Entwicklung der SDC4Lit-Forschungsplattform. Die Repositories werden vom Projekt und seinen Kooperationspartnern regelmäßig erweitert und bilden den zentralen Speicher für das Harvesting von Netzliteratur und weiteren Formen elektronischer Literatur im künftigen Betrieb des SDC. Die Forschungsplattform bietet die Möglichkeit zum computergestützten Arbeiten mit den Beständen der Repositories.
         Bereits entwickelte oder in der Entwicklung befindliche Ansätze zur Archivierung und Bereitstellung von WARC-Archiven (Lin et al. 2017), Textkorpora (Fischer et al. 2019) und Analysefunktionen (Hinrichs et al. 2010) sowie strukturierte Reflexionen eigener Strategien (Kramski, von Bülow 2011) weisen auf eine modulare und integrierte Lösung bei der Bereitstellung von Daten und Services. Die entsprechend geplante modulare Architektur der bereitgestellten Services ermöglicht eine nachhaltige Integration von Repositories und Analysemethoden sowie die Möglichkeit zur späteren bedarfsorientierten Einbindung von Korpora und Analysewerkzeugen.
         Für die Entwicklung des Repositories und der Forschungsplattform ist der Kontakt zu an der Herstellung, Verbreitung, Erforschung und Vermittlung von elektronischer Literatur beteiligten Communities ein entscheidendes Element. Diese Beteiligung wird über einen mehrköpfigen Beirat und Outreach-Maßnahmen wie Workshops, Seminare und die Arbeit mit Fokusgruppen erreicht. Eine wichtige Aufgabe des Projekts ist in diesem Zusammenhang die Modellierung von Formen digitaler Literatur, die zunächst beispielorientiert im Umgang mit einem bereits vorhandenen Corpus digitaler Literatur erfolgt. Daraus entstehen sowohl technische als auch gattungspoetologische Herausforderungen, etwa bei der Begriffsbildung (digitale vs. elektronische Literatur), bei der medienbezogenen Abgrenzung von digitaler und nicht-digitaler und post-digitaler Literatur, und schließlich in Bezug auf gattungspoetologische und literaturgeschichtliche Fragen zur elektronischen Literatur seit den 1950er Jahren mit einem Fokus auf den deutschsprachigen Raum und mit Blick auf internationale Entwicklungen in Literatur und Literaturforschung. (Block, 2004; Gould, 2012; Rettberg, 2019; Seiça, 2015)
         Neben digitalen Objekten und entsprechenden Metadaten wird auch ein Repository der anfallenden Forschungsdaten nachvollziehbar und nachhaltig gespeichert. Zu den Forschungsdaten zählen erstens die bei der Arbeit des SDC anfallenden Forschungsdaten, insbesondere solche, die für das Anbieten von Diensten auf der Plattform notwendig sind, etwa mittels Machine Learning errechnete Datenmodelle für an das Corpus angepasste computerlinguistische Analysewerkzeuge (Eigennamenerkenner, Parser, Topic Models etc.). Zweitens soll das Repository die Möglichkeit bieten, die von Nutzer*innen der Forschungsplattform generierten Forschungsdaten strukturiert zu speichern und für die weitere Forschung zur Verfügung zu stellen, etwa Annotationen oder ergänzte Metadaten zu einzelnen Objekten oder zu Objektklassen.
         Die Sammlung, Bereitstellung, Erforschung und Vermittlung von Literatur im medialen Wandel ist eine Aufgabe, die Forschung und Archive gleichermaßen betrifft. SDC4Lit verfolgt deshalb das Ziel, diese Aufgabe und die entsprechenden Unteraufgaben interdisziplinär zu bearbeiten.
      
      
         
            
      Deutsches Literaturarchiv Marbach: Literatur im Netz, 
               , Zugriff 20.9.2019.
    
         
         
            
               Bibliographie
               
                  
                  Block, Friedrich W.  (2004): 
	p0es1s. Ästhetik digitaler Poesie = The aesthetics of digital poetry. Erscheint anlässlich der Ausstellung "p0es1s. Digitale Poesie" im Kulturforum Potsdamer Platz, Berlin, 13. Februar bis 4. April 2004. Ostfildern: Hatje Cantz.
      
               
                  Gould, Amanda Starling  (2012): „A Bibliographic Overview of Electronic Literature“. In: 
	Electronic Literature Directory  o.V.
      
               
                  Hinrichs, Erhard W., Marie Hinrichs und Thomas	Zastrow  (2010): 
	WebLicht: Web-Based LRT Services for German, Proceedings of the ACL 2010 System Demonstrations, S. 25–29.
      
               
                  Kramski, Heinz Werner, Ulrich von Bülow
	(2011):
	„Es füllt sich der Speicher mit köstlicher Habe“ – Erfahrungen mit digitalen Archivmaterialien im Deutschen Literaturarchiv Marbach, in: Caroline Y. Robertson von Trotha. Robert Hauser (Hg.), Neues Erbe : Aspekte, Perspektiven und Konsequenzen der digitalen Überlieferung, Karlsruhe: KIT Scientific Publishing, S. 141-162.
      
               
                  Rettberg, Scott (2019): Electronic literature. Cambridge, UK: Polity Press.
      
               
                  Seiça, Álvaro (2015): 
	Um Feixe Luminoso: Uma Leitura da Coleção de Literatura Electrônica Portuguesa. Florianópolis: Universidade Federal de Santa Catarina.
      
               
                  Suter, Beat (2012): 
	Von Theo Lutz zur Netzliteratur. Die Entwicklung der deutschsprachigen elektronischen Literatur, , Zugriff am 31.12.2019.
      
               
                  Tomaszek, Patricia (2011): German Net Literature: In the Exile of Invisibility, >, Zugriff am 19.9.2019.

            
         
      
   



      
         
Spätestens mit der Herausbildung des Social Web (auch Web 2.0) seit
knapp 15 Jahren, das nicht nur für die Verteilung von Information,
sondern tatsächlich auch zur Mitgestaltung von Inhalten genutzt werden
kann, hat das Internet die gesellschaftliche Kommunikationskultur
(jedenfalls die derjenigen, die über verlässlichen Zugang verfügen und
diesen nutzen) entscheidend gewandelt. Mit ResearchGate, Academia.edu,
Mendeley und als neue, explizit nicht-kommerzielle Variante 
 HCommons
 entstanden eine Reihe sozialer Medien spezifisch für den
 wissenschaftlichen Bereich, über die Forschungsergebnisse
 ausgetauscht und bewertet werden können und mit denen v.a. der
 Kontakt zu Kolleg|inn|en aufgenommen werden kann (Sugimoto et
 al. 2016). Jenseits dieser spezialisierten sozialen Medien nutzen
 Wissenschaftler|innen auch die allgemeinen Plattformen wie Facebook
 und Twitter, letzteres vor allem, um wissenschaftlichen Diskussionen
 zu folgen, Forschung zu kommentieren und auf eigene
 Veröffentlichungen - von Ergebnissen, jedoch auch von Daten und
 Software - aufmerksam zu machen (vgl. van Noorden 2014). Über die
 allgemein gebräuchlichen sozialen Medien ist es möglich, auch Laien
 zu erreichen, sei es, um die 
eigene Reichweite zu erhöhen oder um neue Nutzerkreise zu gewinnen, die mitunter sogar am Forschungsprozess partizipieren können. Entsprechende Programme wie public engagement
oder Citizen bzw. Crowd
Science sind institutionell erwünscht (vgl. Deutsche Akademie der Technikwissenschaften et al. 2014) und innerhalb der Wissenschaften durchaus verbreitet (vgl. Franzoni & Sauermann 2014).

         
            Die öffentliche Publikation von Forschungsdaten
            
Forschung – nicht zuletzt die in den Geisteswissenschaften – generiert große Mengen an Daten, Information und Wissen, die für (Teil)Öffentlichkeiten interessant und relevant sein können. 
Nun ist die Publikation von Forschungsdaten – zusätzlich zu  den
bisher gebräuchlichen  Publikationsmedien – zwar weithin erwünscht (siehe RFII 2016), zur Zeit allerdings alles andere als weitreichend umgesetzt. 
Dafür können sehr viele unterschiedliche Ursachen ausgemacht werden
(vgl. Kaden 2018). Auf der anderen Seite bieten soziale Medien, hier
vor allem Twitter, die Möglichkeit, granulare  Informationshäppchen fein dosiert in den Timelines von Nutzer|inne|n erscheinen zu lassen und über diesen Weg deren Aufmerksamkeit zu gewinnen. 
Die Nutzung von privatwirtschaftlichen Plattformen, die vorwiegend
monetäre Interessen verfolgen,  für die Wissenschaftskommunikation ist
nicht unproblematisch. Momentan existieren allerdings schlicht keine
nicht-kommerziellen Alternativen Plattformen, über die man auf relativ
simple Weise ein ähnlich großes Publikum erreichen könnte. 

            
  Ein Twitterprojekt, das weitreichende Beachtung fand bis hin zu einem Artikel in der New York Times
, war das Projekt @9nov38 -
heute vor 75 Jahren, in dem fünf Historiker|innen 
die zeitliche Dimension in die Erzählung von Ereignissen der
Reichspogromnacht über Twitter mit einbezogen. Nun ist die manuelle
Erstellung einzelner Tweets sehr aufwendig und für größere Datensätze
eigentlich nicht ohne weiteres zu leisten. Doch im Grunde liegen die
Daten, die für derartige Projekte gesammelt wurden, im Normalfall
bereits in einem strukturierten Format vor, etwa in einer Datenbank
oder als Spreadsheet. Auf dieser Grundlage wurde nach einem Austausch
mit den am @9Nov38-Projekt beteiligten Historiker|innen auf dem Histocamp 2015 der Webservice
autoChirp entwickelt, zunächst im Rahmen eines Projektseminars, seither weiter betreut durch das Institut für Digital Humanities
 (IDH) in Köln (Hermes et al. 2017). autoChirp ist ein Webservice, der nicht auf eine spezifische Anwendung hin entwickelt wurde, sondern eine Plattform bietet, um diversen, u.a. historischen Projekten einen niedrigschwelligen Zugang zu für sie hilfreicher Technologie zu ermöglichen. In dem bewusst einfach gehaltenen Webinterface können strukturierte Daten hochgeladen werden, um sie automatisiert auf spezifizierte Zeitpunkte zu schedulen und zu veröffentlichen. Das erste Projekt, das autoChirp nutzte, war @NRWHistory, bei dem in einem Projektseminar von Düsseldorfer Historiker|innen die Entstehung des Landes NRW um 70 Jahre zeitversetzt nacherzählt wurde 
(siehe ). Kurz darauf wurde über
autoChirp mit @TiwoliChirp ein
weiterer Veröffentlichungskanal für bereits über eine Smartphone-App
veröffentlichten Forschungsergebnisse der Literaturwissenschaft
eingesetzt. 

            
Inzwischen greifen eine ganze Reihe von Projekten, die regelmäßige Tweets publizieren, auf autoChirp zurück. Das mit mehr als 4000 Followern mit Abstand reichweitenstärkste davon ist 
@Die_Reklame, mit
dem Akteure aus dem Projekt  @9Nov38 
bemerkenswerte historische Werbeanzeigen twittern. Von Interesse sind
diese, weil gerade Werbung extrem gegenwartsbezogen ist, was einen
Einblick in die entsprechende Zeit der ursprünglichen Publikation gibt
(vgl. Hoffmann 2018). Ein weiteres Projekt mit historischem Bezug ist
Verbrannte Orte  (@pictureXnet), das die Orte von Bücherverbrennungen im Dritten Reich auf einer Karte sammelt 
(siehe ) und diese an den entsprechenden Jahrestagen der Verbrennungen vertwittert. Die Twitter-Plattform hilft hier dabei, Aufmerksamkeit zu generieren und auch Daten zu den Ereignissen zu sammeln. Einen sehr ähnlichen Ansatz verfolgt das Projekt @gedenkplaetze.

            
  2019 jährte sich zum 250. Mal des Geburtstag von Alexander von Humboldt. In diesem Jahr von besonderem Interesse war daher seine Chronik 
  (siehe ), die von der Berlin-Brandenburgischen Akademie der Wissenschaften (BBAW) herausgegeben wird und inzwischen auch über autoChirp an Twitter angebunden wurde (Hermes 2017). Bemerkenswert hier ist, dass die Chronik unter den Twitter-Account @AvHChrono
  jahrestagsaktuell verfolgt werden kann, was von knapp 200 Leser|innen in Anspruch genommen wird. Diese tagesaktuelle Konsultation der Daten hat auch schon zur Feststellung von Fehlern geführt, die an die BBAW rückgemeldet und anschließend korrigiert wurden. Auch dieses Beispiel zeigt, dass Social Media keine Kommunikation auf der Einbahnstraße sein muss.

         
         
            Die Perspektive der Kunstgeschichte
            Zwei der neuesten autoChirp-nutzenden Projekte kommen aus dem Bereich der Kunstgeschichte, einer bildbasierten Wissenschaft, deren Grundlage historische visuelle Objekte sind. Daher bedeutet die Einführung digitaler Methoden in das Fach vor allem die Entwicklung von Analyseprozessen, die sich auf Bild- und Metadaten beziehen (Klinke 2018). Diese werden nicht nur in der Forschung erzeugt, sondern kommen bisher vor allem aus den Sammlungsinstitutionen (GLAM).
            
               
                  
                   Abbildung 1: Ein Tweet aus dem Fundus des ClevelandFunFacts-Twitterbots
               
            
            Museen sind einer umfangreichen Transformation unterworfen, in der sie ihre Aufgaben unter dem Vorzeichen der Digitalisierung, Social Media und Virtual Reality neu definieren müssen (Kohle 2019). So eröffnet die Publikation der Sammlungsdaten als Open Data neue Möglichkeiten, die kulturellen Artefakte in neue, zeitgenössische Zusammenhänge zu bringen, in denen sie neue Bedeutungszuschreibungen erhalten können. Durch die Verwendung von autoChirp können offene Sammlungsdaten und globale Öffentlichkeit durch das visuelle Medium Twitter zusammengebracht werden. Auch hier erlaubt Twitter nicht nur die Kommunikation in eine Richtung, sondern auch die Partizipation des Publikums durch Kommentare, Retweets und das Einbinden in neue Kontexte.
            
Zwei Beispiele aus dem Jahr 2019 machen dies deutlich: Der Tweetbot @cart_fun_facts
baut auf der Open Data-Strategie des Cleveland Museum of Art auf. Das
1916 gegründete Museum ist eines der umfassendsten Kunstmuseen der
Welt, das am 23. Januar 2019 bekannt gegeben hat, dass es sich ab
sofort als eine Open-Access-Institution betrachtet, die die
Bezeichnung Creative Commons Zero (CC0) für hochauflösende Bilder und
Daten im Zusammenhang mit ihrer Sammlung verwendet (siehe ). 
 Die Öffentlichkeit hat damit jetzt die Möglichkeit, Bilder von mehr als 30.000 gemeinfreien Kunstwerken zu kommerziellen und nichtkommerziellen Zwecke zu teilen, neu zu mischen und wiederzuverwenden. Der von Harald Klinke (LMU München) entwickelte Tweetbot verwendet die in der Datenbank befindlichen “Fun Facts”, die täglich auf Flashcards zusammen mit den Abbildungen der Kunstwerke im Format eines visuellen Memes über den autoChirp-Service getwittert werden 
(siehe Abbildung 1).

            
Ein weiteres Beispiel ist der auf der Digital Art History Summer School 2019 in Malaga (DAHSS) durch Studierende entwickelte Tweetbot 
@thyssenmlgbot. Dieser twittert die Werke des dortigen Museum Carmen Thyssen unter Zuhilfenahme von NLP-Techniken und autoChirp, wodurch die Beschreibungstexte auf relevante Topics untersucht und diese in Hashtags umgewandelt werden. Dieses Projekt hat einerseits gezeigt, wie Studierende mithilfe von digitalen Kompetenzen einer GLAM-Institution helfen können, ihre Werke einer breiteren Öffentlichkeit zu vermitteln. Andererseits, wie diese Vermittlung einen Rückkanal erhalten kann, der es dem Publikum erlaubt, auf die Werke zu reagieren (beispielsweise durch in die Tweets integrierte Frage nach der vermuteten Entstehungszeit des Werks). Auf diese Weise können auch Werke, die üblicherweise nicht in der Ausstellung gezeigt werden, sondern im Depot verbleiben, sichtbar gemacht werden. Ein Online-Tool wie autoChirp ist dafür ein Hilfsmittel, das einen niederschwelligen Zugang zu neuen Formen der digitalen Museumskommunikation ermöglicht und deshalb gerade auch in der Lehre eingesetzt werden kann.

         
         
            autoChirp und autoPost
            
Das IDH betreibt inzwischen neben autoChirp zur automatisierten Veröffentlichung auf Twitter auch autoPost
für analoge Aufträge für Facebook-Seiten. Beide Services basieren auf
Spring, einem quelloffenen Java-Framework für Web-Anwendungen.
Der Quellcode ist unter Open Source-Lizenz (Eclipse
Public Licence) auf GitHub beziehbar (siehe 
 und ),
so dass eigene Services betrieben werden können. Das IDH stellt aber
auch beide Services für alle Interessierten zur Verfügung (siehe 
https://autochirp.spinfo.uni-koeln.de
und ). Bei der Implementation wurde vor allem auf Modularität und Erweiterbarkeit geachtet, um das Programm ohne größeren Aufwand auf weitere Social Media Plattformen, wie z.B. Instagram portieren zu können, sofern diese eine entsprechende API (Application-Programming-Interface) anbieten.

            
               
                  
                   Abbildung 2: Screenshot des autoPost-Services, mit dem große
  Mengen von geplanten Facebook-Posts realisiert  werden können (hier zum Tiwoli-Projekt).
               
            
            
               Bei der Datenpersistenz wurde bei autoPost auf eine schwergewichtigere, aber performantere Datenbank gesetzt, da die Erfahrung mit autoChirp gezeigt hat, dass ein freier Scheduling-Service sehr gut angenommen wird und die Zahl der Datenbankeinträge dementsprechend groß werden kann. Um Nutzer|inne|n von autoChirp die Möglichkeit zu bieten, ihre Inhalte, die in autoChirp schon geplant sind, auch auf Facebook zu veröffentlichen, wurde für autoChirp eine Export-Funktion angelegt. Tweets können gruppenweise als TSV-Datei heruntergeladen und in autoPost als Facebook Posts importiert werden.
                
         
         
            Zwischenfazit zum Nutzerzuspruch
            Während autoChirp schon seit 2016 läuft und für knapp 150 Nutzer|innen-Accounts bereits über 17.500 Tweets veröffentlicht hat (weitere 10.000 Tweets sind terminiert, aktuelle Zahlen erhält man über die 
Statistik-Seitedes
Services), startete autoPost erst im Herbst 2019.
Mit 
                  Syrian Modern History
                und

                  Public History Weekly
                konnten aber bereits zwei wissenschaftlich betreute Accounts mit kombiniert über 36.000 Facebook-Abonnent|inn|en gewonnen werden, die autoPost täglich zur Bewerbung von Archiv-Artikeln nutzen. 

            Die Services autoChirp und autoPost sind Beispiele, an denen sich eine der wichtigen Aufgaben für die Digital Humanities spezifizieren lässt: Die Entwicklung erfolgte, weil Wissenschaftler|innen (nicht nur) aus den Geisteswissenschaften einen Bedarf hatten, ihre Daten auf Social Media Plattformen zu teilen. Dafür benötigten sie Tools, die eine niedrige Einstiegsschwelle haben und ihnen dabei Arbeit abnehmen können, wenn sie Aspekte ihrer Forschung öffentlich sichtbar machen und Studierende sowie die interessierte Öffentlichkeit in den Forschungsprozess (hier zuvorderst: In die Datensammlung) einbinden wollen. Insofern verstehen wir die Entwicklung von autoChirp und autoPost als Hilfsmittel zur Etablierung einer offenen, transparenten und partizipativen Wissenschaft (Open Science). Die Erfahrungen mit den hier vorgestellten Tools zeigt, dass die Methoden sowohl von den Wissenschaftler|inne|n, als auch vom Publikum angenommen werden und mithin das Potenzial haben, den Geisteswissenschaften eine größere Präsenz in der Öffentlichkeit zu ermöglichen und damit eine höhere Relevanz in der Gesellschaft zu erzielen.
         
      
      
         
            
               Bibliographie
               
                  Deutsche Akademie der Technikwissenschaften / Union der Deutschen Akademien der Wissenschaften / Deutsche Akademie der Naturforscher Leopoldina 
  (2014): “Zur 
  Gestaltung der Kommunikation zwischen Wissenschaft, Öffentlichkeit und den Medien. Empfehlungen vor dem Hintergrund aktueller Entwicklungen.” München: acatech – Deutsche Akademie der Technikwissenschaften e.V. / Mainz: Union der Deutschen Akademien der Wissenschaften e.V. / Halle (Saale): Deutsche Akademie der Naturforscher Leopoldina e.V. – Nationale Akademie der Wissenschaften.

               
                  Fischer, Frank / Strötgen, Jannik (2015): 
  „When Does German Literature Take Place? – On the Analysis of Temporal Expressions in Large Corpora”, in: 
  Proceedings of DH2015, Sydney: Alliance of Digital Humanities Organisations.

               
                  Franzoni, Chiara / Sauermann, Henry 
  (2014): “Crowd science: The organization of scientific research in open collaborative projects”, in: 
  Research Policy, Amsterdam: Elsevier Volume 43/1, 1-20.

               
                  Hermes, Jürgen
  (2017): „Neu: Alex von Humboldt auf Twitter!", in 
  TEXperimenTales, 17/08/2017,
  .
  [letzter Zugriff 18.12.2019]

               
                  Hermes, Jürgen / Hoffmann, Moritz / Eide, Øyvind / Geduldig, Alena / Schildkamp, Philip
  (2017): „Twhistory with autoChirp" in: 
  DHd 2017 Bern – Digitale Nachhaltigkeit. Abstractband. Bern: DHd, 277ff.

               
                  Hoffmann, Moritz
  (2018): „Von Funden und Schwellen: Die Reklame“, 
  
  [letzter Zugriff 18.12.2019]

               
                  Kaden, Ben
  (2018): „Warum Forschungsdaten nicht publiziert werden“, in: 
  LIBREAS, Library Ideas  33.

               
                  Klinke, Harald
  (2018): „Daten­analyse in der Digitalen Kunstgeschichte. Neue
  Methoden in Forschung und Lehre und der Einsatz des DHVLab in der
  Lehre”, in: Harald Klinke (Hg.): 
  #DigiCampus. Digitale Forschung und Lehre in den Geisteswissenschaften, München, 2018, S. 19-34.

               
                  Kohle, Hubertus
  (2019): 
  Museen digital. Eine Gedächtnisinstitution sucht den Anschluss an die Zukunft. Heidelberg: Heidelberg University Publishing.

               
                  RfII – Rat für Informationsinfrastrukturen 
  (2016): Leistung aus Vielfalt. Empfehlungen zu Strukturen, Prozessen und Finanzierung des Forschungsdatenmanagements in Deutschland, Göttingen. URL: 
   [letzter Zugriff 18.12.2019]

               
                  Schwarz, Ingo
  (2019): „Zur Alexander von Humboldt-Chronologie”, in Ottmar Ette
  (Hg): 
  edition humboldt digital, hg. v. Berlin-Brandenburgische Akademie der Wissenschaften, Berlin. Version 5 vom 11.09.2019. URL: 
   [letzter Zugriff 18.12.2019]

            
         
      
   



      
         
            Vortragende
            
               David Lassner
               
                  Master Informatik David Lassner, Doktorand an der TU Berlin im Bereich Maschinelles Lernen für Digital Humanities, insbesondere für quantitative Literaturanalyse. 
                  
                  lassner@tu-berlin.de
               
            
            
               Stephanie Brandl
               
                  Dipl. Math. Stephanie Brandl, Technische Universität Berlin. Forschungsschwerpunkte: Maschinelles Lernen, Natural Language Processing. 
                  
                  stephanie.brandl@tu-berlin.de
               
            
            
               Louisa Guy
               
                  Louisa Guy, Doktorandin, Le Mans Université. Forschungsinteressen: Digitale Textanalyse, Anwendung von Methoden der Computerlinguistik auf sozialwissenschaftliche Kontexte. 
                  
                  louisa.guy.etu@univ-lemans.fr
               
            
            
               Anne Baillot
               
                  Prof. Dr. Anne Baillot, Le Mans Université. Forschungsschwerpunkte: Digitale Philologie, Digital Humanities, Translation Studies. 
                  
                  anne.baillot@univ-lemans.fr
               
            
         
         
            Anforderungen
            Maximalanzahl Teilnehmender: 25
             
             Räumliche Anforderungen:
    
                   Beamer
                  Whiteboard/Tafel 
                   Stromversorgung für Laptops der Teilnehmenden
                   Wifi
               
            
             
            
               Anforderungen an die Teilnehmenden:
               Wir erwarten, dass die Teilnehmenden ihre eigenen Laptops mitbringen, die bestenfalls schon die nötige Software vorinstalliert haben. Wir werden kurz vor der Konferenz eine Willkommens-E-Mail mit den Softwareanforderungen verschicken. Die praktischen Sitzungen werden mithilfe von Jupyter Notebooks (Python3, Jupyter) abgehalten. Wir planen zusätzlich als Absicherung einen Online-Zugang zu einem JupyterHub Server mit vorinstallierten Paketen für Teilnehmende, bei denen die Installation Schwierigkeiten macht. Die praktischen Sitzungen sind so konzipiert, dass nur sehr geringe, bis gar keine Programmierkenntnisse notwendig sind. Im Wesentlichen sollen die Teilnehmenden die Parameter und Eingabedaten der vorgegebenen Programme modifizieren, Teilnehmende mit mehr Programmierkenntnissen ermutigen wir natürlich tiefer in die Programme einzusteigen und auch diese zu modifizieren.
            
         
         
            Beschreibung
            Der Workshop besteht aus einem allgemeineren Teil zu Bias im Maschinellen Lernen, in dem grundlegend in die Thematik eingeführt wird, und einem spezifischeren Teil, in dem ML-Biases im Kontext von DH behandelt werden. Beide Teile beinhalten Vortrags- sowie Mitmachsessions. Ziel des Workshops ist es, dass die Teilnehmenden sich des Problems von Bias in Machine Learning Modellen bewusst werden und die grundlegenden Techniken zur Erkennung und zur Unterdrückung von Biases kennenlernen. Es soll außerdem gemeinsam erarbeitet werden, auf welche Weise DH-ForscherInnen mit den Biases umgehen können - denn in vielen Anwendungen sind diese nicht gewünscht: Ein System zur Vorauswahl von Bewerbern sollte Männer nicht bevorzugen, ein Modell zur Gesichtserkennung sollte keinen Unterschied in der Genauigkeit haben, weil sich die Hautfarbe der Personen auf den Bildern ändert (Buolamwini et al. 2018), und ein Modell zur Erkennung von Hate-Speech im Internet sollte nicht kontextfrei bspw. Begriffe wie “homosexuell” als toxisch einstufen.
            
            Gleichzeitig können Biases in ML Modellen erwünscht sein, wenn man beispielsweise die Veränderung von Biases in Sprache analysiert.
            Teilnehmende werden im Vorfeld ermutigt eigene Daten mitzubringen, mit denen sie im zweiten praktischen Teil experimentieren können.
            Das Workshopprogramm wird online unter 
    bias-ml-dh.davidlassner.com öffentlich zur Verfügung gestellt und die Kursmaterialien auf Github unter 
    github.com/millawell/bias-ml-dh veröffentlicht. Dort sollen die Teilnehmenden auch schon im Vorfeld einen Eindruck bekommen, welche ihrer eigenen Daten möglicherweise zum Workshop mitgebracht werden könnten.
    
            
               Zeittafel
               
                  
                     Zeit
                     Titel
                     Vortragende
                  
                  
                     Halbtag 1.1
                     Einleitung, Motivation
                     Anne Baillot, David Lassner
                  
                  
                     Halbtag 1.2
                     Erkennung von Biases in ML
                     David Lassner
                  
                  
                     Kaffepause
                     
                     
                  
                  
                     Halbtag 1.3
                     Verhinderung von Biases in ML
                     Stephanie Brandl
                  
                  
                     Halbtag 1.4
                     Praktische Sitzung 1
                     
                  
                  
                     Halbtag 2.1
                     Autorinnen um 1800
                     Anne Baillot
                  
                  
                     Halbtag 2.2
                     Revolte auf Twitter
                     Louisa Guy
                  
                  
                     Kaffepause
                     
                     
                  
                  
                     Halbtag 2.3
                     Praktische Sitzung 2
                     
                  
                  
                     Halbtag 2.4
                     Abschlussdiskussion
                     
                  
               
            
            
               Erkennung von Biases
               Zu Beginn steht die Begriffsklärung (Datenbias, Modellbias, etc.) und konkrete Beispiele zur Erkundung verschiedener Biases in verschiedenen Datensätzen, sowie Modellarchitekturen. Beispielsweise anhand konkreter Architekturen neuronaler Netze zur Textklassifikation, deren erster Layer ein Embedding-Layer auf Word2Vec-Basis ist (Mikolov et al. 2013).
               Es werden verschiedene Methoden vorgestellt, wie Biases in Modellen und Daten erkannt werden können (Caliskan et al. 2017, May et al. 2019, Garg et al. 2018, Bolukbasi et al. 2016, Swinger et al. 2019).
            
            
               Wie lassen sich Biases verhindern?
               Innerhalb der letzten 3 Jahre wurden zahlreiche Methoden veröffentlicht, die darauf abzielen Biases in Word Embeddings und anderen NLP Anwendungen zu verringern. In diesem Teil wollen wir einen Überblick über die wichtigsten Methoden verschaffen, ihre Stärken und Schwächen aufzeigen und diskutieren.
               Aktuell können diese Methoden in 3 Kategorien eingeteilt werden:
               
                  Manipulation von Datensätzen
                  Datensätze werden so verändert, beispielsweise durch Datenanreicherung, dass Biases im Datensatz nicht mehr zu finden sind und so auch nicht mitgelernt werden. Zum Beispiel schlagen Zhao et al (2018) vor, jeden Satz in einem Datensatz zu kopieren, sodass dieser in mehreren Varianten vorkommt: eine für jedes grammatikalische Geschlecht. So wird eine balancierte Repräsentation zwischen den (binären) Geschlechtern garantiert. Bestehende ML-Methoden die ansonsten biased Ergebnisse erzeugen, können so faire Modelle lernen.
               
               
                  Anpassung der Methode
                  Zhang et al (2018) schlagen vor den Einfluss geschützter demografischer Informationen wie Geschlecht oder Postleitzahl auf das Klassifikationsergebnis mit Adversarial Learning zu verringern. Drei verschiedene Definitionen von „equality“ und Parität werden analysiert und für jeden Definition wird eine entsprechende Strategie vorgestellt um demografische Parität zu sichern.
               
               
                  Zusätzlicher Analyseschritt
                  Bolukbasi et al (2016) zeigen, dass mit Hilfe von Wortlisten ein Unterraum errechnet werden kann, der die geschlechtsbezogene Information in Word Embeddings beinhaltet. Wörter werden mit Hilfe dieser Wortlisten in geschlechtsneutral (z.B. doctor) und geschlechtsspezifisch (z.B. grandmother) eingeteilt. In dem entsprechenden Unterraum werden dann alle Wörter, die grammatikalisch geschlechtsneutral sind, auch neutralisiert, so dass beispielsweise 
    doctor zentriert zwischen den Word Embeddings für “Mann” und “Frau” liegt.
    
                  Allerdings zeigen auch einige dieser Methoden Schwächen und es wurde bereits gezeigt, dass in vielen Fällen Biases weiterhin rekonstruiert werden können (Gonen & Goldberg, 2019). 
               
            
            
               Praktische Sitzung 1
               Im ersten praktischen Teil sollen dann ML Modelle selbst ausprobiert und werden und, anhand von verschiedenen Analysemethoden, Biases explorativ erkundet werden.
               Wir stellen eine fertige ML-Pipeline zur Textklassifizierung zur Verfügung, die mit vortrainierten Word Embeddings arbeitet. Die Klassifizierung soll dahingehend analysiert werden, welche Biases sie enthält. Dann sollen die vortrainierten Word Embeddings mithilfe von Tensorflow Projector erkundet werden und es sollen Richtungen identifiziert werden, die für die Biases in den Ergebnissen verantwortlich sein könnten. Die Teilnehmenden sollen die vortrainierten Word Embeddings auf Grundlage ihrer Erkenntnisse modifizieren und untersuchen, wie sich das Klassifikationsergebnis dadurch ändert.
               Des Weiteren sollen die Biases dieser Pipeline mithilfe von standardisierten Wort-list Tests (SEAT, May et al. 2019 / WEAT, Caliskan et al. 2017) analysiert werden.
               Zuletzt soll den Teilnehmenden auch die Möglichkeit gegeben werden, die Korpuszusammensetzung für das Training der Word Embeddings zu modifizieren und selber trainierte Word Embeddings anstelle der vortrainierten zu verwenden, beispielsweise mithilfe von Sampling, Vereinigung, Mitteln.
            
            
               Erkenntnisgewinn für DH durch Untersuchung von Biases 
               Biases in historischen Textdatensätzen können auf Biases in den Gesellschaften ihrer Entstehung sowie in ihrer Aufbewahrungs- und Tradierungsgeschichte aufdecken. Mit Blick auf die wachsende Wichtigkeit von Cultural Heritage Studies in den Digital Humanities sind diese Art von Biases ein hochaktuelles Forschungsfeld (Garg et al 2018). Der Korpuskonstruktion muss in diesen Fällen allerdings besondere Sorgfalt beigemessen werden, da nur bei einem für die jeweilige Forschungsfrage möglichst ausgewogenen Korpus auch tatsächlich durch die Biases im Korpus auch auf die Biases in der Gesellschaft Rückschlüsse gezogen werden können (Underwood 2019, Bode 2020). Kurz gesagt birgt jeder Schritt in der Geschichte der zu untersuchenden Objekte die Gefahr eines unbewusst und ungewollt induzierten Bias, die der bewussten und gewollten Analyse von Biases im Wege stehen können.
            
            
               Autorinnen um 1800
               Digitale Methoden machen es möglich, das traditionelle Narrativ der Literaturgeschichte zu überdenken und damit Literatur in den Vordergrund zu rücken, die etwa aus Gendergründen im Kanon als zweitrangig überliefert worden war. Zumindest machen sie es theoretisch möglich: Es soll nämlich gezeigt werden, dass digitale Korpora und Methoden die Biases der traditionellen Historiographie auch im literarischen Bereich nur zu leicht reproduzieren und dass die Korpusbildung und der Trainingsprozess einer besonderen Zuspitzung brauchen, um z.B. die Rolle von schreibenden Frauen deutlich machen zu können. Argumentiert wird hier am Beispiel von Autorinnen aus der Zeit um 1800 – der Phase nämlich, wo der (wohl männliche) Autor sich als literarischer, wirtschaftlich tragfähiger Wert etabliert.
            
            
               Tweetanalyse von #aufschrei und #blacklivesmatter
               Auf dem sozialen Netzwerk Twitter führten die Hashtags „aufschrei“ und „blacklivesmatter“ 2013 zu kollektiven Revolten, die online begannen, sich dann aber auch auf den Alltagsdiskurs ausweiteten. Unter #aufschrei berichteten Frauen über ihre Erfahrungen mit Sexismus und unter #blacklivematters ging es um Erlebnisse mit Rassismus. An diesem Beispiel werden Methoden zur Quellenanalyse vorgestellt. Ziel ist es, die Dynamik der digitalen Bewegungen von #aufschrei und #blacklivesmatter anschaulich zu machen.
            
            
               Praktische Sitzung 2 und Abschlussdiskussion
               Im zweiten praktischen Teil sollen die Teilnehmenden ihre eigene Expertise einbringen und in Gruppen individuelle Fragestellungen formulieren, die mithilfe der zuvor kennengelernten Modelle untersucht werden können. Wenn möglich, sollen sofort erste Prototypen entwickelt werden.
               Falls Teilnehmende keine eigenen Korpora bzw. Fragestellungen mitbringen, stellen wir eine ML-Pipeline zur Verfügung, die existierende Systeme zur Erkennung von Hatespeech im Internet auf Tweets mit dem Hashtag #aufschrei bzw. #blacklivesmatter sowie einer Kontrollgruppe aus zufälligen anderen Tweets anwendet. Mithilfe dieser Pipeline sollen Teilnehmende untersuchen, wie Sprache einer neu entstehenden Bewegung, die nicht dem Mainstream entspricht, möglicherweise automatisch als Hatespeech erkannt wird.
            
         
      
      
         
            
      Was tatsächlich bei Amazon passiert ist: 
      https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G
            
            
      Beispiel von https://twitter.com/jessamyn/status/900867154412699649
      bezüglich des www.perspectiveapi.com
      Interfaces, außerdem Davidson et al. (2019)
    
         
         
            
               Bibliographie
               
                  Bode, Katherine (Forthcoming 2020): Why You Can’t Model Away Bias, Modern Language Quarterly 81.1. preprint: katherinebode.files.wordpress.com/2019/08/mlq2019_preprintbode_why.pdf [letzter Zugriff 27. September 2019].
                    
               
                  Bolukbasi, Tolga / Kai-Wei Chang / James Y Zou / Venkatesh Saligrama /  Adam T Kalai (2016): Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Conference of NIPS.
                    
               
                  Buolamwini, Joy / Timnit Gebru (2018): Gender shades: Intersectional accuracy disparities in commercial gender classification. Conference on fairness, accountability and transparency.
                    
               
                  Caliskan, Aylin / Joanna J. Bryson  /  Arvind Narayanan. (2017): Semantics derived automatically from language corpora contain human-like biases. Science 356.
                    
               
                  Davidson, Thomas / Debasmita Bhattacharya  / Ingmar Weber (2019): Racial Bias in Hate Speech and Abusive Language Detection Datasets. arXiv preprint arXiv:1905.12516. 
                    
               
                  Garg, Nikhil /  Londa Schiebinger / Dan Jurafsky  /  James Zou (2018): Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences.
                    
               
                  Gonen, H.  /  Yoav Goldberg (2019): Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them. Conference of the NAACL.
                    
               
                  May, Chandler /  Alex Wang / Shikha Bordia / Samuel R. Bowman  /  Rachel Rudinger (2019): On Measuring Social Biases in Sentence Encoders. Conference of the NAACL.
                    
               
                  Mikolov, T. / Chen, K. /  Corrado, G.  /  Dean, J. (2013): Efficient estimation of word representations in vector space. In ICLR.
                    
               
                  Sap, Maarten / Dallas Card / Saadia Gabriel / Yejin Choi  /  Noah A. Smith (2019): The Risk of Racial Bias in Hate Speech Detection. Conference of the ACL.
                    
               
                  Swinger, Nathaniel / Maria De-Arteaga /  Neil Heffernan IV / Mark Leiserson  /  Adam Kalai (2019): What are the biases in my word embedding?. Conference on Artificial Intelligence, Ethics, and Society (AIES).
                    
               
                  Underwood, Ted (2019). Distant Horizons: Digital Evidence and Literary Change. University of Chicago Press.
                    
               
                  Zhang, B. H. /  Lemoine, B. / Mitchell, M. (2018): Mitigating unwanted biases with adversarial learning. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society.
                    
               
                  Zhao, J. / Wang, T. / Yatskar, M. / Ordonez, V. /  Chang, K. W. (2018): Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876.
                    
            
         
      
   



      
         Obwohl mediävistische Forschung sich bereits seit
	    geraumer Zeit digitaler Methoden bedient und in der
	    Gründungsgeschichte des Humanities Computing, heute als
	    Digital Humanities bekannt, eine herausragende Stellung
	    einnimmt (s. Bleier et al. 2019: 1-12) , spiegelt sich
	    diese real existierende Interdisziplinarität bisher kaum
	    bis gar nicht in den im deutschsprachigen Raum etablierten
	    mediävistischen Studiengängen wider. Dies mag umso mehr
	    verwundern, als die digitale Mediävistik über
	    vergleichsweise klar umgrenzte Fragestellungen,
	    Anwendungsfelder und Vorgehensweisen verfügt; im Gegensatz
	    zu den allgemeiner gehaltenen Digital Humanities, zu denen
	    es einige Lehrstühle und Studiengänge gibt,

		wenngleich lokal wiederum meist mit fachlicher Einschränkung (s. Trier, Leipzig, Stuttgart, Würzburg, Köln, siehe außerdem Sahle 2016) . Themenfelder, denen sich die digitale Mediävistik dezidiert widmet, sind – um nur einige Beispiele zu nennen – computergestützte Analysen von paläographischen Befunden (DigiPal, KPDZ/ CPDA 1-4), historisch-geographische Informationssysteme im mediävistischen Kontext (Mapping Medieval Conflict) und der Einsatz des Maschinellen Sehens zur Mustererkennung in mediävistischen Bildwerken (Computer Vision Lab Heidelberg).
            
         Zu den Herausforderungen einer solch interdisziplinären und im besten Fall auch innovativen Forschung kommen für den wissenschaftlichen Nachwuchs, wie bereits angedeutet, weitere Herausforderungen hinzu: Neben der Frage der Ausbildung stehen hier vor allen Dingen Fragen nach Karriereperspektiven, Anerkennung alternativer Publikationsformen (die Publikation von Forschungsdaten (vgl. Andorfer 2015) , die Debatte um die Vorzüge einer kumulativen Dissertation versus Monographie) und die Einbettung in bestehende fachliche Infrastrukturen im Vordergrund. Das neu gegründete Subcommittee der 
                Digital Medievalist Community, das sich an Early Stage Researcher richtet, hat sich zum Ziel gesetzt, dieser Gruppe eine Plattform zu bieten und den Gesprächsbedarf in einen Dialog mit der größeren Fachgemeinde zu übersetzen.
            
         
            Digital Medievalist ist eine internationale interessenbasierte virtuelle Forschungsgemeinschaft, die mit einem breiten thematischen Zuschnitt über Disziplingrenzen hinweg Wissenschaftler*innen unterschiedlicher Statusgruppen miteinander vernetzt und verschiedene Wege geht, um gemeinschaftlich Spielräume der einzelnen Fachdisziplinen zu erweitern. Die Gemeinschaft wurde bereits 2003 gegründet, seit 2005 ist sie Herausgeberin des gleichnamigen Open Access Journals. Unabhängig von Standorten bietet die 
                Digital Medievalist Community beispielsweise im Rahmen von gemeinsam organisierten Konferenzaktivitäten ein Netzwerk sowohl für etablierte Wissenschaftler*innen als auch für solche am Beginn ihrer wissenschaftlichen Karriere. Digital Medievalist steht allen Interessierten offen, unabhängig bestehender Erfahrungen in den Digital Humanities oder den mediävistischen Disziplinen, von absoluten Neulingen bis hin zu sogenannten Pionieren im Bereich der (digitalen) Mediävistik. 
            
         Spielräume der DM Community sind bisher bereits, 
                in a nutshell:
         
         
            Die 
                    Digital Medievalist Mailingliste : Mehr als 1.300 Abonnenten (Stand September 2019) nutzen diesen Kanal als Diskussionsplattform, um Rat einzuholen und um Informationen jeglicher Art im Bereich der (digitalen) Mediävistik zu teilen.
                
            Das 
                    Digital Medievalist Journal: Verlagsunabhängige (APC freie) Open-Access Fachzeitschrift der Community; die wissenschaftliche Qualität der Artikel wird im Peer-Review-Verfahren gesichert. 
                
            Die 
                    Digital Medievalist Webseite: Die Onlinepräsenz der Community versammelt alle Informationen über die Community: Wie wird man Mitglied? Wie ist die Organisation aufgebaut, wie lautet die Satzung? Darüber hinaus finden sich hier Ankündigungen sowie wie eine stetig aktualisierte Liste von vergangenen und anstehenden Konferenzen, Kolloquien, Workshops und Sommerschulen mit Relevanz für die (digitale) Mediävistik. Im Webblog werden zukünftig neben CfPs und Veranstaltungshinweisen Projekte und Tools aus der digitalen Mediävistik vorgestellt, auf aktuelle Veröffentlichungen verwiesen sowie die Reihe “ What do Digital Medievalists do? ” (Campagnolo 2017) weitergeführt.
                
            
               Digital Medievalist Zotero Bibliographie : Sammlung einschlägiger Literatur zu allen themenbereichen der digitalen Mediävistik.
                
            Die 
                    Digital Medievalist Facebookgruppe mit mehr als 2.500 Mitgliedern sowie ein Twitteraccount @digitalmedieval mit derzeit über 6.000 Followern erweitern den Spielraum der Wissenschaftskommunikation. 
                
         
         Während der Postersession möchten wir die verschiedenen Initiativen der 
                Digital Medievalist Community vorstellen und die Vernetzung innerhalb der deutschsprachigen DH vorantreiben. Hierbei möchten wir vor allen Dingen die geplanten Aktivitäten des neugegründeten Postgraduate Subcommittees skizzieren und einen Peer-to-Peer-Austausch fördern. Das Subcommittee hat es sich zum Ziel gesetzt, einerseits bereits bestehende Aktivitäten wie den Blog auf der Webseite oder die Präsenz der Community auf Twitter zu beleben und andererseits ab 2020 neue eigene Aktivitäten in Angriff zu nehmen; hierzu zählen insbesondere die Organisation von gemeinsamen Panels (so etwa auf dem International Medieval Congress in Leeds) und die Produktion von einem Podcast, in dem Nachwuchsforscher zu Wort kommen sollen. Die Erhöhung der Sichtbarkeit der existenten Infrastrukturen soll außerdem Denkanstöße für eine Diskussion um interdisziplinäres Arbeiten, erforderliche Skills und eine Reform der universitären Curricula im mediävistischen Kontext liefern und damit auch in übergreifender Perspektive beispielhaft zu aktuellen Debatten um die Profilierung geisteswissenschaftlicher Disziplinen zwischen Tradition und gegenwärtigen Anforderungen beitragen.
            
      
      
         
            
	      Für ein ausführliches Verzeichnis der DH Studiengänge
	      siehe https://registries.clarin-dariah.eu/courses/
    
         
         
            
               Bibliographie
               
                  Andorfer, Peter (2015):
  „Forschungsdaten in den (digitalen) Geisteswissenschaften: Versuch
  einer Konkretisierung“, Göttingen: GOEDOC (DARIAH-DE working papers
  14)
  http://nbn-resolving.de/urn:nbn:de:gbv:7-dariah-2015-7-2
               
               
                  Bleier, Roman / Fischer, Franz / Hiltmann, Torsten /
  Viehhauser, Gabriel / Vogeler, Georg (2019):
  „Digitale Mediävistik und der deutschsprachige Raum“ in: Das
  Mittelalter 24. 1 : 1-12 DOI:
  
               
               
                  Campagnolo, Alberto (2017): „What do digital
  medievalists do?“
  https://digitalmedievalist.wordpress.com/2017/08/10/what-do-digital-medievalists-do/
               
               
                  Computer Vision Lab Uni Heidelberg:
  https://hciweb.iwr.uni-heidelberg.de/compvis/
               
               
                  DigiPal (2011-2014): „Digital Resource and
  Database of Palaeography, Manuscript Studies and Diplomatic“
  
               
               
                  Digital Medievalist Webseite: 
  https://digitalmedievalist.wordpress.com/
               
               
                  Digital Medievalist Journal:
  https://journal.digitalmedievalist.org/
               
               
                  Digital Medievalist Mailingliste: 
  https://digitalmedievalist.wordpress.com/mailing-list/
               
               
                  Digital Medievalist on Facebook:
  https://www.facebook.com/groups/49320313760/
               
               
                  Digital Medievalist on Twitter:
  https://twitter.com/digitalmedieval
               
               
                  Digital Medievalist on Zotero:
  https://www.zotero.org/groups/2138266/digitalmedievalist
               
               
                  Kodikologie und Paläographie im digitalen
  Zeitalter 1-4 / Codicology and Palaeography in the Digital Age
  1-4 (2009, 2010, 2015, 2017):
  Herausgegeben vom Institut für Dokumentologie und Editorik,
  Norderstedt: BoD.
  https://www.i-d-e.de/publikationen/schriften/
               
               
                  Mapping Medieval Conflict:
 https://www.i-d-e.de/publikationen/schriften/
               
               
                  Sahle, Patrick (2016) : “Zur
  Professoralisierung der Digital Humanities” in: DHdBlog 23. März
  2016
  
               
               
Alle angegeben Links wurden am 4. Januar 2020 geprüft.
            
         
      
   



      
         
            Einleitung
            
  Täglich werden auf der ganzen Welt Onlineartikel, Blogbeiträge etc. veröffentlicht, zu denen Leserinnen und Leser (i.F. generisches Femininum) Kommentare verfassen. Aufgrund der hohen Anzahl an Partizipierenden gelten Nutzerbeiträge als besonders authentische Echtzeitrückmeldungen und erlauben einen Zugang zu heterogenen Meinungsäußerungen (Busch, 2017). Auch durch Partizipation auf Social Media Plattformen, in Onlineforen und öffentlichen Chats werden Daten generiert, die wertvolle Informationen über Nutzerverhalten und menschliches Denken beinhalten. Dies gilt umso mehr, als diese Plattformen Orte sind, an denen Menschen miteinander in Verbindung treten, in verschiedenen Formen und Dimensionen Gemeinschaft pflegen, Informationen verbreiten und ihre Meinungen austauschen. Dabei generierte Daten zeichnen sich durch ihren interaktiven, kontemporären und personenbezogenen Charakter aus und ermöglichen folglich Rückschlüsse auf Meinungen, Interessen und Stimmungen in der Bevölkerung. Entsprechend sind sie von besonderem Interesse für private Unternehmen oder öffentliche Institutionen (Schoen, 2002; Holtz-Bacha, 2019: 276). Zunehmend wird auch im akademischen Kontext auf Nutzerdaten zurückgegriffen (u.a. Mohammad, 2016; Aker et al., 2016). 
    
         
         
            Forschungsgegenstand
            

Gegenstand des vorzustellenden Projektes ist eine Pilotstudie, in der zwei Masterthesisprojekte in Beziehung zueinander gesetzt wurden. Bei dem ersten Thesisprojekt (Guhr, 2019) handelt es sich um eine im Bereich der Digital Humanities durchgeführte computergestützte Analyse von Leserkommentaren in französischen Onlinemedien. Das zweite Thesisprojekt aus dem Bereich des IT-/Datenschutzrechts betrachtet ethische und rechtliche Erwägungen bei der Analyse von nutzergenerierten Inhalten auf Social Media Plattformen und die Frage, wie deren Berücksichtigung in wissenschaftlichen Datenanalyseprojekten unterstützt werden kann. Infolge der Auseinandersetzung mit dem jeweils anderen Masterthesisprojekt entstand ein interdisziplinäres Streitgespräch zwischen den Autorinnen. 
    
            
      Die Masterthesis (Guhr, 2019) umfasst u.a. verschiedene gemischt qualitativ-quantitative Analysen von Onlinezeitungsartikeln und zugehörigen Leserkommentaren zur französischen Präsidentschaftswahl 2017. Die Daten sind über den Onlineauftritt einer großen französischen Tageszeitung öffentlich zugänglich. 40 ausgewählte Onlineartikel mit den dazugehörigen 3.127 Leserkommentaren wurden zu einem Korpus zusammengestellt. Die extrahierten Leserkommentardaten beinhalteten zusätzlich zu den nutzergenerierten Beitragstexten auch Datum und Uhrzeit der Beitragserstellung sowie die Nicknames und teilweise bürgerliche Namen der Nutzerinnen. Mithilfe von Distant Reading Methoden wurden im Korpus behandelte Themen identifiziert. Anschließend wurde eine automatisierte Sentimentanalyse der Kommentare durchgeführt, um Informationen über die emotionale Einstellung der Nutzerinnen zu Wahlkampfthemen und zum/zur Präsidentschaftskandidat/in herausstellen zu können.
    
            
      Der zweiten Masterthesis (Brokering, 2019) lag die Frage zugrunde, wie Datenanalyseprojekte im akademischen Kontext rechtskonform und ethischer gestaltet werden können. Am Beispiel von Forschung mit Social Media Daten wurde herausgestellt, wie durch Analysen von nutzergenerierten Inhalten Interessen und Rechte der Nutzerinnen berührt werden. Für die hierdurch aufgeworfenen, neuen ethischen und besonders datenschutzrechtlichen Fragestellungen fehlt es bestehenden inhaltlichen und institutionellen Ansätzen der Forschungsethik noch an befriedigenden Antworten, die eine ethische Praxis von Social Media Datenanalysen gewährleisten. Daraufhin wurde evaluiert, inwiefern das IT-rechtliche Konzept des
      Regulation by Design
      eine effektivere Implementierung ethischer und rechtlicher Erwägungen in Social Media Datenanalysen unterstützen kann. 
      Regulation by Design
      zielt auf eine proaktive Berücksichtigung regulatorischer Erwägungen bereits im Zeitpunkt des Designs, d.h. der Planung und Entwicklung, von Produkten und Aktivitäten wie auch Forschung. Es findet seine bekannteste Ausprägung im datenschutzrechtlichen Prinzip des 
      Privacy by Design.
    
         
         
            Interdisziplinäres Streitgespräch
            

Im Dialog der Autorinnen trafen die Perspektiven der praxisorientierten und der juristischen Forschung aufeinander. Aus letzterer wurde Kritik am Umgang mit persönlichen Informationen geäußert und für eine höhere Sensibilität gegenüber den Interessen der Nutzerinnen und insbesondere datenschutzrechtlichen Erwägungen plädiert. Sobald Social Media Daten eine Identifizierbarkeit der postenden Personen auch nur ermöglichen, z.B. weil sie die Nutzernamen oder auch die IP-Adresse der Nutzerin enthalten, handelt es sich um persönliche Daten und damit finden datenschutzrechtliche Vorgaben wie die europäische DSGVO Anwendung. Diese erfordert typischerweise die Information der betroffenen Nutzerin über die konkrete Verwertung ihrer Daten und die Einwilligung in diese. Die Wirksamkeit einer mittels der AGB des jeweiligen Social Media Anbieters erteilten Einwilligung ist als zweifelhaft zu bewerten, da sie nicht projektspezifisch ist. Angesichts des Umstands, dass die verwendeten Nutzerdaten zu Forschungszwecken umgewidmet werden und originär im Rahmen der privaten Nutzung von Social Media Diensten entstanden sind, ist in Erwägung zu ziehen, ob über das datenschutzrechtlich erforderliche Mindestmaß hinausgehende Maßnahmen zum Schutz der Interessen der Nutzerinnen ethisch geboten sind. Auch eine Anonymisierung der Nutzerdaten, z.B. durch Entfernen des Nutzernamens kann das Re-Identifikationsrisiko angesichts fortschrittlicher De-Anonymisierungstechniken nur reduzieren. Hier sind weitergehende u.a. auch im Verhältnis zum Grad an Sensibilität der betroffenen Nutzerinhalte angemessene Anonymisierungsmaßnahmen in Erwägung zu ziehen. Gleichzeitig ist zu berücksichtigen, dass Nutzerinnen an ihren originell und kreativ gestalteten Beiträgen auch urheberrechtliche Interessen und Rechte haben, sodass andererseits eine Erkennbarkeit der Autorin durch die Forschenden sicherzustellen sein kann. Die praxisorientierte Forscherin wies demgegenüber auf die schwierige Umsetzbarkeit aufwendiger Maßnahmen zum Schutz der Nutzerinnen angesichts begrenzter finanzieller, technischer und zeitlicher Spielräume in der Forschungspraxis hin sowie auf die Gefahr, dass Forschungsdaten durch Datenschutzmaßnahmen an Wert/Aussagekraft verlieren würden. Als Beispiele nannte sie den Wert von Nutzernamen als potenzielle Informationsquelle hinsichtlich Gender und Nationalität sowie für die kumulative Betrachtung verschiedener Beiträge einer Person. Auch die Erhebung von Datum und Uhrzeit der Beitragserstellung ermögliche eine chronologische Ordnung von Beiträgen. Dabei kritisierte die Juristin, dass bereits aus derartigen Informationen umfangreiche Aktivitätsprofile über einzelne Nutzerinnen erstellt werden könnten, die ggf. in Verbindung mit Nutzungsdaten derselben Nutzerinnen auf weiteren Social Media Plattformen Rückschlüsse auf Tagesabläufe, Vorlieben und Social Media Verhalten einzelner Nutzerinnen erlauben. Im daraus resultierenden Streitgespräch wurde erkennbar, wie schwierig es ist, die jeweiligen Positionen in einer der anderen Forschenden verständlichen Weise zu kommunizieren. 
    
            
      Weiteres Vorgehen im Projekt war es, die rechtlich-ethischen Herausforderungen gemeinsam zu definieren, wobei die Perspektiven beider Forschungsrichtungen Beachtung finden sollten. Auf dieser gemeinsamen Grundlage und unter Berücksichtigung verschiedener Ansätze des 
      Regulation by Design-Konzeptes wurden Methoden und Herangehensweisen diskutiert, die eine effektive Berücksichtigung der Herausforderungen in der Forschungspraxis erreichen sollen.
    
            
Das Projekt verfolgt damit das Ziel, den Dialog zwischen datenbasierter Forschung und IT-Recht anzuregen und insbesondere das Bewusstsein für Nutzerinteressen und Datenschutzerwägungen unter Forschenden zu erhöhen. Es soll reflektiert werden, wie die Kommunikation zwischen IT-Rechtlerinnen und Datenforschenden unter Berücksichtigung der unterschiedlichen Perspektiven, Interessen und Limitierungen verbessert werden kann. Die gemeinsamen Definitionen der Herausforderungen und die diskutierten Lösungsvorschläge sollen Datenforschenden ermöglichen, ihre Forschungsarbeit ohne größeren Mehraufwand bereits im Stadium der Vorbereitung und Durchführung von Datenanalysen rechtskonform und ethisch sensibel zu gestalten. 
    
         
      
      
         
            
               Bibliographie
               
                  Aker, Ahmet / Paramita, Monica / Kurtic, Emina / Funk, Adam / Barker, Emma
                   (2016): „Automatic label generation for news comment clusters“, in:
                  Proceedings of the 9th International Natural Language Generation Conference
                  , 
                  Edinburgh, UK:  61–69
	https://pdfs.semanticscholar.org/4da7/ac02c56d43312425a854d63e71f89dd288ec.pdf [letzter Zugriff 26. Juni 2019].
      
               
                  Brokering, Annalena
                   (2019): 
                  Drawing from approaches in regulatory theory for the regulation of new technologies and design theory, how can ethical considerations be effectively incorporated into data science activities?.
	Masterthesis, The University of Edinburgh, Edinburgh Law School.
      
               
                  Buchanan, Elizabeth / Zimmer, Michael
                   (2016): „Internet Research Ethics“, in: 
                  Stanford Encyclopedia of Philosophy
                   https://plato.stanford.edu/entries/ethics-internet-research/ [letzter Zugriff 19. August 2019].
               
               
                  Busch, Andreas
	(2017): Informationsinflation: Herausforderungen an die politische Willensbildung in der digitalen Gesellschaft, in: Gapski, Harald / Oberle, Monika / Staufer, Walter (eds.): 
	Medienkompetenz. Herausforderungen für Politik, politische Bildung und Medienbildung, Schriftenreihe der Bundeszentrale für Politische Bildung.
	Bonn: Bundeszentrale für Politische Bildung 53-62
	http://www.bpb.de/lernen/digitale-bildung/medienpaedagogik/medienkompetenz-schriftenreihe/257594/informationsinflation
	[letzter Zugriff 11. Juni 2019].
      
               
                  Dashtipour, Kia / Poria, Soujanya / Hussain, Amir / Cambria, Erik / Hawalah, Ahmad Y. A. / Gelbukh, Alexander / Zhou, Qiang
                   (2016): „Multilingual Sentiment Analysis: State of the Art and Independent Comparison of Techniques“, in:
                  Cognitive Computation
	(2016) 8: 757-771 https://link.springer.com/article/10.1007/s12559-016-9415-7 [letzter Zugriff 26. Juni 2019].
      
               
                  Golder, Susan P. / Ahmed, Shahd / Norman, Gill / Booth, Andrew
                   (2017): „Attitudes Toward the Ethics of Research Using Social Media: A Systematic Review“, in: 
                  Journal of Medical Internet Research 19 http://eprints.whiterose.ac.uk/117721/ [letzter Zugriff 26. September 2019].
      
               
                  Guhr, Svenja
                   (2019): 
                  Computergestützte Analyse von französischen Onlinemedien zur Präsidentschaftswahl 2017, Masterthesis, Georg-August-Universität Göttingen.
      
               
                  Holtz-Bacha, Christina
                   (2019): „Demoskopie - Medien - Politik. Umfragen im Bundestagswahlkampf 2017“, in: Holtz-Bacha, Christina: 
                  Die (Massen-)Medien im Wahlkampf. Die Bundestagswahl 2017.
	Wiesbaden: Springer Fachmedien Wiesbaden GmbH 263-280.
      
               
                  Locatelli, Elisabetta
                   (2018): „Ethics of Social Media Research: State of the Debate and Future Challenges“, in: Hunsinger, Jeremy / Klastrup, Lisbeth / Allen, Matthew M. (eds.): 
                  Second International Handbook of Internet Research.
	Dordrecht: Springer 1-22.
      
               
                  McKee, Heidi / Porter, James E.
                   (2008): „The Ethics of Digital Writing Research: A Rhetorical Approach“, 
                  College Composition and Communication
	59: 711 http://wrconf08.writing.ucsb.edu/Pdf_Articles/McKee_Article.pdf [letzter Zugriff 26. September 2019].
      
               
                  Mohammad, Saif
                   (2016): „A Practical Guide to Sentiment Annotation: Challenges and Solutions“, in: 
                  Proceedings of the NAACL 2016 Workshop on Computational Approaches to Subjectivity, Sentiment, and Social Media (WASSA) 
	174-179 http://www.aclweb.org/anthology/W16-0429 [letzter Zugriff 16. Mai 2019].
      
               
                  Moreno, Megan A. / Goniu, Natalie / Moreno, Peter S. / Diekema, Douglas
                   (2013): „Ethics of Social Media Research: Common Concerns and Practical Considerations“, in:
                  Cyberpsychology, Behavior, and Social
	Networking 16: 708-713 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3942703/ [letzter Zugriff 16. Mai 2019].
      
               
                  Perez Vallejos, Elvira / Koene, Ansgar / Carter, Christopher J. / Hunt, Daniel / Woodard, Christopher / Urquhart, Lachlan / Bergin, Aislinn / Statche, Ramona
                   (2019): „Accessing Online Data for Youth Mental Health Research: Meeting the Ethical Challenges“, in:
                  Philosophy & Technology
	32: 87-110 https://link.springer.com/article/10.1007/s13347-017-0286-y [letzter Zugriff 26. September 2019].
      
               
                  Schoen, Harald
                   (2002): „Wirkung von Wahlprognosen auf Wahlen“ in: Berg, Thomas (ed.) (2002): 
                  Moderner Wahlkampf
                  . 
                  Blick hinter die Kulissen.
                   Opladen: Leske und Budrich 171-191.
               
               
                  Williams, Matthew L. / Burnap, Pete / Sloan, Luke
                   (2017): „Towards an Ethical Framework for Publishing Twitter Data in Social Research: Taking into Account Users’ Views, Online Context and Algorithmic Estimation“, in:
                  Sociology
	51: 1149-1168 https://journals.sagepub.com/doi/pdf/10.1177/0038038517708140 [letzter Zugriff 26. September 2019].
      
            
         
      
   



      
         Die vorliegende Arbeit versucht, im Rahmen einer empirisch fundierten Diskursanalyse von Texten sozialer Medien eine Brücke zwischen qualitativ-hermeneutischer Kulturwissenschaft (hier: Literatur- und Politikwissenschaft) und quantitativ-komputationeller digitaler Geisteswissenschaft zu bauen und beide Methodenlinien synergetisch miteinander zu verschränken. In diesem erweiterten Abstract beschreiben wir einen neuen Datensatz von Twitter-Beiträgen deutscher Parlamentarier des 19. Deutschen Bundestags als Datengrundlage der Diskursanalyse und erste Teilergebnisse, die aus der Analyse dieses Datensatzes resultieren. Ein Fixpunkt dieses Vorgehens ist das historisch markierte Epochenkonstrukt der Romantik in seiner literarischen und sozialen Ausformung (Lebensform, Wertekanon usw.) und seine (Wieder-)Aufnahme bzw. Adaption im aktuellen parteipolitischen Diskurs in Deutschland. 
         Ausgangspunkt unserer Arbeiten waren Beobachtungen, die einen Bezug zwischen rechtspopulistischen Parteien und Symbolen der deutschen Romantik nahelegten. Während der AfD-Politiker Björn Höcke von seinem Parteikollegen beispielsweise als „romantischer Nationalist“ bezeichnet wurde, trug sein Parteigenosse Andreas Wild bei einem Auftritt im Bundestag eine blaue Kornblume an seinem Revers. Diese Blume, ein zentrales Symbol der Romantik, wurde in den 1930er Jahren sogar zu einem Erkennungszeichen der illegalen Nationalsozialisten in Österreich. Die semantische Doppelbesetzung der blauen Kornblume eröffnet folglich rechtspopulistischen Politikern einen diskursiven Spielraum, sich einerseits implizit an den Nationalsozialismus anzulehnen, andererseits diese Identifikation in der Öffentlichkeit nicht eindeutig zum Ausdruck bringen zu müssen. 
         Um diese Einzelbeobachtungen systematischer einordnen und die Hypothese von der auffälligen Verwendung von Konzepten der Romantik-Epoche im Diskursverhalten einer rechtspopulistischen Partei einer strengeren Prüfung unterziehen zu können, entwickelten wir ein Korpus von Twitter-Beiträgen aller Abgeordneten des (aktuellen) 19. Bundestags (es kann damit als Ergänzung der Redenkorpora des Bundestags von Barbaresi (2018) bzw. Blätte & Blessing (2018) betrachtet werden, die aber auch frühere Legislaturperioden umfassen). Dieses Korpus sollte Grundlage für eine computerlinguistische Diskursanalyse zur Prüfung der Hypothese sein (einen ähnlichen Studienansatz zur Überprüfung sprachlich markierter Stereotypen zwischen politischen Parteien beschreiben Sylwester & Purver (2015)).
         
            Korpus: Für unsere Untersuchung haben wir
DeBAC (
            Deutscher Bundestags
            Abgeordnete-
            Corpus), das nach unserem Kenntnisstand erste Twitter-Korpus deutscher Bundestags-abgeordneter für die laufende 19. Legislaturperiode, aufgebaut. Es umfasst zum Zeitpunkt der Abfassung dieses Abstracts (Januar 2020) 887.008 Tweets von 478 Parlamentariern über einen Zeitraum vom 21.11.2008 bis 2.1.2020; dieses Korpus wird fortlaufend aktualisiert. Es umfasst 
                alle im Bundestag vertretenen Parteien sowie parteilose Abgeordnete.
            
         Da dieser Datensatz natürlich nicht nur für Fragestellungen im Romantik-Kontext, sondern für die deutschsprachige politische Diskursanalyse generell wertvoll sein kann, stellen wir es der Fachöffentlichkeit zur Verfügung (
                https://github.com/JULIELab/DeBAC). Aus rechtlichen Gründen distribuieren wir dabei nur die Tweet-IDs und dazugehörigen Meta-Daten (u.a. Autor, Erstellungszeitpunkt und Parteizugehörigkeit), während die Rohtexte über ein ebenfalls mitgeliefertes Skript heruntergeladen werden können. 
            
         
            Analytik: Im ersten Anlauf suchten wir nach
Stichwörtern, die Romantik-Konzepte indizieren. Hierzu wurde eine
explorative Umfrage unter mehreren Literaturwissenschaftlern (allesamt
Mitglieder des Graduiertenkollegs „Modell Romantik“ an der
Friedrich-Schiller-Universität Jena)
durchgeführt, um gebräuchliche lexikalische Signale für diese Epoche
zu bestimmen. Dabei stellte sich heraus, dass nicht nur direkte
Lexikalisierungen wie
„
            Romantik“,
„
            Romantiker“,
„
            romantisch“
romantikrelevant sind, sondern auch solche wie
„
            Gemeinschaft“,
„
            Wesen“,
„
            Glauben“,
„
            Heimat“ (man denke an Friedrich Schlegels Über den Republikanismus, Novalis' 
                Glauben und Liebe usw.). Das Suchergebnis wurde sowohl quantitativ analysiert als auch qualitativ interpretiert. Die folgende Tabelle zeigt die Häufigkeiten von Tweets mit diesen Stichwörtern und ihre Zuordnung zu Parteien:
            
         
            Tabelle 1: Häufigkeit der Stichwörter mit Romantikbezug, gruppiert nach Parteien im Bundestag. Tweets der insgesamt vier fraktionslosen Abgeordneten (mit sehr niedrigen Belegzahlen) sind zur Übersichtlichkeit nicht aufgeführt
            
               Suchwort (Regulärer Ausdruck)
               
                  CDU/CSU
               
               SPD
               AfD
               FDP
               LINKE
               GRÜNE
               S
            
            
               
                  /[Rr]omantik/
               
               
                  29
               
               14
               7
               11
               20
               15
               96
            
            
               
                  /[Rr]omantisch/
               
               9
               10
               7
               13
               2
               3
               44
            
            
               
                  /[Rr]omantisier/
               
               1
               2
               2
               5
               0
               4
               14
            
            
               
                  /[Gg]lauben/
               
               
                  375
               
               298
               350
               252
               198
               277
               1750
            
            
               
                  /[Gg]emeinschaft/
               
               424
               399
               104
               234
               
                  260
               
               343
               1764
            
            
               
                  /[Ww]esen/
               
               925
               844
               504
               700
               688
               835
               4496
            
            
               
                  /[Hh]eimat/
               
               
                  1504
               
               941
               562
               312
               314
               639
               4272
            
            
               Insgesamt
               3267
               2508
               1536
               1527
               1478
               2116
               12436
            
         
         Die Tabelle zeigt, dass die direkten Lexikalisierungen „
                Romantik“, „
                Romantiker“ und „
                romantisch“ vergleichsweise selten vorkommen und wenn, dann verweisen sie meist auf eine Lesart im Sinne von „
                realitätsfern“, z.B.:
            
         #Grüne und #Linke wollen, dass #Karlsruhe die Patenschaft für ein Seenotrettungsschiff einer Nichtregierungsorganisation (NGO) im Mittelmeer übernimmt. Eine romantische, realitätsferne Weltsicht.
                (https://twitter.com/MarcBernhardAfD/status/1062048613923201026)
            
         Dagegen kommen indirektere Lexeme wie „
                Gemeinschaft“ und „
                Heimat“ weitaus häufiger vor und werden im Sinne eines abgrenzenden und ausschließenden Charakters eingesetzt, z.B.: 
            
         
            Feste, Feiern, Schwimmbäder: Der Verlust öffentlicher Orte und von Gemeinschaftserlebnissen. Nicht alle haben private Pools. 
                https://t.co/jZsxnmFjCP(https://twitter.com/Renner_AfD/status/1155441711105134592)
            
         #Bayern gibt Unsummen für illegale Migranten aus. Geld, das vielen älteren Menschen fehlt, die Jahrzehnte für unsere Heimat und unsere Gesellschaft hart gearbeitet haben. Schützen Sie unser Sozialsystem gegen Armutseinwanderung und geben wir den Rentnern mehr. #AfD zur #LtwBayern 
                https://t.co/0imAQg3oCj(https://twitter.com/ProfMaier/status/1044102746411073536)
            
         Diese überwiegend qualitative inhaltsanalytische Vorgehensweise haben wir anschließend durch eine einfache quantitative Untersuchung im Rahmen einer automatischen Emotionsanalyse ergänzt (s.a. entsprechende Vorarbeiten von Hellrich et al. (2019) bzw. Buechel et al. (2017). Hierzu haben wir sämtliche Tweets unseres Korpus mithilfe des Software-Werkzeugs JEmAS (Buechel & Hahn 2016) analysiert und ihnen so einen emotionalen Stimmungswert anhand der darin vorkommenden Lexeme zugewiesen. 
         Dieses Verfahren liefert für relativ häufige Wörter intuitiv
plausible Ergebnisse. Das Lexem
„
            Heimat“, das in insgesamt 4.325
Tweets vorkommt, wird etwa von CDU und CSU am
positivsten verwendet und von Der Linken am wenigsten
(aber immer noch) positiv. Demgegenüber mussten wir
feststellen, dass für unsere Ausgangsforschungsfrage
zentrale Begriffe (
„
            Romantik“,
„
            romantisch“,
„
            romantisieren“) in unserem derzeitigen Korpus zu selten vorkommen, um damit auf Grundlage von reinen Worthäufigkeiten zuverlässige Daten erheben zu können. Eine sinnvolle Erweiterung unserer bisherigen Arbeiten besteht daher in der Anwendung fortgeschrittenerer komputationaler Modelle zur Emotionserkennung, die etwa auf Deep Learning (Nay 2016) oder Topic Modeling (Nguyen et al., 2015) beruhen. Unsere Studie ist damit dem weiteren Kontext der Meinungsklima- und Emotionsanalytik im Umfeld parlamentarischer politischer Akteure zuzuordnen (vgl. a. Abercrombie & Batista-Navarro 2018, Green & Larasati 2018, Blätte 2018, van der Zwaan et al. 2016, Rheault et al. 2016, Nguyen et al. 2015, Zirn 2014, Lietz et al. 2014), ein aktueller Schwerpunkt im zur Zeit stark expandierenden Bereich 
                Computational Social Science.
            
         
            Danksagung. Tinghui Duan ist Doktorand des Graduiertenkollegs „Modell Romantik“, das von der DFG unter Fördernummer GRK 2041 gefördert wird; Sven Buechel ist Mitarbeiter eines unter der Förderlinie „Big Data in der makrooökonomischen Analyse“ (Fachlos 2; GZ 23305/003#002) geförderten Projekts des Bundesministeriums für Wirtschaft; Udo Hahn ist PI in beiden Projekten. Die Autoren bedanken sich bei den zwei anonymen Gutachtern für Ihre kritische Anmerkungen und bei Christof Schöch für seine verständnisvolle Kommunikation.
            
      
      
         
            
               http://modellromantik.uni-jena.de/
            
         
         
            
               Bibliographie
               
                  Abercrombie, Gavin / Batista-Navarro, Riza T. (2018): "Identifying opinion-topics and polarity of parliamentary debate motions", in: 
                        WASSA 2018 – Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis @ EMNLP 2018 280-285.
                    
               
                  Barbaresi, Adrien (2018): "A corpus of German political speeches from the 21st century", in: 
                        LREC 2018 – Proceedings of the 11
                  th
                  International Conference on Language Resources and Evaluation 792-797.
                    
               
                  Blätte, Andreas (2018): "Zum Verwechseln ähnlich? Eine Klassifikationsanalyse parlamentarischen Diskursverhaltens auf Basis des PolMine-Plenarprotokollkorpus", in: 
                        Computational Social Science. Die Analyse von Big Data, Nomos 139-162.
                    
               
                  Blätte, Andreas / Blessing, André (2018): "The GermaParl corpus of parliamentary protocols", in: 
                        LREC 2018 – Proceedings of the 11
                  th
                  International Conference on Language Resources and Evaluation 810-816.
                    
               
                  Buechel, Sven / Hahn, Udo (2016): "Emotion analysis as a regression problem: dimensional models and their implications on emotion representation and metrical evaluation", in: 
                        ECAI 2016 – Proceedings of the 22
                  nd
                   European Conference on Artificial Intelligence 1114-1122.
                    
               
                  Buechel, Sven / Hellrich, Johannes / Hahn, Udo (2017): "The course of emotion in three centuries of German text: a methodological framework", in: 
                        dh 2017 – Digital Humanities 2017: Conference Abstracts of the 2017 Conference of the Alliance of Digital Humanities Organizations (ADHO).
                    
               
                  Green, Nathan / Larasati, Septina (2018): "The first 100 days: a corpus of political agendas on Twitter", in: 
                        LREC 2018 – Proceedings of the 11
                  th
                  International Conference on Language Resources and Evaluation 2785-2789.
                    
               
                  Hellrich, Johannes / Buechel, Sven / Hahn, Udo (2019): "Modeling word emotion in historical language: quantity beats supposed stability in seed word selection", in: 
                        LaTeCH-CLfL 2019 – Proceedings of the 3
                  rd
                  Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature @ NAACL-HLT 2019 1-11.
                    
               
                  Lietz, Haiko / Wagner, Claudia / Bleier, Arnim / Strohmaier, Markus (2014): "When politicians talk: assessing online conversational practices of political parties on Twitter", in: 
                        ICWSM 2014 – Proceedings of the 8
                  th
                  International AAAI Conference on Weblogs and Social Media 285-294.
                    
               
                  Nay, John J. (2016): "gov2vec: learning distributed representations of institutions and their legal text", in: 
                        NLP + CSS 2016 – Proceedings of the [1
                  st
                  ] Workshop on Natural Language Processing and Computational Social Science @ EMNLP 2016 49-54.
                    
               
                  Nguyen, Viet-An / Boyd-Graber, Jordan / Resnik, Philip / Miler, Kristina (2015): "Tea Party in the House: a hierarchical ideal point topic model and its application to Republican legislators in the 112
                        th Congress", in: 
                        ACL-IJCNLP 2015 – Proceedings of the 53
                  rd
                  Annual Meeting of the Association for Computational Linguistics & 7
                  th
                  International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing 1438-1448.
                    
               
                  Rheault, Ludovic / Beelen, Kaspar / Cochrane, Christopher / Hirst, Graeme (2016): "Measuring emotion in parliamentary debates with automated textual analysis", in: 
                        PLoS ONE, 11, e0168843.
                    
               
                  Sylwester, Karolina / Purver, Matthew (2015): "Twitter language use reflects psychological differences between Democrats and Republicans", in: 
                        PLoS ONE, 10, e0137422.
                    
               
                  van der Zwaan, Janneke M. / Marx, Maarten / Kamps, Jaap (2016): "Validating cross-perspective topic modeling for extracting political parties' positions from parliamentary proceedings", in: 
                        ECAI 2016 – Proceedings of the 22
                  nd
                  European Conference on Artificial Intelligence 28-36.
                    
               
                  Zirn, Cäcilia (2014): "Analyzing positions and topics in political discussions of the German Bundestag", in: 
                        Proceedings of the Student Research Workshop @ ACL 2014 26-33.
                    
            
         
      
   



      
         Die Digital Humanities (DH) existieren als Forschungsfeld (wenn auch nicht unter diesem Label) bereits seit den 1940er Jahren, so man Roberto Busas Projekt des Index Thomisticus als Grundstein ansieht. Wechselt man von der wissenschaftshistorischen zu einer wissenschaftspolitischen Perspektive und betrachtet Faktoren wie z. B. Forschungsförderungen und Projekte, sind die DH erst seit gut 10–20 Jahren Teil der (deutschen) Wissenschaftslandschaft. Ein Ringen um Akzeptanz ist teilweise bis heute zu beobachten. Gleichwohl lässt sich sagen, dass die DH mit Kuhn gesprochen durchaus mittlerweile den Status einer Normalwissenschaft erlangt haben und sich im Produktivbetrieb befinden.
         Typischerweise lässt sich in der wissenschaftlich-disziplinären Ontogenese nach der Etablierung einer Disziplin bzw. eines Forschungsfeldes der Übergang in eine neue Phase verzeichnen. Einerseits, weil sich durch den Produktivbetrieb und das vermehrte Einbringen vergleichbarer wissenschaftlicher Erkenntnisse Fragen nach dem epistemischen Status, der Validität, der Verwertbarkeit und der weiterführenden Fragenentwicklung stellen, andererseits, weil der wissenschaftspolitische Legitimationsdruck abnimmt und dadurch Ressourcen frei werden und sich neue Handlungsspielräume eröffnen. In dieser Phase der Reife und Entwicklung von Selbst-bewusstsein befinden sich die DH derzeit.
         
Vereinzelte Beiträge zur theoretischen Reflexion der DH als solche, ihrer Objekte und Methoden, Diskussionen auf Twitter und Blogs, sowie Konferenzthemen (exemplarisch: der Titel der DHd-Konferenz 2018 “Kritik der digitalen Vernunft”) stellen eindeutige Marker für diesen Befund dar. Es ist allerdings auch zu konstatieren, dass sich an der von Thiel in der FAZ 2012 vorgebrachten Kritik an der Theorielosigkeit der DH (, zuletzt abgerufen am 27.09.2019) bislang wenig verändert hat. Denn obwohl die Relevanz der Theoriebildung für die DH schon verschiedentlich betont wurde und immer wieder auch Theoriebeiträge vorgelegt werden, sind derlei Überlegungen bislang eher nebenbei und wenig zentralisiert in einzelnen, unabhängigen Projekten oder projektlosen Einzelarbeiten angestellt worden. Ähnlich institutionalisierte Diskurse wie z. B. im angelsächsischen Raum die “Debates in the Digital Humanities“ sucht man vergeblich. Im deutschsprachigen Raum entstanden so in der (bisweilen naiven) Akzentuierung der Entwicklung digitaler Werkzeuge klaffende Lücken in der Theoretisierung der Aktivitäten und Gegenstände, welche hinter der vordergründigen 
Methoden-Orientierung nicht sofort ins Auge springen. Da die Werkzeuge und Methoden ja einfach scheinbar “funktionieren”, werden Fragen nach ihrem epistemologischen Status verhängnisvollerweise allzu leicht in die zweite Reihe gestellt.
         
         Das Gebot der Stunde ist also die systematische wissenschaftlich-disziplinäre Selbstreflexion, Theoriebildung und epistemologische Positionierung der DH. Nachdrücklich sollte daher ein tiefgreifendes, akademisches Aufspüren jener Besonderheiten des Digitalen gefordert werden, die unter dem Signum des fundamentalen und allumfassenden Wandels einen Wissenschaftsbereich wie die DH nun schon seit einigen Jahrzehnten glaubwürdig rechtfertigen. Jene Suche sollte sich über die spezifischen Methoden der digitalen Transformation spannen. Was offenkundig fehlt sind z. B. informationstheoretische, kulturwissenschaftliche und philosophische Grundlagen und ein theoretisches Fundament, welches mit Hilfe einer flächendeckenden, systematischen Untersuchung die isolierten und verstreuten Ansätze sinnbringend und letztlich auch den einzelnen digitalen GeisteswissenschaftlerInnen Souveränität stiftend miteinander verknüpfen könnte. Insbesondere die Geisteswissenschaften, die sich durch ihre epistemische Sensibilität auszeichnen, besitzen das theoretische und methodische Rüstzeug um die unreflektierte Anwendung digitaler Werkzeuge und den naiven Glauben in digital konstruierte wissenschaftliche Erkenntnisse vermeiden bzw. überwinden zu können.
         Der Workshop “Spielplätze der Theoriebildung in den Digital Humanities” möchte an genau dieser Stelle ansetzen. Es soll ein Impuls gesetzt werden, der sich auf mehreren Ebenen erstreckt: Mit der dezidierten Thematisierung der Theoretisierung der DH wird die Community für die Relevanz des Themas sensibilisiert und gleichermaßen wird der Status Quo bestimmt, inwiefern Interesse und Kapazitäten von Seiten der einzelnen ForscherInnen für dieses Thema bereits vorhanden sind. Dies dient auch als Grundlage für etwaige Verstetigungsansätze dieser Forschungsrichtung auf mittelfristiger Perspektive hin, z. B. in Form einer eigenen Zeitschrift oder einer AG in der DHd. Inhaltlich wird eine Kartographierung der Objekte, Perspektiven und Methoden als Teil einer kritischen Refraktion der Digital Humanities unternommen, sowie Ansätze zur wissenschaftlichen Selbstdeutung der DH (als Disziplin, Feld oder Hilfswissenschaft) gesammelt. Hierzu loten die Teilnehmenden gemeinsam die Spielräume wissenschaftstheoretischer Grundlagen und Arbeitsfelder aus und schaffen damit eine Basis für systematische Deutungen.
         Der Workshop hat die Struktur eines World Cafés: Nach einer kurzen Begrüßung und Vorstellung des Programms im Plenum rotieren die Teilnehmende zwischen einer Reihe unterschiedlicher Themenfelder bzw. Thementische. So erhalten sie die Möglichkeit, sich innerhalb stetig aktualisierter Gruppenkonstellationen in Diskussionen über Perspektiven, Thesen und Themen der DH einzubringen. Insbesondere kontroverse sachliche Diskussionen sollen provoziert werden, um eine möglichst differenzierte und breite Grundlage für ein Theorienfundament zu schaffen. Die Diversität der beteiligten wissenschaftlichen Disziplinen auf dem Gebiet der DH, die möglicherweise in der Vergangenheit einer holistischen Theoriebildung der DH im Wege stand, wird dabei als Trumpfkarte gespielt. Für die Einigung auf eine gemeinsame Sprache und die Integration der Perspektiven werden im Rahmen des Workshops erste Ansätze konturiert und protokolliert. Moderierende an den Thementischen leiten die Gespräche, geben Denkimpulse, dokumentieren die Ergebnisse analog und digital und stellen diese abschließend dem gesamten Plenum vor. Eine zusammenfassende Reflexion der Ergebnisse und die Entwicklung eines thematischen Ausblicks runden den Workshop ab. Für die Moderation stehen zunächst die Einreichenden zur Verfügung. Sie werden ergänzt von verschiedenen einschlägigen KollegInnen, die an der Entwicklung des Workshops beteiligt waren (u. a. Patrick Sahle, Enes Türkoğlu und Rabea Kleymann). Nach der Bewilligung und Veröffentlichung des Workshops können sich außerdem noch weitere Interessenten für die Moderation einzelner Thementische melden und werden dann von den Organisatoren ausgewählt. Das Format des World Cafés ist für die Zielsetzungen des Workshops optimal, da die materialen Beiträge von Seiten der Teilnehmenden kommen, zudem kann ihre Heterogenität nicht nur aufgefangen, sondern produktiv genutzt werden. Die Unterteilung in Thementische gibt nur eine lockere Strukturierung vor und dient auch der Feststellung von Interessensprioritäten der Community. Weiterhin findet “am Rande” eine wechselseitige Identifikation und Vernetzung der Teilnehmenden statt.
         Die Themeninseln sollen folgende Schwerpunkte haben:
         
            
               Objekte der DH: Aus geisteswissenschaftlicher Sicht stellen sich die Gegenstände der Informatik alles andere als selbstverständlich dar: Daten sind nicht “neutral”, sondern bereits Interpretationen und Produkte von Forschungsprozessen und -methoden. Dasselbe gilt für Datenmodelle und letztlich auch für Algorithmen, deren Einfluss auf die Transformation von Daten für den geisteswissenschaftlichen Forschungsprozess selbstverständlich mitreflektiert werden muss. Aus informatischer Sicht sollte außerdem die Frage nach digitalen bzw. digitalisierten Objekten neu gestellt werden, welche durch Verfahren der technischen Reproduzierbarkeit notwendigerweise einen neuen ontologischen Status aufweisen.
                
            
               Methoden der DH: Die Forschungsgegenstände werden maßgeblich durch die Forschungsmethoden geprägt. Man könnte auch sagen, dass sie durch Forschungsmethoden erst als Gegenstände hervorgebracht werden. Eine Reflexion der Methoden ist daher unerlässlich und stellt sich nicht nur aus wissenschaftssoziologischer und wissenschaftspolitischer Perspektive im Hinblick auf Forschungsgelder, sondern auch aufgrund des neuen Zuganges, den die DH zu Forschungsgegenständen ermöglichen, z. B. in Form einer digitalen Hermeneutik und des Distant Readings.
                
            
               Werkzeuge der DH: Forschungsmethoden und Werkzeuge stehen in einem dialektischen Verhältnis zueinander. Software wird geformt von den Daten und den Datentransformationsaufgaben, die Daten hingegen werden strukturiert nach der verarbeitenden Software. Es ergeben sich Sachzwänge, deren Ausläufer sich bis hinein in das Research Software Engineering, die Prototypenkonzeption und Usability-Testing bemerkbar machen.
                
            
               Medialität und Digitalität der DH: Die Digitalität ist kein Phänomen der Geisteswissenschaften, sondern muss in einem größeren gesellschaftlichen Rahmen gedacht werden – die Digitalität der Kultur ist der Kontext einer Digitalisierung von Kultur. Logiken der Algorithmizität, Hyperreferentialität und technischer Performativität werden in die Forschung eingeschrieben und müssen bei einer Theorie der DH mitberücksichtigt werden (Beispiele: Informationstheorie geisteswissenschaftlicher Forschungsdaten, Transmedialisierung, Materialität des Digitalen).
                
            
               Wissenschaftstheorie der DH: Dies ist der bis dato wohl am prominentesten diskutierte Punkt, der sich auf das Verhältnis der DH zu den “klassischen” Geisteswissenschaften und der Informatik bezieht, sowie auf den Status der DH als eigenständige Disziplin, als Feld oder als Hilfswissenschaft. Es stellt sich die Frage, ob die DH eine eigene Wissenschaftstheorie brauchen oder befriedigend über etablierte Wissenschaftstheorien (z. B. von Fleck, Kuhn, Popper) beschrieben werden können. Der Theorienpluralismus und neue epistemische Forschungsdarstellungen sind hier ebenso zu diskutieren, wie das Problem der Inkommensurabilität, dass sich mit dem Semantic Web für Forschungsdaten neu stellt.
                
            
               DH und Öffentlichkeit: An das wissenschaftlich-disziplinäre Selbstverständnis der DH als Forschungsfeld schließen sich auch die Untersuchung des Verhältnisses zwischen DH und Öffentlichkeit an. Dies umfasst Fragen nach der Positionierung der DH im öffentlichen Diskurs rund um (geisteswissenschaftliche) Forschung, Fragen der Forschungsethik und Forschungsförderung, Open Access und Bürgerbeteiligung (“citizen science”).
                
         
         Der Workshop bietet damit Raum für verschiedene “Spielplätze” im Bereich der Theoriebildung der Digital Humanities: Spielräume des Theoretischen durchsetzen dann die Spielräume der Forschungspraxis und machen diese wissenschaftstheoretisch greifbar. Es lässt sich argumentieren, dass die DH gewappnet dafür sind, ein neues Kapitel ihrer jungen Wissenschaftsgeschichte zu schreiben. In diesem Sinne besteht die Hoffnung, dass der Workshop “Spielplätze der Theoriebildung in den Digital Humanities” durch die Zusammenführung interessierter WissenschaftlerInnen zur Initialzündung wird, nach der ForscherInnen gemeinsam und engagiert die Fundamentbildung der DH vorantreiben.
         Interessierte ForscherInnen haben auch nach dem Workshop die Möglichkeit in Kontakt zu bleiben, nicht nur, weil die Ergebnisse des Workshops digital zur Verfügung gestellt werden, sondern auch weil die Organisatoren die Möglichkeit für weitere Kollaboration anbieten wollen. Glückt das Vorhaben des Workshops als Inkubator, ist eine Verstetigung und Institutionalisierung des Forschungsinteresses geplant, um einerseits eine offene Plattform des Austausches und der Diskussion zu bieten und andererseits um Forschungsergebnisse wieder in die Community (und auch die Öffentlichkeit) zurückzuspielen.
         
            Einreichende:
         
         
            Jonathan D. Geiger, M. A.
                    
                  Akademie der Wissenschaften und der Literatur | Mainz, Digitale Akademie
                  Geschwister-Scholl-Str. 2 in 55131 Mainz
                  Forschungsinteressen: (Sozial)Epistemologie, Wissenssoziologie, Philosophie der Digitalität, Digital Humanities, Theorie von Informatik, Informations- und Dokumentationswissenschaft
               
            
            Jasmin Pfeiffer, M. A.
                    
                  Universität des Saarlandes, Lehrstuhl für Neuere deutsche Literaturwissenschaft | Medienwissenschaft
                  Campus, Gebäude A22, Raum 0.20, 66123 Saarbrücken
                  Forschungsinteressen: Theorie und Analyse des Computerspiels, Fiktionstheorien, Virtuelle Realitäten, Medialität und Materialität, Digitalität, Theorie der Algorithmen 
               
            
         
         
            Teilnehmende: max. 40
         
            Anforderungen an die Raumausstattung:
         
         
            Beamer: Ja
            Tafel/Whiteboard: Nein
            Flipchart: Nein (aber Flipchart-Papierbögen, die dann an die Pinnwände geheftet werden können)
            Moderationskoffer: 6 Stück
            Pinnwand: 6 Stück
            Steckdosenleisten: 3 Stück
            weitere Anmerkungen:
                    
                  Ein Laptop für den Beamer im Plenum wäre gut.
                  Ideal wäre die Möglichkeit neben dem Raum für das Plenum auch Zugang zu 1–3 weiteren (kleineren) Räumlichkeiten zu haben bzw. überhaupt Raum zum Ausweichen zu haben, sodass sich die Arbeitsgruppen etwas verteilen können.
                  Ein Bonus (wenn auch keine Notwendigkeit) wäre ein kleiner Stehtisch für jede Arbeitsgruppe, also vor jede Pinnwand (insgesamt 6 Stück).
               
            
         
      
      
         
            
               Bibliographie
               
                  
                  Bauer, J. (2011): ‘Who are you Calling Untheoretical?’, 
	Journal of Digital Humanities, 1(1). Available at:
	.
      
               
                  Brügger, N.  (2016): ‘Digital Humanities in the 21st Century: Digital Material as a Driving Force’, 
	digital humanities quarterly, 10(2). Available at:
	.
      
               
                  Capurro, R.  (1978): 
	Information: ein Beitrag zur etymologischen und ideengeschichtlichen Begründung des Informationsbegriffs. München: Saur.
      
               
                  Capurro, R.  (2017): 
	Homo Digitalis: Beiträge zur Ontologie, Anthropologie und Ethik der digitalen Technik. Wiesbaden: Springer.
      
               
                  Castells, M.  (2003): 
	Das Informationszeitalter: Wirtschaft, Gesellschaft, Kultur. Wiesbaden: Springer.
      
               
                  Cecire, N.  (2011): ‘Theory and the Virtues of Digital Humanites’, 
	Journal of Digital Humanities, 1(1). Available at:
	.
      
               
                  Ciula, A. / Eide, Ø.  (2017): ‘Modelling in digital humanities. Signs in context’, 
	Digital Scholarship in the Humanities, 32(1), pp. 33–46. DOI:
	.
      
               
                  Dahlström, M.  (2011): ‘Critical Editing And Critical Digitisation’, 
	Text Comparison and Digital Creativity, (The Production of Presence and Meaning in Digital Text Scholarship). DOI:
	.
      
               
                  Deck, K.-G.  (2018): ‘Digital Humanities – Eine Herausforderung an die Informatik und an die Geisteswissenschaften’, 
	Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3. DOI: 
	10.17175/sb003_002.
      
               
                  Flanders, J.  /  Jannidis, F.  (2019): 
	The shape of data in the digital humanities. Modeling texts and text-based resources.
	London, New York: Routhledge (Digital research in the arts and humanities).
      
               
                  Floridi, L.  (2013): 
	The philosophy of information. Oxford: Oxford Univ. Press.
      
               
                  Frabetti, F.  (2015): 
	Software theory: a cultural and philosophical study. London; New York: Rowman & Littlefield International (Media philosophy).
      
               
                  Friedewald, M.  /  Leimbach, T.  (2011): ‘Computersoftware als digitales Erbe: Probleme aus Sicht der Technikgeschichte’, in 
	Neues Erbe. Aspekte, Perspektiven und Konsequenzen der digitalen Überlieferung. KIT Scientific Publishing.
      
               
                  Gius, E.  /  Jacke, J.  (2017): ‘The Hermeneutic Profit of Annotation. On Preventing and Fostering Disagreement in Literary Analysis’, in 
	International Journal of Humanities and Arts Computing 11(2), 233–254.
      
               
                  Gnadt, T.  et al. (2017): ‘Faktoren und Kriterien für den Impact von DH-Tools und Infrastrukturen’. DARIAH-DE, Niedersächsische Staats- und Universitätsbibliothek. Available at:
	.
      
               
                  Hall, G. (2012):  “Blog Post: Has Critical Theory Run Out of Time for Data-Driven Scholarship?, 
	Debates in the Digital Humanities. Available at:
	
	(Zugriff: 19. August 2019).
      
               
                  Heßbrüggen-Walter, S.  (2018): ‘Philosophie als digitale Geisteswissenschaft’, 
	Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3. DOI: 
	10.17175/sb003_006.
      
               
                  Hui, Y.  (2016): 
	On the existence of digital objects. London: University of Minnesota Press.
      
               
                  Kaden, B.  (2016): ‘Zur Epistemologie digitaler Methoden in den Geisteswissenschaften’, 
	Berliner Beiträge zu Digital Humanities.
      
               
                  Koch, G. (ed.)  (2017): 
	Digitalisierung. Theorien und Konzepte für die empirische Kulturforschung. Köln: Herbert von Halem Verlag.
      
               
                  Matzner, T.  (2016): ‘Beyond data as representation. The performativity of Big Data in surveillance.’, 
	Surveillance & Society, 14(2), pp. 197–204.
      
               
                  McCarty, W.  (2014): ‘Getting there from here. Remembering the future of digital humanities: Roberto Busa Award lecture 2013’, 
	Literary and Linguistic Computing, Volume 29(Issue 3), pp. 283–306. DOI: 
	10.1093/llc/fqu022.
      
               
                  Mohabbat Kar, R.  /  Parycek, P.  (2018): ‘Berechnen, ermöglichen, verhindern: Algorithmen als Ordnungs- und Steuerungsinstrumente in der digitalen Gesellschaft’, in 
	(Un)berechenbar? Algorithmen und Automatisierung in Staat und Gesellschaft. Berlin: Fraunhofer-Institut für Offene Kommunikationssysteme FOKUS, Kompetenzzentrum Öffentliche IT (ÖFIT), pp. 7–39. Available at: 
	https://nbn-resolving.org/urn:nbn:de:0168-ssoar-57562-7.
      
               
                  Nassehi, A.  (2019): 
	Muster. Theorie der digitalen Gesellschaft. München: Beck Verlag.
      
               
                  Nerbonne, J.  (2015): ‘Die Informatik als Geisteswissenschaft’, 
	Sonderband der Zeitschrift für digitale Geisteswissenschaften, 1(1). DOI: 
	10.17175/sb001_003.
      
               
                  Porter, D.  (no date): 
	The Uncanny Valley and the Ghost in the Machine: a discussion of analogies for thinking about digitized medieval manuscripts, 
	Dot Porter Digital. Available at:
	.
      
               
                  Reiche, R. et al. (2014): ‘Verfahren der Digital Humanities in den Geistes- und Kulturwissenschaften’. (DARIAH-DE Working Papers), (4).
      
               
                  Sahle, P.  (2015): ‘Digital Humanities? Gibt’s doch gar nicht!’, 
	Grenzen und Möglichkeiten der Digital Humanities. DOI: 
	10.17175/sb001_004.
      
               
                  Scheinfeldt, T.  (2012): 
	“Blog Post: Where’s the Beef? Does Digital Humanities Have to Answer Questions?,
	Debates in the Digital Humanities. Available at: 
	
	(Zugriff: 19 August 2019).
      
               
                  Schröter, J.  /  Böhnke, A. (eds)  (2004): 
	Analog/Digital – Opposition oder Kontinuum? Zur Theorie und Geschichte einer Unterscheidung. Bielefeld: transcript Verlag.
      
               
                  Stalder, F.  (2016): 
	Kultur der Digitalität. Berlin: Suhrkamp.
      
               
                  Türkoglu, E.  (2019): ‘Vom Digitalisat zum Kontextualisat – einige Gedanken zu digitalen Objekten’, in. 
	DHd 2019 Digital Humanities: multimedial & multimodal. Konferenzabstracts, Frankfurt am Main. DOI: 
	10.5281/zenodo.2600812.
      
               
                  Wettlaufer, J.  (2016): ‘Neue Erkenntnisse durch digitalisierte Geschichtswissenschaft(en)? Zur hermeneutischen Reichweite aktueller digitaler Methoden in informationszentrierten Fächern’, 
	Zeitschrift für digitale Geisteswissenschaften. DOI: 
	10.17175/2016_011.
      
            
         
      
   



        
            
                Introduction
                Societies are increasingly divided and polarized. This polarization is driven by two connected issues: the lack of communication between groups, and the use of hate speech. With social media speeding up the spread of hateful ideologies, polarization and technology go hand in hand. Statistics reveal the scale of the problem; 41% of people have been the target of hate speech. As communities recede into themselves, the prospect of conflict grows. Social media is also providing new opportunities for polarization and hate speech. Shielded behind anonymity, state actors and political entities are using social media to manipulate public opinion on an industrial scale, driving polarization with disinformation and hate speech to serve often extremist agendas. Combined with bots - automated accounts - these partisan entities can achieve a negative impact in society (KS Hasan, 2013 ; Howell (2013). Social media companies have been slow to tackle the problem, for instance, Facebook redefined hate speech pages as controversial humor. While Twitter introduced a new policy stating "You may not dehumanize anyone based on membership in an identifiable group, as this speech can lead to offline harm", the business model of social media companies may also not be conducive to tackling hate speech. Indeed, hate speech pages can be popular, encouraging clicks and driving advertising revenue to web companies. This is why tackling hate speech and polarization requires multilateral efforts involving the companies themselves, academics and civil society. There have been efforts to address the problem such as the efforts done by the European Commission to tackle hate speech by signing a code of conduct with social media companies to fight hate speech. However, the problem is a global issue. Despite the widespread adoption of social media in the MENA region, most efforts in tackling hate speech also tend to focus on the developed world, with little research targeting Arabic. Some of the research targetting Hate speech in Arabic were limited to a specific categories such as hate targetting religious groups (Albadi 2018) or only covering abusive language detection as in (Mubarak 2017).
                Without adequate, contextual-based research, countries in the developing world in particular risk becoming social media blackspots - spaces where hate speech flourishes in unregulated and permissive online environments. The main aim of this project is to address this gap and pave the way for further research on Polarization and Hate Speech in Arab societies. 
            
            
                Methodology
                Our research will address different problems that contribute to the detection of polarization and hate speech: 1) Stance detection with respect to controversial topics (a topic generating a polarized discussion: in favor vs. against); 2) Identification of polarized communities; 3) Hate speech detection; 4) Bot versus human identification and 5) Behavioral interventions to address hate speech. These components will be considered from a holistic perspective unlike some of the existing research works, which address them as isolated problems. Our project focuses on five components: 1) Annotated Language Resources; 2) Polarized Communities Analysis; 3) Methods and Tools based on Natural Language Processing methods as in Fersini et al. (2018); 4) Behavioral interventions and experiments to address hate speech 5) Application Scenarios with the stakeholders. We will create annotated Arabic corpora from Twitter with the stance information (in favor, against or neutral) with respect to controversial topics (e.g., Qatar vs. UAE), polarized communities (e.g Liberals vs Conservatives) and the hateful usage of the language (e.g. insults, aggressive words). This will include creating an Arabic multi-dialectal lexicon of hate speech and aggressive language. The project has several application scenarios. In the context of cyber-security, government agencies could detect individuals and groups that spread hate speech and take appropriate countermeasures. Furthermore, bots spreading hate speech who increase the tension and polarization on society can be detected automatically. In a recent study by Jones (2016) and Jones (2019), 17% of a random sample of tweets in Arabic that mention Qatar were tweeted by bots in May 2017 and that increased to 29% in May 2018.
            
            
                Conclusion
                The main novelty of this proposal is in the scope, the multidisciplinary nature and the coverage of the addressed problems: we will address the main related problems to polarization as a whole, and not as isolated problems as it was done in some existing projects. Behavioral experiments and interventions will be conducted to address the issue of hate speech. We will test the state-of the-art methods of artificial intelligence to automatically approach the aforementioned problems in Arab social media. By taking into account the legal, the behavioral and the ethical dimensions of the software solutions as well as data protection considerations, we plan to create tools that will allow others to use them to detect polarization, hate speech, and bots.
            
            
                Acknowledgments
                This project is funded by NPRP grant NPRP13S-0206-200281 from the Qatar National Research Fund (a member of Qatar Foundation).
            
        
        
            
                
                Bibliography
                Albadi, N., Kurdi, M., & Mishra, S. (2018). Are they our brothers? analysis and detection of religious hate speech in the Arabic Twittersphere. In 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM) (pp. 69-76). IEEE. https://doi.org/10.1109/ASONAM.2018.8508247
                
                    Fersini, E., Rosso, P., Anzovino, M. (2018). Overview of the Task on Automatic Misogyny Identification at IberEval. In Proceedings of the Third Workshop on Evaluation of Human Language Technologies for Iberian Languages (IberEval 2018).
                
                
                    Hasan, K.S., Ng, V. (2013). Stance Classification of Ideological Debates: Data, Models, Features, and Constraints. In Proceeding of the Sixth International Joint Conference on Natural Language Processing
                
                Howell, Lee. (2013). Digital Wildfires in a Hyperconnected World. WEF Report 3.
                
                    Jones, M. O. (2016). Automated sectarianism and pro-Saudi propaganda on Twitter. Exposing the Invisible (Tactical Technology Collective).
                
                
                    Jones, M. O. (2019). The Gulf Information War| Propaganda, Fake News, and Fake Trends: The Weaponization of Twitter Bots in the Gulf Crisis. International Journal of Communication
                
                Mubarak, H., Darwish, K., & Magdy, W. (2017). Abusive language detection on Arabic social media. In Proceedings of the first workshop on abusive language online (pp. 52-56).
                
            
        
    



        
            
                Introduction
                Analyzing social media data (e.g., Twitter and Reddit) is essential to characterize social and political problems and understand them. Practically it is impossible to read all the social media posts and digest them manually. Thus, we have to extract underlying topics from the text automatically. 
                
                    Clustering is a suitable tool for extracting topics from social media data. However, it is still challenging to discover interpretable topics from a corpus of noisy and short texts, including social media posts. A limitation of the existing clustering methods, such as topic models (e.g., LDA [1]), is that they extract topics based on only word information (e.g., word frequency in the post). It is a notable feature that the social media data is the streaming data, i.e., the data consists of the text and the timestamp (the posted times). In contrast, it is not apparent how to utilize temporal information to find interpretable topics. For instance, we may be able to classify the tweets based on only the timestamps. However, this method would not find interpretable topics because many users post various topics of tweets simultaneously. Here we propose a clustering algorithm that utilizes the word and temporal information of the posts.  
                
            
            
                
                    Proposed Clustering Algorithm
                
                We propose a two-stage clustering algorithm that discovers course-grained topics by leveraging textual and temporal information [2]. Suppose that a huge volume of Twitter data, with text bodies (tweets) and timestamps (posted times), is available for us. 
                
                    In the first stage, we extract fine-grained topics (micro-clusters) by clustering the word co-occurrence graph of the tweets. We used the Data Polishing algorithm [3,4] to obtain the micro-clusters, that is, the tweets sharing a fine-grained topic. Data polishing algorithm was modified to incorporate the bias of the tweet data due to many retweets. Data polishing algorithm was modified to incorporate the bias of the tweet data due to many retweets. This modification improved the computational efficiency of the algorithm significantly. The temporal pattern of a micro-cluster is obtained by calculating the posting frequency of all the tweets in a micro-cluster. In the second stage, we discover the coarse-grained topics (e.g., a reaction to breaking news) by clustering the time series of the micro-clusters. We used K-Spectral Centroid (K-SC) clustering [5] to obtain the cluster of micro-clusters sharing a similar temporal pattern. K-SC algorithm is an extension of K-means, which improves the robustness to scaling and shifting. If the fine-grained topics originate from an item of news, we can expect their temporal pattern should be similar. This is the motivation for focusing on the temporal pattern to find course-grained topics.
                
                
                    First, the proposed algorithm was applied to large-scale Twitter data related to the "COVID-19 vaccine" [1]. We discover three types of coarse-grained topics from the Twitter data (26 million tweets): A. Reaction to the news (8 topics), B. Reaction to the tweets (2 topics), and C. Others (2 topics). While topics A and B exhibit a clear peak in their temporal pattern, topic C does not exhibit peaks: it shows a persistent activity. This topic comprises rumors, fake news, alerts for fake news, jokes, tips, etc., which are continuously posted on Twitter. Notably, the data polishing method can extract even such a less popular topic.
                
                Next, we evaluate the computational efficacy of the proposed algorithm; we ask how fast the algorithm process the data? We compare the proposed algorithm with five existing algorithms for finding topics from tweets: LDA [1], K-means, MeanShift, Agglomerative clustering, and Data Polishing [3, 4]. We found that the proposed algorithm is much faster than the existing methods. For instance, the proposed algorithm is more than 300 times faster than LDA with a data set of 100,000 tweets.
            
        
        
            
                
                    Bibliography
                    
                        Blei, D. M. (2012), Probabilistic topic models. Communications of the ACM, 55:77–84
                    
                    
                        Hashimoto, T., et al. (2021), Two-stage Clustering Method for Discovering People's Perceptions: A Case Study of the COVID-19 Vaccine from Twitter, IEEE BigData 2021.
                    
                    
                        Hashimoto, T., et al. (2021), Analyzing Temporal Patterns of Topic Diversity using Graph Clustering, The Journal of Supercomputing, 77:4375-4388
                    
                    
                        Uno, T., et al. (2017), Micro-clustering by Data Polishing. IEEE BigData 2017, 1012-1018
                    
                    
                        Yang, J. and Leskovec, J. (2011), Patterns of temporal variation in online media. In Proceedings of the fourth ACM international conference on Web Search and Data Mining, pages 177–186
                    
                
            
        
    



        
            Background
            The #MeToo movement in India, as a manifestation of the global #MeToo has invested in the empowerment of Indian women since its inception in October 2017. The digital movement sent reverberations across the country in unprecedented ways and took off on a massive scale in October 2018 (Mathur 2018). The movement constructed around the central hashtag, #MeTooIndia, has successfully created a much-needed discourse on sexual abuse, and harassment at the intersection of sex, power and politics on Twitter. The digital public sphere has facilitated feminist experiences of ‘coming-out’ with personal stories and experiences for the urban Indian woman. 
            Research Problem
            
                Cyberspaces in the South are complex and have a different set of constraints from the North, particularly for women and other marginalized collectives. In the discussion of the cyber-South, Gajjala and Oh pose the most important questions: “Will women of the South be allowed or able to use technologies under the conditions that are contextually empowering to them? Within which internet-based contexts can the women of the South truly be heard (2012, 8) or seek to be independent? “Hierarchies of power are deeply embedded in Internet culture as a form of invisible control” (15) of digital feminist spaces of the Global South. Furthermore, the issue of who speaks for whom (Spivak 1988) becomes even more important owing to lack of access, resources, voluntary participation, and representation of a significantly large population.
            
            
                This is apparent in the discourse surrounding the #MeTooIndia movement where heteropatriarchal and masculinist Hindutva activists organize campaigns of harassment and misogyny against women through a call for men’s rights activism that infiltrates the Indian digital feminist movement. Digital platforms in India are becoming increasingly violent, misogynistic, and sexist spaces that enable the congregation of far-right communities that further the everyday offline trauma that women face in the Global South. The presence of hate speech, violence, and masculine toxicity directed towards women acts as a deterrent in participation of women online and in their coming-out in public without anonymity. In addition, this creates unsafe spaces for women and other gendered minorities to recount their experiences with sexual abuse. As feminist activism is becoming increasingly visible on social media platforms; as feminist communities expand and are re-imagined through the use of new media (Mendes et al. 2019, 1), it must be noted that digital culture can be an incredibly complex and toxic space for gendered bodies that are constantly vilified, and objectified. In the context of #MeTooIndia, participants recede into toxic behaviour and vilify the movement for its fake cases, and accusations, or trivialize it for its lack of legal process. 
            
            According to authors Vickery and Everback, “mediated misogyny” (2018) oftentimes deliberately infiltrates feminist movements to incite violence, hate, and toxicity within the movement and directed towards members. Serisier’s work on rape culture also engages with the emergent culture of speaking out effectively about sexual abuse and rape using online platforms, while underscoring that the act of speaking out can be attributed to certain privileged voices (2007). Under what conditions are we as a collective authorized to speak out, who is receiving, listening, and verifying are important questions that any digital feminist movement needs to consider (2007). 
            Research Question
            This paper explores the emergence of a misogynistic Men’s Rights Movement surrounding the #MeToo in India on the social media platform Twitter that leads to gendered exclusion and silencing of feminist collectives in the name of free speech. Using discourse analysis, inductive coding, and close-reading of the corpus of tweets, the research asks the following questions – Does the discourse surrounding #MeTooIndia demonstrate mediated hate speech, violence against women, misogyny, and an emerging men’s rights activism? If so, what is the language of violence against women? How do the far-right heteropatriarchal societies on social media employ assertions of independence in their call for men’s rights activism? How are feminist digital movements affected by the hijacking and appropriation by men’s rights activists. 
            Relevance
            
                Although feminist social activism, particularly in the Global South, benefits from Internet culture, and its ability to enable the construction of safe feminist subaltern counterpublics (Fraser 1991) for self-expression, congregation and interaction, Indian women, particularly from marginalized communities, who employ the hashtag #MeTooIndia to participate in the discourse surrounding sexual abuse in India are left feeling unsafe, alienated, isolated, and silenced on social media platforms. The study of the emergence of men’s rights activism and misogyny in this context is significant because the discourse around #MeTooIndia, ‘hijacked’ and targeted by men’s rights activists, oftentimes drowns the feminist voice within the movement, leads to the effective erasure of feminist struggles, and creates barriers in participation on digital spaces. In keeping with the theme of the conference, this study also speaks to the congregation of far-right heteropatriarchal societies on social media that employ assertions of independence, and free speech to not merely engage in harassment campaigns against supporters of #MeToo, but also in their call for men’s rights activism in India. 
            
            Method
            
                10,000 unique tweets were collected and filtered through Twitter Web API between October 1
                st
                , 2018 and October 3rd, 2019 surrounding the #MeToo movement in India. All tweets were collected in the English language to maintain methodological consistency, and retweets were excluded from the sample dataset. Hashtags employed as filters include #metooindia, #indiametoo, #LoSHA, #womanhood+#metooindia, #womanhood+#indiametoo, #sisterhood+#metooindia, and #sisterhood+#indiametoo. Hashtags specifically used in the Indian context have been selected. All tweets are manually annotated and labelled for the specific criterion (below) that is marked on a binary scale of 0/1 where ‘Yes’ denotes 1, and ‘No’ denotes 0 - 
            
            
                Does this tweet indicate misogyny, violence against women, hate speech or advocation for men’s rights activism?
            
            In addition to the inductive coding method, this research employs the critical discourse analysis framework as well as close-reading of tweets to further understand how feminist spaces are dominated by a call for men’s rights activism, how heteropatriarchal societies employ assertions of independence on Twitter, and how digital feminist movements are impacted by the hijacking of their spaces in the Indian context. As this research is ongoing, the specific number of tweets that indicate misogyny and the emergence of men’s rights activism are pending. 
        
        
            
                
                    Bibliography
                    Gajjala, Radhika, and Yeon Ju Oh. 2012. 
                        Cyberfeminism 2.0. New York: Peter Lang. Print. 
                         
                    
                    Fraser, Nancy. 1990. “Rethinking the Public Sphere: A Contribution to the Critique of 
                    Actually Existing Democracy.” Print.
                    Mathur, Swati. 2018. “India most vocal about #MeToo in October: Global data 
                    analytics co.” The Times of India. November 1 
                        https://timesofindia.indiatimes.com/india/india-most-vocal-about-metoo-in-october-global-data-analytics-co/articleshow/66452967.cms.
                    
                    Mendes, Kaitlynn, Jessica Ringrose, and Jessalynn Keller. 2019. 
                        Digital Feminist 
                    
                    
                        Activism: Girls and Women Fight Back Against Rape Culture. Kettering: Oxford University Press.
                    
                    Serisier, Tanya. 2007. “Speaking Out against Rape: Feminist (Her)stories and Anti-rape Politics.” 
                        Lilith: A Feminist History Journal (16): 84–95. 
                    
                    Spivak, G. 1988. “Can the Subaltern Speak?” In Marxism and the Interpretation Culture, edited by C. Nelsson and L. Grossberg’s (66–111). Urbana: University of Illinois Press.
                    Vickery, Jacqueline, and Tracy Everback. 2018. 
                        Mediating Misogyny: Gender, Technology, and Harassment. Cham, Switzerland: Palgrave Macmillan.
                    
                
            
        
    



        
            
                Panel abstract
            
            The social media platform Reddit understands itself as a “home to thousands of communities”, where every used can find their community (
                https://www.redditinc.com). As researchers in humanities, we find that the submissions and comments posted to Reddit’s subreddits do indeed comprise authentic digital human interaction by groups of people that are in some cases prototypical communities and in other cases merely chance encounters of users who find themselves oriented towards the same virtual space. The collective communicative acts of Reddit users can be positioned in the tradition of computer-mediated communication (CMC) – as one key site of digitalised communication, shaped partially by the affordances provided by the platform, and uniquely available to researchers not just in terms of their linguistic content, but also their multimodal context and discursive structure. Importantly, however, Reddit (sub-)communities are not necessarily subject to identical communicative patterns – within each community, user types and even individual users communicate following particular patterns or even idiosyncratically. 
            
            Our recently formed interdisciplinary network, copRe (
                communicative 
                practices on 
                Reddit – copre.org), is dedicated to exploring Reddit discourse(s) from different theoretical perspectives, but all with the aim to contribute to the understanding of Reddit’s own communicative culture as well as the exploration of digital practices more generally. 
            
            Specifically, our panel at the DH2022 conference explores aspects of digital culture and participatory culture, manifest in the communicative acts of different (sub-)communities on the online social platform Reddit. These subreddit-communities and the digital genres they give rise to are sites of linguistic innovation as well as of new debating practices – from the combative far-right subreddit r/The_Donald to the more harmonious r/changemyview. They let us gain insights into individual and group identities, as on r/Mountaineering, and they raise question of methodology, such as the understanding of text length as both a challenge for research and a motivated choice of text authors and the employment of mixed-methods to gain insights that are both driven by big data as well as by in-depth understanding of individual and collective communicative acts.
            
                Language innovation and diffusion online.
                Lisa Donlan (University of Manchester)
                Who are the innovators of lexical terms online? What are the community roles of the early adopters who successfully diffuse linguistic innovations?
                In offline communities, the weak-tie theory of language change envisions the innovators of linguistic forms as peripheral to a community while early adopters are the community's central members. However, the only study to explore the applicability of the theory in an online Community of Practice (CoFP) was grounded in an unusual linguistic context. My research addresses this gap in the literature by using a mixed-methods approach to analyse the status of the innovators and early adopters of four community-salient innovative linguistic forms which diffused through an online music-orientated CofP, Popheads.
                Contrary to expectations, three of the four forms studied were innovated by non-peripheral members who scored highly across multiple markers of status. This departure from previous findings may be related to the fact that linguistic creativity is highly valued in many virtual contexts. Consequently, high-status members may perceive linguistic innovation as desirable behaviour online. 
                This research also found that identifying the hierarchical structures that underpin a community leads to more precise descriptions of the characteristics of early adopters. Specifically, it has been possible to conclude that early adopters are prolific contributors, whose posts are successful at generating discussion, and who are on inbound trajectories in the community. Therefore, to speak of an early-adopter as being 'central' or 'high-status' is, I argue, ultimately too vague and fails to acknowledge the multidimensional nature of status. 
            
            
                Functions of text length on Reddit
                Aatu Liimatta (University of Helsinki)
                In corpus-linguistic studies, text length is typically seen as a potential confounding factor (see e.g. Liimatta, 2020), largely because its effects have been difficult to study using even the largest traditional corpora. However, like any other linguistic choice, the length of a text is also a choice made by the writer or speaker: it is also affected by the communicative purpose of the text and the limitations and affordances of the communicative situation.
                Fortunately, large social media datasets with a range of text lengths have allowed us to approach this previously unassailable topic. Reddit is particularly interesting in terms of text length, since the length of a Reddit comment is free to vary according to the commenter’s needs. Recent studies have shown that Reddit comment length is linked to the distribution of functional linguistic features: for instance, simple information-seeking comments tend to be very short, whereas narrative registers appear to favor longer comments on average (Liimatta, forthc.).
                In order to further explore the role and functions of comment length on Reddit, I analyze a number of subreddits in terms of both the distribution of comment lengths and the distribution of functional linguistic features across comment lengths. To do this, I make use of a large-scale dataset of Reddit comments and a simple but powerful pooling-based computational methodology.
            
            
                Register variation in Reddit comments - A multidimensional analysis
                Hanna Mahler, Kyla McConnell, Axel Bohmann, Gustavo Maccori Kozma, Rafaela Tosin (University of Freiburg)
                Researchers are increasingly becoming interested in the many opportunities that Reddit provides for linguistic analysis. In this large-scale natural language processing project, we focus on register variation within Reddit comments (inspired by Liimatta 2016, 2020).
                We analyze Biber’s (1998) linguistic features for register analysis, as well as platform-specific features, on all Reddit comments since 2005, using the Pushshift Reddit Corpus (Baumgartner et al. 2020). We are using this feature annotation to implement a short-text MDA (Clarke & Grieve 2019), a version of Biber’s (1988) multi-dimensional analysis, to find out which dimensions describe the linguistic variation found on the platform and whether the topical "subreddits" can be described as different registers. Our method also promises to serve as a useful tool for analysing other topics such as adaptation of linguistic norms or register diversification over time.
                Our study therefore adds to the state of knowledge in several ways:
                1. We regard a single comment as one text (with features extracted on the sentence level), which allows us to accurately locate linguistic variation within individual users.
                2. We train a tagger specifically to overcome previous difficulties of tagging social media data (e.g. Banga & Mehndiratta 2017), based on data from Behzad & Zeldes (2020) and Gessler et al. (2020).
                3. The feature extraction script, a refined and elaborated version of Biber's (1988) initial features, is written in Python and will be made openly available.
                4. Our long-term goal is to develop an MDA solution that captures variation within and among all (English) subreddits.
            
            
                Combating the Far-/Alt-Right on Reddit: Lessons from r/AgainstHateSubreddits
                Adrienne Massanari (American University, Washington D.C.)
                Reddit embodies a carnivalesque spirit, often reflecting a kind of geek masculinity (Kendall, 2011) that champions both niche, technical prowess and clever humor (Massanari, 2015). At the same time, communities engaging in far-right rhetoric, such as the now-banned (and widely popular) r/The_Donald, have flourished in part because the platform relies almost exclusively on volunteer labor to moderate and grow communities (Matias, 2019). Shifting the responsibility and risk of moderating onto unpaid individuals allows Reddit to remain a “lean” organization with few employees, but also creates a kind of plausible deniability when it comes to so-called “alt-right” subreddits.
                In response to the growing threat that these subreddits present, and the lack of response from Reddit administrators, activists on the platform have created their own communities focused on highlighting hate speech pervasive on the platform. One such example is r/AgainstHateSubreddits, which is dedicated to exposing subreddits that may superficially conform to Reddit’s few rules, but also engage in transphobic, misogynistic, Islamophobic, and racist rhetoric. Through a critical discourse analysis (Fairclough, 2013) of popular postings on the subreddit, I explore how this community challenges Reddit’s politics and offers an ethical counterpoint to the toxic geek masculinity that pervades much of the platform. Drawing on work from platform studies (Bucher, 2018; Gillespie, 2010) and design justice (Costanza-Chock, 2020; D'Ignazio & Klein, 2020), I argue that Reddit’s governance, design, and platform policies work implicitly welcome and mainstream far-right communities, but that spaces like r/AgainstHateSubreddits provide critical forms of resistance and community for activists.
            
            
                Share my view: Harmonious debating culture on r/changemyview
                Thomas C. Messerli (University of Basel), Daria Dayter (University of Tampere)
                In current times, digital discourses are often understood in terms of polarization. Public lay metadiscourses are full of references to social bubbles and disparate parts of society, whereas academic scholars give a lot of focus to binary categories such as information/disinformation, truth/post-truth or outrage culture. Within this context, the debating culture on the subreddit r/ChangeMyView (CMV) stands out because it encourages what we could term persuasibility – the capacity or willingness of someone to change their opinion when encountering new information. While some work has been done on the specific strategies that commenters use to achieve the task at hand, i.e. to change the original poster’s (OP) view, little attention has been paid to the question how prepared OPs actually are to change their mind and how this “malleability of opinion” (Tan et al. 2016: 621) is discursively constructed. From this perspective, original posts – 
                    submissions in Reddit terminology – are firstly performances of persuasibility, and secondly access points to persuasible-persuasive pairings, in which the subreddit community enacts its codified and tacit norms. In order to explore these pairings, we make use of the CMV corpus we have compiled and specifically compare submissions, delta-awarded comments, i.e. those comments that have changed the OP’s view, and the OP’s responses to delta-awarded comments. We do this comparison itself with a mixed-methods approach that is grounded in qualitative annotation of persuasibility in a sample of r/changemyview threads and scaled up to the corpus using corpus linguistic methods.
                
            
            
                “Science has no business in the mountains”: Stance-taking and expert knowledge on r/Mountaineering
                Sven Leuckert (TU Dresden)
                Stance-taking, as popularised in pragmatics and sociolinguistics by Du Bois (2007), refers to “the speaker’s (or writer)’s relationship to (a) the topic of discussion, (b) the interlocutor or audience, and (c) the talk (or writing) itself” (Kiesling et al. 2018: 684). On social media, stance-taking plays an important role in the discursive construction of relationships and may be employed as a gatekeeping device. In this talk, I focus on strategies of stance-taking as it is linked to the expression of expert knowledge on the subreddit r/Mountaineering. On this subreddit, stance-taking represents a dominant tool to establish who can be considered an expert and, hence, part of the knowledgeable in-group.
                In this talk, I explore which specific linguistic phenomena are employed by users of the subreddit to express stance in situations where expertise in mountaineering is in focus. After an initial manual assessment of recurring phenomena on the basis of randomly selected threads, a quantitative approach inspired by Kiesling et al.’s (2018) annotation scheme is used to establish the bigger picture of how stance-taking is employed as a gatekeeping device on r/Mountaineering. For this study, the entirety of r/Mountaineering from 2012 to August 2021 has been scraped and is taken into consideration. In sum, the findings suggest that, while quantitative methods are a useful addition in the investigation of stance on Reddit, they can only be complementary to an in-depth study of stance-taking phenomena in their discursive context.
            
        
        
            
                
                    Bibliography
                    
                        Banga, R. and Mehndiratta, P. (2017). Tagging Efficiency Analysis on Part of Speech Taggers: International Conference on Information Technology (ICIT: 264–267.
                    
                    
                        Baumgartner, J., Zannettou, S., Keegan, B., Squire, M. and Blackburn, J. (2020). The Pushshift Reddit Dataset: Proceedings of the International AAAI Conference on Web and Social Media, 14th edn.
                    
                    
                        Behzad, S. and Zeldes, A. (2020). A Cross-Genre Ensemble Approach to Robust Reddit Part of Speech Tagging. http://arxiv.org/pdf/2004.14312v1.
                    
                    
                        Biber, D. (1988). Variation Across Speech and Writing. Cambridge University Press.
                    
                    
                        Bucher, T. (2018). If...then : algorithmic power and politics. Oxford University Press.
                    
                    
                        Clarke, I. and Grieve, J. (2019). Stylistic Variation on the Donald Trump Twitter Account: A Linguistic Analysis of Tweets Posted between 2009 and 2018. PloS one, 14(9), e0222062.
                    
                    
                        Costanza-Chock, S. (2020). Design justice: Community-led practices to build the worlds we need. The MIT Press.
                    
                    
                        D'Ignazio, C. and Klein, L. F. (2020). Data feminism
                        . The MIT Press.
                    
                    
                        Dayter, D, & Messerli, T. C. (2021). Persuasive language and features of formality on the r/ChangeMyView subreddit. Internet Pragmatics, 5(1): 165–195. 
                        
                            https://doi.org/10.1075/ip.00072.day
                        
                    
                    
                        Du Bois, J. W. (2007). The stance triangle. In Engelbretson, R. (Ed.), Stancetaking in Discourse. John Benjamins, pp. 139–182
                    
                    
                        Fairclough, N. (2013). Critical discourse analysis: The critical study of language. Routledge.
                    
                    
                        Gessler, L., Peng, S., Liu, Y., Zhu, Y., Behzad, S. and Zeldes, A. (2020). AMALGUM – A Free, Balanced, Multilayer English Web Corpus: Proceedings of The 12th Language Resources and Evaluation Conference: 5267–5275.
                    
                    
                        Gillespie, T. (2010). The politics of ‘platforms’. New Media & Society, 12(3): 347–364.
                    
                    
                        Kendall, L. (2011). ‘White and nerdy’: Computers, race, and the nerd stereotype. The Journal of Popular Culture, 44(3): 505–524.
                    
                    
                        Kiesling, S. F, Pavalanathan, U., Fitzpatrick, J., Han, X. and Eisenstein, J. (2018). Interactional Stancetaking in Online Forums. Computational Linguistics 44(4): 683-718
                    
                    
                        Liimatta, A. (2016). Exploring Register Variation on Reddit: A Mulit-Dimensional Study
                        . Master thesis, University of Helsinki.
                    
                    
                        Liimatta, A. (2020). Using lengthwise scaling to compare feature frequencies across text lengths on Reddit. In Rüdiger, S. and Dayter, D. (Eds.), Corpus Approaches to Social Media
                        . John Benjamins, pp. 111–130.
                    
                    
                        Liimatta, A. (forthc.). Register variation across text lengths: Evidence from social media. International Journal of Corpus Linguistics.
                    
                    
                        Massanari, A. L. (2015). Participatory culture, community, and play: Learning from reddit
                        . Peter Lang.
                    
                    
                        Matias, J. N. (2019). The Civic Labor of Volunteer Moderators Online. Social Media + Society, 5(2). https://doi.org/10.1177/2056305119836778
                    
                    
                        Tan, C., Niculae, V., Danescu-Niculescu-Mizil, C. and Lee, L. (2016). Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions: Proceedings of the 25th International Conference on World Wide Web: 613–624.
                        
                             https://doi.org/10.1145/2872427.2883081
                        
                    
                
            
        
    



        
            
                Introduction
                The Digital Humanities offer immense possibilities for interdisciplinarity, cross-cultural, and epistemic knowledge exchange. Nevertheless, the DH continue to privilege centralized Anglophone practices and epistemologies that hinder access and contributions from marginalized communities and researchers across the globe. This panel challenges such privileges by engaging varied issues around the (im)possibilities of multilingual DH practices. The presentations in this panel contrast the multilingual aspirations of DH with the Anglophone and Anglocentric realities of DH practices regarding access, audience distributions, curation, metadata practices, pedagogical approaches, and epistemic production. Drawing upon myriad disciplinary homes, presenters in this panel engage with topics of translation, particularly translation difficulties and so-called untranslatables (Apter, 2014). For example, one panelist examines challenges in new media studies for translating contemporary DH lexica, while another maps untranslatables through computational exploration. How can languages other than English be implemented in DH? What are the barriers to entry for those engaging with non-Anglophone community translation practices? How can this linguistic negotiation be undertaken? What is gained and lost in the translation of core DH concepts? Furthermore, this panel scrutinizes the implementation gap between English and non-English DH in settings related to pedagogy and archival and metadata practices, focusing on and emphasizing the consequences of Anglocentric approaches and the advantages of multilingual DH in providing equitable access to diverse audiences, students, researchers, and linguistic communities. What does it mean to be global? How open and accessible is open access? Who can and cannot be a digital humanist when we favour English at training sites and elsewhere? How can DH transcend Anglocentrism? How can we implement multilingual DH in pedagogy and archival practices? In a range of explorations, this panel probes the possibilities and obstacles faced by multilingualism vis-à-vis English as lingua franca. In doing so, presenters in this panel engage with questions of inclusivity through a critical approach to various facets of multilingual DH.
            
            
                Multilingual bio-diversity and the decolonization of the 
                    Biodiversity Heritage Library: The case of Latin America
                
                Author: Lidia Ponce de la Vega, McGill University 
                
                    The 
                    Biodiversity Heritage Library 
                    (BHL) is an online repository for global biodiversity-related literature that advocates for multilingualism but operates within (digital) Anglocentrism. With over 80% of its collection in English, the BHL promotes this language as the lingua franca of the Internet and of biodiversity-related knowledge production, positing the Global North as the epistemic center of such production and perpetuating colonial dynamics that hinder bio-diverse epistemologies from the Global South.
                
                
                    This presentation constitutes a reflection around best decolonial practices for online libraries and archives affiliated to the Global North but that incorporate epistemologies pertaining to the Global South. By focusing on the collections of the BHL and the case of Latin America (and Mexico, specifically), this paper discusses the role of language representation and inclusion in the decolonization of biodiversity-related knowledge production. In so doing, it explores the importance of multilingualism for the BHL in terms of access, audience distribution, curation, and epistemic production (Chan). Additionally, this presentation considers the case of BHL México, a partnership between the BHL and Mexico’s Comisión Nacional para el Conocimiento y Uso de la Biodiversidad, to showcase the possibilities and limitations for the decolonization of bio-diverse online collections vis-á-vis Latin American audiences and human and nonhuman subjects–that is, our relationships with and within biodiversity. This presentation thus argues for a non-Anglocentric, non-Global-North-centric, and sympoietic (Haraway) approach to archival practices—anchored in non-hegemonic multilingualism—that aims for the diversification of bio-diverse narratives, breaks with the colonial roots of (digital) archives (Risam), and promotes a cycle of bio-diverse knowledge production in and from the Global South.
                
            
            
                The Function of Translation in DH: Reflections from a Bangla Translator
                
                    Author: Samya Brata Roy
                
                
                    While engaging with the act of social media translation in DH, I found myself asking, who am I translating for? And what is the purpose of this act? Ideally, translation or an attempt at multilingualism is meant to increase reach beyond the hegemonic claws of English. But, when it comes to expressing the key terms anchored in English terminology, there often is no suitable alternative in “minor” languages, or at least, recognizable terms for non-specialist readers. Official terminologies can be valid in the literal sense, but in practical terms they can confuse readers even further. This is why using English words offers an easy way out. However, that means harking back to the dominant language. The question that comes here is: Why and how should we translate DH, a field which exists in the disciplinary intersections? Is it counterproductive to attempt a purist translation or should one use both languages wherever necessary, a decision that presupposes an understanding of English? Sharing my experience as a Bangla Twitter translator for DHSI 21, I will raise these issues to start a conversation regarding the tussle between access (provided by a multilingual approach) and convenience (using English terminology to avoid confusion). I will build on Ortega’s (2014) reflections on translating at the DH2014 conference, and Renée Desjardins’ (2017) monograph on 
                    Translation and Social Media
                     to ground my own ethnographic case study of social media translation. The insights drawn from my personal experience will contribute to shedding light on DH translation with regards to the positionality of the translator, in particular within minority language settings 
                    .
                
            
            
                
                    Pogrom: Translational and Translocational Journey of a Slavic Word through Mass Media
                
                Author: Eric Kim, Stanford University 
                
                    As Steven Zipperstein explains in the first chapter to his monograph on the Kishinev pogrom, the word 
                    pogrom
                     first enters English-language mass media during the early 20
                    th
                     century, often accompanied by an explanatory note or offset with italics to indicate the foreignness of the term. With the growing migration of Jews beginning in the 1880s, the word eventually developed a currency and began to signify antisemitic violence on its own (Zipperstein). However, 
                    pogrom
                     in the Russian context has never conveyed this specific meaning, but rather, it has referred, and continues to refer, to any instance of government-organized mass violence against groups determined along class, social, or other boundaries (Ushakov). In order to specify the Jewish victims of the riot, Russian newspapers necessarily attached the adjective Jewish, 
                    evreiskii
                    , to the word 
                    pogrom
                    .
                
                
                    By comparing the distributions and appearances of 
                    pogrom
                     in variously languaged print media, I hope to explore the different perceptions of the word and the differing instantiations of antisemitic violence across national borders. Included in this analysis will be other terms synonymous with 
                    pogrom
                     to signify antisemitic catastrophe, such as 
                    besporiadok
                     in Russian or 
                    riot
                     in English, and use of images to depict these traumatic events, while the initial corpora of interest are 
                    Ogonek
                     and 
                    Life
                    . Between the distinct uses and frequencies of the term in these two periodicals, I aim to find patterns in subject formation, both of the self and the other, through levied accusations, and also glaring unacknowledgments, of antisemitism.
                
                 
            
            
                Strategies for Multilingual DH Pedagogy
                Author: Quinn Dombrowski, Stanford University  
                
                    The DH 2019 pre-conference workshop on pedagogy highlighted efforts across the globe to put digital tools and methods into the hands of students. While “international” pedagogical spaces (such as summer or winter workshops, or open-access course materials) still most commonly treat English as the default language of instruction and most likely object of study, efforts to cultivate learning communities centered on materials in other language have recently expanded, including 
                    Digital Humanities for Japanese Culture
                     at DHSI in 2019 (Kiyonori Nagasaki et al.), 
                    East Asian Studies and Digital Humanities
                     at DReAM Lab in 2021 and 2022 (Paula R. Curtis and Paul Vierthaler), and Slavic DH workshops at the Herder Institute in Marburg, Germany and Princeton University in 2018 and 2019 (Natasha Ermolaev et al.) These linguistically-focused pedagogical encounters are invaluable for both skill development and community-building, but they are limited in each: far more students are exposed to DH through general-purpose “intro to DH” courses that do not center language-specific issues. As a result, students with research materials in English (or the dominant language of the course) leave with practical skills, while students working with other languages face an implementation gap when they try to apply what they’ve learned. This talk will draw on experiences teaching a deliberately multilingual DH course and participating in language-centric workshops, in order to propose actionable strategies for making DH survey courses better support the kinds of multilingual work described in the other talks on this panel.
                
            
            
                Multilingual DH as a Political, Cultural and Ideological Statement on Accessibility
                Authors: Marie-France Guénette, Université Laval; Cecily Raynor, McGill University 
                
                    What is it about English that continuously reaffirms its position as a 
                    lingua franca
                    , even in interdisciplinary and emerging fields like Digital Humanities? What can, on the surface, seem like a brilliant strategy for shared knowledge on a global scale actually camouflages the imposition of an intellectual blockade. In this presentation we point to strategies that could universally level the playing field for DH scholars through open access publications, quality translations, multilingual knowledge dissemination and exposure to datasets that lie outside of English. In order to illuminate these strategies in practical ways, we will provide case studies from non-English language contexts to show how we can collectively work towards greater language inclusion and exposure in the field, an undertaking that we argue has broader consequences for social justice. 
                
                
                    Indeed, in her work on the imperialist origins of English as a 
                    lingua franca
                    , Denise Rhéaume astutely remarked that “the current usefulness of English stems from the economic, political and cultural power of, first, the British Empire, and more lately the American Empire” (2015: 151). This implies that by doing DH in English, we are (perhaps unknowingly) asserting a renewed form of scholarly imperialism that privileges Anglophone territories and long-standing, oppressive historical relationships. Breaking the cycle of Anglophone dominance means challenging the field of DH to embrace multilingualism, but, as Rhéaume argues in her piece on language politics, “obstacles to multilingualism [...] have more to do with entrenched privilege or the profit imperative than concern for the ideal conditions for democratic debate” (2015: 155). What if we, as a community of scholars, decide to change things? As we explore in this talk, challenging the 
                    lingua franca
                     is a fair strategy to resist the economic, political and cultural heritage which impedes academia from moving forward in equitable ways. Scholarly just futures are possible, especially in DH, if we work to increase accessibility and promote multilingualism.
                
            
        
        
            
                
                    Bibliography
                    
                        Cassin, Barbara
                        . 
                        Dictionary of Untranslatables: A Philosophical Lexicon
                        . Princeton, N.J: Princeton University Press, 2014.
                    
                    
                        Chan, Leslie
                        . ‘Situating Openness: Whose Open Science?’ 
                        Contextualizing Openness. Situating Open Science
                        , edited by Leslie Chan, University of Ottawa Press, 2019, pp. 5–22.
                    
                    
                        Desjardins, Renée
                        . 
                        Translation and Social Media
                        . London: Palgrave Pivot, 2017.
                    
                    
                        Haraway, Donna J
                        . 
                        Staying with the Trouble: Making Kin in the Chthulucene
                        . Duke University Press, 2016.
                    
                    
                        Ortega, Élika
                        , 2014. Whispering/Translating during DH2014: Five Things We Learned. [Blog] 
                        Élika Ortega
                        , Available at:  [Accessed 8 December 2021]
                    
                    
                        Risam, Roopika
                        . ‘Colonial Violence and the Postcolonial Digital Archive’. 
                        New Digital Worlds: Postcolonial Digital Humanities in Theory, Praxis, and Pedagogy.
                        , Northwestern University Press, 2018, pp. 47–64, 
                        
                            https://muse.jhu.edu/book/62714
                        
                        .
                    
                    
                        Ushakov, Dmitrii
                        . 
                        Tolkovyi slovar’ russkogo iazyka
                        , tom III. Moscow: Sovetskaia entsiklopediia, 1935
                    
                    
                        Zipperstein, Steven
                        . 
                        Pogrom: Kishinev and the Tilt of History
                        . New York: W. W. Norton & Company, 2018
                    
                    
                        Venuti, Lawrence
                        . 
                        The Translation Studies Reader
                        . United Kingdom, Taylor & Francis, 2000.
                    
                
            
        
    



        
            
                Session Overview
                In November 2021, a heated discussion broke out on social media about Johanna Drucker’s article on “Sustainability and Complexity,” which detailed her experiences with digital project migration and obsolescence. While many researchers applauded Drucker for drawing attention to the sustainability problem in DH, information professionals pointed out that they have long recognized and worked to address the challenges of hosting, maintenance, and preservation Drucker identifies. The fact that Drucker’s piece cited little of this work reflects a pernicious tendency among humanities scholars to disregard the expertise of librarians, designers, and developers (Posner 2013; @ThatAndromeda). Our panel addresses the widespread degradation of DH projects as a crisis, not of inattention or negligence, but of misalignment between the technical, institutional, and social infrastructures of DH work. 
                Challenges of project endurance and afterlife management are well documented in DH (for example, see Barats, Schafer, and Fickers 2020). In the wake of high-profile collapses like Project Bamboo (Dombrowski 2014, Almas 2017), funders including ACLS and NEH have offered grants for recovery, extension, and preservation of existing work. In addition, projects like the Socio-Technical Sustainability Roadmap (Visual Media Workshop at the University of Pittsburgh) and the Endings Project (Humanities Computing Media Centre at the University of Victoria) have emerged to educate teams about the reality of impermanence and to assist with the sunsetting process (see also Otis 2021). Recognizing the labor and environmental costs of maintenance, a growing number of DH thinkers and institutions are rejecting the imperative to keep projects alive forever, embracing ephemerality and a minimalist ethos (Lincoln 2020; Sayers 2016; Gil 2015).
                Drawing on insights from critical infrastructure studies, our panel begins from the premise that technical infrastructures are inseparable from social and political concerns (Thylstrup 2018; Deger-Pawlicka 2021). Attempting to address sustainability as a technical challenge without attention to its sociopolitical dimensions leads to tribalism, retrenchment, and re-inscription of existing power imbalances and resource inequities. This panel will therefore explore the sustainability problem in DH as a 
                    matter of concern (Latour 2005), a complex object constructed by multiple intersecting and competing agencies. 
                
                Our panel is led by two faculty project directors and brings together academic researchers, teachers, librarians, information technologists, software designers, engineers, and project managers with a range of experience as DH makers and maintainers. By highlighting the perspectives of different kinds of workers within and adjacent to different kinds of institutions, we hope to identify specific frictions between the material and technical conditions of our work and the sociocultural realities in which it takes place. Together, we will take a fresh look at well-known sustainability issues, reframing them through the lens of critical infrastructure and critical university studies in order to imagine a new set of interconnected disciplinary, institutional, and technical infrastructures that would enable DH scholarship to thrive.
            
            
                Presentations
                
                    
                        Alex Gil,
                        The Post-Infrastructures of Our Trash Futures
                    
                    In this paper, I propose that minimal computing operates in a future and past that repurposes that which has been discarded, trashed, ignored. The dream of a shared and reliable humanities infrastructure has been a mainstay of digital humanities for decades: projects like Humanities Commons, DARIAH, Omeka and others, are results of the pursuit of our collective scholarly independence from corporate solutions and control of our own work. In this paper, I argue for a Plan B where all of our infrastructural dreams fall by the wayside and we are left in the position of scavengers of corporate tech infrastructure and open standards without sacrificing the mission of the humanities—to steward and interpret human culture.
                    In this talk, I will also directly address the social and political challenge posed by faculty project directors who fail to understand, or understand too late, the real cost of invisible labor, and their often unrealistic expectations around the stewardship of their projects—made vanity by their own failure in transforming tenure & promotion guidelines and other material reward mechanisms in their departments. Within the context of shifting blame to neoliberalism, I will argue for the vacuity of theory absent critical infrastructural practice tied to the means of production of humanistic knowledge. Plan B, I will argue,  makes all the more sense within the grim prospect that faculty project directors will continue to behave as they have in the past few decades. 
                
                
                    
                        Lauren Liebe,
                        Flexibility as Sustainability in Digital Humanities Projects
                    
                    Johanna Drucker’s recent article “Sustainability and Complexity: Knowledge and Authority in the Digital Humanities” raises important questions about knowledge specialization within digital humanities projects. While Drucker asserts that the digital humanities must incorporate humanistic methodologies, not just humanistic content, the reverse is also true: humanist digital projects must also make use of the full potential of their digital expressions. To do this successfully requires that both the humanities scholars and the technical experts possess nuanced understandings of both the project’s content and the technology upon which it relies.
                    Such an understanding allows digital humanities projects to explore the nuances of their work as both digital 
                        and humanist. One of these nuances, as Drucker points out, is that “we need to think of the work of digital humanities as radically incomplete, always ongoing” (93). While physical media creates a sense of stability, the digital always operates with a level of ephemerality. Even archival projects like the Internet Archive capture only snapshots, not projects in their entirety. Key to this notion of digital ephemerality is the need to embrace flexible technology, particularly in the creation and storage of data and metadata, to allow for a wider user base and interoperability with other projects. In this presentation, I approach this problem from the perspective of my work as the project manager for the Advanced Research Consortium, an aggregator of data from digital humanities, proprietary scholarly resources, and library databases, to discuss how flexible data management enriches digital humanities projects.
                    
                
                
                    
                        Marissa Nicosia,
                        Secretary Hand, Digital Interface: Sustainable Collaborative Research with Undergraduate Students
                    
                    This paper has been withdrawn.
                
                
                    
                        Jessica Otis,
                        Cui Bono? Costs, Benefits, and Priorities in Digital Sustainability
                    
                    Digital humanists are good at making do: cobbling together projects with "free" resources, volunteer labor, time-limited funding, and access to university infrastructures. But while we are generally more aware of economic realities than other humanists—recognizing concepts such as overhead costs and fringe benefit rates—there is still a widespread misunderstanding about the real costs of digital scholarship, especially sustainability costs. In part this is because digital humanists from book-based disciplines have been conditioned to expect scholarly immortality. Books are distributed into university libraries with low marginal costs and survive for decades or centuries. Eventually, librarians must decide if books are valuable enough or being used enough to justify their ongoing preservation costs, but there is no additional work required from the author. Yet unlike books, digital projects have significant sustainability costs that escalate over time and require regular reassessment over how, and how long, to keep them online.
                    This paper argues digital humanists must learn to understand the real costs of digital sustainability and assess the benefits of keeping projects online. It will frankly discuss RRCHNM's backlist of digital projects, and the socioeconomic and political foundations of our analyses of costs and benefits for sustaining those projects. Crucially, it will filter that discussion through the lens of potential institutional priorities, from avoiding hacking to benefiting current students to advancing certain types of scholarship to generating prestige. When infrastructural resources are finite, we must learn to consider costs, benefits, and priorities when deciding how to employ them for digital sustainability.
                
                
                    
                        Olivia Wikle, Evan Peter Williamson, and Devin Becker,
                        Using Static Web Methodology as a Sustainable Approach to Digital Humanities Projects
                    
                    The web platforms adopted for digital humanities projects come with significant short and long term costs. In the realities of academic funding, this often results in huge sums sunk into outsourced development, contract work, and 3rd party subscriptions, reflecting an economic model that prioritizes purchasing systems over internal development of people and capacity. As DH practitioners, the time (or money paid to contractors) we must invest in managing servers, maintaining platform updates, and learning idiosyncratic administrative systems ultimately limits our ability to create and sustain unique, innovative projects. In response, librarians and DH practitioners are reexamining DH platforms through a minimal computing lens, pursuing new project-development methods that minimize digital infrastructure as a means to maximize investment in people, growing agency, agility, and long term sustainability in both the organization and digital outputs. Eager to explore this potential, faculty librarians at University of Idaho have been developing digital collections, scholarship projects, and instructional content using static web tools for more than five years, beginning with the digital collections template CollectionBuilder and expanding to include projects such as oral history exhibits, deep maps, and digital editions. This development approach, which we call Lib-Static, seeks to increase the return on learning new technical skills that all digital projects require, while also establishing technical solutions and social workflows that more closely match the structure of academic work cycles and DH project needs. In particular, the static web approach encourages the creation of preservation-ready project data, enables periods of iterative development, and capitalizes on the low-cost/low-maintenance characteristics of statically-generated sites to optimize limited economic resources and personnel time. This presentation will introduce the Lib-Static development methodology as a provocation to rethink DH infrastructure choices, asking how our frameworks can build internal skills, collaboration, and empowerment to generate more sustainable digital projects.
                
            
        
        
            
                
                    Bibliography
                    
                        @ThatAndromeda. (2021). 
                        Twitter. 10 Nov 2021,  
                        https://web.archive.org/web/20211122202905/https://threadreaderapp.com/thread/1458445616409939971.html (accessed 1 Dec 2021).
                    
                    
                        Almas, Bridget. (2017). “Perseids: Experimenting with Infrastructure for Creating and Sharing Research Data in the Digital Humanities.” 
                        Data Science Journal, 16: 19, DOI: 
                        https://doi.org/10.5334/dsj-2017-019. 
                    
                    
                        Barats, Christine, Valerie Schafer, and Andreas Fickers. (2021). “Fading Away… The challenge of sustainability in digital studies”, 
                        DHQ 14.3, 
                        http://www.digitalhumanities.org/dhq/vol/14/3/000484/000484.html (accessed November 22, 2021).
                    
                    
                        Deger-Pawlicka, Urszula. (2021). “Infrastructuring digital humanities: On relational infrastructure and global reconfiguration of the field. 
                        Digital Scholarship in the Humanities fqab086: 
                        https://doi.org/10.1093/llc/fqab086 (accessed 18 Nov 2021).
                    
                    
                        Dombrowski, Quinn. (2014). “What Ever Happened to Project Bamboo?” 
                        Literary and Linguistic Computing 29.3: 326–339 
                        https://doi.org/10.1093/llc/fqu026 (accessed 22 Nov 2021).
                    
                    
                        Drucker, Johanna. (2021). “Sustainability and Complexity: Knowledge and Authority in the Digital Humanities.” 
                        Digital Scholarship in the Humanities, 36 supp_2: ii86–ii94, 
                        https://doi.org/10.1093/llc/fqab025 (accessed 20 Nov 2021).
                    
                    
                        Gil, Alex. (2015). “The User, the Learner and the Machines We Make.” 
                        Minimal Computin,. 
                        http://go-dh.github.io/mincomp/thoughts/2015/05/21/user-vs-learner (accessed 19 Nov 2021).
                    
                    
                        Humanities Computing Media Centre at the University of Victoria. (2021). 
                        The Endings Project. 
                        https://endings.uvic.ca (accessed 22 Nov 2021).
                    
                    
                        Latour, Bruno. (2005). 
                        Reassembling the Social: An Introduction to Actor-Network Theory. Oxford University Press. 
                    
                    
                        Lincoln, Matthew. (2020). “From Supercomputer to Static Site: Boiling Down Big Research Data for Preservation and Usability.” code4lib 2020, Pittsburgh, PA, https://kilthub.cmu.edu/articles/presentation/From_​Supercomputer_​to_​Static_​Site_​Boiling_​Down_​Big_​Research_​Data_​for_​Preservation_​and_​Usability/18280082 (accessed 2 April 2022).
                    
                    
                        Otis, Jessica. (2021). “Managing the Digital Backlist: Sustaining, Preserving, and Deleting Old Projects.” Digital Humanities Summer Institute Colloquium (virtual). June 15, 2021, 
                        https://dhsi.org/timetable/event/institute-lecture-jessica-otis (accessed 15 Nov 2021).
                    
                    
                        Posner, Miriam. (2013). “No Half Measures: Overcoming Common Challenges to Doing Digital Humanities in the Library.” 
                        Journal of Library Administration, 53.1: 43-52, 
                        https://doi.org/10.1080/01930826.2013.756694 (accessed 19 Nov 2021). 
                    
                    
                        Sayers, Jentery. (2016). “Minimal Definitions.” 
                        Minimal Computing, 
                        http://go-dh.github.io/mincomp/thoughts/2016/10/02/minimal-definitions (accessed 18 Nov 2021).
                    
                    
                        Thylstrup, Nanna Bonde. (2018). 
                        The Politics of Mass Digitization. Cambridge, MA: MIT Press.
                    
                    
                        Visual Media Workshop at the University of Pittsburgh. (2021). 
                        The Socio-Technical Sustainability Roadmap, 
                        http://sustainingdh.net (accessed 22 Nov 2021).
                    
                
            
        
    



        
            
                Following myriad controversies, including harassment campaigns and dissemination of conspiratorial narratives 
                
                    (Bounegru
                
                
                    et al.
                
                
                    , 2018; Jeong, 2019)
                
                , genocides
                
                    (Mozur, 2018)
                
                 and political extremism
                
                    (Ganesh and Bright, 2020)
                
                , social media platforms and cloud hosts have deployed additional measures to moderate user-generated contents. YouTube, Facebook and Twitter, in particular, have further developed content moderation techniques that prevent the circulation of “problematic information” 
                
                    (Jack, 2017)
                
                 via deletion or “deplatforming” 
                
                    (Rogers, 2020)
                
                , automatically flagging content as “misleading” 
                
                    (Gorwa, Binns and Katzenbach, 2020)
                
                , and demoting or “shadow-banning” 
                
                    (Myers West, 2018)
                
                user posts across search and recommendation results
                
                    (Goldman, 2019)
                
                . To date, millions of content associated with hate speech, COVID-19 conspiracy theories and incitement to violence have been wiped out from Twitter 
                
                    (Al Jazeera, 2021)
                
                , YouTube 
                
                    (Keulenaar, Burton and Kisjes, 2021)
                
                , Instagram 
                
                    (Francis, 2021)
                
                , and Facebook 
                
                    (Lerman, 2020)
                
                .
            
            
                Though it is essential for regulating any kind of public sphere, the imperative to delete and otherwise obfuscate problematic content has brought new challenges to new media and digital humanities scholars in at least three ways. Most directly, it renders problematic content unarchivable by default, making almost impossible the study of already precariously archived Web and social media data
                
                    (Brügger and Schroeder, 2017)
                
                . Second, moderated platforms provide little information on how they practice moderation. This prevents public efforts from scrutinizing the normative framework platforms adopt to determine what can and cannot be said. In turn, the obfuscation of moderated content and moderation practices further hampers studies on speech moderation and norms as a key historical and societal practices to any modern-day society. Speech norms join a long history of legal, social and political measures to prevent the normalization of problematic histories, and have been implemented in the form of laws, social conventions and civil right campaigns in a range of media types.
            
            
                Thus far, scholars have relied on a handful of improvised methods for studying moderation and moderated contents. With the exception of Twitter’s Enterprise API 
                
                    (Twitter, 2021)
                
                , platforms rarely outsource information about what specific contents they have moderated and why. While most studies in platform governance focus on written documentation and leaked documents from ex-platform employees
                
                    (Wall Street Journal, 2021)
                
                , digital methods research tends to rely on information provided by Application Program Interfaces (APIs). These researcher practices have yet to be systematically combined, and are still prey to changing APIs 
                
                    (Perriam, Birkbak and Freeman, 2020)
                
                 and reprisals for illegal scraping of information that is otherwise not publicly provided
                
                    (Bond, 2021)
                
                .
            
            
                On this matter, this paper discusses a new “digital forensics”: a set of methods one can use to reconstruct platform and user traces. In other words, it proposes methods to reconstruct the scene after or on which platform or user data has disappeared in the context of one specific platform effect: content moderation. Drawing from two case studies on Twitter and YouTube’s moderation of COVID-19 misinformation between January and April of 2020 
                
                    (de Keulenaar
                
                
                    et al.
                
                
                    , Forthcoming)
                
                , and hate speech on YouTube between 2007 and 2018 
                
                    (de Keulenaar
                
                
                    et al.
                
                
                    , 2021)
                
                , it proposes a web historiography (Brügger, 2013) of content moderation policies and techniques with a combination of HTML scraping, retrieval of moderation metadata via APIs, and detection of content availability through dynamic archival of moderated contents. 
            
            
                It describes the potentials and shortcomings of four methods: 
            
            
                
                    A contextualization of content moderation practices
                    . Using the Wayback Machine to trace changes in content moderation policies, this method consists in systematically annotating changes to (a) how the platform decides what is and is not problematic; and (b) observing and documenting the techniques the platform uses to moderate contents respectively.
                
                
                    Dynamic archiving of content susceptible to being moderated
                    . This consists in first developing a taxonomy of problematic speech based on what platform content moderation policies consider to be problematic, such as hate speech (examples being speech that targets gender, racial and other identities), misinformation (such as information that contradicts COVID medical authorities) and “borderline content” (information likely to infringe upon policies in the future, usually described as conspiratorial or fringe discourses). This allows us to design queries that reflect problematic speech, which we use to collect corresponding tweets, videos and comments daily, as well as its corresponding metadata and platform effects (search and recommendation rankings, etc.). 
                
                
                    Reverse-engineering content moderation practices
                    . Using Twitter Academic and YouTube’s standard APIs to reverse-engineer moderation practices, collecting metadata for: (1) the availability of problematic contents per day; (2) by-products of algorithmic moderation, such as their ranking in search and recommendation results over time, and flags and prompts we scrape using Selenium; and (3) user engagement in moderated contents. 
                
                
                    Tracing the effects of the disappearance of moderated data
                     in one platform by looking at its migration in other platforms. This implies looking at how users react to the practice of moderation (for example, what they say about “cancelling”, “deplatforming”, “deleting”, “shadowbanning” and other forms of platform interventions), as well as how they curtail these sanctions by access sanctioned content in alternative or “alt-tech” platforms like Telegram, Bitchute and Parler.
                
            
            
                Though imperfect, we aim to demonstrate how this ensemble of methods allows one to contextualize moderation practices, such as deplatforming, algorithmic demotion and flagging, within content moderation policies around hate speech and misinformation. We argue that they allow researchers to surface volatile content moderation practices, as well as map the larger effects of deplatforming across the Web in the fragmentation of users’ information diets across a fringe-to-mainstream social media ecology. Most importantly, we propose this method as a way to systematically document online speech moderation practices and contribute to a history of speech norms across political contexts and media types.
            
        
    



        
            
                Idea
                In literary and linguistic studies, the first sentence of a narratological context is a regularly studied object (on this, among others, Alt 2020, Haubrichs 1995, Hirdt 1974, Queng 2019, Miller 1965, Neuhaus 2019, Raulff 2019, Retsch 2000, Selbmann 2019). This is hardly surprising, since the first sentence has been regarded since Wolfgang Iser's study The Act of Reading as the entrance into the text through reading, as the key point of interaction between text and reader (1976: 38). In the richness of its various forms, the first sentence reveals “the treasures of literature in nuce” (Alt 2020: 18) and, with Alain Robbe-Grillet, it could be put forward that literary history is to be written from the study of its opening sentences (1992: 38).
                A systematic, digitally supported study of “first sentences” has yet to be carried out. Occasionally, corpora of first sentences in German have been collected by hand (Beck 1992, Beck 1993, Wolkersdorf 1994) and attempts have been made to draw up a typology of the first sentence in literature on the basis of selected individual analyses (most recently Alt 2020). A systematic categorisation on the basis of a semi-automated, larger corpus of research – as presented here – seems helpful. There are similar studies that inquire into the quintessence of the poetic in literature through its countability (cf. for example Moretti 2009, also Fischer/Strötgen 2015, Fischer/Jäschke 2018a/b); a single quantifying study dealing decidedly with German-language narrative beginnings (not first sentences) can be found in the work of Herrmann 2018.
                The aim of the corpus “First Sentences in German-Language Literature” is to address the “lack of an overall view” (Alt 2020: 246) of all previous studies on first sentences. To this end, a data corpus is created and published, on which an initial evaluation will be undertaken in an interlocking of quantitative and text-analytical approaches.
            
            
                Project
                Several full-text, open access corpora (
                    Deutsches Textarchiv, 
                    Zeno, etc.), from which texts were extracted according to genre, serve as source material. It is clear that although the existing full-text offerings provide varying degrees of structural information about the respective document, the automatic delimitation of closed text units is often non-trivial and not possible reliably without individual examination (e.g. in the case of anthologies, texts with several chapters, texts in several volumes). However, this is the prerequisite for extracting the first sentences. In addition, the beginning of the "poetic text" cannot always be clearly localised automatically, e.g. due to prefaces, dedication texts or introductions.
                
                Furthermore, the delimitation of “first sentences” is a semantic problem. Sentences can be understood as grammatical-analytical units that are separated from each other by certain punctuation marks, which accommodates machine processing. However, the signs used to delimit a sentence differ and change considerably. The absolute selectivity of some punctuation marks is also questionable depending on the context, which is why sentences are sometimes to be understood as units of meaning in which punctuation marks have a structuring but not interrupting function (cf. fig. 2a/b). Should we therefore rather speak of a flowing “beginning” or “start”? Thus, areas of vagueness play into the determination of "first sentences", which in turn can affect corpus consistency and comparability.
            
            
                Evaluation
                The currently created corpuses of novels, novellas and fairytales is completely encoded in TEI, including metadata and source information, including positional information (available at 
                    ). Depending on the genre, the number of first sentences ranges between 100 and 1,000 entries. With the help of the manually and automatically created annotations, the corpus can be analysed and visualised according to various parameters, such as date of publication, text genre, gender of author, references to persons, places or time in the text (cf. Fig. 1c) or length of the entire text. In addition, it is documented which selection criteria the respective data sources were subject to and how this should be taken into account in the evaluation with regard to the balance of the corpus (cf. Hug/Boenig 2021). To disseminate the corpus, the Twitter project 
                    @satzomat was launched in 2021, which sends two first sentences daily (cf. Figures 1–3).
                
                The aim is to create a “typology of incipits” with the help of computer-philological evaluation methods and to ask to what extent genres determined certain types of first sentences in the course of history (e.g. landscape image, frame story) and whether further correlations can be determined with the help of the metadata and annotations (see the project page 
                    for more information).
                
            
            
                Figures
                
                    
                        
                    
                
                
                    
                        
                    
                
                
                    
                        
                    
                
                Fig. 1a/b/c: Novella beginnings
                
                    
                        
                    
                
                
                    
                        
                    
                
                
                    
                        
                    
                
                Fig. 2a/b/c: Novel beginnings
                
                    
                        
                    
                
                
                    
                        
                    
                
                
                    
                        
                    
                
                Fig. 3a/b/c: Fairy tale beginnings
            
        
        
            
                
                    Bibliography
                    
                        Alt, Peter-André (2020): 
                        ‘Jemand musste Josef K. verleumdet haben …’ Erste Sätze der Weltliteratur und was sie uns verraten. München: Beck.
                    
                    
                        Beck, Harald (1992)
                        :
                        Roman-Anfänge. Rund 500 erste Sätze. Zürich: Haffmans.
                    
                    
                        Beck, Harald (1993)
                        :
                        Romanenden. Rund 500 letzte Sätze. Zürich: Haffmans.
                    
                    
                        Fischer, Frank / Strötgen, Jannik (2015): “Wann findet die deutsche Literatur statt? – Zur Untersuchung von Zeitausdrücken in großen Korpora.” Presented at the DHd2015 
                        Von Daten zu Erkenntnissen: Digitale Geisteswissenschaften als Mittler zwischen Information und Interpretation. 2. Tagung des Verbands "Digital Humanities im deutschsprachigen Raum" (DHd2015), Graz: Zenodo. 
                         [last access: 9. December 2021]
                    
                    
                        Fischer, Frank / Jäschke, Robert (2018a): “Liebe und Tod in der Deutschen Nationalbibliothek. Der DNB-Katalog als Forschungsobjekt der digitalen Literaturwissenschaft.” Presented at the DHd 2018 Kritik der digitalen Vernunft. 5. Tagung des Verbands "Digital Humanities im deutschsprachigen Raum" (DHd 2018), Köln: Zenodo. 
                         [last access: 9. December 2021]
                    
                    
                        Fischer, Frank / Jäschke, Robert (2018b): “Ein Quantum Literatur. Empirische Daten zu einer Theorie des literarischen Textumfangs.” DFG-Symposium “Digitale Literaturwissenschaft”. Villa Vigoni, 9.–13. Oktober 2017. [unpublished]
                    
                    
                        Haubrichs, Wolfgang (1995): “Kleine Bibliographie zu “Anfang” und “Ende” in narrativen Texten (seit 1965)”, in: Zeitschrift für Literaturwissenschaft und Linguistik 25, 99: 36-50.
                    
                    
                        Herrmann, Berenike (2018): “Anschaulichkeit messen. Eine quantitative Metaphernanalyse an deutschsprachigen Erzählanfängen zwischen 1880 und 1926”, in: Köppe, Tilmann / Singer, Rüdiger (eds.): 
                        Show, don’t tell: Konzepte und Strategien anschaulichen Erzählens. Bielefeld: Aisthesis 167-212.
                    
                    
                        Hirdt, Willi (1974): “Incipit. Zu einer Poetik des Romananfangs”, in: Romanische Forschungen LXXXVI: 419-436.
                    
                    
                        Hug, Marius / Boenig, Matthias (2021): Die Geschichte der Digitalen Bibliothek, oder: Aller guten Kurationen sind drei+: 
                         [last access: 9. December 2021]
                    
                    
                        Iser, Wolfgang (1976): Der Akt des Lesens. Theorie ästhetischer Wirkung. München: Fink.
                    
                    
                        Miller, Norbert (1965): 
                        Romananfänge. Versuch zu einer Poetik des Romans. Berlin: Verl. Literarisches Colloquium.
                    
                    
                        Moretti, Franco (2009): Style, Inc Reflections on Seven Thousand Titles (British Novels, 1740-1850), in: 
                        Critical Inquiry 36, I: 134-158.
                    
                    
                        Neuhaus, Stefan (2019): “Aber wehe, wehe, wehe! Wenn ich auf das Ende sehe!!” Wie in Romanen und Erzählungen durch Anfang und Ende ein Rahmen erzeugt wird, in: Neuhaus, Stefan / Weber, Petra (eds.): 
                        Anfangen und Aufhören. Paderborn: Wilhelm Fink 141-157.
                    
                    
                        Queng, Jesse (2019): “Syntaktische Strukturen als poetologisches Mittel des Anfangens in der Prosa: Der erste Satz von Heinrich Bölls Irischem Tagebuch”, in: Neuhaus, Stefan / Weber, Petra (eds.): 
                        Anfangen und Aufhören. Paderborn: Wilhelm Fink 89-101.
                    
                    
                        Raulff, Ulrich (2019): “Letzte Sätze”, in: 
                        Zeitschrift für Ideengeschichte 13: 129-142.
                    
                    
                        Retsch, Annette (2000): 
                        Paratext und Textanfang. Würzburg: Königshausen & Neumann.
                    
                    
                        Richardson, Brian (2008): 
                        Narrative Beginnings: Theories and Practices. University of Nebraska Press.
                    
                    
                        Robbe-Grillet, Alain (1992): “Warum und für wen schreibe ich”, in: Bühler, Karl Alfred (ed.): 
                        Robbe-Grillet zwischen Moderne und Postmoderne - "nouveau roman", "nouveau cinéma" und "nouvelle autobiographie". Tübingen: Narr.
                    
                    
                        Selbmann, Rolf (2019): “Lauter erste Sätze”, in: Neuhaus, Stefan / Weber, Petra (eds.): 
                        Anfangen und Aufhören. Paderborn: Wilhelm Fink 67-87.
                    
                    
                        Wolkersdorfer, Andreas (1994): 
                        Der erste Satz. Österreichische Romananfänge 1960-1980. Wien: WUV Univ.-Verl.
                    
                
            
        
    



        
            
                The #Metoo movement, which burgeoned on social networking sites in October 2017, saw a series of private confessions and a large number of people experiencing the massive social movement. This movement became a milestone of the possibilities of transnational solidarity and had an unparalleled and powerful impact on society. In contrast, it has provoked a huge backlash, with social networking sites becoming the place of a wide variety of slanderous exchanges; some hurled abuse, others criticized that the confessions seemed unreliable and undermined their value.
            
            The offensive expressions seriously impact the persons to whom they are directly addressed and those around them. People shrank at outrageous attacks, their dignity is violated, and they are often forced to be silent. However, if we can confirm any patterns in the abusive expressions and actions of others, which we currently perceive only as absurdity, and if we can make sure that these are clichés, our fear and psychological damage may be alleviated. Furthermore, if we wish to dispirit this kind of slander, we must begin by understanding the mechanisms of slander. Then how do we understand the apparent variety of slanderous expressions that have appeared on social networking sites in the #MeToo movement?
            The appearance of slanderous expressions is diverse. However, there seem to be some typical psychological mechanisms and styles of expression that lead to those writing. Kate Manne, for example, uses the methods of analytic philosophy to examine the logic of misogyny and present a typology of the system (Manne, 2017). She argues that excessive bashing occurs when women visibly resist or violate social norms. In her view, these attacks are "not a matter of the psychology of individuals," but by the collective surveillance of women and the punishment of those who do not comply. She further distinguishes misogyny as a "law enforcement" branch; a combative system that attacks violators of the patriarchal order, and sexism as a "justificatory" branch of a patriarchal order; a theoretical system that justifies and theorizes patriarchal social norms and gender roles.
            In this research, we aim to understand the typology theory obtained in the field of humanities, as exemplified above, from the quantitative point of view in the case of the #Metoo movement on Twitter. That is, we examine correspondences between qualitative theory and quantitative results from the Twitter data. 
            First, we collect social media posts (tweets) on Twitter about the following cases that have been popular in the #Metoo movement in Japan: 1) A case of Shiori Ito, who later became the symbol of Japan's #MeToo movement; she held a press conference in May 2017 accusing the journalist and published the book, 2) #KuToo movement; Yumi Ishikawa started a campaign to outlaw corporate practices that force women to wear high heels as sexual discrimination or harassment, 3) Flower Demo; The acquittals in four sexual assault cases in March 2019 triggered a nationwide grassroots movement.
            
                Next, we extract slanderous words by discovering topics from the Twitter data. Specifically, we will apply topic models (e.g.,Latent Dirichlet Allocation, clustering algorithm (Blei, 2012)) and topic mining methods (e.g., Data Polishing (Uno, 2017; Hashimoto, 2021.)) for automatically finding the topics. By referring to the qualitative study of humanities, we typify and interpret the micro topics. In this way, we add objective explanations with an exhaustive approach to the concepts of the qualitative approach. At the same time, we apply the concepts from the qualitative approach to make sense of the typologies that are difficult to understand by using the exhaustive approach. In this way, we aim to fill in the methodological gaps between the two approaches and to get a better overall picture of slander.
            
        
        
            
                
                    Bibliography
                    
                        Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55, 77-84.
                    
                    
                        Hashimoto, T., Shepard D., Kuboyama, T., Shin, K., Kobayashi, R., and Uno T. (2021). Analyzing Temporal Patterns of Topic Diversity using Graph Clustering, The Journal of Supercomputing, 77, 4375-4388.
                    
                    Manne, K. (2017). Down Girl: The Logic of Misogyny. Oxford University Press.
                    
                        Uno, T., Maegawa, H., Nakahara, T., Hamuro, Y., Yoshinaka, R., and Tatsuta, M. (2017). Micro-clustering by Data Polishing. IEEE BigData 2017, 1012-1018.
                    
                
            
        
    



        
            
                Introduction
                Invarsson (2021) suggests a digital epistemology that is to be “understood as an attempt to do digital humanities without being committed to digital tools and objects”. While it is an intriguing idea to leave digital tools and methods out of the equation in order to discern the true epistemological core of DH, we believe that it is equally possible to argue that the use of specific tools virtually shapes and influences the epistemology of DH, or as Nietzsche (1882) put it: “Unser Schreibzeug arbeitet mit an unseren Gedanken
                    
                        
                            Translation: “Our writing tools [in Nietzsche’s case: his new typewriter] shape our thoughts”.
                        
                    ”. Today, Nietzsche’s observations on writing tools can be easily extended to all kinds of research tools, allowing us to ask questions about the epistemological implications of tools for the digital humanities (see Dalbello, 2011; Drucker, 2002; Ramsay & Rockwell, 2012). As there are manifold, rather diverse tools that are used in DH, there is a certain tradition for tool directories that systematically list and categorize different tools. One of the most popular directories is TAPoR
                    
                        
                            TAPoR: https://tapor.ca/home
                        
                    , which has steadily evolved and by now includes more than 1,600 tools. 
                
                The TAPoR list of tools has been used lately to extract and analyze tools mentioned in DH abstracts (Barbot et al. 2019, Fischer & Moranville, 2020b) and tutorials (Fischer & Moranville, 2020a). The motivation of these analyses is primarily to identify relevant and widely used tools in order to make them sustainably available via infrastructures like the 
                    Social Sciences & Humanities Open Marketplace
                    
                        
                             SSH Open Marketplace: https://marketplace.sshopencloud.eu/
                        
                    . While the previous studies so far have only looked at comparatively small corpora, we suggest to enhance the scope of DH tool studies by using a large corpus of DH journal articles (
                    Computers and the Humanities, 
                    Digital Humanities Quarterly, 
                    Literary and Linguistic Computing/
                    Digital Scholarship in the Humanities). The corpus comprises 3,737 articles and covers a time span from 1966-2020, which allows for diachronic analyses of tool usage in DH. In addition to using a larger corpus, we also propose an approach to automatically increase the size of the TAPoR tool list.
                
            
            
                Experiments with the TAPoR tool list
                For our experiments we followed the approach described by Barbot et al. 2019 and also used the TaPOR directory to derive queries to find tool occurrences in our corpus. All in all, we found 319 different tools being mentioned throughout the corpus
                    
                        
                            For a complete list see https://docs.google.com/spreadsheets/d/1BtDVo_2A6a1cLPQCZ8CriNcSGHz3UuVc2mxTsM-ZCxo/edit?usp=sharing
                        
                    . Figure 1 shows the most frequent 25 tools in the overall corpus.
                
                
                    
                    Top 25 tools mentioned in the corpus.
                
                It is noticeable that among the top 25 DH tools we find many tools for text and data analysis, but also a large proportion of high-level programming languages. This is certainly a bias of the early days of humanities computing. Taking a closer look at the diachronic development reveals the natural rise and fall of programming languages, with the steady rise of 
                    Python in the DH since the beginning of the 2010s being particularly prominent (see Figure 2)
                    
                        
                             An interactive version of the plots in Figures 2+3 alongside with more plots can be found here: https://bbrause.github.io/tools-in-dh/
                        
                    .
                
                
                    
                    The rise and fall of programming languages in DH.
                
                To provide some more high-level insights, we also did a cooccurrence analysis of the most frequent 150 tools, i.e. tools that are mentioned together in an article (see Figure 3). Such cooccurrence analyses could yield insights into typical tool combinations and more complex workflows, for instance the use of 
                    Brat to annotate 
                    Twitter data and the use of 
                    SentiStrength to perform sentiment analyses of tweets.
                
                All in all, Figures 1-3 show some strong potential to analyze tools to find out more about their epistemological implications of DH. However, the predominance of outdated programming languages and the absence of state-of-the-art tools such as 
                    spaCy or transformer architectures clearly shows large gaps in the TAPoR list.
                
                
                    
                    UMAP 2D projection of the top 150 most-frequent tools and their nPMI cooccurrence scores (point size indicates numbers of occurrences).
                
            
            
                Query expansion experiments by means of tool embeddings
                
                    To fill these gaps, we conducted a second experiment in which we used BERT embeddings to expand our list of tools, as proposed by Wevers & Koolen (2020). We created embeddings for the 25 most frequent tools (see Figure 1) and looked for their nearest neighbors, i.e. words that have similar embedding vectors as the 25 most frequent tools (see Figure 4). 
                
                
                    
                    Ten nearest neighbors for the embeddings of “python”, “rstudio” and “fortran”, ranked by their cosine similarity.
                
                This approach allows us to identify tools that are not listed in the TAPoR directory directly, but that are mentioned in similar article contexts as the TAPoR-listed tools. Obviously, not all the nearest neighbors identified in this way are actual tools, but if we rank the results according to the number of nearest neighborhoods for the top 25 tools, there are indeed many promising results in the higher ranks
                    
                        
                             The nearest neighbors for each of the 25 most frequent tools as well as a ranked overall list is available here: https://docs.google.com/spreadsheets/d/1iipWwyk7wVcaSzzpEq-W5_vjTe2FO-dmjk_IGjMitEc/edit?usp=sharing
                        
                    . To give just one example: 
                    XSLT (Extensible Stylesheet Language Transformations) has a fairly high score of 16, which means it was in the nearest neighbors of 16 of the 25 most frequent tools from the initial list. In the top ranks we find many other promising tool candidates, such as 
                    RDF, 
                    SGML, 
                    mySQL and also more generic concepts, such as 
                    NLP and 
                    parsing, which could be interpreted as tool super categories.
                
            
            
                Conclusion and next steps
                The experiments by Barbot et al. 2019 and Fischer & Moranville, 2020a/b as well as our follow-up experiments with a larger corpus of texts demonstrate that the empirical analysis of tool mentions in DH publications can be used to discern patterns in the diachronic use of different types of tools. This allows us to explore the effects of tools as rapidly evolving epistemological frameworks in the DH. At the same time, it became clear that the static list of tools as provided by TAPoR has obvious gaps, as the tool landscape is evolving swiftly. We therefore plan to include further directories in follow-up studies, including 
                    ProgrammingHistorian
                    
                         Programming Historian: https://programminghistorian.org/en/lessons/
                    , 
                    forTEXT
                    
                         forTEXT: https://fortext.net/
                    , 
                    DigiHum
                    
                         DigiHum: https://digihum.de/tools/
                    , 
                    DMI (Digital Methods Initiative)
                    
                         DMI: https://wiki.digitalmethods.net/Dmi/ToolDatabase
                    , 
                    DH Toychest
                    
                         DH Toychest: http://dhresourcesforprojectbuilding.pbworks.com/w/page/69244319/Digita
                    , etc. In this article, we illustrated the benefits of an embeddings-based approach to further expand these static lists of tools. Our next steps will be to extend our corpus to also include articles from neighboring disciplines, such as computational linguistics, computational social sciences, information science and others. We also plan to expand the nearest neighbor search beyond the limit of the 25 most frequent tools and to filter the results list manually, to identify reasonable tools. 
                
            
        
        
            
                
                    Bibliography
                    
                        Barbot, L., Fischer, F., Moranville, Y. & Pozdniakov, I. (2019). Which DH tools are actually used in research? Published via weltliteratur.net – A Black Market for the Digital Humanities, https://weltliteratur.net/dh-tools-used-in-research/
                    
                    Bush, V. (1945). As we may think. The Atlantic Monthly, 176(1), 101-108.
                    
                        Dalbello, M. (2011). A genealogy of digital humanities. Journal of Documentation.
                    
                    Drucker, J. (2002), “Theory as praxis: the poetics of electronic textuality”, Modernism/Modernity, Vol. 9, November, pp. 683-91. 
                    
                        Fischer, F. & Moranville, Y. (2020a). DH tools mentioned in "The Programming Historian"? Published via weltliteratur.net – A Black Market for the Digital Humanities, https://weltliteratur.net/dh-tools-programming-historian/
                    
                    
                        Fischer, F. & Moranville, Y. (2020b). Tools mentioned in DH2020 abstracts. ublished via weltliteratur.net – A Black Market for the Digital Humanities, https://weltliteratur.net/tools-mentioned-in-dh2020-abstracts/.
                    
                    
                        Ingvarsson, J. (2020). Digital Epistemology: An Introduction. In Towards a Digital Epistemology (pp. 1-28). Palgrave Macmillan, Cham.
                    
                    
                        Nietzsche, F. (1882). Letter 202. An Heinrich Köselitz in Venedig (Typoskript). Nietzsche Source – Digital Critical Edition (eKGWB): http://www.nietzschesource.org/#eKGWB/BVN-1882,202
                    
                    
                        Ramsay, S., & Rockwell, G. (2012). Developing things: Notes toward an epistemology of building in the digital humanities. Debates in the digital humanities, 75-84.
                    
                    
                        Wevers, M., & Koolen, M. (2020). Digital begriffsgeschichte: Tracing semantic change using word embeddings. Historical Methods: A Journal of Quantitative and Interdisciplinary History, 53(4), 226-243.
                    
                
            
        
    



        
            DH 2022
            
                Digital Archives and Political Legacies: Will the Obama Corpus Stand the Test of Time?
            
            Short Abstract: 
            
                Drawing on extensive interviews with archivists and digital strategists associated with the WhiteHouse.gov domain developed during the administration of U.S. President Barack Obama (2009-2017), as well as primary research in archival collections, this presentation explores how attempting to preserve a large and important digital archive for posterity can both solidify and undermine a political legacy.
            
            
                Longer Description: 
            
            
                The enormous treasure trove of digital materials associated with the first Black president of the United States included text from hundreds of speeches and executive orders, video and sound files, scrapes of social media postings, and a vast repository of structured data available in various open formats. This bounty of digital information would obviously be of interest to future historians, political scientists, rhetoricians, media scholars, researchers in Black studies, and scholars in science and technology studies. Archivists at the National Archives, Internet Archive, and Library of Congress seemed to be ready to spring into action to preserve it all, beginning with harvesting materials from eyewitnesses to Obama’s historic inauguration and ending with high-profile releases of tidy zipped .csv files from the POTUS and FLOTUS Twitter accounts after the transfer of power to his Republican successor had been completed. Yet behind the scenes, the story of preserving a political legacy is much more complex, particularly when issues about personal privacy, national security, the ownership of intellectual property, and platform governance come into play. This presentation draws on extensive field research done in Washington D.C. – including interviews with digital designers and strategists who managed both the front and the back end of Obama’s communication infrastructure –as well as archival research in both spaces for born digital materials and traditional documentary evidence from the written record. 
            
            
                This presentation also discusses how political legacies of interest to digital humanists might be eroded with attention to comparative case studies from government records of world leaders from the UK, France, and Germany. It also looks at how a digital legacy exists in a larger rhetorical and political historical context. For example, the Trump administration was quick to take down many public records made digitally available in the Obama era and even removed records about troop mobilization that had been posted during the pre-Obama Bush era of Republican political control. This context involves having digital humanists consider issues about personalization and surveillance, as the Obama administration reversed longstanding cookie policies from the Bush and Clinton White Houses. At the same time as official digital archiving practices might be noteworthy, it is also important to acknowledge the labor of DIY archivers and hacktivists, such as the maintainer of the Trump Twitter Archive. Finally, this presentation will dramatize the importance of reconstructing missing digital objects of study, such as the White House website designed for would-be president Hillary Clinton, which is not available to researchers and may become irretrievably lost without a clear mandate for preservation. This lively, provocative, and media-rich short presentation by a scholar of DH with a two-decade record of deep involvement in the field should be of interest to many attendees at DH2022.     
            
        
    



        
            Historians have since long stressed the political and cultural functions of memory and heritage for societies. The more born-digital data have a past of their own, the more memory has also become a topic of interest for data scientists (Au Yeung and Jatowt 2011, Keegan and Brubaker 2015, Graus et.al. 2018, West and Leskovec 2021). Stringently connected to memory but much less studied in data science is our experience of time (Jatowt et.al. 2015 and Van Eijnatten and Huijnen 2021 being notable exceptions). The influential theory of François Hartog, for example, explains the memory boom from the establishment of a ‘time regime of the present’ at the end of the 1980s (Hartog 2015). Aleida Assmann goes against Hartog’s subsequent assertion that an increasingly shorter present is all we are left with (Assmann 2020, 139). Instead, she puts forward her notion of cultural memory to replace ideas of temporal ruptures with a model that ‘emphasizes the ineluctable entanglement of [past, present and future]’ (Assmann 2020, 195-6). 
            Departing from this theory, this study proposes a computational approach to study the relation between the present and the past in twentieth century newspapers by analysing trends in phrases of the format ‘n years ago’. Obviously, there is a plethora of ways in language can evoke the past. However, there are two arguments that justify singling out ‘n years ago’. First, ‘n years ago’ is, as an expression, ubiquitous and syntactically stable in Dutch newspaper language throughout the studied period of the twentieth century. Second, unlike explicit references to years, events or persons in the past, ‘n years ago’ intricately ties the past to the present. The phrase presents the past—and keeps it present—
                as something useful for the present. This notion of ‘useful past’ or ‘present past’ (Paul 2015, 25-27) forms part of Assmann’s critique of Hartog’s theory of temporal orders. 
            
            
                Questions and method
                The questions that are at the center of this study are how trends in references to present pasts relate to named philosophies of time experience, but also which past remains present and how these trends change over time. It takes newspapers as data, because of the vital role they as the ‘first rough draft of history’ have played in memory culture throughout the twentieth century. This study is based on the digitized versions of the most important nation-wide and regional newspapers the Dutch National Library holds and has made available.
                    
                        
                            
                                www.delpher.nl/kranten
                            
                            . 
                        
                     The preliminary results presented here are based on the example of the national newspaper Telegraaf (1893-1989) of 10,000 documents (articles and advertisements) per year.
                    
                        
                            For the final paper, these titles will be complemented with other available newspaper titles with similar century-spanning scopes. Subsequently, all results will be aggregated to come to an encompassing picture of the present past in Dutch newspapers by means of references to ‘n years ago’. The Telegraaf was chosen as an example here, because it shows trends that are paradigmatic for most other newspapers.
                        
                     These documents have been rid of duplicates and cleaned with the help of Python’s NLTK package.
                    
                        
                            
                                https://github.com/PimHuijnen/looking_back_newspapers
                            
                            . Cleaning in this script is mostly restricted to the removal of punctuation and caps. Stop words are not removed to guarantee that the Dutch word for the English indefinite article ‘a’ (‘een’), which is part of any standard stop word list, remains part of the data. Similarly, the removal of numbers is no part of preprocessing to allow for phrases like ’10 years ago’(even though Dutch linguistic convention formally does not allow for numbers in running text).
                        
                    
                
            
            
                Analysis and results
                The analysis is done with Python scripts in Jupyter Notebooks and consists of three subsequent steps: 
                The first step is the extraction of a list of the most common trigrams ending with ‘year(s) ago’
                    
                        
                             The Dutch equivalent of ‘year(s) ago’ is for both one and more years written in the singular form ‘jaar geleden’.
                        
                     from the cleaned and sampled dataset. This list is sorted by decade and by frequency to get an idea of the years that newspapers most often use in the phrase ‘n years ago’ throughout the twentieth century. This indicates that single digit years (one – nine) make up the most common phrases of ‘n years ago’, along with decades (ten, twenty, etc.) and one hundred.
                
                
                    Table 1: Most common trigrams ending in ‘year(s) ago’ with their English translation from a sample of 10,000 documents from the national newspaper De Telegraaf per decade per million trigrams, 1890-1980.
                
                
                    
                        92
                        92
                        32
                        32
                        32
                        32
                        32
                        32
                        32
                        32
                        32
                        32
                    
                    
                        Trigram
                        English translation
                        1890
                        1900
                        1910
                        1920
                        1930
                        1940
                        1950
                        1960
                        1970
                        1980
                    
                    
                        
                            Een jaar geleden
                        
                        One year ago
                        1,95
                        4,02
                        4,62
                        8,81
                        21,77
                        13,23
                        16,06
                        19,51
                        17,90
                        16,16
                    
                    
                        
                            Twee jaar geleden
                        
                        Two years ago
                        1,70
                        2,10
                        2,70
                        3,18
                        7,51
                        5,60
                        12,67
                        22,43
                        27,79
                        31,44
                    
                    
                        
                            Twintig jaar geleden
                        
                        Twenty years ago
                        1,27
                        0,50
                        0,59
                        2,34
                        3,28
                        4,24
                        3,39
                        3,34
                        4,07
                        6,08
                    
                    
                        
                            Paar jaar geleden
                        
                        Few years ago
                        1,10
                        1,26
                        1,06
                        1,50
                        1,53
                        3,39
                        2,74
                        7,05
                        9,22
                        10,01
                    
                    
                        
                            Vier jaar geleden
                        
                        Four years ago
                        0,76
                        0,17
                        0,53
                        1,20
                        2,48
                        1,36
                        3,39
                        7,37
                        10,85
                        11,99
                    
                    
                        
                            Drie jaar geleden
                        
                        Three years ago
                        0,68
                        1,26
                        1,06
                        1,86
                        3,54
                        2,88
                        6,22
                        11,08
                        14,50
                        14,75
                    
                    
                        
                            Dertig jaar geleden
                        
                        Thirty years ago
                        0,59
                        0,42
                        0,26
                        0,72
                        1,86
                        2,71
                        2,42
                        2,92
                        1,89
                        3,09
                    
                    
                        
                            Tien jaar geleden
                        
                        Ten years ago
                        0,51
                        1,01
                        0,79
                        1,80
                        4,16
                        4,24
                        5,25
                        9,54
                        12,39
                        15,42
                    
                    
                        
                            Honderd jaar geleden
                        
                        One hundred years ago
                        0,42
                        0,50
                        0,73
                        1,14
                        2,08
                        2,54
                        1,86
                        2,86
                        2,02
                        1,98
                    
                    
                        
                            Acht jaar geleden
                        
                        
                            Eight years ago
                        
                        0,42
                        0,25
                        0,20
                        0,60
                        0,91
                        0,85
                        1,05
                        2,97
                        3,20
                        3,59
                    
                
                In a second step, the most common references of ‘n years ago’ are plotted, relative to one another, over time. Figures 1-3 show the trajectories of all single years together (figure 1), of all decades from ten to one hundred (figure 2) and of the single years one, ten, one hundred and two hundred (figure 3). These figures show that Dutch newspapers started to look back in time by the use of the phrase ‘n years ago’ since the 1930s. Once they did, the use of some variations of this phrase strongly gained traction, particularly in reference to the near past (one to ten years ago). 
                The final step of the analysis is to look at the actual years that the phrase ‘n years ago’ referred to throughout the twentieth century. Do newspapers tend to look back at specific years, as in the end of the Second World War being ‘five years ago’ in 1950? Or are ‘notable’ years submerged in what is here called ‘everyday memory culture’? The latter seems to be the case if these years are calculated for ‘n years ago’, where n stands for the years from one to twenty and decades from twenty to two hundred. Figure 5 shows that ‘n years ago’ tends to evoke the recent past itself above any particular year. No single year really stands out.
                    
                         Given the nature of this method, the decline in the frequency of years after 1975 that Figure 5 shows is as necessary as it is meaningless. There is, after all, increasingly less data (that ends in 1989) on which these numbers can be based. 
                    
                
            
            
                Conclusion
                In the light of Hartog’s theory of an all-encompassing present, one would have expected a steady decrease of references to the past and the future. This study shows the opposite and substantiates Assmann’s contention that interest for the past returns, particularly in the second half of the twentieth century, in the form of memory culture. With the growing popularity of the studied phrase, Dutch newspapers became mediators of a form of memory culture than can be seen as ‘latent’ or ‘everyday’ in that it emphasizes recurring events rather than returning to a specific thing in the past. This is, particularly, true for the phrases ‘two years ago’ and ‘four years ago’ (Figure 4), the spikes in the diagrams of which indicate references to important sport (Olympics, European and World Championship soccer) and political events (national parliamentary elections).
                The use of the phrase ‘n years ago’ is by no means the only, nor the most important manifestation of memory culture. This limits the explanatory power of this study. In contrast with official and cultivated forms of memory culture, however, it does shed light on the latent, almost oblique, everyday invocation of the past that forms just as much part of that culture.
                
                    Figure 1: Frequency over time of ‘n years ago’, where n stands for one to ten, per million words in a sample of 10,000 documents per year of the national newspaper 
                    De Telegraaf
                    , 1893-1989. For similar diagrams (figures 1-4) for other newspapers, see: https://github.com/PimHuijnen/looking_back_newspapers/tree/main/Data.
                
                
                    
                
                
                    Figure 2: Frequency over time of ‘n years ago’, where n stands for decades from ten to one hundred, per million words in a sample of 10,000 documents per year of the national newspaper 
                    De Telegraaf
                    , 1893-1989.
                
                
                    
                
                
                    Figure 3: Frequency over time of ‘n years ago’, where n stands for one, ten, one hundred and two hundred, per million words in a sample of 10,000 documents per year of the national newspaper 
                    De Telegraaf
                    , 1893-1989.
                
                
                    
                
                
                    Figure 4: Frequency over time of ‘four years ago’ per million words in a sample of 10,000 documents per year of the national newspaper 
                    De Telegraaf
                    , 1893-1989.
                
                
                    
                
                
                    Figure 5: Frequency over time of dates (in years) that are referred to via the phrase ‘n years ago’, where n stands for years from one to twenty and decades from twenty to two hundred, per million words in a sample of 10,000 documents per year of the national newspaper 
                    De Telegraaf
                    , 1893-1989.
                
                
                    
                
            
        
        
            
                
                    Bibliography
                    
                        Assmann, Aleida (2020). 
                        Is Time out of Joint? On the Rise and Fall of the Modern Time Regime. Ithaca: Cornell University Press.
                    
                    
                        Au Yeung, Ching-man and Jatowt, Adam (2011). 
                        Studying how the Past is Remembered: Towards Computational History through Lare Scale Text Mining. 
                        Proceedings of the 20
                        th
                         ACM International Conference on Information and Knowledge Management, 1231-1240. DOI: 
                        https://doi.org/10.1145/2063576.2063755. 
                    
                    
                        Eijnatten, Joris van and Pim Huijnen (2021). Something Happened to the Future: Reconstructing Temporalities in Dutch Parliamentary Debate, 1814-2018. 
                        Contributions to the History of Concepts, 16: 52-82. DOI: 
                        https://doi.org/10.3167/choc.2021.160204. 
                    
                    
                        Graus, David, Daan Odijk and Maarten de Rijke (2018). The Birth of Collective Memories: Analyzing Emerging Entities in Text Streams. 
                        Journal of the Association for Information Science and Technology, 69: 773-786. DOI: 
                        http://dx.doi.org/10.1002/asi.24004. 
                    
                    
                        Hartog, François (2015). 
                        Regimes of Historicity: Presentism and Experiences of Time. New York: Columbia University Press.
                    
                    
                        Jatowt, Adam et.al. (2015). Mapping Temporal Horizons: Analysis of Collective Future and Past related Attention in Twitter. 
                        WW ’15: Proceedings of the 24
                        th
                         International Conference on World Wide Web, 484-494. DOI: 
                        http://dx.doi.org/10.1145/2736277.2741632. 
                    
                    
                        Keegan, Brian C. and Jed R. Brubaker (2015). ”Is” to “Was”: Coordination and Commemoration in Posthumous Activity on Wikipedia Biographies. 
                        Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, 533-546. DOI: 
                        https://doi.org/10.1145/2675133.2675238. 
                    
                    
                        Paul, Herman (2015). 
                        Key Issues in Historical Theory. New York and London: Routledge.
                    
                    
                        West, Robert, Jure Leskovec and Christopher Potts (2021). Postmortem Memory of Public Figures in News and Social Media. 
                        Proceedings of the National Academy of Science 118. DOI: 
                        https://doi.org/10.1073/pnas.2106152118. 
                    
                
            
        
    

