1632	2013	This short paper seeks to introduce the Digital Humanities community to three ongoing, interrelated projects: the Media History Digital Library (an open access digital resource), Lantern (a search tool) and Coding Media History (a text mining research project). Together, these three projects aim to use digital technology to transform the field of Film and Media Studies, a discipline that has lagged English and History in the creation of high impact DH work.

I would also like to suggest that the three interrelated projects demonstrate a productive model for scaffolding work in the Digital Humanities. The three sides of this work—enabling access, building tools, and text analysis — support and enhance one another. In the space below, I will briefly address all three projects and suggest the ways they enrich one another.

In terms of enabling access, the Media History Digital Library (www.mediahistoryproject.org) has digitized over 500,000 pages of out-of-copyright periodicals relating to the histories of film, broadcasting, and recorded sound. Prior to the launch of the MHDL, scholars wrote the histories of film and television through page-by-page microfilm readings of key periodicals, such as Moving Picture World and Photoplay. By scanning these publications along with previously unavailable materials, the MHDL enables scholars to conduct research more efficiently, ask new questions, and write new histories.

The MHDL’s collections are open access and built on a collaborative model. David Pierce and I lead the project, and we work closely with collectors, who loan materials, and sponsors, who pay for the scanning. The scanning is carried out by the Internet Archive (www.archive.org), which also hosts and preserves the digital files. By using the Internet Archive as a scanning vendor and provider of backend infrastructure, the MHDL follows in the tradition of other collaboratively built digital collections, including the Biodiversity Heritage Library (http://www.biodiversitylibrary.org/), Medical Heritage Library (http://www.medicalheritage.org/), International Children’s Digital Library (http://en.childrenslibrary.org/), and International Music Score Library Project (http://imslp.org/).

Film and media educators at institutions around the world are already incorporating the Media History Digital Library into their teaching. In one especially creative assignment, Elizabeth Clarke is having her students at Wilfrid Laurier University in Waterloo, Ontario read the MHDL’s digital editions of early cinema magazines and imagine they are the intended audience of motion picture exhibitors in the 1910s. Students are asked to design their own programs of short films and live entertainment based on what they discover inside the magazines.

The MHDL’s diverse user-base encompasses students, educators, expert researchers, and casual classic movie fans. In order to better serve all of these groups, I have been leading the development of Lantern, a software tool that is a co-production of the Media History Digital Library and UW-Madison’s Department of Communication Arts. Lantern offers users the ability to perform fulltext searches across the Media History Digital Library’s entire corpus. Eventually, we also hope to equip Lantern with powerful functionalities beyond search, such as topic modeling and network visualizations.

My team and I are developing Lantern through using Ruby on Rails, Python, XML, and CSS and customizing three open source technologies: Apache’s Solr search engine; the University of Virginia Library’s Blacklight interface; and the Internet Archive’s BookReader. We are currently indexing more materials into Lantern, overhauling its graphic interface, and enhancing its speed and functionality. We anticipate publicly launching Lantern in Summer 2013. In the meantime, you may view a work-in-progress demo at http://lantern-demo.commarts.wisc.edu/

The third project I want to address is a work-in-progress called “Coding Media History: Computational Analysis of the Hollywood Trade Press.” Despite the heavy reliance of film and television scholars on Variety and other industry trade papers, there has been little work that reflexively examines these sources. My research project, Coding Media History, uses computer analytics both to enrich our understanding of these key sources and destabilize the notion that we can conceive of 60 years of Variety as a singular “text.” In pursuit of these goals, I borrow from the text mining methods (and warnings) of Stephen Rasmsay and, especially, from Andrew J. Torget, Rada Mihalcea, Jon Christensen, and Geoff McGhee’s work on applying topic modeling and text mining to historical newspapers.

I have begun the process of working with a research assistant, who is marking-up the XML of the digitized publications. We will soon be able to start asking research questions over the marked-up corpus. In a 1905 issue of Variety, for instance, what percent of the pages were dedicated to vaudeville compared to motion pictures? How were these page allocations different in 1915, 1925, 1935, 1945, and 1955? When were radio and television introduced as their own sections? How did the buyers and amounts of advertising change over time? These are questions that I can answer by starting with the digitized magazines, adding a research assistant’s tags, and finally running my own algorithms over the marked-up corpus.

One of the questions I am exploring is the extent to which the various trade papers were truly similar or different from one another. The Hollywood trade papers have an infamous reputation for publishing the exact same studio press releases. By using open source plagiarism software, we can test whether this reputation is warranted. The answers to these questions hold real stakes. Consider the case of Motion Picture Herald, a trade paper that proclaimed to represent the interests of independent movie theatre owners. What does it mean if we discover that Motion Picture Herald published 40% of the same content as the trade papers that spoke to producers and the major studios? Can we truly think of Motion Picture Herald as representing the independent theatre owners’ interests?

In conclusion, the Media History Digital Library, Lantern, and Coding Media History are already making a positive intervention in the field of Film and Media Studies. I also hope that this suite of interrelated projects can serve as a useful model for scholars in other fields pursuing Digital Humanities projects. In the course of my work, I’ve found that being involved across a suite of activities (digitization, tool building, and text analysis) leads to better decision-making at every stage in the process. Although it’s unrealistic to expect that we’ll all become hybrid librarian-programmer-scholars, we need to better understand the integrated range of activities in order for the Digital Humanities to tackle bold new projects and break free of our tokenized comfort zones.

References
Hoyt, E., W. Hagenmaier, and C. Hagenmaier (2013). “Media + History + Digital + Library: An Experiment in Synthesis.” Journal of E-Media Studies 3: forthcoming.
Ramsay, S. (2011). Reading Machines: Toward an Algorithmic Criticism. Urbana: University of Illinois Press.
Torget, A. J., et al. (2011). Mapping Texts: Combining Text-Mining and Geo-Visualization To Unlock The Research Potential of Historical Newspapers. UNT Digital Library. http://digital.library.unt.edu/ark:/67531/metadc83797/. accessed March 9, 2013).
Yang, T.-I., A. J. Torget, and R. Mihalcea. (2011). “Topic Modeling on Historical Newspapers.” Proceedings of the Association for Computational Linguistics workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (June): 96-104. http://aclweb.org/anthology/W/W11/W11-15.pdf. (accessed March 9, 2013).
1651	2013	This paper is concerned with stylometric classification applied to French seventeenth-century plays. It reports on ongoing investigations into parameter setting and its impact on classification of such texts by author, genre, or form, using Eder & Rybicky's stylometric scripts for R (Eder & Rybicky 2011). Based on an investigation into the Corneille-Molière controversy, several methodological issues standing in the way of reliable results have been identified. One issue concerns the degree to which such authorship classification tasks are influenced by genre (here, comedy or tragedy) and form (here, verse or prose). Investigation of this issue shows that input parameters have indeed effects on the relative influence of authorship, genre and form in the classification of plays. Stylometry today: advances and challenges
Stylometry has made significant advances in recent years, due no doubt to the increased availability of electronic texts, of sophisticated and accessible stylometric tools, and of proposed classification methods and distance measures. Based on this range of resources, researchers in stylometry are able to use various linguistic features as input for classification tasks and may adjust a wide range of parameters.

This situation, however, also brings renewed urgency to the issue of fine-tuning input parameters and distance measures, depending on the materials under scrutiny and the type of inquiry. Arguably, this is somewhat less of an issue today for a language such as English, where a well-established stylometric tradition exists. However, despite recent advances for some languages (Van Dalen-Oskam & Van Zundert 2007, Rybicky & Eder 2011), parameter setting remains an insufficiently explored issue for languages such as German, French, Spanish, or Latin, and many more.

The Corneille-Molière controversy
This has been particularly apparent in the domain of seventeenth-century French drama, because work in this area has recently fuelled a controversy over whether or not Corneille was in fact the author of some or several plays traditionally attributed to Molière. In this controversy, traditional biographical and archival research (Boissier 2004) was complemented with results from stylometric analyses (Labbé & Labbé 2001; for a knowledgeable critique, see Brunet 2004; for a more recent approach, see Marusenko & Rodionova 2010). However, the methodological basis for stylometric analyses of this type of material seems to have been insufficiently investigated..

The conditions for reliable stylometric attribution results in this domain are challenging. The strong codification of classical literary discourse and the prevalence of stringent metrical forms mean that stylistic differences between authors are often subtle. At the same time, the available plays vary widely as to dramatic genre (e.g. comedy or tragedy) and form (i.e. verse or prose). Preliminary investigations into the Corneille-Molière corpus using Eder and Rybicky's stylometric scripts have indeed shown the fragility of the results. Depending on the composition of the text collection, on the linguistic material used as input, and on distance calculation measures, results vary widely.

Fine-tuning for author, genre and form
On the one hand, then, it can be challenging to make clear author attributions on material that is heterogeneous as to genre or form. For example, relevant research relying on the „unmasking“ technique showed unsatisfactory results for cross-genre authorship attribution (Kestemont et al. 2012). On the other hand, if only relatively homogeneous material is taken into account, the overall amount of data available for classification may be significantly reduced. What is needed is knowledge about how to limit the influence of factors other than the one of concern in any given classification task. The most relevant factors in the present case are authorship, genre (here, comedy or tragedy) and form (here, verse or prose). If the goal is to make reliable author attributions, how can the influence of genre and form be limited? The research reported on here was designed in order to explore such issues. All investigations are based on the Théatre classique collection (Fièvre 2007-2013), which provides XML/TEI versions of all plays. Texts were uniformely preprocessed to retain only character speeches, but no lemmatization was applied.

A first collection of plays was investigated limiting the number of relevant categories to just two, authorship and genre, and balancing the number of plays for each category. This resulted in a collection of 32 plays by Pierre Corneille and Thomas Corneille, with an equal number of comedies and tragedies by each author. The question at hand was to find out at which settings the classification would be dominated by either one of the author or genre category, and to what extent. Systematic variation was introduced as to the range of words from the frequency list taken into account: All runs relied on 100 words from the word frequency list, and each run took these from a moving onset point onwards, at an interval of 50 words.

For each run, the data was subjected to a distance measurement using Burrows' Delta (Burrows 2002), the results forming the basis of a cluster analysis. The distance tables were saved for each run, and the proportions of the different low-level pairs for each run were extracted. The low-level pairs found can be of the following types: author-and-genre match, author-only match, genre-only match, or pairs without a match. The proportions of author-only and genre-only matches is assumed to indicate to which extent the chosen settings give precedence to textual features associated with authorship or genre, respectively. Figure 1 visualizes the results from this investigation.


Figure 1

Author-and-genre matches decrease overall with increasing onset points, and the proportion of author-only matches remains relatively stable. However, the proportion of genre-only matches increases markedly with increasing onset points. In the range of onsets points between 0 and 1150 words (with two exceptions at 300 and 750 words), pairing is predominantly related to authorship, not genre. In the range of onset points from 1150 to 1650 words, pairing is related both to authorship and to genre, in varying proportions, while genre seems to be taking over more markedly beyond an onset point of 1650 words.

A similar investigation was run with a collection of plays with variation only as to authorship and form (verse or prose). The collection of plays consisted of 28 comedies, with an equal number of prose and verse plays by each of the following authors: Dufresny, Scudéry, Regnard, and Molière. Again, the nature and proportions of the different low-level pairs was assessed. With adjustment for the asymmetrical number of authors and forms, the graph shown here as figure 2 was constructed.


Figure 2

There is a very strong fall-off for author-and-form matches from an onset point of around 800 words. Although author-only and form-only matches increase somewhat over the range of onset points, this does not correspond to the decrease in author-and-form matches. The most important result is that over the entire frequency range, form-only matches are always present and in many though not all cases, they have a higher proportion than the author-only matches. Compared to the authorship vs. genre comparison (figure 1), there is certainly no clear cut-off point below or above which author-only matches would dominate form-only matches.

Conclusions
Despite their limitations, these preliminary results give some useful indications for authorship attribution studies in French classical verse drama, and may increase reliability of attributions. First, text collections of mixed dramatic sub-genre may be used in authorship classification tasks, provided that the wordlist used does not exceed the first 1150 most frequent words, so that influence from features related to genre remains limited. Second, form is a prevalent factor in the entire range of the frequency list, and should be controlled for when creating text collections. Applying these insights to the Molière-Corneille problem permits to enlarge the corpus of comparison texts beyond comedies, thus yielding a broader basis for classification tasks, but not beyond verse plays. While the procedure described here could be used for other languages and genre pairs, the results may be difficult to generalize: the best distinguishing parameters will likely be different from the ones found here for French classical drama.

However, more work needs to be done before the results obtained are sufficiently reliable. On the one hand, the approach taken here could be improved by enhancing the assessment of the dendrograms to take higher-level groupings into account as well. On the other hand, the fact that there are quite a few exceptions to overall trends shows the limit of this approach. In fact, a mechanism like feature selection may be more appropriate to solve the issue. Using supervised machine learning techniques with authorship, genre or form as separate target classes, and combining this with information gain analysis for each target class, would allow generating lists of features relevant for each target category.

Acknowledgements
I would like to thank Jan Rybicky and Maciej Eder for introducing me to their tools as well as João Guerra for helping me with some Python coding.

References
Boissier, D. (2004). L'affaire Molière. La grande supercherie littéraire, Jean-Cyrille Godefroy.
Brunet, É. (2004). Où l’on mesure la distance entre les distances. Texto!. (4) http://www.revue-texto.net/Inedits/Brunet/Brunet_Distance.html (accessed 10 March 2013).
Burrows, J. (2002). ‘Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship. LLC 17 (3) 267-287. 10.1093/llc/17.3.267
Eder, M., and J. Rybicki (2011). Stylometry with R. In DH2011: Conference Abstracts. Stanford University, Stanford, 308-11.
Fièvre, P., (ed.) (2007-2013). Théâtre classique, http://www.theatre-classique.fr/ (accessed 10 March 2013).
Kestemont, M., K. Luyckx, W. Daelemans, and T. Crombez (2012). Cross-Genre Authorship Verification Using Unmasking. English Studies
Labbé, C., and D. Labbé (2001). Inter-textual Distance and Autorship Attribution Corneille and Molière. Journal of Quantitative Linguistics. 8(3): 213-231. 10.1076/jqul.8.3.213.4100
Marusenko, M., and E. Rodionova (2010). Mathematical Methods for Attributing Literary Works When Solving the ‘Corneille-Molière’ Problem. Journal of Quantitative Linguistics 17(1): 30-54.
Rybicki, J., and M. Eder (2011). Deeper Delta Across Genres and Languages: Do We Really Need the Most Frequent Words? LLC. 26(3): 315-321. 10.1093/llc/fqr031.
Van Dalen-Oskam, K., and J. van Zundert (2007). Delta for Middle Dutch. Author and Copyist Distinction in Walewein. LLC. 22(3): 345-362. 10.1093/llc/fqm012.
1666	2013	I. Introduction
Digging into Human Rights Violations (DHRV) is developing a computational reader for large text archives of human rights abuses so as to discover the stories of hidden victims and unidentified perpetrators only apparent when reading across large numbers of related documents. In part, this project began with an observation drawn from Benetech’s Human Rights Data Analysis Group’s (HRDAG) report on the Bosnian Book of Dead (Ball 2007). In their report on the tabulation of fatalities resulting from ethnic cleansing undertaken by the Milosevic regime, HRDAG was highly concerned with the de-duplication of entries. This over-reporting of individual victims within human rights corpora is endemic, and represents an opportunity for a system that can read across a corpus. For example, in the 511 interviews comprising our test corpus, one named individual appears more than 60 times. How many times might an unnamed individual reoccur? Automated readers exist that classify documents, produce summaries (Nenkova 2011), extract significant information (Strassel 2008) and highlight sentiment on a perentity, sentence, paragraph, or document basis (Pang 2008). This type of analysis works best with well-defined figures, such as occur in newspaper articles or government documents. That partially describes reports of human rights violations, as each report generally describes a victim’s perspective of one limited event. Currently, these systems have difficulty parsing peripheral entities, indeterminate language, or references that go beyond the boundaries of one document and are only significant when traced across documents; many reports peripherally describe the fates of other victims. These implicit, buried links and duplicate reports amongst records enable horizontally reading across records collections, rather than vertical reading through one record.

The technical goal of DHRV is to create an NLP system that facilitates cross-document coreference of entities in collections of witness statements and interviews within the domain of rights. This project’s approach to resolving the task (Kibble and Deemeter 2000) described as “whether or not two mentions of entities refer to the same person,” begins by considering the subtype of anaphora (indicative language within a document) known as exophora (indicative language across more than one document), and relies on placing pronominal entities within a high-order Event Trigraph of location, time, and name. Because temporal information is so often referential and ambiguous, and therefore difficult to extract and correlate (Northwood 2000) our approach uses a phrase-based establishment of semantic context to support identifying the temporal context. This noun- and verb-phrase extraction, collocation detection, and semi-automated matching, feeds a 2D planar visualization similar to network graph models. Uncertainties in the document and information retrieval processes are visualized to allow researchers to confirm whether entity occurrences should be conflated. Because much human rights documentation contains sensitive information that cannot be made public, this project is prototyping with another historically significant corpus that shares many structural features to our primary data: the World Trade Center Task Force Interviews conducted with first responders to the attacks of September 11, 2001.

II. Event Summarization Based on Matching Phrases
Our main stratagem is to situate entities in the series of events that define their appearances. Phrases useful for this process accord to a “journalist template,” of Who, What, When, Where, and Why, and are situated in the events reporting schema developed by Patrick Ball for human rights violations reporting (Chang 2012, Ball 1996). The goal is a system that can automatically extract these important entities as phrases, and based on these extracts, allow for the recognition of duplicate entities across documents. A perceptual diagram of the system is shown in Fig. 1.


Figure 1:
Phrase mining perceptual diagram

In the system, the noun phrases and verb phrases are first extracted from a parser. Then a phrase classifier is used to determine which phrases fall into important entity categories such as Person names/Geographic Locations/Date/Time or depiction of an event. After classifying these phrases into categories, a module called Collocation detector is ran to detect which of the detected entities are described in the same context within a passage in the corpus. This collocation of phrases is different from the collocation of words usually used in NLP, in that it captures instances of the collocations, instead of a global probability. A collocation is only true when multiple elements correlate. After a set of collocated phrases have been detected, they are placed into the event template and fed to a visualization engine. Human observers then decide which cross-document entities are identical. The engine computes automatic scores to make suggestions to the observers on which events and entities should be merged.

III. Phrase Extraction and Classification
The first step of phrase extraction is done by running a full parser on each document and then extracting all the retrieved noun phrases and verb phrases from the parse tree. We decided against using a shallow parser (chunker) because it has lower recall (may not capture all the desired phrases) than a full parser. The parser we are using is the Stanford parser (Klein 2003(1), Klein 2003(2)). From the extracted phrases, we formulate a classification task for labeling important phrases for event extraction. The important phrases in our research is different from the traditional named entity recognition (NER) problem in NLP, in that we are seeking to connect names to unnamed entities. We have 8 categories for important phrases: Organization, Person, Title, Location, Date, Time, Event, Miscellaneous and the background category of Unimportant. Of these categories, some are traditional NER or TimeML categories. Event and Miscellaneous labels are new, and determine some important phrases that might not be readily interpreted as named entities. Phrases such as “the pedestrian bridge,” “the ferry,” or “the second tower” which are not identifiable as a particular named entity, but might be crucial in depicting the event are classified as Miscellaneous.

To maximally utilize human knowledge in the phrase labeling phase, an unsupervised selection mechanism selects the phrases to be labeled. In this mechanism, phrases are ranked by a score that is similar to a frequency or N-gram model, but discounts the probability of a phrase if it is very common in a background corpus


where log P(phrase) is computed by an N-gram language model trained on the current corpus, and log Pbg(phrase) is based on a Ngram language model trained on a background corpus that is supposed to contain documents of all kinds. Under this model, the probability of a phrase is only discounted if Pbg(phrase) > P(phrase). This application of Term Frequency–Inverse Document Frequency (Cohen 2002) helps us to find frequent phrases in the corpus which are not popular in the background corpus. The phrases with top scores are manually labeled. By this approach we can obtain the labels for the most frequent and unique phrases in the corpus, which are likely to be more important in isolating an event. Our N-gram training uses the modified Kneser-Ney smoothing (Chen 1999) from the MitLMpackage (Hsu 2008). The background language model is obtained from Microsoft Web Ngram Services. Given a set of human-labeled phrases, we then train two levels of classifiers on these phrases. At the first level, a binary Important versus Unimportant phrase classifier is trained. At the second level, a one-against-all multi-class classifier is trained for each of the phrase category described above, except Miscellaneous, which serves as the background category for Important phrases. The features used for the classifiers are common NER features (Zhang 2003, Ratinov 2009), plus standard bagof- words features. For the Date and Time phrases, we make use of the SUTime library (Chang 2012) which matches date and time expressions using an extensive set of rules defined by regular expressions. The classification of these Time and Date phrases do not depend on our own human annotation.

IV. Collocation Detection and Event Templates
For the collocation we use a simple metric: a Gaussian kernel on the distance between mentions of different phrases. Formally, the collocation probability of one occurrence of a phrase, given a set of other phrases is defined as:


where S(pi) is the sentence number where pi occurred. Given the defined conditionals, one can compute the joint probability P(p1, p2, p3, . . . , pk) and use a threshold to determine which phrase set goes to an event template.

V. Visualizing Unvertainty in Event Trigraphs
Depending upon the context, uncertainty refers to statistical uncertainty, ranged values or missing data (Pang 1997). Uncertainty can be introduced in the data during acquisition, processing or even visualization. In this paper, since the underlying data has been extracted from narratives, which in turn lacks precision, uncertainty is introduced right from the data acquisition phase. These uncertainties include the temporal, “By this time, it had to be 11:00 o'clock at night” and “at that time I noticed,” locative, “I guess that would be North End Avenue,” and entity, “At this point I had my five guys" [WTCTF 9110250]. One of the major goals of this project is to visualize the triadic relationship amongst character, time, and location while incorporating these uncertainties.

In (Skeels 2010), the authors present a survey of the existing works on uncertainty visualization. The survey shows that most research in this area is in the field of geospatial (MacEachren 2005) or scientific visualizations (Lodha 1996, Grigoryan 2004, Wittenbrink1996). It also states, "[t]he main techniques developed include adding glyphs, (Wittenbrink 1996, Lodha 1996) adding geometry, modifying geometry, (Grigoryan 2004) uncertainty in a model to decision makers (Walker 2003) modifying attributes, animation (Lodha 1996, Gershon 1992) and sonification.(LodhaSonic 1996).” To convey the time-location-character data and the associated uncertainty visually, in this paper, we introduce a trigram based visualization called an Event Trigraph.

An Event Trigraph is a 2D planar diagram consisting of events as its building blocks. An event in this case is a 3-tuple consisting of the basic elements derived from the phrase mining method detailed above to yield three elements: time, location and entity. Events are represented visually as triangles with the aforementioned basic elements as vertices connected with weighted edges. The weights represent the confidence value in the relation as obtained via our text mining methods. These confidence values show how likely is the connection both between two elements and amongst the trigram. Figure 2 shows a trigram with confidence values and elements.


Figure 2.
Event trigraph with uncertainty values and voids

Confidence values peak at 1, or a certain relationship between two elements. To reduce visual clutter due to the excess number of edges and vertices, we employ details on demand (Shneiderman 1996) and filtering capabilities on the dataset. The users can also drag and drop the events over other events and manually enter the confidence values to present the associativity between two or more events. Since the user loads multiple documents at a time, this feature enables users to visually associate events in different documents — the main forte and a unique aspect of our visualization. Furthermore, since the resulting data structure is a weighted network graph, many graph theory algorithms can be readily applied, thus making it scalable.

VI. Conclusion
As reports are processed by this method, the diagram builds in complexity and supplements each individual event trigraph with ones that potentially correlate. In the figure above, John-Café-12:00 pm is correlated to Jill-Playground-12:00pm across the time element, and Jill-Playground-12:00 pm correlates to Eleanor-Playground-12:00 pm across both time and location elements. This aggregated event trigraph builds to represent a corpus, and offers a method for correlating the collocation of entities across documents, and potentially identifying unnamed entities within said corpus.

References
Ball, P. (1996). Who Did What to Whom?: Planning and implementing a large scale human rights data project. Washington, D.C.: American Assoc. for the Advancement of Science, Science and Human Rights Program.
Ball, P., E. Tabeau and P. Verwimp (2007). The Bosnian Book of Dead: Assessment of the Database. Sussex: Households in Conflict Network, Institute of Development Studies.
Chang, A. X., and C. D. Manning (2012). Sutime: A library for recognizing and normalizing time expressions. In International Conference on Language Resources and Evaluation (LREC 2012).
Chen, S. F., and J. T. Goodman (1999). An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13:359–393.
Cohen, W. W., and J. Richman (2002). “Learning to Match and Cluster Large High-Dimensional Data Sets for Data Integration.” Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD).
Gershon, N. D. (1992). “Visualization of fuzzy data using generalized animation.” IEEE Symposium on Visualization 1992. Chicago: IEEE Computer Society Press, 268–273.
Grigoryan, G., and P. Rheingans (2004). “Point-based probabilistic surfaces to show surface uncertainty.” IEEE Transactions on Visualization and Computer Graphics 10(5): 564–573.
Hsu, B. J. and J. Glass (2008). Iterative language model estimation: Efficient data structure and algorithms. In Proceedings of Interspeech.
Klein, D. and C. D. Manning (2003). Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics, 423–430.
Klein, D. and C. D. Manning (2003). Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15, 3–10.
Lodha, S. K., A. Pang, R. E. Sheehan, and C. M. Wittenbrink (1996). “UFLOW: Visualizing Uncertainty in Fluid Flow.” IEEE Symposium on Visualization Chicago, IL: IEEE Computer Society Press, (1996): 249–255.
Lodha, S. K., C. M. Wilson, and R. E. Sheehan (1996). “LISTEN: Sounding uncertainty visualization.” Proceedings of the Visualization 1996. Los Alamitos, CA: IEEE Computer Society, 189–195.
MacEachren, A. M., et al. (2005). "Visualizing geospatial information uncertainty: What we know and what we need to know." Cartography and Geographic Information Science 32.3: 139-160.
Nenkova, A., and K. McKeown (2011). “Automatic Summarization.” In Functions and Trends in Information Retrieval. 5.2-3: 103-233.
Northwood, C. (2010). TERNIP: Temporal Expression Recognition and Normalisation in Python. Sheffield: Department of Computer Science.
Pang, A. T., C. M. Wittenbrink, and S. K. Lodha (1997). "Approaches to uncertainty visualization." The Visual Computer 13(8): 370-390.
Pang, B., and L. Lee (2008). “Opinion Mining and Sentiment Analysis.” In Foundations and Trends in Information Retrieval. 2(1-2): 1-135.
Ratinov, L. and D. Roth (2009). Design challenges and misconceptions in named entity recognition. In Proceedings of the Thirteenth Conference on Natural Language Learning (CONLL 2009).
Shneiderman, B. (1996). "The eyes have it: A task by data type taxonomy for information visualizations." Visual Languages. Proceedings, IEEE Symposium on IEEE, 1996.
Skeels, M., et al. (2010). "Revealing uncertainty for information visualization." Information Visualization 9(1): 70-81.
Strassel, S, M. Przybocki, K. Peterson, Z. Song, and K. Maeda (2008). “Linguistic Resources and Evaluation Techniques for Evaluation of Cross-Document Automatic Content Extraction.” In Proceedings of the Sixth International Language Resources and Evaluation (LREC'08).
Walker, W. E., et al. (2003). “Defining uncertainty: A conceptual basis for uncertainty management in model-based decision support.” Integrated Assessment 4(1): 5–17
Wittenbrink, C.M., A. T. Pang, and S. K. Lodha. (1996). “Glyphs for visualizing uncertainty in vector fields.” IEEE Transactions on Visualization and Computer Graphics 2(3): 266–279.
Zhang, T., and D. Johnson (2003). A robust risk minimization based named entity recognition system. In Proceedings of the Seventh Conference on Natural Language Learning (CONLL 2003).
1679	2013	Our paper examines the similarities and differences between two technologies for representing topical information from different historical periods. Taking the 1784 index to Adam Smith’s seminal theory of political economy, An Inquiry into the Nature and Causes of the Wealth of Nations (1776), as a case study, we investigate the contrasting logics and aesthetics of knowledge organization apparent in the late­eighteenth­century index and the twenty­first­century topic modeling algorithm known as latent dirichlet allocation. To accomplish this, we have collaborated to create a tool we call the Networked Corpus. [1] Inspired by the way readers mark passages that share a common theme, trope, or other feature, the Networked Corpus provides a computational way to navigate a corpus based on a topic model, while also facilitating comparison between this mechanical model and the human­created index of Smith’s text. Our presentation will suggest a historical lineage between modern topic modeling and the eighteenth­century concepts of topic (which developed from the rhetorical idea of “topos”) and system (which emerged in the all­encompassing moral philosophy of the period) that are both exhibited in the index. We argue that the intentionally anachronistic comparison of topic modeling with the eighteenth­century index reveals similarities and differences between what each approach counts as a salient feature of a text for the purpose of organizing and representing its topical, informational, or conceptual content.

Topic modeling is a family of statistical methods that attempt to find “latent” semantic content in texts, under the assumption that texts exhibit mixtures of “topics” that have characteristic vocabularies. [2] Topic modeling software attempts to find the topic definitions that best fit a given set of texts, while inferring which texts exhibit which “topics” based on the words that they use. This method was originally proposed as an information retrieval tool, with the goal of enabling people to search for broad themes that cannot necessarily be identified with particular words. As such, it competes with the subject index; but it differs from traditional indexing both in its assumptions and in the form the output takes. Preparing a subject index generally involves imagining what a reader might want to find — “think of the user” is the “motto” given in G. Norman Knight’s classic indexing textbook — and results in a product that suits those who can describe what they are looking for. [3] Topic modeling, by contrast, is based on a generative model —an abstract description of the process through which texts were produced — and constructs “topics” that do not necessarily correspond to anything that can be easily described. Using the output of a topic model as an index requires that the topics be labeled, something that requires an interpretive judgment that is often very difficult to make, and that can often, as scholars using topic modeling have argued, be misleading. [4]

The first stage of our project was an attempt to create an information retrieval program that better suits this limitation of topic modeling than forms that are tailored for users who already know what they are looking for, such as the index or the search engine. We wrote a Python script that takes in a collection of texts and the output of the topic modeling program MALLET, and produces an HTML version of the corpus with interactive navigation features. [5] In addition to an index of the “topics” in the model, the output includes asterisks in the margin next to passages where there is a particularly high concentration of a given topic relative to the concentration in the text as a whole — “exemplary” passages, as we are calling them. Clicking on an asterisk summons a popup box that contains links to other “exemplary” passages for the same topic. This construct is intended to enable navigation not from a “topic” to a passage, as in an index, but from one passage to another. It encourages the user to read until they come across something interesting that has been marked with an asterisk, and then see where the links go. The network­like structure of this tool gives the topic model an exploratory function that does not depend on any prior knowledge of what topics there are, or of what, if any, significance the topics have.

The Networked Corpus also includes features that are intended to make topic models easier to interpret. The user has the option to “explain the relevance” of a topic, showing a box listing the words most strongly associated with that topic, the “exemplary” passages that the program found for that topic, and other texts in the corpus that also contain a high concentration of that topic. The “explain” feature also highlights all of the words in the text that arose from the selected topic according to the model, and shows the density of the topic over the course of the text as a sideways line graph that runs in the margin. This visualization gives an idea of which parts of a text contribute to its association with a given topic and which do not, providing a rich body of both positive and negative evidence by which the topic model can be interpreted.

In the second stage of our project, we are investigating how the “topics” of a topic model differ from the notion of topic or subject that was employed in the late­eighteenth­century index. Recently literary and intellectual historians including Ann M. Blair and Leah Price have described the historical development of practices of indexing, commonplacing, and generally what we might call “topical” knowledge in the seventeenth and eighteenth centuries. [6] As a lecturer in rhetoric and belles lettres and later a professor of moral philosophy, Adam Smith participated in defining the significance of those activities for the emergence of modern disciplines such as literature, history, and political economy. Upon the publication of the first edition of The Wealth of Nations without an index, Smith’s friend and fellow university professor, Hugh Blair, encouraged him to add an index and a syllabus like the ones they used “to give in [their] college lectures” because those additions would offer “Exhibit a Scientifical View of the Whole System." [7] For Blair, like Smith, representing the knowledge a text contained in various comprehensible and manageable forms was a critial aspect of the production of new knowledge.

Our paper considers whether new epistemological units produced by digital methods may facilitate comparative examination of similar ones from earlier periods. More specifically, we track the development of topical knowledge in the eighteenth century by comparing indices and commonplace books of the period to algorithmically produced “topics.” During the last year we have collaborated to create a tool we call the Networked Corpus. Inspired by the way readers mark passages that share a common theme, trope, or other feature, the Networked Corpus provides a computational way to connect topics across the entire range of a corpus. We are currently working on a project that compares the historical changes to the content and form of eighteenth-century indices and commonplace books and topic modeling output from the corpus.

This “scientifical” approach to indexing suggests a particular model of the text, in the sense that Willard McCarty uses the word “model” — a “fictional or idealized representation” that one can see the text through. [8] The models actually employed in the construction of Scottish Enlightenment texts cannot, of course, be directly observed, but as a way of investigating one of these models speculatively, we produced a special version of the Networked Corpus for The Wealth of Nations that presents the highly detailed index that appeared in the 1784 edition of the book alongside a topic model generated from the text (Figure 1). To facilitate comparison of these two constructs at a conceptual level, we transformed both the index and the topic model into something similar to marginal annotations, showing all of the index headings that reference a page on the screen when the page is displayed, along with a list of prominent topics on the page. We also used the Spearman rank correlation to find topics that tend to strongly match the pages referenced under particular index headings, and indicated the pages on which these correlations break down. Based on a reading of these points of disagreement, we contend that the topic model is able to pick up on rhetorical moves in the text that are not represented within the sort of system of concepts that the index constructs, at the cost of never being able to claim the sort of exhaustiveness that the Scottish Enlightenment writers sought.

In this presentation, we suggest that the approach we have taken in our study of The Wealth of Nations — comparing constructs from different time periods that address a problem in radically different ways — opens up a new avenue for examining both contemporary text mining models and the models that are implicit in the organization of historical texts. As McCarty has observed, modeling supports an “orientation to questioning rather than to answers, and opening up rather than glossing over the inevitable discrepancies between representation and reality.” [9] Our deformation of The Wealth of Nations employs a statistical model not as a way of studying the text itself, but as a vantage point from which we can examine the assumptions and blind spots of another, historical model of textual organization, turning this discrepancy into something of hermeneutic use. We thus agree with Alan Liu, who encourages scholars to pursue “any mediation that produces a sense of anachronism (residual or emergent, in Raymond Williams’s vocabulary) able to make us see history as a compound relation of proximity and distance between past and present.” [10] With the Networked Corpus, we suggest a way of doing this that converts the alienness of mechanical methods of reading in comparison to older models into a productive source of tension.


Figure 1:
Screen shot from the Networked Corpus

References
Blair, A. M. (2011). Too Much to Know: Managing Scholarly Information Before the Modern Age. New Haven: Yale University Press.
Blei, D. M., A. Y. Ng, and M. I. Jordan. (2003). Latent Dirichlet Allocation. The Journal of Machine Learning Research 3: 993–1022.
Blei, D. M. (2012). Probabilistic Topic Models. Communications of the ACM 55.4: 77–84.
McCallum, A. K. (2002). MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.
McCarty, W. (2005). Humanities Computing. Palgrave Macmillan.
Price, L. (2003). The Anthology and the Rise of the Novel: From Richardson to George Eliot. Cambridge: Cambridge University Press.
Notes
Our tool can be found at networkedcorpus.com.
See D. M. Blei, “Probabilistic Topic Models,” Communications of the ACM 55, no. 4 (2012): 77–84 for a brief review of topic modeling and D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent Dirichlet Allocation,” The Journal of Machine Learning Research 3 (2003): 993–1022 for a more detailed explanation.
G. Norman Knight, Indexing, the Art of (George Allen & Unwin, 1979), 159.
See recent discussions of this challenge on Ted Underwood’s blog post, “Visualizing Topic Models,” The Shell and the Stone, 11 November 2012, Accessed 13 March 2013, and Ben Schmidt’s blog post, “When you have a MALLET, everything looks like a nail,” Sapping Attention, 2 November 2012, Accessed 13 March 2013.
Andrew Kachites McCallum, “MALLET: A Machine Learning for Language Toolkit.”, 2002, http://mallet.cs.umass.edu.
See Ann M. Blair, Too Much to Know: Managing Scholarly Information Before the Modern Age (New Haven, CT: Yale University Press, 2011), 137–144 and Leah Price, The Anthology and the Rise of the Novel: From Richardson to George Eliot (Cambridge; New York: Cambridge University Press, 2003), 67–99.
Hugh Blair, “Letter to Adam Smith, 3 April 1776,” The Correspondence of Adam Smith, ed. by Ernest Campbell Mossner and Ian Simpson Ross (Oxford: Clarendon Press: 1977), 189.
Willard McCarty, “Modelling,” Humanities Computing (Palgrave Macmillan, 2005), 24.
McCarty, Humanities Computing, 38.
Alan Liu, Local Transcendence: Essays on Postmodern Historicism and the Database (University of Chicago, 2008), 25.
1696	2013	1. Introduction
Due to recent efforts to digitize literary works, researchers have been able to perform meaningful large-scale analyses of millions of texts and reach meaningful conclusions about literature, language, and culture using statistical analysis. This approach is powerful, but frequently ignores subtleties in literary works, reducing complex texts to bags of words. Literary theorists take a different approach, performing in-depth qualitative studies examining plot intricacies and character interactions. Unfortunately, such deep analysis does not scale well due to human time constraints.

In our project we combine these two approaches to literary analysis, allowing us to benefit from the advantages of both. More specifically, we develop and apply methods for automatically extracting character interaction networks from works of entertainment and use properties of the resulting networks to draw conclusions about these works.

There are three main components:

(1) Extracting character interaction networks as weighted graphs, with characters as nodes and interaction scores as edges
(2) Computing informative properties (e.g., clustering coefficient) of the resulting networks
(3) Using those properties to answer broad questions about the works (e.g., whether different media types are characterized by distinctive interaction networks) by constructing machine learning classifiers.
2. Related work
As mentioned earlier, most computational literary analysis has been at the word level. There are, however, several exceptions. Most notably, Elson et al. (Elson et al. 2010) effectively utilized dialogue interactions in sixty 19th century literary works to form social networks and make interesting discoveries about a particular genre. Other researchers used network theory to analyze small groups of texts, such as Hamlet (Moretti 2011), Greek tragedies (Rydberg-Cox 2011), Shakespeare (Stiller and Hudson 2011), and Marvel comics (Alberich et al. 2002). These studies were all relatively narrow in focus, leading to valuable discoveries about a small number of texts. More recently, C.-Y. Weng et al. (Weng et al. 2009) proposed a network extraction method for movies and T.V. shows based on co-occurrence, successfully identifying lead roles and other attributes for several movies.

Overall, previous work primarily focused on using character interaction networks to improve understanding of individual texts or movies. We feel humans already do a very good job—better than computers—of analyzing small collections of works; our main limitation is insufficient brainpower to simultaneously analyze and compare hundreds or thousands of works. As such, we are interested in conducting a large-scale study of character interaction networks for diverse works of entertainment. Our goal is not to examine literature from a specific time period or a particular film’s plot, but rather to discover sweeping trends in literature and movies across genres and over time.

3. Methodology
3.1 Building Networks
We focused on play and movie scripts because their structured format is well suited for systematically detecting interactions between characters. We obtained scripts and relevant metadata from a variety of sources (Internet Movie Script Database (2011); Project Gutenberg (2011); The Complete Works of Shakespeare (2011); EOneill.com EText Archive (1999); Read Plays Online-Read Print (2011); The EServer Drama Collection (2011); Rotten Tomatoes (2011); Robnik-Sikonja and Kononenko (1997), automating the process with Python scripts. For consistency, we then converted all data into a standardized intermediate format using more regular expressions, and a blacklist of non-verbal action commands (e.g. “fade in”). In total, we extracted 173 plays and 580 movie scripts.

We experimented with four extraction algorithms for constructing character interaction networks. Our first approach, used by Weng et al. (Weng et al. 2009), defined the interaction score for two characters as the number of scenes in which both appear. Our second algorithm extended this concept, incorporating the number of lines spoken in each scene. Unfortunately, many scripts had long scenes, resulting in falsely high interaction scores between two characters in different parts of the same scene.

We then used what we call the Closeness approach to consider an interaction to have occurred between two characters only when they have spoken nearby lines in the same scene, increasing their scores by an amount linearly decreasing with increased distance. Our fourth and final algorithm weights interactions by the total number of words exchanged.

3.2 Property Calculation
For each character interaction network, we computed the following network properties, which represent different concepts in literary works:

Average clustering coefficient: how much groups of characters tend to cluster together
Single character and relationship centrality: how much the work focuses on a single character above all others
Single relationship centrality: how much the work focuses on a single relationship between characters above all others
Top character weight variance: whether the group has a large group of similarly prominent characters or a few main characters and many less important roles
Top relationship strength variance: whether relationships are emphasized roughly equally, or if there is an emphasis on a select few
Entropy of node degrees and edge weights: an alternate approach to quantifying the spread in the distribution of character and relationship importance
Mean and variance of top character relationship strengths: whether the work has one or several main storylines
Percentage of existing edges: an alternate approach to determining number of storylines
Betweenness centrality — maximum, difference, and entropy: another alternate method of determining the relative importance of main characters
Number of characters: used as a final feature in our classifiers
3.3 Classification
We used our network properties as features in binary classifiers for various media aspects:

Media type: plays or movies
Date of movie: before or after 2000
Date of play: before or after 1800
MPAA rating
Audience and critic ratings
Single genre (e.g. romance or not)
Between genres (e.g. romance or horror)
Author (e.g. Shakespeare or George Bernard Shaw)
We experimented with logistic regression classifiers and decision trees, because these classifier types easily allowed us to understand how features were being used to arrive at predictions. We used the Orange library for Python, normalized our features, used k-fold cross validation to test our classifiers, and used the Relief algorithm [14] for top feature selection.

Because two classification classes did not always have the same number of examples, classification accuracies were sometimes misleadingly high even for poor classifiers. Thus we used area under the curve (AUC) as our primary performance metric.

4. Results
We found logistic regression to have higher AUC’s for 26 of our 35 classification tasks. Of the remaining 9 tasks, 8 performed relatively poorly on both classifiers (AUC < 0.65). Decision trees had consistently high AUC’s (0.8-0.9) on training data, suggesting overfitting despite our parameter selection efforts. The logistic regression classifiers did not suffer from this problem, so we focused on logistic regression results and used decision trees as means of gaining intuition for the role of certain features in the classification step.



Table 1: Logistic regression classifier AUCs for various classification tasks



Table 2: Logistic regression classifier AUCs for genre-related classification tasks

Our results are shown in the above tables. Dashes indicate insufficient data for proper classification.

5. Analysis
5.1 Media type classifier
We were very successful in classifying plays versus movies. We found that plays are characterized by high top character relationships, high single character centrality, and low top character weight variance relative to movies, suggesting that plays tend to have a clear-cut main character with several important supporting characters that interact primarily with the main character. A classic example is Hamlet, as can be observed by its interaction graph:


Results for movies suggest they tend to have several main characters, as in Charlie’s Angels:


5.2 Play date classifier
Important features from our pre or post 1800 play classifier, which also performed well, suggest older plays had more disjoint groups of characters and more distinct plotlines than newer ones. Misclassifications such as Shakespeare’s The Tempest (set on an island where most characters interact with each other), which was misclassified as new, corroborated our hypothesis.

5.3 Movie date classifier
Our movie date classifiers performed poorly. We think this may be due to insufficient data, or no marked difference in interaction patterns between old and new films.

5.4 MPAA and rating classifiers
These classifiers performed poorly, aligning with our expectations because there is a great diversity in the types of movies (and their interaction networks) that are enjoyed by audiences, praised by critics, or given a certain MPAA rating.

5.5 Genre classifiers
Overall, our classifier analysis confirms several common assumptions about genre stereotypes and assumptions. For example, “horror” classifiers performed particularly well, and were often characterized by high average top character relationship strength. This implies that most horror movies have one simple storyline, which is the stereotype.

As another example, romance and comedy proved far too similar to be successfully classified. Upon further reflection, character interaction networks for romances and comedies would be similar; comedies such as Harold and Kumar feature a dynamic duo that interacts much as love interests in a romance would.

5.6 Play author classifiers
Our classifiers achieved rather high AUC’s, and an analysis of the decision trees shows that one of Shakespeare’s defining characteristics is a large spread in the importance of main characters:


6. Conclusion
In this project, we developed a network extraction and classification strategy that sheds light on characteristics that define movies and plays. We automated a literary scholar’s general approach to extracting meaning from movies and plays, leading us to valuable insights about large numbers of works. It is our hope that scriptwriters will be able to use these insights to increase the breadth and diversity of character interactions and counter our generalizations with unique works of entertainment!

References
Elson, D., N. Dames, and K. McKeown (2010). Extracting Social Networks from Literary Fiction. In Proc. 48th Annual Meeting for the Association for Computational Linguistics, 138-147.
Moretti, F. (2011). Network Theory, Plot Analysis. New Left Review 68.
Rydberg-Cox, J. (2011). Social Networks and the Language of Greek Tragedy. Journal of the Chicago Colloquium on Digital Humanities and Computer Science 1.
Stiller, J., and M. Hudson (2005). Weak Links and Scene Cliques within the Small World of Shakespeare. Journal of Cultural and Evolutionary Psychology 3.
Alberich, R., J. Miro-Julia, and F. Rossello (2002). Marvel Universe looks almost like a real social network. e-print arXiv:cond-mat/0202174
Weng, C.-Y., W.-T. Chu, and J.-L. Wu (2009). RoleNet: Movie Analysis from the Perspective of Social Networks. IEEE Transactions on Multimedia 11.
The Internet Movie Script Database (2011). IMSDb. http://www.imsdb.com/.
Project Gutenberg (2011). Project Gutenberg. http://www.gutenberg.org/.
The Complete Works of Shakespeare (2011). MIT. http://shakespeare.mit.edu/.
EOneill.com EText Archive (1999). EOneill. http://www.eoneill.com/texts/index.htm.
Read Plays Online-Read Print Read Print Library (2011). http://www.readprint.com/.
The EServer Drama Collection EServer (2011). http://drama.eserver.org/plays/.
Rotten Tomatoes Flixster, Inc. (2011). http://www.rottentomatoes.com/.
Robnik-Sikonja, M., and I. Kononenko (1997). An adaptation of relief for attribute estimation in regression. In Proc. 14th ICML. 296-304.
1762	2013	The “preliminaries” section of a 17th-century book encompasses the pages appearing in the printed text before the beginning of the work itself. This information is divided into seven different types of documents: details of publication, documentation of censorship (both civil and ecclesiastical), licensing, selling price, dedications, letters, and errors. The importance of the preliminaries for this project lies in the information present in these sections: the names of the officials signing the documents, their governmental/institutional affiliation, dates, place of issue, and literary circles that appear in the form of dedications and poetry written by various authors and published in their friend’s or associate’s books. In a few pages, the preliminaries give a complete image of the formal process required for the publication of each work of literature. By compiling all this information into a graph database and performing queries specific to various research questions, we have at hand a valuable source of information about the historical networks that influenced the publication of Early Modern Spanish literature.
To get a comprehensive look at this information, we generated lists of every edition of what we consider literary texts (fiction in prose, theatre, poetry, chronicles) published during the 17th -century in the Spanish empire (Jiménez et al. 1980)(Calvo et al. 2003). As shown by the following screen shot, we have focused on acquiring every available edition of each literary work.
 

Sample of one of our acquisitions lists
We then divided the 17th Century into periods corresponding to the different “validos” —royal favorites that served as head of government or “prime ministers” — of the various kings in order to address the changing power structures of the time and their influence in literary production (Hernán et al. 2002). Through interlibrary loans and, in some cases, trips to the libraries that hold the edition, we acquired copies of the pages of each book that make up the preliminaries section. Then, we manually built a graph database using sylvadb.com, an open source software and free graph database management service developed in the CulturePlex Lab. Within Sylva, data was stored and organized using a custom designed system of schemas based on a node/edge relationship system. Finally, we exported the database to Gephi (https://gephi.org/), a software package that allows for visualization and statistical/metric analysis of the network using built-in algorithms and Python based scripting (Bastian et al. 2009). This allows us to detect important communities within the network, key players, important objects, and hubs of production.
For this study, we have unearthed the social networks of publishing and literary creation in 17th-century Spanish literature, focusing particularly on the period during the rule of the Duke of Lerma (1598-1618). Currently the first of our editions lists (Duke of Lerma) consists of 330 editions, out of which we have successfully obtained 228 scanned copies of preliminaries sections: approximately 70% of the total number. Of these scans, 121 have been entered into the database, producing a graph with 1612 nodes and 3472 relationships. Rendered in Gephi using the built-in OpenOrd algorithm, the graph looks like this:
 

The Preliminaries graph rendered in Gephi
Using the algorithms, metric analysis tools, and filters built into Gephi we pinpointed the individuals, governmental and ecclesiastical bodies that influenced publication in this period. Also, by using the concept of “ego network” from social network analysis, we established what we call the “publication network” of some of the authors that interest us (Carrington et al. 2011). A publication network includes the editors, censors, and other individuals important in the formal process of publication, as well as any other individuals that are more directly connected to the author: friends, family, patrons, literary colleagues, etc. We determined the range of the publication network based on the internal data structure of the Preliminaries database as follows. Due to bibliographic concerns (Bowers et al. 1962) and organizational aspects of our data schema, in order to establish a connection between the author and those involved in the approval, licensing, and publication of an edition there are four steps e.g., Author->Work, Work->Edition, Edition->Approval, Approval->Censor. Therefore, to establish an author’s publication network we needed to find neighbors for up to four degrees of separation. Although Gephi does not include ego network filters that extend to four degrees, using its Python based scripting console we were able to code functions that allowed us to isolate subsets within the graph to generate ego networks for any node to n degrees of separation. For instance, in the graph below we can see the publication networks of two authors associated with Mexico; Bernardo de Balbuena, author of Grandeza Mexicana; Juan de Torquemada, author of Monarquía Indiana; and the intersecting nodes in their publication networks:
 

Publications Networks: Balbuena=Black, Torquemada=Grey, Intersecting Nodes=White
Using the above techniques, we set out to find and isolate the main nodes of this social network that made possible the creation and sustainability of a transatlantic network of cultural agents. The first thing that stands out in the graph is Lope de Vega and his powerful, Madrid based publication network (Martínez et al. 2011). Using the Python scripting console, we determined that Lope’s publication network consists of 1083 nodes, or 67% of the nodes in the graph. This information is not new, based on the extremely prolific nature of his literary production we can assume that he was very well connected. However, we can also determine who wasn’t in his publication network. Departing from Lope’s publication network, we were able to locate the successful political and institutional connections that help us explain the central position of institutions such as the House of Zúñiga in the cultural fabric of the period.
 

Publication network: Lope de Vega=Black
To do this we used the scripting console to remove the subset of nodes representing Lope’s publication network from the other nodes that make up the graph, and returned a list of the names of all of the people who are not in Lope’s publication network. A quick review of this list produced some interesting results: we found several authors based in Spain including Gonzalo de Céspedes y Meneses and El Inca Garcilaso; and two authors active in Peru, Diego Dávalos y Figueroa and Pedro de Oña. While a quick look at both Céspedes and El Inca produced interesting results, the two Lima based authors attracted our attention. In this period social circles were highly influenced by geography, and it is logical that these authors find themselves at the periphery of a network centered geographically in Madrid. However, despite geographic concerns both authors remain connected to Lope de Vega’s network. We found that both Oña and Dávalos y Figueroa are connected to Lope’s network at 3 degrees of separation through their dedications to the Viceroy of Peru, Luis de Velasco y Castilla; and at four degrees through Juan de Zúñiga, Diego de Ojeda, and the Order of Santiago:
 

Publication networks: Dávalos y Figueroa=Black, Lope de Vega=Grey, Intersecting nodes=White
In order to contextualize the Peruvian network we compared the aforementioned “Mexican” authors with the “Peruvian” authors. Combining the four social networks into two based on geographic constraints, we found that at 4 degrees of separation there was no direct overlap, so we upped the parameter to 5 degrees of separation and produced the following image:
 

Publication Networks: Intersection between Mexican and Peruvian Networks
As shown here, even at five degrees of separation there are few overlaps between the networks. However, in the above image we begin to notice the importance of the House of Zúñiga. It is well known that the House of Zúñiga was powerful in both Spain and the Americas, and also that certain members of this house were important patrons of the arts and literature (Cátedra 2003; Díez Fernández et al. 2005). Nonetheless, we don’t think that their role in transatlantic literary production has been adequately explored. The political importance of this family in New Spain is obviously important (an Archbishop and a Viceroy); however, the Preliminaries graph illustrates not only the political role this house played in America, but also the importance of political figures/nobility in publication circles and how the members of one house can spread their cultural influence throughout geographic space. To take this concept one step further, we followed the Zúñigas back the Spain. Here we find the Duke of Béjar, Alonso López de Zúñiga y Pérez de Guzmán, and the first part of Don Quixote. It turns out that American authors were not the only artists soliciting support from the House of Zúñiga: Miguel de Cervantes dedicated part 1 of Don Quixote to the famous Duke of Béjar(Rico 2005).
The above samples show the potential of a research model that combines network-based analysis with quantitative and qualitative studies of cultural production, providing evidence of the interaction between political structures and cultural production in the Spanish Empire (Martínez et al 2008). By repurposing bibliographic data, the Preliminaries Project allows us to explore the concept of cultural networks within the framework of transatlantic studies and complexity theory (Wood 2010; Suárez 2007). Furthermore, this study demonstrates the effectiveness of digital humanities methods as a tool to locate previously overlooked areas for further study using a more traditional humanistic approach.
References
1. Pedraza Jiménez, F. B., and M. R. Cáceres. (1980). Manual de literature española. Pamploa: Cénlit.
2. Huerta Calvo, J. (dir.) (2003). Historia del teatro español. Madrid: Gredos.
3. García Hernán, E. (2002). Políticos de la monarquía hispánica (1469-1700). Madrid: Fernández Ciudad.
4. Bastian M., S. Heymann, and M. Jacomy (2009). “Gephi: an open source software for exploring and manipulating networks.” International AAAI Conference on Weblogs and Social Media.
5. Carrington, P. J., and J. Scott (2011). The SAGE Handbook of Social Network Analysis. Los Angeles: Sage.
6. Bowers, Fredson. (1962). Principles of Bibliographic Description. New York: Russell & Russell.
7. Martínez, J. F. (2011).Biografía de Lope de Vega, 1562-1635: un friso literario del Siglo de Oro. Barcelona, PPU.
8. Cátedra, P. M. (2003). La "Historia de la Casa de Zúñiga" otrora atribuida a Mosén Diego de Valera. Salamanca: Gráficas Cervantes.
9. Díez Fernández, J.; I., and G. Santonja. (2005). El mecenazgo literario en la casa ducal de Béjar. Burgos: Instituto Castellano y Leonés de la Lengua.
10. Rico, F. (2005). El texto del "Quijote”: preliminares a una ecdótica del Siglo de Oro. Barcelona: Ediciónes Destino.
11. Martínez Millán, J.;, and M. A. Visceglia (eds.) (2008). La monarquía de Felipe III. >Madrid: Cyan, Proyectos y Producciones Editoriales. Print.
12. Wood, A. T. (2010). Fire, Water, Earth, and Sky: Global Systems History and the Human Prospect. The Journal of the Historical Society. X:3: 287-318.
13. Suárez, J. L. (2007). Hispanic Baroque: A Model for the Study of Cultural Complexity in the Atlantic World. South Atlantic Review. 72(1): 31-47.
1783	2013	The University of Canterbury has recently completed development of New Zealand (and Australasia’s) first digital humanities degree program that is also standards-approved on a national level. The process required the development of document sets that were submitted for review by the University of Canterbury Faculty, Academic Advisory Committee, Academic Board, the New Zealand Vice-Chancellor’s Committee on University Academic Programs (CUAP), the New Zealand Vice-Chancellor’s Committee, and the Tertiary Education Commission. Fourteen national and international reviewers, drawn from technology education, information science, computer science, high performance computing and the digital humanities also provided their opinions. The program represents a significant baseline for future digital humanities programs, and the lessons learned during its development are of importance to the broader digital humanities community. Although New Zealand universities operate with basically the same degree of independence in course and program development as universities elsewhere in the world, the requirement to submit all new programs to a national standards body is unusual, if not unique. It may be that the University of Canterbury digital humanities program is the most closely scrutinised example the digital humanities community have seen. This has resulted in a program that is embedded within both the culture of Canterbury, and the national educational policies of NZ. It therefore comes with a higher degree of legitimacy, but also a complex set of stake holders. Moreover, because of the close policy ties between New Zealand and Australia (in education as well as other areas) the program has implications for the Australasian region as a whole.
The implications of national accreditation
Programs and curriculums have pedagogical, methodological, administrative and indeed philosophical issues embedded in them. Their final form reflects not only the ‘state of the art’ in the discipline in question, but the ‘state of the art’ as parsed through academic staff, informed (and uninformed) reviewers, institutional context (and necessity), national educational politics, and the shifting sands of methodological and critical best practice. The forces are such that it is quite possible for the final program of study to be quite different from that originally intended, although for obvious reasons the applicants tend to press on regardless, making modifications where necessary but attempting to safeguard the core pedagogical principles wherever possible. This is a process that many digital humanities teams should be expected to go through in the coming years as more institutions attempt to establish programs; it is a period in time when the digital humanities are going to begin to be influenced not only by internal pressure, but external ones such as the need to conform to national educational standards.
Teaching applied and critical DH in the context of standards-approved accreditation
The program will be delivered to fourth year students undertaking their ‘Honours’ year, a first year of post-graduate study often taken before embarking on more advanced Masters or Ph.D. study. The program was informed by existing programs at Kings College London, University College London, the Open University (Wilks 2011) and the University of California, but the author drew most heavily on theoretical and pedagogical perspectives raised through DH social media and publishing channels over the past five years. A balance has been struck between the ‘hack’ and ‘yak’ positions (Cecire 2011; Ramsay 2011; Koh 2012 and others), in the light of what Alexander Reid has suggested is a need for the field to equip students with a broad “yet undefined digital literacy” (Reid 2012, 354) encompassing both technical and critical skills. The position taken is similar to that espoused by Alan Liu and Andrew Prescott, who argue that tomorrows students and scholars will need to function in a world in which computers are not only ubiquitous, but knowledge itself is a commodity (Liu, 2004; Prescott, 2012). In this sense, the program assumes an ethical imperative to prepare students for work in the post-industrial society that was envisaged by Daniel Bell in 1973, and now forms the basis of both graduate employment structures (Castells and Aoyama, 1994; Aneesh, 2001; Cohen 2010) and tertiary education systems (Donoghue 2008; Brier 2012). In keeping with the core values of the digital humanities community, emphasis has been placed on the development of technical skills that can enhance and extend humanities research activities, and promote awareness of the engineered nature of the digital world.
The program is structured around two core assessment papers: DIGI 401: Introduction to Digital Humanities and DIGI 402: Humanities and New Media. DIGI 480: Research Essay will also be available, to students interested in exploring a topic in detail via a 10,000 word essay. A variety of other (assessment) papers will be rolled out in future years, including Applied Digital Humanities, Digital Literary Studies, and Digital History. Masters and Ph.D. offerings are expected to follow. DIGI 401: Introduction to Digital Humanities is modelled on courses in historical method that are well known to History students. The course provides a broad and challenging overview of the digital humanities, organised into History, Theory and Applied modules. Topics include technological determinism, systems theory, materiality and digital forensics, the nature of digital texts, and data visualization. Introductory lectures on TEI and GIS will prepare students for further study in Digital Literary Studies and Digital History. In order to provide students with generically useful programming knowledge an applied module will concentrate on teaching TEI, GIS, Python and use of APIs. Lecturers will be drawn from University of Canterbury’s Digital Humanities program, Human Interface Technology Laboratory, Computer Science, Information Systems, and Geography. The aim is to offer the students an overview of tools and methods in the digital humanities, and encourage them to think about how the digital world is engineered. DIGI 402: Humanities and New Media is an overt attempt to blend the ‘hack’ and ‘yak’ sides to DH as a practice. Students will be strongly encouraged to take DIGI 401 before taking 402 so they have a solid understanding of the technical side to new media culture and politics. Topics in this course include digital modernity, technocracy, cybernetics, knowledge economies, the Internet, open and closed data, open and closed ecosystems. Focus will be placed on both the engineered nature of the digital world, and the concepts required to critique it. Assessment will include traditional essay-based assessment, blog posts, forum posts, and quizzes designed to ensure students are capable of analysing the digital world as an engineered phenomenon.
Pedagogical focus will be placed on graduate outcomes across the program as a whole, and students will be offered opportunities for student exchanges, internship and work experience opportunities. The aim is for graduates to have a blend of traditional humanities-related skills and applied computing skills. They should have an understanding of the moral and ethical issues surrounding digital technologies, the ability to write clear, concise prose, and an understanding of the technical constraints and opportunities provided by digital technologies. Students should be well suited to work in all new media and digital industries, but especially ones requiring a blend of analytical and technical skills. Graduates would be suitable for work in research, relationship management, business analysis, digital archiving, project management, and the creative and cultural heritage sectors. They should be particularly suited to policy analysis positions related to technology and culture, and any position that requires communication across technical and non-technical audiences. The aim is to create a ‘porous’ educational environment that encourages interaction both inside and outside the university, equipping students with experiences and relationships that can translate into enhanced employment prospects. Inter-disciplinarily will be encouraged, and it is hoped that a DH Commons can be developed to integrate university service support teams in the library and digital media group into the learning experience.
The accreditation process means that, while reflecting the core aims and values of the digital humanities community, the program is also relevant to the pedagogical and strategic aims of the University of Canterbury and the wider New Zealand tertiary education sector. Although challenging, once successfully negotiated the accreditation process effectively embeds the digital humanities into the New Zealand government’s long-term education strategy, providing significant pedagogical sanction, integration with the secondary education sector, and a strong platform for future growth. All New Zealand, and undoubtedly Australian, universities aiming to develop digital humanities programs will need to reference the University of Canterbury as a baseline. The implications of this for the development of the digital humanities across Australasia are significant, and (as long as the Canterbury program enshrines core DH aims and values) largely positive.
This paper will provide an overview of the program from intellectual, pedagogical and strategic perspectives in an attempt to share lessons learned with the international DH community, and redress some of the “emphasis on research over teaching” prevalent in the field (Brier 2012, 391). Specific focus will be placed on the implications of the program for Australia and the development of the digital humanities across Australasia as a whole. All program documentation will be made available online so that conference participants have full-text access to the issues being discussed.
References
Aneesh, A. (2001). Skill Saturation: Rationalization and Post-Industrial Work. Theory and Society 30(3). 363–396.
Bell, D. (1973). The Coming of Post-industrial Society: a Venture in Social Forecasting. New York: Basic Books.

Brier, S. (2012). Where’s the Pedagogy: The Role of Teaching and Learning in the Digital Humanities. in Gold, M. K. (ed), Debates in the Digital Humanities. Ann Arbor: University of Michigan Press.
Castells, M., and Y. Aoyama (1994). Paths towards the informational society: Employment structure in G-7 countries, 1920-90. International Labour Review 133(1). 5.
Cecire, N. (2011). When DH Was in Vogue; or, THATCamp Theory. Works Cited, October 19 2011.http://nataliacecire.blogspot.com/2011/10/when-dh-was-in-vogue-or-thatcamp-theory.html.
Cohen, D. (2008). Three Lectures on Post-industrial Society. Cambridge, MA: MIT Press.
Smart, B.(2010). Post Industrial Society. Sage.
Donoghue, F. (2008). The Last Professors: The Corporate University and the Fate of the Humanities. 1st edn. New York: Fordham University Press.
Kent, E. F. (2012). What Are You Going to Do with a Degree in That? Arguing for the Humanities in an Era of Efficiency. Arts and Humanities in Higher Education 11 (3). (July 1, 2012) 273–284.
Koh, A. (2012). More Hack, Less Yack?: Modularity, Theory and Habitus in the Digital Humanitieshttp://www.adelinekoh.org/blog/2012/05/21/more-hack-less-yack-modularity-theory-and-habitus-in-the-digital-humanities/. (accessed May 21, 2012).
Liu, A. (2004). The Laws of Cool: Knowledge Work and the Culture of Information. Chicago: University of Chicago Press.
Prescott, A. (2012). An Electric Current of the Imagination: What the Digital Humanities Are and What They Might Become. Journal of Digital Humanities. http://journalofdigitalhumanities.org/1-2/an-electric-current-of-the-imagination-by-andrew-prescott/. (accesssed June 26, 2012).
Ramsay, S. (2011). On Building. Stephen Ramsay, http://lenz.unl.edu/papers/2011/01/11/on-building.html. (accessed January 11, 2011).
Reid, A. (2012). Graduate Education and the Ethics of the Digital Humanities. Debates in the Digital Humanities. Ed. Matthew Gold. Ann Arbor: University of Michigan Press.
Thaller, M. (2012). Controversies Around the Digital Humanities: An Agenda. Historical Social Research 37(3). 7–23.
Wilks, L. (2011). Developing the Digital Humanities at the Open University. Open University, June 2011.
1792	2013	Introduction
This paper describes VizOR, a new digital resource currently under development, that visualizes Mark Z. Danielewski’s 2006 novel Only Revolutions. VizOR is built on top of a MySQL database comprised of the complete text of Only Revolutions and is programmed in Python to produce a dynamic, database-driven visualization of the novel. In this paper, we discuss the methods and procedures we are currently developing to create this visualization and highlight the implications of these methods and procedures for theoretical concerns in both digital humanities and media studies. This project, we propose, is both a re-reading of and an exploration of the process of reading Danielewski’s novel: as such, it joins the novel in examining the interrelation of human and machine “reading” and “authorship,” pointing to a procedural understanding of reading and writing and suggesting that both activities occur across a wide variety of actors and platforms.
Context
The form of Danielewski’s novel is unique: the narrative consists of two parallel yet interrelated narratives, one by Hailey and one by Sam, and which one the reader reads depends on how the reader holds the book. If the reader is reading Hailey’s narrative, for instance, she must flip the book upside down to read Sam’s (and vice versa). Apart from the main narrative, each page also contains what we call a chronology section — a date with a list of historical people and events associated with that time or date, which Danielewski crowdsourced from his fans while writing the novel. Thus, each page is divided into four sections: Sam’s narrative, Sam’s chronology (which runs from November 22, 1863 to November 22, 1963), Hailey’s narrative, and Hailey’s chronology (which runs from November 22, 1963 to June 19, 2063).
In addition to its formal innovations, Only Revolutions is interesting for this project because it proliferates numbers, playing with the boundaries between “data” and “narrative.” For example, the numbers 360 and 8 and their factors and multiples are particularly important. Each narrative is 360 pages long, and each narrative and chronology section on each page contains 90 words (90 X 4 = 360). Furthermore, the narrative is divided up into sections of eight pages each, and the number of lines in each narrative section decreases, at regular intervals, from 22 to 14 as the reader progresses through these sections (22 - 14 = 8). “H,” for Hailey, is the eighth letter in the alphabet, and “S,” for Sam, is the eighth letter from the end of the alphabet; “Mark Z. Danielewski” has 16 letters; and Sam and Hailey are described as being “allways sixteen.” There are many, many more examples we could cite here. In this way, Only Revolutions encourages readers — human and machine alike — to count, and count again, the many different numbers that emerge from its pages.
In How We Think: Digital Media and Contemporary Technogenesis, N. Katherine Hayles includes a coda featuring visualizations of Only Revolutions produced with Google Maps. Collaboratively designed with Allen Beye Riddell, these three visualizations trace the geographical “place-names” of Sam and Hailey as they travel throughout the story, and then layer the two to create a composite map of the characters’ movement through the text (243-244). The resulting visualization shows that Hailey and Sam take similar paths across the map and that their overall directionality is nothing if not inconsistent. They move on a whim, together, wherever their overwhelming affection for one another points them. On one hand, VizOR is a response to Hayles’s map-based visualization and is concerned with the assumptions underpinning her choice of visualization platform. On the other hand, the project is addressing a larger trend within the digital humanities: the increased prevalence of data visualization as a mode of literary interpretation.
Due in large part to its often powerful and aesthetically pleasing visual impact, relatively quick learning curve, and overall “cool,” the practice of visualizing textual data has been widely adopted by the digital humanities. This prevalence is evidenced by, for instance, the high frequency of the term “data visualization” in the 2011-2012 Digital Humanities conference abstracts as well as the 2011-2013 Modern Language Association digital humanities panel descriptions. If the first wave of large-scale database projects in the digital humanities is exemplified by the practices of digitizing texts, constructing archives, and determining best practices for digital preservation, then the practice of data visualization is emblematic of the second wave of projects devoted to mining and interpreting this newly available data.
VizOR is influenced by the thinking of scholars and practitioners like Franco Moretti, Matthew Jockers, Jeremy Douglass, and Lev Manovich, as well as by visualization projects like UC San Diego’s “Cultural Analytics” initiative and Stanford’s “Gephi” visualization engine. Like other critical DH projects, VizOR is not only interested in engaging with literature via data visualization, but also in performing this engagement to ask what is at stake in this new mode of interpretation, both in terms of the individual scholar and the digital humanities as a field.
Project Description
The database is designed to be highly flexible, and this architecture allows us to enact the same linguistic layering that occurs while reading the text. We can query a specific word of a particular character’s narrative or chronology, the text from a specific line of a character’s narrative or chronology on a specific page, or the narrative or chronology text from a whole page for a specific character. Querying the database in this way allows us to instantly compare the words and phrases used by one character with those used by the other character on the same page and in the same position. For example, on Sam’s page seven, line five, we see “Gold Eyes with flecks of Green,” while the words in the same position on Hailey’s page seven, line five read,“Green Eyes with flecks of Gold” (Danielewski). VizOR surfaces this kind of reading through its very design, allowing readers to see the similarities and, perhaps more importantly, the differences between the narratives.
The visualization is currently still under development using the Python programming language. Python was selected as the programming language for VizOR due to its general flexibility and its ability to produce dynamic, database-driven visualizations. However, static mock-ups have been designed using Adobe Illustrator and are included at the end of this document.
The visualization reproduces the image of the page numbers in the novel, the two small rotating circles enclosed within the larger circle. In the visualization, however, each of the two smaller circles represents the narratives of Hailey or Sam. All three circles are comprised of absent centers with the text literally expanding outward. The larger circle is comprised of the chronological headings and entries. Clicking on one of the text strings in either narrative will query the database for the corresponding narrative’s string. This correspondence, as well as the inclusion of the Boolean double pipe (symbol for “or”) as link, is shown in the mock-ups at the end of the document. This, in effect, mirrors the layering that occurs during the act of reading. The visualization, though, has the ability to position the corresponding lines on the same plane at the same time, a phenomenon in the novel that is always already delayed or fading away. Rather than attempting to flip the book fast enough to see both sides of the coin, so to speak, VizOR attempts to freeze the text at each moment of mirroring. This moment of pause opens up the possibility of interpretation without disrupting the line’s relationship to other lines or removing it from its context.
Users can navigate the visualization in a number of ways, hovering over any of the terms to magnify chosen words or lines. Once users click on a given line, the circles will rotate to realign the corresponding terms. Further, rotating the encircling chronology forward or backward in time will result in the rotation of the two inner narratives, mirroring the motion of the flip-book page numbers of the print text.Users can navigate the visualization in a number of ways, hovering over any of the terms to magnify chosen words or lines. Once users click on a given line, the circles will rotate to realign the corresponding terms. Further, rotating the encircling chronology forward or backward in time will result in the rotation of the two inner narratives, mirroring the motion of the flip-book page numbers of the print text.
The finished form of the visualization will be searchable and will also contain external hyperlinks. In keeping with the novel’s data-driven construction, produced in some ways by a crowd-sourced or “collaborative” author, the visualization produces a similarly data-driven reading experience. The goal here is to mirror the outward push of the novel, its awareness and incorporation of external databases like online encyclopedias, and the uniquely distributed reading experience of excitedly setting the book down to search for one of its vague historical entries.
Conclusion
In the name of this speed and efficiency, however, these technologies often strip data of its context and idiosyncracies, creating what Tara McPherson has called “a system of interchangeable equivalencies” (35). VizOR, through its emphasis on the distributed processes of reading and writing across different technologies and media, pushes against this seamless homogenization. By highlighting particular, idiosyncratic moments of reading, we hope to activate the messy, "seamy" place where data meets narrative.
References
Barthes, R. (1977). Rhetoric of the Image. Image, Music, Text. New York: Hill and Wang.
Borner, K. (2003). Visualizing Knowledge Domains. Annual Review of Information Science & Technology. 37. Medford, NJ: Information Today, Inc./American Society for Information Science and Technology. 179-255.
Cartwright, L. (2009). Practices of Looking: An Introduction to Visual Culture. Oxford: Oxford University Press.
Danielewski, M. (2006). Only Revolutions. New York: Pantheon.
Drucker, J. (2008). The Virtual Codex from Page Space to E-space. In Schreibman, S., and R. Siemens, (eds). A Companion to Digital Literary Studies. Oxford: Blackwell. http://www.digitalhumanities.org/companionDLS/ (accessed September 21, 2012.
Hayles, N. K. (2012). How We Think: Digital Media and Contemporary Technogenesis. Chicago: The University of Chicago Press.
Lima, M. (2011). Visual Complexity: Mapping Patterns of Information. New York: Princeton Architectural Press.
Manovich, L. and J. Douglass (2007). Cultural Analytics. UC San Diego Software Studies Initiative.http://lab.softwarestudies.com/2008/09/cultural-analytics.html (accessed: September 21, 2012.)
McPherson, T. (2012). U.S. Operating Systems at Mid-Century: The Intertwining of Race and UNIX. In Nakamura, L. and Chow-White, P. A., (eds). Race and the Internet. New York: Routledge.
Terras, M. (2008). Digital Images for the Information Professional. London: Ashgate.
Tufte, E. (1983). The Visual Display of Quantitative Information. Cheshire, CT: Graphics Press.
Vesna, V. (2007). Database Aesthetics: Art in the Age of Information Overflow. Minneapolis: University of Minnesota Press.
Yau, N. (2011). Visualize This: The FlowingData Guide to Design, Visualization, and Statistics. Hoboken: Wiley Press.
1811	2014	Introduction

The Riddle of Literary Quality is a project funded by the Computational Humanities Program of the Royal Netherlands Academy of Arts and Sciences (KNAW). It runs at Huygens ING in partnership with the Institute for Logic, Language and Computation of the University of Amsterdam, and the Fryske Akademy in Leeuwarden. The aim of the project is to develop a method and the necessary software to analyze low-level and high-level formal features in a corpus of modern Dutch long fiction, to find out whether formal features in the texts play a role in the reception and evaluation of the text by the readers. Can we get more insight into the responses of readers to, for instance, texts with on average longer versus shorter sentences, or using a larger vocabulary, or on average showing a more complex syntactical structure (cf. Jautze et al.)? Is there a difference between those texts that readers consider to be highly literary and those that are experienced as more lowbrow? Can we distinguish texts found good or bad by readers based on formal features in these texts? And how do the opinions of readers correlate with the kind of reader they are?
The project thus aims to correlate formal features with readers’ opinions and readers’ roles. The analysis of the formal features is done through a chain of μServices that we will deal with in the second part of this paper. The first part is addressed to the analysis of readers’ opinions and readers’ roles.
Survey

To gather information about readers and their responses we set up a large online survey in which we asked respondents some personal information (age, gender, postal code, level of education) and sixteen questions to find out what kind of reader they predominantly are: autonomous or ‘distanced’: reading for aesthical pleasure; or heteronomous ‘identifying’: reading for fun, to discover other cultures or places, or to identify with the main characters. We based our distinction and our questions on work done by Von Heydebrand & Winko on sociological aspects of (literary) reading. Next to that, we presented a list of 400 recent novels, Dutch originals or translations into Dutch, and asked them to mark the ones they read. A selection of these novels was presented to them, with the question to evaluate these works on two scales: from ‘not so very literary’ to ‘highly literary’ and from ‘bad’ to ‘good’. The survey ran for six months, and received almost 14000 respondents. Analysis of the results has just started.
The results of the survey will be correlated with the results of the measurements of formal features. We would like to describe the technical set-up we have devised to enable the scholars to analyze the texts in the corpus in a way that is trustworthy and sustainable, using μServices written in Java that can also be used by others to repeat and to verify our analyses.
μServices

Research infrastructure for the Riddle of Literary Quality is designed with three goals in mind: research results must be reproducible; analytical tools must be reusable; the entire workflow must be maintainable and reliable. We aim to provide a toolset that allows for a verifiable system that will focus the discussion on the selected methodology - the procedures and algorithms. This also means that we will make the code behind each μService open source.
To accomplish this goal the digital humanities engineering group at Huygens ING based the research infrastructure both on the results of COST Action IS0704: An Interoperable Supranational Infrastructure for Digital Editions (Interedition) – of which the institute was grant-holder – and the work of Joris van Zundert – chair of the Action. Van Zundert (Huygens ING) specified as an objective of the COST Action the development of lightweight and distributed interoperability solutions. These solutions were implemented through webservices. The CollateX algorithm of Ronald Haentjens Dekker (Huygens ING) and Gregor Middell (University of Würzburg) was among the first and most successful of a series of compact analytical demonstrators called μServices.
The Riddle of Literary Quality does not aim to build a workflow management system. Such a top-down standardization methodology is left to large infrastructural programs like CLARIN, DARIAH or the Dutch Nederlab project. Instead we continue Interedition’s grassroots approach and leave (computational) researchers and PhD students free to experiment with high-level and low-level analytical algorithms in languages that range from Python to Java. These algorithms may or may not grow out to be part of the Riddle’s μService infrastructure and those that are deemed useful are eventually hosted at the institute’s servers. 
The current services fall in three distinct categories: data import and preparation; analysis and visualization and export. In the first group we offer e.g. a series of tools that convert documents to specified standards (such as ePub/PDF to TEI) and set the data in the correct character encoding (such as a conversion from Windows-1252, ISO8859 to UTF-8). To prepare the data for further analysis we have converted parsers like the Dutch Ucto: Unicode Tokenizer (Radboud University of Nijmegen/University of Tilburg) to a μService. The output data of services in this group is a standardized json format that can be read by the analytical services in the second category. Experiments in The Riddle currently focus on this analysis group. μServices in the third category perform output operations. Some create visualizations while others export the data to external environments for further analysis. For stylometric research e.g. we created a μService to export data from The Riddle to R and integrate it with the Stylo() package created at the Universities of Krakow (Macej Eder/Jan Rybicki) and Antwerp (Mike Kestemont).
The entire suite of μServices will remain available for persistent access and may be used in alternate workflows or by external third-party software. Thus the suite does not only allow reproduction of the results of The Riddle but will also support entirely new and original research.
Sample Workflow

As an example of a µServices-driven workflow we present one possible use of gathering statistical data from a corpus of ePubs. First, each ePub is sent to a service that prepares it for analysis by converting the book into a structured TEI document. Character-encoding issues are resolved by a second µService, resulting in a normalized, platform-independent UTF-8 version of the TEI-document. Subsequently, a third service offers extraction operations on the structural level of the file. This service is used to extract all relevant paragraphs. These paragraphs are split into sentences and words by one of a family of (TEI agnostic) tokenizers, such as the Ucto-µService. Statistical analysis of these tokens is possible by sending the resulting list of tokens to the exporter µService, which transforms the extracted tokens into a format suitable for use in R.
Conclusion

To make sure that we are able to answer the main questions of The Riddle of Literary Quality – whether there are any correlations between readers' opinions about certain novels, readers' predominant reading role, and the values for a list of formal low-level and high-level features of the novels – we have chosen to develop a set of µServices that deal with single aspects of the needed analysis. By making these µServices available to other scholars we enable them to repeat and verify our research results. We provide users with tools that can be used to answer different questions than we have in The Riddle, thereby making the tools also useful in a wider sense for new original research. We hope our approach invites others to contribute µServices for further textual humanities research.
References

CollateX. collatex.net
Interedition. www.interedition.eu
>Riddle of Literary Quality.literaryquality.huygens.knaw.nl
Ucto. ilk.uvt.nl/ucto
Eder, M., Kestemont, M. & Rybicki, J., (2013). Stylometry with R: a suite of tools.Digital Humanities 2013: Conference Abstracts. Lincoln: University of Nebraska-Lincoln, pp. 487-89. dh2013.unl.edu/abstracts/ab-136.html
Heydebrand, R. von and Winko, S., (1996), Einfuehrung in die Wertung von Literatur. Systematik – Geschichte – Legitimation. Paderborn etc.: Ferdinand Schoeningh, 1996
Jautze, K., Koolen, C., Cranenburgh, A. van and Jong, H. de, (2013). From high heels to weed attics: a syntactic investigation of chick lit and literature.Proceedings of the Workshop on Computational Linguistics for Literature 2013. aclweb.org/anthology/W/W13/W13-1410.pdf
Zundert, J. van, Middell, G., Hulle, D. Van, Haentjens Dekker, R., et al., (2011). Interedition: Principles, Practice and Products of an Open Collaborative Development Model for Digital Scholarly Editions. Digital Humanities 2011: Conference Abstracts. Stanford: Stanford University. dh2011abstracts.stanford.edu/xtf/view?docId=tei/ab-227.xml
1850	2014	Tesserae is a open-source, online tool for detecting allusions in Classical literature on an automated basis. Originally limited to Latin poetry, the corpus of texts available to Tesserae has recently expanded to include Greek poetry and drama. Word-level n-grams form the foundation of the existing detection algorithm: a standard search returns all instances wherein two words a phrase in a later text shares two words with a phrase in an earlier text. This method has been previously demonstrated to reliably capture intertextual parallels already noted by philologists and to identify significant, previously unrecorded intertexts.
The ability to detect allusions across the language barrier would represent an evolutionary expansion in Tesserae’s functionality as well as a significant contribution to Classical philology. Roman poets openly acknowledged their indebtedness to Greek literature (Horace famously remarked, “Greece, being conquered, tamed its wild conqueror, and brought the Arts to rustic Latium") and scholarly studies of Latin poetry have long commented on allusions to earlier Greek sources. To apply the existing system where Latin text alludes to Greek, Tesserae requires a translation dictionary linking Greek lemmata to associated Latin lemmata. This paper details two methods for building such a dictionary on an automated basis and compares their relative merits as measured by their ability to capture parallels between book one of Vergil's Aeneid and the Iliad of Homer, as noted by G.N. Knauer in his commentary.
The first method represents an original application of Bayes' theorem to a word-by- word alignment of the Greek New Testament with Jerome's Latin Vulgate.

For a given Greek word Gi, the set of Greek Bible verses in which it appears is identified. The words contained in the Latin translation of these verses become the set of possible translation candidates L. For each Li, the set of possible Greek words G is gathered from the set of Greek verses corresponding to the Latin verses in which Li appears. P(Gi|Li) is represented by the number of words in set G which may share a lemma with Gi, divided by the total number of words in that set. The probability of Gi is represented by a similar calculation, where the set of all words within the Greek text is substituted for G. The value of P(Li) is analogous. The success of this relatively simple alignment algorithm as compared with more classical IBM Models or Hidden Markov Models may be explained by the grammatical similarity of these two inflected languages and importance placed by the translator in remaining precisely faithful to the syntax of the original text. 
The second method employs English as a pivot language, in a method inspired by work done previously by Jeffrey Rydberg-Cox at Perseus on Latin-Greek synonymy. Using the XML-encoded digital editions of Lewis and Short’s Latin-English Lexicon and Liddell and Scott’s Greek-English Lexicon, two dictionaries widely considered authoritative for Classical languages and available through the Perseus Digital Library, each Latin or Greek headword is characterized by a feature set composed of the English words appearing in its definition. The Python-based Gensim topic modelling tools are then used to transform the English word counts to TF-IDF weights and calculate similarities between the dictionary entries. The similarity scores between entries are then interpreted as similarities in meaning between the respective headwords. 
Each of the two methods described above produces pairwise similarities between all Greek and Latin words considered, with those pairings rated by a probability measure between 0 and 1. Because each Greek word may have more than one possible Latin translation, each method accepts the top two translation candidates as valid. 
The text of Homer’s Iliad is then indexed according to a feature set made up of Latin translation candidates. Each Greek token is lemmatized, and the token is then indexed according to all possible Latin translation candidates. Because lemmatization is unsupervised, ambiguous forms may have multiple possible Greek lemmata. Each possible Greek lemma will have two translation candidates if the respective translation method is successful, or zero if no translations are found. The text of Vergil’s Aeneid is indexed simply according to the possible Latin lemmata of each token. A given token in Vergil matches a token in Homer where one or more possible lemmata for the Latin word match against the set of translation candidates for the Greek word. A pair of phrases, one in Greek and the other in Latin, which share two or more words that match in this way, is returned as a possible allusion.
The two methods are evaluated by their ability to detect a subset of Aeneid-Iliad parallels collated from the commentary of G.N. Knauer. Each method retrieves a distinct, though partially overlapping, subset of the parallels noted by Knauer. Comparison of the respective performance of both methods suggests that, while each method can be shown to identify significant Latin-Greek allusions, the Bayesian alignment method provides better recall of the benchmark set than the 'pivot' method at the expense of precision. We ultimately aim to combine the output of both approaches into a single feature set.
References

Tesserae, tesserae.caset.buffalo.edu (Accessed on November 1, 2013).
Coffee, N. et. al (2012).: "Intertextuality in the Digital Age." Transactions of the American Philological Association, Volume 142, Number 2, Autumn 2012 pp. 383-422
Epistles, 2.1.156–7
G.N. Knauer (1964): "Die Aeneis und Homer: Studien zur poetischen Technik Vergils mit Listen der Homerzitate in der Aeneis." Gottingen: Vandenhoeck & Ruprecht.
Personal communication with author; tool archived at perseus.mpiwg-berlin.mpg.de/PR/syn.ann.html
www.perseus.tufts.edu
radimrehurek.com/gensim
1888	2014	The orthography/identity hypothesis proposes that a speaker’s motivation for selecting between available orthographic variants in a language (e.g. between British colour and American color in English) is to some extent informed by the speaker’s desire to express a certain identity 1 2 3.  In the Canadian context, where a mixture of American/British variants are used – often with non-categorical preference 45 – Heffernan et al. 6 developed a method to qualify the orthography/identity connection in terms of ideology and show that during periods of increased “anti-Americanism,” specifically during unpopular American-led wars, American variant use declines relative to the British.  Heffernan et al.’s data cover the years 1921 to 2004, and are derived from the student newspaper The Gateway at the University of Alberta in Alberta, Canada.  Their method involved locating expressions of national sentiment for each year of the data, rating “anti-American sentiment” on a 7-point Likert scale (255 ratings over 85 years performed by each author) and correlating this with the relative frequencies of 15 orthographic variables (Table 2, though color / colour is my addition): the negative correlation obtained was quite high, with Pearson-r -0.715, p = 0.001.
However, follow-up work by the present author, using data in the same timeframe from the archive of the University of British Columbia’s student newspaper The Ubyssey (~50 million words) in the neighbouring province of British Columbia, failed to find similar short-term diachronic changes in variant use correlated with periods of increased “anti-American sentiment” 7.  Following Heffernan et al.’s method, an insignificant correlation was obtained: Pearson-r -0.434, p = 0.064.  Historical relative orthographies do differ between Canada’s provinces 8 910 but, assuming the strong connection between orthography and identity, no clear explanation remains for the lack of correlation in other Canadian data.  Without dispensing with the orthography/identity hypothesis, I hypothesize that proximal linguistic contexts are also motivating factors in variant selection, and propose to integrate a more context sensitive model into this top-down, language-external theory of linguistic identity performance.
To test for contextual differences, I treated the problem as one of word sense disambiguation, where the goal is to distinguish lexemes using a set of features and a computational language model—a technique most often used to distinguish between ambiguous meanings of homonyms (such as judge, bank, bow, etc.) 11.  Features used were a window of words surrounding each variant (8 words either side of the target was found optimal, excluding other instances of variables if present) and the model was Naïve Bayes.  If orthographic variants can be discriminated based on surrounding context, we can assume that those words are in some way unique—with the interesting implication, in the extreme case, that spelling variants might not just diverge orthographically but semantically, as well 12.  Maybe they mean different things.  My experiments attempted to disambiguate variants in each variable from one another using unsupervised and supervised classification, in both cases using the Naïve Bayes form.
Though Naïve Bayes makes the linguistically improbable assumption of feature independence, it has been noted for its precision in classification problems in spite of this simplification (i.e. its ‘naiveté’) 13.  For unsupervised classification I used my own Python implementation of a Naïve Bayes classifier where the parameter estimates are learned through Estimation Maximization (EM), as described in e.g. Manning and Schütze 14.  As Pedersen 15 observes, testing the results of unsupervised classification is complicated by the fact that the algorithm does not assign labels to inputs, instead clustering them, but accuracy can be represented as the proportion of the dominant variant in each cluster.  My classifier outputs two ‘sense groups’, which would ideally correspond to the American or Canadian variant.  After performing 10 trials and averaging the results, I found that only three variables out of 16 (Heffernan et al. exclude color / colour—I include it) produced significant results, in that their prediction accuracies departed from – or improved upon – their ‘lower bound’ accuracies, where the lower bound is the relative frequency of each variant and therefore the accuracy one would achieve simply by assigning each variant to a category based on its occurrence.  For brevity, only these three are represented in Table 1.
  American variant  accuracy   lower bound   Canadian variant   accuracy   lower bound 
 color 55.5%  54%  colour  65.6%  46% 
 gray 23.2%  17%  grey  86.2%  83% 
 jewelry 51.3%  49%  jewellery  55.4%  51% 
Unsupervised classification for colour improves accuracy by 19.4% over its lower bound, but increases for other variables are marginal and – like colour – generally apply to one variant only.
For supervised classification I used the Naïve Bayes classifier in the Python library Natural Language Tool Kit (NLTK)16, a similar method to Mahowald’s 17 recent study in which y- and ­th- pronouns were disambiguated in a corpus of Shakespeare’s plays based on context.  Whereas unsupervised classification performed poorly, supervised classification obtained surprising accuracy for multiple variables after 10 validation trials.  These results are summarized in Table 2, ranked by accuracy, with an asterisk denoting significance at the p = 0.001 level.  In this experiment, the lower bound for each variable is 50% because a random subset of the tokens was evaluated and counts were set equal for each variant during testing.
           variable                    accuracy     total count
jewelry / jewellery 81.6%*          564
gray / grey 79.3%*          3112
color /colour 74.5%*          5312
program / programme 70.1%*          5704
honor / honour 62.0%*          2538
enrollment / enrolment 61.2%*          2086
humor / humour 61.1%*          2862
neighbor / neighbour 60.7%*          494
defense / defence 58.6%*          5640
judgment / judgement 56.8%          928
offense / offence 56.3%*          1568
centered / centred 55.7%          488
marvelous / marvellous 55.6%          316
fulfill / fulfil 54.7%          312
labeled / labelled         54.4%          270
kilometers / kilometres 40.0%          72
It would seem that we are able to predict, sometimes with high accuracy, whether certain variables will realize their American or British variant based on context.  But why?  If orthographic variations are simply different graphemes of the same lexeme, decided rather capriciously by an American lexicographer in the nineteenth century 18, why should this be possible?
The Naïve Bayes module in NLTK provides output for identifying features most useful in making its decisions, and can help answer this question.  For gray / grey, the case is clear, since the terms most likely to indicate British grey are Pt. and Point (Point Grey is the name of the land on which the University of British Columbia lies), and terms indicating American gray are proper nouns like Bob, John, and Stuart (Gray is a common surname).  A revealing result, but only so far as it reveals a highly restrictive context in non-compositional forms, and might suggest this variable be excluded from further testing.  Contexts for color / colour are more interesting, however, and fall into two large subjective categories: ‘cultural’ and ‘technological’ (Table 3).
  variant   category   informative features
colour cultural diversity, women, people, racism, queer
color cultural people
colour technological connected, jet, print, modem, monitor
color technological cartoons, TV  
As a collocation analysis reveals, in the ‘cultural’ category phrases such as women of colour, people of colour, andqueers of colour occur often with colour – 225 total instances, its most frequent collocate – but hardly ever with color (11 instances).  In the ‘technological’ category, computer terms appear with colour and entertainment terms with color, where these terms are often found in advertisements and the site of these interactions tend to be local for colour and global for color (a local transaction for a colour monitor, at least prior to the expansion of current global markets, but the international consumption of color television).  However, these phrases are easily recognizable as historically specific (to around post-1980).  Indeed, the unsupervised classification of colour backs up this historical selectivity: significantly more of the items grouped at 65.6% accuracy are from this decade—context and history are intertwined, of course, and it exceeds my scope to disambiguate these here.  But the more a-historical distribution of terms predicting jewelry / jewellery suggests historical clustering is not inevitably the rule: local activities like piercing and repairs, and localities denoted by West and Point (i.e. the location of a shop in WestPoint Grey) predict British jewellery, but generic sales terms accessories, fine, place, and giftware predict American jewelry.  In sum, advertisements, or, more generically, ‘solicitations’, are the dominant vehicle of these variants and prefer the British when the activity is local (both economically and socially—these will be further described) and the American generally.  I will also further discuss how accounting for genre affects classification accuracy.  Overall, British variants are more uniquely contextualized, and therefore more easily discriminated, than American.
Qualitative sociolinguistic approaches like Heffernan et al.’s (2010) locate identity as an exterior motivating condition for language, with the necessary assumption that orthography is selected independently of linguistic context.  And though this paper finds that this assumption does not hold, the ability to disambiguate orthographic variants based on context is interesting, but not explanatory in its own right.  These contexts are also motivated, and computational techniques take us full-circle back to considering ideological – but more interactional – motivations for linguistic context. 
References

1. Lipski, J. (1975). Orthographic variation and linguistic nationalism. La Monda Lingvo-Problemo 6. 37-48.
2. Schieffelin, B. B., and Doucet, R. C. (1994). The ‘real’ Haitian Creole: Ideology, metalinguistics and orthographic choice. American Ethnologist 21. 176–200.
3. Sebba, M. (2000). Orthography and ideology: Issues in Sranan spelling. Linguistics 38. 925–948.
4. Chambers, J. K. (2011). ‘Canadian dainty’: The rise and decline of Briticisms in Canada. In Legacies of Colonial English: Studies in Transported Dialects, 224-241. Cambridge: Cambridge UP.
5. Pratt, T. K. (1993). The hobgoblin of Canadian English spelling. In S. Clarke (ed.), Focus on Canada, 45–64. Amsterdam: Benjamins.
6. Heffernan, K., Borden, A., Erath, A. C., and Yang, J-L. (2010). Preserving Canada’s ‘honour’: Ideology and diachronic change in Canadian spelling variants. Written Language and Literacy 13(1). 1-23.
7. Grue, D. (forthcoming). Testing Canada's 'honour': Does orthography index ideology? Strathy Student Working Papers on Canadian English.
8. Brinton, L. and Fee, M. (2001). Canadian English. In John Algeo (ed.), The Cambridge history of the English language, vol. 6, 422-439. Cambridge: Cambridge UP.
9. Ireland, R. J. (1979). Canadian spelling: An empirical and historical survey of selected words. Ph.D. dissertation, York University, Toronto.
10. Ireland, R. J. (1980). Canadian spelling: How much British? How much American? English Quarterly 12(4). 64-80.
11. Pedersen, T. (2002). A baseline methodology for word sense disambiguation. Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics. 126-135.
12. Miller, G. A. and Charles, W. G. (1991). Contextual correlates of semantic similarity. Language and Cognitive Processes 6(1). 1-28.
13. Abney, S. (2008). Semisupervised Learning for Computational Linguistics. New York: Chapman & Hall.
14. Manning, C. D. and Schütze, H. (1999). Foundations of Statistical Natural Language Processing. Cambridge, Mass.: MIT Press.
15. Pedersen, T. (1998). Learning Probabilistic Models of Word Sense Disambiguation. Ph.D. Dissertation, Southern Methodist University, University Park, Texas.
16. Bird, S., Klein, E., and E. Lopez. (2009). Natural Language Processing with Python. Sebastopol, CA: O'Reilly Media.
17. Mahowald, K. (2012). A Naïve Bayes classifier for Shakespeare's second-person pronoun. Literary and Linguistic Computing 27(1). 17-23.
18. Webster, N. (1846). A dictionary of the English language; abridged from the American dictionary. [American Dictionary]. New York: Huntington and Savage.
1904	2014	In the early modern period, rather than having access to a full-text play, actors learned their lines using “Actors’ parts,” hastily handwritten documents that provided them with only their cues and lines. Traditionally, today’s actors learn their lines from full-text plays, without any computer assistance. Digital Actors’ Parts (DAP) is an online environment that both mimics and enhances the early modern acting experience in order to facilitate actors learning their lines. DAP is the first project to give users an interactive experience with an early-modern-inspired “actor’s part,” which encourages both active reading and memorization, in turn leading to a better understanding of the texts themselves.

In Orality and Literacy: The Technologizing of the Word, Walter Ong theorized that we were in a “second orality” based on the “use of writing and print” and considered how memorization worked in oral and written circumstances;Digital Actor’s Parts allows us to consider memory as it relates to online technologies. In Shakespeare studies, the function of memory has been an important area of study for years, as scholars debate whether certain texts are “memorially reconstructed,” that is, printed from actors’ memories rather than a playwright’s written text. Our tool argues for the importance of understanding memorization when it comes to cognition and the comprehension of literary and theatrical works. DAP will not only provide expert scholars with a platform for reconsidering memory specifically as it relates to Shakespeare, it will also offer a tool that will be of use for beginners and professionals both in the classroom and the theatre.

As the most prominent figure in English theatre, Shakespeare’s plays have been encoded multiple times; digital Shakespeare projects abound. Unlike most single-source projects, Digital Actors’ Parts brings together open-access data from multiple sources, including the encoded texts from the MLA Committee on the New Variorum Edition of Shakespeare, the Folger Shakespeare Library’s Digital Texts, the Internet Shakespeare Editions, and Open Source Shakespeare. Our project is designed to incorporate new data from these sources, because, forinstance, the MLA and Folger Digital Texts are still encoding Shakespeare’s works and have not yet released their complete data-set. These online Shakespearean texts are as diverse as the original printed editions. By building on existing Shakespearean projects, DAP is not limited to a single copy-text, so an actor or director could choose either the folio or quarto version of, for instance, King Lear.

Digital Actors’ Parts helps students, researchers, and theatre practitioners learn lines by allowing them to select a Shakespearean play, edition, and character from a pre-populated dropdown menu. The users are then presented with their first cue and the ability to type in their line(s). Users will also be able to ask for prompts: their first word, first line, or entire speech. By entering the text of their speeches and checking what they have typed, users receive a score depending on their accuracy. This feature makes Shakespeare’s text interactive in a way beyond most digital editions. The accuracy score allows for the potential gamification of becoming a Shakespearean actor, which will especially promote student learning. In the classroom, DAP encourages students and teachers to memorize and perform Shakespeare’s language, that is, to engage with it beyond simply reading. This online tool makes it easy to transition from, as the saying goes, the page to the stage.

Although Shakespeare’s plays continue to be popular both online and in the theatre, most digital tools for the study of Shakespeare are aimed at literary scholars, teachers, and K-12 students. The intended audience of Digital Actors’ Parts also includes theatre professionals (actors, directors, dramaturgs): indeed, this is not simply a digital humanities project, but also a digital fine arts endeavor.

Digital Actor’s Parts is currently a work in progress. The prototype for DAP runs on an instance of the Web.py framework. Web.py is a Python web framework that is both simple and powerful. We chose to use this framework because it allows rapid prototyping allowing us to concentrate on the specific features that we need to implement. On the other hand, the plays that we are utilizing are encoded and distributed in different formats. For example: The plays from Folger Digital Texts are encoded in TEI-compliant XML, MLA’s are encoded only in XML, and the materials from Open Source Shakespeare are bundled in a Microsoft Access database. Therefore, before being displayed through the interface, the plays must be harvested from their original institutional repositories, parsed, transformed using XSL Transformations and stored into a database. The accuracy scores and metadata for the individual user records are maintained and stored in a using a separate table structure. Figure 1 shows a prototype of the user interface.


Fig. 1: Prototype of the user interface for Digital Actors’ Parts.

Unlike other online memorization tools such as Memorizer and Memorize Now, this project does not require users to first input the text they memorize—which could be a particularly fraught process for texts as complicated as Shakespeare.Digital Actors’ Parts, furthermore, is ideal to help users memorizing all of a character’s lines (and not just a single speech) as it delivers the appropriate cues, unlike existing tools. The automatically-generated accuracy score, another feature other programs lack, can add elements of fun and competition as users strive for mastery of Shakespeare’s text.

In its first iteration, Digital Actors’ Parts relies on a modern web browser to deliver its content and experience. For future releases, we hope to implement an application for mobile devices. This app will allow users to expand the possibilities of interaction by taking DAP to the rehearsal space, theater, or experimenting with entirely new locations. Having this tool in hand will encourage earlier inclusion of blocking and business in the rehearsal process and will also be useful for active classroom learning.

In 2014, Shakespeare 450 will mark the anniversary of Shakespeare’s birth with performances, workshops, and lectures. We envision Digital Actors’ Parts participating in this global celebration by making it easier for performance troupes from amateur to professional to take part in staging Shakespeare’s plays around the world. In “To the Memory of my Beloved the Author, Mr. William Shakespeare,” Ben Jonson famously declared that Shakespeare was “not of an age, but for all time”:DAP is part of the larger movement bringing Shakespeare into the twenty-first century with new digital resources and tool. DAP will help us understand how we engage with Shakespeare’s works in the most fundamental ways and will allow us to theorize memory as it relates not simply to orality or written texts, but also to innovative, interactive, digital tools.

Ultimately, Digital Actors’ Parts capitalizes on the proliferation of open-source Shakespeare texts, offering one answer to the question of “where do we go from here?” with digital projects. This project goes beyond aggregation by suggesting one way these texts can be fruitfully combined. While offering a valuable rehearsal tool in itself, DAP also encourages further research on Shakespeare’s works, the digital practice of combining multiple corpora, and interactive online learning methods.

References
Simon Palfrey and Tiffany Stern, Shakespeare in Parts, Oxford: OUP, 2007.

Walter J. Ong, Orality and Literacy: The Technologizing of the Word, London & New York: Methuen, 1982, 136. See also esp. 57-68.

On “memorial reconstruction” and the function of memory in Shakespearean texts, see Laurie Maguire, Shakespeare’s Suspect Texts: The ‘Bad’ Quartos and their Contexts, Cambridge: CUP, 1996 and Paul Werstine’s work, especially “A Century of ‘Bad’ Shakespeare Quartos” Shakespeare Quarterly 50 (1999): 310-33.

MLA New Variorium Shakespeare encoding: https://github.com/mlaa/nvs-challenge Folger Digital Texts: www.folgerdigitaltexts.org The Internet Shakespeare Editions: ise.uvic.ca Open-Source Shakespeare: www.opensourceshakespeare.org

Web.py: webpy.org

Memorizer: www.memorizer.me Memorize Now: www.memorizenow.com

Shakespeare 450: www.shakespeareanniversary.org

Ben Jonson, “To the Memory of my Beloved, the Author, Mr. William Shakespeare,” in Shakespeare’s first folio (1623). Facsimile: internetshakespeare.uvic.ca/Library/facsimile/book/SLNSW_F1/9
1925	2014	The orthography/identity hypothesis proposes that a speaker’s motivation for selecting between available orthographic variants in a language (e.g. between British colour and American color in English) is to some extent informed by the speaker’s desire to express a certain identity 1 2 3.  In the Canadian context, where a mixture of American/British variants are used – often with non-categorical preference 45 – Heffernan et al. 6 developed a method to qualify the orthography/identity connection in terms of ideology and show that during periods of increased “anti-Americanism,” specifically during unpopular American-led wars, American variant use declines relative to the British.  Heffernan et al.’s data cover the years 1921 to 2004, and are derived from the student newspaper The Gateway at the University of Alberta in Alberta, Canada.  Their method involved locating expressions of national sentiment for each year of the data, rating “anti-American sentiment” on a 7-point Likert scale (255 ratings over 85 years performed by each author) and correlating this with the relative frequencies of 15 orthographic variables (Table 2, though color / colour is my addition): the negative correlation obtained was quite high, with Pearson-r -0.715, p = 0.001.

However, follow-up work by the present author, using data in the same timeframe from the archive of the University of British Columbia’s student newspaper The Ubyssey (~50 million words) in the neighbouring province of British Columbia, failed to find similar short-term diachronic changes in variant use correlated with periods of increased “anti-American sentiment” 7.  Following Heffernan et al.’s method, an insignificant correlation was obtained: Pearson-r -0.434, p = 0.064.  Historical relative orthographies do differ between Canada’s provinces 8 910 but, assuming the strong connection between orthography and identity, no clear explanation remains for the lack of correlation in other Canadian data.  Without dispensing with the orthography/identity hypothesis, I hypothesize that proximal linguistic contexts are also motivating factors in variant selection, and propose to integrate a more context sensitive model into this top-down, language-external theory of linguistic identity performance.

To test for contextual differences, I treated the problem as one of word sense disambiguation, where the goal is to distinguish lexemes using a set of features and a computational language model—a technique most often used to distinguish between ambiguous meanings of homonyms (such as judge, bank, bow, etc.) 11.  Features used were a window of words surrounding each variant (8 words either side of the target was found optimal, excluding other instances of variables if present) and the model was Naïve Bayes.  If orthographic variants can be discriminated based on surrounding context, we can assume that those words are in some way unique—with the interesting implication, in the extreme case, that spelling variants might not just diverge orthographically but semantically, as well 12.  Maybe they mean different things.  My experiments attempted to disambiguate variants in each variable from one another using unsupervised and supervised classification, in both cases using the Naïve Bayes form.

Though Naïve Bayes makes the linguistically improbable assumption of feature independence, it has been noted for its precision in classification problems in spite of this simplification (i.e. its ‘naiveté’) 13.  For unsupervised classification I used my own Python implementation of a Naïve Bayes classifier where the parameter estimates are learned through Estimation Maximization (EM), as described in e.g. Manning and Schütze 14.  As Pedersen 15 observes, testing the results of unsupervised classification is complicated by the fact that the algorithm does not assign labels to inputs, instead clustering them, but accuracy can be represented as the proportion of the dominant variant in each cluster.  My classifier outputs two ‘sense groups’, which would ideally correspond to the American or Canadian variant.  After performing 10 trials and averaging the results, I found that only three variables out of 16 (Heffernan et al. exclude color / colour—I include it) produced significant results, in that their prediction accuracies departed from – or improved upon – their ‘lower bound’ accuracies, where the lower bound is the relative frequency of each variant and therefore the accuracy one would achieve simply by assigning each variant to a category based on its occurrence.  For brevity, only these three are represented in Table 1.

  American variant	 accuracy 	 lower bound 	 Canadian variant 	 accuracy 	 lower bound 
 color	55.5% 	54% 	colour 	65.6% 	46% 
 gray	23.2% 	17% 	grey 	86.2% 	83% 
 jewelry	51.3% 	49% 	jewellery 	55.4% 	51% 
Unsupervised classification for colour improves accuracy by 19.4% over its lower bound, but increases for other variables are marginal and – like colour – generally apply to one variant only.

For supervised classification I used the Naïve Bayes classifier in the Python library Natural Language Tool Kit (NLTK)16, a similar method to Mahowald’s 17 recent study in which y- and ­th- pronouns were disambiguated in a corpus of Shakespeare’s plays based on context.  Whereas unsupervised classification performed poorly, supervised classification obtained surprising accuracy for multiple variables after 10 validation trials.  These results are summarized in Table 2, ranked by accuracy, with an asterisk denoting significance at the p = 0.001 level.  In this experiment, the lower bound for each variable is 50% because a random subset of the tokens was evaluated and counts were set equal for each variant during testing.

           variable                   	accuracy	    total count
jewelry / jewellery	81.6%*	         564
gray / grey	79.3%*	         3112
color /colour	74.5%*	         5312
program / programme	70.1%*	         5704
honor / honour	62.0%*	         2538
enrollment / enrolment	61.2%*	         2086
humor / humour	61.1%*	         2862
neighbor / neighbour	60.7%*	         494
defense / defence	58.6%*	         5640
judgment / judgement	56.8%	         928
offense / offence	56.3%*	         1568
centered / centred	55.7%	         488
marvelous / marvellous	55.6%	         316
fulfill / fulfil	54.7%	         312
labeled / labelled        	54.4%	         270
kilometers / kilometres	40.0%	         72
It would seem that we are able to predict, sometimes with high accuracy, whether certain variables will realize their American or British variant based on context.  But why?  If orthographic variations are simply different graphemes of the same lexeme, decided rather capriciously by an American lexicographer in the nineteenth century 18, why should this be possible?

The Naïve Bayes module in NLTK provides output for identifying features most useful in making its decisions, and can help answer this question.  For gray / grey, the case is clear, since the terms most likely to indicate British grey are Pt. and Point (Point Grey is the name of the land on which the University of British Columbia lies), and terms indicating American gray are proper nouns like Bob, John, and Stuart (Gray is a common surname).  A revealing result, but only so far as it reveals a highly restrictive context in non-compositional forms, and might suggest this variable be excluded from further testing.  Contexts for color / colour are more interesting, however, and fall into two large subjective categories: ‘cultural’ and ‘technological’ (Table 3).

  variant	  category	  informative features
colour	cultural	diversity, women, people, racism, queer
color	cultural	people
colour	technological	connected, jet, print, modem, monitor
color	technological	cartoons, TV  
As a collocation analysis reveals, in the ‘cultural’ category phrases such as women of colour, people of colour, andqueers of colour occur often with colour – 225 total instances, its most frequent collocate – but hardly ever with color (11 instances).  In the ‘technological’ category, computer terms appear with colour and entertainment terms with color, where these terms are often found in advertisements and the site of these interactions tend to be local for colour and global for color (a local transaction for a colour monitor, at least prior to the expansion of current global markets, but the international consumption of color television).  However, these phrases are easily recognizable as historically specific (to around post-1980).  Indeed, the unsupervised classification of colour backs up this historical selectivity: significantly more of the items grouped at 65.6% accuracy are from this decade—context and history are intertwined, of course, and it exceeds my scope to disambiguate these here.  But the more a-historical distribution of terms predicting jewelry / jewellery suggests historical clustering is not inevitably the rule: local activities like piercing and repairs, and localities denoted by West and Point (i.e. the location of a shop in WestPoint Grey) predict British jewellery, but generic sales terms accessories, fine, place, and giftware predict American jewelry.  In sum, advertisements, or, more generically, ‘solicitations’, are the dominant vehicle of these variants and prefer the British when the activity is local (both economically and socially—these will be further described) and the American generally.  I will also further discuss how accounting for genre affects classification accuracy.  Overall, British variants are more uniquely contextualized, and therefore more easily discriminated, than American.

Qualitative sociolinguistic approaches like Heffernan et al.’s (2010) locate identity as an exterior motivating condition for language, with the necessary assumption that orthography is selected independently of linguistic context.  And though this paper finds that this assumption does not hold, the ability to disambiguate orthographic variants based on context is interesting, but not explanatory in its own right.  These contexts are also motivated, and computational techniques take us full-circle back to considering ideological – but more interactional – motivations for linguistic context. 

References
1. Lipski, J. (1975). Orthographic variation and linguistic nationalism. La Monda Lingvo-Problemo 6. 37-48.

2. Schieffelin, B. B., and Doucet, R. C. (1994). The ‘real’ Haitian Creole: Ideology, metalinguistics and orthographic choice. American Ethnologist 21. 176–200.

3. Sebba, M. (2000). Orthography and ideology: Issues in Sranan spelling. Linguistics 38. 925–948.

4. Chambers, J. K. (2011). ‘Canadian dainty’: The rise and decline of Briticisms in Canada. In Legacies of Colonial English: Studies in Transported Dialects, 224-241. Cambridge: Cambridge UP.

5. Pratt, T. K. (1993). The hobgoblin of Canadian English spelling. In S. Clarke (ed.), Focus on Canada, 45–64. Amsterdam: Benjamins.

6. Heffernan, K., Borden, A., Erath, A. C., and Yang, J-L. (2010). Preserving Canada’s ‘honour’: Ideology and diachronic change in Canadian spelling variants. Written Language and Literacy 13(1). 1-23.

7. Grue, D. (forthcoming). Testing Canada's 'honour': Does orthography index ideology? Strathy Student Working Papers on Canadian English.

8. Brinton, L. and Fee, M. (2001). Canadian English. In John Algeo (ed.), The Cambridge history of the English language, vol. 6, 422-439. Cambridge: Cambridge UP.

9. Ireland, R. J. (1979). Canadian spelling: An empirical and historical survey of selected words. Ph.D. dissertation, York University, Toronto.

10. Ireland, R. J. (1980). Canadian spelling: How much British? How much American? English Quarterly 12(4). 64-80.

11. Pedersen, T. (2002). A baseline methodology for word sense disambiguation. Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics. 126-135.

12. Miller, G. A. and Charles, W. G. (1991). Contextual correlates of semantic similarity. Language and Cognitive Processes 6(1). 1-28.

13. Abney, S. (2008). Semisupervised Learning for Computational Linguistics. New York: Chapman & Hall.

14. Manning, C. D. and Schütze, H. (1999). Foundations of Statistical Natural Language Processing. Cambridge, Mass.: MIT Press.

15. Pedersen, T. (1998). Learning Probabilistic Models of Word Sense Disambiguation. Ph.D. Dissertation, Southern Methodist University, University Park, Texas.

16. Bird, S., Klein, E., and E. Lopez. (2009). Natural Language Processing with Python. Sebastopol, CA: O'Reilly Media.

17. Mahowald, K. (2012). A Naïve Bayes classifier for Shakespeare's second-person pronoun. Literary and Linguistic Computing 27(1). 17-23.

18. Webster, N. (1846). A dictionary of the English language; abridged from the American dictionary. [American Dictionary]. New York: Huntington and Savage.
1947	2014	1. Introduction
1.1 Overview
Scripts are usually seen as simple carriers of languages. Research on scripts until recently has been minimal and niche, except for the field of paleography. Scripts are an important part of the cultural heritage of humanity and its analysis and study requires more research. Fortunately, there is a growing interest in analysis of scripts. Altmann 1 published a volume titled “Analyses of Scripts: Properties of Characters and Writing Systems” to explore various properties of writing systems and scripts such as complexity, ornamentality and distinctivity.

Changizi et al 2 discuss the various contour configurations of written symbols and their similarity to the environment in which they were produced.  They also study the distribution of the configurations of various scripts. They 3 further discuss the character complexity and the redundancy of stroke combinations of various writing systems in human history. It is to be noted that analysis in [2] [3] and most methods in [1] were performed manually. Traditionally, analysis and study in paleography have also been done manually. Digital paleographic methods are at present making more inroads into the field. However, applying quantitative analysis on paleographic data is not yet popular and standardized 4. This is partially due to the difficulty of quantifying paleographical features, and partially due to the lack of defined metrics with theoretical and qualitative underpinnings.

1.2 Proposed Framework
We propose here a quantitative analysis framework for scripts that is largely computational and requires minimal user interaction. We do not particularly aim at providing a completely autonomous framework, but rather to aid the user as much as possible, with the ability to manually intervene/override as required. The framework is based on various methods and techniques employed in the field of graphonomics. Our framework is grounded on the principles of handwriting production and handwriting analysis.

We also explore various features used in the related area of gesture design and recognition and its application to the analysis of scripts. Through this, we attempt to find relevant metrics (with qualitative significance) with sufficient evaluation that might be used for glyphs and scripts for various purposes such as classification, visualization etc. The computed quantitative features could serve as descriptors for scripts, and  be used for comparing and analyzing scripts. This is especially applicable to the field of paleography, where such quantitative features are much needed. 

2. Quantitative Analysis Framework
Our proposed framework consists of the following modules. 

2.1 Spline Conversion
The characters of scripts are externally represented as B-splines. B-splines are very efficient in preserving the shape and curvature of glyphic segments. Additionally, they can be manipulated without significant effort. Rather than representing glyphs as pixelated data, converting them into splines eases analysis. This conversion of the glyphic shape of a character can be done automatically or manually. In a manual process, the user defines each shape of a character directly using a set of B-splines, or explicitly draws the shape, which is then internally converted into B-splines. An automatic conversion of glyphs involves thinning and then its conversion into splines. 

2.2 Trajectory Reconstruction
The shape of a character is static and does not contain all information required for analysis. Dynamic information relating to pen movements are not present in the shape. Trajectory reconstruction attempts to recover this temporal information 5. This kinematic information is essential in defining the character. With paleographic scripts reconstructing dynamic information is necessary as the trajectory is usually unknown. Also by altering the trajectory, the changes in dynamic features can be observed. 

The recovery is performed by conducting a global search using a set of heuristics, such as length minimization and curvature minimization 6. Especially in case of paleographic scripts, the algorithm is able to provide several alternative viable writing trajectories.

2.3 Stroke Segmentation
Characters are best analyzed as sequences of natural strokes. Breaking them down into basic strokes is the optimal way for analyzing written characters. It also enables us to understand the process of handwriting. Stroke segmentation retrieves the structure of a character based on its trajectory. This is performed by segmentation of the character at various important landmark points of the recovered trajectory such as the extrema of curvature 7.

2.4 Character Representation
Writing is usually considered to consist of two fundamental types of strokes – up-strokes and down-strokes 8. This distinction is necessary since both these two types behave differently. It has been proved that down-strokes usually do not show a lot of variation in handwriting compared to up-strokes 9.

A character is internally represented as a set of strokes. This is consistent with the way that the character is internalized and produced by humans. This allows us to derive better features that are more natural and descriptive. Later, it will be possible to apply handwriting modelling to generate alternative scribal variants. 

2.5 Feature Extraction
For quantitative analysis, features need to be computed from the characters. These serve to quantify several aspects of the characters. We considered various features used in the field of gesture recognition 1011 1213 and found the features listed below to be relevant to the analysis of characters. We also propose some features in addition to those found in the literature. These features/metrics could additionally serve as descriptors for the scripts. As quantitative features these can be widely used in analysis and/or visualization. 

2.5.1 Production Features
The effort that is required to realize and produce a character is an important element of its analysis.  It is related to the number of velocity inversions, number of velocity breaks, number of pen lifts, etc., which are computed from the stroke representation of the character. These features are calculated from the temporal information that was re-constructed at the earlier stage. These are relevant as they quantify the dynamic handwriting behavior present in the character. 

2.5.2 Geometric Features
Geometric features throw more light on the visual aspect of characters. These are essential for the study of the judged (visual) complexity 14. The features that relate to visual appearance are compactness, openness, number of crossings, average curvature, sum of internal angles, bounding area, etc.  Some of these, like compactness and openness, are ratios of several parameters such as length of strokes, distance between first and last points,  while others, like average curvature, are derived directly from the glyph structure.

2.5.3 Cognitive Features
Though cognitive features cannot be directly measured, some cognitive features could be interpreted from the geometry of a character. The number of unique landmark points required to plan the trajectory is a possible feature that has correlation with the cognitive load of the glyph. An additional measure is the number of minimal points required to recreate the character.

2.5.4 Stroke Features
Stroke based features such as the primary direction of the glyph, ratio of upstrokes to downstrokes, direction change, histogram of inter-stroke angles etc. are also computed for a character. 

3. Prototype Implementation 
A prototype of the framework has been implemented in Python with the modules discussed above. We are planning to analyze the development of Indic scripts using the framework. The source code will be released under an open source license when the project reaches maturity. Its repository will also include a complete a set of Indian paleographic scripts. Below, we briefly describe functionality yet to be implemented in the prototype.

From the perspective of paleographic scripts, the available glyphs in the literature are usually very noisy. Scanning and importing them would require several layers of pre-processing and noise-removal. For modern scripts, importing the Bezier curves from the respective fonts could be done directly.

Trajectory reconstruction has been implemented only for single stroke characters. A primitive implementation exists for multi-stroke characters. This needs to be made more rigorous and accurate.

Based on the stroke structure of a glyph several additional features such as entropy of writing could be calculated as required. Also, the features are to be used for normalized glyph shapes. The behavior of the features with respect to various scribal variants needs to be analyzed further. 

Proper visualization of the various quantitative features would help to better study and understand the characters within a script and also compare several scripts. Such visualization is particularly helpful in studying paleographic scripts and analyzing the changes that took place over time. Various statistical analyses of the features and visualization techniques would be built into the implementation. 


Fig. 1: Spline Conversion and Trajectory Generation


Fig. 2: Stroke Segmentation & Feature Extraction

4. Similar Projects
There are other digital paleographical projects, such as Hand Analyser by Peter Strokes, which work on pixelated images. The current project is more focussed on using strokes to derive various features. Integration/Adaptation of those techniques needs to be further looked into. 

5. Summary
We have presented a computational  framework for quantitative analysis of scripts. The framework requires minimal user interaction, and is based on the principles of handwriting analysis and handwriting production. We also present a prototype implementation of the proposed framework.  We believe this framework and its implementation would facilitate more quantitative study on scripts.

References
1. Altmann, Gabriel, and Fan Fengxiang (2008), eds. Analyses of script: properties of characters and writing systems. Vol. 63. Walter de Gruyter. APA

2. Changizi, Mark A., et al. (2006) The structures of letters and symbols throughout human history are selected to match those found in objects in natural scenes. The American Naturalist 167.5: E117-E139.

3. Changizi, Mark A., and Shinsuke Shimojo. (2005) Character complexity and redundancy in writing systems over human history. Proceedings of the Royal Society B: Biological Sciences 272.1560: 267-275.

4. Stokes, Peter. (2009) Computer-aided palaeography, present and future. Kodikologie und Paläographie im digitalen Zeitalter - Codicology and Palaeography in the Digital Age. Schriften des Instituts für Dokumentologie und Editorik, 2: 309-338.

5. Doermann, David S., and Azriel Rosenfeld. (1995) Recovery of temporal information from static images of handwriting. International Journal of Computer Vision 15.1-2: 143-164.

6. Jager, Stefan (1996). Recovering writing traces in off-line handwriting recognition: Using a global optimization technique. Pattern Recognition,., Proceedings of the 13th International Conference on. Vol. 3. IEEE, 1996.

7. Li, Xiaolin, Marc Parizeau, and Réjean Plamondon (1998). Segmentation and reconstruction of on-line handwritten scripts. Pattern recognition 31.6: 675-684.

8. Noordzij, Gerrit, and Peter Enneson (2006). The stroke: Theory of writing. Hyphen.

9. Teulings, Hans-Leo, and Lambert RB Schomaker (1993). Invariant properties between stroke features in handwriting. Acta psychologica 82.1: 69-88.

10. Rubine, Dean (1991). Specifying gestures by example. Vol. 25. No. 4. ACM.

11. Long Jr, A. Chris, et al (2000). Visual similarity of pen gestures. Proceedings of the SIGCHI conference on Human factors in computing systems. ACM.

12. Willems, Don, et al (2009). Iconic and multi-stroke gesture recognition. Pattern Recognition 42.12: 3303-3312.

13. Tucha, Oliver, Lara Tucha, and Klaus W. Lange. (2008) Graphonomics, automaticity and handwriting assessment. Literacy 42.3: 145-155.

14. Stenson, Herbert H. (1966) The physical factor structure of random forms and their judged complexity. Perception & Psychophysics 1.9: 303-310.
1949	2014	1. Introduction
Discourse analysis has for the past half century been a staple of the text-based historical and social sciences. Part and parcel of the ‘linguistic turn’ of the 1960s, French discourse analysis was furthermore one of the first disciplines to embrace computational text processing when Michel Pêcheux developed a computer program to identify ideological processes in textual corpora 1. Steeped in contemporary linguistic theory, Pêcheux and his team sought an automated method for uncovering hidden ideological meanings in text corpora. That same year, Michel Foucault’s L’Archéologie du savoir broadened the conception of ‘discourse’ and of the underlying power politics at play in its formation 2. Diverging significantly with Pêcheux, Foucault’s analytical model of ‘archaeology’ brought with it a less strictly-linguistic approach to the discursive. By “loosening the embrace [...] of words and things”, discourses are understood “as practices that systematically form the objects of which they speak” 3.

This expanded notion of discourse would go on to exert a strong influence on French historical studies, and in particular, on the historiography of the French Enlightenment and Revolutionary periods, as is evident in the work of François Furet, Lynn Hunt or Keith Baker 456. More recently, Sophia Rosenfeld and Dan Edelstein have re-introduced the specifically linguistic elements of discourse analysis back into the historian’s and literary scholar’s toolbox, most notably through the analytical use of historical and linguistic databases 78.

With the rapid growth of digital text collections, a revisiting of Pêcheux’s earlier notion of an ‘Automatic Discourse Analysis’ approach would seem warranted, particularly given recent developments in information retrieval such as topic modeling 9. This paper is thus an attempt at reconciling the computational and the discursive, using topic modeling to uncover Enlightenment discourses in the Encyclopédie of Diderot and d'Alembert. Moreover, Foucault’s concept of archaeology is used to justify topic modeling’s ‘bag of words’ analytical model, as it frees us from exclusive interest in language structure, and what that structure conveys, and orients us more towards the association of the various ideas or ‘topics’ that form a discourse.

2. Topic Modeling as Discourse Analysis
Topic modeling is a machine learning approach that was originally designed as a way to classify large amounts of text with minimal human intervention 10. David Newman and Sharon Block have furthermore demonstrated through their use of pLSA (Probabilistic Latent Semantic Analysis) that such unsupervised algorithms can provide a unique overall picture of the contents of a corpus by organizing the data in a manner that gives researchers an objective and wholly original perspective on the texts being analyzed 11. Of course, categorizing texts with no human interaction is not something humanists can accept without question, and this critical point forms the basis of our previous experiments with supervised classification algorithms such as Naive Bayes and Vector Space 1213.

For this project, we employ the Latent Dirichlet Allocation (LDA) algorithm as it is built upon the important premise that documents, however focused, are never about one single topic, but are the result of multiple topics bound together in a single text unit 14. Consequently, the documents analyzed by this algorithm will be identified by a unique signature: a distribution of topics that represents the variety of things discussed in them. As these clusters of words do not necessarily map onto what humanists consider a ‘topic’ or theme, we judge their coherence based less on their thematic consistency and more as the representation of a particular discourse, where closely related concepts are used together in a given context. One could imagine, for instance, finding a discourse that never seems to be the main subject of any document, but that nevertheless runs through a significant number of them.

3. Use Case: Topic Modeling the Encyclopédie
As a use case, we have chosen to examine one the Enlightenment’s exemplary texts, the Encyclopédie of Diderot and d’Alembert 15. Our aim here is to use LDA to go beyond the disciplinary boundaries of the editors’ original classification scheme, which was designed (along with the cross-references) to connect articles amongst themselves across the whole work, but in reality did little to provide guidance to its readers. The physical structure of the text, which caused articles to be read in relative isolation from others, made obtaining a full dialogic perspective of any given class or article unrealistic. By using topic modeling as a discourse analysis tool we aim to highlight each article’s unique discursive makeup. This will allow us to generate a more transversal view of the encyclopaedia and its contents. Whereas David Blei has asked: “What is the likely hidden topical structure that generated my observed documents?” 16; we likewise ask, “what are the non-obvious discourses than span across multiple disciplines in the Encyclopédie?”

From a methodological perspective, we are using the well-known machine learning toolkit MALLET with several Python wrap-arounds 17. Since our goal is to uncover discourses across the entire Encyclopédie, we settled on a relatively low number of topics (between 280 and 360) compared to the total number of classes of knowledge (2,900), but this number was consistent with our previous machine classification experiments 18. Once our topic model was generated, we stored the results in a SQLite table, along with all available metadata. We then wrote a web interface to visualize this database and run queries against the original metadata.

Using the above interface we were able to identify many of the Encyclopédie's disciplinary vocabularies in our topic lists, which were verified using article metadata. Not surprisingly, the ‘chemistry’ topic was found most in chemistry articles, the ‘botany’ topic in botanical articles, mathematics in mathematics, etc. What interests us, however, are topics that are both distinct in nature -- i.e., identifiable with a particular ‘discourse’ -- and that span multiple disciplinary boundaries. Mapping these discourses through the various classes and articles in which they are prevalent leads to a greater understanding of the dialogic and discursive elements at play in the seemingly innocuous encyclopaedic classification system.

The topic we have identified with the discourse ‘droit naturel’, for instance, is present in more than 60 grammar articles, almost double that of its own class (Figure 1).


Fig. 1: Topic #56: "Droit Naturel"

Among the top grammar articles we find the small unsigned article ‘Inviolable’ that has since been attributed to Diderot. In it, alongside the grammatical definition of the term, we find a usage example that reads: “La liberté de conscience est un privilege inviolable” (8:864) -- a reference that subtly places freedom of thought amongst other ‘natural’ and unalienable rights. We find a similar treatment in the article ‘Supplanter’, which contains a thinly-veiled condemnation of tyranny as an unnatural state of governance.

Other classes function in much the same way as Grammar, allowing the philosophes to smuggle controversial opinions into articles of a seemingly neutral scope. By tracing the presence of various discourses in an inter-disciplinary manner we can begin to uncover the various subversive, discursive, and ideological practices in play over the entirety of the Encyclopédie. The discourse around morality, for instance, is found in no less than 94 articles from the ‘géographie ancienne’ class (Figure 2).


Fig. 2: Topic #227: "Morale"

Diderot is here again exemplary in his discursive acrobatics. Whilst describing a tribe of ancient Thracians in the article ‘Dranses’, he quickly turns the discussion towards moral relativism (in a move the prefigures his later work, Le Supplément au voyage de Bougainville), with the assertion that: “Ce n'est pas la nature, c'est la tyrannie qui impose sur la tête des hommes un poids qui les fait gémir & détester leur condition” (5:106).

A similar deployment of the discourse around ‘le culte religieux’ -- a subject on which the encyclopédistes were forced to tread lightly -- can be found in the more than 100 articles labeled as ‘histoire moderne’ (Figure 3).


Fig. 3: Topic #242: "Culte Religieux"

The article ‘Schooubiak’, for instance, is another unsigned (but later attributed) article by Diderot, in which he describes an Islamic sect that practices an unusual form of religious tolerance. This seeming incongruence with the accepted cultural stereotypes of the time allows Diderot to raise the issue of religious intolerance thus using the sect as a proxy for the philosophes themselves, and condemning those who would oppose them: "les prêtres étant les mêmes par-tout, il faut que la tolérance soit détestée par-tout" (14:778). 

As we have seen above, many of these encyclopaedic discourses were deployed subversively in order to move the narrative of Enlightenment forward, and indeed, the discursive nature of the Enlightenment in France has recently been brought to light 19. By extending the reach of our topic modeling approach to other Enlightenment texts we can begin to identify the discursive practices of texts and authors on an even greater scale and with a greater level of systematicity. As a philosophic war machine, as well as a contemporary reference work, the Encyclopédie is an ideal starting point for this sort of work. Many of the discourses we find therein may have been lost to the modern reader through the classification process itself, and still others may prove useful in uncovering interdisciplinary connections that would have otherwise gone unnoticed.

References
1. Michel Pêcheux (1969), Analyse automatique du discours, Paris: Dunod.

2. Michel Foucault (1989), L'Archéologie du savoir, Paris: Gallimard, (1969) [English translation: The Archaeology of Knowledge, London: Routledge].

3. Foucault (1989), p. 54.

4. François Furet (1978), Penser la Révolution française, Paris: Gallimard.

5. Lynn Hunt (1984), Politics, Culture and Class in the French Revolution Berkeley: University of California Press.

6. Keith Michael Baker (1990), Inventing the French Revolution: Essays on French Political Culture in the Eighteenth Century Cambridge: Cambridge University Press.

7. Sophia Rosenfeld (2001), A Revolution in Language: The Problems of Signs in Late Eighteenth-Century France, Stanford: Stanford University Press.

8. Dan Edelstein (2009), The Terror of Natural Right: Republicanism, the Cult of Nature, and the French Revolution Chicago: University of Chicago Press.

9. For a thorough introduction and discussion of topic modeling for humanistic research, see the special issue of the Journal of Digital Humanities edited by Scott Weingart and Elijah Meeks (2012): Journal of Digital Humanities Vol. 2, No. 1 Winter [journalofdigitalhumanities.org/2-1/].

10. David M. Blei, Andrew Y. Ng, and Michael I. Jordan (2003), Latent Dirichlet Allocation, Journal of Machine Learning Research 3 (4–5): 993–1022.

11. David J. Newman and Sharon Block (2006), Probabilistic Topic Decomposition of an Eighteenth-Century American Newspaper, Journal of the American Society for Information Science and Technology 57(6): 753–67.

12. Russell Horton, Robert Morrissey, Mark Olsen, Glenn Roe, and Robert Voyer (2009), Mining Eighteenth Century Ontologies: Machine Learning and Knowledge Classification in the Encyclopédie, in: Digital Humanities Quarterly 3.2 Spring.

13. Charles Cooney, Russell Horton, Mark Olsen, Glenn Roe, and Robert Voyer (2008), Hidden Roads and Twisted Paths: Intertextual Discovery using Clusters, Classiﬁcations, and Similarities, Digital Humanities 2008, eds. Lisa Opas-Hänninen, Mikko Jokelainen, Ikka Juuso, and Tapio Seppänen, Univeristy of Oulu, Finland: 93-4.

14. Blei et al. (2003).

15. All data and references are drawn from the ARTFL digital edition of the Encyclopédie, developed by the University of Chicago's ARTFL Project: [encyclopedie.uchicago.edu].

16. David Blei (2013), Topic Modeling and Digital Humanities, Journal of Digital Humanities 2.1.

17. See [mallet.cs.umass.edu/].

18. Horton et al. (2009).

19. Dan Edelstein (2010), The Enlightenment: A Genealogy Chicago: University of Chicago Press.
2004	2014	1. Introduction
From colonial times to World War Two, most of Australia’s many newspapers incorporated serial fiction, including local and overseas titles. The Australian fiction in these periodicals has largely been identified (Austlit), and important research in this area is ongoing (Bode 2012; Gelder 2011). However, very little is known about the overseas works, including the titles, authors and themes, and their circulation and reception in Australia.

An important reason for this lack of knowledge is the size of the archive. With hundreds of newspapers – many containing multiple instalments of novels per edition – a systematic manual search for fiction is unfeasible. The search possibilities for this archive have dramatically expanded with the creation of the National Library of Australia’s (NLA) Trove database. From 2007 to 2012, the NLA digitised over four million pages of Australian newspapers, from every state and territory, published from 1803 to the 1950s. Combined with digital humanities methods for data mining and analysis, this ongoing digitisation project makes identifying serial fiction in Australian newspapers possible for the first time in a systematic and reliable way.

The project reported in this paper describes a computer-enabled approach to exploring the presence, circulation and reception of fiction in Australian newspapers that enables research, and advances arguments, relevant to bibliographical, book historical and literary studies as well as digital humanities.

2. Bibliography
The project showcases how digital humanities methods can significantly enhance bibliographical records and knowledge. Searching Trove using terms associated with serial fiction – including ‘chapter’, ‘story’ and ‘fiction’ – enables identification of potentially relevant records. The bibliographic information and full text results of these searches are extracted as CSV and text files using a Python harvesting tool developed by Sherratt (2013). These files are supplemented through additional research (for instance, the authors’ nationalities and gender), and transfer to a database that will be freely accessible to researchers and the public.

This approach is providing extremely effective in identifying serial fiction. The first search – of ‘chapter’ – yielded approximately 200,000 individual instances of fiction in Australian newspapers. Many other searches remain to be done; however, even this initial result demonstrates this method’s capacity to enhance bibliographical records. This search process will undoubtedly reveal previously unrecorded instances of publication, particularly of non-Australian fiction. Some of these instances will almost certainly be of titles that have not been indexed previously, including by well-known authors. More broadly, this project demonstrates the potential of digital humanities methods to maximise the utility, and thus enhance the value and consequence, of digital collections.

3. Book History
The collected bibliographic data enables quantitative analysis of the transnational movement of fiction. This approach builds on earlier studies, most prominently, Moretti’s ‘distant reading’ (2005) and, more recently, Jockers’s ‘macroanalysis’ (2013). In terms of the archive searched and the cultural phenomena analysed, it is also indebted to Nicholson’s identification and analysis of American jokes in digitised nineteenth-century British newspapers (2012). Importantly, however, unlike these other works, the body of data underpinning this project’s arguments and findings will be publicly available, so other scholars can explore, check, extend and potentially challenge the findings; and so this data can be reused in future research.

Findings of initial data analysis, for 1830 to 1880, already indicate trends that challenge existing perceptions of Australian literary culture. Where metropolitan newspapers are routinely identified as the main Australian serial fiction publishers (e.g. Webby 2000), this study highlights the strong involvement of regional newspapers. This finding challenges the existing centre/periphery understanding of colonial literary culture. Also contesting this model is the revelation that – while overseas fiction has been estimated to vastly outnumber local titles (Morrison 1998) – in this period, more local than overseas fiction was published. One interesting outcome of this strong local publication is a reversal of the much-discussed female-dominance of nineteenth-century serial fiction authorship. Although most American and British serial fiction was by women (Casey 1996; Coultrap-McQuin 1990; Thompson 1999), men wrote the majority of titles in Australian periodicals in this period. While local titles outnumbered overseas fiction, this initial search has identified a significant amount of non-Australian titles, including a higher-than-anticipated number of American stories, as well as fiction from a wide range of countries besides Britain, including China, Russia, France and Germany. As well as highlighting the status of Australian periodicals as ‘contact zones’ (Pratt 1990) for literature, this range of national literatures further challenges a centre/periphery understanding of colonial literary culture.

This project’s combination of digital humanities and book history suggests important directions for the former as well as the latter field of study. Book history is increasingly recognised as playing an important role in the development of digital humanities. Alan Liu describes book history as a Levi-Straussian ‘trickster figure’ for digital humanities, uniting the field’s commitment to older humanities disciplines, and the value of the old itself, with more recent interest in emergent media and design (2013, 410). Elsewhere he points to the way book historians ‘increasingly compare, and not just contrast, earlier writing/reading practices to their digital successors’ (2012, 16), and the potential of this approach to enhance understanding the digital age and the digital humanities.

The project employs this comparative framework to consider reading practices. While one might assume nineteenth-century newspapers differ entirely from the Internet, in fact both are networked interfaces uniting various content, including that previously published elsewhere, for readers who have significant autonomy in deciding what to read and what connections to draw. Notwithstanding these significant parallels, it is equally important that the use of digitised archives, and digital humanities search and retrieval methods, not occlude historical context. In particular, this project works to maintain a view of nineteenth-century newspapers as coherent and interconnected cultural artefacts rather than containers of discrete content (a perception potentially encouraged by search results in the form of individual articles).

4. Literary Studies
The full-text records extracted from Trove provide the basis for computer-assisted textual analysis, particularly topic modelling. This aspect of the project will follow, and in so doing, test and extend Jockers’s analysis of influence in relation to Irish, English and American literature (2013). Topic modelling will be used to investigate whether, and if so, to what extent, local stories in Australian newspapers employed similar themes, language, or generic strategies to the other-national literatures alongside which they were published. The same method will be used to consider relationships between other-national literary forms. Like Moretti’s and Jockers’s analyses, this project will contribute to shifting literary studies beyond a nation-based framework. However, where these earlier studies consider general bibliographic corpora, in exploring texts published alongside one another, this project provides an important opportunity to consider influence in relation to a specific material context: that is, fiction received and experienced by particular readers at particular times.

5. Digital Humanities
McCarty's notion of modelling is a key concept in this project's formulation and development. In McCarty’s words, a model is ‘an abstraction or simple representation of a more complex real phenomena’ (2008), and modelling enables exploration of and experimentation with phenomena that would otherwise be intractable or inaccessible (2005: 27). This project will complicate and extend this methodological framework by highlighting the multiple number and layers of models and modelling processes involved in exploring serial fiction in Australian periodicals. These layers include the digitised newspaper pages (themselves created from other models, predominantly microfiche), the Trove database more broadly, the database in which the search and harvesting results are represented, as well as the subsequent quantitative analyses of bibliometric and textual data. Where McCarty has always insisted upon the status of models as fictions, this foregrounding of multiple and layered models emphasises the radical contingency of this foundational concept for digital humanities, as well as the theoretical nature of its outcomes.

Foregrounding the contingent and theoretical nature of modelling has two key implications for this project, and for digital humanities research broadly. First, it provides the groundwork for working with an historical record that necessarily contains multiple gaps: Trove has not digitised all Australian newspapers; some records have been lost, others are still to emerge; the quality of OCR for the texts differs radically; and the search process will not discover all serial fiction in Trove. Second, it enables a recognition that even the historical record we have – including what might be considered its obvious facts – needs to be treated as contingent and theoretical. For instance, bibliographic details added to the database – such as the name and gender of authors – are obviously facts, but may not have been present to historical readers (stories were published anonymously or under pseudonyms) and thus cannot be taken as absolute points of reference for understanding the historical circulation and reception of fiction. In moving away from understanding quantitative analysis of archival records as proof of historical phenomena, the underlying framework seeks to forge a conversation between bibliographers, archivists, book historians, literary critics and digital humanists that is data-rich, but oriented towards theoretical possibilities and constructs rather than proof and measures.

References
Austlit: The Australian Literature Resource. (2002–). www.austlit.edu.au. 

Bode, K. (2012). Reading by Numbers: Recalibrating the Literary Field. London: Anthem Press. 

Casey, E. (1996). Edging Women Out? Reviews of Women Novelists in the Athenaeum, 1860-1900. Victorian Studies39.2: 151-71. 

Coultrap-McQuin, S. (1990). Doing Literary Business: American Women Writers in the Nineteenth Century. Chapel Hill: University of North Carolina Press.

Gelder, K. (2011). Negotiating the Colonial Australian Popular Fiction Archive. JASAL Special Issue: Archive Madness: 1-12.  

Jockers, M. (2013). Macroanalysis: Digital Methods and Literary History. Champaign: University of Illinois Press. 

Liu, A. (2012). The State of the Digital Humanities: A Report and a Critique. Arts and Humanities in Higher Education11.1-2: 8-41.

Liu, A. (2013). The Meaning of the Digital Humanities. PMLA128.2: 409-23.

McCarty, W. (2005). Humanities Computing. London: Palgrave Macmillan.

McCarty, W. (2008). Knowing …: Modeling in Literary Studies. In Susan Schreibman and Ray Siemens (eds), Companion to Digital Literary Studies. Oxford: Blackwell. http://www.digitalhumanties.org/companionDLS/.

Moretti, F. (2005). Maps, Graphs, and Trees: Abstract Models for Literary History. London: Verso.

Morrison, E. (1998). Serial Fiction in Australian Colonial Newspapers. In John O. Jordan and Robert L. Patten (eds), Literature in the Marketplace: Nineteenth-Century British Publishing and Reading Practices (2nd ed.). Cambridge: Cambridge University Press, pp. 306-24.

National Library of Australia. (2007–). Trove Database. http://trove.nla.gov.au/.

Nicholson, B. (2012). 'You kick the bucket; we'll do the rest': Jokes and the Culture of Reprinting in the Transatlantic Press. Journal of Victorian Culture17.3: 273-86. 

Pratt, M. L. (1991). Arts of the Contact Zone. Profession: 33-40.

Sherratt, T. (2013). Mining the Treasures of Trove (part 1). Discontents. Blog. http://discontents.com.au/mining-the-treasures-of-trove-part-1/.

Thompson, N. (1999). Responding to the Woman Questions: Rereading Noncanonical Victorian Women Novelists. In Nicola Diane Thompson (ed.), Victorian Women Writers and the Woman Question. Cambridge: Cambridge University Press, pp. 1-23.

Webby, E. (2000). Colonial Writers and Readers. In Elizabeth Webby (ed.), The Cambridge Companion to Australian Literature. Cambridge: Cambridge University Press, pp. 50-73.
2062	2014	TEI is good at what it does: static documents rendered in glorious detail. But TEI is old. Its age doesn’t make TEI irrelevant, but it’s important to be conscious of how the way we weave the fabric of the web has changed since TEI was conceived in 1994, and reevaluate some of our assumptions about its use. In this early work, we are exploring this rethinking as part of a larger study within the center on general methods for isolating the complexity frequently associated with XML-based frameworks.

The Richmond Times Dispatch corpus of TEI-encoded newspapers comprises the Confederate newspaper’s Civil War run, 1860 — 1865. It is compelling both in terms of organization and content and amounts to a comprehensive textual index. In addition to the historical allure of its content, the formal properties of the digitized documents made available through the Perseus Collection make the Dispatch an extraordinary raw material for building a rich interactive visual experience that augments the textual one.

The Dispatch is not in need of a new home; the Perseus Collection hosts a perfectly functional version and uses best practices for data encoding and organization. Rather than strive to be the primary resource for the source material, our project uses the source material to explore recent patterns in web development as well as alternative, more visually compelling ways to interact with XML corpora in a web application. The goal is to produce a powerful reading environment that is tailored to its source material to an extent that the generalized project of the Perseus Collection can afford.

With respect to its implementation, our project fits the genre of a ‘single page application’ (SPA). This project demonstrates best practices for implementing this type of software project using a particular suite of tools; as an open source example of an SPA that is considerably more complex than the usual teaching examples for this kind of thing, we hope that our implementation will be useful to other people who are considering using the same tools, and especially to humanists interested in presenting TEI-encoded documents.

The SPA is du juor. We prefer beautiful URLs and smooth transitions. We are less fond of all these big lists cluttering our sidebars and clunky arrays of checkboxes. We don’t expect websites to always be inert collections of documents. We want to be able to control the connective tissues. The web development community has responded to our current expectations with tools to suite them.

The client-side MVC (Model-View-Controller) libraries that have recently emerged have reached a high level of maturity A client-side MVC library codifies conventional solutions to the generic problems posed by web traffic. It provides semantics for describing the interaction layer between data and presentation. The codification of conventions that MVC libraries manifest is exciting. It deeply simplifies matters for those who want to make interactive documents. Humanists who have a grasp of the language and concepts involved will be that much better able to articulate and realize project architectures that delight the contemporary reader.

For instance, our application is built around a client-side router. The router formalizes protocols for state transitions that allow for timely and efficient request management. We rely heavily on the concept of the run loop, which exposes powerful document management techniques and is tightly linked with a client-driven templating engine. We are able to achieve a remarkably clean separation of concerns in a highly condensed space by exploiting the conventional roles organized and implemented by these libraries. And by shifting our application’s emphasis to the client, we have constant access to a unified programming environment, limiting the context-switching required when developing different parts of the application.

In addition to our project’s strong client-focused application architecture, we also demonstrate a data architecture solution to the problems posed by the corpus’ rich TEI markup. To expose the facets embedded in the source XML, the implementation transforms the deeply nested structure inherent into flat relational representations that can be searched efficiently. Furthermore our project demonstrates a novel, pythonic approach to transforming the source XML to browser-ready HTML that is particularly amenable to the constraints of an SPA.

XSLT wasn’t very well suited to our TEI transformation problem. One of the key UI features of our application was the ability to discover and search for special entities such as people and places in the text. By implementing a custom transformer in python, we had the flexibility to both translate the TEI tag names into valid HTML versions and retain the original TEI tag names and attributes as attributes on the HTML element.

In addition to serving content thus transformed as needed, the role of the server in our application is limited to various precomputation and preprocessing tasks that only need to be run whenever the source material changes--a process that is fully automated with Unix batch processing (via cron) in the cloud. Users never notice. Research projects are often quagmired in a chaotic sprawl of one-off scripts; we demonstrate a coherent architectural pattern for orchestrating these preprocessors.

Sometimes the affordances of an SPA make it worthwhile to depart from the original document’s presentation. Content on the web wants a different kind of exposure than a stack of newspapers. You want to be able to find things quickly. You want to be able to highlight and hyperlink, associate and drill down. Once you’ve computed a graph of your stack of newspapers, now you can move laterally, staying in the same section and moving from date to date, just as easily as you could stay on the same date and move top-to-bottom through the articles. We demonstrate a novel, minimalistic navigation scheme for the Dispatch.

If you’re taking full advantage of a Javascript environment to render your XML content, you can use modern libraries to plug in visualizations with simplicity, and furthermore to turn these visualizations into interactive filters for a very powerful browsing experience. Using a cluster of technologies surrounding Mike Bostock’s work, we demonstrate how to integrate a visualization library into an SPA.

And yet we believe that datafication shouldn’t overwhelm the content. You want to be discrete about placing your controls lest you scare the casual user, but they should be powerful. Live feedback from search inputs has come to be a common expectation for user interfaces and the SPA environment makes it easy to architect that. We show one effective way to make your XML live-searchable.

We close with just a few screenshots of the work in progress. It is important to note that this interface is being further refined based on new work Trevor is doing in his new role as a front-end software developer. 


Fig. 1: An early version of the splash page, presenting user interface controls for the issue date, section, and subsection. Selecting a subsection would reveal the list of headlines it contains. The date selectors reload the issue content asynchronously, without reloading the page.


Fig. 2: The early version of the article reader, presenting the text in a modal context. The summary of facets across the top act as toggles for corresponding highlights in the text. You can page through the section content using the controls at the bottom of the modal.


Fig. 3: This screen capture shows the direction taken in the latest development. The URL bar demonstrates stateful client-side routing. The content selection controls have been flattened into a trio of type-ahead controls with pagination buttons for navigating forward and backwards both in the document structure and across documents.

This stuff is fun. The tools are a joy to use. The free/open source community behind it is excellent and innovative. We want to see more humanists building applications, and moving away from consuming and rather heavyweight content management systems such as Drupal. Based on our experience, humanists can learn the tools and frameworks quickly with excellent results to boot. We hope that our implementation of the Dispatch will set a strong example for our (and others’) future DH projects.

References
We note that we are proponents of using XML, especially for its originally intended purposes of self-describing data interchange, which remains tremendously valuable in developing type safe RESTful web services. 

George K. Thiruvathukal is leading a separate and parallel effort to develop the Standoff Markup text editor, standoffmarkup.org,  which is aimed at simplifying the encoding and maintenance of XML texts (without exposing tree-oriented abstractions). This is where we started exploring the use of SPA when it comes to building DH-facing tools in general.

Richmond Times Collection, www.perseus.tufts.edu/hopper/collection?collection=Perseus:collection:RichTimes.

Single Page Applications original conception, code.google.com/p/trimpath/wiki/SinglePageApplications

Conventionally, a to-do list. See, for instance, todomvc.com.

The Model-View-Controller design pattern (and paradigm) was introduced as part of the Xerox PARC Alto computer, which used the Smalltalk programming language. An excellent historical read about this paradigm can be found at heim.ifi.uio.no/~trygver/themes/mvc/mvc-index.html.

We use Ember.js: emberjs.com. Our technical term for “high level of maturity” is “rock” but we eschew this Americanism for the purpose of a conference paper submission.

We rely throughout on the wonderful lxml library for Python: lxml.de.

Our present implementation is deployed on Heroku, an agile and scalable framework for deploying apps like ours. The overall project is moving to Linode (a dedicated Linux-based cloud hosting provider).

In this we rely on the large-scale application planning features offered by the Flask web framework for python: flask.pocoo.org.

Author of d3js.org among other things.

We have found dc.js (nickqizhu.github.io/dc.js) to be a perfect storm of visualization functionality.

We use the well-known elasticsearch library (www.elasticsearch.org) to achieve an effect like Google’s live search results.
2118	2014	The Workspace for Collaborative Editing is a project funded by the AHRC (UK) and DFG (Germany) between September 2010 and December 2013. It has the goal of creating an online workspace to support the production of the Editio Critica Maiorof the Greek New Testament by teams based in Birmingham, Münster and Wuppertal and collaborators dispersed all over the world.1 The edition has been in progress since the 1990s, but the obsolescence of key tools and encodings have led to this ambitious project to connect all the different stages of the editorial process through online interfaces and shared databases. 

The production of a critical edition involves the identification and selection of manuscripts to be included, the acquisition of images, the creation of full-text electronic transcriptions (which are themselves published as separate electronic editions, linked to the electronic apparatus, enabling further research in related fields), the automatic comparison of these transcriptions to generate a critical apparatus of all variant readings, the editing of this apparatus by scholarly editors to filter out ‘noise’ and prepare the data for analysis using genealogical tools, the addition of evidence from early translations and biblical quotations and the publication of the material in electronic and printed form.

The aim of the Workspace project has been to adopt existing standards and open-source solutions in order to create a lightweight architecture capable of being easily renewed and updated, so that both the data and software created may be reused by other projects. The result is an open-source browser-based environment written in Python and Javascript. The core software consists of a MongoDB database and the asynchronous web application framework MAGPY. Data is stored in JSON and made available via a RESTful interface. On top of this is a layer of applications which call the relevant data objects for the individual editing processes. The goal of transparency at every level of the editing process means that a record is kept of each object at each stage of the process, and any modifications introduced are treated as additional records rather than replacing existing data.2

The Greek New Testament provides a very specific use-case, with a large amount of data already created and highly developed editorial principles. In addition, ongoing work by existing editorial teams offers the opportunity for immediate testing in real-life situations. Developing in these circumstances can be a challenge, with the evolution of guidelines, changes of editorial practice and 'creeping featurism'. The system needed to make existing legacy data compatible with the much more detailed XML encoding developed by the project and cater for as many known and potential scenarios as practicable. The dispersed team of editors was often called upon to codify their procedures and reach a common mind on problems presented by live data, including agreeing changes in policy. As a result, the creation of the Workspace has proceeded hand in hand with the development of different stages of the edition as a whole. 

The two principal areas in which the Workspace meets a pressing need are the development of a transcription editor, which produces and allows the editing of valid XML in a WYSIWYG environment, and a collation editor which enables the scholarly creation of a critical apparatus. Both of these are browser-based, in order to enable dispersed collaborators to work with differing operating systems and contribute directly to the central data store.

The Transcription Editor has been created by team members at the Trier Center for Digital Humanities and released as open-source at the end of the project.3 Its basis is the platform independent TinyMCE package.4 A set of options for mark-up was then developed through a series of menus and shortcuts (cf. Figure 1). The aim is to allow student and volunteer transcribers not familiar with XML to work in an environment which matches as closely as possible the format of the transcriptions already published in the system. The mark-up in the browser uses HTML encoding. An export function converts this into XML matching the specifications developed by the project.5 Likewise, an import function is required in order to support the editing of existing transcriptions. Some of the problems include the encoding and display of paratextual information, normally located in the margins of a manuscript. The dialogue box for entering this information has to have the same functionality as the main transcription interface for recording unclear or supplied text, corrections and so on. The concept has therefore been developed of the “editor-within-an-editor” which makes this possible. A problem with the import of existing transcriptions is the sequence within which elements were nested within the XML. As a result, it has been necessary to establish a system of tag sequences supported by the editor. The standalone nature of the Transcription Editor and its use of an agreed set of TEI encoding means that it can be installed as a plug-in to different environments, including the New Testament Virtual Manuscript Room (NTVMR 2.0)6 as well as the Workspace for the production of the critical edition.


Fig. 1: The Transcription Editor in the NTVMR environment. Based on the selection different menu options for breaks, corrections, deficiency, ornamentation, abbreviations, marginals, notes and punctuation are offered. Mouseovers and different colours help the users to identify different structures.

The Collation Editor provides an interface to the CollateX engine developed by the INTEREDITION project, the successor to the COLLATE program by Peter Robinson.7 This software performs one of the most mechanical and error-prone tasks in an edition, namely the comparison of all witnesses in each variation unit to build up a critical apparatus. Each file is aligned using an algorithm taking into account not just spelling variations, additions, omissions and substitutions, but also transpositions within each block of text. However, the output still requires considerable input from scholars in order to clean up the raw data for publication as a critical apparatus. The first stage is regularisation, the elimination of insignificant variations such as spelling errors. The variant readings are set out underneath a base text, with the witnesses attesting each reading visible in a mouseover box (see Figure 2). An interface built using the redips drag-and-drop library allows editors to drag-and-drop the readings for regularisation onto the correct form.8 For each regularisation, a dialogue box requires users to state the scope of the regularisation and also its nature. Once this is completed, the regularisation is marked in grey and a rule is saved to the database. The ‘recollate’ button sends the data back through CollateX, preferring the regularised token to the original form where present. This means that a different configuration of readings may appear in each column, as the data is cleaned up and a better match is made by the collation algorithm. The second stage involves setting the length of each variant unit, again implemented through a user-friendly drag-and-drop interface for combining or splitting neighbouring columns. One of the dangers with this interface is changing the overall sequence of words in a manuscript by combining different units and repositioning readings. A checking mechanism has therefore been developed which warns the user as soon as the sequence of any manuscript has been disrupted. On some occasions, the data is best displayed as two units of different lengths. By right-clicking on the relevant reading, it can be sent to a line below as an “overlapping variant”, which can then be combined and manipulated like the other columns. One further complication is that an overlapping variant such as a lengthy transposition of words may also contain a reading which should cited in the main sequence. The system therefore makes it possible to duplicate such readings. The final stage is the ordering of variant readings within each unit and assigning the appropriate reading identifier. From here, the apparatus can be output in a number of forms, such as a positive or negative plain text apparatus, an XML encoded apparatus, or a set of values for incorporation into a database for phylogenetic analysis. The information added in the regularisation dialogue box makes it possible to generate automatically the lists of original forms for orthographic variants and erroneous readings which are printed in an Appendix in the Editio Critica Maior.


Fig. 2: The regularisation interface with the dialogue box displayed.

The presentation will briefly demonstrate the Workspace, especially the two interfaces described above. We will discuss some of the problems encountered during its development, along with their solutions. Although the scope of the original project was specifically to support an edition of the Greek New Testament, a pilot project to customise the environment for an edition of Avestan texts will be outlined: from here, we hope that it will be possible to develop the Workspace for use with other textual traditions.

 

 

References
For the history of the ECM project, see Klaus Wachtel, Editing the Greek New Testament on the Threshold of the Twenty-first Century” Literary and Linguistic Computing 15 (2000), 43–50; D.C. Parker and Klaus Wachtel, The Joint IGNTP/INTF Editio Critica Maior of the Gospel of John: its goals and their significance for New Testament scholarship, presented at the Annual Meeting of SNTS, August 2-6, 2005, Halle. epapers.bham.ac.uk/754/.

2. For documentation, see zeth.github.io/magpy/index.html. The source code may be downloaded from github.com/zeth/magpy.

3. sourceforge.net/projects/wfce-ote/

4. www.tinymce.com/

5. H.A.G. Houghton, The Electronic Scriptorium: Markup for New Testament Manuscripts, in Claire Clivaz, Andrew Gregory and David Hamidovic (edd.), Digital Humanities in Biblical, Early Jewish and Early Christian Studies, Leiden: Brill (2013), pp. 31–60; the latest version of the specifications is at epapers.bham.ac.uk/1727.

6. ntvmr.uni-muenster.de/en_GB/transcribing

7. P.M.W. Robinson (1994), Collate: Interactive Collation of Large Textual Traditions, Version 2. Computer Program distributed by the Oxford University Centre for Humanities Computing, Oxford. The history of Collate is described by Robinson in a 2007 blog post at: www.sd-editions.com/blog/?p=15. For CollateX, see www.interedition.eu/. The source code is available from collatex.net/.

8. www.redips.net/javascript/drag-and-drop-table-content/
2122	2014	How have text analysis tools in the humanities been imagined in the past? What did humanities computing developers think they were addressing with now dated technologies like punch cards, printed concordances and verbose command languages? Whether the analytic functionality is at the surface, as with Voyant Tools, or embedded at deeper levels, as with the Lucene-powered searching and browsing capabilities of the Old Bailey, the web-based text analysis tools that we use today are very different from the first tentative technologies developed by computing humanists. Following Siegfried Zieliniski's exploration of forgotten media technologies, this paper will look at three forgotten text analysis technologies and how they were introduced by their developers at the time. Specifically we will:

Discuss why is it important to recover forgotten tools and the discourse around these instruments,
Look at how punch cards were used in Roberto Busa’s Index Thomisticus project as a way of understanding data entry,
Look at Glickman’s ideas about custom card output from PRORA, as a way of recovering the importance of output,
Discuss the command language developed by John Smith for interacting with ARRAS, and
Conclude with a more general call for digital humanities archaeology. 
Zieliniski and Media Archaeology
Siegfried Zielinski, in Deep Time of the Media, argues that technology does not evolve smoothly and that we therefore need to look at periods of intense development and then look at the dead ends that get overlooked to understand the history of media technology. In particular he shows how important it is to look at technologies that are not in canonical histories as precursors to “successful” technologies, because they provide insight into the thinking at the time. A study of forgotten technologies can help us understand opportunities and challenges as they were perceived at the time and on their own terms rather than imposing our prejudices. From the 1950s until the early 1990s there was just such a period of technology development around mainframe and personal computer text analysis tools. The tools developed, the challenges they addressed, and the debates around these technologies have largely been forgotten in an age of web-mediated digital humanities. For this reason we recover three important mainframe projects that can help us understand how differently data entry, output and interaction were thought through before born- digital content, output to wall-sized screens, and interaction on a touchscreen. 

Busa and Tasman on Literary Data Processing 
The first case study we will present is about the methods that Father Busa and his collaborator Paul Tasman developed for the Index Thomisticus (Busa could hardly be considered a forgotten figure, but he's often referred to metonymically as a founder of the field, with relatively little attention paid to the specifics of his work and his collaborations). Busa, when reflecting back on the project justified his technical approach as supporting a philological method of research aimed at recapturing the way a past author used words, much as we want to recapture past development. He argued in 1980 that, “The reader should not simply attach to the words he reads the significance they have in his mind, but should try to find out what significance they had in the writer’s mind.” (Busa 1980, p. 83) Concordances could help redirect readers towards the “verbal system of an author” or how the author used words in their time and away from the temptation to interpret the text at hand using contemporary conceptual categories. Concording creates a new text that shows the verbal system, not the doctrine.

Busa’s collaborator Paul Tasman, however, presents a much more prosaic picture of their methodology that focuses on data entry using punch cards so you can actually get concordances of words. He published a paper in 1957 on “Literary Data Processing” in the IBM Journal of Research and Development that focuses on how they prepared their texts accounting for human error and other problems. Tasman writes, “It is evident, of course, that the transcription of the documents in these other fields necessitates special sets of ground rules and codes in order to provide for information retrieval, and the results will depend entirely upon the degree and refinement of coding and the variety of cross referencing desired.” (p. 256) This case study takes us back to a forgotten set of problems (representing text using punch cards) which led to more mature issues in text encoding. In the full presentation we will look closely at the data entry challenges faced by Busa’s team and how they were resolved with the card technology of the time. 

Glickman and Stallman on Printed Interfaces
The second case study we will look at is the development of the PRORA programs at the University of Toronto in the 1960s. PRORA was reviewed in the first issue of CHUM and with the publication of the Manual for the Printing of Literary Texts and Concordances by Computer by the University of Toronto Press in 1966 is one of the first academic analytical tools to be formally published in some fashion. What is particularly interesting, for our purposes, is the discussion in the Manual of how concordances might be printed. Glickman had idiosyncratic ideas about how concordances could be printed as cards for 2-ring binders so that they could be taken out and arranged on a table by users. He was combining binder technology with computing to reimagine the concordance text. Today we no longer think about output to paper as important to tools, and yet that is what the early tools were designed to do as they were not interactive. We will use this case study to recover what at the time was one of the most important features of a concording tool – how it could output something that could be published for others to use. 


Fig. 1: Example of PRORA output from the Manual

Smith and Interaction
One of the first text analysis tools designed to support interactive research was John Smith’s ARRAS. In ARRAS Smith developed a number of ideas about analysis that we now take for granted. ARRAS was interactive in the sense that it was not a batch program that you ran for output. It could generate visualizations and it was explicitly designed to be part of a multi-tasking research environment where you might be switching back and forth between analysis and word processing. Many of these ideas influenced the interactive PC concordancing tools that followed like TACT. In this paper, however, we are not going to focus on all the prescient features of ARRAS, but look at the now rather dated command language which Smith was so proud of. Almost no one uses a command language for text analysis any more; we expect our tools to have graphical user interfaces that provide affordances for direct manipulation. If you need to do something more than what Voyant, Tableau, Lucene, Gephi or Weka let you do, then you learn to program in a language like R or Python. John Smith by contrast, spent a lot of time trying to design a natural command language for ARRAS that humanists would find easy to use and this comes through in his publications on the tool (1984 & 1985). Command languages were, for a while, the way you interacted with such systems and attention to their design could make a difference. Smith tried to develop a command language that was conversational so humanists could learn to use it to explore “vast continents of literature or history or other realms of information, much as our ancestors explored new lands.” (Smith 1984, p. 31) Close commanding for distant reading. 

Conclusions
In the 2013 Busa Award lecture Willard McCarty called us to look to our history and specifically to look at the “incunabular” years before the web when humanists and artists were imagining what could be done. One challenge we face in reanimating this history is that so much of the story is in tools, standards and web sites – instruments difficult to interrogate the way we do texts. This paper looks back at one major thread of development - text analysis tools – not for the entertainment of outdated technology, but recover a way of thinking about technology. We will conclude by discussing other ways back including the need for better documentation about past tools, along the lines of what TAPoR 2.0 is supporting, and the need to preserve tools or at least a record of their usage. 

References
Busa, R. (1980). "The Annals of Humanities Computing: The Index Thomisticus." Computers and the Humanities. 14(2): 83-90.

Glickman, Robert Jay, and Gerrit Joseph Staalman. Manual for the Printing of Literary Texts and Concordances by Computer. Toronto: University of Toronto Press, 1966.

Liu, Alan. (2012) “Where is Cultural Criticism in the Digital Humanities.” In Debates in the Digital Humanities. Ed. Matthew K. Gold. University of Minnesota Press. Liu’s essay is online at <http://dhdebates.gc.cuny.edu/debates/part/11>.

Smith, J. B. (1978). "Computer Criticism." STYLE XII(4): 326-356.

Smith, J. B. (1984). "A New Environment For Literary Analysis." Perspectives in Computing 4(2/3): 20-31.

Smith, J. B. (1985). Arras User's Manual: TR85-036. Chapel Hill, NC, The University of North Carolina at Chapel Hill.

Tasman, P. (1957). "Literary Data Processing." IBM Journal of Research and Development 1(3): 249-256.

Zieliniski, Siegfried. (2008) Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. Cambridge, Massachusetts: The MIT Press.
2228	2015	

    
        
            
                Digital Network Analysis of Dramatic Texts
                
                    
                        Trilcke
                        Peer
                    
                    University of Göttingen, Germany
                    trilcke@phil.uni-goettingen.de
                
                
                    
                        Fischer
                        Frank
                    
                    Göttingen Centre for Digital Humanities, Germany
                    frank.fischer@zentr.uni-goettingen.de
                
                
                    
                        Kampkaspar
                        Dario
                    
                    Herzog August Library Wolfenbüttel, Germany
                    kampkaspar@hab.de
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    network analysis
                    dramatic texts
                    literary history
                
                
                    literary studies
                    visualisation
                    networks
                    relationships
                    graphs
                    English
                
            
        
    
    
        
            The project ‘Digital Network Analysis of Dramatic Texts’ follows in the tradition of structural analysis approaches in the literary studies (Titzmann, 1977) and is aimed at advancing them in recourse to established methods like Social Network Analysis (Wasserman and Faust, 1998). The project also enhances these elder approaches through automated data acquisition and analysis in order to handle much larger corpora and to be able to generate comprehensive relational data to analyse structural transformations in the history of literature.
            As theoretical foundation we used a network-analytic conceptualisation of dramatic interaction (first ideas in Moretti, 2011; critical reading and theoretical reconceptualisation in Trilcke, 2013; also a detailed research summary). In continuation of concepts of dramatic configuration (Marcus, 1973; Pfister, 1977; Pohlheim, 1997), we resorted to a basic operationalisation according to which an ‘interaction’ takes place if two characters are listed as speakers within a given segment of a text (usually a ‘scene’).
            For our first automation purposes, we considered ‘interaction’ as ‘scenic co-presence of two speakers’. Based on this concept of relations, network data is extracted automatically—both the global networks of ‘interactions’ of the plays (density, average degree, connectedness, etc.) and data that characterises individual actors (degree and various other centrality indices). The workflow we created also allows the collection of data on a meso-level (e.g., identification of clusters) and includes visualisations of the extracted network data, which in turn contributes to the analysis of the structural transformation in the history of literature.
            Corpora of Dramatic Texts
            For the automated analysis of theatre plays, a reliable and sufficiently large (German-language) corpus was needed. The following corpora were reviewed:
             • Deutsches Textarchiv / German Text Archive (DTA): 54 dramatic texts.
             • Wikisource: 50 dramatic texts.
             • Projekt Gutenberg-DE: 641 dramatic texts.
             • TextGrid Repository: 690 dramatic texts.
            The DTA corpus has the best quality of TEI markup, but so far only incorporates few texts. The latter also applies to the German-language branch of Wikisource. The Gutenberg-DE archive is problematic due to the poor markup it provides (just some basic XHTML). Thus, only the TextGrid Repository (containing basic TEI markup) was really applicable.
            First, we extracted all dramatic texts from the repository, 690 texts altogether that are marked as ‘drama’ in the ‘genre’-field of the metadata. Most of them are German plays from about 1500 to 1930, plus a dozen translations of Greek tragedies and some Shakespeare plays.
            Acquisition of Network Data
            As an intermediate step, we created a list of relations between all the persons appearing in a play for each of the 690 TEI files and wrote them into a CSV file, one of the standard formats for the storage of network data. To extract the speaker data, usually two separate steps are required: The recognition of the individual segments of a play and the recognition of individual speakers.
            To facilitate the following work, the script first splits the files. For each level recognised in the document tree, a subdirectory is created containing all the individual parts of the TEI files, along with the respective index files. Different kinds of outputs are generated this way: for one, a detailed register of all &lt;speaker&gt; tags, but also all the &lt;rs&gt; and &lt;person&gt; tags. To obtain unambiguous reference targets, ID numbers are assigned (also facilitating later interventions, if erroneously assigned names must be manually corrected). Furthermore, co-occurrence lists are created. In the bottom directory, the occurrence of all speaker pairs in all files are counted. In the upper directories, the values of all subdirectories are added.
            In addition to the recognition of the structure, the correct assignment of names is one of the bigger challenges. Ideally, all &lt;speaker&gt; tags would contain a @who attribute to provide a unique ID for each character. If this is not the case (or if &lt;rs&gt; or &lt;person&gt; tags were used instead), the script has to analyse the textual content of the tag. Possible misspellings (due to the transcription or the original) and grammatical changes have to be considered. For instance, in Lessing, 
                Nathan the Wise V/1, we encounter different sorts of Mamalukes, be it ‘A Mamaluke’, ‘The Mamaluke’, ‘A second Mamaluke’, or just ‘Second Mamaluke’. In this case, some linguistic knowledge about adjectives is enough to rule out mistakes when assembling the lists of relations. But there are more complicated cases when automatising this process, e.g., if multiple characters speak up at once (‘All’).
            
            In addition to trying to clarify these cases automatically, there is still the possibility of manual intervention in cases of doubt. Here, the generated index files with unique IDs will be of help. For an upcoming version of the script, it is intended to provide a simple GUI to allow easy editing in such cases.
            Data Analysis and Visualisation
            The data analysis was done by using the igraph package via Python (3.4.x). It was used for both the visualisation of the graphs and the calculation of the network data.
            For a first visualization, we fed the graph data into a spring-embedding method (Fruchterman-Reingold), which tries to arrange related nodes closer together (clustering). A first impression of the entire corpus is provided in Figure 1. It comprises nearly 700 plays from 2,500 years of theatre history, starting chronologically at the top left with the ancient Greeks ranging to the bottom right and the second quarter of the 20th century.
            
                
            
            The visualised graphs (a ‘distant reading’ of its own) also suggested that most of the calculated CSV had at least minor flaws in them due to ambiguous markup. These findings contributed to the error handling in the previous step (Acquisition of Network Data).
            Some basic network calculations were done on the basis of the 12 (completed) theatre plays by Gotthold Ephraim Lessing. Corresponding diagrams are show in Figure 2.
            
                
            
            Conclusions
            The extracted (and adjusted) network data will serve as a basis for further statistical calculations and also be made publicly available. Our research focus will now shift to implementing further calculation tools for the network analysis of theatre plays (e.g., to calculate the betweenness centrality to determine the importance of individual characters in a network). We will also work on enhancing the network data (quantify speech units, include stage presence of non-speaking persons, etc.) and try to build multiplex networks that not only capture the ‘interactions’, but also consider parental or instrumental relations.
        
        
            
                
                    Bibliography
                    
                        Marcus, S. (1973). 
                        Mathematische Poetik. Athenäum-Verlag, Frankfurt.
                    
                    
                        Moretti, F. (2011). Network Theory, Plot Analysis. Stanford Literary Lab Pamphlets 2, http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf.
                    
                    
                        Pfister, M. (1977). 
                        Das Drama. Theorie und Analyse. Fink, Munich.
                    
                    
                        Pohlheim, K. K. (ed.). (1997). 
                        Die dramatische Konfiguration. Schöningh, Paderborn.
                    
                    
                        Titzmann, M. (1977). 
                        Strukturale Textanalyse. Theorie und Praxis der Interpretation. Fink, Munich.
                    
                    
                        Trilcke, P. (2013). Social Network Analysis (SNA) als Methode einer textempirischen Literaturwissenschaft. In Ajouri, P., Mellmann, K. and Rauen, C. (eds), 
                        Empirie in der Literaturwissenschaft. Münster: Mentis, pp. 201–47.
                    
                    
                        Wasserman, S. and Faust, K. (1998). 
                        Social Network Analysis: Methods and Applications. Cambridge University Press, Cambridge.
                    
                
            
        
    

2240	2015	

    
        
            
                Electronic Literature and the Politics of Process
                
                    
                        O'Sullivan
                        James Christopher
                    
                    Pennsylvania State University
                    josullivan.c@gmail.com
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    Electronic Literature
                    E-lit
                
                
                    literary studies
                    media studies
                    English
                
            
        
    
    
        
            Critics have long examined the ‘multimodal capacity of electronic literature’ (Page and Thomas, 2011, 2). The material semantics of electronic literature are not inherently new, but rather a rejuvenation of established literary practices exemplified by the material modernists and other such movements to have sought control over the paratextuality of a literary work. Yet new media does deliver rejuvenation through enhanced paratextual potential. Materiality, and in turn paratextuality, is concerned with experience: it governs the reception of a work and the ways in which its audience might interact with it. In this paper I will engage with works of electronic literature in an attempt to delineate how some of the field’s most prominent authors and practitioners have used the manipulability of digital paratext for artistic purposes. 
            Electronic Literature as Avant-Garde
            When an author selects a digital medium, there are ideological considerations that cannot be ignored—they are making a statement about the aesthetic they desire and what that aesthetic represents. In this respect, we see how it is that electronic literature is very much connected to concepts of the avant-garde, so much so that its works are often criticised as being overtly experimental. Responding to Andrew Gallix’s challenge that electronic literature sacrifices literary quality, Grigar accepts that ‘the hybridity of the forms and technological innovation that artists bring to their work result in a high level of experimentation that may at first obfuscate literary content’ (n.d.), but she is quick to point out that obfuscation should not deny the digital’s claim to the mantle of literary. If electronic literature serves as little more than a jolt in literature’s long history, then surely it has achieved the very thing that literature sets out to achieve? Perhaps electronic literature is more valuable ideologically than it is semantically or aesthetically, but having such political value is in itself a very literary trait. Grigar raises another interesting point in that same article, referring to electronic literature’s brief moment within the spotlight of the press, back when it still held its ‘shock of the new’. She offers this as a rebuttal to the claim that electronic literature, once popularised within the media, has since been relegated to a tertiary note on specialised blogs. On the contrary, electronic literature may no longer be a media darling, but it has found itself a place within the academy, and within the focus of respected authors and critics from across the digital humanities. 
            In this sense, electronic literature’s ability to disrupt the status quo has faded, but it has traded the ease by which it can shock for an ability to achieve problematisation through defamiliarisation—so much so, that electronic literature has become self-reflective to the point where it is now querying its most essential of properties: being born digital. 
            
                
            
            Figure 1. 
                Closed Room, Soft Whispers, by Jacob Garbe. From Pathfinders Exhibit, MLA 2014 convention, Chicago, January 2014.
            
            The Pathfinders Exhibit at the 2014 MLA convention presented a selection of early electronic literature, alongside some more recent pieces. Many of the works on show displayed a new iteration in the juxtaposition between literature and technology, in that digital and physical materiality had been fused beyond hardware and software. What Strickland achieves with her hybrid 
                V: WaveSon.nets/Losing L’una, for example, has been taken to its conclusion: electronic literature is moving, diachronically, at an immense pace, its authors beginning to deconstruct the very boundaries that they themselves put up. 
            
            When code is open access, the reader can pierce the veil in a manner rarely seen before. Readers, and indeed authors, have insights into the worlds of their precursors and contemporaries. Authors in this field began by swerving away from the page, but now they are, while perhaps not swerving away from the digital, certainly seeking its convergence with other, seemingly disparate, materialities. It is re-creation of the re-created, a tension that has been consciously instigated in an effort to achieve further defamiliarisation of that which is already defamiliar. This is precisely what Pressman labels as ‘digital modernism’, a movement that, she argues, interrogates ‘cultural infrastructures, technological networks, and critical practices’ (2014, 10), and in doing so, ‘offers a surprising counterstance to this privileging of newness’ (1). 
            The Politics of the Screen
            Within the electronic literature community, there is a marked reaction against the intentions of software’s function. Authors in this realm practice perverse engineering, where a tool is not modified for literary purposes but rather adapted, at a surface level, so that it satisfies some authorial desire. Flash, for example, was never intended for literary purposes, but it is at the very heart of the first two volumes within the 
                ELO Collection. The exploitation of computer systems is fundamental to contemporary electronic literature in that the majority of works are produced using technologies that were designed for some other purpose. This act, the manipulation of hardware and software for literary purposes, is a fundamental aspect of the movements that promote and produce much of this work. Reducing the process of writing electronic literature to a purpose-built intuitive graphical user interface would be akin to creating a program that structured the metre for nondigital poetry. In this sense, electronic literature is the product of exploited technologies; it is expression through manipulation. This presents an interesting reversal of Adorno and Horkheimer’s view, with technology becoming an instrument in the problematisation of those structures that they argue it enforces. Ideological arguments cannot always account for an author’s technological selections, but there are many instances where both the form and tools adopted for a work of electronic literature are inherently ideological, if not overtly political.
            
            In exploring the influence of digital materiality and the politics of e-lit processes and practices, I will look at 
                Flight Paths (see Figure 2), by Kate Pullinger and Chris Joseph, as well as Mark Marino’s ‘a show of hands’, a piece inspired by the 2006 immigration reform in Los Angeles.
            
            
                
            
            Figure 2. ‘Previous Contributions’, from 
                Flight Paths, by Kate Pullinger and Chris Joseph. 
                Electronic Literature Collection, Vol. 2. Electronic Literature Organization, February 2011, collection.elitarture.org.
            
            Conclusions
            While we cannot separate ideology from any literature, we can see that electronic forms are married to the politics of process. Each of these works has meaning in its materiality, considerably more than would typically be the case, even amongst the foremost of the material modernists. While Yeats was able to embed the esoteric in his bindings, the screen permits semantic permeations throughout a work as a whole. Even concrete poets, with typographic arrangements, are restricted by the surface of the page. I do not want to represent the page as a media with particularly negative limitations: all forms have limitations that can at times be just as readily considered strengths. Concrete poetry, like the cover of a Yeatsian edition, is manifested as a permanent entity. There is no such permanence in the electronic environment, where works of art, when reduced to substance, are merely the graphically rendered representations of bits and bytes. This impermanence offers something authors that is not found in other forms, and as has been seen, it allows them a freedom of ideological expression that is very much bound to materiality. Process is also essential. Traditionally, authors collaborate with publishers to see their literature produced in book form. I think it fair to assume that the majority of writers do not actually possess a working knowledge of how it is that manuscripts are transformed into physical books. That is not to say that nondigital writing practices are without process, but that process plays a more significant role in the digital arena, where authors must be familiar with processes of production if they are to write. Even where collaboration exists, an understanding of how technology can—and cannot—manipulate the linguistic content of a piece must be possessed. It is not simply about words on the surface but rather about the surface and the underlying code that dictates part of the reader experience. Garbe did not just write 
                Closed Room, Soft Whispers, he 
                made it: he composed the surface-level text, he wrote the underlying code, and he built the wooden box. Marino captured his own images; he worked directly with the platform that presents his narrative. In this respect, the politics of the screen are, unlike the page, materially layered, with authors being as much makers as they are writers.
            
            Throughout this paper, I will use these works of electronic literature to delineate how contemporary juxtapositions of traditional and modern literary practices have presented authors with hyper-paratextuality, and their engagement with such is significant. It is easier for an author to put down words on a page than it is, for example, to learn Python and produce a generative poem. Thus, it is always significant when an author chooses digital encapsulation for expression. This act, the selection of an electronic rather than nondigital medium, is an authorial statement from which immediate conclusions can be drawn. The significance of the influence exerted by digital paratextuality may be questioned on the basis that much of that which makes it digital sits behind the interface, but the recognition of digitality is in itself influential. Authors do not complicate the process of writing with additional technical requirements on a whim. 
        
        
            
                
                    Bibliography
                    
                        Grigar, D. (n.d.). Electronic Literature: Where Is It? 
                        Electronic Book Review.
                    
                    
                        Page, R. and Thomas, B. (2011). 
                        New Narratives: Stories and Storytelling in the Digital Age. University of Nebraska Press, Lincoln.
                    
                    
                        Pressman, J. (2014). 
                        Digital Modernism: Making It New in New Media. Oxford University Press, New York.
                    
                    
                        Pullinger, K. and Joseph, C. (n.d.). Flight Paths. 
                        Electronic Literature Collection,
                        2.
                    
                
            
        
    

2275	2015	

    
        
            
                Improving Burrows’ Delta – An empirical evaluation of text distance measures
                
                    
                        Jannidis
                        Fotis
                    
                    Universität Würzburg, Germany
                    fotis.jannidis@uni-wuerzburg.de
                
                
                    
                        Pielström
                        Steffen
                    
                    Universität Würzburg, Germany
                    pielstroem@biozentrum.uni-wuerzburg.de
                
                
                    
                        Schöch
                        Christof
                    
                    Universität Würzburg, Germany
                    christof.schoech@uni-wuerzburg.de
                
                
                    
                        Vitt
                        Thorsten
                    
                    Universität Würzburg, Germany
                    thorsten.vitt@uni-wuerzburg.de
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    Burrows' Delta
                    Argamon
                    stylo
                
                
                    stylistics and stylometry
                    authorship attribution / authority
                    English
                
            
        
    
    
        
            Since John Burrows first proposed Delta as a new stylometric measure (Burrows, 2002), it has become one of the most robust distance measures for authorship attribution (Juola, 2006; Stamatatos, 2009; Koppel et al., 2009). It has been shown to render very useful results in different text genres (Hoover, 2004a) and languages (Eder and Rybicki, 2013). Nowadays, Delta is widely used not the least because there is the free 
                stylo package in R (Eder et al., 2013). There have been several proposals to improve Delta (Hoover, 2004b; Argamon, 2008; Eder et al., 2013; Smith and Aldridge, 2011; Kestemont and Rybicki, 2013). In the following, we report on a series of experiments to test these proposals using collections of novels in three languages. Our results will show that one of Hoover’s and one of Argamon’s measures show good results, but are outperformed in general by Burrows’ Delta and by Eder’s Delta. Most notably, the modification of Delta proposed by Smith and Aldridge shows a remarkable improvement of the results in all languages and has the advantage of providing a stable increase of performance up to a specific point, unlike the other measures, which are very sensitive to the amount of most frequent words (mfw) used. These results also allow discussion of some of the theoretical assumptions for the success of these measures, even if we are still far away from providing a ‘compelling theoretical justification’ (Hoover, 2005) for their success.
            
            Material and Methods
            Text Collections
            We built three collections of novels (English, French, German), each consisting of 75 texts: 25 authors, each represented with three novels. The collection of British novels contains texts published between 1838 and 1921 coming from Project Gutenberg (www.gutenberg.org). The collection of French novels contains texts published between 1827 and 1934 originating mainly from Ebooks libres et gratuits (www.ebooksgratuits.com). The collection of German novels consists of texts from the 19th and the first half of the 20th centuries, which come from the TextGrid collection (http://www.textgrid.de/Digitale-Bibliothek).
            Text Distance Measures
            Argamon (2008) proposed three variants of Delta, each one improving on one aspect of Burrows’ Delta. 
            He argues that Burrows’ Delta inherently assumes that the distribution of a particular word across multiple texts follows the multivariate Laplace distribution, but it uses the standard deviation as a normalization factor, which only makes sense for a Gaussian distribution. Quadratic Delta assumes a multivariate Gaussian distribution of the words. Linear Delta, on the other hand, adjusts the normalization to take into account the calculation of spread for a Laplace distribution. Like Burrows’ Delta, both variants are based on the (implausible) assumption that the frequencies of the words are independent. The third variant, Rotated Delta, rotates the frequency differences into a space where they are maximally independent. It also assumes a Gaussian distribution. An analysis of the word frequency distribution in our English test set shows indeed that the normal distribution represents the data much better than the Laplace distribution (Figure 1a). The same is true for German high-frequency words (Figure 1b). We should therefore expect Rotated Delta to perform best, and Quadratic Delta to yield better results than Linear Delta or Burrows’ Delta. Eder’s Delta slightly increases the weights of frequent words and is meant to perform better with highly inflected languages. It should perform better for German and French than for English. Smith and Aldridge replace the Manhattan distance used by Burrows with cosine based on findings in text mining that the latter shows greater reliability with large word vectors (Smith and Aldridge, 2011). Their experiments show an impressive improvement for English texts.
            We implemented these measures in Python, using the output of 
                stylo, where a parallel implementation existed, as a benchmark (see the appendix for the formulas).
            
            Evaluating Distance Measures
            In order to provide useful performance indicators for these measures for authorship attribution, we concentrated on the question of how well a particular distance measure allows distinguishing between a situation where two compared texts are written by the same author from a situation where two texts are from different authors. Besides relying on the Adjusted Rand Index as a well-known but rather abstract measure for clustering quality (Everitt et al., 2011, 264f.), we also established a simple algorithm to count clustering errors representing the researcher’s intuition of correct clustering. In order to obtain another more subtle performance indicator independent of any clustering algorithm, we grouped the calculated distance scores into two sets: ingroup comparisons containing distances between texts actually written by the same author, and outgroup comparisons. The larger the difference between the ingroup distances and the outgroup distances, the better a distance measure is assumed to perform. After evaluating several potential performance indicators (for example, t-values, or using proportions of distribution overlap) in terms of how well they correlate with the number of clustering errors, we settled on the simple difference of z-transformed means because it showed the best correlation with the clustering error measure.
            Results and Discussion
            Most interestingly, we could not only confirm the findings of Smith and Aldridge on our English texts, but also show that Cosine Delta outperforms all other measures on our three collections (Figures 2–4). Equally important, it proves to be more stable with increasing mfw size. While Burrows’ and Eder’s Delta usually show a peak around 1,000–1,500 mfw, and then behave a bit erratically on longer word vectors, Cosine Delta reaches a plateau at 2,000 and stays there (Figure 5). As Smith and Aldridge argue, based on their very different data, that using word vectors longer than 500 words doesn’t improve the performance, we assume that this number is a function of the corpus size.
            Our empirical tests did not substantiate Argamon’s theoretical arguments. Eder’s variant didn’t show consistently better results with more highly inflected languages like German and French and performed similarly to Burrows’ Delta. Both Quadratic Delta and Rotated Delta perform much worse than should be expected on theoretical grounds. And Linear Delta, although being among the top-five distance measures, seems to be an improvement over Burrows’ Delta only under special circumstances. Argamon’s modifications were based on correct assumptions about the kind of distributions Delta is working on, but nevertheless those algorithms did not perform better, something that points to the operation of factors not yet understood. The fact that those algorithms consistently perform differently in different languages and that these differences cannot, or at least only partially, be explained by the degree of inflection (Eder and Rybicki, 2013), adds to this enigma at the moment. There is almost no other algorithm in stylometry we know as much about as Delta, and yet there is still no theoretical framework that can explain its success.
            Future Work
            We tried to assemble corpora similar in genre, time, etc., but there is the real possibility that the variation we attribute to the languages is really only one between the specifics of the corpora. So it is important to test the robustness of our findings using different corpora. Another line of future investigations is the testing of more variations of Delta, using Cosine Delta as a starting point. Also, a systematic study of how the length of the mfw list determines Cosine Delta’s performance in relation to the size of the texts and the corpora could allow the automatic choice of the best parameters. And last but not least we have to analyze the connection between the performances of some variants of Delta and specifics of different languages in order to gain a deeper theoretical insight into the working of Delta in general.
            _____
            The python script implementing these distance measures can be found at 
            https://github.com/fotis007/pydelta.
            
                
            
            Figure 1. Empirical distribution of word frequencies compared to idealised Gauss and Laplace distributions. Indicated for the three most frequent words in the English (a) and German (b) dataset.
            
                
            
            Figure 2. Performance of distance measures on English texts. Indicated in terms of both the difference between z-transformed means of ingroup (same author) and outgroup distances, as Adjusted Rand Index (higher values indicate better differentiation), and in terms of clustering errors (lower values indicate better differentiation). Distance measures are sorted according to their maximum performance in all test conditions.
            
                
            
            Figure 3. Performance of distance measures on French texts. For a detailed explanation, see Figure 2. 
            
                
            
            Figure 4. Performance of distance measures on German texts. For a detailed explanation, see Figure 2. 
            
                
            
            Figure 5. Difference between z-transformed means of ingroup and outgroup distances as a function of word list length. Indicated for selected delta measures on the German text collection.
            Appendix: Text Distance Measures
            Most delta measures normalize features first and then apply a basic distance function.
            
                
                    Distance Measure
                    Basic Distance Function 
                    Feature Normalisation
                    
                
                
                    Burrows’ Delta
                    Manhattan Distance
                    z-score
                
                
                    Argamon’s Linear Delta
                    Manhattan Distance
                    diversity
                
                
                    Eder's Delta
                    Manhattan Distance
                    z-score · Eder's Ranking Factor: 
                        
                            
                                
                                    
                                        n
                                        -
                                        
                                            
                                                n
                                            
                                            
                                                i
                                            
                                        
                                        +
                                        1
                                    
                                    
                                        n
                                    
                                
                            
                        
                    
                
                
                    Eder’s Simple Delta
                    Manhattan Distance
                    square root
                
                
                    Manhattan Distance
                    Manhattan Distance
                    –
                
                
                    Argamon’s Quadratic Delta
                    Euclidean Distance
                    z-score
                
                
                    Argamon’s Rotated Delta
                    Euclidean Distance
                    Stripped-down eigenvectors of covariance matrix
                
                
                    Euclidean Distance
                    Euclidean Distance
                    –
                
                
                    Cosine Delta
                    Cosine Distance
                    z-score
                
                
                    Cosine Distance
                    Cosine Distance
                    –
                
                
                    Correlation Distance
                    Cosine Distance
                    center
                
                
                    Hoover's Delta-P1
                    (own measure)
                    z-score
                
                
                    Canberra distance
                    Canberra Distance
                    –
                
                
                    Bray-Curtis distance
                    Bray-Curtis Distance
                    –
                
                
                    Chebishev Distance
                    max abs. distance
                    –
                
                
                    Basic Distance Function
                    Definition
                
                
                    Manhattan Distance
                    
                        
                            
                                
                                    ∑
                                    
                                        i
                                        =
                                        1
                                    
                                    
                                        n
                                    
                                
                                
                                    ∣
                                
                                
                                    
                                        f
                                    
                                    
                                        i
                                    
                                
                                (
                                D
                                )
                                -
                                
                                    
                                        f
                                    
                                    
                                        i
                                    
                                
                                (
                                D
                                ʹ
                                )
                                ∣
                            
                        
                    
                
                
                    Euclidean Distance
                    
                        
                            
                                
                                    
                                        
                                            ∑
                                            
                                                i
                                                =
                                                1
                                            
                                            
                                                n
                                            
                                        
                                        
                                            ∣
                                        
                                        
                                            
                                                f
                                            
                                            
                                                i
                                            
                                        
                                        (
                                        D
                                        
                                            
                                                )
                                            
                                            
                                                2
                                            
                                        
                                        -
                                        
                                            
                                                f
                                            
                                            
                                                i
                                            
                                        
                                        (
                                        D
                                        ʹ
                                        
                                            
                                                )
                                            
                                            
                                                2
                                            
                                        
                                    
                                    
                                
                            
                        
                    
                
                
                    Cosine Distance
                    
                        
                            
                                1
                                -
                                
                                    
                                        
                                            
                                                f
                                            
                                            ⃗
                                        
                                        (
                                        D
                                        )
                                        ⋅
                                        
                                            
                                                f
                                            
                                            ⃗
                                        
                                        (
                                        D
                                        ʹ
                                        )
                                    
                                    
                                        ∥
                                        
                                            
                                                f
                                            
                                            ⃗
                                        
                                        (
                                        D
                                        )
                                        
                                            
                                                ∥
                                            
                                            
                                                2
                                            
                                        
                                        ∥
                                        
                                            
                                                f
                                            
                                            ⃗
                                        
                                        (
                                        D
                                        ʹ
                                        )
                                        
                                            
                                                ∥
                                            
                                            
                                                2
                                            
                                        
                                    
                                
                            
                        
                    
                
                
                    Canberra Distance
                    
                        
                            
                                
                                    ∑
                                    
                                        i
                                        =
                                        1
                                    
                                    
                                        n
                                    
                                
                                
                                    
                                        
                                            ∣
                                            
                                                
                                                    f
                                                
                                                
                                                    i
                                                
                                            
                                            (
                                            D
                                            )
                                            -
                                            
                                                
                                                    f
                                                
                                                
                                                    i
                                                
                                            
                                            (
                                            D
                                            ʹ
                                            )
                                            ∣
                                        
                                        
                                            ∣
                                            
                                                
                                                    f
                                                
                                                
                                                    i
                                                
                                            
                                            (
                                            D
                                            )
                                            ∣
                                            ∣
                                            
                                                
                                                    f
                                                
                                                
                                                    i
                                                
                                            
                                            (
                                            D
                                            ʹ
                                            )
                                        
                                    
                                
                            
                        
                    
                
                
                    Bray-Curtis Distance
                    
                        
                            
                                
                                    
                                        ∑
                                        ∣
                                        
                                            
                                                f
                                            
                                            
                                                i
                                            
                                        
                                        (
                                        D
                                        )
                                        -
                                        
                                            
                                                f
                                            
                                            
                                                i
                                            
                                        
                                        (
                                        D
                                        ʹ
                                        )
                                        ∣
                                    
                                    
                                        ∑
                                        
                                            
                                                f
                                            
                                            
                                                i
                                            
                                        
                                        (
                                        D
                                        )
                                        +
                                        
                                            
                                                f
                                            
                                            
                                                i
                                            
                                        
                                        (
                                        D
                                        ʹ
                                        )
                                    
                                
                            
                        
                    
                
                
                    Hoover’s Delta-P1
                    
                        
                            
                                (
                                
                                    
                                        P
                                    
                                    ¯
                                
                                +
                                1
                                
                                    
                                        )
                                    
                                    
                                        2
                                    
                                
                                -
                                
                                    
                                        N
                                    
                                    ¯
                                
                                 
                                ,
                                P
                                =
                                {
                                
                                    
                                        z
                                    
                                    
                                        i
                                    
                                
                                ∣
                                
                                    
                                        z
                                    
                                    
                                        i
                                    
                                
                                ≥
                                0
                                }
                                ,
                                N
                                =
                                {
                                
                                    
                                        z
                                    
                                    
                                        i
                                    
                                
                                ∣
                                
                                    
                                        z
                                    
                                    
                                        i
                                    
                                
                                &lt;
                                0
                                }
                            
                        
                    
                
                
                    z-score
                    
                        
                            
                                
                                    
                                        
                                            
                                                f
                                            
                                            
                                                i
                                            
                                        
                                        (
                                        D
                                        )
                                        -
                                        
                                            
                                                μ
                                            
                                            
                                                i
                                            
                                        
                                    
                                    
                                        
                                            
                                                σ
                                            
                                            
                                                i
                                            
                                        
                                    
                                
                            
                        
                    
                
                
                    Diversity
                    
                        
                            
                                
                                    
                                        1
                                    
                                    
                                        n
                                    
                                
                                
                                    ∑
                                    
                                        j
                                        =
                                        1
                                    
                                    
                                        m
                                    
                                
                                
                                    
                                        
                                            
                                                
                                                    f
                                                
                                                
                                                    i
                                                
                                            
                                            (
                                            
                                                
                                                    D
                                                
                                                
                                                    j
                                                
                                            
                                            )
                                            -
                                            
                                                
                                                    a
                                                
                                                
                                                    i
                                                
                                            
                                        
                                    
                                
                                 
                                ,
                                 
                                
                                    
                                        a
                                    
                                    
                                        i
                                    
                                
                                =
                                m
                                e
                                d
                                i
                                a
                                n
                                (
                                ⟨
                                
                                    
                                        f
                                    
                                    
                                        i
                                    
                                
                                (
                                
                                    
                                        D
                                    
                                    
                                        1
                                    
                                
                                )
                                ,
                                ⋯
                                ,
                                
                                    
                                        f
                                    
                                    
                                        i
                                    
                                
                                (
                                
                                    
                                        D
                                    
                                    
                                        m
                                    
                                
                                )
                                ⟩
                                )
                            
                        
                    
                
                
                    Eder’s Ranking Factor
                    
                        
                            
                                
                                    
                                        n
                                        -
                                        
                                            
                                                n
                                            
                                            
                                                i
                                            
                                        
                                        +
                                        1
                                    
                                    
                                        n
                                    
                                
                            
                        
                    
                
            
        
        
            
                
                    Bibliography
                    
                        Argamon, S. (2008). Interpreting Burrows’ Delta: Geometric and Probabilistic Foundations. 
                        Literary and Linguistic Computing,
                        23(2): 131–47.
                    
                    
                        Burrows, J. (2002). ‘Delta’—A Measure of Stylistic Difference and a Guide to Likely Authorship. 
                        Literary and Linguistic Computing, 
                        17(3): 267–87.
                    
                    
                        Eder, M., Kestemont, M. and Rybicki, J. (2013). Stylometry with R. 
                        Digital Humanities 2013: Conference Abstracts. Lincoln: University of Nebraska-Lincoln, pp. 487–89.
                    
                    
                        Eder, M. and Rybicki, J. (2013). Do Birds of a Feather Really Flock Together, Or How to Choose Training Samples for Authorship Attribution. 
                        Literary and Linguistic Computing,
                        28(2): 229–36.
                    
                    
                        Everitt, B., et al. (2011). 
                        Cluster Analysis. 5th ed. Chichester.
                    
                    
                        Hoover, D. (2004a). Testing Burrows’ Delta. 
                        Literary and Linguistic Computing,
                         19(4): 453–75.
                    
                    
                        Hoover, D. (2004b). Delta Prime? 
                        Literary and Linguistic Computing, 
                        19(4): 477–95.
                    
                    
                        Hoover, D. (2005). Delta, Delta Prime, and Modern American Poetry: Authorship Attribution Theory and Method. 
                        Proceedings of the 2005 ALLC/ACH Conference, http://tomcat-stable.hcmc.uvic.ca:8080/ach/site/xhtml.xq?id=73.
                    
                    
                        Juola, P. (2006). Authorship Attribution. 
                        Foundations and Trends in Information Retrieval
                        ,
                        1(3): 233–334.
                    
                    
                        Koppel, M., Schler, J. and Argamon, S. (2009). Computational Methods in Authorship Attribution. 
                        Journal of the American Society for Information Science and Technology, 
                        60(1): 9–26.
                    
                    
                        Rybicki, J. and Eder, M. (2011). Deeper Delta across Genres and Languages: Do We Really Need the Most Frequent Words? 
                        Literary and Linguistic Computing,
                        26(3): 315–21.
                    
                    
                        Smith, P. and Aldridge, W. (2011). Improving Authorship Attribution: Optimizing Burrows’ Delta Method. 
                        Journal of Quantitative Linguistics, 
                        18(1): 63–88.
                    
                    
                        Stamatatos, E. (2009). A Survey of Modern Authorship Attribution Methods. 
                        Journal of the American Society for Information Science and Technology,
                        60(3): 538–56.
                    
                
            
        
    

2285	2015	

    
        
            
                Kapital: An Interactive Fiction Game
                
                    
                        Milstein
                        Dana
                    
                    Yale University, United States of America
                    dana.milstein@yale.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Short Paper
                
                
                    serious game
                    Marxism
                    Python
                    interactive fiction
                
                
                    audio
                    video
                    multimedia
                    virtual and augmented reality
                    knowledge representation
                    games and meaningful play
                    interdisciplinary collaboration
                    programming
                    digital humanities - pedagogy and curriculum
                    creative and performing arts
                    including writing
                    cultural studies
                    german studies
                    English
                
            
        
    
    
        
            Japanese publisher East Press published a manga edition of Karl Marx’s multivolume 
                Das Kapital in 2007, and in that same year released 507,000 copies. Several scholars have written on (and created) the practice of transposing difficult philosophy or classic literature into graphic novels and comic books. However, what happens when the manga themselves are transposed into a more interactive art form? 
            
            Visual novels are interactive fiction games that incorporate game play and are usually centered on dialogue, non-linear narratives, and multiple perspectives. In this presentation I immerse the audience in 
                Kapital using Ren’Py, a visual novel engine based on simplified Python scripting. (Please see the game’s information site here: http://campuspress.yale.edu/marxifg/.)
            
            Session Audience
            Teaching and learning professionals / faculty / instructional designers, frontline practitioners.
            Session Description
            The focus of this presentation is to immerse our audience in game play of 
                Kapital. I anticipate that audience participation will serve as experiential learning for the visual novel as a serious game, and will therefore lead to a robust discussion about the merits and pitfalls of this learning approach. For learners of 
                Kapital, I am concerned that long philosophical documents that hold importance for current cultural issues are, nevertheless, inaccessible to the majority of students because they are dense, weighty, and of astronomical length. I endeavor through gameplay to have players embodied in the philosophy, to learn theory through doing, and to react in ways that will reveal that they are able to make appropriate decisions using their new knowledge base. 
            
            Value of Serious Games and Interactive Fiction for Higher Education
             1. This is a method for promoting literacy of and exposure to inaccessible philosophical texts whose ideologies are vogue in culture and criticism. 
             2. The form of the visual novel is gaining popularity and will find some use value in education (is it serious game, edutainment, or literary?). 
             3. The issues of digital learning—to code in Python, techniques for storyboarding, and translation issues—are paramount. Increasingly, IT will need to develop these types of games, offer workshops to support student and faculty learning of programming, and mentor groups in project management and theories of game design. 
            Learning Outcomes
             1. Understand and apply critical concepts from 
                Kapital.
            
             2. Engage in and comprehend the structure and form of the visual novel.
             3. Identify the process for designing an IF game.
             4. Envision use value for the genre in learning environments.
            Of interest to the presentation is the unique collaborative background for this project: The presenter is partnering as a staff-faculty member with undergraduate student Alex Lew to create 
                Kapital. They have successfully presented the game design, history, and practice in two conferences, including the inaugural presentation of Yale’s Performance Studies Working (fall 2014), and in the first session of the INKE (Implementing New Knowledge Environments) Conference (Chicago, 2014). They have also submitted this proposal to Educause 2015 for joint presentation of the interactive demo. 
            
        
    

2318	2015	

    
        
            
                Nocht: An Open Source Tool for Text Analysis
                
                    
                        O'Sullivan
                        James Christopher
                    
                    Pennsylvania State University
                    josullivan@psu.edu
                
                
                    
                        Hswe
                        Patricia
                    
                    Pennsylvania State University
                    phswe@psu.edu
                
                
                    
                        Long
                        Christopher P.
                    
                    Pennsylvania State University
                    cplong@psu.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Short Paper
                
                
                    Tools
                    Text Analysis
                    Python
                    Digital Literary Studies
                
                
                    software design and development
                    text analysis
                    English
                
            
        
    
    
        
            Computational approaches to text analysis have revolutionised the ways in which scholarly research is being conducted. A number of tools exist that help scholars, from a variety of disciplines, analyse textual data, whether literary, historical, or otherwise, using scientific methodologies. However, many of these tools are either proprietary, present a steep learning curve, or are constructed without much transparency, often leaving users with results whose means of production they do not understand. This poster will outline the development of a tool that is intuitive and completely free and open-source, so that scholars in literary studies, and indeed the broader humanities, can leverage computational methods and big data analytics in their research.
            Nocht
            Nocht (trans.: to reveal, uncover), is developed, primarily, in Python, so that it is flexible, scalable, and cross-platform. It has been developed in accordance with the following principles:
             • It offers users a low-barrier means of using computational approaches to text analysis.
             • It is designed and developed in a humanities / arts / social sciences context.
             • It is completely open-source, removing the ‘black-box’, closed-code issues.
             • It brings together existing libraries and code-sets, acting as a ‘script portal’ of sorts.
            At present, Nocht supports the following methodologies, though with some limitations:
             • Wordcount and most frequent wordlists.
             • Word / wordlist frequency plotting.
             • Syntax and sentence analysis.
             • Sentiment analysis.
             • Topic modeling.
             • Zeta analysis.
            It is hoped that Nocht will further contribute to our field’s ongoing commitment to open and sustainable research tools, complementing highly regarded projects like Voyant
                1 and Stylo (Eder et al., 2013). Its name is an obvious tribute to the former, which has for so long been one of our field’s fundamental tools. It is hoped that Nocht will add further to the DH toolkit, as well as complement the ongoing work of Voyant’s creators in leveraging the iPython architecture. 
            
            From a technical perspective, Nocht is scalable and robust, and satisfies the needs of a wide range of scholars, many of whom wish to conduct this form of research but lack the expertise or resources to do so. In this respect, it enables scholars, both emerging and established, to engage with cutting-edge analyses across a variety of disciplines. In many cases, it draws on a series of existing libraries and proven methodologies, such as NLTK
                2 and matplotlib,
                3 and so acts as a set of original scripts as well as a portal to existing tools. A complete technical overview of the project’s features, as well as the components utilised in its modular development, will be provided at the session. 
            
            Discussion
            This poster proposes to introduce Nocht to the field, discussing possible future development directions, as well as issues to date. Some of the disciplinary particularities identified by Gibbs and Owens (2012), such as our need to enhance the usability of our tools, will be addressed. The tension between having an intuitive interface and the need for scholarly tools to produce verifiable results is particularly clear in this project. While there is a long-established requirement that such tools be user-friendly (Krug 2005), one might argue that this must be balanced with a commitment to avoiding ‘black-box’ projects; usability does not necessarily equate to understanding. 
            Measuring the value and success of development projects also remains problematic for scholars and practitioners working across the digital humanities. Schreibman and Hanlon’s survey (2010) finds that the majority of respondents were satisfied that their tools had been ‘successful’, enabling themselves and others to further their research. However, respondents also outlined that they had measured this success from a ‘controlled list’. As our methods continue to gain prominence beyond the core digital humanities community, we must find new metrics through which we can reliably measure the impact of our tools, not just in terms of user volumes, but in relation to the quality of research output. As a project that has sacrificed some aspects of usability and marketability in favour of broad functionality and a commitment to open principles, perhaps to its detriment, Nocht is an ideal catalyst for this debate. It is a small development with limited financial support, so it will be interesting to see if projects of this scale have a future in our discipline.
            Notes
            1. See Stéfan Sinclair and Geoffrey Rockwell, http://voyant-tools.org/.
            2. Natural Language Toolkit, http://www.nltk.org/.
            3. matplotlib, http://matplotlib.org/.
        
        
            
                
                    Bibliography
                    
                        Eder, M., Kestemont, M. and Rybicki, J. (2013). Stylometry with R: A Suite of Tools. 
                        Digital Humanities 2013: Conference Abstracts, University of Nebraska–Lincoln, pp. 487–89.
                    
                    
                        Gibbs, F. and Owens, T. Building Better Digital Humanities Tools: Toward Broader Audiences and User-Centered Designs. 
                        Digital Humanities Quarterly,
                        6(2).
                    
                    
                        Krug, S. (2005). 
                        Don’t Make Me Think! A Common Sense Approach to Web Usability. 2nd ed. New Riders Press, New York.
                    
                    
                        Schreibman, S. and Hanlon, A. M. (2010). Determining Value for Digital Humanities Tools: Report on a Survey of Tool Developers. 
                        Digital Humanities Quarterly,
                        4(2).
                    
                
            
        
    

2345	2015	

    
        
            
                Renderings: Translating Literary Works in the Digital Age
                
                    
                        Marecki
                        Piotr
                    
                    Jagiellonian University, Poland
                    piotr.marecki@ha.art.pl
                
                
                    
                        Montfort
                        Nick
                    
                    Massachusetts Institute of Technology, US
                    nickm@nickm.com
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    translation theory
                    creative computing
                    platform studies
                    expressive processing
                
                
                    archives
                    repositories
                    sustainability and preservation
                    literary studies
                    multilingual / multicultural approaches
                    natural language processing
                    publishing and delivery systems
                    semantic analysis
                    text analysis
                    text generation
                    philology
                    internet / world wide web
                    interdisciplinary collaboration
                    linguistics
                    programming
                    creative and performing arts
                    including writing
                    translation studies
                    history of Humanities Computing/Digital Humanities
                    English
                
            
        
    
    
        
            The point of departure for this paper is the Renderings project 
                (http://trope-tank.mit.edu/renderings/) established in 2014 and developed at the Massachusetts Institute of Technology in a lab called the Trope Tank. The project is described as concentrating on translations of highly computational and otherwise unusual digital literature into English. Its members ‘not only employ established literary translation techniques, but also consider how computation and language interact. Literary and computational experts worldwide participate’. The current team includes Nick Montfort (the initiator and leader of the project), Patsy Baudoin, Andrew Campana, Sally Chen, Aleksandra Małecka, Piotr Marecki, and Erik Stayton. During the project’s first year, 13 translations or bilingual works, by 12 authors, have been produced in the following languages: Chinese (1), French (3), German (1), Japanese (4), Polish (2), and Spanish (2). The translated works are 
            
             • Automation (2013) by Andrew Campana.
             • Contemporary Japanese Poetry Generator (2012) by Shinonome Nodoka.
             • Dizains (1985) by Marcel Bénabou.
             • Hallelujah (2012) by ni_ka.
             • MAZ—Mutantist Autonomous Zone (2014) by Mathias Richard.
             • Poem 21 (1988) by Amílcar Romero.
             • Poet (2003) by Michał Rudolf.
             • Sample Automatic Poem (2009) by Féliz Remirez.
             • Seika no Kôshô (2013) by Andrew Campana.
             • Shanshui by Sally Chen.
             • Speeches (1993) by Marek Pampuch.
             • Tötan das Gedich (1997) by Johannes Auer.
             • Triolets by Paul Braffort.
            The programming languages of the original works include Basic, Perl, and Java Script. They were selected to represent the wide variety of genres of electronic literature and creative computing, and the productions of cultures/literatures not currently well known in this dominantly English-language field. Thus, the first Renderings set of works includes genres characteristic of specific cultures, such as Japanese ‘monitor poetry’ (a blog that bursts of flowers, hearts, and other graphics dense enough to obscure the screen), a Polish generator of communist speeches, electronic ‘landscape poetry’ from China, and electronic OULIPO texts (France). The selected works also present different approaches to computation in literature. The project itself thus has the aim of describing the experiences of the margins of digital culture and exploring the hitherto overlooked fringes of the digital heritage.
            The Renderings is not the first project exploring translations of electronic literature. There have already been translations of Michael Joyce’s, Stuart Moulthrop’s, Nick Montfort’s, and Stephanie Strickland’s works from English into other languages. In addition, the Electronic Literature Organization was a co-sponsor of the conference Translating E-Literature in 2012 in Paris. The Renderings project continues these threads, but focusing on the direction from other languages into English, its goal being to give English-speakers access to works from other traditions. The project involves also meetings and brainstorming with literary translators: Robert Pinsky, Marc Lowenthal, John Cayley, and David Ferry.
            Translating digital works written in code requires the translator to face new challenges in addition to those tackled by the regular translator of literature. It is a type of translation akin to the translation of experimental, conceptual or constrained works. It is not rare that the task requires the translator or translators to reinvent the work in a new linguistic and cultural context, and sometimes also another programming language. The history of literature is already familiar with similar cases, like the translations of works of the French OULIPO group; for instance, Georges Perec’s La dispariton, which is written without the most frequently occurring vowel of the French language, has been be rendered in other languages with the omission of the most frequent vowel in the language of the translator, e in English but a in Spanish.
            In the case of highly computational digital works there are additional difficulties and challenges, first and foremost, the formal and material properties of the code of the program. If we assume after Noah Wardrip-Fruin that a digital work has three layers: the input, process and output, the task of translation will be operated mostly on the first two layers: the input and process. It will require establishing a lexicon, determining the input data, which may differ given the discrepancies between grammars (inflection, declension, genre) and translating the process, that is the lines of code responsible for producing a given output. Noah Wardrip-Fruin explains to humanities scholars analyzing digital works that they focus mainly on the output level, which he considers a superficial approach. It seems that the work of the translator of a digital work is the ideal activity for performing what this scholar calls ‘expressive processing’. Translating a highly computational work without the knowledge of its inner workings and operation on all the three levels of analysis should not occur. For instance, the novel World Clock, written by Nick Montfort in Python, has 165 lines of code in its original English version. Its Polish translation has an additional 60 lines of code. Indeed it is not unusual for the output of the translated work to be the result of processes different than in the original code. An important category for the translation of digital works is collaborative work, in a team including a translator and a programmer, where translation and programming competences overlap and complete.
            The translation of a digital work is not only a matter of language, but also requires awareness of the code and the platform for which it was designed. Especially in the case of older works, the translator has to consider porting the work to a platform more accessible to the contemporary reader. The textual generator Poet was written in Perl in 2003 and archived online as a .pl file. It will be thus available to those readers who know how to run the program in the terminal and are willing to download and execute it. Given the goals of the project, it seemed justified to port the program to Java Script in order to publish it online, to provide easier access to readers on the Web. In such a case the translator has to preserve as many aspects of the program’s functioning as possible in the original programming language. Yet another aspect connected to this problem is the change of platform. Platform consciousness and platform description are described according to the methodology developed by Nick Montfort and Ian Bogost in books from the Platform Studies series published by MIT Press. It is a method that ‘investigates the relationships between the hardware and software design of computing systems and the creative works produced on those systems.’ And so when describing the generator of communist speeches translated and published online as part of the Renderings project, an important aspect of its historical, formal and material analysis will be the consideration of the consequences of the fact that it was first written in a popular programming language for the Amiga.
            The presented paper describes a phenomenon belonging to the broadly understood discipline of creative computing and studies the work of the translator as taking place both in code and language, drawing from the methodology developed by the fields of code studies, platform studies and expressive processing. 
        
    

2388	2015	

    
        
            
                The Trials of Tokenization
                
                    
                        Hoover
                        David L.
                    
                    New York University, United States of America
                    david.hoover@nyu.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    Python
                    tokenization
                    word frequency lists
                    programming
                    punctuation
                
                
                    natural language processing
                    software design and development
                    text analysis
                    programming
                    standards and interoperability
                    English
                
            
        
    
    
        
            The process of tokenizing texts is typically out of sight and almost out of mind—often handled invisibly by the analyst’s program or R script, and rarely described, discussed, or even mentioned. For ‘big data’, even if questions did arise about the nature of the word list produced, testing is not feasible. Furthermore, tokenizer accuracy is so critically affected by the state and nature of the texts that probably no general measure of accuracy or appropriateness is possible. Finally, built-in programming functions and libraries are all too often used uncritically with little realization that their output does not conform to the assumptions or expectations of the analyst. I suggest that we should pay a little more attention to the theory and practice of tokenization.
                1
            
            Consider a hypothetical case. Let’s say I want to analyze 5,000 novels, have access to the texts at HathiTrust, download 5,000 novels in plain text, and tokenize them. Below is part of a page from Elizabeth Gaskell’s 
                Cranford, from HathiTrust (Gaskell, 1910 [1851], 107):
            
            
                
            
            Figure 1. 
                Cranford, Elizabeth Gaskell, from page 107.
            
            A human reader would have little trouble tokenizing this passage, and it is not extremely problematic, though minor OCR problems exist (mainly spacing issues around single quotation marks / apostrophes and dashes, and the line-end hyphen). I tokenized this passage with The Intelligent Archive (2012), KWIC (Tsukamoto, 2004) WordSmith Tools (Scott, 2012), Voyant (Sinclair et al., 2012), and Stylo (Eder et al., 2014).
                2 Even on this short text, the five programs identify three different numbers of types and two different numbers of tokens, largely because of the handling of single quotation marks. KWIC and WordSmith produce identical lists, as do Voyant and Stylo, but neither of these match The Intelligent Archive.
            
            Now consider Charles Chesnutt’s 
                The House Behind the Cedars (1900, 13), also from HathiTrust:
            
            
                
            
            Figure 2. 
                The House Behind the Cedars, Charles Chesnutt, from page 13.
            
            The dialect in this passage is challenging even for human readers, and the OCR is more problematic. For example, the printed text (judging from the PDF) had spaced contractions, which explains ‘you 're’ in the fourth line from the bottom and the space in ‘lie 's’ in the first line, where the text reads “he 's.” This classic OCR problem occurs several times in this novel. And in the last line ‘you '11’ has both a space and an erroneous number 11 for the ‘ll’ (double el), another common OCR problem. Those analyzing big data usually rely on the insignificance of random error, but these and many other kinds of error are not random, and systematic error within one text, one author, one genre, or one collection could easily lead to thousands of inaccurate word frequency counts in this hypothetical study of 5,000 texts.
            The use of apostrophes in the Chesnutt passage to indicate dialect pronunciations can also severely affect tokenization. Although The Intelligent Archive, KWIC, and WordSmith Tools produce exactly the same lists for this brief passage, and Voyant has the same number of types and tokens, Voyant removes all initial (but not final) apostrophes, creating different words for eight of the 97 types. Stylo removes all numbers, all initial and final apostrophes, and many internal apostrophes, retaining them only in 
                ain^t, gentleman^s, and 
                spen^s (replaced with a caret). It produces six more tokens and four more types than the other programs, and many more differences in the word list. Unfortunately, in Chesnutt’s short novel, more than 650 words begin and/or end with apostrophes crucial to the identity of the word, so that the word lists produced by Voyant and Stylo are quite inaccurate. Furthermore, only KWIC and WordSmith Tools let the user choose whether apostrophes and hyphens are part of a word, and whether numbers can appear in the word list or not. Only WordSmith Tools allows the user to choose whether to allow apostrophes at the beginnings and/or ends of the word as well as internally.
            
            Obviously, the two texts examined above cause different problems, and different tokenizers are more accurate for one than for the other. Worse yet, these problems are found even in relatively carefully edited texts like those from Project Gutenberg. Although Gutenberg’s 
                The House Behind the Cedars does not have spaced contractions, and correctly has 
                he’s in the first line and 
                you’ll in the final line, the 29 initial and final dialect apostrophes remain problematic. The Gutenberg text also represents dashes as two hyphens without spaces, creating more problems for tokenizers. The Intelligent Archive and Stylo treat these double-hyphen dashes as breaking characters, while retaining single hyphens within compound words, but KWIC, WordSmith Tools, and Voyant treat them like single hyphens, creating compounds with double hyphens where dashes are needed. The situation is still more complex if a double-hyphen is preceded or followed by a breaking character. If this sounds esoteric, consider that this short novel contains nearly 400 double-hyphen dashes (Dickens’ 
                Dombey and Son has more than 2,200). And this problem, too, is highly systematic: words vary considerably in how frequently they are preceded or followed by a dash, and 1,000 dash errors per text would produce 5,000,000 errors in our hypothetical 5,000 novels. (For a practical example of the effects of error, see Matt Jockers’ discussion of topic modeling and several ‘topics’ that arose from OCR error and metadata (Jockers, 2013, 135).
            
            It might seem that we just need more sophisticated tokenizers, but the required level of sophistication to handle double-hyphen dashes correctly is quite high, and the problems caused by apostrophes and single quotation marks cannot be correctly solved computationally at all. In some cases, not even a human reader can tokenize with certainty; in others, a computer can solve problems a human cannot. 
            Let’s consider a few further tokenization questions:
            He said, “That’s ’bout ‘nough, sho’.”
            “That’s ‘bout’, not ‘fight’; ’nough said,” Nough said.
            “John tried that ‘Nough told me to’ on me,” Bill whined.
            He remarked, “John said, ‘Bout starts at nine.’”
            He remarked, “John said, ‘It’s ’bout time.’”
            He remarked, “John said, ‘‘Bout time.’” Can these apostrophes/single quotes be handled correctly computationally? How about the two single quotes before ‘Bout’ in the last example?
            I visited the U.S.S.R. Four tokens? Seven? Is the final period part of the final token?
            I visited the U.S.S.R.! Four tokens? Seven? Is the final period part of the final token?
            Is that C------? Is ‘C------’ the token ‘C’ followed by a dash, or the token ‘C------’? What about ‘C—’? Or ‘C-’?
            C------ is here. Same questions.
            Oh d--n it! Is ‘d--n’ the tokens ‘d’ and ‘n’ separated by a dash, or the token ‘d--n’? How about ‘d---n’? or ‘d-n’? or ‘G-d’?
            I said--never mind. If ‘d--n’ is a token, can we prevent ‘said--never’ from being a token here?
            That’s what I--a mistake, sorry. How do we get ‘d--n’ correct without failing here?
            You’re a real %#@$! Three tokens? Four? Does the last include the final ‘!’? What if there were a period after the ‘!’?
            You’re a real %#@$!. How about now?
            I am working on a Python tokenizer that can handle most of these issues correctly, and some of these problems are fairly rare, but I despair of the possibility of creating a word frequency list that is ‘correct’ even in my own opinion. For many years I have ‘corrected’ the texts before tokenizing, but that is not a practical solution for 5,000 novels and presents its own problems.
            Perhaps in sufficiently big data, the error introduced by tokenizers will not significantly alter the results, and Maciej Eder (2013) has recently shown that some corpora are remarkably resistant to some kinds of intentionally introduced error. And improving the quality of the corpus had a relatively small effect on the attribution of the Federalist Papers (Levitan and Argamon, 2006). More study seems needed before we can be complacent, however, even in large-scale problems involving only authorship or classification. For smaller-scale stylistic studies, tokenization decisions can clearly have serious repercussions. Consider Ramsay’s (2011) analysis of 
                The Waves, where decisions about tokenization significantly alter the lists of men-only and women-only words and words that characterize the six narrative voices (see Hoover [2014a] and Plasek and Hoover [2014], for discussion). Another example that replicates an experience I have had several times is that a Full Spectrum analysis (Hoover, 2014b), based on Craig’s version of Burrows’s Zeta (Burrows, 2007; Craig and Kinney, 2010) can give strange results if uncorrected texts are inadvertently included. For example, in a test of Charlotte Brontë versus Anne and Emily Brontë, 11 of the 100 most distinctive words were words with inappropriate initial “apostrophes” because the novels of Anne and Emily in the analysis both used single quotation marks for dialogue.
            
            Far from being an insignificant tool that can be taken for granted, a tokenizer expresses its author’s theory of text and can significantly affect the results of many kinds of text analysis.
            Notes
            1. As a reviewer of this paper has pointed out, the problems of tokenization have been more widely recognized recently in the NLP community. For example, Dridan and Oepen (2012) and Chiarcos et al. (2012) address and suggest partial solutions for some of the problems discussed here. Even if the problems had all been solved within the NLP community (a fact not in evidence), however, this would not diminish the force of my argument for the DH community, where there has been much less attention paid to them.
            2. These programs represent a variety of those used in DH work (in order): a mature Java program with a database function, a venerable corpus linguistics program with lots of functions and user-options, a highly customizable and powerful commercial program from OUP, a widely used online tool, and a recently developed set of tools written in the currently popular R.
        
        
            
                
                    Bibliography
                    
                        Burrows, J. F. (2007). All the Way Through: Testing for Authorship in Different Frequency Strata. 
                        LLC,
                        22(1): 27–47.
                    
                    
                        Chesnutt, C. W. (1900). 
                        The House Behind the Cedars. Houghton Mifflin, Boston, http://babel.hathitrust.org/cgi/pt?view=plaintext;size=100;id=nc01.ark%3A%2F13960%2Ft7cr7221k;page=root;seq=25;num=13.
                    
                    
                        Chiarcos, C., Ritz, J. and Stede, M. (2012). By All These Lovely Tokens . . . : Merging Conflicting Tokenizations. 
                        Language Resources and Evaluation,
                        46(1): 53–74. 
                    
                    
                        Craig, H. and Kinney, A. F. (2010). 
                        Shakespeare, Computers, and the Mystery of Authorship. Cambridge University Press, Cambridge. 
                    
                    
                        Dridan, R., and Oepen, S. (2012). Tokenization: Returning to a Long Solved Problem: A Survey, Contrastive Experiment, Recommendations, and Toolkit. 
                        Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pp. 378–82.
                    
                    
                        Eder, M. (2013). Mind Your Corpus: Systematic Errors in Authorship Attribution. 
                        LLC,
                        28(4): 603–14.
                    
                    
                        Eder, M., Rybicki, J. and Kestemont, M. (2014). Stylo.
                    
                    
                        Gaskell, E. (1910 [1851]). 
                        Cranford. Houghton Mifflin, Boston, http://babel.hathitrust.org/cgi/pt?q1=twelve;id=hvd.32044097042071;view=plaintext;start=1;sz=10;page=root;size=100;seq=143;num=107.
                    
                    
                        Hoover, D. L. (2014a). Making Waves: Algorithmic Criticism Revisited. 
                        DH2014, Lausanne, Switzerland: EPFL-UNIL, pp. 202–4.
                    
                    
                        Hoover, D. L. (2014b). The Full-Spectrum Text-Analysis Spreadsheet. 
                        Digital Humanities 2013, Center for Digital Research in the Humanities, Lincoln, NE, University of Nebraska, pp. 226–29.
                    
                    
                        The Intelligent Archive. (2012). Centre for Literary and Linguistic Computing, University of Newcastle, Australia.
                    
                    
                        Jockers, M. L. (2013). 
                        Macroanalysis: Digital Methods and Literary History. University of Illinois Press, Urbana-Champaign.
                    
                    
                        Levitan, S. and Argamon, S. (2006). Fixing the Federalist: Correcting Results and Evaluating Editions for Automated Attribution. 
                        Digital Humanities 2006. Paris: Centre de Recherche Cultures Anglophones et Technologies de l’Information, pp. 323–26.
                    
                    
                        Plasek, A. and Hoover, D. L. (2014). Starting the Conversation: Literary Studies, Algorithmic Opacity, and Computer-Assisted Literary Insight. 
                        DH2014, Lausanne: EPFL-UNIL, pp. 305–6.
                    
                    
                        Ramsay, S. (2011). 
                        Reading Machines: Toward an Algorithmic Criticism. University of Illinois Press, Urbana.
                    
                    
                        Scott, M. (2012). WordSmith Tools version 6. Liverpool: Lexical Analysis Software.
                    
                    
                        Sinclair, S., Rockwell, G. and the Voyant Tools Team. (2012). Voyant Tools (web application).
                    
                    
                        Tsukamoto, S. (2004). KWIC Concordance for Windows version 4.7.
                    
                
            
        
    

2485	2016	
        
            
                Introduction
                Russian rhyme was described thoroughly in the 20th century, especially by M. Gasparov (Gasparov, 2000). Though we now have a powerful tool to analyze rhymes further in the form of the poetic corpus of the Russian National Corpus (henceforth RNC), not much recent research has been carried out in this area (Orekhov, 2015). As I am particularly interested in visualizing corpus data, I applied graphs to rhyme analysis. 
                Rhymes are convenient entities to be described in graph terms. In a rhyming pair, words are nodes, and rhyme relationship between them is an edge between nodes. Certain properties can be assigned to the nodes and to the edges. For example, word nodes may contain grammatical information and rhyme edges may bear all the rhyme classification (meter, position, etc.). 
                Furthermore, nowadays, there are tools available for storing graph information in a database. Information from such databases can be retrieved easily and in several formats. 
                My aim was to build a graph database using the data from the poetic corpus of the RNC. I want to show that the manual research done previously can be supported and extended in a vivid graphic way.
                Graphs can provide us with much information about rhyme diachrony:
                
                    Degree of connectivity in different periods (different rhyming tendencies); 
                    The longest path (chain of rhymes) and clusterization (popular rhymes in different periods);
                    Tendency flow from exact rhymes to inexact (requested by parameter of exactness in the different periods);
                    Appearance of the dissonance rhymes;
                    Tendency flow in rhyming types, position;
                    Domination of a certain rhyming type within the rhymes of one poet.
                
            
            
                Data
                The whole poetic corpus of the RNC was used for analysis. The data covers 775 Russian authors, born between 1658—1939. Overall, the corpus contains 85,996 documents, 229,968,798 words.
            
            
                Analysis
                Technical work included the following steps:
                
                    Transcribing words in a rhyme position; 
                    Retrieving rhymes from poems according to the phonetic transcription;
                    Rhyme classification;
                    Building new nodes and edges in the graph database.
                
                I used Python for rhyme extraction and classification and a Neo4j database for storing the data. 
                As I could not find any available modules for Russian transcription, I made this module myself using the transcribing rules from [http://www.philol.msu.ru/~fonetica/index.htm]. The module takes into account almost every rule, but the exception word lists are quite small. 
                The rhyme extraction algorithm I used was the following. The Python program finds all the tags  (tagged everywhere except blank verse) in the XML documents with poems (last words in lines). Then the program tries to find a possible rhyming pair within 4 lines before and after the current rhyme-zone word. Afterwards comes the transcriptions comparison; if stressed vowels are the same, then the process of classification begins. If the stressed vowels are different, then the dissonance rhyming type is checked.
                The classification of the rhymes was based on (Surkov, 1962), (Kvjatkovskij, 1966) and (Timofeev, 1935). I took 8 parameters into account:
                
                    Exactness (exact or inexact); 
                    Richness (rich or poor);
                    Depth (deep or not);
                    Ending (open or closed);
                    Position of the stressed vowel (male, female, dactyl, hyperdactyl);
                    Rhyming type (paired, crossed, encircling);
                    Assonance;
                    Dissonance.
                
                As soon as a new rhyming pair is found and classified, new nodes and edges are automatically created in the graph database. If any of the words existed in the database before, then an edge is created to the existing node; otherwise, a new node is created.
            
            
                Results
                For now, I have rendered several graph images for certain poets with approximately 30% of their rhymes. In Figure 1 there are 2570 rhymes from the poems of P. Vjazemskij. Figure 2 shows 3866 rhymes from the poetry of A. Pushkin. 
                
                    
                    Figure 1. Graph of the 2570 P. Vjazemskij's rhymes
                
                
                    
                    Figure 2. Graph of the 3866 A. Pushkin's rhymes
                
                From the figures, we can see that connectivity in Pushkin's graph is much higher than in Vjazemskij's graph. Furthermore, the graph of Vjazemskij's rhymes demonstrates certain clusters which can be analyzed in detail.
                I plan to continue my research and obtain other graphs for the whole epoch. I hope that further work will provide the information I listed in the introductory part, especially regarding connectivity and clusterization over different time periods. Quantitative analysis remains to be done as well. Firstly, I would like to look at the graph patterns, and then go deeper into calculations of graph characteristics and their interpretations.
            
        
        
            
                
                    Bibliography
                    
                        Gasparov, M. (2000). 
                        Očerk istorii russkogo stikha [Studies of the Russian verse history]. Fortuna limited.
                    
                    
                        Kvjatkovskij, A. (1966). 
                        Poetičeskij slovar' [Poetic dictionary]. Sovetskaja Enciklopedija.
                    
                    
                        Orekhov, B. (2015). 
                        Ešče raz ob issledovatel'skom potenciale poetičeskogo korpusa: metr, leksika, formula [One more time about the research potential of the poetic corpus: meter, lexicon, formula]. Russian National Corpus, in print.
                    
                    
                        Surkov, A. (1962). 
                        Kratkaja literaturnaja enciklopedija [Short literary encyclopedia], 1. Sovetskaja enciklopedija.
                    
                    
                        Timofeev, L. (1935). 
                        Literaturnaja enciklopedija [Literary encyclopedia], 9. Sovetskaja enciklopedija.
                    
                
            
        
    

2499	2016	
        
            Distributed version control technologies, the most popular protocols of which are git, subversion, and mercurial, have long been popular among computer programmers for their abilities to track changes in a codebase and foster collaboration among coders. When combined with code management platforms such as GitHub, Bitbucket, or GitLab, they become even more powerful, enabling sophisticated bug tracking, project planning, and open-source code publication. Although these technologies have not yet been in widespread use in the humanities, their potential for use with corpus creation and textual editing is far-reaching. This paper describes Git-Lit, an open-source, community-centered initiative to parse, version control, and publish to GitHub roughly 50,000 scanned public-domain books from the British Library, thereby facilitating decentralized, open-access, and democratic scholarly editing.
            The Git-Lit initiative addresses these problems:
            
                
                    Electronic texts are difficult to edit. Traditional text repositories like Project Gutenberg and the Oxford Text Archive maintain central, canonical versions of their texts that, in most cases, are virtually immutable. If a reader spots an OCR error in an ebook, he or she must rely on contacting the publisher to propose a correction. Even with an infrastructure such as Project Gutenberg’s Distributed Proofreaders, the process of releasing a corrected edition may take months or years. Git-Lit aims to radically streamline the improvement of an electronic text in two ways. First, ease of editing is achieved through GitHub's push-button forking (making a copy of a repository in one's user account) and in-browser editing---a reader may spot a mistake, correct it, and submit a pull request for the change in mere seconds, all without leaving the browser. Second, the decentralized model ensures that no single text may be considered unquestionably canonical, although 
                    de facto canonicity might be democratically achieved through repository voting mechanisms such as GitHub’s stars.
                
                
                    Electronic texts often lack editorial history. Owing, in some cases, to the age of an electronic text, its editorial provenance is often lost. Many Project Gutenberg editions, for instance, are transcribed from unknown print editions, and the history of their revisions is similarly opaque. Version control mitigates these problems by recording every edit, editor, and edition in the history of the text. When two editions diverge, git provides sophisticated tools for analyzing the differences between these editions. Sites like GitHub further provide graphical network charts, showing the genealogy of each version. Since contributions to a given text are logged according to individual contributor, credit for a given edition may be assigned according to the individual’s percentage of total contributions, minimizing the danger, for instance, that a professor may take credit for his or her graduate student's work.
                
                
                    Textual corpora are difficult to assemble. With some exceptions, notably the download function of the NLTK corpus module, downloading a text corpus involves compiling texts from diverse and heterogeneous sources. A would-be text analyst must click through a sequence of web pages to find the corpus he or she wants, and then either download a number of .zip files, or email the corpus creator to request a copy. With multiple texts, this can be a labor-intensive process that is not easily scriptable or automated. Git provides an easy way to solve these problems: by making texts available through the git protocol on GitHub, anyone that wishes to download a text corpus can simply run the command git clone followed by the repository URL. Parent repositories can then be assembled for collections of texts using git submodules. A parent corpus repository might be created for nineteenth-century 
                    Bildungsromane, for instance, and that repository would contain pointers to individual text repositories. These categories would not necessarily be mutually exclusive, and would allow for arbitrary curation of custom corpora. This provides a major advantage over the traditional directory structure model, where the existence of overlapping datasets necessitates the storage and maintenance of redundant data.
                
                
                    ALTO XML is not comfortably human-readable. ALTO XML, the OCR output format used by the British Library texts, as well as texts created by the Library of Congress, is extremely verbose. It encodes the location of each word on the page, and often gives the OCR certainty for each word. While this format is useful for archival purposes, plain text editions are more useful for reading and for most brands of computational text analysis. Git-Lit parses the British Library’s ALTO XML, and creates markdown versions of each text that are easily edited. Since markdown is readily converted to other document formats using tools such as Pandoc, this allows each text to be easily exported to PDF, EPUB, LaTeX, Docbook, and others. Additionally, Git-Lit is currently working on a system that leverages GitHub's built-in Jekyll HTML compilers to convert each text into a web page hosted on github.io, effectively creating 50,000 readable web editions. These new editions will exist as git branches in parallel with the markdown and ALTO XML editions. Since git maintains efficient copies of every historical version of the text, no information about the text is lost in these conversions. Anyone that wishes to improve the conversion script and create newer, better editions of the original files may freely do so by branching the text from an earlier git commit.
                
            
            Git-Lit software works by first parsing the XML metadata included with each text. This metadata is used to programmatically generate a repository name and a README.md file that describes the text, a document which GitHub will automatically render into a web page at the repository root. This file, along with standard CONTRIBUTING and LICENSE files, is then committed to local git repositories, initiating version control of the texts. The resulting local repository is then uploaded to GitHub via Python bindings to the GitHub API. Parent repositories are then created using git submodules for each collection of texts based on the their associated Library of Congress subjects. This enables a text analyst interested in 19th century poetry, for instance, to download all of the British Library’s released works in this genre simply by running git clone https://github.com/git-lit/19th-century-poetry.git && git submodule update --init --recursive.
            Since British Library texts are not the only ones being published to git-based platforms like GitHub---notable version-controlled corpora on GitHub include texts from the Text Creation Partnership and the early modern corpus Shakespeare His Contemporaries---git provides a common protocol for sharing, modifying, and distributing texts and textual corpora. Anyone may aggregate these corpora into parent repositories using git submodules. The Git-Lit project will soon launch a web application that will routinely scrape GitHub and other open repository sites for any textual corpus, thereby automating the process of discovering and indexing available corpora. This mechanism will also serve to democratize the curation of corpora, since the corpus index will be sorted by the number of GitHub “stars”, or votes, a repository has engendered from the community.
            The 50,000 British Library texts processed by Git-Lit, as well as many of the other open corpora described here, are currently being integrated into DHBox, the cloud-based Digital Humanities software suite. Soon, these corpora and many others will be available for download by selecting them from a web-based interface, where they will then be available for analysis using pre-installed versions of the Python NLTK, R, and other textual analytic tools.
            This paper discusses how Git-Lit’s methods might be used by other digital humanities projects involved in the creation or analysis of large text corpora, and how digital humanists may contribute to the Git-Lit project. (As an open-source project, Git-Lit welcomes contributions in the form of bug reports, feature requests, or code.) The paper also discusses some of the storage and computation limitations of electronically publishing texts via code repositories, and some of the technical problems encountered by the Git-Lit project. Finally, it suggests pedagogical uses of git-based collaborative digital editing, such as classroom compilation of anthologies or digital scholarly editions. The applications of these technologies are wide-ranging, and are neither proprietary to this project nor to services such as GitHub, but remain concepts of openness and collaboration with powerful implications for the digital humanities.
        
    

2501	2016	
        
            New York City English (NYCE) has long been a stigmatized variety of English. In his seminal research on language use in the New York City dialect, the sociolinguist William Labov referred to New York City as “a great sink of negative prestige” (Labov, 1966)—a characterization that reflected the negative view of NYCE speech shared by non-New Yorkers and New Yorkers alike. Decades later, Preston (2003) elicited extremely low ratings of the New York City dialect on scales of both “correctness” and “pleasantness” by participants from across the US. While these studies present strong evidence of the prevalence of negative language attitudes toward NYCE speech, a more complete picture of linguistic ideology would include what speakers say about NYCE when they are not participating in an academic study. This project seeks to accomplish just that, by examining linguistic ideology with respect to NYCE as espoused by users of the social networking service, Twitter.
            Twitter has been recognized as an important resource for humanists and social scientists alike. Scholars have collected and analyzed Twitter messages (tweets) in order to investigate numerous textual and linguistic phenomena such as 
                lexical variation (differences in use of synonymous words and phrases, such as 
                pop vs. 
                soda vs. 
                coke). Russ (2012) in particular (see also Bamman, 2011) illustrates the utility of Twitter for examining regionally defined lexical variation through comparison of the geographic distribution of word choices in 
                geotagged tweets (with GPS coordinates from which they originated) to more traditionally collected dialectology data. All related research has focused on differences in production. However, I argue that Twitter represents an untapped resource for the investigation of 
                perceptions of language use, particularly language attitudes toward regional dialects and differences in their phonetic features (which can be identified by non-standard orthography). Using Twitter solves a primary quandary for language attitude researchers—how to acquire naturally occurring data given the fact that participation in research decreases naturalness.
            
            Tweets containing attitudes and ideology were collected using a range of strategies, including text mining for words—and, crucially, spellings—that reference individual features. To do this, however, it is necessary to determine which features get noted and then which lexical items—and which spellings—are used to signal them. For instance, 
                cawfee (also, 
                cawffee) is a common orthographic representation of the word “coffee” as pronounced with a raised-THOUGHT vowel, one of the signature dialect features of NYCE. Widely used spellings that reflect 
                r-vocalization, another key feature of the NYCE dialect, include 
                New Yawk and 
                fuhgeddaboudit. In addition to collecting tweets containing orthographic representations of nonstandard features, Twitter search parameters included over 20 terms related to possible names for the dialect itself (e.g., 
                New York accent, Manhattan dialect, Brooklynese). These were included in part to determine the extent to which the general public perceives a distinction among speakers from the five boroughs (a distinction which has not been borne out by linguistic analysis).
            
            Repeated automated text mining of Twitter using a Python script to interact with the Twitter API yielded 6,384 tweets that match the aforementioned criteria. Elimination of retweets that did not introduce additional linguistic content and inspection to ensure the tweets reference NYCE produced a final corpus of 1,773 tweets. Relative frequencies of the borough-specific and pan-regional terms in the 1,315 tweets that explicitly reference NYCE by some name reveal that Twitter users most frequently refer to NYCE as the 
                New York accent (N=805; 61.2%), though 
                Brooklyn accent (N=359; 27.4%) accounts for more than a quarter, with 
                Bronx accent (N=54), 
                Queens accent (N=29, tied with 
                Brooklynese—the most frequent 
                -ese moniker), and 
                Staten Island accent (N=10) being used much less often. Whether 
                New York accents and 
                Brooklyn accents are perceived as linguistically or socially distinct, or two names for the same dialect region, will be explored in the paper.
            
            All tweets were manually coded to determine their sentiment with respect to NYCE.
            
                POSITIVE: 
                    I swear girls from New York accent sound so sexy 
                
                NEUTRAL: 
                    GAWGEOUS idea she said in her New Yawk accent 
                
                NEGATIVE: 
                    If you have a Brooklyn accent I automatically want to punch you. 
                
            
            Almost half of these tweets are neutral in sentiment (N=584, 44.4%); 378 were positive (28.7%) and 200 negative (15.2%). However, 154 tweets were classified as UNCLEAR (8.7%)—many are ambiguous as to whether they evaluate an imitation of an accent or the accent itself, such as when describing an actor’s performance (which is common among these types of tweets):
            
                UNCLEAR: 
                    his New York accent is so bad /: 
                
            
            Examples such as these pose significant obstacles to automated sentiment analysis—which has been extended to Twitter data (see for instance Pak and Paroubek, 2010)—particularly of language attitudes. Automatic methods would simply code the tweet as negative without recognizing the need to differentiate its underlying meaning. It is noteworthy, however, that even if every UNCLEAR tweet is actually expressing negative sentiment, there would 
                still be a greater number of tweets with positive opinions of NYCE speech than negative ones. Furthermore, when Twitter users reference a specific NYCE feature, their evaluation of it is more likely to be positive, regardless of whether they use standard (N=79) or nonstandard (N=568) orthography to represent the feature.
            
            These findings portray a broader range of reactions to NYCE than the language attitudes speakers have presented when engaged in academic research. The paper will include discussion of both negative and positive language attitudes that Twitter users espouse concerning the dialect features associated with NYCE. For instance, any tweets with positive sentiment will be examined to determine if they represent instances of “covert prestige” (Labov 1966), whereby speakers use stigmatized varieties for in-group identification and solidarity. Additional discussion will focus on which regional features evoke the most meta-commentary. Furthermore, I will explore the extent to which Twitter users draw (additional) attention to non-standard forms they employ through capitalization (
                NEW YAWK), hashtags (
                #newyawk), and other orthographic means.
            
        
        
            
                
                    Bibliography
                    
                        Bamman, D. (2011). Lexicalist. http://www.lexicalist.com/ (accessed 30 August 2015). 
                    
                    
                        Labov, W. (1966). 
                        The Social Stratification of English in New York City. Washington, D. C.: Center for Applied Linguistics.
                    
                    
                        Pak, A. and Paroubek, P. (2010). Twitter as a Corpus for Sentiment Analysis and Opinion Mining. 
                        Proceedings of Language Resource and Evaluation Conference (LREC), Valletta, Malta. 
                    
                    
                        Preston, D. (2003). Language with an attitude. In J. K. Chambers, Peter Trudgill and Natalie Schilling-Estes (eds),  
                        The Handbook of Language Variation and Change. Oxford: Wiley- Blackwell, pp. 40-66. 
                    
                    
                        Russ, B. (2012). 
                        Examining large-scale regional variation through online geotagged corpora. Presented at the 2012 American Dialect Society Annual Meeting.
                    
                
            
        
    

2524	2016	
        
            
                Introduction
                Can we find and track theory, especially literary theory, in very large collections of texts using computers? This panel discusses a pragmatic two-step approach to trying to track and then visually explore theory through its textual traces in large collections like those of the HathiTrust.
                
                    
                        Subsetting: The first problem we will discuss is how to extract thematic subsets of texts from very large collections like those of the HathiTrust. We experimented with two methods for identifying “theoretical” subsets of texts from large collections, using keyword lists and machine learning. The first two panel presentations will look at developing two different types of theoretical keyword lists. The third presentation will discuss a machine learning approach to extracting the same sorts of subsets.
                    
                    
                        Topic Modelling: The second problem we tackled was what to do with such subsets, especially since they are likely to still be too large for conventional text analysis tools like Voyant (voyant-tools.org) and users will want to explore the results to understand what they got. The fourth panel presentation will therefore discuss how the HathiTrust Research Center (HTRC) adapted Topic Modelling tools to work on large collections to help exploring subsets. The fifth panel talk will then show an adapted visualization tool, the Galaxy Viewer, that allows one to explore the results of Topic Modelling. 
                    
                
                The panel brings together a team of researchers who are part of the “Text Mining the Novel” (TMN) project that is funded by the Social Sciences and Humanities Research Council of Canada (SSHRC) and led by Andrew Piper at McGill University. Text Mining the Novel (novel-tm.ca) is a multi-year and multi-university cross-cultural study looking at the use of quantitative methods in the study of literature, with the HathiTrust Research Center is a project partner.
                The issue of how to extract thematic subsets from very large corpora such as the HathiTrust is a problem common to many projects that want to use diachronic collections to study the history of ideas or other phenomena. To conclude the panel, a summary reflective presentation will discuss the support the HTRC offers to DH researchers and how the HTRC notion of “worksets” can help with the challenges posed by creating useful subsets. It will further show how the techniques developed in this project can be used by the HTRC to help other future scholarly investigations.
            
            
                Using Word Lists to Subset
                Geoffrey Rockwell (Kevin Schenk, Zachary Palmer, Robert Budac and Boris Capitanu)
                How can one extract subsets from a corpus without appropriate metadata? Extracting subsets is a problem particular to very large corpora like those kept by the HathiTrust (www.hathitrust.org/). Such collections are too large to be manually curated and their metadata is of limited use in many cases. And yet, one needs ways to classify all the texts in a collection in order to extract subsets if one wants to study particular themes, genres or types of works. In our case we wanted to extract theoretical works which for the purpose of this project we defined as Philosophical works or Literary Critical works. In this first panel presentation we will discuss the use of keyword lists as a way of identifying a subset of “philosophical” texts.
                Why philosophical? We choose to experiment extracting philosophical texts first as philosophy is a discipline with a long history and a vocabulary that we hypothesized would lend itself to a keyword approach. Unlike more recent theoretical traditions, philosophical words might allow us to extract works from the HathiTrust going back thousands of years.
                
                    Keywords. For the first part of this project we adapted a list of philosophical keywords from the Indiana Philosophy Ontology Project (inpho.cogs.indiana.edu/). Our adapted list has 4,437 words and names starting with "abauzit", "abbagnano", "abdolkarim", "abduction", "abduh", "abel", and so on. There are a number of ways of generating such lists of keywords or features, in our case we were able to start with a very large curated list. The second paper in this panel discusses generating a list of literary critical keywords.
                
                
                    Process. We used this list with a process we wrote in Python that calculates the relative frequency of each word in a text and does this over a collection. The process also calculates the sum of the relative frequencies giving us a simple measurement of the use of philosophical keywords in a text. The word frequency process generates a CSV with the titles, author, frequency sum and individual keyword frequencies which can be checked and manipulated in Excel.
                
                
                    Testing. We iteratively tested this keyword approach on larger and larger collections. First we gathered a collection of 20 philosophical and 20 non-philosophical texts from Project Gutenberg (www.gutenberg.org/). We found the summed frequency accurately distinguished the philosophical from the non-philosophical texts. The process was then run by the HTRC on a larger collection of some 9,000 volumes and the results returned to us. We used the results to refine our list of keywords so that a summed relative frequency of .09 gave us mostly philosophical works with a few false positives. We did this by sorting the false positives by which words contributed to their summed relative frequency and then eliminating those words from the larger list that seemed to be ambiguous.
                
                The process was then run on the HathiTust Open Open collection of 254,000 volumes. This generated some 3230 volumes that had a summed relative frequency over .1, which seemed a safe cut-off point given how .09 had worked with a smaller collection. To assess the accuracy of this method we manually went through these 3,230 and categorized them using the titles producing a CSV that could be used with other classification methods.
                
                    
                
                The table below summarizes the categories of volumes that we found, though it should be noted that the categorization was based on the titles, which can be misleading. “Unsure” was for works which we weren’t sure about. “Not-Philosophical” were those works that we were reasonably sure were not philosophical from the title. The categories like Science and Education were for works about science and philosophy or education and philosophy.
                
                    
                        Tag (Type)
                        Number of Volumes
                        Example
                    
                    
                        Unsure
                        349
                        The coming revolution (1918)
                    
                    
                        Education
                        473
                        Education and national character (1904)
                    
                    
                        Philosophy
                        813
                        Outlines of metaphysics (1911)
                    
                    
                        Science
                        189
                        Relativity; a new view of the universe (1922)
                    
                    
                        Social
                        526
                        The study of history and sociology (1890)
                    
                    
                        Religion
                        722
                        Prolegomena to theism (1910)
                    
                    
                        Not Philosophical
                        158
                        Pennsylvania archives (1874)
                    
                
                One of the things that stands out is the overlap between religious titles and philosophical ones. This is not surprising given that the fields have been intertwined for centuries and often treat of the same issues. We also note how many educational works and works dealing with society can have a philosophical bent. It was gratifying to find only 4.9% of the volumes classified seemed clearly not philosophical. If one includes the Unsure category it is 15.7%, but the Unsure category is in many ways the most interesting as one reason for classifying by computer is to find unexpected texts that challenge assumptions about what is theory.
                
                    Conclusions. Using large keyword lists to classify texts is a conceptually simple method that can be understood and used by humanists. We have lists of words and names at hand in specialized dictionaries and existing classification systems. Lists can be managed to suit different purposes. Our list from InPhO had the advantage that is was large and inclusive, but also the disadvantage that included words like “being” and “affairs” that have philosophical uses but are also used in everyday prose. The same is true of the names gathered like Croce that can refer to the philosopher or the cross (in Italian). Further trimming and then weighting of words/names could improve the classification of strictly philosophical texts. We also need to look deeper into the results to find not just the false positives, but also the true negatives. In sum, this method has the virtue of simplicity and accessibility and in the case of philosophical texts can be used to extract useful, though not complete, subsets. 
                
            
            
                The Problem with Literary Theory
                Laura Mandell (Boris Capitanu, Stefan Sinclair, and Susan Brown)
                In this short paper, I describe adapting the word list approach developed by Geoffrey Rockwell for extracting a subset of philosophical texts from a large, undifferentiated corpus, to the task of identifying works of literary theory. The degree to which running the list of terms did in fact pull out and gather together works of literary criticism and theory is very high, despite potential problems with such an enterprise, which we discuss in this talk in detail.
                
                    
                        Developing the list of literary terms. Susan Brown and I decided to gather lists of literary terms. Susan initiated a discussion with the MLA about using terms from the 
                        MLA Bibliography but upon consideration these were in fact not at all what we needed: they classified subjects of texts as opposed to listing terms that would appear in those texts. I had recently spent some time learning about JSTOR’s new initiative in which sets of terms are created by what they call “SMEs”--Subject Matter Experts--and then used to locate articles all participating in an interdisciplinary subject. Their first foray is available in Beta: it gathers together all articles in no matter what field on the topic of Environmental Sustainabilty (labs.jstor.org/sustainability/). The terms collected are terms that would appear 
                        in the relevant texts, not in the metadata about them; the goal is to collect documents across multiple categories related to specialization, discipline, and field, since the desired result to gather together interdisciplinary texts concerning a common topic. 
                    
                    
                        Anachronism. JSTOR had started a “literary terms” list, and I finished the list of terms relying on encyclopedias of literary theory. Could a list of terms significant in the late-twentieth-century theories of literature as expressed in articles gathered in JSTOR be used to extract a set of texts published much earlier that analyze literature? What about the historical inaccuracy of using twentieth-century terms to find eighteenth- and nineteenth-century literary criticism?
                    
                
                Results:
                
                    
                
                In fact, results show solidly that this anachronistic list of terms developed by experts do work to gather materials that preceded and fed into, served to develop, the discipline of literary theory. One of two falsely identified texts among the top relevant documents has to do with water distribution systems which had, as part of its most frequent terms, “meter” and “collection,” two terms relevant to analyzing the medium and content of poetry. Other false positives are similarly explicable, and, most important, they are rare.
                In this paper, we report upon the effects of running these frequent words on very large datasets using both unsupervised to supervised learning.
            
            
                Machine Learning
                Stefan Sinclair (and Matthew Wilkens)
                The third panel presentation deals with machine learning techniques to extract subsets. Unsupervised learning techniques allow us to evaluate the relative coherence of theoretical clusters within large textual fields and to identify distinct theoretical subclasses in the absence of any firmly established anatomy of the discipline. For these reasons, we performed unsupervised classification on three corpora: (1) A large collection (c. 250,000 volumes) of mixed fiction and nonfiction published in the nineteenth and twentieth centuries. (2) A subset of that corpus identified by algorithmic and manual methods as highly philosophical. And (3) A subset similarly identified as literary-critical.
                In the case of the large corpus, the goal was to identify subsets containing high proportions of philosophy and criticism. For the smaller sets, we sought to produce coherent groupings of texts that would resemble subfields or concentrations within those areas. In each case, we extracted textual features including word frequency distributions, formal and stylistic measures, and basic metadata information, then performed both 
                    k-means and DBSCAN clustering on the derived Euclidean distances between volumes.
                
                As in past work on literary texts (Wilkens, 105), we found that we were able to identify highly distinct groups of texts, often those dealing with specialized and comparatively codified subdomains, and that we could subdivide larger fields with reasonable but lower accuracy. The model that emerges from this work, however, is one emphasizing continuity over clear distinction. Subfields and areas of intensely shared textual focus do exist, but a systematic view of large corpora in the philosophical and literary critical domains suggests a more fluid conception of knowledge space in the nineteenth and twentieth centuries.
                In parallel with the unsupervised classification performed – an attempt to allow distinctive features to emerge without, or with less, bias – we also performed supervised classification, starting with the training set of 40 texts labelled as Philosophical and Other (mentioned in "Using Word Lists to Subset" above). We experimented with several machine learning algorithms and several parameters to determine which ones seemed most suitable for our dataset. Indeed, part of this work was to recognize and and normalize the situation of the budding digital humanist confronting a dizzying array of choices: stoplists, keywords, relative frequencies, TF-IDF values, number of terms to use, Naïve Bayes Multinomial, 
                    Linear Support Vector Classification, penalty parameter, iterations, and so on ad infinitum. Some testing is desirable; some guesswork and some craftwork are essential. We reflect on these tensions more in the iPython notebook (Sinclair et al., 2016) and we will discuss them during the presentation as well.
                
                One of the surprises from these initial experiments in machine learning was that using an unbiased list of terms from the full corpus (with stopwords removed) was considerably more effective than attempting to classify using the constrained philosophical vocabulary. Again, this may be because the keywords list was overly greedy.
                
                    Just as we experimented with ever-larger corpora for the "Using Lists to Subset" sub-project, the supervised learning subproject broadened its scope gradually in an attempt to identify theoretical texts unknown to us while examining the efficacy of the methodologies along the way. Indeed, the overarching purpose of adopting all three approaches (keyword-based, unsupervised classification, machine learning) was to compare and contrast different ways of studying theory in a large-scale corpus.
                
            
            
                Working with HTRC datasets
                Boris Capitanu
                The fourth panel presentation focuses on working with the HathiTrust and the particular format of HathiTrust texts. Researchers may obtain datasets directly from HathiTrust [1] by making a special request, after having fulfilled appropriate security and licensing requirements. Datasets in HathiTrust and HTRC are available in two different ways:
                
                    via rsync in Pairtree format
                    via Data API
                
                According to “Pairtrees for Object Storage (V0.1)” [2], the Pairtree is "a filesystem hierarchy for holding objects that are located within that hierarchy by mapping identifier strings to object directory (or folder) paths, two characters at a time”. In the HathiTrust, the objects consist of the individual volume and associated metadata. Volumes are stored as ZIP files containing text files, one text file for each page, where the text file is named by the page number. A volume ZIP file may contain additional non-page text files, whose purpose can be identified from the file name. The metadata for the volume is encoded in METS XML [3] and lives in a file next to the volume ZIP file. For example, a volume with id “loc.ark:/13960/t8pc38p4b” is stored in Pairtree as:
                loc/pairtree_root/ar/k+/=1/39/60/=t/8p/c3/8p/4b/ark+=13960=t8pc38p4b/ark+=13960=t8pc38p4b.zip loc/pairtree_root/ar/k+/=1/39/60/=t/8p/c3/8p/4b/ark+=13960=t8pc38p4b/ark+=13960=t8pc38p4b.mets.xml
                where “loc” represents the 3-letter code of the library of origin (in this case Library of Congress). As mentioned, the volume ZIP files contain text files named for the page number. For example, here are the first few entries when listing the contents of the above ZIP file:
                 ark+=13960=t8pc38p4b/
                 ark+=13960=t8pc38p4b/00000001.txt
                 ark+=13960=t8pc38p4b/00000002.txt
                 ark+=13960=t8pc38p4b/00000003.txt
                 …
                Note that the strings that encode the volume id and the ZIP filename are different. Before a volume id can be encoded as a file name, it goes through a “cleaning” process that converts any character that is not a valid character to be used in a filename into one that is (for example “:” was converted to “+” and “/” to “=“), also dropping the 3-letter library code. The specific conversion rules are obscure, but library code already exists [4][5] for multiple languages that is able to perform this conversion both ways.
                The pairtree is an efficient structure for storing a large number of files. However, working with this structure can pose certain challenges. One of the issues is that this deeply nested folder hierarchy is slow to traverse. Applications needing to recursively process the volumes in a particular dataset stored in pairtree will have to traverse a large number of folders to “discover” every volume. A second inconvenience stems from the use of ZIP to store the content of a volume. While efficient in terms of disk space usage, it’s inconvenient when applications need to process the text data of the volume as they would need to uncompress the ZIP file and read its contents, in the proper order, concatenating all pages, in order to obtain the entire volume text content. A further complication is due to the fact that the exact ordering and naming of the page text files in the ZIP file is only provided as part of the METS XML metadata file. So, if the goal is to create a large blob of text containing all the pages of a volume (and only the pages, in the proper order, without any additional non-page data), the most correct way of doing so is to first parse the METS XML to determine the page sequence and file names, and then uncompress the ZIP file concatenating the pages in the exact sequence specified. This, of course, has a large performance penalty if it needs to be done on a large dataset every time this dataset is used to address some research question.
                An alternative way to obtain a particular dataset is to use the Data API [6]. Currently, access to Data API is limited, and is allowed only from the Data Capsule [7] while in Secure Mode. Using the Data API a researcher can retrieve multiple volumes, pages of volumes, token counts, and METS metadata documents. Authentication via the OAuth protocol is required when making requests to the Data API. The advantage of using the Data API in place of the pairtree (other than disk storage savings) is that one can request already-concatenated text blobs for volumes, and make more granular requests for token counts or page ranges without having to traverse deeply-nested folder structures or parse METS metadata.
                In this panel presentation we will show how the tools developed for the Trace of Theory project were adapted to work with the Pairtree format. The goal is to help others be able to work with the HathiTrust data format.
                Notes:
                [1] https://www.hathitrust.org/datasets
                [2] http://tools.ietf.org/html/draft-kunze-pairtree-01
                [3] http://www.loc.gov/standards/mets/
                [4] https://confluence.ucop.edu/display/Curation/PairTree
                [5] https://github.com/htrc/HTRC-Tools-PairtreeHelper
                [6] https://wiki.htrc.illinois.edu/display/COM/HTRC+Data+API+Users+Guide
                [7] https://wiki.htrc.illinois.edu/display/COM/HTRC+Data+Capsule
            
            
                Topic Modelling and Visualization for Exploration
                Susan Brown (Geoffrey Rockwell, Boris Capitanu, Ryan Chartier, and John Montague)
                When working with very large collections even subsets can be too large to manage with conventional text analysis tools. Further, one needs ways of exploring the results of extraction techniques to figure out if you got what you were expecting or something surprising in an interesting way. In the fifth panel presentation we will discuss the adaptation of a tool called the Galaxy Viewer for visualizing the results of Topic Modelling (Montague et al., 2015). Topic modeling is an automated text mining technique that has proven popular in the humanities that tries to identify groups of words with a tendency to occur together within the same documents in a corpus. Chaney and Blei explain that, “One of the main applications of topic models is for exploratory data analysis, that is, to help browse, understand, and summarize otherwise unstructured collections.” (Chaney et al., 2012)
                
                    
                
                The Galaxy Viewer prototype was developed to explore the results of topic modelling over large collections. It combines different views so that one can select topics, compare topics, explore the words in topics, follow topic tokens over time, and see the document titles associated with topics. In this presentation we will demonstrate the Galaxy Viewer and then discuss how it was scaled to handle much larger collections.
                The prototype Galaxy Viewer backend code uses Mallet (McCallum, 2002) to infer the set of topics, topic distributions per document, and word probabilities per topic. Unfortunately, Mallet is meant to be used on small- to medium-sized corpora as it requires that the entire dataset be loaded into RAM during training. An additional constraint with Mallet is the fact that although Mallet can fully utilize all the CPU cores on a single machine, it’s not designed to work in a distributed-computing fashion across a number of machines, to speed up execution. As such, processing very large datasets (if even possible) might take a very long time (as the algorithm makes multiple passes over the entire dataset). Many implementations of LDA exist, which primarily fall into one of two categories: Batch LDA, or Online LDA. The core difference between batch and online LDA stems from what happens during each iteration of the algorithm. In batch mode, as mentioned earlier, each iteration of the algorithm makes a full pass over all the documents in the dataset in order to re-estimate the parameters, checking each time for convergence. In contrast, online LDA only makes a single sweep over the dataset, analyzing a subset of the documents each iteration. The memory requirement for online LDA depends on the chosen batch size only, not on the size of the dataset - as is the case with batch LDA.
                We are currently in the process of researching/comparing the available implementations of LDA to establish which one would be best suited to use for the Galaxy Viewer. We are also considering the option of not fixing the LDA implementation, but instead make the backend flexible so that any LDA implementation can be used (as long as it provides the appropriate results that are needed). In the latter case we’d have to create specific result interpreters that can translate the output from the specific implementation of LDA to the appropriate format to be used to store in the database (to be served by the web service).
                Given that Topic Modeling results do not expose the textual content of the documents analyzed, and cannot be used to reconstruct the original text, they are safe to be publicly shared without fear of violating copyright law. This is great news for researchers working with collections like those of the HathiTrust as they should be able to gain insight into datasets which are still currently in-copyright and would, otherwise, not be available to be inspected freely.
                In the prototype Galaxy Viewer implementation, the output of the topic modeling step is processed through a set of R functions that reshape the data and augment it with additional calculated metrics that are used by the web frontend to construct the visualization. These post-processing results are saved to the filesystem as a set of five CSV files. One of these CSV files is quite large as it contains the topic modeling state data from Mallet (containing topic assignments for each document and word, and associated frequency count). The visual web frontend code loads this set of five files into memory when the interface is accessed the first time, which can take several minutes. For the prototype this approach was tolerated, but it has serious scalability and performance issue that needs to be addressed before the tool can be truly usable by other researchers.
                Scaling the Galaxy Viewer therefore consists of creating a web service backed with a (NoSQL) database which will service AJAX requests from the front-end for the data needed to construct the topic visualization and related graphs. We are developing the set of service calls that need to be implemented/exposed by the web service to fulfill the needs of the front-end web-app. The backend service will query the database to retrieve the necessary data to service the requests. The database will be created based on the output of the Topic Modeling process, after required post-processing of the results is completed (to calculate the topic trends, topic distances, and other metrics used in the display). Relevant metadata at the volume and dataset level will also be stored to be made available to the front-end upon request. This work will be completed by the end of December 2015 so that it can be demonstrated in the new year. The scaled Galaxy Viewer will then provide a non-consumptive way of allowing users of the HathiTrust to explore the copyrighted collections. Extraction of subsets and Topic Modelling can take place under the supervision of the HTRC and the results database can then be exposed to visualization tools like the Galaxy Viewer (and others) for exploration.
            
            
                Closing reflections: How “Trace of Theory” will improve the HTRC
                    . 
                
                J. Stephen Downie
                The HathiTrust Research Center exists to give the Digital Humanities community analytic access to the HathiTrust’s 13.7 million volumes. The HT volumes comprise over 4.8 billion pages each in turn represented by a high-resolution image file and two OCR files yielding some 14.4 billion data files! Thus, as the earlier papers have highlighted, the sheer size of the collection, along with the idiosyncratic nature of the HT data, together create several hurdles that impede meaningful analytic research.The HTRC is engaged in two ongoing endeavours designed to assist DH researchers in overcoming these obstacles: The Advance Collaborative Support (ACS) program [1]; and, the Workset Creation for Scholarly Analysis (WCSA) project [2]. 
                The ACS program at HTRC provides no-cost senior developer time, data wrangling assistance, computation time and analytic consultations to DH researchers who are prototyping new research ideas using the HT data resources. The ACS program is an integral part of the HTRC’s operation mission and was part of its value-added proposition when the HTRC launched its recent four-year operations plan (2014-2018). It is a fundamental component of the HTRC’s outreach activities and as such, has staff dedicated to its planning, management and day-to-delivery. The ACS team was responsible for creating, and then reviewing, the competitive ACS Request for Proposals (RFP) that ask interested DH researchers outline their intellectual goals, describe their data needs, and estimate their computational requirements. The ACS team is generally looking for new projects that could benefit from some kickstarting help from HTRC. HTRC welcomes proposals from researchers with a wide range of experience and skills. Projects run 6 to 12 months.
                Originally funded by the Andrew W. Mellon Foundation (2013-2015), the current WCSA program is building upon, extending and implementing the development made during the funding period. The HTRC project team, along with subaward collaborators at University of Oxford, University of Maryland, Texas Agriculture and Marine University and University of Waikato, developed a group of prototype techniques for empowering scholars who want to do computational analyses of the HT materials to more efficiently and effectively create user-specific analytic subsets (called “worksets”). A formal model has been designed to describe the items in a workset along with necessary bibliographic and provenance metadata that is now being incorporated into the HTRC infrastructure (Jett, 2015). 
                The Trace of Theory project was selected from the first round of ACS proposals. This concluding panel presentation will discuss in what ways the Trace of Theory project has been both a representative and a unique exemplar of the ACS program. It will present some emergent themes that evolved from the HTRC-Trace of Theory interactions that we believe will have an important influence on the delivery of future ACS projects. In the same manner, it will reflect upon the problems the team of researchers had in subsetting the data to build their necessary worksets along with the solutions that the HRTC-Trace of Theory collaboration developed to surmount those difficulties. The panel will finish with a summary of how HTRC intends to incorporate the lessons learned into its day-to-day operations as well as future ACS projects. 
                Notes:
                [1] The 2014 ACS RFP is available at: https://www.hathitrust.org/htrc/acs-rfp
                [2] https://www.lis.illinois.edu/research/projects/workset-creation-scholarly-analysis-prototyping-project
            
        
        
            
                
                    Bibliography
                    
                        Chaney, A. J. and Blei, D. M. (2012). Visualizing Topic Models, ICWSM. 
                        http://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/download/4645%26lt%3B/5021 (accessed Dec 2015).
                    
                    
                        Jett, J. (2015). Modeling worksets in the HathiTrust Research Center. CIRSS Technical Report WCSA0715. Champaign, IL: University of Illinois at Urbana-Champaign. Available via: http://hdl.handle.net/2142/78149 (accessed Dec 2015)
                    
                    
                        McCallum, A. K. (2002). MALLET: A Machine Learning for Language Toolkit, http://mallet.cs.umass.edu (accessed Dec 2015).
                    
                    
                        Montague, J., Simpson, J., Brown, S., Rockwell, G. and Ruecker, S. (2015). Exploring Large Datasets with Topic Model Visualization. Paper presented by Montague at DH 2015 at the University of Western Sydney, Australia.
                    
                    
                        Sinclair, S., G. Rockwell and the Trace of Theory Team. (2016). Classifying Philosophical Texts. Online at 
                        http://bit.ly/1kHBy56 (accessed Dec 2015). 
                    
                    
                        Wilkens, M. (2016). Genre, Computation, and the Weird Canonicity of Recently Dead White Men. NovelTM Working Paper.
                    
                
            
        
    

2528	2016	
        
            
                Introduction
                Computer-assisted text analysis is now witnessing the phenomenon of ever-growing computer power and, more importantly, an unprecedented aggregation of textual data. Certainly, it gives us an unique opportunity to see more than our predecessors, but at the same time it presents non-trivial challenges. To name but a few, these include information retrieval, data analysis, classification, genre recognition, sentiment analysis, and many others. It can be said that, after centuries of producing textual data and decades of digitisation them, the scholars now face another great challenge - that of beginning to make good use of this treasure.
                Generally speaking, the problem of large amounts of textual data can be perceived from at least three different perspectives. Firstly, there is a need of asking new research questions that would take advantage of thousands of texts that can be compared. Secondly, one has to introduce and evaluate statistical techniques to deal with vast amounts of data. Thirdly, there is a need of new computational algorithms that would be able to handle enormous corpora, e.g. containing billions of tokens, in a reasonable amount of time. The present study addresses the third of the aforementioned issues.
                Stylometric techniques are known for their high accuracy of text classification, but at the same time they are usually quite difficult to be used by, say, an average literary scholar. In this paper we present a general idea, followed by a fully functional prototype of an open stylometric system that facilitates its wide use with respect to two aspects: technical and research flexibility. The system relies on a server installation combined with a web-based user interface. This frees the user from the necessity of installing any additional software. Moreover, we plan to enlarge the set of standard stylometric features with style-markers referring to various levels of the natural language description and based on NLP methods.
            
            
                Multi-aspectual Document Representation
                Computing word frequencies is simple for English, but relatively complicated for highly inflected languages, e.g. Polish, with many word forms, resulting in data sparseness. Thus, it might be better first to map the inflected forms onto 
                    lemmas (i.e. basic morphological forms) with the help of a morpho-syntactic tagger, and next to calculate the lemma frequencies.
                
                Most frequent words or lemmas as descriptive features proved to be useful in authorship attribution. However, for some text types or genres they do not provide sufficient information to tell the authors apart, e.g. see (Rygl, 2014). Moreover, in other types of classification tasks, where the goal is to trace signals of individual style, literary style or gender, it usually turns out that they appear on different levels of the linguistic structures. Thus, one needs to enhance text description.
                In practice, every language tool introduces errors. However, if the error level is relatively small and the errors are not systematic (i.e. their distribution is not strongly biased), than the results of such a tool can be still valuable for stylometric analysis. Bearing this in mind, we have evaluated a number of language tools for Polish, and selected types of features to be implemented:
                
                    length of: documents, paragraphs or sentences (a segmentation tool),
                    morphological features
                        
                            word forms or tokens and punctuation marks,
                            pseudo-suffixes (last several letters),
                            lemmas (from WCRFT2 morpho-syntactic tagger for Polish (Radziszewski, 2013))
                        
                    
                    grammatical classes
                        
                            35 grammatical classes defined in the tagset (Szałkiewicz and Przepiórkowski, 2012) of the Polish National Corpus (Przepiórkowski et al., 2012), e.g. pseudo-past participle, non-past form, ad-adjectival adjective; recognised by WCRFT2,
                            parts of speech (by grouping grammatical classes),
                            grammatical categories, e.g. gender, number, person, etc.; (WCRFT2),
                        
                    
                    sequences
                        
                            lemma sequences (e.g. potential collocations),
                            sequences of grammatical classes (bigrams or trigrams - hints about the grammatical structures),
                        
                    
                    semantic features
                        
                            semantic 
                                Proper Name classes – recognised by a Named Entity Recogniser Liner2 (Marcińczuk, 2013),
                            
                            
                                lexical meanings – represented by synsets in plWordNet (the Polish wordnet); assigned by Word Sense Disambiguation tool WoSeDon (Kędzia, et al., 2015)
                            
                            generalised lexical meanings – more general meanings from plWordNet, e.g. 
                                 an animal instead of 
                                a cheetah,
                            
                            formalised concepts from a formal ontology SUMO that plWordNet is mapped to,
                            lexicographic domains from wordnet.
                        
                    
                
                Semantic features go beyond a typical stylometric description, but allow for crossing borders between the style and the content description.
                There are no features overtly describing the syntactic structure, as the available parsers for Polish express too high level of errors. The set of features can be further expanded by user-defined patterns expressed in the WCCL language (Radziszewski et al., 2011) that can be used to recognise binary relations between words and their combinations.
                WebSty allows for testing the performance of the aforementioned features in different stylometric tasks, several case-studies will be presented on a set of Polish novels.
            
            
                Processing and Results
                The proposed system follows a typical stylometric workflow which was adapted to the chosen language tools and other components of the architecture (see  Section 4).
                
                    Uploading a corpus of documents together with meta-data in CMDI format (Broeder et al., 2012) from the CLARIN infrastructure.
                    Choosing the features for the description of documents – done by the users (see Fig. 1).
                    Setting up the parameters for processing (users).
                    Pre-processing texts with the help of language tools.
                    Extracting the features from the pre-processed texts.
                    Calculating feature values.
                    Filtering and/or transforming the original feature values.
                    Clustering the feature vectors representing documents.
                    Extracting features that are characteristic for different clusters.
                    Presenting the results: visualisation or export of data.
                
                
                    
                    Fig.1 Choice of features GUI
                
                The step 5 can be performed as: simple counting of words or lemmas, processing and counting annotations matching some patterns or running patterns for every position in a document. This is why a dedicated feature extraction module comes into stage, namely 
                    Fextor (Broda et al., 2013).
                
                Filtering and transformation functions can be built into the clustering packages (see below) or implemented by the dedicated systems, e.g. 
                    SuperMatrix system (Broda and Piasecki, 2013).
                
                The WebSty architecture can be relatively easy adapted to the use of any clustering package. The prototype is integrated with 
                    Stylo – an elaborated clustering package dedicated to stylometric analysis, and 
                    Cluto – a general clustering package (Zhao and Karypis, 2005). Stylo offers very good visualisation functionality, while 
                    Cluto delivers richer possibilities for formal analysis of the generated clusters. The obtained results are displayed by the web browser (see Fig. 2). Users can also download log files with formalised description of the clustering results.
                
                
                    
                    Fig.2 Stylometric results
                
                Moreover, features that are characteristic for the description of individual clusters or differentiation between clusters can be identified.  Ten different functions (implemented in Weka
                    
                         http://www.cs.waikato.ac.nz/ml/weka/
                     (Witten et al., 2011) and SciPy packages
                    
                         http://www.scipy.org/
                    ), based on mathematical statistics and information theory, are offered. The ranking lists of features are presented on the screen for interactive browsing (Fig. 3) and can be downloaded.
                
                The system has a web-based user interface that allows for uploading input documents from a local machine or from a public repository and enables selecting a feature set, as well as options for a clustering algorithm.
                
                    
                    Fig.3 Browsing significant features identified for the extracted clusters
                
            
            
                Technical Architecture
                Application of language tools is inevitably more complex than calculating statistics on the level of words or letters. Moreover, processing of Polish is mostly far more complex than English (in terms of the processing time and memory used). Thus, higher computing power and bigger resources are required. In order to cope with this, the entire analysis in WebSty is performed on a computing cluster. Users do not need to install any software - which might be non-trivial particularly in the case of the language tools. The system processes the documents using a parallel and distributed architecture (Walkowiak, 2015). 
                The workflow is as follows. Input documents are processed in parallel. The uploaded documents are first converted to an uniform text format. Next, each text is analysed by a part-of-speech tagger (we use WCRFT2 for Polish (Radziszewski, 2013)) and then it is piped to a name entity recognizer - Liner2 (Marcińczuk et al., 2013) in our case. After the annotation phase is completed for all texts, the feature extraction module comes into stage, i.e. Fextor (Broda et al., 2013).  Clustering tools requires input data in different formats: sparse or dense matrices, text (ARRF, Cluto format) or binary files (R objects, Python objects). Thus data received from the feature extraction for each input file has to be unified  and converted. The extracted raw features can be filtered or transformed by a range of methods inside the clustering packages or in a system for distributional semantics called SuperMatrix (Broda and Piasecki, 2013).  Finally, the R package Stylo (Eder et al., 2013) or a text clustering tool called Cluto (Zhao and Karypis, 2005) are run to perform exploratory analysis, e.g. multidimensional scaling.
                To prevent the system from overloading and long response time the input data size is limited to 20 files. However, large text collections can be processed, if they are deposited in the dSpace repository.
                    
                        
                            https://clarin-pl.eu/dspace/
                        
                     All corpora in dSpace can be annotated stored for further processing. Therefore, it is only left to run feature extraction and clustering tools inside WebSty.
                    
                         http://ws.clarin-pl.eu/demo/cluto2.html
                    
                
            
            
                Conclusion and future plans
                The paper presented opened, web-based system for exploring stylometric structures in Polish document collections. The web based interface and the lack of the technical requirements facilitates the application of text clustering methods beyond the typical tasks of the stylometry, e.g. analysis of types of blogs (Maryl, 2012), recognition of the corpus internal structure, analysis of the subgroups and subcultures, etc.
                The system is currently focused on processing Polish. However, as the feature representation is quite language independent, we plan to add converters for for other languages. 
            
        
        
            
                
                    Bibliography
                    
                        Broda, B., Kędzia, P., Marcińczuk, M., Radziszewski, A., Ramocki, R. and Wardyński, A. (2013). Fextor: A feature extraction framework for natural language processing: A case study in word sense disambiguation, relation recognition and anaphora resolution. In Przepiórkowski, A., Piasecki, M., Jassem, K., Fuglewicz, P. (Eds.), Computational Linguistics: Applications, Series: 
                        Studies in Computational Intelligence, Vol. 458, Springer Berlin Heidelberg, pp. 41-62.
                    
                    
                        Broda, B. and Piasecki, M. (2013). Parallel, Massive Processing in SuperMatrix – a General Tool for Distributional Semantic Analysis of Corpora. 
                        International Journal of Data Mining, Modelling and Management, 5(1): 1–19.
                    
                    
                        Broeder, D., Van Uytvanck, D., Gavrilidou, M., Trippel, T. and Windhouwer, M. (2012). Standardizing a component metadata infrastructure. In: Calzolari, N., Choukri, K., Declerck, T., Doğan, M.U., Maegaard, B., Mariani, J., Moreno, A., Odijk, J., Piperidis, S. (Eds.), 
                        Proceedings of LREC 2012: 8th International Conference on Language Resources and Evaluation. European Language Resources Association (ELRA), pp. 1387-90.
                    
                    
                        Eder, M., Kestemont, M. and Rybicki, J. (2013). Stylometry with R: a suite of tools. In: 
                        Digital Humanities 2013: Conference Abstracts. University of Nebraska-Lincoln, NE, pp. 487-89.
                    
                    
                        Kędzia, P., Piasecki, M. and Orlińska, M. J. (2015). Word Sense Disambiguation Based on Large Scale Polish CLARIN Heterogeneous Lexical Resources. Cognitive Studies | Études cognitives, 15: 269-92.
                    
                    
                        Marcińczuk, M., Kocoń, J. and Janicki, M. (2013). Liner2 - A Customizable Framework for Proper Names Recognition for Polish. In Bembenik, R., Skonieczny, Ł., Rybiński, H., Kryszkiewicz, M., Niezgódka, M., Intelligent Tools for Building a Scientific Information Platform, Series: 
                        Studies in Computational Intelligence, Springer: Berlin Heidelberg, 467: 231-53.
                    
                    
                        Maryl, M. (2012). Kim jest pisarz (w internecie?). Teksty Drugie, 6: 77-100.
                    
                    
                        Przepiórkowski, A., Bańko, M., Górski, R. L. and Lewandowska-Tomaszczyk, B. (Eds.),(2012). 
                        Narodowy Korpus Języka Polskiego. Warszawa: PWN.
                    
                    
                        Radziszewski, A. (2013). A tiered CRF tagger for Polish, In Bembenik, R., Skonieczny, Ł., Rybiński, H., Kryszkiewicz, M., Niezgódka, M., Intelligent Tools for Building a Scientific Information Platform, Series: 
                        Studies in Computational Intelligence. Springer Berlin Heidelberg, 467: 215-30.
                    
                    
                        Radziszewski, A., Wardyński, A., and Śniatowski, T. (2011). WCCL: A morpho-syntactic feature toolkit. In Habernal, I. and Matoušek, V. (Eds.), Text, Speech and Dialogue, Plzen, Springer: Berlin Heidelberg, LNAI 6836, pp. 434–41.
                    
                    
                        Rygl, J. (2014) Automatic Adaptation of Author’s Stylometric Features to Document Types, In Sojka, P., Horák, A., Kopeček, I. and Pala, K. (Eds.), 
                        Proceedings of 17th International Conference TSD 2014. Brno, Czech Republic, LNCS 8655, Springer: Berlin Heidelberg, pp. 53-61.
                    
                    
                        Szałkiewicz, Ł. and Przepiórkowski, A. (2012). Anotacja morfoskładniowa. In Przepiórkowski et al., pp. 59-96.
                    
                    
                        Walkowiak, T. (2015). Web based engine for processing and clustering of Polish texts. In Zamojski, W., Mazurkiewicz, J., Sugier, J., Walkowiak, T., Kacprzyk, J., Proceedings of the Tenth International Conference on Dependability and Complex Systems DepCoS-RELCOMEX, Brunów, Poland, Series: Advances in Intelligent Systems and Computing Springer, Springer Berlin Heidelberg, pp. 515-22.
                    
                    
                        Witten, I. H., Frank, E. and Hall, M. A. (2011). Data Mining: Practical Machine Learning Tools and Techniques, Third Edition. Series in Data Management Systems, Morgan Kaufmann.
                    
                    
                        Zhao, Y. and Karypis, G. (2005). Hierarchical Clustering Algorithms for Document Datasets. Data Mining and Knowledge Discovery, 
                        10(2): 141-68.
                    
                
            
        
    
2544	2016	
        
            
                Introduction: Quantitative movie analysis
                Film studies make use of both, qualitative as well as quantitative methods (Korte, 2004). While there is a large variety of qualitative approaches to analyze movies (cf. e.g. Monaco, 2009; Sikov, 2010), most quantitative attempts seem to be focused on the analysis of the length and frequency of a film’s shots, which are understood to be the “single definable elements which can be nominated and described” (Salt, 2006: 14). After Barry Salt’s
                    
                         For a concise review of Barry Salt’s work on quantitative movie analysis cf. Buckland (2008).
                     seminal article “Statistical Style Analysis of Motion Pictures” appeared in 1974, numerous other quantitative studies were to follow
                    
                         For a comprehensive overview of Cinemetrics-related research cf. the bibliography compiled by Mike Baxter, available online at http://www.cinemetrics.lv/dev/bibliography_with_essay_Baxter.pdf (accessed 3 March 2016).
                    . “Cinemetrics” (Tsivian, 2009) has been suggested as a term to describe these quantitative, shot-based approaches for analyzing movies. Cinemetrics is also the name of a large online database that contains information about shot lengths and frequencies for several thousand films
                    
                         http://www.cinemetrics.lv (accessed 3 March 2016).
                    . 
                
                Studies that take into account quantifiable parameters other than shots are, however, rather rare. Among the few examples are Hoyt et al. (2014), who describe a tool that can be used to visualize relations between the characters of a film. 
                    Another example can be found in Ewerth et al. (2009), who present a toolkit that allows researchers to automatically detect shots and camera motion, superimposed text, faces and audio signals. While the latter example is rather focused on the automatic annotation of quantitative features, other projects, such as Lev Manovich’s (2013) „Visualizing Vertov“, focus on the presentation and visualization of quantitative parameters.
                
                In this paper, we suggest to enhance the existing, shot-focused approaches to quantitative movie analysis, by considering additional parameters, such as language (cf. Forchini, 2012) and color use (cf. Flückinger, 2011). We present a prototype that can be used to automatically extract and analyze these parameters from movies and that makes the results accessible in an interactive visualization. 
            
            
                A prototype for the analysis of language and color use in movies
                Language use as well as the use of colors in movies have long been known in the area of qualitative film studies. We argue, that these parameters are equally suited for a quantitative approach and present an experimental prototype that can be used to quantify the language and color used in different movies. Much like Clement et al. (2008), who discuss the drawbacks and opportunities of computer-based methods in the field of literary studies, we believe that digital, quantitative movie analysis tools can be helpful in “offering provocations, surfacing evidence, suggesting patterns and structures, or adumbrating trends”. As the prototype allows researchers to investigate potential correlations between color usage and corresponding language in a movie, it can be used to examine questions such as the following:
                
                    Are there characteristic patterns in color or language use for movies from different eras, genres, or directors (e.g. dark colors and words such as “kill” or “blood” in horror movies)?
                    Are there characteristic color patterns within a film that correlate with the occurrence of certain characters or objects (e.g. bright colors whenever the hero is speaking)?
                    Are there characteristic color patterns within a film that correlate with the sentiment of the language (e.g. dark colors for language with negative sentiment)?
                
                
                    Obtaining language and color data
                    Machine-readable instances of movie language can be obtained fairly easy in the form of subtitle files, which are freely available via sites such as OpenSubtitles
                        
                             www.opensubtitles.org (accessed 3 March 2016).
                        ; for a precompiled corpus of subtitles also cf. Tiedemann (2012). The standard file structure of such subtitles contains a timestamp as well as a transcription of the actual dialog fragments. 
                    
                    Information about color usage can be extracted directly from the movie itself, by cutting the digital movie into single frames and by calculating the most frequent colors for each frame (color histograms). 
                
                
                    Analyzer component
                    Our prototype comprises an analyzer and a viewer component. The analyzer can be used to extract single frames from a movie by using the open source tool FFmpeg
                        
                             https://www.ffmpeg.org/ (accessed 3 March 2016).
                        . We used a K-means Cluster algorithm (Wu, 2012) to group together similar RGB values in each frame, as the actual variation of distinct RGB values is too high to allow for any kind of meaningful, quantitative interpretation. The analyzer also processes the subtitle file of a movie and uses Python NLTK
                        
                             http://www.nltk.org/ (accessed 3 March 2016).
                         to perform basic POS tagging, as we are mainly interested in how nouns correlate with certain colors. We used the Python library TextBlob
                        
                             http://textblob.readthedocs.org/ (accessed 3 March 2016).
                         to perform a simple sentiment analysis for each of the adjectives, tagging them with a polarity score between -1 (negative) and +1 (positive). After the analysis, each frame is saved as a JPG file; all quantitative data is stored in a JSON file. 
                    
                
                
                    Viewer component
                    The viewer component uses this data to generate an interactive HTML page that can be viewed in any recent web browser. A popular visualization of the most frequent colors that occur in a movie can be found in the MovieBarcodes
                        
                             http://moviebarcode.tumblr.com (accessed 3 March 2016).
                         project. In a MovieBarcode, each frame of a movie is skewed to be only one pixel wide; all frames are then lined up in a row that looks very much like a colored barcode. On the overview page of our tool, all movies that have been analyzed before are rendered in a MovieBarcode visualization, together with information of the four most frequent colors in the movie (cf. Fig. 1). This view can be used to compare various movies with each other from a more distant perspective. 
                    
                    
                        
                        Figure 1: Overview of analyzed films in a MovieBarcode visualization (“The Lion King”, top; “True Detective, season 1, episode 1”, middle; “True Detective, season 1, episode 2”, bottom). 
                    
                    By clicking on one of the MovieBarcodes, the tool zooms into the respective movie and renders different kinds of information in a more detailed view (cf. Fig. 2). 
                    
                        
                        Figure 2: Detailed view with MovieBarcode, sentiment graph and noun distribution.
                    
                    Below the MovieBarcode appears a sentiment graph that aggregates a score between -1 and +1 for each dialog. In the bottom row, the most frequent nouns are displayed. All different types of information are also aligned to the time axis of the movie. The visualization is fully interactive, i.e. by hovering over one of the frames in the MovieBarcode, or a node in the sentiment graph or the noun distribution, the corresponding frame and subtitle appear as an overlay. The complete movie can also be navigated back and forth by means of the arrow keys.
                
            
            
                Conclusion and future directions
                We believe the real strength of a quantitative approach that makes use of language and color information lies in a mix of “distant watching” (cf. Howanitz, 2015) and close watching, i.e. characteristic language-color patterns identified in specific movies can be used as a query to search other movies for similar patterns. Our next steps will therefore go into the direction of an information system that allows researchers to search and compare a collection of movies according to language and color characteristics.
            
        
        
            
                
                    Bibliography
                    
                        Buckland, W. (2008). What Does the Statistical Style Analysis of Film Involve? A Review of Moving into Pictures. More on Film History, Style, and Analysis. 
                            Literary and Linguistic Computing, 23(2): 219-30.
                    
                    
                        Flückiger, B. (2011). Die Vermessung ästhetischer Erscheinungen. 
                        Zeitschrift für Medienwissenschaft, 5(2): 44–60. 
                    
                    
                        Forchini, P. (2012). 
                        Movie language revisited. Evidence from multi-dimensional analysis and corpora. Bern et al.: Peter Lang Verlag. 
                    
                    
                        Clement, T., Steger, S., Unsworth, J. and Uszkalo, K. (2008). How not to read a million books. http://people.brandeis.edu/~unsworth/hownot2read.html (accessed 3 March 2016).
                    
                    
                        
                            Hoyt, E.
                            , 
                            Ponot
                            , K
                            .
                             and
                             Roy, C. (2014). Visualizing and Analyzing the Hollywood Screenplay with ScripThreads. 
                            Digital Humanities Quarterly, 8(4). http://www.digitalhumanities.org/dhq/vol/8/4/000190/000190.html 
                        (accessed 3 March 2016).
                    
                    
                        
                            Howanitz, G. (2015). Distant Waching: Ein quantitativer Zugang zu YouTube-Videos.
                             Digital Humanities im deutschsprachigen Raum (Dhd) 2015: Conference Abstracts. Graz, pp. 33-39. http://gams.uni-graz.at/o:dhd2015.abstracts-gesamt 
                        (accessed 3 March 2016).
                    
                    
                        
                            Korte, H. (2004). 
                            Einführung in die Systematische Filmanalyse. Berlin: Erich Schmidt Verlag.
                        
                    
                    
                        
                            Manovich, L. (2013). Visualizing Vertov. 
                            Softwarestudies.com. http://softwarestudies.com/cultural_analytics/Manovich.Visualizing_Vertov.2013.pdf 
                        (accessed 3 March 2016).
                    
                    
                        
                            Monaco, J. (2009). 
                            How
                            to
                             Read a Film: Movies, Media, and 
                            Beyond. Oxford (NY): Oxford University Press.
                        
                    
                    
                        
                            Salt, B. (2006). 
                            Moving
                            into
                             Pictures. More on Film 
                            History
                            , Style, and Analysis. London: Starword Publishing.
                        
                    
                    
                        
                            Salt, B. (1974). Statistical Style Analysis of Motion Pictures. 
                            Film Quarterly, 28(1): 13-22.
                        
                    
                    
                        
                            Sikov, E. (2010). 
                            Film Studies. An Introduction. New York: Columbia University Press.
                        
                    
                    
                        
                            Tiedemann, J. (2012). Parallel Data, Tools and Interfaces in OPUS. Proceedings of the 8th International Conference on Language Resources and Evaluation
                             (LREC)
                             2012. Istanbul, pp. 2214-18.
                        
                    
                    
                        
                            Tsivian, Y. (2009). Cinemetrics, Part of the Humanities’ Cyberinfrastructure. In Ross, M., Grauer, M. and Freisleben, B. (eds.), 
                            Digital Tools in Media Studies – Analysis and Research. An Overview. Bielefeld: tanscript Verlag, pp. 93-100.
                        
                    
                    
                        
                            Ewerth, R., Mühling, M., Stadelmann, T., Gllavata, J., Grauer, M. and Freisleben, B. (2009). Videana: A Software Toolkit for Scientific Film Studies. In Ross, M., Grauer, M. and Freisleben, B. (eds.), 
                            Digital Tools in Media Studies – Analysis and Research. An Overview. Bielefeld: tanscript Verlag, pp. 100-16.
                        
                    
                    
                        
                            Wu, J. (2012). 
                            Advances
                             in K
                            ‑
                            means
                             Clustering: A Data Mining 
                            Thinking. Berlin and Heidelberg: Springer-Verlag.
                        
                    
                
            
        
    
2579	2016	
        
            
                Finnegans Wake is the last, most mysterious book by the Irish writer James Joyce. Usually described as a novel, it is a fascinating text written primarily in English (more strictly Hiberno-English), but the words are often fused with any of several dozen languages (McHugh, 1991). This dream-like narrative features a Dublin pub owner Humphrey Chimpden Earwicker, his wife Anna Livia Plurabelle, their twin sons Shem and Shaun and their daughter Issy. They travel through space and time to discover the truth about a scandalous incident in Phoenix Park in which HCE was implicated and to deliver a letter written by ALP in his defense. Drawn into a whirlpool of the past, they metamorphose into historical and legendary figures, a hill, a river, a cloud, a tree and a stone (Campbell and Robinson, 1944/1959; Bishop, 1986). The story of HCE’s fall overlaps with the story of a drunken bricklayer Tim Finnegan who fell off a ladder but was resurrected when whisky splashed on his face during a fight at his wake. This fuses with the original Fall, with sexual falls of politicians and celebrities, with Napoleon and Wellington on the battlefield of Waterloo, with Tristan and Iseult’s romance, a hen discovering an ancient manuscript in a rubbish heap, and more. Full of sexual innuendoes, historical, literary, autobiographical allusions and hilarious wordplays, multiple plots of 
                Finnegans Wake develop in non-linear ways and can be followed like a maze, or a hypertext (Hart, 1962; Hayman, 1978; Loska, 1999 and 2000; Armand, 2003; Bazarnik, 2011).
            
            In 
                The Middle Ages of James Joyce. The Aesthetics of Chaosmos Umberto Eco offers a diagram that visualizes hypertextual complexity of the Wakean language (Eco, 1989). It shows how MEANDERTALE and MEANDERTHALLTALE, two of innumerable puns making up the textual labyrinth of 
                Finnegans Wake, can be unpacked into separate words. The image represents a network connecting major components: 
                meander, German 
                Tal, 
                tale, and 
                Neandertal, which combine to suggest a cluster of meanings. This Wakean pun nudges us about how to read the book: as a “tall tale” wandering waywardly, looping backward and flashing forward, into the pre-historic past, and the origins of the human species. And as a watercourse – of course, ‘the Meander’ is a river which (giving us the word) meanders. Working on the logic of associations, it hints at different interpretative paths, visualised by the wavy lines of the diagram. 
            
            In our project – 
                First We Feel Then We Fall – we aspire to offer a similar, dynamic, visual translation of hypertextuality and simultaneity of 
                Finnegans Wake  (Joyce, 1939/2002). Inspired by Eco’s diagram, our project is based on a comparable analysis of narrative streams in Joyce’s text. Having analysed the Wakean imagery, euphonies, rhythms and polyphonic contexts, we have selected four narrative strands, or “plots,” which are translated into an up-to-date audiovisual form of an interactive Internet app. Thus networks of linguistic, historical, symbolic, and mathematical meanings entailed in Wakean puns are transposed into a dynamic audio-visual structure that the audience can co-shape in the process of interactive viewing. The viewer can switch at will between four simultaneous streams of film clips accompanied by sound (and optional captions with the 
                FW text). The interactive and immersive nature of 
                First We Feel Then We Fall will go beyond previous cinematic adaptations of Joyce’s novel. It is the advance of digital technologies that has enabled us to approach complexity of 
                Finnegans Wake in this novel way.
            
            The application devised for this project is a multichannel, interactive video app. It will enable the viewers to experience Joyce’s text in an audiovisual format consisting of simultaneously running streams, and thanks to this, offer them a portmanteaux-like audio-visual experience. The app will also contain an option suggesting a narrative path modelled on selections made by previous viewers. It will be available on the Internet browsers and mobile devices (Wróblewski, 2015). The software used includes: Final Cut for editing; Adobe After Effects and Processing for animation, and DaVinci Resolve for colour grading. Video capture was made with Canon 5D Mark III, Sony A7SII, Sony FX7e, Phantom HD Gold, and Found Footage, whereas the website was developed with Python, Django, JavaScript, HTML 5, and CSS3. 
            In our presentation we will briefly describe the rationale for our multimedia adaptation of 
                Finnegans Wake, and let the audience experience it in an individual, interactive viewing on an available mobile device and/or a computer.
            
        
        
            
                
                    Bibliography
                    
                        Armand, L. (2003). 
                        Technē. 
                        James Joyce, Hypertext and Technology. Prague: Karolinum.
                    
                    
                        Bazarnik, K. (2007). Joyce, Liberature and Writing of the Book. 
                        Hypermedia Joyce Studies, 8(2). 
                        http://hjs.ff.cuni.cz/archives/v8_2/main/essays.php?essay=bazarnik (accessed 4 March 2016).
                    
                    
                        Bazarnik, K. (2011). 
                        Joyce and Liberature. Prague: Litteraria Pragensia.
                    
                    
                        Bishop, J. (1986). 
                        Joyce’s Book of the Dark, FINNEGANS WAKE. Madison, Wisc.: The University of Wisconsin Press.
                    
                    
                        Campbell, J. and Robinson, H. M. (1944/1959). 
                        A Skeleton Key to Finnegans Wake. London: Faber and Faber.
                    
                    
                        Eco, U. (1989). 
                        The Aesthetics of Chaosmos: The Middle Ages of James Joyce. Trans. E. Esrock and D. Robey. London: Hutchinson Radius.
                    
                    
                        Hart, C. (1962). 
                        Structure and Motif in FINNEGANS WAKE. London: Faber.
                    
                    
                        Hayman, D. (1978). Nodality and the Infra-Structure of FINNEGANS WAKE. James Joyce Quarterly, 16(1/2): 135-50.
                    
                    
                        Joyce, J. (1939/1989). 
                        Finnegans Wake. London: and Faber.
                    
                    
                        Joyce, J. (1939/2002). Finnegans Wake. Theall J. B. and Theall D. F. (eds).A Webified version of James Joyce's 
                         Finnegans Wake. Sd2cx1.webring.org. 
                        http://sd2cx1.webring.org/l/rd?ring=jamesjoyce;id=4;url=http%3A%2F%2Fwww.trentu.ca%2Ffaculty%2Fjjoyce%2Ffw.htm (accesed 18 June 2015).
                    
                    
                        Loska, K. (1999). 
                        Wokół FINNEGANS WAKE. James Joyce i komunikacja wizualna. Kraków: Wydawnictwo Uniwersytetu Jagiellońskiego.
                    
                    
                        Loska, K. (2000). 
                        Finnegans Wake: James Joyce a rozumienie i interpretacja. Kraków: Rabid.
                    
                    
                        McHugh, R. (1991). 
                        Annotations to 
                        Finnegans Wake. Revised ed. Baltimore: John Hopkins UP.
                    
                    
                        Wróblewski, J. (2015). 
                        First We Feel Then We Fall teaser #1. 
                        Adaptation of James Joyce’s Finnegans Wake by J. Wróblewski and K. Bazarnik.[video] 
                        https://vimeo.com/137422246 (accessed 31 Oct 2015).
                    
                
            
        
    

2602	2016	
        
            Figurative language poses a challenge to Natural Language Processing systems, while being a ubiquitous phenomenon that is deeply ingrained in every-day language. As corpus studies suggest, figurative language devices appear on average in every third sentence of general-domain text (Shutova, 2015), thus making the development of automatic recognition and interpretation systems play an important role in many text mining use cases, especially those aiming for a deeper semantic understanding of texts. Furthermore, acknowledging the pervasiveness of such language forms - and of the emblematic device of metaphor in particular - allows for a change in perspective, not conceiving it as a merely rhetorical device but as a genuinely cognitive mechanism that manifests itself in language in the form of surface metaphorical expressions. Such surface expressions usually follow a directionality principle common in figurative language which is to project one domain of experience (the source, e.g. war) onto another (the target, e.g. argument), the first one typically being more concrete and the second one being more abstract. Taken together, such surface expressions (e.g. She shot down all of my arguments) consitute a conceptual metaphor argument is war, a cognitive phenomenon that can be studied through its expression in language. This line of thought has been established as Conceptual Metaphor Theory (Lakoff and Johnson, 1980), by now a widely adopted and empirically grounded approach (e.g. Gibbs, 2008) that has opened up a interdisciplinary field of research, not least with involvement from computational linguistics. 
            Analysing metaphorical language use from a (cognitive) anthropologic and psycholinguistic point of view has various possible applications: The approach qualifies for research questions from the fields of critical discourse analysis, media studies, and philosophy, as it sheds light on a collective subconscious, encompassing ideological subtexts, and maybe even pre-discursive existential territories (Guattari, 2008) as traced out in late 20th century philosophy. Another area of application is text classification in literary studies: Found metaphorical expressions and conceptual mappings can be used as features to describe the relative similarity of observed texts and thus lend themselves to genre identification and authorship attribution (Lodge, 1988). 
            The research described here takes up this theoretical framework and builds upon a computational metaphor identification and aggregation approach as proposed by Shutova and Sun (2013). Unsupervised machine learning, namely a hierarchical soft clustering strategy known as Hierarchical Graph Factorization Clustering (HGFC), is employed to build up a graph of concepts that reflects aggregate metaphorical mappings. Using conceptual metaphor as a unit of observation allows for a sensible aggregation and tracing of surface metaphorical expressions in large scale corpora, and in this case is also used to follow diachronic developments in a corpus of historical German literature. Furthermore, as a correlate of cognitive processes it should provide an empirically grounded access to the conceptual systems, e.g. cultural and moral models, of examined texts and their times. 
            The main idea of the approach is to cluster nouns - which are taken to be concepts - according to their selectional preferences, that is, "the tendency for a word to semantically select or constrain which other words may appear in a direct syntactic relation with it" (Roberts and Egg, 2014). In the resulting clustering, figurative language use becomes visible as violation of the most frequent selectional preferences representing the normal, non-figurative case. It is an approach that determines the metaphorical in relation to the normal, which also entails that a sufficient amount of non-metaphorical language use needs to be present in the data. In the case of a diachronic corpus of literature that means to balance the corpus using historical dictionaries and encyclopaedias in order to introduce more prosaic language use. 
            The dataset is drawn from a large text collection (The Digital Library, 2016) and contains up to 1700 German novels from the early 16th up to the beginning of the 20th century. Preprocessing consists of POS-tagging, lemmatization, and dependency parsing, allowing for an extraction of nouns and their corresponding verbs according to certain grammatical relations - subject, direct object, and indirect object relations. Verbal constructions are only one type of realization, but they do cover a significant part of metaphors usually encountered in the wild. Furthermore, it should be straightforward to generalize the approach in order to include adjectival constructions and similes, which would allow to cover most of the possible metaphorical expressions. Preprocessing is performed using a modular pipeline (Jannidis et al., forthcoming), tailored to the processing of book-length German texts. Subsequently, a number of most frequent nouns (e.g. 2000) and corresponding verbs are extracted. The verbs act as features for the concept clustering and can come from various sources, not necessarily the same corpus as the most frequent nouns. This could be used as a way to introduce balancing text types into the model, without altering the concept graph as derived from the literary corpus. The resulting noun-verb feature matrix is then normalized for each noun vector to sum to 1 and the Jensen-Shannon divergence between pairs of noun vectors is used as a measure to calculate the similarity matrix (the initial concept graph). 
            With the similarity matrix in place, clustering methods can be applied in order to generate a suitable tree of concepts. Different approaches were tested at this point (using implementations from Python machine learning library scikit-learn, cf. Pedregosa et al., 2011): 1) connectivity-based or agglomerative clustering, which includes average, complete, and - the baseline from Shutova and Sun (2013) - Ward linkage 2) density-based clustering, namely DBSCAN and HDBSCAN, and 3) for subspace-based methods, spectral clustering, as well as spectral bi- and co-clustering. Results where manually reviewed and an internal evaluation measure, the silhouette coefficient, was used to assess the quality of generated clusterings. Results indicate that in this setting, spectral clustering performs very similar to the baseline, while the other methods produce clusterings of inferior quality. This exploration of readily available methods shed some light onto the requirements for unsupervised metaphor identification and aggregation. In addition, tests with balancing and pruning were performed on smaller development corpora: Solely using encyclopedias produces a model that contains mostly synonym and antonym relations but no metaphorical mappings. Similarly, models consisting only of literary texts can lack non-figurative uses for concepts. What can also be observed is that the balancing leads to deeper models, e.g. concepts accumulate more features and aggregate better. 
            To give an intuition, example clusters from the baseline results on a subset of 383 novels are reproduced here, showing the top ten features for each concept: 
            IDEAS ARE FOOD
            education / bildung (10): geben-dobj beanspruchen-dobj taxieren-dobj voraneilen-subj überstrahlt-dobj ausspräch-subj nahestehen-subj heraustreiben-dobj ermangelnd-dobj abschöpfen-dobj 
            memory / erinnerung (48): geben-dobj wachzurufen-dobj stören-dobj mahnen-dobj aufgrischen-dobj verlöschen-subj wiederzuerwecken-dobj neubeleben-dobj frischen-dobj hervorschießen-subj 
            hunger / hunger (10): geben-dobj erweren-dobj büssen-dobj schaben-subj überhen-iobj verschmachten-dobj stärkern-dobj bittern-subj hinausgetreiben-subj trainieren-iobj 
            EMOTIONS ARE PLANTS
            flower / blume (47): pflücken-dobj liegen-subj lieben-dobj begießen-dobj welken-dobj duften-dobj duftet-subj durchhauten-subj hingesenken-subj erblüht-dobj 
            emotion / gefühl (90): liegen-subj ersticken-dobj abstumpfen-dobj hervorraufen-dobj halten-iobj entspinnen-dobj hinausdehnen-dobj aufwekken-dobj anhielen-subj arten-subj 
            In principle, cluster labels are manually assigned using categories from Lakoff’s master metaphor list (Lakoff et al., 1991). Such is the case with the first example, IDEAS ARE FOOD, while the second one, EMOTIONS ARE PLANTS, is not present in the list and was created to appropriately describe the cluster. 
            Pending work includes testing HGFC and providing means to include metadata for modeling the diachronicity of the data. Considering the time span covered by the corpus, some orthographic and lexical variation will have to be handled, either by use of a specialized spelling normalization system or a more rigoros treatment such as stemming. It can be noted that HGFC combines some of the characteristics exhibited by the surveyed approaches and running it on the full size corpus will significantly improve on the baseline in terms of the amount of metaphorical expressions and conceptual mappings induced. The system will be evaluated using either a small gold standard of annotated sample sentences or manually compiled conceptual mappings in a confined domain (e.g. using Lakoff et al., 1991), which should give some indication of its precision in the domain of historical German literary texts.
        
        
            
                
                    Bibliography
                    
                        Gibbs, R. W. (2008). Metaphor and Thought. The State of the Art. In Gibbs, R.W. (Ed), 
                        The Cambridge Handbook of Metaphor and Thought. Cambridge University Press, pp. 3-14. 
                    
                    
                        Guattari, F. (2008). The Three Ecologies. Continuum.
                    
                    
                        Jannidis, F., Reimers, N., Vitt, T., Pernes, S. and Pielström, S. (forthcoming). DARIAH-DKPro-Wrapper Output Format (DOF) Specification. DARIAH-DE Working Papers.
                    
                    
                        Lakoff, G. and Johnson, M. (1980). Metaphors We Live by. University of Chicago Press.
                    
                    
                        Lakoff, G., Espenson, J. and Schwartz, A. (1991). The Master Metaphor List. University of California at Berkeley.
                    
                    
                        Lodge, D. (1988). The Modes of Modern Writing: Metaphor, Metonymy, and the Typology of Modern Literature. University of Chicago Press.
                    
                    
                        Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V. and Vanderplas, J. (2011). Scikit-learn: Machine learning in Python. 
                        The Journal of Machine Learning Research, 12: 2825-30.
                    
                    
                        Roberts, W. and Egg, M. (2014). A Comparison of Selectional Preference Models for Automatic Verb Classification. 
                        Proceedings of EMNLP 2014, pp. 511-22.
                    
                    
                        Shutova, E. (2015). Design and Evaluation of Metaphor Processing Systems. 
                        Computational Linguistics, 41(4): 579-623.
                    
                    
                        Shutova, E. and Sun, L. (2013). Unsupervised Metaphor Identification Using Hierarchical Graph Factorization Clustering. 
                        Proceedings of NAACL-HLT 2013, pp. 978–88. 
                    
                    
                        The Digital Library (2016). The Digital Library in TextGrid. https://textgrid.de/en/digitale-bibliothek
                    
                
            
        
    

2603	2016	
        
            Cet exposé a pour vocation de présenter un projet en cours qui associe les méthodes traditionnelles des sciences humaines (philologie, histoire et histoire des idées) aux approches computationnelles et statistiques. La création et la mise en place du logiciel d’analyse textuelle MACHIATO (Lasson et Manchio, 2015) constitue le point de convergence de deux perspectives de recherche initialement séparées, celle d’un chercheur en informatique spécialiste de la formalisation mathématique et d’une doctorante en études italiennes, travaillant sur la correspondance diplomatique et administrative de Machiavel (
                Legazioni e Commissarie, Scritti di governo, 2002-2012). La mise en place du site internet relève donc en premier lieu d’une coopération, terme central pour tout chercheur en sciences humaines et sociales qui entreprend de se lancer dans un projet relevant des humanités numériques. En effet, la rencontre entre des champs disciplinaires, implique un constant pour créer un langage commun pour appréhender les textes, pour identifier le type de questionnements qu’il est possible de soumettre aux machines et pour dégager les potentialités et les limites de certaines procédures d’automatisation.
            
            
                1. Question de méthodes
                Nous avons suivi un cheminement complexe depuis la philologie traditionnelle, entendue comme combinaison de critique littéraire, historique et linguistique ou comme pratique d’établissement de texte à partir de sources différentes jusqu’à la philologie dite numérique. L’école qui a le plus influencé nos travaux est celle de Jean-Claude Zancarini et Jean-Louis Fournel et leur approche de ce qu'ils appellent « philologie politique ». Ils se sont attachés à historiciser la pensée de Machiavel, et plus généralement cette génération de la guerre, à travers une analyse minutieuse de son usage des mots dans le cadre de la traduction des textes politiques majeurs de la période des guerres d'Italie. Or, le fait de revenir à chaque usage pour parvenir à une élucidation sémantique est d’autant plus ardu que le corpus est important : pour exploiter nos 2214 lettres, nous avons donc rapidement pensé à croiser notre méthode initiale avec les potentialités des méthodes computationnelles et statistiques, qui nous ont ainsi permis de multiplier les points de vue sur le texte (diachronique, synchronique), de modifier l’unité de base de l’analyse (lemme, champ lexical, champ sémantique) et dégager les tendances générales du corpus et les spécificités de sous-corpus. De telles possibilités modifient inévitablement le rapport aux textes du chercheur en SHS qui ne peut parvenir à l’interprétation des résultats qu’à condition d’en comprendre le sens et les enjeux et donc, de suivre une formation (notamment en statistique et en visualisation de données). 
            
            
                2. Les mots et les nombres: machiato
                
                Le logiciel est commandé par une interface Web qui confère à l'utilisateur un accès facile aux ressources du programme avec son navigateur. Les analyses et les pré-calculs statistiques sont faits par un back-end mis en œuvre avec le langage de programmation Python dont le rôle est d'initialiser la base de données. Nous utilisons le Framework 
                    open-source Django pour générer du contenu HTML. Enfin, nous utilisons le langage de programmation Javascript pour afficher les données et les résultats côté utilisateur. Le corpus a tout d’abord été normalisé : différents types d’entrée permettent de le découper en fonction de nos besoins. L’index des lettres permet d’accéder à chaque missive en obtenant les indications de base sur chacune d’elles ; l’index des occurrences donne accès à chacune des 25 007 formes, permettant de cibler des graphies particulières, des noms propres ou des hapax) ; l’index des 6399 familles de mots (qui rassemble toutes les flexions d’un vocable, très nombreuses pour certains verbes du fait de leur emploi et de l’instabilité graphique très forte) consent de mesurer le poids d’un champ lexical. En outre, à chaque missive ont été associés son destinataire, sa date et son lieu de rédaction. Ce 
                    tagging préliminaires nous a ensuite donné la possibilité de formuler de nouvelles questions plus complexes et de croiser différentes variables (à titre d’exemple, quel terme est le plus employé lorsque Machiavel se trouve en mission auprès du roi Louis XII ?). 
                
                Nous avons repris une large part des outils proposés dans les logiciels d’analyse textuelle existants et utilisés par les linguistes à l’instar des concordances, particulièrement utiles pour dégager rapidement les nombreux homographes de notre corpus, et des cooccurrences indispensables pour repérer les cas de polysémie, voire de double isotopie typiques de la langue machiavélienne. Nous avons en outre emprunté un modèle d’analyse des cooccurrences multiples (Heiden, 2004) et les principes de mesure et de comparaison développés en statistique textuelle par Lebart et Salem. Les différents types de calcul permettent de décrire de façon toujours plus précise non seulement la nature des usages, mais aussi leur poids et les relations qui les lient les unités du texte entre elles. Nous utilisons trois types de calcul : les fréquences, l’indice de dispersion et l’indice de spécificité qui ont en outre permis de dépasser certains reproches récurrents faits aux méthodes computationnelles (dont celui de ne considérer la langue comme un ensemble d’unités abstraites, sans lien et décontextualisées). Nous avons fait le choix de limiter les types de visualisation pour des raisons de cohérence, de temps et de compétence. Nous proposons d’en donner quelques exemples tels que l’histogramme de répartition des fréquences, celui des graphies, les structures arborescentes (permettant de représenter tous les environnements immédiats en une image) et les histogrammes interactifs représentant la chronologie des missives. Toutes les visualisations de données sont exportables : nous utilisons Excel pour les tables, ainsi que le format SVG pour les données graphiques. Les critères de recherches avancées (permettent d’effectuer des recherches complexes en fonction du nombre d’occurrences ou/et de la fréquence et/ou des indices de répartition et de dispersion). À partir de l’index des lettres, il est aussi possible de questionner un sous-corpus particulier en vertu de critères variés tels que les bornes chronologiques ou la nature des missives (lieu de rédaction). Il est aussi possible de sélectionner un sous-corpus spécifiques et de procéder à une recherche précise en activant (ou non) les filtres sur les occurrences suivants : taille, nombre d’occurrences dans le sous-corpus interrogé, nombre d’occurrences dans l’ensemble du corpus, fréquence dans le sous-corpus, fréquence dans le corpus total, nombre de lettres, indice de dispersion et indice de spécificité. Enfin, nous avons ajouté une fonctionnalité pour comparer deux sous-corpus. 
            
            
                3. Premiers résultats
                Le premier résultat que nous ayons obtenu est la réalisation du site internet en lui-même, à savoir la création d’une interface dynamique donnant accès aux textes et consentant de les interroger en fonction de différentes variables. En second lieu, plusieurs types de visualisation permettent de représenter l’évolution (dans une perspective globale) ou les rythmes (en fonction des lieux ou du contexte historique et politique de rédaction) des usages langagiers. Les données et leur visualisation permettent de vérifier ou d’écarter certaines thèses que nous avions postulées initialement mais elles ouvrent également la voie à une nouvelle approche des textes qui implique de nuancer certains phénomènes que nous avions identifiés au niveau local. Les tendances de l’écriture qui était invisibles apparaissent alors. L’analyse systématique des relevés d’occurrences et des mesures effectués mettent ainsi en exergue certaines micro-spécificités de la langue telles que le recours constant à la forme hypothétique et aux métaphores de l’incompréhension pour exprimer l’incertitude face aux événements en cours et à venir. L’examen des affinités lexicales a confirmé cette thèse et a permis de mettre au jour une caractéristique déterminante de notre corpus : l’extrême instabilité du monde politique pendant les guerres d’Italie, qui s’exprime à la fois par la centralité du champ sémantique du conflit (qui semble commander les autres champs sémantiques) et par la présence de différents éléments langagiers traduisant une précarité des temps et une fragilisation de la praxis politique.
            
        
        
            
                
                    Bibliography
                    
                        Blumenthal, P. and Hausmann F. J. (éd). (2006). 
                        Collocations, corpus, dictionnaires. Langue française, pp. 150. 
                    
                    
                        Burdick, A., Drucker, J., Lunenfeld, P., Presner, T. and Scnapp, J. (2012). 
                        Digital humanities. Massachusetts Institute of Technology.
                    
                    
                        Genet, J. P. and Zorzi, A. (2011).  
                        Les historiens et l’informatique : un métier à réinventer. Rome: École française de Rome.
                    
                    
                        Guillot, C., Heiden, S., Lavrentiev, A., Marchello-Nizia, C. and Rainsford, T. (2013). 
                        La « philologie numérique » : tentative de définition d’un nouvel objet éditorial du point de vue des linguistes. 27e Congrès international de philologie et de linguistique romanes. 
                        https://halshs.archives-ouvertes.fr/halshs-00846767. 
                    
                    
                        Heiden, S. (2004). Interface hypertextuelle à un espace de cooccurrences: implémentation dans Weblex. In Purnelle, G. Fairon, C., Dister, A. (eds),
                        JADT 2004 - Le poids des mots – Actes des 7ème Journées Internationales d’Analyse Statistique des Données Textuelles. Presses Universitaires de Louvain, pp. 577-88.
                    
                    
                        Lafon, P. (1980). Sur la variabilité de la fréquence des formes dans un corpus. 
                        Mots. n.1 
                        http://www.persee.fr/web/revues/home/prescript/article/mots_0243-6450_1980_num_1_1_1008. 
                    
                    
                        Lasson, M. and Manchio C. (2015). Measuring the Words: Digital Approach to the Official Correspondence of Machiavelli. In Francesca Tomasi, Roberto Rosselli Del Turco, and Anna Maria Tammaro, (eds), 
                        Humanities and Their Methods in the Digital Ecosystem. Proceedings of the Third AIUCD Annual Conference (AIUCD2014). Selected papers. ACM: New York. 
                        http://dl.acm.org/citation.cfm?doid=2802612.2802643. 
                    
                    
                        Lebart, L. and Salem, A. (1994). 
                        Statistique textuelle. Paris: Dunod. 
                        http://lexicometrica.univ-paris3.fr/livre/st94/st94-tdm.html. 
                    
                    
                        Machiavelli, N. Edizione Nazionale delle Opere di Niccolò Machiavelli, 
                        Legazioni. Commissarie. Scritti di governo. Roma: Salerno : t. I (1498-1500), Marchand, J. J. (eds), 2002 ; t. II (1501-1503), Fachard, D. et Cutinelli-Rèndina, E. (eds), 2003 ; t. III (1503-1504), Marchand et Melera-Morettini, M. (eds), 2005 ; t. IV (1504-1505), Fachard et Cutinelli-Rèndina éd., 2006 ; t. V (1505-1507), Marchand, Guidi, A. et Melera-Morettini (eds), 2009 ; t. VI (1507-1510), Fachard et Cutinelli-Rèndina (eds), 2011 ; t. VII (1510-1527), Marchand, Guidi et Melera-Morettini (eds), 2012.
                    
                    
                        Mayaffre, D. (2007). 
                        Philologie et/ou herméneutique numérique: nouveaux concepts pour de nouvelles pratiques?
                        http://www.revue-texto.net/Parutions/Livres-E/Albi-2006/Mayaffre.pdf. 
                    
                    
                        Pincemin, B. (2012). Sémantique interprétative et textométrie. 
                        Corpus. 10 | 2011. 
                        http://corpus.revues.org/2121. 
                    
                    
                        Schöch, C. (2012). Nouvelles configurations : textes, outils, méthodes, et infrastructures de recherche dans les études de lettres. 
                        Configuration(s).
                        https://hal.archives-ouvertes.fr/hal-00951518/document. 
                    
                    
                        ThatCamp. (2011).  
                        Manifeste des Digital humanities. 
                        http://tcp.hypotheses.org/318. 
                    
                    
                        Zancarini, J. C. (2007). Une philologie politique. 
                        Laboratoire italien, pp. 7. 
                        http://laboratoireitalien.revues.org/132.
                    
                
            
        
    

2607	2016	
        
            Visualizations have a central role in the Digital Humanities. The second most popular author-chosen topic word of DH2015 was visualization (cf. Figure 1 from Weingart, 2015). Yet, when one revisits all accepted abstracts at DH2015 with the keyword “vis(z)ualization”, one may notice that not many of these texts indicate which libraries were used for interactivity (only some mention D3), and even fewer had direct links to their websites for testing (many were prototypes). 
            
            
                
                Figure 1. Fragment from graphic with the topical coverage of DH2015 (Weingart, 2015)
            
            One of the more common visualizations are those of relational data. In Katrien Verbert’s more thorough survey of interactive visuals in DH2014's presentations, relations-visualizations represent 50% of all prototypes (that is 29 out of 58). The most popular way to represent relationships are uni- or bi-directional graphs (23 out of 29) and only one of them used a matrix to display connections (see figure 2 from Verbert, 2015). To crack open the discussion on the pros and cons of this visualization technique, I will show how I answered some questions of cultural history, more specifically of Latin American Literary History, with a tailored interactive matrix (to this the visualizations visit: 
                http://www.sgutierrez.seewes.de/).
            
            
                
            
            Figure 2. Visualization techniques used by work presented at DH14 (Verbert, 2015)
            Theoretical Framework
            The history of associations is a goldmine for the intellectual queries of scholars interested in literary and intellectual history. In Latin America, for instance, the appearance of Angel Rama's posthumous book 
                The lettered city (1984), lead to a series of studies concerned with the constitution of enlightened groups, especially in new nation-states. As the capital city of one of the most powerful ex-Spanish Colonies, the lettered network in Mexico City makes an interesting case study. However, despite the valuable monographic studies on this subject (most notably Perales Ojeda, 2000 and Sánchez, 1951), which register around 200 active literary societies during the 19
                th century, no overview on the subject has been possible and not all questions have been resolved. How diverse or homogeneous were these groups? Who were their most recurrent actors? Were certain generations more likely to be part of groups from a certain literary movement? I will propose a way of using visual and interactive displays of literary societies’ membership data to answer these three questions.
            
            Before me, others have sought to gain new insights by exploring the possibilities of data modeling to understand modern sociability. The 
                Berliner Klassik Gesseligkeit Datenbank (The Societies Database of the Berlin Classical Period, 2013) aimed to understand the cultural bloom of the early 19
                th century and Stanford’s 
                Salons Project (2012) was designed to get an understanding of the social composition of the French Enlightenment network. However, to date, there are no online dynamic visualizations of either of these projects.
            Methodology
            a) data collection
            The network's information was obtained by scrapping each associations' entry in the Encyclopedia of Mexican Literature (ELEM). Since ELEM is the most complete source of biographical data for 19th century Mexican writers, it is very unlikely that information about these writers can be found elsewhere; thus, I only considered members with an entry in this source. This procedure leaves out many characters, but it is at least representative of the known characters of the lettered city. It contains information of 51 associations (founded between 1808 and 1894) and of 195 members born between 1781 and 1870.
            b) data model
            The database derived from these two nodes (members and associations) was modelled to answer my research questions, but its metadata is designed to be reusable: members were assigned standard identifiers using Jeff Chiu's VIAF reconciliation service for OpenRefine (Chiu, 2015), and neutral aspects about these nodes -- such as birth and death dates or founding and closing dates—were included. In addition to these neutral aspects, I added two categories that scholars have used to cluster literary characters and societies, namely, generations and literary movements.
            c) visualization
            My first attempt was to follow the most common visualization for networks in the digital humanities, the Gephi-spaguetti (see figure 3). I did everything I could to enhance readability. I set the societies-nodes’ size according to the number of connections they had with other associations and the thickness of the edges to vary depending on how many common members two groups had. Even more, in order to get a chronologically-ordered layout I used Spekkink’s useful plugin, the Event Graph Layout (Spekkink, 2014). From this display, I was able to confirm kinship-relationships between societies. That is, that although persistence was never their 
                forte, when one looks at the number of members that went from an extinct society to the next new one, one gets the impression that despite the ephemeral nature of these groups, there was still a type of continuity among them. 
            
            
                
            
            Figure 3. Network visualization where nodes are 19th century Mexican societies and edges represent the number of common members between them
            Yet, even when I created an interactive graph with Sigma.js it was very hard to read the quantitative differences between my nodes’ connections. On the one side, I was interested in creating a visual display that allowed interactivity, providing end-users with both additional information for each data-point and the possibility to select specific ranges of the network. On the other, I wanted to control the order of my data and the quantities’ color-coding for readability. The solution was provided by a Python-library, Bokeh.
            Results
            The first visual I created was a co-occurrence matrix where each literary association was compared against all others. This display allowed me to understand how many members each pair of associations had in common. In order to enhance the identification of meaningful co-occurrences, I followed the principles of sequential color schemes –where low data values are represented by light colors and high values by dark ones (Wyssen, 2014) – and I assigned different colors and alphas according to the quantities’ distribution of my data: associations’ pairs above or equal to five common members were coded in red, and below five, in blue (see Figure 4). Additionally, I set different and consecutive alpha values to each glyph according to their exact value (intersections of less density had lower alphas). This display was helpful to address the question on the diversity or homogeneity of literary societies: with this tailored visual I was able to identify the homogenous hub of ten literary associations around the 
                Liceo Hidalgo that had a considerable amount of common members, suggesting that although they had different approaches they were nonetheless constituted by recurring members (cf. Figure 5).
            
                
            
            Figure 4. Co-occurrence matrix of literary societies ordered by the sum of common members with other associations
            
                
            
            Figure 5. Selection of societies with the highest common-members’ density
            
                
            
            Figure 6. Associations’ co-occurrence matrix by founding date
            Moreover, when I changed the order of the matrix (by founding date, see Figure 6) I was able to understand these connections in its temporal dimensions. For instance, when zooming on the glyphs for the 
                Liceo Hidalgo (see Figure 7) one can easily identify the previous societies with which this association had enough common members to consider them as predecessors, or which other later groups could be considered as successors for the same reasons. 
            
            
                
            
            Figure 7. Liceo Hidalgos’ co-occurrences, a box-selection of the associations’ matrix by founding date
            Finally, in another color coding of the glyphs (by the literary movement that was in vogue when these societies were established) I could identify which societies of the same period had more common members (see Figure 8). 
            
                
            
            Figure 8. Societies’ co-occurrence matrix with literary movements’ color-coding.
            Conversely, I created another matrix –this time comparing members— which was useful to understand which characters co-occurred more often in the same associations and thus address the second question, namely, which were the most recurrent actors in the network (see Figure 9). The result: thirteen characters formed the core of actors who were most involved (see Figure 10). This information, however, could have been obtained with a simple bar-chart. The difference in perspective that this matrix offers is that it allows the user to see that these characters were not only in many but also similar associations (which can be retrieved by hovering the glyphs), and, additionally, it makes evident how proportionally small this core is when compared to the whole matrix.
            
                
            
            Figure 9. Members’ co-occurrence matrix ordered by maximum summed values.
            
                
            
            Figure 10. Members’ co-occurrence snapshot done with the selection tool of Bokeh’s visuals.
            Finally, to address the second question –namely, the correlation between generations and literary movements–, I created a matrix where associations were ordered by founding date on the y-Axe, and members by birth date on the x-Axe, and where the colors were coded according to their correspondent literary movement (see Figure 11). The dark colors of the glyphs represent the literary movement of a given society (all the blue ones are from the neoclassic movement, for instance), and the light colors in the background represent the members’ generations (for example, in light orange -in a vertical division- are all the members of the 
                Renacimiento generation). Arranging them like this enabled me to take snapshots of different societies and observe the generations’ patterns of membership-adscription. For instance, I could note that although the group formed around the 
                Renacimiento magazine was heavily constituted by its homonym generation (see Figure 12), almost half of its members were born in the timeframe of the previous generation (coded with a light yellow background).
            
            
                
            
            Figure 11. Generations versus literary movements: a co-occurrence matrix
                           
                
            
            Figure 12. Active members in 
                Grupo de la Revista el Renacimiento
            
            Conclusions
            In this paper I have shown how customized visualization of modeled data can enable new readings and lead to new understandings of how societies were formed in a key period of national history. Among other things, matrices help us “see” connections between previous categories of literary history (like generations and literary movements), between societies, but also between members, thus supporting new narratives of the lettered city were the alleged homogeneity of this “elite” group can be seen in a nuanced perspective that integrates complexity without sacrificing abstraction.
        
        
            
                
                    Bibliography
                    Chiu, J. (2015). An OpenRefine Reconciliation Service That Queries VIAF. Java https://github.com/codeforkjeff/refine_viaf.
                    Perales Ojeda, A. (2000).  Asociaciones literarias mexicanas: siglo XIX. 2nd ed. (Al siglo XIX. Ida y vuelta). México: Universidad Nacional Autónoma de México.
                    Rama, A. (1984).  La ciudad letrada. Hanover, N.H., U.S.A.: Ediciones del Norte.
                    Sánchez, J. (1951).  Academias y sociedades literarias de México. University of North Carolina.
                    Spekkink, W. (2014).  Event Graph Layout Wouter Spekkink. http://www.wouterspekkink.org/?page_id=93 (accessed 20 October 2015).
                    Verbert, K. V. K. L. (2015).  On the Use of Visualization for the Digital Humanities. Sydney, Australia http://dh2015.org/abstracts/xml/VERBERT_Katrien_On_the_Use_of_Visualization_for_t/VERBERT_Katrien_On_the_Use_of_Visualization_for_the_Dig.html (accessed 15 December 2015).
                    Weingart, S. (2015). Acceptances to Digital Humanities 2015, (part 2). The Scottbot Irregular http://www.scottbot.net/HIAL/?p=41347 (accessed 23 January 2016).
                    Wyssen, J. (2014). How We Created Color Scales, Website Datavisualization.ch http://datavisualization.ch/inside/how-we-created-color-scales/ (accessed 14 September 2014).
                    (2012). The Salons Project Mapping the Republic of Letters. http://republicofletters.stanford.edu/casestudies/salons.html (accessed 12 November 2014).
                    (2013). Berliner Klassik Geselligkeit-Datenbank Website Berliner Klassik Datenbanken. http://berlinerklassik.bbaw.de/BK/geselligkeit/Suche.html (accessed 25 February 2016).
                
            
        
    

2615	2016	
        
            In questo lavoro descriviamo un progetto di ricerca, attualmente in corso, dedicato all’analisi dell’universo linguistico e semantico della musica rap, con particolare attenzione rivolta alla realtà italiana. L’obiettivo del lavoro è quello di arrivare ad offrire una mappatura panoramica, una “distant reading” della lingua usata dal rap italiano. 
            La scelta di questo genere è motivata dal fatto che il rap è tra i fenomeni più vitali e dal maggiore impatto socioculturale della musica e delle sottoculture giovanili degli ultimi decenni (Lena, 1995; Toop, 1999; Forman and Neal, 2004; Pinkney, 2007), esteso ormai ben oltre gli originari confini statunitensi per divenire fenomeno globale (Androutsopoulos and Arno 2003; Osumare, 2007; Alim et al., 2008) e all’interno del quale è possibile riscontrare una ricchissima produzione testuale ed un alto tasso di innovazione e sperimentazione di forme linguistiche (Cutler, 2007; Bradley, 2009; Terkourafi, 2010).
            L’idea alla base del lavoro è quella di ottenere una “cartografia” della lingua del rap, che permetta di osservare e analizzare nel suo complesso un settore della produzione culturale contemporanea estremamente diffuso e popolare anche in Italia (Pacoda 1996; Filippone and Papini, 2002; Attolino, 2003; Scholz, 2005). In questo lavoro focalizziamo l’attenzione principalmente sulla dimensione testuale del rap piuttosto che su quella musicale, pur trattandosi di un genere in cui il rapporto tra parola e ritmo è inestricabile (Bradley, 2009). In ogni caso, è possibile affermare che la componente testuale nel rap occupa un ruolo centrale e che la specificità del vocabolario, dei temi, della capacità di invenzione linguistica nonché l’importanza dell’aspetto narrativo (Attolino, 2012) fanno dei testi del rap un corpus linguistico interessante da analizzare in sè.   
            A tal fine, piuttosto che soffermare l’attenzione su di un numero limitato di testi da analizzare in profondità, mettiamo in campo una metodologia di lavoro multidisciplinare - in cui convergono web data-mining, linguistica e information design – con l’obiettivo di giungere alla costruzione di un database testuale molto ampio da sottoporre ad analisi mediante strumenti di text-mining e di linguistica computazionale e da rendere esplorabile mediante una serie di visualizzazioni interattive elaborate ad-hoc.  
            In una prima fase si è proceduto all’individuazione di alcune web-repository contenenti le trascrizioni dei testi delle canzoni rap in lingua italiana. Non essendo le fonti ufficiali (siti personali degli artisti, siti delle etichette, libretti dei CD, ecc.) particolarmente ricche di informazioni, sono stati individuati alcuni popolari siti di text-sharing, dove fan e ascoltatori forniscono spontaneamente le proprie trascrizioni dei testi degli artisti.
            Sulle fonti selezionate è stato addestrato uno script di web-scraping, sviluppato appositamente, in grado di estrarre, per ogni brano presente sul sito, il testo e i meta-dati di riferimento (titolo brano, nome autore, collaborazioni, album). Una volta addestrato lo script si è passati alla fase di estrazione dati vera e propria che ha portato alla costruzione di un database di circa quindicimila brani. Il risultante database è stato poi sottoposto ad una prima fase di pre-processing e data-wrangling per renderlo disponibile all’analisi successiva. Sul testo estratto dal web è stata effettuata una profonda ripulitura con metodi semi-automatici in modo da ottenere un corpus omogeneo di testi trattabili computazionalmente. 
            Alla fase di estrazione e standardizzazione del dataset segue la fase di analisi linguistica. In questa fase teniamo conto di alcuni studi precedenti condotti nell’ambito del MIR - Music Information Retrieval, in particolare quelli rivolti all’analisi automatica dei testi delle canzoni (Mahedero et al., 2005; Kleedorfer et al., 2008; Hu et al., 2009) e dei testi rap in particolare (Hirjee and Brown, 2009; Hirjee and Brown, 2010; Malmi et. al, 2015).
            Il corpus è processato usando l’intera pipeline di analisi linguistica (Manning and Schütze, 1999) già ampiamente nota nei task di NLP: tokenizzazione, lemmatizzazione e pos tagging. Successivamente si è passato ad un’analisi statistica per ottenere le frequenze assolute dei termini, le frequenze relative per autore, le collocazioni, i bigrammi e i trigrammi ricorrenti e la forza di associazione tra le parole espressa in termini di PMI (Pointwise Mutual Information). Gli strumenti utilizzati per effettuare queste analisi sono basati sulla libreria NLTK in Python (Loper and Bird, 2002). Una volta estratti i Lemmi con le rispettive frequenze, viene calcolato il valore di Term Frequency/ Inverse Document Frequency (TF/IDF) per ogni lemma in modo da estrarre le parole più significative per ciascun autore. Una matrice di co-occorrenza, precedentemente costruita su un corpus di circa 3 milioni di parole, attraverso l'applicazione di un algoritmo di Distributional Semantics chiamato HAL - Hyperspace Analogue to Language (Burges and Lund, 1995), è utilizzata per estrarre le parole con valori di similitudine semantica maggiori per ogni lemma, allo scopo di creare un network di significati che identifichi lo spazio semantico di ciascun autore e permetta la loro classificazione attraverso algoritmi di machine learning (clustering).
            I dati risultanti dall’analisi linguistica sono strutturati in un database adatto all’elaborazione dei software e dei processi di data visualization. L’obiettivo di questa parte del progetto è quello di costruire un tool interattivo che utilizzi tecnologie web (html, css, javascript) per rendere il dataset esplorabile, comunicabile e analizzabile ulteriormente. Per l’elaborazione del sistema di visualizzazione prendiamo in esame le specifiche problematiche poste dalla visualizzazione di grandi corpora testuali (Wise et al., 1995; Fortuna et al., 2005; Alencar et al., 2012; Sinclair et al., 2013; Kucher, 2014; Brath and Banissi, 2015) e le soluzioni approntate da alcuni lavori precedenti sulla visualizzazione di database composti da testi di canzoni (Labrecque, 2009; Baur et al., 2010; Oh, 2010; Sasaki et al., 2014). 
            Il tool di visualizzazione si compone di una serie di “viste” e di filtri di navigazione che permettono di osservare il dataset da più angolazioni e attraverso diversi livelli di dettaglio, secondo il classico pattern Overview first, zoom and filter, then details-on-demand (Shneidermann, 1996). Oltre agli approcci classici dell’Information Visualization, la progettazione delle visualizzazioni tiene conto dell’approccio maturato dal design della comunicazione nell’ambito delle Digital Humanities (Uboldi and Caviglia, 2015) in paritcolare per quanto riguarda la definizione della user experience. 
            Una serie di layer di visualizzazione sono combinati in delle viste panoramiche che offrono uno sguardo complessivo su diversi aspetti del database: statistiche di base come le frequenze e le distribuzioni dei termini più utilizzati; la varietà complessiva del vocabolario; una serie di ranking; reti bipartite tra autori e termini; reti tra parole e relativi cluster semantici più evidenti. 
            Filtri e viste secondarie sono progettati invece per muoversi rapidamente tra diversi livelli e prospettive sul dataset e di scendere nel dettaglio per analizzare i dati relativi al singolo autore (termini più frequenti, temi dominanti, ecc) o al singolo brano. E’ inoltre possibile operare comparazioni tra autori (o gruppi di autori), o tra brani (o gruppi di brani). La visualizzazione è progettata dunque principalmente come strumento esplorativo in modo tale da rendere possibile l’analisi dell’universo testuale del rap  a diversi livelli di profondità e granularità.
            
                
                Fig : Rapscape: Single Artist Explorer view
            
        
        
            
                
                    Bibliography
                    
                        Alencar, A. B., Oliveira, M. C. F. de and Paulovich, F. V.
                         (2012). Seeing beyond reading: a survey on visual text analytics. 
                        Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,
                        2
                        (6): 476–92.
                    
                    
                        Alim, H. S., Ibrahim, A. and Pennycook, A.
                         (2008). 
                        Global Linguistic Flows: Hip Hop Cultures, Youth Identities, and the Politics of Language.
                        Routledge.
                    
                    
                        Androutsopoulos, J. and Scholz, A.
                        (2003). Spaghetti Funk: Appropriations of Hip-Hop Culture and Rap Music in Europe. 
                        Popular Music and Society,
                        
                        26(4): 463–79.
                    
                    
                        Attolino, P.
                         (2003). 
                        Stile ostile,
                        CUEN.
                    
                    
                        Attolino, P.
                         (2012). Iconicity in Rap Music The challenge of an anti-language. 
                        Testi e Linguaggi,
                        6: 17–35.
                    
                    
                        Baur, D., Steinmayr, B. and Butz, A. (2010). SongWords: Exploring Music Collections Through Lyrics. In 
                        ISMIR, pp. 531-36.
                    
                    
                        Bradley, A.
                         (2009). 
                        Book of Rhymes: The Poetics of Hip Hop.
                        Basic Books.
                    
                    
                        Brath, R., and Banissi, E.
                         (2015). Using Text in Visualizations for Micro / Macro Readings. 
                        Proceedings of the IUI Workshop on Visual Text Analytics.
                        Retrieved from http://vialab.science.uoit.ca/textvis2015/papers/Brath-textvis2015.pdf
                    
                    
                        Burgess, C., and Lund, K.
                         (1995). Hyperspace analog to language (hal): A general model of semantic representation. In 
                        Proceedings of the annual meeting of the Psychonomic Society,
                        
                        12: 177-210.
                    
                    
                        Church, K. W. and Hanks, P.
                         (1990). Word Association Norms, Mutual Information, and Lexicography. 
                        Comput. Linguist.,
                        16(1): 22–29.
                    
                    
                        Cutler, C.
                         (2007). Hip-Hop Language in Sociolinguistics and Beyond. 
                        Language and Linguistics Compass,
                        1
                        (5): 519–38.
                    
                    
                        Filippone, A. and Papini, L.
                         (2002). La parola e il suo potere: Il linguaggio del rap italiano. 
                        Rassegna italiana di linguistica applicata,
                        33(3): 71–86.
                    
                    
                        Forman, M. and Neal, M. A.
                         (2004). 
                        That’s the Joint!: The Hip-Hop Studies Reader.
                        Psychology Press.
                    
                    
                        Fortuna, B., Grobelnik, M. and Mladenić, D.
                         (2005). Visualization of text document corpus. 
                        Informatica,
                        pp. 497–502.
                    
                    
                        Hirjee, H. and Brown, D. G.
                         (2009). Automatic Detection of Internal and Imperfect Rhymes in Rap Lyrics. 
                        Proceedings of the 10th International Society for Music Information Retrieval Conference, pp. 711–16.
                    
                    
                        Hirjee, H. and Brown, D. G.(2010). Rhyme Analyzer: An Analysis Tool for Rap Lyrics. 
                        Proceedings of the 11th International Society for Music Information Retrieval Conference.
                    
                    
                        Hu, X., Stephen, J., Andreas, D. and Ehmann, F.
                         (2009). Lyric text mining in music mood classification. 
                        Proceedings of the International Society for Music Information Retrieval Conference.
                    
                    
                        Kleedorfer, F., Knees, P. and Pohle, T. (2008). Oh Oh Oh Whoah! Towards Automatic Topic Detection In Song Lyrics. 
                        Proceedings of the 9th International Conference on Music Information Retrieval (ISMIR 2008), pp. 287–92.
                    
                    
                        Kucher, K., and Kerren, A.
                         (2014). Text Visualization Browser: A Visual Survey of Text Visualization Techniques. In 
                        IEEE Information Visualization (InfoVis' 14), Paris, France.
                        
                    
                    
                        Labrecque, A.
                         (2009). 
                        Computer Visualization of Song Lyrics.
                        Doctoral dissertation: Worcester Polytechnic Institute.
                    
                    
                        Lena, J. C.
                         (2006). Social Context and Musical Content of Rap Music, 1979–1995. 
                        Social Forces,
                        85(1): 479–95.
                    
                    
                        Loper, E. and Bird, S.
                         (2002). NLTK: The Natural Language Toolkit. 
                        Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics - Volume 1.
                        (ETMTNLP ’02). Stroudsburg, PA, USA: Association for Computational Linguistics, pp. 63–70 
                    
                    
                        Mahedero, J. P. G., MartÍnez, Á., Cano, P., Koppenberger, M. and Gouyon, F.
                         (2005). Natural Language Processing of Lyrics. 
                        Proceedings of the 13th Annual ACM International Conference on Multimedia.
                        (MULTIMEDIA ’05). New York, NY, USA: ACM, pp. 475–78.
                    
                    
                        Malmi, E., Takala, P., Toivonen, H., Raiko, T. and Gionis, A.
                         (2015). DopeLearning: A Computational Approach to Rap Lyrics Generation. 
                        arXiv:1505.04771 [cs]
                         http://arxiv.org/abs/1505.04771 (accessed 5 March 2016).
                    
                    
                        Manning, C. D., and Schütze, H.
                         (1999). 
                        Foundations of statistical natural language processing.
                        Cambridge: MIT press.
                    
                    
                        Oh, J. 
                        (2010). Text Visualization of Song Lyrics. 
                        Center for Computer Research in Music and Acoustics, Stanford University.
                    
                    
                        Osumare, H.
                         (2008). 
                        The Africanist Aesthetic in Global Hip-Hop: Power Moves.
                        Palgrave Macmillan US.
                    
                    
                        Pacoda, P.
                         (1996). 
                        Potere alla parola: antologia del rap italiano.
                        Feltrinelli.
                    
                    
                        Pinckney, C.
                         (2007). 
                        The Influence of Hip-Hop Culture on the Perceptions, Attitudes, Values, and Lifestyles of African-American College Students.
                        ProQuest.
                    
                    
                        Sasaki, S., Yoshii, K., Nakano, T., Goto, M., and Morishima, S.
                        (2014). LyricsRadar: A Lyrics Retrieval System Based on Latent Topics of Lyrics. In 
                        ISMIR,
                        pp. 585-90.
                    
                    
                        Scholz, A.
                        (2005). 
                        Subcultura e lingua giovanile in Italia: hip-hop e dintorni.
                        Aracne.
                    
                    
                        Shneiderman, B.
                         (1996). The eyes have it: a task by data type taxonomy for information visualizations. 
                        IEEE Symposium on Visual Languages, 1996. Proceedings,
                        pp. 336–43.
                    
                    
                        Sinclair, S., Ruecker, S., and Radzikowska, M.
                        (2013). Information Visualization for Humanities Scholars. 
                        Literary Studies In The Digital Age.
                    
                    
                        Terkourafi, M.
                         (2010). 
                        The Languages of Global Hip Hop.
                        A&C Black.
                    
                    
                        Toop, D. (2000). 
                        Rap Attack 3: African Rap to Global Hip Hop.
                        Serpent’s Tail.
                    
                    
                        Uboldi, G. and Caviglia, G.
                        (2015). Information Visualizations and Interfaces in the Humanities. In Bihanic, D. (Ed), 
                        New Challenges for Data Design.
                        Springer London, pp. 207–18 http://link.springer.com/chapter/10.1007/978-1-4471-6596-5_11 (accessed 5 March 2016).
                    
                    
                        Wise, J. A., Thomas, J. J., Pennock, K., Lantrip, D., Pottier, M., Schur, A. and Crow, V. (1995). Visualizing the non-visual: spatial analysis and interaction with information from text documents. 
                        Information Visualization, 1995. Proceedings. pp. 51–58.
                    
                
            
        
    
2650	2016	
        
            
                Approach
                Decades ago, alongside more traditional structuralist paradigms that were largely based on linguistic theorems (Lotman 1972, Titzmann 1977), literary studies began to undertake structural analyses based on empirical sociology, in particular the social network analysis. Structure was no longer solely defined by semantic relations (such as opposition or equivalence), but by social interactions, too (Marcus 1973; Stiller, Nettle and Dunbar 2003; de Nooy 2005; Stiller and Hudson 2005; Elson, Dames and McKeown 2010; Agarwal et al., 2012). In the context of the Digital Humanities, this kind of approaches has gained a new dynamic in shape of a dedicated literary network analysis (Moretti 2011; Rydberg-Cox 2011; Park, Kim and Cho 2013; Trilcke 2013). This method is based on the analysis of bigger literary corpora (i.e., quantitative data) and promises insights into the history of literature as well as generic characteristics of literary texts. In our project, "dlina. Digital LIterary Network Analysis", we already developed a workflow for the extraction, analysis and visualisation of network data from dramatic texts built on basic TEI markup (Fischer, Kampkaspar, Göbel, Trilcke, 2015). This paper will present results of our analysis of the network data gathered so far and discuss them in the light of current theories in the field of social network analysis.
            
            
                Data Collection and Analysis
                Our current corpus comprises 465 German-language dramas (from 1730 to 1930), the better part of the Digitale Bibliothek corpus contained in the TextGrid repository (
                    ). The structural data crucial for the network analysis of these dramas (segmentation, character identification, etc.) was revised manually in a rule-based process to straighten out issues with the OCR and TEI tagging. We also had to level out philological peculiarities that would otherwise distort our results (such as different names for identical figures or groups of characters like 'Both' or 'All'). All the structural data is stored in an XML format we especially developed for that purpose (DLINA format). Network visualisation and network-value calculation has been automated (via Python and, alternatively, JavaScript to facilitate a direct embedding of our results into webpages). The scripts are fed with the data stored in DLINA files. In addition to graphs and simple network values that globally describe networks (like network size, density, average degree, average path length, clustering coefficent), we also calculate centrality values for the characters of each play (like degree, average distance, closeness centrality, betweenness centrality). In addition, we most recently implemented the calculation of random graphs based on the observed drama networks. All data and visualisations are freely available online on the project website (
                     and 
                    ).
                
            
            
                Evaluation, Part I: History of Drama
                The diachronic extension of our corpus over 200 years of German literary history allows the observation of larger developments in the structural composition of dramatic texts (we outlined some reflections on this in a blog post: 
                    ). Values referring to networks as a whole will be broached (incl. network size, density, average degree; as an example, we put average-path-length values by decades in Fig. 1), as will be character-related values for each character of each play (centrality measures, primarily) providing information on the distribution of the personae dramatis or their division into 'central' and 'less central' characters. These values will lay the groundwork for the discussion of some global hypotheses of literary history. We will discuss, firstly, the extent to which we can observe a differentiation of the structural composition of drama at the end of the 18th century on the basis of network analysis values: Such a differentiation is to be expected given the coexistence of 'closed' drama (following the doctrines of French classicism) and 'open' drama (mostly influenced by Shakespeare). Secondly, we will discuss some common literary periodising hypotheses (originating from structuralism, social history, or other directions). We will have a closer look at correlations between our network data and well-established traditional periodisations.
                
                
                    
                        
                        Fig. 1: Average path length by decades (mean)
                    
                
            
            
                Evaluation, Part II: Types of Drama
                The data raised so far shows how very differently theatre plays were structured in the focal period. Traditional literary studies have developed various typologies for such different types, the most popular in German studies being Volker Klotz's subdivision into 'open' and 'closed' drama. We want to build on this kind of typological impulse and propose a method as to how certain types of structural composition can be distinguished by means of network analysis (and also placed in their historic context). With this proposal we want to take up reflections from research on so-called small-world networks. This branch of research assumes that the values of empirically collected networks often differ significantly from those of corresponding random networks (e.g., graphs generated with the Erdős–Rényi model). Following the approach of Stiller, Nettle and Dunbar 2003, but relying on a much larger set of texts, we investigate the plays in our corpus with regard to their small-world properties (clustering coefficient, average path length, node degree distribution). The results show that there are just a few plays that meet all the criteria (a total of five plays, i.e., just about one percent of the corpus) – see figs. 2.1 to 2.5.
                These findings will give us a deeper understanding of different types of structural composition. We shall first direct our attention to forms of networks that – unlike dramas with small-world properties – occur much more frequently in our corpus. Eventually, we will discuss structural characteristics of drama networks exhibiting properties exactly opposite to the properties of small-world dramas (e.g., reverse power-law form in the node degree distribution). It will also be discussed in this context whether we can contrast the strong hierarchical type of small-world drama with an anti-hierarchical type.
                
                    
                        
                        Fig. 2.1: Goethe, “Götz von Berlichingen” (1773): Spring Embedder Layout, Circular Layout, Node Degree Distribution
                    
                
                
                    
                        
                        Fig. 2.2: Arnim, “Jerusalem” (1811): Spring Embedder Layout, Circular Layout, Node Degree Distribution
                    
                
                
                    
                        
                        Fig. 2.3: Soden, “Doktor Faust” (1797): Spring Embedder Layout, Circular Layout, Node Degree Distribution
                    
                
                
                    
                        
                        Fig. 2.4: Nestroy, “Der böse Geist” (1833): Spring Embedder Layout, Circular Layout, Node Degree Distribution
                    
                
                
                    
                        
                        Fig. 2.5: Raimund, “Der Barometermacher” (1823): Spring Embedder Layout, Circular Layout, Node Degree Distribution
                    
                
            
        
        
            
                
                    Bibliography
                    
                        Réka, A. and Barabási, A.-L. (2002). Statistical mechanics of complex networks. 
                        Reviews of Modern Physics
                        74: 47–97.
                    
                    
                        Agarwal, A. et al. (2012). Social Network Analysis of Alice in Wonderland. 
                        Proceedings of the Workshop on Computational Linguistics for Literature, Montréal: 88–96.
                    
                    
                        de Nooy, Wouter (2006). Stories, Scripts, Roles, and Networks. 
                        Structure and Dynamics 
                        1(2) 
                         (accessed 4 March 2016)
                    
                    
                        Elson, D. K., Dames, N.
                        and
                         McKeown, K. R. (2010). Extracting Social Networks from Literary Fiction. 
                        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, pp. 138–47.
                    
                    
                        Fischer, F., Kampkaspar, D.; Göbel, M. 
                        and 
                        Trilcke, P. (2015). Digital Network Analysis of Dramatic Texts. 
                        DH2015, script: 
                        , slides: 
                         (accessed 4 March 2016).
                    
                    
                        Klotz, Volker (1960). 
                        Geschlossene und offene Form im Drama. München.
                    
                    
                        Lotman, Jurij M. (1972).
                        Die Struktur literarischer Texte. München.
                    
                    
                        Marcus, Solomon (1973). 
                        Mathematische Poetik. Frankfurt/M.
                    
                    
                        Moretti, Franco: Network Theory, Plot Analysis. 
                        Stanford Literary Lab Pamphlets, 
                        No. 2, 1 May 2011, 
                         (accessed 4 March 2016).
                    
                    
                        Park, G.-M., Sung-Hwan, K. 
                        and
                         Cho, H.-G. (2013). Structural Analysis on Social Network Constructed from Characters in Literature Texts. 
                        Journal of Computers
                        8(9): 2442–47, 
                         (accessed 4 March 2016).
                    
                    
                        Rydberg-Cox, J. (2011). Social Networks and the Language of Greek Tragedy. 
                        Journal of the Chicago Colloquium on Digital Humanities and Computer Science, 
                        1(3), 
                        
                    
                    
                        Stiller, J., Nettle, D. 
                        and 
                        Dunbar, Robin I. M. (2003). The Small World of Shakespeare's Plays. 
                        Human Nature
                        14: 397–408.
                    
                    
                        Stiller, J. 
                        and
                         Hudson, M. (2005). Weak Links and Scene Cliques Within the Small World of Shakespeare. 
                        Journal of Cultural and Evolutionary Psychology
                        3: 57–73.
                    
                    
                        Titzmann, M. (1977). 
                        Strukturale Textanalyse. Theorie und Praxis der Interpretation. München.
                    
                    
                        Trilcke, P. (2013). Social Network Analysis (SNA) als Methode einer textempirischen Literaturwissenschaft. Ajouri, Philip; Mellmann, Katja and Rauen, Christoph (eds.): 
                        Empirie in der Literaturwissenschaft. Münster, pp. 201–47.
                    
                
            
        
    
2651	2016	
        
            
                Introduction
                Many text-classification techniques have been proposed and used for authorship attribution (Holmes, 1994; Grieve, 2007; Juola, 2008; Koppel et al., 2011), genre categorization (Biber, 1988; Argamon et al., 2003), stylochronometry (Forsyth, 1999) and other tasks within computational stylistics. However, until quite recently, it has been extremely difficult to assess novel and existing techniques on comparable benchmark problems within a common framework using statistically robust methods.
                Toccata is a resource for computational stylometry which aims to address that lack, freely available at
                
                    http://www.richardsandesforsyth.net/software.html
                
                under the GNU public licence.
                The main program is a test harness in which a variety of text-classification algorithms can be evaluated on unproblematic cases and, if required, applied to disputed cases. The package supplies four pre-existing classification methods as modules (including Delta (Burrows, 2002), widely regarded as a standard in this area) as well as five sample corpora (including the famous 
                    Federalist Papers) so that users who don't wish to write Python code can use it simply as an off-the-shelf classifier and those who do can familiarize themselves with the system before implementing their own algorithms.
                
                Noteworthy features of the system include:
                
                    sample corpora provided for familiarization;
                    test phase using random subsampling to give robust error-rate estimation;
                    ability to plug in new techniques or to employ existing standards;
                    option of post-hoc phase applying trained model(s) to unseen holdout data;
                    empirically grounded computation of post-hoc confidence weights to deal with 'open' problems where the unseen cases may not belong to any of the training-set categories;
                    accompanying export file readable by R or similar statistical packages for optional further processing.
                
            
            
                Sketch of the System's Operation
                Toccata performs three main functions, in sequence:
                (a) testmode: leave-n-out random resampling test of the classifier on the training corpus to provide statistics by which the classifier can be evaluated;
                (b) holdout: application of the classifier to an unseen holdout sample of texts, if given;
                (c) posthoc: re-application to the holdout sample of texts (if given) using the results from phase (a) to estimate empirical probabilities.
                Steps (b) and (c) are optional.
            
            
                Sample corpora
                Toccata is a document-oriented system. Thus a training corpus consists of a number of text files, in UTF8 encoding, without markup such as HTML tags. Each file is treated as an individual document, belonging to a particular category. Example corpora are supplied to enable users to start using the system, prior to collecting or reformatting their own corpora.
                
                    ajps: ninety poems by 2 eminent 19th-century Hungarian poets, Arany József and Petőfi Sándor. Arany was godfather to Petőfi's child, so we might expect their writing styles to be relatively similar.
                
                
                    cics: Latin texts relevant to the authorship of the 
                    Consolatio which Cicero wrote in 45 BC. This was thought to have been lost until in 1583 AD when Sigonio claimed to have rediscovered it. Background information can be found in Forsyth et al. (1999).
                
                
                    feds: writings by Alexander Hamilton and James Madison, as well as some contemporaries of theirs. This corpus is related to another notable authorship dispute, concerning the 
                    Federalist Papers, which were published in New York in 1788. See Holmes and Forsyth (1995).
                
                
                    mags: 144 texts from 2 different learned journals, namely 
                    Literary and Linguistic Computing and 
                    Machine Learning. Each text is an excerpt consisting of the Abstract plus initial paragraph of an article in one of those journals, written during the period 1987-1995.
                
                
                    sonnets: 196 English sonnets, 14 each by 14 different authors, with an additional holdout sample of 24 texts, half of which are by authors absent from the main sample.
                
            
            
                Validation by Random Subsampling
                A major objective of the system is to assess the effectiveness of text-classification methods by a form of cross validation. For this purpose the training corpus of undisputed texts is repeatedly divided into two portions, one used to form a classification model and the other used to test the accuracy of this model. After this cycle a number of quality statistics are computed and printed, along with a confusion matrix. This helps to establish a relatively honest estimate of the likely future error rate of the classifier. After subsampling, the program will construct a model on the full training set. This may then be applied to a genuine holdout sample, if provided.
            
            
                Classifier Modules
                A classifier module is expected to develop trained models of each text category and deliver matching scores of a text to each model, with more positive scores indicating stronger matching. The category with the highest match-score relative to the average of all scores for the text, is the assigned class. Four library modules are supplied "off the shelf".
                Module 
                    docalib_deltoid.py is an implementation of Burrows's delta (Burrows, 2002) which has become a standard technique in authorship attribution studies. Module 
                    docalib_keytoks.py works by first finding the 1024 most common word tokens in the corpus, then keeping from these the most distinctive. For classification, relative word frequencies in the text being classified are correlated with relative frequencies in each class. Module 
                    docalib_maws.py is a version of what Mosteller and Wallace in their classic work (1964/1984) on the 
                    Federalist Papers call their "robust Bayesian analysis", as implemented by Forsyth (1995). Module 
                    docalib_topvocs.py implements another classifier inspired by the approach of Burrows (1992), which uses the most frequent tokens in the training corpus as features.
                
            
            
                The Holdout and Posthoc Phases
                The subsampling test phase (above) is primarily concerned with assessing the quality of a classification method. The holdout and posthoc phases are when that method is applied in earnest.
                If a holdout sample is given, the model developed on the training set is applied to that sample. The holdout texts may belong to categories that were not present in the training set, so each decision is categorized as correct (+), incorrect (-) or undetermined (?) and the success rate statistics computed accordingly.
                This is illustrated in Table 1, below, from an application of the MAWS (Mosteller and Wallace) method to a collection of sonnets. Here the training set consists of 196 short English poems -- 14 sonnets by 14 different authors. This is a challenging problem firstly because the median length of each text in the training corpus is 116 words, secondly because 14 is a relatively large number of candidates.
                Table 1 shows the ranking produced on a holdout sample of 24 texts, absent from the training set. Note that 12 of these 24 items are 'distractors', i.e. texts by authors not present in the training set. The program assigns these a question mark (?) in assessing its own decision.
                The listing ranks the program's decisions from most to least credible. The upper third include 6 correct assignments, 1 clear mistake and a distractor. The middle third contains 1 correct classification, 3 mistakes and 4 distractors. The last third contains no correct answers, 1 mistake and 7 distractors. (Incidentally, the distractor poem by the Earl of Oxford, ranked twentieth, is more congruent with Wordsworth than any other author, including Shakespeare, and not confidently assigned to any of the training categories.)
                This output addresses the very real problem of documents from outside the known training categories. The listing is ordered by a quantity labelled 'credit'. This is the geometric mean of the last two numbers in each line, labelled 'confidence' and 'congruity'. Confidence is derived from the preceding subsampling phase. It is computed from the differential matching score of the text under consideration as W / (W+L), where W is the number of correct answers which received a lower differential score during the subsampling phase and L is the number of wrong answers with a higher score. Congruity is simply the proportion of matching scores of the chosen category that were lower, in the subsampling phase, than the score for the case in question. It is an empirically based index of compatibility between the assigned category of the text and the training examples of that category.
                In all kinds of classification, the problem of never-before-seen categories can loom large. (See, for instance, Eder, 2013.) Like most trainable classifiers, Toccata always picks the most likely category from those it has encountered in training, but the most likely may not be very likely. The confidence and congruity scores give useful information in this regard. For example, if we only consider the classifications which obtain a score of at least 0.5 on both confidence and congruity, we find 6 correct decisions, 1 incorrect and 1 distractor. Treating the distractor (assigning a sonnet by Dylan Thomas to Edna Millay) as incorrect still represents a 75% success rate in an "open" authorship problem on texts only slightly more than a hundred word tokens in length, where the training sample for each known category consists of approximately 1600 words, with a chance expectation of 7% success. In other words, three crucial parameters -- training corpus size, text length and number of categories -- are all well "outside the envelope" of most previously reported authorship studies.
                Table 1 -- Posthoc ranking of 24 decisions on unseen texts, including 12 'distractors'
                
                    
                        rank
                        credit
                        filename
                        pred:true
                        conf.
                        congruity
                    
                    
                         1
                        0.9163
                         ChrRoss_WinterSecret.t
                         ChrRoss + ChrRoss
                         0.9530
                         0.8810
                    
                    
                         2
                        0.8768
                         WilShak_6.txt 
                         WilShak + WilShak
                         0.9425
                         0.8158
                    
                    
                         3
                        0.8142
                         DylThom_Altar09.txt 
                         EdnMill ? DylThom
                         0.8838
                         0.7500
                    
                    
                         4
                        0.7664
                         MicDray_Idea000.txt 
                         MicDray + MicDray
                         0.6378
                         0.9211
                    
                    
                         5
                        0.7595
                         WilShak_137.txt 
                         WilShak + WilShak
                         0.8118
                         0.7105
                    
                    
                         6
                        0.6950
                         JohDonn_Nativity.txt 
                         JohDonn + JohDonn
                         0.6720
                         0.7188
                    
                    
                         7
                        0.6247
                         MicDray_Idea048.txt 
                         JohDonn - MicDray
                         0.5430
                         0.7188
                    
                    
                         8
                        0.5356
                         WilShak_109.txt 
                         WilShak + WilShak
                         0.5737
                         0.5000
                    
                    
                         9
                        0.5225
                         DylThom_Altar05.txt 
                         RupBroo ? DylThom
                         0.4150
                         0.6579
                    
                    
                         10
                        0.4684
                         TomWyat_THEY_FLEE_FROM
                         EdmSpen ? ThoWyat
                         0.4596
                         0.4773
                    
                    
                         11
                        0.4226
                         PerShel_Ozymandias.txt
                         EliBrow ? PerShel
                         0.2217
                         0.8056
                    
                    
                         12
                        0.4027
                         EliBrow_SP23.txt 
                         DanRoss - EliBrow
                         0.2237
                         0.7250
                    
                    
                         13
                        0.3061
                         WilShak_RomeoJuliet.tx
                         WilShak + WilShak
                         0.2094
                         0.4474
                    
                    
                         14
                        0.2739
                         PhiSidn_astel108.txt 
                         EliBrow - PhiSidn
                         0.1080
                         0.6944
                    
                    
                         15
                        0.2625
                         DylThom_Altar06.txt 
                         EliBrow ? DylThom
                         0.0992
                         0.6944
                    
                    
                         16
                        0.2283
                         JohDonn_Temple.txt 
                         EdnMill - JohDonn
                         0.1179
                         0.4423
                    
                    
                         17
                        0.2014
                         Lincoln1863Gettysburg.
                         SamDani ? AbeLinc
                         0.0649
                         0.6250
                    
                    
                         18
                        0.1894
                         RicFors_LaBocca.txt 
                         RupBroo ? RicFors
                         0.0649
                         0.5526
                    
                    
                         19
                        0.1352
                         HelFors_1958.txt 
                         EliBrow ? HelFors
                         0.0263
                         0.6944
                    
                    
                         20
                        0.1089
                         oxford_13.txt 
                         WilWord ? Oxford 
                         0.0265
                         0.4474
                    
                    
                         21
                        0.0977
                         RicFors_Underworld.txt
                         EdnMill ? RicFors
                         0.0261
                         0.3654
                    
                    
                         22
                        0.0755
                         HelFors_1982.txt 
                         DanRoss ? HelFors
                         0.0109
                         0.5250
                    
                    
                         23
                        0.0690
                         DylThom_Altar03.txt 
                         RupBroo ? DylThom
                         0.0106
                         0.4474
                    
                    
                         24
                        0.0411
                         PhiSidn_astel030.txt 
                         EdmSpen - PhiSidn
                         0.0106
                         0.1591
                    
                
                ++?+++-+???-+-?-???????-
            
        
        
            
                
                    Bibliography
                    
                        Argamon, S., et al. (2003). Gender, genre, and writing style in formal written texts. 
                        Text, 23(3): 321-46.
                    
                    
                        Biber, D. (1988). 
                        Variation across speech and writing. Cambridge: Cambridge University Press.
                    
                    
                        Burrows, J.F. (1992). Not unless you ask nicely: the interpretive nexus between analysis and information. 
                        Literary and Linguistic Computing, 7(2): 91-109.
                    
                    
                        Burrows, J.F. (2002). 'Delta': a measure of stylistic difference and a guide to likely authorship. 
                        Literary and Linguistic Computing, 17(3): 267-87.
                    
                    
                        Eder, M. (2013). Bootstrapping Delta: a safety net in open-set authorship attribution. 
                        
                            Digital Humanities 2013: Conference Abstracts
                        . Lincoln: University of Nebraska-Lincoln, pp. 169-72.
                    
                    
                        Forsyth, R.S. (1995). 
                        Stylistic Structures: a Computational Approach to Text Classification. Unpublished Doctoral Thesis, Faculty of Science, University of Nottingham. 
                        http://www.richardsandesforsyth.net/doctoral.html
                    
                    
                        Forsyth, R.S. (1999). Stylochronometry with substrings, or: a poet young and old. 
                        Literary and Linguistic Computing, 14(4): 467-77.
                    
                    
                        Forsyth, R.S., Holmes, D.I. and Tse, E.K. (1999). Cicero, Sigonio, and Burrows: investigating the authenticity of the 'Consolatio'. 
                        Literary and Linguistic Computing, 14(3): 1-26.
                    
                    
                        Grieve, J. (2007). Quantitative authorship attribution: an evaluation of techniques. 
                        Literary and Linguistic Computing, 22(3): 251-70.
                    
                    
                        Holmes, D. (1994). Authorship attribution. 
                        Computers and the Humanities, 28: 1-20.
                    
                    
                        Holmes, D.I. and Forsyth, R.S. (1995). The 'Federalist' revisited: new directions in authorship attribution. 
                        Literary and Linguistic Computing, 10(2): 111-27.
                    
                    
                        Juola, P. (2006). Authorship attribution. 
                        Foundations and Trends in Information Retrieval, 1(3): 233-334.
                    
                    
                        Koppel, M., Schler, J. and Argamon, S. (2011). Authorship attribution in the wild. 
                        Language Resources and Evaluation, 45, pp. 83-94. DOI 10.1007/s10579-009-9111-2.
                    
                    
                        Mosteller, F. and Wallace, D.L. (1984). 
                        Applied Bayesian and Classical Inference: the Case of the Federalist Papers. New York: Springer. [First edition, 1964.]
                    
                
            
        
    

2674	2016	
        
            The possibilities for widening the spectrum of research questions by adopting new computational methodology seem to be almost unlimited for literary scholars with considerable programming skills. Researchers with little or no such skills, however, have to rely on user-friendly tools. Simple word counts are still among the most common, and admittedly often very useful features used in computational text analysis. Usually, linguistic annotations are needed for using more complex features in the analysis of style or content of a literary text. For example, a researcher might want to investigate style in terms of syntactic preferences by applying stylometric analysis on part-of-speech tag n-grams, to run topic modelling on specific word types only or to characterize the way an author describes figures by extracting all the adjectives that refer to a named entity. All these examples require of the scholar to first extract linguistic information from the text and use that information to define complex features. 
            Computer linguists have developed several tools for the various tasks of natural language processing (NLP) that can automatically analyze a digital text and annotate it with such information. In the present spectrum of solutions for NLP tasks, there is a gap between tools for rather simple tasks and full programming frameworks which require significant programming skills. The one end of the spectrum is represented by WebLicht,
                
                        http://weblicht.sfs.uni-tuebingen.de/weblichtwiki/index.php/Main_Page
                 a web service that allows users to upload and process single files very comfortably. On the other end are GATE,
                
                        https://gate.ac.uk/
                 NLTK
                
                        http://www.nltk.org/
                , BookNLP
                
                        https://github.com/dbamman/book-nlp
                 and the Darmstadt Knowledge Processing Repository (DKPro).
                
                        https://www.ukp.tu-darmstadt.de/research/current-projects/dkpro/
                
            
            DKPro provides a programming framework in which many such NLP tools can be combined into an analysis pipeline. The pipelining approach is especially useful, often even necessary, when one NLP tool needs the annotations provided by another NLP tool in advance for extracting more complex linguistic features. DKPro thus provides access to tools like sentence splitters, tokenizers, part-of-speech taggers, named-entity recognizers, lemmatizers, morphological analyzers and parsers in many languages.
                 Not every kind of tool is available in all languages; it depends on the native support of the tools, not on the framework provided by DKPro.
                
            
            While making NLP significantly easier by integrating many NLP tools into a single framework, the use of DKPro still requires a substantial knowledge of technologies like UIMA, Java and Maven. To further lower the skill threshold for literary scholars to use complex NLP output in computational text analysis, DARIAH-DE (the German branch of the European project Digital Research Infrastructure for the Arts and Humanities, funded by the German Federal Ministry of Education and Research) developed the DARIAH-DKPro-Wrapper (DDW).
                
                        https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper
                 The DDW bundles a pipeline with a set of commonly used NLP components into a java program to be executed with a single command. As DKPro in general, the wrapper provides transparent access to a whole set of different NLP tools which are downloaded as needed behind the scenes. Command line options and configuration files allow users a considerable degree of control over the pipeline and its components, giving partial access to DKPro functionality without requiring any programming knowledge. The DDW also solves the problem of different input and output formats of the tools, offering a unified access. Therefore, the DDW positions itself in between the two ends of the aforementioned spectrum: It runs locally, allows for the processing of multiple files and can be configured to a considerable extent to one’s own needs. Whereas the user of classical DKPro is a UIMA programmer, the DDW can be used by anybody who can copy a command into the command line. Nonetheless, the DDW in some cases offers more features than other more advanced solutions, as DKPro supports more tools and languages. It also integrates Stanford NLP and supports the highly efficient Treetagger. A list of components available for both DKPro and the DDW can be found of the DKPro project page.
            
            Furthermore, the DDW stores its output in a tab-separated plain text format inspired by CoNLL2009.
                
                        https://ufal.mff.cuni.cz/conll2009-st/task-description.html
                 The format provides information on paragraph id, sentence id, token id, token, lemma, POS, chunk, morphology, named entity, parsing information and more. This format can be comfortably accessed in common scripting languages for further analysis, i.e. it can be directly read as a dataframe object in R or Python Pandas; it can even be opened in a common datasheet editor like Microsoft Excel.
            
            Scripts connecting the output format to popular text analysis tools like the R package 
                stylo
                Eder, Maciej, Mike Kestemont, and Jan Rybicki. "Stylometry with R: a suite of tools." 
                        Digital Humanities 2013: Conference Abstracts. 2013. For the software see: 
                        https://sites.google.com/site/computationalstylistics/stylo
                 are currently under development. Dariah also prepared some tutorials explaining how to use the wrapper and showing the use of the output format in research in three use cases.
                
                    
                        https://rawgit.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/tutorial.html
                    
                
            
            This poster will present the DDW and its file format as a new and comfortable means of providing linguistic annotations, thus significantly lowering the threshold for using complex NLP-based features in computational literary analysis.
        
    

2684	2016	
        
            As a community of practice, digital humanists deal with data and metadata not as static artifacts, but rather as complex, multi-dimensional and multi-layered datasets that can be analyzed, annotated and manipulated in order to produce new knowledge. One of the most important challenges facing DH today is how to consolidate and repurpose available tools; how to create reusable but flexible workflows; and, ultimately, how to integrate and disseminate knowledge, instead of merely capturing it and encapsulating it. This technical and intellectual shift can be seen as the “infrastructural turn” in digital humanities (Tasovac et al. 2015).  
            Application Programming Interfaces (APIs) have the potential to be powerful, practical building blocks of digital humanities infrastructures.
                
                    
                        Infrastructures are installations and services that function as “mediating interfaces” or “structures ‘in between’ that allow things, people and signs to travel across space by means of more or less standardized paths and protocols for conversion or translation” (Badenoch and Fickers 2010, 11). Digital research infrastructures are no different: they are a mediating set of technologies for research and resource discovery, collaboration, sharing and dissemination of scholarly output. 
                    
                 On the technical level, they let heterogeneous agents dynamically access and reuse the same sets of data and standardized workflows. On the social level, they help overcome the problem of “shy data”, i.e. data you can “meet in public places but you can’t take home with you”  (Cooper 2010). Some 10 years ago, Dan Cohen started the conversation about APIs in DH by pointing out that, despite their potential, Andreas few humanities projects — in contrast to those in the sciences and commercial realms — were developing APIs for their resources and tools (Cohen 2005). In the decade since, API development in the digital humanities has certainly increased: today, both large-scale, national and international initiatives, such as HathiTrust, DPLA or Europeana, as well as individual projects, such as Canonical Text Services (CTS), Open Siddur, Folger Digital Texts, correspSearch etc., are focusing their attention and resources on developing APIs. It is now time to reflect on this development: have standards or best-practices evolved?  What workflows are most effective and efficient for creating APIs? What are the challenges or stumbling blocks for creating or using APIs? Are APIs being used by DH researchers? What is the future of API development and use in the humanities community? 
            
            This panel will cover both the theory and practice of APIs in the digital humanities today. It will bring together researchers working on major European and North American projects, who will discuss APIs from the perspectives of design, implementation, and use, as well as technical and social challenges. Each group will have 10 minutes for their statement, and 40 minutes will remain for group discussion and questions from the audience. One of the panel members will serve as the moderator. All speakers have confirmed their intention to participate in the panel. 
            
                Toma Tasovac (Belgrade Centre for Digital Humanities) will discuss an API-centric approach to designing and implementing digital editions. Starting with the notion of text-as-service and textual resources as dynamic components in a virtual knowledge space, Tasovac will show how two recent projects — 
                 Raskovnik: A Serbian Dictionary Platform and 
                Izdanak: A Platform for Digital Editions of Serbian Texts — were implemented using API-focused data modeling at the core of the project design process. The API-first approach to creating TEI-encoded digital editions оffers tangible interfaces to textual data that can be used in tailor-made workflows by humanities researchers and other users, well-suited to distant reading techniques, statistical analysis and computer-assisted semantic annotation. The “infrastructural turn” in Digital Humanities does not only have practical implications for the way we build tools and create resources, but also has theoretical ramifications for the way we distinguish highly from loosely structured data: if text is not an object, but a service; and not a static entity, but an interactive method with clearly and uniquely addressable components, a formal distinction between a dictionary and, say, a novel or a poem, is more difficult to maintain. 
            
            
                Clifford Wulfman and 
                Natalia Ermolaev (Center for Digital Humanities, Princeton) will discuss the design and implementation of Blue Mountain Springs, the API for the Blue Mountain Project’s collection of historic avant-garde periodicals.
                
                    
                         http://bluemountain.princeton.edu
                    
                  By modeling magazine data using the FRBRoo ontology and its periodical-oriented extension PRESSoo (PRESSoo, 2014), this RESTful API exposes the Blue Mountain resource in a variety of data formats (structured metadata, full-text, image, linked data). The authors will provide several examples of how Blue Mountain Springs has been used by researchers, drawing especially from the results of the hackathon they will host at Princeton in February 2016, which will bring together approximately twenty periodical studies scholars, technologists, and librarians to work with the API. Creating APIs is part of a trend in DH to move into a post-digital-library phase, when the traditional library functions of discovery and access are no longer sufficient to support research in the humanities. This trend also suggests that DH researchers must reconceptualize their own engagement with material, to think less in terms of monographs and more in terms of resources, and consequently to promulgate their work not as web sites but as web services. 
            
            
                Thibault Clérice (University of Leipzig) will discuss the design of the Canonical Text Services (CTS) and its URN scheme, which make the traditional citation system used by classicists machine-actionable (Blackwell and Smith 2014)
                
                     Traditional scholars have been citing texts the same way for centuries : for example, “Hom. Il. 1.1”, which corresponds to Homer’s Iliad, Book 1 Line 1. However, although the passage identifier does not change from one scholar to another in most cases, the abbreviation used for the author and the work title will diverge among authors, countries, and publications. 
                . The Homer Multitext (HMT) implementation of CTS requires textual data to be extracted out of its original digital representation into RDF triples in order to be served. The Perseus Digital Library (PDL) implementation, on the other hand, uses extended transformations to slice XML files into multiple records, each representing a passage at a certain level. While relational and RDF database approaches have had some success in scalability and speed (Tiepmar 2015), they also have to deal with maintenance and evolution capacity. There is a real need for this type of DH projects to scale not only in terms of data retrieval speeds, but also in terms of allowing  researchers to correct and enhance their data. In addition, projects need to be able to propose other narratives: sliced data doesn’t easily provide access to the full data model. Clérice will discuss why and how, using both a native XML-based system such as eXist and a Python-based implementation, one can achieve scalability while guaranteeing maintenance and evolution.
            
            
                Adrien Barbaresi from the Austrian Academy of Sciences (ICLTT) will discuss the use of APIs in building resources for linguistic studies. The first case deals with lesser-known social networks (Barbaresi 2013) while the second tackles the role of the Twitter API in building the ICLTT’s "tweets made in Austria" corpus
                
                     http://www.oeaw.ac.at/icltt/node/193
                . For computational linguists, short messages published on social networks constitute a "frontier" area due to their dissimilarity with existing corpora (Lui & Baldwin 2014), most notably with reference corpora of written language. Since data are mainly accessed and collected through APIs and not in the form of web pages, Barbaresi argues that social networks are a frontier area for (web) corpus construction. He will point out the challenges of using Twitter’s API, for example how to reveal the implicit decisions and methodology used by API designers, as well as concrete implementation issues, such as the assessment and optimization of data returned by the API. Free APIs may come at no cost, but they also offer no guarantee, so that the use of commercial APIs for research purposes has to be seen with a critical eye in order to turn a data collection process into a proper corpus.
            
            Finally, 
                Jennifer Edmond and 
                Vicky Garnett (Trinity College Dublin), will provide reflections on the place of APIs within European research infrastructures for the humanities. Their contribution to the panel builds on their recent study on the Europeana Cloud project, which found that while access to data is a real and growing area of interest, very few humanities researchers seem to actively and directly use APIs.
                
                     This is a result of the confluence of several factors: that the research data most humanists want to access is seldom available via an API; that many sources that do offer relevant data lack the structure or metadata to make the API useful; and that humanistic researcher generally lack the skill set to experiment directly with APIs.  At the same time, however, one of the key desires expressed by researchers in the current landscape is the federation of high-quality data (Edmond and Garnett 2015). 
                 They will describe two initiatives, one technical, one social, aiming to better harness the potential of the API to meet researcher’s implicit needs. The first is the Collaborative European Digital Archival Research Infrastructure (CENDARI) project, whose platform is structured around an internal API that will allow multiple data sources (local repository, triple store, metasearch engine) to be aligned, enhanced and then served out to a number of environments and tools, including the project’s native note-taking environment. The second example is the genesis and development of the concept of the ‘inside-out’ archive. This framework, which has arisen out of a collaborative venture between several European humanities research infrastructure projects, seeks to encourage collection holding institutions to look beyond their own digitization programs and platforms and recognize the rising importance of machines-as-users (requiring specific access points and formats) rather than the somewhat outdated model of individual institutional web presence serving individual human resource seekers.
            
            The five speakers on this panel will address some of the most pressing issues related to the ongoing development and future of APIs on the DH research infrastructure landscape. The discussion will cover both micro- and macro levels, ranging from methodological implications and technical scalability to the ways in which API-based data access to collections challenges traditional norms of institutional identity and independence. As such, the panel will offer a timely platform for a multifaceted debate on the potentials and pitfalls of building and using APIs in the digital humanities. 
        
        
            
                
                    Bibliography
                    
                        Badenoch, A. and Fickers A. (2010). Europe Materializing? Toward a Transnational History of European Infrastructures. In Badenoch, A. and Fickers A. (eds.), Materializing Europe: Transnational Infrastructures and the Project of Europe, 1-26. Basingstoke, Hampshire; New York: Palgrave Macmillan.
                    
                    
                        Barbaresi, A. (2013). Crawling microblogging services to gather language-classified URLs. Workflow and case study. In Proceedings of the 51th Annual Meeting of the ACL, Student Research Workshop, pages 9-15.
                    
                    
                        Blackwell, C. and Smith N. (2014). 
                        Canonical Text Services Protocol Specification. http://folio.furman.edu/projects/citedocs/cts/. Accessed October 23, 2015.
                    
                    
                        Cohen, D. Do APIs Have a Place in the Digital Humanities. http://www.dancohen.org/blog/posts/do_apis_have_a_place_in_the_digital_humanities. Accessed October 24, 2015.
                    
                    
                        Cohen, D. (2006). “From Babel to Knowledge: Data Mining Large Digital Collections.” 
                        D-Lib Magazine 12, no. 3.
                    
                    
                        Cooper, D. (2010). When Nice People Won’t Share: Shy Data, Web APIs, and Beyond, 
                        Second International Conference on Global Interoperability for Language Resources (ICGL 2010), n. pag.
                    
                    
                        Edmond, J, Bulatovic N. and O’Connor, A. “The Taste of ‘Data Soup’ and the Creation of a Pipeline for Transnational Historical Research.” Journal of the Japanese Association for Digital Humanities 1, no. 1 (2015): 107–22.
                    
                    
                        Edmond, J. and Garnett V. (2015). APIs and Researchers: The Emperor’s New Clothes, International Journal of Digital Curation, 10(1): 287-97.
                    
                    
                        LeBeuf, Patrick (ed.). PRESSoo. Extension of CIDOC CRM and FRBROO for the modelling of bibliographic information pertaining to continuing resources. Version 0.5, 
                        http://www.ifla.org/files/assets/cataloguing/frbr/pressoo_v0.5.pdf. Accessed, November 1, 2015. 
                    
                    
                        Murdock, J. and Allen C. (2011). InPhO for All: Why APIs Matter, Journal of the Chicago Colloquium on Digital Humanities and Computer Science 1(3): 
                        http://www.jamram.net/docs/jdhcs11-paper.pdf. Accessed, October 23, 2015. 
                    
                    
                        Tasovac, T. Rudan S. and Rudan S. (2015). Developing Morpho-SLaWS: An API for the Morphosyntactic Annotation of the Serbian Language. Systems and Frameworks for Computational Morphology, 137-47. Heidelberg: Springer.
                        
                        Tiepmar, J. (2015). Release of the MySQL based implementation of the CTS protocol. In Bański, P., Biber H., Breiteneder E., Kupietz M., Lüngen H. and Witt A. (eds.), Proceedings of the 3rd Workshop on Challenges in the Management of Large Corpora (CMLC-3), 35-43. Mannheim: Institut für Deutsche Sprache.
                    
                
            
        
    

2731	2016	
        
            Ian Hodder has recently pointed to a “return to things” in the humanities and social sciences, a mode of analysis that explores the relationships between people and the objects we use to construct and make sense of the world (Hodder, 2014, p.19). In digital humanities we can see this in Matthew Kirschenbaum’s focus on the forensics of computer hard disks (Kirschenbaum, 2007), the development of platform studies to investigate the relationship between computing culture and the consoles and other hardware that enables it (Montfort and Bogost, 2009), and the appearance of maker cultures that seek to explore the humanities through practical experimentation (Dieter and Lovink, 2014). It suggests a desire to pay attention to and interact with the material world, rather than retreating to a purely digital one. Some commentators go beyond this. They propose that entanglement with material objects represents a ground of being for humans and their societies, that it presents a postphenomenological “dialectic of dependence and dependency between humans and things” worthy of deep contemplation (Hodder, 2014, p.19). People rely on the things they have created to such a degree, so the argument goes, that our identity has become inseparable from them. In Donald Ihde’s original conception, it amounts to “recognition that “consciousness” is an abstraction, that experience in its deeper and broader sense entails its embeddedness in both the physical or material world and its cultural-social dimensions” (Ihde, 2009). Knowledge, art, religion, and science are entangled, in turn, with books, oil paint, churches, and laboratories: “thing theory” grounds epistemology in the myriad interactions between the physical and non-physical world (Preda, 1999).
            If we apply these insights to digital infrastructure we begin to see how humanists have become entangled with complex systems, a situation that might prompt us to pause for thought. Analog books, archives, and libraries presuppose a degree of entanglement with the material world, undoubtedly, but those are relatively well understood: we have had centuries to understand and critique them. Digital infrastructure, however, is rarely “symbolically or politically construed” (Knorr-Cetina, 1992, p.119). It is merely requested in an email to a manager or Information Technology (IT) helpdesk, or held to be something humanists need to do their work. Little attempt is made to define the critical ground or, much less, to understand the object of enquiry prior to investment. Rather, in denial of the epistemological significance of things, humanities infrastructure is treated as something we should merely go to the store or work with our IT department to buy. The result has been an ongoing failure to provide the kind of infrastructure needed by humanities researchers, a misalignment of the debate, and often a rejection of the very notion of digital infrastructure itself. 
            As Matt Ratto points out, so-called ‘critical making’ provides new ways of dealing with difficult issues like this. Rather than attempting to solve complex problems in their entirety, critical making encourages the development of prototypes and basic models in the context of wider critical discourses, thus blending “practice-based engagement with pragmatic and theoretical issues” and fostering the possibility that creative solutions will be found to long-standing problems. He suggests the approach can be particularly useful in the context of so-called “wicked problems” (Ratto, 2011, p.253), defined by architectural scholar Horst Rittel in the 1960s and 1970s (Rittel and Webber, 1973). This class of problem is characterized by the existence of “many clients and decision makers with conflicting values, and where the ramifications in the whole system are thoroughly confusing” (Rittel was interested in problems associated with large-scale planning projects) (Churchman, 1967, p.B141). Significantly, he claimed there is a moral element to such problems, in that it is immoral to solve only one component of a wicked problem when such an approach will leave the larger issue unresolved. Prototyping and critical making can thus be positioned, not as inadequate tinkering, but as a mode of activity well-suited to the resolution of very complex problems. In this way we come to the intersection of critical making, cyber-infrastructure, and the humanities.
            This project migrated my personal website jamessmithies.org from Wordpress.org (a free, fully hosted service) to a home server running on a Raspberry Pi 2 Model B minimal computer, a device built in the United Kingdom at Sony’s manufacturing plant in Pencoed in South Wales and supported by a registered charity: the Raspberry Pi Foundation. The computer measures 85.60mm x 56mm x 21mm (or roughly 3.37″ x 2.21″ x 0.83″), has 1GB of Random Access Memory (RAM) and is powered by a 900MHz quad-core ARM Cortex-A7 Central Processing Unit (CPU). The VRE application is built using Django, a Python-based web framework designed for newspaper websites but now deployed in a wide variety of scenarios. The social media service Pinterest is one of the largest services to use it, with over 46 million unique visitors between 2011 and 2015 (Statista, 2015). The framework is thus highly adaptable, and could be used to develop almost any functionality a humanities researcher might need. The website is served by the Gunicorn application server and light-weight Nginx web server (used by NASA), with content saved in Postgres, one of the more advanced database systems available. All of these products are available free through the open source community. They require a reasonable level of technical proficiency to install and configure but there are many tutorials available online and their user communities share knowledge openly. It speaks to an interesting aspect of this project. Although there is a massive gap between jamessmithies.org and well-funded cyber-infrastructure projects, the nature of the open source software movement means there is only a small gap (if any) in terms of scalability and potential functionality.
            One of the most powerful things about the project - in both technical and tactical terms - is the level of control conferred by the architecture of the ‘stack’. Not only is the Pi itself accessible and configurable, but its operating system can be changed, and Gunicorn and Nginx can be configured at both an administrative level and through their core code base. Django can be programmed to support an extremely wide range of functionality. To extend the metaphor of control towards the incomprehensibly large infrastructures used by multi-national digital corporates (to escape the criticism that the Pi is a fundamentally limited device, or a mere toy), static files like CSS style sheets and images are hosted on the Amazon Web Services (AWS) cloud, integrating the Pi with a truly enormous global data infrastructure. These could have been hosted on the Pi, but it is considered best practice to deliver them separately for Django projects. It means, essentially, that much of the ‘heavy-lifting’ has been outsourced to a high performance computer, allowing almost limitless options for expansion of the site. 
            Perhaps counter-intuitively given the dominance of ‘bigger is better’ cyberinfrastructure discourse, the migration from Wordpress.org servers to a lowly Raspberry Pi has produced a personal VRE capable of significant further development. The intention is not necessarily to create a finished and reproducible product, but to take control of – and experiment with - all aspects of the computing architecture in order to gain a better understanding of my scholarly infrastructure needs, from the hardware the site runs on, to maintenance of the Internet domain name, the content management system that helps me organize content, and the firewalls that secure it from malicious actors. The conclusion after this phase of the project is that issues like ethical hardware, net neutrality, data sovereignty and security, and the ability to extend and configure the code that supports my research activities, are central to my work – and identity - as a humanities scholar.
        
        
            
                
                    Bibliography
                    
                        Anon., (2015). Pinterest: Unique U.S. Visitors 2015.
                            Statista. April 2015. http://www.statista.com/statistics/277694/number-of-unique-us-visitors-to-pinterestcom/ (accessed July 11, 2015).
                    
                    
                        Churchman, C.W. (1967). Guest Editorial: Wicked Problems. Management Science, 4: 141-42.
                    
                    
                        Dieter, M. and Lovink, G. (2014). Theses on Making in the Digital Age. In 
                            Critical Making. California: Garnet Hertz.
                    
                    
                        Hodder, I. (2014). The Entanglements of Humans and Things: A Long-Term View.
                            New Literary History, 45: 19-36.
                    
                    
                        Ihde, D. (2009). 
                            Postphenomenology and Technoscience: The Peking University Lectures. Albany: SUNY Press.
                    
                    
                        Kirschenbaum, M. (2007). 
                            Mechanisms: New Media and theForensic Imagination. Cambridge Mass.: The MIT Press. 
                      
                    
                    
                        Knorr-Cetina, K. (1992). The Couch, the Cathedral, and the Laboratory: On the Relationship between Experiment and Laboratory in Science. In Pickering, A. (ed.), 
                            Science as Practice and Culture, ed. Chicago: University of Chicago Press, pp. 113-38.
                    
                    
                        Montfort, N. and Bogost, I. (2009). 
                            Racing the Beam: The Atari Video Computer System. Cambridge Mass.: The MIT Press.
                    
                    
                        Preda, A. (1999). The Turn to Things: Arguments for a Sociological Theory of Things. 
                            The Sociological Quarterly, 2: 347–66.
                    
                    
                        Ratto, M. (2011). Critical Making: Conceptual and Material Studies in Technology and Social Life. 
                            The Information Society, 4: 252-60.
                        
                    
                    
                        Rittel, H.W.J. and Webber, M.M. (1973). Dilemmas in a General Theory of Planning. 
                            Policy Sciences, 2: 155-69.
                    
                    
                        Sayers, J. (2014). The Relevance of Remaking. 
                            Maker Lab in the Humanities. November 24, 2014. http://maker.uvic.ca/remaking/ (accessed 01 March 2016).
                    
                
            
        
    

2745	2016	
        
            This project explores the development of a mobile makerspace for graduate and undergraduate DH scholars at the University of Iowa and Grinnell College. Blending the approaches of makerspaces like the DHMakerBus http://dhmakerbus.com> and University of Victoria’s Maker Lab http://maker.uvic.ca/>, this DH makerspace will investigate the use of a suite of tools designed to support the development digital literacy and technological proficiency for students across the DH curriculum. By combining the mobility of the DHMakerBus with the experimental computing of the University of Victoria’s Maker Lab, we will produce a new method of digital humanities pedagogy that welcomes the participation of primary and secondary students and educators, local citizens, and digital humanities practitioners.
            Designed to support experiential learning - learning through making - in undergraduate and graduate classrooms, across campus, and across Iowa, the makerspace focuses on the computer processes that go into making rather than the products produced by them. As such, it emphasizes the process of making itself, whether successful, failed, or flawed.
            In Debates in the Digital Humanities Alexander Reid suggest that most graduate students have had little exposure to digital technology during their undergraduate education, “enter[ing] his or her graduate education as a novice in regards to the digital" (357). It is likely, therefore, that his assessment applies to our broader community of Eastern Iowa. To address the our users’ lack technological expertise, we are scaffolding projects for learners—starting with littlebits, moving to RaspberryPi, and ending with Aruduino. These tools were selected for the pilot study for their mobility, accessibility, and affordability. Each of these factors makes it easy to implement and replicate the makerspace in unconventional venues—secondary schools, adult education classes, and community events. Additionally, these tools gradually increase participants’ comfort and literacy with digital tools.
            
                Te(a)chnology
                This mobile DH makerspace utilizes three “gateway” technologies selected for their robust design, sophisticated abilities, and ease of use, and portability:
                
                    
                        littleBits - a digital building kit based on the logic of Lego that allow students and makers to build digital tools and explore the internal logic of computers 
                    
                    
                        Raspberry Pi - a small, but powerful computer, about the size of a credit card with a GUI interface and Python encoding that integrates touchscreens and digital cameras  
                    
                    
                        Arduino Kit for littleBits - build on the capabilities of the littleBits library by adding the Arduino computer, teaching students how to expand their coding skills by working with Java-based programming 
                    
                
                Together these three technologies scaffold to develop users’ confidence in coding, building, project management, and digital literacy. This particular suite of tools is easily transportable to classrooms and campuses, or may be shipped to students participating in online sections of workshops and seminars. Furthermore, the mobility of the makerspace allows for collaborations with non-traditional and non-academic communities including secondary students, adult education seminars, community partners, and interested citizens. Participants will come away from workshops utilizing this mobile makerspace with increased knowledge of computer processes, an awareness of design thinking, and the confidence to collaborate on a variety of projects with diverse teams.
            
            
                Methodology
                This poster will report on the results of a pilot study of the mobile DH makerspace. Participants in the pilot study included undergraduate and graduate students and faculty completing design challenges and product tests. Working with a convenience sample of graduate and undergraduate students, the makerspace was tested in two modes: (1) students working together in the same space and (2) students working across a distance, collaborating virtually through videoconferencing and other digital collaboration tools.
                We assessed the increased digital literacy and confidence of participants through a series of surveys, visualization exercises, focus group interviews, and participant observation. The results of this pilot study will inform the development of individual course units and workshops for courses within the DH Curriculum at the University of Iowa and Grinnell College, in addition to workshops for faculty and staff across both campuses and outreach efforts to reach the off-campus community.
            
            
                Conclusions
                This mobile makerspace affords opportunities for makers to build confidence creating and experimenting with digital tools while allowing students to build digital literacy skills and address other key aspects of DH education, including: working in interdisciplinary teams, applying digital practices, managing projects, and explaining technology (Rockwell and Sinclair 182-183). Ultimately, we argue that this model can be emulated in other educational settings as a new model for DH pedagogy that is more accessible and more collaborative than traditional makerspaces.
                A special thanks to Miriam Posner for generously agreeing to review this abstract and provide feedback and suggestions.
            
        
        
            
                
                    Bibliography
                    
                        Reid, A. (2012). Graduate Education and the Ethics of the Digital Humanities. In Gold, M. K. (ed), 
                        Debates in the Digital Humanities. Minneapolis: University of Minnesota Press, pp. 350-67.
                    
                    
                        Rockwell, G. and Sinclair, S. (2012). Acculturation and the Digital Humanities Community. In Hirsch, B. D. (ed), 
                        Digital Humanities Pedagogy: Practices, Principles, and Politics. Cambridge: Open Book Publishers, pp. 177-211.
                    
                
            
        
    

2759	2016	
        
            The seventeenth century Dutch language did not know a standardized spelling. Because of this, many different spelling variants of the same word (i.e. 
                ik and 
                ick, both meaning 
                I) coexist. This may largely be the result of an author’s own preferences, background or the book printer he used.
                
                    
                         It is not known who exactly was responsible for the spelling as it was printed. But due to there being examples of texts printed by the same printer that use radically different spelling forms it is plausible that the author of the text was responsible for the spelling.
                    
                 Software to standardize, analyze or process texts is mostly developed for modern Dutch, so processing historical texts with natural language processing (NPL) tools or analysing such texts on stylometric aspects is problematic. Due to the orthographic variation of historical texts, software will not always recognize words that have the same meaning, but different spelling.
            
             For project COLEM we want to normalize spelling differences by having digital re-speller tools form a standardized spelling variant, that could help software better understand the texts. We investigate the possibilities to provide the original text with an orthographic layer containing the normalized words. In this way, the original texts and its morfo-syntactic information are still accessible and it will be possible to search both the original text and the layer. This, for instance, will ease research to language evolution. 
            The Java software VARD2 (Baron 2011; Baron and Rayson, 2008) seems to be a useful tool for this purpose.
                
                    
                         VARD is an acronym for Variant Detector.
                    
                 This tool was originally created to normalize old English texts, but can be adjusted to other languages. VARD2 will compare the words in the input text with an incorporated, but easily adaptable wordlist (a .txt-file). We replaced the default wordlist with a Dutch lexicon and we trained VARD2 on texts of two different seventeenth century Dutch authors: Simon de Vries and Gotfried van Broekhuizen. Texts of these authors are characterized by a significant different orthography and this could therefore help us to train normalizers and test them on different spelling forms existing within the Early Modern Dutch language. 
            
            We trained VARD2 by replacing the variants in the historical texts with a normalization. VARD2 will present suggestions for normalization by using four methods of which just one is language dependent (a modified version of the Soundex phonetic matching algorithm that is based on English phonemes). Two other methods, that of letter rules and that of known variants, we adapted by replacing the default .txt-files ‘letter rules’ and ‘known variants’. The last method, a normalized Levenshtein Distance, does not need to be adapted, since it is language independent. 
             In this presentation we will show the performances of our trained VARD2 tool. We will focus on a specific amount of problems the tools run into, like uncommon words, clitics and combined words. We will also investigate the possibilities of the Norma
                
                    
                         Norma is written in C++11, though bindings for Python are provided as well. 
                    
                 tool (Bollman, 2012) and TICCLops
                
                    
                         TICCLops v.0.2(Text Induced Corpus Clean-up online processing system) is a web application (offered in a JavaScript interface) that is intended to detect and correct typographical errors and OCR (optical character recognition) errors in text. It is usable for every language, since it bases its replacements on the input corpus by making Most Frequent Words-list. However, TICCLops is probably less usable for providing a text with an annotation layer, because the replacements take place in the texts itself, without preserving an original version of the text. 
                            
                        
                    
                 (Reynaert, 2014). By comparing the results that various tools offer, we will decide what tool(s) is/are most successful in dealing with these problems. This could give us ideas for follow-up research or the development of tools for normalizing Early Modern Dutch, but probably also for normalizing other languages with unstable orthographies.
            
        
        
            
                
                    Bibliography
                    
                        Baron, A. (2011). 
                        Dealing with spelling variation in Early Modern English texts. Ph.D. thesis, University of Lancaster. 
                    
                    
                        Baron, A. and Rayson, P. (2008). VARD2: A tool for dealing with spelling variation in historical corpora. 
                        Proceedings of Postgraduate Conference in Corpus Linguistics. Birmingham: Aston University, May 2008.
                    
                    
                        Bollmann, M. (2012). (Semi-)automatic normalization of historical texts using distance measures and the Norma tool. In Mambrini, F., Passarotti M. and Sporleder C. (eds), 
                        Proceedings of the Second Workshop on Annotation of Corpora for Research in the Humanities (ARCH-2). Lisbon, Portugal, pp. 3-14.
                    
                    
                        Reynaert, M.W.C. (2014). Synergy of Nederlab and PhilosTEI: diachronic and multilingual Text-Induced Corpus Clean-up. In 
                        Proceedings of the Sixth International Language Resources and Evaluation (LREC'14). Reykjavik, Iceland, 2014. 
                    
                
            
        
    
2785	2016	
        
            
                Starting Point
                The project objective of 
                    Digitizing 
                    Early 
                    Farming 
                    Cultures (DEFC) is the standardization and integration of research data from sites and finds from the Neolithic and Copper Age (7000–3000 BC) located in Greece and Western Anatolia. These datasets are based on digital and analog resources of research projects of the research group Anatolian Aegean Prehistoric Phenomena (AAPP) at the Institute for Oriental and European Archaeology (OREA) of the Austrian Academy of Sciences.
                
                Greece and Western Anatolia are two neighbouring and archaeologically closely related regions. They are, however, usually studied in isolation from each other and have therefore developed different terminologies and chronologies. Direct results of this de facto separation are not only huge amounts of fragmented research data but also several different models and standards for ordering and describing more or less the same kind of data. To pose and answer archaeological research questions concerning the whole territory, the information must be harmonized.
                The aim of the DEFC project is now to harmonize the existing data, to digitize analog resources and make metadata available to facilitate access and reuse this data. To achieve those goals an archaeological data management system is needed.
            
            
                Data model and Application
                The particular requirements to the data model are to reflect the high granularity of the archaeological data structure which correlates on different levels to the excavation process workflow, geographical location, chronological periodization and at the same time to keep the complex relationships between the data objects. After an evaluation of already existing solutions for managing (archaeological) data (e.g. Microsoft Access, Arches Project) it turned out that those were not comprehensive enough for modeling and capturing the very heterogeneous datasets the DEFC project is confronted with. Therefore the development of a more customizable application to collect, standardize, analyze and visualize archaeological data was necessary.
                To meet the needs of researchers a clear conceptual data model based on archaeological objects relationships has been defined with the following main model classes:
                
                    Site (location where research took place/observations were made)
                    Research Event (project and type of archaeological research that was carried out)
                    Area (particular part of the site, defined by its geolocation, period, as well as its type)
                    Finds (artefacts, animal and plant remains found)
                    Interpretation (archaeologist's interpretation of areas/finds etc.)
                
                Each of those classes is defined through several properties, most of them linked to a carefully curated set of controlled vocabulary.
                
                    
                  Figure 1. Simplified data model
                
                The DEFC-App is based on the Python web framework Django. As one of the application's design principles is to keep things as simple as possible, the application tries to leverage Django´s built-in generic functionality as far as possible. The application's web interface is based on Bootstrap. Client-side scripting, which is needed for a better user guidance and enabling a more responsive data querying and presentation, is implemented with JavaScript, jQuery, Tablesorter and Leaflet.
                
                    
                  Figure 2. Site details page
                
                
                    
                  Figure 3. Create new Research Event page
                
                
                    
                  Figure 4. Referencing Geonames
                
            
            
                Development and Upcoming tasks
                The project aims to integrate open access resources by using Web APIs and Linked Data practices. At the time being Geonames referencing is implemented for the archaeological locations and provided via a user interface. Hence the fetched Geonames IDs are stored within the database to later be linked to the Pelagios project.
                The bibliographic data formerly stored in proprietary formats MS-Access and AskSam was imported to a Zotero library and linked to the DEFC-Database so that every reference record in DEFC redirects to a Zotero library record, where the entire bibliography can also be explored.
                To make the published data available ‘open access’ for further reuse in research, a REST-API (Django REST framework) was implemented along with the web user interface for querying and exporting data.
                The outlook of the project is to turn the data into Linked Open Data and make it available via a SPARQL endpoint. Moreover, the thesaurus consisting of hierarchically structured archaeological data units (respectively the aforementioned controlled vocabulary) has been partially mapped to the CIDOC CRM ontology and will later be mapped to the SKOS schema. This will, overall, enhance the quality of the RDF data in the future.
            
            
                Conclusion
                The development of the DEFC-App and its underlying data model could be understood as a very common use case in the broad field of digital humanities as it involves a tight cooperation between archaeologists, data analysts and developers. 
            
        
        
            
                
                    Bibliography
                    
                        Arches project. [Online] Available from: 
                        http://archesproject.org/ [Accessed 4 March 2016].
                    
                    Christian Bach. 
                        Tablesorter. [Online] Available from: 
                        http://tablesorter.com/docs/ [Accessed 4 March 2016].
                    
                    
                        DEFC-App. [Online] Available from: 
                        http://defc.digital-humanities.at/ [Accessed 4 March 2016].
                    
                    Django Software Foundation and individual contributors. 
                        Django. [Online] Available from: 
                        https://www.djangoproject.com/ [Accessed 4 March 2016].
                    
                    
                        Django REST framework. [Online] Available from: 
                        http://www.django-rest-framework.org/ [Accessed 4 March 2016].
                    
                    
                        Geonames. [Online] Available from: 
                        http://www.geonames.org/ [Accessed 4 March 2016].
                    
                    
                        Pelagios: Enable Linked Ancient Geodata In Open Systems. [Online] Available from: 
                        http://pelagios-project.blogspot.co.at/p/about-pelagios.html [Accessed 4 March 2016].
                    
                    Vladimir Agafonkin. 
                        Leaflet. [Online] Available from: 
                        http://leafletjs.com/ [Accessed 4 March 2016].
                    
                    
                        Zotero. [Online] Available from: 
                        https://www.zotero.org/ [Accessed 4 March 2016].
                    
                
            
        
    

2801	2016	
        
            
                Sea and Spar Between by Stephanie Strickland and Nick Montfort is a poetry generator that repurposes, combines and re-mixes words and phrases from Emily Dickinson’s poems with those taken from Herman Melville’s 
                Moby Dick into an (almost) endless sea of stanzas (the program produces about 225 trillion stanzas arranged on a toroidal surface). In his keynote speech to the U.S. Library of Congress, Stuart Moulthrop defined 
                Sea and Spar Between as both “an immensely long-form computational poem” and “a remarkably compact poem generator.” Transferring such a complex e-literary work into a different language and cultural context raises vital questions regarding the very nature of translation and adaptation in the digital age.
            
            The first Polish version of the program, presented by us in 2013 at Electronic Literature Organization Conference in Paris and published online in 2014 in Techsty (both as a program and as a glossed code with Montfort's and Strickland's comments), greatly multiplied the distributive authorship of the work as a whole, revealing new culturally and linguistically determined aspects of code, grammar and style, and adding a complex layer of interdependencies. 
            
                See and Spar Between poses a translational challenge which in some languages might seem impossible to accomplish. Polish, our target language, imposed some serious constraints: one-syllable words became disyllabic or multisyllabic; kennings taken from Melville’s work required a different morphological, lexical and grammatical arrangement; and most of the generative rhetoric of the original (like anaphors) had to take into consideration the grammatical gender of Polish words. As a result, the javascript code, instructions that accompany the javascript file, and arrays of words that the poetry generator draws from, needed to be expanded and rewritten. Moreover, at several crucial points of this rule-driven work, the nature of Polish language forced us to modify the code. 
            
            In 2015, we started work on porting the 
                Sea and Spar Between generator from its javascript+html5 web browser context into XBOX 360 Kinect motion-sensing environment, where the input is controlled by user gestures. This adaptation, which will be available both in English and Polish will be semantically oriented. We aim to use Kinect affordances to program various haptic gestures recognized by the sensor in a way that ensures coherence between interactive gestures and the content of the poem, between the user’s haptic activity and their cognitive processes. Moving from one platform to another, from the Web to Kinect, will involve translating mouse and keyboard gestures (point-and-click, numerical input) into a gesture vocabulary of sensor technology. As an adaptation, our work aims at incorporating the dominant themes of Strickland and Montfort’s work into user movements during the multidimensional navigation of the work. If the generated stanzas are compared to fish in the ocean, the screen to an infinite canvas and the reader’s navigation to a sea voyage, then Kinect port transforms these very metaphors from subject of “translation” into a finite set of gestures that are in sync with the work’s semantics and the authorial intentions behind the generator. For this reason alone, the port to Kinect cannot be a translation of a more radical type, in which Dickinson and Melville are replaced with any other poets from any other language.
            
            Once again, the question arises of what we translate/adapt/port when we translate a digital work of art for digitally enhanced venues and for an audience of “digital natives.” 
            In the course of translation of 
                Sea and Spar Between and its adaptation for different platforms, the process of negotiation between the source language and the target language involves the factors unseen in traditional translation. Strickland and Montfort read Dickinson and Melville and parse their readings into a computer program, which as a translation, or port, from Python to javascript, is already a derivative. This collision of cultures, languages and tools is amplified when transposed into a different language. The transposition involves the original authors of 
                Sea and Spar Between, the original translators of Dickinson and Melville into Polish, and us, turning the process into a multilayered translational challenge, something we propose to call a distributed translation. The forthcoming port to Kinect makes these issues even more challenging and exciting. 
            
        
        
            
                
                    Bibliography
                    
                        Montfort, N., Strickland, S. (2013). Cut to fit the toolspoon course. 
                        Digital Humanities Quarterly, 7(1).
                    
                    
                        Montfort, N., Strickland, S. (2014). Spars of Language Find at Sea. 
                        Formules, 18. 
                    
                
            
        
    

2829	2016	
        
            The HathiTrust Digital Library holds digitized copies of nearly 15 million scanned volumes from libraries around the world. These volumes are a significant resource for large-scale research: with their scale and breadth of material, a digital humanities scholar can make new inferences on history, language use, cultural trends, or even the structure of the printed word. However, access is complicated by the complexities of navigating copyright laws around the world, while use of the materials is impeded by the effort and technical demands of a researcher. To address both of these issues, the Extracted Features (EF) dataset from the HathiTrust Research Center (HTRC) provides volumes in a format that has already been cleaned, extracted, and tagged for computation use.
            In this hands-on tutorial, participants will learn to use the Extracted Features dataset for text analysis alongside the HTRC Feature Reader library, equipping them to perform research on millions of publicly-accessible volumes. Through the HTRC Feature Reader, participants will be make use of popular data science tools in Python for EF dataset analysis, and will be left with demonstrative materials to repurpose in their future work.
            
                Data
                The Extracted Features (EF) dataset from the HathiTrust Research Center (Capitanu et al., 2015) provides an open and permissive download of page-level extracted information for every page of 4.8 million volumes from the HathiTrust Digital Library. A “feature” refers broadly to a quantitative measure of some property in a dataset; for example, the number of times a word appear on a page. The EF data features include part-of-speech-tagged term counts, line and sentence counts, counts of the characters occurring on the far left and far right side of a page, and inferred language probabilities. Most notably, this information is provided 
                    for every page. Also, headers, body, and footer have been identified and features are provided separately for each part.
                
                In the tutorial, participants will learn the significance of each feature, such as using line counts and character information to identify the type of content on a page, or using part-of-speech tags for improving topic models based on content.
            
            
                Skills
                This tutorial introduces participants to introductory text analysis in Python using the Extracted Features dataset with the HTRC Feature Reader. This includes accessing term counts and other raw information, slicing within that data, visualization trends within or across books, and leading into advanced techniques like topic modeling and sentiment tagging.
                The skills taught in this tutorial are underpinned by programming in Python using a popular set of data science libraries. All code examples are provided, though they are most useful if participants are comfortable in tinkering and have a familiarity with Python's basic conventions. Our intention is to make the code examples transparent enough so as to be easily modifiable by beginner users.
                A participant completing the workshop will understand:
                
                    the structure and possibilities of the HTRC Extracted Features Dataset;
                    how to access the EF dataset files, both for individual and bulk use;
                    how to start a Jupyter notebook, an accessible browser-based approach to data science in Python;
                    the fundamentals of reading volume files, accessing metadata, and slicing and grouping token lists;
                    basic visualization of EF data; and
                    an advanced analytic technique modeled on recent digital humanities methods, discussed below.
                
                The first part of the tutorial teaches the fundamental skills for working with the HTRC Feature Reader. For the final exercise of the tutorial, participants have a choice from prepared advanced exercises, which instructors will assist individually. This structure accommodates more intensive approaches in the time given, while also leaving participants with more examples for practicing their newly-acquired skills in the future.
                The advanced exercises to be provided are: classification of paratext, using features suggested by Underwood (2014); visualization of sentiment trends in books as a proxy for a plot arc as previously performed by Jockers (2015), and within-book topic modeling using the inference approach presented by Organisciak et al. (2015).
            
            
                Summary
                The EF dataset offers a diverse and incredibly large collections of works for analysis in an easily accessible way. By providing these works as already extracted data, the EF dataset covers a large part of the text analysis workflow for researchers and is thus particularly suited for learners. This tutorial will use EF to teach text analysis through Python, using new software called the HTRC Feature Reader. By the end, students will be able to slice, group, and manipulate individual volumes for their needs, and will be familiar with techniques for modeling texts, identifying pertinent pages, and plotting trends across books.
            
        
        
            
                
                    Bibliography
                    Capitanu, B., Underwood, T., Organisciak, P., Bhattacharyya, S., Auvil, L., Fallaw, C., J. and Downie J. C. (2015). 
                        Extracted Feature Dataset from 4.8 Million HathiTrust Digital Library Public Domain Vol.  2,[Dataset]. HathiTrust Research Center, doi:10.13012/j8td9v7m.
                    
                    Jockers, M. L. (2015). Revealing Sentiment and Plot Arcs with the Syuzhet Package. 
                        Matthew L. Jockers. Blog. http://www.matthewjockers.net/2015/02/02/syuzhet/.
                    
                    Organisciak, P., Auvil, L. and Downie J. S. (2015). Remembering books: A within-book topic mapping technique. 
                        Digital Humanities 2015. Sydney, Australia.
                    
                    Underwood T. (2014). 
                        Understanding Genre in a Collection of a Million Volumes. Interim Report. https://dx.doi.org/10.6084/m9.figshare.1281251.v1.
                    
                
            
        
    

2837	2016	
        
            Brief Description
            The application of computational tools to textual data is a growing area of inquiry in the humanities. From the culling of “Culturomics” via the 30 million document Google books collections, to the painstakingly detailed process of analyzing the text of Shakespeare’s plays to ascertain their ‘true’ creator, a wide range of techniques and methods have been employed and developed. Text analysis in the humanities has also garnered an impressive level of interest in the mainstream media. For example, a study analyzing the relationship of a professor’s gender to their teaching reviews and an overview of Franco Moretti’s ‘distant reading’ both recently appeared in the New York Times. The Atlantic featured an historical critique of the language used in the period drama ‘Mad Men’, where textual analysis of the script revealed departures from the standard American English spoken in the 1960s. The majority of this work, however, relies on techniques such as n-grams and bag-of-word models. Recent developments in computational linguists, which have attempted to mimic the complex process by which humans parse and interpret language, are finding increased use within the humanities. 
            This workshop will introduce the basic components of modern natural language processing. Techniques include tokenization, lemmatization, part of speech tagging, and coreference detection. These will be introduced by way of examples on small snippets of text before being applied to a larger collection of short stories. Applications to stylometric analysis, document clustering, and topic detection will be briefly mentioned along the way. Our focus will be on a high-level, conceptual understanding of these techniques and the potential benefits of using them over models commonly employed for text analysis within humanities research. We will also introduce open-source software that is available for a wide range of programming languages (i.e., Java, R, Python, Ruby, Perl) and applicable for parsing an increasingly large number of natural languages (i.e., English, French, Spanish, Chinese, German, Turkish, Arabic). The workshop is based on a chapter from the instructor’s book Humanities Data in R: Exploring Networks, Geospatial Data, Images, and Text (Spring, 2015).
            Instructors
            Taylor Arnold, is currently a lecturer in the department of statistics at Yale and senior scientist at ATandT Labs. His research focuses on the analysis of large, complex datasets and the resulting computational challenges. A particular area of focus is the sparse representation of highly structured objects such as text corpora and digital images. He is the technical co-director of the NEH funded project Photogrammar. Together with Lauren Tilton, he is the co-author of the text Humanities Data in R: Exploring Networks, Geospatial Data, Images, and Text.
            Lauren Tilton, is a doctoral candidate in American Studies at Yale University. She is the Co-Director of Photogrammar (photogrammar.yale.edu) and Participatory Media (http://participatorymediaproject.org/). Research interests include 20th century U.S. history and visual culture as well as digital and public humanities. She is the co-author with Taylor Arnold of Humanities Data in R: Exploring Networks, Geospatial Data, Images, and Text. She will be joining the faculty at the University of Richmond as a Visiting Assistant Professor of Digital Humanities in Fall 2016.
            
                Target Audience and Size
            
            This workshop is accessible to participants from all backgrounds.
            
                Brief Outline of Workshop
            
            
                Introduction to NLP
                Tokenization and Sentence Splitting
                Lemmatization
                Part of Speech Tagging
                Dependencies
                Named Entity Recognition
                Coreference resolution
                Overview and comparison of current software
                    
                        Stanford CoreNLP
                        Apache OpenNLP
                        spaCy.io
                    
                
            
        
    

3777	2017	Summary
Reproducing experimental results is a hallmark of empirical investigation and serves both to verify and inspire. This paper is a call for more systematic documentation of computational stylistic experiments. Publishing only summaries of the methods and results of empirical work is an artifact of traditional print media. To facilitate experimental reproducibility and to help the growing community who wish to learn how to apply computational methods and subsequently teach the next generation of scholars, the publication of results must include (i) access to the digitized texts, (ii) a clear workflow and most essentially (iii) the source code that led to each and all of the experimental results. By way of example, we present the steps and process in a GitHub repository for computationally probing the unknown and contested authorship of an 1831 short story entitled “A Dream” as we seek evidence if this work is similar to other attributed works by Edgar Allan Poe. The entire framework is intended as a pedagogical jumpstart for others, especially those new to computational stylometry. If Poe did write the story, it would be his first published work.

Introduction
As the Digital Humanities gains access to a wide array of digitized corpora and matures to a discipline that creatively defines new methods for computationally close and distant readings, a growing gap has emerged between those who apply sophisticated programming, e.g., Stylo In R (Eder et al., 2016) and those who are new to the game and need an introduction to the field. Typical of the community spirit in DH, significant efforts are underway to bridge this gap, including web-based tools for entry-level exploration including

Voyant Tools (Sinclair and Rockwell, 2016) and Lexos (Kleinman et al., 2016) and domain-specific introductions to programming, including Jockers' text (2014) and the Programming Historian (Crymble et al., 2016). This paper attempts to narrow the gap by encouraging both sides to document their experimental methods more fully to embrace previous calls for the replication of experimental methods (Rudman, 2012 et al.) and thereby teach effective practices by “leaving a trail” of experimental methods that enable others to execute and extend.

A Good Mystery: Towards Reproducibility
A GitHub repository or “repo” offers a workflow

that explores whether an 1831 story published under

the attribution of only ‘P' might have been written by Edgar Allan Poe. If so, it would be Poe's first published work. In addition to sharing a set of analytical methods applied in this experiment, the broader methodological-pedagogical goals are two-fold: (i) the dissemination of data and code should be championed as a cornerstone of DH research, thereby facilitating the replication of results and (ii) to share a workflow so that others may apply similar analyses to their texts of interest.

The workflow is stored as a set of numbered folders containing the texts and scripts (code) needed to complete each step. The workflow includes: collecting texts, the preprocessing, tokenization, and culling decisions made, unsupervised cluster analyses (k-means, hierarchical-agglomerative, bootstrap consensus tree), and supervised classification methods using Stylo in R's Delta, SVM, and NSC models. Each step represents scaffolding for a “teachable moment” with materials provided so faculty can more easily use them with students.

Scrubbing, Tokenization, Cutting, and
Culling
Lexos, a web-based, open-source workflow of tools (Kleinman, et al., 2016) was used to upload texts and “scrub” them by applying the following options: (i) convert words to lowercase, (ii) all punctuation was removed, (iii) however, a single word-internal hyphen and word-internal apostrophes were kept, and (iv) all digits were removed. Each individual word is considered as its own token. Larger stories were segmented (“cut”) into pieces. We experimented with various culling options, e.g., keeping only the most frequent words that appear in each text at least once.

Cluster Analysis
As a set of initial probes, we compared the contested story “A Dream” to (i) other stories attributed to Poe and (ii) mixed in with stories by other contemporaries. In the repo, we share four variations using cluster analysis:

1. K-means clustering on only Poe's stories (using Lexos)

2. Hierarchical agglomerative clustering on only Poe's stories (uses a Python sklearn module and a script to convert the cluster to ETE and Newick formats)

3. K-means clustering when all stories by each author are concatenated together (Lexos)

4. Bootstrap Consensus Tree (using Stylo in R).

The result from the Bootstrap Consensus Tree is shown in Figure 1. Of interest is that each author's stories cluster consistently together (with the exception that Bird's initial section of “Sheppard Lee” and his “Calavar” are found in different clades, at six and eight o'clock). “A Dream” clusters with the smaller Poe texts. As you'll see, we couldn't resist tossing in the four stories sometimes attributed to Edgar's brother Henry (“Monte Video”, “A Fragment”, “The Pirate”, and “Recollections”). These four stories are found within the cluster of Poe's known works (c.f. Collins, 2013).

A series of cluster analyses often serves well as a preliminary exploration, especially for scholars who are new to this game. Some of the file sizes are very small (e.g., one-half of the Poe stories in this corpus have fewer than 2000 words) and when strict culling is enforced (top-N words that appear at least once in each segment), the available set of words is reduced to only 38 when dealing with “A Dream” and the other eighteen Poe stories. That noted, these exploratory investigations shed some light on why some scholars consider that Poe's “first published tale may have been ‘A Dream'” (Silverman, 1991, p87).


Figure 1. Using Stylo in R Bootstrap Consensus Tree (BCT) showing “A Dream” consistently clustering with other Poe stories. The BCT aggregates results over multiple cluster analyses and shows those texts that satisfy a consensus number of the individual trials. Using 12 different authors

and at least two texts by each author for a total of 46 stories, Stylo formed clusters of the texts for the following frequency bands when using the most-frequent words: 100 to 1000 MFW.

Classification
Three classification models differentiated authorial writing style as implemented in Stylo in R. We scripted in R alongside Stylo to test “A Dream” over N-trials (N=10, 100) using a random selection of files for training sets in each trial. At least one text from each author is also included in the test set for each trial. A followup Python script parses the collected results to build confusion matrices for each author to provide metrics on how well the models predict each author's works. The most-frequently occurring, top-40 words (MFW, 1-grams) that appear in all the texts at least once were used.

Confusion Matrix values for all Poe Stories

Model

Attributions of

“A Dream” to Poe

True+

True-

False+

False-

Delta

9

13

200

0

7

NSC

10

16

170

30

4

SVM

7

14

198

2

6

Table 1: Attributions of the contested story “A Dream” over ten (10) trials with “A Dream” and another randomly selected Poe story in the test set in every trial. Confusion matrix values for results of testing Poe texts over all trials provide overall measures of model effectiveness. In the three cases

where “A Dream” was attributed to a different author, Poe was ranked second.

Summary
We offer a start to an exploration to collect evidence

as to whether Poe may have written the 1831 story “A

Dream” (c.f., Schoberlein (2016) who used the most frequent character 3-grams and attributed the story to Poe using Delta, but not so when using NSC nor SVM models). Evidence and methods aside, a GitHub repo provides a framework to share experimental workflows in a spirit similar to Jupyter notebooks, as well as one that facilitates both reproducible results and opportunities for subsequent contributions.

Notes
Forming an appropriate corpus is hard: thanks to Sam Coale, Ryan Cordell, Cary Gouldin, David Hoover, Shirrel Rhoades, and Ted Underwood. Four undergraduates: Weiqi Feng, Alec Horwitz, Jingxian Liu, and Khaled Sharafaddin worked with us on this problem. Thanks to Maciej Eder for his help with Stylo in R.

Sinclair, S. and Rockwell, G., (2016). Voyant Tools. Web: http://voyant-tools.org/.

Bibliography
Crymble, A., Gibbs, F., Hegel, A., McDaniel, C., Milligan, I., Taparata, E., Visconti, A., and Wieringa, J.,

eds. (2016). The Programming Historian. 2nd ed.. Web: http://programminghistorian.org/.

Eder, M., Kestemont, M. and Rybicki, J. (2016). Stylometry with R: A package for computational text analysis. R Journal, 16(1): 107-121.

GitHub repository:    A Good    Mystery.    .

https://github.com/WheatonCS/aGoodMystery

Jockers, M. (2014). Text Analysis with R for Students of Literature. Springer, New York.

Kleinman, S., LeBlanc, M.D., Drout, M., and Zhang, C.

(2016). Lexos v3.0. Web: http://lexos.wheatoncol-lege.edu.

Rudman, J. (2012). The State of Non-Traditional Authorship Attribution Studies -- 2012: Some Problems and Solutions. English Studies, v93(3), 259-274.

Schoberlein, S. (2016). Poe or Not Poe? A Stylometric Analysis of Edgar Allan Poe's Disputed Writings. Digital Scholarship in the Humanities, July 24, 2016.

Silverman, K. (1991). Edgar A. Poe: Mournful and Never-Ending Remembrance. HarperCollins, New York.
3780	2017	Introduction
This panel reports on the open, shareable, and reproducible workflow methodology for digital humanities research developed by the 4Humanities.org "WhatEvery1Says" (WE1S) project. WE1S is topic modeling a large corpus of articles related to the humanities in newspapers, magazines, and other media sources in the U.S., U.K., and Canada from 1981 on. While the panel presents WE1S's conceptual goals and prototype experiments in using outcomes in humanities advocacy, its focus is on the technical and interpretive workflow developed by the project for humani-ties-oriented data work. WE1S's manifest system for data provenance and workflow management, its virtual workspace manager for integrated, containerized data manipulation and processing, and its interpretation protocol for how humans read topic models suggest a generalizable open approach based not on particular technologies and methods but on annotated methods. Moreover, there is a philosophical fit between such an approach and the public-facing goals of the WE1S project. WE1S is about opening public culture to view through analytics, while its DH methodology is about opening up scholarly expertise itself through shareable, transparent processes not locked into technically complex, pre-established, or largescale research frameworks.

Project Research and Advocacy Goals WE1S uses topic modeling to explore the idea of

"the humanities" in public discourse. A complex concept of the kind that Peter de Bolla treated in his 2013 The Architecture of Concepts (his main example: "human rights"), "the humanities" as they are perceived are both tightly bunched in academic disciplines and broadly dispersed in extra-academic domains. Discussion focused on "the humanities crisis," "the decline in humanities majors," etc. creates flash points in the discourse. Yet the overall heat map of articles about the humanities, WE1S discovers, also extends into vast stretches of warm or cool discussion about humanities subjects intricately interwoven into the background of other domains of social life. Even articles so seemingly unremarkable (yet fully remarkable when we think about it) as an obituary or wedding announcement can mention the humanities as part of their donnée. WE1S seeks to open to view this whole conceptual architecture of "the humanities" as it exists in robust, living relation with culture at large.

Project Methodological Goals

Due to the lack of widely shared technical conven

tions and appropriate scholarly and publishing practices, today it is very difficult for a DH scholar to answer with documentation such questions as: Where did you get those thousands of works in your corpus? Where did the metadata come from? What steps did you take to prepare and process the material? How many variations did you try? Where in the process was it critical for there to be "humans in the loop"? The WE1S project addresses a growing need for ways to share and reproduce data-workflow in digital humanities research in order to make DH comparable to "open science" (see Bare, 2014). Indeed, data-inten-sive work in the sciences offers an especially good paradigm because of the degree to which it makes workflow and provenance management itself a

thoughtful research field (i.e., research about, and not just tools for, managing workflow and provenance) (e.g., see Gil et al., 2007; and Garijo et al., 2012). The WE1S project is developing a technical framework that explores how the digital humanities can evolve similar, but also necessarily different, humanities-

adapted standards of openness, shareability, and re

producibility. What amount of data and metadata, in

what detail, at which processing stages, with what accompanying scripts, and so on, should be shared to support rich and persuasive scholarly discourse

based on digital humanities research in the future?

How will the criteria of "reproducibility, replication,

and generalizability" (on the different shades of

meaning of these terms in the sciences, see Bollen et

al., 2015) join more traditional ideas of excellence in

the humanities (e.g., "critical rigor") in the various

contexts of collecting, curation, exhibition, editing,

analysis, interpretation, and other work?

Overview

Alan Liu

WE1S explores public thought about the humanities, especially as mediated in journalistic articles that stage a dialogic relation between leaders in government, business, universities, the arts, and others with citizens. The project's end goal is to use such research to guide humanities advocacy. But rather than create a one-off project, the WE1S group has developed a robust technical and interpretive methodology comparable (though customized for DH) to scientific data workflow management systems (e.g., Apache Taverna, Kepler, Wings), provenance tracking systems (e.g., ProvONE), and similar schemes (see Gil et al., 2007; and Bose and Frew, 2005).

Manifest System for Data Provenance and Workflow Management
Scott Kleinman

The WE1S manifest schema uses a JSON Schema-based model to produce "manifests" that document the provenance of articles studied by WE1S as well as later transformations of the data, tying together scripts, stop word lists, outputs, visualizations, etc.

used in project work. Manifests make the workflow transparent and facilitate on-the-fly reiterations or ad-justments--e.g., staging a subset of the WE1S corpus for topic modeling or defining variant numbers of topics. Manifests are human-readable JSON files that are highly interoperable with other systems; they can be used programmatically to drive scripts or to crosswalk information to other workflow tools or metadata frameworks. The WE1S workflow management system uses the manifest schema to generate web forms, enabling non-technical users to create and query manifests, which are stored using the same JSON-like format in its MongoDB database. The system is easy to deploy and can be adapted for other humanities research projects simply by modifying the manifest schema.


Fig. 1: Web interface for WE1S manifest system.

Virtual Workspace Manager for Integrated, Containerized Data Manipulation and Processing

Jeremy Douglass

To address a range of computing demands from geographically distributed participants, the WE1S Workspace Manager facilitates open, reproducible DH research through a defined computing platform, a shareable online environment, integrated customizable workflows, and on-demand publishing of results. Tools for topic modeling workflows are configured on a virtual machine (a Docker container). Open data science notebooks (iPython / Jupyter) are the interface. Project templates are collections of notebooks (Python, R) chained into flows. Each new data exploration flow customizes a template, imports manifest data (from the WE1S manifest system), builds a topic model (Mallet), generates a visual browser (Andrew Gold-stone's dfr-browser), publishes the browser to an interactive website, and packages a project for download and offline viewing. WE1S hosts a shared workspace online; it also runs on a laptop. Design and implementation of this virtualized, integrated workflow environment may be relevant to other DH projects, and is consonant with the philosophy of such other online or containerized integrated systems as Lexos or DH Box designed to make advanced DH research environments accessible.


Fig. 2 Architecture of WE1S virtual workspace manager for integrated topic-modeling and visualization as implemented in a Docker virtual machine.

Constructing a "Random" Comparison Corpus
Lindsay Thomas

In public discourse, there are no natural boundaries between what does and does not count as "hu-manities-related" discussion. The humanities, for example, can appear in both precise and general ways: as a focal topic, as part of arts and culture, in particular forms (such as literature), as part of social and ethical concerns, as part of the biographies or obituaries of individuals, etc. Indeed, it may be that one feature of the humanities is their capacity to forge multiple links between tightly focused and general themes. There is thus no pre-definable "control corpus" of public discussion on the humanities that can serve as ground truth for WE1S's topic modeling experiments. WE1S is thus using a sampled, "random" corpus as a snapshot of the larger, unclosed set of media articles to assist in exploring by contrast what articles can sensibly be defined as "humanities-specific." Doing so not only constitutes a novel approach to topic modeling in the digital humanities; it also reveals intriguing issues about the philosophy behind statistical randomization (see

Holland, 1986). This part of the panel discusses the

"random" corpus WE1S created, its use within the

WE1S project workflow, and includes theoretical reflections on incorporating methodology borrowed from the sciences and social sciences in DH work.

"Interpretation Protocol" for Topic Models.

Ashley Champagne

One of the needs in DH research is a for way to declare not just technical but interpretive workflows so that they can be shared, reproduced, and evolved by the research community. In the case of DH topic model studies, for instance, rarely are there transparent descriptions of the interpretive assumptions, steps, and iterations needed to decide how many topics to seek, what topics are interesting, how the topic model guides the human interpreter back to specific articles for examination (and vice versa), and how groups of researchers collaborate in using a topic model to generate hypotheses or come to conclusions. WE1S has created an initial declaration of its topic-model interpretation process that defines step-by-step interactions between machine learning and human interpre-tation/collaboration (e.g., when in the process humans convene to interpret a topic model; what outputs, visualizations, and secondary algorithmic products such as clusterings are used; how humans discuss a topic model; how topic models and interpretive acts are iterated; etc.). The goal is to make it possible for the larger DH community to improve or vary the topic-model interpretation process in open, shareable ways.

Prototyping How the WE1S Project Can Guide Humanities Advocacy

Jamal Russell

When WE1S has completed its topic models and interpretive studies, it will produce a public-facing site allowing others to explore the models and follow links to the original articles. But how can the project fulfill its ultimate ambition of guiding humanities advocacy? This final part of the panel reports on a unique early experiment in applying WE1S research. In 2016-17, a funded group of undergraduates studied sample articles from the WE1S corpus under the guidance of the project's topic models. They wrote a white paper on their findings with recommendations for humanities advocacy. And they created practical advocacy projects based on those recommendations. Using this concrete example as a springboard, the panel concludes by reflecting on the relationship between interpreting topic models and creating publicly accessible narratives about the humanities.

Bibliography

4Humanities: Advocating for the Humanities. Home page, n. d. http://4humanities.org.

4Humanities: Advocating for the Humanities. "'What Every One Says About the Humanities' Research Project (WhatEvery1Says)." 25 April 2013. http://4humani-ties.org/2013/04/what-everyone-says-about-the-hu-

manities-research-project.

Apache Taverna (Taverna Workflow System). Home page, n. d. https://taverna.incubator.apache.org.

Bare, C. (2014). "Guide to Open Science." Digithead's Lab Notebook, 9 January 2014. http://digitheadslabnote-book.blogspot.co.uk/2014/01/guide-to-open-sci-

ence.html.

de Bolla, P. (2013). The Architecture of Concepts: The Historical Formation of Human Rights. New York: Fordham University Press.

Bollen, K., J. T. Cacioppo, R. M. Kaplan, J. A. Krosnick, J. I. Olds. "Social, Behavioral, and Economic Sciences Perspectives on Robust and Reliable Science -- Report of the Subcommittee on Replicability in Science Advisory Committee to the National Science Foundation Directorate for Social, Behavioral, and Economic Sciences." National Science Foundation, 2015. http://www.nsf.gov/sbe/AC_Materials/SBE_Ro-

bust_and_Reliable_Research_Report.pdf.

Bose, R., and J. Frew. (2005). Lineage Retrieval for Scientific Data Processing: A Survey. ACM Computing Surveys

37.1:    1-28.    https://pdfs.seman-

ticscholar.org/3a05/2feb019328487068c8efc4c5dced8

eb51a87.pdf.

DH Box. Home Page, n. d. CUNY Graduate Center.

http://dhbox.org.

Garijo, D., P. Alper, K. Belhajjamey, O. Corcho, Y. Gil, and C. Goble. "Common Motifs in Scientific Workflows: An Empirical Analysis." 2012 IEEE 8th International Conference on E-Science, 2012: 1-8. DOI: 10.1109/eSci-ence.2012.6404427.

Goldstone, A. dfr-browser, v. v0.8a (June 8, 2016). Home page, n. d. http://agoldst.github.io/dfr-browser/.

Holland, P. W. "Statistics and Causal Inference." Journal of the American Statistical Association, 81.396 (1986): 945-960.

http://www.tandfonline.com/doi/abs/10.1080/01621

459.1986.10478354.

iPython Notebook. (See Jupyter Notebook.)

Jupyter Notebook (formerly iPython Notebook). Home page, 5 March 2017. http://jupyter.org/.

Kepler Project. Home page, n. d. https://kepler-pro-ject.org.

Kleinman, S., LeBlanc, M.D., Drout, M. and Zhang, C.

Lexos. v3.0. 2016.

https://github.com/WheatonCS/Lexos/.

doi:10.5281/zenodo.56751.

Lexos. (See Kleinman et al.)

Mallet. (See McCallum, A. K.)

McCallum, A. K. "MALLET: A Machine Learning for Language Toolkit." 2002/2016. http://mal-let.cs.umass.edu/.

ProvONE. "ProvONE: A PROV Extension Data Model for Scientific Workflow Provenance -- Unofficial draft." 27 March 2014. http://vcvcompu-

ting.com/provone/provone.html.

WINGS (Semantic Workflow System). Home page, n. d. http://www.wings-workflows.org.

Gil, Y., et al. (2007). "Examining the Challenges of Scientific Workflows." IEEE Computer, 40.12: 24-32. https://pdfs.seman-

ticscholar.org/e45d/4aedd10229cbbeef8b2ec009f87ae

1a4065e.pdf.
3782	2017	Summary
We propose a way to work with the stylometric distance measure Delta to analyse the subgenre of texts written by different authors. For that, we neutralize the author signal by penalizing the texts from the same writer, allowing the texts to have their shortest distances to other authors' works. We test this method with several subcorpora of Spanish prose and a corpus of French theatre.

Stylometry and Delta beyond Authorship

Since John Burrows proposed it in 2002, Delta has been one of the most used and researched methods in stylometry and authorship attribution. Burrows explained it as “expression of difference, pure difference” (2002: 269) and is based on basic statistical concepts

like most frequent words, z-scores and the Manhattan

distance between each pair of texts.2 Burrows closes his paper with an unanswered question about why Delta works so well.

Other researchers such as Hoover (2004b: 454), Argamon (2008), Plasek (2014) or Evert et al (2015:

79) have confirmed that we are still far from being able to answer this question. This lack of understanding has not stopped the stylometric community of trying to improve Delta (Hoover 2004a; Argamon 2008; Eder 2013). Smith and Aldridge (2011) have proposed Cosine Delta which gives the best results in different languages (Jannidis et al. 2015).

Since Delta is sensitive to aspects or signals like genre or period (Burrows 2002), the corpora for authorship attribution tend to be homogenous in those aspects. Research has been conducted to try to separate signals (Schöch 2013 and 2014) or selecting the words that contribute to them using Recursive Feature Elimination (Büttner and Proisl 2016). Jannidis and Lauer (2014) and Hoover (2014) show how Delta can be used to distinguish genre and periods within the works of a single author. Other researchers have used other methods such as classification (Hettinger et al., 2016; Underwood 2014) or logistic regression (Jock-ers 2013; Riddell and Schöch 2014) to similar ends.

Neutralizing Author Signal in Delta

Our proposal is to neutralize the author signal directly on the Delta matrix. We use a testing corpus of texts from three Spanish authors and three subgenres.

Detailed information about the corpora, files, parameters and scripts is in our GitHubrepository.-We applied Cosine Delta (5000 MFW) with Stylo (Eder, Rybicki and Kestemont 2016) and visualized the resulting distance matrix with Python:

Hierarchical Clustering Dendrogram (Ward)


sample index

Figure 1: Dendrogram from Cosine Delta

As expected, the texts are clustered by author, with sub-clusters of subgenres. The underlying Delta Matrix contains distances between all texts:



Figure 2: Cosine Delta Matrix

We see a tendency of lower Delta values for documents of the same author (below 1.0) in comparison to documents of different authors (above 1.0). But what about the closest texts written by a different author? For the historical novel in column E, they are in the rows 14 and 15 and are historical novels, as well. This pattern is found for the majority of the texts. How could we cluster the texts preferring the closest text from other authors? And if we are able to neutralize the author signal, will we see noise or subgenre clusters?

Our proposal is to penalize the distances between the texts of the same author (cf. Lu and Leen 2007 for penalization in image clustering), making them closer to the average distance of texts of different authors, then cluster the neutralized distance matrix and measure the cluster homogeneity by author and subgenre.

We define the set of all documents by an author a as Aa, the collection containing all documents by all authors as C and total number of documents in the collection is defined as c:

Aa •—    ‘ * i

C = {Ai, • • • , An}

c:=|UC|

Note that each document is in exactly one author-document set Ai.

First, we calculate the average distance of texts of all pairwise different authors (in fig. 2, all the distances

in black). We call this value the mean of different authors or M(C) and for this collection its value is 1.16.

53 A(di,dj)
Aa , EC,a/6

EMI • (c-Ml)
A





For each author, we subtract his/her mean value from the mean of different authors M(C) - M(Aa) resulting in the difference of the author. This value represents how far the texts of a specific author are to the mean of different authors:4

author

mean

difference

Miro

0.607

0.552

Baroja

0.669

0.490

Valle    0.752

0.407

Figure 3: Means and differences of author Third, we add the difference of the author M(C) - M(Aa) to the Delta values of text of the same author. This gives a Neutralized Delta-function as follows:

ydi E Aa, dj E Ab

A^U^’^    f°ra/6

v 31    [A(</j,dj) + (Ai(C)-M(4o)) for a = 6

This converts the table from Figure 1 into a Neutralized Delta matrix:


Figure 4: Author-Neutralized Delta matrix

The values in grey are now in general above 1.0: the texts of the same author have been separated, showing relations between texts independently of authorship. Now the adventure and historical novels of Baroja in columns C and D have their closest text in works of different authors but belonging to the same subgenre.

Second, we calculate the mean of the texts of each author a M(Aa) (in fig. 2, the distances in grey).


Hierarchical Clustering Dendrogram (Ward)


sample index

Fig. 5: Author-neutralized Delta dendrogram

In comparison with Figure 1, this dendrogram allows us to see new text relations beyond authorship but within subgenre, showing clusters with different authors but the same subgenre: for example, the cluster of historical novels by Baroja and Valle or the two very close subclusters of erotic novels by Miró and Valle.

Tests and Evaluation

For the evaluation, the homogeneity of the clusters (Rosenberg and Hirschberg, 2007) was measured. This measure yields values between 0 and 1. As ground truth, the metadata about author and subgenre have been used. The results for the dummy corpus:


Figure 6: Homogeneity of Cosine and Neutralized Delta for author and subgenre

The homogeneity of the clusters of Cosine Delta (see fig. 1) are perfect for authors and much lower for subgenre, because the author clusters contain subgenre subclusters. The homogeneity of the clusters of Neutralized Delta (see fig. 5) is lower for authorship (as expected), but not for subgenre. In this case the neutralization of the author signal only deteriorates the homogeneity for authorship but improves the homogeneity for subgenre.

We have analysed different subgenres present in the whole corpus for test the method. We created subcorpora of historical, bildungsroman, erotic and adventure novels:5


Figure 7: Homogeneities for Spanish prose subcorpora

As expected, the neutralization consistently deteriorates the homogeneity for author (between -0.26 and -0.1) while the homogeneity for subgenre is not deteriorated (between -0.08 and 0.06). The homogeneity for subgenre of adventure compared to erotic and bildungsroman get the best results (over 0.9) and they even improved on results with Cosine. Adventure novels are also best recognized in classification tasks (Hettinger et al. 2016). Subgenres which are very difficult to differentiate like historical and adventure (Pedraza

Jiménez and Rodríguez Cáceres 1983: 672 and 1987: 459) get one of the worst results.

The results are similar when testing other corpora,

such as a corpus of French drama (Schoch et al. 2015) and a corpus of Spanish American novels:


Figure 8: Homogeneity values for French drama and Spanish American novels

Conclusion and future work

Our main goal was to present a method to neutralize the Delta distances of the same author using the difference between the mean of the author and the mean of different authors. Tested on eight subcorpora, this procedure, as we expected, deteriorates the homogeneity of authorship clusters but maintains the subgenre homogeneity, improving it for some cases. That discovers relations between texts (see fig. 5) that were hidden by authorship. This procedure brings a new way of working with Delta beyond authorship attribution.

Both Cosine and Neutralized Delta show very different results for the comparison of different subgenres, something which points to the different internal structure of the subgenres. The comparison of very different subgenres (like adventure against erotic or bildungsroman) gets higher subgenre cluster homogeneity. Neutralized Delta could be used for comparing different corpora of specific subgenres and test the significance of the results to better characterize these subgenres. In an ideal scenario, we would like to test on a perfect balanced corpus where a set of authors are represented in all subgenres of the same period.

For future work, we will analyse how different parameters like versions of Delta or number of MFW affect the results. We also plan to transfer the approach to an earlier step in the Delta procedure and penalize the word z-score vectors.

We look forward to the feedback of the international DH community about this new use of the very effective “expression of difference, pure difference” which is Delta.

Acknowledgements
To avoid confusion regarding intellectual property, we would like to make it clear that the main idea and implementation are the work of the first author. Other authors have brought important remarks, feedback, some of the corpora and have helped with the redaction and the formalisations.

Bibliography
Argamon, S. (2008). Interpreting Burrows’s Delta: Geometric and Probabilistic Foundations. Literary and Linguistic Computing, 23(2): 131-47.

Burrows, J. (2002). ‘Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship. Literary and Linguistic Computing,    17(3):    267-87 http://revistacarac-

teres.net/revista/vol5n1mayo2016/entendiendo-delta.

Büttner, A. and Proisl, T. (2016). Stilometrie interdisziplinär: Merkmalsselektion zur Differenzierung zwischen Übersetzer- und Fachvokabular. DHd 2016: Modellierung, Vernetzung, Visualisierung. Leipzig: Universität Leipzig, pp. 66-69 http://www.dhd2016.de/ab-stracts/sektionen-002.html.

Calvo Tello, J. (2016). Entendiendo Delta desde las Humanidades. Caracteres. Estudios culturales y críticos de la esfera digital, 5(1): 140-76.

Eder, M. (2013). Bootstrapping Delta: a safety-net in openset authorship attribution. DH2013. Lincoln: UNL https://sites.google.com/site/computationalstylis-tics/preprints/m-eder_bootstrapping_delta.pdf?attredi-rects=0.

Eder, M., Kestemont, M. and Rybicki, J. (2016). Stylometry with R: A package for computational text analysis. The R Journal, 16(1): 1-15 https://journal.r-project.org/ar-chive/accepted/eder-rybicki-kestemont.pdf.

Evert, S., Proisl, T., Jannidis, F., Pielström, S., Schöch, C. and Vitt, T. (2015). Towards a better understanding of Burrows’s Delta in literary authorship attribution. Proceedings of the Fourth Workshop on Computational Linguistics for Literature. Denver CO: Association for Computational Linguistics, pp. 79-88 .

Hettinger, L., Reger, I., Jannidis, F. and Hotho, A. (2016). Classification of Literary Subgenres. DHd 2016. Leipzig: Universität    Leipzig,    pp.    154-58

http://dhd2016.de/boa.pdf.

Hoover, D. L. (2004a). Testing Burrows’s Delta. Literary and Linguistic Computing, 19(4): 453-75.

Hoover, D. L. (2004b). Delta Prime?. Literary and Linguistic

Computing, 19(4): 477-95.

Hoover, D. L. (2014). A Conversation Among Himselves: Change and the Styles of Henry James. In Hoover, D. L., Culpeper, J. and O'Halloran, K. (eds), Digital Literary Studies. New York & London: Routledge, pp. 90-119.

Jannidis, F. and Lauer, G. (2014). Burrows's Delta and Its Use in German Literary History. In Erlin, M. and Tatlock, L. (eds), Distant Readings. Topologies of German Culture in the Long Nineteenth Century. Rochester: Camden House, pp. 29-54 gerhardlauer.de/index.php/down-load_file/view/335/1/.

Jannidis, F., Pielstrom, S., Schoch, C. and Vitt, T. (2015).

Improving Burrows' Delta - An empirical evaluation of text distance measures. DH 2015. Sydney: ADHO http://dh2015.org/abstracts/xml/JANNIDIS_Fotis_Im-

proving_Burrows__Delta___An_empi/JANNIDIS_Fo-tis_Improving_Burrows__Delta___An_empirical_.html.

Jockers, M. L. (2013). Macroanalysis - Digital Methods and Literary History. Champaign, IL: University of Illinois Press.

Schöch, C. (2014). Corneille, Molière et les autres. Stilometrische Analysen zu Autorschaft und Gattungszugehörigkeit im französischen Theater der Klassik. In

Schöch, C. and Schneider, L. (eds), Literaturwissenschaft

im digitalen Medienwandel. pp. 130-57 http://web.fu-

berlin.de/phin/beiheft7/b7t08.pdf.

Schöch, C., Henny, U., Calvo Tello, J. and Popp, S. (2015). The CLiGS Textbox. Würzburg: University of Würzburg

https://github.com/cligs/textbox.

Smith, P. W. H. and Aldridge, W. (2011). Improving Authorship Attribution: Optimizing Burrows' Delta

Method. Journal of Quantitative Linguistics, 18(1): 6388

Underwood, T. (2014). Understanding Genre in a Collection of a Million Volumes, Interim Report. https://figshare.com/articles/Understand-ing_Genre_in_a_Collection_of_a_Million_Volumes_In-

terim_Report/1281251

Lu, Z. and Leen, T. K. (2007). Penalized Probabilistic Clustering. Neural Computation, 19(6): 1528-67

Pedraza Jiménez, F. B. and Rodríguez Cáceres, M.

(1983). Manual de literatura española. 7: Época del realismo. Pamplona: Cénlit.

Pedraza Jiménez, F. B. and Rodríguez Cáceres, M.

(1987). Manual de Literatura Española. 9: Generación de

Fin de Siglo: Prosistas. Pamplona: Cénlit.

Plasek, A. (2014). Incommensurability? Authorship, Style, and the Need for Theory. DH2014: Lausanne: ADHO http://dharchive.org/paper/DH2014/Paper-755.xml.

Riddell, A. and Schoch, C. (2014). Progress through Regression. Digital Humanities DH2014:. Lausanne: ADHO http://dharchive.org/paper/DH2014/Paper-60.xml.

Rosenberg, A. and Hirschberg, J. (2007). V-Measure: A conditional entropy-based external cluster evaluation measure. Prague: Association for Computational Linguistics, pp. 410-20 https: //aclweb.org/anthol-

ogy/D/D07/D07-1043.pdf.

Schoch, C. (2013). Fine-tuning Our Stylometric Tools: Investigating Authorship and Genre in French Classical Theater. DH2013. Lincoln: UNL

http://dh2013.unl.edu/abstracts/ab-270.html.
3786	2017	Introduction

The digital humanities (DH) are perhaps unique amongst humanities endeavours. They force us to confront the conceptual and ethical implications that attend the union of the humanities with engineering and organizational thinking. They demand attention to tools, methods, ethics, and pedagogy, but also organizational bureaucracy, human resource management, economics, and systems maintenance. Rather than merely prompting a bland mechanization of the humanities, as critics suggest, this offers a fascinating epistemological challenge. It challenges us to rethink how human meaning and knowledge are constructed, and how they will be remade as the twenty-first century progresses. This requires a step-change in our epistemic, ethical, and collective assumptions as much as our methods.

These issues are distilled in laboratory settings. Sociologist of science Karin Knorr-Cetina (1999) notes that labs function as blended communities, uniting researchers with technicians and administrators. Access to equipment, chemicals, data, and funding, impact the production of knowledge as much as pure research questions. Issues of finance and organizational power compete with creativity and the need for diversity. Scientists have been gaining insight into this since the nineteenth century, fostering traditions (and injustices) that are well understood . (Adas, 1999). Digital humanities labs challenge us to create parallel traditions, appropriate to the humanities and GLAM communities. King's Digital Lab, launched in November 2016, embraces this challenge, positioning itself as an experiment in infrastructural as well as conceptual terms.

Postphenomenological perspectives current in the philosophy of technology and Science and Technology Studies (STS) help explain our approach. Writers like Donald Ihde (2009b) and Peter Kroes (Verbeek, 2010) embrace the entanglement of humans with technological tools, systems, and processes, and meditate on the material reality that informs our experience of the world. They reject the Heideggerean critique of technology, based on the modernist division of subject and object, in favour of acceptance that entanglement with culture, technology, and ideology is not only unavoidable but provides a window into the nature of human experience (Ihde 2009a). Rather than mechanizing the soul of the humanities, digital humanities laboratories force us to confront our entanglement with technology, along with its enabling infrastructures and ideologies.

King's Digital Lab

King’s Digital Lab (KDL) builds on a 30-year legacy in digital humanities at King’s College London. The lab represents one half of a new digital humanities model, in conjunction with the Department of Digital Humanities (DDH). KDL provides software development and infrastructure to departments in the faculties of Arts & Humanities and Social Science and Public Policy, focusing on software engineering and implementing the systems and processes needed to produce high quality digital scholarly outputs. The department focuses on delivering quality teaching to its postgraduate students and growing cohort of undergraduates, and producing research outputs in line with its status as an academic department. In combination KDL and DDH include close to 40 staff, host 160 projects, served 130 million webpage views in 2014, and teach over 200 students.

KDL’s business, operational, and human resource plans define its research values alongside its business and technological model. It has been established with 12 permanent full-time staff: director; project manager; analysts, software engineer, developers, designers; and systems manager. Contract and temporary staff are used as infrequently as possible, ideally to offer student experience in a software development environment. The HR model is explicitly designed to foster sustainable #alt-ac Research Software Engineering (RSE) career paths. All KDL team members, permanent or contract, are encouraged to use 10% of their time on personal projects (either on their own or in collaboration with colleagues), leading to work with Raspberry PIs, virtual reality, and an interest in maker culture.

The KDL model is based on deeply felt humanistic values, but reflects a level of organization required to manage entanglement with technological systems. The lab manages over 90 projects, including up to 20 that are active in some form, and ~5 million digital objects. The team manage over 180 virtual machines, on an infrastructure that uses 400 GB of RAM and 27TB of data. New infrastructure platforms are being trialed that include access to cloud and high performance computing options, in a nod towards a future working with big data, visualization, and simulation. The goal is to facilitate a transition from twentieth to twenty-first century modes of computationally intensive humanities and social science research, but to do so in consciously humanistic terms.

In a rejection of a simplistically ‘mechanized’ future, development tools are proactively managed and the lab has a ‘design first’ philosophy (Verbeek, 2006). This is partly a way to manage the considerable complexities that come with advanced DH research and the delivery and management of multiple projects, but it also recognizes that digital tools and methods are, at their best, beautiful. Aesthetic and quality values can extend from front-end design to technological platforms, code, tools, and methods. Data, similarly, can and should be beautiful, not only in adherence to appropriate technical standards but in its conformance to scholarly best practice and deep domain knowledge. Infrastructure and systems, likewise, are always compromised by their material design and ideology (Russell,

2014)    , but decisions to choose open source components and emphasize access and sustainability enhances control and agency (Friedman et al,

2015)    .

To reduce complexity and improve sustainability, the lab uses the Python programming language, and Django web framework in preference to other options. The full technology stack is consciously oriented towards open source components, and a balance between functionality and sustainability.


Figure 1

This level of organization helps us manage technology, but also promotes critical awareness: the current technological state of the lab is far from perfect, but it is under control and guided by known critical values. The concept of the ‘laboratory’ is important in this context. Rather than mechanization, it implies experimentation and risk, but also a certain intellectual seriousness. Scientists learned what a laboratory means to them over a century ago; the humanities and social sciences are only just starting to explore the implications. They are profound, not only in terms of the epistemological implications of putting tools between the researcher and the object of study (with inevitable technical constraints), but in terms of the ideological implications of using industry approaches to software development and financial management. Consciousness of this ensures the lab is sustainable, and can continue to support scholarship as well as the careers of our team members.

The technological inheritance of the lab is considerable. It includes over 90 live web-based projects, built using heterogeneous tools and programming languages by the (historic) Centre for Computing in the Humanities, (historic) Centre for eResearch in the Humanities, and the Department of Digital Humanities. Funding agencies paid for them to be built, but not to sustain them. Some Primary Investigators (PIs) have retired, or are no longer in contact with King's. Support for these projects is currently borne by the lab, generously supported by the Faculty of Arts & Humanities, but is being managed by an evolving archiving and sustainability plan that will assess each of the projects, determine their intellectual merit, and work with their owners to find the best way to maintain or archive them. The archiving and sustainability model used for this task will be published, as well as being included in the lab's Software Development Lifecycle (SDLC), to ensure sustainability will be considered on day one of every new project.

The organizational chart of King's Digital Lab is flat rather than hierarchical, reflecting an aspiration to be role-based and collaborative: a shared intellectual and scholarly space that exists to experiment with new approaches as well as deliver projects on time and budget. The scale is such that the lab design has needed to be outsourced to multiple authors: director and project manager working with line management to define the business plan and financial model, analysts and developers developing the software life-cycle, UI developer leading the design vision, systems manager ensuring the infrastructure and networking model is appropriate. Together, it amounts to something complex and technologically dependent, but redeemed through a philosophy of shared ownership and conscious experimentation.

Bibliography
Adas, M. (1989). Machines as the Measure of Men:

Science, Technology, and Ideologies of Western

Dominance. Cornell Studies in Comparative History. Ithaca: Cornell University Press.

Friedman, B., Kahn, P.H., and Borning, A. (2015).

“Value Sensitive Design and Information Systems.” In Human-Computer Interaction and Management Information Systems: Foundations. New York: Routledge.

Ihde, D. (2009) ‘Foreword', in Jan Kyrre Berg Olsen et al, New Waves in Philosophy of Technology. Basingstoke: Palgrave Macmillan, 2009), p.xii.

Ihde, D. (2009). Postphenomenology and Technoscience: The Peking University Lectures. SUNY Series in the Philosophy of the Social Sciences. Albany: SUNY Press.

Knorr-Cetina, K. (1999). Epistemic Cultures: How the Sciences Make Knowledge. Cambridge, Mass: Harvard University Press.

Russell, A.L. (2014). Open Standards and the Digital Age: History, Ideology, and Networks. Cambridge: Cambridge University Press.

Verbeek, P. (2006). “Materializing Morality

Design Ethics and Technological Mediation.” Science, Technology & Human Values 31 (3): 361-80.

Verbeek, P. (2010). What Things Do: Philosophical Reflections on Technology, Agency, and Design.

Penn State Press.
3794	2017	Lexos is a browser-based suite of tools that helps lower barriers of entry to computational text analysis for humanities scholars and students. Situated within a clean and simple interface, Lexos consolidates the common pre-processing operations needed for subsequent analysis, either with Lexos or with external tools. It is especially useful for scholars who wish to engage in research involving computational text analysis and/or wish to teach their students how to do so but lack the time for a manual preparation of texts, the skill sets needed to prepare their texts analysis, or the intellectual contexts for situating computational methods within their work. Lexos is also targeted at researchers studying early texts and texts in non-Western languages, which may involve specialized processing rules. It is thus designed to facilitate advanced research in these fields even for users more familiar with computational techniques. Lexos is developed by the Lexomics research group led by Michael Drout (Wheaton College), Mark LeBlanc (Wheaton College), and Scott Kleinman (California State University, Northridge). It is built on Python 2.7-Flask microframework, with jQuery-Bootstrap UI, and visualizations in d3.js. The Lexomics research group provides access to an public installation of Lexos which does not retain data after a session has expired. Users may also install Lexos locally by cloning the GitHub repository.

Lexos guides users through a workflow of steps that reflects effective practices when working with digitized texts. The workflow includes: (i) uploading Unicode-encoded texts in plain text, HTML, or XML formats; (ii) “scrubbing” functions for consolidating preprocessing decisions such as the handling of punctuation, white-space, and stop words, the use of lemmati-zation rules, and the handling of embedded markup tags and special character entities; (iii) “cutting” texts into segments based on the number of characters, tokens, or lines, or by embedded milestones such as chapter breaks; (iv) tokenization into a Document Term Matrix of raw or proportional counts using character or word n-grams; (v) visualizations such as comparative word clouds per segment (including the ability to visualize topic models generated by MALLET); Rolling Window Analysis that plots the frequency of string, phrase, or regular expression patterns or pattern-pair ratios over the course of a document or collection; and (vi) analysis tools including statistical summaries, hierarchical and k-means clustering, cosine similarity rankings, and Z-tests to identify the relative prominence of terms in documents, document classes, and the collection as whole. At each stage in the workflow the user may download data, visualizations, or the results of the analytical tools, along with metadata about their preprocessing decisions or the parameters selected for their experiments. Lexos thus enables the export of data for use with other tools and facilitates experimental reproducibility.

Lexos {scrubber} An Integrated Lexomics Workflow

Scrubbing Options

Q Remove All Punctuation

B Keep Hyphens ©

Q Make Lowercase

B Keep Word-Internal Apostrophes©

Q Remove Digits

■ Remove Whitespace 0

B Scrub Tags 0

Additional Options

Stop Words/Keep Words O >


Previews of Documents


A1.3_Dan_T00030.txt




Gefr&ae;gn ic Hebreos eadge lifgean in Hierusalem goldhord d&ae;lan cyningdom hab ban swa him gecynde w&ae;s si&d;&d;an &t;urh metodes m&ae;gen on Moyses hand w

ealra gesceafta drihten and waldend se him dom forgeaf unscyndne bl&ae;d eor&d;an rices and &t;u lignest nu &t;&ae;t sie lifgende se ofer deoflum duge&t;um wealde&d;. A3.3_Az_T00130.txt


orn dryhten herede wis in weorcum ond &t;as Word acw&ae;&d;: Meotud allwihta &t;u eart meahtum swi&d; ni&t;as to nerganne. Is &t;in noma m&ae;re wütig ond wul h




Special Characters 0 v


Lemmas 0 v


Consolidations 0 v


Figure 1: The Lexos Scrubber Tool

Lexos addresses three significant challenges for our intended users. The first challenge involves the adoption of computational text analysis methods. Many approaches require proficiency with command line scripting or the use of complex user interfaces that require time to master. Lexos addresses this problem through a simple, browser-based interface that manages workflow through the three major steps of text analysis: pre-processing, generation of statistical data, and visualization. In this, Lexos resembles Voyant Tools (Sinclair and Rockwell, 2016), although Lexos places more emphasis on and providing more tools for preprocessing and segmenting texts. Lexos also shares with tools like Stylometry with R (Eder, et al., 2013; Eder, 2013) and emphasis on cluster analysis, providing both hierarchical and K-Means clustering with silhouette scores as limited form of statistical validation. While Lexos is not a topic modeling tool, it provides a useful “topic cloud” feature for MALLET data that will be useful for beginners since there are few accessible ways to visualize MALLET output that work well out of the box.


Figure 2: The Lexos Multicloud tool showing Chinese "topic clouds"

The second challenge is the opacity of the procedures required to move between computational and traditional forms of text analysis. In order to reduce the “black boxiness” of algorithmic methods, Lexos contains an embedded component called “In the Margins” which provides non-technical explanations of the statistical methods used and effective practices for

handling situations typical of humanities data. “In the

Margins” is a Scalar “book” which can be read separately; however, its individual pages are embedded in Lexos using Scalar's API, making them easily accessible for users of the tool. Lexos shares with tools like Voyant an engagement with the hermeneutics of text analysis and attempts to embed “In the Margins” discussion of these issues in the user interface close to the user's workflow. We hope “In the Margins” will host advice and commentary from contributors with the Digital Humanities community.

A third challenge is the tension between quantitative and computational approaches and the traditions of theoretical and cultural criticism that dominate the humanities in the academy. As Alan Liu (2013) has recently argued, the challenge is to give a better theoretical grounding to the hybrid quantitative-qualitative method of the Digital Humanities by exploring the ways in which we negotiate the difficulties imposed by “the aporia between tabula rasa quantitative interpretation and humanly meaningful qualitative interpretation” (414). The design of Lexos and the discussions in “In the Margins” are intended to open a space for discussion of issues related to the opacity of algorithmic approaches and the limitations and epistemological challenges of computational stylistic analysis and visual representation of humanities data.

This poster presentation provides demonstrations of Lexos using some literature from Old, Middle, and Modern English, as well Chinese, which are in our current test suite. We also discuss use cases and best practices, how to install Lexos locally, and how scholars may contribute to the still growing content of “In the Margins”.

Bibliography

Drout, M., Kleinman, S., and LeBlanc, M. 2016-. “In the Margins.” http://scalar.usc.edu/works/lexos./

Eder, M. (2013). “Mind Your Corpus: Systematic Errors in Authorship Attribution.” Literary and Linguistic Computing 28 (4): 603-14.

Eder, M., Kestemont, M., and Rybiki, J. 2013. “Stylometry with R: A Suite of Tools (Abstract of Poster Session)”. Presented at Digital Humanities 2013, Lincoln, Nebraska. http://dh2013.unl.edu/abstracts/ab-136.html, https://sites.google.com/site/computationalstylistics/

Kleinman, S., LeBlanc, M.D., Drout, M. and Zhang, C. 2016. Lexos v3.0. https://github.com/WheatonCS/Lexos/.

Liu, A. (2013). “The Meaning of the Digital Humanities.” PMLA 128 (2): 409-23.

McCallum, A.K. (2002). MALLET: A Machine Learning for

Language Toolkit. http://mallet.cs.umass.edu.

Sinclair, S., and Rockwell, G. (2016). Voyant Tools. Web.

http://voyant-tools.org/.
3799	2017	In recent years, the Latent Dirichlet allocation (LDA) topic model (Blei, Ng, and Jordan, 2003) has become one of the most employed text mining techniques (Meeks and Weingart 2012) in the digital humanities (DH). Scholars have often noted its potential for text exploration and distant reading analyses, even when it is well known that its results are difficult to interpret (Chang et al, 2009) and to evaluate (Wallach et al, 2009).

At last year's edition of the Digital Humanities conference, we introduced a new corpus exploration method able to produce topics that are easier to interpret and evaluate than standard LDA topic models (Nanni and Ruiz, 2016). We did so by combining two existing techniques, namely Entity linking and Labeled LDA (L-LDA). At its heart, our method first identifies a collection of descriptive labels for the topics of arbitrary documents from a corpus, as provided from the vocabulary of entities found within wide-coverage knowledge resources (e.g., Wikipedia, DBpedia). Then it generates a specific topic for each label. Having a direct relation between topics and labels makes interpretation easier, and using a disambiguated knowledge resource as background knowledge limits label ambiguity. As our topics are described with a limited number of unambiguous labels, they promote in-terpretability, and this may sustain the use of the results as quantitative evidence in humanities research (Lauscher et al, 2016).

The contributions of this poster cover the release of: a) a complete implementation of the processing pipeline for our entity-based LDA approach; b) a three-step evaluation platform that enables its extensive quantitative analysis.

Entity-based Topic Modeling Pipeline
Figure 1 illustrates the computational pipeline of our system; python classes are represented in rectangles. First of all, a set of text files is imported into the system and several preprocessing steps are applied to

the textual content. Next, the data is sent to the entity

linking system TagMe (Ferragina and Scaiella, 2010), which disambiguates against Wikipedia. As a result of this step, for each document a set of related Wikipedia entities is retrieved. Now, the data is inserted into a MySQL database.


Figure 1. Architecture of the pipeline

Afterwards, the TF-IDF measure is computed over the entities, which we use to rank all the entities for each document in descending order. Then, the top k entities as well as their corresponding documents are exported into a comma-separated values file that is given as input to the L-LDA implementation of the Stanford Topic Modeling Toolbox. Finally, after running L-LDA and applying several post-processing steps, we obtain a document-topic distribution saved in the database in which each topic is described by an unambiguous label linked to Wikipedia.

The whole source code is available for public download on Github. Given a working Python, Java, and Scala runtime as well as a running MySQL installation our pipeline is ready directly out-of-the-box. The specific configuration according to the user's needs can be made via a simple text file.

Three-Step Evaluation Platform
Document Labels

In order to assess the quality of the detected entities as labels we developed a specific browser-based evaluation platform, which permits manual annotations. This platform presents a document on the right of the screen and a set of possible labels on the left (See Figure 2). Annotators are asked to pick labels that precisely describe the content of each document. In case the annotator does not select any label, this is also recorded by our evaluation system.

Entities and Topic Words

In order to establish if the selected entities were the right labels for the topics produced, we developed two additional evaluation steps. Inspired by the topic intrusion task (Chang et al, 2009), we designed a platform that permits to evaluate the relations between labels and topics using two evaluation modes: For one evaluation mode (that we called Label Mode - Figure 3), the annotator is asked to choose, when possible, the correct list of topic-words given a label. For the other, he/she was asked to pick the right label given a list of topic words (aee Figure 4). In both cases, the annotator is shown three options: one of them is the correct match, while the other two (be they words or labels) come from other topics related to the same document.


Figure 2. Entities as Labels evaluation interface.


Figure 3. Label-Mode Evaluation

Option 1: Agriculture

Option 2: Organization

Option 3: Business cycle

A HOME    «2 LABEL MODE

Figure 4. Term-Mode Evaluation

Bibliography
Blei, D. M., Ng, A.Y., and Jordan, M. (2003) "Latent dirichlet allocation." Journal of machine Learning research.

Chang, J., Boyd-Graber, J.L., Gerrish, S., Wang, C. and Blei,

D.M. (2009) "Reading tea leaves: How humans interpret topic models." Advances in neural information processing systems. 21. 288-296.

Ferragina, P., and Scaiella, U. (2010) “TAGME: On-the-Fly Annotation of Short Text Fragments (by Wikipedia Entities).” In Proceedings of the 19th ACM International Conference on Information and Knowledge Management.

Lauscher, A., Nanni, F., Ruiz Fabo, P., and Paolo Ponzetto,

S. (2016). "Entities as topic labels: combining entity linking and labeled LDA to improve topic interpretability and evaluability." IJCol-Italian journal of computational linguistics. 2(2): 67-88.

Meeks, E., and Weingart, S.B. (2012). "The digital humanities contribution to topic modeling." Journal of Digital Humanities 2.1.

Nanni, F., and Ruiz Fabo, P. (2016) "Entities as topic labels: Improving topic interpretability and evaluability combining Entity Linking and Labeled LDA." DH2016.

Wallach, H. M., Murray, I., Salakhutdinov, R. and Mimno,

D. (2009). "Evaluation methods for topic models." Proceedings of the 26th Annual International Conference on Machine Learning.
3822	2017	Approaches to the Evaluation of DH Research Processes

The documentation of digital research processes has been a heavily discussed topic for some years now. It is most often addressed by the term provenance. In most cases, provenance data is created to record the chain of production of digital research results, in order to increase transparency in research and make such results reproducible.

In more recent times, a slightly different version of this topic has appeared in the field of digital humanities. Accordingly, digital research processes are modeled and documented with the aim to identify methodologies and practices of digital research in the arts and humanities on a broader scale. Two models have been introduced in this respect. One is the Scholarly Domain Model (SDM) (Gradmann et al, 2015) the other is the NeDiMAH Method Ontology (NeMO) (Constantopoulos, Dallas, and Bernadou, 2016). Both models provide formal semantics for the description of research processes as well as for their methodological evaluation.

The project environment in which these models were defined was dominated by European infrastructure projects, specifically DARIAH in the case of NeMO and Europeana in the case of SDM. Accordingly, such models aim to identify user needs and the qualitative use of infrastructure services as a first priority. However, it is also easily possible to refer to them in a broader perspective of laboratory research and science studies. For the case of the digital humanities community such a perspective corresponds with this community's wish to develop a unique methodological selfawareness.

Upon closer observation, the two models take up a

different approach to accomplish their goals. In terms

of NeMo, applications of the model describe research processes after they have taken place. In contrast, SDM makes reference to the concept of "modeling for" by Clifford Geertz and describes research processes in advance.

Such difference calls for a more concise evaluation of the terminology that was used before. In the research literature three concepts are used to distinguish between three possible viewpoints from which research processes can be described (Hunter 2006). These concepts are: workflow, provenance and lineage. As it has been indicated in the evaluation of SDM and NeMO the difference of these viewpoints is marked by the place in time from which they look at a research process. As such, it is also possible to call these concepts “prescribing”, “inscribing”, and “describing”.

In accordance with this systematization SDM defines workflows while NeMO presents lineages. What is missing however, is real provenance data that is created and modeled in order to systematically evaluate digital humanities practices. More specifically, this means data which is recorded during the research process. The main goal of this presentation is to introduce an approach for how such provenance data can be created and modeled meaningfully to reach its goal by way of example. The example is the Wissensspeicher at the Berlin-Brandenburg Academy of Sciences and Humanities. The Wissensspeicher connects all digital resources of the academy in a way that lets the user interact not just with metadata but with parts of the content itself. The work which will be presented is part of the evaluation phase of the DARIAH-DE project that started in March 2016.

When evaluating research practices provenance data has some advantages in comparison to workflow or lineage data. On the one hand, it is easier to obtain very detailed data. On the other hand, semantic implications which might predefine the results of the evaluation are less necessary. For instance, in the SDM primer the concept of annotating exists before it is applied to an activity in a specific situation. In consequence, research results about digital annotation practices are biased. They depend on personal decisions of someone who classifies activities as annotating, or on a normative concept of annotating. For the identification of new research practices in digital environments, both aspects are problematic. Provenance data does not have the same risk because it is mostly created before classification takes place. The only aspect which is predefined is the fact that recordable actions take place and that these actions form part of a broader research context.

Nevertheless, the implementation of technological solutions for the creation of provenance data in these circumstance is more complicated than in common situations where provenance data is created. It is not enough to record which software component modifies a digital resource at a certain point of time as it happens in e-science. The “resource” is the research process itself, the actions take place on multiple levels and such actions are carried out not only by software but also by humans.

User Activity Analysis and Digital Humani

ties Research Processes

In fact, there is one field of research which addresses a comparable situation and this field is user activity analysis. In user activity analysis, humancomputer-interaction is recorded in order to evaluate the user with respect to a specific research interest. The approach is used in areas like e-commerce and online social networks research in order to create services like recommendation systems (Plumbaum, Stelter, and Korth 2009) or to analyze social behavior (Dang et al. 2016). There are few examples of user activity analysis in academic digital environments. Suire et al. (2016) use this approach in the cultural heritage domain while Vozniuk et al. (2016) applies it to model learning processes in e-learning environments.

Having said that, no ready-made solution exists which can be easily used in the present context. Instead different approaches to user-activity analysis have to be evaluated in order to decide which ones can be adopted. Nevertheless, under the circumstances of evaluating digital research practices these decisions remain contingent. Digital research takes place in very different digital environments and under different conditions. Thus, in every situation in which digital research practice should be evaluated a different selection from the existing set of options might be the best. An overview of these options will be published in a DARIAH-DE report in the future.

The advantage of the Wissensspeicher use-case is

the fact that it is a web platform- most user activity analysis takes place on websites and in web environments. There are two major tasks which need to be distinguished. The first task is user activity tracking and concerns how the data is created. The second task is the actual analysis. It demands to evaluate in which sense the created data constitute meaningful events and how to make sense out of these events.

Use-Case: Wissensspeicher

The Wissensspeicher implements user activity tracking by combining three different strategies: http-request logging, browser-event parsing and user annotations. Http-requests are logged by virtue of the Django request object and the logger library in the Python Django app that creates the website. Thereby request information can be pre-processed when it is detected. When a page is loaded in the browser a JavaScript client registers event listeners for page elements and certain user actions. Each event that is triggered causes the client to parse relevant information in the DOM of the HTML including microdata which has been created in the Django app in advance. Additionally, the user is able to directly give feedback in some situations. The created data is stored in a MongoDB database.

User activity analysis is also realized by virtue of three steps. In a certain way these steps resemble the three angles of workflow, provenance and lineage. First, events are evaluated in a so called task model. This task model describes ideal sequences of actions and user goals as conceived by the project employees. Second, users are evaluated by applying the thinking aloud technique from the field of usability testing. Finally, existing data will be evaluated to identify common event sequences by computing its clusters. A systematization of the results from these evaluations will enables researchers to associate certain meanings with events in such a way that the data can be analyzed to permit insights into research practices within the use case.

A Dialogue of Approaches

This presentation will summarize activities to evaluate research practices and methods in the digital humanities. It will outline a unique and complementary approach and indicate how this approach can be used in conjunction with existing digital humanities research practices. Finally, the implementation and results will be described up to the point that such results are present after two-thirds of the project time has elapsed.

Bibliography

Constantopoulos, P., Dallas, C., and Bernadou, A.

(2016). “Digital Methods in the Humanities: Understanding and Describing Their Use Across the Disciplines.” In A New Companion to Digital Humanities, edited by Susan Schreibman, Ray Siemens, and

John Unsworth, 1st ed. Chichester, West Sussex, UK: John Wiley & Sons.

Dang, A., Moh'd, A., Milios, E., and Minghim, R. 2016. “What Is in a Rumour: Combined Visual Analysis of

Rumour Flow and User Activity.” In Proceedings of

the 33rd Computer Graphics International, 17-20.

ACM.

Gradmann, S., Hennicke, S., Tschumpel, G., Dill, K., Thoden, K., Pichler, A., and Morbidoni, C. (2015).

“Beyond Infrastructure! Modelling the Scholarly Domain.”

Hunter, J. (2006). “Scientific Models: A User-Oriented

Approach to the Integration of Scientific Data and Digital Libraries.” In VALA2006, 1-16.

Plumbaum, T., Stelter, T., and Korth, A. (2009). “Semantic Web Usage Mining: Using Semantics to Understand User Intentions.” In User Modeling, Adaptation, and Personalization, edited by Geert-Jan

Houben, Gord McCalla, Fabio Pianesi, and Massimo Zancanaro, 391-96. Lecture Notes in Computer Science 5535. Springer Berlin Heidelberg.

doi:10.1007/978-3-642-02247-0 42.

Suire, C., Jean-Caurant, A., Courboulay, V., Burie, J.-C., and Estraillier, P. (2016). “User Activity Characterization in a Cultural Heritage Digital Library

System.” In Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, 257-58. ACM.

Vozniuk, A., Rodriguez-Triana, M.J., Holzer, A, and Gillet, D. (2016). “Combining Content Analytics and Activity Tracking to Identify User Interests and Enable Knowledge Discovery.” In Proceedings of the 6th Workshop on Personalization Approaches in Learning Environments (PALE 2016). 24th conference on User Modeling, Adaptation, and Personalization (UMAP 2016), CEUR workshop proceedings, this volume.
3839	2017	Introduction
Humanities scholarship is becoming increasingly collaborative, participatory, and public facing. As humanists take up digital tools to conduct and share research, larger teams are needed to complete ever more complex computational tasks. When blending these heterogeneous teams--which may include faculty, librarians, staff, undergraduates, graduate students, postdocs, and community contributors--humanists have an ethical responsibility to offer a fair and transparent accounting of research activities. Tracing the evolution of research contributions is necessary for a range of issues facing digital scholarship such as authorship allocation, promotion and tenure, and reports to funders. The allocation of credit and authorship is an increasingly thorny issue for teams with a range of possible roles and a variety of research outputs and media types. There is, however, a large amount of data being generated by these teams that is capable of describing and measuring the contributions made on a variety of platforms and by multiple team member and community partners.

Despite our inheritance of social and collaborative tools, many of these systems elide the nuance and process of humanities based research. Knowledge creation is not merely a function of how much code is produced. New knowledge is often the result of a key insight made by a team of students, staff, and faculty. These insights are generated in a complex and overlapping system of mentorship, service, teaching, learning, and authorship that are deeply dependent, social, and human. With a system that is possible of visualizing the history of a digital project over the course of years, which may see the ranks of team members change over time, primary investigators and project funders will be better able to address often thorny and ethically charged issues relating to student assessment, mentorship, authorship, promotion, and tenure. Credit, promotion, funding, and credentialing are more complex topics than ever, yet many individuals and institutions rely on simple, outdated structures to assess the value of insights made by networked teams.

Social Knowledge Creation
The Penn State Digital Humanities Lab (Penn State Behrend) in partnership with the Teaching and Learning with Technology group (Penn State University Park) has developed a prototype of an ongoing project entitled the Social Knowledge Timeline (sktimeline.net). By linking together popular collaboration tools, the SKTimeline stores, analyzes, and communicates user data in three distinct areas of social knowledge creation:

•    Collaboration Platforms: Many scholars are turning to collaboration platforms like Slack, Yammer, and Basecamp to organize teams and foster communication within teams. These systems use an interface similar to a social media feed to pool project member input into a single narrative and eliminate the need for email. These systems help share documents and support conversations that may lead to drafting manuscripts on Google Drive and other services. These platforms offer a rich, conversational natural language data set that describes how team members mentor and support each other over time.

•    Version Control Systems: Github and Bitbucket are two of the most common version control platforms. These tools help facilitate large programming and encoding projects by allowing multiple coders to work simultaneously. When a team member “commits” code, a commit message describes the nature of the contribution as well as the date and time. This message will offer a highly granular view of coding projects as they unfold. Similarly, by including a feed from Google Drive’s own version control system, document authorship may be traced with similar precision.

•    Social Media: Platforms like Twitter, LinkedIn, and Facebook have proven to be fast paced and engaging areas for social and cultural exchange. Twitter has long been a particularly important site for digital humanists. The SKTimeline draws together multiple hashtags and user handles to frame preserve and contextualize this often ephemeral site of both popular and scholarly debate. Hashtags associated with digital projects, conferences, publications, and even course work can be analyzed and set in real time with other platforms.

Credit allocation in large teams is dependent on our ability to describe, quantify, and visualize our activities. By analyzing the rich natural language conversations generated by teams, the SKTimeline solves these ethical and institutional problems. The appearance of “Collaborators’ Bill of Rights” for digital humanities projects in 2011 is symptom of a need for greater clarity in heterogeneous collaborative teams (Clement et al 2011). The Modern Language Association’s “Guidelines for Evaluating Work in Digital Humanities and Digital Media” are similarly responding to appropriate credit allocation for researchers. There is a need for a more formalized and automated system of data collection and analysis for collaborative researchers across the university.

Machine Learning Contributor Taxonomies
The Taxonomy of Digital Research Activities in the Humanities (TaDiRAH) is used to quantify and describe user contributions. Machine learning systems like Google’s Cloud Platform is used to conduct language analysis, and translation, image recognition, sentiment analysis, and keyword extraction. Custom machine learning systems has also been layered on to these services using the Tensor Flow library to learn the project specific phrasing for contributions. Additional text analysis will be conducted using standard tools like the Natural Language Toolkit (NLTK) to link to TaDiRAH’s defined contributions. This project will reshape authorship and credit allocation in the humanities and beyond, but it will also be a perfect test bed for an emerging set of artificial intelligence tools that are now finding common application throughout society. In this way, the SKTimeline is representative of a broader cultural trend toward AI systems in aiding research.


Figure 1. The Social Knowledge Timeline displaying Slack channels posts and Twitter hashtags chronologically. Images associated with posts are used for backgrounds on the timeline

Conclusion
Undergraduate course projects, ongoing faculty research with graduate researchers, digital humanities labs, and library based digital research projects are just some of the contexts this round of user testing will examine. The data collected on participating teams against interview and form based user surveys. This kind of socially oriented knowledge creation emerges from a community of practice that moves fluidly between curricular experiences and co-curricular research experiences often hosted in DH labs, libraries, and centers. The SKTimeline seeks to solve a critical problem within scholarly communication in a digital context. The SKTimeline offers a means to capture complex narratives that constitute the organic and nuanced unfolding of humanities research.

Bibliography
Alphabet. (n.d.) Google Cloud Platform.

<https://cloud.google.com/products/machine-

learning/>

Alphabet.    (n.d.)    Tensor    Flow.

<https://www.tensorflow.org/>.

Borek,    L.    (n.d.)    TaDiRAH.

<https://github.com/dhtaxonomy/TaDiRAH>.

Clement et al. Eds. (2011) Off the Tracks: Laying New Lines for Digital Humanities Scholars.

Media Commons Press,    (2011).    Web.

<http://mcpress.media-commons.org/offthetracks/>.

Committee on Information Technology (2012)

“Guidelines for Evaluating Work in Digital Humanities and Digital Media.” Modern Language Association. Web.

<https://www.mla.org/About-

Us/Governance/Committees/Committee-

Listings/Professional-Issues/Committee-on-

Information-Technology/Guidelines-for-Evaluating-

Work-in-Digital-Humanities-and-Digital-Media>.

Di Pressi et al. (2015). “A Student Collaborators’ Bill of Rights.” UCLA Digital    Humanities.    Web.

<http://www.cdh.ucla.edu/news-events/a-student-collaborators-bill-of-rights/>.

Python Software Foundation. (n.d.) Python Language Reference, version 3.6. Available at <http://www.python.org>.

Ronacher, A. (n.d.) Flask. <http://flask.pocoo.org/>.
3843	2017	Introduction
In the mid 1990s, the Natural Language

Processing Group at the University of Leipzig began

work on the Wortschatz project which aims to provide corpora in hundreds of languages and in different size-normalisations, be that 100K, 300K or 1M sentences. As the resources grew in size, so did the number of requests for the data. In the early stages of the project a specific dump was created, parts of which even came with a small user-interface. The database dump was shared with interested researchers and partners in the business sector.

After some time, however, the personnel costs of this kind of collaboration became unsustainable. For this reason, a new plan was put into motion in 2004, consisting of the development of a SOAP-based API -the Leipzig Linguistic Services (LLS) - that enabled any interested person to access the data of the Wortschatz databases in any provided language (Quasthoff et al. 2006, Eckart et al. 2012). Overall 20 services were provided, delivering specific information such as baseform, category classifications, and thesaurus data. The aim of the LLS was to establish a Service Oriented Architecture (SOA) for linguistic resources based on small and atomic micro-services that could be combined by users for particular needs. Users were then not only able to browse through the Wortschatz website, but also to integrate those services with their own existing digital ecosystems.

In 2005 these services were made publicly

available and by September 2006 all requests were systematically logged. In July 2014 the number of logged requests reached nearly one billion. While at the beginning the use was limited to academia, over time the services were increasingly used by the private and business sectors as well.


Figure 2. Four workflow modes with separation of concern: editing (yellow); managing, compiling and deploying (red); hosting and operating (blue); using the LLS infrastructure (green).

The Leipzig Linguistic Services

The intention of the overall LLS architecture was to be as simple and generic as possible. A generic architecture can be reused in different scenarios but tends to have too many parameters and options, while a simple architecture claims usability and guarantees a faster learning curve. In the following, we briefly describe the architecture of the LLS.

In order to create the server-side Java code for a specific webservice, a data-set needed to be added to the webservice management (yellow zone in figure 1). The necessary edits contain, besides others, information on the name and type of the webservice (see also table 1) or parameters. Apache Ant was used as the central tool for generating the back-end services and deploying them in a Tomcat server (see red zone in figure 1). The blue zone illustrates the operations of the Wortschatz databases. Using the generic description of the webservice in the WSDL-files a number of wrappers of generated source code were created and made publicly available by LLS users such as for C# as part of .NET, Perl, Python, Delphi, PHP, Ruby and JavaScript (see green zone in figure 1).

Independently from the underlying programming languages, over the past ten years we have observed different uses in research, business and in the private sector. In research, the LLS were used in the areas of text profiles and author classification (Borchardt 2005). The services were also used as data resources for sentiment analysis or for query expansion. Users from the business field were mainly interested in using Baseform or Synonym services for improving internal search indexes. The LLS data was also used for information retrieval tasks in portals for weighting words in a word cloud or to display enriching information. Private users accessed the LLS to complete crossword puzzles. A dedicated service was installed upon request just for this purpose (see also table 1), since it was possible to query a pattern of an incomplete word with a given word length limitation. From 2008 the SOA-based

cyberinfrastructure of LLS was re-used in Digital

Humanities projects such as eAQUA and eTRACES (Buchler et al. 2008).

Results

Table 1 provides an overview of the 20 services offered with a breakdown of the requests and the responses. Over half of the requests (64.6%) were made to the Baseform service. Similarly, services with high-quality and often manually-curated data, such as the Thesaurus and Synonyms services, were requested more often than the quantitatively-computed Similarity service, which provided similarly used words by assuming the distributional hypothesis (Harris 1954), and thus compared the cooccurrence vectors of two words. Even if the coverage for this service, 66.02%, is significantly higher than, for example, the Category (35.92%) or the Synonyms (4.47%) services, users appeared to prefer precision over recall for their end-user applications.

Service

Requests

Requests

(%)

Non-empty

responses

Coverage

(%)

Fields

Webservice

Type

Access level

Installation

Baseform

Category

Thesaurus

Synonyms

Sentences

Wordforms

Frequencies

LeftCollocationFinder

RightCollocationFinder

Cooccurrences

RightNeighbours

LeftNeighbours

Similarity

Cooccurrences All ExperimentalSynonyms Crossword puzzling MARSService

NGrams

NGramReferences Common co-occurrence

624,275,884

120,476,452

69,573,648

60,745,973

60,087,714

12,671,302

11,932,213

1,416,001

1,379,356

1,057,722

959,560

731,449

467,809

20,852

20,779

2,902

616

564

409

55

64.636%

12.473%

7.203%

6.289%

6.221%

1.311%

1.235%

0.146%

0.142%

0.109%

0.099%

0.075%

0.048%

0.002%

0.002%

< 0.001%

< 0.001%

< 0.001%

< 0.001%

< 0.001%

315,724,185

43,276,840

37,151,565

2,719,544

11,536,172

4,309,791

8,095,420

295,714

235,323

629,795

567,870

473,600

308,877

20,848

14,860

1,306

616

149

87

43

50.57%

35.92%

53.39%

4.47%

19.19%

34.01%

67.84%

20.88%

17.06%

59.54%

59.18%

64.74%

66.02%

99.98%

71.51%

45.00%

100.00%

26.41%

21.27%

78.18%

W

W

W, L

W, L

W, L

W, L

W

W, PoS, L

W, PoS, L

W, ST, L

W, L

W, L

W, L

W, ST, L

W, L

W, WL, L

W, L

P, L

P, L

Wl, W2, L

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MySQLSelect

MARS

MySQLSelect

MySQLSelect

MySQLSelect

FREE

FREE

FREE

FREE

FREE

FREE

FREE

FREE

FREE

FREE

FREE

FREE

FREE

INTERN

FREE

FREE

INTERN

FREE

FREE

INTERN

04/2005

04/2005

04/2005

04/2005

04/2005

04/2005

04/2005

10/2005

10/2005

04/2005

04/2005

04/2005

10/2005

05/2009

12/2009

10/2005

10/2006

08/2011

08/2011

10/2005

TOTAL

965,821,260

425,362,605

Table 1. Overview of requests made to LLS between 2006-2014, in descending order. The Responses columns only list responses whose value was not empty. For space

constraints, the values in the Input Fields column are abbreviated: Word (W.), Limit (L.), Pa

Low coverage is also caused by requests to German language databases, especially by compound nouns that cannot all be included in a Baseform or Category service. Many multi-word units (MWU) were also requested. Out of all the requests, 84,760,875 (8.78%) were MWUs. With regard to the distribution of the webservice usage, only the two most frequently requested services, Baseform and Category, were queried more often than the total count of the MWU requests. This speaks to the impact of MWUs.

The less frequently used webservices in table 1 were primarily limited to internal uses, to newly installed services or, as was the case for the Crossword Puzzling service, to manual usage instead of automatic bulk requests.

The following questions are discussed in the paper:

1.    Geographical distribution and spread of requests

2.    Requested languages distribution

3.    Requests by cleanliness in terms of broken encodings or sending HTML code

4.    Temporal distribution including lessons learnt from incompatibility issues of used software and their new versions causing a decrease in service usage

5.    Identified service chains of the atomic LLS micro-services that users built on the client-side

6.    Experiences for load balancing of linguistic services

7.    Interoperability issues of programming languages and interpreting the WSDL-files differently

8.    Comparisons of SOAP- and REST-based webservices

Conclusion
“If you build it, they will come“ is an infrastructure mantra that we can answer given the atomic micro-services of the LLS (more critical view by van Zundert 2012). However, with regard to easy-to-integrate and atomic micro-services we found that users were generally very pragmatic as they requested everything that they had found in texts or on webpages, such as RGB colour-sets, URLs and other meta-information. Based on the log-files, we conclude that it is easier to request a token and look for a match in the LLS database of millions of words rather than to invest only little time in conventional pre-processing and pre-selection on the client-side. Similarly, users repeatedly requested function words, sometimes only a few minutes apart. This user behaviour entailed a significant server load and user control over the requests. This type of recurring request on unchanged data could only be considered as spam.

We found that providing an infrastructure like the LLS over the course of a decade challenges the compatibility of used software components.

Moreover, from a Natural Language Processing (NLP) standpoint, the results contribute to existing conversations about the difficulty of building balanced and representative corpora. In fact, user

interests detected in the LLS log-files can help to enrich corpora by adding further topics. The contribution also touches upon discussions about qualitative and manually-curated data versus automatically-computed    and    quantitatively-

available results of language technology algorithms. Notwithstanding the improvement of NLP algorithms, our results show that users prefer qualitative data and that they often request these services even if the domain and concept coverage is relatively low. The conclusion we draw from the user behaviour observed in almost one billion requests is

that research fields, including the Digital

Humanities, should share their data -no matter how small- through large infrastructure initiatives like DARIAH and CLARIN in order to increase the textual coverage of linguistic resources.

Bibliography
Borchardt, S. (2005) Generierbarkeit einer XML Topic Map aus E-Mails unter Verwendung von Text-Mining-Methoden und Nutzung von Web Services. Bachelor thesis.

Büchler, M., Heyer, G., Gründer, S. (2008) Bringing Modern Text Mining Approaches to Two Thousand Years Old Ancient Texts e-Humanities At: Workshop in the 4th IEEE International Conference on e-Science.

Eckart, T., Quasthoff, U., and Goldhahn, D. (2012) Language Statistics-Based Quality Assurance for Large Corpora, Proceedings of Asia Pacific Corpus Linguistics Conference.

Harris, Z. (1954) Distributional structure, Word, 10, 2-3, pp. 146162.

Quasthoff, U., Richter, M., and Biemann, C. (2006) Corpus Portal for Search in Monolingual Corpora Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC).

Van Zundert, J. (2012) , If You Build It, Will We Come? Large Scale Digital Infrastructures as a Dead End for Digital Humanities, Historical Social Research / Historische Sozialforschung, vol. 37, no. 3, pp. 165-86.
3892	2017	Introduction
This paper reports on work in progress aimed at facilitating the creation, sharing, linking, and analysis of data about the movement of people, ideas, cultural practices, and commodities between places, over the course of history. Products of the Linked Places project include: conceptual and logical models for historical routes; a temporal extension of the popular GeoJSON data format, called GeoJSON-T; several varied exemplar data sets converted to GeoJSON-T format; prototype web software for browsing and visualizing that data; and Python scripts to convert data between CSV, GeoJSON-T, and RDF compatible with the Pelagios Gazetteer Interconnection Format. Substantial interim work products are shared in the Linked Places and GeoJSON-T GitHub repositories and have been reported in some detail in two blog posts (1, 2) . Motivation

A growing number of historical gazetteers are being developed in the course of digital humanities research projects (Berman, Mostern & Southall, 2016). Their spatial temporal coverage is typically limited to a particular area and period due to factors of scholarly quality, cost, and relevance to a given project. Coverage extents do vary considerably, from a single city for a few generations to a region for several centuries. With few exceptions, these gazetteers are unpublished as such; instead they are spatial tables contained within, and integral to, the larger project data store.

Because historical gazetteers are difficult and timeconsuming to produce, it is vital they be published, when possible, in a way that permits linking them—an activity that the Pelagios project has made great strides in facilitating. An emergent network of specialized gazetteers holds terrific promise, not only for re-use, but ultimately as a distributed, increasingly comprehensive geographical (i.e. spatial-temporal) index to linked data from numerous domains, including history, archaeology, literary studies, philology, and several of the social sciences. The focus of such an index, and encyclopedic applications it enables, will be on individual places, typically at the scales of cities and points of interest.

Such systems are highly desirable, but given a large volume of data about individual places we can also begin harvesting, creating, and sharing data about the connections between them. We should be able to ask of historical gazetteers: What journeys and historical routes has a given place been a waypoint on? And, what flows of people, ideas, and commodities has it been a source or sink for?

But the Linked Places and GeoJSON-T projects have been undertaken with an even larger, “moonshot” vision in mind: a system allowing scholars and the general public to visualize and analyze the emergence, growth and spread of human settlements, their changing attributes, and the dynamic connections between them, including the diffusion of technologies and cultural practices.

To realize these ideas, we need a) lots of data, and b) methods and means for merging or linking them. In some respects, we are starting from scratch; data about historical movement is sparse and stored in disparate forms. Much of it will be newly generated, for example by parsing texts, transforming tabular records, or digitally tracing lines on historical maps. Merging and linking operations will require that the form of data from different sources (or abbreviated catalogues thereof) be either standardized (in the case of merging), or similar enough that automated alignment is feasible.

The majority of works on geographic networks concerns physical media like roads and rail, whereas movement data is eventive. Geographers have modeled migration flows and disease diffusion for several decades, providing theoretical bases for their analysis that are outside our present scope. An overview of that work is found in (Lowe & Moryadas 1975). An excellent and more recent work on mobility and geographic movement is Tim Cresswell’s “On the Move” (2006). We are not aware of any efforts to model data for historical routes computationally, however the core abstraction we build upon is the traditional graph/network model of nodes and edges credited to 18th century work of Euler (Biggs, et al 1986).

A Modeling Pattern
Data modeling is as much an art as a science (Simsion & Witt 2004), but some core best practices are well-known. A typical first step is establishing what entities are to be represented, what their essential attributes are, and what relationships obtain between them (cf. Chen, 1976). This step is often best accomplished collaboratively, in an iterative process undertaken by domain experts. Our results were immediately published to blog posts and relevant listservs, and the resulting input was useful in refining the model.

When the modeling context is an individual research project, it hardly matters what names are given those entities and relationships—only that the data store’s internal logic be sound and well understood by project members. But if, as in this case, the system will accommodate data from many sources or be accessed by others, we need to find broad agreement on a conceptual model and a vocabulary for its constituents between as many prospective participants as possible—that is, to describe the ontology of the research domain. Although much ontology engineering of this sort has involved comprehensive high-level ontologies such as the CIDOC-CRM , the development and implementation of small ontology design patterns (ODP) has been gaining favor since the introduction of that paradigm by Aldo Gangemi (2005). Such patterns, by any name, are “reusable successful solutions to a recurrent modeling problem” (definition provided by the Association for Ontology Design & Patterns (ODPA) ) which can be used alone or assembled in modular fashion for larger requirements. Examples include patterns for “Place,” “Event,” “Participation,” and “Region.”

And so the first step taken in the Linked Places project has been to develop an ontology design pattern for the historical movement of something between two or more places over some physical channel, either for some time during or throughout a timespan. The pattern, visualized in Figure 1, comprises the following conceptual understandings:

A route describes an attestation of one or more occurrences of the movement of something (e.g. people, commodities, information) between two or more places, either for some time during or throughout a time_period. Routes are composed of one or more segment, each of which is composed of two places and a path (corresponding to nodes and edges in network parlance), the locations and temporal attributes for which may be unknown or unspecified. Movement between places occurred upon ways (the term used by OpenStreetMap) —physical channels such as roads, rivers, canals, railways, footpaths, and sea lanes—and may have been directional.

The three types of routes considered here are journeys, flows, and historical_routes:

A journey is the record of a specific instance of travel by one or more individuals. Examples include: the 7th century pilgrimage of the Buddhist monk Xuanzang across China and India; the first voyage of Captain James Cook, between 1768 and 1771.

A flow is the record of the movement of something (commodities, people, ideas) between two places, aggregated as a magnitude over a period of time. Examples include: the transport of captive Africans between West Africa and Bahia in the 17th century; letters between certain correspondents in Paris and Prague in the 18th century; a source network of late Neolithic obsidian artifacts and known source locations on the Anatolian Plateau.

A historical_route asserts a single or composite named course of travel between places, taken repeatedly by unspecified individuals over time, usually for purposes of commerce. Examples include the Silk Road and the Amber Routes. Some correspond with named roads, for instance the Via Salaria in Italy is both a way and a historical_route.

Additional axioms indicated by the relations and cardinality expressions (e.g. 0...*) in Figure 1 include:

•    All routes are sourced, normally to textual or cartographic documents

•    The way for a segment (its physical path

described by a geometry) may be known and represented,    unknown, or ignored

(Segments with unspecified ways will typically be visualized as a line or arc)

•    Each segment has one or more temporal attribute (“when”), which can be a time_period, (possibly named) or a sequence (e.g. after segment n)

•    Routes and their component segments can have any number of attributes (properties), dependent upon data sources and project requirements


Figure 1. A conceptual model for historical movement (routes)

The ontology pattern we introduce here is specialized, as compared to high level ontologies like CIDOC-CRM. We have not yet mapped our distinctive entities (route, journey, flow, historical_route, segment, when) to existing ontologies. The term place is commonly found, but usually is synonymous with location; the sense we are adopting is that of the Pleiades gazetteer, but is not in a published ontology that we're aware of. In any case, we feel it is best to first lay down a logically coherent set of terms and at a later date attempt to align them with other ontologies. Formats

The route ODP has informed our development and implementation of recommended standard data formats. It turns out all three types of routes can be effectively described in GeoJSON-T, an extended version of GeoJSON, the widely-used format for representing geographic FeatureCollections. A FeatureCollection of routes will include both Place and Route features. Route segments are articulated as an array of one or more geometries in a route’s GeometryCollection. GeoJSON-T allows optional “when” objects, both for each feature at the same level as its geometry object and for segment geometries (Figure 2). Features and segments have certain required properties as shown, and can have unlimited project-specific properties.

FeatureCollection {

Features [

{ (afeatureType: Place W id: fl234 => type: Feature,

<    geometry: {

u- type: Point, o coordinates: [ <lng>, <lat> ]

{ @featureType: [ Journey | Flow | hRoute ] type: Feature, id: f9876, when: { ... },

W geometry: {

type: GeometryCollection

Z3 geometries: [ t I { type: Linestring,

LU    coordinates: [[    ],[    ]]

LL. I when: { timespan: J0651,, ,0651, "|3 months in 649"b Uj    ¡2    duration:    "3m", follows:

I—    z    properties: {

w source: fl234,

O    g    target: f2345,

uj directional: 1,

io

I } ’

>•

H...}

]

}

]

Figure 2. GeoJSON-T applied to route data

Data
To date, seven exemplar datasets have been converted from a typical CSV format to GeoJSON-T, using a newly developed Python program. Three are for journeys: two by individuals (a 7th century pilgrimage and a modern circumnavigation), the third by 840 Venetian ship convoys in the 13-15th centuries. Another dataset aggregates those ship journeys as flows having magnitudes of journeys and ships. The last three are historical_routes: the Roman era itinerary of the Vicarello Beakers, the route system between courier stations in Ming Dynasty China, and a large set of “Old World” trade and pilgrimage routes . Software

The widespread adoption of GeoJSON has demonstrated that for a data format to be useful, there must be software with visualization and analysis capabilities that supports it. Accordingly, an essential element of the Linked Places project is development of proof of concept web software to render GeoJSON-T data, for both routes and places alone, to a map and timeline together. The development of that software is ongoing, and publicly available. (Figure 3).


Figure 3. Linked Places interface (partial view as of March 2017)

Bibliography
Berman, M.L., Mostern, R., & Southall, H. (2016). Placing Names:    Enriching and Integrating Gazetteers.

Bloomington: Indiana University Press.

Biggs, N.; Lloyd, E.; Wilson, R. (1986), Graph Theory, 17361936, Oxford: Oxford University Press

Chen, P. P. S. (1976). The entity-relationship model—toward a unified view of data. ACM Transactions on Database Systems (TODS), 1(1), 9-36.

Cresswell, T. (2006). On the move: Mobility in the modern western world. New York: Routledge.

Gangemi, A. (2005). Ontology design patterns for semantic web content. In International semantic web conference (pp. 262-276). Springer Berlin Heidelberg.

Lowe, J. C., & Moryadas, S. (1975). The geography of movement. Boston: Houghton Mifflin

Simsion, G., & Witt, G. (2004). Data modeling essentials. San Francisco: Morgan Kaufmann.
3900	2017	Introduction
A lot of Digital Humanities (DH) research involves applying Natural Language Processing (NLP) tasks, such as, sentiment analysis, named entity recognition, or topic modeling. A large amount of NLP software is already available. On the one hand, there are frameworks that bundle software for different tasks and languages (e.g., NLTK [Bird et al, 2009], or xtasl), and on the other hand there are tools that target specific tasks (e.g., gensim, Rehurek and Sojka, 2010). As long as researchers do not need to combine tools from different packages, it is usually relatively easy to write scripts that perform the task. However, for innovative research, combining tools often is required, especially when working with non-English text. This abstract presents work in progress on NLP Pipeline (nlppln), an open source tool that improves access to

NLP software by facilitating combining NLP functionality from different software packages2.

nlppln is based on Common Workflow Language (CWL), a standard for describing data analysis workflows and tools (Amstutz et al, 2016). The main advantage of using a standard is that any existing NLP tool can be integrated into a workflow, as long as it can be run as a command line tool. This flexibility is missing from existing frameworks for creating NLP pipelines, such as DKPro (Eckart de Castilho, and Gurevych, 2015) using the UIMA framework (Ferrucci, and Lally, 2004). In addition to improved reuse of existing software, CWL increases research reproducibility, as it provides a standardized, formal record of all steps taken in a processing pipeline. Finally, CWL workflows are modular. This means that individual processing steps can easily be swapped in and out.

To demonstrate how NLP tools can be combined using nlppln, we show what need to be done to create a pipeline that removes named entities from a directory of text files. This is a common NLP task, that can be used as part of a data anonymization procedure. The Software

An NLP pipeline or workflow is a sequence of natural language processing steps. A ‘step’ represents a specific NLP task, that is executed by a single tool. Tools require input and produce output. The basic units in CWL are command line tools (i.e., tools that can be run from the command line). In order to be able to run a command line tool, CWL needs a specification. The nlppln software helps creating those specifications. In addition, nlppln provides functionality to convert existing NLP tools written in Python to command line tools. Finally, the software helps users to combine (existing and new) processing steps to pipelines.

In the next section, we explain how nlppln facilitates creating NLP steps, and in “Constructing Pipelines” we demonstrate the creation of an NLP pipeline for data anonymization.

Generating Steps
nlppln allows users to generate CWL specifications for existing NLP tools. To simplify the generation of CWL specifications, we use a convention for NLP tasks. The convention assumes that there can be two types of input parameters: a list of files for which the command should be executed, and/or a file containing metadata about the texts in the corpus. Output parameters consist of a directory where output files are stored (usually there is one output file for every input file) and/or a file in which metadata is stored. So far, almost all steps that are currently available in nlppln follow this convention. Be that as it may, we would like to emphasize that it is possible to deviate from this convention; for example, when existing NLP functionality requires different parameters (e.g., the name of a directory containing the input files instead of a list of input files). This does however mean that the user has to adapt the CWL specification by hand.

In addition to CWL specifications, nlppln allows users to generate boilerplate Python command line tools. A boilerplate command line tool contains generic functionality, such as opening input files and saving output files, but lacks implementation of the specific NLP task. The generated Python command can be used to turn existing NLP functionality into command line tools, and to create Python command line tools for new NLP tasks.

Python commands and associated CWL steps are generated using a command line tool that requires the user to answer a sequence of yes / no questions. Listing 1 shows what that looks like for a (hypothetical) command ‘command’, that takes as input a metadata file and multiple input files, and produces as output multiple text files and metadata.

Generatepythoncommand?[y]:

Generate cwl step? [y]:

Commandname[command]:

Multiple input files? [y]T Multipleoutput files?[y]:

Extension of output files ? [ json ]: txt Metadata output file? [n] : y Savepythoncommandto[nlppln/command.py]: Savemetadatato?[metadataout.csv]:

Save cwl step to [ cwl /command. cwl ] :

Listing 1: Generating a CWL specification and associated boilerplate Python command using nlppln.

Constructing Pipelines
To combine text processing steps into a CWL pipeline, nlppln provides an interface that allows users to write a simple Python script. We demonstrate this functionality by creating a pipeline that replaces named entities in a collection of text documents. Named entities are objects in text referred to by proper names, such as persons, organizations, and locations. In the example pipeline, named entities will be replaced with their named entity type (i.e., PER (person), ORG (organization), LOC (location), or UNSP (unspecified)). The pipeline can be used as part of a data anonymization procedure.

The pipeline consists of the following steps:

1.    Extract named entities from text documents using frog (van den Bosch et al, 2007), an existing parser/tagger for Dutch

2.    Convert frog output to SAF, a generic representation for text data3

3.    Aggregate data about named entities that occur in the text files

4.    Replace named entities with their named entity type in the SAF documents

5.    Convert SAF documents to text

All steps required for this pipeline are available through nlppln. Listing 2 shows the script that creates a CWL workflow for this pipeline. After importing nlppln (line 1), a new WorkflowGenerator object is created (line 3), and the available NLP steps are listed (line 4). Next, the script specifies the workflow inputs (line 6). In this case, there is a single input, which is a directory containing text files. This directory is the input of the first step, which is frog_dir (line 8). The output argument txts contains the internal CWL name of the input parameter (line 6). By assigning its value to the input argument dir_in of frog_dir (line 8), the output is connected to the input. Steps 1 to 5 from the pipeline description correspond to lines 8 to 12 in listing 2. After the remaining steps steps of the workflow are added (lines 9-12), the workflow outputs are specified (line 14). Finally, the workflow is saved to a CWL file (line 16).

1.    impo r tnlppln

2.

3.    wf=nlppln.Workf lowGenerator ( )

4.    printof ,list_steps()

5.

6.    txtawf. add_inputs (txt_dir=' Directory' )

7.

8.    frogoutwf. frog_dir (dir in=txts)

9.    safwf. f rog_to_saf (in_f lies=frogout)

10.    ner_stata?f. save_ner_data(in_files=sa£)

11.    new_sa^/f. replace_rner_Unetadata=ner_stats ¿n_f iles=sa±)

12.    txtwf. saf_to_txt (Tn_f iles=new_saf)

13.

14.    wf. add_outputXner_stats=ner_statt3{,t=txt)

15.

16.    wf. save (1 anonymize ..cwl1)

Listing 2: Python script for constructing the pipeline to replace named entities in text files.

Conclusion
To help DH researchers to (re)use and combine existing NLP software, we presented nlppln, an open source Python package for creating flexible and reusable NLP pipelines in CWL. nlppln comes with ready-to-use NLP steps, facilitates creating new steps, and helps combining steps into standardized workflows that are portable across different software and hardware environments. Compared to existing frameworks for creating NLP pipelines, CWL and

nlppln add flexibility and improved research reproducibility.

nlppln is a work in progress. An important challenge that needs to be addressed is the fact that there is no standard data format for representing text and/or information extracted from text. This means that we will have to add NLP steps that convert different data formats (cf. Eckart de Castilho, 2016)). For future work, we plan to implement additional NLP steps and pipelines, including functionality that targets more languages. We would also like to add visualizations of pipelines and allow users to run pipelines directly from nlppln.

Bibliography
Amstutz, P., Crusoe, M. R., Tijanic, N., Chapman, B., Chilton, J., Heuer, M., Kartashov, A., Leehr, D., Ménager, H., Nedeljkovich, M., Scales, M., Soiland-

Reyes, S., and Stojanovic, L. (2016). Common Workflow Language, v1.0,.

Bird, S., Loper, E., and Klein, E. (2009) Natural Language Processing with Python. O'Reilly Media Inc.

van den Bosch, A., B Busser, B., Dealemans, G. J., and Canisius, S. (2007) An efficient memory-based mor-

phosyntactic tagger and parser for Dutch. In Proceedings of the 17th Meeting of Computational Linguistics in

the Netherlands, pages 191-206, 2007.

Eckart de Castilho, R. (2016). Interoperability = f(commu-nity, division of labour). In Proceedings of the Workshop

on Cross-Platform Text Mining and Natural Language

Processing Interoperability (INTEROP 2016) at LREC

2016, pages 24-28, 2016.

Eckart de Castilho, R., and Gurevych, I. (2014). A broad-

coverage collection of portable NLP components for building shareable analysis pipelines. In Proceedings of

the Workshop on Open Infrastructures and Analysis Frameworks for HLT (OIAF4HLT) at COLING 2014, pages 1-11.

Ferrucci, D., and Lally., A. (2004) UIMA: an architectural approach to unstructured information processing in the

corporate research environment. Natural Language Engineering 10.3-4, pages 327-348.

Rehurek, R., and Sojka, P. (2010). Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP

Frameworks, pages 45-50.
3901	2017	The project "Hybrid Narrativity" combines work by language and literature studies, cognitive psychology, and computer science with the overarching goal to arrive at an empirically founded narratology of graphic literature, including comics and graphic novels. Comics and graphic novels provide a unique cultural form that has developed its own vocabulary, allowing for a fascinating interplay of text and visual art. After a period of neglect, they have recently been theoretically analyzed in detail by scholars in the arts, humanities and linguistics (McCloud, 1993; Groensteen, 2007; Cohn, 2013). Our aim is to provide an empirical testbed for these theories. The foundation of this endeavor is a large collection of graphic novels, which are annotated using a variety of methods. These include high-level descriptions of the work, mid-level descriptions of pages and panels, including the actors / characters, text, objects, and panel transitions, and low-level descriptions of visual elements in terms of descriptors developed in computer vision such as color histograms, GIST, SIFT, and SURF features. We are currently evaluating the addition of mid-level features from deep networks trained on photographs of real-world scenes, with quite promising first results.

These descriptions in terms of material properties are complemented by eye-tracking data, providing an empirical measure of the reader-level attention distribution and the time course of attention shifts. Thus, the digitized representation of literary and artistic works includes information on the side of "recipients", that is, readers, viewers, spectators and appreciators with their psychological and physiological responses. In a first step, eye-tracking data on a small sample of pages from selected works were collected from a large number of participants in order to evaluate general principles of attentional selection in graphic literature. Results show that reading of graphic novels is primarily governed by reading of text, and that inspection of graphical elements is apparently governed by top-down selection of story-relevant elements. In perspective, eye-tracking data will be collected for each of the works in the corpus, using a sample of pages and a smaller number of readers.

A graphical annotation tool is in development and has first been released to the public at DH 2016. This tool is based on an XML dialect that allows for the annotation of language as well as graphical elements. Future versions will include OCR support for comics fonts, and provide customizable annotation schemes, allowing other researchers to implement their own research ideas. We will also briefly present ideas on the potential to incorporate gaze-based interaction in the user interface of the tool, e.g., for the intuitive selection of objects, which will become important with the projected availability of low-cost eye trackers in the near future.

We have developed the Graphic Novel Markup Language (GBML) as an extension of John Walsh's Comic Book Markup Language (CBML; Walsh, 2012) to facilitate the description of graphical elements. These descriptions are imperative for defining regions-of-interest based mapping of eye movement data to the stimulus material. Material has been annotated using our editor, and a custom R package is under development and in use for statistical analysis of eye movements. Visual features are currently extracted using OpenCV (Bradski, 2000, 2016) and VLFEAT (Vedaldi & Fulkerson, 2008) libraries from Python and Matlab, since R does not yet provide sufficiently extensive packages for this purpose (for a promising approach see imager, Barthelme, 2016). Deep features are based on Deep Gaze II (Kümmerer, Wallis, & Bethge, 2016), which is in turn based on the VGG-19 network (Simonyan & Zisserman, 2014). A description of artworks in terms of visual features has shown promising results in other domains (Elgammal & Saleh, 2015).

A number of analyses using the corpus data show the potential for comparative studies as well as detailed study of individual works. Many of the testable hypotheses can be derived from the theoretical work cited above. For example, McCloud (1993) speculated that the cognitive effort of the recipient depends on the kind of panel transition, or that the empty space between panels is used to signal the passage of time. We provide empirical support for both of these hypotheses. Other examples include visual trends that can be identified across time or between regions, linguistic and visual analyses that can be used to compare text and visual complexity between different genres, and network analyses of interactions between characters that allow for an easy quantification and visualization of roles within a work, and for a comparison between works. They can also be used, e.g., to compare the complexity of a novel and its adaptation to the graphic novel format.

An in-depth analysis of a single work, the graphic novel adaptation of Paul Auster's City of Glass by Paul Karasik and David Mazzucchelli, shows that readers of graphic literature benefit from a specific expertise in decoding the different channels of information conveyed by image and text. Comics experts spend significantly more time on the image part of the panels, and this is correlated with a significantly deeper understanding of the narrative. New data suggests that this pattern replicates across samples, labs, and languages.

Taken together, we present the design of a corpus of graphic literature that is annotated using a variety of levels, including readers' eye movements. Ideas for how to make use of these data for interactive future versions are developed, and analyses of the collected data in terms of description as well as reception of the works of art are presented.

Groensteen, T. (2007). The System of Comics. Translated by B. Beaty and N. Nguyen. Jackson, MI: University of Mississippi Press.

Kümmerer, M., Wallis, T.S.A., & Bethge, M (n.d.).:

DeepGaze II: Reading fixations from deep features trained on object recognition. arXiv:1610.01563

McCloud, S. (1993). Understanding Comics: The Invisible Art. New York: Harper Collins.

Simonyan, K. & Zisserman, A. (2014). Very Deep Convolutional Networks for Large-Scale Image Recognition. In: CoRR abs/1409.1556. url: http://arxiv.org/abs/1409.1556.

Vedaldi, A. & Fulkerson, B. (2008). VLFeat: An open and portable library of computer vision algorithms. [Computer Software: http://www.vlfeat.org/ ]

Walsh, J. (2012). Comic Book Markup Language: An Introduction and Rationale. Digital Humanities Quarterly, 6(1).

Bibliography

Barthelme, S. (2016). imgager: Image Processing Library Based on 'CImg'. R package version 0.31. [Computer Software:    https://CRAN.R-

project.org/package=imager]

Bradski, G. (2000). The OpenCV library. Dr. Dobb’s Journal of Software Tools, 25(11):120, 122-125.

Cohn, N. (2013). The Visual Language of Comics. Introduction to the Structure and Cognition of Sequential Images. London: Bloomsbury.

Elgammal, A. & Saleh, B. (2015). Quantifying Creativity in Art Networks. 6th International Conference on Computational Creativity (ICCC’15).
3964	2017	Introduction
The Martian is a best-selling science fiction novel by Andy Weir that became a hit film in 2015. The novel exists in two versions, or variants: Weir self-published The Martian on his personal website in 2011 (hereafter, “Martianl”) and began selling it on Amazon.com in 2012. Crown Publishing subsequently bought the rights, edited the book, and re-released it (hereafter, “Martian2”).

The research presented here investigates what exactly changed when The Martian got edited. At first glance, the two versions appear essentially the same, with no major changes to plot, character, or structure. A closer look using a combination of quantitative and qualitative methods, however, reveals a number of noteworthy changes, as well as notable changes that result from thousands of seemingly minor copyedits. Aims

The aim of our research is to identify what changed between the two variants of The Martian using a combination of close reading and digital methods, analyze why those changes are important, and propose a methodology for comparing self-published and later-edited novels, an increasingly common phenomenon. We hypothesize that the editing process of a leading publishing house results in a novel that is more "mainstream", i.e. socialised, domesticated, and appealing to a general audience. In order to test this hypothesis, we explore a range of aspects, including style, content, and character. Our research also aims to bring a critical perspective to the strengths and weaknesses of a variety of qualitative and technical methods in identifying the edits and assessing their importance.

Related Work
In addition to work in digital genetic criticism (e.g. van Hulle 2008), a small number of studies use digital methods to explore variants of contemporary fiction. Yufang Ho (2011) compared the 1966 and revised 1977 versions of John Fowles’s novel The Magus, while Martin Paul Eve (2016) looked at differences in the US and UK versions of David Mitchell’s Cloud Atlas. As both Ho and Eve use different methods from one another and from us, it appears that no standard method has emerged so far for this type of research. Data

The data used for this research is primarily two plain text files of the variants of The Martian. Martianl was obtained in PDF format from Andy Weir’s website. Martian2 was obtained by scanning a print copy, performing OCR with manual corrections. We consider this our best option given the legal issues regarding text protected by copyright.

Methods and Results
Basic collation

We used the Wdiff frontend to the “diff” algorithm (Hunt & McIlroy 1975) to produce a collated version of Martianl and Martian2 and assess the number and extent of the edits. We then used bespoke Python scripts to classify the edits identified by Wdiff.

We found a total of 5146 edits were made to the novel. While 92% of the 101,000 words in Martianl remain unchanged in Martian2, the remaining 8% of the words undergo some type of edit, whether they are deleted or modified (Figure 1). The sheer number of edits calls for automatic means to classify them and detect any patterns.

120000 - 120000 —


Figure 1: Visualization of edits to The Martian as grouped by Wdiff.

Automatic Classification of Edits

Edits were automatically classified into two broad categories: script-detectable copyedits, and all other edits. Script-detectable copyedits includes changes in capitalization, whitespace, hyphenation, spelling of numbers, abbreviations, or combinations thereof (Figure 2). All other edits were classified as insertion, deletion, expansion or condensation and as “minor” or “major”, depending on the Levenshtein distance (Figure 3). Of the 5146 edits, 2863 (or 55%) were script-detectable copyedits, while 2283 (or 45%) comprised the rest. The code used as well as the collation data obtained are available on GitHub.


Figure 2: Script-identifiable copyedits to The Martian.


Figure 3: All other edits to The Martian.

narration or in sections written in the third person (Figure 4). When Watney narrates, the hundreds of misspellings, numerals, and scientific abbreviations in Martianl support the fiction that he is a scientist working in extreme conditions. Martian2 increases readability but eliminates the stylistic realism of Watney’s text. When Weir uses, for instance, numerals in the dialogue of other characters, the effect can be jarring. Martian2 corrects this for the better.

Martianl

Martian2

My idea is to make 600L of water (limited by the hydrogen 1 can get from the Hydrazine). That means I'll need 300L of liquid 02.

1 can create the 02 easily enough. It takes 20 hours for the MAV fuel plant to fill Its 10L tank with CO2.

My idea is to make 600 liters of water (limited by the hydrogen I can get from the hydrazine). That means I'll need 300 liters of liquid 02.

I can create the 02 easily enough. It takes twenty hours for the MAV fuel plant to fill its 10-liter tank with CO2.

"What's the biggest gap in coverage we have on Watney right now?"

"Urn," Mindy said. "Once every 41 hours, we'll have a 17 minute gap. The orbits work out that way."

"Whafs the biggest gap in coverage we have on Watney right now?"

"Urn," Mindy said. "Once every forty-one hours, we'll have a seventeen-minute gap. The orbits work out that way."

Figure 4: Edits to numerals and scientific abbreviations in Watney's narration (top) and third-person character dialogue (bottom).

Detecting transpositions with CollateX

Wdiff does not detect transpositions, or text that has been moved to a different location in the novel. Using CollateX (Dekker & Middell 2011) as described in Schoch (2016) revealed a total of 126 transpositions. Twenty-eight (or 22%) involve punctuation and should be considered artefacts of the method; 43 (or 34%) represent transpositions of a single word, showing stylistic preferences on the word-order level; 55 (or 44%) concern multi-word expressions which change the overall construction of a sentence or paragraph more substantially.

Figure 5 shows a relatively minor transposition appearing in combination with a contraction of a sentence.

Cumulative Effect of the Script-Identifiable Copyedits


Taken together, the 2863 script-identifiable copyedits have substantial effects upon the text. Weir’s many misspellings and misuse of hyphens and capitalization are corrected. Numbers in Martianl are overwhelmingly written numerically, and 765 of these become words in Martian2, e.g. “8” becomes “eight”. We found 231 instances of edits involving abbreviations, e.g. “L” becomes “liters”.

The copyedits work together in different ways when they appear in protagonist Mark Watney’s



Figure 5: An example of a transposition identified by CollateX.

We conclude that, quantitatively and qualitatively, transpositions were not a major part of the edit to The Martian. However, future work could apply the same method to other, comparable variants of novels to gain better reference points.

and expresses despair, loneliness, and introspection more often.


Figure 6: examples of toned-down profanity in the editing of The Martian

Additionally, Martianl contains an epilogue that is completely cut in the edit. It portrays Watney, back on Earth, being openly and profanely rude to a young fan. In Martian2, meanwhile, text is added to have Watney express gracious appreciation for all the parties involved in his rescue and a widespread faith in human nature. The edit therefore alters the tone of the ending substantially.

We believe that all of these changes, analyzed together with close reading, serve to align Watney's character with our overall hypothesized goal of the edit: to make Watney more “relatable,” “nice,” and “human,” and thus to appeal to a wider audience.

Close Reading of Other Edits


Edits Over the Course of the Novel

When we grouped the other edits, placed them into a spreadsheet, and manually inspected them, a number of thematic and stylistic shifts between Martianl and Martian2 became apparent.

Profanity is a key stylistic feature of The Martian that is substantially cut and softened by the edit. Words like “fuck” and “shit” are substantially reduced (by about 33% and 15%, respectively), while numerous other words and phrases are softened with “lesser” profanity or simple non-profanity (e.g. “the shit hits the fan” becomes “all hell breaks loose”). Figure 6 shows a selection of these edits. Similarly, crude and sophomoric humor is cut in key instances. The plot of The Martian revolves around solving one problem after another to rescue an astronaut, Mark Watney, stranded on Mars, while relatively little text is devoted to Watney's emotions or inner world. In Martian2, however, Watney expresses significantly more emotion: he misses his family and friends more

Patterns in the edits related to textual progression are revealed by measuring the absolute Levenshtein distance of the script-identifiable copyedits and other edits line by line (Levenshtein distance is a metric for measuring the difference between two sequences, see Navarro 2001).


Figure 7: Sum of absolute Levenshtein distance per line over textual progression (script-identifiable copyedits in red, other edits in blue).

Figure 7 shows the sum of the absolute Levenshtein distances for each line of the novel (with Savitzky-Golay smoothing applied). The graph shows the substantial modifications to the ending of the novel, but also a large number of locations with smaller but nonetheless above-average modifications.

Conclusion and Further Research
We have identified and analyzed a number of key features that emerged from the editing of The Martian, notably on the level of style and character, which combine to make the novel more appealing to a wider audience.

Ongoing research into The Martian concerns the relative frequency and function of parts of speech, quantifying the amount of syntactic change, and the legal issues affecting the obtaining and processing of the texts. We hope to present these additional findings in the near future.

As for our typology of edits, an established methodology for classifying edits in the companion fields of textual analysis and scholarly editing is the distinction between the “accidentals” and “substantives” used by the Greg-Bowers tradition and included in the MLA Committee on Scholarly Editions’ Guidelines for Editors of Scholarly Editions (Modern Language Association, 2011). Scholars are not unanimous, however, in supporting this. G. Thomas Tanselle, for instance, found these terms "misleading and often untenable in their implication of a firm distinction in all cases” (Greetham 1992, pp.335-336). Further, there appears to be no widely-applicable typology of edits in digital scholarly editing and collation, with different materials calling for different typologies (see TEI-L 2016).

Our typology of edits departs from previously proposed ones by focusing entirely on types which can be identified automatically, based on surface features. While limited in scope and excluding any semantic criteria, our typology may serve as a first approach to the edits of any text and allow quantitative comparison of some key phenomena. We believe that our method could be applied to other variants of fiction — by itself or incorporated alongside another taxonomy, including accidentals/substantives — particularly to novels which begin as self-published works but are later edited and re-released, an increasingly important phenomenon in contemporary fiction.

Bibliography
Dekker, R. and Middell, G. (2011). Computer-Supported Collation with CollateX: Managing Textual Variance in an Environment with Varying Requirements. Supporting Digital Humanities 2011. University of Copenhagen, Denmark. 17-18 November 2011.

Eve, M. P. (2016). "You have to keep track of your changes”: The Version Variants and Publishing History of David Mitchell's Cloud Atlas, Open Library of Humanities. https://olh.openlibhums.org/article/10.16995/olh.82/

Greetham, D. (1992). Textual scholarship: An introduction. New York/London: Garland Publishing.

Ho, Y. (2011). Corpus Stylistics in Principles and Practice: A Stylistic Exploration of John Fowles' The Magus. New York: Continuum.

Hunt, J. W. & Mcilroy, M. D. (1975). An algorithm for differential file comparison. Computer Science.

Modern Language Association (2011). Reports from the MLA Committee on Scholarly Editions, Guidelines for Editors of Scholarly Editions, available at: https://www.mla.org/Resources/Research/Surveys-Reports-and-Other-Documents/Publishing-and-Scholarship/Reports-from-the-MLA-Committee-on-Scholarly-Editions/Guidelines-for-Editors-of-Scholarly-Editions

Navarro, G. (2001). A guided tour to approximate string matching. ACM Computing Surveys. 33 (1): 31-88. doi:10.1145/375360.375365.

Schoch, C. (2016). Detecting Transpositions when Comparing Text Versions using CollateX. The Dragonfly's Gaze. http://dragonfly.hypotheses.org/954

TEI-L (2016). Types of Edits. TEI-List. http://tei-l.970651.n3.nabble.com/Types-of-edits-tp4028495.html

van Hulle, D. (2008). Manuscript Genetics, Joyce's KnowHow, Beckett's Nohow. Gainesville: University Press of Florida.

Weir, A. (2011). The Martian. Self-published.

Weir, A. (2014). The Martian. New York: Crown Publishing Group.
3968	2017	This paper presents a project, currently underway, that combines 3D fabrication and digital audio processing with text mining and sentiment analysis to gauge attitudes toward digital fabrication processes (and maker culture more generally) among a large online community of musicians.

In the last two years, traditional musical instrument manufacturing materials have been joined by laser-sintered plastics and photopolymer resins, as new Additive Manufacturing techniques have been employed either to replicate vintage instrument parts, or redesign them altogether. In no case, however, have digital fabrication methods managed to entirely displace artisanal tradition.

This study explores how digital fabrication has been negotiated by one large and active online musician’s community that values artistry, craft, and industrial history alike. Using low-end desktop 3D fabrication methods with open-source software, I am reproducing a line of replica instrument parts based on vintage originals. Once all reproductions are complete, I plan to engage a professional musician to test-play the printed replicas, and compare them with their original vintage models. These tests will be recorded using audio freeware, and shared with members of a select online musician’s community, in order to determine whether they distinguish my 3D-prints from their original models.

My aim is less to determine whether these replicas are in fact morphologically and sonically identical to their period models than to determine how 3D digital fabrication is received by musicians strongly devoted to vintage equipment and artisanal craft. Using the Python Natural Language Toolkit (NLTK 3.0) with a naïve Bayes classifier trained on a valenced wordlist (e.g. AFINN), I will conduct sentiment analyses of the online forum to gauge how the community of musicians responds to the introduction of new manufacturing techniques that are neither industrial nor conventionally artisanal.

Ultimately, this project addresses the “emergence” of digital humanities (Jones 2013) into artisanal practices as a sign of our contemporary “post-digital” condition, under which we need “no longer talk about digital versus analog but instead about modulations of the digital or different intensities of the computational” (Berry 2014). By engaging one active and prominent community of analog musicians in a discussion of digital fabrication, I hope to use their insights into the materiality of music production to address questions crucial to maker epistemology, such as: “For which methodologies do tactility and texture especially matter?,” and “When is scholarly communication most persuasive off the screen?” (Sayers 2015).

Bibliography

Berry, D.M. (2014). “Post-Digital Humanities: Computation and Cultural Critique in the Arts and Humanities.” Educause Review 49.3: 22-6.

Jones, S.E. (2013). The Emergence of the Digital Humanities. New York: Routledge.

Sayers, J. (2015). “Why Fabricate?” Scholarly and Research Communication 6.3: 1-11.
3975	2017	Introduction
This paper explores the development of the Persons tool (code available on Github), an interactive resource for exploring patterns of speeches by and mentions of characters in dramatic texts. Initially developed to examine works by Shakespeare, the tool has broad application to dramatic texts.

Visualising the frequency, extent, and position of dialogue relating to a particular character presents users with a simple and immediate measure of that character’s prominence within the play. The Persons tool enables users to select and visualise individual characters’ involvement, producing a novel means of exploring large-scale structural, narrative, or character-focused patterns within the text.

The tool is intended to facilitate character-based analysis and reveal structural patterns at the scale of the play. The tool was conceived with exploratory potential in mind, and is designed to allow users to customise the visualisation according to their particular interests or to follow a more speculative and disinterested reading of the play’s character-based features.

This deliberate aim emerged from the heuristic development process described below, and a desire to produce an extensible exploratory tool for dramatic texts. From an initial focus on using digital tools to visualise the tangling and disentangling of character names and identities in THe ComeDy oF Errors, our interest broadened into exploring the potential for using character data to visualise larger structural and narrative patterns.

We were also motivated by the use of network analysis and visualisation for scholarship on Shakespearean and other literary texts, including work by Yose et al (2016), Grandjean (2015), Moretti (2011), and Stiller, et al. (2003). These analyses are similarly character-based and have yielded many interesting insights. But in the reduction of the textual data to nodes and edges (characters and their interactions), network analysis has obscured the temporal. The work of Xanthos, et al. (2016) maintains this temporal dimension, while exploring the dynamics of the character networks as they evolve. In contrast, by visualising the characters at the level of the play as a whole, we aim to preserve characters’ locations within the space of the text, thereby enabling analysis of the dramatic time and structural duration of the play.

Tool Development

Tool development took part in two phases. First, the data was extracted and transformed into a suitable format. The user interface was then designed using an iterative process that enabled the exploration of various approaches to data presentation and interaction.

Data Preparation

The tool uses data contained in XML files provided in the New Variorum Shakespeare editions of THe ComeDy oF Errors and THe Winter's TaLe.

Data was extracted using a custom-developed Python script which iterates through each play’s XML file extracting character and name data, along with line number, scene and act identifiers. The data as output as JSON, which reduces the complexity of using it with the JavaScript-based user interface.

User Interface Design

The tool’s web-based user interface (Figure 1) was developed using the open source Javascript library, D3 (Bostock et al, 2011). Persons developed from a fixed and static visualisation of THe ComeDy oF Errors to a more interactive and exploratory tool. In the heuristic spirit of the tool itself, we describe here its various iterations, the stages of its development, and the motivations for various changes to its design and functionality throughout the process.





The Comedy of Errors

Text Visualisation of Speakers


Figure 1: Persons Character Visualisation Interface - From the First to the Second Iteration


Expanding the Second Iteration

The second iteration of the tool adopted the circular layout of the tool to plot character involvement across the entire play, as shown in Figure 3. At this point, the tool was still static, and its focus on the two pairs of twin characters in THe ComeDy of Errors represented a desire to deploy visualisation for a particular exploratory purpose. The play operates on the basis of identity and confusion, as Antipholus and Dromio of Syracuse are mistaken for their Ephesian counterparts, and vice versa. Our aim was to plot the speeches of these four characters to see if the visualisation revealed any insights into how the identity question was introduced and managed at a structural level.

Persona's focus on character and temporal visualisation is present in the first iteration of the tool (Figure 2). Speeches and mentions are plotted along a timeline, with a tabular view switching between the five acts of the play. All speeches and mentions are colour-coded, resulting in some interesting patterns and densities at certain parts of the text, but lacking the facility for isolating chosen characters. In addition, the tabular view of the five acts lacked the desired holistic view of the entire play.

A timeline-based visualisation of stage entrances, speakers, names mentioned, and stage exits

Act 1 Act 2 Act 3 Act 4 Act 5

Line Number

Stage Entrance

••

Speaker

•

1

Mentions    Stage Exit

•

1

•

•

80

•

•

•

•

140

•

•

•

•

160

—

•

•

•

•

180

1

200

•

•

•

•

220

!

1

•

i

•

Figure 2:

First iteration design


Figure 3: Second iteration design

Indeed, the visualisation presents several clustered scenes of engagement between the pairs of twins through which various errors and misunderstandings are played out. The tell-tale single appearance of Dromio of Syracuse's orange marker in Act 1, Scene 2 precisely represents the beginnings of the error and confusion: “What now? How chance thou art returned so soon?”

An additional avenue of exploration in the second iteration of the tool was the geographical mapping of locations mentioned in the play. THe ComeDy of Errors is known for including the only mention of America in Shakespeare's plays, among several other placenames in its text. In some respects, this visualisation gives a false impression of THe ComeDy of Errors as a worldly play. While eighteen locations are mentioned in the text, several of these are ironically located by Dromio of Syracuse on Nell the kitchen-maid's body, because “she is sphericall, like a globe: I could find out / Countries in her” (Act 3, Scene 2).

The final interface

As useful as this view of the play proved, we felt at this point that a more dynamic and interactive interface was required to allow users to test hypotheses like our own, or to undertake more exploratory and experimental visualisations of the data, as illustrated in Figure 4. The circular layout was retained, as it provided a useful method of presenting the play as a whole, while maintaining the temporal dimension of the character interactions. The character-selection menu and the scene-divisions in the outer ring were thus added in the final stage of development.


Figure 4: Final interface design

Also added were visualisations of higher level metrics to illustrate the number of times a character speaks, and the number of lines they speak.

Conclusion
A major part of the tool's value is its extensibility. It may be used to create character visualisations for any play which is XML-encoded according to quite minimal specifications, and offer the opportunity to undertake comparative analysis of structural, narrative, and character-based patterns in different plays. Indeed, while the development of the tool focused on THe ComeDy oF Errors, a similar visualisation of THe Winter's TaLe (Figure 5) was generated from New Variorum Shakespeare XML files with no revision to our code.

Highlight Speakers

The Winter's Tale

Text Visualisation of Speakers

□    Gaoler

□    Ladies


Toggle Checkboxes

Figure 5: Visualisation of The Winter's Tale

The trajectory of Persona's development from fixity to interactivity represents a conclusion that we drew in the course of this project: that a visualisation tool developed for a particular purpose need not be confined to its use for that objective alone. The modular and open-source principles of software development have contributed to a rich and fruitful habit of sharing within the field of Digital Humanities, and we hope that others will build upon the tool that we have developed here.

Indeed, we have plans for further developments and improvements to Persons. Working towards a tool which will enable structural and thematic comparison of the thirty-six plays in the First Folio, the next phase will test for structural correlations in a thematic grouping of five additional Shakespearean plays. This development will strengthen Persona's potential for generating insights into macro-level structural analysis of dramatic texts, while testing its technical extensibility by incorporating XML files from another source, THe BoDLeian First Folio.

Bibliography
Bostock, M., Ogievetsky, V., and Heer, J. (2011) "D3: Data-

Driven Documents." IEEE TraNsactioNS on VisualizatioN aND CoMputer GrapHics 17.12 (2011): 2301-2309. Web.

Grandjean, M. (2015) “Network Visualization: Mapping

Shakespeare's Tragedies.” Martin Grandjean. N.p., 23

Dec. Web. 26 Oct. 2016.

Moretti, F. (2011). “Network Theory, Plot Analysis.” New

Left Review 68: 80-102. Print.

Stiller, J., Nettle, D., and Dunbar, R. (2003) "The Small World of Shakespeare’s Plays." Human Nature 14.4: 397408. Print

Xanthos, A., Pante, I., Rochat, Y., Grandjean, M. (2016). “Visualising the Dynamics of Character Networks.” Digital Humanities 2016: Conference Abstracts. Jagiellonian University & Pedagogical University, Krakow, 2016. 417-419. Print.

Yose, J., Kenna, R., Carron, P. M, Platini, T., and Tonra, J.

(2016). “A Networks-Science Investigation into the Epic Poems of Ossian.” Advances in CompLex Systems (2016): 1650008. Web. 26 Oct. 2016.
3986	2017	Introduction

When an earthquake struck Nepal in 2015, the band One Direction sent tweets encouraging their fans to donate to relief efforts, while an Indian activist tweeted accusations of Christian missionaries trading conversions for aid. While Twitter users were quick to bring their own agendas to the Nepal earthquake, does the same hold true for earthquakes in other parts of the world? A series of earthquakes that struck Kumamoto, Japan, and then Muisne, Ecuador in 2016 attracted a substantial amount of Twitter attention as well, yet as far as we are aware, the One Direction fans and the Indian activist made no comment. These users are onlookers to all three earthquakes: in other words, they are not directly affected by these events, but they tweet about them.

This paper explores onlookers’ responses across three different earthquakes: the 2015 Nepal earthquake, and the nearly-simultaneous earthquakes in Kumamoto and Ecuador in 2016, which we treat as a single event. This present work expands on our previous conclusion that onlookers tend to bring their own agendas to disasters. This paper shows that users who tweeted about the Kumamoto and Ecuador earthquakes were generally more interested in the earthquake or the affected areas than their own agendas, as their interest in the earthquake could not be predicted by interests in other topics.

Background

A substantial amount of research has explored how social media causes users to engage with political, social, and humanitarian problems; however, opinions on social media’s effectiveness—whether it causes users to donate money or participate in campaigns— are mixed. Some argue that displaying concern in social media is more about acquiring social capital than effecting change (Shulman; Gladwell; Morozov, The Net Delusion; Morozov, To Save Everything, Click Here), while a Pew Research Center survey finds that social media does create change (Raine, Purcell, and Smith). One analysis found that charities’ use of social media does not increase donations (Malcolm), while another finds that certain tweeting strategies do (Gasso Climent) although tweets may not raise awareness about the charity’s causes (Bravo and Hoffman-Goetz). All these studies concur that social media enable substantial discourse about crises. The question we explore here is how much of this conversation is predicted by a user’s preexisting interests, and how this varies even among the same type of event in different areas.

Methodology

We followed a similar data collection process for both Nepal and the Kumamoto and Ecuadorean earthquakes: we sampled data from Twitter’s REST API to attain a broad sample of onlookers. For Nepal, we had gathered a dataset of tweets sent during the three weeks following the Nepal earthquake by searching for any tweets that mentioned the word “Nepal” from April 24, 2015 to May 8, 2015. We then randomly selected 15,000 users from this set and harvested all tweets they sent between April 24, 2014 and May 8, 2015. We attempted to capture only English-speaking users to increase the likelihood that we would capture users not directly affected by the earthquake, but we still found some users who tweeted in multiple languages. This left roughly 11,000 onlookers for Nepal. For Kumamoto and Ecuador, we gathered a dataset of tweets sent in the two weeks following the Kumamoto earthquake that mentioned “Kumamoto” or “earthquake.” We randomly selected 30,000 users and harvested every tweet they sent between March 16 and May 16, 2016. We collected more users, but fewer tweets for each user, than we did in the Nepal dataset so as to look for users who displayed a broader set of interests. This left around 25,000 onlookers in Kumamoto and Ecuador. We were able to filter out non-English tweets much more effectively in the latter dataset than the Nepal dataset.

For the tweets for each event, we made a bipartite graph of users to words, and performed community detection using a method proposed by Okamoto and Qiu (2015)    [2], which allows for overlapping

communities. Okamoto and Qui’s method takes a single parameter, alpha, which controls the resolution of community detection: the smaller its magnitude, the larger the number of detected communities. We set alpha to 0.001 in both cases. The output of this method was a list of each node (users and words), and a percentage ranking rating its affinity with each community. We used these results to generate a list of top words in each community, which told us what users who tweeted about that community were interested in. From this process, a number of topics emerged, which we labelled manually according to our interpretations of the top words in each.

Since this method also gave us a ranking for users’ affinities to each community, it allowed us to examine the influence of other topics on a user’s likelihood to tweet about either event. We wanted to examine how much a user’s propensity to tweet about other topics predicted the probability that he or she would tweet about topics related to the earthquake. We ran multivariate linear regressions on each topic in the dataset using the Python sklearn module (Pedregosa et al.). We ran one regression for each topic, in which we treated a user’s propensity to tweet about the topic under consideration as a dependent variable predicted by his or her propensity to tweet about other topics.

Results

Our analysis demonstrated a certain predictive power for some topics in each dataset. Applying this process to the Nepal tweets produced 17 topics about a variety of concerns, from entertainment to world events. Table 1 shows these topics. Two of them, topics 5 and 15, treat the earthquake directly.

A correlation exists between tweeting about entertainment topics and tweeting about the earthquake. Tweeting about topic 15 predicts that a user will tweet about topic 2, which is about pop music: the top words include “fifth,” “harmony,” “video,” and “Justin.” This correlation is the strongest in the dataset; few other topics show nearly as much correlation. Consequently, we observe a degree of correlation between tweeting about entertainment topics and tweeting about the disaster in Nepal. While the more targeted topics, like the One Direction topic, do not show much correlation with other topics, the more general entertainment topic does.

Title

One Direction

Fifth Harmony

Earthquake

Earthquake

0 One Direction

1.000

0.117

-0.004

0.000

1

Weather

0.005

0.008

0.007

0.001

2

Pop Music

0.099

1.000

-0.003

0.004

3

India

-0.028

-0014

0.135

-0.001

4

England News

0.005

0.023

0.003

0.001

5

Earthquake

-0.007

-0.007

1.000

0.006

6 Good Feelings

-0.012

0.028

0.050

0.000

7 Good Feelings

0.182

0.115

0.004

0.004

8

Pera Sacha Sauda

0.015

0.031

0.097

0.002

9

China

-0.025

-0015

0.181

-0.001

10

Twitter

0.001

0.003

0.001

0.001

11

Emotions

0.174

0.223

-0007

0.000

12

Shopping

-0015

-0.006

-0.001

0.001

13

US Politics

-0.025

-0.018

0.095

0.000

14

Technology

-0017

0.005

0.034

0.000

15

Nepal

-0025

7 0Q4

1 493

1.000

16

US News

-0013

0.017

0.049

0.000

Table 1: Topics for Nepal, showing probability of tweeting about one topic (X-axis) given likelihood of tweeting about other topics (Y-axis)

In summary, we observe correlation between tweeting about entertainment topics and tweeting about the Nepal earthquake. Those who bring other agendas such as an interest in a particular musical group to the disaster tend to tweet mostly about those topics.

Does the same hold true for Kumamoto and Ecuador? Table 2 shows a few topics from the Kumamoto and Ecuador earthquakes. Our analysis demonstrates that an onlooker’s propensity to tweet about some topics could be predicted by interest in others. For example, a user who tweeted about news topics, such as U.S. politics (specifically, topic 43) or Asian news (topic 4), was likely to tweet about Nigerian politics (topic 2). Likewise, a user who tweeted about Japanese Entertainment (37) was also likely to tweet about other entertainment topics.

Topics as Independent    Nigerian Politics Entertainment Kumamoto 1 Kumamoto 2

Variables

0 Good Feelings

-0.01335

-0.01

0.00070

0.00086

1 US Politics

-0.01958

-002

0.00069

0.00063

2 Nigerian Politics

1.00000

-002

0.00068

0.000o8

3 Middle East

0.01191

-0.01

0.00082

0.00082

4 Asian News

551858

2.72

0.00419

0.00707

5 Roberta Lange

0.90105

0.43

0.00101

0.00138

6 Kumamoto 1

3.31768

1.92

1.00000

0.00578

7 MSG

-0,01637

-0.02

0.00046

0.00047

8 Anime/Good Feelings

-001770

-002

0.00040

0.00037

9 Good Feelings

0.26131

0.26

0.00114

0.00281

10 Music

0.01621

0.04

0.00046

0.00074

11 BTS

-0.01426

-0.01

0.00059

0.00065

12 MSG

-0.01521

-0.02

0.00047

0.00059

13 Good Feelings

-0.00662

-0.01

0.00084

0.00056

14 Social Media

-0.01055

-001

0.00036

0.00043

15 Good Feelings

0.96011

1

0.00201

0.00269

16 Good Feelings

-001305

-001

0.00056

0.00062

17    US Politics

18    Entertainment

-0.02949

-0.02242

-0.03

-0.02

0.00063

0.00042

0.00059

0.00046

19    Entertainment

20    Prince’s Death

-002122

-0.02317

-002

-0.02

0.00041

O.OOO58

0.00040

0.00080

21 Entertainment

-0.02253

-002

0.00051

0.00075

22 Kumamoto 2

4.43153

1.81

0.00775

1.00000

23 Good Feelings

-0.00948

-001

0.00032

0.0111)41

24 Team Seymour

-0.01254

-0.02

0.00045

0.00045

25 Soccer

0.01116

0.03

0.00054

0.00050

26 Entertainment

-0.01708

-0.02

0.00037

0.00028

27 Tobacco

-001941

-0.02

O.OOO55

0.00046

28 Good Feelings

-001928

-0.02

0.00044

0.00061

29 Captain America

14.44602

27.88

0.00722

0.00669

30 Pom

-001421

-001

0.00027

0.00034

31 Emotions

-0.01924

-0.02

0.00037

0.00042

32 MSG

-001707

-0.02

0.00045

0.00054

33    Help

34    Disasters

35    Emotions

-0.01220

-0.02593

-0.01023

-0.02

-002

-001

0.00053

0.00146

0.00044

0.00071

0.00293

0.00060

36 Entertainment

-0.03060

I]

0.00063

0.00044

37 Japan and Entertainment

4.87650

4.88

0.00512

0.00865

38 Indian Politics

0.06220

0.01

0.00088

0.00061

39 Good Feelings

0.01971

0.06

0.00063

0.00060

40 Good Feelings

-0.02143

-002

0.00061

0.00049

41 US Politics

-0.01536

-0.02

0.00068

0.00063

42 US Politics

-0.02893

-0.03

0.00057

0.00049

43 US Politics

1529698

17.34

0.00777

0.02044

Table 2: Topics for Kumamoto and Ecuador, showing probability of tweeting about one topic (X-axis) given likelihood of tweeting about other topics (Y-axis) (truncated for space)

On the other hand, no such correlation was observed in the opposite direction: no topic predicted a user’s tendency to tweet about topics 6 and 22, the earthquake topics. All coefficients in those regressions were under 0.01. The two topics that focus on Kumamoto are relatively closed: users who tweet most about the Kumamoto earthquake tweet about little else during this period.

Our interpretation is that users who tweeted about Kumamoto or Ecuador were specifically interested in earthquakes, Japanese culture, or the affected regions. The majority of users who tweeted about the Kumamoto and Ecuador earthquake topics were interested in specialized topics relevant to the events: they were not, for example, One Direction fans. We therefore conclude that while some users tweeting about Kumamoto and Ecuador were motivated by general interests in news or entertainment, they were a much smaller group than in the Nepal dataset. Conclusions

We find that while users often brought their own agendas to tweeting about Nepal, fewer did so when tweeting about Ecuador and Japan. Users who tweeted about Kumamoto and Ecuador tended to focus on topics related to the earthquakes, and less on issues that the earthquakes might demonstrate.

Our future work will test these conclusions with other earthquakes. In particular, we will examine the 2011 Tohoku Earthquake which raised serious political issues. Additionally, in our present work, we treat the Kumamoto and Ecuador earthquakes as a single event because distinct “Kumamoto” and “Ecuador” topics did not emerge from our text mining, which itself is suggestive of how Twitter users understood them. In our future work, we will probe more deeply for differences between the two earthquakes.

Bibliography

Bravo, C. A., and Hoffman-Goetz, L. (2015) “Tweeting

About Prostate and Testicular Cancers: What Are

Individuals Saying in Their Discussions About the 2013

Movember Canada Campaign?” Journal of Cancer

Education. 1-8. link.springer.com. Web. 20 Feb. 2016.

Gasso Climent, C. (2015) “Twitter as a Social Marketing

Tool: Modifying Tweeting Behavior in Order to

Encourage    Donations.”    info:eu-

repo/semantics/bachelorThesis. N.p., 27 Aug. 2015. Web. 20 Feb. 2016.

Gladwell, M. (2011) Outliers: The Story of Success. Reprint edition. Back Bay Books. Print.

Malcolm, K. (2016). “How Social Media Affects the Annual

Fund Revenues of Nonprofit Organizations.” Walden

Dissertations and Doctoral Studies (2016): n. pag. Web.

Morozov, E. (2012) The Net Delusion: The Dark Side of

Internet Freedom. Reprint edition. New York:

PublicAffairs. Print.

Morozov, E. (2014) To Save Everything, Click Here: The Folly of Technological Solutionism. First Trade Paper Edition edition. New York: PublicAffairs. Print.

Okamoto, H., and Qiu, X.-L. (2015) “Modular Decomposition of Markov Chain: Detecting Overlapping and Hierarchically Organized Communities in Networks.” Abstracts of NetSci-X. Rio de Janeiro, Brasil: N.p. Print.

Pedregosa, F. et al. (2011) “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825-2830. Print.

Raine, L., Purcell, K., and Smith, A. (2016) “The Social Side of the Internet | Pew Research Center.” Pew Research Center: Numbers, Facts and Trends Shaping Your World. N.p., 1 Mar.Web. 21 Feb. 2016.

Shulman, S. W. (2009) “The Case Against Mass E-Mails: Perverse Incentives and Low Quality Public Participation in U.S. Federal Rulemaking.” Policy & Internet 1.1: 23-53. Wiley Online Library. Web. 21 Feb. 2016.
4011	2017	Introduction
GutenTag is a cutting-edge resource that allows literary researchers of all levels of technical expertise to perform large-scale computational literary analysis. It allows users to build large, clean, highly customized worksets and then either analyse them in-system or export them as plain text or richly-encoded TEI. It has been built from the ground up by literary scholars for literary scholars: rather than relying on off-the-shelf tools poorly suited to the domain of literature, we have developed many of the components ourselves based on the specific demands of literary research. GutenTag is fully open-source, its analyses are based on entirely open corpora, and researchers can save and distribute all the parameters of their analyses, allowing for unprecedented reproducibility of research in a field plagued by siloed corpora. GutenTag is easy to use, permitting casual non-programmers to perform complex computational literary analysis via an online interface, while offering additional offline customization options to more advanced users. Although GutenTag was initially designed to facilitate our own research in polyvocality and dialogism, we show here that it can be leveraged to intervene in pressing debates unrelated to our specific research, such as the discussion surrounding Matthew Jockers's analysis of gender in Macroanalysis.

Overview of GutenTag
The system has grown considerably since our initial proposal, presented to an audience of computer scientists (Brooke et al., 2015). Below, we review the main features of the software with particular emphasis on recent improvements.

Interface: GutenTag is primarily accessed through an HTML GUI, accessible via the web or as a downloadable tool (both can be accessed from http://www.projectgutentag.org). In offline mode, the configuration files can be saved and loaded, and additional lexicons and other lists used for analysis can be specified by the user. A Python API is also included.

Corpora: The original version supported only the 2010 image of Project Gutenberg USA, but we have expanded support to all texts from Project Gutenberg USA as well as Project Gutenberg Canada and Australia, which include many additional texts published after 1922 and still under copyright in the USA.

Metadata: Document collections of interest can be defined using a variety of metadata tags. These include metadata provided by Project Gutenberg (title, author, author birth, author death, and, for some texts, Library of Congress classification and subjects). We have added genre (fiction, non-fiction, poetry, drama), determined using a sophisticated machine classifier, as well as author and text information (author gender, author nationality, publication date, publication country, single work or collection, etc.) derived from (mostly) unstructured resources including Wikipedia and the texts themselves.


Figure 1: The GutenTag interface, showing the creation of a workset based on advanced metadata (Genre, Author Sex, Author Nationality, Date of Publication)

Text cleaning and tokenization: Sophisticated regex-based heuristics are applied to remove metatext elements related to Project Gutenberg before, after, and sometimes within the text boundaries. Literature-specific tokenization is provided, preserving important information needed for downstream analysis.

Structural Tagging: This module identifies the main structural elements of the texts. First, heuristics are used to identify the likely boundaries between front matter, body, and back matter. Identification of structure within the main text is driven primarily by the identification of headers, and fully supports recursive structures including entire embedded texts which can have their own front and back matter separate from that of the anthology. Structural tagging is sensitive to genre: in the context of fiction, we identify parts, chapters, and speech; for poetry, we identify poems, cantos, stanzas, and lines; for drama, we identify acts, scenes, speakers, speech, and stage directions.

Lexical tagging: GutenTag includes lemmatization and POS tagging. There are several built-in lexicons which capture semantic and stylistic distinctions, and users can define their own lexicons, including multiword lexicons. Most recently, and most relevant to our case study below, we have added our own state-of-the-art literature-specific named entity recognition system (LitNER) which bootstraps from context-based clustering of common named entities to distinguish previously unseen people and locations from other named entities (Brooke et al. 2016b). For fiction, we group individual person names into collections of characters, and then assign speech events to these characters in the vicinity, using efficient, rule-based logic inspired by work in He et al. (2013). We identify the indicated sex of these characters primarily using large lists of names and titles; when a name does not appear on our list, we fall back to matching common sex-indicative character n-grams automatically derived from those lists (e.g. names ending with “a” tend to be female).

TEI output: When corpus output is required, we use XML-based TEI format as the default output format when structure (rather than simply tokens) is requested.


Figure 2: The GutenTag interface, showing in-system options for analysis via textual measure

Analysis: In addition to building corpora for exporting, GutenTag users can directly compare the distribution of relevant lexical tags or other textual metrics across multiple corpora as defined in the metadata filtering phase. The latest version includes a selection of standard textual metrics (e.g. average sentence length), part-of-speech based metrics such as lexical density, and metrics that rely on structural/lexical tagging, such as the amount of dialogue and the amount of dialogue that has been assigned to female characters. Advanced users can easily define their own textual metrics using Python; these then become available through the main interface. We also welcome requests for metrics from the DH community.

Research Applications

GutenTag was initially developed to facilitate our own research in literary dialogism (Hammond et al. 2016, Brooke et al. 2016a). GutenTag allows us to perform three crucial steps in our research process: first, to build customized corpora (a set of novels published from 1880-1950, for which it yields 4,088 results); second, to identify passages of character speech in each novel and assign a unique character to each passage of speech; and third, to calculate a measure of dialogism for each text using an algorithm based on our six-style approach (Brooke et al. 2016a). Further, GutenTag allows us to save our workflow in a parameter file so that it can be reproduced by other researchers.

GutenTag is designed as a general system, however — not merely as a vehicle for our specialized research. We thus present an example of how it can be employed (by a non-programmer) to investigate a prominent debate in Digital Literary Studies, Matthew L. Jockers’s discussion of gender and authorship in Macroanalysis. Jockers argues that female authorship can be predicted reliably through topic modelling, based on the presence of themes that “correspond rather closely to our expectations and our stereotypes” such as “Affection and Happiness," “Female Fashion," and “Infants” (Jockers 2013). A reader might respond to Jockers’s analysis by querying his assumptions about literary authorship; specifically, his failure to distinguish between authors and characters. Suppose that female characters were just as likely to discuss “Female Fashion" in novels written by men as those written by women, but that female authors tended to include more female character speech in their novels, as Muzny et al. (2016) suggest. If this were so, Jockers’s findings would not confirm stereotypes about female authorship, but simply reveal the tendency of female authors to include more female voices in their texts than men.

GutenTag is uniquely suited to investigating such a question. Its advanced metadata and sophisticated lexical tagging allow it to easily and rapidly analyze the question of female character speech in a large corpus of English-language novels.

50%


^^Female Authors • Male Authors

Figure 3: Mean proportion of text which Is dialogue in prose fiction, female vs. male authors, 1850-1949.

Sample sizes as follow, in number of texts. 18501859: 53 female, 97 male. 1860-1869: 86 female,

128 male. 1870-1879: 110 female, 137 male. 18801889: 122 female, 262 male. 1890-1899: 221 female, 583 male. 1900-1909: 299 female, 975 male. 1910-1919: 354 female, 960 male. 19201929: 148 female, 656 male. 1930-1939: 77 female,

413 male. 1940-1949: 52 female, 135 male.

Figure 3 shows that female authors in the twentieth century included approximately the same amount of dialogue as a proportion of total text length as male authors, but that in the latter half nineteenth century, they included approximately 5% more than men. Since Jockers focuses on the nineteenth century, this finding alone might impact his conclusions.


Figure 4: Mean proportion of dialogue allotted to female characters in prose fiction, female vs. male authors, 18501949

Sample sizes as follow, in number of texts. 18501859: 53 female, 97 male. 1860-1869: 88 female, 128 male. 1870-1879: 110 female, 137 male. 1880-1889:

122 female, 261 male. 1890-1899: 220 female, 583 male. 1900-1909: 300 female, 795 male. 1910-1919:

354 female, 960 male. 1920-1929: 148 female, 655 male. 1930-1939: 77 female, 413 male. 1940-1949:

54 female, 135 male.

As Figure 4 shows, GutenTag supports Muzny et al.'s contention that female novelists incorporate far more (approximately twice as much) female dialogue compared with male novelists. The finding that the proportion of female dialogue decreased from the late nineteenth to the mid-twentieth century, in both female and male authors, is one that bears further investigation — particularly in relation to the emergence in that period of popular genres, such as children’s literature, Westerns, and romance novels.


Figure 5: Mean proportion of dialogue allotted to female characters in prose fiction, female vs. male authors, by nationality, 1850-1949

Sample sizes as follow, in number of texts. Scottish:

31 female, 80 male. Canadian: 49 female, 78 male.

English: 339 female, 1308 male. American: 572 female, 1545 male. Australian: 38 female, 104 male. Irish: 21 female, 92 male.

In Figure 5, we employ GutenTag’s ability to filter results by author nationality. The marked discrepancy between proportion of female dialogue in male authors from England and the United States again suggests the need for an further investigation of genre; for instance, whether the American preference for male-centred genres like the Western might explain the result. Looking at GutenTag’s fine-grained outputs, we observe that the texts with the lowest proportion of female dialogue are those directed at a young male audience (especially adventure fiction for boys) while those with the highest proportion consist largely of fiction for young women (L. M. Montgomery’s Anne of Green Gables devotes over 90% of its dialogue to female characters). These findings might prompt our hypothetical researcher to engage in a smaller-scale study of the representation of gender in children’s literature. Because all texts in GutenTag are accessible to users, it easily accommodates such movements from large-scale analysis to close reading.

Conclusion
GutenTag allows researchers of all levels of technical expertise to perform advanced large-scale literary analysis, as well as to independently test the hypotheses and conclusions of prominent research in the field. Our case study further shows how the integrated, end-to-end GutenTag system allows users to raise new research questions in the course of their analyses (such as the correlation between the emergence of children’s fiction and the proportion of female dialogue) and then, since all its corpora are accessible, to shift scales and explore these questions through close reading.

Literature. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL ’16).

Brooke, J., Hammond, A., and Hirst, G. (2015). GutenTag: an NLP-driven Tool for Digital Humanities Research in the Project Gutenberg Corpus. Workshop on Computational Linguistics for Literature. Denver: NAACL, pp. 1-6.

Hammond, A., Brooke, J. (2016). Project Dialogism: Toward a Computational History of Vocal Diversity in English-Language Literature. In Digital Humanities 2016: Conference    Abstracts. Jagiellonian University &

Pedagogical University, Krakow, pp. 543-544.

He, H., Barbosa, D. and Kondrak, G. (2013). Identification of Speakers in Novels. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL ’13).

Jockers, M. (2013). Macroanalysis: Digital Methods and Literary History. Urbana-Champaign: University of Illinois Press.

Muzny, G., Algee-Hewitt, M., Jurafsky, D. (2016). The Dialogic Turn and the Performance of Gender: the English Canon 1782-2011. In Digital Humanities 2016: Conference Abstracts. Jagiellonian    University    &

Pedagogical University, Krakow, pp. 296-299.

Bibliography
Brooke, J., Hammond, A., and Hirst, G. (2016a). Using Models of Lexical Style to Quantify Free Indirect Discourse in Modernist Fiction. Digital Scholarship in the Humanities, 2(2): 1-17.

Brooke, J., Hammond, A., and Baldwin, T. (2016b). Bootstrapped Text-level Named Entity Recognition for
4026	2017	Topic modeling, a method for the semantic analysis of large text collections, has been in the focus of interest in digital literary studies during the recent years. The method uses probabilistic procedures to generate probability distributions for words out of a collection of texts, sorting many single word distributions into distinct semantic groups called ‘topics'. These topics constitute groups of semantically related words, and the contribution of each topic to the composition of each text can be quantified mathematically (Blei 2012, Steyvers und Griffiths 2006). In digital literary studies, topic models can be interesting in themselves. For example their dynamic development either during the plot of single literary texts or over multiple texts in a

stage of literary history can be analyzed (Jockers 2013,

Blevins 2012, Rhody 2012, Schoch to appear), though comparing literary themes and the probabilistic concept of ‘topics' described here is obviously not unproblematic. And topic models can also be interesting features for classifying or clustering texts (Blei 2012). There are currently two state-of-the-art implementations of the relevant algorithms: ‘Mallet' (McCallum 2002) and ‘Gensim' (Rehurek 2010). But usually more is required than simply running a topic modeling algorithm (Fig. 1):

•    Longer texts like novels need to be split into smaller parts (e.g. paragraphs, scenes, or a fixed amount of characters or words).

•    NLP based preprocessing is necessary

•    To achieve optimal results, texts must be reduced to content words, either by filtering out function words with stopword lists, or by using a part-of-speech tagger to exclude unwanted word classes.

•    Similarly, lemmatization and elimination of proper names can be useful.

•    After the topics have been generated, results are usually visualized based on the relevant metadata.

•    Results need to be evaluated with regard to internal or external criteria rather than just being left to interpretation.

The aim of our work is to provide digital literary scholars with a consistent, extensive and well documented programming library that allows them to do the necessary preprocessing, to generate topic models relying on the existing implementations, and to visualize and evaluate results within a single convenient scripting environment. We want to empower users with little or no previous experience and programming skills to create custom workflows mostly using predefined functions within a familiar environment (see the current stage of development on Github). Hereby, we want to lower the access threshold to topic modeling as a method, facilitating researchers in spending time experimenting with topic modeling and understanding how it generates results, rather than spending it for acquiring advanced technical skills before being able to try topic modeling at all.

The library will be developed for the programming language ‘Python' that is well suited for NLP and data analysis tasks, and popular among digital literary scholars already. In addition, development can be partially based on functions from, and experiences made with a previous Python-based implementation of a topic modeling workflow developed by Christof Schoch et al., that can be regarded as a proof of concept.

A convenient tool for NLP analysis during preprocessing does exist in the DARIAH-DKPro-Wrapper (DDW) that covers a wide range of NLP tasks for many different languages and generates annotations in a Python-Pandas compatible format easily usable within our library.

Evaluating the results of topic modeling is not a trivial task but has proven to be rather challenging (Wallach et al. 2009, Chang et al. 2009). In order to evaluate topics we will provide functions for intrinsic evaluations, for example perplexity, and external evaluations, for example path length in a resource like wordnet. We will also support task based evaluation where topics are used for a classification task and evaluated on the basis of the results.

For the visualization of the results we can build on the tmw library mentioned above and provide plots of topics over time (or other dimensions), distribution of topics over texts using heat maps and others.

Functions will be designed with consistent syntax and in a way that allows users to grasp easily what they do and why, so users can combine them into scripts to implement their own project ideas with minimal learning effort. The ability to customize workflows will be facilitated by a thorough tutorial describing all functions, outputs and potential combinations in    detail    (see    https://www.pen-

flip.com/c.schoech/tmw-tutorial as an example for what we are planning).

Preprocessing

X

Corpus

\

Formatting

_I_

NLP-Analysis

Modelling

+

Topic Modeling - Algorithm

I    \

Postprocessing

X

Visualization

Evaluation

Figure 1: Topic modeling project workflow

Jockers, M. L. (2013). Macroanalysis - Digital Methods and Literary History. Champaign, IL: University of Illinois Press.

McCallum, A. K. (2002): MALLET: A Machine Learning for Language Toolkit. http: //mallet.cs.umass.edu.

Rehurek, R., and Sojka, P. (2010): "Software framework for topic modelling with large corpora." In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks.

Rhody, L. M. (2012): „Topic Modeling and Figurative Language“. Journal of Digital Humanities 2.1. http://jour-nalofdigitalhumanities.org/2-1/topic-modeling-and-fig-

urative-language-by-lisa-m-rhody/

Schoch, C. (to appear): „Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama“. Digital Humanities Quarterly.    http: //digitalhumani-

ties.org/dhq.    Preprint:    https://zenodo.org/rec-

ord/48356.

Steyvers, M., and Griffiths, T. (2006). „Probabilistic Topic Models“. In Latent Semantic Analysis: A Road to Meaning, herausgegeben von T. Landauer, D. McNamara, S. Dennis, und W. Kintsch. Laurence Erlbaum.

Wallach, H. M. (2009)et.al.: Evaluation Methods for Topic Models. In: Proceedings of the 26th International Conference on Machine Learning, Montreal, Canada, 2009.

Bibliography
Blei, D. M. (2012): „Probabilistic Topic Models“. Communication of the ACM 55, Nr. 4 (2012):    77-84.

doi:10.1145/2133806.2133826.

Blevins, C. (2010): „Topic Modeling Martha Ballard's Diary“. Historying.    http: //historying.org/2010/04/01 /topic-

modeling-martha-ballards-diary/.

Chang, J. (2009): Reading Tea Leaves: How Humans Interpret Topic Models. In: Y. Bengio et al.: Advances in Neural Information Processing Systems 22, 288--296.
4030	2017	In its ability to extract feature sets, relate texts within an abstract space, and semantically parse groups of texts, computational textual analysis has functioned primarily as a formalist intervention into literary study. When practitioners venture outside of the formal features of the texts themselves, it is author or date that serves as the point of contact between the text and its wider context. And yet, texts offer a rich history of reception: as different interpretive communities (Fish 1980) receive and reinterpret novels, poems or plays, they recontextualize the literary object to suit the particular socio-cultural goals of their period or nationality. Lacking detailed accounts of reading practices at large scales, even traditional practitioners of literary history have been unable to reconstruct the history of reception of even the most historically canonical texts. In this project, we leverage the ability of Digital Humanities to recover, at least provisionally, a large-scale history of textual reception by exploring the patterns of citation that reveal the attention paid to specific texts across their history as read-erly objects. How are certain canonical texts cited over time and what can the attention paid to different segments of texts with a rich reception history tell us about the reading or social practices of different historical periods? How do different groups of readers (particularly authors and critics) quote text differently as they make use of passages in their own writing? And how do specialists and non-specialists cite the same text differently? By exploring the locus of attention within a canonical text, both across groups of readers and across history, we provisionally reconstruct a historically and socially contingent map of a text's reception history.

As Piper and Algee-Hewitt have argued in “The Werther Effect” (Piper and Algee-Hewitt 2014), practices of citation, the embedding of the language of a text within other works, can reveal patterns of reception even within a single author's corpus. In this project, we expand this approach multi-dimensionally, identifying passages of canonical works quoted in other novels, in critical articles by specialists and nonfield specialists, and in a larger undifferentiated corpus of text. The scale of our analysis enables us to identify what parts of a text have received the most writerly attention overall and how that attention has been shaped over time. By moving from semantics to passages, we switch our attention from the intangible metrics of semantic similarity, to specific, quotation-level instances of citation that demonstrate specific attention to identifiable parts of our target texts. Drawing on work in sequence alignment by David Smith et al (2013) and Richard So et al. (forthcoming), we will therefore be able to explore patterns of attention that have been paid to a text by identifiable groups of readers. We argue that these patterns of citational-ity serve as a proxy for the reception of a text: while necessarily limited to readers who themselves were authors (or critics), they nevertheless represent an important category of reception available to analysis.

To extract the quotations, we used Python's “dif-flib” module, wrapped up as a parallelized MPI program that runs on an HPC cluster. To compare any two individual texts - for example, when checking for passages from Hamlet inside of a novel from the Gale American Fiction corpus - the texts are first split into tokens and passed through a filter that removes a set of 200 stopwords. This speeds up the alignment algorithm (the high-frequency words that get pulled out make up a significant portion of the total words in any given text, producing shorter sequences) and also has the advantage of making the alignment process less sensitive to small changes in function words, which seem to get shuffled around or changed fairly frequently when a text is quoted. For example, a change from:

And crook the pregnant hinges of the knee

Where thrift may follow fawning

to

And crook the pregnant hinges of the knee

That thrift may follow fawning

still gets picked up as a quotation, since the semantically significant words - crook, pregnant, hinges, knee, thrift, fawning - stay the same.

These filtered sequences of tokens then get passed

through the alignment algorithm, which produces a set of matches, recoded in terms of their starting positions in each text and the length of the matching subsequence. To ensure that the matches represent actual quotations, we discarded matches shorter than 5 tokens (not counting stopwords), since alignments shorter than this include a fair number of false positives, generic word sequences that likely don't represent any kind of meaningful quotation or intertextual-ity - for example, many are numbers, things like "five hundred thousand." This gives us high "precision" - almost all of the alignments that are included in the final analysis represent legitimate quotations to the play -but it also drops down the "recall" somewhat, since some of the shorter alignments are, in fact, real quotes - things like "weighing delight and dole." We are currently evaluating a couple of strategies for identifying these alignments that are short but semantically "focused" enough that we can say with confidence that they should be included in the set of valid matches.

We use this method of sequence alignment to trace the quotations of five canonical texts with a rich citation history across four corpora. Our selected texts include Shakespeare's Hamlet, Milton's Paradise Lost, Dickens' A Christmas Carol, Caroll's Alice in Wonderland and Wordsworth's Prelude. Not only are all of these texts heavily quoted by critics, but, we argue, they have entered the literary and cultural consciousness of both Britain and America such that the passages that are cited by authors and critics reveal interpretive and readerly practices both across time and between different groups. For each text, we extract all of the citations from it that are five words (excluding stopwords) or longer, that occur in each of our four corpora: the full-text Hathi trust corpus, representing a massive sample of writing in the nineteenth and early twentieth centuries; a literature-specific corpus of 28,000 novels from England and America dating from 1789-2016; and two corpora of articles on literary studies, one a corpus of 10 journals of literary criticism and history (e.g. PMLA, NLH, Critical Inquiry) and one a corpus of 10 field-specific journals focused specifically on the authors represented in our group of

canonical texts (e.g. Shakespeare Quarterly, Milton

Studies, Wordsworth Circle).

Between these four corpora, we are able to differentiate the kind of attention paid to our primary canonical texts by four different groups of readers. Do novelists pay attention to different parts of a text than authors in general in the nineteenth and twentieth centuries? Do general literary critics quote different parts of Hamlet than Shakespeare specialists? And for each of these corpora, how does the citation map of each of our texts change over time?

For example, a citation map of Hamlet in our novel corpus revealed 1,693 quotations of five or more non-stopword tokens, which collectively cover about 25% of all words in the play. When we plot the frequency of citations across the narrative of the drama (broken into 500 bins), our method reveals the passages most quoted by novelists of the nineteenth and twentieth centuries (Figure 1). From this citation map, we can see that Hamlet's soliloquy (“to be or not to be”) is among the top three passages cited by novelists; however, the quotation that clearly dominates the use of Hamlet by this group of readers comes from Act 5, Scene 2 “There's a divinity that shapes our ends, / Rough-hew them how we will.—“


twentieth-century novels. Each passage is 1/500 of the text.

Although not among the most identifiable passages today, this quotation clearly had a resonance for the readers of the nineteenth and early twentieth centuries.

By comparing this map of citations across the narrative of Shakespeare's plays to ones that are both drawn from our comparative corpora and periodized across the two centuries they represent, we are able to show how different passages gain and lose meaning across time and between kinds of reading. As we expand this to all five of our canonical texts read into all four of our corpora, we can shed light on how historically and disciplinarily specific practices of reading shaped the horizons of interpretation for specific works, and begin to reconstruct these reader-based practices in ways that are open and tractable to the Digital Humanities.

Bibliography
Fish, S. (1980) Is there a text in this class? Cambridge: Harvard UP.

Piper, A., and Algee-Hewitt, M. (2014). “The Werther Effect I: Goethe, objecthood and the handling of knowledge.” Distant Readings: Topologies of German Culture in the Long Nineteenth Century. Ed. Matt Erlin and Lynn Tatlock. Rochester: Camden House. 155-184.

Smith, D., Cordell, R., Dillon, E. M. (2013). “Infectious texts: modeling text reuse in nineteenth-century newpapers.” Proceedings of the IEEE International Conference on Big Data. 86-94.

So, R. J.; Long, H, Yuancheng, Z. (forthcoming). “The Dark Code: Modeling White-Black Literary Relations, 18802000. Forthcoming.
4045	2017	Originally funded by a startup grant from the National Endowment for the Humanities, Le Dictionnaire Vivant de la Langue Française (DVLF) is an experimental approach to dictionary compilation that aims to push the boundaries of what typical dictionaries offer. It is being developed as an interactive, community-oriented alternative to traditional methods of French lexicography, bringing together user-submitted definitions and definitions from standard French dictionaries (multiple editions of the Dictionnaire de l'Académie Française, Dictionnaire de Littré...), synonyms, pronunciation, as well as a wealth of information on word usage through the use of various computational methods. Our poster will show the DVLF’s functionality, as well as demonstrate its originality.

Fundamentally, the most important aspect of the DVLF is that it aims to create an environment in which its user community rates, critiques, and adds to the collection’s resources as it sees fit. As such, we have made a conscious effort to facilitate community engagement by allowing users to contribute to most sections of our website: definitions, usage examples, synonyms and antonyms. To date, users have added or submitted definitions/examples for hundreds of words, including alexithymie, digiscopie, foucade, and nyctalope. The new version of our site (released in January 2017) has been further enhanced to enable such engagement thanks to more visible links inviting users to contribute content, as well as a new responsive interface designed to work equally well on mobile devices and desktop computers.

In order to attract and engage the largest possible user community, the DVLF is entirely free and open source, and requires no registration. Internet users at large can access the site's resources and contribute material. The DVLF thus tries to adapt to changing word usage and incorporate neologisms so that users will be able to select the word senses and usage examples that they feel are most consistent with contemporary usage. In this manner, the DVLF mirrors the social and evolving nature of language, expanding upon the dictionary's traditionally normative role by giving French speakers and learners access to lexicographic tools so that they might interact with the evolving meaning of words and determine their own understanding of the language.

Over the last year, ARTFL has been working to develop and improve the DVLF. We noticed from statistics gathered from Google Analytics that our community of users was very diverse, accessing our website from Morocco, Tunisia, Canada, and many other francophone and non-francophone countries. This led us to focus a large part of our effort on diversifying content, including usage examples from a wider range of francophone sources in order to provide more coherent descriptions of emerging word usage from Francophone communities around the world, and therefore attract a larger global user base.

Over the course of this redevelopment effort, we decided to rewrite the entire codebase given the various reliability and performance issues we had experienced, switching from a Python-only infrastructure to a Go/Javascript environment. We have also worked to add additional contextual information to our content, starting with the most frequent collocates of any given word based a corpus of over 7,000 texts of French literature from the Middle-Ages to the late 20th century. Additionally, following the lead of recent work in computational linguistics and natural language processing, we also now provide the nearest neighbors based on the computation of word vectors using a word-embedding technique called Swivel. While we could have used alternative algorithms such as word2vec or Glove, we decided to use Swivel because it does not solely rely

only on word co-occurrence to construct vectors. The benefit of Swivel's approach is that it can yield similar words that do not actually occur together in the corpus (see Shazeer et al, 2016), which we thought was a valuable approach given that our dictionaries span across multiple centuries. To generate this word vector model, we used the same 7,000 texts and then extracted the top 20 words for every headword in our dictionary index, which is now displayed on the new version of our site.


Figure 2. New version of the site (went live on January 2017)


Appendix: Screenshots


Figure 1. Old version of the site (retired on January 2017)
4084	2017	Summary

This paper describes a new method for dimensionality reduction, “stable random projection,” (hereafter “SRP”) distinctly suited for large textual corpora like those used in the digital humanities. The method is computationally efficient and easily parallelizable; scales to the largest digital libraries; and creates a standard dimensionality reduction space for all texts so that corpora and models can be easily exchanged. The resulting space makes a wide variety of applications suitable to bag-of-words data, such as nearest neighbor searches, classification, and semantic querying possible with data sets an order of magnitude smaller in size than traditional feature counts.

SRP is a minimal, universal dimensionality reduction with two distinctive features:

1. It makes no distinction between in- and out-ofdomain vocabularies. In particular, unlike standard dimensionality reduction it creates a single space that can hold documents of any language.

2. It is trivially parallelizable, both on a local machine and through web-based architectures because it relies only on code that can be easily transferred across servers, rather than requiring large matrices or model parameters.

These two features allow dimensionality reduction to be conceived of as a piece of infrastructure for digital humanities work, rather than just an ad-hoc convention used in a particular project. This method is particularly useful for provisioners and users of text data on extremely large and/or multilingual corpora. This creates a number of new applications for dimensionality reduction, both in scale and in type. SRP features could usefully be distributed by libraries as a (much smaller and easier to work with) supplement to feature counts. After a description of the method, some novel uses for dimensionality reduction on such libraries are shown using a sharable dataset of approximately 4,500,000 books projected into SRP-space from the Hathi Trust.

Description of the method

The goal of SRP is to reduce of text of uncertain length to a much smaller fixed- length vector to which the many tools of textual analysis, machine learning, and linear algebra can be applied. The core technique used here for dimensionality reduction is random projection. Random matrix theory has emerged in the past few decades as an useful alternative to more computationally complex forms of dimensionality reduc-tion.(Halko, Martinsson, and Tropp 2009) I make use here of the observation that it is possible to project into a space where points as determined purely by sampling randomly from the set [-1,1].(Achlioptas 2003) A true random number generator is not suitable for reproduction. The other core element of SRP, therefore, is a quasi-random projection for every individual word created using cryptographic hashes (specifically, SHA-1).

This allows the method to be defined algorithmically, making it easy to apply to any text. I have written short code libraries to implement the transformation in the three most important language for DH tool development: Python, R, and Javascript. These include a few necessary additional conventions such as minimal tokenization rules, a method for expanding beyond the 160 dimensions provided by SHA, and the byte-encoding of the Unicode character sets.

Comparison to existing methods

The gold standard for dimensionality reduction are techniques that make use of co-occurrences in the term-document matrix such as latent semantic indexing and independent components analysis. More recent techniques such as semantic hashing can be even faster and more efficient at optimally organizing documents in various types of vector spaces designed especially for particular documents.(Salakhutdinov and Hinton 2009) Another strategy finding recent use in the digital humanities is using an LDA topic model as

dimensionality reduction, which produces neatly interpretable dimensions for analysis (Schoch, 2016; Fitzgerald,2016). In both the digital humanities and computer science, scholars frequently use “top-N” words as a good enough approximation of the textual footprint, limiting the dimensions to a few hundred of the most common words in the corpus, producing what Maciej Eder has called “endless discussions of how many frequent words or n-grams should be taken into account” for stylometry.(Underwood 2014, Eder (2015))

These methods suffer two problems that make

them problematic as a general-use feature reduction. First, the better ones are computationally complex, and quite difficult to perform on a very large corpus. Second, it is difficult or impossible to project out-of-domain documents into the space from a standard projection if they contain vocabulary different than the training corpus. This out-of-domain problem presents a particularly great problem for multilingual corpora, because texts that are missing or in sparsely-represented languages will behave erratically in the new environment.

Some other work in the digital humanities and computer science has used hashes, random projection, and other similar methods as an ad-hoc rather than infrastructural technique. SRP can be thought of as a particular species of locality-sensitive hashing, another version of which has been used by Douglas Duhaime to identify reuse in poetic texts based on three-letter phrases.(Duhaime 2016). Also related is the “hashing trick” in computer science(Weinberger et al. 2009), which is better than SRP in many ways for the short documents computer scientists frequently study, but takes significantly more memory to store for book-length documents (an edge case in the computer science literature, but among the most important for humanists).

Applications

This reduced space can be put to many of the same uses as a standard bag-of- words model in considerably less space and with the potential for building web facing tools. Among those to be described are:

1.    Duplicate detection. SRP is quite accurate at identifying duplicate books in a computationally tractable space using cosine similarity, both inside a corpus and across disparate corpora.

2.    Similarity Search. A prototype web page allows any user to paste in any text; it will hashed on the client side into the standard space, and a server can return in a few seconds the most similar documents. The top entries can function for duplicate detection;

the lower ones presenting interesting opportunities for exploratory analysis. A search for Huckleberry Finn, for example, finds a large number of other American adventure novels about boys in the American west.

3. Classification

• SRP features perform approximately as well as top-n words (~77%) on a pre-existing task described by Ted Underwood, separating high- from low-prestige poetry.(Under-wood 2015)

• A single hidden layer neural network trained with 640-dimensional SRP features can accurately classify a held-out sample of books into one of 225 Library of Congress Classification subclasses (for example, whether a work is PR: British Literature or PS: American Literature) with ~78% accuracy based on about 1 million training examples. A single classifier works in multiple languages simultaneously; its determinations on arbitrary pasted text are accessible for inspection through a web site.

• A different single hidden layer neural network trained with SRP features and a novel encoding scheme for years using Google's TensorFlow framework can accurately predict the years for withheld books with a median error of four years from the true publication date.

SRP as Access

SRP fits in the DH2017's theme of “Access” in two ways.

First, it makes many forms of text analysis on huge digital libraries far more feasible for scholars without access to high performance computing resources. On large corpora, data storage and dimensionality reduction can be more resource- intensive than the actual analysis. The dimensionality-reduced dataset for the full Hathi Trust corpus can fit into 10 GB, easily storable on most computers; subsets are suitable for use in classroom or workshop settings.

Second, the ease with which it works with distributed web architectures, and its language agnosticism, can create new routes into neglected portions of large archives, particularly those with insufficient metadata.

Bibliography

Achlioptas, D. (2003). “Database-Friendly Random Projections: Johnson- Lindenstrauss with Binary Coins.” Journal

of Computer and System Sciences, Special issue on PODS

2001,    66    (4):    671-87.    doi:10.1016/S0022-

0000(03)00025-4.

Duhaime, D. (2016). “Plagiary Poets. Plagiary Poets.” http://plagiarypoets. io/.

Eder, M. (2015). “Visualization in Stylometry: Cluster Analysis Using Networks.” Digital Scholarship in the Humanities, November, fqv061. doi:10.1093/llc/fqv061.

Fitzgerald, J. D. (2016) “What Made the Front Page in the 19th Century?: Computationally Classifying Genre in ‘Viral Texts”. July 13 2016 http://jonathandfitzger-ald.com/blog/2016/07/13/keystone-paper.html

Halko, N., Martinsson,P.-G., and Tropp, J. A. (2009). “Finding Structure with Randomness: Probabilistic Algorithms for Constructing Ap- proximate Matrix Decompositions.” arXiv:0909.4061 [Math], August. http:

//arxiv.org/abs/0909.4061.

Salakhutdinov, R., and Hinton, G. (2009). “Semantic

Hashing.” International Journal of Approximate Reasoning, Special section on graphical models and information retrieval, 50    (7):    969-78.

doi:10.1016/j.ijar.2008.11.006.

Schoch, C. (2016, pre-publication) “Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama”. Digital Humanities Quarterly.

Underwood, T. (2014). “Understanding Genre in a Collection of a Million Volumes, Interim Report.” http://figshare.com/articles/Understanding_Genre_ in_a_Collection_of_a_Million_Volumes_Interim_Re-

port/1281251.

Underwood, T.. (2015). “The Literary Uses of High-Dimensional Space.” Big Data & Society 2 (2): 2053951715602494. doi:10.1177/2053951715602494.

Weinberger, K., Dasgupta, A., Langford, J., Smola, A., and Attenberg, J. (2009). “Feature Hashing for Large Scale Multitask Learning.” In Proceedings of the 26th Annual International Conference on Machine Learning, 1113-20. ICML '09. New York, NY, USA: ACM. doi:10.1145/1553374.1553516.
4097	2017	This poster describes one preliminary aspect of a new project about the history of beer and brewing in London. I am building a prosopography of brewers' apprentices in the years from about 1530 to approximately 1800. A catalogue of apprentices and their masters can teach us not only about the social and cultural history of British beer during that time but about the social network of people working in the industry. During this 270-year period, the Worshipful Company of Brewers, the medieval brewing guild first established by Henry VI in 1438, logged nearly 10,000 apprenticeship records (Webb), a collection that serves as just one of many potential datasets that can yield insights into England's brewing culture.

The more immediate goal described in this poster, then, is how to parse these 10,000 records into component parts— people, places, occupations, and dates—so that these relationships can be analyzed and mapped over time. A typical apprenticeship record

looks something like this:

AMBROSE John s John, JlsLey, I3rk, xn^JX§i6Tto Samuel May 14 Jul 1703

In other words: John Ambrose, whose father is John Ambrose from Ilsley, Berkshire and is a maltster by profession, was apprenticed to Samuel May and the fee was paid on 14 July 1703. The record lists three people, a parish, a county, a profession, and a date—a typical dataset found when tracking apprenticeships (Lane).

The simplicity and regularity of that template tantalizingly suggests writing an automated parser, which I am doing with an open source Python module called pyparsing. Although these recursive descent parsers, as they are called, are designed for more elaborate projects (like writing compilers), they are the perfect tool for a job like this because they allow users to construct grammatical “rules” that look like simple additions. For example, the record above can be parsed according a grammatical rule that looks like simple Python:

apprentice + father + location + occupation + to + master + date

But the wide variety of apprenticeship records presents a challenge. As it turns out, the first 10% of the apprenticeship records require nearly 40 different

template “grammars.” The effectiveness of an automated parsing approach—useful but nonetheless somewhat limited—is the main point of the poster. Supplementary strategies (like natural entity parsing and dictionary lookups) may provide some help and, if they prove themselves worthy, they will become part of the presentation as well.

Bibliography

Lane, J. (1996). Apprenticeship in England, 1600-1914. London: University College London Press.

Pyparsing. http://pyparsing.wikispaces.com/.

Webb, C. (1996). London Apprentices: Brewers' Company,

1685-1800. Volume I. London: Society of Genealogists. Webb, C. (2001). London Livery Company Apprenticeship

Registers. Brewers' Company, 1531-1685. Volume 36.

London: Society of Genealogists.
4113	2017	Overview
Wellcome Collection is one of the world's major resources for the study of health and histories. Over the past few years Wellcome have been developing a world-class digital library by digitising a substantial proportion of their holdings. As part of this effort, approximately 5,500 Medical Officer of Health (MOH) reports for London spanning from 1848-1972 were digitised in 2012. Currently Wellcome holds the most comprehensive digital collection of the London MOH reports. Since September 2016 Wellcome have been digitising 70,000 more reports covering the rest of the United Kingdom (UK).

The MOH reports were published annually by the Medical Officers of Health employed by local authorities across the UK. These reports provided vital statistics and a general statement on the health of the population. MOH reports concentrated on reporting infectious diseases and resolving the problems as well as covering other areas of social responsibilities. (Chave, 1987) They have been long regarded as an important source for the 19th and 20th century history of Public Health and stem from reaction to infectious disease in the mid-19th century. Although there were attempts at standardisation, the reports display each MOH's interest, idiosyncrasies and particular strengths. Therefore, they also provide a particular perspective on the everyday lives of Londoners over several generations. No digital techniques have yet been applied successfully to add value to this very rich resource.

As part of the Smelly London project, the OCR-ed

text of the MOH London reports has been text-mined

using the Python programming language. Through text mining we produced a geo-referenced dataset containing smell categories for visualisation to explore the data. At the end of the Smelly London project the

MOH smell data will also be available through other platforms such as Good City Life and Layers of London. This will allow the public and other researchers to compare smells in London from the 19th century to present day. This has the further potential benefit of engaging with the public. This is a collaborative, interdisciplinary project which will allow us to enhance and demonstrate the capabilities of innovative text mining tools we design to allow the automatic extraction of information from OCR-ed text. This paper presents the intended aims of the project; how this was achieved; an analysis of the findings; an interactive map of the results and a browser game of smells and disease. Data and visualisation

As Roy Porter famously remarked that “todays history comes deodorised”, sensory history is a relatively new historical approach. Historians rarely provide us an opportunity to hear, taste or smell the past. Medical historians have incorporated some aspects of sensory history into their research and explored the past belief that bad smells were causes of disease. However, there is very little research carried out covering this period.

Furthermore, smell has a great influence over how we perceive places and contributes to the construction of a place’s identity (Quercia et al., 2015). During the 19th century the paranoia surrounding smells associated with poor hygiene heightened in many European cities (Reinarz, 2014). The Great Stink of 1858 resulted in the discussion of moving Parliament outside London for example. Despite the rise of germ theory (Pasteur and Koch) in the 1880s, concerns with disease-causing miasma (smells) did not disappear entirely. The MOH reports are one of the richest available sources on local public health administration and patterns of disease.

We enriched the text-mining pipeline with Natural Language Processing (NLP), including lemmatisation and part-of-speech tagging. The first iteration of the project has a feature to identify the category of the smells found by using a mapping table to work out the most common smell types. This step complements the close reading analysis and enables us to scale up the amount of information extracted from the texts. Our next research plan is to work on automatic identification of smell terms based on their contextual features to discover new categories that escaped previous classifications. This will allow us to identify smell categories in a data-driven fashion.

As the data becomes more structured, they can be more readily overlaid with other maps and images such as Charles Booth’s London Poverty Map and 19th

century disease maps. Having multiple layers will enable us to run various comparisons and assess if there are any correlations between smells and diseases as well as links to the socio-economic identity of areas in London.

During the first phase of the project we created a smelly map based on the number of smell hits to visualise the first set of results.

smelly_london_20160828

Map of smells across London from Medical Officer of Health reports from 1848 to 1972.


Watford

Enfield

Chalfont St Qles,

Pinner TE. Ruislip Harrow

Wembley

IZB<

•xbndge (Cnro)

rSIOUgh-

London

Hayes-

Hounslow *3131]

racknell

Walton-on

Thames

Croyclon

Sutton.

Figure 1. Smelly Map of London showing all smells

From the list of the existing London local authorities for the MOH reports we compiled, the geographic coordinates of present-day equivalents were extracted using an API. For the places that did not exist in the API, we manually added the geographic coordinates from Wikipedia. On the map each of the points marks the number of smells occurring at the centroid of each of the locations. We grouped the number of smells into sets of ten (e.g. 1- 10, 11 - 20) to avoid having giant points on the map for the places where there are almost 100 smells recorded. Finally, the map scrolls through the years. The data displayed in the mapping visualisation was obtained using text-mining via Python scripting. Python was the language of choice due to its high productivity rate and the fact that there are a large amount of third party libraries that offer highly useful functionality with just a few lines of code. For example, NLTK is a popular Python set of libraries that can achieve advanced NLP.

The next generation of map we produced during the second phase displays different smell categories that are colour-coded. The smell categories used for this map are Sewer; Waste-rubbish; Waste-excrement; Thames; Water; Food; Trade; Animal; Factory-fuel; Disinfectant; School; Air; Decomposition; Habitation;

and Absence of smell. These categories were obtained through manual inspection of the data produced from searching for sentences containing smell-related words. In our codebase, we first analysed 5500 MOH London reports to find sentences that contained smell related words. Once the sentences were further analysed and categorised manually, the results were stored down into a local database by year, borough and a unique ID programmatically.


Figure 2. Smelly map of London showing smell categories

Computer programming can be used to perform tasks thousands of times faster than humans. In the Python code written to extract the data from the MOH reports, parallel processing was employed to speed up the running time of the program. Inside a computer there is a CPU which runs the tasks given by the program. Modern CPUs have multiple cores which allows the calculations to be run concurrently. In our project the CPU had four cores which allowed the running time of the program to be shortened by as much as three times. The next objective for the project is to scale up the size of the text-mining from 5,500 reports to over 70,000 reports covering the entire UK. In order to process such large datasets we are investigating the use of distributed computing resources such as Amazon Web Service (AWS). The code written for this project has been made open source under the MIT license along with documentation so that other programmers or researchers can use the codebase in their own text mining projects. The code has already been used in another project at Wellcome to investigate the idea of women's right to work during the 19th and 20th century London

Vision
The Smelly London project aims to bring together historical data with modern digitisation and visualisation to give us a unique, revealing and visceral glimpse into a London of the past and what it tells us about

London today. Analysing the MOH reports tells the intimate narratives of the everyday experiences of 19th and 20th century Londoners through the ‘smellscape'. The Smelly London project provides a great opportunity to demonstrate how new knowledge and insights have risen from the use of powerful digital applications. This project will produce models that facilitate new kinds of humanities research. All outputs

generated from the project will be open access and

open source. Our data is available in a public repository on GitHub and other platforms.

Bibliography
Bynum, W. F. (1993) Medicine and the Five Senses, Cambridge; New York: Cambridge University Press.

Chave, S.. Recalling the Medical Officer of Health: Writings by

Sydney Chave, London: King’s Fund Publishing Office.

Classen, C, et al. (1994) Aroma: The Cultural History of Smell, London; New York: Routledge.

Cockayne, E. (2007). Hubbub: Filth, Noise and Stench in England 1600 - 1700, New Haven [Conn.]; London: Yale University Press.

Corbin, A. (1986) The Foul and the Fragrant: odor and the French Social Imagination, Leamington Spa: Berg.

Dobson, M. (1994). Malaria in England: A Geographical and Historical Perspective, Parassitologia 36 (1994): 35-60

Dobson, M. (1980)“Marsh fever”-The geography of malaria in England, Journal of Historical Geography 6(4) : 35789.

Jenner, M. (2011) ‘Follow your nose? Smell, smelling, and their histories’, The American Historical Review, 116,

350

Quercia, D., Schifanella, R., Aiello, L. M., McLean, K.

(2015). Smelly Maps: The Digital Life of Urban Smellscapes, Proceeding of the 9th International AAAI Conference on Web and Social Media (ICWSM).

Reinarz, J. (2014) Past Scents: Historical Perspectives on Smell, Chicago: University of Illnois Press, 2014.

Thompson P, Batista-Navarro RT, Kontonatsios G, Carter J, Toon E, McNaught J, et al. (2016) Text Mining the History of Medicine, PLoS ONE 11(1): e0144717. doi:10.1371/journal.pone.0144717
4120	2017	This poster describes the development and uses of Archivlo, an application for improving the archival research workflow and enabling a more collaborative digital research community. In recent years, digital history has emerged as a vibrant subfield of the digital humanities community (see Robertson, 2016; and Weingart, 2016).Currently, the majority of digital history projects rely on digitized corpuses or community compiled datasets. However, the archival materials used in these projects represent only a small fraction of the archival sources that scholars currently utilize in their research.

Moreover, the proliferation of digital cameras and scanners has resulted in a wealth of archival material for scholars, but this digitized archival data is usually scattered across hard drives. To organize this data, scholars currently either keep notes or re-purpose bibliographic software.

Data management software provide some solutions to dealing with this abundance of material (such as Devonthink, Evernote, Zotero, and most recently, Tropy) but individual scholars often must invest a great deal of energy and time replicating the organizational structure of the archives to make sense of their research. This siloed approach to archival research makes finding information about archival collections or other scholars working in the archives difficult. Ar-chivlo is designed to solve these problems, and create a more coherent workflow for organizing archival data.

This poster will outline the development and design of Archivlo, from the early idea stages to our initial beta model. Archivlo is currently in progress, and the poster will share our experience building a web-based application, as well as designing a user interface that privileges data interoperability and flexibility. Archi-vlo is written in Python and Angular, and is fully opensource on Github. To access archival data, Archivlo utilizes archives' APIs and web page annotations to allow researchers to find collections. Users are able to save their archival collection research in their profile, and indicate whether they have worked in these archives or are interested in using the archive. This functionality adds efficiencies to how scholars locate and keep track of their archival research. Users can also export their records to multiple file formats, as well as other data management software, such as Zotero andDevon-think.

Additionally, Archivlo enables users to share their lists of visited and interested in archives, which we believe will help scholars share information about archives and potentially even form collaborations. For archivists, Archivlo can also provide data on user interest vis-a-vis usage of their archival collections. We believe our experience with Archivlo will be of interest to other digital humanities developers and project managers, as well as digital humanists who work with archival collections.

Previous efforts to encourage digital collaboration among researchers in archives have, with a few exceptions, largely faltered, with most of these projects requiring a high technical literacy to contribute to a database or extensive time to transcribe records (see Mostern and Arksey, 2016. Moreover, these efforts to construct large databases of archival data have been forced, through copyright restrictions, to limit their scope to material that is either from prior to the early twentieth century or born digital materials). Instead of requiring large resources to digitize materials or standardize collections, Archivlo presents an alternative solution to this problem - focusing on how scholars work with archives to enable more digital and collaborative research. We believe Archivlo will encourage more productive data management practices among scholars, and reduce inefficiencies in the archival research workflow. Much of Archivlo's goals remain experimental, and the opportunity to present our work at DH 2017 would help us share our progress and consider future directions for the tool.

Ultimately, we hope that Archivlo can help further the digital humanities ethos of digital collaboration, and present one solution for using tools to help foster digital research communities.

Bibliography

Mostern, R., and Arksey, M. (2016) “Don't Just Build It,

They Probably Won't Come: Data Sharing and the Social Life of Data in the Historical Quantitative Social Sciences”, International Journal of Humanities and Arts Computing, Volume 10 Issue 2, 205-224.

Robertson, S. (2016) “The Differences between Digital Humanities and Digital History” in Debates in Digital Humanities 2016, ed. Matthew K. Gold and Lauren F. Klein, (University of Minnesota Press, 2016), 289-307

Weingart, S. (2016) “Acceptances to DH2016 (pt. 1)”,

March 22, 2016, <https://scottbot.net/acceptances-

to-dh2016-pt-1/>.
4147	2017	Research Objects and scholarship in the digital age

As scientific research practice has grown to include ever greater quantities of data, larger collaborations, and distributed methods, Research Objects (Bechhofer et al, 2013) have been introduced as a means to gather together the context surrounding an investigation and to support its future validation, understanding, and reuse. In many cases these Research Objects build upon the methods, output, and provenance already captured and encoded by workflow systems -- the digital environments in which the science is conducted.

More recently there have been proposals for the use of Research Objects within the digital humanities and musicology (Dreyfus and Rindfleisch 2014; De Roure et al. 2016). Digital editions and annotations of digitally encoded works can be viewed as manifestations of workflows deployed in musicological scholarship, raising the question of how e.g. editorial annotations in a digital score should reference other digital items within the Research Object and vice versa.

For example, a study of the Mariinsky Opera's rendition of Wagner's Ring cycle in November 2014 produced a multimedia dataset including audio, annotations made by a musicologist on a short score before the performance (annotations which could be identified a priori from the score such as dynamics, appearances of a leitmotif, etc.), further annotations captured digitally by the musicologist during the live performance (typically staging and interpretive commentary), and free-form text from a digital pen (Page et al.

2016). If the score in use had been a digital edition encoded in MEI (the XML-based Music Encoding Initiative. as reviewed by Crawford and Lewis 2016) how might we reference the musicologist's annotations? Or the other media objects captured and the metadata describing them; existing Linked Data references to Wagner and leitmotifs; and to earlier surveys and studies made by the musicologist on leitmotif interpretation? Linked notation in support of musicology

Notation examples are a vital part of analytical essays in musicology, helping to illustrate analytical observations and justify hypotheses, arguments and conclusions. They can be excerpts from a score, or custom-made notations which add annotations or comments to the original notation.

Furthermore, the presentation of multiple analogous musical extracts for comparison is often required to support a musicological narrative. Paradigmatic analysis, for example, involves passages of music placed one above another such that analogous elements are directly juxtaposed, with gaps left as necessary to ensure that vertical alignment is preserved. Stacked presentation of different scores or different parts of the same score have been used for well over a century, but they quickly become unwieldy and hard to interpret, especially as the number of extracts increases. What is not available in such paper-based approaches is the interactivity that can make complex comparisons between many extracts practical by turning a static presentation into an iterative exploration of digital materials.

In this paper we consider the example of a digital companion presenting the contents of a Research Object studying the interpretation of leitmotif examples from Wagner's compositions, specifically the Ring cycle, as they are presented in numerous historical introductions, opera guides, leitmotivic threads and leitmotif lists included in libretto editions and piano scores. The study of the incidences of these particular leitmotif identifications consists of both the gathering of source material and its digitisation and cataloguing, and a musicological study of the potential relationships, influence and evolution between leitmotif interpretations. To enable the extension and repurposing of the identified leitmotif relationships they are structured using an ontology.

Notation examples in leitmotif guides are usually abstractions drawn from a piano score. When reporting findings from this research it is desirable to present and relate the scholarly arguments back to the musicological context within which they are made: from the score excerpts in the source material; and to MEI encodings that illustrate and encode both the examples from which they are drawn and to complete (piano) scores of the overall operas.

This enables matching and linking of the examples as they are described in the scholarly text, via semantic hyperlinks, to and from the score, including exact matches and variants, illustrating interpretations, and situating the examples back in context. Encoding interpretations in the form of notation examples as variant readings of a certain passage could thereby chart the ‘understanding' of the work as a history of its variants.

(For example comparing: Richard Wagner, Die Walküre, piano score by Felix Mottl, Leipzig,

Peters, 1914, p.165; Hans von Wolzogen, Thematischer Leitfaden durch die Musik zu Richard Wagners Festspiel Der Ring des Nibelungen, 2nd ed., Leipzig, Schloemp, 1876, p.58, ‘Schicksalsmotiv'; Gustav Kobbe, Wagner's Music Dramas Analyzed With the Leading Motives, New York: Schirmer, 1923, p. 57, ‘Motive of Fate'; George Dunning Gribble, The Master Works of Richard Wagner, London, Everett, 1913, p. 289, ‘Fate Motif'.)

Introducing MELD: Music Encoding and Linked Data

To realise the digital notation companion we have developed the MELD framework (Music Encoding and Linked Data). MELD enables the interactive presentation of multimedia contents of the Research Object, such as the images, text, audio, and MEI encoded music notation described in the previous section. These can be explored contextually alongside each other through the use of semantic links, encoded using RDF, which describe the musicological relationships between the resources (and elements within them). In contrast to earlier technologies which have typically aligned resources against a timeline (e.g. in milliseconds, or using MIDI), MELD expresses relationships anchored to musically meaningful items scoped using MEI. Figure 1 shows a screenshot of MELD displaying text and notation, highlighting leitmotifs as identified in different historical guides.


Figure 1. MELD displaying contextualised text and music notation.

To render our music notation (encoded using MEI) we use Verovio (Pugin et al. 2013), an open-source MEI renderer that produces beautiful SVG renditions of the score. In addition, Verovio provides an architecture in which identifiers (in other words, anchors for our relational Linked Data expressions in the MEI XML) are persisted through to the rendering (in SVG) which can be connected to identifiers in our contextual information (in RDF). When rendered (and re-rendered) for the user in our web based application interface, the browser uses these identifiers to generate interface elements and undertake actions that combine information from the MEI and the Linked Data.


Figure 2. Musicological relationships, encoded using Open Annotations, within the Research Object (simplified).

Within the Research Object, we treat the XML IDs of elements within the MEI resource as fragment identifiers, so URIs can be straightforwardly generated for each notation element of interest. We employ the Web Annotation Data Model (Sanderson et al. 2017), using these URIs as targets of annotations representing each musicological marking. Corresponding annotation bodies are associated with semantic tags defined to encode the different musicological interpretations, which are in turn the annotation bodies of a top-level annotation targeting the URI of the files currently being viewed, including the music encoding (MEI) and scholarly interpretation (HTML). A simplified example of such relationships is shown in Figure 2.


Figure 3. The MELD framework (shading corresponds to that in Figure 2).

The MELD client then uses HTML/CSS and JavaScript, served by a simple web service implemented with Python Flask, and illustrated in Figure 3. The procedure driving the rendering and user interaction is illustrated in Figure 1. The client processes a framed (see the explanation of framing) JSON-LD representation of the RDF graph instantiating the data model. It then performs an HTTP GET call to acquire the MEI resource targeted by the top-level annotation, and renders the corresponding musical score to SVG using Verovio. User interactions are captured using HTML divs drawn as bounding boxes over portions of the SVG corresponding to MEI elements of interest; this is simplified by Verovio's retention of MEI identifiers in the produced SVG output.

Acknowledgements

This work has been supported by the UK Arts and Humanities Research Council grant AH/L006820/1 ‘Transforming Musicology', and EPSRC grant EP/L019981/1 ‘Fusing Audio and Semantic Technologies for Intelligent Music Production and Consumption'. We thank all our colleagues on these projects who have supported and encouraged this work, particularly Carolin Rindfleisch for her complementary work on musicological ontologies, and Laurence Dreyfus for wider inspiration and motivation in developing these technologies.

Crawford, T., & Lewis, R. (2016). Review: Music Encoding Initiative. Journal of the American Musicological Society, 69(1), 273-285.

De Roure, D., Klyne, G., Page, K.R., Pybus, J., Weigl, D.M., Wilcoxson, M., Willcox, P. (2016). Plans and Performances: Parallels in the Production of Science and Music. Proceedings of the 2016 IEEE 12th International Conference on e-Science. IEEE, pp. 185-192.

Dreyfus, L., & Rindfleisch, C. (2014). Using Digital Libraries in the Research of the Reception and Interpretation of Richard Wagner's Leitmotifs. Proceedings of the 1st International Workshop on Digital Libraries for Musicology. ACM, pp. 1-3

Page, K., Nurmikko-Fuller, T., Rindfleisch, C., Weigl, D.

(2016). Digital Annotation Tooling for Opera Performance Studies. Digital Humanities 2016: Conference Abstracts. Jagiellonian University & Pedagogical University,

Krakow, pp. 306-309.

Pugin, L., Zitellini, R., & Roland, P. (2014). Verovio: A Library for Engraving MEI Music Notation into SVG. Proceedings of the 15th International Society for Music Information Retrieval Conference, pp. 107-112.

Sanderson, R., Ciccarese, P., Young, B. (2017). Web Annotation Data Model. W3C recommendation.

Bibliography

Bechhofer, S., Buchan, I., De Roure, D., Missier, P., Ainsworth, J., Bhagat, J., Couch, P., Cruickshank, D., Delderfield, M., Dunlop, I. and Gamble, M. (2013). Why linked data is not enough for scientists. Future Generation Computer Systems, 29(2), pp.599-611.
4154	2017	Emerging from the crucible of the religious upheaval that characterizes the English Renaissance is arguably the most influential English book ever printed in terms of its impact on Anglophone religious, literary, popular, and legal culture: The Book of Common Prayer (BCP). Encoded within its pages is a kind of algorithm, an annually recurring process, a ritual-ization of both private devotion and public worship for generations of post-Reformation English readers. Taking our cue from Brad Pasanek (Metaphors of Mind: An Eighteenth-Century Dictionary, 2015) and Peter Stally-brass (“Against Thinking.” PMLA, Vol. 122, No. 5), both of whom have drawn useful analogies between the database and the commonplace book, we employ the creative anachronism of the “Bible app” to describe the function of the BCP in early modern England. As the first such “app” of its kind, the BCP choreographed religious meaning and ritualized worship for a whole generation of English Bible readers, shaping them into religio-political subjects who were then able to situate their lived experiences within a communally shared time and space. From the perspective of the Early Modern layperson, the BCP provides mediated access to the newly translated biblical text. Of course, from the abstracted perspective of the nascent nation-state of England, the BCP functions as a way to mitigate new anxieties surrounding the democratization of sacred scripture. As the legally established, official means by which sacred text is encountered, the BCP is nothing less than a masterpiece of social engineering.

To extend the metaphor of text as program, the BCP can also be thought of as a class, one which can be inherited and sub-classed, instantiated and “hacked” according to the agenda of particular readers who would produce, via their nuanced reading of BCP ritual, slightly different kinds of subjects according to the specific context in which they find themselves. Of particular interest to the project at hand is a 1586 BCP which has been highly “sub-classed” by one of its owners. Bound together with the prayer book is an entire psalter, whose collection of 150 psalms is cross-referenced in a single hand, which also makes occasional thematic/tonal annotations. In our examination of this prayer book, we wish to develop a methodology for accessing the kind of subject such a “re-engineered” BCP might have produced.

Implied in the very notion of access, of course, is mediation. Within the limited scope of our project, we do not have recourse to the intense amount of labor required to perform a rigorous exegesis of the entire psalter according to how its 16th-century readers might have read it. What we do have, via the psalter's marginalia, is what one (or perhaps two) reader(s) selected as noteworthy in their BCP-regulated practice of reading the Psalms. We also have our own attempts to thematize and register the tone of those same texts. Given these assets, we attempt to provide via the Psalter Project a representation of how a subject produced by this prayer book might look from our perspective. Our hope is that, despite the inherently mediated nature of such a representation, we might provide students and scholars alike a better understanding of the “programmatic” nature of religious para-texts like the BCP.

The Psalter Project was born out of Dr. Nandra Perry's scholarship in Early Modern English literature in partnership with Bryan Tarpley's work as Lead Software Applications Developer for the Initiative for Digital Humanities, Media, and Culture (IDHMC) at Texas A&M University. Perry's work with the 1586 prayer book led her to apply for a Summer Technical Assistance Grant with the IDHMC, at which point she was awarded some of Tarpley's development time. Together, they (we) designed a relational database schema for recording the cross-references and tags found in the 1586 prayer book, as well as a web-interface for entering and viewing them. A beta version of the project is available for viewing.

Before commencing development on the Psalter Project web app, a survey of extant, web-based tools was performed to determine whether any one tool (or collection of tools) already satisfied our requirements. Given the marginalia we have to work with (a large amount of scriptural cross-references and thematic

tags), we wanted a tool to facilitate the capture and

that they too might be able to provide (mediated) access to religio-political subjects.


analysis of this data. While there are indeed several

digital annotation tools available, such as MIT’s Annotation Studio, or the University of Virginia’s PRISM, none of these tools allow for the rapid entry of scriptural verse range references and tags. Tarpley implemented the database schema by creating a MySQL database and wrote the web application in Python using the Django web framework, as well as other web-related technologies, such as the jQuery Javascript library and the Bootstrap CSS framework. The database and web app are both hosted by the IDHMC’s server infrastructure at Texas A&M. As a way of beta-testing the app, Perry then recorded a sampling of the psalter’s cross-references and tags, and also included her own tagging of the referenced verses in terms of both thematic content and perceived emotional impact (affect).

While the Psalter Project in still in development both in terms of methodology and finished product, a preliminary sketch of our reader is emerging via the various views made available through the web app. There is a visualization of the thematic and affective tags in the form of tag-clouds weighted by frequency, a view of the most frequently referenced verses along with their tags, a break-down of all referenced verses (and associated tags) by any specific thematic or affective tag, a presentation of the network of referenced verses (and associated tags) by any specific verse, and a presentation of any of the 150 Psalms in its entirety where referenced verses can be hovered over, displaying tags and referenced verses. At this initial stage, we believe that this multi-faceted portrayal is revealing a reader with a profound sense of group identity as the loyal subject of a just God who provides deliverance to the deserving and punishment to “bad” subjects and oppressive outsiders. The effect, it would seem, is the sacralization of a religio-politics in which the reader’s relationship to God is analogous to his/her relationship to the nation-state (and vice versa), thereby justifying, in turn, a posture of hostility toward outsiders and “bad” subjects.

With this paper, we intend to not only provide a more fully fleshed-out representation of the Early Modern religio-political English subject, but to interrogate the various assumptions and methodologies we use to provide this representation so that we might improve the Psalter Project web application. We hope to be able to provide this web application (both in terms of an open- source repository and as an IDHMC hosted web service) to other scholars in the future, so
4162	2017	Background and General Motivation
The HathiTrust Digital Library

The HathiTrust Digital Library (HTDL) comprises digitized representations of 15.1 million volumes: approximately 7.47 million book titles, 418,216 serial titles, and 5.3 billion pages, across 460 languages. HTDL is best described as “a partnership of major research institutions and libraries working to ensure that the cultural record is preserved and accessible long into the future”.

The HathiTrust Research Center (HTRC) develops software models, tools, and infrastructure to help digital humanities (DH) scholars conduct new computational analyses of works in the HTDL. For many scholars the size of the HTDL corpus is both attractive and daunting: many existing DH tools are designed for smaller collections, and many research inquiries are facilitated by more focused, homogeneous collections of texts (Gibbs and Owens, 2012).

Worksets

In many, if not most, DH research endeavours, performing an analytical task across the whole HTDL is neither practical nor productive (Kambatla et al., 2014). For example, a tool trained to identify genre attributes of 18th century English language prose fiction may not be applicable to 20th century French poetry. The first step is to identify the subset -- of works, editions, volumes, chapters, pages -- to set an initial investigative scope and, indeed, subsequent iterative refinements of a subset as research proceeds. In a corpus as large and complex as the HTDL, finding materials and then defining the sought after subset can be extraordinarily difficult.

HTRC has come to call collections of digital items brought together by a scholar for her analyses a “workset”, created to help the researcher build, manipulate, iteratively define and compare their collections. Reflecting upon input and advice from the DH community, Jett (2015) defines a workset as a machine-actionable research collection realised as:

1.    An aggregation of members (volumes, pages, etc.);

2.    Metadata intrinsic to the workset's essential nature (e.g., creator, selection criteria);

3.    Metadata intrinsic to digital architectures (i.e. creation date & number of members);

4.    Metadata supportive of human interactions (i.e. title & description);

5.    Derivative metadata from workset members (e.g. format(s), language(s), etc.); and,

6.    Metadata concerning workset provenance (e.g. derived from, used by, etc.).

Broadly, item 1 identifies the actual data used in an analysis; whereas the remaining metadata items describe the workset itself, aiding workset management throughout the research cycle.

Cross-corpus worksets

As alluded above, numerous criteria can be used to select the constituents of a workset; and several technological implementations could, in theory, realise worksets. In researching the design and realisation of worksets and associated tooling, we are also mindful to remain grounded in their practical application and the needs of scholarly users. We have therefore undertaken our work through discipline-based scenarios in which we can explore the strengths and weaknesses of the HTDL viewed through the prism of worksets.

We report one such exploration here, questioning whether (relatively) small, well explored, and well understood corpora can be superimposed over the HTDL to aid navigation and investigation of the much larger and superficially understood HTDL collection?

From a system perspective, a cross-corpus workset requires exposing compatible metadata (items 2-6 above) from multiple collections, first used to align common elements, and then to assemble worksets. We take a Linked Data approach and achieve compatibility through ontologies, which might initially be bibliographic (and derived from library records) but should be iteratively extensible into the domain of the subject of study.

Examples in early English print

Early English Books Online Text Creation Partnership (EEBO-TCP) is a partnership with ProQuest and over 150 libraries and universities, led by Michigan and Oxford, to generate highly accurate, fully-searcha-ble texts tracing the history of English thought and learning from the first book printed in English in 1473 through to 1700. Between 2000-2009 EEBO-TCP Phase I converted 25,000 selected texts from the EEBO corpus into TEI-compliant, XML-encoded hand-transcribed texts, subsequently freely released in January 2015.

In the work reported here, we have conjoined EEBO-TCP with a HathiTrust subset consisting of all materials described in their metadata as being in English and published between 1470 and 1700.

To ensure a prototype which simultaneously explored the fit of scholars' needs to the technology and exercised the technical challenges outlined in the previous section, we undertook a ‘complete circuit' through the datasets (Figure 1). We: (i) ran a consultative workshop to choose investigations which might form the basis of worksets; (ii) used these abstract worksets to identify concrete requirements for the conjoined metadata; (iii) generated metadata from both corpora according to these specifications; (iv) aligned elements from both datasets in an overlapping

superset; (v) realised the worksets identified in (i) using this metadata.


Figure 1. Overview of the metadata circuit leading to our cross-corpora workset

Motivating worksets

Following the workshop we identified the following workset selections; we describe their implementation in subsequent sections:

• Find all the works, appearing in both datasets, written by Richard Baxter.

• Find works in both datasets published in Oxford.

• Find works published outside of London (where the bulk were published).

• Find works from both datasets published outside of London in the mid-to late 1600s.

• Find all works in the two datasets for authors who have at least once published on the subject of “Political science”.

• Find all works in these two datasets for authors who have at least once published works which are categorised as “biography”.

Regarding the penultimate workset, it is of particular note that this returns results across both datasets, since our EEBO-TCP import did not contain genre or topic information; this association must be entirely inferred from the semantic links via the technology described below.

Implementation

Metadata requirements and ontology selection

Building on Nurmikko-Fuller et al. (2015) and Jett et al. (2016) we surveyed the addressable resources and the schema expressivity of ontologies that could parameterise these classes of workset. We identified

parsable information structures in the EEBO-TCP TEI

data, appropriate to the test worksets, and selected ontology terms to encode this EEBO-TCP metadata, ensuring compatibility (or at least, for our purposes, comparison) with RDF from the HathiTrust. The resultant ontology collection - the EEBO Ontology, or EEBOO - includes selections from MODS, Bibframe, and PROV, along with custom elements encoding additional structures (e.g. dates).

Creating EEBOO RDF and alignment with HTDL

Python scripts manipulated TEI P5 XML, then the Karma Data Integration Tool mapped EEBO-TCP data structures into the EEBOO ontology. Particular attention was paid to dates encoded within strings, an example of rich semi-structured data that can be extracted into structured RDF. Links to author records in

VIAF and the Library of Congress (LoC), and multimedia pages in the HTDL and ‘JISC Digital Books' website, were generated and added. Finally, author names were aligned between the EEBOO and HTDL triples using a reconfiguration of the SALT tool (Weigl et al. 2016).

24,926 EEBO-TCP Phase 1 records were processed, with 22 distinct types of information in the headers, including 6 different ID types and 3 types of date (publication date of historical work, author associated historical date(s), XML publication/editing dates). EEBOO incorporates 7 of these datatypes, and extends into subcategories for author names and date types. EEBOO contains 713 unique places, 6,489 unique expressions of Person of which 3,588 have VIAF and LoC IDs.


Figure 2. Architecture providing cross-corpus worksets for early English print

Workset construction and viewing
A Virtuoso triplestore (see also, the Virtuoso Github repository) stores the RDF data (totalling 1,137,502 triples) and provides a SPARQL query interface. Figure 2 shows the overall system architecture. The workset constructor user interface (Figure 3) allows the user to select parameters in a web interface which are, in the background, assembled into SPARQL queries used to create a workset. The interface automatically populates valid attributes that are themselves retrieved from the triplestore, using ontological terms having equivalent meaning across datasets. In combination, the generated triples and SPARQL queries are fully sufficient for expressing the motivating workset definitions described earlier.

The workset viewer (also Figure 3) then retrieves RDF workset contents, record metadata, data links, and multimedia links (to the Historic Books collection or the HTDL). Both web applications are written in Python, using the Flask framework, and both rely on the semantic information encoded in RDF and queried using SPARQL.

Current Workset Parameters    Elephant Workset Viewer p«m.u.«r ;    All works published in Oxford g> ® ®


Figure 3. Prototype workset constructor and viewer (example worksets shown)

Conclusion and future work
We have demonstrated the general feasibility of cross-corpus worksets in bringing together HathiTrust content with specialised collections through a specific implementation for early English printed books linking the HathiTrust to EEBO-TCP. Using Linked Data, we see that metadata can be extended in a piecemeal or iterative fashion, potentially moving beyond traditional bibliographic metadata to include semantic structures emerging from scholarly investigation of the worksets themselves; and in doing so support academic motivations and requirements for workset creation.

Acknowledgements

We are grateful to our colleague Pip Willcox for her valuable input and organisation of scholars' workshop, and Jacob Jett for his workset ontology. This work was supported by the Andrew W. Mellon Foundation through the Workset Creation for Scholarly Analysis project award.

Bibliography

Gibbs, F., Owens, T. (2012). Building better digital humanities tools: Toward broader audiences and user-centered designs. Digital Humanities Quarterly 6(2). Accessible via:    http://www.digitalhumani-

ties.org/dhq/vol/6/2/000136/000136.html

Jett, J. (2015). Modeling worksets in the HathiTrust Research Center: CIRSS Technical Report WCSA0715. University of Illinois at Urbana-Champaign. Available

via: http://hdl.handle.net/2142/78149

Jett, J., Nurmikko-Fuller, T., Cole, T.W., Page, K.R., Downie, J.S. (2016). Enhancing scholarly use of digital

libraries: A comparative survey and review of bibliographic metadata ontologies. IEEE/ACM Joint Conference on Digital Libraries (JCDL) pp. 35-44, 2016.

Kambatla, K., Kollias, G., Kumar, V., Grama, A. (2014). Trends in big data analysis. Journal of Parallel & Distributed Computing 74(7), pp 2561-2573.

Nurmikko-Fuller, T., Page, K., Willcox, P., Jett, J. Maden, C., Cole, T., Fallaw, C., Senseney, M., Downie, J.S.

(2015). Building Complex Research Collections in Digital Libraries: A Survey of Ontology Implications. ACM/IEEE Joint Conference on Digital Libraries (JCDL) p. 169-172, 2015.

Weigl, D. M., Lewis, D. L., Crawford, T., Knopke, I., Page, K. R. (2017, in press). On providing semantic alignment and unified access to music-library metadata. International Journal on Digital Libraries. Springer.
4196	2017	Computationally-intensive research methods have seen increasing adoption among digital humanities scholars, but for scholars outside R1 institutions with robust computing environments, techniques like pho-togrammetry or text recognition within images can easily monopolize desktop computers for days at a time. Even at institutions with a research computing program, systems are configured for scientific applications, and IT staff may be unaccustomed to working with humanities scholars, particularly those who are not already proficient at using the command line. National compute infrastructures in North America (Compute Canada and XSEDE) are a compelling alternative, providing no-cost compute allocations for researchers and offering support from technical staff interested in and familiar with humanities computing needs. This workshop will start by introducing participants to Compute Canada and XSEDE, cover how to obtain a compute allocation (including for researchers outside of the US and Canada), and proceed through two hands-on tutorials on research methods that benefit from the additional compute power provided by these infrastructures: 1) photogrammetry using PhotoScan and 2) using OCR via Tesseract to extract metadata from images.

Photogrammetry
Photogrammetry (generating 3D models from a series of partially-overlapping 2D images) is quickly gaining favor as an efficient way to develop models of everything from small artifacts that fit in a light box to large archaeological sites, using drone photography. Stitching photographs together, generating point clouds, and generating the dense mesh that underlies a final model are all computationally-intensive processes that can take up to tens of hours for a small object to weeks for a landscape to be stitched on a high-powered desktop. Using a high-performance compute cluster can reduce the computation time to about ten hours for human-sized statues and twenty-four hours for small landscapes. Generating a dense cloud, in particular, sees a significant performance when run on GPU nodes, which are increasingly common in institutional HPC clusters and available through Compute Canada and XSEDE.

One disadvantage of doing photogrammetry on an HPC cluster is that it requires use of the command line and Photoscan's Python API. Since it is not reasonable to expect that all, or even most, scholars who would benefit from photogrammetry are proficient with Python, UC Berkeley has developed a Jupyter notebook that walks through the steps of the photogrammetry process, with opportunities for users to configure the settings along the way. Jupyter notebooks embed documentation along with code, and can serve both as a resource tool for researchers who are learning Python, and as a stand-alone utility for those who want to simply run the code, rather than write it. Indiana University, on the other hand, has developed a workflow using a remote desktop interface so that all the GUI capabilities and workflows of PhotoScan are still available. A python script is still needed so that the user may avail herself of the compute nodes, but the rest of the workflow is very similar traditional PhotoScan usage. Finally, both methods offload the processing the HPC cluster, allowing users to continue to work on a computer that might normally be tied up by the processing demands of photogrammetry.

The workshop will give participants hands-on experience creating a 3D model using two different approaches: first, by accessing the Photoscan graphical user interface on a virtual desktop running on XSEDE's Jetstream cloud resource; and second, by using a Jupy-ter notebook running on an HPC cluster.

OCR
Optical Character Recognition (OCR) is a tool used for extracting text from images and is perhaps most well known as a core technology behind the creation of the Google Books and HathiTrust corpora. OCR continues to open historical texts for analysis at large scale, fuelling a significant portion of research work

within the digital humanities to the point that it would

be difficult to think of the “million books problem” existing without this technology. While there are many OCR tools available the most popular tool that is also free and open source is Tesseract.

This portion of the workshop will also make use of Jupyter Notebooks to provide templates for learning the development process and that can then be taken away to speed development of future code. We will feature two projects for participants to practice with. A “traditional” OCR task that will have workshop participants processing images from the London Times in a demonstration of the improvements in OCR over the past few years and a task focusing on processing historical photographs to find text that can be added to the associated metadata to improve the searchability of an index.

Target Audience
We anticipate that this workshop will appeal particularly to scholars who work with cultural heritage materials (a field where photogrammetry is an increasingly common method for generating digital surrogates), as well as those who work with archival photographs, and scholars with large corpora of photographs. It will also be relevant for scholars who already engage in computational analysis of primary sources, who wish to increase the efficiency of their analysis by leveraging high-performance compute environments. No previous experience with HPC environments is necessary. This workshop can accommodate 25 participants.

Instructors
Quinn Dombrowski
Quinn is the Humanities Domain Expert at Berkeley Research Computing. At UC Berkeley, Quinn works with humanities researchers and research computing staff at Research IT to bridge the gap between humanities research questions and campus-provided resources for computation and research data management. She was previously a member of the program team for the Mellon-funded cyberinfrastructure initiative Project Bamboo, has led the DiRT tool directory and served as the technical editor of DHCommons. Quinn has an MLIS from the University of Illinois, and a BA and MA in Slavic linguistics from the University of Chicago.

Tassie Gniady
Tassie manages the Cyberinfrastructure for Digital Humanities group at Indiana University. She has a PhD in Early Modern English Literature from the University of California-Santa Barbara where she began her digital humanities journey in 2002 under the wing of Patricia Fumerton. She coded the first version of the NEH-funded English Broadside Ballad Archive, making many mistakes and learning much along the way. She now has an MIS from Indiana University, teaches a digital humanities course in the Department of Information and Library Science at IU, and holds regular workshops on text analysis with R and photogramme-try.

Megan Meredith-Lobay
Megan Meredith-Lobay is the digital humanities and social sciences analyst, as well as the Vice President, for Advanced Research Computing at the University of Briitsh Columbia. She holds a PhD from the University of Cambridge in medieval archaeology where she used a variety of computing resources to investigate ritual landscapes in early medieval Scotland Scotland. Megan has worked at the University of Alberta where she supported research computing for the Faculty of Arts, and at the University of Oxford where she was the programme coordinator for Digital Social Research, an Economic and Social Research Council project to promote advanced ICT in Social Science research.

John Simpson
John Simpson joined Compute Canada in January 2015 as a Digital Humanities Specialist and bringing a diverse background in Philosophy and Computing. Prior to Compute Canada, he was involved in a research-intensive postdoctoral fellowship focusing on developing semantic web expertise and prototyping tools capable of assisting academics in consuming and curating the new data made available by digital environments. He has a PhD in Philosophy from the University of Alberta, and an MA in Philosophy and BA in Philosophy & Economics from the University of Waterloo. In addition to his role at WestGrid, John is also a Member-at-Large of the Canadian Society for Digital Humanities (CSDH-SCHN), a Programming Instructor

with the Digital Humanities Summer Institute (DHSI), and the national coordinator for Software
6258	2018	
        
            Description
            This half day workshop is an introduction to word vectors and text vectorization broadly. We will focus on building intuition of how word vectors work, incorporating visualization methods, using pre-trained vectors, and exploring applications of word embeddings. We will teach you both the high-level concepts and the practical usages of these widely used analytical tools for text analysis in digital humanities (DH). It is a hands-on workshop with practical activities for the participants starting with a review of word vectors by way of visualization, an overview of downloadable word vectors, and examining the potential pitfalls of using word vectors in humanistic analysis and the methods for mitigating these issues. Given the general applicability of machine learning models in real life, addressing issues concerning biased models, datasets, and algorithms, is of vital importance for correct interpretation of their applications.
            We will provide a Python Jupyter Notebook and an accompanying text corpus that we will work through as a group. By the end of the workshop, the participants will have working knowledge of how and where to download or train word embeddings and the caveats of using them.
                
            
            Relevance to the DH Community
            Since the apparition of analytical approaches to distant reading and macro-analysis, popularized by Moretti and Jockers, and the possibility of access to huge amounts of textual data and long-term studies such as Culturomics, new tools were needed to tackle the increasing complexity of large corpora. Borrowing from advances in machine learning and computational linguistics, digital humanists have experimented with various methods of text quantification for interpreting macro contours of culture and language. In particular, word vectors have gained recognition for their versatility in DH studies. Scholars have used word vectors in a variety of tasks such as measuring similarity in word meaning (Caliskan et al., 2017), authorship attribution (Kocher et al., 2017), or dialogism in novels (Muzny et al., 2017).
            
                This workshop is both a theoretical and practical introduction to humanist applications of these methods. Those interested in large scale text-analysis of any corpora will learn the basics of transforming textual data into numerical form.
            
            Instructors
            Eun Seo Jo researches the language of American foreign relations in historical contexts and applications of NLP and ML in history. She is a PhD candidate in history at Stanford University where she is also a member of the Literary Lab and a Digital Humanities Fellow. She has presented at various DH conferences and is a DH methodology consultant at Stanford.
            
                Scott Bailey is a Research Developer in the Center for Interdisciplinary Digital Research in the Stanford University Libraries. He collaborates and consults on research projects across the humanities and social sciences, and teaches workshops on tools and methods in digital scholarship, such as natural language processing. His research ranges from vulnerability in the context of theological anthropology to computational approaches to systematic and historical theological works, such as Karl Barth’s 
                Church Dogmatics
                . 
            
            Javier de la Rosa is a Research Engineer at the Center for Interdisciplinary Digital Research, a unit at the Stanford University Libraries focused on digital scholarship. He is an active member of the DH scholarly community at Stanford and regularly participates in conferences, professional organizations, and teaches workshops and tutorials to faculty and graduate students. He holds a Post-doctorate research fellowship and a PhD in Hispanic Studies at Western University, Ontario, where he also served as Tech Lead for the CulturePlex Lab. He completed both his MSc. in Artificial Intelligence and BSc. in Computer Engineering at University of Seville, Spain. His work and interests span from cultural network analysis and computer vision, to text mining and authorship attribution in the Spanish Golden Age of literature.
            Fernando Sancho is an Associate Professor at the Dept. of Computational Sciences and Artificial Intelligence at the University of Seville, and holds a PhD by the same university. He has worked in topics ranging from complex systems, and data analysis to cultural objects studies. He has regularly collaborated with the CulturePlex Lab at the University of Western Ontario, and the Complex Systems Modeling Group at University of Central Ecuador.
            Target Audience and Prereqs
            Post-docs, faculty, and advanced graduate students with Python prerequisites. Although the main concepts will be overviewed, knowledge of basic word embeddings and word2vec specifically would be desirable. In order to participate fully in all activities, participants must have working knowledge of basic programming concepts, the Python language, data structures, and the Numpy library. 
            
                Technical Support: Microphones and Projector
                Proposed Length: Half-day (4 hours; 4 sessions)
                Medium: Notebook (Jupyter)
                Libraries: Numpy, Pandas, Textacy, SpaCy, Gensim, scikit-learn, matplotlib
            
            Workshop Outline
            The workshop is split into four 50 min sessions with 10 minutes breaks in-between. We teach several methods in each unit with increasing difficulty. The schedule is broken down below:
            
                Understanding Word Vectors with Visualization
            
            This unit will give a brief introduction of word vectors and word embeddings. Concepts needed to understand the internal mechanics of how they work will also be explained, with the help of plots and visualizations that are commonly used when working with them.
            
                0:00 - 0:20 From word counts to ML-derived Word Vectors (SVD, PMI, etc.)
                0:20 - 0:35 Clustering, Vector Math, Vector Space Theory (Euclidean Distance, etc.)
                0:35 - 0:50 [Activity 1] Visualizations (Clustering, PCA, t-SNE) [We provide vectors]
            
            
                Word Vectors via Word2Vec
            
            This unit will focus on Word2Vec as an example of neural net-based approaches of vector encodings, starting with a conceptual overview of the algorithm itself and end with an activity to train participants’ own vectors.
            
                0:00 - 0:15 Conceptual explanation of Word2Vec 
                0:15 - 0:30 Word2Vec Visualization and Vectorial Features and Math
                0:30 - 0:50 [Activity 2] Word2Vec Construction [using Gensim] and Visualization(from part 1) [We provide corpus]
            
            
                Extended Vector Algorithms and Pre-trained Models
            
            This unit will explore the various flavors of word embeddings specifically tailored to sentences, word meaning, paragraph, or entire documents. We will give an overview of pre-trained embeddings including where they can be found and how to use them. 
            
                0:00 - 0:20 Overview of other 2Vecs &amp; other vector engineering: Paragraph2Vec, Sense2Vec, Doc2Vec, etc. 
                0:20 - 0:35 Pre-trained word embeddings (where to find them, which are good, configurations, trained corpus, etc.)
                0:35 - 0:50 [Activity 3] Choose, download, and use a pre-trained model 
            
            
                Role of Bias in Word Embeddings
            
            In this unit, we will explore an application and caveat of using word embeddings -- cultural bias. Presenting methods and results from recent articles, we will show how word embeddings can carry historical bias of the corpora trained on and lead an activity that shows these human-biases on vectors and how they can be mitigated.
            
                0:00 - 0:10 Algorithmic bias vs human bias 
                0:10 - 0:40 [Activity 4] Identifying bias in corpora (occupations, gender, ...) [GloVe] (Caliskan et al., 2017)
                0:40 - 0:50 Towards unbiased embeddings; Examine “debiased” embeddings 
                0:50 - 0:60 Conclusion remarks and debate
            
        
        
            
                
                    Bibliography
                    Caliskan, A., Bryson, J.J., Narayanan, A., 2017. Semantics derived automatically from language corpora contain human-like biases. 
                        Science 356, 183–186.
                         
                        https://doi.org/10.1126/science.aal4230
                    
                    Kocher, M., Savoy, J., 2017. Distributed language representation for authorship attribution. 
                        Digital Scholarship in the Humanities.
                         https://doi.org/10.1093/llc/fqx046
                    
                    Nanni, F., Dietz, L., Ponzetto, S.P., 2017. Toward a computational history of universities: Evaluating text mining methods for interdisciplinarity detection from PhD dissertation abstracts. 
                        Digital Scholarship in the Humanities.
                         https://doi.org/10.1093/llc/fqx062
                    
                
            
        
    
6262	2018	
        
            
                Short Description
                This tutorial provides a hands-on introduction to the use of deep learning techniques in the study of large image corpora. The TensorFlow and Keras libraries within the Python programming language are used to facilitate this analysis. No prior programming experience is required.
                Image analysis tasks covered in the tutorial include object detection, facial recognition, image similarity, and image clustering. We will make three open-access image corpora (historic photographs, still frames from moving images, and scanned works of art) available in order to test these methods. Alternatively, participants may bring and use an image dataset of interest to them. At the conclusion of the tutorial, participants will have created an interactive website running locally on their machines. This website will provide tools for analyzing their selected dataset. Additional instructions for making the website publicly available will be provided.
            
            
                Audience and Number of Participants
                This tutorial is aimed at scholars who work with visual materials who want to integrate DH methods into their analysis of image corpora. Our tutorial is based off of lectures notes used in a non-major, undergraduate-level course at the University of Richmond. It is accessible to participants with little to no programming background. However, as the tutorial will focus on the methods behind image processing rather than low-level coding, it will also be interesting and useful for experienced programmers new to image processing.
                Following the large number of participants at the AVinDH SIG sponsored Workshop in Montreal for DH20167 and our popular tutorial at DH2016 in Krakow, we expect the workshop participation to be equally popular with somewhere between 15 and 25 participants.
            
            
                Presenter Information
                
                    Taylor Arnold is Assistant Professor of Statistics at the University of Richmond. A recipient of grants from the NEH and ACLS, Arnold's research focuses on computational statistics, text analysis, image processing, and applications within the humanities. His first book 
                    Humanities Data in R (Springer, 2015) explores four core analytical areas applicable to data analysis in the humanities: networks, text, geospatial data, and images. His second book, the forthcoming 
                    A Computational Approach to Statistical Learning (CRC Press 2018), explores connections between modern machine learning techniques with theories of statistical estimation. Numerous journal articles extrapolate on these ideas in the context of particular applications. Arnold has also released several open-source libraries in R, Python, Javascript and C. Visiting appointments have included Invited Professor at Université Paris Diderot and Senior Scientist at AT&amp;T Labs.
                
                
                    Lauren Tilton is Assistant Professor of Digital Humanities in the Department of Rhetoric and Communications at the University of Richmond and a member of Richmond's Digital Scholarship Lab. Her current book project focuses on participatory media in the 1960s and 1970s. She is the Co-PI of the project 
                    Participatory Media, which interactively engages with and presents participatory community media from the 1960s and 1970s. She is also a director of 
                    Photogrammar, a web-based platform for organizing, searching and visualizing the 170,000 photographs from 1935 to 1945 created by the United States Farm Security Administration and Office of War Information (FSA-OWI). She is the co-author of 
                    Humanities Data in R (Springer, 2015). She is co-chair of the American Studies Association's Digital Humanities Caucus.
                
            
            
                Detailed Outline
                In this three hour tutorial we plan to spend the first 15 minutes getting all participants set up with the software and datasets required for the tutorial. The tutorial participants will we able to work on any reasonably recent version of Windows, macOS, or Linux. All of the software is free and open source. The remained of the workshop will consist of two 75-minutes sessions with a 15 minute break between them.
                Each of the two 75-minute sessions will consist of working collectively through “labs” formatted as IPython notebooks. Participants will have the option of using one of three pre-compiled datasets during the workshop depending on their interests:
                
                    historic photographs
                    still frames from moving images
                    scanned works of art
                
                Alternatively, tutorial participants may alternatively work with their own collection of images.
                The first session will focus on describing the potential difficulties of working with image data and explaining how deep learning can be used to address several of these challenges. Working at a conceptual level we will work through the following tasks:
                
                    how to structure a large collection of images as files on a computer
                    how to load images into Python as multidimensional arrays
                    the concepts behind applying neural networks to image data
                    code for projecting images into the penultimate layer of the YOLOv4 neural network
                    methods for visualizing the output projects from the neural networks
                
                The second session will focus on how the features detect in the first session can be used to annotate higher level features and measure the similarity between images. Specifically:
                
                    the application of image projections to image similarity metrics
                    the application of image projections to object detection
                    the application of image projections to face detection
                
                In the final 30 minutes, we will discuss how these techniques ultimately can be used to address humanities questions. This will culminate in running Python code that will output the constructed annotations as an interactive website running locally on each user's computer. This will open up further possibilities for extending the methods of the tutorial without the need for an extensive programming background.
            
            
                Special requirements
                The tutorial will require an overhead projector and internet access for all participants. We will forward information for downloading the required software ahead of time, however, to reduce the load on the internet on the day of the tutorial. Similarly, we will bring USB drives with the image corpora in case the internet connection speed makes manual downloading large files prohibitively slow.
            
        
        
            
                
                    Bibliography
                    
                        Arnold, T. and Tilton, C. (2015). 
                        Humanities Data in R. New York, NY: Springer.
                    
                    
                        Arnold, T., Kane, M., and Lewis, B. (2017). 
                        A Computational Approach to Statistical Learning. New York, NY: CRC Press.
                    
                
            
        
    
6268	2018	
        
            
                The tutors
                This one-day tutorial will be given by Marco Büchler, Greta Franzini, Mike Kestemont and Enrique Manjavacas.
                
                    Endorsement: This workshop is formally endorsed by the Special Interest Group on 
                    Digital Literary Stylistics (SIG-DLS).
                
                
                    Mike Kestemont (mike.kestemont@uantwerpen.be) is assistant research professor in the department of Literature at the University of Antwerp. He specializes in computational text analysis for the Digital Humanities, in particular stylometry and machine learning, topics on which he has given dozens of hands-on courses. Whereas his work has a strong focus on historical literature, his present research projects cover a wide range of topics in literary history, including classical, medieval, early modern and modernist texts. Mike currently takes a strong interest in representation learning via neural networks.
                
                
                    Marco Büchler (mbuechler@etrap.eu) is a computer scientist and leader of the 
                    Electronic Text Reuse Acquisition Project (eTRAP) research group at the University of Göttingen. Marco’s research interests concern the processing of natural languages with a specialization in the detection of historical text reuse. Furthermore, he is interested in the mining process and the systematization of changes of text reuse. He has worked in this field for over eight years. Together with his eTRAP team, in the past three years he has organized ten text reuse tutorials.
                
                
                    Greta Franzini (gfranzini@etrap.eu) is a Classicist and member of the 
                    Electronic Text Reuse Acquisition Project (eTRAP) research group at the University of Göttingen. Greta’s research interests concern the production of digital editions of texts as well as the combination of quantitative and qualitative methods to advance computational analyses and linguistic resources for Classical literature. Together with her team, Greta has already given eight text reuse tutorials.
                
                
                    Enrique Manjavacas (enrique.manjavacas@uantwerpen.be) is a PhD student at the University of Antwerp. He is associated with the Antwerp Centre for Digital Humanities and Literary Criticism. His current research focuses on sequential methods based on recurrent neural networks to develop semantically-infused models for Stylometry and text reuse detection. He is also interested in Natural Language Generation and has been involved in various projects around the concept of Synthetic Literature.
                
            
            
                
                    Description
                
                Computer-assisted text analysis is a core research area in the Digital Humanities. It embraces a wide variety of applications (stylometry, text reuse detection, topic modelling, etc.) and can assist researchers in complex tasks, particularly when it comes to processing large amounts of text. This tutorial brings together two popular and complementary text analysis tasks, stylometry (the quantitative study of writing style) and text reuse detection. While stylometry typically focuses on stylistic similarities between texts (i.e. 
                    how texts are written), text reuse studies are geared towards the reuse of elements across works (i.e. 
                    what texts are written about). As such, both methodologies tie into the theoretical notion of 
                    intertextuality (Orr 2003), albeit in complementary ways
                    .
                
                Creativity and individuality are important phenomena at stake in both fields: are writers at liberty to escape their own ‘stylome’ - or unique stylistic fingerprint - and to which extent can they free themselves from the many predecessors to which they are intertextually indebted? (Harold Bloom (1973) famously spoke of the ‘Anxiety of Influence’ in this respect) This leads to interesting theoretical tensions: if authors are stylistically close to one another, does that imply that we can also expect a more elevated level of text reuse between them (and vice versa)? Or can authors frequently reuse textual elements while developing an independent stylistic profile? To which extent is it theoretically possible to oppose style and content?
                In this workshop we offer a hands-on introduction to these topics using the case study of Rowling’s Harry Potter novels. The vast body of academic scholarship of these writings attests to the relevance of this series, including the highly mediatized stylometric study by Patrick Juola (2013) unmasking Rowling as “Robert Galbraith”, the pseudonym under which she temporarily managed to escape her own fame. Intertextuality is also a major concern of Rowling scholarship and scholars as Karin Westman (2007) have meticulously analyzed Rowling’s nuanced indebtedness to British authors such as Jane Austen. Rowling herself has invited much intertextual offspring by now too, not in the least in the form of so-called fanfiction (Milli &amp; Bamman 2016), the global phenomenon where (typically non-professional) writers read, reinterpret and expand literary universes (
                    fandoms) originally created by acclaimed authors in their own writings (or 
                    fanfics).
                
                The workshop’s tutorial will focus on offering scholars the practical tools and skills to begin to tackle such complex issues. For text reuse detection, participants will learn how to operate TRACER, a language-independent suite of state-of-the-art Natural Language Processing (NLP) algorithms aimed at discovering text reuse in both historical and modern texts, helping users to identify different types of text reuse ranging from verbatim quotations to paraphrase. For the stylometric analyses and visualizations, participants will mainly use custom scripts that exploit the numerous possibilities of the popular Python library 
                    scikit-learn for Machine Learning. Stylometry with R (Eder et al. 2016), a software package for text analysis in 
                    R, is another tool that will be used in the introductory sessions.
                
            
            
                Data
                Participants will practise with data provided by the organizers to better familiarize themselves with the software. The texts under analysis will be the seven English language Harry Potter novels by J. K. Rowling (the so-called core canon of the fandom), a large corpus selection of Harry Potter fanfiction (harvested from 
                    Archive of Our Own) as well as the Harry Potter movie subtitles. 
                
            
            
                Objectives
                The first objective of the tutorial is to introduce participants to two popular applications of text analysis that tie in closely with intertextuality studies, providing them with an understanding of some of the challenges, methods and strategies proper to this area of research. To this end, we use the illustrative Rowling case study to identify which proportion of the original novels and how much of their style the movies and fanfiction both retain. Additionally, the tutorial seeks to equip participants with the necessary knowledge to independently use the demonstrated software at home (and on their own corpora). Finally, it introduces visualization techniques to display results in an intuitive fashion, provoking new hermeneutic questions.
            
        
        
            
                
                    Bibliography
                    Bloom, H. (1973). 
                        The Anxiety of Influence: A Theory of Poetry. Oxford, New York: Oxford University Press.
                    
                    Eder, M., Rybicki, J., Kestemont, M. (2016). Stylometry with R: A Package for Computational Text Analysis. 
                        The R Journal, 8: 107–121.
                    
                    Juola, P. (2013). Rowling and “Galbraith”: an authorial analysis. 
                        Language Log. http://languagelog.ldc.upenn.edu/nll/?p=5315 (accessed 2 May 2018).
                    
                    Milli, S., Bamman, D. (2016). Beyond Canonical Texts: A Computational Analysis of Fanfiction. 
                        Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2048–2053. 
                        
                            https://doi.org/10.18653/v1/D16-1218
                        .
                    
                    Orr, M. (2003). 
                        Intertextuality: Debates and Contexts. Polity.
                    
                    Westman, K.E. (2007). Perspective, Memory, and Moral Authority: The Legacy of Jane Austen in J. K. Rowling’s Harry Potter. 
                        Children’s Literature, 35: 145–165. 
                        
                            https://doi.org/10.1353/chl.2007.0021
                        .
                    
                
            
        
    
6281	2018	
        
            
                Related Studies
                The idea that "quantitative dominance relations" represent an "important parameter for the central or peripheral position of a character" in a drama has been established by Pfister in his crucial structuralist monograph on the analysis of drama (Pfister 1997, p. 227). Digital-empirical studies after Pfister have tested different approaches to provide quantitative descriptions of the dramatis personae. Moretti's suggestion to tie the detection of the protagonist to the network analytical criterion of average distance (Moretti 2011, p. 4) was rejected as too simplistic (Trilcke 2013, p. 204), although this network-analytical approach was taken up by numerous studies. In this vein, Jannidis et al. (2016) not only calculated quantitative measures for the frequency of a character's appearance, but also the weighted degree to determine the accuracy of the identification of main characters. Moretti himself has adjusted his approach conceptually, insofar as he has shifted the focus from the 'protagonist' to a relationally defined concept of 'centrality'. He also emphasised the tension between different criteria (like 
                    word space and 
                    character space) not as a deficit, but as a productive factor of a multidimensional quantitative analysis of the dramatis personae (Moretti 2013, pp. 5–9). Moretti's basic ideas – the productive multidimensionality of quantitative analysis and the insight into the relational conceptualisation of quantitative character classification – have been taken up by Algee-Hewitt (2017), who worked with two network-analytical centrality measures (betweenness centrality and eigenvector centrality) and tried to examine the quantitative distribution of the cast of a play.
                
            
            
                Goal and Procedure
                We will conceptually discuss and complement available approaches to the quantitative description of characters in dramatic texts and test them on the basis of a corpus of 465 German-language dramas. The aim in theoretical and conceptual terms is to gain a better understanding of the dimensions of quantitative character analysis and to present diachronic and typological insights into quantitative dominance relations in German-language drama from 1730 to 1930. The subject of the analyses is the DLINA corpus (Fischer &amp; Trilcke 2015). The data is calculated using the Python tool "dramavis", which has been supplemented for this purpose with new analysis modules (Kittel &amp; Fischer 2017).
                In the first step, we examine the multidimensionality of quantitative descriptions as determined by Moretti (chapter 3). In a second step, we will take up Algee-Hewitt's (2017) proposed approach of working with quartiles and discuss it on the basis of the data from the DLINA corpus (chapter 4). In the third step, we present an approach that describes the quantitative distribution of characters in a play (section 5).
            
            
                The Correlation of Count-Based and Network-Based Rankings of Characters
                The above-mentioned approaches have made use of various measures for the quantitative description of dramatic characters, which can be divided in two groups: count-based measures, such as the number of words expressed by a character, and network-based measures, mostly centrality measures. According to Moretti 2013 (cf. also Jannidis et al. 2016), these two descriptive 'dimensions' can differ considerably.
                In order to systematically describe the extent of this deviation, we calculate eight values for each character of the 465 dramas of our corpus, three count-based measures (number of scenes a character appears in, number of speech acts, number of spoken words) and five network-related measures (degree, weighted degree, betweenness centrality, closeness centrality, eigenvector centrality). For each measurement a ranking is created. The rankings are then merged into two meta-rankings: one count-based and one network-based. The two meta-rankings are then combined into an overall ranking.
                To determine the deviation between the two meta-rankings, we calculate the ranking correlation coefficient Spearmans Rho and check how strongly the two meta-rankings correlate with each other for all dramas of our corpus (fig. 1).
                
                    
                        
                        Spearmans Rho for the correlation of count-based and network-based measures.
                    
                
                Complete congruence of the meta-rankings is an exception. In fact, the different measures capture different 'dimensions' of the quantitative character hierarchy. In order to better understand these dimensions, five dramas (marked by the green dots in fig. 1) are examined in more detail and discussed in this paper (see figs. 2, 4, 6, 8, 10 for rankings, figs. 3, 5, 7, 9, 11 for the network graphs of these dramas).
                
                    
                        
                        Rankings for Lessing's "Emilia Galotti" (1772) – Spearmans Rho: 0,917.
                    
                
                
                    
                        
                        Network graph for Lessing's "Emilia Galotti" (1772).
                    
                
                
                    
                        
                        Rankings for Iffland's "Das Erbtheil des Vaters" (1802) – Spearmans Rho: 0.806.
                    
                
                
                    
                        
                        Network graph for Iffland's "Das Erbtheil des Vaters" (1802).
                    
                
                
                    
                        
                        Rankings for Schiller's "Die Jungfrau von Orleans" (1801) – Spearmans Rho: 0.672.
                    
                
                
                    
                        
                        Network graph for Schiller's "Die Jungfrau von Orleans" (1801).
                    
                
                
                    
                        
                        Rankings for Anzengruber's "Der Meineidbauer" (1871) – Spearmans Rho: 0.442.
                    
                
                
                    
                        
                        Network graph for Anzengruber's "Der Meineidbauer" (1871).
                    
                
                
                    
                        
                        Rankings for Wedekind's "Franziska" (1912) – Spearmans Rho: -0.222.
                    
                
                
                    
                        
                        Network graph for Wedekind's "Franziska" (1912).
                    
                
                As it turns out, the deviations usually affect characters at the bottom of the hierarchy. Here the network-related measures are particularly sensitive to types of clustering in secondary scenes of a drama (figs. 8 and 9), which has even more severe effects if a drama is quantitatively dominated by very few characters (figs. 10 and 11). On the other hand, both meta-rankings are very similar for the quantitatively dominant characters (top 1 and top 1 or 2). These observations can serve as an argument that the multidimensional description is less relevant to discuss protagonists, but rather for the characterisation of quantitative dominance relations within the cast as a whole.
            
            
                Percentage of Quantitative Dominant Characters
                Algee-Hewitt 2017 made a suggestion for the characterisation of the quantitative distribution of a cast, albeit with a continuing focus on quantitatively dominant characters. Working with an English-language drama corpus of several thousands of plays, he calculated the eigenvector centrality of the characters and then calculated the percentage of characters located in the upper quartile of the distribution. We have reproduced this test with our corpus – for all measures mentioned above. As an example, the box plots show the values for the eigenvector centrality (fig. 12) as well as for the count-based measure 'number of words' (fig. 13).
                
                    
                        
                        Percentage of characters in the upper quartile according to their eigenvector centrality, grouped by decades. Blue line: median.
                    
                
                
                    
                        
                        Percentage of upper quartile characters according to number of words, grouped by decades. Blue line: median.
                    
                
                It is interesting to note that the values we calculated for eigenvector centrality (fig. 12) are significantly higher than the values presented by Algee-Hewitt (for the period of time covered by our corpus the median is lower than 0.15). A comparison of fig. 12 and fig. 13 shows that the network-related measures usually locate a larger percentage of characters in the upper quartile. From a network-analytical point of view, a drama tends to be dominated by several characters. The two curves also tend to follow the same course, especially regarding the big flattening curve in the decades after 1740. What we can see there is the reduction of the percentage of quantitatively dominant characters ('main characters') and the emergence of quantitatively less dominant characters ('secondary characters', 'atmospheric characters').
            
            
                More Detailed Distribution Analysis
                With a focus on quantitatively dominant 'main characters', Algee-Hewitt's approach attempts to describe types of distribution of the dramatis personae and thus to identify 'dominance relations' via distribution analyses. Following analyses of the 'small world' phenomenon in drama (Trilcke et al. 2016), we propose to extend this approach and develop a typology of quantitative distribution of characters in dramatic texts. To this end, we take the above-mentioned eight quantitative measures, calculate the distribution of character values across deciles and subject this distribution to a regression analysis (tests on linear, exponential, quadratic and power-law distribution; typologisation according to the regression line with the highest coefficient of determination). Examples in figs. 14 to 19 show the results for three of the dramas discussed above, for a network-related measure (weighted degree) and a count-based measure (number of words).
                
                    
                        
                        Lessing's "Emilia Galotti" (1772) – decile distribution of weighted degree, y-axis: number of characters.
                    
                    
                        
                        Lessing's "Emilia Galotti" (1772) – decile distribution of number of words, y-axis: number of characters.
                    
                
                
                    
                        
                        Anzengruber's "Der Meineidbauer" (1871) – decile distribution of weighted degree, y-axis: number of characters.
                    
                    
                        
                        Anzengruber's "Der Meineidbauer" (1871) – decile distribution of number of words, y-axis: number of characters.
                    
                
                
                    
                        
                        Schiller's "Die Jungfrau von Orleans" (1801) – decile distribution of weighted degree, y-axis: number of characters.
                    
                    
                        
                        Schiller's "Die Jungfrau von Orleans" (1801) – decile distribution of number of words, y-axis: number of characters.
                    
                
                These typologies are calculated for all eight values and for all 465 dramas – we will present the results for the corpus as a whole at the conference. With our approach, a more precise, multidimensional description of typical quantitative dominance relations in drama will be possible. The increase in the number of less dominant characters observed on the basis of our quartile analysis (figs. 12–13) will be described with more precision and supplemented by a more differentiated examination of types of 'middle characters'.
            
            
                Summary
                This talk brings together several approaches for the quantitative analysis of characters in literary texts, discusses the potential of a multidimensional description beyond top characters (protagonists) and suggests an approach for typologising quantitative dominance relations within the cast of a drama.
            
        
        
            
                
                    Bibliography
                    
                        Algee-Hewitt, Mark (2017): "Distributed Character: Quantitative Models of the English Stage, 1500–1920". 
                        Digital Humanities 2017. Conference Abstracts. McGill University &amp; Université de Montréal, 119–121. URL: &lt;
                        &gt;.
                    
                    
                        Fischer, Frank / Trilcke, Peer (2015): "Introducing dlina Corpus 15.07 (Codename: Sydney)". 
                        dlina blog, 20 June 2015. URL: &lt;
                        &gt;.
                    
                    
                        Kittel, Christopher / Fischer, Frank (2017): "dramavis (v0.4)". 
                        GitHub, September 2017. URL: &lt;
                        &gt;.
                    
                    
                        Jannidis, Fotis / Reger, Isabella / Krug, Markus / Weimer, Lukas / Macharowsky, Luisa / Puppe, Frank (2016): "Comparison of Methods for the Identification of Main Characters in German Novels". 
                        Digital Humanities 2016: Conference Abstracts. Jagiellonian University &amp; Pedagogical University, Kraków, 578–582.
                    
                    
                        Moretti, Franco (2011): 
                        Network Theory, Plot Analysis (= Literary Lab Pamphlet 2), May 2011. URL: &lt;
                        &gt;.
                    
                    
                        Moretti, Franco (2013): 
                        "Operationalizing": or, the function of measurement in modern literary theory (= Literary Lab Pamphlet 6), December 2013. URL: &lt;
                        &gt;.
                    
                    
                        Pfister, Manfred (1997): 
                        Das Drama. Theorie und Analyse. 9th ed. Munich: Fink.
                    
                    
                        Trilcke, Peer (2013): "Social Network Analysis (SNA) als Methode einer textempirischen Literaturwissenschaft". Ajouri, Philip / Mellmann, Katja / Rauen, Christoph (eds.): 
                        Empirie in der Literaturwissenschaft. Münster: mentis, 201–247.
                    
                    
                        Trilcke, Peer / Fischer, Frank / Göbel, Mathias / Kampkaspar, Dario (2016): "Theatre Plays as 'Small Worlds'? Network Data on the History and Typology of German Drama, 1730–1930". 
                        Digital Humanities 2016. Conference Abstracts. Jagiellonian University &amp; Pedagogical University, Kraków, 385–387.
                    
                
            
        
    
6292	2018	
        
            
                Introduction
                Traditional full-text digital libraries, including those in the field of pre-modern Chinese, have typically followed top-down, centralized, and static models of content creation and curation. In this type of model, written materials are scanned, transcribed by manual effort and/or Optical Character Recognition (OCR), then corrected manually, reviewed, annotated, and finally imported into a system in their final, usable form. This is a natural and well-grounded strategy for design and implementation of such systems, with strong roots in traditional academic publishing models, and offering greatly reduced technical complexity over alternative approaches. This strategy, however, is unable to adequately meet the challenges of increasingly large-scale digitization and the resulting rapid growth in potential corpus size as ever larger volumes of historical materials are digitized by libraries around the world.
                The Chinese Text Project (https://ctext.org) is a full-text digital library of pre-modern Chinese written materials which implements an alternative model for creation and curation of full-text materials, adapting methodologies from crowdsourcing projects such as Wikipedia and Distributed Proofreaders (Newby and Franks 2003) while also integrating them with full-text database functionality. In contrast to the traditional linear approach, in which all stages of processing including correction and review must be completed before transcribed material is ingested into a database system, this approach works by immediately ingesting unreviewed materials into a publicly available, managed system, within which these materials can be navigated and used, as well as improved through an ongoing collaborative correction and annotation process. From a user perspective, this has the consequence that the utility of the system does not rest upon prior expert review of materials, but instead derives from provision to individual users of the ability to interact directly and effectively with primary source materials and verify accuracy of transcription and annotation for themselves. Combined with specialized Optical Character Recognition techniques leveraging features common to pre-modern Chinese written works (Sturgeon 2017a), this has enabled the creation of a scalable system providing access to a long tail of historical works which would otherwise not be available in transcribed form. The system is highly scalable and currently contains over 25 million pages of primary source material while being used by over 25,000 users around the world every day.
            
            
                Creating transcriptions
                The most fundamental type of material contained in the Chinese Text Project consists of digital facsimiles of pre-modern published works. These are typically ingested in bulk through collaboration with university libraries which have created high quality digital images of works in their collections. After ingestion, the next step in making these materials more useful to users is creation of approximate transcriptions from page images using OCR. Producing accurate OCR results for historical materials is challenging due to a number of issues, including variation in handwriting and printing styles, varying degrees of contrast between text and paper, bleed-through from reverse sheets, complex and unusual layouts, and physical, water or insect damage to the materials themselves prior to digitization. In addition to these challenges which are common to OCR of historical documents generally, OCR for premodern Chinese works faces additional difficulties in extracting training data due to the large number of distinct character types in the Chinese language. Most OCR techniques apply machine learning to infer from an image of a character which character type it is that the image represents, and these techniques require comprehensive training data in the form of clear and correctly labeled images in the same writing style for every possible character. This is challenging for Chinese due to the large number of character types needed for useful OCR (on the order of 5000); unlike historical OCR of writing systems with much smaller character sets, it is not feasible to simply create this data manually. Instead, training data is extracted through an automated procedure (Sturgeon 2017a) which leverages knowledge about existing transcriptions of other texts to assemble clean labeled character images extracted from historical works for every character to be recognized (Figure 1). Together with image processing and language modeling tailored to pre-modern Chinese, this significantly reduces the error rate in comparison with off-the-shelf OCR software.
                
                    
                
                Figure 1. OCR training data is extracted automatically from handwritten and block-printed primary source texts.
            
            
                Navigating texts and page images
                Once transcriptions of page images have been created, they are directly imported into the public database system. The system represents textual transcriptions as sequences of XML fragments, in which markup is used to express both the relationship between transcribed text and the page image to which it corresponds, as well as the logical structure of the document as a whole. This facilitates two distinct methods of interacting with the transcribed material: firstly, as a single document consisting of the transcribed contents of each page concatenated in sequence to give readable plain-text with logical structure (divisions into chapters, sections, and paragraphs); secondly, as a sequence of page-wise transcriptions, in which a direct visual comparison can be made between the transcription and the image from which it is derived (Figure 2). In both cases, an important contribution of the transcription is that it enables full-text search; the primary utility of the page-wise view is that it enables efficient comparison of transcribed material with the facsimile of the primary source itself. As these two views are linked to one another and created from the same underlying data, this makes it feasible to read and navigate a text according to its logical structure, and at any stage of the process jump to the corresponding location in the sequence of page images to confirm accuracy of the transcription.
                
                    
                    
                
                
                    Figure 2. Full-text search results can be displayed in context in a logical transcription view (left), as well as aligned directly together with the source image in an image and transcription view (right).
                
            
            
                Crowdsourced editing and curation
                As initial transcriptions are created using OCR, they inevitably contain mistakes. Users of the system have the option to correct mistakes they identify, as well as to annotate texts in a number of ways. Two distinct editing interfaces are provided: a direct editor, which enables direct editing of the underlying XML representation, and a visual editor allowing simplified editing of page-level transcriptions, which edits the same underlying content but does not require direct understanding or modification of XML. Regardless of which mechanism is used to submit an edit, all edits are committed immediately to the public system. Edits are versioned, allowing visualization of changes between versions and simple reversion of a text to its state at an earlier point in time. At present, the system receives on the order of 100 edits each day, representing much larger numbers of corrections, as editors frequently choose to correct multiple errors and sometimes entire pages in a single operation.
                Further visual editing tools supplement these mechanisms to enable crowdsourcing of more complex information. Illustrations are entered by the user drawing a rectangular box on the page image to indicate the location of the illustration, then filling in a simple form describing various aspects of it (Figure 3). This results in an XML fragment describing the illustration, which can simply be inserted into the text at the appropriate location to represent it. This allows the illustration to be extracted from its context on the page and represented in the full-text transcription view as well as in the page-wise view. It also facilitates illustration search functionality, where illustrations can be searched by caption across all materials contained in the system (Figure 4). A similar visual editing interface is used to enable the inputting of rare and variant characters which do not yet exist in Unicode. These characters are no longer in common use, but occur in many historical documents. The visual editing interface for rare character input also uses metadata provided by the user to identify whether a given character is the same as any existing character known to the system, and if so, assigns a common identifier so that data about these characters can be aggregated, and text containing such characters searched.
                
                    
                
                Figure 3. Identification and markup of illustrations within source materials are crowdsourced using purpose-designed visual editing tools which convert user input into XML.
                
                    
                
                
                    Figure 4. Image search: individual images are extracted from (and linked to) the precise locations at which they occur in source materials, and can be searched by caption.
                
            
            
                Exporting data and integrating with external systems
                In addition to the main user interface, a web-based Application Programming Interface (API) provides machine-readable access to data and metadata stored in the system. This facilitates text mining applications, as well as integration with other online systems. An example of the latter is the MARKUS textual markup system (De Weerdt et al. 2016), which can use the API to search for texts and load their transcriptions directly into this externally developed and maintained tool. An XML-based plugin system for the Chinese Text Project user interface also enables users to define and share extensions to the web interface which can be used to create connections to external projects and resources. This allows third-party tools such as MARKUS to integrate directly into the web interface, facilitating seamless connections between separately developed online projects. Text mining access is further facilitated by the provision of a Python module capable of accessing the API (Sturgeon 2017c), which is already in use in teaching and research (Sturgeon 2017b).
            
        
        
            
                
                    Bibliography
                    
                        Newby, G. B. and Franks, C. (2003). Distributed proofreading. 
                        2003 Joint Conference on Digital Libraries, 2003. Proceedings. pp. 361–63 doi:
                        10.1109/JCDL.2003.1204888.
                    
                    
                        Sturgeon, D. (2017a). Unsupervised Extraction of Training Data for Pre-Modern Chinese OCR.
                         Florida Artificial Intelligence Research Society. 
                        Proceedings.
                    
                    
                        Sturgeon, D. (2017b). Classical Chinese DH: Getting Started. 
                        Digital Sinology
                        https://digitalsinology.org/classical-chinese-dh-getting-started/ (accessed 27 November 2017).
                    
                    
                        Sturgeon, D. (2017c). Chinese Text Project API wrapper 
                        https://pypi.python.org/pypi/ctext/ (accessed 27 November 2017).
                    
                    
                        Sturgeon, D. (2017d). Unsupervised identification of text reuse in early Chinese literature. 
                        Digital Scholarship in the Humanities doi:
                        10.1093/llc/fqx024.
                        https://academic.oup.com/dsh/advance-article/doi/10.1093/llc/fqx024/4583485 (accessed 27 April 2018).
                    
                    
                        Weerdt, H. D., Ming-Kin, C. and Hou-Ieong, H. (2016). Chinese Empires in Comparative Perspective: A Digital Approach. 
                        Verge: Studies in Global Asias, 
                        2(2): 58–69 doi:
                        10.5749/vergstudglobasia.2.2.0058.
                    
                
            
        
    
6295	2018	
        
            Science fiction has been theorized as a laboratory in which text serves as the medium for experimentation with perspective and epistemology.
                
                     Jones, Gwyneth, 
                        Deconstructing Starships: Science, Fiction and Reality (Liverpool UP, 1999) 4.
                        
                    
                 Yet scientific methods are more practicably applicable to the systematic efforts of textual scholars. Computationally assisted collation demands continual refinements to verify the accuracy of textual data and metadata and challenges a singular view of any documentary edition. Moreover, collation can test hypotheses about change over time, and the output of machine collation can serve as an experiment to identify, quantify, survey, and analyze the data of textual change. Digital collation of science fiction seems to combine the practical with the theoretical in its lab space.
            
            An early form of modern science fiction, the 19th-century novel 
                Frankenstein has itself been the subject of digital variorum experiment since the mid-1990s production of the Pennsylvania Electronic Edition (PAEE) by Stuart Curran and Jack Lynch, a daring effort to prioritize the critical apparatus, pulling it from the obscurity of small type at the bottom of printed pages to make it front and center on screen displays.
                
                     See a representative page at 
                        
                            http://knarf.english.upenn.edu/Colv1/f1101.html
                        . Curran, Stuart and Jack Lynch. 
                        Frankenstein; or, the Modern Prometheus. The Pennsylvania Electronic Edition. Est. 1994. 
                        
                            http://knarf.english.upenn.edu/
                        .
                        
                    
                 The PAEE challenges us to find new ways to tell the variorum narrative of change over time. Much like Victor Frankenstein's composition of the Creature from multiple bodies, the effort to aggregate the distinct versions of this novel into a variorum might succeed in communicating a multi-dimensional narrative of its own composition and decomposition, inviting the reader to evaluate its successive stages just as the reader is invited to evaluate the three storytellers within the novel. 
            
            In the history of preparing digital texts with markup languages, whether in early HTML, SGML, or XML, markup standards tensed between two poles: a) the acknowledgement of a coexistence of multiple hierarchical structures and b) the need to prioritize a single document hierarchy in the interests of machine-readability, while permitting signposts of overlapping or conflicting hierarchies as of secondary importance.
                
                     See for example, P. M. W. Robinson, "
                        The Collation and Textual Criticism of Icelandic Manuscripts" 
                        Literary and Linguistic Computing
                        , Volume 4, Issue 2, 1 January 1989, 99–105, 
                        
                            https://doi.org/10.1093/llc/4.2.99
                        
                        ; Dekker, Ronald Haentjens, Dirk van Hulle, Gregor Middell, Vincent Neyt, and Joris van Zundert, "Computer-supported collation of modern manuscripts: CollateX and the Beckett Digital Manuscript Project"
                    
                    
                        Volume 30, Issue 3, 1 September 2015, 452–470, 
                        
                            https://doi.org/10.1093/llc/fqu007; 
                        
                        Eggert, Paul, "
                        
                            The reader-oriented scholarly edition" 
                        
                        
                            Digital Scholarship in the Humanities
                        
                        
                            , Volume 31, Issue 4, 1 December 2016, 797–810, 
                        
                        
                            https://doi.org/10.1093/llc/fqw043; 
                        
                        and Holmes, Martin, "
                        
                            Whatever happened to interchange?" 
                        
                        
                            Digital Scholarship in the Humanities
                        
                        
                            , Volume 32, Issue suppl_1, 1 April 2017, 163–168, 
                        
                        
                            https://doi.org/10.1093/llc/fqw048
                        .
                        
                    
                 In this paper we present a view of texts that emerges from the experience of comparing documents encoded in conflicting ways. Like the makers of 
                
                    the genetic 
                
                
                    Faust
                
                
                     edition
                , we find that multiple encoding structures must co-exist and correlate to achieve a meaningful comparison of editions.
                
                     See Gerrit Brüning, Katrin Henzel, and Dietmar Pravida, "Multiple Encoding in Genetic Editions: The Case of 'Faust'", Journal of the Text Encoding Initiative (4: March 2013)
                        . 
                    
                 Further, hierarchies need to be reconceived in dynamic terms—where are their flex points for conversion from containment structures to 
                loci of intersection? In the process of collation, hierarchies must be dismantled and flattened in order for meaningful multiplicity to be represented, and in order for us to understand a dialogic relationship among textual variants. To study variation over time vexes the organizing principle of any singular hierarchy, but hierarchy in the context of collation may nevertheless build a robust architecture that 
                bridges distinct encodings rather than isolating them. In this architecture, arches and connecting spans are more viable than monoliths.
                
                     An inspiration for the bridging concept are the visualizations in 
                        Haentjens Dekker, Ronald, and David J. Birnbaum, “It's more than just overlap: Text As Graph,” Balisage: The Markup Conference 2017, Washington, DC, August 1 - 4, 2017; in 
                        Proceedings of Balisage: The Markup Conference 2017
                        : 
                        
                            https://www.balisage.net/Proceedings/vol19/html/Dekker01/BalisageVol19-Dekker01.html#d11284e1180
                         . The authors conceptualize an ideal model of texts in a graph structure organized primarily by their semantic sequencing and in which structural features are a matter of descriptive annotation rather than elemental hierarchy. 
                    
                
            
            This paper addresses the serious issues of collating digital editions made at different times by different editors, and it discusses the bicentennial 
                Frankenstein variorum project as a challenging, illustrative case in point. We are preparing a variorum edition of Frankenstein in TEI P5 based on the 1818 and 1831 
                Frankenstein digital texts due to be released in 2018 in celebration of the bicentennial of 
                Frankenstein's first publication. Our collation source documents are adapted from the 1990s encoding of the PAEE (for the 1818 and 1831 editions), and the Shelley-Godwin Archive's diplomatic edition of the manuscript notebooks.
                
                     The Shelley-Godwin Archive's edition of the manuscript notebooks of 
                        Frankenstein builds on decades of intensive scholarly research to create TEI diplomatic encoding: 
                        
                            http://shelleygodwinarchive.org/contents/frankenstein/
                        . 
                    
                 We are also newly incorporating a little-known edition of 1823 produced from corrected OCR. Our collation should yield a meta-narrative of how 
                Frankenstein changed over time in four versions that passed through multiple editorial hands. It is widely understood that the 1831 edition diverges sharply from the first print edition of 1818, adding new material and changing the relationships of characters. Less known is how the notebook material compares with the print editions, and how much we can identify of the 
                persistence of various hands involved in composing, amending, and substantially revising the novel over the three editions. For example, to build on Charlie Robinson's identification of Percy Bysshe Shelley's hand in the notebooks,
                
                     See Charlie Robinson's Introduction to the Frankenstein Notebooks (Garland 1996), reproduced here: 
                        
                            http://shelleygodwinarchive.org/contents/frankenstein/the-frankenstein-notebooks-introduction/
                        .
                        
                    
                 our collation can reveal how much of Percy's insertions and deletions survive in the later print editions. Our work should permit us to survey when and how the major changes of the 1831 text (for example, to Victor Frankenstein's family members and the compression and reduction of a chapter in part I) occurred. We preserve information about hands, insertions, and deletions in the output collation, to serve as the basis for better quantifying, characterizing, and surveying the contexts of collaboration and revision in textual scholarship. 
            
            The three print editions and extant material from three manuscripts are compared in parallel, to indicate the presence of variants in the other texts and to be able to highlight them based on intensity of variance, to be displayed like the highlighted passages in each visible edition of 
                The Origin of Species in Darwin Online.
                
                     Barbara Bordalejo, ed. 
                        Darwin Online. See for example the illumination of variant passages in 
                        The Origin of Species here: 
                        
                            http://darwin-online.org.uk/Variorum/1859/1859-1-dns.html
                        
                        
                    
                 Rather than any edition serving as the lemma or grounds for collation comparison, we hold the collation information in stand-off markup, in its own XML hierarchy. That XML "bridge" expresses relationships among the distinct encodings of diplomatic manuscript markup in which the highest level of hierarchy is a unit leaf of the notebook, with the structural encoding of print editions organized in chapters, letters, and volumes. While the apparently nested structure of these divisions might seem the most meaningful way to model 
                Frankenstein, these pose a challenge to textual scholarship in their own right. As Wendell Piez has discussed, 
                Frankenstein's overlapping hierarchies of framing letters and chapters have led to inconsistencies in the novel's print production. Piez deploys a non-hierarchical encoding of the novel on which he constructs an SVG modeling (in ordered XML syntax) of the overlap itself.
                
                     These hierarchical issues provided an application for Piez’s invented LMNL "sawtooth" syntax to highlight overlap as semantically important to the novel; see Wendell Piez, "Hierarchies within range space: From LMNL to OHCO" 
                        Balisage: The Markup Conference Proceedings (2014): 
                        
                            https://www.balisage.net/Proceedings/vol13/html/Piez01/BalisageVol13-Piez01.html
                        
                        
                    
                 Our work with collation depends on a similar interdependency of structurally inconsistent encoding. 
            
            Our method involves three stages of structural transformation, each of which disrupts the hierarchies of its source documents: 
            
                Preparing texts for collation with CollateX
                    
                         CollateX software applies a graph-based model of text to locate variants in documents. See 
                            
                                https://collatex.net/doc/
                            
                        
                    , 
                
                Collating a new "braided" structure in CollateX XML output, which positions each variant in its own reading witness. 
                Transforming the collation output to survey the extents and kinds of variation, and to build a digital variorum edition. 
            
            In the first stage, we adapt the original code from the Shelley-Godwin Archive and from the PA-EE to create new forms of XML to carry predictable markers to assist in alignment. These new, pre-collation editions are resequenced (as when we move marginal annotations from the end of the XML document into their marked places 
                as they would be read in the manuscript notebook). They are also differently "chunked" than their source texts, resizing the unit file so that each represents an equivalent portion small enough to collate meaningfully and large enough that each document demonstrably aligns with the others at its start and end points. 
            
            Stage two weaves these surrogate editions together and transfers information from tags that we want to preserve for the variorum. Interacting with the angle brackets as patterned strings with Python, we mask several elements from the diplomatic code of the ms notebooks so that they are not processed in terms of comparison but are nevertheless output to preserve their distinct information. In CollateX's informationally-rich XML output, these tags render as flattened text with character entities replacing angle brackets so as not to introduce overlap problems with its critical apparatus. In Stage three, we work delicately with strings that represent flattened composite of preserved tag information and representations of the text, using XSLT string-manipulation functions to construct new files for analysis. We can then study, for example, where the strings associated with Percy Shelley are repeated in the later editions, and how many were preserved by 1831. We also build a scaffolding in stand-off markup for the digital variorum that bridges multiple editions, as modelled in Figure 1. 
            
                
            
            
                Fig. 1 An example variant with two different readings, showing Percy Bysshe Shelley's hand in the ms notebook. While the print editions of 1818, 1823, and the manuscript agree (yellow reading), the print edition of 1831 introduces new text (blue reading). The pointers are expressed according to the TEI XPointer Schemes defined in Chapter 16 of the TEI Guidelines and are subject to change.
            
            This example shows how the stand-off collation identifies variant readings between texts by grouping pointers as opposed to grouping strings of text according to the parallel segmentation technique described in Chapter 12 of the TEI Guidelines.
                
                     See especially the TEI P5 Guidelines, 12.2.3 and 12.2.4: 
                        
                            http://www.tei-c.org/release/doc/tei-p5-doc/en/html/TC.html#TCAPPS
                        
                        
                    
                 The TEI offers a stand-off method for encoding variants, called “double-end-point-attachment”, in which variants can be encoded separately from the base text by specifying the start and end point of the lemma of which they are a variant. This allows encoders to refer to overlapping areas on the base text, but despite its flexibility, this method still requires choosing a base text to which anchor variant readings. While choosing a lemma for each variant may be necessary for a critical edition, it is not ideal for a variorum edition that, by design, does not choose a base text.
                
                     For a related example, see Viglianti, R. Music and Words: reconciling libretto and score editions in the digital medium. 
                        “Ei, dem alten Herrn zoll’ ich Achtung gern’”, ed. Kristina Richts and Peter Stadler, 2016, 727-755. 
                    
                 Our approach, therefore, simply identifies variance and groups readings from multiple sources without conflating them into one document and with accommodation of multiple hierarchies. 
            
            Though we think of XML as a stable sustainable archiving medium, the repeated collapsing and expansion of hierarchies in our collation process makes us consider that for the viability of digital textual scholarship, ordered hierarchies of content objects might best be designed with leveling in mind, and that building with XML may be optimized when it is open to transformation. Preparing diversely encoded documents for collation challenges us to consider inconsistent and overlapping hierarchies as a tractable matter for computational alignment
                —where alignment becomes an organizing principle that fractures hierarchies, chunking if not atomizing them at the level of the smallest meaningfully sharable semantic features. 
            
        
    
6297	2018	
        
            My research is multi- and interdisciplinary focusing on electronic literature and cybercultures in/of Latin America. My latest articles and book manuscript explore the divide and convergence in literature and technology. This project lends itself well to the application of those theories and the evaluation of how they can best be implemented in classroom practices and complemented with co-curricular modules. I will therefore present my research findings on the use of Digital Humanities components specifically for the teaching of Latin American Studies. The presentation would thus serve as a report of: 1) initial research findings and best practices found at other institutions; 2) work accomplished at the DHSI 2018 Workshop (Victoria, Canada) “Critical Pedagogy and Digital Praxis in the Humanities”; 3) feedback gained from presentation at the DHSI 2018 Conference &amp; Colloquium; and, 4) samples of syllabi to foster a lively discussion on the application of such a course with co-curricular components for Latin American Studies programs.
            The goal of this project is to do a detailed study of program and curriculum design at other institutions on the use of DH modules specifically for Latin America/US Latino culture with a focus on pedagogical methodologies that engage critically about the problems that DH platforms do and do not resolve in Latin American Studies. The course design and the co-curricular components complement and intersect each other. This project will facilitate the assessment of various curriculums and specialized courses for the digital humanities and would ultimately lead to develop a course for all students interested in DH Latin American Studies.
                
                     Students in this course would include (but not limited to) those in the Latin American Studies Minor Program, International Business, International Studies (BAIS and GPIS), Humanities, Political Science, Spanish majors and Minors, World Cultural Studies majors and minors.
                 Course components will include developing language proficiency, learning and using DH tools, and analyzing the effectiveness and drawbacks of such technologies specifically to Latin American Studies.
            
            The interactive, systematic, and innovative features of digital humanities have been proven to advance language learning both in and outside of the classroom. Through exploring different forms of digital humanities, including multi-media, online archives, as well as existing web tools like Google Earth and Twitter, instructors and scholars of foreign languages not only facilitate collective and immersive language learning, but also broaden and deepen students’ exposure and knowledge of foreign culture. These projects break the traditional geographical and cultural boundaries in learning a foreign culture and/or language. Therefore, it is essential for instructors to reflect on how best to incorporate digital humanities in language/culture learning, and to determine to what extent digital learning complements and even replaces traditional ways of teaching and learning. 
            Students will be encouraged to adapt these new tools of analysis to their own future career objectives. The field of Digital Humanities is collaborative and very interdisciplinary as it produces new scales of analysis with varying modules (texts, maps, audio-mapping and networks) which may include experiments across modalities with: distant reading alongside close reading techniques, programming language, audio creation, geotagging, speech recognition encoding documents in TEI (Text Encoding Initiative
                
                     Text Encoding Initiative Markup Language at the University of Virginia, 
                        https://dh.virginia.edu/tool/text-encoding-initiative-markup-language-tei (for my future reference)
                    
                ), learning the basics of computational text analysis, programming chatbots using the Python programming language, etc. The course will also note the drawbacks or pitfalls of the use of technology.
            
            However, the skills needed in DH have less to do with a particular hardware or commercial software and more about engaging in digital literacy (train interpretative methods necessary for critical analysis), and showcase how digital humanities is valuable to better understand Latin America’s transformations in the production, circulation and reception as well as its impact on culture, politics, history, literature, music, etc. The course will encourage students to develop more analytical projects from the use of such modalities. The focus will also be to analyze and address 
                why this method of learning is complementary or even superior to traditional methods, specifically addressing the impact and implications that technology involved on ideologies, ethics and ideas. For example, a more involved topic would approach the idea of “mapping” as interpretation of geospatial data in GIS, georectify historical maps in 
                Map Warper, manage digital archival objects in 
                Omeka, and use 
                Neatline to build “deep maps” of particular neighborhoods or landmarks in a city, layering historical photographs, maps, geospatial data, literary texts, and other elements to build analysis about their city. 
            
            Additionally, the course will attempt to link to public libraries (Slover in Norfolk), museums (Chrysler, Mariners, Living Museum), research centers, community groups (Norfolk Chamber of Commerce, Hispanic Chamber of Commerce, Hispanic Community Dialogue) or other campus-level initiatives (ODU’s Institute of Humanities “Mapping Lambert’s Point Project,” for instance). The goal is to build projects that make use of the University and community’s collections. These public projects can energize students to work that much harder, as they can create materials with a chance of life beyond the classroom itself. The course will draw on resources from, participate in and continue their learning with the Regional, National and International Network
                
                     To be featured in the Latin American Studies Program website
                 aimed to promote digital humanities initiatives to Old Dominion University faculty and to learn from and collaborate with external groups.
                
                     I already have established contacts and am in current collaborations with: Centro de Cultura Electrónica in Mexico City; the project Cultura Digital Chile (Universidad Diego Portales, Chile); the Latin American and Digital Humanities/Cybercultures at University of Georgia; the Digital Latin American Cultures Network: Researching the Cultural Dimensions of New Media in the United Kingdom; I am also a board member of the organization Lit-e-Lat: Red de Literatura Electrónica.
                 This network would be dedicated to exploring, analyzing, and sharing the cultural and visual modalities of digital humanities in the research and teaching of Latin America. The network would engage in these discussions through symposia for faculty and students with guest speakers or virtual conferences, virtual exhibitions, and online or hybrid workshops.
                
                     For example, “Tecnoestética y sensorium contemporáneo: arte, literatura, diseño y tecnología” in September 2017 in Córdoba Argentina; 
                 The network and initiatives that I foresee fostering and/or facilitating may include: 
            
            
                K-12 Service Education: Working with the College of Education and the Licensure Students in the World Languages and Cultures Department to: Expand on its longstanding educational outreach commitments with K-12 educators and students at the local and state level; and, serve as a resource to K-12 educators working to meet Virginia Performance Standards as they relate to Latin American content in the social, natural, and life sciences by 
                Language Without Borders Initiative: Create the next generation of global professionals through innovative language education, with Superior level proficiency in Spanish and overseas internship experience.
                DH and Latin/o American Cybercultures Initiative: Exposure to the digital culture of Latin America through seminars, symposia, courses, exhibitions, and workshops.
            
        
    
6344	2018	
        
            
                Introduction
                Digital literary studies advance in their research, requiring more specific metadata about literary phenomena: narrator (Hoover 2004), characters (Kastorp et al. 2015), place and period, etcetera. This metadata can be used to explain results in tasks like authorship attribution or genre detection, or to evaluate digital methods (Calvo Tello 2017). What could be the most efficient way to start annotating this information in corpora of thousand of texts in languages, genres and historical periods for which many NLP tools are not trained for? In this proposal, the aim is to identify specific literary metadata about entire texts with methods that are either language-independent or easily adaptable for humanists.
            
            
                Two Ways from Text to Metadata
                The two approaches to classify unlabeled samples applied here are rule-based classification and supervised machine learning. In rule-based classification (Witten et al. 2011), domain experts define formalised rules that correctly classify the samples. For example a rule based on a single token can be defined for each class to predict whether a text is written in third person (83% of the corpus) or first person using tokens for the two values are the Spanish words 
                    dije ('I said') and 
                    dijo (‘he said’), and the rule:
                
                
                    if 
                        dijo appears 90% more than 
                        dije, the novel is written in third person
                    
                    if 
                        dijo appears less, in first person
                    
                
                The results of applying this rule can be presented as a confusion matrix:
                
                    
                        
                    
                
                Fig 1. Confusion Matrix of rule-based results about narrator
                For supervised methods (Müller and Guido 2016; VanderPlas 2016), we need labeled samples to train and evaluate the method. In the following table, the different classifiers and document-representations achieve different accuracy scores:
                
                    
                        
                        raw
                        relative
                        tfidf
                        zscores
                    
                    
                        SVC
                        0.90
                        0.83
                        0.83
                        0.88
                    
                    
                        KNN
                        0.83
                        0.88
                        0.81
                        0.81
                    
                    
                        RF
                        0.88
                        0.88
                        0.86
                        0.90
                    
                    
                        DT
                        0.84
                        0.83
                        0.84
                        0.82
                    
                    
                        LR
                        0.88
                        0.83
                        0.83
                        0.17
                    
                    
                        BN
                        0.72
                        0.72
                        0.72
                        0.82
                    
                    
                        GN
                        0.72
                        0.80
                        0.80
                        0.81
                    
                
                Fig 2. Accuracy (F1-score) for narrator
            
            
                Corpus and Metadata
                The data is part of the 
                    Corpus of Spanish Novels of the Silver Age (1880-1939) (used in Calvo Tello et al. 2017), with 350 novels in XML-TEI by 58 authors. Each text has been annotated manually with metadata and its degree of certainty has been assigned. 262 texts with either high or medium certainty have been used to create a gold-standard with the following classes:
                
                
                    protagonist.gender
                    protagonist.age
                    protagonist.socLevel
                    setting.type
                    setting.continent
                    setting.country
                    setting.name
                    narrator
                    representation
                    time.period
                    end
                
            
            
                Modelisation and Methods
                The scripts have been written in Python (available on GitHub)
                     &lt;https://github.com/cligs/projects2018/tree/master/text2metadata-dh&gt;. The features have been represented as different document models (Kestemont et al. 2016):
                
                
                    raw frequencies
                    relative frequencies
                    tf-idf
                    z-scores
                
                Different classify algorithms (cross validation, 10 folds) and amount of Most Frequent Words have been evaluated. For each class a single token was used to represent each class value and a ratio was assigned for the default class value (see repository in GitHub for rules). Both approaches were compared to a “most populated class” baseline, quite high in many cases. 
            
            
                Results
                The results of both approaches are as following:
                
                    
                        Class
                        F1 baseline
                        F1 Rule
                        F1 Cross Mean
                        F1 Cross Std
                        Algorithm
                        Model
                        MFW
                        Winner
                    
                    
                        end
                        0.60
                        0.54
                        0.60
                        0.02
                        LR
                        tfidf
                        100
                        Baseline
                    
                    
                        narrator
                        0.83
                        0.80
                        0.91
                        0.04
                        RF
                        tfidf
                        1000
                        ML
                    
                    
                        protagonist.age
                        0.55
                        0.25
                        0.55
                        0.01
                        LR
                        tfidf
                        100
                        Baseline
                    
                    
                        protagonist.gender
                        0.80
                        0.68
                        0.80
                        0.01
                        BN
                        tfidf
                        100
                        Baseline
                    
                    
                        protagonist.socLevel
                        0.63
                        0.49
                        0.64
                        0.07
                        SVC
                        zscores
                        5000
                        Baseline
                    
                    
                        representation
                        0.88
                        0.80
                        0.88
                        0.01
                        LR
                        tfidf
                        100
                        Baseline
                    
                    
                        setting.continent
                        0.95
                        0.94
                        0.96
                        0.01
                        SVC
                        zscores
                        5000
                        Baseline
                    
                    
                        setting.continent.binar
                        0.95
                        0.95
                        0.95
                        0.19
                        LR
                        zscores
                        500
                        Baseline
                    
                    
                        setting.country
                        0.93
                        0.38
                        0.94
                        0.01
                        SVC
                        zscores
                        1000
                        Baseline
                    
                    
                        setting.country.binar
                        0.87
                        0.47
                        0.88
                        0.03
                        SVC
                        zscores
                        1000
                        Baseline
                    
                    
                        setting.name
                        0.64
                        0.85
                        0.71
                        0.02
                        SVC
                        zscores
                        1000
                        Rule
                    
                    
                        setting.type
                        0.48
                        0.46
                        0.71
                        0.05
                        SVC
                        zscores
                        5000
                        ML
                    
                    
                        time.period
                        0.95
                        0.95
                        0.97
                        0.01
                        BN
                        zscores
                        5000
                        Baseline
                    
                
                Fig 3. Results
                In many cases the baselines are higher than the results of both approaches. The rule outperformed the baseline in the case of name of the setting with very good results. In two cases (narrator and setting's type), Machine Learning is the most successful approach and its F1 is statistically higher than the baseline (one sample t-test, ɑ = 5%). The algorithms Supported Vector Machines, Logistic Regression and Random Forest are most successful, while tf-idf and speacilly z-scores got the best results, the last one a data representation “highly uncommon in other applications” different from stylometry (Kestemont et al, 2016).
            
            
                Conclusions
                In this proposal I have used simple rules and simple features in order to detect relatively complex literary metadata in many cases with high baselines. While Machine Learning showed a statistically significant improvement in detection for two classes (type of setting and narrator), rules worked better for the name of the setting. This is a promising point to continue researching in order to annotate the rest of the corpus.
            
        
        
            
                
                    Bibliography
                    
                        Calvo Tello, J. (2017). What does Delta see inside the Author?: Evaluating Stylometric Clusters with Literary Metadata. III Congreso de La Sociedad Internacional Humanidades Digitales Hispánicas: Sociedades, Políticas, Saberes. Málaga: HDH, pp. 153–61 &lt;
                        &gt;.
                    
                    
                        Calvo Tello, J., Schlör, D., Henny-Krahmer, U. and Schöch, C. (2017). Neutralising the Authorial Signal in Delta by Penalization: Stylometric Clustering of Genre in Spanish Novels. Montréal: ADHO, pp. 181–83 &lt;
                        &gt;.
                    
                    
                        Hoover, D. L. (2004). Testing Burrows’s Delta. Literary and Linguistic Computing, 19(4): 453–75.
                    
                    
                        Kastorp, F., Kestemont, M., Schöch, C. and Bosch, A. Van den (2015). The Love Equation: Computational Modeling of Romantic Relationships in French Classical Drama. Sixth International Workshop on Computational Models of Narrative. Atlanta, GA, USA. &lt;https://zenodo.org/record/18343&gt;.
                    
                    
                        Kestemont, M., Stover, J., Koppel, M., Karsdorp, F. and Daelemans, W. (2016). Authenticating the writings of Julius Caesar. Expert Systems with Applications, 63: 86–96 &lt;http://dx.doi.org/10.1016/j.eswa.2016.06.029&gt;.
                    
                    
                        Müller, A. C. and Guido, S. (2016). Introduction to Machine Learning with Python: A Guide for Data Scientist. Beijing: O’Reilly.
                    
                    
                        VanderPlas, J. (2016). Python Data Science Handbook: Essential Tools for Working with Data. First edition. Beijing Boston Farnham: O’Reilly.
                    
                    
                        Witten, I., Frank, E. and Hall, M. (2011). Data Mining: Practical Machine Learning Tools and Techniques. 3rd edition. San Francisco: Morgan Kaufmann.
                    
                
            
        
    
6356	2018	
        
            
                In his preface to 
                Doing Digital Humanities: Practice, Training, Research 
                (2016), Ray Siemens points out that imagining digital humanities as a community of practice wherein participants come into conversation with one another over shared approaches to craft establishes a “methodological commons” where fields intersect by sharing their work processes. Presenting a taxonomy of approaches to training that span from the informal to the formal within the methodological commons, Siemens suggests that the variety of possible approaches builds an infrastructure for “self-determination” in humanists’ approach to learning useful skills. Somewhere between informal consultations and formal degree programs, short courses and “bootcamps” offer professional and research skill development opportunities that scholars can choose from based on their most pressing needs. 
            
            Digital humanities skill development cannot be automated; it is resource intensive. It depends upon a limited number of people to deliver highly personalized training to relatively small cohorts of scholars--a model that is difficult to fund and harder to scale. As interest in and demand for training in digital humanities research methods continues to increase, overall capacity to reach the needs and interests of diverse populations of scholars in the wide range of institutional contexts where they do their work has not kept pace.
            
                Committed to building a vibrant community of scholars who deploy a critical use of digital technologies in their teaching and research, the CUNY Graduate Center will run its fourth week-long digital research institute in January 2018. Between 2016 and 2017, GC Digital Initiatives offered a combined 100 hours of instruction on digital research methods to more than 100 students, faculty, staff, and librarians across the CUNY system.
                
                    
                         GC Digital Research Institute 
                            http://cuny.is/gcdri
                        
                    
                
                 Our institute model has focused on reducing the time required to develop new curricula through sharing and versioning, expanding the number of participants per institute through collaborative learning environments, and supporting participants through community-building. The success of our model is demonstrated by continued, growing interest from students, faculty, and staff each year. 
            
            
                As interest in digital humanities at universities, museums, libraries, and archives increases, so too does the demand for faculty, administrative staff, librarians, postdocs and graduate students who are tasked with expanding DH research and teaching capacity with relatively few resources. With funding from the National Endowment for the Humanities, we will be expanding our model to create a sustainable, reproducible model for digital methods training that can be adapted and used in a variety of institutional contexts. Our institute model is designed to integrate feedback so that it can be replicated, modified, and reproduced in new contexts, lowering the barrier to entry for digital humanities scholars by meeting scholars where they are rather than requiring participants to travel to receive training. 
            
            
                In June 2018, 15 individual participants will participate in the first Digital Humanities Research Institute. The DHRI emphasizes foundational technical skills, such as the command line, git, Python, and databases, that provide a flexible technology “stack” and that better enable DH researchers to become more confident autodidacts and mentors in their own right. While participants develop familiarity with useful tools, they learn more importantly how to navigate a computer’s information architecture, read technical documentation, and reason through simple systems, leading to a greater conceptual vocabulary and increased confidence approaching technology with a critical eye. 
                As participants learn skills to support their individual research goals and professional growth, they will also learn how to lead similar digital humanities institutes in their local communities over the following academic year. Through the process of iterating, refining, and building the institute model, we intend to share the lessons learned to increasingly wider communities of learners and build a network of curricular models and support. 
            
            
                Our poster will feature curricula, pedagogical materials such as datasets, and resources developed for the ten-day residential institute, where participants will explore interdisciplinary digital humanities research and teaching with leading DH scholars, develop core computational research skills through hands-on workshops, and begin developing versions of the DHRI for their own communities. We will share lessons learned and provide information about forthcoming institutes. Short video clips will feature our unique approach to digital humanities pedagogy and 
                interviews with previous institute instructors and participants. 
            
        
        
            
                
                    Bibliography
                    Crompton, Constance, Richard J. Lane, and Ray Siemens. 
                        Doing Digital Humanities: Practice, Training, Research. Routledge, 2016.
                    
                
            
        
    
6364	2018	
        
            En 1985 se publicó el libro 
                México: del antiguo régimen a la revolución del historiador Françoise Xavier Guerra, una referencia fundamental para el estudio de la Revolución mexicana. La obra revela las relaciones y tensiones entre la sociedad tradicional, un sistema heredado de la colonia y el Estado moderno proveniente en gran medida de los ideales liberales de la revolución francesa.
            
            El trabajo de Xavier Guerra se inscribe en una amplia tradición de la investigación prosopográfica que se extiende desde el siglo XIX (Verboven, Carlier y Dumolyn, 2007) hasta el relativamente reciente uso de herramientas computacionales para el manejo de bases de datos (Blust, 1989; Keats-Rohan, 2010). El cuerpo biográfico de su investigación está compuesto por más de siete mil actores sociales entre los que figuran individuos y colectividades, con aproximadamente cien mil datos asociados a los movimientos políticos. Para su análisis se construyó una base que sistematizó los datos en más de cincuenta categorías que codifican dos tipos de sucesos; aquellos personales como fecha de nacimiento, muerte y ascendencia familiar y aquellos sucesos relacionados con la vida política y social del actor como participación en batallas o los cargos públicos ocupados. Los sucesos se organizaron en módulos independientes, lo cual permitió enriquecer la base de datos con la captura de nuevos módulos para personajes ya establecidos.
            Dicha base de datos fue almacenada originalmente en tres cintas magnéticas de las cuales no se refieren más detalles, realizar nuevos análisis resulta inviable ya que el único medio en que está disponible actualmente es el impreso en los anexos de la obra señalada. El objetivo de este trabajo es la digitalización de la base de datos de Xavier Guerra, que permita la reproducción de los análisis del autor, así como la generación de nuevo conocimiento a partir del cruce de variables. 
            Con ese fin, se creó un programa en Python que ocupa Tesseract, una biblioteca de reconocimiento de caracteres. Debido a la estructura modular de la base de datos, los renglones, columnas y espacios en blanco son significativos. Por lo tanto, se realizó un pre-procesamiento de las imágenes, para detectar la estructura espacial del texto, de manera que Tesseract procesará pedazos de texto organizados. En esta etapa se ocupó el framework OpenCV y la biblioteca Pytesseract. Posteriormente, el programa organizó la información en un esquema de base de datos dentro de un archivo SQL.
            En este póster presentamos el código desarrollado para la recuperación y organización de la base de datos, el funcionamiento de la base mediante algunas réplicas de los análisis que realizó Xavier-Guerra en su obra, así como el resultado de queries inéditos y por último el diseño inicial de la página que permita interactuar con los datos, de modo que los usuarios puedan consultar al sistema en términos de tiempo, geografía, compromisos políticos y relaciones de parentesco o sociales.
        
        
            
                
                    Bibliography
                    Ingrese sus referencias aquí:
                    
                        Blust, N. (1989). Prosopography and the computer: problems and possibilities, en Denley, P. (ed.) 
                        History and computing . no.2. Manchester, UK: Manchester University Press, pp. 12–18.
                    
                    
                        Keats-Rohan, K. (2010). Prosopography and Computing: a Marriage Made in Heaven?, 
                        History and Computing, 12(1), pp. 1–11.
                    
                    
                        Verboven, K., Carlier, M. y
                         Dumolyn, J. (2007). A Short Manual to the Art of Prosopography, en Keats-Rohan, K. (ed.) 
                        Prosopography Approaches and Applications. A Handbook. Oxford: University of Oxford, pp. 35–69.
                    
                
            
        
    
6381	2018	
        
            Since the invention of Latent Dirichlet Allocation (Blei, et al. 2003) and early demonstrations of its utility for identifying lexical clusters in collections of historical and literary texts (Block and Newman 2006, Blevins 2010), topic models have become a mainstay of the digital humanities. However, the use of topic models within the field remains narrowly conceived, restricted largely, with some exceptions, to the discovery of topics and topic trends within corpora, even though the method has been extended significantly since first introduced. One reason for this conservativism may be that, like many methods drawn from data science, both the process and the output of topic model algorithms remain interpetively opaque to the humanists (and, arguably, to the computer scientist as well). Aside from the complexity of the math involved, a contributing factor to this opacity has been the limited way in which the results of topic models are presented to the user. One the one hand, the data provided by standard topic modeling tools (whether in Java, Python, or R) are often trapped in data files or shielded by objects that cannot be queried directly or visualized freely without the use of ad hoc programming or spreadsheet software. On the other hand, the outputs typically provided by these tools, such as top words per topic (often visualized as word clouds), show a highly restricted, decontextualized, and potentially distorted picture of the model (Schmidt 2013). Recently, various tools have emerged to fill this gap, such TOME (Klein et al. 2015), which is designed to allow scholars to explore topic models more fully. In this talk I will present Polo, a topic model browser developed at the Data Science Institute at the University of Virginia designed to present topic models to users in a direct, transparent, and complete manner, so that the representational quality of models may be explored, questions, and adjusted interactively. Built on top of MALLET, Gensim, and NLTK, Polo is a Python package that provides tools to both create topic models and to inspect them by combining the source corpus with all of the data produced by the core software into a single, normalized relational database (in SQLite). This database in turn forms the foundation of an interactive web application that effectively converts the output model with associated data and the source corpus into a single hypertext relating words, topics, and documents. A key design feature of Polo is that it employs the statistical properties of the model -- such as topic entropy in documents or mutual information among topics -- not simply as readouts on a dashboard but as navigational devices that allow the user to move from a reduced dimension, high-level perspective of a corpus to its source documents, and to move laterally through the network of topics and documents that compose the model. Using examples from both newspaper and journal collections, I will demonstrate how Polo enables scholars both to investigate implied cultural newtworks in these corpora and to explore the various ways in which topics may be said to convey meaning.
        
        
            
                
                    Bibliography
                    Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2002. “Latent Dirichlet Allocation.” In 
                        Advances in Neural Information Processing Systems 14, edited by T. G. Dietterich, S. Becker, and Z. Ghahramani, 601–608. MIT Press.
                    
                    Blevins, Cameron. 2010. “Topic Modeling Martha Ballard’s Diary.” 
                        Cameron Blevins (blog). April 1, 2010, 
                        http://www.cameronblevins.org/posts/topic-modeling-martha-ballards-diary/.
                    
                    Klein, Lauren F., Jacob Eisenstein, and Iris Sun. 2015. “Exploratory Thematic Analysis for Digitized Archival Collections.” 
                        Digital Scholarship in the Humanities 30 (suppl_1):i130–41.
                    
                    Newman, David J., and Sharon Block. 2006. “Probabilistic Topic Decomposition of an Eighteenth-Century American Newspaper.” 
                        Journal of the American Society for Information Science and Technology 57 (6):753–767.
                    
                    Schmidt, Benjamin M. 2013. “Words Alone: Dismantling Topic Models in the Humanities.” 
                        Journal of Digital Humanities. April 5, 2013.
                    
                
            
        
    
6393	2018	
        
            1. Introduction and Overview
            In the past several years, the utility of topic modeling for the humanities has been clearly established. Scholars can now point to projects that convincingly employ topic modeling to explore the figurative language employed in ekphrastic poetry (Rhody 2012), to trace the “quiet transformations” of literary studies (Goldstone and Underwood 2014), and to distill the epistemic dimensions of novels (Erlin 2017), among others. And yet, broader applications of the technique remain limited by the computational and statistical expertise required to implement a topic model and interpret its results. While there has been some work to develop topic model “browsers” (e.g. Goldstone 2014, Murdock and Allen 2015), these projects are designed to facilitate the exploration of the model itself, rather than to leverage the affordances of topic modeling for humanities scholars. By contrast, our interface was conceived so that non-technical humanities scholars can employ a topic model of their corpus in order to discover the documents most salient to their research (Klein et al. 2015).
                
                    
                         The first round of research on TOME was conducted between 2013 and 2015 in collaboration with Jacob Eisenstein, Assistant Professor of Interactive Computing at Georgia Tech, funded by NEH Office of Digital Humanities Startup Grant HD-51705-13. See Klein et al. 2015. 
                    
                
            
            2. Corpus, Model, and Database
            Our corpus consists of nearly 300,000 documents drawn from a collection of nineteenth-century abolitionist newspapers. The documents were scraped from the Accessible Archives website, as per an agreement with Accessible. Additional cleaning of the data, as well as metadata creation, was performed through custom Python scripts. 
            The topic model of our corpus was created using gensim, the vector space and topic modeling library (Rehurek and Sojka 2010). We employed gensim’s wrapper for Latent Dirichlet Allocation (LDA) from MALLET (McCallum 2002). We generated 100 topics after 100 iterations, filtering the 100 most common words. We printed the topics and topical composition of each document to CSV files. We then ingested the data into a MySQL database using Django’s ORM framework.
                
                     The topic model and related processing scripts can be found at: 
                        https://github.com/GeorgiaTechDHLab/TOME/. 
                    
                
            
            4. Interface and Sample Interaction
            Our interface is the result of a several-month design process during which we considered a variety of user scenarios. Our goal was to scaffold the process of document discovery so that the user could draw new insights as they moved through each section of the interface: Topic Overview, Topic Details, Document Overview, and Document Details.
                
                     A live version of this interface can be found at: 
                        http://tome.lmc.gatech.edu/. 
                    
                
            
            The user begins with the Topic Overview section (Figure 1), which employs a custom visualization in order to display each of the 100 topics according to its change in rank over time. The user can also filter the topics by keyword or sort according to overall prevalence. 
            
                
                Topic Overview
            
            When the user has selected their topics of interest, they scroll to see details about those topics: change in percentage of the corpus over time; distribution in each newspaper over time; and geographic distribution (Figure 2). These visualizations work together to show which topics were most prevalent at which times; which sources were reporting on which topics at particular times; and where each topic was being reported on. From there, the user can either return to the Topic Overview to further refine the topic set (Gelman 2004), or scroll down to the Document Overview section. 
            
                
                Topic Details
            
            The Document Overview (figure 3) section allows the user to further refine the set of documents they will eventually read. They can toggle between a standard list view of all the documents, ranked in terms of what percentage of the selected topics they contain, and a dust-and-magnets view (Yi et al. 2005). 
            
                
                Document Overview
            
            From there, they move to Document Details (figure 4), which displays the metadata associated with each article in the corpus, ordered according to the percentage of the selected topics they contain. This allows the user to click through to the articles themselves, having narrowed down a set of articles relevant to their research. 
            
                
                Document Details
            
            The interface is implemented using HTML and JavaScript, including D3.js, the JavaScript-based visualization library, and AJAX for client-side data retrieval. 
            
                Initial research on TOME was conducted from 2013 to 2015 in collaboration with Jacob Eisenstein, School of Interactive Computing, Georgia Institute of Technology, funded by NEH Office of Digital Humanities Startup Grant HD-51705-13. 
            
        
        
            
                
                    Bibliography
                    
                        Erlin, M. (2017). Topic Modeling, Epistemology, and the English and German Novel. 
                        Cultural Analytics. 
                    
                    
                        Gelman, A. (2004). Exploratory Data Analysis for Complex Models. 
                        Journal of Computational and Graphical Statistics 13 (4): 755–779.
                    
                    
                        Goldstone, A., and Underwood T.
                        (2014). The Quiet Transformations of Literary Studies: What Thirteen Thousand Scholars Could Tell Us.
                        New Literary History
                         45 (3): 359-384. 
                    
                    
                        Goldstone, A. (n.d.). DfR Browser. 
                        
                            https://agoldst.github.io/dfr-browser/
                         (accessed 25 April 2018).
                    
                    
                        Klein, L., Eisenstein, J., and Sun, I. (2015). Exploratory Thematic Analysis for Digitized Archival Collections. 
                        Digital Scholarship in the Humanities 30 (Supp. 1): i130-i141.
                    
                    
                        McCallum, A. (2002). MALLET: A Machine Learning for Language Toolkit.
                         
                        
                            http://mallet.cs.umass.edu
                         (accessed 25 April 2018).
                    
                    
                        Murdock, J. and Allen, C.
                        (2015). Visualization Techniques for Topic Model Checking. 
                        AAAI Conference on Artificial Intelligence
                        , Austin, TX, January 2015.
                    
                    
                        Rehurek, R. and Sojka, P. (2010). Software Framework for Topic Modelling with Large Corpora. 
                        LREC 2010 Workshop on New Challenges for NLP Frameworks, Valetta, Malta, May 2010. 
                    
                    
                        Rhody, L. M. (2012). “Topic Modeling and Figurative Language.” 
                        Journal of Digital Humanities 2 (1). 
                    
                    
                        Yi, J.S. (2005). Dust &amp; Magnet: Multivariate Information Visualization Using a Magnet Metaphor. 
                        Information Visualization 4 (4): 239-256.
                    
                
            
        
    
6407	2018	
        
            
                Introduction
                This poster presents the textbox
                    
                        .
                     published by the CLiGS
                    CliGS (Computational Literary Genre Stylistics) is an early career research group funded by the German Federal Ministry for Research and Education (BMBF). 
                        http://www.cligs.hypotheses.org.
                     group both from the perspective of creating the textbox and of using it for research. The poster will highlight key aspects of the manner in which the collections of literary texts included in the textbox have been compiled, annotated and published. Furthermore it suggests several ways in which the text collections can be used for research in literary studies. This poster aims to showcase the unique XML-TEI-based collections we make available and to encourage their re-use by others.
                
            
            
                What is the textbox?
                The CLiGS textbox is dedicated to making collections of literary texts in Romance languages freely available. It currently contains novels, novellas and short stories published between 1830 and 1940 in France
                    The French collection was partly compiled and annotated by Stefanie Popp., Spain, Italy, Portugal, and Spanish-America as well as plays published between 1640 and 1680 in France with a total of 357 texts or about 14 million words. The texts are published in XML-TEI as well as in plain text versions and include detailed document-level metadata. All texts are in the public domain and the XML-TEI markup including the metadata is published with a Creative Commons Attribution license (CC-BY) or in case of the Italian novels with a NC-SA-BY. The text collections are curated and published using a public GitHub repository. In addition, main releases are automatically archived on Zenodo.org, a long-term data and publications archiving service for researchers across Europe managed by OpenAire and supported by CERN (see Nielsen, 2013). Each release receives a DOI (Digital Object Identifier), providing the unambiguous identification and long-term availability of the resource.
                
            
            
                Text selection
                The individual text collections were created with various usage scenarios in mind, and each collection has been compiled in a slightly different manner. For example, the two collections of Spanish novels, the 
                    Corpus of Spanish Novels (1880-1940) and the 
                    Collection of 19th century Spanish-American Novels (1880-1916), have been prepared to be used for authorship attribution. Accordingly, the two collections have been balanced with regard to the number of texts from different authors. The poster will give an overview of the sub-collections of the textbox and also about the principles guiding their compilations.
                
            
            
                File Formats
                Independently of their original source format (e.g. html or EPUB), the texts are prepared (with Python scripts or XSLT) according to a common TEI schema established by the CLiGS group
                    
                        .
                    . In addition to that reference format, each collection is made available in a simple plain text format automatically derived from the XML-TEI version, containing only the text included in the body of the novels and plays (in particular, excluding prefaces, other paratext, or notes) and with external metadata provided in tabular format.
                
                Moreover, the collections of French, Spanish, Spanish-American, and Portuguese novels as well as the Italian short stories are made available in a version combining basic structural markup (chapter and sentence divisions) with token-level linguistic annotation, including lemma, part-of-speech, morphology, and basic semantic annotation using Freeling (cf. Padró and Stanislovsky, 2012) and WordNet (see Figure 1). Finally, the collection of French plays is not only available in XML-TEI, but also in the “Zwischenformat” developed by the DLINA group (Kampkaspar et al., 2015).
                
                    
                        
                        
                            Linguistic annotations in an XML format that is a minimal departure from the TEI standard to allow multiple token-level annotations
                        
                    
                
            
            
                Metadata
                Besides the administrative metadata like license, responsibility etc. the collections focus on descriptive metadata. There are four main areas about which information is documented: metadata concerning the authorship (VIAF, name, country, gender), metadata concerning the literary work and editions (VIAF or other identifier, extent of the texts, print and the digital source), and finally metadata concerning the genre: Since the main focus of the project is literary genre, a considerable part of the metadata is directly connected to it. Any reference to genre in the title of the work is collected as a genre label. Besides that, a hierarchical system is used, comprising supergenre (e.g. “narrative” or “drama”), genre (that is, novels or novellas), subgenre (the subtype of the novel, for example “adventure novel” or “political novel”) and subsubgenre (optional, used for further differentiations like “war novel”).
            
            
                Usage Scenarios
                There are many possible use cases for the textbox collections. The poster will demonstrate some results of these methods from the areas of authorship attribution (using the stylo package for R; Eder et al., 2016), network analysis (using NetworkX in Python), and topic modeling (using MALLET with “tmw” for Python). These scenarios are intended not only as examples of analyses conducted within the CLiGS group, but also as suggestions for potential users of the CliGS textbox, Figure 2 and 3 demonstrate some results for authorship attribution and network analysis.
                
                    
                        
                        
                            Authorship attribution, results of cosine delta on the Corpus of Spanish Novels (cf. Smith and Alridge, 2011; Evert et al., 2017)
                        
                    
                
                
                    
                        
                        
                            Character network based on number of words spoken in mutual presence (represented by the thickness of the lines), for Jean Racine's tragedy Britannicus (1669)
                        
                    
                
            
        
        
            
                
                    Bibliography
                    
                        E
                        der, M., Kestemont, M. and Rybicki, J. (2016). Stylometry in R: A package for computational text analysis. In 
                        The R Journal, 16 (1): 1-15.
                    
                    
                        E
                        vert, S., Proisl, T., Jannidis, F., Reger, I., Pielström, S., Schöch, C. and Vitt, T. (2017). Understanding and explaining Delta measures for authorship attribution. In 
                        Digital Scholarship in the Humanities, 32 (suppl_2): ii4-ii16. doi: 10.1093/llc/fqx023 
                        (accessed April 26 2018).
                    
                    
                        K
                        ampkaspar, D., Fischer, F. and Trilcke, P. (2015). Introducing Our ‘Zwischenformat’. In 
                        Network Analysis of Dramatic Texts. 
                        (accessed April 26 2018).
                    
                    
                        Nielsen, L. H. (2013). ZENODO – An innovative service for sharing all research outputs. In 
                        Zenodo. doi: 10.5281/zenodo.6815 
                         (accessed April 26 2018).
                    
                    
                        Padró, L. and Stanislovsky, E. (2012). FreeLing 3.0: Towards Wider Multilinguality. In 
                        Proceedings of the Language Resources and Evaluation Conference (LREC 2012). Istanbul, Turkey: ELRA. 
                         (accessed April 26 2018): 2473-2479.
                    
                    
                        Smith, P. W. H. and Alridge, W. (2011). Improving Authorship Attribution: Optimizing Burrows’ Delta Method. In 
                        Journal of Quantitative Linguistics, 18(1): 63-88. doi: 10.1080/09296174.2011.533591.
                    
                
            
        
    
6410	2018	
        
            
                Using LDA (Latent Dirichlet Allocation) for analyzing the content structure of digital text collections is a possibility, that aroused the interest of many digital humanists in the recent years. The method allows to generate a so called ‘topic model’ from a text corpus, each ‘topic’ in the model being represented by a probability distribution over the words in the corpus. In each of these topics, another group of semantically related words appears with high probability scores. By labeling topics with their most probable words and then calculating the relative contributions of the topics to each text or text segment, researchers can use LDA as an unsupervised method to survey the contents of a text corpus (Blei 2012, Steyvers and Griffiths 2006).
            
            However, to actually use LDA, technical skills lacked by the majority of humanities scholars is necessary. There is a number of accessible implementations of the LDA algorithm, the most popular being in MALLET (McCallum 2002), a Java program that has to be run and controlled from the command line and Gensim (Rehurek und Sojka 2010),  a text analysis library for the Python programming language. Basically, most existing implementations of the algorithm require programming skills to be used efficiently, and for most use cases one has to switch between systems, tools and programming languages to complete the entire workflow from preprocessing to the analysis of results.
            
                With the aim of lowering the threshold to use LDA for humanities scholars, we developed a programming library in Python that significantly reduces the complications to control the whole process of topic modeling from preprocessing to the visualization of results with a single Python script. The library, developed with funding from the European infrastructure project DARIAH (
                
                ), allows to choose from three different LDA implementations (MALLET, Gensim, and the ‘LDA’ package by Allan Riddell; 
                
                ). It provides a number of interactive, extensively annotated 
                jupyter notebooks (
                
                ) that can be used as tutorials for beginners and template workflows that can be adjusted to individual needs.
            
            Many potential users are not yet familiar with programming at all, but interested in the method and eager to experiment with it a little before deciding if it is worth learning a new set of skills to use it to its full extent. For them the learning curve of a jupyter notebook is still too steep.  That at least was the feedback we received in our workshops which we organized to get feedback from scholars: the wish for a GUI to access at least the basic functionalities was expressed frequently. To meet this demand, we started the development of a ‘GUI Demonstrator’ that mirrors the working steps and explanations in the notebooks, and allows users to analyse their own texts using LDA with a limited set of options.
            
                The current version, that is implemented in the FLASK microframework (
                
                ) and runs within a browser window (Fig 1.), includes all steps necessary to get from a number of raw text files (txt and xml file formats are supported) to a visualized output, currently an interactive heat map showing the distribution of topics over texts (Fig. 2). As the quality of results depends on removing frequent words that appear in all texts, users can decide on the number of most frequent words to remove, or provide their own stopword list. They can control the number of topics to be generated, and the number of iterations the algorithm should run. The latter is important, because a large number of iterations will produce more stable results, but the algorithm will take longer to finish the task.
            
            
                The next working steps include the implementation of standalone graphics in the Qt library (
                
                ), and in allowing for flexibility in the choice and use of the results and outputs users are specifically interested in. The possibility to include metadata and evaluation results is another focus for upcoming developments, e.g. to sort text in the output heatmap according to different categories, or topics according their quality indicated by evaluation metrics.
            
            
                Both the library and the Demonstrator as a standalone executable for Windows and OSX are open source and available on Github (
                
                ). 
            
            
                
                
                    
                
            
            
                Figure 1: Screenshot of the upper end of the input screen in the current version of the GUI Demonstrator.
            
            
                
                
                    
                
            
            
                Figure 2: Example for an interactive heatmap output in the current version of the GUI Demonstrator.
            
        
        
            
                
                    Bibliography
                    
                        
                        Blei, David M.
                         
                        (2012): „Probabilistic Topic Models“, in 
                        Communication of the ACM
                         
                        55, Nr. 4 (2012): 77–84. doi:10.1145/2133806.2133826.
                    
                    
                        McCallum, Andrew K.
                         
                        (2002): 
                        MALLET : A Machine Learning for Language Toolkit
                        . 
                        
                            http://mallet.cs.umass.edu
                        
                        .
                    
                    
                        Rehurek, Radim/ Sojka, Petr
                         
                        (2010): "Software framework for topic modelling with large corpora." 
                        In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks
                        .
                    
                    
                        Steyvers, Mark/ Griffiths, Tom
                         
                        (2006): „Probabilistic Topic Models“, in 
                        Latent Semantic Analysis: A Road to Meaning
                        , herausgegeben von T. Landauer, D. McNamara, S. Dennis, und W. Kintsch. Laurence Erlbaum.
                    
                
            
        
    
6417	2018	
        
            This poster presents the library metadata aspects of a web-based text mining application for sifting corpora of unstructured text in order to find particular passages that deal with a concept of interest. In addition to overcoming the limitations of vendor-supplied search platforms, which tend to be based on simple keyword searches that place the burden of interpreting, refining, and iterating on search results on the laborious grunt work of scholarly users (De Bolla, 2013), this tool demonstrates the utility of reconciling named entities with external structured data to refine its results and to enrich its output for use in research, visualizations, and secondary analytic tools by leveraging demographic (Hwang, 2015), temporal, and geographic data from the linked open data cloud. This necessitates the creation of entity resolution workflows with both automated matching tools and practices for manual reconciliation and maintenance, exploring a variety of open-source tools including OpenRefine (Van Hooland, 2014; Hwang, 2017), Python, and Mix’n’Match (Knoblock, 2017) and contributing to the development of “functional requirements for how [library] systems use and maintain these identifiers and associated data” (Folsom, 2017) by metadata librarians and researchers and “the complexities inherent in managing both locally-created and externally-assigned identifiers” in the context of library infrastructure (Tarver, 2017). Our goal is to integrate a tool catering to advanced researchers into library discovery platforms by “[exploring] partnerships with external entities to create game changing discovery” (Wones, 2017) and leveraging those users’ domain expertise to “interrogate corpora of resources directly ... to discover new patterns that exist across the literature, perform their own ranking of relevance against particular parameters, and find new pathways for discovery more efficiently than could be enabled through existing information portals” (MIT Libraries, 2016). The process is as follows:
            1. Combine vendor metadata for large corpora with bibliographic metadata from Harvard Library collections
            2. Reconcile authors, including persons and organizations, in those metadata resources, with external URIs, including those of ISNI (International Standard Name Identifier), Wikidata, and Geonames entities, generating batches of new entities in external resources at scale as needed (Mika, 2017)
            3. Integrate data from external URIs into a text mining tool for sifting large corpora to drive filters and enrich data extracted from that tool
            4. Work with library technology staff and metadata librarians to facilitate retrieval of rare materials in Harvard Library collections, as well as their electronic reproductions, based on results of text mining tool and integration of URIs in library metadata
            5. Export resulting data to produce visualizations and secondary analytic tools
            Through this process, we hope to enable the serendipitous discovery (Bourg, 2017) of relevant but unknown works in library collections: traditional reading of the “great unread” (Cohen, 1999) facilitated by distant reading (Moretti, 2013). Our poster includes: an explanation of the linked data principles underlying the metadata aspects of the text mining tool, our entity reconciliation workflow, implications for library metadata and name authority practices in support of digital research projects, and an example of combined and enriched metadata for a work of eighteenth century literature, and an example of an iterative concept search and its output presented both as a static flowchart on the poster as well as an interactive prototype on a laptop.
        
        
            
                
                    Bibliography
                    
                        Bourg, Chris. (2017). Serendipity as prick https://chrisbourg.wordpress.com/2017/02/11/serendipity-as-prick/ (accessed 18 November 2017).
                    
                    
                        Cohen, Margaret. (1999). The Sentimental Education of the Novel. Princeton, N.J.: Princeton University Press.
                    
                    
                        De Bolla, Peter. (2013). The architecture of concepts : The historical formation of human rights (First ed.). New York: Fordham University Press.
                    
                    
                        Folsom, Steven. (2017). New Models Require New Action Plans: Implementing Linked Data within the PCC. PCC (Project for Cooperative Cataloging) Strategic Planning Meeting Keynote, 1 November 2017. https://docs.google.com/presentation/d/11DHYRy24F4aQjYbPsVmInO2ovcb_pujBPIwJBQ0RrAQ/edit?usp=sharing (accessed 27 November 2017).
                    
                    
                        Hwang, Karen. (2015).  Enriching the Linked Jazz Name List with Gender Information https://linkedjazz.org/enriching-the-linked-jazz-name-list-with-gender-information/ (accessed 1 November 2017).
                    
                    
                        Hwang, Karen. (2017). Using OpenRefine to Reconcile Name Entities http://mnylc.org/fellows/2017/03/17/using-openrefine-to-reconcile-name-entities/ (accessed 10 October 2017)
                    
                    
                        Knoblock, C.A., et al. (2017). Lessons Learned in Building Linked Data for the American Art Collaborative. In: d'Amato C., et al. (eds) The Semantic Web – ISWC 2017 : 16th International Semantic Web Conference, Vienna, Austria, October 21-25, 2017, Proceedings, Part II (Lecture Notes in Computer Science, 10588). Cham: Springer International Publishing : Imprint: Springer.
                    
                    
                        Mika, Katie. (2016). The Role of Librarians in Wikidata and Wikicite. https://library.mcz.harvard.edu/blog/role-librarians-wikidata-and%C2%A0wikicite (accessed 1 November 2017).
                    
                    
                        MIT Libraries, Ad Hoc Task Force on the Future of Libraries. (2016). Institute-Wide Task Force on the Future of Libraries—Preliminary Report https://future-of-libraries.mit.edu/sites/default/files/FutureLibraries-PrelimReport-Final.pdf (accessed 25 November 2017).
                    
                    
                        Moretti, Franco. (2013). Distant Reading. London: Verso.
                    
                    
                        Tarver, Hannah, &amp; Phillips, Mark. (2017). Identifier Usage and Maintenance in the UNT Libraries’ Digital Collections http://dcevents.dublincore.org/IntConf/dc-2016/paper/download/458/546 (accessed 27 November 2017).
                    
                    
                        Van Hooland, Seth, &amp; Verborgh, Ruben. (2014). Linked data for libraries : How to clean, link and publish your metadata. Chicago, IL: Neal-Schuman.
                    
                    
                        Wones, Suzanne. (2017). Harvard Library Digital Strategy, Version 1.0. http://projects.iq.harvard.edu/files/overseers/files/vc_3_hl_digital_strategy_v2.pdf (accessed 10 November 2017).
                    
                
            
        
    
6437	2018	
        
            
                Introducción
                Encabalgamiento es el desajuste entre la pausa métrica y la sintáctica 
                    (Domínguez Caparrós, 2000: 103) que ocurre cuando una unidad de sentido se rompe entre dos versos. Este fenómeno, desde siempre utilizado con distintos fines expresivos (énfasis, ambigüedad, etc.) es difícil de delimitar formalmente. 
                
                El estudio más sistemático realizado para su caracterización en español sigue siendo el realizado en su tesis por Quilis 
                    (1964). El estudioso experimentó con lecturas de prosa, buscando demostrar qué unidades sintácticas no permiten pausa de sentido en su interior. Basándose en los resultados definió una serie de categorías gramaticales y sintácticas cuya separación en versos distintos produce encabalgamiento. La tipología allí establecida se considera ya clásica. El estudio de Quilis proporciona una definición formal y empírica del fenómeno. Con base en sus reglas se ha creado una herramienta capaz de detectar el encabalgamiento y sus tipos. 
                
                Este póster presenta la interfaz ANJA para el análisis automático del encabalgamiento desde una sencilla aplicación web: 
                    http://prf1.org/anja/index/, desarrollada dentro del proyecto ERC POSTDATA GA- 679528
                    
                         Este trabajo se enmarca dentro del proyecto de investigación Starting Grant Poetry Standardization and Linked Open Data: POSTDATA (ERC-2015-STG-679528), financiado por el European Research Council (ERC) bajo el programa: European Union´s Horizon 2020 research and innovation programme, dirigido como Investigador Principal por la profesora Elena González-Blanco, LINHD UNED (
                            http://postdata.linhd.es/).
                        
                    .
                
            
            
                Estado del arte
                La naturaleza formal del análisis métrico lo hace un campo propicio para su tratamiento computacional 
                    (Birnbaum and Thorsen, 2015;
                     Delente and Renault, 2015). El procesamiento del lenguaje natural (PLN) ofrece muchas posibilidades para la métrica, pues las reglas de definición lingüística permiten llevar a cabo análisis y extracción automática de grandes cantidades de información de corpus textuales. 
                
                Para la automatización del análisis métrico en español destacamos los estudios de escansión silábica y acentual de Navarro-Colorado 
                    (2017), Agirrezabal 
                    (2017) y Gervás 
                    (2000). También los trabajos de generación automática de poesía con patrones métricos 
                    (Gervás, 2000b) y 
                    (Gervás, 2015). 
                
                En el campo de las interfaces cabe distinguir entre aquellas que exploran datos de textos ya analizados, recogidos en una base de datos, y aquellas que permiten la entrada y análisis de cualquier poema. Del primer tipo destacamos For Better For Verse
                    
                        
                            http://prosody.lib.virginia.edu/
                        
                    
                    (Tucker, 2011) y Database of Czech Metre
                    
                        
                            http://versologie.cz/v2/web_content/
                        
                    
                    (Plecháč and Kolár, 2015). Entre las que permiten introducir textos destacamos, en español, la ligada a la herramienta de Navarro-Colorado
                    
                        
                            http://adso.gplsi.es/index.php/es/demostracion/
                        
                    , que analiza versos endecasílabos. Otros sitios con interfaz de entrada para análisis métrico son Separarensílabas
                    
                        
                            http://www.separarensilabas.com/index.php
                        
                     o Lexiquetos
                    
                        
                            http://lexiquetos.org/silio/
                        
                    . En otras lenguas destacamos Metricalizer
                    
                        
                            https://metricalizer.de/en/metrikanalyse/poem
                        
                    
                    (Bobenhausen and Hammerich, 2015) para alemán, Aoidos
                    
                        
                            http://aoidos.ufsc.br/
                        
                    
                    (Mittmann, 2016) para portugués y español, y RhymeDesign
                    
                        
                            http://www.sci.utah.edu/~nmccurdy/rhymeDesign/
                        
                    
                    (McCurdy et al., 2015) especializado en rima en inglés.
                
                Una interfaz para el análisis del encabalgamiento representa, sin embargo, una novedad en el campo.
            
            
                 Herramienta y resultados 
                El programa de detección del encabalgamiento en español, basado en PLN, se desarrolló en 2016-2017 y fue evaluado sobre dos corpus de test de distintos periodos 
                    (Ruiz et al., 2017). ANJA proporciona una interfaz web simple para este programa. El sistema consta de tres componentes: módulo de preprocesado para uniformar el formato de los poemas, pipeline de PLN (basada en IXA Pipes 
                    (Agerri et al., 2014) para POS-tagging, constituyentes y dependencias sintácticas) y módulo de detección de encabalgamiento (basado en reglas y diccionarios) y ampliamente documentado en el sitio web
                    
                        
                            
                                https://sites.google.com/site/spanishenjambment/
                            
                        
                    . Se ha utilizado esta herramienta para etiquetar un corpus de más de 4000 sonetos alojado y documentado en
                    
                        https://github.com/postdataproject/disco
                    .
                
                El código de la herramienta de detección de encabalgamientos está disponible en 
                    https://bitbucket.org/pruizf/anja_public/.
                
            
            
                Interfaz gráfica de usuario
                ANJA es una interfaz pública y gratuita, alojada en: 
                    http://prf1.org/anja/index/. Permite cargar los poemas que el usuario decida y analizarlos en el momento. También ofrece la carga de archivos ZIP que contengan archivos en texto plano.
                
                La interfaz de usuario está construida con el framework Django (Python), con las plantillas de Bootstrap 3. Las vistas de Django se llaman con AJAX para poblar los elementos de la UI. Para el análisis de PLN, Django accede a servicios web Java (IXA Pipes) implementados en nuestro servidor.
                ANJA presenta dos ventanas de navegación (Fig. 1), la principal, para introducir poemas, a la derecha y, a la izquierda, una mínima guía de uso que explica su funcionamiento y enlaza a la web del proyecto.
                
                    
                    Captura de ANJA
                
                Los resultados se ofrecen dos formatos: 
                    Standoff (tipo de encabalgamiento y línea), e 
                    Inline (etiquetado gramatical y tipo de encabalgamiento por línea, ver Fig. 2 para 
                    Inline). Las anotaciones PLN en que se basa en sistema se ofrecen en las pestañas 
                    PosTags (etiquetas gramaticales) y 
                    FullNLP (pipeline completa). 
                
                El enlace 
                    legend
                    
                        
                            https://sites.google.com/site/spanishenjambment/legend
                        
                     da acceso a la leyenda que explica los tipos de encabalgamiento, las etiquetas gramaticales y otras convenciones de representación: 
                
                
                    
                    Anotaciones de encabalgamiento en formato 
                        Inline
                    
                
                La existencia de una aplicación web simple para la utilización esta herramienta la hace accesible para una gama mucho más amplia de usuarios.
            
        
        
            
                
                    Bibliografía
                    
                        Agerri, R., Bermudez, J. and Rigau, G. (2014). IXA pipeline: Efficient and Ready to Use Multilingual NLP tools. 
                        Proceedings of LREC 2014, the 9th International Language Resources and Evaluation Conference, vol. 2014. Reykjavik,Iceland, pp. 3823–3828 http://www.lrec-conf.org/proceedings/lrec2014/pdf/775_Paper.pdf (accessed 20 April 2017).
                        
                    
                    
                        Agirrezabal, M. (2017). Automatic Scansion of Poetry San Sebastián/Donosti: Universidad del País Vasco.
                    
                    
                        Birnbaum, D. J. and Thorsen, E. (2015). Markup and meter: Using XML tools to teach a computer to think about versification. 
                        Balisage: The Markup Conference http://www.balisage.net/Proceedings/vol15/print/Birnbaum01/BalisageVol15-Birnbaum01.html (accessed 22 April 2017).
                    
                    
                        Bobenhausen, K. and Hammerich, B. (2015). Métrique littéraire, métrique linguistique et métrique algorithmique de l’allemand mises en jeu dans le programme Metricalizer2. 
                        Langages(3): 67–88.
                    
                    
                        Delente, É. and Renault, R. (2015). Outils et métrique: un tour d’horizon. 
                        Langages(3): 5–22.
                    
                    
                        Domínguez Caparrós, J. (2000). 
                        Métrica Española. Madrid: Síntesis.
                    
                    
                        Gervás, P. (2000a). A Logic Programming Application for the Analysis of Spanish Verse. 
                        Computational Logic—CL 2000. Berlin: Springer Berlin Heidelberg, pp. 1330–44.
                    
                    
                        Gervás, P. (2000b). Wasp: Evaluation of different strategies for the automatic generation of spanish verse. 
                        Proceedings of the AISB-00 Symposium on Creative &amp; Cultural Aspects of AI. pp. 93–100 https://www.researchgate.net/profile/Pablo_Gervas/publication/228609235_Wasp_Evaluation_of_different_strategies_for_the_automatic_generation_of_spanish_verse/links/00b4952aada6407047000000.pdf (accessed 22 April 2017).
                    
                    
                        Gervás, P. (2015). Tightening the Constraints on Form and Content for an Existing Computer Poet. 
                        AISB Convention 2015 http://eprints.sim.ucm.es/37000/ (accessed 22 April 2017).
                    
                    
                        McCurdy, N., Srikumar, V. and Meyer, M. (2015). Rhymedesign: A tool for analyzing sonic devices in poetry. 
                        Proceedings of the Fourth Workshop on Computational Linguistics for Literature. pp. 12–22.
                    
                    
                        Mittmann, A. (2016). Escansão automática de versos em português. https://repositorio.ufsc.br/handle/123456789/175819.
                    
                    
                        Navarro-Colorado, B. (2017). A metrical scansion system for fixed-metre Spanish poetry. 
                        Digital Scholarship in the Humanities doi:10.1093/llc/fqx009. https://academic.oup.com/dsh/article-abstract/doi/10.1093/llc/fqx009/3064339/A-metrical-scansion-system-for-fixed-metre-Spanish (accessed 19 April 2017).
                    
                    
                        Plecháč, P. and Kolár, R. (2015). The Corpus of Czech Verse. 
                        Studia Metrica et Poetica, 
                        2(1): 107–118.
                    
                    
                        Quilis, A. (1964). 
                        Estructura Del Encabalgamiento En La Métrica Española. Consejo Superior de Investigaciones Científicas, patronato‘ Menéndez y Pelayo,’ Instituto‘ Miguel de Cervantes,’.
                    
                    
                        Ruiz Fabo, P., Bermúdez Sabel, H., Martínez Cantón, C. I., González-Blanco, E. and Navarro-Colorado, B. (2018). The Diachronic Spanish Sonnet Corpus (DISCO): TEI and Linked Open Data Encoding, Data Distribution and Metrical Findings. Humanidades Digitales 2018 (DH 2018). Ciudad de México, México. 
                    
                    
                        Ruiz Fabo, P., Bermúdez-Sabel, H., Martínez Cantón, C. I. and Calvo Tello, J. (2017). 
                        Diachronic Spanish Sonnet Corpus (DISCO). Madrid: UNED. Madrid https://doi.org/10.5281/zenodo.1012567.
                    
                    
                        Ruiz, P., Martínez Cantón, C., Poibeau, T. and González-Blanco, E. (2017). Enjambment Detection in a Large Diachronic Corpus of Spanish Sonnets. 
                        Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature. Association for Computational Linguistics, pp. 27–32.
                    
                    
                        Tucker, H. F. (2011). Poetic data and the news from poems: A for better for verse memoir. 
                        Victorian Poetry, 
                        49(2): 267–281.
                    
                
            
        
    
6443	2018	
        
            Software development is often an integral aspect of Digital Humanities projects.  By working to generalize and build small modules or utilities targeting specific needs rather than large-scale systems, DH software developers have the capacity to generate tools with greater potential for scholarly reuse, which should enable more rapid development on future projects, and allow developers to focus on innovative work. This poster demonstrates a case study of modular software developed as part of ongoing DH projects.
            There is a tendency among some institutions, particularly libraries, to adopt existing large-scale Open Source Software solutions and adapt them for local needs; but as Hector Correa points out, this approach results in skipping the work of thinking carefully about users and local needs (Correa, 2017).  If large-scale software solutions developed by coalitions of libraries are problematic (Princeton University Library Systems, 2017) where needs are at least similar, even where content structures or workflows differ, this problem is redoubled for research software, which is much more likely bespoke to a particular problem.  As Correa argues, single-purpose software is less complex and easier to understand and manage; and understanding the logic of code is crucial for research that is based on or otherwise makes use of software (Koeser, 2015).
            Applying best practices from software development such as modular design can mitigate these problems through an emphasis on delivering working components of software and focusing on simplicity of purpose—a single, well-honed and balanced knife rather than a multi-tool with every imaginable attachment.  This approach is consistent with the design philosophy from one of the greatest success stories of modern open-source software, UNIX and its derivatives (Raymond, 2003).
            There are certainly possible drawbacks and concerns about this approach.  It may require more effort, and perhaps different skills, to create, release, and manage independent software packages or modules.  According to Glass’ 
                Facts and Fallacies of Software Engineering, it is “three times as difficult to build reusable components as single use components” (Glass, 2003: 49). In our case, when new software modules were being developed and extended in tandem with an existing software project, finalizing a new release of that project involved releasing and publishing multiple software modules.  There is also a danger of generalizing too soon; another familiar rule of thumb in software is that you have to do something three times before you know how to generalize it properly (Glass, 2003).
            
            As a case study, our poster will present an overview of the software written for two annotation projects that were developed at the same time. “Derrida’s Margins” analyzes the work of Jacques Derrida through references in 
                De la grammatologie and corresponding annotations in the books he cited. “The Winthrop Family on the Page” examines a community of readers connected through books over time via annotations.  This software ecosystem includes two project codebases (Koeser et al., 2018; Koeser and Hicks, 2018a) that make use of four new reusable components (Koeser and Hicks, 2018b; Koeser, 2018b), two of which (Koeser, 2018a; Koeser and Hicks, 2018c) were adapted from the “Readux” codebase (Koeser et al., 2017), which was previous developed at Emory University. In the process, we also used and made minor updates to a related, pre-existing module (Koeser, 2018c).
            
            For each of these tools, a use case emerged in one project which could be generalized for other projects, with potential for broader reuse. As an example, “viapy”—a Python module for searching and providing VIAF data to a web framework—was adapted from previous work, and first existed as code for one of the annotation projects, but it proved generalizable.  In fact, it proved easier to extract as a reusable component rather than duplicate; one project team discovered a bug that had previously gone undetected, and creating a reusable package allowed us to correct the problem once for both projects. Likewise, code for storing and displaying annotations from the Readux project was ripe for repackaging as a general module because of its relatively direct purpose despite the different intellectual aims of these projects. However, these codebases also contain similar, potentially reusable functionality that is not yet ready for generalization. 
            These projects provide a view into the ongoing process of balancing customized solutions to DH projects with generalizing focused portions of functionality. Modular design aimed at ‘doing one thing and doing it well’ offers the possibility of creating an ecosystem of reusable packages that are widely useful and applicable, and can participate in a larger community of open source and other DH software research.
        
        
            
                
                    Bibliography
                    
                        
                        Correa, H. (2017). Build your own software 
                        Hector Correa http://hectorcorrea.com/blog/build-your-own-software/70 (accessed 28 November 2017).
                    
                    
                        Glass, R. L. (2003). 
                        Facts and Fallacies of Software Engineering. Addison-Wesley Professional.
                    
                    
                        Koeser, R. S. (2015). Trusting Others to ‘Do the Math’. 
                        Interdisciplinary Science Reviews, 
                        40(4): 376–92 doi:10.1080/03080188.2016.1165454. http://dx.doi.org/10.1080/03080188.2016.1165454 (accessed 29 June 2016).
                    
                    
                        Koeser, R. S. (2018a). 
                        Django-Annotator-Store: Django Application to Act as an Annotator.js 2.x Annotator-Store Backend. Python Center for Digital Humanities at Princeton https://github.com/Princeton-CDH/django-annotator-store.
                    
                    
                        Koeser, R. S. (2018b). 
                        Viapy: VIAF via Python. Python Center for Digital Humanities at Princeton https://github.com/Princeton-CDH/viapy.
                    
                    
                        Koeser, R. S., Glover, K., Li, Y., Varner, J. and Thomas, A. (2017). 
                        Readux: Django Web Application to Display, Annotate, and Export Digitized Books in a Fedora Commons Repository. JavaScript Emory Center for Digital Scholarship https://github.com/ecds/readux.
                    
                    
                        Koeser, R. S. and Hicks, B. W. (2018a). 
                        Winthrop-Django: Django Web Application for the Winthrop Family on the Page Project. Python Center for Digital Humanities at Princeton https://github.com/Princeton-CDH/winthrop-django.
                    
                    
                        Koeser, R. S. and Hicks, B. W. (2018b). 
                        Django-Pucas: Django App to Streamline CAS Auth and Populate User Attributes from LDAP. Python Center for Digital Humanities at Princeton https://github.com/Princeton-CDH/django-pucas.
                    
                    
                        Koeser, R. S. and Hicks, B. W. (2018c). 
                        Djiffy: Django Application to Index and Display IIIF Manifests for Books. Python Center for Digital Humanities at Princeton https://github.com/Princeton-CDH/djiffy.
                    
                    
                        Koeser, R. S., Hicks, B. W., Glover, K. and Budak, N. (2018). 
                        Derrida-Django: Django Web Application for Derrida’s Margins. Python Center for Digital Humanities at Princeton https://github.com/Princeton-CDH/derrida-django.
                    
                    
                        Koeser, R. S. (2018). 
                        Piffle: Python Library for Generating and Parsing IIIF Image API URLs. Python Center for Digital Humanities at Princeton https://github.com/Princeton-CDH/piffle.
                    
                    
                        Princeton University Library Systems (2017). Valkyrie 
                        Princeton University Library Systems by Pulibrary https://pulibrary.github.io/2017-07-06-valkyrie (accessed 28 November 2017).
                    
                    
                        Raymond, E. S. (2003). 
                        Art of Unix Programming, The. Addison-Wesley Professional http://proquest.safaribooksonline.com/book/operating-systems-and-server-administration/unix/0131429019.
                    
                
            
        
    
6455	2018	
        
            A recent article has propped up the strawman that is the scientism of Digital Humanities (Brennan, 2017). This is not the first time that DH has been called out in terms of its supposed lack of success as a science (Allington, Brouillette, Golumbia, 2016). To be sure, scientific achievements are wonderfully measurable; they can be measured in terms of a year of progress – or ten years or the last 100 years – are marked by notions of societal impact. What has science done for us lately? We ask, as we point to discoveries and solutions. Likewise, a reputable humanities scholar asks, what has DH accomplished with all the money it has been bestowed? DH hasn’t “reveal[ed] the secrets 
                of complex social and cultural processes” and perhaps it is has not even made one of its central cases well since the author finds activities such as the “digitization, classification, description and metadata, organization, and navigation” of our cultural artifacts to be uncritical, apolitical work; rather, it is “a list, which leaves out that contradictory and negating quality of what is normally called “thinking” (Brennan, 2017). Yet, while these critics posit DH as a neoliberal pursuit to change higher education and thus the liberal arts into an education-for-hire endeavor, those who are the loudest critics of such a DH tend to measure DH’s “success” according to a pay-for-hire scale. We paid for it, they seem to be saying, What did we get?
            
            In marked contrast, DH scholars have long maintained “that scientific method and metaphor is, for the most part, incompatible with the terms of humanistic endeavor” (Ramsay, 2011; Drucker 2012; Binder 2016; Witmore 2016). To be sure, what have the 
                Humanities “accomplished” in the last decade, digital or no? Have the Humanities revealed The Secrets? As humanists, we tend to work towards fissures and fractional 
                tectonic shifts that resut in longer, slower, more nuanced and, in many cases, immeasurable impacts. For many of us involved in DH scholarship, singular accomplishments and “success” measured in terms of “done and out” or “problem solved” are not typically our goals.
            
            This panel interrogates the polarities that remain present in the perceived differences between the proposed scientism of the “digital” on the one hand and the “humanities” on the other by discussing a current trend in DH towards establishing “digital humanities labs.” Often, this oxymoronic title points to spaces of seemingly unscientific goings-on, of small doings, little happenings, and turtle-paced epistemological shiftings at the level of, and articulated through, infrastructure development. In the DH lab, “infrastructure” is understood as a “socio-technical phenomenon that enforces constraints on human experience at the same scale, complexity, and general cultural impact as the idea of ‘culture’ itself (Liu, 2016). This role of the DH lab has its roots in feminist inquiry by imbuing DH with science only in the sense of the “science question”, which considers the politics underlying epistemologies of “purportedly value-neutral claims and practices” [
                
                    Harding 1986
                , 23] and resonates with the work (the research, theory, and practices) being done to build information infrastructure in the humanities today. Ultimately, this panel situates “lab work” in DH 
                as a site for humanistic rather than scientific work, as a site for interrogating what it means to be a “
                lab”; for generativity, legibility, and creativity; for exploring what is engendered by 
                ad hoc arrangements, small scale problems, 
                and low tech tools; and for considering how the co-construction of knowledge with stakeholders and community members can introduce 
                participants to the theoretical, practical, and political implications of considering collections as data in the humanities. 
            
            
                Framing Potential
            
            
                Thomas Padilla, Visiting Digital Research Services Librarian, University of Nevada Las Vegas
            
            
                Collections as data represents a mode of engagement that aims to surface the data driven potential of digitized and born digital library, museum, and archival collections. In the United States the Institute for Museum and Library Services supported 
                Always Already Computational: Collections as Data
                 project and the Library of Congress’
                Collections as Data 
                events represent significant development initiatives that extend the aim of contemporary efforts like the Hathitrust Research Center into a more diversified set of institutional contexts. In Europe, related projects like DARIAH run apace. Increasingly, the collections as data concept is bolstered by attempts to play out the implications in theory and practice. Praxis in action spans emergent research library initiatives, national library initiatives, museum initiatives, cultural heritage position formation, and  Information Science and Digital Humanities curricula development. It stands to reason that labs, including but not limited to those operating in the Humanities, can benefit from partnering on collections as data efforts. Collections as data provide malleable grounds for enhancing cross campus, cross institutional, and cross community partnerships that aim to support research, pedagogy, and civic engagement in a contemporary knowledge environment that shifts ever toward de facto digital knowledge creation. Possibilities in this space can be effectively pursued via resolution through three conceptual collections as data frames: 
                (1)
                 generativity - a question of meaning making capacity 
                (2) 
                legibility - a question of ability to convey provenance and possibility and 
                (3) 
                creativity - a question of the extent to which the effort provides the means or a context for empowered experimentation. 
            
            
                Surfacing the data driven potential of collections works toward the purpose of cultivating contemporary agency in a digital environment. Collections as data work within and beyond labs constitutes a social imperative. With predominantly smooth GUI and application driven commodification of digital environments it becomes more and more difficult to push past the surface to gain purchase on the subjective forces that shape the data that constitute the digital objects that are trafficked throughout them. For many within and outside of academia it is not readily apparent that a Word document is not just a document, a website is not just a projection on a screen, an image is not merely a surrogate, and a tweet is much more than 280 characters. Meanwhile the facility and concordant power to control these composite environments with their composite objects resides in the hands of those that take a data first, representation second mentality - namely corporations, governments, law enforcement agencies, and researchers that exhibit ethically questionable engagement with digital traces of life. The collections as data imperative entails cultivation of the means to help all members of society, across all classes and backgrounds, working within the academy and outside of it to engage critically with digital traces of human activity in the fullest manner possible, native to the complexity of their form, and critically attuned to the possibilities and perils that come with their use. In what follows collections as data frames will be used to evaluate salient data driven research, professional practice, and lab oriented efforts in order to support speculative development of what a collections as data oriented lab could be. 
            
            
                The Unbearable Open-endedness of ‘Lab: A Variantology
            
            
                Lori Emerson, Associate Professor of English and Intermedia Arts, Writing, and Performance and Director of the Media Archaeology Lab
            
            
                The second panelist will first discuss findings that she and her co-authors have accumulated in the course of writing 
                THE LAB BOOK: Situated Practices in Media Studies
                 (forthcoming from the University of Minnesota Press). The project investigates the history as well as the contemporary landscape of humanities-based media labs - including, of course, labs that openly identify as being engaged with the digital humanities - in terms of  situated practices. Part of the book’s documentation of the explosion of labs or lab-like entities around the world over the last decade or so includes a body of over sixty interviews with lab directors and denizens. As the third panelist will discuss, the interviews not only reveal profound variability in terms of these labs’ driving philosophy, funding structures, infrastructures, administration, and outputs; but they also clearly demonstrate how many of hese labs do not explicitly either embody or refute scientificity so much as they pursue 21st century humanities objectives (which could include anything from research into processes of subjectivation, agency and materiality in computational culture to the production of narratives, performances, games, and/or music) in a mode that openly both acknowledges and carefully situates research process as well as research products, the role of collaboration, and the influence of physical and virtual infrastructure. While, outside of higher education, “lab” can now refer to anything from a line of men’s grooming products to a department store display or even a company dedicated to pyschometric tracking, across the arts and humanities “lab” now has the potential to capture a remarkable array of methodically delineated and self-consciously documented entities for experimentation and collaboration.
            
            
                Panelist two also views 
                THE LAB BOOK
                 as an opportunity to position the Media Archaeology Lab (MAL) in the contemporary landscape of these aforementioned humanities/DH/media labs. Since 2009, when panelist two founded the MAL, the lab has become known as one that undoes many assumptions about what labs should be or do; unlike labs that are structured hierarchically and driven by a single person with a single vision, the MAL takes many shapes: it is an archive for original works of early digital art/literature along with their original platforms; it is an apparatus through which we come to understand a complex history of media and the consequences of that history; it is a site for artistic interventions, experiments, and projects; it is a flexible, fluid space for students and faculty from a range of disciplines to undertake practice-based research; it is a means by which graduate students come for hands-on training in fields ranging from digital humanities, literary studies, media studies and curatorial studies to community outreach and education. In other words, the MAL is an intervention in “labness” insofar as it is a place where, depending on your approach, you will find opportunities for research and teaching in myriad configurations as well as a host of other, less clearly defined activities made possible by a collection that is both object and tool.
            
            
                False Equivalencies: Addressing Scientific Positivism from a Feminist Digital Humanities Perspective
            
            
                Elizabeth Losh, Associate Professor, American Studies and English, William &amp; Mary
            
            
                The Equality Lab unites diverse cohorts of scholars, students, and community members working broadly on inequality research in the mid-Atlantic region of the United States. It provides space for digital humanities teams working on community-based archival projects in which descendents and fictive kin may be important stakeholders. For example, its current partners include the Lemon Project, which uncovers and reconciles histories of slavery and segregation in higher education, and the LGBTIQ Research Project, which works with gay, lesbian, and transgender community groups in Virginia. The Equality Lab also provides technical expertise and material support for a broad range of individual student projects by humanities Ph.D. candidates that represent a variety of disciplinary affiliations and theoretical interests in American studies, such as ethnic studies, disability studies, and  environmental humanities. 
            
            
                As a digital humanities initiative, the Equality Lab foregrounds the ways that “equality” as a concept may suggest mathematical and scientific equivalence and thus it partners with the campus center for geospatial analysis and the university’s data sciences initiative to assist in producing DH projects that foster what has been called “counter-data action” or “statactivism.” (For example, its regional partners have worked on data projects involving redlining in housing policy and police shootings of unarmed persons of color.) Yet the Equality Lab also tests, tinkers, and examines scientistic assumptions about measurability, rationality, and surveillance in digital humanities work by approaching equality as a process rather than a product. The Equality Lab also explores the possibility that other formulations (equity, inclusion, etc.) might be more valuable as descriptors for social justice work in the digital humanities than equality itself.  
            
            
                Although making decisions about specific digital scholarship authoring platforms -- such as Omeka, which the Equality Lab uses to facilitate information exchanges with similar projects -- may be important, the Equality Lab avoids an exclusively tool-centric approach and validates the importance of what Christine Borgman has called “little data” as well as “big data.” In creating a multi-functional digital humanities lab space, the Equality Lab adopts perspectives from feminist science and technology studies and its important work on lab culture, such as Adele Clarke and Joan Fujimora’s
                The Right Tools for the Job
                , which argues that “tools,” “jobs,” and “rightness” are all situational and may reflect the specific contexts of ad hoc arrangements, doable problems, and disciplining tools. This feminist STS approach also validates craftwork and tacit knowledge practices as integral to digital humanities work, just as they are central to the labor of more traditional types of laboratories. The Equality Lab often builds on small-scale and relatively low-tech digital humanities interventions, such as a Wikistorming project with FemTechNet or programming bootcamps in Python or Processing with campus computer scientists.
            
            
                The Equality Lab emphasizes the importance of working with communities as sites of the co-construction of knowledge to build trust, acknowledge expectations of reciprocity, and give appropriate credit for contributions, as exemplified in its recent three-day symposium on Race, Memory, and the Digital Humanities that featured digital humanities innovators like Jessica Marie Johnson, Gabrielle Foreman, and Marisa Parham. 
            
            Data as products of human intentions and world views
            
                Tanya Clement, Associate Professor, School of Information, UT Austin
                
            
            
                While collecting institutions at UT Austin such as the Harry Ransom Center (HRC), the LLILAS Benson Latin American Studies and Collections Library, and the Perry Castañeda Library (PCL) all have large repositories of images, audio-visual materials, and text around which UT researchers can conduct scholarly research, basic research around theories and practices that consider how we should prepare, provision, and support the use of these collections as data remains largely uninitiated. At UT Austin, we are conceiving of DH labs as providing new points of access to “collections as data” while also serving as invaluable opportunities for basic research on the very nature of humanities objects and the systems that circulate and represent them to us, now, in the past, and in the future. DH Labs can provide secure and scalable access to digitized and born-digital UT collections as data and support programming but DH labs can also introduce participants to the theoretical and practical implications of considering collections as data.
            
            
                There are three particular “proof of concept” areas where we are focusing our intervention.
            
            
                Ethics and Post-custodial Archives
                : In working with the Guatemalan National Police Archives, LLILAS Benson has made post-custodial archival development and digital scholarship strategic goals its institutional mission. In the post-custodial realm, the physical collection stays with the creator but digital collections can be accessed from other custodians, such as the Benson. The theoretical and political ramifications of this very sensitive work is heightened in the context of human rights documentation initiatives. To further these efforts, LLILAS Benson has been able to secure a Mellon grant to expand its digital holdings and international partnerships, and to establish a dedicated endowment to support digital scholarship initiatives, including a Digital Scholarship in the Americas Speaker Series, workshops, internships, and fellowships for UT and international faculty, graduate, and undergraduate students.
            
            
                Sound Studies and Audio Preservation and Access
                : The HRC is home to world-class collections of images and audiovisual materials. In particular, recordings in the collection belonged to some of the 20th and 21st century’s most notable writers, artists, and performers. As of January 2017, there are 14,682 audio recordings cataloged in the HRC’s Sound Recordings Collection database; of these, 3,226 have been digitized and are available streaming onsite in the Reading and Viewing Room. These are often unique and rare non-commercial recordings often made for private use. A DH Lab with sound analysis technology will provide scholars with an opportunity to reflect on issues of infrastructure development--including data modeling and management; systems for security, integrity, and privacy; and the use of big data and machine learning algorithms—in the context of literary audio scholarship.
            
            
                Design Thinking and Object Cataloguing
                : The Alexander Architectural Archives, The Benson Latin American Collection, the Fine Arts Library, the Blanton Museum of Art, and the HRC all seek to modernize and coordinate their cataloguing processes in the visual arts and to implement shared technologies in order to improve efficiencies, leverage economies of scale, and promote the discovery and use of collections across campus. Structured Design Thinking workshops at the DH Lab at the Perry-Castañeda Library (PCL) will focus on cataloguing methodologies that incorporate multiple internal and external stakeholders; on evaluating available technology platforms; and considering current best practices in cataloguing, universal design, and user experience. A test corpus would include collections data for at least 500 objects-- an encyclopedic collection of approximately 2,000 works with particular depth in Western European art from the fourteenth through twentieth centuries and modern and contemporary art of the Americas from the Blanton and the Gernsheim Collection of approximately 35,000 images amassed by photo-historians Helmut and Alison Gernsheim between 1945 and 1963 currently held at the HRC. Both collections have little-to-no object-level cataloguing, making them ideal subjects for the project for building theory in design thinking and human computer interaction using collections as data.
            
            Data are products of human intentions and world views. Empowering humanities scholars and students to better understand and to act with data entails building an ecosystem that provides opportunities for recognizing, interpreting, and acting upon the affordances of the cultural heritage artifacts that humanists have always studied refigured as data. “Labs” can provide opportunities for basic research in the organization, preservation, curation, analysis, representation, interpretation, and communication of data as well as in the digital tools and platforms, the publishing and open access models, and the social and technological systems these activities afford and inhibit. 
        
        
            
                
                    Bibliography
                    
                        Allington, D., Brouillette, S., Golumbia, D. (2016). Neoliberal Tools (and Archives): A Political History of Digital Humanities. 
                        LA Review of Books.
                    
                    
                        Brennan, T. (2017). The Digital-Humanities Bust. 
                        The Chronicle of Higher Education
                        https://www.chronicle.com/article/The-Digital-Humanities-Bust/241424
                        (accessed 4 May 2018).
                    
                    
                        Binder, J. M. (2016). Alien Reading: Text Mining, Language Standardization, and the
                    
                    Humanities. In 
                        Debates in the Digital Humanities, edited by Matthew K. Gold and Lauren
                    
                    Klein. Minneapolis: University Of Minnesota Press.
                    
                        Borgman, C. L. (2016). 
                        Big Data, Little Data, No Data: Scholarship in the Networked World. Cambridge, MA: The MIT Press.
                    
                    
                        Bruno, I., Didier, E., and Vitale, T. (2014). 
                        Statactivism: Forms of Action between Disclosure and Affirmation. SSRN Scholarly Paper, ID 2466882, Social Science Research Network, 
                        papers.ssrn.com, 
                        
                            https://papers.ssrn.com/abstract=246688
                        
                         (accessed 4 May 2018).
                    
                    
                        Clarke, A. and Fujimora, J. (1992) 
                        The Right Tools for the Job. Princeton: Princeton University Press.
                    
                    
                        Currie, M., Paris, B.S., Pasquetto, I., and Pierre, J. (2016). The Conundrum of Police Officer-Involved Homicides: Counter-Data in Los Angeles County.” 
                        Big Data &amp; Society 3(2) 
                        
                            http://journals.sagepub.com/doi/pdf/10.1177/2053951716663566
                        
                        (accessed 4 May 2018).
                    
                    
                        Drucker, J. (2012). Humanistic Theory and Digital Scholarship. In 
                        Debates in the Digital Humanities, edited by Matthew K. Gold. Minneapolis: University Of Minnesota Press.
                    
                    
                        Harding, S. G
                        . (1986).
                        The Science Question in Feminism
                        . Ithaca: Cornell University Press.
                    
                    
                        Liu, A. “Drafts for Against the Cultural Singularity” Alan Liu. 2 May 2016.
                    
                    Witmore, Michael. 2016. “Latour, the Digital Humanities, and the Divided Kingdom of Knowledge.” 
                        New Literary History 47 (2): 353–75.
                    
                    
                        Padilla, T. (2016) 
                        On a Collections as Data Imperative. Library of Congress. 
                        http://digitalpreservation.gov/meetings/dcs16.html
                        (accessed 4 May 2018).
                    
                
            
        
    
6457	2018	
        
            
                Overview
                Visual culture is often overlooked as an object of study within the digital humanities (DH). Yet, visual culture is central to fields such as art history, cultural studies, history, and media studies.  Cultural forms such as drawings, film, video, painting, photography and drawing continue to shapes culture, politics and society. As the field begins to turns its attention to other forms of media, new methods and tools are necessary to study visual culture at scale. Recent advances in computer vision are proving a promising direction. The panel "Computer Vision in the Digital Humanities" will present three approaches to using computer vision in the Digital Humanities (DH).
                "Distant Viewing: Analyzing Moving Images at Scale" argues that computer vision is a powerful tool for distant viewing time-based media. The authors will outline their method and then describe the Distant Viewing Toolkit, a set of machine learning computer vision algorithms for analyzing features such as color, shot and scene breaks, and object identification such as faces. They will then turn to a case study of the American Network Era (1952-1984) television to show how their method reveals the representational politics of gender during the era.  
                “Seeing History: Analyzing Large-Scale Historical Visual Datasets Using Deep Neural Networks” will then focus in on how convolutional neural networks (CNN) can be used for historical research. The authors focus on two case studies applied to two major Dutch national newspapers. They used CNNs to identify over 400,000 advertisements from 1945-1995 in the first study and  over 110,000 photographs and drawings from 1860 to 1920 in the second study. They then will explain the two tools they developed to support visual and textual search in the new corpuses. 
                Finally, The Media Ecology Project and Visual Learning Group are creating software that allows people to search in untagged films and videos in the same way that they search through the text of a document. The tool takes search queries expressed in textual form and automatically translates them into image recognition models that can identify the desired segments in the film. The image recognition results can be cached for quick searching.  Initial prototype results, funded by The Knight Foundation, focused on educational films at The Dartmouth Library and The Internet Archive that are common to many libraries and archives.
                All of the papers will address the need to develop open access historical humanities data sets for developing computer vision as a DH technique.  
            
            
                Distant Viewing: Analyzing Moving Images at Scale
                Digital humanities' (DH) focus on text and related methodologies such as distant reading and macroanalysis has produced exciting interventions (Jockers 2013; Moretti 2013).  However, there is an increasing call to take seriously visual culture and moving images as objects of study in digital humanities (Posner 2013; Acland and Hoyt 2016; Manovich 2016; ADHO AVinDH Special Interest Group). In this paper, we will discuss how we are using computer vision and machine learning to distant view moving images.
                The paper will begin by outlining our method of distant viewing and then turn to our Distant Viewing toolkit. Using and developing machine learning algorithms, the toolkit analyzes the following features: (1) the dominant colors and lighting over each shot; (2) time codes for shot and scene breaks; (3) bounding boxes for faces and other common objects; (4) consistent identifiers and descriptors for scenes, faces, and objects over time; (5) time codes and descriptions of diegetic and non-diegetic sound; and (6) a transcript of the spoken dialogue (see Figures 1 &amp; 2 for examples of these analyses). These features serve as building blocks for analysis of moving images in the same way words are the foundation for text analysis. From these extracted elements, higher-level features such as camera movement, framing, blocking, and narrative style can then be derived and analyzed. These techniques then allow scholars to see content and style within and across moving images such as films, news broadcasts, and television episodes, revealing how moving images shape cultural norms.
                To illustrate this approach, we have applied our Distant Viewing toolkit to a collection of series from the Network Era (1952-1984) of American television. The Network Era is often considered formulaic and uninteresting from a formal perspective despite how highly influential this era of TV was on U.S culture (Spiegel 1992). Our analysis challenges this characterization using computational methods by showing how the formal elements of the sitcoms serve to reflect, establish, and challenge cultural norms. In particular, we will focus on the representational politics of gender during the Network Era.  For examples of how we are distant viewing TV, please see distanttv.org.
                
                    
                        
                        Shots detected by the Distant Viewing Toolkit on an episode of I Dream of Jeannie.
                    
                    
                        
                        Most similar faces to the reference character, upper left-most frame, from detected faces in a season of episodes from the series I Dream of Jeannie.
                    
                
            
            
                Seeing History: Analyzing Large-Scale Historical Visual Datasets Using Deep Neural Networks 
                Scholars are increasingly applying computational methods to analyze the visual aspects of large-scale digitized visual datasets (Ordelman et al., 2014). Inspiring examples are the work of Seguin (Seguin et al., 2017) on visual pattern discovery in large databases of paintings and Moretti’s and Impett’s (Moretti and Impett, 2017) large-scale analysis of body postures in Aby Warburg’s Atlas Mnemosyne. In our paper, we will present two datasets of historical images and accompanying texts harvested from Dutch digitized newspapers and reflect on ways to improve existing neural networks for historical research. We will discuss how large historical visual datasets can be used for historical research using neural networks. We will do this by describing two case studies, and will end our paper by arguing for the need for a benchmarked dataset with historical visual material.
                The sets were produced during two researcher-in-residence projects at the National Library of the Netherlands. The first set consists of more than 400,000 advertisements published in two major national newspapers between 1945 and 1995. Using the penultimate layer in a Convolutional Neural Network (CNN), 2,048 visual aspects were abstracted from these images, which can be used to group images together (Seguin et al., 2017). The second dataset includes about 110,000 classified images from newspapers published between 1860 and 1920. The images were classified using a pipeline that consists of three classifiers. The first one detects images with faces (Geitgey, 2017), the second categorizes images according to eight different categories (buildings, cartoons, chess problems, crowds, logos, schematics, sheet music, and weather reports), and the last one sorts images as either photographs or drawings (Donahue et al., 2013).
                We developed two tools to query these datasets. The first tool offers exploratory search in the advertisement dataset, which enables users to find images sharing a degree of visual similarity and can be used to detect visual trends in large visual datasets. The second one enables users to find images in the second set by searching for specific (combinations) of visual subjects and keywords. For example, images of ‘buildings’ with ‘faces’ and the keyword ‘protest’ in the text.
                Finally, our paper discusses several challenges and possibilities of computer vision techniques for historical research. Most CNN’s are trained on contemporary materials (ImageNet). As a result, these networks perform well in recognizing the categories of the ImageNet challenge. However, the fact that they were trained on contemporary data can cause problems when working with historical images. For example, detecting bicycles works relatively well because the design of the bicycle has remained more or less similar during the last century, while trains are much more difficult since they have changed significantly over the years. Also, models trained on ImageNet have difficulties detecting objects in illustrations, which are often used in newspapers. They are regularly classified within the uninformative category ‘cartoon.’ In short, we will discuss how to improve these models and argue for the development and benchmarking of datasets with visual historical material.
            
            
                Unlocking Film Libraries for Discovery and Search
                Where the library of the 20th century focused on texts, the 21st century library will be a rich mix of media, fully accessible to library patrons in digital form. Yet the tools that allow people to easily search film and video in the same way that they can search through the full text of a document are still beyond the reach of most libraries. How can we make the rich troves of film/video housed in thousands of libraries searchable and discoverable for the next generation?
                Dartmouth College’s Media Ecology Project, led by Prof. Mark Williams and architect John Bell, and the Visual Learning Group, led by Prof. Lorenzo Torresani, are applying computer vision and machine learning tools to a rich collection of films held by Dartmouth Library and the Internet Archive. Using existing algorithms, we describe what is happening and translate the resulting tags into open linked data using our Semantic Annotation Tool (SAT). SAT provides an easy-to-use and accessible interface for playing back time-based annotations (built upon W3C web annotation standards) in a web browser, allowing simple collection development that can be integrated with discovery and search in an exhibition. What was once a roll of film, indexed only by its card catalog description, will become searchable scene-by-scene, adding immense value for library patrons, scholars and the visually impaired.
                Dartmouth College’s Visual Learning Group is already a leader in computer vision and machine learning, developing new tools for object and action recognition. This project has brought together cross-curricular groups at Dartmouth to collaborate on applying modern artificial intelligence and machine learning to historic film collections held by libraries and archives.
                Our tool takes search queries expressed in textual form and automatically translates them into image recognition models that can identify the desired segments in the film. The entire search takes only a fraction of a second on a regular computer.  We have a working prototype of the search functionality and are creating a demonstration site that will be featured in the conference presentation. Our initial prototype results, funded by The Knight Foundation, focused on educational films at The Dartmouth Library and The Internet Archive that are common to many libraries and archives.  Our software leverages image recognition algorithms to enable content-based search in video and film collections housed in libraries. By utilizing The Semantic Annotation Tool, the project also works to bring together human- and machine-generated metadata into a single, searchable format.
                By improving the cutting edge algorithms used to create time-coded subject-tags (e.g. http://vlg.cs.dartmouth.edu/c3d/), we aim to lay the foundation for a fully-searchable visual encyclopedia and to share our methods and open source code with film libraries and archives everywhere. Our goal is to unlock the rich troves of film held by libraries and make them findable and more useable—scene by scene, and frame by frame--so future generations can discover new layers of meaning and impact.
            
        
        
            
                
                    Bibliography
                    
                        Acland, C. R. and Eric Hoyt
                        , Editors (2016).The Arclight Guidebook to Media History and the Digital Humanities. REFRAME Books.
                    
                    
                        Bertasius, G., Shi, J. and Torresani, L.
                         (2015) “High-for-Low and Low-for-High: Efficient Boundary Detection from Deep Object Features and its Applications to High-Level Vision,” in IEEE International Conference On Computer Vision, ICCV.
                    
                    
                        Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E. and Darrell, T.
                         (2013), “DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition”, ArXiv:1310.1531 [Cs], available at: http://arxiv.org/abs/1310.1531 (accessed 23 November 2017).
                    
                    
                        Geitgey, A.
                         (2017), Face_recognition: The World’s Simplest Facial Recognition Api for Python and the Command Line, Python, , available at: https://github.com/ageitgey/face_recognition (accessed 23 November 2017).
                    
                    
                        Hediger, V. and Vonderau, P.
                         (2009) Films that Work: Industrial Film and the Productivity of Media (Film Culture in Transition) Amsterdam: Amsterdam University Press.
                    
                    
                        Jockers, M. L. (2013).
                         Macroanalysis: Digital methods and literary history. University of Illinois Press, 2013.
                    
                    
                        Manovich, L. (2016).
                         “The Science of Culture? Social Computing, Digital Humanities, and Cultural Analytics”. The Datafied Society. Social Research in the Age of Big Data, edited by Mirko Tobias Schaefer and Karin van Es. Amsterdam University Press.
                    
                    
                        Moretti, F.
                         (2013) Distant Reading. Verso Books.
                    
                    
                        Moretti, F. and Impett, L. 
                        (2017), “Totentanz”, New Left Review, No. 107, pp. 68–97.
                    
                    
                        Ordelman, R., Kleppe, M., Kemman, M. and De Jong, F.
                         (2014), “Sound and (moving images) in focus – How to integrate audiovisual material in Digital Humanities research”, presented at the Digital Humanities 2014, Lausanne, available at: (accessed 15 November 2017).
                    
                    
                        Posner, M. (
                        2013). "Digital Humanities and Film and Media Studies: Staging an Encounter." Workshop della Society for Cinema and Media Studies Annual Conference, Chicago. Vol. 8.
                    
                    
                        Seguin, B., di Leonardo, I. and Kaplan, F. 
                        (2017), “Tracking Transmission of Details in Paintings”, presented at the Digital Humanities 2017, Montreal.
                    
                    
                        Spigel, L.
                         (1992). Make Room for TV: Television and the Family Ideal in Postwar America. University of Chicago Press.
                    
                    
                        Tran, D., Bourdev, L. , Fergus, L. , Torresani, L. and Paluri, M. 
                        (2015)
                        .
                         “Learning Spatiotemporal Features with 3D Convolutional Networks,” in IEEE International Conference On Computer Vision, ICCV.
                    
                    
                        Williams, M.
                         (2016). “Networking Moving Image History: Archives, Scholars, and the Media Ecology Project” in The Arclight Guidebook to Media History and the Digital Humanities, Charles R. Acland and Eric Hoyt, eds.
                    
                
            
        
    
6465	2018	
        
            
                Overview
                Pedagogical exercises in the digital humanities rely on student access to humanities data. While strategies range from instructor-prepared datasets (Sinclair and Rockwell, 2012) to having students digitize texts directly from print materials (Croxall, 2017), data repositories and web-based DH projects are two of the most attractive sources for identifying, appraising, and accessing data for classroom use.
                Yet data for teaching is rarely cited as a prime motivation or rationale for sharing research data. In “The Conundrum of Sharing Research Data,” Christine Borgman examines four rationales for sharing research data: (1) to reproduce or to verify research, (2) to make results of publicly funded research available to the public, (3) to enable others to ask new questions of extant data, and (4) to advance the state of research and innovation (2012). Pedagogy may be included implicitly in the third rationale, but by foregrounding pedagogical intentions, we can more readily operationalize a process for how we enable others to ask new questions of our data, which, in turn, will inform our motivations for sharing as well as the manner in which we do so.
                Web-based DH projects are often conceived and developed for public consumption with short-term support through grant funding. While initiatives such as these have proliferated since the 1990s, they often languish as legacy projects on institutional servers without clear plans for sustainability or sunsetting (Rockwell et al., 2014). Rather than construe long dormant projects as an institutional burden, these artifacts may continue to function as object lessons and raw materials for use in the DH classroom. Evaluating early digital projects based on their fitness for use as pedagogical datasets distinguishes the project from its component parts and allows aspects of the project to live on in new contexts.
                This panel will include representatives from five public research universities across the United States. We will begin with a brief overview, followed by four case studies. Each panelist will speak for fifteen to twenty minutes, leaving time for questions from—and conversations with—the audience. Cases are drawn from the DH 101 course at UCLA, the DH Librarianship course at the University of Washington, the University of Miami Libraries’ Legacy Site Adoption Project, and the Humanities Data workshop at DHOxSS. Our goal is to explore the intersection of data sharing and digital pedagogy to interrogate how past projects (whether formally archived or otherwise) are adopted as data sets for teaching and training; propose evaluation criteria for selecting these data sets; discuss what these classroom efforts indicate about the sustainability of DH projects (and their data); and examine how our knowledge of these classroom cases might inform curatorial decisions in active DH projects.
            
            
                Learning from our mistakes: Using old projects to create better library/faculty collaborations
                The Legacy Sites Adoption Project (LSAP) developed in response to what the library administration saw as a significant problem: the library website hosted nearly 40 digital projects built 5-20 years earlier by a former library faculty member, now malingering in various states of brokenness, but still placed prominently on the website. Retiring and removing the sites would erase the memory of the library’s institutional history, but repairing them would create an impossible burden for the web &amp; application development team; and would reinforce the idea of the library playing a service-and-support role in DH, rather than an active partnership.
                The solution that we are currently implementing is to experiment with making the legacy sites “adoptable”: the content and metadata of each site are made available as a zip file containing CSVs of data and metadata and accompanying images/audio/visual files, along with a readme pointing both to the current site on the library servers and an archived (and often more functional) version of the site in the Internet Archive’s Wayback Machine. Faculty and students are able to use the zip files as base material for creating their own version(s) of the original sites, either carrying on the original concept as stated or taking it in a new direction. The original versions of the sites present opportunities for classes to think about developing DH projects with a direct focus on revision -- potentially reading and critiquing the original sites through the lenses of recent scholarly essays, or considering the choices made by the original creators in the light of how DH practices and tools have changed since the sites were built.
                LSAP engages with ongoing questions about what makes a good entry point into digital humanities work. Instead of building entry points around particular tools (Omeka, Voyant, etc.); or around a particular research question or collection of material that is not a project yet, adopting legacy sites centers and foregrounds the iterative nature and inevitable fragility of project webpages, while making explicit the relationship between the websites and the flat files of their content. 
                With LSAP, we are also attempting a positive intervention into collaborative relationships between departmental faculty and librarians. Frequently, faculty come to librarians to ask for support for a particular idea for a digital project; or to incorporate digital methodologies into a classroom setting. In such instances, the faculty member may have little experience or knowledge with various key factors, including scoping and scaling project milestones, the availability of digitized objects, copyright/permissions restrictions, and the affordances of out-of-the-box tools. Our hope is that by offering projects that are ripe for revision, and focusing on areas that are frequently taught and studied at the university, we can provide an entry-point for collaboration that is more appropriately bounded, resulting in less uncertainty and less labor-intensive experiences for faculty, students, and librarians. 
            
            
                Awakening sleeping data for the DH classroom
                As anyone who teaches digital humanities knows, humanities-related datasets are as hard to find as they are desirable. Since the closure of the Arts and Humanities Data Service in 2008, no centralized repository for humanities data has emerged. The DH instructor is faced with the necessity of scouring the web for data to share with students so that they can practice data-cleaning, -manipulation, and -visualization. Sometimes this data comes from libraries, archives, and museums, but it comes just as often from scholars’ long-hibernating research projects. Indeed, scholars are often surprised to learn that their data has taken on a new life as the basis for student projects.
                The last several decades have seen explosive growth in flexible, accessible tools for working with data. These new platforms offer possibilities for visualization and analysis that would only have been possible with custom programming just 10 or 15 years ago. Because of this palette of tools, even relatively inexperienced students can breathe new life into data left mostly untouched for years.
                This presentation offers some case studies of student projects built on “dormant” data, explaining how students are trained to analyze, contextualize, visualize, and make sense of data they had no involvement in collecting. It discusses best practices for providing this data, as well as a scaffolded approach to helping students become conversant in techniques for understanding and working with data. It suggests a “toolkit” of off-the-shelf platforms that are affordable and easy for students to grasp and shows how one can build on the other until even novice students are able to create full-fledged, sophisticated digital humanities projects in the space of a semester.
                For those who have collected data they wish to share with students, this presentation offers some suggestions for documenting, packaging, and contextualizing research data so that it is not only technically sound, but in a format that students can understand. It also offers a set of best practices for collaborating with students on a data-based research project, including methods for sharing, documenting, citing, and reusing data.
            
            
                Fit for use: Repurposing research data, reconstructing provenance, and refining “clean” data
                When it comes to teaching materials, data curation education may have become a victim of its own success: finding “dirty” data for classroom use is persistently difficult, in part because most published datasets have already been cleaned and curated! However, there are teachable moments to be found even when working with relatively “clean” data. Published data can be mined, re-structured, re-formatted and otherwise curated for new uses. Additionally, the process of tracking down and contextualizing already published datasets can prove instructive in and of itself. The detective work needed to understand someone else’s project, and to reconstruct its provenance, can reveal unexpected idiosyncrasies about the dataset, and thereby reveal useful data wrangling skills to be taught.
                In this talk, we describe our work finding, curating and reconstructing the provenance of “The Pettigrew Papers,” a published (and relatively clean) dataset we have used over two years of teaching week-long workshops on digital humanities data curation at the Digital Humanities at Oxford Summer School (DHOxSS). Thomas J. Pettigrew (also known as Thomas “Mummy” Pettigrew) was a Victorian surgeon, antiquarian, and Egypotologist. Pettigrew wrote several early texts on Egyptian mummies and was the founding treasurer of the British Archaeological Association. Though his correspondence is archived at Yale University’s Beinecke Rare Book and Manuscript Library, it came to our attention via a “data paper” published in the Journal of Open Archaeology Data (Moshenska, 2012), containing transcriptions of select letters. 
                In our first year teaching with the Pettigrew dataset, we wrote simple Python scripts to mine named entities from the letters, and to pull out header information about the letters as a spreadsheet for cleaning in OpenRefine. In hands-on sessions, we asked students to consider how they would clean and curate the dataset for new uses: what steps would need to be taken to create a network diagram of the entities named in his letters? To create a map of his correspondents? To create a timeline? 
                In our second year teaching with this dataset, we spent more time reconstructing the original provenance of the Pettigrew letters themselves. In addition to the hands-on sessions from the first year, we asked students to consider how they might improve the metadata for the original data paper, and how they might resolve discrepancies between the data paper and the original finding aids created by the Beinecke (Ducharme, 2010). We additionally discussed how they might incorporate copies of Pettigrew’s publications available in the HathiTrust Digital Library in their work.
                Overall, we found that asking students to clean and re-curate this already published dataset was only the starting point in our teaching; as we found further connections in digital libraries and archives beyond the original data paper, we identified subtle and important issues in the digital humanities and digital curation that guided our workshop design. In addition to teaching hands-on data cleaning and manipulation skills, we found it important to teach students a nuanced understanding of provenance: both in the sense of the archival “chain of custody” that contextualizes and validates a fonds, and in the sense of the processes that led to a dataset’s current form.
            
            
                Training DH librarians: Using old DH projects to move forward
                The DH Librarianship course at the University of Washington Information School investigates the multiple roles librarians play in DH scholarship and prepares students for a wide range of career options in libraries, DH centers, and academic departments. DH librarian roles range from fully-credited collaborator with faculty to last-minute data cleaner, and everything in between. DH librarians also need to be prepared to support projects and research across the spectrum of disciplines, so we examine varying research methods across the humanities. The final project for the course asks that students locate an abandoned, or complete but aging DH project, and insert themselves as a librarian; they provide an evaluation of the content as well as the technology of the project and suggest ways to improve or update both.
                The data sets in these projects varies and examples include: hand-collated quotations by a famous author on a fan site; census numbers provided in a project about London families in the 17th century; a list of shooting locations for a television show; metadata for photos of logging camps in the Pacific Northwest; multimedia elements in a documentary film; boxes of music programs from a summer camp; and quilting patterns. 
                Some projects also include the more typical (and larger) type of data set, such as those from HathiTrust or Google-generated Ngrams, but they have proven to be the exception. Working with small data sets means that cleaning doesn’t occupy much time during a 10-week quarter, and they can be rearranged quickly to utilize multiple visualization or data processing options.
                Students evaluate the data sets early in the process; in nearly all cases, data sets are either incomplete or inaccurate, and for some, updated data or other content is available. This is where the multi-disciplinary expertise of librarians comes in, as MLIS students are trained in searching out valid information sources from multiple perspectives, whether that’s using vendor-supplied databases, open web search engines, or (gasp) sources in print or microform. This is also where students begin to see the striation of roles between true collaborators, project leaders, subject specialists, technical consultants, or data-wranglers.
                In reviewing aging or abandoned projects, students learn how easily the data, other content, and the functionality of the site/project can be lost. This gives them the added perspective they need to start thinking about curation and preservation, rather than tackling those issues as add-ons if they have time.
                Through these immersive projects, students have a chance to see DH through multiple lenses: those of a potential user, a collaborator, and a disciplinary specialist. They learn how to re-create and improve on a project. In doing so, they gain experience in evaluating and collecting data as well as in multiple platforms and software that are prominent in DH (some current, some defunct). Some students also reach out to the original site or project owner, and in a few cases have worked with that person to update the project, putting preservation or stabilizing features in place for future users.
            
        
        
            
                
                    Bibliography
                    
                        Borgman, C. L. (2012). The conundrum of sharing research data, 
                        Journal of the Association for Information Science and Technology, 63(6): 1059-78.
                    
                    
                        Croxall, B. (2017). Digital humanities from scratch: A pedagogy-driven investigation of an in-copyright corpus
                        , Digital Humanities 2017: Conference Abstracts, Montreal: McGill University, pp. 206-7.
                    
                    
                        Ducharme, D. J. (2010). 
                        Guide to the Pettigrew Papers OSB MSS 113. New Haven: Beinecke Rare Book and Manuscript Library. 
                        
                            http://hdl.handle.net/10079/fa/beinecke.pettis1
                         (accessed 27 April 2018).
                    
                    
                        Moshenska, G. (2012). Selected correspondence from the papers of Thomas Pettigrew (1791-1865), surgeon and antiquary. 
                        Journal of Open Archaeology Data, 1(0). 
                        
                            https://doi.org/10.5334/4f913ca0cbb89
                         (accessed 27 April 2018).
                    
                    
                        Rockwell, G., Day, S., Yu., J., and Engel, M. (2014). Burying dead projects: depositing the Globalization Compendium. 
                        Digital Humanities Quarterly, 8(2). Retrieved from 
                        
                            http://www.digitalhumanities.org/dhq/vol/8/2/000179/000179.html
                         (accessed 27 April 2018).
                    
                    
                        Sinclair, S. and Rockwell, G. (2012). Teaching computer-assisted text analysis. In Hirsch, B. (ed) 
                        Digital Humanities Pedagogy: Practices, Principles, Politics. Open Book Publishers, pp. 241-54. Retrieved from 
                        
                            https://www.openbookpublishers.com/product.php/161
                         (accessed 27 April 2018).
                    
                
            
        
    
6466	2018	
        
            Overview/Panel Abstract
             First developed in 2013, the Praxis Network (praxis-network.org) brought attention to the ways in which digital humanities was being used to rethink the nature of student training, campus partnerships, and pedagogy. The institutions profiled in the project aimed to reorient student training towards new, collaborative practices that would prepare students for forward-looking scholarship and meaningful careers in the humanities. Five years later, at DH 2018, we propose to reflect on these efforts, to assess the current state of digital humanities training and its relationship to and effects on praxis-oriented pedagogy.
             The participants assembled for this roundtable session represent both the past and the future of these efforts to join theory and practice, classroom and public. Part of the roundtable will consist of participants that were part of the original Praxis Network, longstanding presences on their local campuses whose architects will report on the institutional challenges and successes they have faced in the five years since their profiling in the Network. The University of Virginia’s Praxis Program Fellowship, for example, annually engages student cohorts in the development of collaborative research projects, but the program’s success exposes limitations offered by this model even given attempts to expand it to a wider community with regional partners. Similarly, a representative from Michigan State University will discuss attempts to expand the collaborative nature of student work while also turning the projects increasingly towards the public. And the CUNY Graduate Center will share recent efforts by their digital fellows program to develop weeklong institutes in digital methods both locally and nationally.
             The remaining institutions on the roundtable all represent new, like-minded initiatives with unique constituencies and concerns. Many of our programs encourage public-facing scholarship, and a representative from Brown University’s Center for Public Humanities and Cultural Heritage will argue for experiential learning in the digital humanities in a public humanities context. The Five College Digital Humanities and Blended Learning Initiative offers lessons in how to address resistance to incorporating praxis-oriented methodologies in the classroom. Alt.code, an initiative funded by the National Endowment for the Humanities at Renssealaer Polytechnic Institute, offers a model for joining the instruction of technical skills with critical perspectives on technology as linked parts of the same epistemic domain. Finally, a trio of scholars from the Universidad Nacional Autónoma de México and Tecnológico de Monterrey will speak to the difficulties they have encountered developing pedagogical and institutional initiatives based in digital humanities in Mexico.
             This roundtable thus hopes to draw participants new and old into a conversation about methodological training, which necessarily must become inflected differently in diverse local, institutional contexts. Collectively, we argue for a pedagogy that is public, collaborative, and that centers the student. In the spirit of the original Praxis Network, we hope that this collection of programs will offer models, lessons, and cautionary tales. Looking to the future, we hope that the roundtable will start new conversations and seed new ideas. 
            Abstract 1
            Michigan State University’s Cultural Heritage Informatics Initiative: Methods and Models for Building Capacity in Digital Cultural Heritage
            Ethan Watrall, Michigan State University, United States of America
             As with many other domains, cultural heritage has entered a new age in which digital methods and computational approaches are having an unavoidable impact on research, teaching, preservation. public engagement, and all aspects of scholarly communication. The problem is that cultural heritage scholars and professionals who have not traditionally characterized themselves as being particularly digitally inclined are increasingly being asked to engage with issues, methods, models, and practices that are uniquely digital in nature. Unfortunately, while the need for innovative digital praxis exists, we are only starting to establish methods and models to build vital digital capacity among undergraduates, graduate students, and existing professionals and scholars.
             It is within this context that this talk will explore Michigan State University’s Cultural Heritage Informatics Initiative (chi.anthropology.msu.edu). Founded in 2010 and administered by the Michigan State University Department of Anthropology in partnership MATRIX: The Center for Digital Humanities &amp; Social Sciences and the Lab for the Education and Advancement in Digital Research (LEADR), the Cultural Heritage Informatics Initiative was originally conceived as having two primary goals. First, it was intended to serve as a platform for interdisciplinary scholarly collaboration and communication in the domain of digital cultural heritage practice at Michigan State University. Second, it was intended to equip students and professionals with the skills to apply digital methods and computational approaches to cultural heritage materials and questions. Despite these two initial goals, the initiative has shifted over the years to focus almost exclusively on the second, providing cultural heritage students and professionals with an opportunity and environment to learn and build digital skills. The two most tangible expression of this goal are the Cultural Heritage Informatics Graduate Fellowship Program and the Digital Heritage Fieldschool. 
             The intention of this talk is to reflect upon the challenges the initiative has faced since it was founded, exploring successes and failures, and look forward as we move towards a decade of building capacity and community in digital heritage at Michigan State University and beyond.
            Abstract 2
            HD pedagogy and praxis in Mexico: practices, platforms, institutions
            Miriam Peña Pimentel, Universidad Nacional Autónoma de México, Mexico
            Adriana Álvarez Sánchez, Universidad Nacional Autónoma de México, Mexico
            Paola Ricaurte Quijano, Tecnológico de Monterrey, Mexico
             In Mexico as elsewhere, universities differ in their human and financial resources, their physical, technological and administrative infrastructures, their educational models and their programs, among other variables. This multiplicity of conditions frame the dynamics and the spaces for knowledge production and student training. Within this heterogeneous institutional context, Digital Humanities are building an emerging field where different initiatives have been developed. Digital academic publications; DH events; activities in support of the open access movement (MOOC, data, digital libraries, Wikipedia); the design of new courses and DH projects; and the creation of networks and labs, are part of our strategy to consolidate the field. In our experience, implementing a DH curriculum in a higher education setting in Mexico will never be an easy task. We find multiple elements in the institutional structures that hinder the development of DH teaching and research. The training of professionals and the development of digital projects face a series of problems related to bad administrative decisions and lack of vision of DH as a field. There is not enough political will to include DH courses and methodologies within the curriculum of undergraduate programs or to create new DH graduate programs. There is resistance to support DH projects and platforms.  At the praxis level, we are trying to subvert the traditional pedagogical model and developing alternatives to learn in situated contexts and with different methodological and technological tools. Several questions arise from this experience: Is our DH pedagogy embodied and embedded in its sociocultural context? How is DH pedagogy in Mexico different from the DH pedagogy elsewhere? What does praxis mean in our local scenario? What do we bring to the reflection of DH pedagogy and praxis? This paper aims to reflect on these questions and describes two cases of DH pedagogy and praxis in two higher education institutions, UNAM and Tecnológico de Monterrey. We describe the purpose and methodologies of two DH labs: e-labora and Openlabs. We believe that these initiatives are trying to approach the question of how to teach and learn DH in our local setting. Labs have three obvious repercussions: a) promote the appropriation of new methodologies and the reformulation of the disciplines involved; b) allow participants to become involved with their environment by applying what they learn to the solution of everyday situations, and c) create favorable conditions for the development of multiple competencies.
            Abstract 3
            Alt.code: DH as Reconfiguration of the Boundaries of Computer Science Education
            James Malazita, Rensselaer Polytechnic Institute, United States of America
            “alt.code” is a National Endowment for the Humanities funded initiative that combines humanities, the arts, and computer science to teach critical digital literacy, the politics of technology, and technical skills to undergraduates. The initiative uses the digital humanities as a vehicle to articulate the intersections of technical knowledge and critical thought to both computer science and humanities undergraduate students in the same classroom. Though alt.code is directed by Humanities, Arts, and Social Sciences (HASS) faculty, collaboration with Computer Science (CS) faculty allows humanists to teach critical theory directly within foundational CS courses, including “Introduction to Computer Science,” where CS students read critical humanities and social science texts while also learning the fundamentals of programming in Python. In turn, CS undergraduates are encouraged to use a portion of their required humanities credits to enroll in alt.code HASS courses, team-taught classes that encourage students to explore issues of social justice, epistemic bias, and politics of technology through prototyping and critical design exercises.
            The alt.code initiative includes a sequence of four classes held across RPI’s School of Humanities and School of Science, a Digital Humanities guest speaker series, and the development of RPI’s “Tactical Humanities Lab” to support undergraduate, graduate, and faculty praxis-based research. The broader, long-term goals of the initiative include: the redevelopment of the Computer Science curriculum to teach critical theory across the majority of their technical courses; encouraging computer science undergraduates to frame critical perspectives on technology as a 
                core part of their disciplinary expertise; and encouraging HASS undergraduates to frame digital and technical work as forms of political knowledge and action.
            
            While other undergraduate programs have worked to bridge teaching about social “impacts” of technology with teaching technical skills, they often do so in a modularized fashion. That is, CS and STEM students often take a series of technical courses that teach the “fundamentals” of technical production (as in, abstract mathematics, programming, and decontextualized construction), which are supplemented with “politics of technology” classes or humanities electives. Ostensibly, students will combine their “technical” and “non-technical” coursework to practice socially just or conscientious technological development.
            Science &amp; Technology Studies (STS) and Engineering &amp; Liberal Arts scholars, however, have noted that even hybridized curricular structures encourage the bifurcation of technical practice from social thought. STEM students begin defining the “epistemic object” of their studies as abstract, decontextualized technical production, with “social concerns” treated as external constraints that impact, but are never fully a part of, technical practice. Similarly, adjuncting technical practices into humanities classrooms as “methods” may encourage humanities and social science students to frame digital and technical tools as epistemically and politically neutral skills and practices, as opposed to social, political, and epistemological arguments in their own right.
            Alt.code treats teaching critical theory and technical practice infrastructurally, encouraging students to synthesize these bifurcated domains by teaching them critical perspectives on technology and technical skill in the same classroom, framed as inseparable parts of the same epistemic domain. In the future, we hope to propagate that model across both the CS and humanities curriculum at RPI, and that the successes and failures of our program can serve as a curricular and pedagogical model for other institutions grappling with bridging humanistic and technical knowledge.
            Abstract 4
            “‘Yes, but…’: Praxis in a Theory-Focused Environment”
            Amanda Heinrichs, Five College Digital Humanities and Blended Learning, Amherst College, United States of America
             At the DH 2018: Pedagogy and Praxis session, I will report on a series of efforts to introduce faculty to pedagogical praxis at liberal arts institutions that have been historically resistant to praxis. Since 2014, The Five College Digital Humanities and Blended Learning Initiative has been supported by a combination of Mellon and Teagle Foundation grants, and until this current academic year digital humanities and blended learning were conceived of as separate entities with separate goals, audiences, and even directors. On the one hand: DH/research/theory. On the other: BL/teaching/praxis. Yet the founders of the DH/BL initiatives (and administrators across the Consortium) want more investment in DH praxis for undergraduate students; my job this year is to develop modules that will lay the groundwork for a 5 College DH undergraduate certificate. In addition to a single director, under whom the program will necessarily merge DH and BL, theory and praxis, 2017-2018 is the last year of external funding for the 5CollDH/BL program. Thus the program enters a resource-poor environment, even as praxis and theory are (at least in name) integrated. 
             One particular challenge in a consortium of four small liberal arts colleges and one large public land-grant university is conflicting attitudes towards praxis across the Five Colleges. Faculty at Hampshire and Amherst in particular have argued praxis should be left to career development programs: and most praxis-oriented classes are taught at University of Massachusetts-Amherst, the only public institution of the five. Despite this, many faculty at all five colleges have taught themselves digital methods—or employed technical support staff—in order to develop robust DH projects. 
             In this way, faculty at prestigious undergraduate institutions like Amherst say “Yes, but…” to DH praxis. They are open to the idea that DH tools can address complex theoretical questions, but for various institutional, historical, and personal reasons do not often bring those tools to their classrooms. Therefore, if the goal of the roundtable is to discuss the state of praxis-oriented DH education, I would like to tweak the question a bit, and ask, "What happens when faculty say 
                yes to DH praxis, 
                but not for their undergraduates? What are the ethical stakes when professors at prestigious undergraduate institutions push praxis-oriented courses to the one public university in the consortium?”
            
             In addition to drawing on principles of minimal computing, as well as scholars such as Ryan Cordell and Kalani Craig, I will report on a series of faculty seminars titled "A No-Tech Introduction to the Digital Humanities." Funded by the Center for Humanistic Inquiry at Amherst College, this series presented digital humanities tools in their intellectual context: network analysis as an intervention in Heideggerian phenomenology, for example. I pair this theoretical understanding of DH tools with my experience assisting with the practical course at UMass-Amherst where students learned the fundamentals of Python for text and network analysis. Ultimately, I argue that a focus on theory which aligns with minimal computing principles not only allows for praxis in a resource-poor environment as the Five College Digital Humanities Initiative moves away from external grant funding, but also provides a potential response to the ethical dilemma of elite small colleges outsourcing praxis-oriented courses to a public institution.
            Abstract 5
            Public Works: Lessons in Experiential Learning from Digital Public Humanities Classrooms
            Jim McGrath, Brown University, United States of America
            In recent years, institutional desires for digital humanities projects and initiatives that are invested in ideas of “public humanities” have materialized in initiatives like the National Endowment for the Humanities’ “Digital Projects for the Public” grants, the inclusion of chapters on public humanities by Sheila Brennan and Wendy Hsu in 
                Debates in the Digital Humanities, and the creation of a graduate certificate in Digital Public Humanities at George Mason University (among other recent developments). While many digital humanities practitioners publish, debate and disseminate “in public” through the creation of open access scholarship, freely-available datasets, and content published on web sites and social media networks, these activities don’t always interpellate -- or invite collaborations and engagement with -- publics who do not traditionally reside within or in the orbit of academic institutions and discourse communities.  There are notable exceptions: Wendy Hsu's investments in "building at the rate of inclusion" on digital initiatives, Mitchell Whitelaw's call for "generous interfaces" that anticipate a wider range of users and needs, Lori Emerson's reminders of the limitations and restrictions of "invisible interfaces" that fail to acknowledge varied experiences with technology, Mia Ridge's careful considerations of crowdsourcing initiatives, the 
                Documenting The Now initiative’s investment in ethical uses of social media activism. How can we effectively teach digital humanities practitioners to imagine, engage, and collaborate with various publics and their various interests, needs, and uses of digital tools, networks, and resources?
            
            Drawing on the decade-long history of Brown University’s John Nicholas Brown Center for Public Humanities and Cultural Heritage, this panel presentation will discuss the ways digital tools, methodologies, and contexts have materialized in the modes of experiential learning valued by our Master’s Program and Certificate Program in Public Humanities: its postdoctoral fellow in Digital Public Humanities, its courses, independent studies, community collaborations, exhibitions, and working groups. Through a survey of its teaching, collaborations, and projects, the presentation will highlight generative ways to take the challenge and opportunity of public-facing (and public-serving) digital initiatives seriously: by anticipating economic limitations and audience needs that might impact a project's design and accessibility, through collaborating directly with publics rather than assuming a familiarity with their needs, and in favoring, when appropriate, iterative and ephemeral approaches to project implementation that embrace limitations of resources and temporal conditions. In addition to highlighting the benefits of a digital pedagogy informed by public humanities concerns, methodologies, and professional contexts, this presentation will also consider what lessons and ideas public humanities programs, courses, projects, publications, institutional structures, and methodologies interested in digital contexts could and should take from digital humanities practitioners.
            Abstract 6
            Praxis and Scale: On the Virtue of Small
            Brandon Walsh, University of Virginia, United States of America
            The University of Virginia's Praxis Program attempts to redefine graduate training by means of a targeted digital humanities intervention during the early years of graduate students' time in their programs. Each year, staff and faculty work alongside a student cohort to theorize a new digital intervention and train the students to carry it out alongside them in the spring. The result is that the students get an intensive introduction to a variety of digital humanities issues ranging from project management to technical training for their particular project. Drawing upon the pedagogical theories of Cathy Davidson, Paolo Freire, Bethany Nowviskie, and others, the program aims to equip graduate students with the skills and ethos necessary to thrive in collaborative, open work, the very things that can help prepare them for a variety of careers beyond the tenure track, and we do this by putting the students in charge as much as possible.
            Now in its seventh year, the program has a proven track record of success based on exit interviews (both qualitative and quantitative), job placements, and future awards received by our alumni. This presentation will discuss one consistent criticism the program has received throughout its existence: scale. Each cohort consists of six students, and we consistently get requests to expand the program with more students or for new audiences, requests that are difficult to carry out. We have developed a rough stack of technical and administrative lessons that are consistent year-to-year, but, the program relies on flexible instructors that can respond to student interests as they develop over the course of the year. The program represents a significant investment of resources, and, given the emphasis on student-driven work, it can be difficult to predict exactly what staff will be the primary points of contact for the project.
            We are limited in the number of cohorts that we can maintain at any one time, which may point to a fundamental limitation in student-centered programming: for better or worse, there is a limit to the scale at which these programs can be offered. At our institution, we have confronted this difficulty in scale by offering a diverse range of graduate programs with different structures, pedagogies, and audiences. We also engage alumni of the Praxis Program in other initiatives, folding their strengths into our pool of resources to allow recent students to quickly become able teachers. Since the Praxis Program's inception seven years ago, for example, we have piloted a program in digital humanities and library research methods for undergraduates that draws upon some of the lessons from the Praxis Program and engages Praxis alumni as instructional assistants.
            While there are compelling examples of scaling up student-centered programs, most notably in the FutureEd Initiative, for our group, to adopt a program that centers project-based education is to make a strategic investment in the small scale as a meaningful point of intervention. Such investments pay off in big ways for the people involved, and this paper thus advocates for praxis-oriented pedagogical approaches as a means of centering the development of the people and relationships at the core of our work. Far from limiting our impact, I argue that cherishing the small means fully investing in the long-term, future effects that these students have as they become teachers in their own right.
            Abstract 7
            The Digital GC in Praxis: Degrees, Fellowships, and Community-driven Support
            Matthew Gold, The Graduate Center, CUNY, United States of America
            Lisa Rhody, The Graduate Center, CUNY, United States of America
            At The Graduate Center, City University of New York (CUNY), Digital Initiatives continue to grow and change in response to institutional investments in new modes of graduate training. 
                This presentation will offer an overview of our multi-pronged approach to graduate education through degree programs, fellowships, and community-driven support. 
            
             The 
                Masters of Arts in Liberal Studies program features a 2-course introduction to digital humanities through theory and practice. Students participate in workshops, explore datasets, and complete the 
                Fall semester by proposing a potential project to be executed in the Spring. In the 
                Spring's problem-based curriculum, students refine their proposals, learn to develop project and data management plans, evaluate project proposals, select proposals, and form working groups to bring two to three proposed projects to fruition. At the end of the year, students present their prototypes at an annual campus-wide event. The success of the MALS program and the DH Praxis Seminar sequence has led to the creation of 
                2 new master’s degree programs to begin in Fall 2018.
            
            
                The 
                
                    GC Digital Fellows Program
                
                 operates as an in-house think-and-do tank for digital projects, connecting Fellows to digital initiatives throughout The Graduate Center. Digital Fellows work collaboratively to help build out “The Digital GC” — a vision of the Graduate Center that incorporates technology into its core research and teaching missions. The eleven fellows occupy a leadership role as mentors and advisors to peers who are interested in integrating digital methods into their scholarship. Fellows have developed extensive tutorials and workshops on topics ranging from establishing a digital scholarly identity to using Python and R to create data visualizations, and fellows serve as faculty during week-long digital institutes for faculty and students. Fellows develop project management and grant-writing skills while actively participating in on-going digital humanities projects, such as the CUNY Academic Commons and Manifold Scholarship. Fellows have also taken on projects that make use of public humanities data, such as the 
                
                    NEH Impact Index
                
                .  
            
            
                	Graduate Center students can access methods training and support throughout their academic career. Each January during the 
                
                    Digital Research Institute
                
                --a week-long digital methods intensive --participants with no prior technical experience learn foundational skills such as working from the command, version control with git and GitHub, Python, and database management, then choose from electives like natural language processing, machine learning, HTML/CSS, and APIs. The week begins and ends with discussions about project planning and digital research ethics. 
            
            
                
                    Provost’s Digital Innovation Grants
                
                 provide doctoral students in good standing with funds to propose and prototype research projects. Students write proposals and budgets, execute their idea, and present ongoing work at the end-of-year, community showcase. Previous student projects have received external funding and awards, including the 
                Lisa Lena Opas-Hänninen Prize for New and Young Scholars and the Paul Fortier Prize for New and Young Scholars.
            
             Through examples of student projects, research, and professional development workshops, this presentation will offer an overview of the opportunities and challenges of scaling out graduate training in digital humanities methods to wider communities.
        
        
            
                
                    Bibliography
                    
                        #FutureEd. Humanities, Arts, Science, and Technology Alliand and Collaboratory. https://www.hastac.org/initiatives/futureed
                    
                    
                        Bauwens, M. (2005). The political economy of peer production, 
                        CTheory, 1.
                    
                    
                        Berry, D. M. (2012). 
                        Understanding Digital Humanities. New York: Palgrave Macmillan.
                    
                    
                        Davidson, C. (2017). 
                        The New Education: How to Revolutionize the University to Prepare Students for a World In Flux. New York, NY: Basic Books.
                    
                    
                        D'Iori, P. &amp; Barbera, M. (2011). Scholar Source: A Digital Infrastructure for the Humanities. In Bartscherer, T. (Ed.). 
                        Switching Codes. Thinking Through Digital Technology in the Humanities and the Arts. Michigan: University of Chicago Press.
                    
                    
                        Freire, P. (1970). 
                        Pedagogy of the Oppressed. New York, NY: Bloomsbury Academic.
                    
                    
                        Koh, A. (2014). Introducing Digital Humanities Work to Undergraduates: An Overview. 
                        Hybrid Pedagogy. http://www.hybridpedagogy.com/journal/introducing-digital-humanities-work-undergraduates-overview/
                    
                    
                        Lafuente, A. (2016). Laboratorios ciudadanos: ciencia ciudadana y ciencia común. CCCD. Página web.
                    
                    
                        Ostrom, E. (2007).
                        Understanding Knowledge as a Commons. Cambridge, MA: MIT Press.
                    
                    
                        Manovich, L. (2013).
                        Software Takes Command. New York: Bloomsbury.
                    
                    
                        Manovich, L. (2015). The Science of Culture? Social Computing, Digital Humanities and Cultural Analytics. Disponible en https://www.academia.edu/15328596/
                    
                    
                        Mignolo, W. (2010).
                        The geopolitics of knowledge and the colonial difference. Praxis Publica
                         http://praxispublica.org/wp-content/uploads/2010/10/WALTER-MIGNOLO-GEOPOLITICS-OF-KNOWLEDGE-DUKE-UNIVERSITY.pdf
                        
                            
                                Check underlining here
                            
                        
                    
                    
                        Mignolo, W. (2013). Geopolítica de la sensibilidad y del conocimiento. Sobre (de) colonialidad, pensamiento fronterizo y desobediencia epistémica. 
                        Revista de Filosofía, 74(2), 7-23.
                    
                    
                        Mignolo, W. (2003).
                        Historias locales/diseños globales: colonialidad, conocimientos subalternos y pensamiento fronterizo. Madrid: Akal.
                    
                    
                        Mignolo, W. (2002).
                         The geopolitics of knowledge and the colonial difference. 
                        The South Atlantic Quarterly
                        , 101(1), 57-96.
                    
                    
                        Moretti, F. (2004). Gráficos, mapas, árboles. Modelos abstractos para la historia literaria. 
                        New Left Review, 24, pp. 60-85.
                        
                            http://newleftreview.es/24
                        
                    
                    
                        Nowviskie, B. (2011). "Praxis and Prism." http://nowviskie.org/2011/praxis-and-prism/
                    
                    
                        ———. (2012). "Too Small To Fail." http://nowviskie.org/2012/too-small-to-fail/
                    
                    
                        Ramos, I. y Ricaurte, P. (2015). Análisis de redes sociales y comunidades virtuales. 
                        Revista Virtualis, No. 11.
                    
                    
                        Rogers, K.
                         (2015). “Humanities Unbound: Supporting Careers and Scholarship Beyond the Tenure Track.” 
                        Digital Humanities Quarterly
                         9.1.
                    
                    
                        Rogers, R. (2013).
                        Digital Methods: Cambridge: MIT Press.
                    
                    
                        The Praxis Program.
                         University of Virginia Library's Scholars' Lab.
                    
                     http://praxis-network.org/.
                
            
        
    
6472	2018	
        
            Overview
            While scholarship on pedagogy in digital humanities has been growing, its focus has largely been on graduate and, to a lesser extent, undergraduate education. Yet, digital humanities pedagogy—namely its value for cultivating 21st century literacies tied to the production of knowledge and the ability to interpret digital media and computation—is as valuable, this panel argues, for middle- and high- school students as it is in higher education. Given that we are pursuing what Matthew Kirschenbaum describes as forms of "scholarship and pedagogy that are bound up with infrastructure in ways that are deeper and more explicit than we are generally accustomed to" (60), this panel examines the work of instructors who are beginning to plant the seeds of these new “customs” early on in humanities and social science training.
            Using digital humanities pedagogy in the middle- and high-school classroom, panelists argue, can redress gaps in these literacies. It enables students, as Mark Sample suggests, “[to think] through their engagement with seemingly incongruous materials, developing a critical thinking practice about process and product” (405). In this way, the approaches to digital humanities pedagogy in middle and high schools articulated by panelists are not an attempt to teach students particular technical skills, applications, or platforms. Rather, this pedagogical approach enables students to envision a relationship between themselves and knowledge production. 
            The approaches to digital humanities voiced in this panel are rooted in digital humanities pedagogies in higher education, particularly project-based approaches to humanities knowledge that foster collaboration. As Tanya Clement has argued:
            
                Like pedagogy intended to teach students to read more critically, project-based learning in digital humanities demonstrates that when students learn how to study digital media, they are learning how to study knowledge production as it is represented in symbolic constructs that circulate within information systems that are themselves a form of knowledge production. (366)
            
            In the case studies and pedagogical approaches discussed by panelists, the project form complements more traditional forms of knowledge production and evaluation in the classroom. As Brett D. Hirsch argues, this “introduces a new mode of work that emphasizes collectivity and collaboration in the pursuit and creation of new knowledge” (16). While these new modes can be linked to participatory forms of culture, made possible by low barriers for civic engagement and creative expression online (Jenkins et al. 9), panelists make the case for greater attention to pedagogies that offer instruction to middle- and high-school students in collaborative production. 
            However, as panelists argue, middle- and high-school pedagogies for digital humanities require attention to the unique needs of students in curricula, the developmental trajectories of the students, and the socio-economic dimensions of these students’ lives. In light of these concerns, what are the biggest challenges to doing digital humanities in middle and high schools? Which methods are most valuable and practically achievable? And how can we effectively prepare teachers to incorporate digital humanities into their teaching practices? In this panel we bring together an international team of researchers and faculty already engaged in answering these questions and implementing curricula in schools of education, digital humanities centers, and high schools. Our goal is both to present models and facilitate discussion with broader digital humanities communities about pedagogical infrastructures, methods, long-term goals, and the exciting possibility of cultivating digital humanities pipelines through intervention in middle and high schools.
            Panel moderator: Alex Gil, Columbia University Libraries
            Designing Digital Humanities Pedagogy Infrastructures for Teachers
            Roopika Risam, Salem State University
            While digital humanities pedagogy has increasingly received attention from practitioners who want to teach their own students more effectively, how do we prepare 
                teachers for the challenging task of engaging with digital humanities in their own classrooms? This talk offers an answer to this question by examining the digital humanities pedagogy infrastructure for middle- and high-school teachers designed at Salem State University. I first discuss findings from a study undertaken with teachers in Massachusetts to identify their attitudes towards digital humanities. The results indicate lack of knowledge about digital humanities but significant interest in incorporating computational approaches to humanities into teaching. Teachers also raised concerns including the time needed to learn technologies and teach them to students, cost of software and hardware, uneven access to computers or the internet in classrooms and for students at home, fear of implementing unsuccessful lessons, and a lack of professional development opportunities for digital humanities.
            
            This talk then considers the interdisciplinary graduate certificate in digital studies that Salem State University designed in response to the study. The program provides professional development while addressing teachers’ perceived obstacles to including digital humanities in their teaching. I discuss the relationship between study results and program design, focusing on development of core courses, selection of elective courses, differentiation of course delivery methods, integration into existing master’s programs, and creation of a directed study for curriculum design. To illustrate the impact of the program, I describe my work advising a team of teachers and administrators in the graduate certificate program who were planning technology needs for a new school building under construction and designing technology-infused curricula in English and History. While core and elective courses gave the teachers and administrators a solid background in digital humanities, a group directed study assisted the team with developing a scaffolded curriculum across middle-school humanities courses, designing classroom technology, and creating a professional learning community to provide in-school pedagogical support for teachers. 
            Finally, this talk discusses a follow-up study with graduates of the certificate programs that assessed program outcomes. These outcomes include assignments implemented by teachers in their classrooms, exemplar student work, and a marked difference in attitudes and perceptions of teachers who completed the certificate in comparison to those who participated in the initial study. Based on the outcomes and the success of the graduate certificate program, Salem State has begun integrating digital humanities pedagogy directly into its teacher training programs. Consequently, this talk argues, this digital humanities pedagogical infrastructure for teachers serves as an effective model for addressing the barriers to incorporating digital humanities into middle- and high-school curricula for teachers who are already in the classroom and those preparing for teaching careers in the humanities. 
            Digital Inquiry: The History of Youth
            Nina Rosenblatt, Trevor Day School
            David Thomas, Trevor Day School
            Stan Golanka, Trevor Day School
            On September 12th, 2017 Trevor Day School, an Independent School on the Upper East Side of New York City, launched two sections of an advanced history course entitled 
                Digital Inquiry: The History of Youth. This course was the culmination of seven years of curriculum development work that began with a November 16th, 2010 article in the 
                New York Times about Humanities 2.0 and the Stanford Republic of Letters Project. After an initial round of research we came to understand that digital projects had a role to play in our High School History curriculum. This realization coincided with our adoption of inquiry-based learning pedagogies. In a fundamental way, we argue, the techniques and disciplines involved in digital humanities allow high school students to conduct their own independent research in digital archives and become producers of history in their own right.
            
            In order to motivate students to collaborate and learn unfamiliar working methods, we developed our course around a subject that would engage all students. We wanted a subject that would not require a textbook, was accessible to juniors and seniors in high school, and would lend itself to seminar style classes. In addition, we wanted to be able to supplement the subject matter with texts illuminating the nature of historical narrative, archives, and the use of digital techniques in academic research such as the paper by Lauren Klein, “The Image of Absence: Archival Silence, Data Visualization, and James Hemings” in 
                American Literature Volume 85, Number 4, December 2013
                .
                The resulting course delved into the history of youth, looking at how being young is experienced and imagined differently in different times and places, and what we can learn about a society from its expectations for and attitude towards its youth, while teaching them production and analysis techniques for them to create new representations of that history.
            
            The final consideration was to craft a series of lesson plans to embed a digital humanities knowledge-production laboratory in the class. The course lab was divided into three modules: Digital editions and markup (an introduction to the fundamentals of plain text and markup), digital collections/exhibits (an introduction to the fundamentals of metadata and databases), and cultural analytics (an introduction to the fundamentals of algorithmic thinking and data mining). Through these modules the students were immersed in the process of selection, digitization, mark-up, the creation of a database/archive, data extraction and cleanup and data analysis, all driven by the imperative to create and interpret history. 
                Technologies taught included, but were not limited to, command line, git, GitHub, plain text editors, Markdown, YAML, Jekyll, Omeka, Python and Voyant Tools. These technologies were directly tied to the variety of ways in which historians collect “data” including using literary, psychological, sociological, statistical, and visual sources, working towards creating our own historical knowledge using the digital tools for collecting, visualizing, mapping, and analyzing the information.
            
            In this panel we will present the results of our two course prototypes, lessons learned, future improvements, and argue for a generalizable model of instruction for high schools in the United States based on our experiences.
            Digital Literary Studies in the High School Environment
            Eric Rettberg, Illinois Mathematics and Science Academy
            What are the challenges of adapting a course in Digital Humanities and Digital Culture from the pedagogical environment of the university to that of the high school classroom? What new challenges arise from asking minors to produce digital and public scholarship, and how can digitally inflected scholars and teachers foster innovative humanities work in school environments bound to pre-existing curricula? In this talk, I use my experience adapting a class in Digital Literary Studies to the high-school level to share unexpected challenges and opportunities and to suggest digital work as a strategy for promoting the humanities to administrators, peers, and students in STEM-oriented high school environments.
            In early 2016, I left higher education to teach in the English department of the Illinois Mathematics and Science Academy, a state-funded boarding school for students talented in math and science. Given the immediate appeal of classes combining humanities with computing for STEM-focused students, I naively expected that I might be able to simply bring a college elective for English majors to my high school students. The actual challenges of doing so, however have been instructive: administrators have been less familiar with the existence of the methods of the Digital Humanities, digital assignments have had to be reframed to accommodate shared practice among teachers in my department, my school’s technology environment has needed to be customized to accommodate the software installations that I took for granted before, oversight from administrators, colleagues, and parents has been more intensive, and without the support staff available to me at my higher education institutions, I’ve had to think creatively around constraints. By demonstrating small-scale digital humanities work in core classes, designing a week-long intersession class on a similar topic, and sharing my knowledge of University-level digital humanities, though, I’ve been able to design a class that has colleagues and students excited.
            Heeding Ryan Cordell’s call to embed digital humanities instruction in larger narratives beyond “recent scholarly revolution,” I treat digital humanities praxis as one of three major components of change in literary production and study in the digital era. In addition to digital humanities projects centered on historical texts of students’ choice, students read and discuss fictive works that represent cultures of technology in the digital era and computationally enabled works of electronic literature. Throughout the class, students sample digital humanities practice in lab sessions and build small-scale web resources and undertake digital-humanities experiments in group projects. By exploring electronic texts, they begin to more fully recognizes the affordances of digital technologies, and by reading print texts that represent digital culture, they think about their own roles as consumers of and creators of digital tools and cultures. While my school’s student population and focus are especially suited to the STEAM focus that a class like this one offers, my experiences suggest that students at a wide variety of high schools would be engaged by these materials and skills.
            Impact od Digital Humanities on High School History and Heritage Teaching and Learning in the Caribbean
            Schuyler K Esprit, Create Caribbean, Inc.
            The experience of Create Caribbean Research Institute, the first Digital Humanities center in the English Speaking Caribbean tells an interesting story of how digital humanities can covertly and explicitly reshape the curriculum in history and literature of the Caribbean without necessarily requiring a massive paradigm shift of the national and regional curriculum requirements.
            In Dominica (where Create Caribbean operates) and the Eastern Caribbean – among other islands – the secondary education curriculum responds to the mandates of the Caribbean Examinations Council (CXC) who sets the CSEC and CAPE syllabi for high school and post-secondary certification in the region. These examinations frame the education curriculum for the five to six years of high school in many islands and many educators in this system find themselves bound to deliver content in limiting and limited methods in order to ensure that students simply meet requirements to excel at subject exams at Caribbean History and English B (Literature), which has a heavy focus on Caribbean Literature.
            However, students leave with an abstract and formalized understand of Caribbean history and culture, without a nuanced understanding of its relevance to their own lived experiences and the implications for their future. Create Caribbean uses digital humanities projects to reframe the conversation and disrupt traditional methods for learning. One of these projects uniquely highlights the potential for technology to change the face of education in Dominica and to get students more invested in Dominica’s history and culture. This project, made by students for students, can be found at 
                www.dominicahistory.org. The college student change-makers of Create Caribbean’s internship program build digital humanities projects with a primary and secondary student audience in mind. The example of dominicahistory.org highlights one way that a collaboration with a national organization has allowed for a broader consideration of the methods of heritage and culture education for students while actually providing solid academic source material for their formal study requirements.
            
            This presentation will discuss the origin, process and impacts of the Dominica History and Imagined Homeland digital projects of Create Caribbean as examples of disruptive secondary education. The presentation will also address the ways in which the projects have attracted the attention of high school teachers and transformed their interests in using technology to revise classroom experiences when they face limitations in adjusting other curricular frameworks.
            Precarity and Practicality: DH, New Media, &amp; Secondary Education
            Matt Appegate, Molloy College
            Jamie Cohen, Molloy College
            In 2015, faculty at Molloy College in Long Island worked with faculty and administrators to found the Baldwin High School New Media Academy, a co-organized effort to bring the study of New Media and Digital Humanities to underserved high school populations in Baldwin, New York. Working collectively, faculty at both institutions have established a curriculum and internship path at Baldwin High School that exposes students to methods of DH praxis and principles of New Media in a variety of means and environments (high school, college, in-person, online).
            Our curriculum is based on five modules and two college-credit bearing courses. Our modules include Critical Making, Digital Storytelling, Multimodal Composition, Online Expression, and Social Media. Our college-credit bearing courses are Introduction to New Media and College Composition (the course is taught entirely on the methods of multimodal composition). Each module is integrated into existing high school courses, i.e., Social Studies, English, Wood Shop, etc., where students take college-credit bearing courses in their junior and senior years. Ultimately, the “academy” concept introduces students to DH methods and New Media in a gradated process--students choose their academy prior to entering their freshman year of high school and are enrolled in courses that employ our modules.
            Our curriculum is based on principles of social good; it emphasizes both civic engagement and social justice, and provides sample assignments with grading rubrics for each module (Ratto). The civic-minded focus of our curriculum was developed in consultation with Baldwin High School, and fleshed out over 18 months of training. Our curriculum attempts to account for the precarious position women and people of color already inhabited in online spaces and demonstrate how DH methods and New Media principles can be mobilized to empower students via digital tools and languages. 
            The focus of this paper is to report on our work with underserved high school populations and relay the challenges of bringing this kind of material to a secondary education setting. We focus on the practicalities of bringing DH methods and New Media principles to high school (i.e., funding, time, expertise, bureaucracy), as well as the necessary training that takes places between high school and college faculty (PT days, on campus conferences, and student events). Finally, we discuss the opportunities that working with underserved high school populations provides both politically and pedagogically. In this context, DH operates on a minimal scale, but addresses communal needs. 
            Bios
            Matt Applegate is an Assistant Professor of English and Digital Humanities at Molloy College. His work focuses on critical theory, digital humanities, digital literacy, and screen studies. His work as appeared in 
                Amodern, 
                Theory &amp; Event, 
                Cultural Politics, 
                Cultural Critique, 
                Telos, and more.
            
            
                Jamie Cohen is the director, co-founder and assistant professor of the New Media program at Molloy College in New York. Jamie is the author of 
                Producing New and Digital Media: Your Guide to Savvy Use of the Web
                 (Routledge 2015) and his published and presented research focuses on memes, YouTubers, populism, VR/AR/MR, and digital media literacy. He is a fellow of the Salzburg Academy on Media and Global Change and the Academy of Television Arts and Sciences.
            
            Schuyler K Esprit is the Director of Create Caribbean Research Institute at Dominica State College, the first Digital Humanities center in the Caribbean. Dr. Esprit holds a PhD in English literature from University of Maryland – College Park. She is a scholar of Caribbean literature and cultural studies, and postcolonial theory. She is now completing her book entitled 
                West Indian Readers: A Social History and its digital companion, both of which are historical explorations of reading culture in the Caribbean. She is currently Dean of Academic Affairs at Dominica State College. 
            
            Stan Golanka is Director of Academic Technology at Trevor Day School. He teaches computer programming and co-teaches Advanced History: Digital Inquiry. He holds a MA in Computing in Education from Teachers College at Columbia University. 
            Eric Rettberg teaches English at the Illinois Mathematics and Science Academy. He remains an active scholar of modernism, experimental poetry, sound studies, and the digital humanities. His work has appeared in 
                Comparative Literature Studies and 
                Jacket 2. 
            
            Roopika Risam is an assistant professor of English and English education at Salem State University. Her research considers the intersections of postcolonial cultures, African diaspora studies, and digital humanities. She is the author of 
                New Digital Worlds: Postcolonial Digital Humanities in Theory, Praxis, and Pedagogy (Northwestern UP 2018) and her work has recently appeared in 
                Debates in the Digital Humanities 2016, 
                Digital Scholarship in the Humanities, and 
                South Asian Review. 
            
            Nina Rosenblatt teaches US History, Art History, and Advanced History: Digital Inquiry at Trevor Day School. She holds a PhD in Art History from Columbia University. 
            David Thomas is Chair of the History Department at Trevor Day School, he teaches European History, Advanced European History, The History of China, and Advanced History: Digital Inquiry.
        
        
            
                
                    Bibliography
                    Clement, Tanya. “Multiliteracies in the Undergraduate Digital Humanities Curriculum: Skills, Principles, and Habits of Mind,” in 
                        Digital Humanities Pedagogy: Practices, Principles, and Politics, ed. Brett D. Hirsch (Cambridge: Open Book Publishers, 2012), 365-88.
                    
                    Cordell, Ryan. "How Not to Teach Digital Humanities." 
                        Ryancordell.org, 1 Feb. 2015, 
                        http://ryancordell.org/teaching/how-not-to-teach-digital-humanities/.
                    
                    Hirsch, Brett D. “&lt;/Parentheses&gt;: Digital Humanities and the Place of Pedagogy.” 
                        Digital Humanities Pedagogy: Practices, Principles, and Politics, ed. Brett D. Hirsch (Cambridge: Open Book Publishers, 2012), 3-30.
                    
                    Jenkins, Henry and Ravi Purushotma, Margaret Weigel, Katie Clinton, Alice J. Robison. 
                        Confronting the Challenges of Participatory Culture: Media Education for the 21
                        st
                         Century (Cambridge, MA: MIT Press, 2009).
                    
                    Kirschenbaum, Matthew. “What Is Digital Humanities and What’s It Doing in English Departments?” 
                        ADE Bulletin 150 (2010): 55-61.
                    
                    Ratto, Matt. “OPEN DESIGN NOW.” 
                        Open Design Now, Netherlands Institute for Design and Fashion and Waag Society, opendesignnow.org/index.html%3Fp=434.html.
                    
                    Sample, Mark. “What’s Wrong with Writing Essays?” 
                        Debates in the Digital Humanities, ed. Matthew K. Gold (Minneapolis: University of Minnesota Press, 2012), 404-5.
                    
                
            
        
    
6499	2018	
        
            Para responder cuáles han sido las tendencias de la circulación de la literatura de México hacia otros espacios lingüísticos, se partió de los datos disponibles en la 
                Enciclopedia de la literatura en México (ELEM, www.elem.mx) para realizar un estudio de las traducciones de obras de escritores mexicanos, escritas en español y traducidas a 33 lenguas (incluidos los 7 idiomas indígenas del país de los que hubo al menos un registro). En esta presentación breve, daremos cuenta de los resultados de una investigación en curso sobre el modelado y 
                puesta en mapa de estos datos.
            
            En México, el estudio cuantitativo de las traducciones de la literatura nacional tiene un antecedente emblemático en la obra pionera de José Ignacio Mantecón, 
                Índice de las traducciones impresas en México, de 1959. En este trabajo se recopilaron 544 traducciones hechas en México en ese año, y se registraron aspectos tales como el género al que pertenecían, lo que permitió derivar conclusiones como el hecho de que el grupo más representativo de traducciones lo constituían las obras literarias (35%), de las cuales un 13% eran libros infantiles (Mantecón, 1959: 14, 18). Sin embargo, además de que este trabajo no ha sido replicado, este estudio sólo da cuenta de las traducciones al español como lengua meta.
            
            Otra referencia, en la que se perfila el objetivo de nuestra investigación –el estado de la traducción de la literatura de México– se encuentra en la introducción que hace Rosenzweig del intercambio epistolar entre Alfonso Reyes y el traductor al checo Zdeněk Šmíd. En ésta se lee lo siguiente:
            Salvo excepciones, la literatura mexicana, al igual que la latinoamericana, se comenzó a traducir a comienzos de los años treinta del siglo 
                xx. Inicialmente se hicieron traducciones al inglés y al francés; en un segundo momento, impulsadas por el francés, a otras lenguas europeas como el alemán, neerlandés, checo e italiano. Las primeras novelas mexicanas que se tradujeron fueron 
                Los de abajo y 
                Mala yerba de Mariano Azuela; 
                El águila y la serpiente y 
                La sombra del Caudillo, de Martín Luis Guzmán; 
                El indio, de Gregorio López y Fuentes; y 
                ¡Vámanos con Pancho Villa! de Rafael F. Muñoz. (Rosenzweig, 2014: 13)
            
            No obstante, este extracto carece de referencias numéricas exactas y tampoco responde quiénes fueron esos primeros traductores al inglés, cuándo comenzaron exactamente las traducciones al francés o cuándo a otras lenguas europeas. Y es que, a excepción de algunas listas de idiomas específicos –como los 327 registros de obras de la literatura mexicana traducidas al inglés en Estados Unidos (Boyd, 2012); el catálogo análogo de 99 registros de obras traducidas al alemán (Küpper, s.f.); o la lista de las traducciones al italiano (Tedeschi, s.f.) – no existe ningún compendio que ofrezca el panorama completo de la proyección de la literatura mexicana en un sentido global. Por tal razón, la bibliografía de más de 1500 traducciones de la ELEM es una base de datos única en su tipo de la que es necesario expandir sus posibilidades heurísticas. Pero antes, algunas palabras sobre esta enciclopedia.
            La ELEM comenzó a organizar el conocimiento en torno a la cultura literaria de México (oral y escrita) desde 2011, cuando fue creada. Cuenta con los registros de 13,040 personas (autores, traductores, investigadores literarios) y más de 40,000 obras impresas (primeras ediciones), que conforman una bibliografía general de la literatura en México, la cual abarca casi 
                v siglos de cultura literaria. Entre sus prioridades se encuentra el registro de las obras traducidas a otros espacios lingüísticos con el propósito de observar, a través de las lenguas meta y los países del mundo en que son impresas, el grado de recepción de la literatura del país.
            
            Por esto, emprendimos un trabajo colaborativo y transdisciplinario en el que se planteó un modelado de los datos disponibles en la enciclopedia (ver Imagen 1) bajo el concepto de 
                puesta en mapa (en analogía de la 
                puesta en página del mundo editorial) y en consonancia con la línea de las Humanidades Digitales denominada 
                spatial humanities. En este caso específico, designa al desarrollo de una interfaz que permite captar geopolíticamente la circulación de la literatura de los autores mexicanos que escriben en español (con algunas traducciones indirectas) hacia 19 lenguas indoeuropeas, 7 lenguas indígenas de México, además de estonio, euskera, finés, hebreo, húngaro, japonés y turco. El corpus del que partimos contempla un universo de 1658 primeras ediciones que se desdobla, a partir de las reimpresiones y reediciones de muchos títulos, en un total de 2088 objetos. 
            
            
                
            
            Imagen 1. Estructura de la base de datos
            En un primer acercamiento, nos interesó indagar las relaciones espacio-temporales de las obras traducidas para responder las siguientes preguntas:
            
                ¿Qué autores o géneros han sido los más traducidos?
                ¿En qué años?
                ¿En qué geografías?
                ¿A qué idiomas?
            
            
                
            
            Imagen 2. Perspectiva general de la puesta en mapa
            Para facilitar la exploración de estas relaciones, se creó un prototipo de interfaz interactiva que permite iniciar investigaciones a partir de la puesta en mapa de los datos. El código en desarrollo de este prototipo se encuentra disponible en GitHub (Gutiérrez, 2017) y su versión para consulta estará en: 
                
                    http://elem.mx/estgrp/datos/1335
                . 
            
            Se describen las etapas de desarrollo a continuación. A partir de una consulta SQL a la base de la ELEM se creó un archivo separado por comas (csv) usando un script de Python (parser.py en el repositorio de GitHub). Estos insumos fueron transformados para obtener un formato adecuado para el consumo en Javascript: JSON. Para la arquitectura de la aplicación web se usó una herramienta para hacer empaques o bundles llamada Webpack (
                
                    https://webpack.js.org/
                ). La biblioteca usada para la creación del mapa es una herramienta de código abierto llamada Leaflet en su última versión 1.2.0 (http://leafletjs.com/). El desarrollo de la aplicación se puso en marcha en Javascript para la interfaz ya que la información, por el momento, existe de manera estática. En el futuro, cuando se integre con la base de datos con la dorsal final o back-end, será deseable que las consultas de datos se realicen desde este punto y se exponga un end-point adecuado para el consumo.
            
            La interfaz pretende facilitar la visualización e interacción con los datos de la base, así como el análisis exploratorio de los mismos (Behrens, 1997). Los usuarios podrán elegir filtros tales como: lengua meta, género literario, año de la traducción y, explorar los registros por ubicación geográfica. Además se provee de la siguiente información sobre los objetos: título de la traducción, autor/a, traductor/a, editorial de la traducción y título original de la obra.
            
                
            
            Imagen 3. Perspectiva del filtro: narrativa/inglés/1945-2017
            Uno de los potenciales usos de esta herramienta puede ilustrarse a partir del siguiente ejemplo en el cual se usó el filtro de idioma (inglés), el de género literario (narrativa) y el rango de años de edición (1945-2017). La vista de los datos nos permitió observar un comportamiento no previsible. El título 
                Kill de Lion! fue editado en México, D. F., en inglés. Es decir, el espacio geográfico no corresponde necesariamente con el espacio lingüístico, como se hubiera podido suponer en un principio.
            
            Los especialistas e interesados en la cultura de la traducción literaria podrán contar con una visión de conjunto para realizar análisis e interpretaciones más minuciosas sobre la circulación de la cultura literaria de México a través de sus traducciones. Además, la puesta en mapa se irá actualizando conforme a las actividades de catalogación de la enciclopedia, lo que permitirá un acercamiento a las traducciones hacia otras lenguas aún no contempladas hasta ahora. Asimismo, los interesados en los contactos entre lenguas contarán con los insumos para poner en perspectiva las relaciones diglósicas, trasladadas a la cultura impresa, entre lenguas hegemónicas y lenguas minorizadas a partir de la traducción.
        
        
            
                
                    Bibliografía
                    
                        Behrens, J. (1997), Principles and procedures of exploratory data analysis. Psychological Methods 
                        2 (2): 131.
                    
                    
                        Boyd, M. (2012). 
                        A Conflict of Narratives: The Influence of US Ideological Constructions of Mexican Identity in the Translation of Mexican Literature into English. Universidad de York, Toronto, Canadá.
                    
                    
                        Burns, P. et al. (2017). Mapping Linked Data Subject Headings in the Library Catalog. 
                        DH2017. Montreal, Canadá.
                    
                    
                        Gutiérrez, A. (2017). Literature Translation. 
                        GitHub.
                         
                        
                            https://github.com/amaurs/literature-translation
                        
                    
                    
                        Küpper, K. (s.f.) Mexiko / Mittelamerika.
                         Archiv für übersetzte Literatur aus Lateinamerika und der Karibik.
                         
                        
                            http://www.lateinamerikaarchiv.de/antiquariat/mittelamerika-mexiko.html
                        
                    
                    
                        Mantecón Navasal, J. (1964). 
                        Índice de las traducciones impresas en México, 1959. México: Biblioteca Nacional de México, Instituto Bibliográfico Mexicano.
                    
                    
                        Petrusko, R. (s.f.). 
                        An Inquiry into the Diffusion of Dr. Adam Smith's The Wealth Of Nations.
                         
                        
                            http://www.mlhplayground.org/demo/smith/
                        
                    
                    
                        Rosenzweig, G. (2014). Presentación
                        . Procurando contactos a la literatura mexicana: Alfonso Reyes-Zdeněk Šmíd, correspondencia (1932-1959). México: El Colegio de México.
                    
                    
                        Tedeschi, S. (2012). Catálogo de obras de autores mexicanos traducidas al italiano. Comunicación personal.
                    
                
            
        
    
6500	2018	
        
            Introduction
            “Anger is loaded with information and energy,” says Audre Lorde in a 1981 speech on its political uses—but the nature of this affective information, sparked by a given political present, becomes highly vexed when articulated through literary objects (Lorde, 1997: 280). On the one hand, the cool detachment of aesthetic mediation keeps the politics of experimental works from being seen as mere propaganda, but runs the risk of appearing elitist or self-indulgent. On the other hand, the red-hot political outrage of a protest poem by Amiri Baraka or Sonia Sanchez grounds itself in the present, but may be attacked for subordinating aesthetic sophistication to political agendas.
            Building on recent scholarship (like the work of Lauren Berlant and Sianne Ngai) suggesting that feeling gives structure to cultural formations, my research investigates the provocation and articulation of emotions like frustration, anger, and discontentment within recent US literary history as they relate to systemic injustice. An agitprop play that ends with shouts for workers to unite in class revolution; a poetic broadside that vents frustrations against white supremacy in America; a novel that indulges in a revenge fantasy against America’s colonial history. Unlike plays, poems, or novels that seem to obscure, submerge, or confound their own political dimensions, these works wear their hearts on their sleeves: they are frustrated, fed up with how things are, and unafraid to speak truth to power in a direct, seemingly “un-literary” way.
            “Measured Unrest in the Poetry of the Black Arts Movement” offers a proof-of-concept for performing sentiment analysis on some of the most politically and affectively charged poetry of the 20
                th century in America, that of the Black Arts Movement of the 1960s and 1970s. The BAM first took shape at the height of the Black Power Movement with the foundation of the Revolutionary Theatre by Amiri Baraka in 1965. As Larry Neal—one of BAM’s principal theorists—says in a 1969 manifesto, the “Black Arts movement seeks to link, in a highly conscious manner, art and politics” toward “the liberation of Black people” (Neal, 1969: 54). Moreover, what Neal calls the movement’s “black esthetic” is famous for its affective dimensions, often exploring the limits and political uses of anger, frustration, and militant poetic rage. But while BAM writers sought to link art and politics through explicitly racial terms, many—though by no means all—were marked by a failure to attend to the intersections of gender with racial injustice.
            
            In this project I ask two questions in particular: first, how are the feelings associated with injustice in this corpus coded in terms of race and gender? And second, what can natural language processing techniques like sentiment analysis show us about the relations between different dimensions of poetry—like affect and gender—given that poetry is highly figurative and notoriously difficult to quantify in terms of sentiment or opinion?
            Method
            In addressing both these questions, this project uses a small corpus of poetry—currently 26 books—from prominent BAM authors. I employ both close reading as well as machine reading techniques, combining the powerful scale of sentiment analysis with the granularity of traditional literary analysis in an effort to explore the intersections of feeling, gender, race, and injustice in the radical poetry of this period. My goal in this project is not to develop a sentiment classifier that works on experimental poetry in English. Rather, it is to see what existing classifiers can show us about a specific corpus of poetry. 
            In this sense, I use pre-existing sentiment classifiers like VADER and Pattern (via TextBlob) to perform a kind of exploratory computational analysis on my corpus (Hutto and Gilbert, 2014; De Smedt, and Daelemans, 2012). Rather than use these tools to make general claims about this incredibly diverse body of poetry, I test, experiment, and make targeted use of sentiment analysis techniques to pursue research questions already present in existing scholarly conversations—for example, how poets might tie heightened affects to an explicitly political quest for racial justice in America. The insights I draw from my computational analyses, then, go hand in hand with more traditional literary practices. Moreover, my methodology aims to acknowledge the fact this poetry was written in the shadow of government surveillance programs, active FBI counterintelligence operations, and a larger culture fearful of radical thought. Because of this, my project explores the fraught methodological implications of using distanced, potentially decontextualizing computational text analysis techniques to think through BAM poetry, and how these methods might best be used to pursue questions, problems, and lines of inquiry centered around black thought and experience.
            The already vibrant conversations on sentiment analysis and natural language processing more generally have been illuminating in forming these thoughts and questions. The discussion between Matthew Jockers and Annie Swafford on the 
                Syuzhet package and “archetypal plot shapes” has helped me not only to consider the current possibilities and limitations of sentiment analysis as applied to literary corpora, but also to think through the kinds of results we expect from digital projects and how we verify those results as an academic community (Swafford, 2015; Jockers, 2015). With regards to poetry and NLP more specifically, Lisa Rhody’s topic modeling of highly figurative ekphrastic poetry is a great model for how unexpected failures in textual analysis can also be productive, prompting us towards new questions as well as new understandings of familiar methods like close reading (Rhody, 2012).
            
            Results
            I have implemented NLP techniques with NLTK and TextBlob, a text-processing Python library, on my collection of 26 books of poetry. I have also used two sentiment classifiers—Pattern (via TextBlob) and VADER—to evaluate my corpus for sentiment and interpret my results. While this work is ongoing, so far my work comprises explorations and experiments in the smaller-scale uses of sentiment analysis in the study of poetry and affect. 
            For example, Pattern considers Quincy Troupe’s “Come Sing a Song”—from his 1972 collection 
                Embryo Poems, 1967-1971—to be the most negative poem in my entire corpus. In a corpus of poetry containing direct attacks, extreme invective, and explicit takedowns of individuals, groups, and institutions, I did not find this poem to contain an exceptional amount of negative sentiment. On the contrary, I found “Come Sing a Song” to be positive and celebratory with regards to black life and black artistic expression. VADER, meanwhile, considers Nikki Giovanni’s “The True Import of the Present Dialogue, Black vs. Negro”—from her 1968 
                Black Feeling, Black Talk—to have the most negative sentiment in the corpus. These results are very much in keeping with other human readers of this poem: critics consider it to be one of the most significant and famous examples of a certain type of angry, militant, even aggressive poem. Where Pattern and I disagree strongly over the feel of Troupe’s “Come Sing a Song,” critics and VADER seem to agree that Giovanni’s “The True Import” has, on the surface, an exceptional amount of negative sentiment compared with its contemporaries. 
            
            Among other things, my project analyzes discrepancies and correspondences such as those described above. Already, my findings have revealed an interpretive disjoint between the denotative affective impact of words—what might be called their surface sentiment—and their more nuanced affective import as shaped by poetic, literary, social, and political contexts. A sentiment classifier like VADER, for example, highlights the intensity of negative sentiment in a poem according to the words and phrases it contains without the literary and historical context of their use. This kind of surface reading, attuned specifically to words’ immediate affective impact, anticipates the space between a surface anger that can spark feelings regardless of context and a poetic form that, in the case of Giovanni’s “The True Import,” leverages negative sentiment to address meaningful social issues in a productive, ultimately positive way. By investigating these poems through conventional literary methods (i.e., historical contextualization, close reading, consideration of relevant scholarship) and computational methods (in this case Pattern and VADER), while also investigating the histories, intended use contexts, and potential biases of the chosen computational methods, this project provides an opportunity to examine what it is, exactly, that provides a book, poem, or poetic line with its emotional charge.
        
        
            
                
                    Bibliography
                    
                        De Smedt, T. and Daelemans, W. (2012). “Pattern for Python.” 
                        Journal of Machine Learning Research 13: 2063–67.
                    
                    
                        Hutto, C. J. and Gilbert, E. (2014). “VADER: A Parsimonious Rule-Based Model for Sentiment Analysis of Social Media Text.” 
                        Eighth International Conference on Weblogs and Social Media. Ann Arbor, MI, June 2014.
                    
                    
                        Jockers, M. (2015). “Revealing Sentiment and Plot Arcs with the Syuzhet Package,” February 2, 2015. 
                        http://www.matthewjockers.net/2015/02/02/syuzhet/ (accessed 27 February 2018).
                    
                    
                        Lorde, A. (1997). “The Uses of Anger.” 
                        Women’s Studies Quarterly 25, no. 1/2: 278–85.
                    
                    
                        Neal, L. (1969). “Any Day Now: Black Art and Black Liberation.” 
                        Ebony 24, no. 10: 54–62.
                    
                    
                        Rhody, L. (2012). “Topic Modeling and Figurative Language.” 
                        Journal of Digital Humanities 2, no. 1. 
                        http://journalofdigitalhumanities.org/2-1/topic-modeling-and-figurative-language-by-lisa-m-rhody/ (accessed 27 February 2018).
                    
                    
                        Swafford, A. (2015). “Why Syuzhet Doesn’t Work and How We Know,” March 30, 2015. 
                        https://annieswafford.wordpress.com/2015/03/30/why-syuzhet-doesnt-work-and-how-we-know/ (accessed 27 February 2018).
                    
                
            
        
    
6514	2018	
        
            
                Introduction
                Building online research components for projects in the digital humanities is a common practice. However, not many researchers have a plan for these online components once the project halts or comes to an end. Consequently, many of these projects become abandoned and slowly degrade over time –some more gracefully than others. Additionally, there is a certain inherent fragility associated with software and our online research tools. In turn, this fragility threatens the completeness and the sustainability of our work over time.
                Previous studies have attempted to harness and manage the fragility of online resources. Studies have been carried out to address their potential reconstruction 
                    (Klein et al., 2011), the overall decay of websites 
                    (Bar-Yossef et al., 2004) and the decomposition of their shared resources 
                    (SalahEldeen and Nelson, 2012). Recently, our research has been focusing on analyzing the perceptions of change in distributed collections 
                    (Meneses et al., 2016). However, we believe that the inherent characteristics of online digital humanities projects present an interesting (and unique) area for inquiry for two reasons. First, the research aspect of digital humanities projects hinders previous approaches –as the methods for identifying change in the Web do not fully apply. And second, digital humanities projects have a limited useful life –which is accompanied by research from primary investigator, which may or may not be indicated by updates in the project’s content and tools.
                
                We presented a paper in Digital Humanities 2017 that explored the abandonment and the average lifespan of online projects in the digital humanities 
                    (Meneses and Furuta, 2017). However, we believe that managing and characterizing the online degradation of digital humanities projects is a complex problem that demands further analysis. In this abstract, we propose to explore further the distinctive signs of abandonment of online digital humanities projects. For this second instalment of our study we took a different direction: we departed from strictly using retrieved HTTP response codes and incorporated additional metrics such as number of redirects, DNS metadata and a detailed analysis of content features. 
                
                This study aims to answer four questions. First, can we identify abandoned projects using computational methods? Second, can the degree of abandonment be quantified? Third, what features are more relevant than others when identifying instances of abandonment? Our final question is philosophical: can an abandoned project still be considered a digital humanities project?
            
            
                Methodology
                A complete listing of research projects in the Digital Humanities does not exist. However, the Alliance of Digital Humanities Organizations publishes a Book of Abstracts after each Digital Humanities conference as a PDF. Each one of these volumes can be treated as a compendium of the research that is carried out in the field. To create a dataset, we downloaded the Books of Abstracts corresponding from 2006 to 2016 –except for 2015 which was not available for download. We must thank and acknowledge Dr. Jason Ensor from Western Sidney University for providing us the abstracts for the 2015 Digital Humanities conference –which completes our dataset of conference abstracts. We obtained these abstracts after we had carried out our preliminary analysis. Therefore, we will present our findings using the complete dataset of abstracts in the presentation of our paper.
                Then we proceeded to extract the text from these documents using Apache Tika and parse the 5845 unique URLs that we found using regular expressions. Then we used Python’s Requests Library to retrieve the HTTP response codes and headers corresponding to the URLs, which we used to classify the websites into two groups depending on their correctness: valid (correct) and decayed (showing signs of degradation). Figure 1 shows the distribution of decay for each year. Based on our preliminary findings we approximate the average lifespan of a research project to 5 years, which aligns with reports from previous work 
                    (Goh and Ng, 2007). The average time in years since the last modification of the websites in the study is shown in figure 2.
                
                
                    
                
                Figure 1: URL decay by year.
                
                    
                
                Figure 2: Average time in years since last modification.
            
            
                Developing classifiers
                To develop classifiers for the degradation identified in the previous section, we considered features computed based on DNS metadata, the initial HTTP request, number of redirects, and the contents and links returned by traversing the base node. The features we included are divided into topology, content-type, anchor-text and child node features. These features stem from concepts we used in our previous work 
                    (Meneses et al., 2016).
                
                
                    The text associated with resources is the most obvious feature for determining the topics. Given that we are dealing with a very specialized domain, we developed a domain-oriented expectation model. In particular, we generated topic and term frequency models to examine the similarity among the documents in a given project (the contents of the base node and the metadata and the contents of the child nodes). We used Latent Dirichlet Allocation to model the content of the text 
                    (Blei et al., 2003) 
                    and a simple Tf-Idf ranking function to measure and compare them. This ranking function is based on adding the Tf-Idf values for the documents, which were calculated using the terms from the topic modelling as a vocabulary. We will present a detailed version of our results on the longer version of our paper.
                
            
            
                Discussion
                This study is an attempt to categorize change in a very specific domain. More so, this study constitutes one step towards addressing potential strategies for the archival and the long-term preservation of abandoned digital projects. It is important to highlight that not all projects are equal and thus require different approaches towards long-term preservation. In the case of dynamically generated projects, a common approach nowadays is to produce a static set of HTML files which are easier to store. However, this approach assumes the backwards compatibility of Web browsers over time –something that has not always been the case. 
                To summarize, in this study we aim to computationally identify the indicators of the abandonment of digital humanities projects –as well as quantify their degrees of neglect. To address our philosophical question, we believe that an abandoned project can still be considered a valid digital humanities project depending on its audience. However, this has several nuances that should be considered. Digital online projects in the humanities have unique characteristics that make them impervious to the metrics that used in the Web as a whole –which make them worthy of study. In the end, we intend this study to be a step forward towards better preservation strategies and for the planned obsolesce of digital humanities projects.
            
        
        
            
                
                    Bibliography
                    
                        Bar-Yossef, Z., Broder, A. Z., Kumar, R. and Tomkins, A. (2004). Sic transit gloria telae: towards an understanding of the web’s decay. ACM doi:10.1145/988672.988716.
                    
                    
                        Blei, D. M., Ng, A. Y. and Jordan, M. I. (2003). Latent dirichlet allocation. 
                        The Journal of Machine Learning Research, 
                        3: 993–1022.
                    
                    
                        Goh, D. H. and Ng, P. K. (2007). Link decay in leading information science journals. 
                        Journal of the American Society for Information Science and Technology, 
                        58(1): 15–24.
                    
                    
                        Klein, M., Ware, J. and Nelson, M. L. (2011). Rediscovering missing web pages using link neighborhood lexical signatures. ACM doi:10.1145/1998076.1998101.
                    
                    
                        Meneses, L. and Furuta, R. (2017). Shelf life: Identifying the Abandonment of Online Digital Humanities Projects Paper presented at the Digital Humanities 2017, Montreal, Canada.
                    
                    
                        Meneses, L., Jayarathna, S., Furuta, R. and Shipman, F. (2016). Analyzing the Perceptions of Change in a Distributed Collection of Web Documents. 
                        Proceedings of the 27th ACM Conference on Hypertext and Social Media. (HT ’16). New York, NY, USA: ACM, pp. 273–278 doi:10.1145/2914586.2914628. http://doi.acm.org/10.1145/2914586.2914628 (accessed 12 April 2017).
                    
                    
                        SalahEldeen, H. M. and Nelson, M. L. (2012). Losing My Revolution: How Many Resources Shared on Social Media Have Been Lost?.
                    
                
            
        
    
6520	2018	
        
            Automated lemmatization, that is the retrieval of dictionary headwords, is an active area of research in Latin text analysis. Latinists have available web-based applications like Collatinus (Ouvard and Verkerk, 2014) and LemLat (Bozzi et al., 1992) and web services like Morpheus (Almas, 2015). LatMor (Springmann, 2016) and TreeTagger (Schmid, 1994) offer lemmatization as a byproduct of their primary tasks as morphological taggers. Recent work, to name a few developments, has seen lexicon-assisted tagging and rule induction (Eger et al., 2015; cf. Juršič, 2010) as well as neural networks (Kestemont and De Gussem, 2017) used as strategies for improving Latin lemmatization.
            In this short paper, I describe the implementation of the Backoff Lemmatizer (https://github.com/cltk/cltk/blob/master/cltk/lemmatize/latin/backoff.py) for the Classical Language Toolkit, an open-source Python platform dedicated to developing natural language processing tools for historical languages (Johnson, 2017). The Backoff Lemmatizer is in fact not a single lemmatizer but rather a customizable suite of sub-lemmatizers, based on the Natural Language Toolkit’s SequentialBackoffTagger. The SequentialBackoffTagger allows the user to “chain taggers together so that if one tagger doesn’t know how to tag a word, it can pass the word on to the next backoff tagger” (Perkins, 2014, 92). While the backoff process was originally designed to handle part-of-speech tagging, and so, a task with a limited tagset, it works well for lemmatization (~90.34% accuracy compared to the 93.49% to 95.30% range reported in Eger et al., 2015). 
            A default class for sequential lemmatization, BackoffLatinLemmatizer, is available through the CLTK “Lemmatize” module using the following backoff sequence: 1. a dictionary-based lemmatizer for high-frequency, indeclinable vocabulary; 2. a unigram-model lemmatizer based on training data; 3. a rules-based lemmatizer based on regular expression patterns; 4. a variation on the previous regular-expression-based lemmatizer that factors in principal-part information; 5. another dictionary-based lemmatizer using the Morpheus lemma dictionary; and finally 6. an identity lemmatizer that returns the token as lemma. 
            Although currently available and tested only for Latin, the Backoff Lemmatizer is in theory language agnostic, since the sub-lemmatizers can be passed language-specific training data and models. So, for example, the UnigramLemmatizer requires training data in the form of a Python list of tuples of the form 
                [(‘token1’, ‘lemma1’), (‘token2’, ‘lemma2’), ...]. A Latin model with data in this form based on The Ancient Greek and Latin Dependency Treebank (Celano, Crane, and Almas, 2017) is available in the CLTK Latin corpora, but a similar model could be built for any language. Similarly, the RegexLemmatizer relies on a custom dictionary of regular expression patterns extracted from Latin morphological patterns. But again, a list of patterns could be written for any language and worked into this sub-lemmatizer. Furthermore, the sub-lemmatizers can be added or removed as necessary, and can be reordered based to optimize accuracy for a given language or language domain. Accordingly, the BackoffLemmatizer is particularly well-suited to less-resourced languages (Piotrowski, 2012, 85): a language without sufficient training data could build a backoff chain that ignores the UnigramLemmatizer and rely only on dictionary- and rules-based sub-lemmatizers. 
            
            Because of its multipass combination of probabilistic tagging based on existing Latin text, Latin lexical data, and a ruleset based on Latin morphology, the Backoff Lemmatizer can be described as following a philological method. By this, I mean that the process reflects the reading, decoding, and disambiguating strategies of the modern Latin reader (McCaffrey, 2006). For example, the process echoes the classroom process of Paul Diederich, who describes groups of students reading together and analyzing their text first through a combination of previous knowledge and dictionary lookups, but then “if no member of the group can clear up the difficulty, they resort to a formal analysis of the endings” (Hampel, 2014, 95). 
            One limitation of the current Backoff Lemmatizer setup is its binary sequential decision making; that is, a token is assigned a lemma based on the first match encountered in the backoff chain. By way of conclusion, I will discuss work in progress on a progressively scored Backoff Lemmatizer, or one that returns the lemma with the highest likelihood found after a token passes through and is assigned a score by every sub-lemmatizer in the chain. 
        
        
            
                
                    Bibliography
                    
                        Almas, B. (2013). Morpheus-Wrapper. https://github.com/PerseusDL/morpheus-wrapper (accessed 21 November 2017).
                    
                    
                        Bozzi, A., G. Cappelli, M. Passarotti, E. Pulcinelli, and P. Ruffolo. (1992). LemLat. http://www.ilc.cnr.it/lemlat/ (accessed 21 November 2017).
                    
                    
                        Celano, G. G. A., G. Crane, and B. Almas. (2017). The Ancient Greek and Latin Dependency Treebank. https://perseusdl.github.io/treebank_data/ (accessed 21 November 2017).
                    
                    
                        Eger, S., T. vor der Brück, and A. Mehler. (2015). Lexicon-Assisted Tagging and Lemmatization in Latin: A Comparison of Six Taggers and Two Lemmatization Methods, in Proceedings of the 9th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities: 105–13. 
                    
                    
                        McCaffrey, D. (2006). Reading Latin Efficiently and the Need for Cognitive Strategies, in When Dead Tongues Speak: Teaching Beginning Greek and Latin, ed. J. Gruber-Miller. New York: Oxford University Press. 
                    
                    
                        Hampel, R. L. (2014). Paul Diederich and the Progressive American High School. Charlotte, NC: Info Age. 
                    
                    
                        Juršič, M., I. Mozetic, T. Erjavec, and N. Lavrac. (2010). LemmaGen: Multilingual Lemmatisation with Induced Ripple-Down Rules. Journal of Universal Computer Science: 1190–1214. https://doi.org/10.3217/jucs-016-09-1190. 
                    
                    
                        Johnson, K. P. (2017). CLTK: The Classical Language Toolkit. https://github.com/cltk/cltk. (accessed 21 November 2017). 
                    
                    
                        Kestemont, M., and J. De Gussem. (2017). Integrated Sequence Tagging for Medieval Latin Using Deep Representation Learning. Journal of Data Mining &amp; Digital Humanities, Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages. https://arxiv.org/abs/1603.01597v2. 
                    
                    
                        Loper, E., S. Bird, and T. Tresoldi. (2017). NLTK 3.2.5 Documentation: nltk.tag.sequential. http://www.nltk.org/_modules/nltk/tag/sequential.html (accessed 21 November 2017). 
                    
                    
                        Ouvard, Y., and P. Verkerk. (2014). Collatinus Web. http://outils.biblissima.fr/en/collatinus-web/index.php (accessed 21 November 2017). 
                    
                    
                        Perkins, J. (2014). Python 3 Text Processing with NLTK 3 Cookbook. Birmingham, UK: Packt Publishing. 
                    
                    
                        Piotrowski, M. (2012). Natural Language Processing for Historical Texts. San Rafael, CA: Morgan &amp; Claypool Publishers 
                    
                    
                        Schmid, H. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees, In Proceedings of the Conference on New Methods in Language Processing, Manchester, UK. 
                    
                    
                        Springmann, U., H. Schmid, and D. Najock. (2016). LatMor: A Latin Finite-State Morphology Encoding Vowel Quantity. Open Linguistics 2(1). https://doi.org/10.1515/opli-2016-0019. (accessed 21 November 2017).
                    
                
            
        
    
6528	2018	
        
            
                What is the purpose of higher education? In the United States, this question dates back to at least the nineteenth century with the passage of the Morrill Acts of 1862 and 1890, and has taken on new urgency in an era of manufactured austerity and neoliberal crisis. In particular, scholars of critical university studies such as Christopher Newfield, Fred Moten and Stefano Harney, Sara Ahmed, Craig Steven Wilder and Roderick Ferguson critique the ways higher education often reproduces the very conditions of inequality it claims to challenge. Often, these compelling analyses are based on the investigation of conditions at a few representative universities, but through leveraging digital methodologies we can gain a wider perspective that enables a more comprehensive analysis of what universities put forward as their purpose.
            
            Our research advances these conversations through large-scale textual analyses of two data sets: university mission statements included in the U.S. Department of Education’s Database of Accredited Postsecondary Institutions and Programs and recent demand statements put forth by activist students. Mission statements offer a public-facing proclamation that bridge universities to larger communities and educational contexts. Often, they present idealized claims that reflect the university’s marketed brand. We use “university” broadly; our data set includes community colleges, public universities, private universities, research institutions, teaching-focused institutions, for-profit and nonprofit schools, and our analysis highlights the variation in their commitments to education. The second data set is a collection of student demands compiled by WeTheProtestors and the Black Liberation Collective, two social justice groups that are working to address institutional inequality across U.S. universities. In many cases, these demands are written to address the institutions of the official university mission statements we are working with, and range from private institutions like Yale University and Ithaca College, to public universities such as Iowa State and UCLA. These demands challenge existing institutional language and require analysis in their own right.
            With this research, we seek to answer two questions: 1) What do contemporary U.S. universities claim as their mission and vision? 2) How do these stated aims of education intersect or diverge with the demands of activist students calling for pedagogical, institutional, and social change? In analyzing this data, we draw from the insights of critical race, gender, and sexuality studies, which have long been sites of institutional critique. Coupling digital tools with a theoretical lens informed by activist pedagogy enables us to better apprehend the power structures and social dynamics at play in public-facing institutional documents and how those interface with the communities they are tasked with serving. By better understanding the professed commitments of academic institutions, we aim to contribute to the project of making education more just, equitable, and inclusive.
            This work is carried out through the web scraping of data, topic modeling, and statistical analysis in Python. Once analyzed, the raw data and findings are also rendered as interactive web-based data visualizations in JavaScript to make the research more accessible to the public and available for refactoring. Initial statistical textual analysis and data visualization that we have conducted has revealed interesting trends among public universities in contrast to demands put forth by students. Mission statements from state universities emphasize a commitment to the objectives of research, knowledge, and professionalism, and the endeavors of providing and serving, and learning and teaching. However, student demand statements have a more expansive understanding of education that stresses inclusivity and community, while also voicing concerns about race, gender, workers, and resources. By comparing and contrasting across data sets, we examine what each type of institution and group is seeking to achieve, and work to determine whether universities are serving the needs of student populations. When universities are more concerned with vocational skills training rather than challenging power hierarchies, structural inequalities, and the distribution of resources along embodied axes of race and gender, there is a clear disconnect between what institutions are offering students and what students in turn demand. If universities aren’t serving their students and communities, who are they serving?
            Our data set and programming files will be made publicly available in a code repository so that others who investigate higher education can perform their own research. While our focus is grounded in the specific histories of higher education in the United States, we hope that sharing this research at an international conference will encourage others to perform similar analysis of institutional and popular discourse in their countries, thus allowing for a more vibrant understanding of how higher education functions in different contexts. By inviting others to add data to our public repository from international institutions, we can begin to consider how globalization impacts learning institutions.
            In an effort to advance intercultural scholarly exchange, a Spanish translation of this research will be available online. 
        
    
6547	2018	
        
            
                
                
                    Figure
                    Heather Agyepong, disrupting an ordinary school day in KIT Theatre’s Alfred the Great Time Travel Adventure
                
            
            
                Introduction
                The Digital Ghost Hunt combines coding education, Augmented Reality and live performance into an immersive storytelling experience. S
                    tudents ten to eleven years old (Key Stage 2 in the UK) will explore the haunted Battersea Arts Centre with devices they’ve learned to program themselves. The key objective of The Digital Ghost Hunt is to present technology to students as an empowering tool, where coding emerges as – and fuses with – different forms of storytelling.  It seeks to shift the context in which students see coding and engage groups who may be uninterested in or feel excluded by digital technology, opening up an imaginative space through play for them to discover the creative potential of technology on their own terms.
                
                The Digital Ghost Hunt has been awarded funding through the UK Arts and Humanities Research Council (AHRC) New Generation of Immersive Experiences call, as part of an application led by Mary Krell, Senior Lecturer in Media Practice at the Centre for Material Digital Culture in the University of Sussex.  A ‘scratch’ – a prototype of the experience – will be developed by Elliott Hall of King’s Digital Lab and Tom Bowtell of Kit Theatre.  It will be performed at the historic Battersea Arts Centre with a two-form entry of students from local schools.
            
            
                Structure of the experience 
                The Digital Ghost Hunt is split into two parts.  The first part begins with a regular coding class that suddenly goes haywire. While the teacher is trying to restore order, the lesson will be interrupted by Ms. Quill, Deputy Undersecretary of Paranormal Hygiene (Ghost Removal Section).  She will enlist their help in the Ministry’s work as apprentice ghost hunters. Students will use a simplified Python library to program their ghost hunting devices, which are based on two microcomputers: the Raspberry Pi and the BBC Micro:bit. 
                The coding in the project will focus in particular on two learning goals of the UK’s National Curriculum: “Design, write and debug programs that accomplish specific goals, including controlling or simulating physical systems; solve problems by decomposing them into smaller parts,” and “Use sequence, selection, and repetition in programs; work with variables and various forms of input and output.”  It will teach students to take the overall goal of their devices – detecting ‘paranormal’ phenomena – and break it down into the discrete input, analysis and output tasks required, aided by the project’s abstracted libraries.  How they combine the functions of these libraries will be up to them, and will rely on their understanding of the fundamental logical structures of programming to analyse sensor data, apply it to an algorithm, and debug when things go wrong.  The project will also introduce students to embedded computing through the devices themselves. The emphasis will be on students taking ownership of their devices, deciding which of the ghost detectors they want to build and how it will work.
                The second part is a ghost hunt, an immersive experience combining Augmented Reality (AR) and live theatre.  Students will work together in small teams, using their devices to find objects and areas touched by the ghost.  These traces will be both virtual objects in Augmented Reality, and actual physical phenomena such as radio waves, ultraviolet paint, and high-frequency sound.  Each device will have different capabilities, forcing the students to work together to get all the clues.  The ghost will in turn communicate with them, given life by actors, practical effects and the poltergeist potential of the Internet of Things.  Only by using the devices they have programmed and working together can students unravel the mystery of why the ghost is haunting the building and set it free.
            
            
                Coding, play and performance
                
                    
                    
                        Figure
                         A proof of concept ghost hunting device using Lego and the Micro:Bit
                    
                
                Young people’s familiarity with digital products are increasing, but their interest in learning the technology behind it is not, as evidenced in the UK by the low take up of the new GCSE in Computer Science (BCS, 2017).  T
                    eaching coding in schools is promoted by the UK Department of Education (DOE, 2014), but students often experience coding education as a classroom assignment, divorced from their intuitive and creative experiences with commercial digital applications.
                
                There are several applications now using AR as a teaching tool (for example, The Battle of Mount Street Bridge (Schreibmen et al., 2017) and Virtual Roman Frontiers (Wilson at al.)) and initiatives to teach children coding, from commercial apps to coding clubs and the work of the Raspberry Pi and Micro:Bit foundations.  These applications all seek the increase in engagement and experimentation that can occur when ‘work’ is reframed as ‘play.’ (Pellegrini, 2009)
                However, these applications all take place within a screen, an approach that creates its own problems. A screen can shift a user’s attention to the digital environment to the exclusion of the physical one. (Chrysanthi, 2012) The Ghost Hunt’s approach is to bring AR interaction fully into the physical space without the mediating influence of a screen, reconnecting audiences to the world around them.  
                The addition of immersive theatre reframes the experience again, from ‘play’ to ‘performance.’ This second shift is important to reach groups not engaging with existing digital resources. In 2016, girls made up just 20% of entrants for the computer science exam, while pupils on free school meals made up just 19% of GCSE entrants even though they are 27% of the population (Cellan-Jones, 2016).  Performance may draw in groups who would otherwise be uninterested in or feel excluded from traditional Computer Science education.  
                However, the performance should not be seen as secondary in any way to the coding elements of the project. The aim of the project is to expand the imaginative possibilities of digital technology through play; the coding elements are the means to that end, not the other way around.  The Ghost Hunt seeks to shift how the context of computer science is perceived, from a skill intended only for a narrow group to a tool of creativity and play available to all.
                 
                As part of its evaluation, the project will use the student’s code and feedback from educators on how the software libraries are used, as well as video, audio and device logging during the experience.  It will be direct engagement with participants through formal and informal methods such as interviews, questionnaires and the creative material they create as part of the experience that will provide the crucial method of evaluation. The only way to assess the pedagogical value of the project in terms of creating a new and sustained interest in the possibilities of digital technology will be if students create new things on their own initiative, independent from the project’s setting and materials.  This metric is beyond the scope of the pilot project but is something the project team are eager to explore in subsequent phases in collaboration with the educational partners.
            
            
                Beyond the hunt
                The lessons of the Digital Ghost Hunt scratch funded by the AHRC will direct refinement of the existing tools towards developing a technical and conceptual framework that can be adapted and implemented for different locations, stories and audiences. This short paper aims to present the practice-based collaborative framework of the Digital Ghost Hunt as conceived by its creators in its first funded iteration to elicit feedback from the Digital Humanities 2018 participants and integrate it into future development. 
            
        
        
            
                
                    Bibliography
                    
                        British Chartered Institute for IT (BCS). (2017) [online] 
                         BCS deeply concerned over stagnation of number of Computer Science GCSE applicants.  Available at: 
                        
                            http://www.bcs.org/content/conWebDoc/57904
                        
                        [Accessed 23/11/2017]
                    
                    
                        Cellan-Jones, Rory
                        .  (2017).  Computing in schools - alarm bells over England's classes.  
                        BBC News.  [online.]  Available at: 
                         
                        
                            http://www.bbc.co.uk/news/technology-40322796
                        
                         [Accessed 23/11/2017]
                    
                    
                        Department of Education (DOE).  (2014.)  Teaching children to code.  In: 
                        D5: London.  London.  Available at:  
                        
                            https://www.gov.uk/government/publications/d5-london-summit-themes/d5-london-teaching-children-to-code
                         [Accessed 23/11/2017]
                    
                    
                        Pellegrini, A. (2009) 
                        The role of play in human development. Oxford: Oxford University Press.
                    
                    
                        Chrysanthi, A., Papadopoulos, C., Frankland, T., and Earl, G. (2013). ‘Tangible Pasts’: User-centred Design of a Mixed Reality Application for Cultural Heritage. In: 
                        Conference of Computer Applications and Quantitative Methods in Archaeology. Southampton: Amsterdam University Press, pp. 31-41.
                    
                    
                        Schreibman, S, Papadopoulos, C., Hughes, B., Rooney, N., Brennan, C., Fionntann, M., Healy, H.  
                        Phygital Augmentations for Enhancing History Teaching and Learning at School.  In: 
                        Digital Humanities 2017
                        . Montreal.  [online] Available at: 
                        
                            https://dh2017.adho.org/abstracts/401/401.pdf
                         [Accessed 23/11/2017]
                    
                    
                        Wilson, L.,  Weeks, P, Rawlinson A., Dobat, E., Fluegel, C., Hermann, C
                        . (2017).  Virtual Roman Frontiers: 3D Visualisation and Innovative Technology Applications for the Antonine Wall.  In: 
                        3D Imaging in Cultural Heritage.  London: The British Museum. Available at: 
                        https://www.3dimaginginculturalheritage.org/resources/3D_Imaging_in_Cultural_Heritage_Abstracts.pdf [Accessed 23/11/2017]
                    
                
            
        
    
6554	2018	
        
            Background
            The infrastructures that we use to navigate the world often become invisible as they become indispensable (Bowker and Star, 2000). However, critical examination of information systems is necessary to understand their implicit biases, and the ways that they invite some types of engagement and restrict others. Structures of power continue to be replicated in the ways that technologies are deployed in our lives (Noble, 2016; Tufekci, 2016), and the inability to access and assess the standards which make digital communication possible risks the uncritical perpetuation of those power structures (Drabinski, 2013). The moments of rupture, when an established system takes on a new facet with unintended consequences, can be an important moment of visibility, where we are able to reveal its ideological foundations, and the ways that its users adapt their own behaviors to it, or push back against its uncomfortable constraints (Raley, 2006; Marino, 2007). The introduction of emojis to the Unicode Standard, and their widespread adoption over the decade from 2006-2017 is one such moment of transition.
             Scholars of standards and standardization argue that the input of users is necessary for a standard to meet the needs of those users (Foray, 1994), and while the process of adding content to the Unicode Standard remains rigid, the unicode.org website provides an explicit record of the development and evolution of the face that Unicode presents to its users, and is able to be read as a text which reveals the contemporary state of Unicode and the cultural ideologies which shape it.
            Methodology
            While major language- and script-based additions are made with each update to the Unicode Standard, my analysis focuses on changes to the unicode.org website, and its role as an intermediary document between the Consortium, the Standard itself, and everyday users. The introduction of emojis in various updates to the Standard has resulted in changes to the content and structure of the unicode.org website that reflect an increased engagement with end users, which I argue is the result of increased semantic value of emoji characters for the user
                
                     A notable exception to this semantic shift is written Chinese, which is already a semantic-character-based language, as opposed to syllable- or alphabet-based, as are the rest of the world’s major languages. Thomas S. Mullaney gives a thorough historical analysis of the implication of this on text-encoding technologies in 
                        The Chinese Typewriter (MIT Press, 2017). 
                    
                , as compared to an individual character in a language's written script. It is my intention, through this analysis, to describe the types of changes that happen to the governing body and public documents of Unicode as major changes happen to the Standard itself.
            
            A timeline was created of the dates of major updates to the Unicode Standard since its introduction in 1991, using the official release dates for updates to the Unicode Standard as maintained by the Unicode Consortium. I cross-reference this document with the rollout of each new version by the major platforms
                
                     https://unicode.org/emoji/format.html#col-vendor lists the major “vendors” of emojis, or platforms with proprietary visual displays of emojis. These vendors are Apple, Google, Twitter, Facebook, Facebook Messenger, Windows, and Samsung.
                , with a particular emphasis on updates featuring new emoji characters, beginning with Unicode 6.0 in 2010
                
                     While the first major batch of emojis were incorporated into Unicode in 2010, and the first official “Emoji 1.0” release was in 2015, work has been done within the standard since late 2006 to consider the addition and management of emoji-like characters within Unicode— hence the specific 2006-2017 emphasis of this research. (https://www.unicode.org/reports/tr51/#Introduction)
                . 
            
            With this timeline in mind, I scraped the unicode.org domain using Python and the Beautiful Soup
                
                     https://www.crummy.com/software/BeautifulSoup/
                 library to collect the URLS of all the unique pages under the parent domain, as well as a table of links between those pages. This serves as a source-target list for the creation of a network visualization of the unicode.org domain, using the network visualization software Gephi.
                
                     http://gephi.io
                 This process is repeated using archived versions of the unicode.org site, available from the Internet Archive’s Wayback Machine
                
                     https://web.archive.org/
                , resulting in several structural snapshots of the unicode.org website over time, which can then be overlaid and compared to one another to note particular areas of change within the site.
            
            Additionally, using points of change within the site structure as a guide, I also collect and code page content data to reflect the type of changes made to those pages during each major update. This coding is done on two axes: The first labels each change as being content- or structure-based (eg. adding text or links to a page, respectively), and the second designates which aspect of the Standard and/or Consortium is being addressed by the change. Examples of this second type of labelling would be “Emoji,” “Membership,” “Meta-Documentation,” or “Language Scripts.” This coding is done in two phases— an initial survey of this data in order to formally create labelling categories, and then a closer examination of the updates to apply those labels.
            Discussion and next steps
            This research project addresses issues of digital infrastructure from a unique angle: one that considers the socially-constructed nature of technology, as well as the meta-narrative of maintenance and upkeep of a system that has become crucial to our ability to communicate in a digital world. Through analysis of the secondary documents relating to the Unicode Standard, it is possible to gain invaluable insights into the ways that knowledge is organized collectively and continuously, as well as the embedded values that shape who can access and influence that knowledge.
            
                This case study will provide a foundation for more expansive examination of systems of digital infrastructure. It is a beginning point both for further analysis of the adoption and adaptation of Unicode (and emojis in particular), but also as a framework for examining other forms of scaffolding which uphold the content of digital spaces. 
            
        
        
            
                
                    Bibliography
                    
                        Bowker, G. C., and Star
                        , S. L. (2000). 
                        Sorting things out: Classification and its consequences
                        . Cambridge: MIT Press.
                    
                    
                        Drabinski, E. (2013). Queering the catalog: queer theory and the politics of correction. 
                        The Library Quarterly 83(2): 94-111. doi:10.1086/669547
                    
                    
                        Foray, D
                        . (1994). Users, standards and the economics of coalitions and committees. 
                        Information Economics and Policy
                        , 
                        6
                        (3): 269-293.
                    
                    
                        Marino, M. C. (2007, December 4). Critical code studies. 
                        Electronic Book Review. Retrieved from http://electronicbookreview.com/thread/electropoetics/codology
                    
                    
                        Noble, S.U. (2016). A future for intersectional black feminist technology studies. 
                        The Scholar &amp; Feminist Online. 13.3 - 14.1. Retrieved from: http://sfonline.barnard.edu/traversing-technologies/safiya-umoja-noble-a-future-for-intersectional-black-feminist-technology-studies/0/ 
                    
                    
                        Raley, R. (2006). Code.surface || Code.depth, 
                        Dichtung Digital. Retreived from http://www.dichtung-digital.org/2006/01/Raley/index.htm
                    
                    
                        Tufekci, Z. (2016, June). 
                        Machine intelligence makes human morals more important. [Video file]. Retrieved from https://www.ted.com/talks/zeynep_tufekci_machine_intelligence_makes_human_morals_more_important
                    
                
            
        
    
6558	2018	
        
            This Digital Humanities project is an interdisciplinary project effort that uses the lens of, and data from, the U.S. TV show 
                Seinfeld to explore questions about television and other media. 
                Seinfeld
                 has significant cultural influence over other media, but what is its 
                reach, meaning the many other media items cast and crew worked on, also known as the 
                overlap? We are starting with data from the Internet Movie Database (IMDb). This makes this project somewhat different from other Digital Humanities projects as we’re using an existing database rather than primary sources. An associate professor of media studies, accustomed to conducting critical analysis of television shows, and an associate professor of information systems, more used to working with non-media studies data, are working to populate a relational database, to use quantitative analysis, and a social science theory--social network theory, particularly “Small Worlds” theory--to explain trends in media industries, including questions of genre, gender, race, and age in entertainment businesses.
            
            
                Seinfeld (NBC 1989-1998) was a US-based half-hour, multi-camera, situation comedy, one of several that featured stand-up comics in stories similar to their own lives. Although it ended nearly 20 years ago, it
                 
                heavily influences TV shows of today, including “hangout” sitcoms, one-camera comedies featuring conversation and digression, and antihero dramas. Journalist Jennifer Keishen Armstrong writes in the bestselling
                Seinfeldia
                that the show “snuck through the network system to become a hit that changed TV’s most cherished rules; from then on, antiheroes would rise to prominence, unique voices would invade the airwaves, and the creative forces behind shows would often gain as much power and fame as the faces in front of the cameras” (Armstrong, 2016). It’s a singularly important show for a variety of reasons.
            
            
                Clearly, 
                Seinfeld
                has significant cultural impact on other shows and movies, but what we wanted to know is, what is its ’reach’? Reach is defined as other media that texts cast and crew from
                
                    Seinfeld
                
                worked on before, during, and after their appearance(s) on the show. Such texts exist in every media type (movies, video games, web-based media). When two media items share cast/crew, we look for overlap.
            
            
                Dr. Conaway worked on the project for two years, using cut and paste and Excel spreadsheets for items and people, before involving Dr. Shichtman, who has created a relational database that may be searched. We first used MySql and an Amazon Web Services server, have recently shifted to the college's virtual machine and the Oracle database management system. We involved two students in a grant funded practicum in the Fall term as well.
            
            
                Our research revealed that the 1551 cast/crew had worked on over 32,500 other discrete media texts, starting in 1936, and with many texts still on the air today, often with an overlap of more than one. Nearly every televison series, TV movie, and TV special we could think of included overlap. Only recently, in “peak TV”—in which there are over 500 scripted TV shows in production this year alone, in addition to reality, sports, and news shows (many of which also have overlap)—are we seeing well-known US TV series with no overlap. Our research found that although most were US-based, there were media items from over 60 countries.
            
            
                Social network theory would help us answer some questions. As Duncan Watts writes in 
                Six Degrees: the Science of a Connected Age
                , "Affiliation networks . . . are . . . networks of overlapping cliques, locked together via the comembership of individuals in multiple groups" (Watts, 2004). Small worlds theory discusses how networks of people influence each other, and each others’ connections. 
            
            
                Questions include, what genres did the cast/crew, presumably chosen for a common comic sensibility, work on other than comedy? What genres included the most cast/crew? What genres have less overlap, none at all, and what might be some reasons for that? What is the importance of gender, race, and age?
            
            
                We looked for other, similar projects that used IMDb and found that there were few that did. Some computer scientists had used IMDb to trace the overlaps among actors involved in ’adult‘ films in the database as an example of a ’small world‘ environment. Media History scholars had traced ’race films‘ that ended before our database started, and Digital Humanities scholars used it to look at patterns of exhibition of films or specifically how Australians worked together, but not to examine how cast circulated among media.
            
            IMDb, it turns, out, is a challenging tool for this purpose. 
                Deb Verhoeven, Associate Dean of Engagement and Innovation of the University of Technology Sidney, who has done a lot of Digital Humanities work on Australian films explained in 2012 that IMDb consists of “elaborated sets of lists” created by fans, writing:
            
            Accordingly, the primary users of filmographic catalogues are not cinema historians, information managers, analytical filmographers, or cinema scholars, but members of the public, film buffs, students and so on who are content to navigate these databases using the small number of structured search fields provided. (Verhoeven, 2012) 
            IMDb, which started in the early 1990s, is very robust, and provides information for free download using Python, but is not usable ’as is.’ Entries may be misleading, incomplete, or unclear, with genres in particular organized in unhelpful ways. The Downloadable information includes the full cast and some types of crew members, but not others. In addition, the fields of the two faculty members made shared vocabulary difficult, and getting complete and clean data that could be turned into tables and graphs meant conducting additional research outside of IMDb, and reorganizing the data significantly from the way Dr. Conaway initially tagged it. SUNY Empire State College also lacks the structures that many institutions have for conducting Digital Humanities work.
            However, we have been able to create some early data visualizations that will show a microcosm of how the US entertainment industry works for various types of actors and crew members, using specifically the data from television programs. We’ve compared 
                Seinfeld’s numbers of actors and crew to that of other shows, analyzed how the media items break down by genre, and visualized how women’s careers wax and wane in different patterns from men’s careers. In the future we will do the same for subgenres, actors of color, and actors of various age groups. 
            
        
        
            
                
                    Bibliography
                    
                        Armstrong, J.K., 2016. 
                        Seinfeldia: How a Show about Nothing Changed Everything. Simon and Schuster.
                    
                    
                        Bajak, A. 2017. 
                        Seinfeld, big data and measuring the Internet's emotional landscape.
                        Mediashift.
                    
                    
                        Gold, M.K. 2012. 
                        
                            Debates in the Digital Humanities
                        . University of Minnesota Press.
                    
                    
                        Gold, M.K. and Klein, L.F., 2014. 
                        
                            Debates in the Digital Humanities
                        . University of Minnesota Press. 
                    
                    
                        Lavery, D. And Dunne, S.L. 2006. 
                        Seinfeld, Master Of its Domain. New York: Continuum.
                    
                    
                        Verhoeven, D.
                        New cinema history and the computational turn. Beyond Art, Beyond Humanities, Beyond Technology: A New Creativity, Proceedings Of the World Congress Of Communication and the Arts Conference, University Of Minho, Portugal. 2012
                    
                    
                        Watts, D.J. 2004. 
                        Six Degrees: The Science Of a Connected Age. WW Norton &amp; Company.
                    
                
            
        
    
6561	2018	
        
            Resumen
            ¿Los nodos centrales de una red social de personajes son los protagonistas de una obra de teatro? Para responder a esta pregunta utilizamos diferentes medidas de centralidad junto con otros valores cuantitativos textuales en un corpus anotado de obras dramáticas de teatro español correspondientes a la Edad de Plata (1868-1936). Los resultados señalan que la centralidad está en correlación moderada con la importancia, siendo mayor la correlación con valores cuantitativos textuales.
            Introducción
            La representación de personajes literarios mediante grafos y redes sociales (Marcus 1973, Moretti 2011) aporta nuevas herramientas al estudio literario. La interpretación del concepto de centralidad en grafos (Jannidis et al., 2017) ha sido investigada en su aplicación a las obras literarias (Moretti 2011; Rochat 2014; Trilcke et al. 2015 y 2016; Jannidis et al., 2016, Rodríguez 2016; Algee-Hewitt 2017). En la tradición hispánica, se han utilizado enfoques cuantitativos para analizar la densidad versal en obras del Siglo de Oro (Hermenegildo 1994 y Espejo 2002), estudiar tanto contenido simbólico y sociopolítico de los personajes de Galdós (Menéndez 1983), así como el origen social o caracterización de los personajes de Lope de Vega (Oleza 1984 y Oleza 2013).
            En este trabajo queremos evaluar cuatro preguntas:
            1. ¿Qué tipo de correlación hay entre las medidas de centralidad y la importancia del personaje?
            2. ¿Aparecen los personajes más importantes al comienzo del 
                dramatis personae?
            
            3. ¿Hay correlación entre importancia y valores textuales (cantidades de unidades textuales del personaje)?
            4. ¿Qué valores podríamos utilizar para distinguir a los protagonistas del resto?
            Textos y metadatos
            A diferencia de otras lenguas europeas, el español no cuenta con un gran corpus teatral anotado en XML-TEI. El proyecto 
                Biblioteca Electrónica Textual del Teatro en Español de la Edad de Plata (1868-1936) (BETTE) ha publicado veinticinco obras en XML-TEI de Lorca, Valle, Galdós, Clarín o Muñoz Seca, como repositorio GitHub (María Jiménez et al., 2017). En la versión 2.0 cada personaje ha sido anotado con diferentes metadatos:
            
            • Sexo
            • Papel en la obra (protagonista, amante, antagonista u otro)
            • Naturaleza (persona, animal, no humano...)
            • Importancia (personaje primordial, secundario o terciario)
            • Persona individual frente a grupo
            Además, se añadieron una serie de valores textuales cuantitativos de manera automática:
            • Posición en el dramatis personae (castList)
            • Cantidad de texto que pronuncia
            • Cantidad de intervenciones
            • Cantidad de referencias a su nombre
            • Cantidad de escenas en las que aparece
            Aquí un ejemplo de esa información en XML-TEI:
            
                
            
            Fig. 1. Metadatos de personaje en XML-TEI
            El valor de importancia fue asignado según los siguientes criterios:
            • Minor: si el personaje no aparece en el resumen (contenido también en el archivo TEI)
            • Secondary: si aparece en el resumen 
            • Primary: si pertenece al grupo de entre dos y cuatro personajes esenciales
            De esta manera por cada personaje (con un total de 516) tenemos:
            1. Un valor de su importancia dentro de la obra (que puede ser utilizado como 
                ground truth)
            
            2. Diferentes valores cuantitativos textuales
            3. Posición en dramatis personae
            4. Diferentes valores según medidas de centralidad
            Metodología
            La implementación para extraer, analizar, evaluar y visualizar los datos se realizó en Python mediante librerías como 
                lxml y 
                networkx. Para la creación de las redes sociales se definió la arista no direccional como la coaparición en escenas (la definición más frecuente en trabajos de este tipo): 
            
            
                
            
            Fig. 2. Red social de personajes en 
                La zapatera prodigiosa de Lorca
            
            A partir de estas redes sociales, calculamos diferentes medidas de centralidad e información sobre los nodos:
            • Degree
            • Betweenness centrality
            • Eccentricity
            • Closeness centrality
            • Load centrality
            • Current flow betweenness centrality
            • Eigenvector centrality
            • Approximate current flow betweenness centrality
            • Communicability centrality exp
            Resultados
            Analizamos la dependencia entre la importancia y el resto de valores, calculado su correlación (Spearman) 
            
                
            
            Fig. 3. Boxplots y correlaciones con importancia de todas las obras de BETTE
            Ninguna de las medidas de centralidad tiene una correlación fuerte (&gt; 0.6 o &lt; -0.6 según Evans 1996). El valor máximo (0.51 en correlación negativa) es de 
                current flow betweenness centrality, también conocida como 
                information centrality (Brandes and Fleischer 2005; Stephenson and Zelen 1989), medida que no está entre el repertorio usual de las HD.
            
            En cuanto a la posición en el 
                dramatis personae, la correlación es solo de 0.42, con una fuerte dispersión, aunque los primeros y terceros cuartiles de personajes primarios y terciarios se posicionan en rangos totalmente diferentes. Es decir, la posición en el dramatis personae sí parece aportar cierta información sobre la importancia, aunque no podemos utilizarlo de manera exclusiva (p.ej. Muñoz Seca los ordena por sexo). 
            
            En tercer lugar, las medidas de cuantitativas textuales tienen todas correlaciones notablemente más altas, llegando hasta 0.67 en la cantidad de intervenciones.
            Ante estos resultados, nos hemos preguntado si las medidas cuantitativas textuales tienen el mismo tipo de correlación con las medidas de centralidad, en concreto si la information centrality tiene una correlación más fuerte que el resto (calculando Spearman o Pearson, dependiendo si las variables son continuas u ordinales):
            
                
            
            
                Fig. 4. 
                Scatterplots 
                mostrando correlación entre las veces que un personaje habla (&lt;sp&gt;s) y otros valores
            
            Como se observa 
                current flow betweenness (o
                 information) centrality, de nuevo, es la medida de centralidad con la correlación más fuerte con la cantidad de intervenciones.
            
            Finalmente hemos observado si la distribución de centralidad o valores textuales son diferentes para los personajes protagonistas de los del resto:
            
                
            
            
                Fig. 5. Boxplots de protagonistas frente al resto de personajes
            
            La mayor diferenciación de ambos 
                boxplots entre las medidas de centralidad se consigue mediante 
                current flow betweenness (o 
                information) 
                centrality. El solapamiento menor se consigue mediante la cantidad de texto pronunciado (pers_mes_characteres). La posición relativa en el 
                dramatis personae en este caso consigue diferenciar de manera bastante clara los protagonistas del resto de personajes.
            
            Conclusiones y futuros pasos
            La anotación en detalle de información sobre los protagonistas nos permite evaluar métodos digitales. En concreto seguimos la propuesta de Moretti (2013) de abandonar la división binaria de personajes, incluyendo en nuestro caso los valores de personajes secundarios.
            Nuestros resultados muestran que, para el caso del corpus BETTE y con las formalizaciones arriba explicadas:
            1. La importancia tiene una correlación solamente entre débil y moderada con cualquier formalización de centralidad, teniendo la correlación más fuerte la 
                information centrality
            
            2. La posición en el 
                dramatis personae puede ser un indicador sobre el protagonismo de personajes o la diferenciación entre primarios y terciarios, pero no para diferenciar a estos de los secundarios
            
            3. Los valores cuantitativos textuales tienen correlaciones más fuertes. Este tipo de unidades son también las que mejor clasificarían personajes entre protagonistas y no protagonistas
            4. Es sorprendente que unidades textuales más sencillas que la centralidad en redes aporten más información tanto sobre la importancia de los personajes, así como su papel de protagonistas.
            Como otros trabajos en redes sociales (cf. Moretti 2011 y 2013; Rochat 2014) hemos trabajado con una cantidad reducida de textos. Nos gustaría comprobar estas hipótesis en mayores corpus literarios. También nos gustaría analizar los efectos que subgéneros literarios, períodos y autores ejercen sobre estos valores.
        
        
            
                
                    Bibliografía
                    
                        Algee-Hewitt, Ma. (2017). 
                        Distributed Character: Quantitative Models of the English Stage, 1500-1920. Montréal: McGill University &amp; Université de Montréal, pp. 119–21.
                    
                    
                        Brandes, U. and Fleischer, D. (2005). Centrality Measures Based on Current Flow. 
                        Theoretical Aspects of Computer Science (STACS ‘05). Springer-Verlag, pp. 533–44 http://www.inf.uni-konstanz.de/algo/publications/bf-cmbcf-05.pdf.
                    
                    
                        Espejo, J. (2002). Algunos aspectos sobre la construcción del personaje en el teatro conservado de Hernán López de Yanguas (1487-¿?). 
                        Scriptura, 17, pp. 113-132.
                    
                    
                        Evans, J. D. (1996). 
                        Straightforward Statistics for the Behavioral Sciences. Pacific Grove: Brooks/Cole Pub. Co.
                    
                    
                        Gómez, S., Calvo Tello, J., González, J. M. and Vilches, R. (2015). Hacia una biblioteca electrónica textual del teatro en español de 1868-1936 (BETTE). Texto Digital, 11(2), pp. 171–84.
                    
                    
                        Hermenegildo, A. (1995). Personaje y teatralidad: la experiencia de Juan del Encina en la Égloga de Cristino y Feba. In Pedraza, F.B. y González, R. (ed
                        .). Los albores de teatro español: actas de las XVII Jornadas de teatro clásico Almagro, julio de 1994. Almagro: Universidad de Castilla-La Mancha, pp. 90-113.
                    
                    
                        Jannidis, F., Reger, I., Krug, M., Weimer, L., Macharowsky, L. and Puppe, F. (2016). Comparison of Methods for the Identification of Main Characters in German Novels. 
                        DH2016. Krakow: ADHO, pp. 578–82 http://webcache.googleusercontent.com/search?q=cache:LjYz88cQhboJ:dh2016.adho.org/abstracts/297+&amp;cd=1&amp;hl=es&amp;ct=clnk&amp;gl=de&amp;client=ubuntu.
                    
                    
                        Jannidis, F., Kohle, H. and Rehbein, M. (eds). (2017). 
                        Digital Humanities: eine Einführung. Stuttgart: J.B. Metzler Verlag.
                    
                    
                        Jiménez, C., Martínez Carro, E., Santa María, M. T., Calvo Tello, J., Simón Parra, M., Martínez Nieto, R. B. and García Sánchez, M. (2017). BETTE: Biblioteca Electrónica Textual del Teatro en Español de la Edad de Plata. 
                        Sociedad, Políticas, Saberes. Málaga: HDH, pp. 88–91 http://hdh2017.es/wp-content/uploads/2017/10/Actas-HDH2017.pdf.
                    
                    
                        Marcus, S. (1973). 
                        Mathematische Poetik. (Trans.) Mândroiu, E. Bucureşti; Frankfurt/Main: Editura Academiei ; Athenäum Verlag.
                    
                    
                        Menéndez, C. (1983). 
                        Introducción al teatro de Benito Pérez Galdós. Madrid: CSIC.
                    
                    
                        Moretti, F. (2011). Network Theory, Plot Analysis. 
                        The New Left Review (68), pp. 80–102.
                    
                    
                        Moretti, F. (2013). “Operationalizing”: or, the function of measurement in modern literary theory. 
                        The New Left Review (84), pp. 103-119.
                    
                    
                        Oleza Simó, J. (2013). 
                        Biblioteca Digital Arte Lope. Valencia: Universitat de València. artelope.uv.es/biblioteca.
                    
                    
                        Rochat, Y. (2014). 
                        Character Networks and Centrality. N.p. Web.
                    
                    
                        Rodríguez, D.I. (2016) 
                        Análisis de grafos en paralelo mediante Graphx. Trabajo de titulación. Universidad Católica de Loja. Ecuador. 
                    
                    
                        Stephenson, K. and Zelen, M. (1989). Rethinking centrality: Methods and examples. 
                        Social Networks, 11(1): 1–37 doi:10.1016/0378-8733(89)90016-6.
                    
                    
                        Trilcke, P., Fischer, F., Göbel, M. and Kampkaspar, D. (2016). Dramen als small worlds? Netzwerkdaten zur Geschichte und Typologie deutschsprachiger Dramen 1730-1930. In Burr, E. (ed), 
                        DHd 2016 Modellierung, Vernetzung, Visualisierung. Leipzig: DHd/nisaba, pp. 254–57.
                    
                    
                        Trilcke, P., Fischer, F. and Kampkaspar, D. (2015). Digitale Netzwerkanalyse dramatischer Texte. 
                        DHd-Tagung. Graz http://gams.uni-graz.at/o:dhd2015.v.040.
                    
                
            
        
    
6571	2018	
        
            In the wake of Michael Brown’s murder in Ferguson, Missouri, on August 9, 2014, and the non-indictment of police officer Darren Wilson on November 25, 2014, backlashing protests and riots took to the streets of Ferguson and to other major American cities across the country. They also took to the Twittersphere. A national conversation about police brutality and the American criminal justice system exploded on Twitter during this time period, eventually elevating the hashtag #Ferguson, tweeted over 27 million times, to the most frequent in Twitter’s ten-year history, and the hashtag #BlackLivesMatter, tweeted over 12 million times, to third place (Sichynsky, 2016). First coined by Alicia Garza, Patrisse Cullors, and Opal Tometi in July 2013, the hashtag #BlackLivesMatter became a banner for a national protest movement and an index for conversations about the systematic devaluing and elimination of black life. Over the last five years, literary scholars and historians have noted that, within this massive social media movement, the novelist, essayist, and civil rights literary icon James Baldwin seemed to be often and increasingly invoked (Maxwell, 2016). The perceived frequency of Baldwin-related tweets has been pointed to by many as evidence of the Harlem-born author’s 21
                st-century resurrection and recent political resonance (Glaude Jr., 2016; Robinson, 2017). Because tweets can be digitally archived and made computationally tractable, they can be collected, measured, and analyzed at scale, and they can offer a picture of Baldwin’s social media reception that goes beyond perception and anecdotal evidence. This talk will share work-in-progress from my project 
                Tweets of a Native Son (
                http://www.tweetsofanativeson.com/), which brings large-scale social media data and computational methods to bear on Baldwin’s 21
                st-century remediation, recirculation, and reimagination. This talk will discuss the methods and progress made in the project thus far, argue that social media analysis might usefully contribute to a growing body of computationally-assisted scholarship focused on readership, reception, and textual circulation, and finally gesture to how such an approach might change our understanding of how texts are shared between communities of people, namely through its emphasis on networks.
            
            Methods, Analysis, Initial Findings
            First I “hydrated,” that is, retrieved the full JSON information for, an archive of over 32 million tweets that were sent between June 1, 2014 and May 31, 2015 and that mentioned Ferguson, Black Lives Matter, and 20 other black individuals who were killed by the police during this time period, which was first purchased from Twitter and shared by Deen Freelon, Charlton McIlwain, and Meredith D. Clark (Freelon, McIlwain, Clark, 2016). I next searched for all the tweets that mentioned “James Baldwin” by his first and last names using the Python and command-line tool “twarc” and the command-line JSON processor “jq,” which returned 7,326 tweets and retweets. By using twarc utilities, a k-means clustering algorithm, and manual tagging, I then identified the most retweeted tweets in the archive and the text that appeared most often across all tweets in the archive, which revealed that the most frequent appeal to Baldwin during this time period was through quotation and overwhelmingly through the quotation of Baldwin’s 1960s-era essays, radio interviews, and television appearances. 
            By studying the text of the most retweeted and most frequently cited tweets, and by tracing tweeted Baldwin quotations back to their literary and historical origins, my project argues that Baldwin’s appeal as a #BlackLivesMatter muse comes, at least in part, from the remediation of much of his non-fictional work into YouTube videos and free online essays; from his aphorisms with deep roots in African American written and oral traditions; and from his sympathetic proximity to but never full embrace of black radicalism. Another goal of 
                Tweets of a Native Son, however, is to let others explore, hypothesize, and learn about Baldwin’s #BlackLivesMatter-related social media reception through a series of interactive data visualizations on the project’s website. These interactive visualizations are meant to provide a perspective on Baldwin’s living legacy, a refracted vision of Baldwin’s life and career through those who actively called upon him in a moment of political and emotional urgency, a means by which others can come to their own conclusions about Baldwin’s resurrection.
            
            DH Reception Studies and Networked Reading
            
                Tweets of a Native Son most broadly hopes to join and affirm recent digital humanities work that is trained on readership, reception, and textual circulation, such as Lincoln Mullen’s 
                American’s Public Bible and Ryan Cordell and David Smith’s 
                Viral Texts, and to amplify Katherine Bode’s call that the digital humanities better attend to and account for the ways in which literary texts “circulated and generated meaning together at particular times and places” (Mullen, 2016; Cordell and Smith, 2017; Bode, 2017). Like the 19
                th-century newspaper archives used by Mullen, Cordell, and Smith, social media archives offer a window into how texts travel, how texts are used and changed by individuals, and what these texts mean in context. Social media archives additionally offer massive amounts of (relatively) clean, recent data. Though of course with these advantages, they also present more ethical challenges, since this data is often tied to corporations and produced by still-living human beings whose consent, possible harm, and creative attribution must always be considered.
            
            Finally, however, I believe that social media data might help us better theorize and make visible the networked structures of readership, reception, and textual circulation, because social media data, such as Twitter data, is often inherently networked in structure, recording retweets, replies, follower communities, hashtag communities, and more. This networked structure emphasizes the way that texts are not only engaged with by individuals but are shared between individuals, taking on social and communal meanings. For the particular case of Baldwin and #BlackLivesMatter in 2014-2015, the quotations of Baldwin’s words were often recirculated as coalition- and community-building material, helping to forge connections between individuals across space, time, and American history. During the future stages of this project, I hope to employ network science and network visualization to better understand Baldwin’s significance within #BlackLivesMatter. 
        
        
            
                
                    Bibliography
                    
                        Bode, K. (2017). The Equivalence of “Close” and “Distant” Reading; or, Toward a New Object for Data-Rich Literary History. 
                        Modern Language Quarterly, 78(1): 94. 
                    
                    
                        Cordell, R. and Smith, D. (2017). Viral Texts: Mapping Networks of Reprinting in 19th-Century Newspapers and Magazines, 
                        http://viraltexts.org.
                    
                    
                        Freelon, D., McIlwain, C. D., and Clark, M. D. (2016). Beyond the hashtags: #Ferguson, #Blacklivesmatter, and the online struggle for offline justice. Center for Media and Social Impact. 
                    
                    
                        Glaude Jr., E. S. (2016). James Baldwin and the Trap of Our History. 
                        Time. 
                    
                    
                        Maxwell, W. J. (2016). Born-Again, Seen-Again James Baldwin: Post-Postracial Criticism and the Literary History of Black Lives Matter. 
                        American Literary History, 28(4).
                    
                    
                        Mullen, L. (2016). America’s Public Bible: Biblical Quotations in U.S. Newspapers, 
                        http://americaspublicbible.org.
                    
                    
                        Robinson, Z. (2017). Ventriloquizing Black Feeling, Re-Voicing Black Life: Speaking Baldwin on the Internet. 
                        Communities in Conversation: Digital Baldwin, Rhodes College.
                    
                    
                        Sichynsky, T. (2016). These 10 Twitter Hashtags Changed the Way We Talk about Social Issues. 
                        The Washington Post. 
                    
                
            
        
    
6575	2018	
        
            This paper presents a practical approach to building digital humanities (DH) at a university, across disciplines with diverse requirements, starting without institutional support, with scarce staff on a low budget. Examples are provided from the Centre For 21
                st Century Humanities (C21CH) at University of Newcastle, Australia (UON).
            
            Digital humanities (DH) requires expertise that crosses many fields from specific humanities disciplines to software development and production management. DH has a broad range in scale – from a scholar learning basic programming to hack a Python script, to multi-institutional collaborations on neural network learning. Few people are experts in all these fields meaning DH is often a collaboration. The requirements for any individual DH project can differ greatly also requiring IT skill sets that may not be easy to find in any one individual. This makes it difficult for university humanities departments with no spare cash, and often reluctant to invest heavily in IT, to successfully support DH, yet DH projects present problems beyond standard service offerings and provisioning and different to STEM. The Digital Lab of C21CH at UON has evolved an approach, here called ‘rapid bricolage’, that has successfully delivered a range of sustained internationally recognised DH projects influencing national debate. Some comparison will be made with other approaches, and while not necessarily suiting all circumstances, ‘rapid bricolage’ has proved an effective approach catering to characteristic issues in DH research, drawing from but differing to established IT practice. 
            This ‘rapid bricolage’ approach draws on ‘rapid’ software development and ‘bricolage’, both common practice in software development and humanities, but modifies them to meet the unique needs of Digital Humanities. These modifications are epistemic, structural, methodological and a matter of degree. It has also crucially involved consultative processes to identify and Pareto prioritise inter-disciplinary interests and achievable, feasible, high impact projects. The success of these feeds back to build interest and support for DH towards funding and growth, and results in project driven infrastructure, bridging the gap between projects without infrastructure and infrastructure without projects by beginning with demonstrable utility and developing with shared human and technical resources.
            
                C21CH projects include (http://c21ch.newcastle.edu.au):
            
            
                Colonial Frontier Massacres (v1.2) – map of massacres in Australia.
                EMWRN archive (v2) – innovative archive of material cultures of early modern women’s writing.
                Intelligent Archive (v3beta) – stylometry software.
                ELDTA site (v1alpha) – linguistics web player for media with tiered glosses, translations etc.
                Text To Map (prototype) – online automatic recognition and mapping of places in texts, linking to and from the text and the map.
                Scriptopict (v1) – annotations for images eg: 
                    Battle of Kurukshetra Mural and 
                    Mixtec Glyph.
                
            
            Rapid
            Rapid application development is an established methodology for software development focusing on getting a prototype working as early as possible followed by regular review with clients and incremental feature additions and bug fixes. For humanities departments this approach ensures that at least some software exists as an outcome of initial spending when the budget is tenuous and provides an encouraging proof of concept. For the cost of a meeting with several professors or executives a working prototype can be developed, making it worth simply trying it out rather than lengthy discussions about the value of proceeding. A rapid approach also helps greatly when the client is unclear of what is needed or has little understanding of IT. An early prototype establishes confidence and commitment early. Gaps in desired functionality immediately become clear through interaction. In particular because research is heuristic and highly changeable it allows for speculative requirements to change as the project progresses. Because of this, an even more rapid than usual approach is suited to humanities research because, as research, not all requirements cannot be known in advance. The process necessarily involves taking some action with ongoing revision, addition and enhancement. Not all aspects of humanities research activity, such as thorough rumination on a nuanced argument on a complicated problem, fit this ‘rapid’ model, but:
            - the speculative, heuristic activities necessary to research are enhanced; 
            - some slower methodical activities essential for rigour and completeness can be sped up, sometimes making research possible that otherwise would not have been, or improved through the need for clear structures and definitions; 
            - the 'slow' process of rumination, of considering complex problems and developing arguments, while irreplaceable, can be augmented.
            Bricolage
            Bricolage is a well-established approach in software development. Software is typically put together from pre-existing libraries, frameworks and cut and pasted code that is modified and added to, to produce something that works in ways that conventional intellectual property and copyright are not practically applicable to. This approach is in sympathy with developments in critical theory in the late 20
                th century and after, with ‘bricolage’ and the problematization of authorship being major themes in describing postmodernity and in contemporary humanities methodology. Just as a very rapid approach suits humanities research so too is bricolage especially suitable for DH. 
            
            As research, DH typically requires constant and regular modification and adjustment, rather than delivery of a working system according to contracted specification. Much software is developed for a STEM or commercial purpose, or has a STEM like approach to problem solving. STEM and the commercial sector have larger budgets and devote larger budgets to software. This means that humanists are often in a pragmatic situation of re-using software from different disciplines despite having divergent requirements. Humanities often focus on complexity, exceptions, structural change and highly contingent historical (not repeated) events, while STEM and commerce focus on systemisation, normalisation, ceteris paribus and repeatability, for example. If humanities researchers are to avoid fitting research to the software limitations this means constantly adapting systems to their own different epistemic, ontological and methodological paradigm, ie: bricolage. 
            The DH research need for these two approaches, rapid application development and bricolage, combined in extremis presents challenges to established IT practice. These challenges can be met with appropriate staffing, strategy and a ‘rapid bricolage’ approach to build DH at a University despite diverse demands and resourcing adversity.
        
        
            
                
                    Bibliography
                    Craig, H., Pascoe, W. (2018).
                         Intelligent Archive v3.0 Newcastle: Centre For 21 Century Humanities
                    
                    Ryan, L., Debenham, J., Brown, M., Pascoe, W. (2017). 
                        Colonial Frontier Massacres Newcastle: Centre For 21 Century Humanities http://hdl.handle.net/1959.13/1340762
                    
                    Smith, R., Pender, P., Pascoe, W. (2017). 
                        Early Modern Women's Research Network Digital Archive Newcastle: Centre For 21 Century Humanities http://hdl.handle.net/1959.13/1326860
                    
                
            
        
    
7854	2015	

    
        
            
                Computer Supported Collation With CollateX
                
                    
                        Haentjens Dekker
                        Ronald
                    
                    Huygens ING, Netherlands, The
                    ronald.dekker@huygens.knaw.nl
                
                
                    
                        Andrews
                        Tara L.
                    
                    University of Bern
                    tla@mit.edu
                
                
                    
                        Birnbaum
                        David J.
                    
                    University of Pittsburgh
                    djbpitt@gmail.com
                
                
                    
                        Olsson
                        Leif-Jöran
                    
                    University of Gothenburg
                    leif-joran.olsson@svenska.gu.se
                
                
                    
                        van Zundert
                        Joris J.
                    
                    Huygens ING, Netherlands, The
                    joris.van.zundert@huygens.knaw.nl
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Pre-Conference Workshop and Tutorial (Round 2)
                
                
                    collation
                    xml
                    programming
                    text comparison
                    python
                
                
                    literary studies
                    scholarly editing
                    text analysis
                    philology
                    xml
                    programming
                    English
                
            
        
    
    
        
            Comparing witnesses of a text is an important part of scholarly editing. Collation is regarded as one of the scholarly primitives (Unsworth, 2000). Comparing texts by hand can be tedious and prone to error, and it can be made more efficient and reliable with the assistance of computers. This workshop will explain how to use the open-source CollateX
                1 collation tool to compare witness of a texts automatically, in a way that can be used to produce critical textual editions and other types of comparative documents. Attendees will learn how to prepare source materials in any language (including those that use non-Latin scripts and directionality that is not left-to-right) for collation, how to perform automated collation using CollateX, and how to edit the results. 
            
            
                Full Contact Information 
            
            Ronald Haentjens Dekker (ronald.dekker@huygens.knaw.nl) 
            Software Architect and Consultant 
            Huygens ING 
            The Netherlands 
            
                Ronald Haentjens Dekker is a software architect and consultant at the Huygens Institute for the History of The Netherlands (http://www.huygens.knaw.nl/dekker/?lang=en). He has been the lead developer of CollateX since 2007. 
            
            Tara L. Andrews (tla@mit.edu) 
            Assistant Professor of Digital Humanities 
            University of Bern 
            
                Tara L. Andrews is assistant professor of digital humanities at the University of Bern. Her research interests include Byzantine history of the middle period (in particular, the 10th to 12th centuries), Armenian history and historiography from the fifth to the 12th centuries, and the application of computational analysis and digital methods to the fields of medieval history and philology. 
            
            David J. Birnbaum (djbpitt@gmail.com) 
            Professor and Chair, Slavic Languages and Literatures 
            University of Pittsburgh 
            
                David J. Birnbaum teaches digital humanities at the University of Pittsburgh (http://dh.obdurodon.org) and has been enhancing CollateX to collate medieval Slavic manuscript materials. Links to some of his digital philology and other digital humanities projects are available at http://www.obdurodon.org. 
            
            Leif-Jöran Olsson (leifjoran. olsson@svenska.gu.se) 
            Language Technologist and System Developer 
            Department of Swedish 
            University of Gothenburg 
            
                Leif-Jöran Olsson is a language technologist and system developer at the Språkbanken (the Swedish Language Bank; http://spraakbanken.gu.se/eng/personal/ljo). He is also a developer of the open-source eXistDB XML database system ( http://existdb.org/exist/apps/homepage/index.html) and has worked on a plug-in to integrate eXistdb and CollateX. 
            
            Joris J. van Zundert (joris.van.zundert@huygens.knaw.nl) 
            Researcher and Developer in Computational and Digital Humanities 
            Huygens ING 
            The Netherlands 
            
                Joris J. van Zundert is scientific researcher and developer in the field of digital and computational humanities at the Huygens Institute for the History of The Netherlands. A scholar of medieval Dutch literature by training, his main interest as a researcher and developer lies in the possibilities of computational algorithms for the analysis of literary and historical texts, and the nature and properties of information and data modeling in the humanities. 
            
            
                Target Audience
            
            Scholars who are interested in using tools to facilitate humanities research, especially with respect to preparing digital critical editions. Participants who wish to work with their own materials will need to bring them (in plain text or TEI markup); the organizers will provide sample data that can be used by participants who do not have their own project materials. Participants are strongly encouraged to install Python 3 and CollateX in preparation for the workshop; the workshop organizers will provide installation instructions in advance. No prior Python programming experience is required. Based on prior workshop experience, we anticipate attracting between 15 and 30 participants. 
            
                Special Requirements for Technical Support 
            
            A computer projector (HDMI or VGA) will be required for the presentation. Participants will be required to bring their laptops, and the room will need to provide sufficient plug-in electrical connections and wireless Internet connectivity for all participants. 
            
                Intended Length and Format of the Workshop 
            
            Full day, two sessions. 
            
                Session 1. 9:00–12:00: The Basics of Automatic Collation
            
            The first session will cover the theory of collation, the basics of using CollateX, and the collation of plain text data. No prior experience with collation tools is required. 
             • Introduction to the theory and uses of collation. 
             • The collation data model: witnesses, tokens, and tokenization.
             • Installing, configuring, and testing CollateX.
             • Collating plain text strings and files.
             • Output options and postprocessing.
             • Introduction to normalization.
            
                Session 2. 13:00–16:00: Collating XML (including TEI) Data 
            
            The second session will cover more advanced topics, most notably the collation of transcriptions that contain XML (including TEI) markup. 
             • The collation data model with XML (especially TEI) input. 
             • Advanced normalization. 
             • Recognizing and tracking markup information during collation. 
             • Processing tokens differently according to markup information. 
             • Output options and post processing for XML (especially TEI) output. 
            
                Call for Participation
            
            We asked applicants on relevant mailing lists (such as Humanist, TEIL, Digital Medievalist) to tell us about their interests, needs, and prior experience with respect to collation. The instructors listed above will serve as the workshop program committee. For participants, up to 30 participants were to be accepted. 
            Note
            1. The main CollateX website is http://collatex.net. CollateX Python is freely available in the Python package repository: https://pypi.python.org/pypi/collatex. The source code is open and available at https://github.com/interedition/collatex. For a report about a recent application of CollateX, see 
                Haentjens (2014).
            
        
        
            
                
                    Bibliography
                    
                        
                            Haentjens
                             Dekker
                            , 
                            R., 
                            van 
                            Hulle
                            ,
                        
                        
                             
                            D.
                        
                        ,
                        
                            Middell
                            ,
                        
                        
                             
                            G.
                        
                        ,
                        
                             
                        
                        
                            Neyt
                            ,
                        
                        
                             
                            B.
                        
                         and 
                        
                            van 
                            Zundert
                            , J. (2014). 
                        Computer-Supported Collation of Modern Manuscripts: CollateX and the Beckett Digital Manuscript Project. 
                        Digital Scholarship in the Humanities (2014), http://dsh.oxfordjournals.org/content/early/2014/12/02/llc.fqu007, 
                        
                            http://dx.doi.org/10.1093/llc/fqu007
                        .
                    
                    
                        Unsworth, J. (2000). Scholarly Primitives: What Methods Do Humanities Researchers Have in Common, and How Might Our Tools Reflect This? In 
                        Symposium on Humanities Computing: Formal Methods, Experimental Practice. London: King’s College, http://people.brandeis.edu/~unsworth/Kings.500/primitives.html.
                    
                
            
        
    

8315	2013	





Preliminaries: The Social Networks of Literary Production in the Spanish Empire During the Administration of the Duke of Lerma (1598-1618)



Brown
,
David Michael

University of Western Ontario, Canada
dbrow52@uwo.ca



Suárez
,
Juan Luis

University of Western Ontario, Canada
jsuarez@uwo.ca




University of Nebraska-Lincoln

Center for Digital Research in the Humanities

319 Love Library
University of Nebraska–Lincoln
Lincoln, NE 68588-4100
cdrh@unl.edu


Lincoln, Nebraska

University of Nebraska-Lincoln
Lincoln, NE 68588-4100







The Preliminaries Project is an ongoing study that uses social network analysis to better understand the publication of literary texts in the 17th century Spanish Empire. Using the metadata provided in the preliminaries sections of these texts, we have constructed a series of graph databases that allow us to organize this information in a custom tailored and meaningful way. The data is exported to Gephi graph visualization software for analysis to discover patterns, key communities, and structures not apparent in traditional humanistic studies. We then use traditional humanistic techniques to study important points located in the graph. This study demonstrates the utility of social network analysis in understanding Early Modern publication in the Spanish Empire. By using modern tools to analyze large data sets, we show the effectiveness of digital humanities as a tool to locate previously overlooked areas for further study using a more traditional humanistic approach.



No source: created in electronic format.




LP20





Paper


Long Paper


Hispanic Studies
Social Networks
Transatlantic Communities
Publication
Gephi


databases & dbms
historical studies
literary studies
metadata

data modeling and architecture including hypothesis-driven modeling

spanish and spanish american studies
bibliographic methods / textual studies
cultural studies
visualisation






Laura Weakly
Initial encoding







The “preliminaries” section of a 17th-century book encompasses the pages appearing in the printed text before the beginning of the work itself. This information is divided into seven different types of documents: details of publication, documentation of censorship (both civil and ecclesiastical), licensing, selling price, dedications, letters, and errors. The importance of the preliminaries for this project lies in the information present in these sections: the names of the officials signing the documents, their governmental/institutional affiliation, dates, place of issue, and literary circles that appear in the form of dedications and poetry written by various authors and published in their friend’s or associate’s books. In a few pages, the preliminaries give a complete image of the formal process required for the publication of each work of literature. By compiling all this information into a graph database and performing queries specific to various research questions, we have at hand a valuable source of information about the historical networks that influenced the publication of Early Modern Spanish literature.


To get a comprehensive look at this information, we generated lists of every edition of what we consider literary texts (fiction in prose, theatre, poetry, chronicles) published during the 17th -century in the Spanish empire (Jiménez et al. 1980)(Calvo et al. 2003). As shown by the following screen shot, we have focused on acquiring every available edition of each literary work.




Sample of one of our
acquisitions
lists



We then divided the 17th Century into periods corresponding to the different “validos” —royal favorites that served as head of government or “prime ministers” — of the various kings in order to address the changing power structures of the time and their influence in literary production (Hernán et al. 2002). Through interlibrary loans and, in some cases, trips to the libraries that hold the edition, we acquired copies of the pages of each book that make up the preliminaries section. Then, we manually built a graph database using sylvadb.com, an open source software and free graph database management service developed in the CulturePlex Lab. Within Sylva, data was stored and organized using a custom designed system of schemas based on a node/edge relationship system. Finally, we exported the database to Gephi (
https://gephi.org/
), a software package that allows for visualization and statistical/metric analysis of the network using built-in algorithms and Python based scripting (Bastian et al. 2009). This allows us to detect important communities within the network, key players, important objects, and hubs of production.


For this study, we have unearthed the social networks of publishing and literary creation in 17th-century Spanish literature, focusing particularly on the period during the rule of the Duke of Lerma (1598-1618). Currently the first of our
editions
lists (Duke of Lerma) consists of 330 editions, out of which we have successfully obtained 228 scanned copies of preliminaries sections: approximately 70% of the total number. Of these scans, 121 have been entered into the database, producing a graph with 1612 nodes and 3472 relationships. Rendered in Gephi using the built-in OpenOrd algorithm, the graph looks like this:



The Preliminaries graph rendered in Gephi


Using the algorithms, metric analysis tools, and filters built into Gephi we pinpointed the individuals, governmental and ecclesiastical bodies that influenced publication in this period. Also, by using the concept of “ego network” from social network analysis, we established what we call the “publication network” of some of the authors that interest us (Carrington et al. 2011). A publication network includes the editors, censors, and other individuals important in the formal process of publication, as well as any other individuals that are more directly connected to the author: friends, family, patrons, literary colleagues, etc. We determined the range of the publication network based on the internal data structure of the Preliminaries database as follows. Due to bibliographic concerns (Bowers et al. 1962) and organizational aspects of our data schema, in order to establish a connection between the author and those involved in the approval, licensing, and publication of an edition there are four steps e.g., Author->Work, Work->Edition, Edition->Approval, Approval->Censor. Therefore, to establish an author’s publication network we needed to find neighbors for up to four degrees of separation. Although Gephi does not include ego network filters that extend to four degrees, using its Python based scripting console we were able to code functions that allowed us to isolate subsets within the graph to generate ego networks for any node to
n
degrees of separation. For instance, in the graph below we can see the publication networks of two authors associated with Mexico; Bernardo de Balbuena, author of
Grandeza Mexicana
; Juan de Torquemada, author of
Monarquía Indiana
; and the intersecting nodes in their publication networks:




Publications Networks: Balbuena=Black, Torquemada=Grey, Intersecting Nodes=White



Using the above techniques, we set out to find and isolate the main nodes of this social network that made possible the creation and sustainability of a transatlantic network of cultural agents. The first thing that stands out in the graph is Lope de Vega and his powerful, Madrid based publication network (Martínez et al. 2011). Using the Python scripting console, we determined that Lope’s publication network consists of 1083 nodes, or 67% of the nodes in the graph. This information is not new, based on the extremely prolific nature of his literary production we can assume that he was very well connected. However, we can also determine who
wasn’t
in his publication network. Departing from Lope’s publication network, we were able to locate the successful political and institutional connections that help us explain the central position of institutions such as the House of Zúñiga in the cultural fabric of the period.



Publication network: Lope de Vega=Black


To do this we used the scripting console to remove the subset of nodes representing Lope’s publication network from the other nodes that make up the graph, and returned a list of the names of all of the people who are not in Lope’s publication network. A quick review of this list produced some interesting results: we found several authors based in Spain including Gonzalo de Céspedes y Meneses and El Inca Garcilaso; and two authors active in Peru, Diego Dávalos y Figueroa and Pedro de Oña. While a quick look at both Céspedes and El Inca produced interesting results, the two Lima based authors attracted our attention. In this period social circles were highly influenced by geography, and it is logical that these authors find themselves at the periphery of a network centered geographically in Madrid. However, despite geographic concerns both authors remain connected to Lope de Vega’s network. We found that both Oña and Dávalos y Figueroa are connected to Lope’s network at 3 degrees of separation through their dedications to the Viceroy of Peru, Luis de Velasco y Castilla; and at four degrees through Juan de Zúñiga, Diego de Ojeda, and the Order of Santiago:




Publication networks: Dávalos y Figueroa=Black, Lope de Vega=Grey, Intersecting nodes=White



In order to contextualize the Peruvian network we compared the aforementioned “Mexican” authors with the “Peruvian” authors. Combining the four social networks into two based on geographic constraints, we found that at 4 degrees of separation there was no direct overlap, so we upped the parameter to 5 degrees of separation and produced the following image:




Publication Networks: Intersection between Mexican and Peruvian Networks



As shown here, even at five degrees of separation there are few overlaps between the networks. However, in the above image we begin to notice the importance of the House of Zúñiga. It is well known that the House of Zúñiga was powerful in both Spain and the Americas, and also that certain members of this house were important patrons of the arts and literature (Cátedra 2003; Díez Fernández et al. 2005). Nonetheless, we don’t think that their role in transatlantic literary production has been adequately explored. The political importance of this family in New Spain is obviously important (an Archbishop and a Viceroy); however, the Preliminaries graph illustrates not only the political role this house played in America, but also the importance of political figures/nobility in publication circles and how the members of one house can spread their cultural influence throughout geographic space. To take this concept one step further, we followed the Zúñigas back the Spain. Here we find the Duke of Béjar, Alonso López de Zúñiga y Pérez de Guzmán, and the first part of Don Quixote. It turns out that American authors were not the only artists soliciting support from the House of Zúñiga: Miguel de Cervantes dedicated part 1 of Don Quixote to the famous Duke of Béjar(Rico 2005).


The above samples show the potential of a research model that combines network-based analysis with quantitative and qualitative studies of cultural production, providing evidence of the interaction between political structures and cultural production in the Spanish Empire (Martínez et al 2008). By repurposing bibliographic data, the Preliminaries Project allows us to explore the concept of cultural networks within the framework of transatlantic studies and complexity theory (Wood 2010; Suárez 2007). Furthermore, this study demonstrates the effectiveness of digital humanities methods as a tool to locate previously overlooked areas for further study using a more traditional humanistic approach.







1.
 Pedraza Jiménez, F. B., and M. R. Cáceres.
(1980).
Manual de literature española
. Pamploa: Cénlit.


2.
Huerta Calvo, J. (dir.)
(2003).
Historia del teatro español
. Madrid: Gredos.


3.
García Hernán, E.
(2002).
Políticos de la monarquía hispánica (1469-1700)
. Madrid: Fernández Ciudad.


4.
Bastian M., S. Heymann, and M. Jacomy
(2009). “Gephi: an open source software for exploring and manipulating networks.” International AAAI Conference on Weblogs and Social Media.


5.
Carrington, P. J., and J. Scott
(2011).
The SAGE Handbook of Social Network Analysis
. Los Angeles: Sage.


6.
Bowers, Fredson.
(1962).
Principles of Bibliographic Description
. New York: Russell & Russell.


7.
Martínez, J. F. 
(2011).

Biografía de Lope de Vega, 1562-1635: un friso literario del Siglo de Oro

. Barcelona, PPU.


8.
 Cátedra, P. M. 
(2003).

La "Historia de la Casa de Zúñiga" otrora atribuida a Mosén Diego de Valera

. Salamanca: Gráficas Cervantes.


9.
Díez Fernández, J.; I., and G. Santonja.
(2005).
El mecenazgo literario en la casa ducal de Béjar
. Burgos: Instituto Castellano y Leonés de la Lengua.


10.
Rico, F. 
(2005).

El texto del "Quijote”: preliminares a una ecdótica del Siglo de Oro

. Barcelona: Ediciónes Destino.


11.
Martínez Millán, J.;, and M. A. Visceglia (eds.)
(2008).
La monarquía de Felipe III
. >Madrid: Cyan, Proyectos y Producciones Editoriales. Print.


12.
Wood, A. T.
(2010). Fire, Water, Earth, and Sky: Global Systems History and the Human Prospect.
The Journal of the Historical Society
. X:3: 287-318.


13.
Suárez, J. L.
(2007). Hispanic Baroque: A Model for the Study of Cultural Complexity in the Atlantic World.
South Atlantic Review
. 72(1): 31-47.






9273	2020	Although the Digital Humanities is fundamentally interdisciplinary in nature, all humanities research questions require a degree of interdisciplinary thinking. History, for example, draws upon most other social sciences and humanities for obtaining and analysing source materials in different contexts. The multi-modal nature of these sources, the mixing of methodologies into bespoke, project-specific frameworks and the collaboration of researchers with overlapping but distinct interpretations all require a flexible workspace. Moreover, growing calls for open research methods put pressure on humanities researchers to rethink how they document the provenance of their source materials as well as their interpretations. Individual scholars often develop extensive, single-use taxonomies to categorise, encode and describe their conclusions; stored in a variety of document, spreadsheet and database systems, these are rarely disseminated and remain offline penumbra of the research process. Moreover, the prescriptive nature of out-of-the-box software may constrain the annotation process. Larger collaborations may spend significant time developing extensive coding criteria resulting in over-fitted schema with little reusability or reach despite often herculean efforts of dissemination. Even when reusable, these schemas may require a degree of familiarity with the bespoke systems that makes them inaccessible to those outside the project. In order to overcome these difficulties, we have developed a highly extensible database development interface, Nisaba. Rather than prescribe a new database structure or encoding format, Nisaba was developed in order to accommodate a wide variety of source materials, encoding schema and dissemination formats. To achieve this, Nisaba leverages World Wide Consortium (W3C) standards and Linked Data publishing practices, which encourage the explicit provision and reuse of vocabulary terms. Written in Python 3.6 using TKinter, a cross-platform graphical user interface (Linux, OS, Windows), Nisaba functions as both an input and retrieval mechanism. Users input data including text transcriptions, images and [in the future] audio/visual files and apply user-created controlled-vocabularies, free-text annotations and an extensible selection of metadata. Once inputted, users create a segment (a selection of words, pixels or seconds of audio-visual information) and apply further metadata or annotations, allowing a single item to have multiple overlapping annotations using different schema by different users. In order to facilitate the documentation and exportation of data that is restricted or within copyright, the database encodes these segments by word number (text), or relative position (image), allowing precise locators without necessarily exporting the original materials. All data inputs are time-stamped and attached to individual user records, allowing for multiple researchers to annotate the same segments while maintaining unambiguous lines of provenance and allowing longitudinal use of the databases by multiple projects. Once inputted, the material can be retrieved through a simple browsing mechanism (controlled vocabulary) or by exporting layers of the data to non-proprietary formats, currently JSON or Turtle (RDF), allowing for deeply humanistic forms of knowledge representation in a format suitable for computational analysis. This talk will demonstrate the use of Nisaba for various project types and provide guidance on how to develop an open, highly documented dataset to accompany humanities research.
9316	2020	Dynamic Systems for Humanities Audio Collections: The Theory and Rationale of Swallow Jason Camlot, Tomasz Neugebauer, Francisco BerrizbeitiaThis paper approaches a system that has been designed, and continues to be in development, for the aggregation of metadata surrounding collections of documentary literary sound recordings, as an object for theoretical and practical discussion of how information about diverse collections of time-based media should be managed, and what such schema and system development means for our engagement with the contents of such collections as artifacts of humanist inquiry. Swallow, the interoperable spoken-audio metadata ingest system project that is the boundary object for this talk, emerged out of the goals of the SpokenWeb SSHRC Partnership Grant research network to digitize, process, describe, and aggregate the metadata of a diverse range of sound collections documenting literary and cultural activity in Canada since the 1950s. Our talk, collaboratively written and delivered by a literary scholar and critical theorist, a digital projects and systems development librarian, and a library developer / programmer, outlines 1) a theoretical rationale for the audiotext as a significant form of data in the humanities, 2) consequent modes of description deemed necessary to render such data useful for humanities scholars, and 3) a rationale for the development of a specific form of database system given the material and systems contexts that inform our national holdings of documentary literary sound recordings at the present time.[Figure 1. Screenshot of Swallow Dashboard]  Rationale of digitized audiotexts as humanities research data Why is the study of literary recordings important? With the introduction of viable sound recording technology in the last decade of the nineteenth century, a new form of material data for literary analysis emerged: the audiotext. This new literary artifact, based on the sound signal of the poet’s acoustic performance, has been defined as “a semantically denser field of linguistic activity” that demands new methods of analysis (Bernstein). Once digitized, engagement with sound recordings move us further down the path of understanding “The Text” as “a methodological field” (Barthes), and towards increasingly elaborate “rationales of audio text” (Clement) realized in relation to information systems for the purposes of software analysis, content modeling, and cataloguing. The digitized archive of voices invites the production of thorough metadata, and thus expands the ways in which we may remodel our understanding of historical cultural artifacts. Over the past two decades, due in part to the founding of online repositories of literary recordings like PennSound (PennSound center for programs in contemporary writing at the University of Pennsylvania 2020) and SpokenWeb (SpokenWeb 2020) the potential of the literary sound archive for research has become discernible. The implications of such research are momentous for future methodologies in the humanities, and yet the realization of such research is limited, at present, by the status and structure of our collections of literary sound recordings. In Canada, thousands of hours of literary readings, performance and related activities have been recorded on magnetic tape since the 1950s. The affordances of tape recording and subsequent media technologies enabled new literary uses of sound recording, including the documentation of literary events, readings and conversations, both public and private. This extended the reach of capturing literary forms and events, and consequently, transformed our understanding of what comprised the literary and cultural events. As valuable archival materials, these recordings represent a massive, largely untapped, undifferentiated, and often undiscovered resource for the study of literary art, culture and society. Research in literary sound studies depends upon a collaborative, interdisciplinary approach to the development of historical knowledge about these valuable cultural heritage materials, and of a networked structure of digital repositories and research-focused interfaces for the preservation and meaningful presentation of these materials to researchers, students, artists and the public. Swallow, the metadata system we launched in 2019 and intend to augment into a robust tool for sharing metadata about our national audio holdings across multiple institutions and a diverse range of metadata and access systems, is a coded software system that represents and works to realize the theoretical idea of audiotexts as significant digital humanities data.A Poetics of Audio Description, or, a Metadata Schema for Literary AudioThe diversity of recordings held at SpokenWeb partner institutions across Canada include recorded conversations, dictations, compilations, performances, interviews and lectures, among numerous other generic audiotextual categories. How do we begin to describe the features and contents of such recordings, and in what ways may we make collections of documentary sound recordings speak to each other, beyond their local venues of origin and siloed media formats and archives?Metadata and its structure has the potential to open and limit avenues of research. The aggregation of metadata from partner institutions requires either choosing an existing access system, such as Islandora (Islandora Open source digital asset management 2020) or Avalon (Avalon Media System 2020), with its own metadata schema, or to design one based on the evolving research needs of the project. We have chosen the latter because we do not wish to limit our research questions to any single digital repository platform. During the extensive metadata collection phase of our research program, we are seeking maximum usability. We are pursuing our work in describing the assets and contents of collections of documentary literary sound recordings using an agile, iterative development process. We have developed our long-form metadata schema over the course of a year through the research, discussion, and regular meetings of a SpokenWeb Metadata Task Force consisting of librarians, literary researchers and students from across our partnership. We have provided the metadata schema (Camlot et al. 2020) and ingest software out to student cataloguers quickly for testing and feedback, so that the schema has been shaped by research use from the outset, and will continue to be shaped by researcher feedback throughout this seven-year cataloguing project. We based the SpokenWeb schema on standards such as IASA (International Association of Sound and Audiovisual Archives 1999), AACR2 (Joint Steering Committee for Revision of AACR 2005) and MODS (Library of Congress 2018), focusing on the most useful simplified instructions for the audiotext metadata context.The initial result was a SpokenWeb Metadata Schema with standard core fields such as: Title, Rights, Creators/Contributors, Production Context, enre, and Contents. The material artifact description recommendations in the schema include detailed sub-fields reflecting research interest, such as: AV Type, Material Designation, Physical Composition, Storage Capacity, Extent, Playing Speed, Track Configuration, Playback Mode, Tape Brand. The digital file description was a multiple field from the beginning, allowing for multiple files per material artifact. We later added Contents and Notes sub-fields that are capable of storing XML markup to Digital File Description, to make it possible to associate content annotations for each of the files in case of multi-file items. Following feedback from cataloguers who encountered event recording spanning multiple artifacts, it made sense to make Material Description into a multiple field as well. Other examples of changes include: the addition of “Performance Date” to Date Types and “Classroom recording” to Production Context.We recognized the research need to catalogue event/venue names and locations early on, and included location subfields for venue names, latitude and longitude, links to OpenStreetMap nodes, and notes for additional context.For qualifying and describing authorship, we predefined a controlled vocabulary by selecting from the Library of Congress relators those roles relevant to audiotext, such as interviewer, author, presenter. In the second iteration of the schema, we made it possible to associate multiple roles for a contributor, and following feedback from one of our partner institutions working with First Nations content, we added the role of “Elder”. In addition, soon after the launch of Swallow, the First Nations Metis and Inuit Indigenous Ontology was released, and we added the Nation subfield and controlled vocabulary lookup based on this for Creators/Contributors. The Contents field has been designed to facilitate navigation, searching and linking audio content of one recording to a wide collection of others. The inclusion of a Related Works field is one way to facilitate linking. The content information needs to be able to include time stamped structural and descriptive information about the works and creators on the recordings. There are many possible tools and formats to perform this type of time stamped metadata. Swallow allows for the storage of contents field information using text or XML markup. We also developed recommendations for how to annotate literary audio for the project and a Python/Flask script to convert text formatted annotations to Avalon XML (Neugebauer 2020). Avalon’s XML format for audio annotations was extended slightly to make it possible to package multiple digital files’ annotations into a single Avalon XML file.  Swallow and the rationale for an iterative database design The SpokenWeb collections are diverse and dispersed in archives and different repository systems. The networked approach we have pursued has allowed the research network to proceed with aggregating the metadata from partners without having to limit the infrastructure by choosing only one repository system with its associated metadata schema. However, we have also needed to move beyond spreadsheets, to improve the accuracy and usability of the cataloguing work. A spreadsheet of metadata quickly becomes difficult to manage as the number of columns increases to accommodate multiple fields, to catalogue multiple creators/contributors, for example. In addition, the research requirements of the project call for a highly customizable and extendible set of metadata fields that will change over time as we analyze and catalogue the content and learn more about the collections. We needed to be able to rapidly iterate new versions of interfaces for evolving metadata schema. Our solution has been to avoid hardcoding the metadata schema into the relational database, and instead to store the metadata as unstructured JSON inside the database. A layer of abstraction then allows us to quickly modify configuration files that define the metadata fields and their associated controlled vocabularies and lookups. The resulting system’s cataloguing interface may then be changed quickly, and is able to accommodate multiple schemas. Indeed, we have implemented numerous additions and changes to the metadata system over the course of our first year of cataloguing in response to the nature of specific collections we are describing.Swallow makes it possible for literary audio holding institutions to aggregate their metadata in a single system. The required metadata fields for access and preservation can change over time and vary by document type and source institution. Swallow allows for the aggregation of metadata in multiple metadata schemas in the same system. It generates a schema-specific cataloguing interface for each item stored, based on the item’s respective schema configuration files that include a list of steps, metadata fields and controlled vocabularies. The Swallow schema specification functionality currently allows for the inclusion of URIs alongside metadata values that are a part of the SpokenWeb Schema, such as links to VIAF, OpenStreetMap, Wikidata, and Rights Statements.  Future developmentFigure 2 shows the proposed flow of information with Swallow at the center acting as a central metadata repository. The functionality that we are proposing to develop includes the capacity to batch ingest metadata from other systems currently used by SpokenWeb partners into the latest version of the SpokenWeb schema in Swallow. The functionality to access batch import mappings on the interface exists, and we will develop the mappings for Islandora, Avalon, and AtoM in collaboration with partner institutions, Concordia Spectrum Research Repository, SFU Radar, University of Alberta Dataverse, University of Toronto (Dataverse), UBC Brain Circuits (FRDR). The proposed functionality includes the development of batch editing that would allow cataloguers to modify the metadata of collections of items. We will also develop mapping specifications that will allow for the migration of items catalogued in one schema version into another. The proposed development would build into the interface the functionality to lookup and export metadata from Swallow to Wikidata. In addition, we will develop mappings for export of collections of metadata as data from Swallow to DataVerse and FRDR (Canadian Association of Research Libraries & Compute Canada 2020) . [Figure 2. Swallow as central metadata repository] Conclusion Our work in metadata and systems development can be read as the development as a poetics of the audiotext, that is a poesis (a making) of the descriptive and digital infrastructure that allows digitized audio artifacts – artifacts that capture the sound of literature in its social contexts during the postwar era – to survive and take flight in the imaginations of contemporary critics, scholars, poets and listeners.
9341	2020	IntroductionThe transmission of text in poetic form is a quasi universal aspect in the oral tradition of every culture. The study of the poetic features of text, especially their rhythmic structure when forming verses, pertains to the different traditions, whose scholars established the rules that might govern poetry. Within this context, the POSTDATA Project formalized a network of ontologies able to express any poetic expression and its analysis at the European level, enabling scholars all over Europe to interchange their data using Linked Open Data. However, varied research interests result in corpora that might not share the same facets of an analysis. To alleviate this concern and foster the completeness of the interchanged corpora, our team set out to build a software toolkit to assist in the analysis of poetry. This paper introduces PoetryLab, an extensible open source toolkit for syllabification, scansion (extraction of stress patterns), enjambment detection (syntactical units split in two lines), rhyme detection, and historical named entity recognition for Spanish poetry. Our toolkit achieves state of the art performance in the tasks for which reproducible alternatives exist.Design PrinciplesManuals for metrical analysis of Spanish poetry exist at least since the 18th Century, although the foundational work and subsequent refined guides for modern analysis would take another century to appear. Despite such a long and rich tradition, not many computational tools have been created to assist scholars in the annotation and analysis of Spanish poetry. With ever increasing corpora sizes and the popularization of distant reading techniques, the possibility of automating part of the analysis became very appealing. Although solutions exist, they are either incomplete (i.e., fixed-metre poetry, mostly hendecasyllables, not applicable to Spanish, or not open nor reproducible. These limitations guided the design of PoetryLab. At its core (see Figure 1), PoetryLab provides a compliant OpenAPI that connects independent packages together. Built on top of the natural language processing framework spaCy, two Python packages perform scansion and enjambment detection, namely, Rantanplan and JollyJumper.1 In Spanish, some words are stressed depending on their function in the sentence, hence the need for a proper part of speech (PoS) tagger. AnCora, the corpus spaCy is trained on for PoS tagging of Spanish texts, splits most affixes thus causing some failures in the tags it assigns. To circumvent this limitation and to ensure clitics were handled properly, we integrated Freeling’s affixes rules via a custom built pipeline for spaCy. The resulting package, spacy-affixes,2 splits words with affixes before assigning PoS, and can be plugged in to a regular spaCy pipeline loading one of the statistical models for Spanish. This pipeline is the foundation for Rantanplan and JollyJumper, which are rule-based algorithms inspired by Ríos Mestre, Caparrós and Navarro Tomás, and Quilis and Spang, respectively. Figure 1. General architecture of PoetryLab.Following the OpenAPI specification, we defined a REST API that unified the internal interface of the different packages and provided a common endpoint for analysis. For external packages developed in languages other than Python, PoetryLab provides a pluggable architecture that allows their integration. This is the case for our named entity recognition system, HisMeTag, developed in Java and connected to the PoetryLab API through an internal REST API. The only requirement for third-party integrations is to consume text and produce both JSON and RDF triples.The PoetryLab API was then used to provide with functionality a React-based web interface that non-technical scholars can use to interact with the packages in a graphical way (see Figure 2). The frontend also allows downloading the generated data. Figure 2. PoetryLab showing stressed syllables (blue), sinalefas (‿) and enjambments (↵).ResultsOne notably difficult aspect of benchmarking automated analysis of Spanish poetry is the lack of a gold standard reference corpus. For the evaluation of the syllabification algorithm in PoetryLab we build a 100k words corpus using a combination of online resources,4 which we named EDFU and are releasing under a Creative Commons license.5 For metrical analysis we used Navarro-Colorado’s corpus. For mixed-metre we are using our own copus obtained from Carjaval’s annotated anthology. Unfortunately, we have not found a public corpus for rhyme and stanza identification yet, and although an enjambment corpus seems to exist, it is not publicly available.Table 1 shows the ratio of success extracting the list of syllables of the words in EDFU, and the correct metrical analysis for the different corpora and tools. Notably, PoetryLab achieves state of the art performance for syllabification and per line metrical analysis.7 We were unable to reproduce Gervás’ approach and are reporting their own ratios.Syllabification (EDFU)PoetryLab (rantanplan): 99.98Navarro-Colorado: 98.74Agirrezabal: 98.06Metrical patterns (fixed-metre)PoetryLab (rantanplan): 96.22Navarro-Colorado:94.44Gervás:88.73Agirrezabal:90.84Metrical pattern (mixed-metre)PoetryLab (rantanplan): 65.02Navarro-Colorado:49.38. Although at an early stage, PoetryLab has proven useful in that it highlights some issues with the existing corpora and techniques developed to this day. First, there was no alternative system to analyze poetry composed of other than hendecasyllables, for which we are using a corpus of mixed-metre poetry based on Carvajal’s original annotations. Moreover, we are contributing with a new corpora to evaluate syllabification procedures, and enriching the ecosystem of Python tools for Spanish by providing a spaCy pipeline that deals with clitics. Finally, we make the data produced by the PoetryLab machine readable, interoperable, and ready to be ingested into a triple store compliant with the POSTDATA Project network of ontologies.Eventually, PoetryLab will be integrated into the larger POSTDATA Project public website, making working with European repositories of poetry a more pleasant task, and assisting whenever possible with the metrical and rhetorical side of the analysis.Founding SourceResearch for this paper has been achieved thanks to the Starting Grant research project Poetry Standardization and Linked Open Data: POSTDATA (ERC-2015-STG-679528) obtained by Elena González-Blanco. This project is funded by the European Research Council (https://erc.europa.eu) (ERC) under the research and innovation program Horizon2020 of the European Union.
9361	2020	This research project employs Python’s Natural Language Toolkit (NLTK) to study John Milton’s references to old and new geography throughout his career and across his wide-ranging genres. This project investigates whether the blend of old and new geography is genre specific, and traces location-based patterns to study whether Milton's geographic focus is also genre specific.
9364	2020	The Shakespeare and Company Project is one of the longest-running projects at the Center for Digital Humanities at Princeton (CDH). Based on archival materials from the Sylvia Beach Papers at Princeton’s Firestone Library, the project recreates the world of the Lost Generation by detailing what members of Beach’s lending library read and where they lived. The full site launched May 2020, and data on library members, books, and events will be published soon.The Shakespeare and Company Project is an exemplar of CDH’s commitment to critical adoption of best practices (Koeser, 2019) and innovation in custom software. Our process leverages rigorous testing, including unit testing (over 95% coverage for Python code), automated accessibility testing with pa11y, and manual feature and usability testing. As a project with both popular and scholarly appeal, we’ve designed a site that foregrounds the complex detail and idiosyncrasy of Beach’s record-keeping practices, but is welcoming and accessible, and also fully responsive on mobile devices. Through collaborative and iterative data modeling, we developed a relational database that allowed the research team to aggregate data , which helped to identify and disambiguate members, and to associate events from different archival sources. This project also includes a sophisticated solution for storing and querying partially-known dates in a SQL database (Koeser, 2019).Our lightning talk will demonstrate aspects of the public interface as well as the administrative backend, provide a brief overview of the published data, and discuss some of the scholarly and technical challenges we encountered.
9371	2020	A journey from Hell to Heaven, investigating the computational opportunities of automating text analysis and producing data visualisations.This poster presents the results of the exploratory work for a reusable tool to generate data visualisations based on automatic text analysis. Its non-functional requirements respond mainly to flexibility (accept different text inputs) and optimisation (produce rich visualisations with minimal set up). The visual outputs produced by the application have an explorative function in that they aim to:offer a different perspective on the text under study;highlight patterns and/or outliers (Meirelles 2013);drive research in formulating new hypotheses;provide support to, or disprove, existing theses.The current version accounts for modules (i.e. software components) designed around one selected test case, namely Dante Alighieri’s Divine Comedy, but serves as a blueprintfor further modules to be plugged in.The Italian version of the Commedia (Petrocchi 1966-67) is used to perform text structural analysis and work on the rhyme scheme, while the English translation (Mandelbaum 1980-84) is used for sentiment analysis. The unique way in which Dante wrote his masterpiece, makes the text an interesting dataset to be explored computationally. Structural (spatial and temporal) textual components lend themselves to be represented graphically, and offer insights into its linguistic content.The visual outputs allows users to interact with both the content and the metadata.The application performs computational text analysis to produce data visualisations representing the following structural, stylistic and semantic features of the text:schematic representation of the poem’s structure and rhythm ;distribution of keywords;visual representation of the sentiment analysis (fig. 3).Figure 1 An example of the schematic representation of the poem’s structure: rhythm imposed by tercets and rhyme prediction.Figure 2 Words like Cristo (Christ) and stelle (stars) are distributed unevenly across the three cantiche: the word “Christ” never appears in the Inferno, while it’s widely used in the Paradiso. One square per line. Figure 3 Sentiment analysis visualisation of the three cantiche. Red is negative, blue is positive and the opacity indicates how close to the polarity (-1, 1) the sentiment is. One square per line.The application has been developed modularly (Martin and Martin 2006), following the separation of concerns design principle (Dijkstra 1982) to allow for flexibility and scalability.The computational aspect of the project is implemented in Python, a flexible programming language that supports object-oriented programming and functional paradigms.The visualisations are produced with the support of d3.js library, “a JavaScript library for manipulating documents based on data” (Bostock D3.js <https://d3js.org/>). The application exploits HTML5 and SVG specifications to allow for greater interaction and portability.Natural language processing (NLP) and machine learning techniques have been applied to process and transform the data. The Naive Bayes Classifier (Perkins 2010) technique has been chosen due to its performance and simple implementation.A training dataset has been manually created collecting random subsets of text from other authors close in language and time, and further work from Dante himself:Ludovico Ariosto, Orlando furioso (Wikisource contributors 2012)Dante Alighieri, Convivio (Gallarino <http://www.italica.it/dante/convivio.html>)Giovanni Boccaccio, Decamerone (Wikisource contributors 2017)The poster illustrates the workflow from input to output, displaying a diagram of the process.The poster demonstrates achievements of this proof of concept and development ideas for the future. The main success lies in its modular development (fig. 4), making it amenable to further development3 (algorithm refinements, visualisation workflows, stylometric analysis). More languages and different text structures will be integrated and a wider range of output visualisations offered, while making use of the same core functionalities for ingesting and processing data.Figure 4 The data model of the application, illustrating the separation of concerns andthe potential for extensibility.
9425	2020	Project Twitter Literature (TwitLit), seeks to address a growing gap in the literary-historical record by establishing a consistent, rigorous, and ethical method for scraping and cleaning up Twitter data for the use of humanities scholars. In particular, my project explores the growing community of amateur writers who are using Twitter as a means of publication and dissemination for their literary output. There are three parts to my project: the research findings related to the global literary community on Twitter, the tools and resources developed as part of the project and made openly available to other scholars, and partnership with a university library to ensure the long-term preservation of the collected data.The data that I have collected shows that social media is altering literary practices by providing a space for amateur writers to publish, disseminate, and receive feedback from a global community of writers. Preliminary figures put the number of active Anglophone writers using Twitter as a publication platform for their literary output at over 1 million users per year since 2015, and writers working in non-English languages on Twitter raise these numbers even higher. This practice is changing how literature is produced, published, and shared. Readerships too are changing, for rather than being tied to print subscriptions or access to physical books, audiences of social media literature are based on online communities and tied to the costs of physical devices and internet access.My presentation will showcase these research findings in order to highlight the importance and necessity of social media archival work. In so doing, I will discuss how I collected the data using a Python script (co-developed by myself and several other scholars), challenges of cleaning up and visualizing this data (using ArcGIS and tools developed by Documenting the Now), and ethical best-practices for using social media data in research. Information relating to this process – including detailed instructions, the Python scripts used to collect Twitter data, and a list of resources – are free and openly accessible on the project’s website (www.twit-lit.com) and GitHub repository (https://github.com/TwitLit/TwitLitSource). Other scholars are invited to use these scripts and other resources to collect their own social media data.Additionally, my project has attempted to plan for the long-term preservation of the over eight million tweets that I have collected. This preservation has been made difficult by Twitter’s strict Developer Policy and Agreement, which prevents individuals from keeping or disseminating large data sets for more than 30 days. The only exception to this policy is made on behalf of academic institutions, which may store Twitter data for unlimited amounts of time on behalf of academic research. The Project TwitLit project thus presents best-practices for establishing a working relationship with university libraries for storing and disseminating Twitter data in a way that is both in accord with Twitter’s legal restrictions and responsive to the needs of scholars. In short, Project TwitLit provides a case-study of a growing community on Twitter while simultaneously developing a set of tools and guidelines for other scholars seeking to engage in similar work. In December 2017, the Library of Congress, which began archiving Twitter in 2010, announced that it would no longer collect all Tweets; instead, Tweets produced after December 2017 would only be collected on a selective basis. There are no other ongoing, systematic efforts to collect and preserve this digital material. See Library of Congress, “Update on the Twitter Archive at the Library of Congress” (December 2017). For more information related to the challenges of collecting and storing Twitter data, please see Christian Howard, “Studying and Preserving the Global Networks of Twitter Literature,” in Post-45.
9437	2020	Haiku Author Recognition1. IntroductionHaiku is a Japanese poetic form renowned for its brevity and expressiveness. Haiku derives from renga/renku – collaborative collections of verses with a 3-line opening hokku verse in the form 5-7-5 on (equiv. syllable). Matsuo Basho made famous the stand-alone hokku form, preserving the 5-7-5 on structure. The name haiku was associated with this form of hokku during 19th century.Four haiku authors rise in prominence above all: Matsuo Basho (17th century) is considered the “father” of haiku. Yosa Buson (18th century) focused on haiku as an art rather than a reflection of reality. Buson combined hokku with painting, inventing haiga (verse-painting). Kobayashi Issa (18-19th century) reinvented haiku through his depth of feeling and humanism. In the second half of the 19th century, Masaoka Shiki critically re-evaluated the art of haiku (coining the term), braking away from the traditional 5-7-5 form, and popularizing the poetic style beyond Japan.We present a study, which employs authorship attribution techniques to determine the distinctiveness of poetic styles in haiku, focusing on the poetry of Basho, Buson, Issa, and Shiki. There has been little work in the field of haiku attribution. A theoretical study of phonological complexity in haiku was presented in [1]. An approach to automatic evaluation of the quality of haiku was presented in [2]. An interesting work [3] deals with identifying unintended haiku in text. We approach haiku attribution as a classification problem: Given a set of attributed haikus, we train classifiers to recognize the writing style of each poet, and apply an ensemble of trained models to unattributed texts.2. Our Haiku Corpus The first step in creating our model was obtaining a haiku corpus. There are three approaches:Use actual haikus written in hiragana (a form of Japanese alphabet)Use Roman alphabet transcriptions (rōmaji) of haikus.Use English translations of haikus.While using hiragana haikus is arguably the best option, our software lacks the capability to process hiragana text. English translations of haikus are readily available, but while research suggests that the authorial signal is stronger than the translators’ [4], we do not know if that applies to haiku. We opted to construct a corpus of rōmaji transcribed haikus. This was difficult since most resources are either hiragana originals or translations. We obtained 723 haikus by Basho from [5], 842 haikus by Buson from [6], and 603 haikus by Issa from [7]. Finding transcribed Shiki haikus proved extremely challenging. Even though Shiki wrote over 24000 haikus, only a handful have been transcribed into rōmaji. Failing to secure transcriptions, we downloaded the full set of 24000 hiragana haikus from [8]. We then used an online hiragana-to-rōmaji transcription tool [9] to transcribe 967 randomly selected haikus by Shiki. Since many of the extracted haikus were organized alphabetically or by topic, we wrote Python code to randomly shuffle the order of the haikus for each author. A different program broke up the haikus into files of size 50 haikus each.3. Attribution MethodologyOur attribution software is a based on JGAAP [10] and implements an ensemble of classifier/stylistic-feature pairs [11,12]. For this study, we limited the set of stylistic features to character-2/3/4/5-grams (CnG), word-2/3-grams (WnG), vowel-initiated words (VIW), and first-word-in-sentence (FWIS). The classifiers used were support vector machines with sequential minimal optimization (SMO) and multilayer perceptrons (MLP).4. ResultsWe conducted several experiments, where we randomly chose one 50-haiku file for each author and removed it from the training set. We trained the classifiers on the remaining set of haikus using leave-one-out (L1O) validation. The results of the training for three sets of experiments are presented in Table 1:Table 1: Training Accuracy for Basho, Buson, Issa, and ShikiNext, we tested the authorship of the 50-haiku files that were left out of the training. The results of those experiments are presented in Table 2:Table 2: Attribution Results for Basho, Buson, Issa, and ShikiIt is quite clear that even with a reduced set of stylistic features, the attribution is very strong and the author identification definitive. We conducted an additional set of experiments, where we used each of the trained models to test the authorship of five haikus by the 18th century haiku poet Takarai Kikaku. The models were not trained on Kikaku, so, as expected, the results were split among two or more authors (Table 3):Table 3: Attribution Results for KikakuInterestingly, Kikaku was a prominent student and disciple of Basho, yet none of the models makes that association. This is most likely due to the small number of Kikaku haikus tested.5. Conclusion and Future WorkWe presented results from haiku author identification experiments, which suggest that haiku authorship can be determined even with a limited set of stylistic features from rōmaji-transcribed haikus. Our next efforts will be to experiment with a larger set of haiku authors, with English translations, and, possibly, with hiragana haikus. Among the questions we wish to answer are:What is the minimal set of haikus sufficient to identify an author?Is the authorial signal stronger than the translator’s for haiku translations?Can prosodic features be used for haiku author identification? Does the historical period affect the accuracy of attribution?
9459	2020	Centrality measures derived from character networks can be used to detect the main characters in a play. For example, previous research has shown that characters with high network centrality typically perform the majority of speech acts and appear in most of the scenes (Fischer, Trilcke, Kittel, Milling, & Skorinkin, 2018). However, one can extract character networks from plays in various ways: Close reading may omit minor characters like attendants or servants, e.g., (Moretti, 2011), while distant reading (e.g., parsing an XML file) may include aggregate characters like “All”, “Both Lords”, or similar. Furthermore, the networks may display either implicit or explicit connections, depending on whether we connect characters because they appear in the same scene or because they are directly addressing each other, respectively. Thus, as adding more characters or connections to the network affects centralities and other network measures, the interpretation of both qualitative and quantitative aspects of characternetworks depends on the extraction method. In this work we are concerned with the specific question whether details of the textual source and the extraction method, such as adding minor or aggregate characters, make the main characters less “central”. A negative answer to this question would provide us with a further evidence for the validity of automated literary network analysis.ApproachWe analyse six versions of the character network of Shakespeare’s “Hamlet”. All networks were extracted via close or distant reading from different XML or text sources and analysed with NetworkX (Python). For each network, we compute four different centrality measures (closeness, betweenness, degree, and eigenvector centrality). Subsequently, for each centrality measure, we rank the 26 characters common in all networks and compare character ranks in different networks by computing their Spearman rank correlation. Basic statistics of the character networks. Observations, Conclusion, and Outlook. The networks including implicit connections are denser than those with only explicit connections. This yields different centrality ranks including the most important characters. For example, Horatio has many more implicit connections and connections to minor characters, which makes him the character with the highest degree centrality in the Haworth network. In the Moretti network, which contains only explicit connections, Hamlet has the highest degree centrality. Degree centralities for the Haworth (implicit connections, distant reading) and Moretti (explicit connections, close reading) networksDespite such individual differences, the groups of main characters derived from different networks exhibit relatively stable rankings, cf. (Fischer, Trilcke, Kittel, Milling, & Skorinkin, 2018). In contrast, rankings for minor characters tend to differ significantly. Therefore, for detecting the group of main characters, the details of the network extraction method do not have a significant effect, at least in the datasets we consider. In future work we aim to validate the generality of this claim by considering larger corpora of dramatic plays. Heatmaps depicting the rank correlation between closeness centralities derived fromdifferent networks for all, the 10 most important, and the 10 least important charactersFinally, we outline some further observations about different centrality measures: In our datasets, degree centrality is the most robust, exhibiting high rank correlation for all considered sets of characters. In contrast, eigenvector centrality has the widest range of rank correlations suggesting its high sensitivity with respect to the network structure.AcknowledgementsThe authors gratefully acknowledge permissions to use material from Martin Grandjean and Roger Haworth. The work was funded by the HRSM project “KONDE – Kompetenznetzwerk Digitale Edition”. The work of Bernhard C. Geiger was partially funded by the Austrian Academy of Sciences within the go!digital Next Generation project “DiSpecs” (GDNG_2018-046_DiSpecs). The Know-Center is funded within the Austrian COMET Program – Competence Centers for Excellent Technologies – under die auspices of the Austrian Federal Ministry of Transport, Innovation and Technology, the Austrian Federal Ministry of Digital and Economic Affairs, and by the State of Styria. COMET is managed by the Austrian Research Promotion Agency FFG.
9472	2020	Born-digital or digitized resources enable researchers to apply computational methods to various research topics in the field of digital humanities (Klingenstein, Hitchcock, & DeDeo, 2014; Nanni, Dietz, & Ponzetto, 2017). There have been tutorials and workshops on how to apply them with multiple tools like R or Python targeted for DH researchers (Unsworth, 2009; Mäkelä, 2019; Mullen, 2018; The Programming Historian, n.d.; The Digital Humanities Summer Institute: Technologies East 2020, n.d.). In addition, there is an initiative, Always Already Computational: Collections as Data to develop a strategic direction and guide libraries and cultural heritage institutions to provide collections as data for researchers so that they can leverage computational methods (Padilla, Allen, Frost, Potvin, Russey Roke, & Varner, 2019).In this lightning talk, I will share my journey to making the NAHO (National Aboriginal Health Organization) WARC (web archive format) file computationally accessible. The University of Ottawa Library initiated a web archiving project to preserve entire NAHO web content which is currently accessible only via Wayback Machine. There is no easy way to extract data from the NAHO collection which prevents researchers to apply computational methods in research tools like R, Python, or Archives Unleashed toolkit. I will also talk about processes, challenges, and resources in order to provide computational access to library collections with an example of the NAHO WARC file.This lightning talk is targeted for DH or librarians, but open for all and they can learn the importance and processes of how to make digital collections computationally accessible.
9511	2020	Many of the online projects in the digital humanities have an implied planned obsolesce –which means that they will degrade over time once they cease to receive updates in their content and software libraries (Fitzpatrick 2011). We presented papers at Digital Humanities 2017, 2018, and 2019 that explored the abandonment and the average lifespan of online projects in the digital humanities (Meneses and Furuta 2017), contrasted how things have changed over the course of a year (Meneses et al. 2018), and introduced a strategy for preservation by creating standalone software executables (Meneses et al. 2019). However, managing and characterizing the degradation of online digital humanities projects is a complex and pressing problem that demands further analysis.In this sense “planned obsolescence” is a nuanced designation —as there are many cases of successful projects in digital humanities that are shifting their focus from active development to data management (for example: http://cervantes.dh.tamu.edu). These are cases where a project’s online presence has not received updates for some time but its online tools are stable and continue to be accessed by its users. However, if updates are not applied to the infrastructure or content of a project over time web requests will eventually start generating errors on the server or the client —affecting the overall user experience (Nowviskie and Porter 2010). These are examples of why the rules for traditional resources do not fully apply and new metrics are needed to identify issues concerning online projects in the digital humanities.In this study we dive deeper into exploring the distinctive signs of abandonment to quantify the planned obsolesce of online digital humanities projects. In our workflow, we use each project included in the Book of Abstracts that is published after each Digital Humanities conference from 2006 to 2019. We then proceed to periodically create a set of WARC files for each project, which are processed using Python (van Rossum 1995) and Apache Spark (Apache Software Foundation 2017) to statistically analyze the retrieved HTTP response codes, number of redirects, DNS metadata and detailed examination of the contents and links returned by traversing the base node. This combination of metrics and techniques has allowed us to assess the degree of change of a project over time. As one of the results from our 2019 presentation, we claimed that the most important signature for degradation comes from the assessing the validity and overall health of the topology of links in a project. Thus, the focus of our study is analyzing this key signature.We acknowledge that research on the preservation of projects in the digital humanities is also carried out by other groups (Larrousse and Marchand 2019) (Arneil, Holmes, and Newton 2019). However, our study is different as it focuses on two points: first, identifying the signals of abandoned projects using computational methods; and second, quantifying their degree of abandonment. In the end, we intend this study to be a step forward towards better preservation strategies for the planned obsolesce of online digital humanities projects.
9520	2020	The International Organization for Migration (IOM) recently reported that the world has more migrants than ever before “both numerically and proportionally” and that the number of environmental migrants alone could reach 1 billion by 2050 (2017, p. 2; 2014). The IOM has also argued that understanding how people think and feel about migration is essential to the development of policy that supports the safe passage and successful integration of migrants into their new communities (2017). The United States receives more migrants per year than any other country (United Nations, 2016). It is also a country where migration is a prominent topic in social, political and media discourse. This poster reports the findings of an exploratory study whose principal research questions are: 1) What do Twitter users in the United States talk about in their migrant- and migration-related tweets, 2) how do authors feel when they tweet about these subjects, and 3) how do their sentiments and topics of interest compare to Twitter users residing outside the United States? To approach these questions, we apply sentiment analysis, topic modeling and time series analysis to 111,785 English language tweets containing the term “migrant” or “migration” collected from the Twitter API between January 21 and March 4, 2019. Our quantitative and computational methodology relied on descriptive statistics, the Python programming language, and associated libraries, such as the Natural Language Toolkit (NLTK) for text processing, Pandas for data manipulation, VADER for sentiment analysis, Gensim for topic modeling and Seaborn for data visualization. The visualized data shows that authors in the United States focused more on politics whereas authors in other countries focused more on humanitarian issues. Tweets from U.S. authors were more negative than those authored by residents of other countries, except when the topics of the tweets involved children. A time series plot revealed three sentiment spikes, one positive and two negative, over the course of the collection period. The negative spikes were partially explained by looking at word frequencies, visualized as word clouds, alongside news headlines for the dates in question. The positive spike was more difficult to explain because of the limitations of sentiment analysis and the negative nature of news reporting. Our findings cannot be generalized because the Twitter Search API does not generate a representative sample (González-Bailón, 2014). Future work could involve an improved sampling method that would permit the use of inferential statistical methods. The addition of multiple languages to the tweet dataset or social network data to the analysis may yield interesting new findings.
9556	2020	We introduce a tablet based interactive application which presents a comprehensive digital exploration as a companion to a musicological article. Taking the opera Lohengrin, the article and digital companion show how one motive is altered each time it recurs, reflecting its role in the drama, and exploring Wagner’s sophisticated treatment of recurring themes. Musicological analysis is encoded using Linked Data as an independent, repurposable, and open Research Object. Interactive user views are generated directly and dynamically in the browser from this knowledge graph using novel visualisations, which in turn enable the user to navigate all possible paths through the evidential multimodal materials. Our companion explores the different compositional devices Wagner uses to vary his motives, browsing the whole opera for motive occurrences and their musical and textual contexts. Visualisations and recordings support the analysis, making it accessible to an audience that may struggle with a Wagnerian orchestral score.Musicological argument has traditionally been communicated in writings that are textual, linear and illustrated by occasional figures, despite almost always being concerned with diverse subjects and evidential materials, each potentially exemplified by different media, and each potentially a springing point for digression and exploration of the author's argument. While exploring the referenced materials may be non-linear, this is neither embodied in nor enabled by traditional communication mediums.  Previously we have digitally enhanced an extract of a musicological article with dynamic and interactive elements. Here, we introduce a tablet based interactive application which presents a comprehensive digital exploration as a companion to a complete musicological article. Taking the opera Lohengrin, the article and digital companion show how one motive is altered each time it recurs, reflecting its role in the drama, and exploring Wagner’s sophisticated treatment of recurring themes. Musicological analysis is encoded – along with relationships to multimedia materials – using Linked Data as an independent, repurposable, and open Research Object. Interactive user views are generated directly and dynamically in the browser from this knowledge graph using novel visualisations, which in turn enable the user to navigate all possible paths through the evidential multimodal materials.Our companion explores the different compositional devices Wagner uses to vary his motives, browsing the whole opera for motive occurrences and their musical and textual contexts. Visualisations and recordings support the analysis, making it accessible to an audience that may struggle with a Wagnerian orchestral score. Exploration of this material can follow or be triggered by the article, but can also be reader-driven, with free browsing of the curated musical landscape. A video essay also provides a source of narrative paths through the companion, as a guide itself and as a source of starting points. Figure 1: The musicological essay view in the digital companion.The application is built with a new version of the MELD (Music Encoding and Linked Data) framework. MELD traverses Linked Data graphs to select and filter relevant information, with reusable components for creating and retrieving annotations, and for displaying and interacting with musical, textual, graphical and audio-visual materials. MELD is written in Javascript and Python, with resources using standards including the MEI music encoding, TEI, the Music Ontology and Web Annotations. Figure 2: A view with labelled vocal score, text and translation.We provide two views for music notational content. Vocal score reductions are rendered from MEI with structural analysis dynamically overlaid; annotations trigger audio playback from that point. A second notational visualisation of MEI simplifies the complexity of a Wagnerian orchestral score: each instrument playing at a particular time is shown as a coloured ribbon, with the instrument's section of the orchestra providing the colour. This highlights differences across orchestration iterations that may seem identical as vocal score.Figure 3. The same iteration showing the orchestral summary view.For an opera thousands of bars long, overviews are crucial. An ever-present timeline shows all occurrences of a motive, providing a visual summary and a base for navigation. In the Time Machine view, users can also flick through motive occurrences – visualised as libretto, vocal score or orchestration – summarising the sequence within the opera, supporting quick comparisons, and as an index to detail views.
9564	2020	How do we bridge the gap between ambitious global schemes, such as Paul Otlet’s “Aims of documentation” (Otlet, 1934) or the FAIR data principles (Wilkinson et al., 2016), and existing information practices? We describe the theoretical basis and practical steps for a subject-oriented approach to this problem, examining data-related expectations through the lens of documentarity.In 1934, Belgian bibliographer Paul Otlet published a Treaty of documentation in which he outlined the “Aims of documentation”:“Universal as to their purpose; reliable and true; complete; fast; up to date; easy to obtain; collected in advance and ready to be communicated; made available to the greatest number of people.” (Otlet, 1934, p. 6)In 2016, the FAIR principles were published along similar lines:“To be Findable; to be Accessible; to be Interoperable; to be Reusable.” (Wilkinson et al., 2016, p. 4)They differ in some ways: Otlet viewed the Aims as a whole, with openness as a critical element, while FAIR is modular and not necessarily synonymous with open data. But more importantly, they both describe a plan which is meant to precede and guide implementation. Otlet’s Aims are broken down into goals related to the actual “biblio-technie” or “bibliothéconomie” (Otlet, 1934, pp. 372–375); similarly, each of the 4 components of FAIR is itself divided in 4 sub-components which delve into technical matters (e.g. data vs. metadata). These are actionable steps to be applied in the field, which is where trouble begins.During and after his time, close collaborators and distant peers alike noted the gap between Otlet’s ambitions and what he was able to achieve: Valère Darchambeau commented on “Mr. Otlet’s mental audacities, his utopias some would say” (Mundaneum archives, PP P0 462); Suzanne Briet called him ironically “the magus” of documentation. Indeed, he had a major impact on the institutionalization of documentation—the development of Library and information science (LIS) in Europe owes much to section 4 of his Treaty—but his work on the relationship between subject and knowledge was largely neglected. The techno-semiotic mediations of information have been far less studied in LIS than human ones; we can arguably trace this back to Otlet’s incomplete legacy. Conversely, the implementation of FAIR principles quickly raised the issues of user experience, expectations and metrics:“FAIRness is aspirational, yet the means of reaching it may be defined by increased adherence to measurable indicators . . . metrics that reflect the expectations of particular communities.” (Wilkinson et al., 2017, pp. 1–2)The interface between person and information seems much thinner for computer-held data than for library books. While this is not actually true (mediations have simply shifted towards human-computer interaction), it means that the feasibility of principles is challenged almost immediately by subjective experience. Data may be FAIR but people may differ: they do not all work on the same data or with the same mindset and therefore have different expectations. This shapes the way we assess data within the framework of documentation and therefore its value to us—its documentarity.Documentarity is the product of interdisciplinary theoretical work, at the intersection between ontology, documentation and linguistics. The first of these two influences have been studied: documentarity can be seen as a philosophy of evidence based on documentation (Day, 2019) and also as the quantifiable documentary quality of things, with applications to digital documents and data (Perret & Le Deuff, 2019). Here, we examine the third influence: how linguistics contribute to documentarity as an epistemological proposal which at the core focuses on the reception of information. We show that documentarity is linked to several works: Roman Jakobson’s “literaturnost”, which in French (“littérarité”) (Jakobson, 1977, p. 16) is very close to documentarity (“documentarité”); Hans Robert Jauss’ adaptation of horizons of expectation (“Erwartungshorizont”) to literature (Jauss, 1970); the shape of enunciation with Mary-Ann Caws “architexture” (Caws, 1981, p. 10) and Roger Laufer’s “scripturation” (Laufer, 1986, p. 75).This array of concepts is dense but its purpose is coherent: we draw from the phenomenology of the reading process to make better sense of the way we assess computer-held data. Our methodology is to track the embodiment of thought in technological mediations, especially in writing. The usefulness of such an approach has been described for the study of information as experience (Gorichanaz, 2017). We argue that the way we perceive the documentarity of data is shaped by our horizons of expectation, especially previous experience of genre-based rules which me must establish if we wish to prevent global principles from falling into abstraction as soon as they enter the field.In this perspective, digital notebooks form a stimulating case study, highly relevant to the conference’s theme on open data. They relate to a tradition and to new practices (data science, data papers). We analyze the way data is presented and interacted with in R, Python and Javascript-based notebooks, and we observe a reflexive impact on our perception of documentarity: it allows us to relate more practically to the intellectual framework behind Otlet’s “Aims of documentation” and the FAIR principles, which could improve their adoption. Through reproducibility and replicability, the practice of the notebook informs us on the relationship between data and truth. It also underlines the status of text as the most basic and universal type of data in science: the way text is handled in notebooks (lightweight markup languages, integration of standards, automation) shifts our perception of ‘text’ to ‘textual data’. This is independent from the field of study: we suggest that any research built from plain text can be considered a data paper and that extending “FAIRness” to scientific writing in general would be an epistemological breakthrough in scientific communication.
9608	2020	The project "SustainLife – Sustaining Living Digital Systems in the Humanities" that is currently running at the Institute of Architecture of Application Systems (IAAS, University of Stuttgart) and the Data Center for the Humanities (DCH, University of Cologne) deals with the conservation of research applications in the field of Digital Humanities (DH). By employing the TOSCA standard (Topology and Orchestration Specification for Cloud Applications) to fully automate the deployment of DH applications and to keep them available in the long term, we try to tackle the problem of software obsolescence in the field of DH. To interactively demonstrate our approach to the international DH community, we would like to give a workshop on the topic "Modelling and Maintaining Research Applications in TOSCA" in the run-up to the DH 2020 conference. Thereinwe will show how to model (DH) software systems with TOSCA and share experiences and best practices on how to work with the OpenTOSCA ecosystem, an open-source implementation of the TOSCA standard.The ProblemThe establishment of the DH as an independent scientific research area as well as the increasing usage of digital methods in the research process require adjustments to common result assurance practices. For example, the long-term archiving (LTA) of primary research data uses well-established practices such as employing standardized data formats and forwarding data to permanent repositories. However, the fact that digital artifacts generated in DH-oriented research do not only consist of primary data but also contain research software is mostly disregarded (Sahle and Kronenwett, 2013). Moreover, the variety of DH research outcomes includes so-called "living systems" in which the software to present, access or analyze the data represents an essential part of the actual research output (Bingert et al., 2016). In contrast to classical research results such as monographs or encyclopedias, living systems cannot be served long-term without maintenance as their instantiation, supervision, and permanent provisioning represent major technical, organizational, and financial challenges. Furthermore, the heterogeneity of the research software generated in the DH requires a highly flexible preservation strategy, i.e., a suitable technology that ensures standardization, reusability, and archiving of as many digital artifacts as possible (Barzen et al., 2018). In addition to the aforementioned challenges, i.e., heterogeneity, underfunding, and obsolescence of digital artifacts, scientific practice requires long-term interoperability and traceability of all research outcomes. With regard to digital systems, these requirements are (1) constant accessibility, (2) the possibility of error-free operation, and (3) the ability to reconstruct any stage of development of a research application at any time without major structural difficulties.Our ApproachThe TOSCA standard (OASIS, 2013 and 2019) allows software systems to be modelled, provisioned, and deployed in a standardized and provider-independent manner. Thus, it is suitable for long-term archiving and operation of research applications produced within the field of DH (Neuefeind et al., 2018 and 2019). Following the TOSCA standard, applications are modelled in “Topology Templates” by describing their components and their relations amongst each other: Components are represented as “Node Templates”, while relations are modeled as “Relationship Templates”. Moreover, the semantics of a Node Template or Relationship Template are dictated by reusable types, i.e., “Node Types” and “Relationship Types” respectively. For example, a Python web application can be modelled as a Node Template that is an instance of the "Python Application" Node Type. To express that the Python Application accesses a MySQL database, a second Node Template that is of type "MySQL Database" can be added to the Topology Template. Then, the connection between both components can be described by a Relationship Template that is an instance of the Relationship Type "connectsTo". Additionally, to specify that both components are running on an Ubuntu virtual machine (VM), a Node Template of type "Ubuntu VM" can be added, while Relationship Templates of type “hostedOn” between the Python Application Node Template and the VM Node Template, as well as between the MySQL Database and the VM describe their respective hosting relations.Hereby, TOSCA's type system enables the modelling of reusable component types, e.g., the "Python Application" Node Type, which can be reused in multiple Topology Templates describing different applications. Therefore, synergic effects emerge as existing Node Types can be reused in other Topology Templates, easing the modelling of new applications. In addition, the open-source TOSCA implementation OpenTOSCA (Breitenbücher et al., 2016) offers the possibility to graphically model applications using the TOSCA editor “Winery” (Kopp et al., 2013) which further simplifies the creation of new applications by providing drag-and-drop modeling capabilities.Workshop CurriculumDuring our four hour workshop, we will (1) give an overview to different solutions for long-term preservation of living systems and (2) describe the modeling language TOSCA. Based on these theoretical units, practical tasks will introduce (3) the modelling of an existing application using TOSCA and (4) how applications can be deployed using the OpenTOSCA ecosystem. Thus, by combining the theoretical foundations and the practical application of TOSCA, the participants will be able to model (research) software systems according to the standard and provision and deploy applications using the OpenTOSCA ecosystem.The practical tasks are structured as follows: (1) Identify the components of an application and (2) describe them and their relations among each other in an TOSCA-based application topology, i.e., in a Topology Template. By fragmenting an application into its components and mapping them to TOSCA Node Types, the Topology Template describing the application can then be modelled using the OpenTOSCA ecosystem. Afterwards (3), the modelled TOSCA application will be deployed by the OpenTOSCA runtime. Moreover, by sharing our experiences and best practices in using OpenTOSCA with the community, we will introduce concepts such as "software stacks" in a practical way.Target GroupThe workshop is primarily designed for data center employees, libraries and other institutions focusing on infrastructures for long-term archiving and operation of heterogeneous software systems. Previous experience in dealing with Linux and writing shell scripts as well as with software stacks and service orchestration are helpful but not necessary for a successful participation. To provide a productive context for communicating the described content and to enable individual consultation and support, we designed the workshop for about 20 participants but limit it to a maximum of 30 participants.Technical PrerequisitesFor a successful participation in the workshop, it is necessary that each participant brings his/her own laptop. Although a shared instance of the OpenTOSCA ecosystem will be provided, it is desirable that all participants set up an OpenTOSCA instance on their work equipment prior to the workshop in order to perform modelling and deployment tasks on their own devices. Therefore, registered participants will be provided with all necessary information about system requirements and how to setup OpenTOSCA prior to the workshop. Furthermore, relevant documentation, publications, and manuals will be provided both in advance and in the context of the workshop. In addition, a stable internet connection as well as a sufficient number of power outlets for all electronic devices are indispensable. About the InstructorsUwe Breitenbücher is a research staff member and postdoc at the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. His research vision is to improve cloud application provisioning and application management by automating the application of management patterns. Uwe was part of the CloudCycle project, in which the OpenTOSCA Ecosystem was developed. His current research interests include cyber-physical systems, blockchains, and microservices.Anna Fischer is a research assistant at the Data Center for the Humanities (DCH) at the University of Cologne and joined the “SustainLife” Project in January 2020. Her recent research and working activities have focused on data management and software development for natural language processing tasks, e.g., in collaboration with one of the chairs for Romance linguistics at the University of Cologne. Lukas Harzenetter is a research associate at the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. He received his Master of Science degree from the University of Stuttgart in Software Engineering in 2018. His research interests are in the field of cloud deployment and management models focusing on the development and change of such models over time. Lukas is part of the “SustainLife” project which is working on sustainable application deployments in the domain of digital humanities.Frank Leymann is a full professor of computer science and director of the Institute of Architecture of Application Systems (IAAS) at the University of Stuttgart, Germany. His research interests include service-oriented architectures and associated middleware, workflow- and business process management, cloud computing and associated systems management aspects, and patterns. Frank is co-author of more than 400 peer-reviewed papers, about 70 patents, and several industry standards. He is an elected member of the Academy of Europe.Brigitte Mathiak is chairwoman of the Data Center for the Humanities (DCH) and is particularly interested in data management and text mining. The idea for the "SustainLife" project arose after she had experienced again and again how living systems have to be abandoned or neglected. She is Junior Professor for Digital Humanities at the University of Cologne and Senior Scientist at the Leibniz Institute for the Social Sciences (GESIS).Claes Neuefeind is a postdoc at the Cologne Center for eHumanities (CCeH) at the University of Cologne. He worked with Philip Schildkamp and Lukas Harzenetter on the DFG-LIS project "SustainLife" until October 2019 and changed for a position that is responsible for coordinating the Digital Humanities of the North Rhine-Westphalian Academy of Sciences and the Arts office.Philip Schildkamp has been researching since 2015 and teaching since 2017 at the University of Cologne. He studied sociology, psychology, and Digital Humanities information processing. The main topics of his employment are technical infrastructure measures in the field of (Digital) Humanities and the orchestration of distributed software systems. Since March 2018, Philip has been part of the DFG-LIS project "SustainLife" at the Data Center for the Humanities (DCH).
9645	2020	This poster presents a technical report and a method for corpus expansion in the humanities, with an application to early modern philosophy, alongside a case study of dealing with heavy data redundancy in several Latin, English, and French title corpora. It enlarges on the steps taken during the initial stages of a data-intensive research project that aims to go beyond established writers and views in natural philosophy between 1600 and 1800 and it reflects on the collaboration between a humanist and a data scientist with respect to web-scraping and redundant multilingual data taming in Python.
9651	2020	The poster will introduce the Distant Viewing Toolkit (DVT), a Python package for the computational analysis of visual culture. The toolkit addresses the challenges of working with moving images through the automated extraction and visualization of metadata summarizing the content (e.g., people/actors, dialogue, scenes, objects) and style (e.g., shot angle, shot length, lighting, framing, sound) of time-based media. DVT is optimized for two purposes: scholarly inquiry of visual culture from the humanities and social sciences, and search and discovery of collections within libraries, archives, and museums.
9655	2020	Las Humanidades Digitales (HD) se han convertido en un campo de interés en España, especialmente en la última década, a pesar de haber llegado más tarde que en la mayoría de los demás países europeos. De hecho, representan una tendencia destacada en la investigación, ya sea como campo de estudio que como tema de financiación preferente. Al mismo tiempo, por su novedad, están siendo objeto de escrutinio por parte de la comunidad investigadora y de las instituciones gubernamentales que financian la investigación. El objetivo del estudio ha sido identificar a los investigadores que trabajan en el campo de las HD en España y explorar su financiación, sus afiliaciones institucionales, las temáticas de investigación y los recursos digitales desarrollados.En el pasado se han promovido iniciativas similares, que han producido mapeos centrados en el ámbito internacional o nacional, algunos de los cuales siguen disponibles en línea, con diferencias que van desde la cobertura geográfica hasta el tipo de datos mapeados (Ortega; Eunice-Gutiérrez, 2014; Romero-Frías; Del-Barrio-García, 2014). Otros estudios sobre este tema han optado por un enfoque diferente, utilizando la bibliografía u otras fuentes para identificar las etapas más relevantes en la evolución y consolidación de esta disciplina en España (Rojas-Castro, 2013; González-Blanco, 2013; Spence; González-Blanco, 2014; Baraibar-Echeverria, 2014). El presente trabajo no pretende ser una revisión histórica exhaustiva, sino ofrecer una visión complementaria y actualizada del panorama de las HD en España, tomando en consideración datos recientes y fuentes de información no explotadas anteriormente.Materiales y métodosA continuación se describen los resultados de la investigación sobre el estado actual de las infraestructuras de investigación digital en España, entendidas como la combinación e integración entre los recursos de información digital, las herramientas analíticas y de visualización y la comunidad activa de investigadores, colaborando a través de proyectos de investigación, financiados por el sector público o privado. Por esta razón, hemos subdividido el objeto de la investigación en cinco entidades principales, concretamente: investigadores, proyectos, recursos, bibliografía y cursos de posgrado.Nuestra metodología de recopilación de datos ha tenido una doble vertiente. Por un lado, hemos seleccionado manualmente información disponible en línea a partir de congresos, seminarios, convocatorias temáticas, mapas participativos, etc. Por el otro, hemos extraído información a partir de bases de datos existentes: publicaciones científicas en el campo de las HD presentes en Dialnet y en ÍnDICEs-CSIC y proyectos de investigación financiados a través de la Agencia Estatal de Investigación, seleccionados a través de una serie de palabras claves (Figura 1).El volumen total de registros recopilados ha sido 1.359, distribuidos de la siguiente manera: 577 investigadores; 368 proyectos; 88 recursos; 9 cursos de posgrado y 8 revistas científicas. El conjunto de datos analizados se encuentra disponible en Acceso Abierto (https://doi.org/10.5281/zenodo.3893546), junto con los Jupiter Notebooks y el código Python para reproducir los análisis.Figura 1. Fuentes de datos utilizadas en la investigación.ResultadosEntre los investigadores identificados, 305 son varones (52,9%) y 272 mujeres (47,1%): una proporción que, comparada con la proporción de género entre los investigadores de España en todas las disciplinas (61,2% de varones y 38,8% de mujeres) o limitada a las Humanidades (59,8% de varones y 40,2% de mujeres), refleja una presencia femenina significativamente mayor de la esperada.La clasificación de los investigadores por disciplinas, 19 en total, muestra una amplia variedad, con una clara prevalencia de filólogos (36%), seguidos por historiadores (16,5%) e informáticos (10,8%).El análisis de las conexiones entre disciplinas y temas de investigación (Figura 2) revela cinco grandes grupos disciplinarios: Historia (32% de los nodos), Filología (24%), Comunicación (20%), Ciencias de la Computación (17,3%) y Documentación (6,7%). Las Ciencias de la Computación, a pesar de ser la cuarta comunidad en términos de nodos, resulta ser la disciplina relacionada con el mayor número de temas de investigación (13,3%).Figura 2. Análisis de redes de disciplinas y temas de investigación. El tamaño de los nodos corresponde al número total de conexiones, el color a la comunidad y el grosor de las aristas al grado de conectividad entre parejas de nodos.Según la afiliación institucional, la mitad de los investigadores (49,2%) pertenece a un total de nueve instituciones, mientras que la otra mitad se encuentra dispersa entre 84 diferentes centros (http://sl.ugr.es/DHmap).La afiliación a departamentos (Figura 3) muestra un patrón mucho más variado en cada institución: por ejemplo, el peso relativo de la Filología es mayor en la Complutense de Madrid o en Santiago de Compostela con respecto a Granada, donde por otra parte se aprecia mucha variedad, con investigadores procedentes de 13 departamentos.Figura 3. Peso relativo de cada departamento en los nueve centros de investigación principales. Para favorecer la comparación, se ha utilizado un histograma apilado.El análisis de la inversión en 337 proyectos de investigación en los últimos 25 años (Figura 4) permite avanzar una periodización en tres fases en la consolidación de las HD en España: 1993-2003; 2004-2014; 2015-2019. La financiación media por proyecto ha sido de 64.313€, pero la mediana se encuentra en 42.350€; el 5% de los proyectos ha recibido hasta 5.000€ (micro-proyectos) y el 90% menos de 100.000€.Figura 4. Total de proyectos de investigación en HD y financiación recibida durante el período 1993-2019.Entre las fuentes de financiación, el rol del ministerio es predominante, con 77% de las propuestas financiadas y 72% de los recursos asignados, a pesar de la gran variedad de organismos implicados, hasta 26, y de un 10% de recursos procedentes del sector privado.Disciplinas como la Filología, la Lingüística y la Biblioteconomía evidencian una tradición más larga que otras, como la Historia, la Arqueología y la Historia del Arte, en el desarrollo de recursos digitales para la investigación (Figura 5). Los artefactos desarrollados con mayor frecuencia (72,4%) son diferentes tipos de bases de datos (bibliotecas digitales, catálogos, repositorios, etc.), mientras que a partir de 2014 se observa un cambio hacia instrumentos más analíticos o participativos. La sostenibilidad de las plataformas en le tiempo se ha garantizado principalmente mediante la financiación en serie de proyectos del Programa Estatal de I+D+i.Figura 5. Recursos digitales clasificados según disciplina y tipología.
9726	2019	
        
            We present here a project to prepare the digital critical edition of the 
                Chronicle of Matthew of Edessa, which is due to finish its first stage in April 2019. The 
                Chronicle is a 12
                th century Armenian-language historical text covering events in the Near East during the 10
                th-12th centuries. This takes the reader from the apogee of the medieval Armenian kingdoms to the fall of most of them during the eleventh century as their lands were annexed to Byzantium and ultimately lost to the Seljuk Turks. The Chronicle is also an important source for the history of the First Crusade, and particularly the Crusader County of Edessa. There are about 40 known manuscripts that contain the text of the Chronicle in whole or in part, copied between the end of the 16
                th century up to the 19
                th.
            
            In our workflow we have adopted the stages of digital critical edition suggested by Robinson 
                (2004) – transcription, collation, stemmatic analysis, edition and publication. We found, in the process, that these stages do not occur in a strict succession; it was quite regularly necessary to move back and forth between stages, refining earlier steps on the basis of later results. One of the central features of our project was to adopt a 
                continuous integration (CI) system
                
                     In this case, the software in question is Concourse (
                        https://concourse-ci.org/).
                    
                 in order to manage the work across these stages in a sensible manner. The primary challenge we then had to overcome was the need to ensure that the data was cleanly maintained from beginning to end, as the nature of CI design does not allow for modifications in the middle of the pipeline.
            
            Beginning with the transcription, where we also followed general guidelines proposed by Robinson and Solopova 
                (1993), diplomatic transcriptions of all available manuscripts were made in T-PEN 
                (Ginther et al., 2009) according to guidelines maintained in the project’s GitHub repository, and converted from T-PEN’s native Shared Canvas JSON format into valid TEI-XML using a Python library developed for the purpose. A major advantage of the digital approach is the ease with which entire categories of transcription error can be identified and corrected automatically. The CI framework enabled parsing and validation errors to be spotted immediately. Rare cases could be corrected manually in T-PEN (that is, the source of our pipeline data), but widespread mistakes required us to revise our workflow tools to behave in a way more akin to functional programming, so that we could insert custom code to handle peculiarities of our specific texts without compromising the more general design of the tools themselves. 
            
            Given transcriptions that passed our litmus tests for validation and basic accuracy of content, we were then ready to collate them using the JSON input functionality of CollateX 
                (Dekker and Middell, 2011). Here again we relied on custom programming within the CI setup – while programmatically taking TEI files and tokenise the text contents into individual readings is a straightforward task in its basics, the specifics will vary massively from text to text. Our tokenizer software thus provides a number of code plug-in interfaces that can be used to generate a correct token, also in the specific XML context of that reading in the document. The CI setup also allowed us to make, and preserve, a number of custom modifications to how we used CollateX, in order to maximise its accuracy.
            
            Since one of our transcription guidelines was to leave abbreviations unexpanded, we also developed a tool using a combination of base text collation, regular expression logic, and user interactivity to (semi-)automatically expand these abbreviations and store the results in a database, which could in turn be fed into the tokenisation step on later runs of the pipeline. Here too we retained the principles of diplomatic transcription: if the word was abbreviated the way that it could be expanded in a canonical orthography, we did so, otherwise, we tried to follow the intended (or perhaps mistaken) spelling of the scribe. 
            Given full collations of the chronological sections within the text, our editorial analysis could begin. We have used the Stemmaweb tool 
                (Andrews and Macé, 2013) both to normalise and to specify classes of relationship between individual readings throughout the text, which in turn eases the stemmatic analysis of the manuscripts. Recent versions of Stemmaweb also provide a means of indicating the ‘lemma’ reading among a set of variants, so that critical edition text can be produced directly. At this time a system for annotation of the textual content is under development, which will enable us to provide a digital commentary on the historical content of the 
                Chronicle, as well as the edition itself.
            
            Although our project has not extended the CI model past the stage of collation – this would require a system to save and “re-play” editorial decisions concerning the collated text, which would have required more of an engineering effort than we had resources for within the framework of the project – we consider this to be an important direction for producing a critical edition that is truly reproducible from the textual evidence at its base. 
        
        
            
                
                    Bibliography
                    
                        Andrews, T. L., and Macé, C. (2013). Beyond the tree of texts: Building an empirical model of scribal variation through graph analysis of texts and stemmata. 
                        Literary and Linguistic Computing, 
                        28(4): 504–21. 10.1093/llc/fqt032.
                    
                    
                        Dekker, R. H., and Middell, G. (2011). Computer-supported collation with CollateX: Managing Textual Variance in an Environment with Varying Requirements. In 
                        Supporting Digital Humanities. Copenhagen. http://crdo.up.univ-aix.fr/SLDRdata/doc/show/copenhagen/SDH-2011/submissions/sdh2011_submission_54.pdf.
                    
                    
                        Ginther, J., Firey, A., O’Sullivan, T., Walker, A., Elliot, M., Gaffield, M., … Cuba, P. (2009, 2012). T-PEN: Transcription for Paleographical and Editorial Notation. http://t-pen.org/TPEN/.
                    
                    
                        Howe, C. J., Connolly, R., and Windram, H. F. (2012). Responding to Criticisms of Phylogenetic Methods in Stemmatology. 
                        Studies in English Literature 1500-1900, 
                        52(1): 51–67. 10.1353/sel.2012.0008.
                    
                    
                        Robinson, P. (2004). Making electronic editions and the fascination of what is difficult. 
                        Linguistica Computazionale, 
                        20–
                        21: 415–38.
                    
                    
                        Robinson, P., and Solopova, E. (1993). Guidelines for Transcription of the Manuscripts of the Wife of Bath’s Prologue. In 
                        In The Canterbury Tales Project Occasional Papers 1
                    
                
            
        
    
9774	2019	
        
            
                
                    Introduction: The Beatles and musical complexity
                
                The Beatles are considered to be one of the most influential bands of the 20th century, who still shape and influence pop music today (Everett, 2001). In the course of their creative history, the band has proven an enormous range and variety of individual compositions. One reason for the unusually large musical diversity of the Beatles is that with Paul McCartney, John Lennon and George Harrison three persons were involved in composing the Beatles’ songs (MacDonald, 1995). In addition, the Beatles’ producer George Martin also had a considerable influence on the composition of many songs (Martin &amp; Hornsby, 1979). 
                In this paper we will investigate how this diversity in composition is reflected in musical complexity in the work of the Beatles by using computational methods. So far, related work studies can be found in Mason (2012), who statistically analyzes the properties of Beatles songs in order to decipher what he calls the “Beatles genome”. The aspect of complexity has already been investigated by Eerola et al. (2000), who use a MIDI corpus to analyze the relationship between musical complexity in Beatles songs and its effect on chart placement. Eerola et al. (2000) also discover a highly significant increasing time-trend in melodic complexity, i.e. the Beatles’ songs became melodically more complex through the course of time. While Eerola et al. (2000) only looked at singular melodies and their complexity, we present a study that takes into account all of the available melodies and also the chords to compute the complexity of a song. We present an exploratory tool for the interactive visualization of musical complexity distributions in the work of the Beatles. The visualizations can be scaled from all of the Beatles’ albums to single songs and composers, to investigate complexity on a more detailed level.
            
            
                Corpus and Method
                
                    Corpus – The corpus used for this study is based on guitar tablatures from the online platform 
                    Ultimate Guitar
                    
                        
                             Ultimate Guitar portal: https://www.ultimate-guitar.com/, (Note: all URLs mentioned in this paper were last checked Nov. 25, 2018)
                        
                    , which were downloaded in 
                    GuitarPro
                    
                        
                             Guitar Pro Tool: https://www.guitar-pro.com/
                        
                     format and converted to 
                    MusicXML
                    
                        
                            MusicXML documentation: https://www.musicxml.com/for-developers/
                        
                     for analysis. This platform has already been successfully used as a data source for other scientific studies (Di Giorgi et al., 2017) and includes 205 songs from the Beatles’ first single in 1962 to their last studio album in 1970. 
                
                
                    Normalization – To be able to compare songs with different scales to each other we normalize the note inventory according to Cuddy et al. (1981), who propose the 
                    Roman Numeral Analysis method. With this method all notes of a diatonic scale are represented by Roman numerals, starting from the basic note of the scale as step I. Tones that are not part of the scale are marked with a sharp (#) (cf. Fig. 1). 
                
                
                    
                    Example for the classification of tones according to the Roman Numerals Analysis using the scales of C Major /A Minor and D Major / B Minor.
                
                
                    Complexity model – To operationalize the concept of musical complexity we rely on experiments by Krumhansl &amp; Shepard (1979), in which test persons were asked to evaluate how a certain tone completes the tone sequence of a scale. The results show that scale tones (the keynote in particular) are rated better than non-diatonic tones. Building on this previous work, we define four levels of complexity (cf. Fig. 2). We use this complexity model for both, the analysis of single notes as well as for chord progressions.
                
                
                    
                    Classification of tones into different levels of expectation-based complexity using the example of C Maj. / A Min.
                
                
                    Computation of results – Our corpus of MusicXML files is analyzed by means of Python scripts that parse individual notes and chords from the data and count their frequencies. For the recognition of chords, the existing 
                    music21
                    
                        
                             Music 21 Toolkit: http://web.mit.edu/music21/ 
                        
                     library is used, as it provides many useful functions for the analysis of transcribed music. Because both the tone material and the chord material of each song are to be analyzed on the basis of the previously described Roman Numeral categories, it is necessary to identify the scale of a song. In many songs the scale is explicitly annotated by means of global accidentals and therefore can be extracted directly from the MusicXML transcription. For those cases where global accidentals are missing, we apply an existing algorithm for scale detection (Madsen et al., 2007). By assigning notes and chords to the previously introduced four complexity levels, a complexity distribution can be calculated for each song and album.
                
            
            
                
                    Results and Discussion
                
                The results can be analyzed statistically, to detect general trends in the development of musical complexity in the work of the Beatles. We conducted a Pearson-Bravais (Pearson, 1895) correlation test to investigate how musical complexity has developed through time. For each year, we calculate the frequency of tones and chords that belong to the complexity levels 1+2 (rather low complexity) and also the frequency of tones that belong to the complexity 3+4 levels (rather high complexity). For the higher complexity levels, we find a weak positive correlation (cf. Cohen, 1988) for both tones (r = 0.208, p=0.005) and chords (r = 0.167, p=0.024). These results indicate a gentle trend toward increased musical complexity over time, but cannot confirm the observation of a highly significant correlation as noted by Eerola et al. (2000). Our results rather seem to correspond with existing research on the musical development of the Beatles, which does not describe a general complexity trend, but rather identifies different phases (of different composers) and individual albums with increased complexity (Everett, 2001). This observation is also illustrated by the following graph (cf. Fig. 3), which does not show a clear trend in the development of complexity levels, but rather goes up and down over time.
                
                    
                    Development of tonal complexity levels
                        
                            
                                 The graphs for chord complexity levels largely correspond to the tonal complexity levels. The detailed graphs are available via the online visualization tool.
                            
                         (level 1 = low complexity, level 4 = high complexity) for the Beatles’ albums over time.
                    
                
                We can also show that a general complexity trend for albums is problematic, as even just a few outlier songs can substantially influence the overall complexity score of an album. This can be best illustrated for the album “A Hard Day’s Night”. While Fig. 1 suggests that tones on the complexity level 4 for the whole album increased as compared to the previous (and also the successive) albums, a closer look (cf. Fig. 4) at the individual songs shows that in fact only two of the 13 songs show high complexity on level 4 (“And I Love Her” = 31%; “When I Get Home” = 42%). 
                
                    
                    Overview of the distribution of tonal complexity levels (level 1 = low complexity, level 4 = high complexity) for all songs of the Beatles’ album “A Hard Day’s Night” (1964).
                
                These observations reflect the initial notion that the work of the Beatles is extremely heterogeneous, which is also a result of the band’s different composers and their individual musical development. When we look at musical complexity from the perspective of individual composers (cf. Fig. 5), Paul McCartney seems to be the most stable composer, i.e. his complexity curves largely correspond to the overall complexity curves of the Beatles albums (cf. Fig. 3). George Harrison and John Lennon each have several albums where they contribute songs with higher complexity.
                
                    
                    Overview of the distribution of tonal complexity levels (level 1 = low complexity, level 4 = high complexity) for the main composers of the Beatles. Note that George Harrison was not involved as a composer for the albums “Please please me”, “A Hard Day’s Night” and “Beatles for Sale” and overall only composed 24 songs.
                
                The absence of a general trend or temporal pattern in musical complexity lead us to present the results of our computational approach in a rather exploratory interface that can be used to look at complexity from different scales. The visualization tool is available online
                    
                        
                            Visualization tool: https://fuchsflo90.github.io/beatles-analysis/#
                        
                     and can be used to explore complexity for both tones and chords, for any album of the Beatles filtered by the main composer. In addition to the complexity scores, it also provides information on the frequencies of different scales and rhythms.
                
            
            
                Conclusion
                In this paper we showcased the application of a computational approach to measure musical complexity in a corpus of user-generated transcriptions of Beatles songs. We were able to demonstrate that musical complexity did not consistently increase over time (only a weak correlation was measured) and that high complexity seems to be a situational phenomenon that can occur for single songs rather than for a whole album. The approach presented in this paper can be considered as a case study for further computational studies on musical complexity, thus adding to the branch of computational musicology as part of the Digital Humanities. As next steps we plan to extend the approach to comparative complexity analyses for different bands and genres.
            
        
        
            
                
                    Bibliography
                    
                        Cohen, J.
                         (1988). Statistical power analysis for the behavioral sciences. Lawrence Erlbaum Associates Publishers.
                    
                    
                        Cuddy, L. L., Cohen, A. J., &amp; Mewhort, D. J. K.
                         (1981). Perception of structure in short melodic sequences. Journal of Experimental Psychology: Human Perception and Performance, 7(4), 869–883.
                    
                    
                        Di Giorgi, B., Dixon, S., Zanoni, M. &amp; Sarti, A.
                         (2017). A data
                        ‐
                        driven model of tonal chord sequence complexity. IEEE/ACM Transactions on Audio Speech and Language Processing.
                    
                    
                        Eerola, T., North, A. C., &amp; Le, L. 
                        (2000). Expectancy
                        ‐
                        Based Model of Melodic Complexity. Proceedings of the Sixth International Conference on Music Perception and Cognition, (January), 1–7.
                    
                    
                        Everett, W.
                         (2001). The Beatles as Musicians: The Quarry Men through Rubber Soul. New York: Oxford University Press.
                    
                    
                        Krumhansl, C. L., &amp; Shepard, R. N.
                         (1979). Quantification of the hierarchy of tonal functions within a diatonic context. Journal of Experimental Psychology. Human Perception and Performance, 5(4), 579–594.
                    
                    
                        MacDonald, I. 
                        (1995). Revolution In The Head: The Beatles’ Records and the Sixties. Pimlico.
                    
                    
                        Madsen, S., T. Widmer, G., &amp; Kepler, J.
                         (2007). Key
                        ‐
                        Finding with Interval Profiles. ICMC.
                    
                    
                        Martin, G., &amp; Hornsby, J.
                         (1979). All you need is ears. New York: St. Martin’s.
                    
                    
                        Mason, D.
                         (2012). The Beatles Genome Project : Cluster Analysis of Popular Music in R. Book of Contributed Abstracts. The R User Conference, useR! 2013, (2010), 2012.
                    
                    
                        Pearson, K.
                         (1895). Contributions to the Mathematical Theory of Evolution. III. Regres
                        ‐
                        sion, Heredity, and Panmixia. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 186(0), 254–317.
                    
                
            
        
    
9817	2019	
        
            Urban exploration or urbex is the exploration of human-made spaces that are generally inaccessible and hidden away from the general public. Recording the visit of these ‘forgotten’ spaces through photography is a main component of this phenomenon which has resulted in a wealth of urban exploration photos and videos of abandoned sites. 
            Urbex destinations are located worldwide and include a wide range of abandoned sites. Belgium has been a very popular destination for urban explorers and Château de Noisy, a neo-gothic castle in Belgium dating back to the 19
                th century was a very famous destination which was demolished in 2017. There is a rich collection of urbex materials on this building which urban explorers have shared through various online platforms, such as personal websites, Facebook, YouTube, and Flickr. The latter has become a significant repository of urban exploration photographs. 
            
            Regardless of the social and political complexities of this phenomenon, urban exploration is intertwined with abandoned historic sites and in recent years the potential of urban exploration for preservation of heritage has been brought to the attention of academia. Considering that urban exploration is becoming increasingly popular, the importance and possible contribution of this activity and its records for research on abandoned heritage sites cannot be neglected.
            This research focuses on the documentation and information management of abandoned heritage sites and looks into the potentials of the rich collection of existing digital urbex resources for their preservation by exploring their content and new means of representation and engagement. The unique value of such iconographic data can be attributed to the fact that normally these abandoned sites are inaccessible to the general public. Hence these photos and videos can shed light on these unknown places, and with the right utilization can not only document and digitally preserve some aspects of the valuable heritage but also can bring public attention to heritage sites that may still be saved from deterioration and revived. 
            To explore the potentials of urbex produced materials for heritage preservation, concentrating on the rich collection of urbex data of numerous abandoned sites, this research aims to gain insights into the urbex scene and its evolution. Moreover, focusing on Château de Noisy, considering that the prevalent methods of documentation of a historic site which require physical access and presence are not applicable, it aims to explore the potential of ‘distant documentation’ by investigating the application of existing tools and software to create a new approach for the preservation of abandoned and even demolished heritage sites and their story. To reach these objectives, focusing on Flickr and using the Flickr API service, two Flickr Dataset are collected: One of general photos related to urban exploration on Flickr (from 2000 to 2017) and another which includes the specific photos of Château de Noisy on Flickr. To collect, prepare, visualize, analyse, create and present the data for this study multiple tools and methods are employed: Python Scripting Language (collection and preparation), Tableau Desktop (visualization and analysis), Voyant (textual analysis), ContextCapture (creation/reconstruction) and WebStorm (presentation via the creation of a website). 
            Terminology of the urbex Flickr photo titles and visualizing the distribution of the urbex Flickr photos, led to interesting insights into the urbex scene. Furthermore, the collected and downloaded images of Château de Noisy from Flickr offer insights into this abandoned building carrying information on diverse aspects such as its function, materials (and pathology), structure and context over the course of many years. 
            Château de Noisy was demolished without being given the chance for detailed documentation through advanced 
                in situ techniques. Using the ContextCapture software and a selection of the images and videos of the castle that were identified through retrieving the images of the Château de Noisy Flickr Dataset, a 3D mesh model of the building and its immediate context is created. This scalable 3D reconstructed model can allow a flexible interactive experience of the site and can be used to curate and create an immersive experience of the exterior of the building and its immediate context while providing additional heritage information. A digital reconstruction of the building and subsequent narration that builds upon this vessel can create an engaging and immersive experience for the public and digitally preserve the ‘fairytale castle’ building that once stood in Celles. The experience of such distant documentation of Château de Noisy can also be implemented in other heritage sites which are demolished or inaccessible. For buildings that still exist, raising awareness of its current state and heritage values can lead to their potential preservation and revival.
            
            
                
            
            Figure 1. Image extract from the reconstructed 3D model of Château de Noisy
        
        
            
                
                    Bibliography
                    
                        Arboleda, P. (2016). Heritage views through urban exploration: The case of ‘Abandoned Berlin’. 
                        International Journal of Heritage Studies, 22(5): 368-381.
                    
                    
                        Bennett, L. (2013). Who goes there? Accounting for gender in the urge to explore abandoned military bunkers. 
                        Gender, Place &amp; Culture, 20(5): 630-646.
                    
                    
                        De Liedekerke Beaufort, C. (n.d.). 
                        Le Château de Noisy. [Historic 16 page text on Château de Noisy available at Augustijns Historisch Instit. in Heverlee, Belgium]
                    
                    
                        DeSilvey, C. (2006). Observed decay: Telling stories with mutable things. 
                        Journal of Material Culture, 11(3): 318-338.
                    
                    
                        Garrett, B. (2010). Urban explorers: Quests for myth, mystery, and meaning. 
                        Geography Compass, 4(10): 1448-1461.
                    
                    
                        Garrett, B. (2013). 
                        Explore Everything: Place-Hacking the City. London: Verso Books.
                    
                    
                        Garrett, B. (2014). Undertaking recreational trespass: urban exploration and infiltration. 
                        Transactions of the Institute of British Geographers. 39(1): 1-13.
                    
                    
                        Kindynis, T. (2017). Urban exploration: From subterranea to spectacle. 
                        British Journal of Criminology, 57(4): 982-1001.
                    
                    
                        Mott, C. and Roberts, S.M. (2014). Not everyone has (the) balls: Urban exploration and the persistence of masculinist geography. 
                        Antipode, 46(1): 229-245.
                    
                    
                        Ninjalicious [Chapman, J.] (2005). 
                        Access All Areas: A User’s Guide to the Art of Urban Exploration. Toronto: Infilpress.
                    
                    
                        Novel, C., Kervien, R., Graindorge, P. and Poux, F. (2016). Comparing aerial photogrammetry and 3d laser scanning methods for creating 3d models of complex objects [White paper], https://www.bentley.com/en/products/brands/contextcapture
                    
                    
                        Paiva, T. (2008). 
                        Night Vision: The Art of Urban Exploration. San Francisco: Chronicle Books.
                    
                    
                        Palombini, A. (2017). Storytelling and telling history. Towards a grammar of narratives for Cultural Heritage dissemination in the Digital Era. 
                        Journal of Cultural Heritage, 24: 134-139.
                    
                    
                        Pinder, D. (2005). Arts of urban exploration. 
                        Cultural Geographies, 12(4): 383-411.
                    
                    
                        Sansivero, B. (2015). Belgium's Abandoned Fairytale Castle. 
                        Atlas Obscura, https://www.atlasobscura.com/articles/belgiums-abandoned-fairytale-castle 
                    
                    
                        Spyrou, E. and Mylonas, P. (2016). A survey on Flickr multimedia research challenges. 
                        Engineering Applications of Artificial Intelligence, 51(C): 71-91.
                    
                    
                        Stones, S. (2016). The value of heritage: Urban exploration and
                        the historic environment. 
                        The Historic Environment: Policy &amp; Practice, 7(4): 301-320.
                    
                
            
        
    
9824	2019	
        
            
                Introduction
                Although there have been some infrastructural developments of late, the main 
                    modus operandi in digital literary studies is still to apply a certain research method to an ephemeral corpus. In a best-case scenario, the results are 
                    somehow reproducible, in a worst-case scenario they are not reproducible at all. At best, there is an openly accessible corpus in a standard format such as TEI, another markup language, or at least TXT. At worst, the corpus is not even accessible, i.e., the research results cannot be questioned.
                
                However, there are signs that this is slowly changing. Some projects provide interfaces that allow for multiple ways of access to corpora. One of these projects is DraCor, an open platform for research on (European) drama, which will be introduced in this paper (accessible at 
                     or via its GitHub repositories or its API). DraCor transforms existing text collections into 'Programmable Corpora' – a new term we bring into play with this talk.
                
            
            
                Building Blocks
                
                    Vanilla Corpora
                    Similar to the COST Action on European novels (Schöch et al. 2018), the DraCor project seeks to establish a bundle of multilingual drama corpora encoded in basic TEI as basis for digital comparative studies. To date, the platform enables access to a Russian-language (
                        ) and a German-language corpus of plays (
                        ). Similar to Paul Fièvre's collection "Théâtre classique", these corpora are designed as vanilla corpora, which initially contain hardly any special markup beyond the necessary, but are freely available and can therefore be forked, enriched and expanded. To demonstrate that other corpora can be easily linked to the platform, we forked the Shakespeare Folger Corpus and the Swedish Dramawebben corpus and connected it, and all existing extraction and visualisation methods of the platform are readily applicable to the newly added corpora (
                         and 
                        ). Other corpora of dramatic texts are to follow; the only prerequisite is that they are encoded in TEI.
                    
                    The advantages of a freely available corpus hosted on GitHub are obvious. Not only can the corpus be cloned and loaded directly into an XML database like eXist-db. Using the SVN wrapper from GitHub, the entire corpus can also be downloaded directly, in its current state and without version history if this is not needed:
                    svn export https://github.com/dracor-org/rusdracor/trunk/tei
                    An openly accessible GitHub repository also means that pull requests for error correction are possible and welcome.
                
                
                    XML Database (eXist-db) and Frontend
                    DraCor relies on eXist as XML database to process TEI files and to provide functions for researching the corpora. The frontend is built with React (
                        ), it is responsive and easily extensible. However, the focus is not on the GUI, but on the API (on the general differences between these two approaches to interfaces cf. Bleier/Klug 2018).
                    
                
                
                    API
                    To come close to the ideal and the possibility of applying "all methods to all texts" in a simple manner (Frank/Ivanovic 2018), it takes more than open corpora. The article by Frank/Ivanovic advocates SPARQL endpoints (for which there is also a readily-available app for eXist-db: 
                        ).
                    
                    DraCoroffers such endpoint, but also features a rich general API documented and explained via Swagger (
                        ). In a subarea of corpus philology, the digital scholarly editions, discussions about more proactive use of APIs have already begun (for background information cf. Bleier/Klug 2018), the Folger Digital Texts API may serve as an example (
                        ). The advantage of a more modern solution like Swagger is that API queries can be executed live and directly and that the output can be controlled more precisely.
                    
                    A simple use-case scenario would look like this: using RStudio you can throw a quick glance into a corpus with just a few lines of code, maybe regarding the development of the number of characters in Russian drama between 1740 and 1940, stored in the metadata table (
                        ). This table, which can be obtained in JSON or CSV format, is read into a Data.Table, whereupon the values of two columns (year of publication and number of speakers) can simply be visualised via ggplot (Fig. 1).
                    
                    
                        
                            
                            Figure 1: Number of characters per play in chronological order (source: RusDraCor).
                        This very simple example is able to show the starting point of a decisive structural diversification of the Russian drama landscape. Pushkin's historical drama "Boris Godunov" (1825), result of his reading of Shakespeare, features speech acts of 79 characters, a number previously unthinkable in Russia drama.
                    
                    However, the possibilities are not limited to using ready-made API functions. New research ideas always create new needs for easily obtainable and reproducible data and metrics; the API can be extended accordingly, i.e., new research ideas can be implemented centrally in the API layer. This is made even easier by the fact that Apache Ant can be used to rebuild the entire development environment on your own system.
                    In addition to structural data and metadata, full texts without markup can also be obtained, e.g., if methods such as stylometry or topic modelling are the purpose, i.e., methods that need a "bag of words" and do not require markup.
                    All in all, the structure and documentation of open APIs makes it much easier to reproduce research results, which up to now has often been a time-consuming (or impossible) process.
                
                
                    Shiny App
                    An example of the versatility of the DraCor API is the Shiny App created by Ivan Pozdniakov (
                        ). Shiny is a framework based on R, which makes it possible to display interactive visualisations in the browser. The DraCor Shiny App does just that, relying entirely on the DraCor API for data retrieval. Thus, visualisations of the current database can be used for teaching and research purposes, but also for easier data correction.
                    
                
                
                    Didactics
                    The formalisation of literary texts, for example via markup, is not self-explanatory. Although the community can rely on some standards, every operationalisation depends on the actual research question. To give an example: if you would like to extract network data based on character interactions in a literary text, you would have dozens of different ways of doing this (e.g., Grayson et al. 2016 test different extraction methods for novels and compare the results). This also applies to plays. In order to sharpen the senses for this in teaching, we developed the tool "ezlinavis" (
                        ) and integrated it into the DraCor toolchain. Network data can be extracted from literary texts manually, also to raise the awareness for the contingency of this process, an important preliminary step to the eventual step of operationalisation.
                    
                    In addition to an approach to the gamification of the process of correcting TEI-encoded corpora (Göbel/Meiners 2016), we also developed a card game for teaching purposes in order to playfully train the understanding of network values (Fischer at al. 2018).
                    These didactic tools wrapped around the platform are an integral part of the whole project as they are based on the project data and operationalisations. While building the platform, it was important to recognise that data can take several forms and be equally important for research and teaching.
                
                
                    Linked Open Data
                    The TEI files contain GND [Integrated Authority File of the German National Library] and Wikidata identifiers for both authors and works. In this way, various data and facts that lie beyond one's own corpus work can be included. Something like an automatically created gallery of authors has a more illustrative character (de la Iglesia/Fischer 2016). But using the same identifiers, we can also determine if a corpus has a regional bias. Via Wikidata, we can easily display the distribution of the authors' places of birth and death on a map (by doing so, we could rule out that our German-language corpus GerDraCor has a regional bias, cf. Göbel/Fischer 2015).
                    Similarly, the Wikidata ID of the plays can be used to find out where they were first performed (example query: 
                        ), i.e., aspects of the performance history can be switched on, even though they are not the focus of the core project and based on data curated elsewhere.
                    
                
                
                    Infrastructure Instead of Rapid Prototyping
                    Projects like DraCor seek to provide the digital literary studies with a reliable and extensible infrastructure so that the research community can focus on research questions.
                    An important conclusion for us was that we would give up the further development of our all-in-one Python script collection "dramavis" (Kittel/Fischer 2014–2018 and Fischer et al. 2017), which we have been developing for four years now. From here on, we would rather devote our time to the API. "Dramavis" followed the idea of rapid prototyping and had to do all by itself, including the preprocessing of data (Trilcke/Fischer 2018), which is not untypical in the Digital Humanities. The code base has grown quite a bit in the meantime and its maintenance has become difficult and often enough led away from actual research questions.
                
            
            
                Outlook
                In allusion to the project "ProgrammableWeb" – which maintains a database of open APIs and whose slogan is: "APIs, Mashups and the Web as Platform" (accessible at 
                    ) – we propose the term 'Programmable Corpora' for research-oriented corpora providing an API.
                
                Programmable Corpora facilitate the implemention of research questions around corpora. It is to be expected that infrastructural efforts of this kind will pay off for the entire community with effects such as those listed by John Womersley in his presentation at the ICRI2018 conference in Vienna: a) dramatically increase scientific reach; b) address research questions of long duration requiring pooled effort; c) promote collaboration, interdisciplinarity, interaction.
                The are numerous ways to connect to Programmable Corpora, no matter if you don't want to code at all and only need a CSV file for Excel or LibreOffice Calc or a GEXF file for Gephi, if you want to research a corpus via its connections to the Linked Open Data cloud or just want to get specific data for your R or Python script without having to worry about the corpus and its maintenance and reproducibility (all this remains an option, though). Programmable Corpora make it easier to decide on which level of the platform your own research process starts.
            
        
        
            
                
                    Bibliography
                    
                        Bleier, Roman; Klug, Helmut W. (2018): Discussing Interfaces in Digital Scholarly Editing. In: 
                        Digital Scholarly Editions as Interfaces. BoD, Norderstedt, pp. V–XV. URL: 
                        
                    
                    
                        de la Iglesia, Martin; Fischer, Frank (2016): 
                        The Facebook of German Playwrights. URL: 
                        
                    
                    
                        Fischer, Frank; Dazord, Gilles; Göbel, Mathias; Kittel, Christopher; Trilcke, Peer (2017): Le drame comme réseau de relations. Une application de l‘analyse automatisée pour l’histoire littéraire du théâtre. In: 
                        Revue d'historiographie du théâtre. № 4 (2017). URL: 
                        
                    
                    
                        Fischer, Frank; Kittel, Christopher; Milling, Carsten; Schultz, Anika; Trilcke, Peer; Wolf, Jana (2018): Dramenquartett – Eine didaktische Intervention. In: 
                        Conference proceedings of 
                        DHd2018. University of Cologne, pp. 397–398. DOI: 
                        
                    
                    
                        Göbel, Mathias; Fischer, Frank (2015): 
                        The Birth and Death of German Playwrights. URL: 
                        
                    
                    
                        Göbel, Mathias; Meiners, Hanna-Lena (2016): Play(s): Crowdbasierte Anreicherung eines literarischen Volltext-Korpus. In: 
                        Conference proceedings o
                        f
                         
                        DHd2016. University of Leipzig, pp. 140–143. URL: 
                        http://www.dhd2016.de/abstracts/vortr%C3%A4ge-007.html
                    
                    
                        Grayson, Siobhán; Wade, Karen; Meaney, Gerardine; Greene, Derek (2016): The Sense and Sensibility of Different Sliding Windows in Constructing Co-Occurrence Networks from Literature. In: 
                        2nd IFIP International Workshop on Computational History and Data-Driven Humanities. Trinity College Dublin 2016. PDF: 
                        
                    
                    
                        Kittel; Christopher; Fischer, Frank (2014–2018): 
                        dramavis. Python script collection. Repository: 
                        
                    
                    
                        Schöch, Christoph et al. (2018): Distant Reading for European Literary History. A COST Action [Poster]. In: 
                        DH2018: Book of Abstracts / Libro de resúmenes. Mexico: Red de Humanidades Digitales A. C. URL: 
                        
                    
                    
                        Trilcke, Peer; Fischer, Frank (2018): Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen. In: Martin Huber and Sybille Krämer (eds.): 
                        Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden. (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3). DOI: 
                        
                    
                
            
        
    
9826	2019	
        
            
                Introduction
                Over the last years various corpus projects have started all over the world documenting sign languages. The purposes of such corpora focus primarily on the linguistic study of the languages as well as the preservation of the languages themselves.
                As Drew and Ney deduct (Dreuw and Ney, 2008), the processing and storing phase of these corpora require a textual representation of the signs. Although different notation systems have been created over the years, gloss notation seems the prevalent one. Instead of using an annotation system with components representing the main formal components of a sign, ID glosses are typically used. These consist of a uniquely identifying spoken language word (written in capitals) that by definition refers to a particular sign form.
                During the annotation process the researcher has to determine the precise time a sign occurs and properly identify and gloss it. As a result, the annotation process is extremely labor intensive, but it is a condition for a reliable quantitative analysis of the sign language corpora.
                The focus of this project is the development of a tool for automatic annotation of sign occurrences in video corpora as a first step towards fully automatic annotation. This study presents a new approach to automatic annotation for sign languages using as little data for training as possible and taking advantage of a state-of-the-art pose estimation framework for a robust and unbiased tracking.
            
            
                Literature review
                Recent developments in the field of sign language recognition depict the advantages of machine and deep learning for tasks related to recognition and classification (Agha et al., 2018; Pigou et al., 2015; Masood et al., 2018). However, they require a vast amount of data to be trained and they are bounded in the sign language they have been trained on (hard to generalize in other sign languages)
                Additionally, approaches in automatic annotation for sign languages require manual annotation of the hands and body joints prior to the training process of the recognizer models (Pfister et al., 2015; Aitpayev et al., 2016). Furthermore, most studies apply skin color and motion detection algorithms (Kumar, 2017) that are prone to errors and possibly skin color bias. It is also often the case that in order to assist the hand tracking model, corpora are compiled using colored gloves for the subjects (Masood et al., 2018) or captured using Kinect (Pigou et al., 2015) making the result of such studies unusable in real-life conditions in the corpora.
                Pose estimation, as a technique to detect human figures in images and video, showed enormous improvement over the last years. OpenPose (Cao et al., 2017) is the state-of-the-art framework when it comes to accurately detect human body and hands keypoints. The model takes as input a color image or video and through a 2-branch multi-stage Convolutional Neural Network predicts the 2D locations of keypoints for each person in the image. This framework was chosen to be used in this study as it has been trained on the Multi-Person (MPI) and COCO datasets making it exceptionally robust and fast.
            
            
                Methodology
                A data-set of 7805 frames in total (approximately 4 minutes of videos) has been compiled and labeled as signing or not signing. The dimensions of the frames were 352 by 288 pixels and were extracted from the Adamorobe and Berbey sign language corpora (Nyst, 2007; Nyst et al., 2012). These corpora portray an additional challenge as they are extremely noisy and low quality. Furthermore, they contain signing from one and two people at the same time. The original data-set was split into a training and testing set of 6150 and 1655 frames respectively. Using OpenPose, the positions of the hands, elbows, shoulders and head were extracted from each frame. The positions of the rest of the body joints were disregarded as most of the time they were out of the frame bounds.
                It is important to compare the performance of multiple different machine learning algorithms consistently. Thus, four different classification methods were used and optimized, namely: Support Vector Machines, Random Forests, Artificial Neural Networks and XGBoost. The majority of these algorithms have been extensively used in machine learning studies as well as in sign language applications (Agha et al., 2018). Performance was measured using the metric of Area Under the Receiver Operating Characteristics (AUROC).
            
            
                Results
                All classifiers performed adequately well. However, the best AUC score was found in XGBoost (0.92). Figure 1 presents the AUROC curve after a 10-fold cross-validation. The Artificial Neural Network was found to perform sufficiently well (AUC: 0.88). While the performance of the model is satisfactory, it is important to explore the features that contribute to the classification task. Figure 2 shows the importance of each feature as measured by the classifier. The result is reasonable as the position of the dominant hand (i.e. right) has the highest importance on how the classifier weights the features.
                To account for multiple people signing in one frame, an extra module was added. The module creates bounding boxes around each person recognized by OpenPose, normalizes the positions of the body joints and runs the classifier. This process makes it possible to classify sign occurrences for multiple people in a frame irrespective of their positions (figure 3).
                Once all the frames have been classified, the "cleaning up" and annotation phase starts. A sign occurrence is annotated only if at least 12 consecutive frames have been classified as "signing" frames. This way I account for the false positive errors. This sets the stage for the annotation step. By using the PyMpi python library (Lubbers and Torreira, 2013) the classifications are translated into annotations that can be imported directly to ELAN. Figure 4 shows the result of the overall output.
                
                    
                    AUROC curve of XGBoost after a 10-fold cross-validation.
                
                
                    
                    The importance of each feature based on XGBoost classifier.
                
                
                    
                    Recognition module in multiple people.
                
                
                    
                    Final output of the tool as seen in ELAN.
                
            
            
                Conclusion
                This is the first step towards fully automated sign language annotation. The results show that a frame-to-frame classification using XGBoost is a promising tool for the annotation of sign occurrences in a video. The significance of this study lies on the fact that the tool created can be easily adjusted and used in any kind of sign language corpus regardless of its quality, the sign language presented or the number of people in the video. Furthermore, one needs approximately only 4 minutes of annotated video in order to retrain the model making the process as easy as possible. Finally, the tool has the potential to be extended and used in gestural corpora as well.
            
        
        
            
                
                    Bibliography
                    
                        Agha, R. A., Sefer, M. N. and Fattah, P. (2018). A Comprehensive Study on Sign Languages Recognition Systems Using (SVM, KNN, CNN and ANN). 
                        Proceedings of the First International Conference on Data Science, E-Learning and Information Systems. (DATA ’18). New York: ACM, pp. 28:1–28:6.
                    
                    
                        Aitpayev, K., Islam, S. and Imashev, A. (2016). Semi-automatic Annotation Tool for Sign Languages. 
                        2016 IEEE 10th International Conference on Application of Information and Communication Technologies (AICT). Baku, Azerbaijan: IEEE, pp. 1–4.
                    
                    
                        Cao, Z., Simon, T., Wei, S. E. and Sheikh, Y. (2017). Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields. 
                        2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Honolulu, HI: IEEE, pp. 1302–10.
                    
                    
                        Dreuw, P. and Ney, H. (2008). Towards Automatic Sign Language Annotation for the Elan Tool. 
                        LREC 3rd Workshop on the Representation and Processing of Sign Languages: Construction and Exploitation of Sign Language Corpora. Marrakech, Morocco, pp. 50-53
                    
                    
                        Kumar, N. (2017). Motion Trajectory Based Human Face and Hands Tracking for Sign Language Recognition. 
                        2017 4th IEEE Uttar Pradesh Section International Conference on Electrical, Computer and Electronics (UPCON). Mathura: IEEE, pp. 211–16.
                    
                    
                        Lubbers, M. and Torreira, F. (2013). 
                        A Python Module for Processing ELAN and Praat Annotation Files: Dopefishh/Pympi. https://github.com/dopefishh/pympi.
                    
                    
                        Masood, S., Srivastava, A., Thuwal, H. C. and Ahmad, M. (2018). Real-Time Sign Language Gesture (Word) Recognition from Video Sequences Using CNN and RNN. In Bhateja, V., Coello Coello, C. A., Satapathy, S. C. and Pattnaik, P. K. (eds), 
                        Intelligent Engineering Informatics, vol. 695. Singapore: Springer Singapore, pp. 623–32.
                    
                    
                        Nyst, V. A. S. (2007). 
                        A Descriptive Analysis of Adamorobe Sign Language (Ghana). (LOT 151). Utrecht: LOT.
                    
                    
                        Nyst, V. A. S., Magassouba, M. M. and Sylla, K. (2012). Un Corpus de Reference de la Langue des Signes Malienne II. A Digital, Annotated Video Corpus of Local Sign Language Use in the Dogon Area of Mali. Leiden University Centre for Linguistics, Universiteit Leiden.
                    
                    
                        Pfister, T., Simonyan, K., Charles, J. and Zisserman, A. (2015). Deep Convolutional Neural Networks for Efficient Pose Estimation in Gesture Videos. In Cremers, D., Reid, I., Saito, H. and Yang, M. H. (eds), 
                        Computer Vision - ACCV 2014, vol. 9003. Cham: Springer International Publishing, pp. 538–52.
                    
                    
                        Pigou, L., Dieleman, S., Kindermans, P. J. and Schrauwen, B. (2015). Sign Language Recognition Using Convolutional Neural Networks. In Agapito, L., Bronstein, M. M. and Rother, C. (eds), 
                        Computer Vision - ECCV 2014 Workshops. Springer International Publishing, pp. 572–78.
                    
                
            
        
    
9841	2019	
        
            
                Introduction
                Libraries are sources of large-scale data: both in terms of their collections and the information they collate on their spaces, users, and systems. These data provide opportunities to explore technical, methodological, and ethical questions from the valuable interdisciplinary perspective of Data Science and the Digital Humanities. In light of this, we will explore our analysis of library datasets using 
                    Subjectify
                    
                         The code and documentation for 
                            Subjectify is available on Github at 
                            https://github.com/mbennett-uoe/librarytools. 
                        
                    , an automatic classification matching tool developed to assist analysis of UK Non-Print Legal Deposit (NPLD) collections. NPLD regulations were introduced to the UK in 2013 to support legal deposit libraries to collect electronic publications 
                    (2013). Access restrictions mean that readers may only use these materials on fixed terminals within the physical walls of the six legal deposit libraries 
                    (see British Library, 2014 for details). The resultant web logs are therefore unambiguous sources of NPLD collection usage within UK legal deposit libraries.
                
                 Our study is part of an established tradition of user studies in the digital humanities. To date, these have focused on user behaviour with digital resources 
                    (Warwick et al., 2008; Ross and Terras, 2011; Sinn and Soares, 2014). Web log analysis has been used successfully in this context for over twenty years 
                    (Almind and Ingwersen, 1997; Nicholas et al., 2005; Gooding, 2016). These studies adopt methodological approaches and topics of study that contribute directly to our understanding of information sources in the digital humanities. However, there have been fewer studies that address critical humanistic perspectives to inform approaches to the data itself. This paper addresses that gap by describing our research into the users of NPLD materials in the United Kingdom, and the implications of automatic classification matching for library dataset analysis. It will address the following questions: what insights into users of digital library collections can be derived from automatic classification matching? What limitations are introduced by the use of existing classification schemes? And, in light of ongoing debates on responsible data curation in DH 
                    (Weingart, 2014; Brown et al., 2016), how might DH and LIS scholars collaborate to inform ethical analysis of large-scale library datasets?
                
            
            
                Methodology
                Our analysis follows Bates’ observation that scholarly communication practices function differently across domains, and that “these many differences 
                    do make a difference” 
                    3.3.CO;2-M","author":[{"family":"Bates","given":"Marcia"}],"issued":{"date-parts":[["1998",11]]},"accessed":{"date-parts":[["2011",3,23]]}},"locator":"1200","suppress-author":true}],"schema":"https://github.com/citation-style-language/schema/raw/master/csl-citation.json"}?&gt;(1998: 1200) to information access and use: as such, we should be able to identify differences in behaviour by studying the subjects requested by users. To this end we were provided with two datasets comprising title-level NPLD access logs from the reading rooms of the six UK legal deposit libraries
                    
                         The six libraries are the British Library, National Library of Scotland, National Library of Wales, Bodleian Libraries, Cambridge University Library, and Trinity College Dublin Library.
                    . The anonymous logs contained only bibliographic records of NPLD materials accessed by users, excluding both identifiable information about users and interactions with discovery systems and other materials. The first dataset comprised metadata for all eBook requests (total 91,809) from 31
                    st July 2015 to 31
                    st March 2017. The second dataset comprised metadata for all eJournal article requests (total 36,506) over the same period. Each dataset contained the following metadata: date and time of access request; originating legal deposit library; title of book or article; journal title (where applicable); publisher; and ISBN or ISSN. Each dataset was provided as a CSV file, then cleaned by the research team in OpenRefine to address metadata errors.
                
                 Although our dataset contained no identifiable information about users, it may still be possible to infer information about users from the works they consult. We therefore decided to abstract our data and undertake a macroanalysis of user behaviour. To achieve this, we created a small Python-based tool called 
                    Subjectify. This tool uses the OCLC Classify2 API service
                    
                        
                            http://classify.oclc.org/classify2/api_docs/index.html
                        
                     to automatically obtain Dewey Decimal (DDC) and Library of Congress (LCC) classmarks from CSV files using key data fields such as title, author, and ISBN. It additionally provides for different options to locate relevant fields to allow input from different data sources. 
                    Subjectify found a matching classmark for 76.42% for eBooks, and 55.53% for eJournals. This was partly due to missing key data fields in records for eJournals, and partly because many records did not have a corresponding classmark: time-consuming manual classification samples via OCLC Classify2 achieved only slightly higher accuracy rates. We discarded unclassified records used the remaining records to represent patterns of usage by DDC subject. Due to the large number of repeat requests due to system timeouts, we split the remaining records into unique titles (each title counted once regardless of number of requests), and total results (including repeat requests). Our results show that findings were not unduly influenced by repeat requests. 
                
            
            
                Findings
                The following charts show the most commonly accessed subject, by DDC, for titles viewed by users of eBook and eJournal materials. We found that usage by DDC differs distinctly from the spread of classmarks across, for instance, the BL’s entire collections
                    
                         The sub-subroutine blog has produced a fascinating visualisation of the BL’s collections, which is worthy of comparison 
                            (sub-subroutine, 2015).
                        
                    :
                
                
                    
                
                
                    
                
                Social Sciences texts were notably among the most commonly accessed titles for both formats. The most common subject for eJournals was Technology, whereas for eBooks both Social Sciences and Literature subjects were more frequently accessed. Our findings reflect differing information behaviour across domains: books, for instance, remain a vital source for the Arts and Humanities 
                    (Stone, 1982; Palmer and Cragin, 2008) whereas technology and science subjects rely on journals 
                    (Talja and Maula, 2003), which tend to provide faster publication of new research. Indeed, Stone’s flagship early study noted that “retrospective coverage may be more important to the humanists than having access to current material” 
                    (1982: 296). The Social Sciences, on the other hand, are shown by our findings to be more hybrid in their reading patterns. 
                
                 Analysing individual subject categories can derive a better sense of the difference between institutions. The following charts show usage of books in the DDC600-699 classmark (Technology), for the Bodleian Libraries, British Library and Cambridge University Library.
                
                    
                
                
                    
                    
                
                The British Library receives proportionally fewer requests for NPLD medical and health materials. Our interviews with staff at the Bodleian and Cambridge University libraries showed that local medical school staff were a key user group, so we can trace a direct correlation between the user communities of these libraries, and the subjects of the NPLD material used. 
                 Finally, the following table demonstrates usage of 800-899 (Literature) sources to underline a key problem with DDC for automatic classification:
                Library classification is a subjective process undertaken by humans within the biased frameworks provided by existing classification schemes 
                    (Mai, 2010). Here, for instance, DDC provides distinct categories for English, American, and classical European schools of literature, while lumping the rest of the world under “other literatures” (800-899). This bias emerges from the nineteenth century North American perspective embedded within DDC 
                    (Kua, 2008). By relying on automatic matching, we inevitably embed problematic perspectives into our data: while our case study uses UK legal deposit collections, which comprise works represented strongly by DDC, the applicability of this method for library datasets in other parts of the world is questionable. Each time we zoom in with the macroscope, the bias of our chosen classification scheme becomes increasingly evident in the resultant data structures – yet in order to report on literature from without the established canon, we have to do precisely this. The use of established classification schemes is therefore both a methodological and epistemological problem, and future work will be needed to refine our approach. 
                
            
            
                Conclusion
                Our results demonstrate that 
                    Subjectify was successful in allowing us to analyse user behaviour at scale. It contributes to macroscopic analysis of library data in two ways: first, it allows us to report on bibliographic library users while maintaining privacy through data abstraction; and second, this abstraction allows us to identify patterns of user behaviour with NPLD materials. We believe this approach would work for subject-based analysis of similar collections of bibliographic data, and that it does so in a way that closely reflects how collections are represented in libraries. However, we are also aware that automated classification introduces the biases of those classification schemes into our own data 
                    (Adler, 2017). This is an unfortunate side effect of the growing scale of library data. Weingart (2014) notes that the role of the humanities is to tie the very distant to the very close, in order to become ethical stewards of our data. It is therefore essential, when viewing library datasets from a humanistic perspective, to consider the ethics of data representation in our own work. Our priority for future work is to consider how a fruitful conversation between DH and Information Science might develop more nuanced approaches to representing 
                    (in the sense meant by Unsworth, 2000) of library data. This should include further consideration of the consequences of how bias in library classification schemes affects microanalytic approaches to bibliographic datasets.
                
            
        
        
            
                
                    Bibliography
                    
                         (2017). 
                        Adler, M.
                        Cruising the Library: Perversities in the Organization of Knowledge. New York: Fordham University Press.
                    
                    
                        Almind, T. C. and Ingwersen, P. (1997). Infometric Analyses on the World Wide Web: Methodological Approaches to ‘Webometrics’. 
                        Journal of Documentation, 
                        53(4).
                    
                    
                        Bates, M. (1998). Indexing and access for digital libraries and the Internet: human, database, and domain factors. 
                        Journal of the American Society for Information Science, 
                        49(13) doi:10.1002/(SICI)1097-4571(1998110)49:13&lt;1185::AID-ASI6&gt;3.3.CO;2-M (accessed 23 March 2011).
                    
                    
                        British Library (2014). Legal Deposit 
                        British Library - About Us http://www.bl.uk/aboutus/legaldeposit/.
                    
                    
                        Brown, S., Clement, T., Mandell, L., Verhoeven, D. and Wernimont, J. (2016). Creating Feminist Infrastructures in the Digital Humanities. Krakow http://dh2016.adho.org/abstracts/233 (accessed 10 March 2017).
                    
                    
                        Gooding, P. (2016). Exploring the information behaviour of users of Welsh Newspapers Online through web log analysis. 
                        Journal of Documentation, 
                        72(2): 232–46.
                    
                    
                        Kua, E. (2008). Non-Western Languages and Literatures in the Dewey Decimal Classification Scheme. 
                        Libri, 
                        54(4): 256–265 doi:10.1515/LIBR.2004.256.
                    
                    
                        Mai, J. (2010). Classification in a social world: bias and trust. 
                        Journal of Documentation, 
                        66(5): 627–42 doi:10.1108/00220411011066763.
                    
                    
                        Nicholas, D., Jamali, H. R. and Huntington, P. (2005). The use and users of scholarly e-journals: a review of log analysis studies. 
                        Aslib Proceedings, 
                        57(6): 554–71.
                    
                    
                        Palmer, C. L. and Cragin, M. H. (2008). Scholarship and disciplinary practices. 
                        Annual Review of Information Science and Technology, 
                        42(1): 163–212 doi:10.1002/aris.2008.1440420112.
                    
                    
                        Ross, C. and Terras, M. (2011). Scholarly Information-Seeking Behaviour in the British Museum Online Collection. Philadelphia, PA http://www.museumsandtheweb.com/mw2011/papers/scholarly_information_seeking_behaviour_in_the (accessed 10 October 2012).
                    
                    
                        Sinn, D. and Soares, N. (2014). Historian’s Use of Digital Archival Collections: The Web, Historical Scholarship, and Archival Research. 
                        Journal of the Association for Information Science and Technology, 
                        Online First http://onlinelibrary.wiley.com/doi/10.1002/asi.23091/abstract (accessed 26 June 2014).
                    
                    
                        Stone, S. (1982). Humanities Scholars: Information Needs and Uses. 
                        Journal of Documentation, 
                        38(4): 292–313.
                    
                    
                        sub-subroutine (2015). Visualising the distribution of human knowledge in the Dewey Decimal System 
                        Sub-Subroutine http://www.subsubroutine.com/sub-subroutine/2014/12/29/dewey-decimal-system-and-books-filed-therein (accessed 11 November 2018).
                    
                    
                        Talja, S. and Maula, H. (2003). Reasons for the use and non
                        ‐use of electronic journals and databases: A domain analytic study in four scholarly disciplines. 
                        Journal of Documentation, 
                        59(6): 673–91 doi:10.1108/00220410310506312.
                    
                    
                        Unsworth, J. (2000). Scholarly Primitives: What Methods do Humanities Researchers Have in Common, and How Might our Tools Reflect This?. King’s College London http://people.brandeis.edu/~unsworth/Kings.5-00/primitives.html (accessed 5 August 2014).
                    
                    
                        Warwick, C., Terras, M., Huntington, P. and Pappa, N. (2008). If You Build It Will They Come? The LAIRAH Study: Quantifying the Use of Online Resources in the Arts and Humanities. 
                        Literary and Linguistic Computing, 
                        23(1): 85–102.
                    
                    
                        Weingart, S. (2014). The moral role of DH in a data-driven world. Lawrence, Kansas http://www.scottbot.net/HIAL/?p=40944 (accessed 2 November 2018).
                    
                    (2013). 
                        The Legal Deposit Libraries (Non-Print Works) Regulations 2013. http://www.legislation.gov.uk/uksi/2013/777/contents/made (accessed 15 August 2013).
                    
                
            
        
    
9846	2019	
        
            This paper reports on World-Historical Gazetteer (WHGazetteer), a three-year project funded by the US National Endowment for the Humanities, now two-thirds complete. 
            
                Goals and Purpose
                WHGazetteer is a scholarly infrastructure project intended to support historical research across many disciplines. It is principally a web-based software system for aggregating open access data about places and linking it with data about historical entities associated with those places. We have seeded the system with a global ‘spine’ dataset of some 30,000 places, developed expressly for this purpose.
                There are existing repositories of historical place data, and many more are in development. A few are explicitly termed gazetteers
                    
                         E.g. 
                            Pleiades; 
                            Vision of Britain
                        
                    , some are historical GIS web resources
                    
                        
                            E.g. 
                            China Historical GIS
                            ; 
                            HGIS de las Indias
                        
                    , and others comprise place data tables developed and published in the course of other research projects. Typically, each concerns a particular geographic area and a particular temporal scope
                    
                        
                            E.g. 
                            Map of Early Modern London
                            ; 
                            Syriaca.org
                        
                    .
                
                Bu contrast, WHGazetteer is soliciting and aggregating attestations of historical places for all regions and periods, along with annotations of records about historical entities with identifiers for those places. The project is conceived from a world-historical perspective. By this we mean several things. First, that it is intended to facilitate representations of connection and movement; second, that it scales up to global processes and the longue durée; and finally, the places for us includes ethnonyms for cultural regions, physical features, and ecoregions, helping ensure coverage for all parts of the globe and providing geographic context.
                In many respects WHGazetteer follows and extends the pilot implementation of 
                    Peripleo, the linked data gazetteer system developed by the 
                    Pelagios Commons project. We are working closely with the Pelagios group to leverage their accomplishments, maintaining and extending the trajectory they established. 
                
            
            
                Places and Traces
                WHGazetteer will solicit contributions of two kinds of place-related data. One is attestations of place names found in historical texts and maps: 
                    Places. The other is annotations of records about any sort of historical entity (artifacts, creative works, persons, events, etc.) with place identifiers from published gazetteers: 
                    Traces. 
                
                
                    Places
                    WHGazetteer catalogues place references associated with a time period, which may be a date of publication for an historical source or temporal information about a name as in a modern historical atlas. In either case, a historical gazetteer records attestations from sources that assert that a name existed at a certain time. This approach to temporal scoping differentiates us from modern gazetteer data sources like GeoNames and Wikidata. A single place may (and ultimately should) have many linked attestations for different periods, where names, spellings, place type, relations, and geometry vary.
                
                
                    Traces
                    Traces are assertions of spatial and temporal scope for historical items of almost any kind: the footprint of an individual’s lifepath; the findspot of a coin hoard; the itinerary of a journey; the extent of a war; the places referred to in any sort of text, from a treaty to a novel; and so on.
                
            
            
                Standard Data Formats
                We are developing two standard contribution formats conforming to linked data requirements. The first, 
                    Linked Places format, is essentially complete and is being tested with early contributions. 
                    Linked Traces format, based on the W3C Web Annotation Data Model, will be completed in late spring, 2019.
                
            
            
                Contribution Pipeline
                Projects contributing to WHGazetteer come in all sizes and have varying levels of technical capability. Some have elaborate web interfaces and maintain permanent URIs for thousands of place records. These projects will have little difficulty exporting abbreviated records transformed to Linked Places format. Others have dozens or at most hundreds of place references drawn from sources specific to their domain of interest and are unable to stand up per-place pages with permanent URIs. The WHGazetteer contribution pipeline (Fig. 1) allows both groups to a) upload compatible Linked Places or CSV files, b) reconcile their records against Getty TGN, DBpedia, GeoNames and Wikidata as well as the WHGazetteer index itself, c) review and validate results of that matching process, thereby augmenting their records with closeMatch or exactMatch links, and d) contribute reviewed records to the index, published under permanent URIs provided by WHGazetteer.
            
            
                Backend Stores and Middleware
                The WHGazetteer system backend is comprised of a PostgreSQL relational database and multiple Elasticsearch index stores. These and the APIs for internal use and public access are managed with Django, a Python-based web development framework. 
            
            
                Interfaces
                WHGazetteer will provide a public API supporting machine queries to all indexed data. A graphical web interface will support contribution activities for authenticated users, and provide search and mapping capabilities for both Places and Traces.
                Linking Place and Trace data in a single backend allows us to present linked data ‘portals’ for all indexed places, which will grow richer over time as our indexes expand. Using either the public API or graphical interface one might discover, for a given place: its names, shapes, and relations with other places over time; people whose lifepaths have intersected it; journeys for which it has been a waypoint; and texts and artworks for which it is a subject. 
                Our contribution pipeline interface will enable several kinds of pedagogical applications. For example, students and instructors will be able to create custom Traces associated with course material. Advanced students will be able to upload CSVs of gazetteers they have created, and use the WHGazetteer reconciliation tool to augment their records with information in the WHGazetteer index.
            
            
                Status
                Version 1 of WHGazetteer is scheduled for launch in January 2020. A beta version will be available by July 2019. 
                We have established data partnerships with roughly a dozen contributors of large datasets covering a range of regions and periods, and also have a queue of many smaller projects. Data pipeline functionality to receive these is nearly complete (Fig. 1).
            
            
                Sustainability
                WHGazetteer has been designed to require minimal hand-holding for contributions, and tools for efficient curation and maintenance. University of Pittsburgh’s World History Center is committed to maintaining the system for the foreseeable future.
                
                    
                
                Figure 1 – Data pipeline (partial): (a) upload dataset(s); (b) perform reconciliation against modern authorities; (c) review/validate reconciliation ‘hits’
            
        
    
9847	2019	
        
            
                Introduction
                Statistical topic models are increasingly and popularly used by Digital Humanities scholars to perform distant reading tasks on literary data (Navarro-Colorado, 2018), (Hettinger et al., 2016). It allows us to estimate what people talk about. Especially Latent Dirichlet Allocation (LDA), see (Blei et al., 2003), has shown its usefulness, as it is unsupervised, robust, easy to use, scalable, and it offers interpretable results. In a preliminary study, we apply LDA to a corpus of New High German poetry (textgrid, with 51k poems, 8m token) and interpret salient topics, their trend over time (1575–1925 A.D.), and use the distribution of topics over documents for a classification of poems into time periods and for authorship attribution.
            
            
                Corpus
                The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form (Vanscheidt et al., 2016). It was mined from 
                    http://zeno.organd covers a time period from the 16th century up to the first decades of the 20th century. It contains many important texts that can be considered as part of the literary canon, even though it is far from complete (e.g. it contains only half of Rilke’s work). We find that around 51k texts are annotated with the label ’verse’, not distinguishing between ’lyric verse’ and ’epic verse’. We will further call this verse portion TGRID-V. However, the average length of these texts is around 150 token, dismissing most epic verse tales. Furthermore, the poems are distributed over 229 authors, where the average author contributed 240 poems with a median of 131. A drawback of TGRID-V is the circumstance that it contains a noticeable amount of French and Dutch (around 200 poems). To constrain our dataset to German, we filter foreign language material with a stopword list, as training a dedicated language identification classifier is far beyond the scope of this work.
                
                
                    
                        
                        Figure 1: 25 year Time Slices of Textgrid Poetry (1575--1925)
                    
                
            
            
                Experiments
                We approach diachronic variation of poetry from two perspectives. First, as distant reading task to visualize the development of clearly interpretable topics over time. Second, as a downstream task, i.e. supervised machine learning task to determine the year (the time-slot) of publication for a given poem. We infer topic distributions over documents as features and pit them against a simple style baseline.
                We use the implementation of LDA as it is provided in genism (Rehurek and Stoja, 2011). LDA assumes that a particular document contains a mixture of few salient topics, where words are semantically related. We transform our documents (of wordforms) to a bag of words representation, filter stopwords (function words), and set the desired number of topics=100 and train for 50 epochs to attain a reasonable distinctness of topics. We choose 100 topics (rather than a lower number that might be more straightforward to interpret) as we want to later use these topics as features for downstream tasks. We find that wordforms (instead of lemma) are more useful for poetry topic models, as these capture style features (rhyme), orthographic variations ('hertz' instead of 'herz'), and generally offer more interpretable results.
                
                    Topic Trends
                    We retrieve the most important (likely) words for all 100 topics and interpret these (sorted) word lists as aggregated topics, e.g. topic 27 (figure 2) contains: 
                        Tugend (virtue), Kunst (art), 
                        Ruhm (fame), Geist (spirit), Verstand (mind) and Lob (praise). This topic as a whole describes the concept of ’artistic virtue’.
                    
                    In certain clusters (topics) we find poetic residuals, such that rhyme words often cluster together (as they stand in proximity), e.g. topic 52 with:
                        Mund (mouth), Grund (cause, ground), rund (round).
                    
                    To discover trends of topics over time, we bin our documents into time slots of 25 years width each. See figure 1 for a plot of the number of documents per bin. The chosen binning slots offer enough documents per slot for our experiments. To visualize trends of singular topics over time, we aggregate all documents d in slot s and add the probabilities of topic t given d and divide by the number of all d in s. This gives us the average probability of a topic per timeslot. We then plot the trajectories for each single topic. See figures 2–6 for a selection of interpretable topic trends. Please note that the scaling on the y-axis differ for each topic, as some topics are more pronounced in the whole dataset overall.
                    
                        
                            
                            Figure 2: left: Topic 27 'Virtue, Arts' (Period: Enlightenment), right: Topic 55 'Flowers, Spring, Garden' (Period: Early Romanticism)
                        
                    
                    
                        
                            
                            Figure 3: left: Topic 63 'Song' (Period: Romanticism), right: Topic 33 'German Nation' (Period: Vormärz, Young Germany)
                        
                    
                    
                        
                            
                            Figure 4: left: Topic 28 'Beautiful Girls' (Period: Omnipresent, Romanticism), right: Topic 77 'Life &amp; Death' (Period: Omnipresent, Barock)
                        
                    
                    
                        
                            
                            Figure 5: left: Topic 60 'Fire' (Period: Modernity), right: Topic 42 'Family' (no period, fluctuating)
                        
                    
                    
                        
                            
                            Figure 6: Most informative topics for classification; left: Topic 11 'World, Power, Time' (Period: Barock), right: Topic 19 'Heaven, Depth, Silence' (Period: Romanticism, Modernity)
                        
                    
                    Some topic plots are already very revealing. The topic ‘artistic virtue’ (figure 2, left) shows a sharp peak around 1700—1750, outlining the period of Enlightenment.
                    Several topics indicate Romanticism, such as ‘flowers’ (figure 2, right), ‘song’ (figure 3, left) or ‘dust, ghosts, depths’. The period of 'Vormärz' or 'Young Germany' is quite clear with the topic ‘German Nation’ (figure 3, right). It is however hardly distinguishable from romantic topics.
                    We find that the topics 'Beautiful Girls' (figure 4, left) and 'Life &amp; Death' (figure 4, right) are always quite present over time, while 'Girls' is more prounounced in Romanticism, and 'Death' in Barock.
                    We find that the topic 'Fire' (figure 5, left) is a fairly modern concept, that steadily rises into modernity, possibly because of the trope 'love is fire'. Next to it, the topic 'Family' (figure 5, right) shows wild fluctuation over time.
                    Finally, figure 6 shows topics that are most informative for the downstream classification task: Topic 11 'World, Power, Time' (left) is very clearly a Barock topic, ending at 1750, while topic 19 'Heaven, Depth, Silence' is a topic that rises from Romanticism into Modernity.
                
                
                    Classification of Time Periods and Authorship
                    To test whether topic models can be used for dating poetry or attributing authorship, we perform supervised classification experiments with Random Forest Ensemble classifiers. We find that we obtain better results by training and testing on stanzas instead of full poems, as we have more data available. Also, we use 50 year slots (instead of 25) to ease the task.
                    For each document we determine a class label for a time slot. The slot 1575–1624 retrieves the label 0, the slot 1625–1674 the label 1, etc.. In total, we have 7 classes (time slots).
                    As a baseline, we implement rather straightforward style features, such as line length, poem length (in token, syllables, lines), cadence (number of syllables of last word in line), soundscape (ratio of closed to open syllables, see (Hench, 2017)), and a proxy for metre, the number of syllables of the first word in the line.
                    We split the data randomly 70:30 training:testing, where a 50:50 shows (5 points) worse performance. We then train Random Forest Ensemble classifiers and perform a grid search over their parameters to determine the best classifier. Please note that our class sizes are quite imbalanced. 
                    The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42—52%.
                    The mose informative features (with information gain) are: Topic11 (.067), Topic 37 (.055), Syllables Per Line (.046), Length of poem in syllables (.031), Topic19 (.029), Topic98 (.025), Topic27 ('virtue') (.023) and Soundscape (.023).
                    For authorship attribution, we also use a 70:30 random train:test split and use the author name as class label. We only choose the most frequent 180 authors. We find that training on stanzas gives us 71% Accuracy, but when trained on poems, we only get 13% Accuracy. It should be further investigated is this is only because of a surplus of data. 
                
            
            
                Conclusion &amp; Future Work
                We have shown the viability of Latent Dirichlet Allocation for a visualization of topic trends (the evolution of what people talk about in poetry). While most topics are easily interpretable and show a clear trend, others are quite noisy. For an exploratory experiment, the classification into time slots and for authors attribution is very promising, however far from perfect. It should be investigated whether using stanzas instead of whole poems only improves results because of more available data. Also, it needs to be determined if better topic models can deliver a better baseline for diachronic change in poetry, and if better style features will outperform semantics. Finally, only selecting clear trending and peaking topics (through co-variance) might further improve the results.
            
        
        
            
                
                    Bibliography
                    
                        Blei, D.M., Ng, A.Y. and Jordan, M.I., 2003. 
                        Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), pp.993-1022.
                    
                    
                        Hettinger, L., Jannidis, F., Reger, I. and Hotho, A., 2016. 
                        Classification of Literary Subgenres. DHd 2016, pp.154-58.
                    
                    
                        Hench, C.
                         2017. Phonological Soundscapes in Medieval Poetry. In 
                        Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature
                         
                        (pp. 46-56).
                    
                    
                        Navarro-Colorado, B., 2018. 
                        On Poetic Topic Modeling: extracting themes and motifs from a corpus of Spanish poetry. Frontiers in Digital Humanities, 5, p.15.
                    
                    
                        Vanscheidt, P., Rapp, A., Schmid, O., Schmunk, S. and Kollatz, T., 2016. 
                        TextGrid und DARIAH-DE: Forschungsumgebung und Infrastruktur fúr
                        die Geisteswissenschaften. DHd 2016.
                    
                    
                        Rehurek, R. and Sojka, P., 2011. 
                        Gensim: Statistical Semantics in Python.
                    
                
            
        
    
9857	2019	
        
            
                
                    Pre-Conference Workshop “DLS Tool Criticism.
                    An Anatomy of Use Cases” 
                
            
            The current panorama in DLS presents a plethora of tools, protocols and practices for processing, analysing and visualising data. This diversity of practices and tools originating from different areas (e.g., stylometry, NLP, literary studies, corpus linguistics) has resulted in a rich, but atomised situation. 
            Using a broad definition of “tool” understood as method, the ADHO-Special Interest Group “Digital Literary Stylistics” (SIG-DLS) organizes a workshop that taps into the 
                DLS Tool Inventory
                (DLS-TI), which is a first attempt to gather information on the practices of the various traditions present in DLS. The DLS-TI features methods and suites for data analysis, including desktop GUIs, online Virtual Research environments and libraries for R or Python, as well as general purpose tools such as Excel spreadsheets. As tools have the power to reify theoretical a prioris (Jannidis &amp; Flanders, 2015; McCarthy, 2005), the community needs a handle for gauging their validity, applying a sense of craft (Piper, 2017) from the perspective of tool criticism (van Es et al., 2018; Koolen et al., 2018). 
            
            Building on the DLS-TI, in concert with other initiatives (
                LRE-map, Calzolari et al., 2012; 
                DIRT directory; 
                Catalogue of Digital Editions; 
                IDE), we aim at taking stock, but also reflect on, our methods. Three use cases representing different types of digital tools will undergo an “anatomy”: Textométrie, Stylometry, and Semantic Text Mining. Three scholars will each present a use case, elucidating (a) reasons for choosing the method; (b) the method’s impact on the analysis and literary modeling; (c) advantages and limitations. 
            
            The discussion will also address traditions of Digital Literary Stylistics between “digital” (computational linguistics, text mining, corpus linguistics) and “analog” (formal and subjective approaches), addressing the fit of data and method to literary modeling (Piper, 2019; Underwood, 2019; cf. Da, 2019). 
            Anatomy of tools: A closer look at “textual DH” methodologies
            Based on what has emerged so far from the DLS-IT and further observations of research practices, we have identified a three groups of tools that can be covered in this half-day workshop and will be represented by an exemplary use case:
             
            1.Textométrie
            Textometry is a traditionally French approach to statistical text analysis, often based on methods such as Correspondence Analysis, which has produced a number of tools, alongside a productive body of research in the domain of stylistics and corpus linguistics.
            Textometry and stylistics: which tools and practices for literary interpretation?
            (
                Clémence
                 Jacquot, Université Paul-Valéry Montpellier 3, France) 
            
            “Textometric tools” are widely used by researchers to explore literary corpora. This session intends to propose a
                 critical feedback of experience on a stylistic analysis guided by a textometric exploration of Apollinaire's poetic corpus with TXM. Several issues will be discussed: first, how to analyze in concrete terms the evolution of the poetic writing of a single author, Apollinaire, between 1898 and 1918, from a diachronic perspective? In a contrastive study, which are the advantages of TXM. We propose to review the importance of specificities calculations and the various visualizations proposed (AFC, for example), as well as the corpus scores allowed by an annotation of the structural units of the poetic corpus. Finally the methodological and epistemological contributions of a tool such as TXM for stylistic analysis will be discussed from a critical point of view. What observable results does it provide? What does it make visible? How to interpret the salience of certain results? What silence can he throw on other stylistic points of the text?
            
             
            
                2. Stylometry
            
            Stylometry uses a series of tools and methods for the statistical analysis of style, based on advanced calculations on word frequencies, including multi-dimensional measurements and machine learning techniques. Their main applications have been both authorship attribution and distant reading. Initially developed through the use of spreadsheets, they have been fully implemented into programming languages such as R and Python, and integrated by a wide variety of visualizations, derived from research fields such as philogenetics and network theory. 
            Less than countless. Options to move beyond word counting in stylometry 
            
                (Mike
                 Kestemont, University of Antwerp, Belgium)
            
            In a fairly dramatic critique of computational literary studies, Nan Z. Da recently made a controversial case against the application of quantitative methods to literary texts. She argues that much work in this field essentially boils down to "counting words". This view is somewhat reductive but not without merit: it certainly applies to much of the present-day approaches that are dominant in stylometry and, consequently, to many of the tools that are available. While this methodological focus (if not poverty) is to some extent justified by previous empirical work, I will reflect on under-explored options for stylometry to move beyond naive word counting. Stylometrists, for instance, often take pride in the fact that their tools typically work on raw texts that require little preprocessing. In this, stylometry ignores much of the achievements of literary theory in the twentieth century, such as the importance of focalization (perspective) or the (actual, individual) reader. Richer (pre)processing pipelines, that also tap into syntax and discourse, might allow stylometry to revitalize its connection with literary theory, but comes with significant barriers for non-Anglo-Saxon literatures. In this talk, I intend to review some of the less conventional work in stylometry that leads the way in this respect.
            
                
                    3. Semantic Text Mining
                
            
            Semantic Text Mining applies tools for text analysis and visualization based on semantically enriched and co-occurrence methodologies, such as sentiment analysis, topic modelling, and word embeddings. They offer the potential of addressing key questions in literary theory and narratology, from the identification of genre to the visualization of plot. These emerging approaches are now beginning to broaden the scope of computational literary studies and to open up new, still-unexplored potentialities.
            LDA Topic Modeling for Semantic Corpus Analysis
            (
                Steffen
                 
                Pielström, Würzburg University, Germany)
            
            Topic models based on Latent Dirichlet Allocation (LDA) and Gibbs Sampling are a tool for exploring and analyzing the content and semantic structure of digital text corpora that has become popular in digital humanities research in recent years. They allow researchers to model a corpus' content in terms of so called "topics", groups of apparently semantically related words, and show the distribution of these topics within the corpus. Thanks to an increasing number of available tools and libraries, the method is, by this day, accessible to a wide range of users. In contrast to this technical accessibility, the methodology of topic modeling is rather intricate, and users cannot generally use them without making a number of decisions that require some deeper understanding. Additionally, there are aspects of topic modeling that are still in need of systematic methodological research. The session will give a hands-on introduction on goals and method of LDA topic modeling, demonstrate how to experiment with topic models using a simple desktop tool, and address open methodological issues.
             
            
                4. Discussion
            
            During the discussion we will pose questions on methodological and epistemological levels – including humanistic enquiry vs. data science, explorative vs. confirmative, and qualitative vs. quantitative approaches; as well as the range of research questions, from text similarity to aesthetic effects. The aim of each use case-anatomy is to give an overview of the usability and strengths of the tool (group of tools) in research, as well as pointing out problems and formulating specific avenues for further development. Results will be documented on a special SIG-DLS webpage. Through this, we will produce a guide for DLS-scholars’ orientation, as well as the beginning of a roadmap for further tool development.
            
                The target audience is scholars interested in methods/tools and their linkage to literary modeling. We welcome newbies who look for initial orientation, old hands who wish to progress methodological development (“application”), and any scholars interested in modeling, i.e. epistemological aspects of style-tool criticism (“theory”).
            
            Calzolari, N., Del Gratta, R., Francopoulo, G., Mariani, J., Rubino, F., Russo, I. and Soria, C. (2012). The LRE Map. Harmonising Community Descriptions of Resources. Proceedings of LREC 2012, Eighth International Conference on Language Resources and Evaluation. Istanbul, Turkey, pp. 1084–1089 
                http://lrec.elra.info/proceedings/lrec2012/pdf/769_Paper.pdf
            
            Da, N. Z. (2019). The Computational Case against Computational Literary Studies. Critical Inquiry, 45(3), 601–639. 
                https://doi.org/10.1086/702594
            
            Es, K. van, Wieringa, M. and Schäfer, M. T. (2018). Tool Criticism: From Digital Methods to Digital Methodology. Proceedings of the 2Nd International Conference on Web Studies. (WS.2 2018). New York, NY, USA: ACM, pp. 24–27 doi:
                10.1145/3240431.3240436.
                http://doi.acm.org/10.1145/3240431.3240436
            
            Franzini, G. (2012). Catalogue of Digital Editions Zenodo doi:
                10.5281/zenodo.1161425.
                https://zenodo.org/record/1161425#.XCoDJhNKh8c (accessed 31 December 2018).
            
            Flanders, J., &amp; Jannidis, F. (2015). Flanders, J., &amp; Jannidis, F. (2015). Knowledge Organization and Data Modeling in the Humanities. A Whitepaper. 
                https://www.wwp.northeastern.edu/outreach/conference/kodm2012/flanders_jannidis_datamodeling.pdf 
            
            Koolen, J., Gorp, M. van and Ossenbruggen, J. van (2018). Lessons Learned from a Digital Tool Criticism Workshop. Proceedings from DH Benelux 2018. Amsterdam, The Netherlands.
            McCarty, W. (2005). Humanities Computing. London and New York: Palgrave.
            Piper, A. (2017). Think Small: On Literary Modeling. PMLA, 132(3), 651–658. 
                https://doi.org/10.1632/pmla.2017.132.3.651
            
            Piper, A. (2019). Enumerations: data and literary study. Chicago: The University of Chicago Press.
            Underwood, T. (2019). Distant horizons: digital evidence and literary change. Chicago: The University of Chicago Press.
        
    
9859	2019	
        
            
                Introduction
                Is an animal a person? The question is far from idle; it is in fact fraught with urgent ethical and legal consequences for both animal and environmental rights and shapes scientific norms for the study of animal behavior. It remains a constant theme in Western philosophy from Rene Descartes through present-day Critical Animal Studies. However, lawyers, philosophers, and ethologists are not the only deciders in this question: cultural representations of animals also mediate their relation to personhood. Fiction, for instance, excels in the representation of human individuality, interiority, and action; complex, “round” characters of course populate the long history of prose fiction. How, then, does fiction engage with the personhood of animals? In fiction, is an animal a 
                    character? What do animals do in the pages of fiction? Do they make decisions, have feelings, express interiority? Do animals function more similarly to human characters, or to things, objects, and machines?
                
                In what follows, we approach these questions with computational methods in one of the first attempts to apply digital methods to animal studies.
                    
                        
                             For an experiment on biodiversity archives and species extinction, see Ursula Heise, 
                            Imagining Extinction: The Cultural Meanings of Endangered Species
                             (Chicago: University of Chicago Press, 2016), 55-86.
                        
                     In a variety of corpora—from popular natural history to scientific writing about animal behavior to animal-driven fictions historically accused of anthropomorphism—we compare the semantic and syntactic footprints left behind by animals and humans. We discover that, from a computational standpoint, animals in fiction are indeed recognizable as characters, albeit characters who register intentionality through physical movement over speech and display a mental paradigm delimited by instinct and associative learning. Natural history writing, on the other hand, narrates animals in ways that seem surprisingly human-like when compared to animal representations in fiction more broadly.
                
            
            
                Corpus
                Around the turn of the twentieth century, reading audiences in the United States developed a penchant for fiction about nonhuman animals. These so-called “wild animal stories” appeared in popular magazines and short story collections by writers such as E.T. Seton, Charles G.D. Roberts, William J. Long and Jack London. Despite their popularity, the stories proved to be a flash-point for scientific controversy over the appropriate way to narrate animal behavior. Over a period of several years, prominent public intellectuals—including the naturalist John Burroughs and then-president Teddy Roosevelt—would attack the writers of wild animal stories for attributing inaccurately human-like qualities to their nonhuman characters. The entire debate would come to be known as “The Nature Fakers” episode.
                    
                        
                             For a detailed literary-historical account, see Ralph Lutts, 
                            The Nature Fakers: Wildlife, Science, and Sentiment
                            , Fulcrum Publishing, 1990.
                        
                     Though concerned a minor and moderately pulpy literary sub-genre, the controversy marks a moment when science and fiction butt heads over the meaning and appropriateness of anthropomorphism, as natural historians attempt to police the limits of fictional character. This corpus thus offers a high-stakes proving ground for the very concept of anthropomorphism in narrative fiction and science writing.
                
                We have assembled 54 texts from the writers involved in this controversy, comprising short story collections and novels by eight of the most prominent animal story authors, published between the 1870s and the 1930s and concentrated in the first two decades of the twentieth century when the Nature Fakers debate was at its most intense. As a point of comparison for these fictions, we have also assembled 17 of John Burroughs’ natural history monographs, which detail the doings of wild animals in the idiom of popular science. Finally, we selected approximately 400 American novels published between 1870 and 1930 as a control corpus with no particular interest in nonhuman animals.
                    
                        
                             These 444 novels derive from two sources. For the late nineteenth century, we turned to a collection of about 325 American novels published between 1875 and 1905. Compiled by Marissa Gemma, these texts were selected based on their inclusion in the 
                            Annals of American Literature
                             (Richard Ludwig and Clifford Nault, Oxford: Oxford University Press, 1989) and their availability in Project Gutenberg. Twentieth-century novels are those compiled by Mark McGurl and Mark Algee-Hewitt in “Between Canon and Corpus: Six Perspectives on 20th-Century Novels,” 
                            Stanford Literary Lab 
                            8 (2016), https://litlab.stanford.edu/LiteraryLabPamphlet8.pdf.
                        
                    
                
            
            
                Methods
                Our digital methods collect the words that characterize animals, humans, objects in fiction. We base our methods on BookNLP, a Java program which clusters fictional character names together (“Elizabeth” and “Elizabeth Bennet”), and then collects the words associated with each character in specific ways: verbs for which each character is either a subject (“Elizabeth wondered”) or an object (“Mrs. Gardiner ... reminded Elizabeth”); as well as the nouns each character possesses (“Elizabeth’s wishes”), the adjectives attributed to each character (“Elizabeth was watchful”), and all words spoken by the character in moments of dialogue.
                    
                        
                             David Bamman, Ted Underwood, and Noah Smith, “A Bayesian Mixed Effects Model of Literary Character,” 
                            ACL 2014
                            , http://www.cs.cmu.edu/~ark/literaryCharacter/. For a recent application of BookNLP to historical fictional practices of gendering human characters, see the recent article by Ted Underwood, David Bamman, and Sabrina Lee, “The Transformation of Gender in English-Language Fiction,” 
                            Cultural Analytics 
                            (Feb 2018), DOI: 10.31235/osf.io/fr9bk.
                        
                     We applied BookNLP to our corpus of wild animal stories, and then annotated its returned characters for whether they were human or animal in 15 of the cleanest texts. BookNLP was in general accurate at identifying named animal characters—for example, in London’s 
                    White Fang, BookNLP failed to recognize only one repeatedly occurring named character, White Fang’s father One Eye. The results of our annotations are recorded below in Table 1.
                
                
                    
                        Author
                        #Stories
                        #Words in stories
                        #Chars (Animal)
                        #Chars (Human)
                        #Words (Animal)
                        #Words (Human)
                        #Words (per char)
                        %Effect on Totals
                    
                    
                        Ernest Seton
                        5
                        162,595
                        70
                        59
                        6,830
                        4,072
                        85
                        42.2%
                    
                    
                        William Long
                        3
                        117,069
                        21
                        5
                        768
                        684
                        56
                        5.6%
                    
                    
                        Charles Roberts
                        2
                        129,408
                        0
                        6
                        0
                        1,394
                        232
                        5.4%
                    
                    
                        Harriet Miller
                        2
                        113,588
                        2
                        6
                        76
                        502
                        72
                        2.2%
                    
                    
                        Jack London
                        2
                        103,676
                        20
                        26
                        4,049
                        5,107
                        199
                        35.4%
                    
                    
                        Clarence Hawkes
                        1
                        33,784
                        3
                        5
                        1,372
                        1,009
                        298
                        9.2%
                    
                
                
                    Table 1
                    . Statistics regarding the small corpus of 15 texts whose BookNLP-identified characters have been annotated for their species (whether human or animal). Sorted by the number of stories they contribute, these authors vary widely in the number of word-to-character associations they generate. For instance, although William Long contributes more stories, words, and characters than Jack London, his effect on the aggregate data is smaller (6% to London’s 35%): this is because he attributes on average only 56 words per character, whereas London attributes on average 199 words to each character. This is likely owing to Long’s heavier usage of non-proper nouns, often referring to characters as “the mother,” “the cub,” etc.
                
                BookNLP has the obvious limitation of overlooking unnamed characters—both human and animal alike. To address this limitation, we designed a Python program to extend BookNLP’s logic to all nouns. Our program collected approximately the same information: the verbs of which each noun is a subject or object; the nouns it possesses; and the adjectives it is modified by.
                    
                        
                             Our Python program ran the corpus through spaCy, a syntactic dependency parser for Python, and collected all moments when a noun had a syntactic relation to another word in a sentence of any of the following kinds (shown in parentheses are the syntactic dependency tags): subject (nsubj); passive subject (nsubjpass); direct and indirect objects (dobj and dative); possessives (poss); and modifiers (amod [adjective], compound [noun compound], appos [appositive], and attr [predicate]). We used SpaCy's default English-language model (en_core_web_sm), trained on web discourse. That this model was trained on the hyper-modern form of language of the internet is of course regrettable, given the historical material of our project, but unfortunately this is common and largely unavoidable problem in literary text mining.
                        
                     We then produced a lists of nouns for animals and humans, which were drawn from the Harvard General Inquirer lists of those names.
                    
                        
                             Roger Hurtwitz, “General Inquirer Home Page,” http://www.wjh.harvard.edu/~inquirer/Home.html (accessed July 22, 2018). Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith and Daniel M. Ogilvie, 
                            The General Inquirer: A Computer Approach to Content Analysis
                             (Cambridge: MIT Press, 1966).
                        
                     Uncertain or ambiguous entries were pruned, reducing an original list of 930 words to a refined list of 371 words.
                    
                        
                             For example, the refined list of 287 human words included words like detective, ambassador, pope, freshmen, executive, commoner, manager, scientist, and human; the refined list of 96 animal words included words like turtle, owl, shark, oxen, grouse, roachback, moth, crow, hare, and jackrabbit.
                        
                     Whenever a noun from these lists appeared in the parsed sentences of the corpus, we record its appearance along with its associated syntactic relation.
                
                Finally, to determine whether a word is (statistically) significantly more likely to associate with a human or animal character, we conduct a Fisher’s exact test on a 2x2 contingency table: (# of times a given word appears in the context of an animal/human character) x (# of times an animal / human character appears.) This weights our expectation for the number of times words appear by the number of times characters appear. The Fisher’s exact test returns an odds ratio (the odds of a word’s human association / its odds of animal association) along with a p-value. Words with a p-value of less than 0.1 are shown in the results below.
            
            
                Results
                Due to limited space, we present only a brief summary of some of our findings in experiments that draw on BookNLP and named characters (Experiment 1) and on our own program’s collection of words associated with animal and human nouns (Experiment 2). Finally, we apply machine learning techniques to assess the overall distinctiveness of human and animal nouns (Experiment 3).
                
                    Experiment 1: Comparing how animal and human characters are narrated in the Wild Animal Stories using Book NLP
                    As seen in Figure 1, human characters are more likely to be the subject of speaking verbs such as “asked,” “called,” “cried,” “announced,” and, most especially, “answered.” Even in these wild animal stories, animals do not (as a rule) speak. Instead, animal characters in these texts are far more likely than humans to register intentional response through embodied activity--such verbs as “fought,” “bristled,” and “licked.” Emotionally, these results suggest that animals are characterized more by aggression (“hated”), humans by sociability (“love”).
                        
                            
                                Think of London’s dog heroes, who only learn to love toward the end of their stories, when paired with the appropriate human master, but who never experience love in the company of other dogs.
                            
                         More subtly, animals “learn” and “know” things, while a human character is more likely to have “thought,” a result that could suggest something about the prevailing paradigm of animal mental life, characterized by instinct and associative learning, rather than rational reflection.
                    
                    
                        
                    
                    Figure 1: The verbs for which animal and human characters most often act as subject. Shown are words that are statistically significantly distinctive of animal or human characters (p&lt;0.1). The farther to the left, the more strongly a word is associated with animal characters; the farther to the right, the more to human characters.
                
                
                    Experiment 2: Comparing how animal nouns are narrated differently in Wild Animal Stories and Natural History writing
                    Our comparison of Burroughs’ natural history writings with the Wild Animal Stories does not, on the face, suggest that animals are any less person-like in Burroughs anti-anthropomorphic paradigm than they are in fiction. Indeed, as seen in Figure 2 below, Burroughs’ animals are statistically more likely to “know,” “make,” and “become” than their fictional counterparts. Many of the most distinctive active verbs between these two corpora might be explained by their relative interest in different species--for example, Burroughs writes about birds more frequently, so his animals are more likely to “fly” and “sing.” On the other hand, the strong influence of Jack London in the fictional corpus results in a high incidence of sled dogs, who are statistically more likely to “draw” a sled.
                    
                        
                    
                    Figure 2: The verbs for which animal nouns most often act as subject. Shown are words that are statistically significantly distinctive of wild animal stories or of natural history writing (p&lt;0.1). The farther to the left, the more strongly a word is associated with the former genre; the farther to the right, the more to the latter.
                
                
                    Experiment 3: Classifying human and animal nouns in wild animal stories, natural history, and novels
                    The computer had only a slightly harder time distinguishing between humans and animals in the fictional corpus than in Burroughs’ natural history texts, with median accuracies at 70% and 71% respectively. Comparatively, when fiction with no particular interest in non-human animals is added to the mix, the computer was able to distinguish humans from animals with a median classification accuracy of 82%. This discrepancy suggests that, contrary to the terms of the Nature Faker debate, both wild animal stories and natural history construct a semantic similarity between humans and animals.
                    
                        
                    
                    Figure 3. Classification accuracy rates to determine whether a noun is a human or an animal. An identical number of instances of human and animal words were sampled from each genre (8,800). All human and animal words with more than 10 instances were used, leaving anywhere from 97 to 145 words depending on the corpus. In one hundred runs per genre, 30 animal words and 30 human words were randomly sampled. These were classified for their species (animal vs. human) by a logistic regression, trained and tested according to a leave-one-out classification model.
                
            
        
        
            
                
                    Bibliography
                    
                        Heise, U.
                        (2016). 
                        Imagining Extinction: The Cultural Meanings of Endangered Species
                        . Chicago: University of Chicago Press, pp. 55-86.
                    
                    
                        Lutts, R.
                         (1990).
                        The Nature Fakers: Wildlife, Science, and Sentiment.
                         Fulcrum Publishing. Charlottesville, VA: University Press of Virginia.
                    
                    
                        Bamman, D. et al.
                        (2014). A Bayesian Mixed Effects Model of Literary Character.
                        Association for Computational Linguistics
                        . http://www.cs.cmu.edu/~ark/literaryCharacter/
                    
                    
                        Underwood, T. et al.
                         (2018). The Transformation of Gender in English-Language Fiction.
                        Cultural Analytics
                        .
                    
                
            
        
    
9869	2019	
        
            
                Introduction
                As part of a larger project in distant reading nineteenth-century British poetry, a method for detecting line-end rhymes was devised that utilizes rhyme dictionaries published in the eighteenth and nineteenth centuries. This method was proposed in order to account for historical debates about the definition of poetic rhymes in English as well as historical changes in pronunciation. This paper describes an evaluation of this approach that compares it to a method commonly used in computational analysis, which is based on the 
                    CMU Pronouncing Dictionary, in order to understand what significant differences occur.
                
            
            
                Historical context 
                Rhyme in English poetry is generally defined as the connection between two syllables “that have identical stressed vowels and subsequent phonemes but differ in initial consonant(s) if any are present” (Brogan and Cushman, 2012: 1184). Line-end rhyme is the most common use of rhyme, and it contributes to the structure and effect of particular poetic forms, like the sonnet and triolet, and to stanza patterns like the Spenserian stanza. 
                One syllable, or masculine rhymes, predominate in English poetry, as do “perfect” or exact rhymes, in which the vowel sounds are identical: cat/hat. Yet many poets have also used “imperfect” or near rhymes, in which the vowels are somewhat different: young/song. Literary critics in the nineteenth century frequently debated the rules for rhyme, either pointing to such examples as justification for a relaxed definition, or deriding them as bad poetry. 
                Alongside these debates, many different rhyme dictionaries were published in the nineteenth century, which offered critical definitions and examples of poetic rhyme, as well as lists of rhyme syllables and rhyming words in English. These dictionaries were aimed at would-be poets, students of poetry, and those wishing to improve their pronunciation of English. Rhyme dictionaries can thus serve as a data source for understanding both historical theories about rhyme and historical British pronunciation. 
            
            
                Rhyme detection with historical rhyme dictionaries
                In previous work, a method for rhyme detection using John Walker’s “Index of Perfect and Allowable Rhymes” was demonstrated. Each entry consists of a rhyme syllable, a list of words that end with that syllable, other perfect rhymes, and a list of allowable rhymes:
                 AM
                Am, dam, ham, pam, ram, sam, cram, dram, fam., sham, swam, epigram, anagram, &amp;c. Perfect rhymes, damn, lamb. Allowable rhymes dame, lame, &amp;c. (Walker, 1824: 642)
                A key-value table was created from these entries. The rhyme detection script uses the key-value pairs to identify perfect rhyme words and syllables first, followed by allowable rhyme words and syllables within the poem. Rhyme patterns are also visualized as a sequence of capital letters (ABABCDCD), as is standard in literary studies. 
                This method makes possible the detection of rhyme words, rhyme syllables, and rhyme patterns in large document sets. This method for computational historical poetics can compare different historical theories of rhyme as well as use them to evaluate rhyme usage in large document collections. This method also contributes to the study of rhyme’s effects on poetic vocabulary more generally in the nineteenth century.
            
            
                Rhyme detection with the 
                    CMU Pronouncing Dictionary
                
                The 
                    Carnegie Mellon University Pronouncing Dictionary is “an open-source machine-readable pronunciation dictionary for North American English that contains over 134,000 words and their pronunciations” (http://www.speech.cs.cmu.edu/cgi-bin/cmudict). It is widely used for a variety of language analysis tasks and is available through NLTK. 
                
                Several researchers have based their work on rhyme detection on the 
                    CMU Pronouncing Dictionary (Hirjee and Brown, 2010; Kao and Jurafsky, 2012; McCurdy, et al., 2015). The 
                    rhyme-plus package for node.js (https://www.npmjs.com/package/rhyme-plus) and the 
                    pronouncing package for Python (https: //pypi.org/project/pronouncing/) include functions for rhyme analysis based on the 
                    CMU Pronouncing Dictionary. This wide availability has made it standard for dictionary-based digital humanities work involving pronunciation. (It should be noted, however, that some other projects use speech transcription or speech synthesizer programs as an alternative to dictionary tables (Clement et al, 2013; Malmi et al, 2016).) 
                
            
            
                Evaluation
                In this evaluation project, rhyme detection using historical rhyme dictionaries is compared to rhyme detection using the 
                    CMU Pronouncing Dictionary. First, the rhyme syllable and word pairs from Walker’s rhyme dictionary are compared against the CMU dictionary to discover which rhyme word pairs are found in both dictionaries; which rhyme word pairs are found only in the CMU dictionary; and which rhyme word pairs are found only in Walker’s dictionary.
                
                Preliminary evaluation with a random sampling from Walker’s dictionary suggests that a significant proportion of historical rhymes labeled as perfect, as well as most of those labeled as “allowable” by Walker, are not discovered by using the CMU dictionary. Several reasons are suggested for this: pronunciation differences between American and British English; vocabulary differences between literary and general English; and vocabulary and pronunciation differences between nineteenth-century and contemporary English.
                A second phase of evaluation tests the historical dictionary method and the 
                    CMU Pronouncing Dictionary over a corpus of 1500 British poems published between 1800-1900 to evaluate how significant the differences between the dictionaries are for the analysis of a literary corpus. 
                
            
        
        
            
                
                    Bibliography
                    
                        Brogan, T. V. F. and Cushman, S. (2012). Rhyme. In Greene, R., et al. (eds), 
                        Princeton Encyclopedia of Poetry and Poetics. Princeton: Princeton University Press, pp. 1182-92.
                    
                    
                        Clement, T. et al. (2013). Sounding for Meaning: Using Theories of Knowledge Representation to Analyze Aural Patterns in Texts. 
                        DHQ, 7(1). http://www.digitalhumanities.org/dhq/vol/7/1/000146/000146.html 
                    
                    
                        Hirjee, H. and Brown, D. (2010). Using Automated Rhyme Detection to Characterize Rhyming Style in Rap Music. 
                        Empirical Musicology Review, 5(4): 121-45.
                    
                    
                        Kao, J. and Jurafsky, D. (2012). A Computational Analysis of Style, Affect, and Imagery in Contemporary Poetry. 
                        Workshop on Computational Linguistics for Literature. Montreal: ACL, pp. 8-17.
                    
                    
                        Malmi, E., et al. (2016). DopeLearning: A Computational Approach to Rap Lyrics Generation. 
                        Proceedings of Knowledge Discovery and Data Mining (KDD). San Francisco: ACM. http://dx.doi.org/10.1145/2939672.293967
                    
                    
                        McCurdy, N., et al. (2015). Rhymedesign: A Tool for Analyzing Sonic Devices in Poetry. 
                        Proceedings of NAACL-HLT Fourth Workshop on Computational Linguistics for Literature. Denver: ACL, pp. 12-22.
                    
                    
                        Walker, J. (1824). 
                        A Rhyming Dictionary; Answering, at the Same Time, the Purposes of Spelling and Pronouncing the English Language, on a Plan not Hitherto Attempted. New Edition. London: William Baynes and Son; Edinburgh: H. S. Baynes and Co.. Accessed in 
                        HathiTrust Digital Library:  http://hdl.handle.net/2027/hvd.hwpa6m
                    
                
            
        
    
9877	2019	
        
            
                Introduction
                Music Information Retrieval offers new options for musicological research, particularly for methodologies which are hard (or impossible) to carry out manually, e.g., large corpus studies and measurings of acoustical properties. One such field of application is the mining of patterns. Patterns – and repetitions in general – play an important role in nearly all music styles and are thus of interest for many sub-fields of musicology. In particular, melodic patterns – or ‘licks’ as patterns are often called in jazz parlance - form a crucial component of jazz improvisation (Berliner, 1994; Norgaard, 2014; Owens, 1974). Given the significance of patterns and licks in jazz, several research questions arise, concerning historical issues, e.g., the oral tradition of melodic patterns and the development of a typical jazz language, or more systematic issues, e.g., the psychology of the creative process, where patterns can be regarded as necessary to accomplish the highly virtuoso feat of modern jazz improvisation:
                
                    To what extent are patterns used to shape an improvisation?
                    When and by whom are patterns created and how are they transmitted between players over time (pattern archaeology)?
                    Does pattern usage change with time and styles?
                    Is there an influence of jazz education on pattern usage (e.g., by published pattern collections)?
                    How are patterns used to build phrases, e.g., to construct a typical bebop line?
                    Which role do external musical influences such as quotes, and signifying references play in jazz improvisation?
                
                This paper presents three novel user interfaces for investigating the pattern usage in monophonic jazz solos and exemplifies how these interfaces could be used for research on jazz improvisation.
            
            
                Related work
                Several web-based melody search engines have been developed in the past, e.g. Themefinder (
                    http://www.themefinder.org) which allows searching for patterns in both classical and folk music and 
                    Musipedia (
                    https://www.musipedia.org
                    ), a user-generated database of melodies, providing more sophisticated and user-friendly search interfaces like a virtual piano keyboard for entering melodic patterns and a query by tapping interface for rhythm search. A more thorough overview of search engines can be found in (Frieler et al., 2018). Unfortunately, many of these projects are discontinued or use out-dated web technology. An example for an up-to-date search interface including metadata filters is the RISM catalogue search (
                    https://opac.rism.info
                    ).
                
            
            
                User Interfaces
                The 
                    Pattern History Explorer is a Shiny application that allows exploring a set of 653 patterns that are most commonly used by eminent players (
                    ). The 
                    Pattern Search is a web interface for a general two-staged pattern search Martin2019-04-30T10:24:00
                    Nicht nur dort!featuring regular expressions (
                    https://dig-that-lick.hfm-
                    weimar.de/pattern_search). The 
                    Similarity Search faces the challenge of finding and grouping similar patterns, i.e., patterns that differ in one or several tones (
                    ). The applications aim on the one hand at an expert audience of jazz researchers to facilitate generating and testing hypotheses about patterns in jazz improvisation, and on the other hand at a wider audience of jazz teachers, students, and fans.
                
                The main goal of the 
                    Pattern History Explorer is to enable the exploration of interval patterns in jazz solos by providing information from many different angles. It maintains an overview of interval patterns frequently used by a selected subset of performers and traces their usage in the Weimar Jazz Database (Pfleiderer, 2017), allowing for the discovery of cross-artist and cross-temporal relationships. Currently, 653 interval patterns with 11,630 instances are included. The pattern corpus was created by mining for interval patterns in solos of eminent performers using the partition mode of the 
                    melpat module from the MeloSpySuite (Frieler, 2017). Subsequently, all instances of these patterns were searched for in the Weimar Jazz Database, and the results were included in the application. In general, the user of the Pattern History Explorer selects a certain interval pattern from the overall set of 653 patterns. Several options are available in order to filter the pattern set or to change the ordering of the patterns according to several criteria (e.g., filtering by performer, length, intrinsic characteristics such as Huron contour (Huron, 1996), tonality type, or tonal content). For the selected pattern, various kinds of information can be accessed in several tabs.
                
                While pre-computing a set of patterns is helpful for exploratory investigations, searching for instances of arbitrary patterns of any length and frequency of occurrence within a database requires a different approach. The web-based 
                    Pattern Search interface provides most of the functionality of the 
                    melpat search module of the MeloSpySuite while also extending it with audio and score snippets (both as isolated patterns and within their melodic context) for visual and aural inspection. To execute a basic search, the user can enter a pattern on a virtual keyboard or as a space or comma separated list of elements and choose a corresponding transformation, that is, a mathematical mapping of the basic melodic representation (for details, see Frieler, 2017). Currently, ten pitch-related transformations for primary search are offered (e.g., MIDI pitch, semitone intervals, extended chordal diatonic pitch class). An additional 18 transformations, such as duration, inter-onset interval classes and various structural markers, are supplied for the optional secondary search. Additionally, the search space can be constrained by several metadata categories, like performer, style, or recording year. Search patterns can be regular expressions (in a hybrid syntax depending on the selected transformation) which allows searches for variants in a single run. The secondary search can be used to refine the result space, e.g., by filtering out certain rhythmic or metrical configurations or by confining instances to a single phrase. The underlying search algorithm is built upon the basic Python regular expression module, which is fed with virtual Unicode strings constructed from the different melodic representations (transformations) with different alphabets. Scores are generated with the help of Lilypond, while audio snippets are directly extracted from the solo audio files using 
                    ffmpeg.
                
                
                    The latest addition to the web-based toolset is the 
                    Similarity Search
                    web application. Basically, the interface follows the design and concept of the 
                    Pattern Search
                    , however, with some significant differences. Not only identical pattern instances, but also similar patterns, that differ in one or more tones from the query, can be searched for. The calculation of search results differs from that of the 
                    Pattern Search
                    in that the actual similarity calculation is done using the underlying PostgreSQL database system implementation of the Levenshtein 
                    distance (edit distance). This distance measure has been shown in various studies (e.g., Frieler &amp; Müllensiefen, 2006; Grachten &amp; Arcos, 2004; Gulati, 2016) to be a good approximation to similarity judgements of melodies by human experts. The similarity search operates on a database of the complete set of pitch and interval n-grams of up to 20 elements that were previously extracted from the Weimar Jazz Database using the 
                    melpat
                    module. This amounts to about 4 million n-grams (instances) for MIDI pitch and 3.5 million for interval n-grams. By entering a pattern (n-gram) as a space or comma separated list of elements and choosing a transformation, similar n-grams can be retrieved from the database. To further control the result set, the search interface provides options for parameters such as ‘minimum similarity’ (calculated using the inverse of Levenshtein distance), ‘maximum length difference’ (allowing for n-grams of differing length), or the preservation of melodic contour and pitch range. All searches can also be refined using metadata filters for performers, instruments, etc.
                
            
            
                Visualizations
                Search results are presented in tabular form together with two graphical representations allowing for visual inspection – an n-gram network graph (Figure 1) and a timeline chart (Figure 2), both generated using the 
                    D3.js data visualization library (Data-Driven Documents, https://d3js.org/). In the case of the network graph utilizing a radial layout, n-grams can be grouped, e.g., by metadata attributes or same similarity value and n-gram class, resp.
                
                
                    
                        
                        Network graph for interval pattern -1,-2,-1,3,3,3,-1,-2 grouped by performer; the biggest bunches stand for those patterns played by Charlie Parker, Dizzy Gillespie, Sonny Rollins and Dexter Gordon. Node colours denote identical patterns which are connected by edges. Node size represents the degree of similarity, where bigger means more similar.
                    
                
                The timeline chart depicts when and by which performer pattern instances were played. Audio snippets for all n-grams found are generated and can be listened to by clicking on the nodes in both the network graph and the timeline chart as well as via the play buttons in the result table.
                
                    
                        
                        Timeline chart for interval pattern -1,-2,-1,3,3,3,-1,-2. Node colours denote identical patterns. Node size represents the degree of similarity, where bigger means more similar.
                    
                
            
            
                Discussion
                The three applications are already usable interfaces for the Weimar Jazz Database and serve as prototypes for applications to explore large databases, which are going to be automatically extracted from large collections of jazz recordings, as aimed at within the transatlantic research project “Dig That Lick: Analysing Large-Scale Data for Melodic Patterns in Jazz Performances” (DTL). All three tools can be primarily viewed as bespoke interfaces for the specific needs of jazz researchers, but could easily be transferred to other melodic corpora, too. They could also be of interest to teachers, students and music fans as well as to training courses in computational music analysis.
            
        
        
            
                
                    Bibliography
                    
                        Berliner, P. F. (1994). 
                        Thinking in Jazz. The Infinite Art of Improvisation. Chicago: University of Chicago Press.
                    
                    
                        Frieler, K. (2017). Computational melody analysis. In M. Pfleiderer, K. Frieler, J. Abeßer, W.-G. Zaddach, &amp; B. Burkhart (Eds.), 
                        Inside the Jazzomat. 
                        New Perspectives for Jazz Research. Mainz: Schott-Campus, pp. 41–84. http://schott-campus.com/jazzomat/
                    
                    
                        Frieler, K., Höger, F., Pfleiderer, M. and Dixon, S. (2018). Two web applications for exploring melodic patterns in jazz solos. 
                        Proceedings of the 19th International Society for Music Information Retrieval Conference (ISMIR), Paris 2018. Paris. http://www.mu-on.org/frieler/docs/frieler_et_al_2018_two-web-applications.pdf
                    
                    
                        Frieler, K., &amp; Müllensiefen, D. (2006). Evaluation of approaches to measuring melodic similarity. In A. Batagelij, V., Bock, H.-H., Ferligoj, A., Ziberna (Ed.), 
                        Data Science and Classification. Berlin: Springer.
                    
                    
                        Grachten, M., &amp; Arcos, J. L. (2004). Using the Implication/Realization Model for Measuring Melodic Similarity. In 
                        Proceedings of the 16th European Conference on Artificial Intelligence, ECAI04. IOS Press.
                    
                    
                        Gulati, S. (2016). 
                        Computational Approaches for Melodic Description in Indian Art Music Corpora (PhD thesis). Universitat Pompeu Fabra, Barcelona.
                    
                    
                        Huron, D. (1996). The Melodic Arch in Western Folksongs. 
                        Computing in Musicology, 
                        10, 3–23.
                    
                    
                        Norgaard, M. (2014). How Jazz Musicians Improvise: The Central Role of Auditory and Motor Patterns. 
                        Music Perception: An Interdisciplinary Journal, 
                        31(3), 271–287. https://doi.org/10.1525/mp.2014.31.3.271
                    
                    
                        Owens, T. (1974). 
                        Charlie Parker. Techniques of Improvisation. University of California, Los Angeles.
                    
                    
                        Pfleiderer, M. (2017). The Weimar Jazz Database. In Pfleiderer, M., Frieler, K., Abeßer, J., Zaddach, W.-G. and Burkhart, B. (eds), 
                        Inside the Jazzomat. 
                        New Perspectives for Jazz Research. Mainz: Schott-Campus. http://schott-campus.com/jazzomat/
                    
                
            
        
    
9897	2019	
        
            Research Background and Question
            
                How did the everyday life of upper-class elite men look like in 17
                th
                century Korea? How did they establish their position and relationships under Confucian norms, which was the dominant ideology of society at the time? How did they navigate these norms? The current image held by scholars of upper-class elite men of Chosǒn is that of antiquated and solemn scholars who strictly followed "Confucian ideology." However, their diaries show us a diversion from these images.
            
            This research aims to provide a new understanding of the past by analyzing a diary from the Chosǒn period through DH methodology. Most of the diaries of upper-class elite men were written in Chinese characters and were not translated into Korean, allowing only professional researchers to read them. However, this research will enable public users and scholars from various disciplines to access and analyze this text by first translating the original cursive Chinese Characters into Korean, and then transforming it into Digital data. 
            
                This research attempts to bridge historical studies and computational methods, increase the depth of the analysis of these types of historical materials, and expand the analytical horizons of the text. By restructuring the real-life diary through text analysis and digitalizing its contents, this research also attempts to open up the possibility to engage with the diary using humanities methodologies.
            
            Material
            
                This research analyzes the text of 
                Jiamilgi
                (支菴日記, 1692-1699), an 8-year-long diary written by a man named Yun Ihu, who lived on the southern coast of Chosǒn during the 17
                th
                 Century. The diary is not only a record of his life, but also of his relationships with family, relatives, and others. Yun Ihu included not only details about his emotions, but also his travels and visits to see relatives; his consumption of medicines and foods; his involvement in land reclamation projects; his property management and construction; and his connections to historical events such as political party strife in the central government.
            
            Analysis and the Method of Organizing the Digital Contents
            
                The goals of this project are to digitalize the 
                Jiamilgi
                , implement a visual navigation interface, and create the semantic connections among the digital archives of this Chosǒn historical text. To achieve these goals, we designed a DH methodology, which consists of 8 major steps, including text translation; content structure examination; noun and verb word extraction; concept, class and property, and semantic class relationship definitions, knowledge base construction; and visual navigation interface implementation. 
            
            
                Firstly, the original text of 
                Jiamilgi
                (written in the cursive style Chinese characters) will be translated into Korean and then constructed into digital contents, which will be made public on the web as “Jiam Wiki”. Both the original and translated texts will be digitally restructured by date sequence. 
            
            
                Secondly, through the content analysis of the original 
                Jiamilgi
                 text, the conceptual elements (person, event, place, slave, literature, object, terms, weather, bibliography, reference, emotion, etc.) will be extracted and based on these, a semantic class model will be constructed and a multilateral data analysis attempted. Our class model will be called the Jiam Ontology, and conceptualize the extracted elements and establish the relationships between these as well as between elements and outside resources such as images, webpages, and ontologies of the subject history. 
            
            
                Thirdly, a visualization and navigation system of the Jiam Ontology will be created by using various digital technologies including Graph Database (Neo4j), Python visual libraries, and D3 JavaScript Library. Fourthly, the instance data construction of elements, such as actor-centered places, objects, persons, actions, emotions, and their interconnected analysis, will recreate a historical depiction of everyday life during the time; these will include, travel routes, the contents of letters, the use of gifts, and the roles slaves played in society. Finally, we will attempt to make a textual interpretation to investigate how the frequency of events or the distribution of time intervals can be applied to Complex Systems Theory. In addition, a quantitative analysis of the events, persons, places, and so forth appearing in the daily records of the diary will be performed. Through these attempts, the project will expand the scope of digital humanities research to human dynamics and Complex Systems Theory.
            
            Impact of the Research
            
                The digital text of 
                Jiamilgi
                 armed with ontology specification (RDF/OWL), visualization, quantitative analysis, and general interpretation, will offer a new research methodology as a case study of a DH-based diary research, and at the same time, will demonstrate the expandability of existing historical research on the Chosǒn period. By making the results of our work public on the web, we intend to increase the openness of DH researches and promote cooperative research internationally.
            
        
    
9929	2019	
        
            Our research is concerned with the dissemination and transformation of scientific knowledge across Europe. The basis of our investigations forms a corpus of, to date, 343 books that have been printed between 1472 and 1650. We assembled the corpus around a specific text: the 
                Tractatus de Sphaera by Johannes de Sacrobosco. This 13th century treatise on cosmology describes the spheres of the universe according to the geocentric worldview. Up until the 17th century it has been repeatedly published as part of university textbooks. In these the treatise is included in original, commented or translated form, and accompanied by other texts that were seen as relevant for the study of cosmology from disciplines such as medicine, astronomy or mathematics. As many of these textbooks were part of the mandatory curriculum at European universities, we regard their contents as representative for the scientific knowledge that was being taught and seen as relevant at the time of publication of the books.
            
            We extract several markers from the individual books that form the material evidence of our research. In addition to bibliographic data such as publishers, printers, date and place of publication, etc., we identified for every book the content structure: which texts it contains and, if applicable, wether the texts are commented or translated versions of existing texts. In doing so we can not only identify how the content of the books changed and – by extension – how certain disciplines gained and lost in importance, but also which publishers might be responsible for certain changes.
            The books also contain various types of visuals: diagrams, illustrations, decorative elements, etc. In the same way as texts, these visuals can offer insights into the kind of knowledge that is being distributed. By identifying and analyzing recurring images, we can evaluate the 'success' of certain imagery. If we find the same images being used by different printers, for example, that might be telling of one printer being influenced by another, or even indicate a physical exchange of wooden printing blocks.
            In this paper we present our approach for analyzing the more than 16.000 illustrations that we have annotated in our corpus. We employ an image hashing algorithm for identifying recurring images and existing visualization tools for analyzing the results. As the algorithm we use is independent of the visual material and – unlike machine learning algorithms – does not need to be trained, it can readily be used on arbitrary image collections. As part of this paper we will offer the entire analysis and visualization workflow for others to reuse.
            
                
                    
                
            
            
                The algorithm organises the images into groups of same or similar ones. While most of the groupings are correct, some diagrams may incorrectly be assigned the same group (e.g. third row from the top)
            
            
                Process
                The images we analyses are being manually annotated by a team of student assistants using the Mirador IIIF viewer and classified as either Content Illustrations, Initials, Frontispieces, Printer's Marks, Title Page Illustrations or Decorations. They are stored in RDF as annotations on the digitised pages of the books, along with the remaining metadata that we gather in the project and store according to a CIDOC-CRM data model in a Blazegraph database. For processing, the respective regions of the original pages are downloaded to a local machine via a IIIF API.
                We want to identify which of the illustrations and diagrams appear several times in our corpus of books. In other words, we want to organise the total set of images into groups that are duplicates or near duplicates of each other. Duplicate detection in a set of digital images can be achieved through an Image Hashing algorithm, as proposed by Venkatesan et al.(2000). A hash function takes an arbitrary sized input and deterministically produces an output of a fixed size, the so-called 
                    digest. For an introduction to hash functions see Knuth (1998). In order to identify images that are not duplicates but variations of each other, a particular type of image hashing algorithm is required. A perceptual image algorithm (Zauner, 2010) is designed to take an arbitrary image as input and produce a digest that bears a deterministic relationship to the input image. We use the difference hash, or dHash, algorithm (Kravetz, 2013) in an implementation for the Python programming language (Buchner, 2017). The algorithm works by scaling down and converting the input image to grayscale, and then produce a digest based on each pixel's difference in brightness to its neighboring pixels. The similarity between two images can then be expressed as the difference – the Hamming distance (Hamming, 1950) – between two digests.
                
                For analyzing the output of the algorithm we make use of a tool initially developed to visualize a collection of coins (Gortana et al., 2018). The web app, which is freely available on GitHub, allows us to visually inspect the entire set of images and evaluate quality of the identified groupings.
            
            
                Results
                After experimenting with different values for the threshold and rescaling resolution we found the ones that are optimal for our particular image collection, resulting in match found for 66% of the images in our set. We use a two-pass process in which the images that have not been matched in the first pass are processed again using adjusted settings.
                The resulting image groups have given us immediate new insights into the reuse of images within this particular corpus of publications. We were able to discriminate clear patterns in the use of certain images, such as, which images have been published throughout the print history of our corpus – there are two – as well as which images have changed context from one text to the other – only one.
                In contrast to more 'sophisticated' methods of image analysis using pre-trained neural networks, the method used works in a transparent fashion and independent of the material at hand. Only two parameters need to be adjusted – threshold and scaled image resolution – to optimise the output for a given image collection. The algorithm as well as the visualisation tool employed here can be applied on any given image collection.
                
                    
                        
                    
                
                
                    Visualizing the identified image groups against date of publication reveals patterns in the use of certain visuals. The large groups of images at the top represents those that have not been assigned a group. Visualization tool: Coins by Flavio Gortana, 
                    
                
            
        
        
            
                
                    Bibliography
                    Buchner, J. (2017). imagehash. online Available at: https://github.com/JohannesBuchner/imagehash Accessed 6 Aug. 2018.
                    Gortana, F., Tenspolde, von, F., Guhlmann, D. and Dörk, M., (2018). Off the Grid: Visualizing a Numismatic Collection as Dynamic Piles and Streams. Open Library of Humanities, 4(2), pp.1245–26.
                    Hamming, R.W. (1950). Error Detecting and Error Correcting Codes. Bell System Technical Journal, 29(2), pp.147–160.
                    Knuth, D. (1998). The art of computer programming, Volume 3: Sorting and searching. Upper Saddle River, NJ: Addison Wesley.
                    Kravetz, N. (2013). Kind of Like That. Online. hackerfactor.com. Available at: http://www.hackerfactor.com/blog/index.php?/archives/529-Kind-of-Like-That.html Accessed 6 Aug. 2018.
                    Venkatesan, R., Koom, S.M., Jakubowski, M. and Moulin, P. (2000). Robust image hashing. In: Proceedings of IEEE ICIP. Vancover: IEEE.
                    Zauner, C. (2010). Implementation and Benchmarking of Perceptual Image Hash Functions. Master's thesis, Upper Austria University of Applied Sciences, Hagenberg Campus, pp.1–107.
                
            
        
    
9939	2019	
        
            
                Introduction
                Since it was first printed, the translation of Shakespeare's plays edited by August Wilhelm Schlegel and Ludwig Tieck has been re-edited many times (Reimer, 1832). A major reference in the first half of the 19th century, it is still regarded as a groundbreaking translation and referred to today. While there is little doubt that Schlegel translated the first edited plays, L. Tieck did not work out the edition of the final volumes by himself, but delegated the main translation work to his daughter Dorothea Tieck and Wolf Graf von Baudissin (Baillot, 2008; Paulin, 1998).
                This paper investigates the contribution of the three actors involved in this joint translation project. Machine Learning methods are used to analyse the plays and translations in order to gain quantitative insights into what may seem a peculiar authorship setting but was quite usual in the context of the 19th century. The method proposed here is hence likely to improve our understanding of co-creation conditions in the 19th century at large.
                Stylometric investigations of collaborative translations to identify translators has already been analyzed by Rybicki and Heydel (Rybicki and Heydel, 2013), who could show that Burrows’ delta features were able to distinguish between the different translators of novels by Virginia Woolf into Polish.
                Based on D. Tieck's statement of the repartition of the plays we start with the setting shown in Figure 1 (Uechtritz, Erinnerungen. Sybel, 1884). Since the manuscript of the raw translation is now lost, the sole material this paper can base its analysis on is the Shakespeare edition and the first German edition. We have no material traces allowing to easily discriminate between what D. Tieck translated, what W. Baudissin translated, and what L. Tieck corrected in the translations. We investigate two questions: firstly, the goal consists in defining the roles and tasks of the three translation partners, especially for scenes where D. Tieck and W. Baudissin collaborated. The second point of interest is to shed light on the cooperation mode between father and daughter Tieck (respective contributions and intervention scope, collaboration issues).
                In contrast to authorship attribution, translators are aiming at preserving the style of the original text – the traces of the translators should therefore be even harder to identify. This paper presents a novel approach to use methods such as Burrows’ delta in the 
                    multilingual context, to compare translation styles and attribute translators.
                
                
                    
                    Plays were written by Shakespeare and were translated by either D. Tieck or W. Baudissin. In some plays they collaborated. All translation drafts were then discussed in common, including L. Tieck.
                
            
            
                Method
                The first two experiments deal with the question of the individual translation properties of D. Tieck and W. Baudissin, while the third experiment assesses the question of L. Tieck's contribution. The data layout and the analysis steps of all experiments are shown in Figure 2. The English corpus is retrieved from 
                    First Folio (Shakespeare, 1623), for the German corpus, TextGrid (TextGrid, 2018) was used. Throughout the experiments, spacy (Honnibal and Montani, 2017) for preprocessing and pyphen for syllable counts are used.
                
                In the first experiment, solely based on the German material, translation-stylistic characteristics are to be found that discriminate the translator. In addition to Nearest Neighbors on Burrows’ delta (Burrows, 2002; Argamon, 2008) that was used by Rybicki (Rybicki and Heydel, 2013), Bag-of-N-Gram features and also pre-trained word vectors using the Fasttext model (Grave et al., 2018) were used and classified by a Support Vector Machine with RBF kernels (Cortes and Vapnik, 1995; Müller et al., 2001). Cross validation was used to find good hyper parameters using sk-learn (Pedregosa et al., 2011).
                In the second experiment, we use the trained classifiers of Experiment 1 on the collaborative works of D. Tieck and W. Baudissin. We compute the predicted class of each scene individually and try to examine who the major translator of each part of the translation was. This explorative experiment enables us to concentrate on scenes for which the classifiers tend to agree, which we then manually evaluate.
                In Experiment 3, cross-language features are compared with respect to its translator. As shown in Figure 2, the first step for analysing the translation is to map the corresponding scenes, to be able to identify deviations on scene level. During the translation process, the scene boundaries were not always preserved and in order to compare intervals of the same contents, an automatic mapping of scenes is performed. Afterwards, two different features on scene level, namely the richness (a) and the number of syllables per line (b), and Burrows’ delta (c) on play level are compared.
                
                    
                    Data layout for all three experiment settings. The first experiment evaluates the possibility of classifying translators based on textual features of the translation. Experiment 2 explores the unknown parts of the corpus with the trained classifiers of Experiment 1. Experiment 3 parallelizes the corpus of the English and the German version and investigates the influence of each of the collaborators.
                
            
            
                Results
                
                    Experiment 1: Classify translator scenes in validation set
                    As shown in Table 1, the individual classifiers on scene level show decent performance. Burrow's delta, however, does not show convincing results. For further improvement, we combined the classifiers by filtering scenes for which all scene-classifiers agree. This results in a smaller test set (57 scenes) but also in a dramatic performance boost. For this subset of the test set, our combined classifier is on average performing with a precision and recall of ≈.93. Overall, the classifiers perform better in identifying scenes by W. Baudissin.
                    
                        
                            Table 1: Scores on held-out test set for various features and groupings. For classification of N-Gram features and Word Vectors, an SVM with RBF Kernel has been used. The Support row denotes the number of scenes in the respective class. Parameters have been optimized using grid search and 5-fold cross validation. For Burrows’ delta, a Nearest Neighbors Classifier has been used. The optimal number of features for the delta has been cross validated.
                        
                        
                            Method
                            
                            Burrows’ Delta
                            Word N-Grams
                            Char N-Grams
                            Word Vectors
                            Combined Classifiers
                        
                        
                            Grouping
                            
                            Play
                            Scene
                            Scene
                            Scene
                            Scene
                        
                        
                            D. Tieck
                            F1
                            .5000
                            0.6216
                            0.6486
                            0.7952
                            0.8947
                        
                        
                            
                            Precision
                            .5000
                            0.6765
                            0.7059
                            0.7674
                            0.9444
                        
                        
                            
                            Recall
                            .5000
                            0.5750
                            0.6000 
                            0.8250
                            0.8500
                        
                        
                            
                            Support
                            2
                            40
                            40
                            40
                            20
                        
                        
                            W. Baudissin
                            F1
                            0.6667
                            0.7705
                            0.7869
                            0.8496
                            0.9474
                        
                        
                            
                            Precision
                            0.6667
                            0.7344
                            0.7500 
                            0.8727
                            0.9231
                        
                        
                            
                            Recall
                            0.6667
                            0.8103
                            0.8276
                            0.8276
                            0.9730
                        
                        
                            
                            Support
                            3
                            58
                            58
                            58
                            37
                        
                        
                            Weighted average
                            F1
                            0.6000
                            0.7097
                            0.7305 
                            0.8274
                            0.9289
                        
                        
                            
                            Precision
                            0.6000
                             0.7105
                            0.7320
                            0.8298
                            0.9306
                        
                        
                            
                            Recall
                            0.6000
                            0.7143
                            0.7347
                            0.8265
                            0.9298
                        
                        
                            
                            Support
                            5
                            98
                            98
                            98
                            57
                        
                    
                
                
                    Experiment 2: Classify translator scenes in the collaboration set
                    In Figure 3, the translator attribution for the collaborative scenes are shown. Additionally, we exploit the finding of Experiment 1 that our classifiers performance is boosted when they are combined. In 
                        Viel Lärmen um nichts (
                        Much adoe about Nothing), fourth act, first scene the highest agreement for D. Tieck, in 
                        Der Widerspenstigen Zähmung (
                        The Taming of the Shrew) first act, second scene the highest agreement for Baudissin is observed. As it turns out, the two scenes are exceptionally long scenes with 302 and 264 speeches respectively, although the mean number of speeches per scene over the whole German corpus is only ≈118.7. The length of the scene may give the classifiers more features to distinguish the translators.
                    
                    The scene from 
                        The Taming of the Shrew alternates between Verses and Prose which may have given the translator the chance to underline their characteristic style. The scene from 
                        Much adoe about Nothing has a much more coherent rhythm which possibly fits D. Tieck's translation style better.
                    
                    
                        
                        This figure shows the average score of all scene-level classifiers of Experiment 1 to attribute each scene to D. Tieck or W. Baudissin for the two plays in which they collaborated.
                    
                
                
                    Experiment 3: Identify Contribution of Ludwig Tieck
                    In Figure 4, the results of the cross-language comparison are shown. Points in all panels that are close to the diagonal do not deviate across language. The richness (a) of the scenes stay very close to the diagonal, however the majority of points is slightly below the diagonal. The original is slightly “richer” in the sense of our measurement than the translation, but there is no difference across translators.
                    The median syllables per line (b) of the translation deviates quite significantly in that the German version often uses more syllables per line than the English version. D. Tieck stated in her letter that she also translated Sonnets even in a play that was otherwise translated by W. Baudissin. Because of this statement we originally expected D. Tieck to follow the number of syllables of the original more strictly. This expectation is also in line with the findings of Experiment 2 where most classifiers agree on D. Tieck as the translator in a scene with a coherent rhythm. However, the findings of (b) cannot verify this hypothesis, because the deviation exists for both translators.
                    In (c), the points visualize Burrow's delta between the two plays in English, the vertical position is the Burrow's delta of the respective pair in German. 
                    Each data point for which both plays are translated by the same person is color-coded accordingly (grey otherwise). Interestingly, the green points are almost exclusively below the diagonal, with only a few exceptions for plays that already exhibit a small delta in the English version. This indicates translations by D. Tieck move closer to each other and thus may incorporate a more consistent style.
                    
                        
                        Three different features that compare original texts and their translations across languages. For each panel, the horizontal axis corresponds to the original version in English, the vertical axis corresponds to the German translation. The richness feature (a) shows little deviation in both languages. The Syllables per line feature (b) shows deviation in the translation for both translators and the Burrow's feature (c) shows deviation especially for one translator: D. Tieck (Green). For (b) gaussian noise (with std. of .2) was added to the points to visualize overlapping points. Also, in (b), a few outliers are not visualized. The points in (c) are grey if both plays were not translated by the same person.
                    
                
            
            
                Conclusion
                We proposed an ensemble of translator attribution methods that result in a very high performance on scenes where they agree (Experiment 1). We show a significant improvement over state-of-the-art methods for translator attribution. This combination of classifiers is used to suggest translators for scenes where the true translator is unknown. 
                A close reading of the scenes revealed distinct characteristics that could explain the decision of the classifiers (Experiment 2). We thus argue that this method likely found scenes where the majority of translation work can be attributed to the proposed translator.
                A novel approach of comparing the material in the source language and the translations yield the result that D. Tieck has a more distinct style in her translations (Experiment 3, c). With regard to the daughter-father relationship this can be seen as a literary independence from her father.
                Also, it could be observed that there is a translation system on which the three collaborators agree (Experiment 3, a and b). In that, we identified candidate features that could signal a contribution of L. Tieck. 
                For further analysis we plan to include original plays by L. Tieck in order to identify distinct characteristics that further narrow down his contribution to the translation. We also plan to include additional cross-language features that characterize a distinct style of W. Baudissin.
            
        
        
            
                
                    Bibliography
                    
                        Argamon, S. (2008). Interpreting Burrows’s Delta: Geometric and probabilistic foundations. 
                        Literary and Linguistic Computing, 
                        23(2): 131–147.
                    
                    
                        Baillot, A. (2008). ‘Ein Freund hier würde diese Arbeit unter meiner Beihülfe übernehmen’. Die Arbeit Dorothea Tiecks an den Übersetzungen ihres Vaters. (Ed.) Wehinger, B. &amp; Brown, H. 
                        Übersetzungskultur Im 18. Jahrhundert: 187–206.
                    
                    
                        Burrows, J. (2002). ‘Delta’: a measure of stylistic difference and a guide to likely authorship. 
                        Literary and Linguistic Computing, 
                        17(3): 267–287.
                    
                    
                        Cortes, C. and Vapnik, V. (1995). Support-vector networks. 
                        Machine Learning, 
                        20(3): 273–297 doi:10.1007/BF00994018.
                    
                    
                        Grave, E., Bojanowski, P., Gupta, P., Joulin, A. and Mikolov, T. (2018). Learning Word Vectors for 157 Languages. 
                        Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018).
                    
                    
                        Honnibal, M. and Montani, I. (2017). spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. 
                        To Appear.
                    
                    
                        Müller, K.-R., Mika, S., Ratsch, G., Tsuda, K. and Schölkopf, B. (2001). An introduction to kernel-based learning algorithms. 
                        IEEE Transactions on Neural Networks, 
                        12(2): 181–201.
                    
                    
                        Paulin, R. (1998). Luise Gottsched und Dorothea Tieck. Vom Schicksal zweier Übersetzerinnen. 
                        Shakespeare-Jahrbuch(134): 108–122.
                    
                    
                        Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., et al. (2011). Scikit-learn: Machine Learning in Python. 
                        Journal of Machine Learning Research, 
                        12: 2825–2830.
                    
                    
                        Reimer, G. A. (ed). (1832). 
                        Shakspeare’s Dramatische Werke. Übersetzt von August Wilhelm Schlegel. Ergänzt Und Erläutert von Ludwig Tieck. 1. Auflage. Berlin.
                    
                    
                        Rybicki, J. and Heydel, M. (2013). The stylistics and stylometry of collaborative translation: Woolf’s Night and Day in Polish. 
                        Literary and Linguistic Computing, 
                        28(4): 708–17 doi:10.1093/llc/fqt027.
                    
                    
                        Shakespeare (1623). 
                        First Folio Home Page. Digital Facsimile of the Bodleian First Folio of Shakespeare’s Plays. . Vol. Arch. G c.7 http://firstfolio.bodleian.ox.ac.uk/.
                    
                    
                        Sybel, H. (ed). (1884). 
                        Erinnerungen an Friedrich von Uechtritz Und Seine Zeit in Briefen von Ihm Und an Ihn. Mit Einem Vorwort von Heinrich von Sybel. Salomon Hirzel Verlag.
                    
                    
                        TextGrid Consortium (ed). (2018). 
                        TextGrid: A Virtual Research Environment for the Humanities. Göttingen: TextGrid Consortium textgrid.de.
                    
                
            
        
    
9961	2019	
        
            
                Introduction: Reading and the materiality of the book
                The late eighteenth century entailed a rapid change in reading and writing books. Rolf Engelsing (1970; 1974) famously suggested a reading revolution took place in the second half of the century in which the public gradually switched from reading a few key works, such as the Bible, intensively and repeatedly, to reading extensive amounts of literature by different authors. The breakthrough of extensive reading and the ensuing exposure to a broader spectrum of ideas is linked to the political mobilization of people, but assessing these links requires better knowledge of when and where reading habits changed. Information about changes in reading habits is available only through individual statements about changes in reading. To trace changing practices of reading, we have focused on one aspect that facilitated a transformation in reading, that is, the changes in the material outlook of books produced in the early modern period. Earlier research has suggested that smaller book formats, in particular the octavo format, became more popular in the eighteenth century and thus coincide with the changes in reading habits (Buringh and van Zanden, 2009; Horstbøll, 2009; 2010; Galbi, 2011; for formats in general, see Gaskell, 1972). 
                Smaller formats cannot be assumed to be a direct result of changes in reading habits or vice versa, but the size of books does make a difference in how people read. Smaller books could be easily transported, carried in a pocket to places where individuals could read in solitude. Larger books were more appropriate for reading out loud to an audience by a desk or from a piedestal. To properly assess the change in the material dimensions of books and other print, we turned to bibliographies as a large-scale data source for early modern publishing.
            
            
                Materials and methods
                Our statistical analyses are based on four large bibliographies, which allows for more reliable and more detailed studies on the change in book formats than what has been possible before. While bibliographies have been compiled to provide as good coverage of the publication record as possible and thus provide an undervalued data source for the exploration of print culture, they have so far been used only to a limited degree. Bibliographic information, such as authors, titles, publishers, languages, publication places, publication years, and book formats, have been used to conduct analyses (for a description see Tolonen et al., 2018; Lahti et al., 2019). To deal with gaps, inconsistencies, bias and ambiguities in the data, we have extensively harmonized selected metadata fields of the Finnish and Swedish National Bibliographies (FNB and SNB, respectively), the English Short-Title Catalogue (ESTC), and the Heritage of the Printed Book database (HPBD) which is a compilation of 45 smaller, mostly national, bibliographies. Data handling was mainly carried out in R and Python using dozens of data science packages. Altogether, these bibliographies cover 2.64 million harmonized entries from the investigated period. In terms of coverage, the ESTC, the SNB and the FNB provide the best possible dataset covering printed material for the early modern period in Britain, Sweden and Finland. The HPBD has a larger geographical scope as it covers most of Europe, but its level of coverage varies depending on the source bibliography.
            
            
                Results and discussion
                A statistical analysis of changes in book formats, their shares in titles and in paper consumption, shows clearly how the octavo format became more popular in Europe toward the end of the eighteenth century, but also indicates that the development was uneven in the sense that the timing and speed of the development varied according to location. The Swedish case (SNB) shows a clear rise in the production of octavo books in the second half of the eighteenth century and a decline of the larger quarto format. In the British case (ESTC), a similar trend is slightly earlier, but there is also an increase in the production of the smaller duodecimo format, indicating an overall shift towards smaller books. In the Finnish case the trend is only visible in the nineteenth century, indicating a slower development in the European periphery. Despite unevenness in the data, the HPBD shows a similar trend for the whole of Europe, but zooming in on individual cities shows remarkable regional differences – for example, most German cities show a growing trend, whereas Spanish cities have a much stronger presence of larger book formats during the century. Overall, capital cities, commercial centres and university towns tend to have slightly different profiles (Lahti et al., 2019). 
            
        
        
            
                
                    Bibliography
                    
                        Buringh E. and J. L. van Zanden.
                         (2009). Charting the “rise of the West”: Manuscripts and printed books in Europe, a long-term perspective from the sixth through eighteenth centuries. 
                        Journal of Economic History
                        . 69 (2): 409–445.
                    
                    
                        Engelsing, R.
                         (1970). Die Perioden der Lesergeschichte in der Neuzeit: Das statistische Ausmass und die soziokulturelle Bedeutung der Lektüre. 
                        Archiv für Geschichte des Buchwesens
                         10: 944–1002.
                    
                    
                        Engelsing, R.
                         (1974). 
                        Der Burger als Leser: Lesergeschichte in Deutschland, 1500–1800
                        . Stuttgart: Metzler.
                    
                    
                        Galbi, D.
                         (2011). Books Shifted to Smaller Formats across Centuries of Print. 
                        Purple Motes. A Journal of Whimsy and Hope
                        , Accessed February 10, 2017,
                        
                             http://www.purplemotes.net/2011/08/21/books-shifted-to-smaller-formats-across-centuries-of-print/
                        
                        .
                    
                    
                        Gaskell, P.
                         (1972). A new introduction to bibliography. Oxford: Clarendon Press.
                    
                    
                        Horstbøll, H.
                         (1999).
                        Menigmands medie. Det folkelige bogtryck i Danmark 1500–1840
                        . Copenhagen: Det Kongelige bibliotek &amp; Museum Tuscalanums Forlag.
                    
                    
                        Horstbøll, H.
                         (2009). In octavo. Formater, form og indhold på det populære litterære marked i 1700-tallets Danmark. In 
                        Bokens materialitet. Bokhistoria och bibliografi
                        , ed. M. Malm, B. Ståhle Sjönell, and P. Söderlund. Stockholm: Svenska vitterhetssamfundet.
                    
                    
                        Lahti, L., Marjanen, J., Roivainen, H. and Tolonen, M.
                         (2019). Bibliographic Data Science and the History of the Book (c. 1500–1800). 
                        Cataloguing &amp; Classification Quarterly 
                        57, 5–23. 
                        
                            https://doi.org/10.1080/01639374.2018.1543747
                        
                    
                    
                        Tanselle, G. T.
                         (1977). ‘Descriptive Bibliography and Library Cataloguing’, 
                        Studies in Bibliography
                        , 30: 1-56.
                    
                    
                        Tolonen, M., Lahti, L., Roivainen, H., and Marjanen, J.
                         (2018). A Quantitative Approach to Book-Printing in Sweden and Finland, 1640–1828. 
                        Historical Methods: A Journal of Quantitative and Interdisciplinary History
                        52, 57–78. 
                        
                            https://doi.org/10.1080/01615440.2018.1526657
                        
                    
                
            
        
    
9969	2019	
        
            
                Introduction
                As researchers in the digital humanities we have been successful in building online components for our work. However, we have failed in making it a priority to devise a plan to gracefully discard our online components once we no longer need them. Thus, many of the online projects in the digital humanities have an implied planned obsolesce —which means that they will degrade over time.
                Previous work presented in Digital Humanities 2017 and 2018 has explored the abandonment, and the average lifespan, of online projects in the digital humanities using metadata from HTTP response headers 
                    (Meneses and Furuta, 2017) and contrasted how things have changed over the course of a year 
                    (Meneses et al., 2018). We believe that managing and characterizing the degradation of online digital humanities projects is a complex problem that demands further analysis because the methods for identifying change in the Web do not fully apply; and the end of life for a digital humanities project may or may not be indicated by updates in its content and tools. 
                
                In this sense “abandonment” is not necessarily a sufficient designation —as there are different nuances involved. We have seen many cases of successful projects in digital humanities that are shifting their focus from active development to data management (for example: http://cervantes.tamu.edu and http://botany.csdl.tamu.edu/). These are cases where a project’s online presence has not received updates for some time but its online tools are stable and continue to be accessed by its users. In this case, the lack of updates and new content is not a signal of abandonment. These are examples of why the rules for traditional resources do not fully apply and new metrics are needed to identify issues concerning online projects in the digital humanities.
                In this abstract, we go one step further into exploring the collectively shared distinctive signs of abandonment to quantify the planned obsolesce of online digital humanities projects. For this purpose, we have created a framework that collectively quantifies their signs of abandonment. This study aims to answer three questions. First, can we systematically identify the signals of abandoned projects using computational methods? Second, can the degree of abandonment be quantified? And third, what features are more relevant than others when identifying instances of abandonment?
            
            
                Methodology
                A complete listing of research projects in the Digital Humanities does not exist. However, the Alliance of Digital Humanities Organizations publishes a Book of Abstracts after each Digital Humanities conference as a PDF. Each one of these volumes can be treated as a compendium of the research that is carried out in the field. To create a dataset, we downloaded the Books of Abstracts from 2006 to 2018. Then we proceeded to extract the text from these documents using Apache Tika 
                    (Apache Software Foundation, 2018) and parse the unique URLs for each Web resource using regular expressions. 
                
                Then we periodically created a set of WARC files 
                    (International Organization for Standardization, 2017) for each resource using Wget 
                    (Free Software Foundation, 2018). The WARC files are systematically processed and analyzed using Python 
                    (van Rossum, 1995) and Apache Spark 
                    (Apache Software Foundation, 2017) to create a hash that represents their contents —pinpointing changes over time— and to extract the analytics that we used in our statistical analysis. More specifically, our analysis has two parts that incorporate the retrieved HTTP response codes, number of redirects, a detailed examination of the contents and links returned by traversing the base node, external resources, HTTP headers and linked files. Figure 1 shows the workflow that we used in our framework to quantify the sings of abandonment.
                
                
                    
                
                Figure 1: Workflow to quantify sings of abandonment in Online Digital Humanities Projects
                First, we carried out a preliminary classification of the websites into two groups depending on their correctness according to their HTTP response codes: valid (responses in the 200 and 300 range) and decayed (all other response codes). If a Web resource reports more than one redirect, we placed it in the decayed category. This is a preliminary classification because a Web resource could return an HTTP response code implying correctness while showing erroneous content 
                    (Meneses et al., 2012: 404) —justifying the second part of our analysis where we cluster the contents of each Web resource in the valid category. We perform the clustering using topic modeling and Term frequency–Inverse document frequency (Tf-Idf).
                
                
                    The textual contents and the links associated with shared resources are the most obvious feature for clustering. Previous work has shown that shared resources are the first to disappear from the Web 
                    (SalahEldeen and Nelson, 2012) —which we interpret as premature indications of degradation. 
                    To detect these early signs, we generated topic and term frequency models to examine the similarity among the documents in a given project (the contents of the base node and the metadata and the contents of the child nodes). We used Latent Dirichlet Allocation (LDA) to model the content of the text 
                    (Blei et al., 2003) 
                    and a simple Tf-Idf ranking function to measure and compare them. This ranking function is based on adding the Tf-Idf values for the documents linked to a Web resource, which were calculated using the terms from the topic modelling as a vocabulary. This combination metrics and techniques allow us to compare and assess the degree of change of online digital humanities projects over time.
                
            
            
                Conclusions
                In this this study we aim to computationally identify the indicators of the abandonment of digital humanities projects —a very specific domain— as well as quantify their degrees of neglect. It is important to highlight that not all projects are equal and thus require different levels of attention. Previous work in this area was based on the metadata from HTTP headers —emphasizing the need for a framework that utilizes robust metrics to identify the collectively shared indicators of degradation. We intend this study to be a step forward towards better preservation mechanisms and for adopting strategies for the planned obsolesce of digital humanities projects.
            
        
        
            
                
                    Bibliography
                    
                        Apache Software Foundation (2017). Apache Spark: Lightning-fast cluster computing http://spark.apache.org (accessed 11 April 2017).
                    
                    
                        Apache Software Foundation (2018). Apache Tika – Apache Tika https://tika.apache.org/ (accessed 25 November 2018).
                    
                    
                        Blei, D. M., Ng, A. Y. and Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning Research, 3: 993–1022.
                    
                    
                        Free Software Foundation (2018). GNU Wget https://www.gnu.org/software/wget/ (accessed 25 November 2018).
                    
                    
                        International Organization for Standardization (2017). ISO 28500:2017 WARC File Format http://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/06/80/68004.html (accessed 25 November 2018).
                    
                    
                        Meneses, L. and Furuta, R. (2017). Shelf life: Identifying the Abandonment of Online Digital Humanities Projects Paper presented at the Digital Humanities 2017, Montreal, Canada.
                    
                    
                        Meneses, L., Furuta, R. and Shipman, F. (2012). Identifying ‘Soft 404’ Error Pages: Analyzing the Lexical Signatures of Documents in Distributed Collections. In Zaphiris, P., Buchanan, G., Rasmussen, E. and Loizides, F. (eds), Theory and Practice of Digital Libraries, vol. 7489. (Lecture Notes in Computer Science). Springer Berlin Heidelberg, pp. 197–208 http://dx.doi.org/10.1007/978-3-642-33290-6_22 http://link.springer.com/chapter/10.1007%2F978-3-642-33290-6_22.
                    
                    
                        Meneses, L., Martin, J., Furuta, R. and Siemens, R. (2018). Part Deux: Exploring the Signs of Abandonment of Online Digital Humanities Projects Paper presented at the Digital Humanities 2018, Mexico City.
                    
                    
                        SalahEldeen, H. M. and Nelson, M. L. (2012). Losing My Revolution: How Many Resources Shared on Social Media Have Been Lost?. Theory and Practice of Digital Libraries. (Lecture Notes in Computer Science). Springer, Berlin, Heidelberg, pp. 125–37 doi:10.1007/978-3-642-33290-6_14. https://link.springer.com/chapter/10.1007/978-3-642-33290-6_14 (accessed 1 August 2018).
                    
                    
                        van Rossum, G. (1995). Python Tutorial, Technical Report CS-R9526. Amsterdam: Centrum voor Wikunde en Informatica (CWI) https://ir.cwi.nl/pub/5007/05007D.pdf.
                    
                
            
        
    
9975	2019	
        
            
                Political terror in the USSR has remained a painful theme for Russian society. In academic and public history different waves of terror and controversial official statistics of victims are being widely discussed. Many documents that accompanied terror operations and even some investigative cases remain classified in archives, and almost 30 years after collapse of the USSR, we still know only 20% of names of political terror victims, i.e. about 3 million real names. These data are mentioned in different “Books of Memory”, created in almost every region of the former USSR and consist of short biographical cards on every victim known. All data of “Books of Memory” are collected in unified database made by International society “Memorial” (http://base.memo.ru/). It’s a SQL-based large dataset of victims, which updates every few years and support only Russian language. We initiated the creation of another database – “Open list” – to help “Memorial” collect names and eradicate mistakes from their data.
            
            
                "Open list" wiki-like open dataset (https://openlist.wiki) is created on the basis of very heterogeneous and diverse dataset by International “Memorial”. That is the only unified dataset on political terror victims in Russia from 1917 to 1991 containing more than 3.1 million records and which has four pages on national languages: Russian, Ukrainian, Belorussian and Georgian. A data card of each wiki-page is unified for any of national subsections of the project. Some historical sources were not included in “Memorial” database, names and bios from these sources are mentioned only in “Open list”. The main advantage of “Open list” is that people can add new names and correct already existing information online. “Open list” updates daily. Users can edit pages via special form with user-friendly interface with 31 fields for personal data and description of arrests, or use wiki-markup to add fields mentioned above manually and upload files on pages. Editors are to verify all crowd-sourced data manually using documents and files that people provide when making their corrections. Most useful documents are digital copies from investigative cases or rehabilitation certificates. If a user cannot provide any file, page remains unapproved with special disclaimer on it. 
            
            
                Crowdsourcing is a very important part of the project. There are several possible activities for our users: they can add new people to the list, parse data from biographies to fields in biographical form or add templates such as "repressed relatives" using inter-wiki to link pages of persons from a kinship family. The special algorithm automatically defines potential relatives according to similar surnames, patronymics and rehabilitation dates (some kind of record linkage approach). We also have tools for our users to identify and unite duplicate pages. It usually requires much historical knowledge to identify description of two different arrests correctly, and the result of such research is almost always manually checked by editors. There are only few people who work with duplicate pages while we have about 100 thousand duplicates, so we need ideally automatic algorithms to merge them. These algorithms vary depending on the quantity of data on the particular page and mentioned type of repression. The simplest method is to compare a full name, a birth year and a birth place, but here additional parameters are required. If we need to find one person in two different historical sources, we add dates of arrest and conviction. In case of possible mistakes in full names and dates of birth we use full coincidence of biographical data in primary sources. Historians provide these algorithms and IT specialist realize them on Python.
            
            
                Advanced search of “Open list” contains 21 search fields; all text fields allow using logical operators. Users can gain a long list of personal pages by request. The search is not strict and sometimes it shows more pages than were requested. Data visualization is now possible only on certain types of information like dates of birth, or arrest, or conviction, so text fields should be normalized in close future. This is also one of the project goals to make analysis of these data easier and persistent. "Occupation" is one of the most difficult fields for normalization. We use classification made by historians on materials of All-Union 1937 and 1939 censuses and NKVD internal instructions for classification of occupations. HISCO is not in use on this step of work because it seems not suitable for linking occupations to Soviet social stratification in the 1930s, as this linking is a necessary step for analyzing social portrait of terror victims. Also such kind of work with HISCO has never been done on materials of early Soviet period. Now we are only preparing for normalizing data and will do most part of work automatically.
            
            
                As the database grows, we can use it for academic needs and deepen knowledge of political repression in the USSR through different ways. “Open list” provides an opportunity to make samples of data and construct a social portrait of terror victims. Pages with templates could be objects of network analysis as well as investigative cases published on some pages. Massive of biographies could be a source to study family history. We can also analyze geography of terror using field “place of living”, which in some sources contains concrete addresses. “Open list” itself conducts an archival work in cooperation with Russian state archive on compiling the united electronic “Book of memory” of Moscow and Moscow region. This work consist of two parts. Firstly, digitizing the materials of archival investigative cases is taking place. Secondly, the crowdsourcing takes the floor: our volunteers decrypts information from archival documents and create new pages in a list or contribute to existed. They use the instruction provided by professional historians based on principles of source-study of investigative cases. Editors verify all new records in electronic books. Thus, new historical source with names that had never been mentioned before is emerging online on “Open list” web site. This project helps not only to supplement the list of names, but also correct mistakes in “Books of Memory”.
            
        
    
9976	2019	
        
            The use of imperial media in representing India and its people was an important aspect in the consolidation of colonial rule. This poster examines how the analysis of the representation of Indian individuals’ links to colonial consolidation of power using highly detailed encoded metadata in sources. I will be demonstrating how the development of an overlapping and layered approach to metadata in encoding can better represent and therefore aid research into media representations. 
            The portrayal of the Indian nobility exemplifies a powerful, exotic, yet ultimately submissive form of Imperialism to the British, and the presentation of the dynasty of the female rulers of Bhopal is a prime example of this. My work charts the ways in which these female rulers were represented and misrepresented as a means of demonstrating Indian animosity and consolidating imperial power. As favoured rulers of a ‘native state’, the Begums are frequently discussed in newspapers or as the subject of photographs and while authoring letters and other personal papers.
                
                     Shaharyar M. Khan, 
                        The Begums of Bhopal: A Dynasty of Women Rulers in Raj India, (London: I.B. Tauris) p. 102, p. 171
                    
                 Combining colonial textual and visual sources allows for a comprehensive view of how these rulers were envisioned to live. Yet, the overall image of the Begums is highly contradictory; for example, in text, these women are typically portrayed as being veiled or in purdah while photographs show them to have almost masculine characteristics, embodying features of any great Indian ruler.
                
                     Bourne and Shepherd, 
                        Sultan Shahjahan Begum, Begum of Bhopal: Prince of Wales Tour of India 1875-6, 1875-6, 27.3 x 23.3 cm, Albumen print
                    
                    Sydney Prior Hall, 
                        The Reception of the Begum of Bhopal, 1877, 16.0 x 24.3 cm, Pencil and watercolour
                    
                
            
            The contradictory nature of representation has been a central theme in my research. How were the Indian nobility represented by the British? Were these contradictions central to the way in which Indian nobles were represented, or were they discrete mistakes, demonstrating inconsistency in the understanding of these figures by various British and Indian commentators? Answering these questions required me to uncover patterns in the depiction of these individuals, and their world, to develop a model of the perceived colonial Indian aristocrat. Moreover, it requires understanding how these patterns can transfer from textual to visual sources. 
            To contrast physical and conceptual elements within a multi-modal dataset in a robust and reproducible manner requires detailed and nuanced hermeneutic encoding.
                
                     mhbeals.com/the-platonic-ideal-of-a-newspaper-article
                 This poster will, therefore, explore the use of digital metadata analysis in the field of representation. It will demonstrate the cross-referencing of digitalised newspaper material from the British Library, Trove (Australia), Papers Past (New Zealand) and Chronicling America (USA), with biographies, autobiographies and personal letters held by the British Library and the India Office Collection and digitalised and physical photographs and drawings from throughout the Commonwealth in various combinations and at multiple resolutions. Current text encoding practises, TEI specifically and XML more broadly, do not allow for two overlapping elements of metadata, nor does annotation of visual material do so without damaging or destroying the integrity of the image.
                
                     ids-pub.bsz-bw.de/frontdoor/index/index/docId/5878
                    Tufte, Edward, Beautiful Evidence, Graphics Press LLC, 2006
                
                Although multiple layers of analysis within XML documents, namely multi-rooted trees or using XML linkages and then subsequent transformations to fully represent multiple hierarchies are possible, this is technically complex and requires both a definitive and transformed version of the document for recording and analysis. My system allows for straightforward annotation of a single text document with multiple overlapping segments in the same manner as it annotates full or segmented images.  This unity of input and analysis is more appropriate to my specific case, comparing visual and textual representations of a common event or group. My poser will demonstrate a custom-designed database system that allows for consistent layering of metadata on textual and material historical objects, digital reproductions, and enhanced fragments – sections of textual or visual objects that have been transcribed, cropped or otherwise computationally transformed. Building upon concepts from the semantic web, the system allows the user to apply metadata and provenance details recursively between the original object and discrete fragments as well as an encoded controlled vocabulary on the content of those objects. This layering of metadata allows for a much deeper analysis of historical sources, as well as being able to maintain the integrity of the original object in the project’s documentation. 
            
            The database, written in Python 3.6 and stored in a plain text JSON file, functions as both an input and retrieval mechanism, allowing users to input data sources either manually or through templated input files. In addition to standardised MARC and Dublin Core provenance metadata, the database will encode both the aesthetic and artistic properties (source medium, structure, composition), and the historical characteristics (subject matter and historical relevance). This will allow for consistent cross-referencing that may be difficult if not impossible to determine by human eye alone. Using metadata allows for a much more robust comparison between textual and visual sources, highlighting trends that may otherwise never be brought to light. Finally, although this level of metadata analysis requires a significant temporal commitment to data entry, the deep data created will inform not only the present research regarding representation but provide consistent training for unsupervised computation analysis in the future. 
        
        
            
                
                    Bibliography
                    
                        Khan, M. Shaharyar, (2000) 
                        The Begums of Bhopal: A Dynasty of Women Rulers in Raj India, London: I.B. Tauris 
                    
                    
                        Beals, M. H., (2016) 
                        The Platonic Ideal of a Newspaper Article, mhbeals.com/the-platonic-ideal-of-a-newspaper-article
                    
                    
                        Tufte, Edward, (2006), 
                        Beautiful Evidence, 
                        Graphics Press LLC, ids-pub.bsz-bw.de/frontdoor/index/index/docId/5878
                    
                
            
        
    
9989	2019	
        
            
                Introduction
                Within the Digital Humanities (DH), research applications such as databases, digital editions, interactive visualizations, and virtual research environments play a central role in securing and presenting research results [16]. Often, such 
                    living systems [15] are the actual bearers of information content, thus representing the added value of the scientific output [16]. However, within the DH a great number of smaller, highly heterogeneous software solutions are produced, which all are subject to the problem of 
                    software aging [14]. Against this background, institutions like the Data Center for the Humanities at the University of Cologne (DCH, 
                    
                        http://dch.uni-koeln.de
                    ) face the challenge of preserving an unknown, potentially unlimited number of research software systems to assure their availability on a permanent basis. While there are well-established methods of preserving primary research data, e.g. in existing data repositories and archives, living systems are part of a constantly changing digital ecosystem and must regularly adapt to it, e. g. they need (security) updates. However, due to their steadily increasing number and their heterogeneity (both technologically and methodologically), permanent maintenance, support and provisioning of such living systems is a major technical, organizational, and therefore ultimately financial challenge. 
                
                This contribution presents an approach to the preservation of web-based research applications in the DH, based on the 
                    Topology and Orchestration Specification for Cloud Applications (TOSCA) [11, 12, 13]. TOSCA is an OASIS standard for modeling, provisioning, and managing cloud applications in a standardized and provider-independent way. The TOSCA standard aims at providing a superset of service modeling and orchestration features and can thus be seen as a meta-framework that includes vendor and domain specific solutions like e. g. Docker, OpenStack or VSphere. In the following, we focus on an exemplary use case to describe the main concepts of our approach. 
                
            
            
                The Musical Competitions Database
                The DFG-funded project 
                    Musical Competitions between 1820 and 1870 is conducted by the Department of Musicology at the University of Cologne in cooperation with the Cologne Center for eHumanities (CCeH). The aim of the project is to gather comprehensive information about music related competitions from 1820 to 1870 [6]. Data is extracted by musicologists from music-related journals and stored as JSON files in a document-oriented database (CouchDB). Access to the data is given through a web application written in React (
                    
                        http://musical-competitions.uni-koeln.de
                    ). Further, Elasticsearch is used to provide advanced options for querying/filtering and analysis of the data. At the time of writing, the database features information on approximately 1300 musical competitions, 1000 corporations and 3100 persons related to those competitions. The 
                    Musical Competitions Database contains and presents a unique data set relevant to the musicology community. To allow for reproducibility in the sense of good scientific practice, a sustainability strategy to keep this data accessible on a permanent basis must include the web application itself, because the separation and archiving of the primary data alone would inevitably lead to a loss of functionality (and thus information).
                
            
            
                TOSCA and OpenTOSCA
                Technological basis of our approach is the OASIS standard TOSCA [11, 12, 13]. TOSCA allows for a portable description of IT systems to automate their provisioning and management. In TOSCA, a cloud application or service [9] is modeled as a 
                    Service 
                    Template. Inside a Service Template, the 
                    Topology Template describes the service’s topology as a directed multigraph, consisting of 
                    Node Templates and 
                    Relationship Templates that specify the edges between the nodes. Thus, this enables to describe arbitrary deployments in the form of 
                    declarative deployment models [5]. Underneath, TOSCA employs a type system defining common properties and attributes in 
                    Node Types and 
                    Relationship Types, respectively. To automatically deploy, provision and manage the modeled service, TOSCA defines an archive format called 
                    Cloud Service Archive (CSAR) which contains the Service Template, including all Node Types and Relationship Types, as well as all required software artifacts, scripts, and binaries needed for provisioning. Moreover, imperative management plans can be added to CSARs, which enables the implementation of arbitrary kinds of management functionality in an automatically executable manner. These plans can be implemented using standardized workflow languages such as BPEL or BPMN, or domain-specific modeling extensions such as BPMN4TOSCA [7]. Any TOSCA runtime environment can consume such a CSAR to automatically deploy and instantiate the enclosed application [2]. 
                
                In a series of projects, the Institute for Architecture of Application Systems (IAAS, 
                    
                        http://iaas.uni-stuttgart.de
                    ) at the University of Stuttgart has developed the OpenTOSCA ecosystem, an open source implementation for the TOSCA standard. OpenTOSCA includes (i) the graphical modeling tool 
                    Winery for the creation of TOSCA-based application models [8], (ii) the runtime environment 
                    OpenTOSCA container for automated provisioning and management of the modeled applications [1], and (iii) the self-service portal 
                    Vinothek [4], which lists all applications installed in the OpenTOSCA container and serves as a graphical user interface. 
                
                We believe that the TOSCA standard is generally suitable for assuring the digital sustainability of research results, as research applications, which are packaged in CSARs, can be executed years later by a TOSCA-compliant runtime environment [3].
            
            
                A TOSCA Model for the Musical Competitions Database
                In the following, we describe an application model for the Musical Competitions Database to exemplify some of the basic concepts of (Open)TOSCA. The application is composed of a CouchDB and Elasticsearch instance and accessed through a React frontend. The resulting TOSCA-compliant topology model is depicted in figure 1.
                
                    
                        
                        Screenshot of OpenTOSCA’s modeling tool Winery, showing the topology of the use case application. On the left side, the available components are listed.
                    
                
                The topology consists of a React web application hosted on an Apache web server, which itself is hosted on a Docker container, where the container operation system can be passed as a Docker image identifier (e.g. 
                    ubuntu:latest). Additionally, the React application connects to a CouchDB database and to Elasticsearch. To accommodate Elasticsearch's dependency on Java, an OpenJDK has to be available. Therefore, seven different node types, namely 
                    React, 
                    Apache, 
                    CouchDB, 
                    Elasticsearch, 
                    OpenJDK,
                     DockerContainer and 
                    DockerEngine, as well as the 
                    HostedOn, the 
                    ConnectsTo and the 
                    DependsOn relationship types must be available. A TOSCA Service Template describing this application will contain those seven node templates and three relationship templates – where each template is an instance of the respective type definition. The resulting service template can then be packed in a CSAR which may be instantiated by any TOSCA runtime or to be archived in a repository. As the TOSCA standard (and therefore OpenTOSCA) thrives on being vendor-independent, the topology root depicted in figure 1, namely 
                    DockerEngine and 
                    DockerContainer, may be substituted for e.g. OpenStack or VSphere and their respective VM/container representations.
                
            
            
                Summary and Outlook
                The concepts described above emerged from 
                    SustainLife [10], a DFG-funded joint project of the DCH Cologne and the IAAS Stuttgart. The overall objective is to develop generic solutions for standards-based operation and maintenance of DH-applications and to implement them in a way that they find practical application, e.g. in humanities data centers like the DCH. 
                
                As TOSCA depends on a generic type system enabling the reuse of recurring components, we work towards providing a set of typical system components. Examples for components, which were identified in further use cases [10] and modeled in TOSCA are Java runtime environments, the Spring framework and several types of databases like MySQL, mongoDB and eXist-db. In addition, reusable Service Templates reflecting typical software stacks are under development. For example, a common pattern for web applications is the so-called LAMP-stack, composed of a Linux operating system, an Apache web server, a MySQL/MariaDB database and a PHP/Perl/Python interpreter. These components can be reused are intended to simplify future application modelling, development and maintenance using TOSCA and the OpenTOSCA ecosystem. 
                Beyond that, a number of further extensions of the OpenTOSCA ecosystem are in the scope of the SustainLife project. For example, applications that are archived in CSARs need to be deployable several years after their development. Therefore, approaches to 
                    freeze and 
                    defrost whole applications and their respective execution states are also part of our research. This includes the possibility to version TOSCA models, to reflect the fact that living systems are subject to constant changes. Another desideratum is to add the possibility to update a service’s components. If a component must be exchanged because of security issues or deprecation, the CSAR may no longer be deployable. We therefore work on additional management functionalities which provide standardized operating and maintenance solutions, e.g. applying updates or software patches. 
                
                With our approach we expect to reduce maintenance costs significantly and will evaluate this expectation on the basis of selected use cases. Findings and best practices are prepared in a way that they can be transferred to partners and are communicated to the scientific community through workshops and publications. Thus, with this contribution, we want to trigger a discussion about the applicability of methods and technologies of professional cloud deployment and provisioning strategies to problems of long-term availability of research software in the DH-community.
            
            
                Acknowledgements
                This work is partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation). Project title: “SustainLife – Erhalt lebender, digitaler Systeme für die Geisteswissenschaften” (see 
                    ).
                
            
        
        
            
                
                    Bibliography
                    T. Binz, U. Breitenbücher, F. Haupt, O. Kopp, F. Leymann, A. Nowak, S. Wagner. “OpenTOSCA - A Runtime for TOSCA-based Cloud Applications”. In: ICSOC, 2013, S. 692-695, 2013. 
                    U. Breitenbücher, T. Binz, K. Képes, O. Kopp, F. Leymann, J. Wettinger. “Combining Declarative and Imperative Cloud Application Provisioning based on TOSCA”, IC2E, S. 87–96, 2014. 
                    U. Breitenbücher, J. Barzen, M. Falkenthal, F. Leymann. “Digitale Nachhaltigkeit in den Geisteswissenschaften durch TOSCA: Nutzung eines standardbasierten Open-Source Öko-systems”. Konferenzabstracts DHd 2017: Digitale Nachhaltigkeit, S. 235-237, 2017. 
                    U. Breitenbücher, T. Binz, O. Kopp und F. Leymann. “Vinothek - A Self-Service Portal for TOSCA”. In: ZEUS 2014, p. 69-72, 2014.
                    C. Endres, U. Breitenbücher, M. Falkenthal, O. Kopp, F. Leymann, J. Wettinger. “Declarative vs. Imperative: Two Modeling Patterns for the Automated Deployment of Applications”. In Proceedings of the 9th International Conference on Pervasive Patterns and Applications (PATTERNS), 2017, pp. 22-27.
                    F. Hentschel. “Institutionalisierung des ästhetischen Werturteils: Musikalische Preisausschreiben im 19. Jahrhundert”. In: Archiv für Musikwissenschaft 69 (2012), pp. 110-121, 2012.
                    O. Kopp, T. Binz, U. Breitenbücher, F. Leymann. „BPMN4TOSCA: A domain-specific language to model management plans for composite applications”. In International Workshop on Business Process Modeling Notation, pp. 38-52. Springer, Berlin, Heidelberg, 2012. 
                    O. Kopp, T. Binz, U. Breitenbücher, F. Leymann. “Winery – A Modeling Tool for TOSCA-based Cloud Applications”. In: ICSOC, 2013, S. 700-704, 2013.
                    F. Leymann, U. Breitenbücher, S. Wagner, J. Wettinger. “Native Cloud Applications: Why Monolithic Virtualization Is Not Their Foundation”. Cloud Computing and Services Science, Springer, pp. 16-40, 2017. 
                    C. Neuefeind, L. Harzenetter, P. Schildkamp, U. Breitenbücher, B. Mathiak, J. Barzen, F. Leymann. "The SustainLife Project – Living Systems in Digital Humanities". In: Proceedings of the 12th Advanced Summer School on Service-Oriented Computing, 2018 (IBM Research Report RC25681), pp.101-112, 2018
                    OASIS: “Topology and Orchestration Specification for Cloud Applications Version 1.0”, 2013. 
                    OASIS. “Topology and Orchestration Specification for Cloud Applications (TOSCA) Primer Version 1.0”. 2013.
                    OASIS: “TOSCA Simple Profile in YAML”, Version 1.0, 2015.
                    D. L. Parnas. “Software Aging”. In: Proceedings of the 16th International Conference on Software Engineering (ICSE 1994). IEEE, May 1994, pp. 279-287, 1994.
                    P. Sahle, S. Kronenwett. “Jenseits der Daten: Überlegungen zu Datenzentren für die Geisteswissenschaften am Beispiel des Kölner Data Center for the Humanities”. In: LIBREAS. Library Ideas 23, pp. 76-96, 2013. 
                    U. Wuttke, C. Engelhardt, S. Buddenbohm. “Angebotsgenese für ein geisteswissenschaftliches Forschungsdatenzentrum”. In: Zeitschrift für digitale Geisteswissenschaften, 2016. 
                
            
        
    
10002	2019	
        
            This one-day workshop offers an exploration of culture analytics, aimed at an audience of students and scholars interested in understanding the intersection of analytics and the humanities. It follows a three-year program organized by Institute for Pure and Applied Mathematics (IPAM) at UCLA, gathering a numerous and lively community of people coming from humanities, media studies, computer science and mathematics.
            Culture Analytics is, by definition, a collaborative, translational data science that explores culture and cultural interaction as a multi-scale / multi-resolution phenomenon. The macroscopic view, that allows a researcher to move from the microscale of close reading, up through the mesoscales, and on to the macroscale of distant reading, is a hallmark of the discipline. Culture analytics as a field is focused on the productive intersection between humanities, mathematics, and data science. 
            Researchers from the Humanities, the Social Sciences, the Mathematical Sciences, and the Data Sciences are now collaborating to identify, document, and integrate concepts, methods and tools that will provide an intellectually and ethically sound approach to the study of cultures across time and across space, leveraging the enormous gains made in the past decade in computation and machine readable cultural archives, from libraries and museum collections to the born digital cultural expressions of billions of people on the internet. This rapid proliferation of digital data has made the role of Culture Analytics all the more central particularly given the potential for significant benefit that lies in harnessing the domain expertise of researchers across these disciplines.
            Time series are a very important issue that can be addressed with culture analytics methodologies. People working with digitized archives, newspapers, huge dataset of images are interested to discover evolutions, patterns, trends over time. This kind of research questions requires specific computational approaches. This workshop after a general introduction about culture analytics (Keynote by John Laudun, one of principal participants of the culture analytics long program at IPAM/UCLA) will present different study cases, followed by other short presentations. After the introductory part the program will be consecrated to a tutorial on time series in scale.
            For this workshop, the target audience within the digital humanities community are those scholars interested in utilizing more mathematical and algorithmic approaches in the analysis of cultural data. The expected number of participants is 25 persons.
            For the afternoon part of the workshop the participants will be sent instructions how to download the needed program and dataset to their laptops. The participants will bring their own laptops to the workshop. 
            Schedule of the workshop
            What is Culture Analytics? - John Laudun (University of Louisiana) 
            Event Flow: Moments of Innovation, Disruption, and Reflection in Public Discourse - Melvin Wevers (Utrecht University) &amp; Kristoffer Nielbo (Aarhus University)
            Tracing collaboration over time in the Leonardo journal - Clarisse Bardiot (Université Polytechnique Hauts-de-France), Peter Broadwell (Stanford), Maria d’Orsogna (California State University at Northridge), Mila Oiva (University of Turku), Pablo Suarez (UNAM), Timothy Tangherlini (UCLA), Melvin Wevers (Utrecht University)
            Measuring and Presenting time series of Astrophotography in Science, Public Discourse and (Virtual) Museums Exhibitions - Ekaterina Lapina-Kratasyuk (National Research University Higher School of Economics &amp; Leon Gurevitch (Victoria University of Wellington)
            Spreading News Globally in the 19th century press - the Oceanic Exchanges - &amp; Mila Oiva (University of Turku)
            Large images dataset overtime : PixPlot new features - Peter Leonard (Yale University) 
            Audio and video time series analysis - Peter Broadwell (Stanford University) &amp; Timothy Tangherlini (UCLA)
            Tutorial: Time Series in Texts: From the Micro to the Macro
            With Kristofer Nielbo, Taylor Arnold and Ross Deans Kristensen-McLachlan 
            This hand-on tutorial offers participants a chance to explore how time series analysis can be used both to examine a single text and to examine a corpus. Examples include a short story, a novel, and a corpus of newspapers. Exercises include parsing texts in various ways and then deriving values through topics and sentiment and then understanding change over time through the Hurst exponent. Workshop participants will need to be familiar with Python or R, or at least interested in becoming so. Interactive possibilities, including application of data from participants, will be made possible through a Jupyter notebook—the default installation of Anaconda is acceptable.
            
                Organizing Committee members
                The organizing committee members were part of the core group of the Culture Analytics Long Program at IPAM (UCLA).
                
                    John Laudun is Doris H. Meriwether/BORSF Endowed Professor of English at the University of Louisiana. His research focuses on folk narrative, both as a textual production in and of itself as well as a networked phenomenon. His published work includes a book-length study of embedded creativity and articles on folklore in both traditional and digital environments. In addition to scholarly journals and anthologies, his work is also featured in archives, CDs, films, and television series, and he has received funding from the NSF, NEH, the Andrew Mellon Foundation, the MacArthur Foundation, and the U.S. Department of Education. His work on textual analytics with Python has been featured at CERN, Duke University, among others.
                
                Email: 
                    
                        jlaudun@me.com
                    
                
                
                    Mila Oiva, postdoctoral researcher, University of Turku, Finland. Mila Oiva (PhD) is Cultural Historian and expert on Russian and Polish 20th century history and digital humanities. She works as a postdoctoral scholar at the Trans-Atlantic Platform/Digging into Data funded Oceanic Exchanges project at the University of Turku. She is currently co-editing a special issue “Lab &amp; Slack. Situated Research Practices in Digital Humanities” to Digital Humanities Quarterly and an edited volume “History in the Digital Era” for the Helsinki University Press. Her research interests consist of transfer of knowledge and information flows and temporal change. 
                    Email: 
                    
                        milaoiv@utu.fi
                    
                
                
                    Ekaterina Lapina-Kratasyuk, PhD, associate professor, National Research University Higher School of Economics (Moscow, Russia). Ekaterina Lapina-Kratasyuk is a professor and researcher in Media &amp; Cultural Studies with an especial focus on Culture Analytics in field of spatial studies (urban studies, astroculture analyses). She is the co-editor (with Evgenia Nim and Oxana Moroz) of Tuning Language: Communication Management in Post-Soviet Space (2016); co-editor (with Oxana Zaporozhets) of Interactive City: Urban Life in New Media Age (2019), and author of numerous papers on digital humanities, new media, spatial media and transmedia. She is also a head of a MediaSpace research and educational program in the Space Museum in Moscow, Russia.
                    Email : 
                    
                        kratio@mail.ru
                    
                
                
                    Clarisse Bardiot, PhD, associate professor, Université Polytechnique Hauts-de-France (France) and research fellow at the CNRS. Clarisse Bardiot is a researcher, a publisher and a curator, who has been working in the fields of digital performances history, digital humanities, documentation and preservation of time based media art works, and digital publishing. Since 2012, she has received various grants to develop Rekall and MemoRekall, an open source environment and a webapp to analyse, document and preserve time based media art.
                
                Workshop Facilitators
                
                    Kristofer Nielbo is associate professor of humanities computing at University of Southern Denmark, where he runs an eScience unit for the humanities in the SDU eScience Center. KLN has specialized in applications of quantitative methods and computational tools in analysis, interpretation and storage of cultural data. He has participated in a range of collaborative and interdisciplinary research projects involving researchers from the humanities, social sciences, health science, and natural sciences. His research covers two areas of interest of which one is more recent (automated text analysis) and the other (modeling of cultural behavior) has followed him during his entire academic career. Both interests explore the cultural information space in new and innovative ways by combining cultural data and humanities theories with statistics, computer algorithms, and visualization.
                
                
                    Taylor Arnold is assistant professor of statistics at the University of Richmond where he is the co-director of the Distant Viewing Lab. He studies massive cultural datasets in order to address new and existing research questions in the humanities and social sciences. He specializes in the application of statistical computing to large text and image corpora. The study of data containing linked text and images, such as newspapers with embedded figures or television shows with associated closed captions, is of particular interest. Dr. Arnold's work has been funded by the National Endowment for the Humanities, American Council of Learned Societies, Defense Advanced Research Projects Agency, and the Collegium de Lyon's Institut d’études avancées. He has authored the books Humanities Data in R (Springer, 2015) and A Computational Approach to Statistical Learning (CRC Press, 2019).
                
                
                    Ross Deans Kristensen-McLachlan is a Research Assistant based at Aarhus University. His academic background is in English Language and Linguistics but he now works more broadly in computational humanities and cultural analytics. He has a strong interest in English historical linguistics, as well as cognitive and computational approaches to lexical semantics and textual analysis. Most recently, he has been involved in the Digital Literacy project at Aarhus, where he provides digital support for a number of research projects working with a diverse range of texts – from 17th-century English drama and Stephen King novels, through to Facebook groups and contemporary Danish church sermons.
                
            
        
    
10015	2019	
        
            One of the algorithms that have been established in digital literary studies during the recent years is LDA (Latent Dirichlet Allocation) topic modeling (Blei 2012; Seyvers and Griffith 2006). This method allows researchers to analyze the distributions of semantic groups in a corpus of texts, which can be useful for both the exploration of contents and automated text classification tasks. With regard to the increasing interest on topic modeling in the DH community, the infrastructure project DARIAH-DE has started in 2017 to develop the TopicsExplorer. Based on the python libraries "DARIAHTopics" (Jannidis et al. 2017) and "LDA" by Allan Riddell, the TopicsExplorer is a locally run standalone software for topic modeling that allows researchers to generate and explore topic models on their own computers, with their own text files, all within the comfort of a single GUI (graphical user interface) tool that supports the entire process from preprocessing to the visualization of results (https://dariah-de.github.io/TopicsExplorer/).
            Though lacking the performance and flexibility of popular command line solutions, such as MALLET (McCallum 2002), or programing libraries, like Gensim (Rehurek and Sojka 2010), the advantage of the TopicsExplorer is its simplicity and usability. It can be used productively - within its limitations - by researchers without any programing skills. It does not even require users to use the command line. It thereby covers a significant gap: curious researchers lacking the technical skills required to use conventional tools are now able to try topic modeling and learn about the potentials and limitations of the method. On the one hand, this enlarges the spectrum of researchers able to participate in an informed discourse about DH-related research that relies in topic modeling as a method. On the other hand, researchers who think about using topic modeling in their own investigations can either directly use the TopicsExplorer for simpler problem, or at least learn beforehand enough about the method to make an informed decision before investing the effort to acquire the technical skills necessary to work with more advanced tools.
            The first version of the TopicsExplorer has been presented to various groups of researchers and students in a number of workshops in 2017 and 2018. Experiences and user feedback collected in these workshops have shaped the development work in the past year, and the changes implemented go far beyond simple debugging and securing sustainable functionality on as many platforms as possible. The tool began as a so-called "GUI Demonstrator" for the DARIAHTopics Python library that required installation of a Python environment, ran as a local server and was displayed in a browser (Simmler et al. 2018). With version 1.0, it became a fully functional standalone software that can be downloaded and run on common Windows, MacOS and Linux systems without further preparation. It features interactive visualizations and csv export of results. A number of smaller enhancements proposed by test users, like a progress bar and abort button (Fig. 1), have incrementally improved the usability of the 1.x versions.
            With the recently published version 2.0 architecture and interface have undergone a major redesign that addresses the more complicated feature demands derived from the feedback from our test users. On the technical level, the former solution for interactive visualization that was based on Python's "Bokeh" library has been replaced by a Javascript-based solution. This allows for more flexibility in the implementation of additional interactivity features. To improve user experience especially for users who want to explore a corpus visually through a topic model, a new visualization concept based on the ideas of Chaney and Blei (2012) was implemented in this version. The concept allows, for example, to display a document from the corpus with all its related information from the topic model, and at the same time to list other documents with similar or related content (Fig. 2).
            Topic modeling already is a research method often encountered in the digital humanities, though one exclusively used and critically discussed by researchers with advanced technical skills. It is our hope that the TopicsExplorer, with all the ongoing improvements, will help to move the method out of that particular niche.
            
                
                    
                
            
            Figure 1: Version 1.x progress bar.
            
                
                    
                
            
            Figure 2: Overview for a single document in version 2.0.
            
                
            
        
        
            
                
                    Bibliography
                    
                        Blei, David M.
                         
                        (2012): „Probabilistic Topic Models“, in 
                        Communication of the ACM
                         
                        55, Nr. 4 (2012): 77–84. doi:10.1145/2133806.2133826.
                    
                    Chaney, Allison J.B./ Blei, David M. (2012): "The Visualizing Topic Models", in: 
                        Proceedings of the S
                        ixth International AAAI
                         Conference on 
                        Weblogs and Social Media 419-422.
                    
                    Jannidis, Fotis/ Pielström, Steffen / Schöch, Christof / Vitt, Thorsten (2017): "Making topic modeling easy: a programming library in Python", in: 
                        Proceedings of the 
                        Digital Humanities 2017 Conference.
                    
                    
                        McCallum, Andrew K.
                         
                        (2002): 
                        MALLET : A Machine Learning for Language Toolkit
                        . 
                        
                            http://mallet.cs.umass.edu
                        
                        .
                    
                    
                        Rehurek, Radim/ Sojka, Petr
                         
                        (2010): "Software framework for topic modelling with large corpora." 
                        In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks
                        .
                    
                    Simmler, Severin/ Vitt, Thorsten / Pielström. Steffen (2018): "LDA Topic Modeling über eine graphisches Interface", in: 
                        Konferenzabstracts
                         
                        der 5. Tagung des Verbands Digital Humanities im deutschsprachigen Raum e.V. 428-429.
                    
                    
                        Steyvers, Mark/ Griffiths, Tom
                         
                        (2006): „Probabilistic Topic Models“, in 
                        Latent Semantic Analysis: A Road to Meaning
                        , herausgegeben von T. Landauer, D. McNamara, S. Dennis, und W. Kintsch. Laurence Erlbaum.
                    
                
            
        
    
10016	2019	
        
            The history of the composition of French saint’s Lives collections in prose is still a mystery. At the beginning of the XIII
                th century, legendaries were already constituted and intermediary steps are missing to understand how the Lives have been gathered. One of the existing hypotheses is that small collections of saint’s Lives might have circulated as small independent units about one saint or a series of saints (Philippart, 1977), sometimes traceable to a single author or translator, under the material form of libelli.
            
            The first milestone in the study of the composition of legendaries was laid by Paul Meyer (1906). His studies led him to discover that some of these legendaries were derived from successive compilations. Using their macrostructure, Meyer dispatched them into families. He also had the intuition that the families were constituted of smaller components, groups or rather sequences of texts. He proposed the existence of primitive series based on authorship and the repetitive grouping of selected lives, such as the hagiographic collection of Wauchier de Denain’s Seint Confessor of the family C or the consecutive and recurrent series in the family B and C of the following three lives: Sixte, Laurent and Hippolyte. However, because most of the French saint’s lives are anonymous and because the collections were rearranged by multiple editors over time, it is extremely difficult to find what could have been the primitive series and Meyer couldn’t go further. This serial composition of the Lives of Saints is a datum also noted by other specialists of Latin hagiography such as Perrot (1992) and Philippart (1977), who even points out that these hagiographic series must be studied in their entirety in the same way as a literary work. But it is still very rare today that hagiographic text editing concerns a complete author’s legendary, mostly because of a lacking certainty about these groupings.
             From there, with an exploratory stylometric analysis, we first want to find if Meyer’s hypotheses are wrong, can be nuanced or completed. In a second time, we would like to discover if the observed proximity between saint’s lives can reveal series from the same author.
            
                
                    
                
            
            We based our study on a legendary from family C, namely the manuscript fr. 412 of the Bibliothèque nationale de France. The task is complex, because textual variants and the absence of a standardised spelling can affect the stylometric analysis (Kestemont et al., 2015). While the lemmatization of the texts could have nullified the problem of spelling variation, in our case, the preparation of the corpus would have been extremely time-consuming. We decided to work from witnesses written by a single hand in the same manuscript in order to limit biases that could have been induced by spelling variations linked to the scribes and not to the author.
            
                 The analysis of the complete legendary is made possible by the use of the OCR software Kraken (Kiessling, 2018), trained on about 8890 transcribed lines of ground truth and tested on 897 lines, which results in up to 95.2% success in Character Recognition Score. Such results allow us to hope that the stylometric analysis is not corrupted by the margin of error (Franzini et al., 2018).
            
            
                
                    
                
            
            
                
                    
                
                We work from the text thus obtained. Because most of the texts of the legendary are anonymous, we follow an unsupervised approach to the analysis of the texts (Camps &amp; Cafiero, 2012), using agglomerative hierarchical clustering with Ward’s criterion (Ward, 1963), guided by its ability to form coherent clusters. 
            
             The texts are, on average, quite short, a known difficulty for stylometry (Eder, 2015), with a median value of
                16,863 characters (space excluded), but with extreme values of 
                1,364 and 
                85,378. Texts that are too short create a problem of reliability, as the observed frequencies may not accurately represent the actual probability of a given variable’s appearance (Moisl, 2011). To limit this issue, we removed texts below 5,000 characters. Because of the errors regarding segmentation in the OCR, we extracted character n-grams, ignoring spaces. We experimented with different lengths, but, following existing benchmarks (Stamatatos, 2013), we retained the 4000 most frequent 3-grams. The metric and choices of normalisation are also an important parameter, one to which much attention has been devoted (Evert et al., 2017; Jannidis et al., 2015).
            
             Following the benchmark by Evert et al. 2017, we chose to use Manhattan distance with z‑transformation (Burrows’ Delta) and vector-length Euclidean normalisation. The results are partially presented in figures 1 &amp; 2.
            
                
                    
                    
                        Dendogram of agglomerative hierarchical clustering using Manhattan distance, z-transformation and vector length normalisation over 4000 most frequent 3-grams
                    
                
            
            
                
                    
                    
                        Dendogram of agglomerative hierarchical clustering using Kohonen SOM coordinates over 4000 most frequent 3-grams
                    
                
            
            Because, at the same time, the corpus is homogeneous, the texts can be quite short, and the data is noisy, separating them in stable clusters can prove quite hard. We tried to improve the quality of the signal by applying, first, a Kohonen self-organising map (Kohonen, 1988:59–69), and then using the coordinates of the points in the SOM for hierarchical clustering (Camps and Cafiero, 2012). 
             In addition, the specificity of composition of the legendary C by successive additions (lives of A, then lives of B, and finally addition of new lives) allows us to ensure a quick control of the likelihood of some proposed groupings. The presence of the hagiographic collection of 
                Seint Confessors of Wauchier de Denain where the author identifies itself twice (both in 
                Dialogues de Sulpice Sévère and in 
                Vie de saint Martial
                de Limoges) also serves as an indicator of validity. 
            
             The study has already shown interesting connections between the legendary of Wauchier de Denain and the 
                Vie de Saint Lambert de Liège and some collections have been revealed. Two of them are quite certain, one of the first five texts of C, all about saint apostles and hypothetically from A, and another one of six virgin saints’ lives, all from B including the Lives of saint Agathe, Lucie, Agnès, Felicité, Christine and Cécile.
            
             To conclude, our analysis attempts to evaluate the best parameters for our study and to overcome certain difficulties inherent to our corpus. Two major obstacles have to be overcome: the lack of spelling standardisation and the lack of homogeneity in the separation of words. At the end of this prospective study, we hope to be able to reveal new hagiographic series prior to the composition of legendaries that were transmitted to us and that could have escaped us so far.
        
        
            
                
                    Bibliography
                    
                        Camps, J.-B. and Cafiero, F. (2012). Setting bounds in a homogeneous corpus: a methodological study applied to medieval literature. 
                        Revue Des Nouvelles Technologies de l’Information, 
                        SHS-
                        1 (MASHS 2011/2012. Modèles et Apprentissages en Sciences Humaines et Sociales Rédacteurs invités : Mar): 55–84.
                    
                    
                        Evert, S., Proisl, T., Jannidis, F., Reger, I., Pielström, S., Schöch, C. and Vitt, T. (2017). Understanding and explaining Delta measures for authorship attribution. 
                        Digital Scholarship in the Humanities, 
                        32 (suppl_2): ii4 – 16 doi:10.1093/llc/fqx023.
                    
                    
                        Franzini, G., Kestemont, M., Rotari, G., Jander, M., Ochab, J. K., Franzini, E., Byszuk, J. and Rybicki, J. (2018). Attributing Authorship in the Noisy Digitized Correspondence of Jacob and Wilhelm Grimm. 
                        Frontiers in Digital Humanities, 
                        5 doi:10.3389/fdigh.2018.00004. https://www.frontiersin.org/articles/10.3389/fdigh.2018.00004/full (accessed 7 August 2018).
                    
                    
                        Jannidis, F., Pielström, S., Schöch, C. and Vitt, T. (2015). Improving Burrows’ Delta – An empirical evaluation of text distance measures. 
                        Digital Humanities Conference: 11.
                    
                    
                        Kestemont, M. (2014). Function Words in Authorship Attribution. From Black Magic to Theory?. 
                        Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLFL). Gothenburg, Sweden: Association for Computational Linguistics, pp. 59–66 doi:10.3115/v1/W14-0908. http://aclweb.org/anthology/W14-0908 (accessed 23 November 2018).
                    
                    
                        Kestemont, M., Moens, S. and Deploige, J. (2015). Collaborative authorship in the twelfth century: a stylometric study of Hildegard of Bingen and Guibert of Gembloux. 
                        Digtial Scholarship in the Humanities, 
                        30 (2): 199–224 doi:http://dx.doi.org/10.1093/llc/fqt063.
                    
                    
                        Kiessling, B. (2018). 
                        OCR Engine for All the Languages. Contribute to Mittagessen/Kraken Development by Creating an Account on GitHub. Python https://github.com/mittagessen/kraken (accessed 26 November 2018).
                    
                    
                        Kohonen, T. (1988). Neurocomputing: Foundations of Research. In Anderson, J. A. and Rosenfeld, E. (eds). Cambridge, MA, USA: MIT Press, pp. 509–521 http://dl.acm.org/citation.cfm?id=65669.104428 (accessed 27 November 2018).
                    
                    
                        Meyer, P. (1906). Légendes hagiographiques en français. 
                        Histoire littéraire de la France, vol. 33. Imprimerie nationale. Paris, pp. 328–458 http://archive.org/details/histoirelittra33riveuoft (accessed 1 May 2018).
                    
                    
                        Moisl, H. (2011). Finding the Minimum Document Length for Reliable Clustering of Multi-Document Natural Language Corpora. 
                        Journal of Quantitative Linguistics, 
                        18 (1): 23–52 doi:10.1080/09296174.2011.533588.
                    
                    
                        Perrot, J.-P. (1992). 
                        Le passionnaire français au Moyen âge. Genève, Suisse: Droz.
                    
                    
                        Philippart, G. (1977). 
                        Les Légendiers latins et autres manuscrits hagiographiques. Turnhout (Belgique), Belgique: Brépols.
                    
                    
                        Stamatatos, E. (2009). A survey of modern authorship attribution methods. 
                        Journal of the American Society for Information Science and Technology, 
                        60 (3): 538–56 doi:10.1002/asi.21001.
                    
                    
                        Stamatatos, E. (2013) On the robustness of authorship attribution based on character n-gram features. 
                        Journal of Law and Policy, 
                        21 (2): 421–439.
                    
                    
                        Ward, J. H. (1963). Hierarchical Grouping to Optimize an Objective Function. 
                        Journal of the American Statistical Association, 
                        58 (301): 236–44 doi:10.1080/01621459.1963.10500845.
                    
                
            
        
    
10023	2019	
        
            The explication of poetic meter in the modern languages of South Asia is a source of consternation even for experienced poets. Even if poets can competently employ established meters in their work, they and their readers or listeners often cannot describe poetic form using established classificatory schema. Traditional prosodic systems, which are inherited from classical languages, do not align well with the phonological features of modern South Asian languages. Modern scholars have developed alternatives that address these deficiencies. We augment that work by presenting an interactive web-based software package under development to visualize poetic meter using directed graphs that accommodate multiple languages and scripts in order to make poetic knowledge accessible to readers, scholars, and poets.
            
                Background
                In South Asia, there are two competing theories of prosody, one derived from Arabic (
                    ‘uruz) and one from Sanskrit (
                    chhanda). The languages of Urdu and Hindi, which share a common grammar but have differing vocabularies, draw upon both systems of meter. The Arabic system traces its origins to the revelation of the eight century Arab prosodist al-Khalil Ibn Ahmad of Basra. Its basic unit is orthography, specifically the written Arabic letter. Though quite precise for classical Arabic, when used for South Asian languages the system breaks down, because there are so many possible combinations of metrical units, i.e. it is combinatorially explosive. The system used for Sanskrit is also defined by writing, based on abugida (segmental writing system), or long and short units. Modern languages typically drop short vowel endings, however. Both systems have a preference for complex and precise classification. That propensity towards complexity, combined with the disalignment of the systems with modern languages, have posed challenges for modern literary critics, poets, and readers, alike. 
                
                Modern prosodists of Urdu and Hindi poetry, often themselves proponents of elegant systems, have attempted to make these prosodies more accessible by referring to patterns of long and short metrical units (Pybus 1924; Pritchett and Khaliq 1987; Fārūqī 1968; Tabassum 1983; Nagasaki and Kim 2012). These are often represented using macrons, breves, or other symbols familiar to readers of English metrical texts. 
                Modern prosodists are also challenged by the difference between the durational basis for poetic prosody in South Asian languages and the basis of meter in English and other European languages on stress. Durational meter is based on metrical unit length. This difference also poses a problem for graphical exploration of poems, which are mostly designed for English and confine their “prosodic domains” to stress (Abdul-Rahman et al. 2013; Delmonte 2015; McCurdy et al. 2016). Our visualization software addresses both of these challenges.
            
            
                Graphical visualization of poetic data
                Our visualization software consists of a Python and Javascript module in a free and opensource software package that turns poetic texts into poetic data. It provides long and short units, as well as labeled rules for particular contexts. The current implementation supports Hindi and Urdu meters, along with Hindi (Figure 2), Urdu (Figure 3), and English (Figure 4) translations of its interface. The graph layout is performed by the Graphviz library (Gansner and North 2000).
                
                    Representing poetic meter as a directed graph
                    By representing poetic meter as a particular walk through a directed graph, this model offers a significant advancement over previous metrical representations. We assume for our graphical model start and end nodes, short and long metrical unit nodes, and edges between them. We use start and end nodes with distinct shapes and colors. We suggest a circle as the shape for a “short” metrical unit, and a rectangle as that of a “long.” For uncounted metrical units at the end of lines or before caesuras, we suggest a dashed circle. Units that compose metrical feet are grouped into labeled clusters (Figure 1). This system resolves the issues of metrical flexibility and complexity that, in traditional prosody, led to excessive categorization, while visualizing the patterns of durational sound that produce meaning for poets and their listeners. 
                
            
            
                Discussion
                This software package works across the multiple scripts of South Asian reading and listening publics. It advances earlier methods of visualizing meter by affording new sorts of interaction, particularly in web-based environments.
                For scholars, directed graphs allow an elegant means to visualize metrical complexity. All of the possible meters of a particular poet can be compared to those of another. The flow through a network also opens new sorts of metrics for comparative analysis.
                Further, as we will demonstrate, a walk through a directed graph can be colored in time with a particular audio or visual recording that has been marked up for phoneme timings as well as metrical units, allowing new sorts of insights that are not easily available in text alone.
                For listeners who have various levels of knowledge about meter, interactive versions of directed graphs can have instructive qualities. Listeners can learn the rules of prosodic systems by clicking on nodes that represent poetic data. 
                For poets, this method offers a visual means for composing verse. 
                While based in Urdu and Hindi, the methodologies described can be easily adapted and applied to a large number of South Asian and other languages to provide renewed access to poetry, conceived as data, in the digital age.
                
                    
                    Graphical representation of a sample Urdu/Hindi metrical pattern. Rectangles represent long units, circles short units, and dashed circles uncounted short units. Clusters represent metrical feet, here in the Perso-Arabic system.
                
                
                    
                    Graphical sample of an Urdu verse in the Hindi language interface. Text is rendered in the Devanagari script. Clusters represent metrical feet, here labeled using the traditional Perso-Arabic nomenclature. Word separation is indicated using an interpunct (⋅). 
                
                
                    
                    Graphical representation of a sample Urdu verse in the Urdu language interface. Text is rendered in the Perso-Arabic script using Perso-Arabic diacritical markings to show short vowels, which are normally unmarked. Clusters represent metrical feet, here labeled using traditional Perso-Arabic nomenclature.
                
                
                    
                    Graphical representation of a sample Urdu verse in the English language Interface. Text is rendered in scholarly transliteration. Clusters represent metrical feet in traditional Perso-Arabic prosody.
                
            
            
                Acknowledgments
                This research was supported by an Andrew W. Mellon New Directions Fellowship (grant number 11600613) and by matching funds from the College of Arts and Letters, Michigan State University.
            
        
        
            
                
                    Bibliography
                    Abdul-Rahman, A.,J. Lein, K. Coles, E. Maguire, M. Meyer, M. Wynne, C. R. Johnson, A. Trefethen, and M. Chen. (2013). Rule-based Visual Mappings –with a case study on poetry visualization. 
                        Computer Graphics Forum, 32: 381–390.
                    
                     Delmonte, R. (2015). Visualizing poetry with SPARSAR — Visual maps from poetic content. 
                        Proceedings of the Fourth Workshop on Computational Linguistics for Literature, Denver, CO, June 2015. 
                    
                    Fārūqī, S. (1968). 
                        Lafz o ma’nī. Allahabad: Shab-k̲h̲ūn kitāb ghar. 
                    
                    Gansner, E. R. and North, S. C. (2000). An open graph visualization system and its applications to software engineering. 
                        Software-Practice and Experience, 30(11): 1203–1233.
                    
                    McCurdy, N., Lein, J., Coles, K. and Meyer, M. (2016). Poemage: Visualizing the sonic topology of a poem. 
                        IEEE transactions on visualization and computer graphics, 
                        22 (1): 439–448.
                    
                    Nagasaki, H and Kim, R. I. (eds.) (2012). 
                        Indian and Persian Prosody and Recitation. Delhi: Saujanya Publications.
                    
                    Pritchett, F. W. and Khaliq, K. A. (1987). 
                        Urdu Meter: A Practical Handbook. Madison, WI: South Asian Studies, University of Wisconsin.
                    
                    Pybus. G.D. (1924). 
                        A Text-Book of Urdu Prosody and Rhetoric. Lahore: Rama Krishna. 
                    
                    Tabassum, M. (1983). 
                        Āvāz aur ādmī. Hyderabad, India: Iliyās Ṭreḍars.
                    
                
            
        
    
10029	2019	
        
            
                Introduction
                Over the last decades, with technological advancements such as growing digitalization and the development of social media platforms, the act of reading has transformed into an interactive experience (Cordón-García et al., 2013; Merga, 2015), where the Internet plays a key role (Murray, 2018). Social reading platforms like Goodreads and Wattpad are online environments where millions of people from all over the world come to share their love for the written word. Members come together to discuss what they have read and what they judge as good or bad literature, they recommend books to one another, and even try their hand at writing fiction.
                While a growing number of studies have been dedicated to this phenomenon (Nakamura, 2013; Ramdarshan Bold, 2016), so far only a few have adopted computational methods (Faggiolani and Vivarelli, 2016; Thelwall and Kousha, 2017) and none has combined these methods with empirical approaches to the study of literature and its effects.
                As the online environment is very different from the literary field as we know it, showing new types of complex interactions, we need to explore social reading and writing from both social and content perspectives. Social questions that should be investigated include interaction among users, questions of power (sources of literary authority), the effects on literacy and on reading behaviors, the changing system of social values (and of aesthetic evaluation). Content questions include questions about style, about the distribution and originality of comments, about the affective, reflective or social nature of content.
                With this panel, we will showcase the potential of studying social reading through the combination of multiple and interrelated approaches: from purely statistical, data-driven, and stylometric analyses, through qualitative and quantitative surveys of key users and a theory-driven qualitative taxonomy of reading valuation, towards a combination of the empirical and the computational, supported by a sound theoretical/methodological awareness. The substantial variety of case studies in four languages (English, German, Italian, and Dutch) will reflect the diversity of social reading, which can and should be studied from multiple points of view as well as with an array of methodological tools.
            
            
                Visualizing Wattpad
                Federico Pianzola
                Simone Rebora
                With this abstract, we focus on 
                    
                        Wattpad
                    , where social reading takes the form of a “discussion in the margin” (Stein, 2010), i.e. where texts are commented upon by millions of users, paragraph by paragraph. A central goal of our study is to understand how the corpus of the most appreciated texts on 
                    Wattpad is different from the canonical corpus of Western literature. For a preliminary analysis, we focused on the categories of “Classics” and “Teen Fiction” (TF).
                
                
                    Analysis I. For a first understanding of readers’ engagement, we examined the progression of the number of comments over the 20 most-commented books for each genre. Figures 1 and 2 show that TF is more stable, whereas for many Classics the majority of comments is on the first chapters. Also the total numbers are very different: 2,569,405 comments for the first TF title and just 42,013 for 
                    Pride and Prejudice.
                
                
                    
                
                Figure 1. Progression of the number of comments over the 20 most-commented Classics books
                
                    
                
                Figure 2. Progression of the number of comments over the 20 most-commented TF books
                
                    Analysis II. For a deeper understanding of reader response, we adopted sentiment analysis and compared the “emotional arcs” (Reagan et al., 2016) of both paragraphs and comments. The analysis was performed using the 
                    
                        Syuzhet
                     package on 6 books per genre: all details and limitations of the approach are described in (Rebora and Pianzola, 2018). Figures 3 and 4 show a better attunement between paragraphs and comments in TF: the highest Pearson's correlation (0.807; p-value &lt; 2.2e-16) was reached by TF title #3.
                
                
                    
                
                Figure 3. Emotional arcs for paragraphs and comments of six Classics books
                
                    
                
                Figure 4. Emotional arcs for paragraphs and comments of six TF books
                
                    Analysis III. To explore the relations between users while reading the books, we adopted network analysis (Scott, 2017) based on a very simple rule: the more two users reply to each other’s comments, the stronger is their connection. The visualizations were realized through the 
                    
                        Gephi
                     platform. Figures 5 and 6 show the networks of the most commented Classics and TF titles. To make the samples comparable, the networks were reduced to the 1,000 strongest connections. As evident, Classics readers tend to group in a single cluster, while TF readers group in multiple sub-clusters. This phenomenon seems to suggest that reading classics enhances the formation of a more homogeneous community.
                
                
                    
                
                Figure 5. Network graph of the 1,000 strongest connections between 
                    Pride and Prejudice readers
                
                
                    
                
                Figure 6. Network graph of the 1,000 strongest connections between 
                    The Bad Boy’s Girl readers
                
                
                    Analysis IV. To visualize 
                    Wattpad’s user distribution geographically, we analyzed a sample of 300,000 user profiles. Out of these, only 34.3% provide locations, many of which are fictitious (e.g. “Hogwarts” and “Wonderland”). We converted these locations into standardized state names with the help of the 
                    
                        Google
                    
                     
                    
                        Maps Place Autocomplete
                     API. Notwithstanding some errors (e.g. “Hell” was located in Denmark), the analysis provided confirmation to the supposed relevance of states like India and Philippines (Miller, 2015), together with the USA (see Fig. 7).
                
                
                    
                
                Figure 7. Geographic map of Wattpad users
                All these analyses are to be considered as preliminary to a more extensive and detailed study, but they efficiently showcase the high potential of investigating the social reading phenomenon through visualizations. 
                    Wattpad data urge us to rethink concepts like "world literature" and its dynamics. Of course, we cannot generalize these findings for books circulating through traditional publishing channels, but we can gain interesting insights for a more in-depth critical reflection on publishing and reading in the 21st century.
                
            
            
                Sources of authority in online book reviews
                Peter Boot
                It is a commonplace to state that with the advent of Amazon, book blogs and readers’ communities the role of traditional authorities in the literary field has become less important (McDonald, 2007). Rather than following the lead of professional critics, teachers or professors, readers are said to take advice from fellow readers. In my contribution to this panel, I will look at the evidence that a corpus of online Dutch book reviews can provide for answering the question which persons or institutions are considered authoritative by online reviewers.
                 In a pilot investigation, we have looked at references to possible authorities from a number of domains: traditional critics, newspapers, prizes, television programs, the book trade (publishers, booksellers, libraries), authors, teachers, websites and private contacts.
                Reviews were downloaded from some prominent weblogs, some dedicated (mass) review sites, the online magazine 8weekly and, for contrast, the most prominent Dutch newspaper NRC (Boot, 2013). We used collections of search terms and regular expressions to search the downloaded reviews using AntConc. Irrelevant hits were removed manually. A subset of the remaining results was annotated and assigned a 
                    type (type of person or institution that is assigned authority), 
                    role (role of reference to authority in review, e.g. the authority is mentioned as supporting the reviewer’s view or as the one who advised the reviewer to read the book) and 
                    agreement (whether the reviewer accepts the view of the authority). In all 1518 references to some form of authority were annotated. Because of many practical limitations and ad-hoc decisions, the procedure won’t give us any firm numbers but it does give a clear indication of sources of authority that are recognised in the online domain.
                
                
                    
                
                Figure 1. Main sources of authority in online reviews
                The main findings are summarized in Figure 1. The main sources of authority in the online reviews are companies (publishers and bookshops) and literary prizes. Authors (of the reviewed book or others) are also important. Online critics (‘peers’) are not unimportant, but print critics are hardly mentioned. Personal contacts (family and friends) are more important than print critics. Frequently, people refer to a medium rather than to a critic by name, but here too, online media are more often mentioned than print media. Teachers are almost invisible.
                 Much can be said about this. For one thing, we counted all references equal but some will weigh heavier than others. There are also important differences between the online platforms. The collection we used did not include reviews from booksellers’ sites. For now, though, the most important conclusion is that no, traditional critics are not highly valued on online platforms; however, the beneficiaries are commercial companies rather than fellow users of online platforms.
            
            
                Classifying the style(s) of criticism. A computational analysis of Italian book reviews
                Massimo Salgaro
                Simone Rebora
                In this abstract, we use a corpus of book reviews (Salgaro and Rebora, 2018) to answer the following question: how do professional critics, journalists and passionate readers differ in the writing of reviews and what features can be used to identify them?
                The corpus is divided into three subsets: reviews published on social reading platforms (source: 
                    aNobii), in paper magazines (
                    Il Sole 24 Ore), and in scientific journals (
                    Between, 
                    Osservatorio critico della germanistica, and 
                    OBLIO). All sub-corpora have an approximate size of 650,000 tokens.
                
                First, we adopted stylometry to classify the texts. As demonstrated by (Eder, 2015), the first element to influence the quality of a stylometric classification is text length. Preliminary tests—ran with the 
                    Stylo R package (Eder et al., 2016) on 5,000-word-long text chunks—showed how Cosine Delta distance, based on just 50 MFW, was able to almost perfectly separate the three subgroups (see Fig. 1). Considered the high variance of text length in our corpus (mean = 259 words; SD = 363 words), we artificially generated a series of sub-corpora composed by text chunks of the same length (varying between 50 and 5,000 words) and we evaluated clustering quality through the adjusted Rand index in the 
                    PyDelta Python library. Figure 2 confirms how Cosine Delta distance with 2,000 MFW is the best performing classifier (Evert et al., 2017), but also 200 MFW (i.e., mainly function words) reach a similar—and, in some cases, even better—efficiency. As for text length, clustering quality is quite poor below 1,000 words, while a plateau is reached at about 3,000 words.
                
                
                    
                
                Figure 1. Network graph of the corpus (Cosine Delta, 50 MFW, ForceAtlas2 in 
                    Gephi). In green: 
                    aNobii; in violet: 
                    Sole 24 Ore; in red: 
                    Between, 
                    Osservatorio, and 
                    OBLIO.
                
                
                    
                
                Figure 2. Clustering quality per slice length and MFW used (distance: Cosine Delta)
                To improve the results for shorter chunks, we developed a framework for a machine learning classifier, by operationalizing a series of traditional definitions of literary criticism (e.g. Eco, 1979; Gardt, 1998; Rodler, 2004; Colussi, 2017). An extensive lexicon of literary criticism (Beck et al., 2007) was translated into Italian; selections of terms related to mental imagery and emotional aesthetic response were extracted from questionnaires and tools in empirical aesthetics (e.g. Knoop et al., 2016; Fialho et al., in press), translated into Italian, and expanded through the 
                    fastText Italian word-embedding model (Joulin et al., 2017). These resources—together with selected features in the 
                    LIWC Italian dictionary—were used to measure emotional and cognitive involvement with the reviewed text. The measurements were combined with the results of the stylometric analysis (Cosine Delta, 2,000 MFW) and used to train an SVM classifier.
                
                For a corpus composed by 500-word-long chunks (the length of this abstract), the sole stylometric analysis reached an attribution accuracy of 90.1%, while the SVM classifier scored 93.2%. A slight but promising improvement, if we consider the simplicity of the framework—that can and should be refined further.
                With this paper, we hope to have cast the groundwork for a research that might fruitfully combine computational methods and literary theory to study the “style of criticism” of professional and non-professional readers.
            
            
                Shared Reading. Digital Reading and Writing of Literature
                Gerhard Lauer
                Maria Kraxenberger
                In the digital age, the practice and habits of reading change fundamentally. Not few speak of the end of the book and of (deep) reading (e.g., Wolf 2007). However, and contrary to this popular statement, reading and writing literature has never been practiced as intensely as within digital societies (Lauer 2018, 2019) – a development that involves in particular young, adolescent readers.
                In preliminary studies, we categorized various reading and writing platforms and conducted a first study on Harry Potter-fanfiction, using a personality questionnaire (Paulus 2016). Results indicate a dominance of romance as well as of fantasy genres (see also Odag 2008) and that authors of fanfiction are usually between 14 and 20 years old. They show both introverted and extroverted personality traits and come from diverse educational backgrounds. Generally, they can be considered rather empathetic. In a similar vein, other reading research suggests that dealing with literary texts increases performance in Theory of Mind, and thus might the support the development of prosocial skills such as empathy (Kidd &amp; Castano 2013).
                Despite justified criticism on over-generalizations (e.g., van Kuijk et al. 2018), research on the emerging changes towards digital, joint reading of texts with presumably much lower literary demands and its potential implications is missing. This is the more surprising, since first explorative studies in schools indicate that digital formats can be used very well for education through writing and reading (Bertschi-Kaufmann &amp; Graber 2007, Wampfler 2016) and not only shed light on, but also foster a variety of effects. Nevertheless, it is still unclear how and with what consequences digitization affects the reading behavior of adolescents, including complex processes such as (higher) language acquisition, establishments of social peer-groups and their interactions, as well as their understanding and appreciation of literature and the written word.
                Our project focuses on the changing social processes that accompany the reading and writing of literature in the digital age. In doing so, we focus on the young readers and their distinct reading and writing socialization on digital reading (and writing) platforms such as Wattpad. Based on a social interactionist-reading model and the theoretical concept of “scaffolding” (see, for example, Bruner 1983, Nation 2018, Quasthoff 2011) the project integrates descriptive, qualitative and quantitative methods in a multi-methodological approach.
                We present a questionnaire that combines established scales from social psychology with items used in reading research to explore online and offline reading habits. Using new methods of field research enables a better understanding of how intensively young readers think and feel about reading and writing literature and what kind of values are used to talk about literature in social media. To arrive at a more comprehensive picture of reading in the digital age, we provide a general theoretical framework that helps to integrate the thus acquired data.
            
            
                Where’s your attention? An empirical assessment of Web 2.0 users’ literary values
                J. Berenike Herrmann
                Thomas Messerli
                Traditionally, reading fiction has been seen as a tool for honing crucial sense-making capacities, enabling an integrated sensual and intellectual personal development (
                    judicium sensitivum, Wolff, 1738; cf. Lauer, 2012). Web 2.0 users, on the other hand, are understood not so much as interested in the challenges and pleasures of literature-as-art, but in the easy gratification of popular genres, driven by an economy of attention (Franck, 2004). Users’ book reviews are held to show an affirmative bias, documenting the lack of a deep and discerning reading engagement (Ingold, 2014). Mirroring a stated trend towards a “digestible” documentary and authentic literature (Röhricht, 2016), lay reviewers are held to operate with a heavy content bias and to neglect formal criteria.
                
                However, many of these verdicts lack comprehensive empirical support, leaving questions such as the following ones open: What do web 2.0 readers actually do when judging literary value? What are the grounds for value judgements? Do readers apply aesthetic and ethical premises? Do they prioritize content over formal criteria? In our paper, we will scrutinize a corpus of lay literature reviews published on the German reading platform lovelybooks.de for the premises underlying users’ valuation statements.
                Using sentiment analysis and rule-based techniques as a bootstrap methodology for semi-automatic identification of valuation statements in our corpus (ca. 1 Mio reviews), our exploratory case study analyzes the lovelybooks.de categories ‘classics’ and ‘novels’ Combining quantitative and qualitative methods, we examine how users evaluate literary texts in terms of quality criteria such as subjectively perceived effects, but also more formal ones, including authorial style, literary character motivation and plot construction (motivation, narrative arc, suspense).
                In addition to semi-automatic coding and running KWICs of recurrent phrases and keywords, we analyze a number of reviews as integral cases. How do users develop their (subjective) assessments? How do assessments appear in terms of tone and habitus? How do users prioritize the different categories for their evaluation? The qualitative coding is aimed at fine-tuning a coding schema that adapts the axiological model by Heydebrand &amp; Winko (1996) for web 2.0 lay reviews.
                Our first results do indeed point to users’ heightened subjectivity and a tendency to plain statements, reporting subjectively perceived effects (
                    a bit too melancholic, I found it a pity) and marking taste (
                    I liked it well). Yet, we found considerable attention to formal aspects such as plot construction (
                    some of the events were not plausible to me) and character motivation (
                    the characters were well crafted), as well as critical (not only affirmative) judgements. What is more, our results indicate that many users do apply a plain and subjectively toned language, but handle a reflected taxonomy of value criteria to support their taste judgements, expressed by statements such as 
                    Noch schwerer wog bei der Sternevergabe … [The awarding of stars was even more influenced by…], 
                    Ein Punkt, der mir sehr wichtig ist [A point that is very important to me]. We thus suggest that instead of propelling a decline of intellectual scrutiny and differentiation, online reading platforms offer a potential for personal insight and development. Our study is one of the first few data-driven approaches to scrutinize the literary values underlying people’s engagement with literary texts in a participatory culture.
                
            
            
                Empirical Goodreads
                Simone Rebora
                Moniek Kuijpers
                Piroska Lendvai
                Our research aims to complement computational linguistics with methods used in empirical literary studies to investigate the experience of “getting lost in a book”. Using software from the field of natural language processing, we match the 18 statements from the Story World Absorption Scale (SWAS, cf. Kuijpers et al., 2014) – a questionnaire used to identify absorbing reading experiences – to reader reviews posted on the online platform
                     
                    
                        Goodreads
                    . The SWAS taps into four different aspects of absorbed reading, namely sustained concentration (Attention), vivid imagery of the story world (Mental Imagery), feelings of empathy or sympathy for the characters of a story (Emotional Engagement) and the sensation of having made a deictic shift from real world to story world (Transportation). The reader reviews on Goodreads more often than not include descriptions of people’s experiences with certain books and evaluations of their reading experiences, and therefore it could be argued that they fall somewhere between accounts of reader response and literary criticism. They certainly constitute a rare treasure trove of qualitative, user-generated data on reading and reading evaluation.
                
                The aims of our project are twofold: 1) validating the SWAS, and 2) enabling comparative analyses of absorption across different books, genres, and reader groups.
                We have performed a manual analysis on 180 Goodreads reviews of three contemporary blockbuster novels (representative of the Fantasy, the Romance, and the Thriller genre), confirming that, in many cases, SWAS statements and particular sentences in Goodreads reviews overlap substantially.
                     For example, 
                    
                        one reviewer
                     writes: “I’m so absorbed in the world Martin produced out of his wits” (a sentence that matches with SWAS statement A3: “I felt absorbed in the story”); while
                     
                    
                        another reviewer
                     expresses her identification with the main character: “I went through all the emotional ups and downs right along with her” (matching with EE4: “I felt how the main character was feeling”). A total of 130 matching sentences were identified (for details, see Fig. 1).
                
                
                    
                
                Figure 1. SWAS/Goodreads matches. 
                    A1-A5: Attention, 
                    T1-T5: Transportation, 
                    EE1-EE5: Emotional Engagement, 
                    MS1-MS3: Mental Imagery
                
                In order to extend the analysis to the entire Goodreads corpus, which collects about 80 million book reviews, we combine two technologies: textual entailment detection software, i.e., EOP (Magnini et al., 2014) and text reuse detection software, i.e., TRACER (Büchler et al., 2018). Preliminary experiments (Rebora et al., 2018) show that both tools need adaptation and training for this specific task, as our best “out of the box” recall score is 0.28, while training on the manually-annotated reviews increases recall to 0.49.
                Based on the 130 identified sentences plus the 18 SWAS statements, we defined a provisional “absorption lexicon” and expanded it through a word-embedding model (Mikolov et al., 2013) based on 2.5 million reviews on Goodreads (total tokens ~ 400 million). Figure 2 shows a synthetic representation of this lexicon.
                
                    
                
                Figure 2. Absorption lexicon (word dimensions symbolize their weights).
                The lexicon was used to identify, through a standard “bag-of-words” approach, the reviews that showed the highest levels of absorption. The results, while significant in themselves (see Figs. 3 and 4 for sample visualizations), are used as the starting point for extensive semi-automatic annotation work – with four annotators working in parallel for a total of 18 months, starting in December 2018. Our goal will be that of producing a ground truth corpus as training data for machine learning algorithms, towards a fully-automated matching of Goodreads reviews with the different aspects of absorbed reading.
                
                    
                
                Figure 3. Network graph based on the absorption scores for the four SWAS categories.
                
                    
                
                Figure 4. Zoom of Fig. 3.
            
        
        
            
                
                    Bibliography
                    
                        Beck, R., Kuester, H. and Kuester, M. (2007). 
                        Basislexikon anglistische Literaturwissenschaft. Paderborn: Fink.
                    
                    
                        Bertschi-Kaufmann, A. (2007). 
                        Lesekompetenz, Leseleistung, Leseförderung Grundlagen, Modelle und Materialien. Seelze-Velber: Klett, Kallmeyer
                    
                    
                        Boot, P. (2013). The desirability of a corpus of online book responses. Second Workshop on Computational Linguistics for Literature. Retrieved 2015-12-27 from http://www.aclweb.org/anthology-new/W/W13/W13-1405.pdf
                    
                    
                        Bruner, J. S. (1983). 
                        Child’s Talk: Learning to Use Language. Oxford: Oxford University Press.
                    
                    
                        Büchler, M., Franzini, G., Franzini, E. and Bulert, K. (2018). TRACER – a multilevel framework for historical text reuse detection, 
                        Journal of Data Mining and Digital Humanities – Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages [in press].
                    
                    
                        Colussi, D. (2017). 
                        Stili della critica novecentesca: Spitzer, Migliorini, Praz, Debenedetti, Sereni. Roma: Carocci.
                    
                    
                        Cordón-García, J.-A., Alonso-Arévalo, J., Gómez-Díaz, R. and Linder, D. (2013). 
                        Social Reading. Oxford: Chandos.
                    
                    
                        Eco, U. (1979). 
                        Lector in fabula: la cooperazione interpretativa nei testi narrativi. Milano: Bompiani.
                    
                    
                        Eder, M. (2013). Does size matter? Authorship attribution, small samples, big problem. 
                        Digital Scholarship in the Humanities, 
                        30(2): 167–82 doi:10.1093/llc/fqt066.
                    
                    
                        Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: A Package for Computational Text Analysis. 
                        The R Journal, 
                        8(1): 107–21.
                    
                    
                        Evert, S., Proisl, T., Jannidis, F., Reger, I., Pielström, S., Schöch, C. and Vitt, T. (2017). Understanding and explaining Delta measures for authorship attribution. 
                        Digital Scholarship in the Humanities, 
                        32(suppl_2): ii4–ii16, doi:10.1093/llc/fqx023.
                    
                    
                        Faggiolani, C. and Vivarelli, M. (eds). (2016). 
                        Le Reti Della Lettura : Tracce, Modelli, Pratiche Del Social Reading. Milano: Editrice Bibliografica.
                    
                    
                        Fialho, O., Hoeken, H. and Hakemulder, F. (in press). Literary Imagination and Changing Perceptions of Self and Others: an Explanatory Model of Transformative Reading.
                    
                    
                        Franck, G. (2004). 
                        Ökonomie der Aufmerksamkeit : ein Entwurf ([8. Aufl.]). München: C. Hanser.
                    
                    
                        Gardt, A. (1998). Die Fachsprache der Literaturwissenschaft im 20. Jahrhundert. In Hoffmann, L., Kalverkämper, H. and Wiegand, H. E. (eds), 
                        Fachsprachen. Berlin, New York: de Gruyter, pp. 1355–62.
                    
                    
                        Heydebrand, R. von and Winko, S. (1996). 
                        Einführung in die Wertung von Literatur : Systematik - Geschichte - Legitimation. Paderborn, Zürich [etc.]: Schöningh.
                    
                    
                        Ingold, F. P. (2014). Laienherrschaft – in Klagenfurt und anderswo. Volltext, 3. Retrieved from
                         
                        
                            https://www.lyriktext.de/ingold-essays/laienherrschaft-n-in-klagenfurt-und-anderswo/
                        
                    
                    
                        Joulin, A., Grave, E., Bojanowski, P. and Mikolov, T. (2017). Bag of Tricks for Efficient Text Classification. 
                        Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers. Association for Computational Linguistics, pp. 427–431.
                    
                    
                        Kidd, D. C. and Castano, E. (2013). Reading literary fiction improves theory of mind. 
                        Science, 
                        342(6156): 377–380.
                    
                    
                        Knoop, C. A., Wagner, V., Jacobsen, T. and Menninghaus, W. (2016). Mapping the aesthetic space of literature ‘from below’. 
                        Poetics, 
                        56: 35–49 doi:10.1016/j.poetic.2016.02.001.
                    
                    
                        Kuijpers, M., Hakemulder, F., Tan, E.E. and Doicaru, M.M. (2014). Exploring absorbing reading experiences. Developing and validating a self-report scale to measure story world absorption. 
                        Scientific Study of Literature, 
                        4(1): 89–122.
                    
                    
                        Lauer, G. (2012). Die Sinne und die Einbildungskraft. Zu Johann Gebhard Ehrenreich Maaß' Versuch über die Einbildungskraft im Kontext der Frühgeschichte der Psychologie. In Décultot, E. and Lauer, G. (eds). 
                        Kunst und Empfindung. Zur Genealogie einer kunsttheoretischen Fragestellung in Deutschland und Frankreich im 18. Jahrhundert. Heidelberg: Winter, pp. 157-173
                    
                    
                        Lauer, G. (2018). Instagram-Poesie. Über das digitale Popmärchen Rupi Kaur. 
                        Neue Zürcher Zeitung, 1. Juni 2018, 
                        
                            https://www.nzz.ch/feuilleton/gefuehl-ist-alles-lyrik-iminternet-ld.1369814
                         (accessed 27-11-2018).
                    
                    
                        Lauer, G. (2019). 
                        Lesen im digitalen Zeitalter. Darmstadt [in press].
                    
                    
                        Magnini, B., Zanoli, R., Dagan, I., Eichler, K., Neumann, G., Noh, T. G. and Levy, O. (2014). The Excitement Open Platform for textual inferences. In 
                        Proceedings of ACL Demo Session. Baltimore: ACL, pp. 43–48.
                    
                    
                        McDonald, R. (2007). 
                        The death of the critic. London, New York: Continuum International Publishing Group.
                    
                    
                        Merga, M. K. (2015). Are Avid Adolescent Readers Social Networking About Books?. 
                        New Review of Children’s Literature and Librarianship, 
                        21(1): 1–16 doi:10.1080/13614541.2015.976073.
                    
                    
                        Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. and Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In 
                        Advances in neural information processing systems, pp. 3111-3119.
                    
                    
                        Miller, M. (2015). What Wattpad Brings to the Publishing Table. PUB800, 
                        
                            https://tkbr.publishing.sfu.ca/pub800/2015/12/what-wattpad-brings-to-the-table/
                         (accessed 27-09-2018).
                    
                    
                        Murray, S. (2018). 
                        The Digital Literary Sphere: Reading, Writing, and Selling Books in the Internet Era. Baltimore: Johns Hopkins University Press.
                    
                    
                        Nakamura, L. (2013). ‘Words with friends’: Socially networked reading on Goodreads. 
                        Pmla, 
                        128(1): 238–43 doi:10.1632/pmla.2013.128.1.238.
                    
                    
                        Nation, K. (2018). What Teachers Need to Know about Shared Reading. 
                        Times Education Supplement, 9 March 2018, 
                        
                            https://www.tes.com/news/what-teachers-need-know-about-shared-reading
                         (accessed 27-11-2018).
                    
                    
                        Odağ, Ö. (2008). Of men who read romance and women who read adventure-stories… An empirical reception study on the emotional engagement of men and women while reading narrative texts. In Auracher, J. and Peer, W. van (eds), 
                        New Beginnings in Literary Studies. Newcastle: Cambridge Scholars Press, pp. 308–29.
                    
                    
                        Paulus, C. (2016). Saarbrücker Persönlichkeits-Fragebogen (SPF). Based on the Interpersonal Reactivity Index (IRI), V6.2.
                    
                    
                        Quasthoff, U. M. (2011). Diskurs- und Textfähigkeiten. Kulturelle Ressourcen ihres Erwerbs. In Hoffmann, L., Leimbrink, K. and Quasthoff, U. M. (eds), 
                        Die Matrix der menschlichen Entwicklung. Berlin: De Gruyter, pp. 210–51.
                    
                    
                        Ramdarshan Bold, M. (2016). The return of the social author: Negotiating authority and influence on Wattpad. 
                        Convergence: The International Journal of Research into New Media Technologies doi:10.1177/1354856516654459. http://con.sagepub.com/cgi/doi/10.1177/1354856516654459.
                    
                    
                        Reagan, A. J., Mitchell, L., Kiley, D., Danforth, C. M. and Dodds, P. S. (2016). The emotional arcs of stories are dominated by six basic shapes. 
                        EPJ Data Science, 
                        5(1): 31.
                    
                    
                        Rebora, S., Lendvai, P. and Kuijpers M. (2018). Reader experience labeling automatized: Text similarity classification of user-generated book reviews. In 
                        EADH 2018 Book of Abstracts [in press]
                    
                    
                        Rebora, S. and Pianzola, F. (2018). A New Research Programme for Reading Research: Analysing Comments in the Margins on Wattpad. 
                        DigitCult - Scientific Journal on Digital Cultures, 
                        3(2): 19–36 doi:10.4399/97888255181532.
                    
                    
                        Rodler, L. (2004). 
                        I termini fondamentali della critica letteraria. Milano: B. Mondadori.
                    
                    
                        Röhricht, K. (2016). 
                        Wettlesen um den Ingeborg-Bachmann-Preis : Korpusanalyse der Anthologie “Klagenfurter Texte” (1977-2011). Innsbruck, Wien, Bozen: Studien Verlag.
                    
                    
                        Salgaro, M. and Rebora, S. (2018). Measuring the ‘Critical Distance’. A Corpus -Based Analysis of Italian Book Reviews. In Spampinato, D. (ed), 
                        AIUCD2018 - Book of Abstracts. pp. 161–63 doi:10.6092/unibo/amsacta/5997. http://amsacta.unibo.it/id/eprint/5997
                    
                    
                        Scott, J. (2017). 
                        Social Network Analysis. Los Angeles; London; New Delhi; Singapore; Washington, DC; Melbourne: Sage.
                    
                    
                        Stein, B. (2010). A Taxonomy of Social Reading: a proposal http://futureofthebook.org/social-reading/.
                    
                    
                        Thelwall, M. and Kousha, K. (2017). Goodreads: A social network site for book readers. 
                        Journal of the Association for Information Science and Technology, 
                        68(4): 972–83 doi:10.1002/asi.23733.
                    
                    
                        Van Kuijk, I., Verkoeijen, P., Dijkstra, K. and Zwaan, R. A. (2018). The Effect of Reading a Short Passage of Literary Fiction on Theory of Mind: A Replication of Kidd and Castano (2013). 
                        Collabra: Psychology, 
                        4(1): 7 doi:10.1525/collabra.117.
                    
                    
                        Wampfler, P. (2016). Schreiben in sozialen Netzwerken - vier Szenarien. In Knopf, J., Abraham, U. and Schneider Verlag Hohengehren GmbH (eds), 
                        Deutsch Didital Band 2 Praxis. Baltmannsweiler: Schneider Verlag Hohengehren, pp. 84–90.
                    
                    
                        Wolf, M. (2007). 
                        Proust and the Squid: The Story and Science of the Reading Brain. New York, N.Y: HarperCollins.
                    
                    
                        Wolff, C. (1738). 
                        Psychologia empirica, methodo scientifica pertractata, qua ea, quae de anima humana indubia experientiae fide constant, continentur et ad solidam universae Philosophiae practicae ad Theologice naturalis tractationem via sternitur. Autore Christiano Wolfio. Potentissimi Suecorum regis etc. Consiliareo Regiminis Mathematum ad Philsosophia Professore Primario in Academia Marburgensi etc. Editio nova priori emendatior. Francofurti; Lipsiae: In Officina Libraria Rengeriana.
                    
                
            
        
    
10036	2019	
        
            The recent appearance of Steven Pinker’s 
                Enlightenment Now is a topical reminder of the enduring importance of 18th-century legacies in contemporary thought, culture, and politics. Considered pernicious or positive, French intellectuals began assessing the legacy of 
                les Lumières almost from the outset of the events of 1789 and continued this polemic throughout the 19th century. While the study of pro- and anti-Enlightenment or Revolution writers active during the 19th century is certainly of great value, our work in this project aims to examine the complexities of Enlightenment legacies using new “distant reading” approaches (Underwood, 2017). To this end, and in conjunction with the Observatoire de la Vie Littéraire (OBVIL) at Sorbonne Université, we have developed a new generation of sequence alignment software that detects reused passages in very large corpora; we use this software to compare several important collections of 18th-century texts to the Très Grande Bibliothèque (TGB) corpus of 19th-century printed materials made available by the Bibliothèque Nationale de France (BNF). Following an overview of the datasets and software developed for this effort, we will sketch some preliminary results arising from this project and conclude with an outline of further work we will carry out based on this database. 
            
            We used three different text collections for this project. The 18th-century sample is drawn from the holdings of the ARTFL Project and includes the 
                Encyclopédie of Diderot and d’Alembert as well as 1,367 17th-18th century texts from the ARTFL Frantext database.
                
                     See http://encyclopedie.uchicago.edu/ and http://artfl-project.uchicago.edu/content/artfl-frantext/.
                 Both are well-curated collections and provide solid samples of Enlightenment discourse. For the 19th century, we were able to employ selections of the TGB collection released by the BNF in conjunction with OBVIL. The collection consists of 128,441 documents by more than 58,000 authors almost all of which have metadata drawn from the BNF catalogue. The vast majority of the collection was published during the 19th century, though this includes a significant number of reprints of older texts. The collection contains a broad selection of themes and subjects with 35,710 documents listed as 
                Littérature, 28,885 as 
                Histoire de la France and 23,776 
                Droit. As expected, the quality of the raw data – based on uncorrected Optical Character Recognition – varies widely depending on a range of factors, including age, preservation status and print quality. In order to identify those texts that were originally published before 1800, we used a series of heuristics based on the metadata provided by the BNF to eliminate duplicates and near-duplicate texts. This left 112,907 documents in our working TGB sample.
            
            While the ARTFL Project has developed text alignment packages in the past (Horton et al., 2010; Roe, 2012), this system is less-suited for very large-scale comparisons, e.g., those in 100,000+ document range. Detecting identical or similar passages requires a one-to-one document comparison of every text in the dataset (Gladstone and Cooney, 2020). Given the scale of the TGB dataset, we developed the TextPAIR system to address limitations of the previous model using new technologies. Installed as a Python package, it includes a text preprocessing component written in Python, a sequence aligner written in Go to maximize speed and scalability, and a single-page web application written with the VueJS framework to guarantee maximum interactivity when text alignments are deployed in the browser. The package is available as open-source code on Github, with accompanying documentation meant to assist other research groups in installing and running their own text-reuse experiments.
                
                     See https://github.com/ARTFL-Project/text-pair.
                
            
            The sequence alignment of the pre 19th-century sample of Frantext and the 
                Encyclopédie against the 112,000 documents of the TGB produced a large number of resulting 
                passage pairs, our basic unit of analysis. Figure One shows a typical alignment pair, in this case a passage from the famous 
                Discours Préliminare reused with some indication of the source in Peignot’s 1801 
                Dictionnaire raisonné de bibliologie. It is important to note that TextPAIR can detect similar passages with considerable variations which can arise from textual insertions, deletions or modifications along with data capture errors, differences in spellings and word order changes. Figure 1 uses the “show differences” feature to highlight the variations between the passage pair.
            
            
                
            
            
                Figure 1
            
            Each record of the result database stores metadata for each document of the pair from the TEI headers, byte locations and offsets in the corresponding text data files, the passages in question, the size of the alignments, and whether or not the alignment is considered “banal” or uninteresting. The databases are loaded into a PostgreSQL relational database with a dedicated interface to allow users to query the document pairs, get summary results and navigate to the original documents at will. Figure 2 shows the query form of the 
                Encyclopédie to TGB alignment database, which supports metadata queries to allow the user to focus on specific questions, in this case a search for all aligned passages from articles written by Rousseau.
            
            
                
            
            
                Figure 2. Searching for similar passages from articles by Rousseau in the Encyclopédie
            
            The query returns 611 passages, as shown in Figure 3, where the first reused passage in this query is his article “Accolade”, which is found almost verbatim in a dictionary of music from 1825. The query interface makes extensive use of facets, allowing the user to consult frequencies broken down by different criteria. Looking at the reuses of Rousseau’s contributions to the 
                Encyclopédie, it is interesting to note that while most of Rousseau’s entries in the 
                Encyclopédie were about music, it is his political philosophy article “ECONOMIE” that is most reused in the 19th century.
            
            
                
            
            Figure 3
            The interface also supports the generation of time series graphs of the results. Figure 4 shows that reuses of the article “ECONOMIE” was fairly consistent through the 19th century. 
            
                
            
            Figure 4
            The Baron d’Holbach presents another interesting case. As one of the 
                philosophes with the most notorious reputation as a free-thinking materialist he contributed some of the most controversial articles to the 
                Encyclopédie, such as “Représentants” and “Prêtres”. As shown in Figure 5, it was however his work on chemistry, mineralogy, and German history that is most reused in the 19th century. Instead of his scandalous article on “Prêtres” being cited, as one would expect, you find resonances of the rather orthodox article “EVÊQUE” which outlines the historical background of elector Bishops under the Holy Roman Empire. In fact, not one reuse of d’Holbach’s controversial material was found in the TGB, which sheds new light on our vision of d’Holbach as not simply an atheist propagandist, but as a man of science whose articles in various domains continued to be cited and used well into the 19th century. This is an image of d’Holbach that rarely, if ever, occurs in modern intellectual and literary histories.
            
            
                
            
            Figure 5
            The 19th-century reuses of passages from the 17 texts by Rousseau found in ARTFL Frantext, show a similar combination of expected and unexpected avenues of influence. It is not particularly surprising to find the nearly 1,500 instances of passages from his 
                Contrat social in works dealing with political theory even if they are used in a negative fashion. As shown in Figure 6, the most frequent reuse is in Pierre Landes’ attack on the 
                philosophe in his 
                Principes du droit politique, mis en opposition avec ceux de J.-J. Rousseau sur le contrat social followed by numerous expositions of Rousseau’s political thought.
            
            
                
            
            Figure 6
            By contrast, as shown in Figure 7, the over 10,000 reuses of Rousseau’s work more generally seem to focus on his reputation as a prose stylist, with the most frequent reuses found in various dictionaries and grammatical works. It is important to note the various vectors through which particular texts or authors can exert influence, even if it is indirect.
            
                
            
            Figure 7
            
                We believe that we can begin to use these techniques and these sorts of large-scale databases to refashion literary history, to give a more expansive vision of literary culture by identifying various forms of intertextual activity, from reuse to referencing, in a broadened set of 18th-century corpora and to eventually make use of various visualisation tools to navigate the output. While our interpretive work on this set of reuses is still in its initial phases, we have already been able to identify significant findings that challenge our understanding of the impact of the 
                Lumières in the 19th century. 
                
                Our full paper will expand on our observations above and begin the systematic exposition of the various complexities of identifying text reuse at such an unprecedented scale. We are aware, however, that these larger questions are well beyond the ability of any small group of researchers to explore, and thus invite interested parties to consult the alignment databases themselves.
                
                     See http://artfl-project.uchicago.edu/legacy_eighteenth.
                
            
        
        
            
                
                    Bibliography
                    
                        Gladstone, G. and Cooney, C. (2020) Opening New Paths for Scholarship: Algorithms to Track Text Reuse in ECCO. In Digitizing Enlightenment: Digital Humanities and the Transformation of Eighteenth-Century Studies. Liverpool: Oxford University Studies in the Enlightenment (forthcoming).
                    
                    
                        Horton, R., Olsen, M., and Roe, G. (2010). Something Borrowed: Sequence Alignment and the Identification of Similar Passages in Large Text Collections. Digital Studies / Le Champ numérique, 2.1. http://doi.org/10.16995/dscn.258 (accessed 2 April 2019).
                    
                    
                        Roe, G. (2012). Intertextuality and Influence in the Age of Enlightenment: Sequence Alignment Applications for Humanities Research. In Digital Humanities 2012. Hamburg: University of Hamburg, pp. 345-47. 
                    
                    
                        Underwood, T. (2017). A Genealogy of Distant Reading. Digital Humanities Quarterly, 11.2. http://www.digitalhumanities.org/dhq/vol/11/2/000317/000317.html (accessed 2 April 2019).
                    
                
            
        
    
10048	2019	
        
            Although there is a scientific consensus that gender is not binary, immutable, and physiological, it is still common to operationalize it in such a way. This can be said of both traditional as well as new sources of digitized or digitally born data. Not long ago, population surveys were treating sex and gender interchangeably requiring interviewers to ask a question about “sex/gender” only if it was not obvious from person’s voice (Bradburn et al., 2004). Big data and Internet sources imposed a challenge of large amounts of messy data, where gender is often operationalized and coded with the help of automatic detection methods (e.g., based on names or face recognition). Recently, there have been more attempts to critically assess and change these exclusive practices in both traditional (Bauer et al., 2017) and digital methods (Keyes, 2018). This contribution joins these efforts by describing our attempt to measure gender of film directors by relying on their own chosen self-representation. We also compare our results to alternative findings when binary manual and automatic gender detection methods are used. In communicating the comparisons, we visualize the data to invite others to critically reflect on current practices of gender operationalization.
            The research we report on in this poster stems from a study that explores a substantive question of temporal and spatial patterns of festival runs (movement of films through the films festival networks) from a film studies perspective (Loist 2011; Loist 2016). The aim of the project is to explore festival runs of 1727 films selected from a sample of six relevant international film festivals starting from their premiere and ending with their commercial release or lack thereof. Gender of directors constitutes an important piece of information due to known discriminatory practices in film industry (Aylett, 2016). Our data sample collected from six festival programs included 1727 international director teams (that corresponds to 1899 directors). In our operationalization of gender, we focused on directors’ ways to use personal pronouns on available web resources (e.g., personal websites, Wikipedia pages). The web search was implemented manually by project assistants. If a person used male or female pronouns, a female or male gender was assigned accordingly. However, if a person used certain cues to indicate a self-identification with non-binary gender, a non-binary category was assigned. We report data on used cues, missing data, as well as challenges and limitations of the implemented web search (e.g., limited data on the authorship of the web resources). In addition, we compare the findings to manual non-consensual assignment of gender based on names and photos as well as automatic detection methods commonly used to assign gender in large databases. For the latter, we applied two widely used tools: Sexmachine implemented in Python (Elmas, 2013) and Genderize implemented in R (Wais, 2016). While Sexmachine primarily relies on name and country data, Genderize is a commercial application that also uses social media data (Karimi et al., 2016).
            Results are communicated via visualizations guided by a feminist approach to information visualization and practice (D’Ignazio and Klein, 2015). This approach stresses the importance of examining the entire life cycle of the project against such principles as challenging binary categories, transparent communication of project’s decision making, embracing horizontal systems of knowledge transmission, embedding data into a context, as well as visibility of labor. We conclude by discussing possible non-binary gender measurements at scale as well as their ethical and data privacy challenges.
        
        
            
                
                    Bibliography
                    
                        Aylett, H. (2016). 
                        Where Are All the Women Directors? Report on Gender Equality for Directors in the European Film Industry. 
                        EWA Women Directors in Film – Comparative Report. 
                        https://www.ewawomen.com/wp-content/uploads/2018/09/Complete-report_compressed.pdf (accessed 1 November 2018)
                        .
                    
                    
                        Bauer, G. R., Braimoh, J., Scheim, A. I., and Dharma, C. (2017). Transgender-inclusive measures of sex/gender for population surveys: Mixed-methods evaluation and recommendations. 
                        PLOS ONE, 12: e0178043.
                    
                    
                        Bradburn, N., Sudman, S., and Wansink, B. (2004). 
                        Asking questions: The Definitive Guide to Questionnaire Design - For Market Research, Political Polls, and Social and Health Questionnaires. San Francisco (CA): Jossey-Bass.
                    
                    
                        D’Ignazio, C., and Klein, L. F. (2016). Feminist data visualization. In 
                        IEEE Workshop on Visualization for the Digital Humanities (VIS4DH). 
                        Baltimore: IEEE.
                        http://www.kanarinka.com/wp-content/uploads/2015/07/IEEE_Feminist_Data_Visualization.pdf (accessed 1 September 2018).
                    
                    
                        Elmas, F. (2013). 
                        Sexmachine [Python package]. 
                        https://pypi.org/project/SexMachine/ (accessed 1 October 2018).
                    
                    
                        Karimi, F., Wagner, C., Lemmerich, F., Jadidi, M., and Strohmaier, M. (2016). Inferring gender from names on the web: a comparative evaluation of gender detection methods. In 
                        WWW ’16 Companion, Proceedings of the 25th International Conference Companion on World Wide Web. Republic and Canton of Geneva, Switzerland: International World Wide Web Conferences Steering Committee, pp. 53–54.
                    
                    
                        Keyes, O. (2018). The misgendering machines: trans/HCI implications of automatic gender recognition. 
                        Proceedings of the ACM on Human-Computer Interaction, 2: 88:1
                        ‐88:22.
                    
                    
                        Loist, S. (2011). On the relationships between film festivals and industry. In Lee, Y. (eds), 
                        Busan Cinema Forum: Seeking the Path of Asian Cinema: East Asia. BIFF - Busan International Film Festival, pp. 381–402.
                    
                    
                        Loist, S. (2016). The film festival circuit: networks, hierarchies, and circulation. In de Valck, M., Kredell, B., and Loist, S. (eds), 
                        Film festivals: History, Theory, Method, Practice. London: Routledge, pp. 49–64.
                    
                    
                        Wais, K. (2016). Gender Prediction Methods Based on First Names with genderizeR. The R Journal, 8(1): 17-37. 
                        https://journal.r-project.org/archive/2016/RJ-2016-002/RJ-2016-002.pdf (accessed 1 October 2018).
                    
                
            
        
    
10049	2019	
        
            In humanistic research, Named Entity Recognition is highly useful, but it mines surface data, rather than revealing the complex nature of relationships between these entities. Named Entity Recognition (NER) extracts the names of people, locations, organizations, and, depending on the model, may also extract references to money, percentages, dates, and times, in addition to a miscellaneous class. Although this is certainly useful, NER does not represent the richness of the documents with which we work. For example, consider this fragment from a nineteenth-century French chronicle of Ottoman Algerian history: ‘To further attach himself [to his ally], Pasha Hassan married his [ally’s] daughter, then he launched troops against the rebel…’ (Mercier, 1903: 200) In just this short passage, which is not even a full sentence, we find several people referenced who are unnamed. If we look back at the text, we see that Pasha Hassan’s ally is Ben-El-Kadi of Kuku, Algeria, and the rebel is Abd-El-Aziz, but Ben-El-Kadi’s daughter is never named. This occurs frequently in historical source material. Those who remain unnamed are most often women, servants, slaves, and Indigenous people – the very people about whom scholars are most anxious to know more. This short paper presents a work-in-progress: a digital workflow and Python script to mine and model the relationships between extracted entities from French-language documents in order to grapple with the complexity of human relationships and cultures, as well as the perspectives of authors and their informants. 
            
                This short presentation will share the complete information extraction code, its accuracy, the resulting visualizations, and a brief analysis from the case study. The method presented has applications far beyond French language and the history of the Middle East and North Africa. For instance, with some adjustments for language, this method would be highly useful in the analysis of The Twenty-Four Histories of China, the official history of the Chinese dynasties between 3000 B.C.E. and the seventeenth century. More broadly, this approach will be of use to scholars interested in identifying and studying relational data, social positions, and networks of both known and previously unknown actors, particularly those who remain unnamed in the source material.
            
            As a test corpus, this project uses four digitized, OCRed, and hand-cleaned nineteenth-century French chronicles of Ottoman Algerian history. The volumes range between 41,341 words and 170,737 words and cover the period 1567 to 1837 with a focus on Constantine, the easternmost province in Algeria. The challenge is to extract not only named entities and their relations to one another, but to extract unnamed persons and their relationships as well. In simple NER, the names Moustafa and Namoun, would be the only extracted data in the following sentence: ‘Moustafa avait épousé une des filles de Namoun,’ but the daughter of Namoun who married Moustafa would not appear. (‘Moustafa married one of Namoun’s daughters.’ Vayssettes, 2003: 52. Author’s translation.) The goal of this project is to uncover the positions and roles of women in Algerian society, so it is essential to locate and retrieve data about unnamed people. 
            The built-in language models and extensibility of the spaCy natural language processing (NLP) library for Python makes it most suitable for this project (Honnibal and Montani, 2017). Specifically, spaCy enables researchers to define entities and build custom information extraction systems. Additionally, spaCy’s library features a French language model that has a built-in tagger, parser, and NER, unlike the Natural Language Toolkit or Stanford’s CoreNLP Open Information Extraction system. 
            To build an information extraction system with spaCy that pulls the desired relational data, we must first identify an extractable pattern by parsing and tracing the dependencies of a sample sentence, as follows:
            SpaCy’s visualizer also allows us to view the dependency parse tree using the following code and sample sentence. 
            An examination of the parse tree above yields a pattern of parts-of-speech around the keyword ‘épousé’ that we can use to extract the desired information about this relationship. Since we are interested in identifying the relationships between both named and unnamed people, we will look for specific patterns in parts of speech and syntax, as well as the location of proper nouns in relation to keywords. Based on an examination using the concordance method with the sample texts, the following keywords generated the best data: fils, fille, mariage, épous*, gendre, and beau pére (son, daughter, marriage, spouse/to marry, son-in-law, father-in-law). For example, the word ‘fils,’ or ‘son,’ yielded more consistent results for father-son pairs than the word ‘pére,’ or ‘father.’ 
            From an examination of the word ‘fils’ in context, as shown above, general patterns emerged. The patterns for ‘fils’ and the proper output format for each pattern are shown below. These outlines then inform the Python script that uses spaCy’s library to extract the relational data. This script will be made freely available on GitHub following the DH 2019 Conference.
            Based on the examples and patterns above, the information extraction system derives relational data that easily translates into node and edge lists. In this case study, network analysis of the extracted data highlights how women, marriage, and kinship connections legitimated Ottoman rule. Initial findings suggest that Algerian women were key links in the chain that bound Algeria to the Ottoman Empire. 
        
        
            
                
                    Bibliography
                    
                        Honnibal, M. and Montani, I. (2017). SpaCy 2: Natural Language Understanding with Bloom Embeddings, Convolutional Neural Networks and Incremental Parsing. Python https://spacy.io/usage/ (accessed 27 November 2018).
                    
                    
                        Mercier, E. (1903). Histoire de Constantine. Constantine, Algeria: J. Marle et F. Biron (accessed 25 April 2012).
                    
                    
                        Vayssettes, E. (2003). Histoire de Constantine Sous La Domination Turque de 1517 à 1837. (Bibliothèque d’histoire Du Maghreb). Saint-Denis: Bouchene.
                    
                
            
        
    
10051	2019	
        
            Variation is a complex phenomenon engaging almost all aspects of folklore. Every cultural performance in daily life gets adapted to time and place, circumstances and audience. In this panel we we are going to explore complex phenomena of variation and stability in folklore on the basis of textual and musical representations of oral tradition with the help of digital and computational methods.
            In many cases, variations can be interpreted as intentional and meaningful. However, folklore seldom changes beyond recognition: there is always a part of narratives and songs that remains stable. Detecting in which points lies the stability in folklore sources, reveals to us what the very essence, the core of tradition is. Approaching the material from the other end, it needs to be analyzed how variation is produced, where the adaptability and creativity of folklore lies, and which are the meaningful possibilities for variation within the limits of tradition. As far as we are dealing with texts or melodies, we can determine in what respect oral performances can be labeled as traditional and to what extent folklore is the product of individual creativity and improvisational skills. After determining what parts of folklore remain the same, what changes and what parts are left out, we need to come up with an explanation: what does this all mean in the light of the culture of daily life?
            Our core material consists of narratives and songs: epics, poetry, myths and other folktales, life testimonies, and folk songs (both texts and melodies). Millions of folklore texts and performances, collected in the folklore archives and nowadays available in digital form can, together with the existing metadata, be used as data for finding out the regularities and irregularities in folklore - a universal kind of natural communication with its specific functions in society.
            
                Computational Analysis of Life Stories
                Theo Meder (Meertens Instituut &amp; University of Groningen)
                In 2013, a new project was started by the Humanitas Foundation, department Almere, which was called "Levensboek" (Life Book). Volunteers from Humanitas would conduct interviews with elderly people who would tell their life stories. This life story was then recorded or edited by a volunteer and, with photos, then printed as a booklet in a limited edition. The booklets with life stories were mainly meant as testimonials for the children and grandchildren, other family and friends. One of the initiators of Humanitas, Veronica Stutvoet, contacted Theo Meder of the Meertens Institute with the question whether such Life Books were also interesting for archiving and studying. Since the study of contemporary folk culture is one of the core tasks of the Meertens Institute, Humanitas also decided to offer a booklet for the archive. Due to privacy legislation, a contract was added in which the narrators could indicate when the book could be studied freely. And from the Meertens Institute a list was drawn up with subjects that would interest the researchers, such as folktales, songs, games, festivities and rituals. The first Life Book was received in May 2013 in a festive manner: it concerned the book Met hart en ziel (With heart and soul) by Mrs Elly IJsendijk. After proven success, the Humanitas departments in Apeldoorn and Zaandam also started to produce life books, and after five years 20 booklets were produced. In addition to a paper copy, the Meertens Institute also receives a digital copy on request, so that the stories can also be subjected to a computational analysis. This may include structure analysis, research into motifs or sentiment analysis. Research into gender is also possible; do women talk about other subjects than men? The storytellers were, without exception, born in the 1920s, 30s or 40s - meaning that some experienced the crisis years as a child, while some were born shortly after the Second World War. In any case, the war has left a mark on many children, even if they only heard the stories. The life stories are always linear: they often start with the parents, then the childhood, the (aftermath of the) war, school, friends, education and profession, marriage, children and grandchildren, holidays, illnesses and deaths of loved ones. And yet the stories are always different, through the emphasis on certain themes, and through many unique personal experiences. Perhaps most revealing are the themes that all (or most) narrators ignore or leave out. In my research, I analysed the digital life books on structure, sentiments, themes and the distribution of motifs, using tools such as AntConc and LIWC2015 (Linguistic Inquiry and Word Count 2015).
            
            
                Stability in folk song transmission
                Berit Janssen (Digital Humanities Lab, University of Utrecht)
                In folk song traditions, melodies are circulated through transmission. In this process, parts of melodies may change, while other parts remain stable, meaning they resist change. Stability has been a long-standing point of interest in folk song research: how can stability be quantified, and can we predict which parts of a melody are stable? In the past, this question has been addressed through experimental research, in which artificial transmission chains were observed. While this direction of research is inspirational, the ecological validity of such approaches may be questioned. With the current computational means and rich digitized corpora of folk songs, we can study the results of real-life transmission of melodies by comparing variants of the same song.
                The current contribution describes such research on a corpus of 4120 Dutch folk song melodies. Two melodic units were investigated: folk song phrases and motifs. For the phrases, the goal was to predict the occurrence of a phrase in a family of related songs: a phrase occurring in many variants in almost identical form was considered more stable, and was expected to have different melodic properties from less stable phrases. To determine the occurrence of folk song phrases, a pattern matching method was developed in Python, which was optimized on a training set of annotated phrase occurrences. Several similarity measures were compared, and those approaching human judgements on phrase occurrences most closely were combined to detect phrase occurrences in the full set of folk songs. For the motifs, a set of motifs considered characteristic melodic material of 360 melodies was compared against random melodic patterns, with the expectation that the characteristic motifs would have different melodic properties from the random melodic material.
                We evaluated prediction success through Generalized Linear Mixed Models. The results show a number of successful predictors for stability of melodic segments in transmission: the length, position and number of repetitions in a melody, conformity to musical expectations, and the presence of repeating motifs can help us to predict whether or not a given melodic segment is stable. Both for folk song phrases and folk song motifs, the melodic predictors explain between 5% and 10% of the variation, constituting a medium-sized effect. Other factors might influence stability in folk song transmission: preference to copy performances of individuals based on their status in society (prestige bias), or preference to copy the most common variants of a melody (conformity bias). Given that such factors cannot be controlled in the current dataset, the extent to which stability can be explained purely on the musical properties of melodic segments is impressive, and shows that stability is certainly not a randomly occurring phenomenon, but arises from the resonance of melodic structures with our cognitive capacities to perceive and memorize music.
            
            
                Rule Mining for Melodic Cadences
                Peter van Kranenburg (Meertens Instituut, Amsterdam)
                The availability of large collections of digitized folk songs enables an empirical approach to the study of various aspects of melodic structure. In this contribution, we focus on melodic patterns that are used to indicate a cadence, or ‘end of phrase’. Most existing approaches for modelling cadential patterns are either based on pre-defined rules or on statistical learning. Rule based approaches include Narmour’s Implication-Realization model (Narmour, 1992), and Cambouropoulos’ Local Boundary Detection Model (Cambouropoulos, 2001), which both are grounded in principles from Gestalt Theory. Statistical approaches include Rens Bod’s Data Oriented Parsing (Bod, 2001), Huron’s ITPRA-model (Huron, 2006), and the IDyOM model by Pearce et al. (2010). The current study takes a hybrid approach by employing a rule-mining algorithm to infer a model of melodic closure (cadence) from a collection of folk melodies. There are many machine learning methods that could be used to learn models from data. The advantage of a rule-mining algorithm is that the resulting model is highly interpretable, as it consists of a series of rules.
                We employ a collection of more than 4,000 melodies in Western tonal idiom from the Meertens Tune Collections (Van Kranenburg, 2014). Since the digitized melodies in these data sets include annotations of phrase boundaries, these are well suited to train cadence-detectors. The data set for rule mining consists of all pitch tri-grams from all melodies. The tri-grams are labelled as either ‘cadential’ or ‘non-cadential’. We represent each tri-gram as a vector of feature values. Features include scale degrees of the three pitches, melodic contour, and metric weights.
                We use the RIPPER algorithm (Cohen, 1995) to perform the rule mining. The output of the algorithm consists of a series of rules to separate the cadential tri-grams from the non-cadential tri-grams.
                In a first run, we obtain a F-measure of 0.789 on a separate test-set. The three most important rules describe cadences on the first and the fifth degree of the melodic scale. The first rule states that a tri-gram which ends on the tonic, has a high metric weight for the third pitch, and has a descending contour is a cadential tri-gram. This rule reflects common knowledge from music theory.
                By closely examining the cases in which the discovered rules fail, we are able to identify possible other features to include. In particular, we find that the position of the tri-gram in a melodic phrase is of importance. Therefore, we include this in our feature set and perform a next run of the algorithm. The newly discovered model achieves a F-measure of 0.839 on a separate test-set. Adding the feature, clearly improved the discovered model.
                From this study, we conclude that cadence patterns obey general rules, and that it is possible to derive these rules from melodic data when including the right features. The advantage of a rule-based model is its interpretability in musical terms.
            
            
                Browsing the corpus of Finnic oral poetry
                Kati Kallio &amp; Eetu Mäkelä (University of Helsinki)
                With a versatile corpus of Finnic oral poems in several related languages and dialects and a wide variety of different orthographies, a central question is how to gather relevant items for each research setting. How to find similar poetic formulas or themes and trace intertextual relationships in a linguistically and poetically heterogenous corpus of oral poetry, and what theoretical possibilities does digital reading offer for the research?
                In this paper, we compare searches made with research interface Octavo (https://github.com/jiemakel/octavo) and the present interfaces of two corpora of historical Finnic oral poetry in runo-song meter (www.skvr.fi, www.folklore.ee/regilaul) to the analyses that were made earlier manually with these collections, discussing both the practical and theoretical possibilities and implementations given by digital browsing possibilities.
                During the last decades, the folklore archives in Estonia and Finland have digitised two large sources of historical Finnic oral poetry, consisting of c. 181,000 poems in various dialects of small related languages around the Baltic sea: Karelian, Izhorian, Votic, Estonian and Finnish. The poems were recorded in 1564–1939 with various orthographical systems. Some words may appear in hundreds of different written forms. The stories and main characters may exist in various ways, with individual, local and regional peculiarities. The language may contain archaisms or special word forms, syllables and words used only in songs. The poetic system is complex and versatile, and there are no comprehensive dictionaries or ready-made parsers for the data.
                Yet, the research history provides a point of comparison. During the first half of the 20th century, a great amount of detailed studies on geographical variation of individual song types was made, with the aim of taking all the collected examples into account. Although the theoretical understanding of oral poetry has since changed, making these studies partly invalid, these studies are still relevant depictions of variation within the data. When compared with searches made with digital tools, they give a baseline for evaluating the possibilities and limitations of the present tools. The paper focuses on three examples:
                1) Analysis by Väinö Kaukonen (1956) of the manuscript sources of oral poetry used by Elias Lönnrot when composing the Finnish national epic 
                    Kalevala.
                
                2) Historical-geographical analysis by Martti Haavio (1948) of the vernacular 
                    Death Song of Saint Henrik, the medieval patron saint of Finland.
                
                3) Typological-stylistic analysis by Matti Kuusi (1949) of the Karelian mythological 
                    Sampo-epics.
                
                Is it possible to find digitally all those variations and intertextual links — or more — of a particular theme or poetic formula that were gathered manually by the past researchers? This is approached 1) with word and collocation searches, 2) by checking the results by using a thematic index of the SKVR-corpus (using also visualisation with Palladio), and 3) finally comparing both strategies with the findings of earlier manual research.
            
            
                Potential of Stylometry in Studying Folkloric Variation: Content, Style, Language
                Mari Sarv (Estonian Literary Museum, Tartu)
                Stylometry - a statistical method comparing sets and share of most frequent words (or other units) in different texts - has been most notably used in the field of authorship attribution, but also in genre studies, in translation studies etc. The main idea lies in the assumption that individual style of an author is represented in the way he/she (unconsciously) uses the most frequent words (usually grammatical function words) or other units. In applying stylometry for the large historical corpora of literary writings one can detect development of style, which is not clearly distinguishable of the changes in natural (and thus also literary) language use (see e.g. Eder and Górski, 2016; Eder, 2018).
                The current paper addresses the potential of stylometry in studying variation in folklore texts. Stylometric analysis could possibly help us to find answers to many questions concerning the nature of folklore and variation inherent to it, say the individuality versus traditionality of performers/creators, similarities/differences of different folklore genres. In addition, stylometry could be used as clustering tool for detecting tradition areas within a bigger area, and even folkloristic text-types within the text corpora when focusing on content words in the analysis.
                At first glance stylometry seems to be an extremely useful and feasible method for getting better knowledge on variation in folklore; there are additional difficulties to solve though. First, the linguistic (dialectal) variation and folkloric variation are inseparable and overlapping. Different words and word forms used in different (micro)dialects do not have to mean differences in content aspects, like modes, genres, types. Non-standard language and non-standard orthographies present in folklore texts do not make the task of comparison easier either. Moreover, the folklore texts are usually not written down by performers themselves, but collectors who have left prints of their personal style into recordings. The complexity of variation in folklore makes it a challenge to tackle, and evokes questions if the variation we are able to detect using stylometry (comparing the presence and share of most frequent words in different text groups), form part of dialectal, stylistic or folkloric variation; is it individual, or reflects the peculiarities of genre, thematic or functional groups of texts.
                My experiments with multilingual corpora of folksongs (www.skvr.fi, www.folklore.ee/regilaul) in several Finnic languages (dialects) on the basis of word forms have revealed that both, linguistic as well as content aspects play a role in clustering, reflecting main dialect boundaries in first instance, but revealing for example also regional predominance of lyric and epic mode in songs, and different thematic accents in regional groups.
            
            
                The network of characters in Estonian animal tales
                Risto Järv (Estonian Literary Museum, Tartu)
                If folklore is characterised by variation and milieu-morphological adaptation of characters, the adaptation of internationally spread animal tales will retain some established dominant characters – it is certain fixed characters that appear as certain types. The presentation observes the variability of animal characters, using network analysis. The study is based on the Estonian folk tale text corpus created by the Estonian Folklore Archives of the Estonian Literary Museum and the Department of Estonian and Comparative Folklore at the University of Tartu. The corpus contains 13,000 Estonian texts, of which approximately a fifth is made up by animal tales. While the Estonian folklore scholar Pille Kippar has noted in an earlier discussion of characters of animals tales (Kippar, 1989) that the characters can be easily interchangeable within the limits of their stereotypes, I am analysing a sample of selected tale types to check how predominant such variability is, which characters in particular appear as interchangeable, and which regularities emerge in the variability as concerns versions of specific tale types as well as versions by particular storytellers.
                I analyse which sets (pairs) of characters are most likely to vary within the tradition and whether there are causal relationships between this feature and the animal characters being active or passive. As several animal tales appear as cycles in the folklore tradition, which combine different tale types within one tale, also this characteristic is taken into account to detect whether any distinct features of character variability emerge in these cases.
            
        
        
            
                
                    Bibliography
                    
                        Bod, R.
                         (2001). Probabilistic grammars for music. 
                        Proceedings of BNAIC
                         2001.
                    
                    
                        Cambouropoulos, E.
                         (2001). The local boundary detection model (LBDM) and its application in the study of expressive timing. 
                        Proc. of the Intl. Computer Music Conf.
                    
                    
                        Cohen, W. W.
                         (1995). Fast Effective Rule Induction. 
                        Proceedings of the Twelfth International Conference on Machine Learning.
                    
                    
                        Eder, M. and Górski, R. L.
                         (2016). Historical Linguistics' New Toys, or Stylometry Applied to the Study of Language Change. 
                        DH 2016
                        , pp. 182-184.
                    
                    
                        Eder, M.
                        (2018). Words that Have Made History, or Modeling the Dynamics of Linguistic Changes. 
                        DH 2018
                        , pp. 362-364.
                    
                    
                        Huron, D.
                         (2006). 
                        Sweet Anticipation.
                         Cambridge, Mass.: MIT Press.
                    
                    
                        Kippar, P.
                        (1989). Eesti loomamuinasjuttude tegelastest. 
                        Paar Sammukest eesti kirjanduse uurimise teed. Uurimusi XII. Jakob Hurda 150. sünniaastapäevaks. ENSV Teaduste Akadeemia Fr. R. Kreutzwaldi nimeline kirjandusmuuseum.
                         Tallinn: Eesti Raamat, pp. 148–157.
                    
                    
                        Narmour, E.
                         (1992). 
                        The Analysis and Cognition of Basic Melodic Structures.
                         Chicago: University of Chicago Press.
                    
                    
                        Pearce, M., Müllensiefen, D. and Wiggins, G.
                         (2010). The role of expectation and probabilistic learning in auditory boundary perception: A model comparison. 
                        Perception
                        ,
                        39 (10)
                        : 1365–1389.
                    
                    
                        Van Kranenburg, P., De Bruin, M., Grijp, L. P. and Wiering, F.
                         (2014). 
                        The meertens tune collections. Meertens Online Reports
                         2014-1, Amsterdam: Meertens Institute.
                    
                
            
        
    
10058	2019	
        
            
                Abstract
                This paper presents a fully automatic approach to the scansion of Ancient Greek hexameter verse. In particular, we describe how finite-state automata can be used to discriminate between the 32 variants of Ancient Greek hexameter. We evaluate the performance of our annotation algorithm against hand-annotated scansion data. The project code is available online
                    
                         https://github.com/anetschka/greek_scansion.
                    .
                
            
            
                Introduction
                Greek literature has, for centuries, served as a paradigm and model for literary writing all over Europe. The epitomes of Ancient Greek literature, the Odyssey and the Iliad, are epic poems that share the same metre: hexameter. Hexameter annotation is crucial for large-scale and data-driven investigations dedicated to these poems, and automatic annotation algorithms open up new opportunities for research in this field.
                Ancient Greek hexameter verses can be described as regular sequences of long and short syllables, with the length of each syllable being determined by the length of the syllable’s vowel. Long and short syllables are organised in 
                    feet of the following form:
                
                
                    
                        Dactyl: The foot is composed of three syllables, the first of which is long, while the others are short.
                    
                    
                        Spondee: The foot is composed of two long syllables.
                    
                
                Six feet make a complete hexameter. Feet 1-5 can be either spondees or dactyls, only the last foot is restricted with respect to its metric form: It is composed of a long syllable and the so-called anceps the length of which is variable. Figure 1 is a generic depiction of the resulting 32 variants of Ancient Greek hexameter. Due to the free flow of either dactyls or spondees, hexameter can accommodate varying syllable counts (from 12 to 17 syllables) and produce a broad range of rhythmic effects.
                Section 2 of this paper provides an overview of related work. Section 3 describes the annotation algorithm. Section 4 gives the evaluation results and section 5 concludes this paper. 
                
                    
                    Figure 1. Generic hexameter scheme. Vertical bars separate feet. Horizontal bars indicate long syllables, bows indicate short syllables. Vertical stacks of symbols indicate that both variants are possible. X marks the 
                        anceps.
                    
                
            
            
                Related Work
                An early, rule-based approach to the semi-automatic scansion of Greek hexameter has been developed by Höflmeier (1982). Höflmeier combines two different kinds of knowledge to resolve hexameter verses:
                
                    Local linguistic rules that establish which vowels are short, and which vowels are long.
                    Knowledge about the overall structure of the verse for the resolution of partially annotated verses.
                
                The approach is semi-automatic since the resolution of verses that possibly exhibit complex linguistic phenomena such as 
                    synizesis (the metric contraction of normally distinct vowels) is delegated to the user. A similar approach has later been proposed by Pavese and Boschetti (2003).
                
                An advanced study in the automatic scansion of metric poetry is the work by Greene, Bodrumlu, and Knight (2010) who use weighted finite-state transducers, trained on a small corpus of manually annotated data, to analyse Shakespearean sonnets. The authors report accuracy values of up to 81.4 %.
                An interesting approach to the problem of Greek hexameter scansion is presented by Papakitsos (2011). Papakitsos performs syllabification and then employs a search strategy to identify dactyls, that is, the verses are not analysed left-to-right. Rather, the search starts in the fifth foot where dactyls are particularly likely. Once the appropriate – for the established number of syllables in the verse – number of dactyls has been identified, the search terminates. Dactyls are, again, identified by means of local linguistic rules. The search, however, is strongly dependent on the correctness of the syllabification. For instance, if the verse under analysis has been found to consist of 13 syllables, the search algorithm will look for exactly one dactyl. Papakitsos reports a recall of 0.98 and a precision of 0.80. 
                A rule-based implementation of a fully automatic Greek hexameter scansion algorithm has been published by Hope Ranker
                    
                         https://github.com/epilanthanomai/hexameter.
                    . This algorithm uses an ensemble of weighted finite-state transducers to resolve the feet one by one.
                
                Alternative approaches to the automatic analysis of metric poetry employ machine learning. In these studies, the problem is usually modelled as syllable-wise classification. For instance, Estes and Hench (2016) employ a Conditional Random Fields classifier to analyse Middle High German epic texts, reaching an f-measure of 0.90. Zabaletak (2017) reports on a very wide range of experiments, but achieves the best results with a combination of a sequential model and deep learning for the classification of English, Spanish, and Basque verses. N-grams, positional and length features as well as linguistic markers are used to train the models.
            
            
                Finite-State Approach to Hexameter Analysis
                Our approach to the scansion of Ancient Greek hexameter is based on the same two types of knowledge that were already used by Höflmeier (1982):
                
                    
                        Local search: We use 5 local linguistic rules to determine whether a pair of syllables can safely be considered long (that is, it forms a spondeus).
                    
                    
                        Global analysis: We exploit knowledge about the overall structure of Greek hexameter to complete partially annotated verses, that is, verses that could not fully be resolved with the help of the linguistic rules.
                    
                
                Moreover, the local search step follows the strategy of Papakitsos (2011) in that it searches for a fixed number of spondees that result from the syllable count established during syllabification. Figure 2 shows a visual representation of our scansion algorithm. The algorithm scans epic Greek text verse by verse:
                
                    
                        Pre-processing consists mainly of lower-casing and the removal of diacritics.
                    
                    Moreover, we have implemented a 
                        syllabification algorithm that uses regular expressions to identify syllables and to establish the syllable count of the verse.
                    
                    The 
                        local search and all following steps are then handled by dedicated deterministic finite-state automata (FSAs). There are specialised FSAs for verses of 13, 14, 15, and 16 syllables and a simpler FSA for all remaining cases. In the local search step, the active FSA performs a targeted search for a given number of spondees, using 5 simple linguistic rules. If enough spondees are found, the plausibility of the solution is checked. Otherwise, the verse is passed to the global analysis step. The FSAs were implemented using an existing Python library
                        
                             https://github.com/pytransitions/transitions
                                .
                            
                        .
                    
                    For 
                        global analysis, we use a non-deterministic finite-state transducer (FST). In this transducer, each syllable corresponds to a state, and alternative solutions are modelled by means of alternative paths. The FST is weighted, but since we did not have access to an appropriate training corpus, we were not able to learn transition weights from data. Instead, they were set manually following the description provided by Papakitsos (2011). The FST was implemented using the Helsinki Finite-State Tools
                        
                             https://hfst.github.io/python/3.12.1/index.html.
                        .
                    
                    If the 
                        plausibility check fails, the verse is passed over to 
                        error handling to compensate for potentially erroneous syllabification. Global analysis then completes the verse. The plausibility of the result is checked again. Depending on this result, the FSA will transition to its final state, that is, either 
                        success or 
                        failure
                        .
                    
                    If the verse, however, passes the plausibility check immediately after the local search step, the FSA transitions directly to the 
                        success state. 
                    
                
                
                    
                    Figure 2. Visual representation of the scansion algorithm.
                
            
            
                Evaluation
                We have evaluated the performance of both our syllabification and our scansion module against hand-annotated verse data. The annotations were carried out by two advanced students of Greek philology, discrepancies and errors were clarified by means of group discussions. For syllabification evaluation, we randomly chose a set of 171 verses (2695 syllables) from both the Odyssey and the Iliad. For scansion evaluation, we randomly selected 346 verses from a broader range of Ancient Greek texts. Table 1 provides an overview of this data set.
                For syllabification, we achieved a syllable-wise accuracy of 0.98. Verse-wise accuracy reached 0.82. Scansion correctness was evaluated by means of precision, recall, and f-measure with the following results:
                
                    
                        Precision: 0.95
                    
                    
                        Recall: 1.00
                    
                    
                        F-measure: 0.98
                    
                
                The evaluation scripts are included in the open-source code package of our software.
                
                    
                    Table . Evaluation data.
                
            
            
                Conclusion
                In this paper, we have presented a fully automatic approach to the analysis of Ancient Greek hexameter text. Automatic annotation tools are crucial for data-driven investigations in Greek philology. Our algorithm integrates various kinds of linguistic knowledge into a set of finite-state automata and thus makes use of well-defined concepts in the field of computational linguistics, while remaining transparent to philologists. Our evaluation results are competitive. Future work will be dedicated to the exploitation of the resulting annotations for research in Greek philology.
            
        
        
            
                
                    Bibliography
                    
                        Estes, A. and Hench, C. (2016). Supervised Machine Learning for Hybrid Meter, Proceedings of the Fifth Workshop on Computational Linguistics for Literature (NAACL-HLT), San Diego, USA, June 2016.
                    
                    
                        Greene, E., Bodrumlu, T. Knight, K. (2010). Automatic Analysis of Rhythmic Poetry with Applications to Generation and Translation, Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), Cambridge, USA, October 2010.
                    
                    
                        Höflmeier, J. (1982). Metrisches zum frühgriechischen Epos. Unpublished thesis. University of Regensburg.
                    
                    
                        Papakitsos, E. C. (2011). Computerized Scansion of Ancient Greek Hexameter, Literary and Linguistic Computing 26.1: 57–69.
                    
                    
                        Pavese, C. O. and Boschetti, F. (2003). Introduction: Description of the Programme. Directions for the Formular Edition. In Pavese, C. O. and Boschetti, F. (eds), A Complete Formular Analysis of the Homeric Poems. Amsterdam: Adolf M. Hakkert, Vol. 1.
                    
                    
                        Zabaletak, M. A. (2017). Automatic Scansion of Poetry. Phd thesis, University of the Basque country.
                    
                
            
        
    
10066	2019	
        
            
                Introduction
                Observing poetic similarity is fundamental for identifying interrelationships and poetic influences among authors. When investigating poetic similarity, intertextuality is always regarded as the most significant factor (Fowler, 1997), and it is also an index that can be calculated computationally (Coffee et al, 2018; Büchler et al., 2018 forthcoming). In terms of verse, homogeneous formal elements like rhyme (Hollander, 1981), the basic rhythmic structure, and meter (Boomsliter et al., 1973) among poems can also act as crucial indicators of poetic similarity.
                The goal of this research is to design a framework to quantitatively measure poetic similarity with digital methods, which can dig into a vast number of data and suggest the interrelationship of different authors’ works. This research focuses on the world-renounced Irish poet William Butler Yeats (1865-1939). The complex poetic influence he received deserves scrutinized investigations and the possible influence of the English Romantic poets is examined here. In this study, “influence” refers to the shaping power of a precursor poet on a later poet’s poetic style and poetic genre, which can be traced and observed. Specifically, a model is constructed to compare Yeats and the English Romantic poets, exploring their similarities in three aspects: (1) intertextuality; (2) formal elements, including rhyme, meter, and enjambment; (3) sentiment.
            
            
                Methods
                
                    
                
                
                    Fig. 1. Framework of model
                
                As Fig. 1 shows, the model includes a preprocessing stage and four parallel quantifications: intertextuality calculation, rhyme and meter detection, enjambment calculation and sentiment analysis. The raw data are crawled from the Bartleby collection (https://www.bartleby.com/verse), including 130 works of Yeats from three collections, 216 works of Blake, 119 works of Byron, 53 works of Keats and 469 works of Wordsworth. 
                For intertextuality, after tokenization, lemmatization, and filtering stop words (words are lemmatized since it is unnecessary to distinguish different forms of the same word, and stop words are filtered out so as prevent function words that have little lexical meaning from being chosen), considering the remaining lists of words generated from Yeats’s works as target and those generated from the English Romantic poets as source, we compare them in turn in the unit of phrases, a segment of text demarcated by a semicolon or a colon. All source-target phrase-pairs that share at least two distinct words are recorded. Next, each recorded phrase-pair is weighted according to the following formula (Forstall et al., 2014): 
                
                    
                
                Here, f(t) and f(s) are the frequency of each matching word in its target and source phrase divided by the length of the phrase respectively, and dt and ds are the distance of the farthest matching word (the number of words between two matching words with the largest distance among all matching words) in their target and source phrase. Phrase-pairs with words of lower frequency and those with closer distance are privileged because these indicate stronger possibility of intertextuality, which is set as the summation of every phrase-pair’s score within them divided by the product of their lengths (number of phrases).
                
                    
                
                Finally, the rate of intertextuality of Yeats and each English Romantic poet is defined as the average value of each verse-pair with non-zero value.
                For formal elements, CMU Pronouncing Dictionary is exploited to identify syllables, stresses and rhyme words (http://www.speech.cs.cmu.edu/cgi-bin/cmudict?in=C+M+U+Dictionary). The results of the identification are recorded as strings and are compared to every standard rhyme type and meter style to calculate their Levenshtein distances (Levenshtein, 1966), and the rhyme and meter types of the target verse are guessed and defined accordingly.
                For enjambment, after line segmentation, the proportion of enjambments is calculated as the number of the lines divided by the number of the lines that contain “,”/”.”/“!”/“?” immediately before the line breaks.
                For sentiment analysis, Python library TextBlob (Loria, 2018) is used to define the emotional tendency of the verses. Each verse is inputted into the system, and a parameter ranging from -1 (totally negative sentiment) to 1 (totally positive sentiment) is outputted. The emotional tendency of each poet is calculated as the average of that parameter in all of his works.
            
            
                Results
                
                    Table 1. Rates of intertextuality of Yeats and the English Romantic poets
                
                
                    
                
                Table 1 shows that the rate of intertextuality between Yeats and Blake is the highest. The results of significance test show that the difference between the rate of intertextuality of Yeats-Blake and Yeats-any other Romantic poet is statistically significant at a significance level of 10-6, which shows that the intertextuality between Yeats and Blake is remarkably higher than those between Yeats and the other English Romantic poets. Since a higher rate of intertextuality shows a stylistic rather than generic imitation (Conte, 1986), the results indicate that Blake may have exerted a stronger influence on Yeats’s poetry than the other poets studied.
                
                    Table 2. Formal elements of Yeats and the English Romantic poets (bold character is used to distinguish the closest percentage with that of Yeats)
                
                
                    
                
                Table 2 shows that the distribution of rhyme types in Wordsworth’s verses is relatively the most similar to that of Yeats, and he also has the closest proportion of enjambment with Yeats (at a significant level of 0.05, the difference between their enjambment proportion is not statistically significant). In terms of meter style, Yeats has a very similar distribution with Blake.
                
                    Table 3. Sentiment of Yeats (and his collections) and the English Romantic poets
                
                
                    
                
                *The abbreviations stand for names of the collections: The Wind among the Reeds, Responsibilities and Other Poems, The Wild Swans at Coole, respectively.
                After the sentiment analysis, two major findings are observed from Table 3: (1) Yeats has the smallest value, while Blake has the second smallest (at a significant level of 0.05, the difference between their sentiment value is not statistically significant, which is unique contrasting the value between Yeats and the other English Romantic poets); (2) Sentiment parameters of Yeats’s different collections ascend in a chronological order.
            
            
                Conclusion
                This research successfully builds a model to quantitatively measure poetic similarity. The results show that Blake, among the English Romantic poets, is the most similar to Yeats both in terms of intertextuality and sentiment. With regard to formal elements, Yeats resembles both Blake and Wordsworth. This study’s possible contribution to Yeats scholarship is to quantitatively measure and prove the prominent influence of Blake on Yeats’ poetry, and concretely shows Yeats’ relationship with such movements as Romanticism. Furthermore, the framework designed by this research can be applied to investigate poetic similarity or intertextuality among other poets or poems, thus making contribution to literary studies in general. We believe that by the means of investigating massive data of poetic similarity, the influence of chanciness in literary interpretation can be substantially weakened. Digital methods can serve as powerful tools to detect latent literary attributes, raising significant topics that can inspire further studies.
            
        
        
            
                
                    Bibliography
                    
                        Boomsliter, P. C., Creel, W. and Hastings, G. S. (1973). Perception and English poetic meter. 
                        PMLA, 88(2): 200-08.
                    
                    
                        Büchler, M., Franzini, G., Franzini, E., Moritz, M. and Bulert, K. (2018 forthcoming). "TRACER -a Multilevel Framework for Historical Text Reuse Detection." 
                        Journal of Data Mining and Digital Humanities - Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages.
                    
                    
                        Coffee, N., Koenig, J., Poornima, S., Forstall, C. W., Ossewaarde R. and Jacobson, S. L. (2013). The Tesserae Project: intertextual analysis of Latin poetry. 
                        Literary and Linguistic Computing, 28(2): 221-28.
                    
                    
                        Conte, G. B. (1986). 
                        The Rhetoric of Imitation: Genre and Poetic Memory in Vergil and Other Latin Poets. Ithaka: Cornell University Press.
                    
                    
                        Forstall, C., Coffee, N., Buck, T., Roache, K. and Jacobson, S. (2014). Modeling the scholars: detecting intertextuality through enhanced word-level N-gram matching. 
                        Literary and Linguistic Computing, 30(4): 503-15.
                    
                    
                        Fowler, D. (1997). On the shoulders of giants: intertextuality and classical studies. 
                        Materiali e Discussioni per l'Analisi dei Testi Classici, 39: 13-34.
                    
                    
                        Hollander, J. (1981). 
                        Rhyme’s Reason: a Guide to English Verse. New Haven and London: Yale University Press.
                    
                    
                        Levenshtein, V. (1966). Binary code capable of correcting deletions, insertions and reversals. 
                        Soviet Physics Doklady, 10(8): 707-10.
                    
                    
                        Loria, S. (2018). 
                        TextBlob: Simplified Text Processing. https://textblob.readthedocs.io/en/dev.
                    
                
            
        
    
10069	2019	
        
            
                Critiques of literary representation
                
                    Literary studies has a long tradition of analysing texts from an ideological perspective. Inspired by feminist (Butler 1990), postcolonial (Said 1978) and Marxist (Eagleton 1976) strands of thinking, these so called critiques of literary representation have been focusing on hierarchies between genders, ethnicities, and classes in literary texts. One way in which these hierarchies can be traced is through comparatively analysing representations of characters with different demographic backgrounds. For the field of Dutch literature, a diverse range of detailed close readings have been conducted analysing the relative importance of certain represented identities as opposed to others (Pattynama 1994, Meijer 1996a, Meijer 1996b, Pattynama 1998, Minnaard 2010, Meijer 2011).
                
                 In recent years, quantitative methods such as social network analysis have made it possible to study character representation on a larger scale (Alberich e.a. 2002, Stiller et al 2003, Elson et al 2010, Lee &amp; Yeung 2012, Karsdorp et al 2012, Agarwal et al 2013, Jayannavar et al 2015, Karsdorp et al. 2015, Lee &amp; Wong 2016, Van der Deijl &amp; Smeets 2018). Insights from e.g. network theory can lead to a broader understanding of the power dynamics between characters. Important aspects of these dynamics are positive (friends) and negative (enemies) relations between characters, as bonds and conflicts in networks are indicative of hierarchical oppositions between represented identities. 
                 In order to gain an empirically informed understanding of character hierarchies in present-day Dutch literary fiction, the present paper models conflicts for all 2137 characters in a corpus of 170 novels that were submitted to one year (2012) of the Libris Literatuurprijs, one of the most prestigious literary prizes in the Dutch language area. It draws on extensive metadata from earlier research in which gender, descent, age, education and profession of all these characters were gathered (Van der Deijl, Pieterse, Prinse, Smeets 2016), as well on more recent research in which relational information (family, lover, colleague, friend, enemy) between these characters was collected (Volker &amp; Smeets 2018). 
            
            
                Methodological design
                Social networks for each of the 170 novels are semi-automatically extracted using the co-occurrence approach described in Smeets &amp; Sanders 2018. These networks are used to model conflicts in two ways, the first of which focuses on conflicts between two characters (dyads), the second on conflicts between three characters (triads). 
                
                    
                        Conflict scores
                        
                            In earlier research (Smeets et al 2018), all characters were ranked with Python’s NetworkX library (Hagberg et al 2008) for five basic network centrality metrics: degree, betweenness, closeness, eigenvector, and Katz. Each of these rankings are an indication of a certain aspect of a character’s relative importance in the story. For every dyad of enemies in the corpus, it is detected who the higher ranked character is. For each of the five centrality metrics, a character’s 
                        
                        conflict score
                         is incremented by 1 in case he/she is higher ranked than his enemy. 
                        
                        	Finally, a multiple linear regression analysis is carried out to test the extent to which a character’s gender, descent, age or education is a predictor of his/her conflict score. The outcome of the regression analysis serves as an indicator of which represented identities are the more powerful ones in the conflict. 
                    
                
                
                    
                        Social (im)balance
                        
                        The social balance theory (Heider 1946) postulates that there is 
                        social balance
                         in a triad when either all three nodes are friends, or when two friends share the same enemy. Conversely, it postulates that there is 
                        social imbalance 
                        when all three nodes are enemies, or when two enemies share the same friend. This is used as a theoretical framework for modelling conflict dynamics between subnetworks of three characters in the corpus.
                        
                        	For every enemy/friend triad, it is automatically established whether it is socially balanced or imbalanced according to the theory. It turns out that the majority of triads, 69%, is socially balanced as opposed to 31% of socially imbalanced triads. Among these two general categories of social balance and imbalance, fully positive and fully negative triads are most present (see Figure 1 for the absolute distributions per type). In light of authoritative narratological theories (Propp 1928, Greimas 1966), the prevalence of social balance is remarkable, as conflict is commonly esteemed to be one of the driving forces behind narrative action. 
                    
                
                
                     For the analysis of conflicts in individual novels, this observed pattern can be used as a general framework to contextualise and evaluate the particularity of (im)balanced triadic subnetworks. One such a contextualisation will be demonstrated by evaluating a single triad in light of the overall pattern.
                    
                
                
                    
                        
                        
                    
                
                Figure 1. Absolute distribution of social (im)balance for all enemy/friend-triads in the corpus divided by type (N =1059)
            
            
                Contribution to the field
                
                    In this paper the two models of conflict will be used to disentangle the complexities of power dynamics in character representation. We will assess the possibilities and challenges of our approach for critiques of literary representation that mainly use qualitative close reading methods. It will be argued that conflict situations co-shape the ideological representation of characters in literature, and the importance of a data-driven and empirically informed approach to character representation will be highlighted. . 
                
                Keywords: 
                    conflict, 
                    social network analysis, Digital Literary Studies, Dutch literature
                
            
        
        
            
                
                    Bibliography
                    
                        Agarwal et al 2013 – A. Agarwal e.a., ‘Automatic Extraction of Social Networks from Literary Text:
                        A Case Study on Alice in Wonderland’. Online, 2013. 
                        
                            http://www.cs.columbia.edu/~apoorv/Homepage/Publications_files/MAIN.pdf
                        
                    
                    
                        Alberich et al 2002 – R. Alberich e.a., ‘Marvel Universe looks almost like a real social network.’ Online, 2002. 
                        
                            https://arxiv.org/abs/cond-mat/0202174v1
                        
                    
                    
                        Butler 1990 – Butler, Judith. 
                        Gender Trouble. USA (Routledge): 1990.
                    
                    
                        Deijl, van der &amp; Smeets 2018 – L. van der Deijl &amp; Roel Smeets, ‘Tussen close en distant. Personage-hiërarchieën in Peter Buwalda’s 
                        Bonita Avenue’. In: 
                        Tijdschrift voor Nederlandse Taal- en Letterkunde 134 (2018) 2, p.123-145 
                    
                    
                        Eagleton 1976 – T. Eagleton, 
                        Marxism and Literary Criticism. USA (Routledge): 1976.
                    
                    
                        Elson et al 2010 – D.K. Elson e.a., ‘Extracting Social Networks from Literary Fiction 2010’. In: 
                        Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (2010), p. 138–147. 
                    
                    
                        Greimas 1966 – A.J. Greimas, 
                        Sémantique structurale, Paris: Presse universitaires de France, 1966.
                    
                    
                        Hagberg et al 2008 – Aric A. Hagberg, Daniel A. Schult and Pieter J. Swart, “Exploring network structure, dynamics, and function using NetworkX”, in Proceedings of the 7th Python in Science Conference (SciPy2008), Gäel Varoquaux, Travis Vaught, and Jarrod Millman (Eds), (Pasadena, CA USA), pp. 11–15, Aug 2008
                    
                    
                        Heider 1946 – Fritz Heider, ‘Attitudes and Cognitive Organization’, In: The Journal of Psychology 21 (1946), pp. 107-112
                    
                    
                        Jayannavar et al
                         2015 – P. Jayannavar e.a., ‘Validating literary theories using automatic social network extraction’. In: 
                        Proceedings of the Fourth Workshop on Computational Linguistics for Literature 
                        (2015), p.32-41. 
                    
                    
                        Karsdorp et al 2012 – Folgert Karsdorp, Peter van Kranenburg, Theo Meder, Antal van den Bosch ‘Casting a spell: Identification and ranking of actors in folktales.’ In: F. Mambrini, M. Passarotti, and C. Sporleder (eds.), 
                        Proceedings of the Second Workshop on Annotation of Corpora for Research in the Humanities (ACRH-2), 2012, p. 39–50. 
                    
                    
                        Karsdorp et al 2015. – Folgert Karsdorp, Mike Kestemont, Christof Schöch, Antal van den Bosch. ‘The ‘Love Equation: Computational Modeling of Romantic Relationships in French Classical Drama.’ In: 
                        Proceedings of the Sixth International Workshop on Computational Models of Narrative, 2015, pp. 98-107.
                    
                    
                        Lee &amp; Yeung 2012 – J. Lee &amp; C.Y. Yeung, ‘Extracting Networks of People and Places from Literary Texts’. In: 
                        Proceedings of the 26th Pacific Asia Conference on Language, Information and Computation 2012 (2012), p.209-218.
                    
                    
                        Lee &amp; Wong 2016 – J. Lee &amp; T.S. Wong, ‘Hierarchy of Characters in the Chinese Buddhist Canon’. In 
                        Proceedings of the Twenty-Ninth International Florida Artificial Intelligence Research Society Conference,(2016), p.531-534.
                    
                    
                        Meijer 1996a – M. Meijer, ‘De verschrikkelijke sneeuwman: projectie, geweld en nieuwe mannelijkheid in het werk van Jan Wolkers’. In: 
                        Het omstreden slachtoffer: geweld van vrouwen en mannen. Ed. R. Romkens &amp; S. Dijkstra. Baarn: Ambo, 1996, p 39-58. 
                    
                    
                        Meijer 1996b – M. Meijer, 
                        In tekst gevat. Inleiding tot de kritiek van de representatie. Amsterdam: Amsterdam University Press, 1996.
                    
                    
                        Meijer 2011 – M. Meijer, ‘Zwartheid in de witte verbeelding’. In: Judit Gera et al. (red.), Diversiteit, Boedapest (Printart Press Kft.): 2011, p.47-74.
                    
                    
                        Minnaard 2010 – L. Minnaard, ‘The Spectacle of an Intercultural Love Affair. Exoticm in Van Deyssel’s Blank en Geel.’ In: 
                        Journal of Dutch Literature 1 (2010) 1, p. 74-90.
                    
                    
                        Pattynama 1994 – P. Pattynama, ‘Oorden en woorden. Over rassenvermenging, interetniciteit, en een Indisch meisje.’ In: 
                        Tijdschrift voor Vrouwenstudies 15 (1994) 1, pp. 30-45. 
                    
                    
                        Pattynama 1988 – P. Pattynama. ‘Secrets and Danger: Interracial Sexuality in Louis Couperus’s 
                        The Hidden Force and Dutch Colonial Culture around 1900’. In: J. Clancy-Smith &amp; F. Gouda, 
                        Domesticating the Empire. Race, Gender, and Family Life in French and Dutch Colonialism. Charlottesville/London: University Press of Virginia, 1998, p. 84-107,
                    
                    
                        Propp 1928 – Vladimir Propp, 
                        Morphology of the Folktale [2nd edition]. First edition translated by Laurence Scott with and introduction by Svatava Prokova-Jakobson, second edition revised and edited with a preface by Louis. A. Wagner and new introduction by Alan dudes. Austin/London (University of Texas Press): 1968 [originally published in 1928]
                    
                    
                        Said 1978 – E. W. Said, 
                        Orientalism. London (Vintage): 1978.
                    
                    
                        Smeets &amp; Sanders 2018 – Smeets, R. &amp; E. Sanders. ‘Character Centrality in Present-Day Dutch Literary Fiction.’ Long Paper at Digital Humanities Benelux, Amsterdam, 8-6-2018. Link.
                    
                    
                        Smeets et al 2018 – Smeets, R., Sanders, E. &amp; A. van den Bosch. ‘Ranking Characters in Present-Day Dutch Literary Fiction.’ Under review in 
                        DH Benelux Journal. 
                    
                    
                        Stiller et al 2003 – J. Stiller e.a. ‘The Small World of Shakespeare's Plays.’ In: 
                        Hum Nat 14 (2003) 4, p. 397-408. 
                    
                    
                        Volker &amp; Smeets 2018 – B. Volker &amp; R. Smeets. ‘Mirrors or alternative worlds? Comparing ego centered networks of characters in contemporary Dutch literature with networks in the population’. 
                        International Network of Social Network Analysis, Sunbelt Social Network Conference, June 2018, Utrecht. R&amp;R in 
                        Poetics.
                    
                
            
        
    
10071	2019	
        
            The study of a historical language like Latin requires a corpus-linguistic perspective. Since we cannot appeal to native speakers of Ciceronian Latin, medieval Latin or pre-classical dialects known from early inscriptions, our knowledge of the language depends on the surviving documents we choose to study.
            Several excellent Latin morphological parsers already exist. (In addition to the list at 
                , note also LatMor 
                 and Parsley 
                .) All of them are designed as comprehensive systems applicable to "Latin" generally, however. Some support adding new vocabulary items, but scholars and teachers studying texts with "non-standard" morphology or orthography have few options. "Normalizing" the text directly contradicts the corpus-linguistic mandate to understand the language as it is attested. Forking one of the existing open-source systems (whether Python, C, Java or a finite state transducer notation like LatMor's SFST-PL) will appeal to few Latinists.
            
            This paper describes an alternate methodology. It differs from current approaches in two ways: by automating the building of parsers tailored to particular corpora, and by identifying all components of a parser's output with canonically citable URN values.
            While it would be possible to build a parser covering all known digital Latin texts, parsers targeted at the language and orthography of specific corpora can reduce the ambiguity of analyses to instances of true morphological ambiguity. In a corpus of Plautus, for example, the surface form "anime" can only be the vocative singular of "animus" (urn:cite2:hmt:ls:n2636 in the citable version of the Lewis-Short dictionary from Furman University). In a diplomatic edition of manuscripts of the Latin Psalms, "e" might represent the orthographic equivalent of classical "ae" so that "anime" could be genitive singular, dative singular, nominative plural or vocative plural of "anima" (urn:cite2:hmt:ls:n2612). A comprehensive morphological parser would have to accept all these possibilities for analyses of "anime". A classical Latin parser, on the other hand, could accept only "ae" as valid first-declension endings; the lexicon for a Latin parser of the Psalms does not need an entry for "animus", since that word does not appear in the Psalms, so the only ambiguity it would identify is the identical form of four case-number combinations of the first-declension noun "anima".
            By using URNs to identify all components of an analysis, we can readily combine analyses from multiple parsers. CITE2 URNs identify collections of discrete objects. (See 
                 At the time of this writing, the CITE2 URN scheme is in expert review with the IETF's URN working group). Since we do not use string values to identify forms or lexical entities, when we analyze Plautus with a classical Latin parser and a diplomatic edition of the Psalms with a parser specific to that corpus, the analyses of each parser can identify the lexical item "anima" with the URN urn:cite2:hmt:ls:n2612. Our parsers can thus recognize that "anime" in the Psalms and "animae" in Plautus are equivalent forms of the same lexical entity, but the token "anime" represents different lexical entities in the two corpora.
            
            This approach opens up new possibilities for research and pedagogy.
            For editors of diplomatic editions, automated morphological analysis is invaluable for validating manually edited texts, but it is only possible when the orthography, lexica of stems and inflectional rules can all be defined for the corpus being edited.
            Morphological data can enrich familiar analytical models such as social networks. Parsing of named entities is often limited because they are precisely the kind of vocabulary missing from standard lexica. If we construct a social network of persons appearing in the same passage and associate with each name its grammatical case, the resulting graph not only distinguishes clusters of co-occuring figures, but indicates what syntactic role they fulfill in relation to each other.
            The approach described here invites a beginning-language pedagogy preparing students to read a particular corpus. It is customary to analyze a digital text in order to determine what vocabulary should be stressed in introductory language courses. In deciding how best to sequence topics, we can also analyze the frequencies of forms and of specific inflectional rules. If supine forms are rare in our target corpus, we might choose not to emphasize them. But we can go further, and evaluate which particular inflectional classes should be emphasized. Does every variation of third-declensions i-stems appear in the corpus we're preparing students to read, or should we devote more time to other topics?
            In this approach, the central technological component is not a Latin parser, but an open-source system for building corpus-specific parsers. It is modelled in part on the Kanónes system for building Greek morphological parsers (described in 
                Bulletin of the Institute for Classical Studies 59-2, 2016, 89-109), but extends and generalizes some of its ideas. As with Kanónes, a digital humanist manages a set of structured text files. A build process managed with sbt (
                ) reads the text files, translates them into source code in SFST-PL, and applies the Stuttgart Finite State Transducer tools to compile binary finite state transducers for working with Latin morphology. The package includes a suite of utility scripts in Scala that can be run from the sbt console and parse the SFST output into an object model. They include scripts to parse a wordlist, summarize a corpus morphologically, and export the morphological analyses to a tabular format suitable for use with tools such as a RDBMS.
            
            Three data sets define the parser for a corpus. First, a plain text file defines the orthography by enumerating all Unicode codepoints allowed in parseable tokens. Second, a set of delimited-text tables defines a lexicon of "stems," recorded in the defined orthography. Each stem is uniquely identified, and associated with a URN for a lexical entity, as well as an inflectional class (roughly corresponding to traditional inflectional classes such as "2nd declension noun stem"). Third, a further set of delimited-text tables defines the inflectional rules that apply to the corpus. The rule is uniquely identified, includes an "ending" recorded in the defined orthography, and is associated with one of the same inflectional classes used in the tables of stems. Sample data sets illustrating how to organize the data for a complete parser include diplomatic editions of Latin manuscripts, Latin texts digitized from print editions from the Tesserae Project, and a "corpus" comprising all paradigms in Allen and Greenough's 
                Latin Grammar.
            
            Other than a text editor to create or modify the data files, the system has only two technical requirements (plus their dependencies): sbt (and therefore a Java SDK), and the SFST toolkit (and its required GNU "make" toolchain).
            Directly using the included scripts is the simplest way to analyze or export results, but the scripts in turn rely on a JVM code library imported by sbt that can be used with any JVM language (including Java, Groovy, Clojure, and Kotlin). DH projects that want to use the parser's output differently can use the code library to ingest the parser's output and have direct access to the data through high-level abstractions (such as a "NounForm", which includes a "Gender" property, which has an enumerated value of "Masculine," "Feminine" or "Neuter").
            While it is less likely that digital humanists will choose to expand on the set of included transducers, the organization of the SFST system supports this, too. The included transducers are chained in a standard design pattern:
            data transducer -&gt; acceptor transducer -&gt; analysis transducers
            The data transducer is the Cartesian product of all stems with all rules. The acceptor transducer filters these so that only combinations of rules and stems belonging to the same inflectional class remain. Analysis transducers suppress some categories of information to provide a mapping from an incomplete set of data to a full analysis. A final transducer that suppresses all analytical information and keeps only stem+ending strings therefore maps surface forms to a full analysis (i.e., it provides mappings like "jacio -&gt; first singular present indicative active of urn:cite2:hmt:ls:n25153"). Alternatively, a transducer that suppresses all data except a URN for a lexical entity and symbols for person, number, tense, mood and voice provides mappings like "first singular present indicative active of urn:cite2:hmt:ls:n25153 -&gt; jacio".
            The theme of DH2019 is "complexities." The approach to morphological analysis presented here respects the complexity of Latin as it is attested in millenia of surviving documents. Managing simple text files to build corpus-specific parsers with analytical output identified by URNs, we can bring a more nuanced corpus-linguistic perspective to research and teaching with digital corpora of Latin.
        
    
10074	2019	
        
            
                Introduction
                Aligning different versions of the same work is both a computational and a philological challenge. In particular, the collation of witnesses of an ancient or medieval text poses specific difficulties due to the coexistence of macro-structural and localised variants, including a large number of formal variants.
                We present an experimental computer-assisted workflow for aligning several witnesses and classifyingvariants. Formal and substantive variants are examples of categories especially relevant for languages which are unstable in their graphic system, as are medieval languages. The case studies are in Old French, and, marginally, Old Spanish. 
                The distinction between formal and substantive variants enables to treat them separately. Stemmatology, for instance, will be mostly interested in the former (even if this has been challenged in Andrews, 2016), while, for linguistic analysis the latter are needed. In automatic collation, based on full transcription of the texts to be compared, the formal variation is generally preserved, but temporarily nullified by means of normalisation or fuzzy match: this enables an accurate alignment of the texts and at the same time the preservation of the original forms.
            
            
                How to handle variation
                Medieval texts, especially in vernacular, often exhibit important variation. At the phrases or words levels, syntactic or graphic variations account for diachronic and diatopic differences, varying scribal practices and the plurality of graphematic standards. This makes it difficult to align sequences between texts, when they have very few letters in common, e.g., 
                    Cait del fuere | 
                    Chiet dou fuerre | 
                    Kiet du feurre (‘[The sword] falls of the scabbard’).
                
                Difficulties due to spelling or flexional variation only add up to already existing variations in word order or substance. Consider the following example taken from Chrétien de Troye's 
                    Chevalier au lion (Meyer, 2006, v. 3701):
                
                
                    H Li frans li dolz ou ert il donques
                
                
                    P Li frans li dous ou estoit donques
                
                
                    V Li franz li doz ou ert il donques
                
                
                    F Li frans li dols ou ert il donques
                
                
                    G Li biaux li preuz ou estoit donques
                
                
                    A Li preus li frans u est il donques
                
                
                    S Li preus li frans u ert il donques
                
                
                    R Li frans li dols u ert il donques
                
                
                    M Li frans li preus ou est il donques
                
                Spelling (e.g., 
                    dolz, 
                    dous, 
                    doz, 
                    dols) and flexional variants (
                    est, 
                    ert, 
                    estoit) go along with substitutions (
                    dous | 
                    preus or 
                    biaux | 
                    frans), additions/deletions (
                    il), or permutations (
                    preuz). In such a case, clearing out spelling and flexional variation might help in resolving the other difficulties. 
                
                This paper offers a new approach to the normalisation task made possible by the developments in the field of NLP and the resources now available for medieval languages, following the steps described in fig. 1.
                
                    
                        
                        Processing workflow
                    
                
                The initial step is the acquisition of the text, from the digital image, done by a combination of manual transcription (for producing ground truth), automated handwritten text recognition, and post-correction. The raw text thus obtained is then structured and stored in an XML/TEI based format. All these tasks are performed before the normalisation step, here represented by lemmatization and linguistic annotation, done with the help of neural network-based taggers/lemmatizers.
                Traditionally, normalisation consists of the preparation of the texts for alignment and might imply lowercasing, removing punctuation or editorial markup, as well as the temporary removal of formal features (Silva and Love, 1969 ; Robinson, 1989). Our proposal is to move to an automatic normalisation performed using NLP tools. Each token (i.e. word) is annotated with linguistic information such as part of speech, lemma and detailed morphological information. This kind of normalisation is only possible when suitable resources are available. For Old Spanish, Freeling (Padró Stanilovsky, 2012) provides a specific module (Boleda, 2011; Porta et al., 2013). For Old French, we used the data provided by the 
                    Geste corpus (Camps et al., 2016), annotated with lemmas, as well as POS and morph tags according to the Cattex scheme (Prevost et al., 2013). With this data, we trained a neural tagger/lemmatizer suitable for variation-rich languages (Kestemont et al., 2017 ; Manjavacas et al., 2019). On the test set, accuracy reached 94.5 and 95% for lemmatization and POS-tagging, and was in the range 94-98.5% for different morphological features.
                
                After normalisation, the texts enriched with linguistic information can be used to perform the alignment. Variation in structure, order or content in medieval texts is favoured by the existence of ‘active textual transmission’ (Vàrvaro, 1970) and by processes of rewriting, prosification/versification, etc. Changes in the order of the structural entities (verses, paragraphs, etc.) are also common. In order to collate these displaced entities, a phase of macro-structural alignment might be needed. This process can be done by a combination of direct expertise and tools conceived for detecting paraphrase, text reuse or computing similarities (Büchler et al., 2014; Jänicke and Wrisley, 2018).
                The very collation is then made by using the collation program CollateX (Dekker et al., 2011 and 2015) in its Python version. CollateX uses multiple alignment algorithms, suitable for the comparison of more than two witnesses (Spadini, 2017); its modular structure, based on the Gothenburg model, enables the user to intervene on each module separately and to add new ones.
            
            
                Automatic categorization of variants
                All these software bricks can be integrated in a more complex pipeline up to the the final output. The modular structure of CollateX enables us to adjust the alignment and the visualization phases, in order to take into account the linguistic annotations added to each token. The alignment is performed directly on the annotation, used as a normalised form. In the creation of the output, some rules are added to compare the original forms with the annotation and to assign a category to the variant. For example, the category 'formal variant' is assigned to aligned tokens which have the same annotations but different original forms, such as:
                
                    mielz (pos: adverb; lemma: 
                    mieus),
                
                
                    miels (pos: adverb; lemma: 
                    mieus),
                
                
                    miaus (pos: adverb; lemma: 
                    mieus).
                
                Additional rules can be used for classifying variants into finer-grained categories, using linguistic annotation (fig. 2).
                
                    
                        
                        Possible classification of variants using linguistic annotation, with examples of possible subcategories and cases. The broad paradigmatic subcategory encompasses synonyms, cohyponyms, hypero-/hyponymes or holo-/meronyms; the semantic subcategory is reserved for lexemes who do not hold this type of relation between them.
                    
                
            
            
                Conclusions and Further research
                This paper presents some early results of an ongoing research on automatic collation and categorization of variants. Performing normalization using NLP tools not only speeds up the task, but also makes the identification of fine-grained categories possible. The case studies show the strong and weak points of this proposal and of the technical solutions for its implementation. Eventually, this research forces us to reflect upon the importance of having software components which are open and modular, in order to improve them and to include them in computational pipelines.
            
        
        
            
                
                    Bibliography
                    
                        Andrews, T. L. (2016). Analysis of Variation Significance in Artificial Traditions Using Stemmaweb. 
                        Digital Scholarship in the Humanities, 
                        31(3): 523–39 doi:
                        10.1093/llc/fqu072.
                    
                    
                        Boleda, G. (2011). Extending the tool, or how to annotate historical language varieties. 
                        Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities. pp. 1–9.
                    
                    
                        Büchler, M., Burns, P. R., Müller, M., Franzini, E. and Franzini, G. (2014). Towards a Historical Text Re-use Detection. In Biemann, C. and Mehler, A. (eds), 
                        Text Mining: From Ontology Learning to Automated Text Processing Applications. (Theory and Applications of Natural Language Processing). Cham: Springer International Publishing, pp. 221–38 doi:
                        10.1007/978-3-319-12655-5_11. 
                         (accessed 25 April 2019).
                    
                    
                        Camps, J.-B., Albarran, E., Cochet, A. and Ing, L. (2016). 
                        Jean-Baptiste-Camps/Geste: Geste: Un Corpus de Chansons de Geste, 2016-…. Zenodo doi:
                        10.5281/zenodo.2630574. 
                         (accessed 25 April 2019).
                    
                    
                        Haentjens Dekker, R., Hulle, D. van, Middell, G., Neyt, V. and Zundert, J. van (2015). Computer-supported collation of modern manuscripts: CollateX and the Beckett Digital Manuscript Project. 
                        Digital Scholarship in the Humanities, 
                        30(3): 452–70 doi:
                        10.1093/llc/fqu007.
                    
                    
                        Haentjens Dekker, R. and Middell, G. (2010). 
                        CollateX. 
                         (accessed 25 April 2019).
                    
                    
                        Jänicke, S. and Wrisley, D. J. (2017). Visualizing Mouvance: Toward a visual analysis of variant medieval text traditions. 
                        Digital Scholarship in the Humanities, 
                        32(suppl_2): ii106–23 doi:
                        10.1093/llc/fqx033.
                    
                    
                        Kestemont, M., Pauw, G. de, Nie, R. van and Daelemans, W. (2017). Lemmatization for variation-rich languages using deep learning. 
                        Digital Scholarship in the Humanities, 
                        32(4): 797–815 doi:
                        10.1093/llc/fqw034.
                    
                    
                        Manjavacas, E., Kádár, Á. and Kestemont, Mike, M. (2019). Improving Lemmatization of Non-Standard Languages with Joint Learning. 
                        arXiv preprint arXiv:1903.06939 (accessed 25 April 2019).
                    
                    
                        Meyer, K. (2006). 
                        Transcription Synoptique Des Manuscrits et Fragments Du Chevalier Au Lion Par Chrétien de Troyes. Université d’Ottawa: Faculté des Arts, Laboratoire de français ancien 
                         (accessed 25 April 2019).
                    
                    
                        Padró, L. and Stanilovsky, E. (2012). FreeLing 3.0: Towards Wider Multilinguality. 
                        Proceedings of the Language Resources and Evaluation Conference (LREC 2012) ELRA. Istanbul, Turkey. May, 2012.
                    
                    
                        Piotrowski, M. (2012). 
                        Natural Language Processing for Historical Texts. San Rafael: Morgan and Claypool.
                    
                    
                        Porta, J., Sancho, J.-L. and Gómez, J. (2013). Edit transducers for spelling variation in Old Spanish. 
                        Proceedings of the Workshop on Computational Historical Linguistics at NODALIDA 2013; May 22-24; 2013; Oslo; Norway. NEALT Proceedings Series 18. Linköping University Electronic Press, pp. 70–79.
                    
                    
                        Prévost, S., Guillot, C., Lavrentiev, A. and Heiden, S. (2013). 
                        Jeu d’étiquettes Morphosyntaxiques CATTEX2009. version 2.0 
                         (accessed 25 April 2019).
                    
                    
                        Robinson, P. M. W. (1989). The Collation and Textual Criticism of Icelandic Manuscripts (1): Collation. 
                        Literary and Linguistic Computing, 
                        4(2): 99–105 doi:
                        10.1093/llc/4.2.99.
                    
                    
                        Silva, G. and Love, H. (1969). The identification of text variants by computer. 
                        Information Storage and Retrieval, 
                        5(3): 89–108 doi:
                        10.1016/0020-0271(69)90014-X.
                    
                    
                        Spadini, E. (2017). The role of the base manuscript in the collation of medieval texts. In Boot, P. and alii (eds), 
                        Advances in Digital Scholarly Editing. Papers Presented at the DiXiT Conferences in The Hague, Cologne, and Antwerp. Leiden: Sidestone Press, pp. 345–50.
                    
                    
                        Varvaro, A. (1970). Critica dei testi classica e romanza. Problemi comuni ed esperienze diverse. 
                        Rendiconti Dell’Accademia Di Archeologia, Lettere e Belle Arti Di Napoli, 
                        XLV: 73–117.
                    
                
            
        
    
10081	2019	
        
            
                Introduction
                Writing often takes place or is displayed on a two dimensional surface, but many of the digital techniques for the representation of language reduce two dimensions to one. This reduction leads to many powerful tools in corpus linguistics, which depends heavily on patterns involving words appearing in linear sequences. However, it is widely acknowledged that language is not wholly captured by a purely sequential representation. As (Gross, 1997, p16) notes in the context of poetry: ‘Texts that are presented as spatial (rather than linear) arrangements in order to activate the visual composition on the page as an element of signification have a long tradition’. 
                More widely than poetry, layout in the two dimensional space of a single page can carry meaning – alignments both horizontal and vertical of lines of text, and placement of blocks of text in relation to images and diagrams are a key feature of multimodal documents (Bateman, 2008). Frameworks for such documents have been proposed by Bateman and others. Further areas where layout is essential, including comics and graphic novels (Bateman et al., 2017), have also been studied. 
                It is striking that although representing two dimensional structure in documents of many kinds is clearly relevant to the digital humanities, existing work has made very little use of the techniques of qualitative spatial representation that have been applied in artificial intelligence over more than the past two decades. This paper describes work currently in progress to apply these particular techniques as a means of representing some aspects of the two dimensional structure of poetry layout.
            
            
                Qualitative Relations in Poetry Layout
                The idea of qualitative spatial representation (QSR) is that spatial relationships, such as 
                    next to, alongside, bordering, overlapping, and many other topological notions, can be represented computationally using logic. An introduction to the technical foundations of QSR is provided by (Cohn &amp; Renz, 2008). The motivation in artificial intelligence is that humans use common sense spatial concepts in everyday situations rather than the numerical coordinates which predominate in most computational representations of space. The origins of QSR go back to philosophical interests in an account of space fitting human experience, such as the theory of extensive connection proposed by (Whitehead, 1929). 
                
                To explain how QSR can be applied to the space of poetry in its printed form we will consider some examples. The following is the result of some standard image processing techniques applied an image of Southey’s 
                    Cataract of Lodore printed in 1823. The text here occupies four pages but initially we only consider relationships between successive lines of text.
                
                
                    
                    Layout in Southey’s Cataract of Lodore
                
                This poem is a well-known example of visual arrangement as the form follows the cascade of water: initially having a variety of directions before a long descent ending in progessively widening course as it reaches the bottom. Many related examples are known and described in (Gross, 1997) and (Hollander, 1975). Two further examples are given in Figure 2.
                
                    
                    Layout in poems by Eavan Boland (left) and by Alasdair Gray (right).
                
                On the left of Figure 2 is Eavan Boland’s poem “CODE. An Ode to Grace Murray Hopper 1906-88 maker of a computer compiler and verifier of COBOL.” from (Boland, 2001) consists of four blocks in which lines start successively further to the right, interleaved with three blocks in which all lines are justified on the left. On the right of Figure 2 is Alasdair Gray’s poem 
                    First of March 1990 (Gray, 2010, p128) consisting of six blocks of text sharing a roughly similar shape. In each block the lines end successively further to the left down the page, although the right hand ends do not display any easily identifiable regularity.
                
                Although the examples have been presented visually, the reduction of poetry to simplified images does not give a computational means of comparing one poem to another. Given a corpus of tens of thousands of poems, how can the space of different shapes be understood? How can one query such a corpus for poems of a particular kind of shape? How could shapes be described, and how might one map changes to favoured shapes over time? These questions suggest a visual kind of distant reading (Moretti, 2005) but one we have begun to investigate using QSR rather than geometrical methods. 
            
            
                Application of Allen’s Interval Relations
                One basic QSR technique comes from the work of Allen (1983) in qualitative relationships between intervals of time. If we consider possible ways two intervals may relate, one answer is the 13 relationships indicated on the left in Figure 3. This shows 7 ways the uppermost interval of each pair relates to the lower one. The last 6 relations have inverse relations. Although the inverse of before might be called after, we use the initial letter of each of the 6 followed by “i” to indicate the inverse. The application of this scheme to a sequence of lines appears on the right in Figure 3.
                
                    
                    The Allen relations applied to layout
                
                This means the spatial arrangement can be represented by: di, di, di, e, e, e, d, d, d. Of course, this loses many features, such as the left-right and up-down symmetry, in this case. However it does capture the structure of: lines becoming shorter on each side, then continuing down the page at a fixed width and finally expanding on both sides. 
                We have developed prototype software by coding in Python which determines qualitative relationships between lines and between blocks of text. This uses standard image processing techniques to segment images of pages into text lines as rectangular regions. Qualitative relationships are then calculated from the positions of these regions.
                In continuing work we are exploring other systems of spatial relationships, since the ones provided by Allen are only one example of many that are mentioned in (Cohn &amp; Renz, 2008). We note that (Dubba et al., 2015) uses spatial relations between bounding boxes in video frames to detect patterns making up events. It is an exciting possibility that QSR can describe patterns making up visual poetic structure in an analogous way.
            
        
        
            
                
                    Bibliography
                    Allen, J. F. (1983). Maintaining knowledge about temporal intervals. 
                        Communications of the ACM, 26, 832–843. 
                    
                    Barker, N. (2016). 
                        Visible Voices. Manchester: Carcanet.
                    
                    Bateman, J. A. (2008). 
                        Multimodality and Genre. A Foundation for the Systematic Analysis of Multimodal Documents. Basingstoke: Palgrave Macmillan.
                    
                    Bateman, J. A. et al. (2017) An open multilevel classification scheme for the visual layout of comics and graphic novels: Motivation and design. 
                        Digital Scholarship in the Humanities, 32(3), 476-510.
                    
                    Boland, E. (2001). 
                        Code. Manchester: Carcanet.
                    
                    Cohn, A. G., &amp; Renz, J. (2008). Qualitative spatial representation and reasoning. In F. van Harmelen, V. Lifschitz, &amp; B. Porter (Eds), 
                        Handbook of knowledge representation. Amsterdam: Elsevier, pp. 551-596.  
                    
                    Gray, A. (2001). 
                        Collected Verse. Two Ravens Press.
                    
                    Gross, S. (1997). The Word Turned Image: Reading Pattern Poems. 
                        Poetics Today, 18(1), 15-32
                        .
                    
                    Hollander, J. (1975). 
                        Vision and Resonance. Two senses of poetic form. New York: Oxford University Press.
                    
                    Moretti, F. (2005). 
                        Graphs, Maps, Trees. Abstract models for a literary history. New York: Verso.
                    
                    Dubba, K. S. R., Cohn, A. G., Hogg, D. C., Bhatt, M., &amp; Dylla, F. (2015). Learning Relational Event Models from Video. 
                        Journal of Artificial Intelligence Research, 53, 41-90.
                    
                    Whitehead, A. N. (1929). 
                        Process and reality: An essay in cosmology. Cambridge: Cambridge University Press. 
                    
                
            
        
    
10082	2019	
        
            
                Brief description
                The goal of this workshop is to teach participants how to use Archetype while introducing them to the theoretical and practical issues around describing and categorising handwriting and other visual communication in a digital context. This will be done primarily through practical application of the software to a range of different cases including not only left-to-right alphabets but also hieroglyphs, ideographs and others as well as right-to-left and top-to-bottom scripts. Attention will also be paid to decoration and other forms of graphic communication, insofar as it comprises repeated visual elements that can be compared by Art Historians or others much as components of letters are compared by palaeographers. 
                
                    
                    A screenshot of Archetype as used in the DigiPal project
                
                Archetype (formerly known as the DigiPal Framework) is a generalised system for the online presentation of images with structured annotations and data which allows users to analyse, describe, search for, view, and organise detailed characteristics of handwriting or other material in both verbal and visual form. Designed primarily for the palaeographical analysis of historical handwriting, it was first developed at King’s College London for the Digital Resource and Database for Palaeography, Manuscript Studies and Diplomatic (DigiPal) project, funded by the European Research Council, and has since been extended by the King’s Digital Lab particularly through the Models of Authority and Exon Domesday projects. Available as FOSS, it now used in around two dozen research projects, most of which are small private studies but some of which are the result of large funded research projects. Further details are available from the URLs and bibliography below. Applications to date range from medieval Latin charters to modern draft manuscripts; parchment to inscriptions on stone and coins; writing in Hebrew and Greek to Chinese or Cuneiform; and decoration in manuscripts, tapestries and paintings. Results can be interrogated via a web API or through XML exports with fully customisable templates.
                For this workshop, participants will be introduced to the principles and practice of the software and will discuss some of the underlying theoretical issues. They will learn to understand and use the Archetype model for writing and other graphic communication, including how to customise the software for their own purposes. They will see different ways in which the software can be and is being used, not only for small and large research projects but also for teaching and public engagement. Example materials will be provided, but participants are strongly encouraged to bring their own materials and research questions to experiment with. Indeed, the workshop will end with an opportunity for participants to work on their own materials, individually or in small groups, while sharing the challenges, problems and solutions that they face with the leaders and other participants.
                
                    
                    Screenshot of Archetype applied to decoration
                
                Two key themes will underlie the workshop. The first is that Archetype is designed primarily to enable manual annotations of images. This emphasis on manual annotations is in deliberate contrast to word spotting, character segmentation and OCR/HTR: the goal is not to annotate every occurrence of every symbol, but rather to enable the researchers to make their own decisions and to communicate those decisions to others, a principle very much in the spirit of ‘algorithmic accountability’ which is often difficult if not impossible to achieve with machine vision and deep learning. 
                The second key theme of the workshop is that Archetype involves 
                    structured annotations, and this distinguishes it from other image annotation systems. To use Archetype requires creating a model of the handwriting or other visual communication, reflecting on which visual elements are significant to the research questions at hand. This then enables users to carry out requests such as ‘show me examples of letters with ascenders written by a given scribe’; ‘show me examples of the hands of different people drawn in a given set of manuscripts’; or ‘show me images of personal names in this document’. However, defining an appropriate model is a significant challenge that we will discuss and experiment with in practice. Comparison and connections will also be made to some other relevant ontologies and descriptive models such as IconClass, the Biblissima and CRMtex adaptations/extensions of CIDOC-CRM, and the IDIOM ontology for Mayan script. 
                
                
                    
                    Screenshot of Archetype as used in the Exon Domesday project
                
                Participants can expect to end the day with a local installation of Archetype which they will have customised during the workshop, and the skills to extend their Archetype installations in a structured manner that will lend itself to effective research. The workshop will focus on how to use Archetype on participants’ own computers for their private use, but the results could later be installed on a server for team work and/or publication.
                
                    
                    The default home page of Archetype before customisation
                
            
            
                Workshop leaders
                Peter A. Stokes, École Pratique des Hautes Études, Université PSL
                    
                    peter.stokes@ephe.psl.eu
                
                Peter has around fifteen years of experience in applying digital methods to palaeography and manuscript studies, including leading the team that developed Archetype. He has a degree in Computer Engineering and a joint degree in Classics and medieval English, and a PhD in palaeography (University of Cambridge). He has led or co-led several large projects in digital humanities, including an European Research Council Starting Grant (DigiPal) and two Arts and Humanities Research Council grants (Models of Authority and Exon Domesday), as well as a Leverhulme Early Career Fellowship in digital palaeography. He has published on palaeography and digital humanities, as well as name-studies, lexicography, and Anglo-Saxon charters.
                Debora Marques de Matos, University of Münster
                    
                    debora.matos@uni-muenster.de
                
                Debora is a researcher at the University of Münster, Germany. With a background in art history, Hebrew palaeography and digital humanities, she currently leads 'The Other Within,’ a project that crosses research areas such as iconography, big data, and computer vision. Before that, she conducted research on the material transition from manuscript to print, and her PhD thesis was dedicated to the production of illuminated Hebrew books in Portugal. She has extensive experience with Archetype applied to Hebrew script and book iconography.
                Neil Jakeman, King’s Digital Lab, King’s College London
                    
                    neil.jakeman@kcl.ac.uk
                
                Neil has been working on DH projects at King’s since 2011, having contributed both as a Developer and more recently as a Research Software Analyst. He has an MSc in Geographical Information Systems and Environmental Management (University of Brighton) and a BSc in Geology (University of Cardiff). Neil’s experience with geospatial systems complements the technological approaches used in the annotation of images and he is now acting as an ambassador for Archetype to push the framework into new domains and applications such as Numismatics, Iconography, and Art History.
            
            
                Target audience and expected numbers 
                The target audience is people interested in structured annotations applied to visual communication, including interest in problems of modelling and communicating such content. The core application is palaeography and epigraphy, including not just alphabetic languages but also hieroglyphs, ideographs, and so on, but the tool is also useful for those interested in manuscripts, inscriptions and other forms of writing more generally, as well as for art history. It is also relevant to those interested in modelling writing and models for deeply structured annotation. Previous experience has shown considerable interest from PhD students and librarians as well as from those in traditional positions of teaching and research. Archetype and its immediate predecessors have been presented several times at DH and have received significant interest. Expected numbers for this event are therefore around ten and potentially up to twenty participants.
            
            
                Length and format
                
                    Length: One day
                
                
                    Format: The workshop will be very hands-on, with participants required to work actively on their laptops throughout the day. The general format will be that the workshop leaders present a new element of Archetype, both in terms of its theoretical rational and its practical usage, and then attendees test this out on the example material that will be provided to them. This pattern will largely be repeated throughout the day. At the end of the day, however, participants will be given time to start developing their own project with the help of the workshop leaders.
                
                A tentative outline is provided. Please note that this will be adjusted according to time and in order to accommodate participants’ interests and existing skill-level.
                
                    Theoretical introduction: Principles and models for describing handwriting and written communication.
                    Getting started: setup, annotating images, adding texts.
                    Adding Images and Hands.
                    Adding Manuscripts (Items and Item Parts).
                    Describing Graphs; adding new Characters, Allographs, Components and Features.
                    Textual markup; linking text and image.
                    Customising the framework: adding static content, changing menu structure, customising the home page.
                    Advanced customisations: customising the search pages (basic Python required); using the Web API for custom searching (basic XSLT required); introduction to the customisation framework.
                    Participants working time: an opportunity for participants to work on Archetype according to their own interests, with the support of workshop leaders and each other.
                    Wrap-up discussion and further steps.
                
            
            
                Requirements for attendees
                Attendees are required to bring their own laptops with administrator rights to install new software. Archetype depends on Docker which works with Linux, MacOS or Windows, but it is somewhat limited on Windows and so Linux or MacOS are strongly recommended if possible (see the installation instructions on DockerHub in the references below). Those running Windows must also ensure in advance that their system supports virtualisation, as Docker and therefore Archetype will not function without it. 
                Attendees will be expected to install the Docker and Archetype software in advance of the workshop. Instructions are available at 
                    https://hub.docker.com/r/kingsdigitallab/archetype/, and attendees should contact the workshop leaders directly if they encounter any problems in the installation process.
                
                Attendees will be provided with sample materials to work with; however, they are encouraged to bring their own materials to use with the system. To do this, they should come with one or more other documents in mind, including digital images of the documents and ideally some research questions to investigate.
                Most of the day will be working with a web-based GUI interface, but the ‘advanced customisation’ session will involve working with Python, XSLT and command-line interfaces. Participants who are not already familiar with these are still extremely welcome and will receive the necessary support from the workshop leaders, but at least a basic knowledge of these will allow participants to gain more from this session.
            
        
        
            
                
                    Bibliography
                    
                        Anon. (2014) 
                        Digital Resource and Database of Palaeography, Manuscript Studies and Diplomatic. London: King’s College. 
                        http://www.digipal.eu (accessed 8 January 2019).
                    
                    
                        Anon. (2017a). 
                        Archetype [Website]. London: King’s College. 
                        https://archetype.ink (accessed 8 January 2019).
                    
                    
                        Anon. (2017b). 
                        Models of Authority: Scottish Charters and the Emergence of Government 1100–1250. London: King’s College. 
                        https://www.modelsofauthority.ac.uk (accessed 8 January 2019).
                    
                    
                        Brookes, S., Stokes, P. A., Watson, M. and Marques de Matos, D. (2015). The DigiPal Project for European scripts and decorations. In Conti, A., Da Rold, O. &amp; Shaw, P. A. (eds), 
                        Writing Europe 500–1450: Texts and Contexts. D.S. Brewer, pp. 25–58.
                    
                    
                        Castro, A. (2017). 
                        VisigothicPal : Dating and Placing Visigothic Script. London: King’s College. 
                        http://visigothicpal.com (accessed 8 January 2019).
                    
                    
                        Felicetti, A. and Murano, F. (2017). Scripta manent: a CIDOC CRM semiotic reading of ancient texts. 
                        International Journal on Digital Libraries, 18(4): 263–70 doi:
                        10.1007/s00799-016-0189-z.
                    
                    
                        Gettatelli, V. (2018). Using Archetype: reflections on my Erasmus traineeship. 
                        King’s Digital Lab Blog. London: King’s College 
                        https://www.kdl.kcl.ac.uk/blog/using-archetype/ (accessed 9 January 2019).
                    
                    
                        Gronemeyer, S. and Diehr, F. (2018). Organising the unknown: a concept for the sign classification of not yet (fully) deciphered writing systems exemplified by a digital sign catalogue for Maya hieroglyphs. 
                        Digital Humanities 2018 Book of Abstracts. Mexico City 
                        https://dh2018.adho.org/en/organising-the-unknown-a-concept-for-the-sign-classification-of-not-yet-fully-deciphered-writing-systems-exemplified-by-a-digital-sign-catalogue-for-maya-hieroglyphs/ (accessed 8 January 2019).
                    
                    
                        Murano, F. and Felicetti, A. (2017). 
                        Definition of the CRMtex: An Extension of CIDOC-CRM to Model Ancient Textual Entities, version 0.8 (CIDOC-CRM) 
                        http://www.cidoc-crm.org/crmtex/sites/default/files/CRMtex_v0.8.pdf (accessed 8 January 2019).
                    
                    
                        Noël, G. et al. (2019b). 
                        Archetype [Source Code]. London: King’s College. 
                        https://github.com/kcl-ddh/digipal (accessed 8 January 2019).
                    
                    
                        Noël, G. et al. (2019c). Archetype [Docker Image]. London: King’s College 
                        https://hub.docker.com/r/kingsdigitallab/archetype/ (accessed 8 January 2019).
                    
                    
                        Stokes, P. A. (2012). Modelling medieval handwriting: a new approach to digital palaeography. 
                        DH2012 Book of Abstracts. Hamburg, pp. 382–85 
                        http://www.dh2012.uni-hamburg.de/conference/programme/abstracts/modeling-medieval-handwriting-a-new-approach-to-digital-palaeography (accessed 8 January 2019).
                    
                    
                        Stokes, P. A. (2017). Scribal attribution across multiple scripts: a digitally aided approach. 
                        Speculum, 92(S1): S65–85 doi:
                        10.1086/693968.
                    
                    
                        Stokes, P. A. (2018). Modelling multigraphism: the digital representation of multiple scripts and alphabets. 
                        Digital Humanities 2018 Book of Abstracts. Mexico City, pp. 292–96.
                    
                    
                        Stokes, P.A. et al. (2018). 
                        Exon: The Domesday Survey of South-West England. London: King’s College. 
                        http://www.exondomesday.ac.uk (accessed 8 January 2019).
                    
                    
                        Stokes, P.A. et al. (2019a). 
                        Archetype [Wiki]. London: King’s College. 
                        https://github.com/kcl-ddh/digipal/wiki (accessed 8 January 2019).
                    
                    
                        Stokes, P.A., Brookes, S., Noël, G., Buomprisco, G., Marques de Matos, D., and Watson, M. (2014). The DigiPal Framework for script and image. 
                        Digital Humanities 2014 Book of Abstracts. Lausanne: pp. 512–14. 
                        http://dharchive.org/paper/DH2014/Poster-193.xml (accessed 8 January 2019). Poster available at 
                        http://www.digipal.eu/media/uploads/uploads/images/blog_posts/2014/DH2014%20Poster.pdf (accessed 8 January 2019)
                    
                    
                        Stokes, P.A., Brookes, S., Noël, G., Davies, J.R., Webber, T., Broun, D., Taylor, A., and Tucker, J. (2016). The Models of Authority project: extending the DigiPal Framework for script and decoration. 
                        Digital Humanities 2016: Conference Abstracts. Krakow: pp. 896-99, 
                        http://dh2016.adho.org/abstracts/387 (accessed 8 January 2019)
                    
                
            
        
    
10095	2019	
        
            
                Eververse is a project which synthesises perspectives from disciplines in the humanities and sciences to develop critical and creative explorations of poetry and poetic identity in the digital age. Deploying tools and methods from poetic theory, data analysis, and Natural Language Generation (NLG): the automatic production of natural language output from a non-linguistic data source (Reiter and Dale, 2000). 
                Eververse uses data from quantified self (QS) devices to automatically generate and publish poetry which correlates to the wearer/poet’s varying physical states.
            
            
                
                    Context
                
                One of the more common ideas to be found in different statements and theories of poetry presents the poet as a creative vessel or conduit, admitting the sensory input of the world into their bodies and minds, and producing poetic output in turn. For Walt Whitman, every atom of being and sensation was an appropriate inspiration for poetry, and his works exhaustively catalogued the varieties of personal and public experience in nineteenth-century America. In poetics such as this, art increasingly collapses into the being and identity of the artist. Poet W. B. Yeats famously articulated this conundrum, asking “How can we know the dancer from the dance?” The production processes of print, which intervened between the poet and their desire for immediate and spontaneous expression, are circumvented in the digital age, as poets use the internet and social media to create a poetry that is “vast, instantaneous, horizontal, globally distributed, paper thin, and, ultimately, disposable” (Goldsmith, 2016: 195). The internet is shaping contemporary poetry with these characteristic forms, and by incorporating the modalities of images, gifs, video, and sound. 
                    Eververse seeks to explore networked technologies and their affordances to articulate new and novel means of being a poet in the digital age.
                
            
            
                
                    Project description
                
                
                    Eververse sends biometric data from a Fitbit fitness tracking device worn by the project PI/poet to its custom-built poetry generator. This generator utilises NLG techniques to output poetic text published in real time, and 24/7, on the 
                    Eververse website.
                
                The form and content of the poetic output is designed to change according to different physical sensations and experiences in the poet’s waking and sleeping life. Following Charles Olson’s injunction that “the line comes...from the breath, from the breathing of the man who writes” (Olson, 2008), 
                    Eververse’s poetic lines decrease in length as the poet’s heart rate increases and breath contracts. Similarly, the response to the randomness of the dream sleep (REM) state is an increased irregularity in the poetic form. Content, too, reflects these variations, as heightened-sentiment vocabulary is produced to reflect the emotional intensification of an increased heart rate, while the dream sleep (REM) state generates 
                    surreal images and vocabulary.
                
            
            
                
                    Technical description
                
                The 
                    Eververse application consists of three main modules. The first module interfaces with the Fitbit device and its data through its Application Programming Interface (API). The activity data of the poet wearing the device is then sent, in JSON form to the NLG module referred to as the ‘generator.’ This generator carries out a number of steps in order to generate and return a poetic couplet based on a conceptual model of states based on the activity information contained within the passed JSON data. The number of words and the frequency of the generated couplets correlate with the heart rate of the poet, whereas the textual content of the couplet is generated from the input corpus which is fed to the generator. Thematic links exist between the content of the input poetic corpora and the conceptual topics of physiology (broadly), and sleep and the body (specifically) corpus comprises poems on the topic of the body; all poems are previously published and none is composed by the 
                    Eververse poet. In order to disassemble and reassemble the corpora for publication, they are arranged in a reverse ngram matrix and further shaped into a frequency lookup table by Poesy, a Markov Model-based Natural Language Poetry Generator. The lookup table is used to create verse lines and a python library, 
                    Pronouncing, is deployed to rhyme the verses. In short, our method takes a language model approach similar to (Barbieri, G., et al., 2012) although we do exploit some semantics, specifically alignment of couplets with Fitbit activity states. 
                
                The generator is written mainly in the Python programming language using the micro web framework, Flask. It consists of a web interface to display the generated poetry and an administrator interface that is used to define heart rate parameters for different zones and to determine the form and content of the verse that corresponds to these zones. The extensible approach we use to build the poetry generator means the project can easily incorporate additional biometric data types along with their associated corpora in future.
                The public user interface created to display the generated poetry relies on a number of Open Source JavaScript libraries. These libraries enable display of generated text (Handlebars.js, Textillate.js) and retrieval of data from the web application’s API and user interface animations (jQuery). The dynamic background images are created in realtime, and utilise the activity data as an input to affect their appearance, representing a visual correlate to the generated poetry.
                Multiple versions of this interface were created for deployment on the web, in a live performance environment, and for display in a standalone exhibition setting. Each interface was adapted to take into account the context in which it would be experienced, for example, differences in how, or if, user interaction was required, and addressing the differing requirements for text size, line spacing, and overall page layouts.
            
            
                
                    Conclusion
                
                The presentation of this paper will report on the technical work completed to develop Eververse, while reflecting on the implications of the project for issues in poetics, authorship, and automated literature generation. In addition, the presentation will describe deployments of the project in web, exhibition, and live contexts, concluding with a brief live demonstration. Accessible supporting materials for the conference presentation will be made available in English, Irish, Italian, and Urdu
                    .
                
            
        
        
            
                
                    Bibliography
                    
                        Barbieri, G., et al. (2012). Markov Constraints for Generating Lyrics with Style. 
                        ECAI’12: 115-20.
                    
                    
                        Goldsmith, K. (2016). 
                        Wasting Time on the Internet. New York: Harper.
                    
                    
                        Olson, C. (2008). Projective Verse. In Cook, J. (ed), 
                        Poetry in Theory: An Anthology, 1900-2000. Blackwell, pp. 288–95.
                    
                    
                        Reiter, E. and Dale, R. (2000). 
                        Building Natural Language Generation Systems. Cambridge: Cambridge University Press.
                    
                    
                        Tonra, J., et al. (2019) Eververse. https://eververse.nuigalway.ie/
                    
                
            
        
    
10103	2019	
        
            
                Introduction / Importance 
                The sonnet is a prodigious poetic form. Since its invention in the 13th century by Giacomo da Lentino, hundreds of poets have written many thousands of sonnets in European literary languages. It was popularized by Petrarch in the 14th century, translated by Wyatt and Camões in the 16th, and reformulated by poets from Shakespeare to Rilke to Frost. The experimental poet Raymond Queneau has even written a machine-generated sequence whose lines can be recombined in a hundred trillion different ways. 
                This project has begun to compile every extant sonnet into a database &lt; acriticismlab.org &gt;, in order to quantify their features through time. Those features include dates, languages, authors, diction (word choices), sentiments, named entities, and form. 
                My research question is straightforward: just what is a sonnet? Definitions have tended to focus on its form: 14 lines of rhymed ten-syllable (pentameter) verse. Subtypes, including the Petrarchan and the Shakespearean sonnet, are often based on rhyme scheme. But another definition is based on generic rather than formal features: a first-person reflection or “dialectical self-confrontation,” often with a volta (or turn) from problem to resolution (Oppenheimer: 1989). To what degree, then, is the sonnet a form or a genre? What subtypes will a comprehensive, quantified taxonomy reveal? 
                I am pursuing these inquiries by gathering as many known specimens of sonnets as possible, and then quantifying my analysis of their metadata. This includes metadata at the level of tokens and lines; of clauses and sentences; of rhyme-units (couplets/quatrains/sestets/octets) and complete sonnets; and of their published sequences. There are many features of these units that can be encoded, largely through automated natural-language processing. Tokens can be lemmatized and tagged with their parts of speech; their order and frequencies can be modelled as topics; their syllables per line can be counted; their rhyme with other tokens can be represented. The only human-dependent encoding the database includes at present leverages the expertise of anthology editors: orthography, punctuation, authors, dates, and copyright. 
                The sonnet genre must be localized in its diction. Some words appear more frequently than others, particularly in the sonnet’s early centuries of first-person lovelorn reflections: words like ‘love’ and ‘she’ and ‘suffer’ and so on. So, too, do words describing the sonnet’s own composition: words like ‘ink’ and ’lines’ and (simply) ‘this’. But genre can be quantified at the level of the sentence, as other scholars have discovered by analyzing topics and principal components in Shakespeare’s sentences (Estill and Meneses: 2018; Hope and Witmore: 2010). This project will determine what generic features the sonnet’s words and sentences reveal. 
            
            
                Methods 
                The ACL Sonnet Database has standardized its texts according to the TEI guidelines, making them available to basic query functions and JSON object serialization. Thus far it contains 1880 Englishlanguage sonnets, including 445 transcribed from a single print anthology (Hirsch and Boland: 2008). My students and I have populated this repository first with English-language sonnets because they are numerous enough to offer a test case for machine-enabled research in any natural language. 
                The database also maintains a Python class for connecting to its data via the RESTful API, automating much of the data parsing for analysis with software like the Natural Language Toolkit (NLTK). Initial student-driven inquiries began with close readings of ten sonnets from the anthology to identify quantifiable features. Students have charted the frequency distributions of the sonnets’ rhyme schemes; enjambment; rhetorical figures (anaphora and epistrophe); and topics, including rhetorical questions and references to celestial objects and classical muses. 
            
            
                Results 
                At this proof-of-concept stage, the database offers results only in these limited domains, and on this limited dataset. At the time of the DH2019 conference, it will have many more thousands of sonnets. I will report on their quantifiable formal characteristics, including rhyme schemes, meter, line lengths, sentence lengths, word frequencies, part-of-speech distributions, and ngrams. I will also report on topic models and the sonnets’ principal components distributed through time, author nationality and gender, and other salient subdivisions. 
            
            
                Discussion 
                Anthologies of sonnets are sufficient for preliminary student-driven inquiries, but to generate insights into the sonnet writ large, a wider net is necessary. I have begun conversations with machine-learning specialists to train a neural network to recognize sonnets in undifferentiated text files, based on the formal and generic characteristics of sonnets isolated by anthology editors. To prepare for this phase my approach will be two-pronged: to give students another dozen anthologies for further transcription; and to use that expanding repository of sonnets as a training set for a machine-learning process that will identify similar poems in a corpus of 70,000 English texts printed before 1700, the Early English Books Online - Text Creation Partnership (EEBO-TCP) corpus. Early sonnets establish conventions to which later English sonnets respond, so they are a valid place to begin this inquiry. That process has already begun with a subset of 18,000 XML files from the EEBO-TCP corpus containing the &lt;l&gt; element, denoting lines of poetry. My lead programmer, who built the database, will write an algorithm that parses these undifferentiated elements into clusters of 14-line sequences, on the provisional assumption that all 14-line stanzas or poems bear a family resemblance to the sonnet. (This, too, is a provisional assumption; there are sonnets, including one by Shakespeare, of irregular lengths.) I will begin with 14-line sequences in order to identify the extra-formal characteristics that are twinned with this form; only then can I unshackle the detection algorithm from the constraints of form, to see which other poetic units bear the nearest affinity.
            
        
        
            
                
                    Bibliography
                    
                        Estill, L., and Meneses, L. (2018). Is Falstaff Falstaff? Is Prince Hal Henry V?: Topic Modeling Shakespeare’s Plays. Digital Studies/le Champ Numérique 8(1). 
                    
                    
                        Hirsch, Edward and Boland, Eavan (eds.) (2008. 
                        The Making of a Sonnet: A Norton Anthology. New York; London: W. W. Norton. 
                    
                    
                        Hope, J., and Witmore, M (2010). The Hundredth Psalm to the Tune of “green Sleeves”: Digital Approaches to Shakespeare’s Language of Genre. Shakespeare Quarterly 61(3). 
                    
                    
                        Oppenheimer, Paul (1989). 
                        The Birth of the Modern Mind: Self, Consciousness, and the Invention of the Sonnet. New York; Oxford: Oxford University Press.
                    
                
            
        
    
10116	2019	
        
            
                This paper reflects on the problem of the ontological status of text in the digital environment. Referring to Briet (1951), Van Zundert and Andrews (2017) drew on Briet’s fluid concept of documentation and pointed to a similar fluidity in the concept of text. Here we draw a parallel to debates in the field of philosophy of consciousness to examine how this may help us to understand the specific textuality of digital text.
            
            In his 2003 article, P.M.S. Hacker confronted Thomas Nagel’s reasoning in “What Is It Like to Be a Bat?” (Nagel, 1974). Nagel had argued that consciousness has a unique individual subjective quality that defies reduction to the materiality of body and mind. Hacker’s refutation of Nagel’s reasoning is almost exclusively based on the observation that Nagel’s argumentation in key places is syntactically faulty (Hacker, 2003:170). For Hacker—a Wittgensteinian philosopher of language—erroneous syntax implies faulty logic and thus incorrect argument. His counter-argument implicitly reduces consciousness to that which can be understood and explained linguistically. But this move ignores thereby the evidence and arguments put forward in fields beyond linguistics and philosophy that speak against the idea of consciousness as a solely linguistic construct: empirical evidence from the neurosciences (cf. Koch, Massimini, Boly, and Tononi, 2016), theories on consciousness that relate consciousness to mind and body such as came forward from integrated information theory (cf. Tononi, 2012), and ideas surrounding embodied cognition (cf. Shapiro, 2012). Sources from many disciplines thus seem to suggest we cannot understand consciousness solely as a linguistic construct, or one that must be explained exclusively by linguistic means. A concrete understanding of consciousness is likely to integrate types of knowledge and experiences that are distinctly non-linguistic and hard to properly linguistically express.
            Arguably then, mind and consciousness are better understood as emergent properties of a body orchestrating multiple types of sensations and information. Explaining consciousness through linguistic properties alone would be akin to trying to explain plastic arts by words solely. As early 20th century artist Vernon Blake (1926) put it in his phenomenological treatise on art: “By means of words one cannot hope to attain to precise transmission of ‘plastic’ thoughts. The plastic arts exist because they are the natural and only way to transmit that species of thought from one human being to another. They necessarily deal in thought factors which are inexpressible in words.” 
            We observe a parallel in digital textual scholarship, which is the field’s preoccupation with the digital mimetic reproduction of printed text. This is the prevalent paradigm (comprehensively described in e.g. Pierazzo, 2013), and most digital scholarly editions that have been produced follow it meticulously. This mimetic philosophy has historically been expounded through the socio-technical system known as TEI-XML and its supporting community.  The vocabulary and syntax of the TEI is geared exclusively towards the structural, documentary, and content description of non-digital texts. Its philosophy and technology confine text to an existence as a book or as a digital reproduction thereof; itsvocabulary and syntax treat textuality only by means of the non-digital idiom of print, manuscript, score, script et cetera. The application of this idiom consequently leads to the reification of text as that which was contained in a document.
            Just as it is an illusion to think that consciousness can be explained entirely by means of linguistics, so it is an illusion to think that text and a full understanding of it can be achieved by digital means that reduce the text to a print paradigm. Barring some eccentric exceptions there are, to our knowledge, no scholarly editors that hold that only what is on the page is the text, that the text exists solely as a semiotic representation. It is in reading that the text becomes to exist. Umberto Eco (1981) regarded a written text as a series of instructions for the reader to create meaning. The meaning of a text is a co-creation that emerges from the interaction between the text and the situated knowledge and embodied cognition of the reader. The uniqueness of this interaction—that stems from two uniquely situated sites of knowledge, namely the text and the reader, mingling—gives rise to the indeterminacy Jerome McGann (1991) calls the textual condition.
            Like consciousness cannot be reduced to linguistics, textuality cannot be reduced to documentary description. We may never be able to experience the uniqueness of another reader’s mind. Consequently we may never be able to fully explain what text is. But the salient point is that we can apply digital technology to understand text beyond its being as a sign on the page. We contend that digital technologies can (and should) be used to explore the properties of these different beings of text. Analogous to what Alan Kay (1997, 2007) said about the computer revolution, we may perhaps say that the digital text has not happened yet.  The medium which differentiates the digital from linear, non-interactive text is still in the process of being imagined.
            So we textual scholars must become more imaginative, and ask ourselves: is there anything it is like to be a text? For us this question serves to displace an understanding of digital text as an inanimate series of electronic signs with an understanding of text as a more dynamic and interactive agent. Being a text in a digital environment involves formal complexities and computational procedural elements that are alien to print or manuscript text. Furthermore, being digital allows a text to transgress the boundaries of the document and to connect to other texts and information in ways that reify connections usually only present in human cognition. Lastly there is an element of code to the existence of text in the digital environment. Software code is text’s animated computational twin. Text will often interact with code, to the point of symbiosis. Our use cases demonstrate these properties of digital text.
            Our first use case focuses on Codex (Neill and Kuczera, 2019), a system for combining text annotation with graph database networks. A standoff property text editor is a central tool allowing users to create multiple overlapping annotations (avoiding ‘OHCO impedance’). The graph network can be constructed out of entities derived from within texts as well as managed independently of the texts themselves. The aspiration of Codex is not so much the reproduction of a manuscript within a document (like a single cell), but the emergence of a multi-cellular network of texts linked by the connective tissue of the graph database. The goal of Codex is to enable scholars to create a kind of ‘meta-text’ formed from out of the relations existing within texts across the corpus.
            Our second use case focuses on the creation of a digital critical edition as a computational model of the text and its witnesses, where a documentary model of edition was considered insufficient to represent not only the individual texts, but the relationship between the text witnesses themselves as well as to the information the text carries. The process of transcription, transformation to valid TEI-XML, collation of the chosen layers of witness text, and analysis of the variation is done with available tools, such as tpen2tei (Andrews, Veigl, and Kaufmann, 2018) and CollateX (Dekker and Middell, 2011), that are as general-purpose as possible. Even so, the editors had to write custom code ‘plugins’ in order to facilitate this process, for example to specify a normalization string for collation, to handle the expansion of abbreviations, or to convert certain elements of TEI markup to HTML display. We consider that this custom software code, insofar as it carries an editor’s interpretation of the text and the significance of its parts, is itself part of the edition that is produced, and thus part of the edited text.
            Our third use case considers the object-oriented modeling of a text using a general-purpose computer language. The objective in this case is to demonstrate that adequate transcription as the scholarly community has performed it up to now does not necessarily encompass adequate re-mediation of that text. Computer languages facilitate the meticulous modeling not only of the linguistic layer of a text, but also of many dimensions of the text beyond that. We may actually program a digital scholarly edition so that, when the code is run, the result is the scholarly edition. But we may also model the objects, events, and relations that exist in the narrative world of the text, in which case running the code becomes a re-enactment of the text’s narrative. In this way digital technology calls into question the limits of transcription and how digital transcription may add to understanding in novel ways.
            These use cases show that to imagine what it is like to be a digital text invites us to reconceive of text in a specifically digital fashion that points to affordances we fail to see and use were we to regard digital text as a mere mimesis of print and manuscript texts. This, we contend, adds to our understanding of what being a text is, and ultimately to understanding what text is.
        
        
            
                
                    Bibliography
                    
                        Andrews, T. L., Veigl, C., and Kaufmann, S. (2018). 
                        tpen2tei. Python, Vienna: DHUniWien. https://github.com/DHUniWien/tpen2tei (accessed 27 November 2018).
                    
                    
                        Blake, V. (1971). 
                        The Art And Craft Of Drawing: A Study (First published 1926). New York: Hacker Art Books.
                    
                    
                        Briet, S. (1951). 
                        Qu’est-ce que la Documentation? Paris: Édit.
                    
                    
                        Eco, U. (1981). The Theory of Signs and the Role of the Reader. 
                        The Bulletin of the Midwest Modern Language Association, 
                        14(1): 35–45. 10.2307/1314865.
                    
                    
                        Hacker, P. M. S. (2002). Is There Anything It Is like to Be a Bat? 
                        Philosophy, 
                        77(300): 157–74. http://www.jstor.org/stable/3752108 (accessed 26 November 2018).
                    
                    
                        Haentjens Dekker, R., and Middell, G. (2011). Computer-supported collation with CollateX: Managing Textual Variance in an Environment with Varying Requirements. In 
                        Supporting Digital Humanities 2011. Presented at the Supporting Digital Humanities 2011, Kopenhagen (DE).
                    
                    
                        Kay, A. C. (2007). The Real Computer Revolution Hasn’t Happened Yet. Viewpoints Research Institute. http://www.vpri.org/pdf/m2007007a_revolution.pdf (accessed 27 November 2018).
                    
                    
                        Kay, 
                        A.
                         C. (1997). The Computer Revolution Hasn’t Happened Yet. Presented at the 12th annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA 1997), Atlanta: ACM SIGPLAN. https://moryton.blogspot.com/2007/12/computer-revolution-hasnt-happened-yet.html (accessed 27 November 2018).
                    
                    
                        Koch, C., Massimini, M., Boly, M., and Tononi, G. (2016). Neural correlates of consciousness: progress and problems. 
                        Nature Reviews Neuroscience, 
                        17: 307–321. 10.1038/nrn.2016.22.
                    
                    
                        McGann, J. (1991). 
                        The Textual Condition. Princeton: Princeton University Press.
                    
                    
                        Nagel, T. (1974). What Is It Like to Be a Bat? 
                        The Philosophical Review, 
                        83(4): 435–50. 10.2307/2183914.
                    
                    
                        Neil, I., and Kuczera, A. (2019). The Codex.net — An Atlas of History. 
                        Die Modellierung des Zweifels – Schlüsselideen und -konzepte zur graphbasierten Modellierung von Unsicherheiten, 
                        Sonderbände der ZfdG 4: (forthcoming).
                    
                    
                        Pierazzo, E. (2015). 
                        Digital Scholarly Editing: Theories, Models and Methods. Farnham: ashgate Publishing limited.
                    
                    
                        Shapiro, L. A. (2012). Embodied Cognition. In Margolis, E., Samuels, R., and Stich, S. P. (eds.), 
                        The Oxford Handbook of Philosophy of Cognitive Science. Oxford, New York: Oxford University Press, pp. 118–46. http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780195309799.001.0001/oxfordhb-9780195309799-e-6 (accessed 26 November 2018).
                    
                    
                        Tononi, G. (2012). 
                        Phi, A Voyage from the Brain to the Soul. New York: Patheon Books.
                    
                    
                        Van Zundert, J. J., and Andrews, T. L. (2017). Qu’est-ce qu’un texte numérique? 
                        Digital Scholarship in the Humanities, 
                        32(Suppl_2): ii89–105. 10.1093/llc/fqx039.
                    
                
            
        
    
10121	2019	
        
            Since October 2018, version 1.0rc of a 
                digital genetic edition of Goethe’s 
                
                    Faust
                
                 is publicly accessible online (Goethe 2018). The poster visualizes and explains the edition’s most specific features and discusses recent research about modelling macrogenesis as a graph.
            
            The 
                archive is the edition’s heart piece with detailed multi-view representations of about 550 manuscripts spanning a period from the early 1770s until 1832. Additionally, it contains printed editions and testimonies about the genesis of Faust (references found in diaries, letters etc.). For each of the manuscripts there are the following views, some of which are combined on screen:
            
            
                visualization of the quire structure (collation) and manuscript description (fig. 1)
                    plus, see fig. 2:
                
                high-resolution digital facsimiles 
                diplomatic transcription overlay
                separate diplomatic rendering of inscriptional phenomena
                textual rendering for a more interpretive account of alterations
            
            
                
                    
                    
                        2 H, collation visualisation (center) with preview images (left) and textual manuscript description (right); extract.
                    
                
            
            
                
                    
                    
                        2 H, verse line 11579f. Views: 
                        digital facsimile – text image overlay – 
                        separate diplomatic transcript – diplomatic rendering with tooltip information about inscriptional phenomena – 
                        textual rendering with tooltip information about alterations
                    
                
            
            Additionally, each manuscript’s final textual stage is rendered in way that closely corresponds to the rendering of the critically established text of the work (see below). All but the diplomatic views are also available for the printed editions.
            The edition’s 
                genesis section comes with navigable graphic visualizations of how witnesses relate to each other. Bar diagrams (see fig. 3) indicate the textual content of each witness by length of and gaps within bars. Their order is based on a global chronological ordering expressed in a directed acyclic graph (see below for details about macrogenesis).
            
            
                
                    
                    
                        Bar diagram for a scene of “Faust II”. Witnesses are chronologically ordered (top-down). Ochre colored bars do not contain elaborate versions, but are semantically connectable to passages of the final text, the canonic numbering of serves as x-axis.
                    
                
            
            The edition’s 
                text was critically established on the basis of the complete textual transmission. By clicking on a line of text all other versions appear synoptically (fig. 4), showing its development within and across all other witnesses in graph based chronological order (see below). All sigils are links that lead to the corresponding passage of the respective witness in the archive (see above). In addition to the synopsis, there are critical apparatus entries that report about editorial decisions.
            
            
                
                    
                    
                        Synoptical view of the development of V. 11580. Line 11578 has additional apparatus entry
                    
                
            
            The views are generated from TEI-encoded source data, the poster will illustrate the process.
            The chronological (i.e. macrogenetic) order of witnesses is far from trivial: Only a few witnesses can be precisely dated. There are many relative chronologies that only span a few witnesses each. In an experimental part of the edition, all relevant chronological information has been extracted from scholarly literature (most notably Goethe, 1888; Fischer-Lamberg, 1955; Bohnenkamp, 1994) and recorded in a simple data model that basically recognizes relative and absolute datings, together with the bibliographic source and optional comments. This information has been modeled as a directed graph, as illustrated in fig. 5: E.g., the path 1825-05-25 → 2 III H.5 → 1825-04-06 means that 2 III H.5 has been written not before February 26th and not after April 4th, 1825. From this graph, additional information can be inferred, e.g., limits for the absolute dating of 2 III H.8.
            
                
                    
                    Subgraph around 2 III H.8
                
            
            Contradictions in model or sources lead to cycles in the graph (fig. 6). In the recorded information, there is one strongly connected component (in which every node is reachable from every other node) composed of 477 witnesses and 2136 assertions, too many to check manually. Identifying and removing all contradictions with the smallest number of interventions means solving the NP-complete (Karp, 1972) 
                minimum feedback arc set problem, for which we can calculate an exact solution using a method by Baharev et al. (2015). The procedure can be manually influenced by placing weights on sources or individual edges or by marking obviously wrong edges as to ignore first.
            
            
                
                    
                    All three dashed-red edges must be removed to make this component acyclic
                
            
            From the cycle-free results, a topological ordering of the witnesses can be created (Manber, 1998:199) that is then used everywhere the edition requires a chronological ordering. Subgraph visualisations created using NetworkX (Hagberg et al., 2008) and GraphViz (Gansner/North, 2000) help to explain the ordering.
        
        
            
                
                    Bibliography
                    
                        Baharev, A., Schichl, H. and Neumaier, A. (2015). An exact method for the minimum feedback arc set problem. 
                         
                        .
                    
                    
                        Bohnenkamp, A. (1994)
                        . “... das Hauptgeschäft nicht außer Augen lassend”. Die Paralipomena zu Goethes “Faust”. Frankfurt am Main / Leipzig: Insel.
                    
                    
                        Fischer-Lamberg, R. (1955): Untersuchungen zur Chronologie von Faust II 2 und 3. Diss. phil. (masch.), Humboldt-Universität Berlin.
                    
                    
                        Gansner, E. R. and North, S. C. (2000). An open graph visualization system and its applications to software engineering. 
                        Software - Practice and Experience, 
                        30(11): 1203–1233.
                    
                    
                        Goethe, J. W. (1888). Goethes Werke, edited under the auspices of the Grand Duchess Sophie of Saxe-Weimar-Eisenach. Vol. 15, 
                        Faust. Zweiter Theil, edited by Erich Schmidt. Weimar: Hermann Böhlau.
                    
                    
                        Goethe, J. W. (2018). 
                        Faust. Historisch-Kritische Edition. (Ed.) Bohnenkamp, A., Henke, S. &amp; Jannidis, F. Version 1.1 RC. Frankfurt am Main ; Weimar ; Würzburg
                         
                         (accessed 24 January 2019).
                    
                    
                        Hagberg, A. A., Schult, D. A. and Swart, P. J. (2008). Exploring Network Structure, Dynamics, and Function using NetworkX. In Varoquaux, G., Vaught, T. and Millman, J. (eds), 
                        Proceedings of the 7th Python in Science Conference. Pasadena, CA USA, pp. 11–15.
                    
                    
                        Manber, U. (1989). 
                        Introduction to Algorithms: A Creative Approach. Vol. 4. Addison-Wesley Reading, MA
                    
                
            
        
    
10133	2019	
        
             This study applies computer vision techniques to examine the representation of gender in historical advertisements. Using information on the relative size, position, and gaze of men and women in thousands of images, we chart gender displays in Dutch newspaper adverts between 1920 and 1990.
            In the 1925 edition of 
                Psychology in Advertising psychologist Alfred Poffenberger encouraged ad makers to ‘short-circuit the consumer's mind through vivid, pictorial appeals to fundamental emotions’ (Marchand, 1985). The images and visual clichés tap into a representational system that produces meaning outside the realm of the advertised product (Goldman, 1992). In the late 1970s, sociologist Erving Goffman examined the semiotic content in advertisements printed in contemporary newspapers and glossy magazines. He noted that displays of gender are often ‘conveyed and perceived as if they were somehow natural, deriving, like temperature and pulse, from the way people are and needful, therefore, of no social or historical analysis’ (Goffman, 1976). Of course, almost the exact opposite is true. Goffman conceived of five different categories to study the depictions of, and the relations between men, women, and children in advertisements: relative size, feminine touch, function ranking, ritualization of subordination, and licensed withdrawal. Drawing from this approach, Jonathan Schroeder, contends that in advertisements the male regularly represents the ‘the active subject, the business-like, self-assured decision maker, while the female occupies the passive object, the observed sexual/sensual body, eroticized and inactive’ (Schroeder, 2004).
            
            Studies like that of Goffman and Schroeder relied on a limited number of advertisements, and they also do not take the historicity of ads into account. For example, Goffman looked at around 400 different advertisements, which he more-or-less randomly selected from ‘newspapers and magazines easy to hand - at least to my hand.’ As Kang (1997) notes, he was often criticized for this method. Instead of relying on a random sample, he purposefully ‘selected images that mirrored gender differences.’ Goffman emphasized that he choose his categories partly on the basis of the fact that he could find almost no images that disproved his categories. For example, images in which women were relatively larger and had the executive role.
            In an earlier project, we developed methods to extract visual material from historical newspapers (Wevers and Smits, 2019). Using a large-scale data set of digitized historical advertisements extracted these methods, we examine continuity and change in displays of gender using computational means. By applying state-of-the-art computer vision methods to a diachronic set of digitized Dutch newspapers advertisements published between 1920-1990, we can test existing hypotheses and contribute to existing replication studies about gender displays in advertisements (Kang, 1997; Belknap, 1991; Lidner, 2004).
                
                    The data is kindly provided by the National Library of the Netherlands.
                 We will start our research after the First World War, because Dutch advertising practice changed rapidly in the Interbellum. Technological advances made it cheaper to print images in newspapers. Influenced by American ad agencies, Dutch ad makers saw the potential of using visual material to increase sales and to convey particular brand identities (Schreurs, 2001).
            
            In this short paper, we operationalize Erving Goffman's theory on gender displays in two ways. First, Goffman argues that ‘differences in size will correlate with differences in social weight’ (Kang, 1997). Using facial recognition software (Geitney, 2018; Zhang, 2016), we select adverts that include people, and then train a gender detection algorithm using a convolutional neural network to estimate whether men or woman were represented in the images (Levi, 2015). This allows us to visually represent the changing faces of men and women in advertisements. Part of the process also entails a reflection on the inherent bias in these algorithms.
            Second, Goffman contends that body postures and the engagement in social settings show that men are often portrayed in ‘executive roles’ and that women are often depicted as being removed from the social situation gazing off into the distance. Using information extracted in the first step, we quantify whether how people were represented on image, e.g. how was position where on the image? In Figure 1, for example, we see a social gathering of four people with the central figure drinking a bottle of Coca-Cola. Using the described computer vision techniques, we can extract faces and determining where on the images these are located. This reveals a man as a central figure with two women and man surrounding the central man. By extracting these features from a large set of advertisements and applying clustering methods, we can detect patterns in advertisements over time that can be compared to findings by Goffman and others. (Impett, 2017).
            
                
                Coca-Cola advertisements. 
                    Nieuwsblad van het Noorden, August 14, 1959.
                
            
            We argue that it is not only possible to test existing theories on displays of gender in a more extensive database, but also to shed light on their historicity: the fact that these displays seem natural and constant, but can, and will, change, sometimes rapidly, over time. In line with the conference theme of complexity, we are aware that it is problematic to reduce displays of gender to binary categories. In this study, we are operationalizing displays of gender to a limited set of features, thereby reducing the complexity of gender. However, the coarse-graining of displays of gender into features that are comprehensible and computable allows us to extract trends from collections of advertisements. These trends offer context to particularities and can reveal the use of visual clichés in advertising discourse. Moreover, in the paper, we evaluate and reflect on the use and performance of models trained on contemporary training data as well as relying on probabilities to define gender.
        
        
            
                
                    Bibliography
                    
                        Belknap, P. and Leonard, W.M. 
                        (1991
                        ). 
                        A Conceptual Replication and Extension of Erving Goffman’s Study of Gender Advertisements. 
                        Sex Roles
                        , 25, 3: 103–118.
                    
                    
                        Geitney, A.
                         (2018). The world's simplest facial recognition api for Python and the command line. https://github.com/ageitgey/face_recognition (accessed 27 april, 2019). 
                    
                    
                        Goffman, E.
                         (1976). 
                        Gender Advertisements
                        . New York: Harper and Row.
                    
                    
                        Goldman, R.
                         (1992). 
                        Reading ads socially
                        . London: Routledge.
                    
                    
                        Impett, L and Moretti, F.
                         (2017). Totentanz. Operationalizing Aby Warburgs Pathosformeln. 
                        Technical report Stanford Literary Lab
                        .
                    
                    
                        Kang, M.E.
                         (1997). “The portrayal of women’s images in magazine advertisements: Goffmans gender analysis revisited”. 
                        Sex roles
                         37, 11-12: 979.
                    
                    
                        Levi, G. and Hassner. T.
                         (2015) “Age and gender classification using convolutional neural networks”. 
                        Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops
                        : 34–42.
                    
                    
                        Lindner, K. (2004).
                         “Images of Women in General Interest and Fashion Magazine Advertisements from 1955 to 2002”. 
                        Sex Roles
                        , 51, 7: 409–421.
                    
                    
                        Marchand, R.
                         (1985). 
                        Advertising the American dream: making way for modernity, 1920-1940
                        . Berkeley: University of California Press.
                    
                    
                        Schreurs, W.
                         (2001). 
                        Geschiedenis van de reclame in Nederland
                        . Utrecht: Het Spectrum.
                    
                    
                        Schroeder, J.E. and Zwick, D.
                         (2004). “Mirrors of Masculinity: Representation and Identity in Advertising Images”. 
                        Consumption Markets &amp; Culture
                        , 7, 1: 21-52.
                    
                    
                        Wevers, M., &amp; Smits, T.
                         (2019). The visual digital turn: Using neural networks to study historical images. 
                        Digital Scholarship in the Humanities
                        .
                    
                    
                        Zhang, K., Zhang, Z., Li, Z., &amp; Qiao, Y.
                         (2016). Joint face detection and alignment using multitask cascaded convolutional networks. 
                        IEEE Signal Processing Letters
                        , 
                        23
                        (10), 1499-1503.
                    
                
            
        
    
10143	2019	
        
            Lexicography is currently embracing rapid change as the traditional methods of publishing dictionaries are replaced by the ubiquity of lexical information on the Web. Furthermore, the application of computational techniques to the processes of lexicography is revolutionizing how dictionaries can be constructed. In this context, the recently established ELEXIS (European Lexicographic Infrastructure) project aims to develop an infrastructure for eLexicography across Europe, that builds a virtuous cycle of lexicography where lexicographic resources are linked across languages, in order to build improved natural language processing tools, which can then aid in the construction of novel resources and retro-digitization of dictionaries, thus driving the cycle. The project is multilingual, covering 15 European countries, and has a strong interest in driving lexicography for under-resourced and minoritized languages.
            The event will be the second iteration of a highly successful 
                workshop first run before the EADH Conference in Galway on 6 December 2018. On that occasion, the Workshop was the highest-subscribed of the ten pre-conference workshops, with 33 regular registrations and approximately fifty attendees from a range of backgrounds in lexicography, computer science, and the humanities.
            
            Specific emphasis in this edition of the workshop will be on complexity in regard of data, technologies and community aspects of lexicography. Complexity in data concerns issues in access to lexicographic data across stakeholders (national institutes, research groups, individuals), representation formats, linguistic assumptions, underlying theories and scope of analysis and representation, legal restrictions and licenses, multilingual, cross-lingual, comparative and typological issues, as well as advanced aspects in multimodality, concerning audio-visual representation. Complexity in technologies concerns challenges in creating and expanding novel lexicographic resources, which require a combination of many technologies, including natural language processing tools as well as machine learning approaches and AI methods in general, for data linking and data management, in order to identify and represent words, their senses and definitions. 
            Complexity in communities, in part, lies in differences related to stakeholders’ type and status (national language institutes, standardization bodies, research groups, individuals), status of the language in question (official, minority, regional, etc.), and involvement in networking activities. In addition, the complexity is related to several academic disciplines involved in eLexicographic research such as linguistics, natural language processing (NLP), digital humanities, artificial intelligence (AI), computational linguistics, computer science, etc. This constitutes a challenge to provide professionals with training opportunities and ensure knowledge exchange among all stakeholders.
            The workshop has three main aims: firstly, we will invite speakers from existing major lexicographic projects to give insights into the complexity in regard of data, technologies and community aspects of lexicography. Secondly, we will provide a hands-on tutorial with the ELEXIS infrastructure to enable participants to become familiar with the technologies being developed in the project. Finally, we will make an open call for posters, which will provide an overview of new projects in the area of electronic lexicography, with a special focus on papers that tackle the topic of complexity. The poster presenters will give a 5 minute lightning talk on their topic in addition to their contribution to the poster session.
            Topics of Interest
            
                Lexicography and Digital Humanities
                Complexity in Data for Lexicography 
                Complexity in Technologies for Lexicography
                Complexity across Lexicography and Related Communities
                Access and usage of dictionaries on the Web
                Retro-digitization of lexicographic resources
                Lexicography for language learning
                Use and applications of NLP for lexicography
                Lexicography for under-resourced languages
                Lexicography for terminology and translation
                Linked Data for lexical resources
                AI for Lexicography
            
            Summary of the Call
            We welcome submissions of abstracts of up to 500 words that will be presented as posters at the workshop. Submissions should present methodologies, experiments, use cases, descriptions of ongoing or planned research projects and position papers on topics related to the topics of interest (given above). Furthermore, we especially welcome papers describing interdisciplinary research combining research in lexicography, linguistics, computer science and digital humanities approaches and giving insights into complexity in regard of data, technologies and community aspects of lexicography. 
            Please submit abstracts by May 6th 2019 in any language (including an English translation for the title for reviewing purposes). Submissions will be reviewed by at least 3 reviewers and will be made available online prior to the workshop. Papers should be submitted via EasyChair. Notifications will be sent by end of May and final versions of abstracts will be required by end of June. More information on the workshop can be found on the 
                workshop website.
            
            Tentative Schedule
            
                
                    9:00-9:30
                    Invited talk: “Unity in Variety: Observation and Lexicographic Treatment of the German Standard Variety used in South Tyrol” Andrea Abel (Eurac Research, Italy)
                
                
                    9:30-10:00
                    Invited talk: 
                        "Framing in the Dutch Language: from structured data to text and back from text to structured data on situations” Piek Vossen (Vrije Universiteit Amsterdam, Netherlands)
                    
                
                
                    10:00-10:30
                    Introduction to ELEXIS infrastructure and technology
                
                
                    10:30-11:00
                    Coffee break
                
                
                    11:00-12:00
                    Lightning Talks
                
                
                    12:00-13:00
                    Poster session
                
            
            Organizing Committee
            
                John P. McCrae is a lecturer above-the-bar at the National University of Ireland Galway in the school of information technology. His work has focussed on the application of linked data to language resources. In particular, he is the original developer of the lemon-OntoLex model, which has become a de-facto standard for representing lexicons on the Web. In addition, he is a board member of the Global WordNet Association. He has also organized many events (e.g. Language Data and Knowledge Conferences, Linked Data in Linguistics Workshops Summer Datathons/Summer Schools on Linguistic Linked Open Data etc.)
                Address: Insight Centre for Data Analytics, National University of Ireland Galway, Ireland
                Email: john.mccrae@insight-centre.org
            
            
                Paul Buitelaar is a Senior Lecturer at the National University of Ireland Galway (NUIG), vice-director of the Insight Centre for Data Analytics at NUIG and head of the Insight Unit for Natural Language Processing. His main research interests are in the development and use of Natural Language Processing methods and solutions for semantic-based information access. He has been involved in a large number of national and international funded projects in this area. In recent years he was involved in the development of the Saffron framework for knowledge extraction and the definition and implementation of lemon, a vocabulary for Linguistic Linked Data.
                Address: Insight Centre for Data Analytics, National University of Ireland Galway, Ireland
                Email: paul.buitelaar@insight-centre.org
            
            
                Toma Tasovac is Director of the Belgrade Center for Digital Humanities (BCDH) and Director of the Digital Research Infrastructure for the Arts and Humanities (DARIAH). His areas of interest include lexicography, data modeling, TEI, digital editions and research infrastructures. Toma was previously a Steering Group member of the European Network for eLexicography (ENeL), and is currently also affiliated with the European Lexicographic Infrastructure (ELEXIS).
                Address: Belgrade Center for Digital Humanities, Belgrade, Serbia
                Email: ttasovac@humanistika.org
            
            
                Justin Tonra is Lecturer in English (Digital Humanities) at the National University of Ireland Galway. His areas of research interest include digital approaches to literary studies, book history, textual studies and bibliography, scholarly editing, and literature of the Romantic period. He is currently joint National Coordinator for DARIAH Ireland, and a working-group leader for COST Action CA16204 Distant Reading for European Literary History.
                Address: English Department, National University of Ireland Galway, Ireland
                Email: justin.tonra@nuigalway.ie
            
            
                Tanja Wissik is a senior researcher at the Austrian Centre for Digital Humanities (ACDH) of the Austrian Academy of Sciences and teaches at the University of Graz. She holds a PhD from the University of Vienna in translation studies with a specialization in the field of terminology and corpus linguistics. She has been working in numerous research projects related to language resources and language technologies and is involved in outreach and network activities.
                Address: Austrian Academy of Sciences, Austria
                Email: Tanja.Wissik@oeaw.ac.at
            
            
                Ksenia Zaytseva is data analyst at the Austrian Centre for Digital Humanities (ACDH) of the Austrian Academy of Sciences. She is primarily involved in development of tools and services for DH projects in archaeological and linguistic domains. Her main research interests are Semantic Web technologies, Linked (Open) Data, controlled vocabularies and reference data services. She is also interested in scientific Python programming, machine learning and web application development. 
                Address: Austrian Academy of Sciences, Austria
                Email: ksenia.zaytseva@oeaw.ac.at
            
            The workshop is supported by the two EU projects 
                ELEXIS (European Lexicographic Infrastructure) and 
                Prêt-à-LLOD (Multilingual Linguistic Linked Data), and 
                DARIAH-IE.
            
            Programme Committee
            
                Fahad Khan (ILC-CNR, Pisa)
                Monica Monachini (ILC-CNR, Pisa)*
                Rute Costa (New University of Lisbon)
                Francesca Frontini (University Paul Valéry, Montpellier)
                Andrea Bellandi (ILC-CNR, Pisa)
                Christophe Roche (University of Savoie)
                Bolette Sandford Pedersen (University of Copenhagen)
                Christiane Fellbaum (Princeton University)*
                Philipp Cimiano (Bielefeld University)
                Simon Krek (Josef Stefan Institute)
                Vera Hildenbrandt (Trier Center for Digital Humanities)
                Karlheinz Mörth (Austrian Academy of Sciences)*
                Thierry Declerck (DFKI)
                Katrien Depuydt (Institute of Dutch Language)*
                Christian Chiarcos (Goethe-University Frankfurt)
                Monika Rind-Pawlowski (Goethe-University Frankfurt)*
                Alexander Geyken (Berlin-Brandenburg Academy of Sciences)*
            
            * Awaiting confirmation
        
    
10453	2016	
      
         Annotation natürlicher Sprachdaten aus sozialen Medien zur
        Erforschung zeitgenössischer Szenen, zur Sprach- und Trendanalyse und zur
        Weiterentwicklung von Sprachtechnologien gewinnt mit der zunehmenden Verfügbarkeit
        großer Datenbestände weiter an Bedeutung (Farzindar / Inkpen 2015). Zeitgenössische
        Kommunikation in sozialen Medien verfügt über inhaltliche und strukturelle
        Besonderheiten und ist von umgangssprachlicher Ausdrucksform geprägt. Beiträge, die
        im Kontext internetbasierter Diskussionskulturen in Foren entstehen, stellen eine
        wichtige Forschungsquelle dar. Diese nutzergenerierten Texte, in Form von semi- oder
        unstrukturierten Kommentaren, repräsentieren Meinungen und Bewertungen einer
        Gemeinschaft zu einem Thema, Produkt oder Werk und beziehen sich in der Regel auf
        inhaltliche, technische oder ästhetische Aspekte. Die Autoren verwenden dabei
        Sprachmittel wie Metaphern, Analogien, Ambiguität, Humor und Ironie sowie
        metalinguistische bildhafte Mittel wie Emoticons oder andere graphische Zeichen
        (Reyes et al. 2012).
         Vor diesem Hintergrund adressiert dieses Projekt Herausforderungen, die bei der linguistischen und statistischen Verarbeitung von realen web-basierten Daten entstehen. Es wird ein Ansatz semi-automatischer Annotation zur Extraktion von Begriffen für die ontologiebasierte Beschreibung von computergenerierten audiovisuellen Kunstwerken einer digitalen Kunstszene präsentiert. Forschungsgegenstand ist die Diskussionskultur der Demoszene, einer spezialisierten Computerkunstszene. Bisher sind die zahlreichen Beiträge der Gemeinschaft, die sich auf ästhetische und technische Aspekte der Kunstwerke beziehen, nicht erschlossen. Bei diesen Beiträgen handelt es sich um informelle, emotionale, kurze und unstrukturierte Kommentartexte. Das verwendete Vokabular ist mehrsprachig und beinhaltet fachspezifische Terminologien, exklusive Neologismen und einen eigenen szenespezifischen orthographischen Stil. Diese Beiträge bieten detaillierte Einblicke in die Charakteristika der Werke, weshalb ihre Erschließung deren Verständnis fördert und eine gezielte Recherche einzelner Werke ermöglicht. Das Projekt befasst sich mit der Fragestellung, in wieweit sich aktuelle Verfahren der natürlichen Sprachverarbeitung (NLP), die auf grammatikalisch korrekte Schriftformen optimiert und auf Zeitungskorpora trainiert sind, anwenden lassen. Somit leistet das präsentierte Projekt einen Beitrag im Bereich der Entwicklung von Ansätzen zur Aufbereitung großer textbasierter Datenbestände sowie der Erforschung des Sprachgebrauchs zeitgenössischer digitaler Kunstszenen, aber auch hinsichtlich Nutzung semantischer Technologien.
         Die Anwendung von NLP-Verfahren für textbasierte Kommunikation
          in soziale Medien bedarf einiger Anpassungen an die sprachlichen Besonderheiten
          (Maynard 2012). Die Nutzung standardisierter Techniken ist bisher nur wenig
          erfolgversprechend (Gimpel 2011; Finin 2010). Bestehende Frameworks, wie das Natural
          Language Toolkit (NLTK, vgl. Bird et al. 2015), bieten die Möglichkeit der
          Implementierung eines individuellen NLP-Prozesses, bei dem verschiedene
          Verarbeitungsschritte modular integriert und miteinander kombiniert werden können.
          Für das vorliegende Projekt wurde eine Pipeline konzipiert und implementiert, die
          die Generierung von Annotationsebenen, begonnen mit der Tokenisierung und
          Part-of-Speech Tagging bis hin zur Extraktion von relevanten werkbeschreibenden
          Begriffen umfasst. Zur Evaluation des entwickelten Ansatzes wird ein regelbasiertes
          überwachtes Experiment mit einer definierten Teilmenge von 1255 Kommentaren
          durchgeführt. Es lässt sich feststellen, dass Emoticons und Partikeln falsch
          verarbeitet werden. Darüber hinaus werden auch Nomen, Verben und Adjektive,
          insbesondere Gerundien häufig falsch annotiert. Das Experiment zeigt, dass die
          konzipierte Pipeline für das vorliegende Kommentarkorpus iterativ optimiert werden
          muss. Der generierte Index werkbeschreibender Terminologie wird ferner für die
          Erweiterung einer domainspezifischen Ontologie zur Unterstützung semantischer
          Annotation verwendet. Hierfür wird ein Ansatz für das Lernen von Ontologien aus
          Texten verfolgt, wobei die ermittelten Begriffe als Kandidaten für Instanzen
          beschrieben werden. Als Referenzontologie wird eine auf CIDOC CRM-basierte Adaption
          verwendet (Hastik et al. 2013).
         Dieses Projekt präsentiert einen innovativen Ansatz, um mit NLTK Kommentartexte aus Onlineforen der Demoszene zu annotieren. Das Standard-Tagset muss jedoch angepasst werden. Die Erweiterung der CIDOC CRM-basierten Ontologie auf Basis des generierten Indexes ermöglicht die semantische Beschreibung der Werke.
      
      
         
            
               Bibliographie
               
                  Bird, Steven / Klein, Ewan / Loper, Edward (2015):
              Natural Language Processing with Python. NLTK
              Book http://www.nltk.org/book/
              [letzter Zugriff 15. Februar 2016].
               
                  Farzindar, Atefeh / Inkpen, Diana (2015): Natural Language Processing for Social Media. San
              Francisco: Morgan & Claypool.
               
                  Finin, Tim / Murnane, Will / Karandikar, Anand / Keller,
                Nicholas / Martineau, Justin (2010): "Annotating Named Entities in
                Twitter Data with Crowdsourcing", in: Proceedings of the
                NAACL HLT 80–88. 
               
                  Gimpel, Kevin / Schneider, Nathan / O'Connor, Brendan /
                  Dipanjan, Das / Mills, Daniel / Eisenstein, Jacob / Heilman, Michael /
                  Yogatama, Dani / Flanigan, Jeffrey / Smith, Noah A. (2011):
                  "Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments",
                  in: Proceedings of the 49th Annual Meeting of the
                  Association for Computational Linguistics 42-47. 
               
                  Hastik, Canan / Steinmetz, Arnd / Thull, Bernhard
                  (2013): "Ontology based Framework for Real-Time Audiovisual Art", in: IFLA World Library and Information Congress. 79th
                  IFLA General Conference and Assembly: Audiovisual and Multimedia with
                  Cataloguing http://library.ifla.org/87/1/124-hastik-en.pdf [letzter Zugriff
                  15. Februar 2016]. 
               
                  Maynard, Diana / Bontcheva, Kalina / Rout, Dominic
                  (2012): "Challenges in Developing Opinion Mining Tools for Social Media",
                  in: Proceedings of @NLP can u tag #usergeneratedcontent?!
                  Workshop at International Conference on Language Resources and
                  Evaluation (LREC 2012) 8.
               
                  Reyes, Antonio / Rosso, Paolo / Buscaldi, Davide
                  (2012): "From Humor Recognition to Irony Detection: The Figurative Language
                  of Social Media", in: Data Knowledge Engineering.
                  Applications of Natural Language to Information Systems 74: 1-12. 
            
         
      
   


10467	2016	
      
         Dieses Poster soll den DARIAH-DKPro-Wrapper vorstellen, der aus einer Kooperation zwischen dem Lehrstuhl für Computerphilologie der Universität Würzburg und dem Ubiquituous Knowledge Processing Lab der TU Darmstadt im Rahmen von DARIAH-DE entstanden ist. 
         DKPro integriert zahlreiche (unabhängig entstandene) Softwarekomponenten zum Natural Language Processing (NLP) und ermöglicht so dem Nutzer die Anwendung typischer NLP-Aufgaben wie Tokenisierung, Part-of-Speech-Tagging, Named Entity Recognition oder Dependency Parsing mit State-of-the-Art Werkzeugen. Es basiert auf dem Framework UIMA. Für Nutzer, die nicht aus dem Umfeld der Informatik oder Computerlinguistik kommen, ist die Schwelle zur Verwendung allerdings recht hoch: das komplexe Framework muss in Java angesprochen werden.
         Um diese Hürde zu senken und einer größeren Zahl auch von weniger technisch
        versierten Nutzern die Verwendung zu ermöglichen, wurde der DARIAH-DKPro-Wrapper
        entwickelt. Dieser ermöglicht es, eine Pipeline mit mehreren Komponenten über die
        Kommandozeile auszuführen und damit auch längere Textdokumente und Textsammlungen zu
        verarbeiten. Zudem können eine ganze Reihe von Einstellungen bequem und individuell
        über Konfigurationsdateien vorgenommen werden: über die Auswahl der Sprache bis hin
        zur Aktivierung und Deaktivierung einzelner Komponenten und der Auswahl bestimmter
        Komponenten oder Modelle. Auf diese Weise kann jeder Nutzer vorgefertigte Pipelines
        verwenden oder eine auf seine Bedürfnisse zugeschnittene Pipeline individuell
        zusammenstellen. Der Wrapper ist stets aktuell über GitHub (https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper) verfügbar, ebenso wie
        die dazugehörige Dokumentation des DARIAH-DKPro-Wrapper v0.4.3 (2016). 
         Um die anschließende Weiterverarbeitung derart prozessierter Dokumente ebenfalls zu vereinfachen, wurde ein entsprechendes Ausgabeformat entwickelt. Dieses lehnt sich an das CoNLL2009-Format
        an und stellt die Ergebnisse der Pipeline in tabellarischer Form dar. Dabei befindet sich in jeder Zeile ein Token, während die dazugehörigen Informationen wie Lemma, POS-Tag und ähnliches je in einer Spalte stehen. Dadurch werden alle durch Komponenten der Pipeline ermittelten Informationen in einer Datei zusammengefasst. Dieses Format hat den Vorteil, dass es für menschliche Nutzer übersichtlich und gut lesbar ist. Zudem ist es als Tabstopp-getrennte Datei auch für gängige Skriptsprachen wie Python oder R, sowie Tabellenkalkulationsprogramme wie Excel leicht zugänglich.
      
         Um die Verwendung des Wrappers und die Weiterarbeit mit dem Ausgabeformat zusätzlich zur Dokumentation anschaulich zu beschreiben, wurden außerdem eine Reihe von Tutorials zu Beispielanwendungen aus Bereichen der digitalen Literaturwissenschaft, wie zum Beispiel der Stilometrie oder dem Topic Modeling, verfasst. Die Dokumentation sowie die Tutorials sind ebenfalls auf GitHub zu finden.
         Das Poster wird all diese Punkte in übersichtlicher Form zusammenführen und potentiellen Nutzern präsentieren. Dabei werden die Funktionsweise der Pipeline, die Arbeit mit den Konfigurationsdateien, der Aufbau und die Verwendung des Ausgabeformats sowie Anwendungsbeispiele im Mittelpunkt stehen.
      
      
         
            
               Bibliographie
               
                  Dokumentation: DARIAH-DKPro-Wrapper v0.4.3 (2016): User guide DARIAH-DKPro-Wrapper v0.4.3 DARIAH2 -
            Cluster 5, Use Case 1 Team. Universität Würzburg, TU Darmstadt - DARIAH-DE
            https://rawgit.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/user-guide.html
              [letzter Zugriff 08. Januar 2016].
               
                  CoNLL-2009 Format (2008-*): CoNLL-2009 Shared Task. Syntactic and Semantic Dependencies in
              Multiple Languages. Institute of Formal and Applied Linguistics, Charles
              University in Prague, Czech Republic, Faculty of Mathematics and Physics
              https://ufal.mff.cuni.cz/conll2009-st/task-description.html
                [letzter Zugriff 08. Januar 2016]. 
            
         
      
   


10516	2016	
      
         Zu den wichtigsten Arbeitsinstrumenten der Digital Humanities gehören die algorithmische Verarbeitung von Daten, die statistische Modellierungen von Zusammenhängen in Daten und visuelle Analysemethoden, um Daten verstehen zu können. Diese Instrumente folgen bestimmten Routinen wissenschaftlicher Praxis: Sie verwenden erprobte Algorithmen und halten sich an Standards der Datenmodellierung. Gleichzeitig konstituieren sie aber auch die wissenschaftliche Praxis mit – sie sind, um mit Ludwick Fleck zu sprechen, Mittel zur Profilierung eines Denkstils innerhalb eines wissenschaftlichen Denkkollektivs. Dazu gehören nicht nur die erwähnten Arbeitsinstrumente, sondern auch sprachliche Mittel: Fachbegriffe, Metaphern, Stile.
         Um die Verknüpfung von Arbeitsinstrumenten und wissenschaftlichen Routinen genauer zu verstehen, müssen die Praktiken und Kulturen, in denen diese Instrumente erstellt werden, reflektiert werden. Welchen Einfluss hat die Wahl einer bestimmten Programmiersprache zur Implementierung eines Algorithmus auf die Digital-Humanities-Praxis? Beispielsweise die Verwendung des „postmodernen“ Perl (Wall 1999) statt Python? Welchen Einfluss hat die Programmiersprache auf die Art der algorithmisch erstellen Visualisierung? Beispielsweise die Verwendung von D3.js, P5.js, R oder der Software Excel (Bubenhofer 2015)? Welchen wissenschaftlichen Paradigmen entspringen populäre statistische Modellierungen? Beispielsweise Topic Modelling oder Support Vector Machines? Und mit welchen sprachlichen Mitteln werden die angewandten Instrumente in den wissenschaftlichen Diskurs eingebracht und legitimiert? 
         Wissenschaftsgeschichtliche Ansätze von Ludwik Fleck (Fleck 1983, 2011) oder auch
        Thomas S. Kuhn (Kuhn 1996) lassen sich fruchtbar verknüpfen mit Überlegungen der
        Software Studies (Fuller 2003; Mackenzie 2006; Manovich 2013; Cox / McLean 2012),
        die den kulturellen Kontext und die soziale Praxis als Einflussfaktoren von
        Software-Erstellung und -Nutzung betonen. Ebenso existiert eine Diskussion um die
        Rolle von Algorithmen, statistischen Modellierungen oder generell Software-„Tools“
        in Forschungsprozessen der Digital Humanities (Berry 2014; Bubenhofer / Scharloth
        2015; Kath et al. 2015; Rieder / Röhle 2012). Die reiche Praxis der
        Informationsvisualisierung und Visual Analytics für Fragen der Digital Humanities
        führt ebenso nicht nur zu methodischen, sondern auch methodologischen und
        theoretischen Diskussionen (Chen et al. 2008; Keim et al. 2010). Auch die Rolle von
        Denkstilen in Wissenschaftsdiskursen und ihre Manifestation auf sprachlicher Ebene
        wird in neuerer Zeit intensiver reflektiert (Czachur 2013; Fix 2011; Schiewe
        1996).
         Der Vortrag möchte vor diesem Hintergrund Code, Modelle, Visualisierungen und Sprache als Mittel und Instrument im Kontext wissenschaftlicher Routinen in den Digital Humanities reflektieren. Inwiefern drücken sich in den gewählten Programmiersprachen, Algorithmen, Visualisierungstypen, statistischen Modellen und sprachlich gefassten Interpretationen unterschiedliche Denkstile der Digital Humanities aus? Wo liegen die Chancen, aber auch die Gefahren, diese Denkstile zu reproduzieren? Welche Auswirkungen haben die Wahl und der reflektierte oder nicht reflektierte Umgang mit den Instrumenten auf wissenschaftliche Innovation in den Digital Humanities?
         Dazu wird zunächst der Einsatz und die Typen von Visualisierungen in den textorientierten Digital Humanities analysiert und dann die technischen aber auch kulturellen Entstehungsbedingungen der algorithmischen Visualisierungen untersucht.
      
      
         
            
               Bibliographie
               
                  Berry, David M. (2014): Critical
            Theory and the Digital. London, Oxford, New York, New Delhi,
            Sydney: Bloomsbury.
               
                  Bubenhofer, Noah (2015): "Coding Cultures: Über den
            Zusammenhang von Programmiersprachen und Denkstilen", in: Sprechtakel. Linguistische Notizen https://www.bubenhofer.com/sprechtakel/2015/08/08/coding-cultures-ueber-den-zusammenhang-von-programmiersprachen-und-denkstilen/
            [letzter Zugriff 17. August 2015].
               
                  Bubenhofer, Noah / Scharloth, Joachim (2015):
            "Maschinelle Textanalyse im Zeichen von Big Data und Data-driven Turn –
            Überblick und Desiderate", in: Zeitschrift für
            Germanistische Linguistik 43, 1: 1–26.
               
                  Chen, Chun-houh / Härdle, Wolfgang / Unwin, Antony
            (eds.) (2008): Handbook of data visualization (=
            Springer handbooks of computational statistics). Heidelberg:
            Springer.
               
                  Cox, Geoff / McLean, Alex (2012): Speaking Code. Coding as Aesthetic and Political Expression.
            Cambridge, Mass.
               
                  Czachur, Waldemar (2013): "Ludwik Flecks
            Denkstilansatz als Inspiration für die Diskurslinguistik", in: Zeitschrift des Verbandes Polnischer Germanisten 2:
            141–150.
               
                  Fix, Ulla (2011): Denkstile und
            Sprache. Die Funktion von „Sinn-Sehen“ und „Sinn-Bildern“ für die
            „Entwicklung einer wissenschaftlichen Tatsache“ http://home.uni-leipzig.de/fix/Fleck.pdf [letzter Zugriff 04.
            März 2014].
               
                  Fleck, Ludwik / Werner, Sylwia / Zittel, Claus (eds.)
            (2011): Denkstile und Tatsachen: Gesammelte Schriften
            und Zeugnisse. Berlin: Suhrkamp.
               
                  Fleck, Ludwik / Schäfer, Lothar / Schnelle, Thomas
            (eds.) (1983): Erfahrung und Tatsache: gesammelte
            Aufsätze. Frankfurt am Main: Suhrkamp.
               
                  Fuller, Matthew (2003): Behind the
            blip: essays on the culture of software. New York:
            Autonomedia.
               
                  Kath, Roxana / Schaal, Gary S. / Dumm, Sebastian
            (2015): "New Visual Hermeneutics", in: Zeitschrift für
            germanistische Linguistik 43, 1: 27–51.
               
                  Keim, Daniel A. / Kohlhammer, Jörn / Ellis, Geoffrey et
              al. (2010): Mastering the information age -
              solving problems with visual analytics. Goslar: Eurographics
              Association.
               
                  Kuhn, Thomas S. (1996): Structure
              of Scientific Revolutions. University of Chicago Press.
               
                  Mackenzie, Adrian (2006): Cutting
              Code: Software And Sociality (Digital Formations). Bern / Berlin /
              Frankfurt am Main / New York / Paris / Wien: Peter Lang.
               
                  Manovich, Lev (2013): Software
              Takes Command. New York / London: INT edition.
               
                  Rieder, Bernhard / Röhle, Theo (2012): "Digital
              Methods: Five Challenges", in: Berry, David M. (eds.): Understanding Digital Humanities. Basingstoke: Palgrave
              67–84.
               
                  Schiewe, Jürgen (1996): Sprachenwechsel - Funktionswandel - Austausch der Denkstile. Die
              Universität Freiburg zwischen Latein und Deutsch. Tübingen: Niemeyer.
               
                  Wall, Larry (1999): Perl, the first
              postmodern computer language
                  http://www.wall.org/~larry/pm.html [letzter Zugriff 19. August
                2015].
            
         
      
   


10544	2016	
      
         
            Einleitung
            Dieser Beitrag stellt eine neuartige Methode zur optischen Zeichenerkennung (
          Optical Character Recognition, OCR) speziell für Textvorlagen des 17. Jahrhunderts vor. Anstatt ein neues OCR-Verfahren zu entwickeln, werden zwei etablierte Open-Source-Lösungen genutzt. Die Ausgaben der Programme werden computergestützt kombiniert, um so eine möglichst genaues Textergebnis zu erhalten. Die Besonderheiten und die Güte der Methode wird anhand der Texterfassung von Gelegenheitsgedichten von Simon Dach illustriert.
        
            
               OCR
               OCR bezeichnet die Gesamtheit von Verfahren, die in der Lage sind, aus
            Rastergrafiken Schriftzeichen zu erkennen. Der Begriff wird sowohl für die
            eigentliche Mustererkennung als auch für den gesamten Prozess der
            Bildverarbeitung verwendet. Letzterer gliedert sich normalerweise in drei
            Schritte: 1. Bildoptimierung: Diese besteht aus der
            Bitonalisierung der Digitalisate, ihrer Begradigung (sog. Deskewing) und aus der Entfernung von Artefakten (sog. Despeckling). Außerdem können beim Scannen
            entstandene Wellen in einzelnen Zeilen automatisch begradigt werden (sog.
            Dewarping). 2.
            Strukturerkennung ( Optical Layout
            Recognition, OLR): Die einzelnen Seiten werden u. a. in Spalten,
            Absätze und Zeilen gegliedert. 3. Mustererkennung
          (OCR): Für diese Aufgabe gibt es verschiedene Lösungsvorschläge sowohl
          im kommerziellen wie auch im Open-Source-Bereich. Besonders verbreitet sind
          die Software FineReader der Firma ABBYY sowie BITAlpha aus dem Hause Tomasi, die u. a. von
          Bibliotheken eingesetzt werden. Die bekanntesten Open-Source-Lösungen sind
          das ursprünglich von Hewlett-Packard entwickelte und heute von Google
          betreute Tesseract (GitHub 2016a) und das
          ursprünglich am DFKI Kaiserslautern entwickelte OCRopus (GitHub 2016b). 
               Grundsätzlich lassen sich bei OCR zwei unterschiedliche Erkennungsansätze unterscheiden: zeichenorientierte Verfahren wie Tesseract vergleichen das Bild eines Zeichens Pixel für Pixel mit einer Datenbasis (dem sog. Modell) und geben das ähnlichste Zeichen zurück. Sequenzorientierte (segmentierungsfreie) Verfahren wie OCRopus legen ein Raster fester Größe über eine Zeile und bestimmen anhand der Folgen der einzelnen Spalten, repräsentiert als Bitvektoren (0 entspricht weiß, 1 schwarz) die wahrscheinlichste Zeichensequenz. 
            
            
               Gelegenheitsgedichte
               Unsere Studie beschäftigt sich mit OCR am Beispiel von Gelegenheitsgedichten
            des 17. Jahrhunderts, denen durch die von Segebrecht (1977) initiierte
            literaturwissenschaftliche Neubewertung eine zunehmende kulturgeschichtliche
            Bedeutung zukommt (vgl. Klöker 2010: 39). Der Zugriff auf diese Drucke wurde
            durch das VD17 (HAB 2007-2016)1 und durch das Handbuch des personalen
            Gelegenheitsschrifttums in europäischen Bibliotheken und Archiven
            (Garber 2001-2013) erleichtert. Dennoch kann ein digitales Korpus für diese
            Textsorte heute nur als Desiderat wahrgenommen werden. Für Werke von Simon
            Dach ist die Ausgangslage scheinbar besser: Mit der digitalisierten
            vierbändigen Ausgabe von Ziesemer (Ziesemer 1936-1938) steht ein großer Teil
            der heute bekannten Gedichte zur Verfügung (vgl. auch Dach o. J.; TextGrid
            2015). 2 Jedoch trübt sich dieser Eindruck beim textkritischen Blick. 3
               
               111 Funeralschriften Simon Dachs wurden im Verlauf des DFG-Pilotprojektes zum
            OCR-Einsatz bei der Digitalisierung der
              Funeralschriften der Staatsbibliothek zu Berlin (2009-2011)
              (Federbusch / Polzin 2013) digitalisiert und per OCR erfasst. Die in der
              vorliegenden Studie genutzten Drucke zeichnen sich dahingehend aus, dass
              eine einheitliche Schrifttype sowie ein einfaches Layout vorliegen. Im
              Unterschied zu Texten des 18. und 19. Jahrhunderts war für diese Drucke noch
              ein relativ hoher manueller Aufwand erforderlich. Die Schrifttypen weisen
              daher eine vergleichsweise hohe Varianz bzgl. ihrer Form auf. Die 111
              Trauergedichte weisen eine Textgenauigkeit von bis zu 95% auf. Der
              Schwerpunkt der folgenden Studie liegt auf der Entwicklung und Prüfung von
              Methoden, die perspektivisch eine korrektere Übertragung der Textquellen aus
              dem 17. Jahrhundert liefern soll. 
            
         
         
            Arbeitsablauf
            
               
               
                  Abb. 1: Modell eines vollständigen Erfassungsworkflows
                (diese Studie betrifft die eingefärbten Stationen). 
            
            Abbildung 1 gibt einen Überblick über den Arbeitsablauf der hier vorgestellten Methode. Im Unterschied zu existierenden Workflows unterteilt unser Vorschlag die Bildoptimierung in zwei Phasen: 1.
                global: Das komplette Digitalisat wird beschnitten, binarisiert, begradigt und von Artefakten befreit. Danach findet die Optische Layouterkennung (OLR) statt. 2.
                lokal: Die identifizierten Textzonen werden aus dem Bild der Seite ausgeschnitten und nochmals begradigt. Dadurch wird die häufig zu beobachtende Trapezform der Digitalisate, die durch Scannen von Büchern ohne Auftrennen des Buchrückens entsteht, behandelt. Die Bilder für die einzelnen Zonen werden anschließend in Zeilen zerschnitten und den OCR-Engines übergeben.
              
            Unser Vorgehen bei der OCR orientiert sich an der manuellen Texterfassung per Double Keying: Dabei werden Texte von zwei unabhängigen
                Erfassern transkribiert. Im Vergleich der beiden Textversionen werden die
                Unterschiede ermittelt und die korrekte Version ausgewählt. Um den
                Genauigkeitsgewinn durch die Mehrfacherfassung zu erhöhen, wurden zwei
                paradigmatisch verschiedene OCR-Verfahren, Tesseract und OCRopus, mit
                unterschiedlichen Stärken und Schwächen eingesetzt. Beide Open-Source-Programme
                erlauben ein Training auf die vorwendeten Typen und die Anwendung spezifischer
                OCR-Modelle. Dies ist wie Springmann et al. (2015) zeigen ein wesentlicher
                Vorteil gegenüber den meisten Closed-Source-Lösungen, da die mitgelieferten
                OCR-Modelle insbesondere für frühe Druckerzeugnisse bzw. gebrochene Schriften
                sehr schlechte Ergebnisse bzgl. der Textgenauigkeit liefern. Die automatische
                Vereinigung der beiden Textversionen findet im Wesentlichen auf Basis einer
                Textdifferenzberechnung mit Hilfe von diff (Hunt /
                McIlroy 1976) statt, wobei im Falle von Unterschieden verschiedene
                Bewertungsheuristiken zur Bestimmung der korrekten
                Textversion eingesetzt werden. Das skizzierte Vorgehen erlaubt auch die
                Kombination von mehr als zwei Textversionen sowie den anschließenden Einsatz von
                OCR-Nachkorrekturverfahren (vgl. z. B. Vobl et al. 2014). 
         
         
            Evaluation
            Die Güte der hier vorgestellten Methode wird anhand der Volltexterfassung von
                  Funeralschriften Simon Dachs (vgl. 1.2) evaluiert. Dabei konzentriert sich die
                  Evaluation auf drei Punkte: 
            
               Welchen Einfluss hat die Wahl der Binarisierungsmethode auf die Textgenauigkeit?
               Wie groß ist der Unterschied zwischen einem Standardmodell und einem speziell für die zu erfassenden Texte trainierten Modell bzgl. der Textgenauigkeit?
               Kann die Vereinigung zweier durch OCR erzeugter Texte die Textgenauigkeit erhöhen?
            
            Ein typisches Beispiel für die Untersuchungsgrundlage sowie die entsprechenden OCR-Ausgaben gibt Abbildung 2.
            
               
               
                  Abb. 2: Vergleich der OCR-Ergebnisse. 
            
         
         
            Material
            
               Ground Truth
               Voraussetzung für die Evaluation und das Modelltraining ist fehlerfreier
                        Volltext ( Ground Truth). Um für die Studie
                        entsprechende Daten zu gewinnen, wurde eine manuelle Korrektur aller 111
                        Texte vorgenommen. Die Korrektur schloss nicht nur die Text-, sondern auch
                        die datenstrukturelle Ebene ein. Der Aufwand belief sich auf 150 Stunden. Im
                        Ergebnis liegen alle Texte im DTA-Basisformat vor und sind über die
                        Qualitätssicherungsplattform DTAQ zugänglich. 
            
            
               Materialauswahl
               Für das Training der spezifischen OCR-Modelle wurden 30 Seiten Ground-Truth zufällig ausgewählt. Für die Evaluation der Modelle wurden 25 andere zufällig ausgewählte Seiten verwendet.
            
            
               Referenzlexikon
               Zur Vereinigung beider OCR-Versionen wurde ein Referenzlexikon gültiger historischer Schreibungen des 17. Jahrhunderts herangezogen. Dazu wurden Wortformen (
                          n=217067) aus DTA-Texten dieses Zeitraums extrahiert.
                        
            
         
         
            Durchführung
            
               Vorverarbeitung
               Für Beschneidung und Begradigung wurde das Programm Scantailor (GitHub 2016 a) eingesetzt. Für die Binarisierung,
                          Artefaktbereinigung und Zeilenglättung wurde sowohl Scantailor als auch das
                          in OCRopus enthaltene Werkzeug nlbin verwendet. 
            
            
               OLR
               Die einzelnen Textzonen (Abschnitte und Kustoden) wurden mit Hilfe von Leptonica (Bloomberg 2001-2015) lokalisiert und
                            manuell nachkorrigiert. Für die Untergliederung der Zonen in Zeilen wurde
                            ebenfalls Leptonica eingesetzt. 
            
            
               OCR
               Die Zeichenerkennung erfolgte sowohl mit OCRopus als auch mit Tesseract. Die erste Versuchsreihe basierte auf mitgelieferten Modellen. Für die zweite Versuchsreihe wurden die OCR-Programme mit Ground-Truth-Daten trainiert. Für das Training der OCRopus-Modelle wurde OCRopus eingesetzt. Dabei wurde für das Training aus Gründen der Modellvergleichbarkeit eine feste Anzahl von Iterationsschritten (
                              n=30000) festgelegt. Die Tesseract-Modelle wurden mit Hilfe von
                              VietOCR erstellt.
                            
            
            
               Textvereinigung
               Die Textvereinigung wurde in
                              Python mit Hilfe des Moduls
                              difflib implementiert. Neben dem Referenzlexikon standen zur Konfliktauflösung auch die von den OCR-Programmen zurückgelieferten Konfidenzen auf Zeichenebene zur Verfügung. Waren sich die beiden Engines bzgl. eines Wortes bzw. einer Textsequenz uneins, wurde zunächst dem Wort Vorrang gegeben, dass sich im Referenzlexikon befindet. Konnte dort keine der beiden Versionen gefunden werden, wurde die Entscheidung auf Basis der Konfidenzwerte getroffen.
                            
            
            
               Qualitätsmessung
               Die Bestimmung der Textqualität erfolgte durch Messung des Anteils falsch erkannter Zeichen (Fehlerrate in Prozent) im Vergleich zum fehlerfreien Volltext.
            
         
         
            Ergebnisse und Diskussion
            Tabelle 1 gibt einen Überblick über die Ergebnisse der Evaluation bzgl. der
                            Fehlerrate auf Zeichenebene unter Berücksichtigung der Vorverarbeitung des
                            Trainings- und Testmaterials, der Modellklasse (standard vs. spezifisch) und der
                            eingesetzten OCR-Software (OCRopus, Tesseract). Das beste (grün) und das schlechteste Ergebnis (rot) sind hervorgehoben. Da wir keinen
                            Einfluss auf die Vorverarbeitung der Trainingsmaterialien der mitgelieferten
                            Modelle haben, ist die Matrix in dieser Hinsicht unvollständig. 
            
               
               
                  Tab. 1: Darstellung der Ergebnisse auf Einzel-OCR-Ebene im Bezug auf
                                Vorverarbeitungsmethode für Trainings- und Testmaterial, Modelltyp und verwendete
                                OCR-Software. 
            
            Die geringste erreichte Fehlerrate (3,89 %) liegt etwa im Bereich der
                                Textgenauigkeit der 111 Gedichte aus der Pilotstudie von Federbusch (Federbusch
                                / Polzin 2013). Die Fehlerrate von Tesseract ist jeweils höher als die von
                                OCRopus. Der sequenzorientierte Ansatz hat klare Vorteile bei der Erkennung von
                                Schriftzeichen, die die typischen Charakteristika früher Drucke aufweisen. 5
            
            Desweiteren zeigt sich, dass die Vorverarbeitung mit nlbin für Tesseract sowohl auf Trainings- als auch auf Testebene jeweils schlechtere Ergebnisse bringt. Für OCRopus sind die Ergebnisse bzgl. der Vorverarbeitung differenzierter: Die beste Kombination liefert eine Vorverarbeitung des Trainingsmaterials mit nlbin bei einer nachfolgenden Vorverarbeitung des Testmaterials mit Scantailor. Unterschiede im Ergebnis der Vorverarbeitung beider Programme illustriert Abbildung 3.
            
               
                Abb. 3: Bild einer Textzeile nach der Vorverarbeitung mit nlbin (oben) und
                                  Scantailor (unten). 
            
            Die von Scantailor durchgeführte Bildvorverarbeitung ist deutlich normativer und für einen zeichenorientierten Ansatz wie Tesseract besser geeignet. Das Training sequenzorientierter Ansätze leidet unter dieser Vergröberung.
            Es zeigt sich erneut, dass spezifisch trainierte Modelle eine massive Textgenauigkeitsverbesserung mit sich bringen können (vgl. auch Springmann et al. 2015).
            
               Textvereinigung
               Betrachtet man die Beispielausgaben in Abbildung 2, so wird der
                                    Qualitätsunterschied zwischen beiden OCR-Programmen ersichtlich. An
                                    einzelnen Stellen jedoch (z. B. Großbuchstaben am Anfang der Zeile im
                                    letzten Abschnitt) hat Tesseract Erkennungsvorteile.
               Ausgehend von diesem Befund wurde der jeweils genaueste Text von OCRopus und Tesseract miteinander vereinigt. Es hat sich gezeigt, dass die Konfidenzen, die die Programme für jedes Zeichen zurückliefern, kein verlässliches Kriterium sind, um Konflikte aufzulösen. Die Fehlerrate nimmt zu. Die Strategie, Wörter bzw. Sequenzen zu bevorzugen, die sich im Referenzlexikon befinden, hat dagegen eine messbare Verbesserung mit sich gebracht. Die Anzahl der falsch erkannten Zeichen konnte um 14 % reduziert werden (Fehlerrate 3,34 %). Es ist zu vermuten, dass der Effekt größer wäre, wenn zwei OCR-Ergebnisse mit vergleichbarer Qualität vorlägen. Dies bleibt jedoch zum jetzigen Zeitpunkt für Drucke des 17. Jahrhunderts ein Desiderat.
            
         
      
      
         
            Verzeichnis der im deutschen Sprachraum erschienenen Drucke des 17. Jahrhunderts.
            Vgl auch Dach (o. J.) in http://www.zeno.org/Literatur/M/Dach,+Simon/Gedichte sowie TextGrid (2015).
            „Ziesemers Dach-Ausgabe ist textlich zu wenig genau, um auch für die dort abgedruckten, fast ausnahmslos deutschsprachigen, Gedichte den Rückgriff auf die kasualen Einzeldrucke und andere zeitgenössische Ausgaben entbehren zu können. Jede Stichprobe erweist für jedes einzelne Gedicht Transkriptionsfehler und unerklärte Texteingriffe.“ (Walter 2008: 466)
            Für Frakturdrucke des 19. Jahrhunderts ist ein solch starker Unterschied zwischen den Tesseract und OCRopus nicht nachgewiesen.
         
         
            
               Bibliographie
               
                  Bloomberg, Dan (2001-2015): Leptonica http://www.leptonica.com/
                                    [letzter Zugriff: 15. Oktober 2015].
               
                  Dach, Simon (o. J.): Gedichte
                  
                                      http://www.zeno.org/Literatur/M/Dach,+Simon/Gedichte [letzter
                                      Zugriff 15. Oktober 2015]. 
               
                  Federbusch, Maria / Polzin, Christian (2013): Volltext via OCR - Möglichkeiten und Grenzen.
                                      Testszenarien zu den Funeralschriften der Staatsbibliothek zu Berlin -
                                      Preußischer Kulturbesitz. Berlin Staatsbibliothek zu Berlin http://staatsbibliothek-berlin.de/fileadmin/user_upload/zentrale_Seiten/historische_drucke/pdf/SBB_OCR_STUDIE_WEBVERSION_Final.pdf
                                      [letzter Zugriff 15. Oktober 2015].
               
                  Garber, Klaus (2001-2013): Handbuch
                                      des personalen Gelegenheitsschrifttums in europäischen Bibliotheken und
                                      Archiven. 13 Bände. Hildesheim / Zürich / New York: Olms /
                                      Weidmann. 
               
                  GitHub Inc. (2016a): ScanTailor
                  http://scantailor.org/ [letzter
                                      Zugriff 15. Oktober 2015].
               
                  GitHub Inc. (2016b): OCRopus
                  https://github.com/tmbdev/ocropy [letzter Zugriff 15. Oktober
                                        2015]. 
               
                  GitHub Inc. (2016c): Tesseract
                  https://github.com/tesseract-ocr [letzter Zugriff 15. Oktober
                                          2015].
               
                  HAB = Herzog August Bibliothek Wolfenbüttel
                                          (2007-2016): VD17. Das Verzeichnis der im deutschen
                                          Sprachraum erschienenen Druck des 17. Jahrhunderts http://www.vd17.de/index.php?category_id=1&article_id=1&clang=0.
               
                  Hunt, James W. / McIlroy, M. Douglas (1976): "An
                                          Algorithm for Differential File Comparison" in: Computing
                                          Science Technical Report (Bell Laboratories) 41 http://www.cs.dartmouth.edu/~doug/diff.pdf
               
               
                  Klöker, Martin (2010): "Das Testfeld der Poesie.
                                          Empirische Betrachtungen aus dem Osnabrücker Projekt zur 'Erfassung und
                                          Erschließung von personalen Gelegenheitsgedichten'", in: Keller, Andreas /
                                          Lösel, Elke / Wels, Ulrike / Wels, Volkhard (eds.): Theorie und Praxis der Kasualdichtung in der Frühen Neuzeit (=
                                          Chloe. Beihefte zu Daphne 43). Amsterdam / New York: Rodopi 39-84. 
               
                  Python Software Fundation (1990-2016): difflib - Helpers for Computing Deltas 
                  https://docs.python.org/2/library/difflib.html [letzter Zugriff
                                          15. Oktober 2015].
               
                  Segebrecht, Wulf (1977): Das
                                          Gelegenheitsgedicht. Ein Beitrag zur Geschichte und Poetik der
                                          deutschen Lyrik. Suttgart: Metzler.
               
                  Springmann, Uwe / Lüdeling, Anke / Schremmer, Felix
                                          (2015): "Zur OCR frühneuzeitlicher Drucke am Beispiel des RIDGES-Korpus von
                                          Kräutertexten (Poster)", in: Tagung der DHd (Digitale
                                          Geisteswissenschaften im deutschsprachigen Raum), Graz https://www.linguistik.hu-berlin.de/de/institut/professuren/korpuslinguistik/mitarbeiter-innen/anke/pdf/SpringmannLuedelingSchremmer2015.pdf
                                          [letzter Zugriff 15. Oktober 2015].
               
                  TextGrid (2015): Die digitale
                                          Bibliothek bei TextGrid
                  https://textgrid.de/digitale-bibliothek [letzter Zugriff 15.
                                            Oktober 2015] 
               
                  VietOCR
                  http://vietocr.sourceforge.net/ [letzter Zugriff: 15. Oktober
                                              2015].
               
                  Vobl, Thorsten / Gotscharek, Annette / Reffle, Uli /
                                                Ringlstetter, Christoph / Schulz, Klaus U. (2014): "PoCoTo - an
                                                open source system for efficient interactive postcorrection of OCRed
                                                historical texts" in: Proceedings of the First
                                                International Conference on Digital Access to Textual Cultural Heritage
                                                (DATeCH '14): 57-61 http://dl.acm.org/citation.cfm?id=2595197 [letzter Zugriff 15.
                                                Oktober 2015].
               
                  Walter, Axel E.(2008): "Dach digital? Vorschläge zu
                                                einer Bibliographie und Edition des Gesamtwerks von Simon Dach nebst einigen
                                                erläuterten Beispielen vernachlässigter bzw. unbekannter Gedichte", in:
                                                Walter, Axel E. (ed.) in: Simon Dach (1605–1659).
                                                Werk und Nachwirken. Tübingen: Niemeyer: 465-522.
               
                  Ziesemer, Walter (ed.) (1936-1938): Simon Dach: Gedichte. Vier Bände. Halle an der Saale:
                                                Niemeyer.
               
            
         
      
   


10546	2016	
      
         
            Einleitung: Digital Humanities und Filmanalyse
            Während sich die „Vermessung der Kultur“ (Lauer 2013) in den textorientierten
          Geisteswissenschaften in den letzten Jahren rasant entwickelt hat (vgl. etwa
          Konzepte wie Culturomics, Distant
          Reading, etc.), so befindet sich die „Vermessung ästhetischer
          Erscheinungen“ (Flückinger 2011) für den Bereich der Filmwissenschaft und
          Filmanalyse noch in den Anfängen. Flückinger (2011: 44) spricht in diesem
          Zusammenhang gar von einem Spannungsfeld zwischen Empirie und Ästhetik, welches
          sich zwangsläufig ergeben muss, wenn man „die eigentümliche Unschärfe, die allen
          künstlerischen Werken eignet, in messbare Einheiten zerlegen will“. Dabei lassen
          sich quantitative Ansätze in der Filmanalyse mindestens bis in das Jahr 1912
          1 zurückverfolgen und auch aktuelle Lehrbücher zur Filmanalyse beschreiben
          gleichermaßen weiche (qualitative) und harte (quantitative) Kategorien und Methoden (Korte 2004: 15). Bei
          quantitativen Ansätzen steht vor allem die Analyse von Dauer und
          Auftretenshäufigkeit einzelner Einstellungen in einem Film im Mittelpunkt (vgl.
          Salt 2006, Kap. „The Numbers Speak“). So stellt etwa die online verfügbare
          Datenbank 
                  Cinemetrics
                (Cinemetrics o. J.) entsprechende Informationen
          zur Länge und Verteilung einzelner Einstellungen für mehrere tausend Filme
          bereit und ermöglicht so vergleichende Analysen von Filmen aus unterschiedlichen
          Genres und Epochen. 
            Während die Segmentierung der Filme in der Cinemetrics-Datenbank von der Community manuell vorgenommen wird, gibt es
            auch Beispiele für Forschungsarbeiten, bei denen die quantifizierbaren Parameter
            automatisch erhoben werden. Hoyt, Ponot und Roy (2014) präsentieren etwa einen
            Prototyp namens ScripThreads, der in der Lage ist, Filme
            der American Film Scripts Online-Datenbank zu parsen und
            die Handlungsentwicklung eines Films anhand der Szenen und Figuren zu
            visualisieren. Ein Beispiel für die vergleichende Analyse von Filmmetadaten
            findet sich im 
                  Cinegraph-Projekt  von Chris Weaver
            (2014). Hier können Filme anhand unterschiedlicher Metadaten (z. B. Filmname,
            Veröffentlichungsdatum, Bewertung, Genre, Oscars, Darsteller, etc.) miteinander
            verglichen und in einer interaktiven Darstellung zueinander in Beziehung gesetzt
            werden. 
            Daneben finden sich im Netz eine ganze Reihe experimenteller Tools, die nicht
              immer einen wissenschaftlichen Anspruch haben, aber gut illustrieren, welche
              weiteren Aspekte von Filmen automatisch analysierbar sind: Beispielhaft sei etwa
              das Python-Tool 
                  VideoGrep
                (Lavigne 2014) genannt, welches das
              Durchsuchen von Filmdialogen nach bestimmten Schlüsselwörtern ermöglicht, um auf
              Basis der Treffer dann einen automatischen Zusammenschnitt („supercut“) all der
              Szenen, in denen das gesuchte Wort vorkommt, zu erstellen. Die Anwendung 
                  Pretentious-O-Meter
                (Beard 2015) analysiert automatisch, wie
              groß die Bewertungslücke zwischen Nutzerbewertungen und professionellen
              Filmkritiken eines Films ist und visualisiert dies in einem Kontinuum, welches
              von „mass-market“ bis „very pretentious“ reicht. Weitere Ansätze der
              automatischen Filmanalyse finden sich für die Farbverwendung in Filmen: Frederic
              Brodbeck (2011) visualisiert in seinem 
                  Cinemetrics-Projekt Filme als kreisförmig angeordnete
              Timelines, in denen u. a. die jeweils dominanten Farben zu sehen sind. Ein
              weiteres Projekt visualisiert Filme als zusammengestauchte Einzelframes, um so
              farbige 
                  MovieBarcodes
                (MovieBarcodes o. J.) zu erstellen. 
            Auch auf der DHd 2015 wurde das Thema der Quantifizierung filmischer Strukturen über Filmbild, Filmschnitt und Filmstil bereits auf methodischer Ebene thematisiert (Heftberger 2015) und Howanitz (2015) präsentierte eine erste
                Distant Watching-Studie für das „Fern-Sehen“ memetischer YouTube-Videos, deren „Schnittkurven“ er auf Frame-Ebene analysiert. In diesem Beitrag knüpfen wir thematisch an die genannten DHd-Vorträge an und diskutieren grundlegende Möglichkeiten der computergestützten Filmanalyse, die über die Quantifizierung von Einstellungen und Szenen hinausgehen. Dabei sollen weitere automatisch quantifizierbare Parameter zur Diskussion gestellt werden, um so neue Perspektiven und Zugänge zur computergestützten Filmanalyse aufzuzeigen und das Thema noch stärker in den Digital Humanities zu verankern. Um die Grenzen und Möglichkeiten dieser Ansätze besser illustrieren zu können, wurde eine Reihe von Prototypen erstellt, die nachfolgend kurz vorgestellt werden.
              
         
         
            Prototypen für die computergestütze Filmanalyse
            In diesem Abschnitt werden drei unterschiedliche Prototypen beschrieben, die
                jeweils auf unterschiedliche quantifizierbare Aspekte von Filmen abzielen und
                damit die Untersuchung ganz unterschiedlicher Fragestellungen erlauben. Die
                Tools greifen allesamt auf im Web frei verfügbare Informationen zu Filmen
                zurück: So stehen etwa über die Plattformen 
                  OpenSubtitles
                oder die 
                  Internet Script Movie Database
               
                maschinenlesbare Dialoge von Filmen und Serien in großem Umfang zur Verfügung.
                Zusätzlich können detaillierte Metadaten sowie auch nutzergenerierte Bewertungen
                und Kommentare zu Filmen über Plattformen wie 
                  IMDb
                (Internet Movie Database)
                abgerufen werden. Darüber hinaus soll als weiterer quantifizierbarer Parameter,
                der direkt aus den Filmen extrahiert werden kann, die Farbverwendung 2 in die Analysen mit einbezogen werden. Alle nachfolgend beschriebenen
                Prototypen wurden jeweils mit Standard-Webtechnologien (HTML / CSS / JavaScript)
                und bestehenden Python-Bibliotheken umgesetzt. 
            
               
                  SubVis – Analyse der Filmsprache
                  
               Das SubVis-Tool analysiert über OpenSubtitles verfügbare Dialoge von beliebigen, zunächst
                  allerdings nur englischsprachigen Filmen anhand typischer linguistischer
                  Parameter wie Wortfrequenzen oder POS-Tagging und visualisiert die
                  Ergebnisse in einem interaktiven Web-Interface. Zusätzlich kann die
                  Auftretenshäufigkeit einzelner Zeichen oder längerer Sprachsequenzen (=
                  jeweils ein eingeblendeten Untertitel) für beliebig definierbare
                  Analyseintervalle (z. B. jeweils für 5 Minuten-Sequenzen) in einer Timeline
                  dargestellt werden, um bspw. auf einen Blick zu sehen, an welchen Stellen im
                  Film besonders viel oder wenig gesprochen wird (vgl. Abbildung 1). 
               
                  
                  
                     Abb. 1: Beispielhafte Visualisierung der Zeichen-
                    und Sprachsequenzhäufigkeiten für jeweils fünfminütige Teilabschnitte
                    des Films „Anchorman: The Legend of Ron Burgundy“.
               
               
                  Beispielhafte Fragestellungen, die mit dem Tool untersucht werden können:
                  
               
                  Gibt es für die Filme unterschiedlicher Regisseure jeweils typische Schlüsselwörter?
                  Kann man für Filme aus unterschiedlichen Genres beobachten, dass an
                      bestimmten Stellen (z. B. Anfang oder Schluss) besonders viel oder wenig
                      gesprochen wird? 
                  Wird in Filmen aus den 1980er Jahren insgesamt mehr gesprochen als in Filmen der 1990er Jahre?
               
            
            
               
                  Series Analysis Tool (SAT) – Analyse von TV-Serien anhand von Nutzerbewertungen, Figuren und Sprache
                    
               Das Series Analysis Tool ( SAT)
                    ermöglicht die Analyse von Serien und einzelnen Episoden. Dabei werden
                    verschiedene Parameter in einer Timeline-Darstellung visualisiert. Ein
                    wesentlicher Analyseaspekt ist dabei die Bewertung einzelner Episoden durch
                    die IMDb-Community, sodass auf einen Blick erkennbar ist, ob eine Serie im
                    Laufe der Zeit besser oder schlechter bewertet wird, oder ob es einzelne
                    Episoden gibt, die auffallend positiv oder negativ bewertet wurden.
                    Zusätzlich liest das Tool das Figureninventar für jede Episode aus und
                    erlaubt es, die Darstellung nach bestimmten Figuren zu filtern. So kann
                    schnell erkannt werden, ob das Auftreten bestimmter Figuren ggf. Einfluss
                    auf die Bewertung einzelner Episoden hat. Weiterhin wurde die Sprache der
                    Serien hinsichtlich Sentiment- und Emotionswörtern analysiert (vgl.
                    Abbildung 2). Als Datengrundlage dient ein bestehendes Korpus (Tiedemann
                    2012), in dem alle auf 
                     OpenSubtitles
                   in englischer
                    Sprache verfügbaren Untertitel von TV-Serien und Filmen bis zum Jahr 2013
                    enthalten sind. Dabei kam für die Sentiment Analyse das AFINN-Lexikon (Nielsen 2011) und für die Identifikation acht
                    grundlegender Emotionen (Angst, Wut, Freude, etc.) das NRC
                    Emotion Lexicon (Mohammad / Turney 2010) zum Einsatz. Sowohl die
                    Sentiment-Scores (positiv / negativ) als auch die Emotionsmarker können für
                    jede Episode in die Visualisierung mit einbezogen werden, um so potenzielle
                    Korrelationen zu den Nutzerbewertungen aufzuzeigen. 
               
                  
                  
                     Abb. 2: Beispielhafte Visualisierung der Serie
                      „Breaking Bad“, mit paralleler Darstellung der Benutzerbewertungen sowie
                      der Sentiment-Analyse der Dialoge für jede einzelne Episode.
               
               
                  Beispielhafte Fragestellungen, die mit dem Tool untersucht werden können:
                    
               
                  Gibt es generelle Trends bei der Bewertung von Serien mit zunehmender Zahl von Staffeln?
                  Wirken sich Sentiment- und Emotionsmarker der Dialoge positiv oder negativ auf die Bewertung einer Episode aus?
                  Wirkt sich das Auftreten bestimmter Nebenfiguren positiv oder negativ auf die Bewertung einer Episode aus?
               
            
            
               
                  MovieColors – Analyse von Filmen anhand von Farbe und Sprache
                    
               Der Prototyp
                      MovieColors erlaubt die computergestützte Analyse von Filmen anhand der Parameter Farbe und Sprache. Dabei wird zunächst der Film in einzelne Frames zerlegt. Mithilfe eines Clustering-Algorithmus werden dann die jeweils dominanten Farben extrahiert. Anhand dieser Farbinformation können charakteristische Farbprofile – ähnlich wie im eingangs erwähnten
                      MovieBarcodes-Projekt – für den gesamten Film erstellt werden. Zusätzlich wird die Sprache des Films über dessen Untertitel anhand von Wortfrequenzen und grundlegenden Sentiment-Werten (positiv / negativ) analysiert.
                    
               Die Visualisierungskomponente des Tools erlaubt es, Farbinformation und
                      Sprachanalyse in einer parallelen Ansicht darzustellen, um so potenzielle
                      Korrelationen zwischen dem Sentiment der Sprache und besonders markanten
                      Schlüsselwörtern sowie auch der Farbverwendung identifizieren zu können
                      (vgl. Abbildung 3). Zusätzlich kann jeder Frame einzeln angezeigt werden,
                      zusammen mit dem entsprechenden Untertitel sowie einer Analyse der
                      dominanten Farben im jeweiligen Bild.
               
                  
                  
                     Abb. 3: Analyse des Films „König der Löwen“, mit
                        Darstellung des Farbprofils (oben), der Sentiment-Analyse (Mitte) sowie
                        der häufigsten Wörter (unten) entlang der Zeitachse des Films.
               
               
                  Beispielhafte Fragestellungen, die mit dem Tool untersucht werden können:
                      
               
                  Gibt es charakteristische Farbprofile für Filme aus verschiedenen Genres oder Epochen?
                  Korrelieren bestimmte Farben mit positiven oder negativen Sentiment-Scores, also etwa dunkle Farben bei negativer Sprache?
                  Korrelieren bestimmte Farben mit Schlüsselwörtern, also etwa schwarz und lila immer dann, wenn der Bösewicht des Films auftritt?
               
            
         
         
            Ausblick
            Die in diesem Beitrag vorgestellten Prototypen beschreiben erste Versuche, Filme
                      computergestützt anhand unterschiedlicher, automatisch quantifizierbarer
                      Parameter zu analysieren. Im Austausch mit Kollegen aus der Medienwissenschaft
                      werden die Tools in den nächsten Monaten praktisch erprobt und je nach
                      Fragestellung iterativ angepasst und gegebenenfalls um weitere Funktionen
                      ergänzt. Sobald die Prototypen weiter ausgearbeitet sind, sollen sie auch der
                      Community über den DH-Regensburg-Blog zugänglich gemacht werden. Gleichzeitig sind
                      weitere Prototypen angedacht, bei denen als zusätzliche Analyseparameter
                      Gesichtserkennung (vgl. Arandjelovic / Zisserman 2005) sowie auch die Auswertung
                      der Audiospur (vgl. Zulko 2014) umgesetzt werden sollen. 
         
         
            Danksagungen
            Alle hier beschriebenen Prototypen wurden im Rahmen des Projektseminars „Digital Humanities“, im Masterstudiengang Medieninformatik an der Universität Regensburg, angefertigt. Besonderer Dank für die engagierte Umsetzung der Tools gebührt Hanns Meißner und Michael Stahl (
                        SubVis), Robert Jackermeier, Florian Ludwig und Alexander Uitz (
                        SAT) sowie Michael Kao (
                        MovieColors).
                      
         
      
      
         
            Vgl. den Vortrag von Tsivian (2014) auf der 1. Cinemetrics Conference, Chicago (Neubauer Collegium 2014).
            Zur historischen Verwendung von Farbe im Film vgl. auch die Online-Datenbank "Timeline of Historical Film Colors" von Flückinger (2011-2013).
         
         
            
               Bibliographie
               
                  Arandjelovic, Ognjen / Zisserman, Andrew (2005):
                          "Automatic face recognition for film character retrieval in feature-length
                          films", in: Proceedings of the Computer Vision and Pattern
                          Recognition Conference (IEEE) 860-867. 
               
                  Beard, Niall (2015): Pretentious-O-Meter http://pretentious-o-meter.co.uk/ [letzter Zugriff 04. Februar
                          2016].
               
                  Brodbeck, Frederic (2011): Cinemetrics. Bachelor graduation project at the Royal Academy of
                          Arts (KABK), Den Haag  http://cinemetrics.fredericbrodbeck.de/ [letzter Zugriff 04.
                          Februar 2016].
               
                  Cinemetrics (o. J.): http://www.cinemetrics.lv/
                          [letzter Zugriff 04.Februar 2016]
               
                  Flückiger, Barbara (2011): "Die Vermessung ästhetischer
                            Erscheinungen", in: Zeitschrift für
                            Medienwissenschaft 5, 2: 44-60. 
               
                  Flückinger, Barbara (2011-2013): Timeline of Historical Film Colors
                  http://zauberklang.ch/filmcolors/ [08. Januar 2016].
               
                  Heftberger, Adelheid (2015): "Filmbild , Filmschnitt ,
                                Filmstil – die Quantifizierung und Visualisierung von filmischen
                                Strukturen", in: Book of Abstracts, DHd 2015. 
               
                  Howanitz, Gernot (2015): „Distant Waching: Ein
                                  quantitativer Zugang zu YouTube-Videos“, in: Book of
                                  Abstracts, DHd 2015. 
               
                  Hoyt, Eric / Ponot, Kevin / Roy, Carrie (2014):
                                    „Visualizing and Analyzing the Hollywood Screenplay with ScripThreads“, in:
                                    Digital Humanities Quarterly 8, 4. 
               
                  IMDb (o. J.): Internet Movie
                                    Database. http://www.imdb.com/ [letzter Zugriff 04. Februar 2016]. 
               
                  IMSDb (o. J.): Internet Script
                                    Movie Database. http://www.imsdb.com/ [letzter Zugriff 04. Februar 2016]. 
               
                  Korte, Helmut (2004): Einführung in
                                      die Systematische Filmanalyse. Berlin: Erich Schmid Verlag. 
               
                  Lauer, Gerhard (2013): "Die digitale Vermessung der
                                        Kultur", in: Geiselberger, Heinrich / Moorstedt, Tobias (eds.): Big Data – Das neue Versprechen der Allwissenheit.
                                        Berlin: Suhrkamp 99-116. 
               
                  Lavigne, Sam (2014): Videogrep. Automatic Supercuts with Python http://lav.io/2014/06/videogrep-automatic-supercuts-with-python/
                                        [letzter Zugriff 04. Februar 2016].
               
                  Mohammad, Saif M. / Turney, Peter D. (2010): "Emotions
                                          evoked by common words and phrases: Using Mechanical Turk to create an
                                          emotion lexicon", in: Proceedings of the NAACL HLT 2010
                                          Workshop on Computational Approaches to Analysis and Generation of
                                          Emotion in Text 26-34. 
               
                  MovieBarcode (o. J.): http://moviebarcode.tumblr.com/ [letzter Zugriff 04. Februar
                                          2016].
               
                  Neubauer Collegium (2014): UChicago
                                          Cinemetrics Conference
                  https://www.youtube.com/watch?v=6ZXj67bygEc [letzter Zugriff 04.
                                            Februar 2016].
               
                  Nielsen, Finn Å. (2011): "A new ANEW: evaluation of a
                                            word list for sentiment analysis in microblogs", in: Proceedings of the ESWC2011 Workshop on „Making Sense of Microposts:
                                            Big things come in small packages“ 93-98. 
               
                  OpenSubtitles (o. J.) www.opensubtitles.org/ [letzter
                                            Zugriff 04. Februar 2016]. 
               
                  Salt, Barry (2006): Moving into
                                              Pictures. London: Starwood. 
               
                  Tiedemann, Jörg (2012): "Parallel Data, Tools and
                                                Interfaces in OPUS", in: Proceedings of the 8th
                                                International Conference on Language Resources and Evaluation
                                                2214-2218.
               
                  Weaver, Chris (2014): Cinegraph
                  http://www.cs.ou.edu/~weaver/improvise/examples/cinegraph/index.html
                                                  [letzter Zugriff 04. Februar 2016].
               
                  Zulko (4.7.2014): "Automatic Soccer Highlights
                                                    Compilations With Python" (Blogpost), in: __del__
                                                    (self) Eaten by the Python http://zulko.github.io/blog/2014/07/04/automatic-soccer-highlights-compilations-with-python/
                                                    [letzter Zugriff 08. Januar 2016].
            
         
      
   


10565	2016	
      
         
            Einleitung
            Der Beitrag möchte zeigen, wie die Berücksichtigung detaillierter,
          gattungsbezogener Metadaten auf produktive Weise mit dem Verfahren des Topic
          Modeling verbunden werden kann, um bisher nicht bekannte thematische Strukturen
          im Textverlauf in einer Sammlung spanischer und hispanoamerikanischer Romane zu
          entdecken. Ausgangshypothese ist, dass die Wichtigkeit bestimmter Topics nicht
          nur im Textverlauf variiert, sondern dies auch in verschiedenen Untergattungen
          auf unterschiedliche Weise tut. Eine Pilotstudie wurde im März 2015 beim
          Workshop zu Computational Narratology bei der DHd-Tagung in Graz vorgestellt. Im
          Rahmen der interdisziplinären Würzburger eHumanities-Nachwuchsgruppe
          "Computergestützte literarische Gattungsstilistik (CLiGS)
          wurde dieser Fragestellung nun mit weiter entwickelten Methodik sowie einer neu
          erstellten Sammlung spanischsprachiger Romane aus Spanien und Hispanoamerika
          nachgegangen. 
         
         
            Stand der Forschung und Fragestellung
            Die Frage nach dem Text- oder Handlungsverlauf in narrativen literarischen Texten hat jüngst zunehmende Aufmerksamkeit in der digitalen Literaturwissenschaft erhalten. Matthew Jockers kam durch Sentiment Analysis im Verlauf zahlreicher Romane zu dem (kontrovers diskutierten) Ergebnis, es gäbe sechs oder sieben grundlegende Plotstrukturen (Jockers 2015). Ben Schmidt hat unter anderem den Verlauf von Topic-Wahrscheinlichkeiten in der "screen time" amerikanischer Fernsehserien verfolgt (Schmidt 2014). Der vorliegende Beitrag verbindet die Frage nach dem Textverlauf mit der nach den Untergattungen, seine zentrale Fragestellung lautet: Können wir nach Untergattung unterschiedliche Verlaufsmuster für bestimmte Topics über den Textverlauf hinweg feststellen?
         
         
            Daten
            Die Textsammlung enthält 150 spanische und hispanoamerikanische Romantexte aus
            der Zeit von 1880 bis 1930 (für den spanischen Roman: Altisent 2008; de Nora
            1963, für den hispanoamerikanischen Roman: Gallo 1981; Williams 2009). Die Texte
            sind in TEI aufbereitet und mit detaillierten Metadaten versehen worden. Es
            wurden vier weit gefasste Untergattungen gewählt, um die Romane miteinander
            vergleichen zu können: novela sentimental, novela histórica, novela
            político-social und novela de tendencia
            subjetiva. Die Auswahl der Texte ist auch von der Verfügbarkeit als
            digitaler Volltext beeinflusst und daher nicht unbedingt repräsentativ.
            Abbildung 1 zeigt die Verteilung der Romane nach ausgewählten Metadaten. 
            
               
               
                  Abb. 1: Verteilung der Romane nach Metadaten 
            
         
         
            Methode
            Topic Modeling ist eine unüberwachte, nicht-deterministische Methode aus dem
              Bereich des Natural Language Processing, die auf Annahmen
              aus der distributionellen Semantik basiert und verborgene semantische Strukturen
              in großen Textsammlungen aufdeckt (einführend Blei 2011, grundlegend Blei 2003).
              Gruppen semantisch verwandter Wörter werden insbesondere aufgrund ihres häufigen
              gemeinsamen Auftretens in den untersuchten Dokumenten entdeckt. Ein Topic ist
              eine Wahrscheinlichkeitsverteilung von Wörtern; ein Dokument wird als
              Wahrscheinlichkeitsverteilung von Topics beschrieben. Topic Modeling ist eine in
              den DH äußerst beliebte Methode (Anwendungsbeispiele: Blevins 2010; Rhody 2012;
              Jockers 2013; Schöch 2015). 
            Hier wurde Topic Modeling als Teil eines umfassenden, weitgehend automatischen Arbeitsablaufes als Serie von Python-Skripten implementiert: Präprozessieren der Texte (Segmentierung, Binning, Lemmatisierung, POS-Tagging), das eigentliche Topic Modeling (mit Mallet, siehe McCallum 2002), Aufbereitung des Mallet-Outputs, zahlreiche Visualisierungen als Perspektiven auf die Ergebnisse. Die wichtigsten Parameter: Berücksichtigung ausschließlich der Substantive, Weglassung der 70 häufigsten Substantive, Romansegmente von ca. 600 Wörtern (unter Berücksichtigung von Absatzgrenzen), Anzahl von 70 Topics. Die Python-Skripte sind frei verfügbar und ausführlich dokumentiert, Begleitmaterialien (Skripte, Parameterdatei, Metadaten, Abbildungen) sind unter
                https://github.com/cligs/projects/tree/master/2016/dhd einsehbar.
              
         
         
            Ergebnisse und Diskussion
            Es werden zunächst die Topics selbst dargestellt, dann Unterschiede in den Topic-Verteilungen nach Untergattungen, über den Textverlauf hinweg und schließlich über den Textverlauf in Abhängigkeit der Untergattung.
            
               Topics
               Die Mehrheit der erhobenen Topics beinhaltet konkrete typische Themen und
                  Motive des spanischsprachigen Romans der Epoche. Man erkennt eine klare
                  semantische Beziehung der Wörter: ein konkreter Bereich menschlicher
                  Tätigkeiten, wie in Topic 19 (maestro-colegi o-escuela, dt.
                  "Lehrer-Schule-Schule") oder Topic 23 (sangre-golpe-arma, dt.
                  "Blut-Schlag-Waffe"); oder abstrakte Begriffe und Gefühle, wie bei Topic 69
                  (conciencia-honor-crimen, dt. "Gewissen-Ehre-Verbrechen"). Weniger kohärent
                  ist Topic 45 (marido-rato-chico, dt. "Ehegatte-Weile-Junge"). Die folgenden
                  Wordclouds (Abbildung 2) veranschaulichen die erwähnten Topics.
               
                  
                  
                     Abb. 2: Wordclouds für ausgewählte Topics.
               
            
            
               Untergattungen und Topics
               Die folgende Heatmap (Abbildung 3) zeigt die Verteilung der
                    durchschnittlichen Topic-Wahrscheinlichkeiten in den vier Untergattungen für
                    diejenigen 20 Topics, deren Werte zwischen den Untergattungen besonders
                    stark schwanken (nach Standardabweichung). Besonders distinktive Topics
                    existieren für die novela de tendencia subjetiva
                    (Topic 11: mirada-huerto-silencio, dt. "Blick-Garten-Stille") und die novela sentimental (Topic 45). Wenig überraschend
                    auch, dass die novela histórica als distinktiven
                    Topic unter anderem Topic 57 hat (rey-caballero-príncipe, dt.
                    "König-Ritter-Prinz"). Für die novela
                    histórico-social, für die aufgrund der großen Zahl von Beispielen
                    eine größere Bandbreite an Topic-Verteilungen zu erwarten ist, gibt es
                    keinen vergleichbar stark distinktiven Topic. Dennoch sind die
                    Untergattungen ein wichtiger Faktor für die Verteilung der Topics in der
                    Sammlung und die thematische Komponente spielt für die Definition der
                    Untergattungen tatsächlich eine wesentliche Rolle. 
               
                  
                  
                     Abb. 3: Verteilung von Topic-Scores nach Untergattungen.
               
            
            
               Topics im Textverlauf
               Die Ausprägung der Topics variiert nicht nur hinsichtlich der Untergattungen,
                      sondern auch über den Textverlauf hinweg. So gibt es einige Topics, deren
                      Vorkommen am Anfang der Romane besonders wahrscheinlich ist (Abbildung 4a).
                      Dazu zählen Topic 10 (vino-plato-pan, dt. "Wein-Teller-Brot"), Topic 17
                      (sombrero-ropa-bota, dt. "Hut-Kleidung-Stiefel") und Topic 19, welche auf
                      die Beschreibung von Ambiente, Situation und Personen hindeuten. Gegen Ende
                      der Romane sind andere Topics wahrscheinlicher (Abbildung 4b), z. B. Topic 2
                      (pecado-caridad-conciencia, dt. "Sünde-Wohltätigkeit-Gewissen"), Topic 23
                      und Topic 69, also abstraktere Themen oder solche, die sich auf
                      Wertvorstellungen beziehen. Dies deutet darauf hin, dass in den Romanen am
                      Ende Bilanz gezogen wird, die Handlung einen drastischen Ausgang nimmt oder
                      das im Textverlauf Behandelte in gesellschaftliche oder religiöse Diskurse
                      eingebunden wird.
               
                  
                  
                     Abb. 4a: Verteilung von Topics im Textverlauf (fallend).
               
               
                  
                  
                     Abb. 4b: Verteilung von Topics im Textverlauf (steigend).
               
            
            
               Textverlauf abhängig von den Untergattungen
               Für einige der genannten Topics, die in bestimmten Bereichen des Textverlaufs
                        wahrscheinlicher sind, kann die Tendenz über alle Untergattungen hinweg
                        bestätigt werden (bspw. bei Topic 10 und 17, siehe oben). Es gibt aber auch
                        Themen, bei denen sich durch die Betrachtung des Verlaufs in den einzelnen
                        Untergattungen ein differenzierteres Bild ergibt. Die Wahrscheinlichkeit von
                        Topic 23 beispielsweise nimmt nur für die novela
                        político-social zum Ende hin zu (Abbidung 5a): 
               
                  
                  
                     Abb. 5a: Topic 23 nach Textverlauf und Untergattung.
               
               Das kann so interpretiert werden, dass die novela
                          político-social im Gegensatz zu den anderen Untergattungen dazu
                          tendiert, am Ende des Textes mit einer gewalttätigen Szene und einem Umbruch
                          zu schließen. Topic 19 ist nicht in allen Untergattungen zu Beginn des
                          Textverlaufes stark ausgeprägt, sondern nur bei der novela
                          de tendencia subjetiva. Dies erklärt sich, weil bei diesen Romanen
                          das Schulthema als Teil einer autofiktionalen Erzählung zu Beginn erscheint
                          (Abbildung 5b): 
               
                  
                  
                     Abb. 5b: Topic 19 nach Textverlauf und Untergattung 
               
               Allgemein gilt, dass die Untergattungen sich in ihrer Topicverteilung im Textverlauf auch dann deutlich unterscheiden können, wenn dies für alle Untergattungen zusammengenommen nicht der Fall ist und so leicht übersehen werden könnte.
               Für die Berechnung wurden die Romansegmente von 600 Wörtern bezüglich des Textverlaufs auf 15 Romanabschnitte (Bins) verteilt, um die unterschiedliche Romanlänge zu berücksichtigen. Diese Bins wurden hinsichtlich der Untergattung gruppiert und jeweils das arithmetische Mittel bestimmt. Die in den Plots eingezeichneten Kurven entsprechen der linearen Interpolation dieser gemittelten Werte. Zusätzlich wurde der Standardfehler vertikal um den jeweiligen Kurvenpunkt eingezeichnet, der deutlich macht, wie sehr die jeweiligen dem Mittelwert zugrunde liegenden Werte streuen, also wie gut der Mittelwert die Gesamtheit der Segmentwerte repräsentiert.
            
         
         
            Die Ergebnisse im literaturgeschichtlichen Kontext
            Insgesamt zeigen sich verschiedene Zusammenhänge: Zwischen bestimmten Topics und einzelnen Roman-Untergattungen, zwischen Topics und dem Textverlauf, und dies zum Teil dann auch wieder in Abhängigkeit von den Untergattungen. Aus literaturgeschichtlicher Perspektive betrachtet erweisen sich die in die Untersuchung einbezogenen Metadaten für eine Einordnung der Topic-Resultate als nützlich. Topics sind für die Romangattungen im vorliegenden Korpus ein wichtiger Faktor, ähnlich wie dies für Gattungen wie die klassische Komödie und Tragödie bereits gezeigt werden konnte (Schöch 2015).
            Ein detaillierterer Blick zeigt beispielsweise Folgendes: Topic 11, welches typisch für die
                          novela de tendencia subjetiva ist, ist vor allem in den 1910er- und 1920er-Jahren wichtig sowie für bestimmte Autoren. Interessanterweise ist dieses bei spanischen und hispanoamerikanischen Modernisten vorkommende Thema auch bei der früher wirkenden Schriftstellerin Juana Manuela Gorriti schon wichtig, die offenbar thematische Präferenzen späterer Autoren vorweggenommen hat. Außerdem kommt Topic 11 bei Larreta in einem (modernistischen) historischen Roman vor, obwohl es ansonsten vor allem für die Romane subjektiver Tendenz typisch ist. Es ist anzunehmen, dass für dieses spezielle Thema eher die literarische Strömung bestimmend ist als die Untergattung. Der Topic enthält einige für die modernistische Strömung typische Wörter, etwa zu Sinneseindrücken (azul, dt. "blau", olor, dt. "Geruch") und Zurückgezogenheit (huerto, silencio, campo, soledad, dt. "Garten, Ruhe, Land, Einsamkeit").
                        
         
         
            Fazit und Ausblick
            Die Nutzung von Topic Modeling als Methode kann für die digitale Literaturwissenschaft verbessert werden, wenn spezifisch literaturwissenschaftliche Metadaten in die Betrachtungen einbezogen werden und die Textstruktur - hier als Sequenz von Textverlaufseinheiten - berücksichtigt wird. Verschiedene Visualisierungsstrategien erweisen sich als entscheidende "Interfaces" zu den Daten (im Sinne von Doueihi 2012), die Muster sichtbar machen und den Blick lenken. Die Ergebnisse des Topic Modelings können differenzierter und aus verschiedenen Perspektiven betrachtet und mit literaturhistorischem Wissen in Verbindung gebracht werden. Die Ergebnisse ergänzen und erweitern etablierte hermeneutische Lektürestrategien, insofern sie einen synthetisierenden Blick auf sehr umfangreiche Textsammlungen erlauben.
            Nächste Schritte betreffen insbesondere die weitere Auseinandersetzung mit der
                          Signifikanz von Unterschieden in den Topic-Wahrscheinlichkeiten im Textverlauf,
                          deren Berechnung u. a. durch die mangelnde Normalverteilung der Werte nicht
                          trivial ist. Zusätzlich zu den Untergattungen sollen auch Kategorien wie das
                          Setting modelliert werden. Zudem sollen die Textverlaufs-Daten für die
                          automatische Klassifikation von Romanen nach Untergattungen genutzt werden.
                          Schließlich wird bereits an der Erweiterung der Textsammlung gearbeitet,
                          insbesondere mit Blick auf den Umfang und ein ausgeglicheneres Verhältnis der
                          Untergattungen.
         
      
      
         
            
               Bibliography
               
                  Altisent, Marta E. (2008): A
                            Companion to the Twentieth-Century Spanish Novel. Woodbridge:
                            Tamesis. 
               
                  Blei, David M. (2011): “Introduction to Probabilistic
                            Topic Models,” in:  Communication of the ACM. 
               
                  Blei, David M. / Ng, Andrew Y. / Jordan, Michael I.
                            (2003): “Latent Dirichlet Allocation,” in:  Journal of
                            Machine Learning Research 3: 993–1022. 
               
                  Blevins, Cameron (2010): “Topic Modeling Martha
                              Ballard’s Diary,” in:  Historying
                  http://historying.org/2010/04/01/topic-modeling-martha-ballards-diary/
                                [letzter Zugriff 16. Februar 2016]. 
               
                  Doueihi, Milad (2012):  Pour un
                                humanisme numérique (2011). Paris: Seuil. 
               
                  Gallo, Marta (1981):  La Novela
                                Hispanoamericana En El Siglo XIX. Madrid: La Muralla. 
               
                  García de Nora, Eugenio (1963):  La
                                Novela Española Contemporánea. Madrid: Gredos. 
               
                  Jockers, Matthew L. (2013):  Macroanalysis - Digital Methods and Literary History. Champaign,
                                IL: University of Illinois Press. 
               
                  Jockers, Matthew L. (2015): “Revealing Sentiment and
                                Plot Arcs with the Syuzhet Package” in:  Matthew. L.
                                Jockers
                  http://www.matthewjockers.net/2015/02/02/syuzhet/ [letzter
                                  Zugriff 09. Februar 2016]. 
               
                  McCallum, Andrew K. (2002): MALLET:
                                  A Machine Learning for Language Toolkit
                  http://mallet.cs.umass.edu
                                  [letzter Zugriff 09. Februar 2016]. 
               
                  Nachwuchsgruppe CLiGS (o.J.): Computergestützte literarische Gattungsstilistik
                  http://cligs.hypotheses.org/ [letzter Zugriff 16. Februar
                                    2016].
               
                  Rhody, Lisa M. (2012): “Topic Modeling and Figurative
                                    Language,” in:  Journal of Digital Humanities 2  http://journalofdigitalhumanities.org/2-1/topic-modeling-and-figurative-language-by-lisa-m-rhody
                                    [letzter Zugriff 09. Februar 2016]. 
               
                  Schmidt, Benjamin M. (2014): “Typical TV Episodes:
                                    Visualizing Topics in Screen Time,” in:  Sapping
                                    Attention
                  http://sappingattention.blogspot.de/2014/12/typical-tv-episodes-visualizing-topics.html
                                      [letzter Zugriff 09. Februar 2016]. 
               
                  Schöch, Christof (2015): “Topic Modeling Genre: An
                                      Exploration of French Classical and Enlightenment Drama [submitted]”, in: 
                                      Digital Humanities Quarterly. 
               
                  Williams, Raymond L. (2009):  The
                                      Twentieth-Century Spanish American Novel. Austin, Texas: University
                                      of Texas Press. 
            
         
      
   


10570	2016	
      
         
            Ansatz 
            Neben dem ›klassischen‹ strukturalistischen Paradigma, das sich wesentlich an
          Theoremen der Linguistik orientiert (u. a. Lotman 1972; Titzmann 1977), gibt es
          in der Literaturwissenschaft bereits seit Jahrzehnten Ansätze zu einer
          Strukturanalyse, die sich auf die empirische Soziologie – insbesondere auf die
          Social Network Analysis – bezieht und Struktur
          entsprechend nicht über basale semantische Relationen (etwa als Opposition oder
          Äquivalenz) definiert, sondern über soziale Interaktionen (Marcus 1973; Stiller
          et al. 2003; de Nooy 2006; Stiller / Hudson 2005; Elson et al. 2010; Agarwal et
          al. 2012). Im Kontext der Digital Humanities haben diese Ansätze zu einer
          literaturwissenschaftlichen Netzwerkanalyse (Trilcke 2013) in den letzten Jahren
          eine neue Dynamik gewonnen (Moretti 2011; Rydberg-Cox 2011; Park et al. 2013).
          Aus literaturwissenschaftlicher Sicht versprechen diese Analyseverfahren dabei
          auf umfangreichen Korpora basierende, von quantitativen Daten gestützte
          Erkenntnisse über die Literaturgeschichte wie auch über die generischen
          Eigenarten literarischer Texte. Im Projekt dlina. Digital
          Literary Network Analysis haben wir einen Workflow zur Extraktion,
          Analyse und Visualisierung von Netzwerkdaten aus dramatischen Texten mit
          rudimentärer TEI-Auszeichnung entwickelt (Fischer et al. 2015). Der hier
          projektierte Vortrag wird Ergebnisse der netzwerkanalytischen Auswertung dieser
          Daten präsentieren und vor dem Hintergrund etablierter fachwissenschaftlicher
          Fragestellungen diskutieren. 
         
         
            Datenerhebung und -analyse
            Unser derzeitiges Korpus umfasst 465 deutschsprachige Dramen (Zeitraum 1730 bis
            1930), die aus dem Textgrid
            Repository extrahiert wurden. Die für die Netzwerkanalyse relevanten
            Strukturdaten dieser Dramen (Segmentierung, Figurenidentifikation) wurden in
            einem regelbasierten Prozess händisch ediert, um OCR- und TEI-Tagging-Fehler zu
            beheben sowie solchen ›Eigenarten‹ der literarischen Texte zu begegnen, die die
            Analyseergebnisse verfälschen würden (u. a. unterschiedliche Bezeichnungen
            identischer Figuren; Bezeichnung von Figurengruppen mit unbestimmten Numeralien
            wie ›beide‹ oder ›alle‹; etc.). Die edierten Strukturdaten liegen in einem
            eigens entwickelten Datenformat, dem dlina-Format, in Form von XML-Dateien vor.
            Die Visualisierung der Netzwerke und die Berechnung netzwerkanalytischer Werte
            erfolgt – mittels Python- und D3-Skripten – automatisiert auf Basis der in den
            dlina-Dateien gespeicherten Strukturdaten. Neben Graphen und basalen Werten, die
            die Netzwerke global beschreiben (Network Size, Density, Average Degree, Average
            Path Length), werden dabei auch Zentralitätswerte für sämtliche Figuren eines
            Dramas erhoben (u. a. Degree, Average Distance, Closeness Centrality,
            Betweenness Centrality). Die Implemtierung weiterer Berechnungsrountinen (u. a.
            Clustering Coefficient, logarithmierte Degree Distribution-Tabellen) ist für den
            Winter 2015/16 vorgesehen. Sämtliche Daten und Visualisierungen werden frei
            verfügbar im Netz publiziert (https://github.com/dlina und https://dlina.github.io/linas/). 
         
         
            Literaturwissenschaftliche Auswertung 1: Dramengeschichte 
            Die diachrone Erstreckung unseres Dramenkorpus über ca. 200 Jahre deutscher
              Literaturgeschichte macht es möglich, größere Entwicklungen im Bereich der
              strukturellen Komposition von dramatischen Texten zu beobachten (erste
              Überlegungen dazu haben wir in einem Blogpost skizziert: https://dlina.github.io/200-Years-of-Literary-Network-Data/). Neben
              Werten, die sich auf die Gesamtnetzwerke der einzelnen Dramen beziehen (u. a.
              Network Size, Density, Average Degree; s. exemplarisch zur Average Path Length,
              Abbildung 1), werden dabei auch figurenbezogene Werte, v.a. Zentralitätsmaße,
              einbezogen, die Aufschluss etwa über die Streuung des Personals eines Dramas
              bzw. dessen Zusammensetzung aus ›zentralen‹ und weniger ›zentralen‹ Figuren
              gibt. Auf Grundlage dieser Werte sollen im Vortrag einige globale Thesen der
              Literaturgeschichte diskutiert werden. So werden wir erstens diskutieren, inwieweit sich anhand der netzwerkanalytischen
              Werte eine Ausdifferenzierung der strukturellen Komposition von dramatischen
              Texten am Ende des 18. Jahrhunderts beobachten lässt: Eine solche
              Ausdifferenzierung wäre angesichts des Nebeinanders von ›geschlossenen‹, in der
              Tradition der Französischen Klassik stehenden Dramen und ›offenen‹ Dramen, die
              sich u. a. an der Drramatik Shakespeares orientieren, zu erwarten. Zweitens werden wir einige geläufige
              literaturwissenschaftliche Periodisierungshypothesen testen (u. a. aus dem
              Strukturalismus und der Sozialgeschichte); gefragt werden soll hier, inwieweit
              die Entwicklung der netzwerkanalytischen Werte mit den von der Forschung
              vorgeschlagenen Periodisierungen korreliert. 
            
               
               
                  Abb. 1: Average Path Length (Mean; nach Dekaden) 
            
         
         
            Literaturwissenschaftliche Auswertung 2: Dramentypen
            Die von uns bisher erhobenen Werte zeigen, dass Dramen in dem untersuchten
                  Zeitraum auf sehr unterschiedliche Weise strukturiert wurden. In der
                  ›traditionellen‹ Literaturwissenschaft wurden für solche unterschiedlichen
                  ›Bauformen‹ diverse Typologien entwickelt, in der Germanistik am bekanntesten
                  ist Volker Klotz’ Unterscheidung in eine ›offene‹ und eine ›geschlossen‹
                  Dramenform (Klotz 1960). Diesen typologischen Impuls wollen wir aufgreifen und
                  einen Vorschlag unterbreiten, wie sich mittels netzwerkanalytischer Daten
                  bestimmte Typen der strukturellen Komposition von Dramen unterscheiden (und dann
                  wiederum historisch verorten) lassen. Unser Vorschlag greift dabei Überlegungen
                  aus der Forschung zu sog. Small-world-Netzwerken auf. Diese Forschungen setzen
                  bei der Beobachtung an, dass die Werte von empirisch erhobenen Netzwerken nicht
                  selten signifikant von entsprechenden Random-Netzwerken (also z. B. nach dem
                  Erdős-Rényi-Modell erstellten Graphen) abweichen. Abweichungen sind dabei
                  insbesondere beim Clustering Coefficient, bei der Averge Path Length sowie bei
                  der Degree Distribution zu beobachten (Albert / Barabási 2002). Für den hier
                  projektierten Vortrag werden wir diese Werte – sowie die Werte für die
                  entsprechenden Random-Netzwerke – für unser Gesamtkorpus erheben (sowie einen
                  Workflow für die automatisierte Erhebung entwickeln) und diskutieren. Erste
                  Testläufe deuten dabei darauf hin, dass sich auf diese Weise tatsächlich
                  unterschiedliche Typen der strukturellen Komposition von Dramen beschreiben
                  lassen könnten. So zeigen sich z. B. auffällige Unterschiede bei der Degree
                  Distribution (s. exemplarisch die Tabellen für vier Dramen in Abbildung 2); und
                  mit Blick auf den Clustering Coefficient zeigt sich, dass im Vergleich zu
                  Random-Netzwerken signifikant höhere Werte, wie sie bei Small-world-Netzwerken
                  zu erwarten sind, zwar in mehreren Fällen vorkommen, jedoch keineswegs für alle
                  Dramennetzwerke charakteristisch sind (siehe exemplarisch die Werte in Abbildung
                  3). Im Vortrag werden wir diese Werte für alle Dramen unseres Korpus
                  präsentieren; wir werden diskutieren, inwieweit sich hier – aufbauend auf dem
                  Small-world-Konzept – netzwerkanalytisch basierte Typen der strukturellen
                  Komposition von Dramen unterscheiden lassen und wir werden literarhistorisch
                  fundiert erörtern, welche Eigenschaften der Dramen für die unterschiedlichen
                  Werte verantwortlich sind.
            
               
            
            
               
            
            
               
            
            
               
               
                  Abb. 2.1 bis 2.4: Node Degree Distribution für »Der
                    sterbende Cato« (1731), »Emilia Galotti« (1772), »Götz von Berlichingen« (1773)
                    und »Die Räuber« (1781) 
            
            
               
               
                  Abb. 3: Vergleich des Clustering Coefficent des
                      Dramen-Netzwerks mit dem eines jeweils entsprechenden Random-Netzwerks 
            
         
      
      
         
            
               Bibliographie
               
                  Albert, Réka / Barabási, Albert-László (2002):
                      "Statistical mechanics of complex networks", in: Reviews
                      of Modern Physics 74: 47–97. 
               
                  Agarwal, Apoorv / Corvalan, Augusto / Jensen, Jacob /
                        Rambow, Owen (2012): "Social Network Analysis of Alice in Wonderland" in: Proceedings of the
                        Workshop on Computational Linguistics for Literature. Montréal
                        88–96. 
               
                  de Nooy, Wouter (2006): "Stories, Scripts, Roles, and
                        Networks" in: Structure and Dynamics 1, 2 http://escholarship.org/uc/item/8508h946#page-1 [letzter Zugriff
                        12. Oktober 2015]. 
               
                  Elson, David K. / Dames, Nicholas / McKeown, Kathleen R.
                        (2010): "Extracting Social Networks from Literary Fiction", in: Proceedings of the 48th Annual Meeting of the Association
                        for Computational Linguistics. Uppsala 138–147. 
               
                  Fischer, Frank / Kampkaspar, Dario / Göbel, Mathias /
                          Trilcke, Peer (2015): "Digital Network Analysis of Dramatic Texts",
                          in: DH 2015, Sydney, 2. Juli 2015
                  https://dlina.github.io/Our-Talk-at-DH2015/ [Skript] und https://dlina.github.io/presentations/2015-sydney/sydney.html#/
                            [Slides] [letzter Zugriff 12. Oktober 2015]. 
               
                  Klotz, Volker (1960): Geschlossene
                            und offene Form im Drama. München: Hanser. 
               
                  Lotman, Jurij M. (1972): Die
                            Struktur literarischer Texte. München: Wilhelm Fink. 
               
                  Marcus, Solomon (1973): Mathematische Poetik. Frankfurt am Main: Editura Academiei. 
               
                  Moretti, Franco (2011): "Network Theory, Plot
                            Analysis" in: Stanford Literary Lab Pamphlets 2 http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf [letzter
                            Zugriff 12. Oktober 2015]. 
               
                  Park, Gyeong-Mi / Kim, Sung-Hwan / Cho, Hwan-Gue
                              (2013): "Structural Analysis on Social Network Constructed from Characters
                              in Literature Texts", in: Journal of Computers 8, 9:
                              2442-2447 http://ojs.academypublisher.com/index.php/jcp/article/view/jcp080924422447/7672
                              [letzter Zugriff 12. Oktober 2015]. 
               
                  Rydberg-Cox, Jeff (2011): "Social Networks and the
                              Language of Greek Tragedy", in: Journal of the Chicago
                              Colloquium on Digital Humanities and Computer Science 1, 3 https://letterpress.uchicago.edu/index.php/jdhcs/article/view/86/91
                              [letzter Zugriff 12. Oktober 2015]. 
               
                  Stiller, James / Nettle, Daniel / Dunbar, Robin I. M.
                              (2003): "The Small World of Shakespeare's Plays", in: Human Nature 14: 397–408. 
               
                  Stiller, James / Hudson, Matthew (2005): "Weak Links
                              and Scene Cliques Within the Small World of Shakespeare", in: Journal of Cultural and Evolutionary Psychology 3:
                              57–73. 
               
                  TextGrid: TextGrid Repository
                  https://textgridrep.de [letzter
                              Zugriff 10. Februar 2016].
               
                  Titzmann, Michael (1977): Strukturale Textanalyse. Theorie und Praxis der Interpretation.
                              München: Wilhelm Fink. 
               
                  Trilcke, Peer (2013): "Social Network Analysis (SNA)
                              als Methode einer textempirischen Literaturwissenschaft", in: Ajouri, Philip
                              / Mellmann, Katja / Rauen, Christoph (eds.): Empirie in
                              der Literaturwissenschaft. Münster: mentis 201–247. 
            
         
      
   


10583	2017	
      
         Das Panel wird durch vier Kurzvorträge in Probleme der Nutzung digitaler Werkzeuge für nicht-indoeuropäische Sprachen einführen. Die Vorträge basieren auf Erfahrungen aus langfristig angelegten Projekten und haben jeweils individuelle Lösungen für die spezifischen Anforderungen gefunden, die hauptsächlich durch die Sprache formuliert werden.
         Allen Projekten gemein sind Probleme der Nutzung von digitalen Werkzeugen, die insbesondere bei der Verwendung von Quellen historischer oder wenig erforschter Sprachen auftreten. 
         Aktuelle Content Management Systeme und Annotationstools wurden selten im Hinblick auf Anforderungen aus Orchideenfächern entwickelt. Dies betrifft beispielsweise einige Sprachen mit nichtkonkatenativer Morphologie oder komplexen Schriftsysteme. Daher müssen für erwähnte Sprachen entweder existierende Anwendungen angepasst oder erschaffen werden.
         Bei der Adaption können während der Modellierung wichtige Eigenschaften nicht berücksichtigt werden oder bleiben nur als Kommentar erhalten, was eine weitere maschinelle Bearbeitung erschwert. Bezüglich der Datenkodierung ergibt sich das Problem der Ineffizienz. So wurden morphologische Tagsets primär für die indo-europäische Sprachfamilie entwickelt. Für eine tiefe linguistische Annotation müssen aber diese Standards beispielweise für einige semitische Sprachen angepasst werden. 
         Nicht selten ist die Alternative die Eigenentwicklung projektbezogener Lösungen, die aber aufgrund der Anforderungen mit eigenen Datenformaten arbeiten, und so nicht mehr den geltenden Standards folgen und den Austausch erschweren. Hinzu kommt der immense Zeit- und Ressourcenaufwand bei der Implementierung.
         Allerdings sind gerade im deutschsprachigen Raum viele langfristige Projekte auf digitale Tools angewiesen.
         Durch eine Vernetzung solcher Projekte können gemeinsame Anforderungen an, und Begrenzungen von aktuellen Lösungen besprochen und Initiativen zur Entwicklung digitaler Tools und Ressourcen koordiniert werden. Daher ist das Ziel dieses Panels eine erste Zusammenführung langfristig ausgerichteter Projekte im deutschen Sprachraum, die mit historischen nicht-indo-europäischen Sprachen im digitalen Kontext arbeiten. Dabei sollen die Probleme der Nachhaltigkeit entwickelter Werkzeuge und Ressourcen, sowie der bearbeiteten Daten besprochen werden. Anschließend werden die vielfältigen Herangehensweisen mit einem Fokus auf drei große Punkte diskutiert:
         
            Nachhaltigkeit von Repositorien
         
         
            Welche Frameworks werden für welche Datentypen benötigt?
            Wie können Informationen über unpräzise Daten gespeichert werden? 
            Wie gehen verfügbare Systeme mit Multilingualität um?
         
         
            Nachhaltigkeit von (Annotations-)Werkzeugen
         
         
            Analyse historischer Daten impliziert die Annotation von Textmaterialien in Sprachen, die aus verschiedenen Gründen zu Problemen führen.
            Welche Annotationstools können genutzt werden? Mit welchen Limitierungen?
            Was bedeutet es, ein neues Tool zu entwerfen?
            Häufige Anforderungen durch strukturell komplexe Sprachen: Multilevel-Annotation, Textkorrektur während der Annotationsphase, Multilevel-Segmentierung
         
         
            Nachhaltigkeit des annotierten Materials (Standards)
         
         
            Während der Standard TEI-XML als Schnittstellenformat sehr nützlich ist, ergeben sich dennoch Probleme wie:
                    
                  für interne Verarbeitung kann dessen Verwendung hinderlich sein. Daher müssen projekt-spezifische Lösungen mit standardisiertem Export entwickelt werden.
                  Können diese Daten von Dritten in TEI-XML verarbeitet werden?
                  Welche anderen Formate können genutzt werden (z.B. JSON)?
                  Sind existierende Tagset-Formate ausreichend spezifiziert, um auch nicht-europäische Sprachen taggen zu können?
               
            
         
         
            Herausforderungen in der Nutzung vorhandener Tools für arabische Daten
            Alicia González, Tillmann Feige
            Universität Hamburg
            ERC- Projekt COBHUNI (
                    )
                
            Email: 
                    alicia.gonzalez@uni-hamburg.de; 
                    tillmann.feige@uni-hamburg.de
            
            Wir beschreiben den Ansatz, einen Korpus der neben modernem auch klassisches Arabisch (siehe Romanov, 2016) enthält, mit computerlinguistischen und semantischen Verfahren analysierbar zu machen. Wir setzen auf bereits vorhandene Software für die Hauptpunkte Annotation und Analyse. Dazu wurde ein Pflichtenheft erstellt, dass mit vorhandenen Tools abgeglichen wurde.
            Da wir mit arabischen Daten arbeiten, ist eine große Herausforderung die Schrift. Es ist eine linksläufige verbundene Schrift, die durch Konsonanten und lange Vokale repräsentiert wird. Kurze Vokale sind Diakritika, die optional gesetzt werden und gerade bei Referenzen auf religiöse Quellen im Textkorpus vorkommen. Dabei ist vollständige UTF-8 Unterstützung und die saubere Darstellung der Schrift unabdingbar. Dies reduziert die Auswahl erheblich. Hinzu kommt, dass wir auf flexible Import- und Exportmöglichkeiten angewiesen sind. Ähnliche Probleme führen Peralta und Verkinderen auf (Peralta / Verkinderen 2016). Durch unsere Herangehensweise gibt es weitere Einschränkungen wie Mehrebenen-, Multitoken- aber auch Subtoken-Annotation.
            Die Auswahl für die semantische Annotation fiel auf WebAnno, dass durch sein spezielles Datenmodell die erforderliche Datenaufbereitung und Kontrolle gestattet.
            Als Visualisierungstool haben wir ANNIS ausgewählt, dass ebenfalls Arabisch unterstützt, einen konfigurierbaren Converter mitbringt und Mehrebenenkorpora erlaubt, so dass auch hier die Hauptkriterien erfüllt wurden. Zusätzlich lassen sich potentielle Probleme in der Darstellung durch eine anpassbare HTML-Visualisierung umgehen. Durch Zusammenarbeit mit den Entwicklern beider Programme wurde die Unterstützung für Arabisch stetig ausgebaut.
            Im Beitrag werden wir die einzelnen Punkte erläutern und darstellen, warum wir uns für die angeführten Programme und gegen eine Eigenentwicklung entschieden haben, sowie welche Implikationen diese Entscheidung für die Nachhaltigkeit des Projekts, der Daten und der genutzten Tools hat.
         
         
            Tiefe Mehrebenen-Annotation für semitische Sprachen: der Fall von Ge'ez
            
               Cristina Vertan
            
            Universität Hamburg
            ERC-Projekt TraCES (
                    )
                
            Email: 
                    cristina.vertan@uni-hamburg.de
            
            Das südsemitische Gәʿәz ist die Sprache des Königreichs Aksum in der heutigen nordäthiopischen Provinz Tigray, von wo aus die im 4. Jahrhundert beginnende Christianisierung Äthiopiens ihren Anfang nahm. Die in der Folge entstehende reiche Literatur ist in großem Umfang geprägt von Übersetzungen, was durch grammatische Interferenzphänomene reflektiert wird. Das Altäthiopische hat aus einer südsemitischen Schrift ein eigenes Silbenalphabet entwickelt, das bis heute in mehreren modernen Sprachen Äthiopiens und Eritreas Verwendung findet. Innerhalb der semitischen Sprachen fällt es durch die verwendete Rechtsläufigkeit auf; außerdem werden die Vokale vollständig geschrieben. Beides unterscheidet das Gәʿәz von verwandten Sprachen wie Altsüdarabisch, Arabisch, Hebräisch und Syro-Aramäisch Mit den genannten eng verwandten semitischen Sprachen teilt das Altäthiopische die nichtkonkatenative Morphologie. Durch das äthiopische Silbenalphabet sind Morphemgrenzen in der Schrift nicht darstellbar, so dass beispielsweise ein einzelner Vokal als Bestandteil einer Silbe eine eigenständige Wortart darstellt und tokenisiert werden muss.
            Die Komplexität des Annotationstools wird sehr vielfältige linguistische Anfragen und detaillierte Analysen der Sprache ermöglichen, aber auch eine vollautomatische Annotation verhindern. Ein alle morphologischen Merkmale abdeckendes Vektorraum-Modell (das für maschinelle Lernverfahren benutzt werden muss) wäre zu groß. Vorstellbar ist lediglich eine flache automatische Annotation (z. B. der Wortarten); jedoch wird auch für eine solche zunächst eine relativ große Menge an Trainingsdaten benötigt. Daher ist die Entwicklung eines Werkzeugs für die manuelle Annotation ein obligatorischer Schritt.
            Die Besonderheit der entwickelten Lösung (Vertan/Ellwardt/Hummel 2016) sind:
            
               automatische Transkription
               manuelle Korrektur der Transkription während des Annotationsprozesses
               semi-automatische Verfahren: automatische Verläufe werden farbig markiert und sind automatisch zur manuellen Korrektur hinterlegt
               Mehrebenenannotation: Linguistik, Edition, Textstruktur
               Anpassungen an unterschiedliche Schriftsysteme und Transkriptionsregeln
            
         
         
            Nutzungs- und Nachhaltigkeitsstategien im Projekt "Textdatenbank und Wörterbuch des Klassischen Maya"
            Christian M. Prager
            NRW Akademie der Wissenschaften und der Künste
            
               
            
            Email: 
                    
            
            Die Mayaschrift ist das einzig lesbare Schriftsystem der vorspanischen Amerikas. Die über 10.000 Texte sind in einer logographisch-syllabischen Hieroglyphenschrift verfasst und von den rund 800 Zeichen sind erst 60% sicher entziffert. Die Texte enthalten taggenaue Kalenderangaben, die es uns ermöglichen die rund 2000jährige Sprach- und Schriftgeschichte genau zu dokumentieren. Das Projekt (Prager 2015) wird sämtliche Inschriften einschließlich Metadaten in einer Datenbank einzupflegen und darauf basierend ein digitales Wörterbuch des Klassischen Maya zu kompilieren. Herausforderung dabei ist, dass die Schrift noch nicht vollständig entziffert ist und bei der Modellierung zu berücksichtigen ist. Unser Projekt verfolgt den Ansatz, wonach die Bedeutung von Wörtern ihre Verwendung ist - Texte nehmen Bezug auf den Textträger und den Verwendungskontext und nur die exakte Dokumentation sogenannter nicht-textueller Informationen erlaubt es, textuelle und nicht-textuelle Informationsbereiche zueinander in Beziehung zu setzen und bei der Entzifferung von Zeichen und Textstellen zu berücksichtigen. Zum Zweck der Nachhaltigkeit und Nachnutzung greift das Projekt bei der Beschreibung der Artefakte und der relevanten objektgeschichtlichen Ereignisse auf CIDOC CRM zurück, das eine erweiterbare Ontologie für Begriffe und Informationen im Bereich des kulturellen Erbes anbietet. Das entstandene Anwendungsprofil wird durch Elemente aus weiteren Standards und Schemata angereichert und wird damit auch für vergleichbare Projekt nachnutzbar. Die Schemata und erstellten Metadaten werden in einer Linked (Open) Data-Struktur (LOD) abgebildet. Durch die Repräsentation im XML-Format, sowie die Nutzung von HTTP-URIs wird eine einfache Austauschbarkeit und Zitierbarkeit der Daten ermöglicht. Durch diese Umsetzung können Objektmetadaten getrennt vom erfassten Text gespeichert werden und durch die Verwendung der HTTP-URI verlinkt werden. Die Nachnutzung bereits bestehender und fachlich anerkannter Terme trägt darüberhinaus auch zu einer hohen Interoperabilität mit anderen Datenbeständen und Informationssystemen bei. Das ausgestaltete Schema hat eine ontologisch-vernetzte Struktur, die komplexe Beziehungen und Zusammenhänge abbildet.
         
         
            Interdisziplinäre Digitale Zusammenarbeit für seltene Sprachen und Kulturen
         
         
            - Eine Fallstudie über jiddische Texte aus der frühen Neuzeit -
            
               Walther v. Hahn (Universität Hamburg), Berndt Strobach (Wolffenbüttel)
            
            Email: 
                    vhahn@informatik.uni-hamburg.
               de, berndt.strobach@freenet.de>
                
            In den Geisteswissenschaften werden häufig die fachlichen Interpretationen und die sprachlichen Erklärungen von verschiedenen Gruppen mit unterschiedlicher Kompetenz bearbeitet. Gute Beispiele sind Studien zu Texten aus semitischen Sprachen, wobei, speziell bei historischen Dokumenten die historische oder geistes- und sozialgeschichtliche Würdigung von Forschern verfasst werden muss, die des Hebräischen, Arabischen, Aramäischen etc. nicht mächtig sind, die sprachwissenschaftlichen Forscher dagegen bei der Interpretation gelegentlich weniger engagiert bleiben. Extremfälle wie Studien über das Sephardische in Spanien (Ladino, Djudezmo) machen etwa solide Kenntnisse zumindest des Spanischen, Hebräischen, Türkischen, Griechischen und Italienischen zur Voraussetzung für seriöse hermeneutische Forschungsergebnisse.Wir berichten über Studien zu jiddischen Texten aus dem Wolffenbüttel des 18. Jahrhunderts, in denen die Rolle der "Hofjuden" und ihres kultur- und sozialgeschichtlichen Hintergrundes diskutiert wird. 
            Die Herausforderung einer interdisziplinären Zusammenarbeit zwischen Historikern, Sprachwissenschaftlern und Informatikern besteht darin,
            1. die Lesbarkeit der Originalquellen für alle Gruppenmitglieder sicher zu stellen (Invertierte Transkriptionen, Vokalisierung, Visualisierung), sowie 
            2. in der Gruppe eine gemeinsame Behandlung von Vagheit, Unsicherheit und Unbekanntem zu definieren, so dass die Unklarheiten in den einzelnen Forschungsstufen erhalten und im Endergebis sichtbar bleiben (Vagheits-Annotationen und vage Inferenzen). Heute werden derartige Unsicherheiten meist bereits in den Annotationen unterschlagen (von Hahn, 2016).
         
      
      
         
            
               Bibliographie
               
                  Hahn, Walther von (2016):
                        „Humanities meet Computer Science – Digital Humanities between Expectations and Reality“,
                        zu erscheinen in: von Hahn, Walter / Papadima, Liviu / Vertan, Cristina (eds.):
                        Humanities2020, New Trends in Education and Research. 
                        Bukarest: University of Bucharest Publishing House.
                    
               
                  Peralta, José Haro / Verkinderen, Peter (2016): 
                        „‚Find for me!‘: Building a Context-Based Search Tool Using Python“,
                        in: Muhanna, Elias (ed.):
                        The Digital Humanities and Islamic & Middle East Studies. 
                        Berlin: Walter de Gruyter GmbH 199–231. 
                    
               
                  Prager, Christian M. (2015): 
                        „Das Textdatenbank- und Wörterbuchprojekt des Klassischen Maya: Möglichkeiten und Herausforderungen digitaler Epigraphik“,
                        in: Neuroth, Heike / Rapp, Andrea / Söring, Sibylle (eds.):
                        TextGrid: Von der Community - für die Community: Eine Virtuelle Forschungsumgebung für die Geisteswissenschaften.
                        Glückstadt: Werner Holsbusch 105–124 
                        https://www.academia.edu/17957108/Das_Textdatenbank-_und_W%C3%B6rterbuchprojekt_des_Klassischen_Maya_M%C3%B6glichkeiten_und_Herausforderungen_ digitaler_Epigraphik.
                    
               
                  Romanov, Maxim (2016): 
                        Creating Frequency-Based Readers for Classical Arabic
                  http://maximromanov.github.io/2016/05-30.html [letzter Zugriff 1. Dezember 2016].
                    
               
                  Vertan, Cristina / Ellwardt, Andreas / Hummerl, Susanne (2016): 
                        „Ein Mehrebenen-Tagging-Modell für die Annotation altäthiopischer Texte“, 
                        in:
                        DHd 2016: Modellierung - Vernetzung - Visualisierung
                  http://www.dhd2016.de/abstracts/vorträge-061.html.
                    
            
         
      
   


10606	2017	
      
         Texte und ihre automatische Analyse stehen im Zentrum vieler Untersuchungen in den Digital Humanities, etwa zur Erforschung sprachlicher und kultureller Wandlungsprozesse (siehe etwa Michel u.a. (2011)) oder im Bereich der Stilometrie (siehe etwa Jannidis (2014)). Die automatische Analyse von Texten beinhaltet typischerweise eine Reihe zunehmend komplexer werdender Schritte, angefangen bei der Segmentierung von Sätzen und Wörtern (Leerzeichen sind kein hinreichendes Kriterium, vgl. 
                „New York“) über die syntaktische und semantische Analyse bis hin zu diskursstrukturellen und pragmatischen Analysen. Die für diese einzelnen Schritte nötigen sprachtechnologischen Komponenten sind oft, zumindest innerhalb einer Anwendungsdomäne, wiederverwendbar. Folglich gibt es mittlerweile eine Fülle von Software-Repositorien, die entsprechende computerlinguistische Komponenten sammeln, und Frameworks, die ihre Integration in sogenannte Pipelines, also funktionsbezogene sequenzielle Kombinationen von einzelnen Komponenten, erleichtern. Die dadurch ermöglichte Wiederverwendung von Komponenten ist im Sinne nachhaltiger Forschung, da diese so nicht mehrfach entwickelt werden müssen und der Software-Austausch zwischen Gruppen unterstützt wird.
            
         
            Uima
            (Unstructured Information Management Architecture)
            
               1
            
             ist ein solches Framework, das sowohl im akademischen Kontext (in Deutschland u.a. DKPro
                
               2
            
             (de Castilho & Gurevych, 2014) und 
                JCoRe
            
               3
             (Hahn u.a., 2016)) als auch in industriellen Anwendungen (etwa bei IBMs 
                Jeopardy Champion 
                Watson (Ferrucci u.a., 2010)) breite Verwendung findet (einen Vergleich unterschiedlicher Frameworks stellen Bank und Schierle (2012) an). 
                Uima ist 
                open source unter der 
                Apache-Lizenz verfügbar und unterstützt mehrere Programmiersprachen, wobei 
                Java in der Praxis eine dominierende Rolle zukommt. 
            
         Wir nutzen mit 
                JCoRe seit fast einem Jahrzehnt 
                Uima für computerlinguistische Problemstellungen in verschiedenen Domänen bzw. Sprachen und stellen die dabei entwickelten Komponenten öffentlich zur Verfügung. Aktuell arbeiten wir daran, unser ursprünglich für bio-medizinische Fragestellungen und englischsprachige Fachtexte entwickeltes Repositorium auf den DH-Bereich, primär für das Deutsche, zu erweitern. 
                JCoRe stellt nicht nur sprachtechnologische Komponenten zur Verfügung, sondern auch die dafür nötigen Modelle für verschiedene Domänen — denn vor allem die Erstellung dieser Modelle ist ein enorm zeit- und rechenintensiver Prozess, der zudem ein hohes Maß an computerlinguistischer Expertise verlangt. Um die Einstiegshürden für die Benutzung solcher Ressourcen zu senken, bieten wir Anleitungen und Beispiele zur deklarativen Erstellung von Textanalyse-Pipelines mit 
                Uima und haben zudem eine interaktive Anwendung entwickelt (Hahn u.a., 2016).
            
         Eine Vielzahl von existierenden Sprachanalyse-Komponenten und Repositorien kann über 
                Uima eingebunden werden, darunter auch einige, die nicht originär für das Framework entwickelt wurden, wie etwa das über 
                DKPro verfügbare Stanford 
                CoreNLP
            
               4
             (Manning u.a., 2014) oder 
                OpenNLP
            
               5
            . Während 
                Uima für den produktiven Einsatz entwickelt wurde, steht beim alternativen 
                Natural Language Toolkit (NLTK)
                
               6
             der Einsatz in der Lehre im Zentrum (Bird u.a., 2009). 
                Uima ist eher mit dem 
                General Architecture for Text Engineering (GATE) Framework (Cunningham u.a., 2011) vergleichbar, das aber ein „geschlossenes“ NLP-System repräsentiert, das exklusiv von den Entwicklern von 
                Gate verwaltet wird. Generell sind integrierte Frameworks vorteilhaft gegenüber Pipelines aus einzelnen Werkzeugen, die mittels Textdateien/-strömen kommunizieren, da nicht bei jedem Schritt zwischen verschiedenen Formaten konvertiert werden muss. Insbesondere werden die bei selbstständigen Werkzeugen verbreiteten 
                in-line-Annotationen (wie etwa 
                „das_Artikel Haus_Nomen“) vermieden, die sich oft als unübersichtlich und fehleranfällig erweisen.
            
         
            Uima und die anderen bisher genannten Frameworks sind primär für den Einsatz auf lokaler Rechner-Infrastruktur gedacht und somit nur bedingt mit Systemen wie 
                WebLicht
            
               7
             (Hinrichs u.a., 2010) vergleichbar, die als Webservice verschiedene dezentral verteilte Komponenten zusammenführen. Dadurch wird zwar der Einstieg in die Nutzung sprachtechnologischer Systeme erleichtert, jedoch sind derartige Systeme nicht für die Verarbeitung großer Datenmengen geeignet und es entsteht eine eher intransparente Abhängigkeit von fremder Infrastruktur. 
                Uima ist somit kein Konkurrent für 
                WebLicht, sondern ermöglicht es vielmehr, Komponenten zu entwickeln, die bei Bedarf auch (durch in 
                DKPro enthaltene Konverter) in 
                WebLicht eingebunden werden können.
            
         Im Kern ist 
                Uima für die sequentielle Anreicherung mit Metadaten ausgelegt. Die möglichen Annotationen werden frei über ein objektorientiertes Typensystem definiert (siehe etwa Hahn u.a., 2007). In 
                Uima wird zwischen Komponenten unterschieden, die Annotationen vornehmen 
                (Analysis Engines), und solchen, die Texte in das interne CAS 
                (Common Analysis System) Format konvertieren 
                (Collection Reader); letztere können dabei auch bereits im Ursprungstext kodierte Metadaten verarbeiten. Die ersten Komponenten, die im Rahmen der Erweiterung 
                JCoRes
                 um DH-Komponenten entstanden und öffentlich zugänglich gemacht wurden, sind ein solcher 
                Collection Reader, der die neuerdings vom 
                Deutschen Textarchiv
            
               8
             (Geyken, 2013) zur Verfügung gestellten Dateien mit TCF-
                
               9
             und 
                Dublin Core-Annotationen
                10 verarbeiten kann, sowie eine entsprechende Erweiterung unseres Typensystems. In der unmittelbaren Zukunft geplante Erweiterungen betreffen 
                Analysis Engines für Text- bzw. Wortsegmentierung und Wortartenerkennung (POS-Tagging) in historischen (literarischen) Texten.
            
         Wir möchten durch unseren Beitrag insbesondere diejenigen, die primär computerlinguistische 
                Anwendungen für Fragestellungen der Digital Humanities realisieren wollen (und damit meist keine computerlinguistischen 
                Entwicklungsinteressen verfolgen), anregen, sich aus dem breiten Fundus existierender Komponenten zu bedienen und diese durch den Einsatz des 
                Uima-Frameworks zu verbinden. Die dadurch implizit eingeführte Modularität erleichtert zudem die Durchführung von Funktionstests, die Anpassung an neue Domänen und darüber hinaus den Austausch mit anderen Forschenden 
                — allesamt Anforderungen an eine nachhaltige Software-Infrastruktur.
            
      
      
         
            
               https://uima.apache.org
            
            
               https://
               
                  DKPro
               
               .github.io
            
            
               http://julielab.github.io
            
            
               http://stanfordnlp.github.io/Core
               
                  NLP
               
            
            
               https://open
               
                  NLP
               
               .apache.org
            
            
               http://www.nltk.org
            
            
               https://weblicht.sfs.uni-tuebingen.de
            
            
               
            
            
               
            
            
               http://dublincore.org
            
         
         
            
               Bibliographie
               
                  Bank, Mathias / Schierle, Martin (2012):
                        „A survey of text mining architectures and the Uima standard“,
                        in: 
                        Proceedings of LREC 2012 3479–3486.
                    
               
                  Bird, Steven / Klein, Ewan / Loper, Edward (2009): 
                        Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit.
                        Sebastopol, CA: O'Reilly.
                    
               
                  de Castilho, Eckart R. / Gurevych, Iryna (2014):
                        „A broad-coverage collection of portable NLP components for building shareable analysis pipelines“,
                        in: 
                        OIAF4HLT 2014 – Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT @ COLING 2014 1–11.
                    
               
                  Cunningham, Hamish / Maynard, Diana / Bontcheva, Kalina (2011): 
                        Text Processing with GATE.
                        Murphys, CA: Gateway Press.
                    
               
                  Ferrucci, David A. / Brown, Eric / Chu-Carroll, Jennifer / Fan, James / Gondek, David C. / Kalyanpur, Aditya A. / Lally, Adam / Murdock, J. William / Nyberg 3rd, Eric H. / Prager, John M. / Schlaefer, Nico / Welty, Christopher A. (2010):
                        „Building Watson: An overview of the DeepQA project“,
                        in: 
                        AI Magazine 31 (3): 59–79.
                    
               
                  Geyken, Alexander (2013):
                        „Wege zu einem historischen Referenzkorpus des Deutschen: das Projekt Deutsches Textarchiv“,
                        in: 
                        Perspektiven einer corpusbasierten historischen Linguistik und Philologie 221–234.
                    
               
                  Hahn, Udo / Buyko, Ekaterina / Tomanek, Katrin / Piao, Scott / McNaught, John / Tsuruoka, Yoshimasa / Ananiadou, Sophia (2007):
                        „An annotation type system for a data-driven NLP pipeline“,
                        in:
                        LAW 2007 – Proceedings of the Linguistic Annotation Workshop @ ACL 2007 33–40.
                    
               
                  Hahn, Udo / Matthies, Franz / Faessler, Erik / Hellrich, Johannes (2016): 
                        „Uima-based JCoRe 2.0 goes GitHub and Maven Central: State-of-the-art software resource engineering and distribution of NLP pipelines“,
                        in: 
                        LREC 2016 – Proceedings of the 10th International Conference on Language Resources and Evaluation 2502–2509.
                    
               
                  Hinrichs, Erhard W. / Hinrichs, Marie / Zastrow, Thomas (2010):
                        „WebLicht: Web-based LRT services for German“,
                        in: 
                        Proceedings of ACL-2010: System Demonstrations 25–29.
                    
               
                  Jannidis, Fotis (2014):
                        „Der Autor ganz nah: Autorstil in Stilistik und Stilometrie“,
                        in: Schaffrick, Matthias / Willand, Marcus (eds.): 
                        Theorien und Praktiken der Autorschaft.
                        Berlin: de Gruyter 169–195.
                    
               
                  Manning, Christopher D. / Surdeanu, Mihai / Bauer, John / Finkel, Jenny Rose / Bethard, Steven J. / McClosky, David (2014): "The Stanford CoreNLP Natural Language Processing Toolkit", in: 
                        Proceedings of ACL-2014: System Demonstrations 55–60.
                    
               
                  Michel, Jean-Baptiste / Shen, Yuan K. / Aiden, Aviva P. / Veres, Adrian / Gray, Matthew K. / The Google Books Team / Pickett, Joseph P. / Hoiberg, Dale / Clancy, Dan / Norvig, Peter / Orwant, Jon / Pinker, Steven / Nowak, Martin A. / Aiden, Erez L. (2011):
                        „Quantitative analysis of culture using millions of digitized books“,
                        in: 
                        Science 331 (6014): 176–182.
                    
            
         
      
   


10611	2017	
      
         Graphische Romane vereinen als hybride Gattung Aspekte von Literatur und bildender Kunst (McCloud, 1993). Wie interagieren Bild und Text beim Lesen graphischer Literatur und ermöglichen das Verstehen des Gesamtwerkes? Worauf fokussiert die Aufmerksamkeit des Lesers? Als Methode zur Beantwortung dieser Fragen ist die Blickbewegungsmessung besonders geeignet. Blickbewegungen haben sich in einer Vielzahl an Studien als valides, nichtreaktives Maß für die Verarbeitung und das Verstehen von Text und Bild erwiesen, in dem sich zudem auch unbewusste Verarbeitungsprozesse niederschlagen (Findlay & Gilchrist, 2003; Wade & Tatler, 2005). 
         In früheren Arbeiten (Laubrock, Hohenstein & Thoß, 2016; Dunst, Hartel, Hohenstein Laubrock, 2016) haben wir mit Eyetracking-Analysen gezeigt, dass beim Lesen grafischer Literatur der größte Teil der Aufmerksamkeit dem Text in Sprechblasen und Beschriftungen (Captions) zugewandt wird und nur ein relativ kleiner Teil den originär visuellen Gestaltungselementen alloziert wird. Wird der visuelle Inhalt gar nicht beachtet, oder kann er möglicherweise bereits im peripheren Sehen während der Fixationen auf dem Text verarbeitet werden? Wir hatten bereits berichtet, dass Comics-Experten den Bildanteil stärker beachten und darauf verstehensrelevante Information extrahieren. In einer neuen Serie von Studien untersuchen wir mittels blickkontingenter Präsentation, ob (a) den Bildanteilen mehr Aufmerksamkeit zugewandt wird, wenn die Vorschau verhindert wird, indem das Bild erst eingeblendet wird, wenn der Blick sich auf ein Panel bewegt und (b) die Aufmerksamkeit andere grafische Elemente auswählt, wenn zwar der visuelle Teil der Panels sichtbar ist, der Text aber erst nach Fokussierung eines Panels eingeblendet wird.
         Das visuelle Material wurde auf zweierlei Weise annotiert. Einerseits annotierten Menschen Personen und einzelne Objekte innerhalb der Panels. Andererseits versuchen wir eine objektiven Beschreibung des visuellen Materials mithilfe von Deskriptoren aus dem maschinellen Sehen (Computer Vision), z.B. mittels Farbhistogrammen, lokalem Fourier-Spektrum oder SIFT-Deskriptoren (Lowe, 1999). Der Vorteil dieser Beschreibung ist neben der Objektivität die skriptgesteuerte Anwendbarkeit auf große Datenmengen, etwa digitalisierte Korpora grafischer Literatur. Vergleichbare Arbeiten aus der Schnittstelle von Kunstgeschichte und Informatik ermöglichen beispielsweise eine automatisierte Klassifikation von Kunstrichtungen (Saleh & Elgammal, 2015) und zeigen das Potenzial eines solchen Ansatzes als Stilometrie visueller Merkmale.
         Für die Zuordnung der Blickbewegungsdaten auf das Stimulusmaterial nutzen wir die im Projekt entwickelte Graphic Novel Markup Language (GNML), eine Erweiterung der Comic Book Markup Language (CBML; Walsh, 2012). Das Material wurde mit unserem Editor annotiert, für Weiterverarbeitung und statistische Analyse der Daten nutzten wir ein in Entwicklung befindliches R-Paket. Die objektive Beschreibung des visuellen Materials mit Deskriptoren aus dem maschinellen Sehen wurde unter Nutzung von OpenCV (Bradski, 2000) und VLFEAT (Vedaldi & Fulkerson, 2008) teils in Python und teils in Matlab implementiert, da für R für diesen Anwendungsbereich keine hinreichend entwickelte Funktionsbibliothek existiert. 
      
      
         
            
               Bibliographie
               
                  Bradski, Gary (2000):
                        „The OpenCV library“,
                        in:
                        Dr. Dobb’s Journal of Software Tools 25 (11): 120–125. 
                    
               
                  Dunst, Alexander / Hartel, Rita / Hohenstein, Sven / Laubrock, Jochen (2016):
                        „Corpus Analyses of Multimodal Narrative: The Example of Graphic Novels“,
                        in:
                        DH2016: Conference Abstracts 178–180.
                    
               
                  Findlay, John M. / Gilchrist, Ian D. (2003):
                        Active Vision. The Psychology of Looking and Seeing.
                        Oxford: Oxford University Press.
                    
               
                  Laubrock, Jochen / Hohenstein, Sven / Thoß, Aalexander (2016):
                        „Moving around the city of glass“,
                        in: 
                        DHd 2016: Modellierung - Vernetzung - Visualisierung 186. 
                    
               
                  Lowe, David G. (1999):
                        „Object recognition from local scale-invariant features“,
                        in:
                        Proceedings of the International Conference on Computer Vision (ICCV'99) 1150–1157.
                    
               
                  McCloud, Scott (1993):
                        Understanding comics: the invisible art.
                        Northampton, MA: Tundra.
                    
               
                  Saleh, Babak / Elgammal, Ahmed M. (2015):
                        „Large-scale classification of fine-art paintings: Learning the right metric on the right feature“,
                        in:
                        CoRR abs/1505.00855, 1–21 
                        http://arxiv.org/pdf/1505.00855v1.pdf.
                    
               
                  Vedaldi, Andrea / Fulkerson, Brian (2008):
                        VLFeat: An open and portable library of computer vision algorithms. [Computer Software: http://www.vlfeat.org/ ]
               
                  Wade, Nicholas J. / Tatler, Benjamin W. (2005): 
                        The Moving Tablet of the Eye: Origins of modern eye movement research. 
                        Oxford: Oxford University Press.
                    
               
                  Walsh, John (2012):
                        „Comic Book Markup Language: An Introduction and Rationale“,
                        in:
                        DHQ: Digital Humanities Quarterly 6 (1).
                    
            
         
      
   


10612	2017	
      
         
         
            Einführung
            Das ÖBL (Österreichisches Biographisches Lexikon 1815-1950) ist ein umfassendes Werk, das derzeit rund 18,000 Biographien von wichtigen historischen Persönlichkeiten aus der österreichisch-ungarischen Monarchie und der Ersten und Zweiten Republik Österreichs enthält. Während an dem Lexikon noch gearbeitet wird, erscheint es in gedruckter Form, und seit 2009 ist es auch online verfügbar.
                    APIS - Mapping historical networks: Building the new Austrian Prosopographical | Biographical Information System - ist ein interdisziplinäres Digital Humanities Projekt, das WissenschaftlerInnen aus unterschiedlichen Themenbereichen (Biografien, Geschichte, Geographie, Sozialwissenschaften, Informationstechnologie) verbesserten Zugriff (Suchabfragen, API etc.) auf die ÖBL-Daten erlauben wird. Dadurch wird es möglich sein innovative, interdisziplinäre Forschung auf der Grundlage dieser einzigartigen Ressource durchzuführen. Als erstes Beispiel für eine solche angewandte wissenschaftliche Forschung und als wichtiger Test der Brauchbarkeit und Eignung der entwickelten Lösung, wird bereits im APIS Projekt eine soziodemografische Analyse, die die Formen und Muster der Migration von gesellschaftlichen Eliten untersucht, umgesetzt. 
                    In unserer Präsentation konzentrieren wir uns auf die zugrunde liegende technische Lösung, vor allem auf die dynamischen Aspekte - Workflow – und die Ergebnisse der verschiedenen angewandten Verfahren, um den aktuellen Stand der Umsetzung zu beschreiben.
                    
            
         
         
            Ansatz
            ÖBL Daten stehen momentan in einem Ad-hoc-XML-Format zur Verfügung. Diese XMLs enthalten einige Fakten (Geburts- und Todesdaten, Orte, Berufsangaben usw.) in strukturierter Form, der Großteil der Information versteckt sich jedoch in dem unstrukturierten Haupttext der Biographie. Das Hauptziel des Projektes ist Informationen automatisch aus dem freien Text zu extrahieren, und sie in strukturierter Form zur Verfügung zu stellen. Um dieses Ziel zu erreichen, wird ein zweifacher Hybrid-Ansatz verfolgt, der einerseits automatische und manuelle Textverarbeitung kombiniert und andererseits erlaubt die erhobenen Daten in verschiedenen Formaten zu serialisieren. Letzteres beinhaltet nicht nur die Bereitstellung in verschiedenen Formaten (z.B. RDF/JSON), sondern auch die Verwendung verschiedener Ontologien (z.B. CIDOC-CRM (Doerr 2003: 75-92), NDB (Historische Kommission bei der Bayerischen Akademie der Wissenschaften 1953)). Die extrahierten Entitäten sind mit mehreren semantischen Referenz Ressourcen wie zum Beispiel GND (Pfeifer 2012: 80-91), GeoNames
                    1 oder DBpedia (Bizer 2009: 154-165) abgeglichen und mit URIs aus diesen versehen (Entity Linking). Dieser kombinierte Ansatz wurde gewählt, um die höchstmögliche Genauigkeit der Annotationen zu gewährleisten, und den manuellen Aufwand so gering wie möglich zu halten. Obwohl es bewährte Techniken und Methoden für die Verarbeitung natürlicher Sprache gibt, wird manuelle Arbeit (Korrektur) der Forscher, die mit den jeweiligen Wissenschaftsgebieten vertraut sind, nach wie vor erforderlich sein.
                
         
         
            Datenmodell
            Das Datenmodell besteht aus fünf Entitäten (Personen, Institutionen, Orte, Werke und Ereignisse) und einer Meta-Entität (Verweis auf den ursprünglichen Artikel). Es gibt Beziehungen zwischen allen Entitäten (z.B. Person - Institution, Person - Ereignis) und Beziehungen sind auch zwischen den gleichen Objekttypen möglich (z.B. Person -> Vater_von -> Person). Die Beziehungen können auch temporalisiert (Start- und Enddatum) und typisiert werden (Typen können je nach Bedarf angegeben werden). Das erlaubt uns praktisch alle möglichen Szenarien zu modellieren. 
                    Der ursprüngliche Plan war, die Daten nach bestehenden, gut definierten Ontologien zu modellieren. In der Evaluierungsphase wurde uns aber klar, dass sehr viele verschiedene Ontologien existieren. Einige sind wie CIDOC-CRM Event basiert, andere verbinden Entitäten direkt. Wir haben uns deshalb entschlossen ein eigenes (internes) Datenmodell zu erstellen und so den technischen Aufwand für die Verarbeitung, Darstellung und Speicherung der Daten möglichst gering zu halten. Gleichzeitig werden wir aber dieses interne Datenmodell mit Hilfe schon existierender Ontologien (NDB, CIDOC-CRM etc.) in verschiedenen Formen serialisieren und der Öffentlichkeit zur Verfügung stellen. Das stellt die möglichst einfache, nachhaltige Nutzung unserer Daten sicher.
                
         
         
            Extraktion
            Um strukturierte semantische Informationen aus den Biographien zu extrahieren, und die dadurch identifizierten Objekte zu Ressourcen wie GND, GeoNames zu verknüpfen verwenden wir automatische Tools. Die Ergebnisse werden von Experten verifiziert und ausgebessert um die Qualität der Daten zu gewährleisten, und um unser System durch manuelle Korrektur zu verbessern. Während die NLP-Tools eine schnelle Verarbeitung ermöglichen sind die Ergebnisse nicht zu 100% korrekt. Um die Genauigkeit zu verbessern, setzen wir mehrere Systeme, Quellen und Analysen ein. Für die automatische Extraktion haben wir mehrere Tools getestet und bewertet, wie z.B. Stanford NER (Finkel 2005: 363-370), GATE (Cunningham 2011), OpenNLP
                    2, Stanbol (Bachmann-Gmur 2013), basierend auf folgende Kriterien: 1) welche Sprachen unterstützt das System 2) Möglichkeit der Anpassung, 3) Entity Linking Fähigkeiten, 4) Output Format und 5) die Verfügbarkeit und Qualität der API. Apache Stanbol hat sich als das am besten geeignete Werkzeug für unsere Zwecke gezeigt. Stanbol ermöglicht die Verknüpfung von Entitäten wie Personen, Institutionen zu Referenzressourcen (Normdateien, Ontologien). Wir haben die Biographien mit GND und GeoNames abgeglichen, und planen weitere LOD
                    3 Ressourcen hinzuzufügen. Durch die Verknüpfung von oben benannten Entitäten zu den semantischen Ressourcen können wir viele zusätzliche Informationen (z.B. Alternative Namen, Titel von Werken usw.) zu unseren Daten hinzufügen, und so Inhalte mit fehlenden Informationen bereichern.
                    
            
         
         
            Anwendung
            Um den manuellen Arbeitsaufwand (Korrektur der Daten etc.) zu minimieren haben wir eine effiziente und einfache Weboberfläche geschaffen, die es den ForscherInnen erlaubt mit den Daten zu interagieren. Im Sinne einer nachhaltigen Nutzung und einfacher weiteren Betreuung des so entstandenen Tools haben wir uns entschlossen auf erprobte Web-Technologien zu setzen (Django
                    4/MySQL). Die Web-Anwendung ist in Django, einem Python-basierten Web-Entwicklungs-Framework, implementiert. Django ist nicht nur ein ausgereiftes und verbreitetes Tool (Websites wie Disqus, Pinterest und die Washington Times nutzen es), sondern bietet auch die Möglichkeit die volle Bandbreite der verschiedenen Python Bibliotheken nativ im Code zu verwenden (NLTK
                    5, scikit-learn
                    6, NumPy
                    7 etc.) .
                    Die Web-Anwendung stellt die Daten der einzelnen Biographien strukturiert in drei Teilen dar: primäre minimale Informationen, Haupttext mit markierten Anmerkungen und die Listen von Orten, Institutionen und Personen, die mit dem Biographierten in Zusammenhang stehen. Die Anwendung bietet auch Funktionen für die Navigation: dropdown Listen sowie einfache Volltextsuche.
                    Eine weitere wichtige Funktion der Anwendung ist die Möglichkeit, den Text manuell mit Annotationen zu versehen. Dieses Feature erlaubt sowohl die Korrektur von automatischen Annotationen, als auch das Hinzufügen von neuen Annotationen. Die Kuratoren können die Entitäten mit der Maus auswählen oder im Kontextmenü identifizieren. 
                
            Derzeit liegt der Schwerpunkt auf der Darstellung von Orten. Dementsprechend wird die Anwendung mit eingebetteten Karten ausgestattet, an denen identifizierte geographische Orte visualisiert werden können. In der nächsten Phase des Projekts wird eine interaktive Visualisierung entwickelt, um das Verständnis der Daten und die Navigation im Datenbestand zu erleichtern.
         
         
            Arbeitsablauf
            Das System unterstützt zwei Workflows: im ersten Schritt schickt die Anwendung (das Extrakt-Modul) die Biographien im Batch-Modus zu einem Extraktionsservice (lokale Stanbol Instanz), welches die Abfragen an externe Services und/oder lokale Indizes weiterleitet und die gematchten Entitäten in einer Liste in JSON-LD Format zurückgibt. Diese Entitäten werden von dem Extrakt-Modul analysiert und in der Datenbank abgespeichert. Danach werden sie in der Web-Anwendung dargestellt und können von den ForscherInnen überprüft und korrigiert werden.
                    Im zweiten Schritt wird der Workflow vom Benutzer gestartet: Der menschliche Annotator markiert einen String und identifiziert ihn als Ort, die Anwendung schickt den ausgewählten String zur Stanbol Instanz, die die verfügbaren Ressourcen abfragt und mögliche Kandidaten zurückgibt. Diese Treffer werden dem/der ForscherIn in Form eines Autocomplete Feldes angezeigt.
                    
            
         
         
            Schlussfolgerung
            Während wir uns in unserem Abstrakt auf die technische Umsetzung konzentriert haben, ist es wichtig im Auge zu behalten, dass das System nur eine Voraussetzung ist die eigentliche Forschungsfragen beantworten zu können. Alle im Projekt generierten Daten sowie die entwickelte Forschungsumgebung wird der Öffentlichkeit zugänglich gemacht (eine erste Version der Forschungsumgebung wird Ende September in unserem Github Account zugänglich gemacht). Wie schon weiter oben angesprochen versuchen wir die Nachhaltigkeit unserer Lösung auf mehrfache Weise zu erreichen. Zum einen verwenden wir gut etablierte Web-Technologien und ermöglichen somit vielen Entwicklern weltweit unseren Code zu warten und/oder weiter zu entwickeln. Zum anderen verbinden wir unsere Daten mit der LOD-Cloud und serialisieren sie mit Hilfe verschiedener weit verbreiteter Ontologien in den gängigsten Formaten und stellen so sicher, dass andere Projekte unsere Daten mit äußerst kleinem Aufwand direkt in ihre Projekte einbetten können.
         
      
      
         
            
               http://www.geonames.org/
            
             https://opennlp.apache.org/
             http://linkeddata.org/
             https://www.djangoproject.com/
             http://www.nltk.org/
             http://scikit-learn.org/stable/
             http://www.numpy.org/
         
         
            
               Bibliographie
               
                        APIS: Mapping historical networks: Building the new Austrian Prosopographical | Biographical Information System (APIS) 
                        http://www.oeaw.ac.at/acdh/en/apis
               
               
                  Bachmann-Gmur, Reto (2013): 
                        Instant Apache Stanbol (1st ed.).
                        Packt Publishing. ISBN 1783281235.
                    
               
                  Bizer, Christian / Lehmann, Jens / Kobilarov, Georgi / Auer, Soren / Becker, Christian / Cyganiak, Richard / Hellmann, Sebastian (2009): 
                        „DBpedia - A crystallization point for the Web of Data“, 
                        in: 
                        Journal of Web Semantics 7 (3): 154–165.
                    
               
                  Cunningham, Hamish / Maynard, Diana / Bontcheva, Kalina (2011): 
                        Text Processing with GATE (Version 6). 
                        University of Sheffield Department of Computer Science. ISBN 0956599311.
                    
               
                  Doerr, Martin (2003): 
                        „The CIDOC CRM – An Ontological Approach to Semantic Interoperability of Metadata“, 
                        in: 
                        AI Magazine 24 (3): 75–92.
                    
               
                  Finkel, Jenny Rose / Grenager, Trond / Manning, Christopher (2005): 
                        „Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling“,
                        in: 
                        Proceedings of ACL-2005 363–370.
                    
               
                  Historische Kommission bei der Bayerischen Akademie der Wissenschaften (seit 1953): 
                        Neue deutsche Biographie, 
                        Berlin: Duncker & Humblot. ISBN 3-428-00181-8
                    
               
                  ÖBL - Österreichisches Biographisches Lexikon/Austrian Biographical Lexicon (1815-1950) Online-Edition und Österreischiches Biographisches Lexikon ab 1815 (2. Überarbeitete Auflage - online). 
                        Verlag der Österreichischen Akademie der Wissenschaften. Wien.
                        http://www.biographien.ac.at/oebl [letzter Zugriff 26. August 2016]
                    
               
                  Pfeifer, Barbara (2012): 
                        „Vom Projekt zum Einsatz. Die gemeinsame Normdatei (GND)“,
                        in: Brintzinger, Klaus-Rainer (ed.): 
                        Bibliotheken: Tore zur Welt des Wissens. 101. Deutscher Bibliothekartag in Hamburg 2012, Olms, Hildesheim u.a. 2013: 80–91.
                    
            
         
      
   


10616	2017	
      
         
            Topic Modeling ist eine Methode zur semantischen Erschließung größerer Textsammlungen, die in den letzten Jahren zunehmend in den Fokus der Aufmerksamkeit digital arbeitender Literaturwissenschaftler gerückt ist. Die Methode nutzt probabilistische Verfahren um aus einer Textsammlung eine Reihe von Verteilungen über die Wahrscheinlichkeiten einzelner Wörter zu erzeugen. Diese werden dann als distinkte semantische Gruppen, sogenannte ‘Topics’, aufgefasst, also als Gruppen inhaltlich zusammenhängender Wörter, die in den einzelnen Texten jeweils mehr oder weniger stark präsent sind (Blei 2012, Steyvers und Griffiths 2006).
            
         Ursprünglich entwickelt, um in größeren Sammlungen kürzerer Fachartikel schnell jene zu identifizieren, die für bestimmte Themen relevant sein könnten, kann diese Methode darüber hinaus für eine Reihe von Problem im Bereich der digitalen Literaturwissenschaft interessante neue Lösungsansätze bieten. Dazu gehört die automatische Identifikation von Romanen, die ähnliche Themen behandeln (wenngleich eine direkte Gleichsetzung probabilistischer ‘Topics’ mit literarischen ‘Themen’ durchaus problematisch ist), ebenso wie die Zuordnung zu bestimmten Genres anhand inhaltlicher Aspekte, oder die quantifizierende Betrachtung der zu- und abnehmenden Bedeutung einzelner Themenfelder über den Verlauf eines einzelnen Romans (vgl. Blevins 2012, Jockers 2011, 
                Rhody 2012, Schöch in Vorbereitung).
            
         Mit den Programmen ‘Mallet’ (vgl. McCallum 2002) und ‘Gensim’ (vgl. Rehurek 2010) stehen zur Zeit zwei State-of-the-Art Implementierungen von Topic Modeling-Algorithmen zur Verfügung. Um die Methode produktiv einzusetzen, sind aber neben der Erzeugung des Modells weitere Arbeitsschritte notwendig (Abb. 1). Im ‘Preprocessing’ gilt es zunächst, die Textsammlungen in eine Form zu bringen, in der sie vom Modellierungsprogramm verarbeitet werden können. Darüber hinaus werden die Texte normalerweise durch das Herausfiltern häufiger Funktionswörter auf die potentiell inhaltsrelevanten Wörter reduziert, was in der Regel den vorhergehenden Einsatz von NLP-Tools (Natural Language Processing) erfordert. Sind die ‘Topics’ dann erst einmal errechnet worden, kann sich eine Visualisierung der Ergebnisse anschließen, oder ihre statistische Evaluierung anhand interner oder externer Kriterien, ein Aspekt dem beim Einsatz von Topic Modeling-Verfahren im DH-Kontext bisher eher zu wenig Beachtung geschenkt wurde.
         Ziel unseres Projektes ist es, den Einstieg in aktuelle Topic Modeling-Verfahren für digital arbeitende Literaturwissenschaftler wesentlich zu vereinfachen, indem wir möglichst viele der notwendigen Arbeitsschritte in einer einheitlichen, umfangreichen und gut dokumentierten Programmbibliothek für die unter digital-quantitativ arbeitenden Geisteswissenschaftlern stark verbreitete Programmiersprache Python anbieten. Hierbei sollen Nutzerinnen und Nutzer bei allen Arbeitsschritten auf vorhandene, in einem ausführlichen Tutorial dokumentierte Funktionen zurückgreifen und so weit wie möglich wie mit einem Kommandozeilentool arbeiten können, ohne selbst programmieren zu müssen. Die Anforderungen an die Programmierkenntnisse der Forschenden, die diese Verfahren einsetzen möchten, werden damit minimiert und die Methode wird so einem größeren Nutzerkreis zugänglich gemacht.
         Für das NLP-Preprocessing steht mit dem DARIAH-DKPro-Wrapper (DDW) ein komfortables Einheitswerkzeug zur Verfügung, das ein großes Spektrum an NLP-Aufgaben abdeckt und linguistische Annotationen in einem Python-Pandas-kompatiblen Ausgabeformat erzeugt. Ein Ziel unserer Bibliothek ist die direkte Anbindung des DDW-Outputs an existierende Implementierungen verschiedener etablierter Varianten von Topic Modeling-Algorithmen. 
         Für die Untersuchung der resultierenden Modelle möchten wir verschiedene Evaluierungsverfahren anbieten, sowohl interne Verfahren wie z.B. das Perplexity-Maß, als auch externe Vefahren, wie z.B. die Weglänge zwischen zwei Begriffen in einem Wörterbuch. Hieran schließen sich verschiedene Optionen zur Visualisierung der Ergebnisse an.
         Im Fokus der Entwicklung steht die Gestaltung schlüssig aufeinander aufbauender Programmbefehle, die einer einheitlichen Syntax folgen und deren Funktion sich schnell erschließen lässt. Sie sollen sich ohne längere Einarbeitung nutzen und zu einer Pipeline zusammenfügen lassen, die die spezifischen Arbeitsschritte eines bestimmten Topic Modeling-Projektes umsetzt. Hierbei können Nutzerinnen und Nutzer auf detaillierte Anleitungen aus einem umfangreichen Tutorial zurückgreifen, in dem alle Funktionen, alle Outputs, und potentielle Kombinationen detailliert dokumentiert und anhand von Beispielen erläutert werden.
         
            Die Entwicklung der Programmbibliothek kann auf Erfahrungen mit einer vorhandenen, Python-basierten Implementierung eines entsprechenden Workflows aufbauen, die allerdings eher “proof of concept”-Character hat (Topic Modeling Workflow “tmw”, vgl. Schöch 2015 und 
            
            ).
         
         
            
               
            
            Abbildung 1: Workflow eines Topic Modeling-Projektes
            
      
      
         
            
               Bibliographie
               
                  Blei, David M. (2012):
                        „Probabilistic Topic Models“,
                        in:
                        Communication of the ACM 55 (4): 77–84
                        10.1145/2133806.2133826.
                    
               
                  Blevins, Cameron (2010):
                        „Topic Modeling Martha Ballard’s Diary“,
                        in:
                        Historying                        . 
                        http://historying.org/2010/04/01/topic-modeling-martha-ballards-diary/.
                    
               
                  Jockers, Matthew L. (2013): 
                        Macroanalysis - Digital Methods and Literary History. 
                        Champaign, IL: University of Illinois Press.
                    
               
                  McCallum, Andrew K.  (2002): 
                        MALLET: A Machine Learning for Language Toolkit
                  http://mallet.cs.umass.edu.
                    
               
                  Rehurek, Radim / Sojka, Petr (2010):
                        „Software framework for topic modelling with large corpora“,
                        in:
                        Proceedings of LREC 2010.
                    
               
                  Rhody, Lisa M. (2012):
                        „Topic Modeling and Figurative Language“,
                        in:
                        Journal of Digital Humanities 2 (1) 
                        http://journalofdigitalhumanities.org/2-1/topic-modeling-and-figurative-language-by-lisa-m-rhody/.
                    
               
                  Richardson, Stephen D. / Braden-Harder, Lisa (1988):
                        „The Experience of Developing a Large-Scale Natural Language Text Processing System: CRITIQUE“,
                        in: 
                        Proceedings of the Second Conference on Applied Natural Language Processing 195–202.
                    
               
                  Schöch, Christof (in Vorbereitung):
                        „Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama“,
                        in:
                        DHQ: Digital Humanities Quarterly
                  http://digitalhumanities.org/dhq.
                        Preprint: 
                        https://zenodo.org/record/48356.
                    
               
                  Steyvers, Mark / Griffiths, Tom  (2006): 
                        „Probabilistic Topic Models“, 
                        in: Landauer, T. / McNamara, D. / Dennis, S. / Kintsch, W.:
                        Latent Semantic Analysis: A Road to Meaning.
                        Laurence Erlbaum.
                    
            
         
      
   


10621	2017	
      
         Mit „Ulysses: A Critical and Synoptic Edition“ erschien 1984 eine der ersten Forschungseditionen, die auf Basis der systematischen Verwendung von Kollationierungssoftware digital erzeugt wurde. Das Münchner Team um Hans Walter Gabler verwendete hierzu TUSTEP sowohl zur Validierung der Transkripte einzelner Zeugen als auch zur Erschließung der zeugenübergreifenden Synopse. Für die gedruckte Edition wurden die halbautomatisch erzeugten Kollationsergebnisse mit einem eigens entwickelten System komplexer Diakritika ausgezeichnet, die es dem geübten Leser ermöglichen sollten, die Textentstehung über stellenweise mehr als zwanzig inter- und intradokumentarische Textstufen hinweg in einer synoptisch integrierten Textfassung nachzuvollziehen. Während die Konzeption und Umsetzung dieser Arbeit bis heute als bahnbrechend im Bereich der Computerphilologie zu bezeichnen ist, konnte das Potenzial der resultierenden Druckausgabe für die Joyce-Forschung nicht annähernd ausgeschöpft werden. Zu komplex war das Markup, dem es gelingen sollte, zu verknüpfen, was zuvor getrennt war und zu hoch war der Aufwand, sich in diese Systematik einzuarbeiten.
         Im Digitalen hingegen führten die Daten jene Odyssee fort, die die Druckedition beenden sollte. Auf der Suche nach einem Markup-Standard, der es vermag, die Inhalte der Druckedition digital zu repräsentieren, wurden die TUSTEP Ergebnisse zunächst von Tobias Rischer im Rahmen seiner Diplomarbeit (1997) in SGML/TEI transformiert und anschließend in mehreren Überarbeitungen über TEI P4 bis hin zur aktuellen Version der TEI P5v3 (2016) migriert. Dieser Beitrag vollzieht die Evolution dieser “Legacy Data” nach, bis hin zu ihrer jüngsten Station - der noch andauernden Bemühung einer Migration nach TEI P5v3, welche im Rahmen des DFG- und NEH-geförderten Kooperationsprojektes “Diachronic Markup and Presentation Practices for Text Editions in Digital Research Environments” am Lehrstuhl für Digital Humanities der Universität Passau durchgeführt wird. 
         Erstmals seit der zweiten, überarbeiteten Ausgabe der synoptisch-kritischen Gabler Edition 1986 gelang es, aus den TEI-Daten die synoptische Visualisierung der Druckedition zu rekonstruieren und somit eine Konsistenzprüfung gegen die ursprünglichen Daten zu ermöglichen. Erst durch diese visuelle Rückführung offenbarten sich migrationsbedingte Fehler und Provisorien, welche zuvor, wenn überhaupt, nur in Fußnoten und privaten Aufzeichnungen vergangener Beteiligter dokumentiert wurden. Neben dem allgemeinen Versuch, die vollzogenen Änderungen aus den Aufzeichnungen und Migrationsergebnissen früherer Projekte zu rekonstruieren, hat es sich das Passauer Team zur Aufgabe gemacht, Strategien zur Entdeckung, Typisierung und Korrektur derartiger „Migrationsverluste“ zu entwickeln. Ein wesentlicher Bestandteil dieser Arbeit ist die Abschätzung der Leistungsfähigkeit und Wirtschaftlichkeit von automatisierten Batch-Konvertierungen mittels XSLT und Python im Vergleich zur manuellen Intervention und Korrektur der Kodierung. 
         Neben der Identifikation und Korrektur von „Migrationsfehlern“, steht die Rekonstruktion der textgenetischen Perspektive, durch welche sich die Druckedition auszeichnete, im Vordergrund. Während Gabler die textuelle Entwicklung, welche er mittels der Kollation chronologisch aufeinander folgender Textzeugen erschlossen hatte, im Druck synoptisch darstellen konnte, beinhalteten die TEI Guidelines bis zur Version P5v2 kein Modell zur Auszeichnung textgenetischer Prozesse. Es fehlte schlicht die Möglichkeit zur formalisierten Dokumentation einer stufenweisen, zeugenübergreifenden Chronologie der Textentwicklung. In der Druckedition wurde jeder auktorialen Textänderung 
                genau eine Textstufe aus der heuristisch erschlossenen Chronologie zugeordnet. Diese lineare Textentwicklung über intra- und interdokumentarische Textstufen, in Gablers Terminologie auch Overlay und Level genannt, musste im Digitalen in eine Auszeichnung überführt werden, welche die Genese in den Hintergrund rückt und zu jeder auktorialen Modifikation anstelle einer Textstufe eine Liste sämtlicher Zeugen verzeichnet, auf welcher die spezifische Änderung Bestand hat. Diese Art der dokumentenorientierten Kodierung von Textgenese entspricht zwar bis heute der gängigen Auszeichnungspraxis historisch-kritischer Editionsprojekte, repräsentierte aber zu keinem Zeitpunkt die textgenetische Intension der 84er 
                Ulysses Edition. Erst mit der Integration eines textgenetischen Modells in die TEI Guidelines, kann die ursprüngliche Intension erstmals auch in TEI kodiert werden. Hierzu bedarf es einer weiteren Episode der Datenmigration auf der Odyssee zum richtigen Standard. 
            
      
      
         
            
               Bibliographie
               
                  Brüning, Gerrit / Henzel, Katrin / Pravida, Dietmar (2014): 
                        „Multiple Encoding in Genetic Editions: The Case of Faust“,
                        in: 
                        Journal of the Text Encoding Initiative 4. Available from: jtei.revues.org.
                    
               
                  Burnard, Lou / O’Brien O’Keeffe, Katherine / Unsworth, John (2006): 
                        Electronic Textual Editing.
                        New York: Modern Language Association of America.
                    
               
                  Burnard, Lou / Jannidis, Fotis / Pierazzo, Elena / Midell, Gregor / Rehbein, Malte (2010): 
                        „An Encoding Model for Genetic Editions“,
                        in:
                        TEI: Text Encoding Initiative. 
                        Retrieved from www.tei-c.org/ Activities/Council/Working/tcw19.html/. 
                    
               
                  Joyce, James / Gabler, Hans Walter (eds.) (1984): 
                        Ulysses: A Critical and Synoptic Edition.
                        New York: Garland.
                    
               
                  Joyce, James (1922): 
                        Ulysses. 
                        Paris: Shakespeare and Company.
                    
               
                  Fordham, Finn (2010): 
                        I do, I undo, I redo: The Textual Genesis of Modernist Selves in Hopkins, Yeats, Conrad, Forster, Joyce, and Woolf. 
                        Oxford / New York: Oxford University Press.
                    
               
                  Rischer, Tobias (1997): 
                        Eine TEI/SGML-Edition der textkritischen Ausgabe von James Joyces Ulysses. 
                        Diplomarbeit, LMU München.
                    
               
                  TEI Consortium (eds.) (2016): 
                        TEI P5: Guidelines for Electronic Text Encoding and Interchange. P5v3.
                        Available from: http://www.tei-c.org/Guidelines/P5/.
                    
            
         
      
   


10623	2017	
      
         
         
            Ausgangslage
            Der Einfluss der unter Digital Humanities (DH) zusammengefassten digitalen Theorien und Methoden auf die geisteswissenschaftlichen Disziplinen wächst stetig. Digitale Projekte erleben in den Geisteswissenschaften einen rasanten Aufschwung (Koller 2016: 43). Damit einher geht der Bedarf an Absolventen geisteswissenschaftlicher Fächer, die bereits während ihres Studiums Kompetenzen im Bereich der Digital Humanities erwerben konnten. Bereits 2013 forderten Vertreter/innen im „Manifest für die DH“ eine Etablierung „digitale[r] Trainingsprogramme in den Geisteswissenschaften“ (DH-Manifest: 2013), angepasst an die unterschiedlichen Bedürfnisse der Fachbereiche und die jeweiligen Karrierestufen. Auch der DHd misst der Ausgestaltung der IT-Ausbildung von Studierenden eine gesteigerte Bedeutung zu. Die Arbeitsgruppe zur Erarbeitung eines „Referenzcurriculums Digital Humanities“
                    2 beschäftigt sich mit der Suche nach einer 
                    bestpraxis, von der Anwender und Institutionen gleichermaßen profitieren (Sahle 2013; Thaller 2015: 3).
                
            Zahlreiche Universitätsstandorte haben auf die neuen Anforderungen mit der Einrichtung unterschiedlich ausgestalteter DH-Studiengänge reagiert (Bartsch/Borek/Rapp 2016: 173; DH Course Registry). Trotz dieser neugeschaffenen Angebote besteht ein zusätzlicher Bedarf an informationstechnologischer Ausbildung in der Breite (Ehrlicher 2016: 625). Zunehmend wird auch in „klassischen“ geisteswissenschaftlichen Berufsfeldern Sicherheit im Umgang mit Software und digitalen Technologien vorausgesetzt. Dieses Grundverständnis digitaler Methoden kann nicht mehr ausschließlich im Selbststudium angeeignet werden (Spiro 2013: 332; Sahle 2016: 79).
         
         
            Projektziele und Rahmenbedingungen
            Hier setzt das Projekt „Digitaler Campus Bayern – Digitale Datenanalyse in den Geisteswissenschaften“ an, welches von der IT-Gruppe Geisteswissenschaften (ITG) der Ludwig-Maximilians-Universität München (LMU) seit Beginn dieses Jahres durchgeführt wird. Grundgedanke ist eine IT-Grundausbildung („
                    IT for all“), welche die Studierenden problemorientiert in die Anwendung digitaler Methoden einführt. Ausgehend von fachwissenschaftlichen Fragestellungen werden Lehrveranstaltungen mit IT-Inhalten in Kooperation mit verschiedenen geschichtswissenschaftlichen Disziplinen und der Kunstgeschichte konzipiert. Dabei soll eine möglichst umfassend angelegte Grundlagenvermittlung in Erfassung, Modellierung, Analyse und anschließender Visualisierung von Daten erfolgen (Lücke/Riepl 2016: 77). Das Verständnis digitaler Methoden steht ebenso im Vordergrund wie eine fachliche Reflexion ihrer Potentiale (Rehbein 2016: 17).
                
            Die Situation der DH an der LMU gestaltete sich bis Projektbeginn (Januar 2016
                    3) ambivalent. In den vorgenannten Studiengängen wurden regelmäßig Überblicksveranstaltungen zur Einführung in die Informatik für Historiker bzw. Kunsthistoriker angeboten. Eine praktische Umsetzung des theoretischen Wissens konnte im Rahmen dieser Veranstaltungen jedoch nicht geleistet werden. Demgegenüber werden durch die ITG, die auf langjährige und umfangreiche Erfahrungen im Bereich des digitalen Projektmanagements
                    4 verweisen kann, optimale Voraussetzungen für eine fortan praxisnahe IT-Ausbildung geschaffen.
                
            Als Mitglied im Münchner Arbeitskreis für digitale Geisteswissenschaften (dhmuc)
                    5 kooperiert die IT-Gruppe zudem fach- und institutionsübergreifend mit zahlreichen kulturellen Einrichtungen. An der Schnittstelle zur universitären Lehre ist es möglich, die „
                    IT for all“-Ausbildung geisteswissenschaftlicher Studierender auf die Anforderungen und Wünsche der potentiellen Arbeitgeberseite im (digitalen) Kultur-, Wissenschafts- und Informationssektor auszurichten.
                
         
         
            Interaktive Lehr- und Lernumgebung 
                    DHVLab
            
            Für die praktische Umsetzung kommt eine interaktive Lehr- und Lernumgebung, das 
                    Digital Humanities Virtual Laboratory – kurz 
                    DHVLab – zum Einsatz
                    6. Die im Entstehen begriffene Plattform umfasst mehrere Komponenten, die im Folgenden vorgestellt werden sollen:
                
            
               Virtuelle Rechenumgebung
               Die virtuelle Rechenumgebung ist das „Herzstück“ der Ausbildungsplattform. Auf dem virtuellen Desktop werden in Abstimmung mit dem/der Kursleiter/in Software und Tools installiert. Dadurch wird die sukzessive Installation durch die Teilnehmer/innen obsolet, wodurch Probleme aufgrund unterschiedlicher Betriebssysteme und Versionierungen vermieden werden. Bei Anmeldung im 
                        DHVLab erhält jede/r Teilnehmer/in eine eigene SQL-Datenbank. Gleichzeitig werden strukturierteDatensammlungen vorgehalten. Diese sind für die Kursteilnehmer/innen zugänglich und für eigene oder im Kurs behandelte Fragestellungen verwendbar. Im Laufe der Lehrveranstaltung können neue Forschungsfragen ausgearbeitet und ein grundsätzliches Verständnis für den sinnvollen Einsatz von Tools und Software
                        7 in den Geisteswissenschaften entwickelt werden.
                    
            
            
               Ausbildungsmaterialien
               Im vergangenen Semester wurde das System testweise in vorgenannten Einführungsveranstaltungen eingesetzt. Die bei der Evaluation gesammelten Erfahrungen fließen unmittelbar in die Erstellung bzw. Erweiterung der Ausbildungsmaterialien. Anhand praxisnaher Manuale wird IT-Grundlagenwissen, in einzelne Lehreinheiten gegliedert, anschaulich dargestellt und erklärt. Die Erstellung von Lehrvideos und Übungsaufgaben ist vorgesehen. Aus diesem Portfolio können Dozentinnen und Dozenten Module entsprechend ihrer fachwissenschaftlichen Schwerpunktsetzung und der Voraussetzungen der Teilnehmer/innen auswählen. Die Seminarplanung und -durchführung erfolgt stets in enger Abstimmung mit den Projektmitarbeitern.
            
            
               Publikationsumgebung
               Für die Vor- und Nachbereitung der einzelnen Sitzungen steht ein WordPress-Blog zur Verfügung. Dort können die Kursleiter/innen Materialen einstellen, die Studierenden ihren Erkenntnisfortschritt und Analyseergebnisse dokumentieren. Dabei erlernen sie gleichzeitig 
                        in praxi das wissenschaftliche Bloggen als innovative Form des Publizierens. Eine abschließende Publikation der studentischen Seminararbeiten ist auf dieser Plattform möglich.
                    
            
            
               Datenrepositorium
               In einem gesonderten Bereich der Datenbankumgebung werden die von den Studierenden im Rahmen einer Lehrveranstaltung erarbeiteten Datenbestände modelliert und nachhaltig abgelegt. Langfristiges Ziel ist der Aufbau eines Forschungsdatenrepositoriums. Nachfolgende Kurse mit ähnlichen Seminarthemen können auf diese Datensammlungen zugreifen, für die eigene Forschungsarbeit verwenden und dadurch sukzessive erweitern. Unterstützung erfährt die ITG durch die Universitätsbibliothek der LMU als Kooperationspartnerin auf dem Gebiet der nachhaltigen und nachnutzbaren elektronischen Publikation von Forschungsdaten.
            
            
               Entwicklung eigener Analyse- und Softwarekomponenten
               Mit dem 
                        DHVLab Analytics Center wurde eine Webanwendung entwickelt, die dazu dient, konkrete geisteswissenschaftliche Fragestellungen mithilfe quantitativer statistischer Methoden zu beantworten, sowie im Stile eines explorativen Werkzeuges neue Forschungsansätze zu eröffnen. Das 
                        Analytics Center kombiniert einführende deskriptive Analysen mit komplexeren Methoden der multivariaten Statistik. Neben diesem Analysetool entsteht derzeit eine Editionsumgebung, die speziell auf die Anforderungen von Studierenden und Promovierenden ausgerichtet wird. Diese wird erstmals im Sommersemester 2017 in einer Übung zur Edition mittelalterlicher Urkunden zum Einsatz kommen. Die Entwicklung weiterer Instrumente ist geplant.
                    
            
         
         
            Der Einsatz der Plattform in der Lehre
            Nach der technischen Realisierung der Plattform und dem Aufbau grundlegender Ausbildungsmaterialien im ersten Projekthalbjahr kommt das System im Wintersemester 2016/2017 erstmals in eigens konzipierten Lehrveranstaltungen zur Anwendung. In der Kunstgeschichte soll das 
                    Analytics Center in einem Seminar zur Beschäftigung mit informatischen und mathematischen Verfahrensweisen anregen. Parallel dazu erfolgt eine Einführung in die Statistiksoftware 
                    RStudio. Ein geschichtswissenschaftliches Hauptseminar beschreitet den Weg von der Originalquelle über die strukturierte Aufnahme und Modellierung von Forschungsdaten sowie die Einführung in die Arbeit mit relationalen Datenbanken hin zur Georeferenzierung. Die in den Seminaren gewonnenen Erfahrungen und Erkenntnisse fließen unmittelbar in die Verbesserung und Ausweitung des bestehenden Lehrmaterials ein (u.a. Erstellung von Anwendungsszenarien). Neben den genannten Kursen wird die Plattform bereits in zahlreichen Lehrveranstaltungen als technische Grundlage verwendet
                    8.
                
         
         
            Konzeption eines fachspezifischen DH-Curriculums
            Die sukzessive wachsende Plattform und die aktuell angebotenen Kurse dienen als Grundlage für eine Institutionalisierung der IT-Grundausbildung in Form eines fachspezifischen DH-Curriculums. Das Konzept für das geplante freiwillige Zusatz-Zertifikat wird derzeit in der Projektgruppe erarbeitet und baut auf Erfahrungen vergleichbarer Angebote im deutschsprachigen Raum auf
                    9. Angedacht ist eine Kombination aus Veranstaltungen, die explizit IT-Grundlagenwissen vermitteln, und praxisorientierten Kursen, in denen die erlernten IT-Inhalte auf fachwissenschaftliche Gegenstände angewendet werden. Wichtig erscheint eine ausgewogene Verschränkung von 
                    eLearning-Angeboten und Präsenzveranstaltungen, da insbesondere letzteren durch den intensiven Austausch der Studierenden mit DH-Spezialisten ein großer Beitrag zum Lernerfolg beigemessen wird
                    10.
                
         
         
            Grundlage einer nachhaltigen IT-Didaktik
            Neben der Langzeitarchivierung der Forschungsdaten wird auch die Nachhaltigkeit der informationstechnologischen Infrastruktur (Serveranlage mit redundant ausgelegten File-, Datenbank- und Web-Servern sowie ausreichenden Storages) durch die IT-Gruppe Geisteswissenschaften dauerhaft gewährleistet. Die Architektur des 
                    DHVLab ist flexibel und skalierbar gestaltet, sodass sie weiter ausgebaut werden kann (bei Bedarf ist ein Hosting der Server am Leibniz-Rechenzentrum in Garching bei München möglich). Für eine nachhaltige IT-Didaktik spielt neben der langfristig gesicherten technischen Infrastruktur insbesondere die inhaltliche Kontinuität eine entscheidende Rolle. Die im Rahmen des Projektes erarbeiteten Lehreinheiten werden dauerhaft zur Verfügung gestellt. Thematisch sind sie so zu gliedern und fachlich anzupassen, dass eine spezifische Auswahl für eine Lehrveranstaltung und damit eine Integration in ein geisteswissenschaftliches Einzelfach möglich ist. Die IT-Gruppe stellt auch nach Ende der Projektlaufzeit die unterstützende Begleitung der Lehrveranstaltungen sicher. Der Vortrag möchte zur Diskussion anregen, inwiefern sich die Anpassung der Materialien an die sich rasch wandelnden Anforderungen im Bereich der Digital Humanities möglichst effizient gestalten lässt. IT-Didaktik scheint nur dann einen Anspruch auf Nachhaltigkeit zu besitzen, wenn sie sich in einem steten Anpassungsprozess befindet.
                
            Ganz im Sinne des „Digitalen Campus Bayern“ ist das Münchener Pilotprojekt auf eine Ausweitung auf andere Studienstandorte ausgerichtet. Die Plattform wird beispielsweise ab 2017 in einem im Aufbau befindlichen Kooperationsprogramm zur DH-Ausbildung der Universitäten Erlangen, München und Regensburg zum Einsatz kommen. Alle Module des 
                    DHVLab können kollaborativ von anderen Hochschulen genutzt werden, um umfassende Sammlungen von Tutorials, Aufgaben, Softwarebeschreibungen, Anwendungsszenarien sowie Sammlungen fachwissenschaftlicher Objekt- und Metadaten aufzubauen und gemeinsam zu pflegen.
                
         
      
      
         
            Vgl. 
                            http://www.dh-curricula.org/index.php?id=1 [letzter Zugriff: 30. November 2016].
                        
             Die Projektlaufzeit beträgt zwei Jahre. Das Vorhaben ist Teil eines Förderprogramms, welches das Bayerische Wissenschaftsministerium aufgelegt hat. Vgl. 
                            https://www.km.bayern.de/pressemitteilung/9340/.html [letzter Zugriff: 30. November 2016].
                        
             Vgl. die Übersicht unter 
                            www.itg.lmu.de/projekte [letzter Zugriff: 30. November 2016].
                        
             Vgl. 
                            http://dhmuc.hypotheses.org/uber [letzter Zugriff: 30. November 2016].
                        
             Für die Dokumentation der technischen Infrastruktur vgl. 
                            http://dhvlab.gwi.uni-muenchen.de/index.php/Category:
               Architektur [letzter Zugriff: 30. November 2016].
                        
             Derzeit stehen in der virtuellen Umgebung u.a. folgende Software und Programme zur Verfügung: LibreOffice-Paket, OCRFeeder und Ocrad (Texterkennung), Python (PyCharm), RStudio (Statistik), Gephi (Visualisierung), epcEdit (XML-Editor), AntConc und TreeTagger (Korpuslinguistik).
             Vgl. die Zusammenstellung auf der Projektseite: 
                            http://dhvlab.gwi.uni-muenchen.de/index.php/Das_DHVLab_im_Einsatz [letzter Zugriff: 30. November 2016].
                        
             Vgl. insbesondere die Angebote in Köln (
                            http://www.itzertifikat.uni-koeln.de/), Passau (
                            http://www.phil.uni-passau.de/zertifikat-dh/) und Stuttgart („Das digitale Archiv“, 
                            http://www.uni-stuttgart.de/dda), letztgenanntes als Vorläufer eines DH-Masterstudienganges [letzter Zugriff: 30. November 2016].
                        
             Vor diesem Hintergrund erscheinen grundständige 
                            eLearning-Angebote wie „The Programming Historian“ (
                            http://programminghistorian.org/) für einen autodidaktischen Einstieg begrüßenswert. Die Initiatoren des DHVLab sind jedoch der Auffassung, dass eine umfassende Präsenzausbildung nicht ersetzt werden kann.
                        
         
         
            
               Bibliographie
               
                  Bartsch, Sabine / Borek, Luise / Rapp, Andrea (2016): 
                        „Aus der Mitte der Fächer, in die Mitte der Fächer: Studiengänge und Curricula – Digital Humanities in der universitären Lehre“,
                        in: 
                        Bibliothek – Forschung und Praxis 40 (2): 172–178 10.1515/bfp-2016-0030.
                    
               
                  DARIAH-EU: 
                        Digital Humanities Registry – Courses
                  https://dh-registry.de.dariah.eu/ [letzter Zugriff 30. November 2016].
                    
               
                  DHI Paris (Teamaccount) (2013): 
                        „Wissenschaftlicher Nachwuchs in den Digital Humanities: Ein Manifest“,
                        in: 
                        Digital Humanities am DHIP, 23. August 2013 
                        http://dhdhi.hypotheses.org/1995 [letzter Zugriff 30. November 2016].
                    
               
                  Ehrlicher, Hanno (2016): 
                        „Fingerübungen in Digitalien. Erfahrungsbericht eines teilnehmenden Beobachters der Digital Humanities aus Anlass eines Lehrexperiments“, 
                        in: 
                        Romanische Studien 4: 623–636 
                        http://www.romanischestudien.de/index.php/rst/article/view/88 [letzter Zugriff 30. November 2016].
                    
               
                  Koller, Guido (2016): 
                        Geschichte digital: Historische Welten neu vermessen. 
                        Stuttgart: Kohlhammer.
                    
               
                  Lücke, Stephan / Riepl, Christian (2016): 
                        „Auf dem Weg zu einem Curriculum in den Digital Humanities“,
                        in: 
                        Akademie Aktuell 57 (1): 74–77 
                        http://badw.de/fileadmin/pub/akademieAktuell/2016/56/0116_17_Riepl_V04.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Rehbein, Malte (2016): 
                        Geschichtsforschung im digitalen Raum. Über die Notwendigkeit der Digital Humanities als historische Grundwissenschaft. (Preprint) 
                        http://www.phil.uni-passau.de/fileadmin/dokumente/lehrstuehle/rehbein/Dokumente/GeschichtsforschungImDigitalenRaum_preprint.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Sahle, Patrick (2013): 
                        DH studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities (= DARIAH-DE Working Papers 1). 
                        Göttingen: GOEDOC 
                        http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-1.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Sahle, Patrick (2016): 
                        „Digital Humanities als Beruf. Wie wird man ein „Digital Humanist“, und was macht man dann eigentlich?“, 
                        in: 
                        Akademie Aktuell 57 (1): 78–83 
                        http://badw.de/fileadmin/pub/akademieAktuell/2016/56/0116_18_Sahle_V04.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Spiro, Lisa (2012): 
                        „Openingup Digital Humanities Education“, 
                        in: Hirsch, Brett D. (ed.):
                        Digital Humanities Pedagogy: Practices, Principlesand Politics 331–363 
                        http://www.openbookpublishers.com/product/161/ [letzter Zugriff 30. November 2016].
                    
               
                  Thaller, Manfred (2015): 
                        „Panel: Digital Humanities als Beruf – Fortschritte auf dem Weg zu einem Curriculum“, 
                        in: 
                        Digital Humanities als Beruf: Fortschritte auf dem Weg zu einem Curriculum, vorgelegt auf der Jahrestagung 2015 3–5 
                        https://www.digitalhumanities.tu-darmstadt.de/fileadmin/dhdarmstadt/materials/Digital_Humanities_als_Beruf_-_Stand_2015.pdf [letzter Zugriff 30. November 2016].
                    
            
         
      
   


10639	2017	
      
         
            Einführung und Forschungslage
            Wenn Peter Paul Rubens als »Tarantino des Barock« beschrieben wird (im 
                    Tagesspiegel, 2014) oder Alice Schwarzer als der »Erich Honecker des Feminismus« (in 
                    Cicero, 2014), dann handelt es sich um eine Vossianische Antonomasie. Diese Trope ist nach dem niederländischen Humanisten und Rhetoriklehrer Vossius benannt (und wird im Folgenden als ›Vossanto‹ abgekürzt, in Anlehnung an den Vorschlag von Fischer/Wälzholz 2014). Generell spricht man von Antonomasie, wenn eine bestimmte Eigenschaft einer Person für diese selbst steht (z. B. »der Leimener« für Boris Becker). Beim Spezialfall der Vossanto wird einer Person über die Nennung einer anderen (bekannteren, populäreren, berüchtigteren) Person als Referenzgröße eine bestimmte Eigenschaft zugeschrieben. Dabei sorgt ein »untypologisches, aktualisierendes Signal« (Lausberg 1960) für den Bedeutungstransfer (in den oben genannten Beispielen wären dies der Barock und der Feminismus). Anders ausgedrückt: Die Vossanto stellt über einen ›modifier‹ einen Zusammenhang zwischen ›source‹ und ›target‹ her (Bergien 2013). Entitäten können sowohl als ›source‹ als auch als ›target‹ auftreten, wie ebd. am Beispiel Obama demonstriert: bis 2011 trat er in Vossantos vor allem als ›target‹ auf, danach diente er immer mehr als ›source‹. Die ›source‹-Referenz wird im Fachdiskurs im Anschluss an Lakoff 1987 auch als ›paragon‹ bezeichnet (»a specific example that comes close to embodying the qualities of the ideal«, ebd.).
                
            Der Begriff »Vossianische Antonomasie« wird international kaum verwendet, stattdessen wird etwa zwischen »Antonomasia1« und »Antonomasia2« unterschieden: »metonymic« vs. »metaphorical antonomasia« (Holmqvist/Płuciennik 2010). Innerhalb dieses Klassifikationsschemas wäre unsere Vossanto ein Spezialfall von »Antonomasia2«, nämlich wenn es um »comparisons with paragons from other spheres of culture« geht: »Lyotard is a pope of postmodernism, Bush is no Demosthenes; and we can buy the Cadillac of vacuum cleaners.« (ebd.)
            Dieses Stilmittel, dessen reger Gebrauch seit der Antike belegt ist, ist heute medial ubiquitär anzutreffen. Oft findet es sich schon in Überschriften, da es zugleich informativ und rätselhaft sein kann und zudem oft unterhaltsame Qualitäten bietet. Eine eigene größere Sammlung an Musterexemplaren (
                    ) gab den Ausschlag, dieses Phänomen systematisch zu erforschen, mit historischer Perspektive und auf Grundlage größerer englischer und deutscher Zeitungskorpora. Ziel dieser Arbeit ist eine erste methodisch-explorative Analyse des Phänomens Vossanto in der Tageszeitung 
                    New York Times (1987–2007) und der Wochenzeitung 
                    Die Zeit (1995–2011). Die Korpora wurden aufgrund ihrer Verfügbarkeit, Bedeutung und ihres Umfangs gewählt. Die Extraktion der Vossantos erfolgte jeweils korpusspezifisch, um den verschiedenen Formaten und Sprachen Rechnung zu tragen.
                
         
         
            Englischsprachiges Korpus
            Aus den XML-Daten des 
                    New York Times-Korpus (Sandhaus 2008) wurde für jeden der 1.854.726 Artikel der Volltext extrahiert. Anschließend wurde der Text mit Hilfe des NLTK (Bird/Loper/Klein 2009) in Sätze zerlegt und die Wörter jedes Satzes mit dem Part-of-Speech-Tagger des NLTK annotiert. Zusätzlich wurden Named Entities (Personen, Orte, Organisationen) mit dem NLTK-eigenen Named-Entity-Extraktor annotiert. Die so annotierten Sätze wurden mit einer Liste von Vossanto-typischen Mustern (in Form von regulären Ausdrücken) abgeglichen. Eine vereinfachte Darstellung eines solchen Musters ist beispielsweise:
                
            \((PERSON|ORGANIZATION|GPE) *\) (is|has) (often|sometimes)? (been)? (called)? the \(PERSON|ORGANIZATION|GPE) *\) (of|among|from) \((PERSON|ORGANIZATION|GPE) *\)
            Die zu findenden drei Elemente einer Vossanto sind darin durch * gekennzeichnet. Passte ein Satz auf eines der Muster, so wurden diese drei Teile extrahiert und tabellarisch ausgegeben. Anschließend wurden die extrahierten 10.744 Kandidaten manuell überprüft. Nicht-Vossantos und Vossantos mit Städten und Firmen wurden entfernt (Treffer der Art »Algarve, the Riviera of Portugal« oder »Pepsi is the Nike of soft drinks« sind eine eigene Untersuchung wert) und der Fokus auf Vossantos gelegt, in denen Individuen (Personen, Tiere, fiktive Figuren) als ›source‹ oder ›target‹ dienen. 246 Vossantos blieben dabei übrig (Übersicht in unserem Arbeitsrepo, siehe Bibliografie), die sich wie folgt über das Korpus verteilen:
            
               
                  
                  vossantos_nyt.png
               
            
            Am häufigsten als ›source‹ verwendet wurden folgende Namen:
            
               
                  Anzahl
                  source
               
               
                  6
                  Michael Jordan
               
               
                  5
                  Michelangelo
               
               
                  4
                  Babe Ruth
               
               
                  3
                  Zelig
               
               
                  3
                  Rodney Dangerfield
               
               
                  3
                  Neil Young
               
               
                  3
                  Elvis
               
               
                  3
                  Don Quixote
               
            
            Als Beispiel für Treffer seien diejenigen für Michael Jordan genannt:
            
               »Romario is the 
                        Michael Jordan of soccer and Bebeto is the Magic Johnson of soccer« (1994)
                    
               »Bonfire, the 
                        Michael Jordan of dressage horses« (1998)
                    
               »Brian Foster, the 
                        Michael Jordan of BMX racing« (1998)
                    
               »The stunt biker Dave Mirra, the 
                        Michael Jordan of the dirt set« (2000)
                    
               »Cynthia Cooper is the 
                        Michael Jordan, the Larry Bird, the Magic Johnson of this league« (2000)
                    
               »McNabb has been called the 
                        Michael Jordan of the National Football League« (2001)
                    
            
            Trotz der zeitlichen Einschränkung des Korpus lassen sich bereits einige vielversprechende Beobachtungen anstellen und Thesen bilden: 1. Produktive Referenzgrößen einer Vossanto sind sowohl reale als auch fiktionale Figuren (Bsp. für letztere aus der obigen Liste: Woody Allens »Zelig«, Cervantes’ »Don Quixote«). 2. Öffentliche Personen oder bekannte fiktionale Charaktere haben bestimmte Eigenschaften, die sie für die Verwendung als Referenzgröße einer Vossanto prädestinieren oder nicht (es bleibt etwa zu erforschen, warum gerade Michael Jordan und Michelangelo sich so gut eignen und nicht andere Sportler bzw. Künstler). 3. Es gibt historisch stabile Referenzgrößen, deren Bekanntheit vorausgesetzt werden kann (z. B. Michelangelo), und es gibt ephemere Referenzgrößen, die ab irgendeinem Zeitpunkt nicht mehr als Bezugspunkt taugen (für das benutzte zeitgenössische Korpus eher noch nicht relevant).
         
         
            Deutschsprachiges Korpus
            Das deutsche Datenset besteht aus einer Sammlung des Archivs der Wochenzeitung 
                    Die Zeit und enthält die Artikel aus den Jahren 1995 bis 2011. Insgesamt umfasst das Korpus 126.702 Dokumente.
                
            Zunächst wurden die Volltexte (inklusive Überschriften) aller Dokumente extrahiert. Diese wurden dann mit Hilfe des Part-of-Speech-Taggers und Named-Entity-Recognition-Tools des Stanford CoreNLP Package verarbeitet. Für die Analyse deutschsprachiger Texte enthält Stanford CoreNLP speziell für das Deutsche trainierte Modelle (Faruqui und Pado 2010). Somit können alle Texte auf drei Ebenen untersucht werden: auf der Wortebene, der Part-of-Speech-Ebene sowie der Named-Entity-Ebene. Mithilfe von regulären Ausdrücken, die auf den verschiedenen Ebenen angewandt werden können, wurde dann nach Vossanto-Mustern gesucht. Im Gegensatz zur Verarbeitung des englischsprachigen Korpus wurde jedoch noch nicht versucht, auch das ›target‹ einer Vossanto zu extrahieren. Stattdessen wurden Muster entworfen, die das ›source‹-Objekt sowie das »aktualisierende Signal« matchen. Ausschlaggebend für diese Herangehensweise waren die in einem Testdurchlauf beobachtete hohe Anzahl an Vossantos ohne unmittelbaren Verweis auf das ›target‹ sowie eine große Vielfalt an möglichen Formulierungen, die auf die Relation zum ›target‹ hinweisen können. Mithilfe relativ strikter Regeln konnte die Anzahl an falschen Extraktionen im Rahmen gehalten werden. Ein vereinfachtes Beispiel für eine Extraktionsregel lautet etwa: »eine Art PERSON (der|des) (ADJECTIVE)? NOUN«.
            Die Produktivität der beiden häufigsten Referenznamen des NYT-Korpus bestätigt sich im verwendeten deutschen Korpus, etwa wenn vom »Michael Jordan der analytischen Philosophie« die Rede ist (
                    Die Zeit 44/1999) oder vom »bulgarischen Michelangelo« (
                    Die Zeit 14/2001). Ansonsten scheint es sprachen- bzw. kulturspezifische Präferenzen zu geben. Die häufigsten ›sources‹ sind:
                
            
               
                  Anzahl
                  source
               
               
                  9
                  Robin Hood
               
               
                  6
                  Bill Gates
               
               
                  4
                  Franz Beckenbauer
               
               
                  3
                  Daniel Düsentrieb
               
               
                  3
                  Heinz Rühmann
               
               
                  3
                  James Dean
               
               
                  3
                  Jesus Christus
               
               
                  3
                  Norbert Blüm
               
               
                  3
                  Willy Brandt
               
            
            Ähnlich wie im NYT-Korpus ist erkennbar, wie stark typisierend mythische bzw. fiktive Figuren sind (Robin Hood, Daniel Düsentrieb). Daneben zeigt sich, dass »Bill Gates«, der im NYT-Korpus nur zweimal als ›source‹ einer Vossanto vorkommt, im 
                    Zeit-Korpus sechs Mal als Referenz vertreten ist:
                
            
               »eine Art Bill Gates des Stolperns« (1998)
               »Der Bill Gates von Aurich« (2001)
               »der Bill Gates von Ostfriesland« (2001)
               »der Bill Gates von Aurich« (2002)
               »der britische Bill Gates« (2008)
               »der Bill Gates von Estland« (2010)
            
            Die wiederholte Verwendung des »Bill Gates von Aurich« zeigt, wie stark ein ›target‹ mit einer ›source‹ verwachsen kann. (Paradebeispiel hierfür ist im Übrigen Vittorio Hösle, »der Boris Becker der Philosophie«, eine Bezeichnung, die es bis in den Wikipedia-Artikel zu Hösle geschafft hat.) Am Beispiel Bill Gates’ lässt sich wie zuvor am Beispiel Obama demonstrieren, dass ein Name sowohl als ›target‹ als auch als ›source‹ vorkommen kann. Bevor Bill Gates selbst als Referenz verwendet wird, wird er in einem Artikel von 1995 noch durch eine andere Person beschrieben: »Bill Gates ist der Henry Ford des Computerzeitalters«.
            Insgesamt wurden aus 1.456 Vossanto-Kandidaten 225 manuell als Vossantos markiert, die sich wie folgt über die im Korpus vorhandenen Jahre verteilen:
            
               
                  
                  vossantos_zeit.png
               
            
            Zu den fälschlich extrahierten Named Entities gehören »der Berliner Klaus Wowereit«, »der deutsche Michel« oder »der Anton aus Tirol«, stehende Wendungen, die grammatisch unseren definierten Vossanto-Mustern entsprechen.
         
         
            Erkenntnisse und Ausblick
            Die Vossanto ist als Stilmittel nur scheinbar einfach strukturiert, das Erstellen von Extraktionsregeln daher alles andere als trivial. Die vorliegenden Skripte weisen bekannte Lücken auf, die Qualität hängt v. a. von der Verlässlichkeit der benutzten NER-Tools und der Präzision der definierten Muster ab. Fehlende Goldannotationen für dieses Phänomen erschweren zudem eine Evaluierung. Die vorliegende Arbeit hat daher explorativen Charakter, die Optimierung von Precision und Recall lag noch nicht in deren Fokus, ist aber das nächste Ziel dieses Projekts.
            Trotz der genannten Einschränkungen konnten durch diesen korpusbasierten Ansatz neue Erkenntnisse zur Vielgestaltigkeit des Phänomens ›Vossianische Antonomasie‹ gewonnen werden. So lassen sich zahlreiche Spezialfälle unterscheiden und systematisch untersuchen (vgl. auch Fischer/Wälzholz 2014), beispielhaft genannt seien:
            
               Tiere als ›target‹ (»
                        Sea Hero is the Bobo Holloman of racing«, NYT, 1993; »
                        Bonfire, the Michael Jordan of dressage horses«, NYT, 1998),
                    
               Feminisierungen (Adele Schopenhauer, »eine Art 
                        Donna Quichotta des Weimarer Musenvereins«, 
                        Die Zeit 18/2002; »Tracey [Emin], die 
                        Donna Giovanna der britischen Gegenwartskunst«, 
                        Die Zeit 9/2006; »Kati Witt ist jetzt eine 
                        Franziska Beckenbauer der Münchner Olympiabewerbung.«, 
                        Die Zeit 39/2010),
                    
               nicht individualisierbare ›sources‹: »the [God, King, Queen, Satan, Emperor, Oracle, Shogun, Czar, Sultan, Buddha] of«,
               mythologische und fiktive Figuren als ›sources‹: »the [Santa Claus, Midas, Godzilla, Pied Piper, Energizer Bunny, Jupiter, Icarus] of«,
               Personifizierungen, also der Einsatz individueller Personen/Figuren als ›source‹ für Firmen, Vereine, Bands oder Orte als ›target‹ (»
                        Sturm, Ruger is the 
                        Benedict Arnold of the gun industry«, NYT, 1989; »
                        Aerosmith, the 
                        Dorian Gray of rock bands«, NYT, 1993; »the 
                        Hudson has been the 
                        John Barrymore of rivers, noble in profile but a sorry wreck«, NYT, 1996; »the 
                        National Collegiate Athletic Association, the 
                        Kenneth Starr of sports«, NYT, 1998).
                    
            
            Zu letzteren Beispielen gehört nun endlich auch der titelgebende »Helmut Kohl unter den Brotaufstrichen« (
                    der Freitag 35/2011).
                
            Auch zur Distribution der Vossantos innerhalb der beiden Zeitungskorpora ließen sich belastbare Ergebnisse gewinnen. Demnach sind Vossantos besonders im Kultur- und Sport-Ressort beliebt (Vorkommen in der Sektion »Arts« der NYT: 78; in der Sektion »Sports«: 57; auf dem nächsten Rang mit großem Abstand »New York and Region«: 28 – im »Feuilleton + Literatur«-Ressort der Zeit: 76, »Politik«: 54, nächstrangig ist weit entfernt »Wirtschaft« mit 23 Vorkommen; »Sport« hat hier keine Treffer, denn die gedruckte 
                    Zeit hat kein dediziertes Sport-Ressort).
                
         
      
      
         
            
               Bibliographie
               
                  Bergien, Angelika (2013):
                        „Names as frames in current-day media discourse“, 
                        in: Felecan, Oliviu (ed.): 
                        Name and Naming. Proceedings of the second international conference on onomastics. Cluj-Napoca: Editura Mega 2013: 19–27.
                    
               
                  Bird, Steven / Loper, Edward / Klein, Ewan (2009): 
                        Natural Language Processing with Python. 
                        O’Reilly Media Inc.
                    
               
                  Faruqui, Manaal / Pado, Sebastian (2010):
                        „Training and Evaluating a German Named Entity Recognizer with Semantic Generalization“, 
                        in: 
                        Proceedings of Konvens 2010.
                    
               
                  Fischer, Frank / Wälzholz, Joseph (2014): 
                        „Jeder kann Napoleon sein: Vossianische Antonomasie: Eine Stilkunde“, 
                        in:
                        Frankfurter Allgemeine Sonntagszeitung 51 (21. Dezember 2014): 34 
                        .
                    
               
                  Holmqvist Kenneth / Płuciennik Jarosław (2010): 
                        „Princess antonomasia and the truth: Two types of metonymic relations“,
                        in: Burkhardt, Armin / Nerlich, Brigitte (eds.): 
                        Tropical Truth(s): The Epistemology of Metaphor and Other Tropes. 
                        Berlin/New York: De Gruyter 373–381 
                        10.1515/9783110230215.
                    
               
                  Lakoff, George (1987): 
                        Women, Fire, and Dangerous Things: What Categories Reveal about the Mind. 
                        Chicago: The University of Chicago Press.
                    
               
                  Lausberg, Heinrich (1960): 
                        Handbuch der literarischen Rhetorik. Eine Grundlegung der Literaturwissenschaft 2. 
                        München: Hueber.
                    
               
                  Sandhaus, Evan (2008): 
                        The New York Times Annotated Corpus LDC2008T19. DVD. 
                        Philadelphia: Linguistic Data Consortium.
                    
               
                  Arbeitsrepositorium: 
                        
               
               
                  Folien zum Vortrag: 
                        
               
            
         
      
   


10661	2017	
      
         
            Forschungsstand
            Die Anwendung von Methoden der Netzwerkanalyse auf literarische Texte hat sich in den letzten Jahren zu einem eigenständigen Forschungsfeld der 
                    Digital Literary Studies entwickelt. Im Vordergrund stehen dabei häufig computerlinguistische Fragen, insbesondere solche nach der automatisierten Extraktion von Netzwerkdaten (z.B. qua 
                    named entity recognition, 
                    co-reference resolution) und deren Evaluation (u. a. Elson et al. 2010; Park et al. 2013; Agrarwal et al. 2013; Rochat 2014; Fischer et al. 2015; Waumans et al. 2015; Jannidis et al. 2016). 
                
            Darüber hinaus wird ausgelotet, inwiefern sich mittels visueller und/oder statistischer Auswertung der Netzwerkdaten genuin literaturwissenschaftliche Erkenntnisse gewinnen bzw. neue Wege der literaturwissenschaftlichen Analyse entwickeln lassen: Neben Ansätzen zur quantitativen Beschreibung und Hierarchisierung des Figurenpersonals (Jannidis et al. 2016) werden hier, im Rahmen korpusbasierter Analysen, Optionen der literaturhistorischen Periodisierung auf Basis von quantitativen Strukturdaten diskutiert (Trilcke et al. 2015) sowie Typen der ästhetischen Modellierung sozialer Formationen in und durch literarische Texte differenziert (Stiller et al. 2003; Stiller & Hudson 2005; Trilcke et al. 2016).
         
         
            Forschungsdesiderat: Plotanalyse
            Nahezu keine Rolle spielte bisher jedoch ein durchaus hehres Erkenntnisversprechen, das – bereits in der prä-automatisierten Zeit formuliert (de Nooy 2006) – auch den Fluchtpunkt des einschlägigen ›Pamphlets‹ von Franco Moretti steht: dass nämlich die Netzwerkanalyse als ein Instrumentarium der quantitativen »plot analysis« (Moretti 2011) fungieren könne. 
            Tatsächlich lässt sich dieses Erkenntnisversprechen mit den derzeit verfolgten Ansätzen im Bereich der literaturwissenschaftlichen Netzwerkanalyse kaum aufgreifen, geschweige denn einlösen (so auch Prado et al. 2016). Denn die sequentielle Dimension literarischer Texte, mithin ihre Temporalität, bleibt hier in der Regel ausgeblendet: Erfasst, visualisiert und analysiert werden statische Netzwerke. Plot ist jedoch wesentlich ein Konzept, das die Temporalität narrativer (wie auch dramatischer) 
                    1 Texte theoretisch fassen soll: »the repeated attempts to redefine parameters of plot reflect both the centrality and the complexity of the temporal dimension of narrative« (Dannenberg 2005: 435). Plot lässt sich begreifen als Konzept zur Beschreibung der »progressive structuration« (Kukkonen 2013, §4) literarischer Texte. 
                
            Versuche, die Netzwerkanalyse in Richtung einer quantitativen Plotanalyse weiterzuentwickeln, stehen also zunächst vor der Aufgabe, bei ihrer Modellierung des Untersuchungsgegenstandes die Zeitdimension zu berücksichtigen. Der Text ist entsprechend nicht lediglich als ein statisches Netzwerk zu modellieren, sondern als eine sich über die Zeit verändernde Folge von Netzwerkzuständen. Erst anhand dieser Netzwerkdynamiken lassen sich die Erkenntnispotenziale, die netzwerkanalytische Zugänge für die quantitative Plotanalyse bergen, überhaupt diskutieren. 
         
         
            Forschungsvorhaben 
            Der projektierte Vortrag wird – in Anschluss an Prado et al. 2016 – aus theoretischer und methodischer Perspektive sowie anhand exemplarischer Fallstudien eine Erweiterung der bisherigen, auf die Analyse 
                    statischer Strukturen fokussierten Forschung zu literarischen Netwerken um die Analyse 
                    progressiver  Strukturierungen vorschlagen. Übergreifendes Ziel ist es, zu prüfen, ob (und mit welchen Einschränkung) sich auf diesem Wege ein Beitrag zur Operationalisierung des literaturwissenschaftlichen Plot-Konzepts erarbeiten lässt. Dabei soll es nicht darum gehen, das semantische reiche und vielseitige Plot-Konzept der ›traditionellen‹ Literaturwissenschaft durch ein quantitatives und insofern notgedrungen reduktionistisches Konzept zu ersetzen. Vielmehr soll zunächst der wesentlich bescheidenere Nachweis erbracht werden, dass sich bestimmte Aspekte dessen, was gemeinhin im Rahmen des Plot-Konzepts diskutiert wird, durchaus mittels der computerbasierten Analyse von Netzwerkdynamik beobachten lassen, etwa ereignishafte Konfliktverläufe (so schon Moretti 2011), Formen der sozialen Integration und Desintegration von Figuren oder basale Techniken der Handlungsführung, z.B. die Komposition von Haupt- und Nebenhandlung(en). 
                
            Entsprechend der zweigleisigen Auswertungsroutinen, die auf netzwerkanalytische Daten angewendet werden, wird der Vortrag zwei Szenarien der netzwerkbasierten Analyse der progressiven Strukturierung literarischer Texte diskutieren: zum einen (3.1) sind Möglichkeiten und Erkenntnispotenziale der 
                    Visualisierung dynamischer Netzwerke, zum anderen (3.2) Möglichkeiten und Erkenntnispotenziale der Berechnung
                     netzwerkanalytischer Maße für dynamische Netzwerke auszuloten.
                
            
               Visualisierung von Netzwerkgraphen
               Während die Visualisierung dynamischer Netzwerke in anderen Domänen bereits seit längerem gang und gäbe ist (vgl. exemplarisch Pohl et al. 2008; Frederico et al. 2011), wurde erst vor Kurzem der Versuch unternommen, entsprechende Visualisierungsverfahren auch auf literarische Netzwerke anzuwenden (Xanthos et al. 2016). Während Xanthos et al.  u.a. auf didaktische Anwendungsszenarien hinweisen, wird ein literaturwissenschaftliches Erkenntnispotenzial lediglich angedeutet; eine Diskussion dessen, was durch eine solche Visualisierung nicht nur 
                        sichtbar, sondern auch 
                        erkennbar wird, bleibt aus. 
                    
               Hingegen zeigen erste, im Vortrag zu vertiefende Zwischenergebnisse unserer Analysen, dass die dynamische Visualisierung insbesondere dann erkenntnisrelevant wird, wenn es darum geht, multiplexe Netzwerke zu modellieren, d. h. Netzwerke, die unterschiedliche Interaktionstypen zugleich erfassen. So zeigt eine statische Visualisierung von Lessings bürgerlichem Trauerspiel 
                        Emilia Galotti die Familie Galotti als eine geschlossene Triade (siehe Abb. 1): Die Kanten symbolisieren hier szenische Kopräsenzen (Interaktionstyp 1), wobei jene Kanten, die 
                        zugleich Verwandtschaftsverhältnisse darstellen, rot erscheinen (Interaktionstyp 2). 
                    
               
                  
               
               Abb. 1: Statisches Netzwerk zu Lessing: 
                        Emilia Galotti (rote Knoten: Familienmitglieder; rote Kanten: Familienmitglieder sind szenisch kopräsent)
                    
               Zerlegt man das statische Dramennetzwerk (Abb. 1) nun nach Akten und dynamisiert es damit, so zeigt sich, dass die Familie Galotti zu keinem Zeitpunkt des Dramas gemeinsam auf der Bühne steht (vgl. Abb. 2). 
               
                  
               
               Abb. 2: Dynamisches Netzwerk zu Lessings 
                        Emilia Galotti, zerlegt nach Akten
                    
               Anschaulich und 
                        erkennbar wird auf diese Weise eine Position der traditionellen Forschung, nach der Lessing in 
                        Emilia Galotti nicht nur die äußere Bedrohung der ›bürgerlichen‹ Kleinfamilie, sondern auch deren innere Problematik inszeniert hat (siehe z. B. Alt 1994: 268). Die Analyse der dynamischen Strukturierung zeigt hier die soziale Desintegration der familäre Triade, die als formal beschreibbarer Teilaspekt des zentralen dramatischen Konflikts verstanden werden kann. 
                    
               Dass dynamische Visualisierungen in diesem Sinne aus literaturwissenschaftlicher Sicht v.a. für die Analyse multiplexer Netzwerke produktiv gemacht werden können, werden wir im Vortrag anhand weiterer Beispiele aus dem dlina-Korpus (philologisch kuratierte Netzwerkdaten zu 465 deutschsprachige Dramen aus der Zeit 1730–1930, siehe 
                        https://dlina.github.io/Introducing-DLINA-Corpus-15-07-Codename-Sydney/) zeigen. Darüber hinaus werden wir zum Zweck eines intergenerischen Vergleichs exemplarisch dynamische Visualisierung von Romannetzwerken diskutieren. Zu reflektieren sind hier insbesondere Fragen der Sequenzierung: Während Dramen mit ihrer Einteilung in Akte und Szenen eine naheliegende Segmentierung vorgeben, liefert die romantypische Einteilung in Kapitel keine vergleichbar überzeugenden Ergebnisse. 
                    
            
            
               Berechnung netzwerkanalytische Maße 
               Mehr noch als die Visualisierung statischer Netzwerke stellt diejenige dynamischer im Grunde keine Option eines korpusbasierten 
                        distant reading dar. Sie ermöglicht zwar die anschauliche Modellierung einzelner Netzwerke, kann aber nur begrenzt Erkenntnisse über eine große Anzahl von Netzwerken liefern: Methoden, mit denen sich die auf algorithmischen Layouts basierenden Netzwerkgraphen kontrolliert miteinander vergleichen lassen, fehlen weitgehend; zudem kostet die Rezeption von dynamischen Visualisierungen – etwa der von Xanthos et al. 2016 präsentierten Prototypen – schlicht Zeit, wir haben es hier also eher mit 
                        fast reading, denn mit 
                        distant reading zu tun. Die Berechnung netzwerkanalytischer Maße und deren statistische Weiterverarbeitung bietet hingegen Möglichkeiten, aus einer dezidierten 
                        distant reading-Perspektive sowohl allgemeine Charakteristika der Netzwerke eines Korpus zu beschreiben als auch, vergleichend, spezifische formale Typen von Netzwerken innerhalb des Korpus zu identifizieren (entsprechend unserer Überlegungen zum 
                        Small World-Phänomen in statischen Netzwerken, siehe Trilcke et al. 2016). 
                    
               Von Carley (2003: 135–136) wurden dabei mehrere rudimentäre globale Maße (i. e. 
                        size, 
                        density, 
                        homogeneity in the distribution of ties, 
                        rate of changes in nodes, 
                        rate of changes in ties) für die Analyse dynamischer Netzwerke vorgeschlagen. Darüber hinaus haben Prado et al. 2016 für die Anwendung von akteursorientierten Maßen, v.a. Zentralitätsindices, bei der Rekonstruktion von Plot
                        -Verläufen plädiert. Im Vortrag werden wir einzelne dieser Maße – u. a. 
                        size pro Akte und Szenen; 
                        density pro Akte und Szene; die 
                        change-
                        rates; sowie einfache Zentralitätsmaße – für das dlina-Korpus berechnen; die dafür nötigen Daten liegen bereits, philologisch kuratiert, in den dlina-Zwischenformat-Dateien vor (zum Zwischenformat: 
                        https://dlina.github.io/Introducing-Our-Zwischenformat/ – die Daten sind offen, siehe unser Github-Repositorium: 
                        https://github.com/dlina); eine entsprechende Erweiterung des in Python geschriebenen Auswertungstools 
                        dramavis (Kittel / Fischer 2016) wird derzeit entwickelt. Die erhobenen Daten werden wir schließlich mit Rekurs auf ausgewählte literaturwissenschaftliche Konzepte für die Beschreibung spezifischer Plot-Phänomene diskutieren, insbesondere in Hinblick auf Expositionstypen (Pfister 1977: 124–136), auf die ›klassische‹ Aktstruktur der Tragödie sowie auf das Kompositionsprinzip von Haupt- und Nebenhandlung (Pfister 1977: 286–289).
                    
            
         
         
            Resümee
            Der Vortrag liefert einen Beitrag zur Methodenentwicklung und -reflektion im Bereich der 
                    Digital Literary Studies. Auf literaturtheoretisch-methodologischer Ebene diskutiert er Möglichkeiten einer netzwerkanalytischen Operationalisierung des literaturwissenschaftlichen Plot
                    -Konzepts, wobei der literarische Text zu diesem Zweck nicht, wie bisher die Regel, als statische Struktur, sondern als ›progressive Strukturierung‹ modelliert wird. Als empirische Grundlage der Methodendiskussion fungieren Analysen von Dramen und Romanen, in denen exemplarisch die Potenziale und die Grenzen des Ansatzes verdeutlich werden. 
                
         
      
      
         
             Unter systematischen Gesichtspunkten können die Unterschiede zwischen narrativen und dramatischen Texten in Hinblick auf das Plot-Konzept zunächst vernachlässigt werden (vgl. Korthals 2003); entsprechend wurden sowohl ›epische‹ als auch ›dramatische‹ Texte bis ins 19. Jahrhundert hinein verschiedentlich unter dem Oberbegriff ›pragmatische Gattung‹ vereint. 
         
         
            
               Bibliographie
               
                  Agarwal, Apoorv / Corvalan, Augusto / Jensen, Jacob / Rambow, Owen (2012): 
                        „Social Network Analysis of Alice in Wonderland“, 
                        in: 
                        Proceedings of the Workshop on Computational Linguistics for Literature. 
                        Montréal 88–96 
                        http://www.aclweb.org/anthology/W12-2513 [letzter Zugriff 25. August 2016].
                    
               
                  Alt, Peter-André (1994): 
                        Die Tragödie der Aufklärung. Eine Einführung. 
                        Tübingen / Basel: Francke.
                    
               
                  Carley, Kathleen M. (2003): 
                        „Dynamic Network Analysis“, 
                        in: Breiger, Ronald / Carley, Kathleen M. / Pattison, Philipp (eds.): 
                        Dynamic Social Network Modeling and Analysis. Workshop Summary and Papers. 
                        Washington D.C.: 133–145 
                        http://www.nap.edu/read/10735/chapter/9.
                    
               
                  Dannenberg, Hilary (2005): 
                        „Plot“, 
                        in: Herman, David / Jahn, Manfred / Ryan, Marie-Laure (eds.): 
                        The Routledge Encyclopedia of Narrative Theory. 
                        London: Routledge 435–439.
                    
               
                  Fischer, Frank / Göbel, Mathias / Kampkaspar, Dario / Trilcke, Peer (2015): 
                        „Digital Network Analysis of Dramatic Texts“, 
                        in: 
                        DH2015: Global Digital Humanities
                  http://dh2015.org/abstracts/xml/FISCHER_Frank_Digital_Network_Analysis_of_ Dramati/FISCHER_Frank_Digital_Network_ Analysis_of_Dramatic_Text.html [letzter Zugriff 25. August 2016].
                    
               
                  de Nooy, Wouter (2006): 
                        „Stories, Scripts, Roles, and Networks“, 
                        in: 
                        Structure and Dynamics 1.2 
                        http://escholarship.org/uc/item/8508h946#page-1 [letzter Zugriff 25. August 2016].
                    
               
                  Elson, David K. / Dames, Nicholas / McKeown, Kathleen R. (2010): 
                        „Extracting Social Networks from Literary Fiction“, 
                        in: 
                        Proceedings of ACL-2010. 
                        Uppsala: 138–147 
                        http://dl.acm.org/ft_gateway.cfm?id=1858696&type=pdf&CFID=659731302 &CFTOKEN=83466756 [letzter Zugriff 25. August 2016].
                    
               
                  Federico, Paolo / Aigner, Wolfgang / Miksch, Silvia / Windhager, Florian / Zenk, Lukas (2011): 
                        „A Visual Analytics Approach to Dynamic Social Networks“, 
                        in: 
                        Proceedings of the 11th International Conference on Knowledge Management and Knowledge Technologies (i-KNOW).
                        Graz 
                        http://publik.tuwien.ac.at/files/PubDat_198995.pdf [letzter Zugriff 25. August 2016].
                    
               
                  Jannidis, Fotis / Reger, Isabella / Krug, Markus / Weimer, Lukas / Macharowsky, Luisa / Puppe, Frank (2016): 
                        „Comparison of Methods for the Identification of Main Characters in German Novels“, 
                        in: 
                        DH2016: Conference Abstracts 578–582 
                        http://dh2016.adho.org/abstracts/297 [letzter Zugriff 25. August 2016].
                    
               
                  Kittel, Christopher / Fischer, Frank (2016): 
                        dramavis (v0.2.1). GitHub 
                        https://github.com/lehkost/dramavis [letzter Zugriff 25. August 2016].
                    
               
                  Korthals, Holger (2003): 
                        Zwischen Drama und Erzählung. Ein Beitrag zur Theorie geschehensdarstellender Literatur. 
                        Berlin: Erich Schmidt
                    
               
                  Kukkonen, Karin (2013): 
                        „Plot“, 
                        in: Hühn, Peter et al. (eds.): 
                        The Living Handbook of Narratology. Hamburg 
                        http://www.lhn.uni-hamburg.de/article/plot [letzter Zugriff 25. August 2016].
                    
               
                  Moretti, Franco (2011): 
                        Network Theory, Plot Analysis (= Stanford Literary Lab Pamphlets, No. 2). 1.5.2011. 
                        http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf [letzter Zugriff 25. August 2016].
                    
               
                  Park, Gyeong-Mi / Kim, Sung-Hwan / Cho, Hwan-Gue (2013): 
                        „Structural Analysis on Social Network Constructed from Characters in Literature Texts“, 
                        in: 
                        Journal of Computers 8.9: 2442–2447 
                        http://ojs.academypublisher.com/index.php/jcp/article/view/jcp080924422447/7672 [letzter Zugriff 25. August 2016].
                    
               
                  Pfister, Manfred (1977): 
                        Das Drama. Theorie und Analyse.
                        München: Fink.
                    
               
                  Pohl, Mathias / Reitz, Florian / Birke, Peter (2008): 
                        „As Time Goes by. Integrated Visualization and Analysis of Dynamic Networks“, 
                        in: 
                        AVI 2008 – Proceedings of the Working Conference on Advanced Visual Interfaces. 
                        Neapel 372–375 
                        http://doi.acm.org/10.1145/1385569.1385636 [letzter Zugriff 25. August 2016].
                    
               
                  Prado, Sandra D. / Dahmen, Silvio R. / Bazzan, Ana L.C. / Carron, Padraig Mac / Kenna, Ralph (2016): 
                        „Temporal Network Analysis of Literary Texts“, 24.2.2016 
                        https://arxiv.org/pdf/1602.07275 [letzter Zugriff 25. August 2016].
                    
               
                  Rochat, Yannick (2014): 
                        Character Networks and Centrality. 
                        Thèse de Doctorat. Lausanne 
                        https://infoscience.epfl.ch/record/203889/files/yrochat_thesis_infoscience.pdf [letzter Zugriff 25. August 2016].
                    
               
                  Stiller, Jaames / Nettle, Daniel / Dunbar, Robin I. M. (2003): 
                        „The Small World of Shakespeare's Plays“, 
                        in: 
                        Human Nature 14: 397–408 
                        https://www.staff.ncl.ac.uk/daniel.nettle/shakespeare.pdf [letzter Zugriff 25. August 2016].
                    
               
                  Stiller, James / Hudson, Mathew (2005): 
                        „Weak Links and Scene Cliques Within the Small World of Shakespeare“, 
                        in: 
                        Journal of Cultural and Evolutionary Psychology 3: 57–73.
                    
               
                  Trilcke, Peer / Fischer, Frank / Göbel, Mathias / Kampkaspar, Dario (2015): 
                        „200 Years of Literary Network Data“ [Blogposts], 
                        https://dlina.github.io/200-Years-of-Literary-Network-Data/ [letzter Zugriff 25. August 2016].
                    
               
                  Trilcke, Peer / Fischer, Frank / Göbel, Mathias / Kampkaspar, Dario / Kittel, Christopher (2016): 
                        „Theatre Plays as ›Small Worlds‹? Network Data on the History and Typology of German Drama, 1730-1930“, 
                        in: 
                        DH2016: Conference Abstracts 417–419 
                        http://dh2016.adho.org/abstracts/407 [letzter Zugriff 25. August 2016].
                    
               
                  Waumans, Michaël C. / Nicodème, Thibaut / Bersini, Hugues (2015): 
                        „Topology Analysis of Social Networks Extracted from Literature“, 
                        in: 
                        Plos One 3. Juni 2015 
                        10.1371/journal.pone.0126470.
                    
               
                  Xanthos, Aris / Pante, Isaac / Rochat, Yannick / Grandjean, Martin (2016): 
                        „Visualising the Dynamics of Character Networks“, 
                        in: 
                        DH2016: Conference Abstracts 417–419 
                        http://dh2016.adho.org/abstracts/407 [letzter Zugriff 25. August 2016].
                    
            
         
      
   


10676	2018	
      
         Die Forschungstätigkeiten Georeferencing und Entity Linking sind wichtiger Bestandteil vieler DH-Projekte. Webservices/APIs und Tools versuchen diese Tätigkeiten zu vereinfachen und zu beschleunigen. Eines der bekannteren Tools, wenigstens im deutschsprachigen Raum ist dabei vermutlich der ‘DARIAH-DE Datasheet Editor’. Dieser zeichnet sich durch seine einfache Benutzung aus, sei es was den Datenimport (ausfüllen einer Tabelle oder Hochladen einer CSV-Tabelle) betrifft oder die anschließende Disambiguierung/Verifizierung der vom ‘Getty Thesaurus of Geographic Names’ zurückgelieferten Treffer über ein Graphical User Interface. 
         Das Projekt TEIHencer greift diese Vorzüge des ‘DARIAH-DE Datasheet Editors’ auf und versucht diese einerseits mit der ‘TEI-Welt’ zu verknüpfen sowie mit GeoNames und der GND zwei alternative Normdaten Ressourcen einzubinden.
         Konkret handelt es sich bei TEIHencer um ein Plug-In zu dem Python/Django basierten prosopographisch-geographischen Informationssystem APIS. Mit Hilfe des TEIHencers ist es möglich, XML/TEI kodierte Texte in denen Lokalitäten, Orte ausgezeichnet sind, über eine Webformular in APIS zu importieren. Während des Imports werden die Orts-Entitäten entsprechend eines vom Benutzer wählbaren X-Path Ausdruckes geparst, gegen GeoNames und GND abgeglichen und im Falle von Übereinstimmung angereichert und in einer relationalen Datenbank gespeichert. Die gespeicherten Entitäten können anschließend über das APIS-Web-Interface im Falle mehrerer Treffer disambiguiert werden. Dies erfolgt über eine Kartendarstellung, in welcher die verschiedenen Treffer zu einer Entität aufscheinen. Darüber hinaus können über das APIS-Web-Interface noch weitere Informationen zu den Entitäten ergänzt (z.B. alternative Schreibweisen, Datierungen) sowie die einzelnen Entitäten miteinander in typisierte Beziehungen gesetzt werden (z.B. Ort A ist Nachfolger von Ort B.; Ort A ist Teil von Ort B).
         Die mit Hilfe von TEIHencer angereicherten Daten können dann wieder als XML/TEI Dokument (kodiert als  Element) exportiert bzw. über HTTP GET request abgerufen und so etwa in andere Applikationen eingebunden werden.
         Im Zuge der Posterpräsentation soll der TEIHencer der einschlägigen DH-Comunity vorgestellt werden und zwar an dem konkreten Fallbeispiel der “Andreas Okopenko: Tagebücher aus dem Nachlass (Hybridedition)”. Dabei handelt es sich um ein digitales Editionsprojekt, das eine Auswahl der Tagebücher Andreas Okopenkos im Zeitraum von 1949 bis 1955 inhaltlich erschließen und einem breiteren Publikum zugänglich machen möchte. Einer der Schwerpunkte des Projekts liegt hierbei auf der inhaltlichen Erschließung des örtlichen Wirkungs- und Schaffensraums des Nachkriegsavantgardisten, indem nicht nur erwähnte Orte (), sondern nach Maßgabe auch Werke und Organisationen ( und ) mit geographischen Normdaten verknüpft werden, um so ein umfassenderes Bild von Okopenkos kulturellem Kontext vermitteln zu können.
         Neben der eigentlich Applikation und des konkreten Use-Cases wird am Poster auch das Konferenzthema “Kritik der Digitalen Vernunft” bzw. das Subthema “Kritik digitaler Angebote, Projekte und Werkzeuge” in Form der Frage nach der Nachhaltigkeit des vorgestellten Tools reflektiert. Eine solche glauben wir nämlich insofern gewährleisten zu können, als das Tool a) in ein konkretes Projekt (Okopenko) eingebettet ist, b) einen weit verbreiteten Standard (TEI) unterstützt, c) auf bestehende Eigenentwicklungen (APIS) aufbaut und d) Teile des Codes als selbstständige Module (TEI-Modul als python-package) konzipiert sind, die auch jenseits der konkreten Applikation Anwendung finden können. Darüber hinaus, e) ist der gesamte Code auf GitHub publiziert [3].  
      
      
         
            
               
                    http://www.getty.edu/research/tools/vocabularies/tgn/index.html
                
            
            
               
                    https://geobrowser.de.dariah.eu/edit/index.html
                
            
            
               
                    https://github.com/acdh-oeaw/teihencer
                
            
            
               
                    https://github.com/acdh-oeaw/apis-core
                
            
            
               
                    https://www.onb.ac.at/bibliothek/sammlungen/literatur/forschung/projekte/andreas-okopenko-tagebuecher-aus-dem-nachlass-hybridedition/
                
            
         
      
   


10691	2018	
      
         
            Einführung
            Das laufende DFG-Projekt „Redewiedergabe“ stellt einen Anwendungsfall quantitativer Sprach- und Literaturwissenschaft dar und beschäftigt sich mit dem Phänomen „Redewiedergabe“ auf der Grundlage großer Datenmengen. Zu diesem Zweck wird zum einen ein Korpus manuell mit Redewiedergabeformen annotiert, zum anderen werden Verfahren zur automatischen Erkennung des Phänomens entwickelt. Ziel ist es, Forschungsfragen nach der Entwicklung von Redewiedergabe vor allem im 19. Jahrhundert zu beantworten. 
            Das Poster präsentiert einen Überblick über das Gesamtprojekt sowie erste Projektergebnisse.
         
         
            Stand der Forschung
            Sowohl aus linguistischer als auch aus literaturwissenschaftlicher Perspektive ist Redewiedergabe ein interessantes Phänomen. Die Art und Weise, wie die Figurenstimme in die Erzählung eingebunden ist, steht in engem Zusammenhang mit Erzählweise und -haltung, sowie der Konstruktion der erzählten Welt. Folglich wird dem Phänomen in der Erzählforschung viel Aufmerk­samkeit geschenkt und es liegen zahlreiche systematische Analysen vor (vgl. z.B. Genette 1998; Martínez / Scheffel 2007). Zu Phänomenen wie der erlebten Rede, dem Bewusstseinsstrom usw. gibt es eine umfangreiche Spezialforschung (Überblick bei McHale 2014). Aus linguistischer Perspektive ist Redewiedergabe vor allem in Bezug auf den Funktionswandel des Konjunktivs im Zusammenhang mit seinem Auftreten in indirekter Rede untersucht worden (vgl. z.B. Übersicht in Ágel 2000). In geringem Umfang sind auch Redewiedergabeverben und ihr Verhältnis zur wiedergegebenen Rede in das Blickfeld der Forschung gerückt (eine kurze Synopse bei Fritz 2005).
            Ein Vorbild für die ausführliche, manuelle Annotation von Redewiedergabe ist v.a. Semino / Short 2004. Implementierungen der automatischen Erkennung stammen vor allem aus dem Bereich der Computerlinguistik und werden oft als Vorverarbeitungsschritt für andere Anwendungen durchgeführt (z.B. Wissensextraktion, Sprechererkennung oder dem Aufbau von sozialen Netzwerken literarischer Figuren, vgl. z.B. Krestel / Bergler / Witte 2008; Elson / Dames / McKeown 2010; Iosif / Mishra 2014). Eine literaturwissenschaftlich motivierte Anwendung ist die Untersuchung von Schöch et al. 2016 zur Erkennung von direkter Wiedergabe in französischen Romantexten. Die wichtigste Vorarbeit für das vorgestellte Projekt ist die Studie Brunner 2015, auf deren Ergebnissen es aufbaut. In dieser Studie wurde ein Korpus von 13 Erzähltexten manuell annotiert und Prototypen für die automatische Erkennung (sowohl regelbasiert als auch mit Hilfe von maschinellem Lernen) wurden entwickelt und ausgewertet.
         
         
            Datengrundlage, Methodik und Ziele
            Das Untersuchungskorpus umfasst die Jahre 1840-1920 und enthält sowohl fiktionale als auch nicht-fiktionale Texte. Der nicht-fiktionale Teil setzt sich zusammen aus Texten des „Mannheimer Korpus Historischer Zeitungen und Zeitschriften“ und der Zeitschrift „Die Grenzboten“ (digitalisiert durch die Staats- und Universitätsbibliothek Bremen), der fiktionale Teil aus Erzählungen der Sammlung der Digitalen Bibliothek (textgrid). So sind sowohl Beobachtungen von Entwicklungen über die Zeit hinweg als auch Vergleiche zwischen Textsorten möglich.
            Auszüge aus den Texten werden manuell annotiert. Das in Brunner 2015 vorgestellte und an Kategoriensystemen der Literaturwissenschaft orientierte Annotationssystem wurde für das Projekt erweitert und präzisiert. Es unterscheidet zwischen Wiedergabe von gesprochener Sprache, von Schrift und von Gedanken sowie den Typen direkte Wiedergabe (
                    Er sagte: "Ich bin hungrig."), indirekte Wiedergabe (
                    Er sagte, er sei hungrig.), erzählte Wiedergabe (
                    Er sprach über das Mittagessen.) und freie indirekte Wiedergabe ('erlebte Rede') (
                    Wo sollte er jetzt nur etwas zu essen bekommen?). Attribute spezifizieren die Annotation (z.B. Verschachtelungstiefe) und markieren Sonderfälle (z.B. nicht-faktische Wiedergabe). Zusätzlich werden Sprecher, Rahmenformeln und die redeeinleitenden Verben bzw. Nomen markiert. 
                
            Die Annotatoren arbeiten mit dem im Projekt Kallimachos (www.kallimachos.de) von Markus Krug entwickelten Eclipse-basierten Annotationswerkzeug ATHEN (
                    ), für welches eine spezielle Annotationsoberfläche für Redewiedergabeformen implementiert wurde. Es werden, zumindest in Teilen, Mehrfachannotationen durchgeführt und Annotatorenvergleiche angestellt. 
                
            Die zweite Projektphase, welche zum Einreichungszeitpunkt dieses Posters gerade beginnt, umfasst die Entwicklung eines automatischen Erkenners für Redewiedergabeformen. Hierbei dient das manuell annotierte Material als Test- und Trainingsmaterial. Die in Brunner 2015 implementierten Prototypen dienen als Ausgangspunkt, die Implementierung erfolgt unter Nutzung des UIMA-Frameworks sowie in Python. Geplant ist eine Verbesserung des maschinellen Lernens durch Optimierung der Attributauswahl sowie Tests mit verschiedenen Lernalgorithmen (RandomForest, SVM, eventuell Conditional Random Fields und Deep Learning) und verschiedenen Parametereinstellungen. Auch regelbasierte Ansätze sollen weiter verfolgt werden, eventuell auf Grundlage einer aufwendigeren Vorverarbeitung (z.B. Parsing). Zudem ist eine Ergänzung und Verfeinerung einer Liste von Wörtern geplant, die auf Redewiedergabe hinweisen, welche sich bereits in Brunner 2015 als wertvolles Werkzeug bei der automatischen Erkennung erwiesen hat.
            Der Redewiedergabe-Erkenner wird dann auf weitere Texte in unserem Untersuchungszeitraum angewendet, um größere Entwicklungslinien beobachten zu können und verschiedene offene narratologische und linguistische Forschungsfragen auf quantitativer Basis zu untersuchen, z.B.: Welche Entwicklungen in der Verwendung und Form von Redewiedergabe lassen sich im Untersuchungszeitraum beobachten? Welche Rolle spielen Textsortenunterschiede bei der Entwicklung von Redewiedergabeformen? Wie kommt die Dynamik im Bestand an Verben zustande, die als Redeeinleiter gebraucht werden?
            Sowohl das manuell annotierte Korpus als auch der automatische Erkenner werden am Ende des Projekts der Forschungsgemeinschaft zur Verfügung gestellt. Es werden dafür sowohl das CLARIN-D-Forschungsdatenrepositorium des Instituts für Deutsche Sprache als auch das DARIAH-DE-Repository genutzt.
         
      
      
         
            
               Bibliographie
               
                  Ágel, Vilmos (2000): "Syntax des Neuhochdeutschen bis zur Mitte des 20. Jahrhunderts", in: Besch, Werner / Betten, Anne / Reichmann, Oskar (eds.): 
                        Sprachgeschichte. Ein Handbuch zur Geschichte der deutschen Sprache und ihrer Erforschung. Berlin / Boston: de Gruyter 1855-1903.
                    
               
                  Brunner, Annelen (2015): 
                        Automatische Erkennung von Redewiedergabe. Ein Beitrag zur quantitativen Narratologie (= Narratologia 47). Berlin / Boston: de Gruyter. 
                    
               
                  Elson, David K. / Dames, Nicholas J. / McKeown, Kathleen (2010): "Extracting Social Networks from Literary Fiction", in: 
                       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics 138-147.
                    
               
                  Fritz, Gerd (2005): 
                        Einführung in die historische Semantik. Tübingen: M. Niemeyer.
                    
               
                  Genette, Gérard (1998): 
                        Die Erzählung. München: Wilhelm Fink.
                    
               
                  Iosif, Elias / Mishra, Taniya (2014): "From Speaker Identification to Affective Analysis: A Multi-Step System from Analyzing Children's Stories", in: 
                       Proceedings of the Third Workshop on Computational Linguistics for Literature 40-49.
                    
               
                  Krestel, Ralf / Bergler, Sabine / Witte, René (2008): "Minding the Source: Automatic Tagging of Reported Speech in Newspaper Articles", in: 
                       Proceedings of the Sixth International Language Resources and Evaluation Conference
                  (LREC 2008) 2823-2828.
                    
               
                  Martínez, Matías / Scheffel, Michael (2007): 
                        Einführung in die Erzähltheorie. München: C. H. Beck.
                    
               
                  McHale, Brian (2014): "Speech Representation" in: Hühn, Peter / Pier, John / Schmid, Wolf / Schönert, Jörg (eds.): 
                       The living handbook of narratology. Hamburg: Hamburg University Press 434-446 
                        [letzter Zugriff 18. September 2017].
                    
               
                  Schöch, Christof / Schlör, Daniel / Popp, Stefanie / Brunner, Annelen / Henny, Ulrike / Calvo Tello, José (2016): "Straight talk! Automatic Recognition of Direct Speech in Nineteenth-century French Novels", in: 
                       Conference Abstracts. Jagiellonian University & Pedagogical University 346-353.
                    
               
                  Semino, Elena / Short, Mick (2004): 
                        Corpus stylistics. Speech, writing and thought presentation in a corpus of English writing. London / New York: Routledge.
                    
            
         
      
   


10694	2018	
      
         
            Einleitung
            Dieser Beitrag behandelt die Frage, warum in den DH entwickelte und angewandte Software häufig schnell altert. Jede Software altert relativ zu der Umgebung, in der sie eingesetzt wird, unabhängig von der Qualität am Beginn ihrer Verwendung (Engels et al. 2009: 393). Wandeln sich Hardware, Infrastruktur oder Anforderungen an die Software, wird sie, um weiter brauchbar zu sein, angepasst. Je nach Beschaffenheit können sich diese Anpassungen positiv, oftmals aber auch negativ auf die Lebensdauer und Fitness einer Software auswirken. 
            Aus der Praxis behaupten wir, dass kontextuelle und inhaltliche Spezifika von DH-Software dazu führen, dass eine langfristige Lauffähigkeit und Brauchbarkeit erschwert werden. Unser Beitrag bringt allgemein die Bedeutung und Relevanz des Themas „Software Evolution“ (2) nahe, beschreibt Spezifika der Software Evolution aus der DH-Praxis (3) und zeigt welche konkreten Maßnahmen im Projekt 
                    monasterium.net (4) dahingehend gesetzt werden. 
                
         
         
            Software Evolution
            Software Evolution umfasst alle Aktivitäten und Prozesse, die Software verändern (Godfrey/German 2008). Änderungen der Hardware, der Infomationsübermittlung sowie der Anforderungen sind Kräfte die auf diesen Evolutionsprozess wirken. Softwareentwicklungsprozesse werden seit den 1970er Jahren definiert und systematisiert, um die Qualität von Software zu steigern. Aus dieser Zeit stammt auch das Konzept des sogenannten Software Lifecycles und die Idee, diesen Zyklus zu managen (Lehman 1980). Unterschiedliche Methoden und Techniken dazu haben sich seither für alle Phasen im Lebenslauf von Softwaresystemen etabliert. Dank der intensiven Auseinandersetzung mit der Qualitätssteigerung in der Softwareentwicklung wurden die Fehlerquoten gesenkt (Thaller 2000: 6). Hochwertige Software ist nicht nur (nahezu) fehlerfrei, sondern auch kompatibel zur ihrer Umgebung. Verläuft die Evolution einer Software nicht in diesem Sinne, spricht man vom „Software Aging“ beziehungsweise sogar von deren Verfall (Parnas 1994). Demeyer et al. (2013: 4f.) fassen die Symptome veralteter Software wie folgt zusammen: Unvollständige oder keine Dokumentation, fehlende Tests, Ausstieg ursprünglicher Entwickler, verlorengegangenes Insiderwissen, fehlender Überblick über Gesamtsystem, zeitintensive Anpassungen, ständige Fehlerkorrekturen und Wartung damit verbundener Abhängigkeiten, lange „Build“-Zeiten und schlechter Code.
            Parnas (1994: 280) erkennt zwei Hauptfaktoren für das Altern von Software. „Lack of movement“, also keine Änderungen an der Software vorzunehmen, und „Ignorant surgery“: Aus der Praxis weiß man, dass bei dringenden Korrekturen am Programmcode, die formale Kriterien für gute Software oftmals nicht eingehalten werden. Ein Beispiel ist das unreflektierte Copy-and-paste aus 
                    Stack Overflow
               . Kurzfristige werden den besten Lösungen vorgezogen. Derartige Eingriffe und nicht-systematisches Vorgehen beschleunigen den Prozess der Softwarealterung. Es wird immer aufwendiger, Änderungen an der Software vorzunehmen.
                
            Demzufolge werden Ideen zur systematischen und automatisierten Verjüngung von Software erforscht und erprobt: Refactoring-Tools, beispielsweise für Java in 
                    Eclipse, 
                    Python Rope, oder aber auch für HTML und CSS (Mazinanian/Tsantalis 2017, Harold 2008), wurden entwickelt. Sogenannte „Prediction“- Modelle werden ermittelt, um Softwareevolution besser verstehen zu können und vor allem dem Problem der „Legacy software“ zu begegnen (Goltz et al. 2015, Paech et al. 2016).
                
         
         
            Software Herausforderungen in der DH Praxis
            
               Diese teilweise schon seit Jahrzehnten bekannten Erkenntnisse aus dem Software Engineering haben für die DH eine besondere Relevanz, da die Projekte hier wesentlich kleinere Budgets, oftmals kurze Projektlaufzeiten und andere Unsicherheiten haben. Aus unserer Erfahrung wird Softwareentwicklung in den DH häufig sehr informell gehandhabt. Diesbezüglich nachhaltiger zu werden, haben unter anderem Czmiel (2017), Schrade (2017) oder Kasper/Grüntgens (2017) gefordert. Nicht nur der Entwicklungsprozess von DH-Software muss längerfristig gedacht werden (Hattrick 2016), auch der Kontext, in dem die Software entsteht und besteht, beeinflusst deren Entwicklung und Veränderung. 
                
            Erstens ist es nicht ungewöhnlich, dass Projekte in den DH von einer einzigen Person technisch umgesetzt werden, wie es etwa im Falle von Dissertationsprojekten typisch ist. Der entstandene Code ist bei Projektende lauffähig, es kann aber nicht vorausgesetzt werden, dass dieser auf einen langfristigen Einsatz ausgelegt ist und entsprechend gewissenhaft programmiert und dokumentiert ist. Forschungsergebnisse sind im Projektkontext meist wichtiger als die Qualität der entwickelten Software. Generell bedeutet ein Projektende nicht die Übergabe eines Produktes an einen Kunden, es bedeutet vielmehr: Die Finanzierung läuft aus und der/die Entwickler/in verlässt das Projekt. Was zurückbleibt, ist Software, die von anderen gewartet werden muss. Dazu ist es notwendig, die Dokumentation und Systemarchitektur zu verstehen, sich in den Fremdcode einzuarbeiten. Veränderungen am Code können oft nicht mehr ihrer ursprünglichen Intention entsprechend vorgenommen werden. Die Wartung wird aufwendig und zeitintensiv. Das heißt, die Organisationsstrukturen des Forschungsbetriebes beeinflussen die Alterung von Software.
            Zweitens bringen die komplexen Anforderungen der Forschungsdaten nicht-klassische Lösungsansätze mit sich. Mit diesen Ansätzen vertraute Entwickler/innen sind schwer zu finden und zu halten, Einarbeitungsphasen dauern lange. Besonders augenfällig wird das am in den DH weit verbreiteten Gebrauch von X-Technologien. Sie werden immer mehr zur Nischenanwendung. Während die Definitionen von XSLT 1.0 und XPath 1.0 noch von einer größeren Breite von Softwareprodukten implementiert wurden, sogar Teil der Browser wurden, gibt es nur noch wenige Implementationen der Weiterentwicklungen XSLT 2.0 und 3.0. Auch die Menge verwendbarer XML-Datenbanksysteme ist heute geringer als noch vor einigen Jahren. In den DH entwickelte Softwarelösungen sind also speziell auf die Bedürfnisse des Gegenstandes ausgelegt und stellen keine Standardlösungen dar. Sie brauchen spezifisches Know-how, um gewartet werden zu können. Fehlt dieses, beziehungsweise ist es nur mangelhaft vorhanden, droht die Software zum unbrauchbaren Altsystem zu verkommen.
            DH-Software verlangt drittens besondere Zuwendung, wenn der Code gleichzeitig die Forschungsergebnisse interpretiert. Wenn die Forschungsleistung also nicht allein in den Daten liegt, braucht es individuelle Wartungslösungen. Eine Digitale Edition kann beispielsweise als die Gesamtheit von Daten, Systemarchitektur, Anwendung und GUI verstanden werden (Andrews/Zundert 2018). Diese Interpretationsleistung als Teil der Forschung muss bei allen Phänomenen der Veränderung an der Edition mitbedacht werden. Die Gefahr ist groß, dass nach einiger Zeit das Argument durch Softwareanpassungen verwässert oder im schlimmsten Fall nicht mehr nachvollziehbar ist und für die Forschung unbrauchbar wird. 
            Zusammenfassend sehen wir in der nicht langfristigen Finanzierung, der hohen Fluktuation an Personen, der Notwendigkeit von Speziallösungen und im Forschungsgegenstand selbst erhöhten Bedarf an Maßnahmen, um unsere Softwareprojekte lauffähig zu halten. 
         
         
            Anti-Aging Maßnahmen im Projekt monasterium.net
            
               Seit 2008 basiert die Urkundenplattform 
                    monasterium.net auf 
                    eXist-db als Applikationsserver und Datenbank. Die Plattform wurde hauptsächlich von drei aufeinanderfolgenden Hauptentwicklern programmiert. Um die Software zu modularisieren, wurde seit 2011 das 
                    mom-ca-Framework entwickelt, eine Webapplikation in XRX-Architektur (XQuery, REST, XForms). Die Architektur galt damals in Verbindung mit XML-Datenbanken als Empfehlung, wird allerdings in der modernen Webentwicklung kaum mehr eingesetzt. Mit Auslaufen eines Projektes 2014 verließ der letzte Entwickler mit Überblick über das Gesamtsystem das Projekt. Zuvor wurde der Gesamtcode in ein öffentliches Repository überführt. Wissen und Intentionen gingen jedoch verloren. 
                    Wir, als das aktuelle, größtenteils projektfinanzierte Entwicklerteam, beschäftigen uns nun aktiv damit, wie der derzeitige Code-Bestand unter unsteten Umständen wartbar und aktuell gehalten werden kann. Im Folgenden beschreiben wir vier Anti-Aging-Maßnahmen, die einerseits Refactoring (das Überarbeiten des Codes), aber auch ganz grundsätzliche Umstellungen des Entwicklungsworkflows betreffen. 
                
            
               Softwareverwaltung durch 
                        Git und Nutzung der Services von 
                        GitHub.
                    
               Sowohl Entwicklung als auch Dokumentation erfolgen über ein öffentliches 
                        GitHub-Repository. Die dadurch verfügbaren Möglichkeiten der Versionsverwaltung, des Bugtracking und des Code Review werden genutzt, um die Qualität des Codes zu verbessern und diesen transparent und nachvollziehbar zu entwickeln.
                    
            
            
               Einrichtung einer Testumgebung. 
               Jede Neuentwicklung wird, vor ihrer Übernahme in das Produktivsystem anhand eines festgelegten Testszenarios evaluiert. Durch die Spiegelung des Livesystems auf einem Testserver soll reales Systemverhalten reproduziert werden. Fehler können so vorzeitig entdeckt und behoben werden.
            
            
               Refactoring von HTML und CSS.
               Die Verwendung eines auf den Konzepten von Material Design basierenden CSS-Frameworks garantiert ein konsistentes Gesamtdesign von 
                        monasterium.net. Teile des Benutzerinterfaces werden dadurch modularisiert und leichter anpassbar. Die Verwendung eines Präprozessors und das Einführen einer Namenskonvention sollen die Wartbarkeit, das Auffinden von Fehlern und die Umsetzung neuer Features erleichtern.
                    
            
            
               Entwicklung einer RESTful API zwischen Client und Datenbank.
               Die zukünftige Kommunikation zwischen Client und Datenbank übernimmt eine neudefinierte REST-API. 
                       Die Datenabfrage aus der XML-Datenbank erfolgt noch per XQuery, zurückgeliefert werden wahlweise in XML oder JSON serialisierte Daten. Diese Form des Reengineerings gewährt eine definierte, standardisierte Verarbeitungsweise sowie die Weiternutzung und Kombination multipler Datenquellen. Die Abstraktion von Datenbank, Programmlogik und Benutzeroberfläche erleichtert so in Zukunft deren entkoppelte Anpassung oder Austausch.
                    
            
         
         
            Fazit
            Softwarealterung ist nicht nur in der Softwareindustrie eine aktuelle und fordernde Problematik. Auch für DH-Forschungsinfrastrukturen ist diesbezüglich ein gezielter Umgang gefragt, um Software fit zu halten. Unwissenheit hinsichtlich der Wartung einer Software kann schlimmstenfalls zu einer zukünftigen Unbrauchbarkeit der Forschungsergebnisse führen. Eine dahingehende Bewusstseinsbildung kann über die empirische Betrachtung vorhandener Praktiken und Lösungswege geschehen.
            Anhand von 
                    monasterium.net haben wir exemplarisch mögliche Verjüngungsmaßnahmen dargestellt. Das Projekt eignet sich als Fallbeispiel, da seine Software eine über zehnjährige Laufzeit aufweist. Geringes Projektbudget und häufiger Personalwechsel mit daraus resultierenden Wissensverlusten haben die Codebasis gezeichnet. Das Projekt zeigt, dass Nachvollziehbarkeit des Entwicklungsprozesses, systematisches und standardisiertes Vorgehen, Modularisierung von Softwarekomponenten sowie kontinuierliches Testing in die Evolution von Software gewinnbringend eingreifen können.
                
            
               Die Verantwortung kann allerdings nicht allein bei den Entwickler/innen liegen. Um Wissensverluste vorzubeugen, müssen langfristige Strukturen aufgebaut und finanziell abgesichert werden. Es muss Teil der Förderungspolitik werden, die Unausweichlichkeit der Softwarealterung zu bedenken. Sollen Entwicklungen auch nach fünf Jahren noch benutzbar sein, muss der Aufwand der nachhaltigen Entwicklung und Wartung in der Antragsplanung verankert werden.
                
         
      
      
         
             Stack Overflow ist eine Online Community, zur gegenseitigen Unterstützung und zur Wissensgenerierung bei Fragen zur Softwareentwicklung: stackoverflow.com
             github.com/icaruseu/mom-ca
             material.io/guidelines/
         
         
            
               Bibliographie
               
                  Andrews, Tara / Zundert, Joris van (2018): “What are you Trying to Say? The Interface as an Integral Element of Argument”, in: Bleier, Roman et al. (eds.): 
                        Digital Scholarly Editions as Interfaces (=Schriften des Instituts für Dokumentologie und Editorik). Norderstedt: Books on Demand.
                    
               
                  Czmiel, Alexander (2017): “Funktionalität Digitaler Editionen“, in: 
                        DHd 2017. Digitale Nachhaltigkeit. Konferenzabstracts. Bern 138-141. 
                        http://www.dhd2017.ch/wp-content/uploads/2017/02/Abstractband_ergaenzt.pdf [letzter Zugriff 24. September 2017].
                    
               
                  Demeyer, Serge / Ducasse, Stéphane / Nierstrasz, Oscar (2013): 
                        Object-Oriented Reengineering Patterns. Bern: Square Bracket Associates. 
                        http://scg.unibe.ch/download/oorp/OORP.pdf
                        [letzter Zugriff 24. September 2017].
                    
               
                  Engels, Gregor et al. (2009) "Design for Future: Legacy-Probleme von morgen vermeidbar?", in: 
                        Informatik Spektrum 32, 5: 393-397. https://doi.org/10.1007/s00287-009-0356-3 [letzter Zugriff 24. September 2017].
                    
               
                  Godfrey, Michael W. / German, Daniel M. (2008): “The Past, Present, and Future of Software Evolution”, in: 
                        Proceedings of the 2008 Frontiers of Software Maintenance.
               
               New York: IEEE 129-138. 
                        https://doi.org/10.1109/FOSM.2008.4659256 [letzter Zugriff 24. September 2017].
                    
               
                  Goltz, Ursula et al. (2015): “Design for future: managed software evolution”, in: 
                        Computer Science - Research and Development 30, 3-4: 321-331. https://doi.org/10.1007/s00450-014-0273-9 [letzter Zugriff 24. September 2017].
                    
               
                  Harold, Rusty Elliotte (2008): 
                        Refactoring HTML. Improving the Design of Existing Web Applications. Upper Saddle River, NJ: Addison-Wesley.
                    
               
                  Hattrick, Simon (2016): Research Software Sustainability. Report on a Knowledge Exchange Workshop. JISC:
                        http://repository.jisc.ac.uk/6332/1/Research_Software_Sustainability_Report_on_KE_Work
                            shop_Feb_2016_FINAL.pdf
                         [letzter Zugriff 24. September 2017].
                    
               
                  Kasper, Dominik / Grüntgens, Max (2017): “Nachhaltige Konzeptionsmethoden für Digital Humanities Projekte am Beispiel der Goethe-Propyläen“, in: 
                        DHd 2017. Digitale Nachhaltigkeit. Konferenzabstracts. Bern 165-168. 
                        http://www.dhd2017.ch/wp-content/uploads/2017/02/Abstractband_ergaenzt.pdf [letzter Zugriff 24. September 2017].
                    
               
                  Lehman, Meir M. (1980): “Programs, life cycles, and laws of software evolution”, in: 
                        Proceedings of the IEEE 68, 9: 1060-1076.
                  https://doi.org/10.1109/PROC.1980.11805
                        [letzter Zugriff 24. September 2017].
                    
               
                  Mazinanian, Davood / Tsantalis, Nikolaos (2017): “CCSDev: Refactoring duplication in Cascading Style Sheets”, in: 
                        Proceedings of the 39th International Conference on Software Engineering Companion. New York: IEEE 63-66. 
                        https://doi.org/10.1109/ICSE-C.2017.7 [letzter Zugriff 24. September 2017].
                    
               
                  Paech, Barbara et al. (2016): “Empirische Forschung zu Software-Evolution”, in: 
                        Informatik Spektrum 39, 3: 186-193. 
                        https://doi.org/10.1007/s00287-015-0910-0
                         [letzter Zugriff 24. September 2017].
                    
               
                  Parnas, David L. (1994): “Software Aging”, in: 
                        Proceedings of 16th International Conference on Software Engineering. New York: IEEE 279-287. 
                        https://doi.org/10.1109/ICSE.1994.296790 [letzter Zugriff 24. September 2017].
                    
               
                  Schrade, Torsten (2017): “Nachhaltige Softwareentwicklung in den Digital Humanities. Konzepte und Methoden“, in: 
                        DHd 2017. Digitale Nachhaltigkeit. Konferenzabstracts. Bern 168-171. 
                        http://www.dhd2017.ch/wp-content/uploads/2017/02/Abstractband_ergaenzt.pdf [letzter Zugriff 24. September 2017].
                    
               
                  Thaller, Georg E. (2000): ISO 9001: 
                        Software-Entwicklung in der Praxis. Hannover: Heise.
                    
            
         
      
   


10696	2018	
      
         
            Hintergrund und Zielsetzung
            Am 13. Oktober 2016 gab die Schwedische Akademie bekannt, dass sie den Nobelpreis in Literatur an Bob Dylan „für seine poetischen Neuschöpfungen in der großen amerikanischen Songtradition“ verleihen werde. 
                    
                    Dass Dylan als Musiker und Songwriter den Literaturnobelpreis erhielt wurde mitunter sehr kontrovers diskutiert. Auf Kritik stieß z.B. die unzulässige Herauslösung von Dylans Texten aus der Musik und die Deutung seiner Lieder als Gedichte. 
                    
                    Unbestritten ist nichtsdestotrotz Bob Dylans Rolle als einer der einflussreichsten Musiker des 20. Jahrhunderts. 
                    
                
            Die (welt-)politischen Entwicklungen, die das Schaffen Dylans inspirierten, sind im Kontext seines Wirkens umfassend diskutiert worden, unter anderem in „Bob Dylan und die sechziger Jahre: Aufbruch und Abkehr“, 
                   
                    und liefern noch immer Diskussionsstoff, wie etwa eine in jüngerer Zeit erschienene Arbeit von Taylor & Israelson (2015) 
                    
                   über Dylans politische Einflüsse zeigt. Erfolgte die Beschäftigung mit Dylans Werk bislang allenfalls episodisch, so muss eine systematische Analyse des Gesamtwerks als Desiderat gelten, welches in gewisser Weise bereits von Bob Dylan selbst formuliert wurde:
                
            All these people who say whatever it is I’m supposed to be doing – that’s all gonna pass, because, obviously, I’m not gonna be around forever. That day’s gonna come when there aren’t gonna be any more records, and then people won’t be able to say ‘Well, this one’s not as good as the last one.’ 
                    They’re gonna have to look at it all (eigene Hervorhebung). And I don’t know what the picture will be, what people’s judgement will be at that time. I can’t help you in that area. 
                    – Bob Dylan 
                
            
            Dieser Beitrag erprobt, inwiefern mithilfe digitaler Methoden im Sinne des 
                    Distant Reading-Paradigmas ein neuer Zugang zu Dylans Gesamtwerk geschaffen werden kann. Bezugnehmend auf das Konferenzmotto einer „Kritik der Digitalen Vernunft“ soll untersucht werden, wo die Grenzen und Möglichkeiten eines solchen digitalen Analyseansatzes liegen, indem überprüft wird, ob sich bestehende qualitative Einteilungen von Dylans Werk in unterschiedliche Schaffensperioden auch anhand statistisch signifikanter Wörter (Rayson, Berridge & Francis 2004) und N-Gramme (Evert 2005) belegen lassen. 
                
         
         
            Stand der Forschung
            Bereits vor der Auszeichnung Dylans mit dem Nobelpreis in Literatur, waren seine Texte Gegenstand wissenschaftlicher Betrachtungen im Sinne des 
                    Close Reading (vgl. etwa Brown 2014). Brown unterteilt Dylans Werk in einzelne Phasen und verknüpft diese jeweils mit allgemeinen, zeitgeschichtlichen Ereignissen sowie biographischen Meilensteinen des Künstlers. Taylor & Israelson (2015) gehen einen ähnlichen Weg, versuchen jedoch Dylans Werk abseits verbreiteter politischer Einordnungen zu betrachten. Etwas anders ausgerichtet ist die Arbeit von Wissolik et al. (1994): Hierbei handelt es sich um eine Art Wörterbuch, in dem Namen und Gegenstände, die in Dylans Texten auftauchen, erläutert werden. 
                
            Eine umfassende Untersuchung Dylans Werks mithilfe computerbasierter Methoden fand sich bis zum Abfassungszeitpunkt des vorliegenden Texts nicht. Allerdings sind quantitative Verfahren zur stilistischen und inhaltlichen Analyse von Liedtexten in den Digital Humanities durchaus verbreitet. So beschreiben etwa, 
                    
                    wie mithilfe von N-Gramm-Modellen ein Liedtext-Korpus anhand der Merkmale Textlänge, Textstruktur, Wortschatz und Semantik analysiert werden kann, um eine automatisierte Genrezuordnung vornehmen zu können. Daneben existieren stilometrische Untersuchungen von Liedern oder Gedichten, die sich mit der Berechnung von autoren- und genrespezifischen Merkmalen befassen, wie z.B. Suzuki & Hosoya (2014), die japanische Pop-Songs analysieren.
                
         
         
            Forschungsmethodik
            
               Bezugsrahmen der Analyse: Schaffensphasen Bob Dylans
               Den analytischen Bezugsrahmen dieser Studie stellt die phasenweise Einteilung von Dylans Schaffen nach Brown (2014)
                         
                        dar. 
                         
                        Brown unterscheidet dabei neun unterschiedliche Phasen, die mit „Becoming Bob Dylan“ (1960-1964) beginnen und vorläufig mit „Bob Dylan Revisited“ (2000-2012) enden. Diese Stilphasen umfassen z.B. Dylans Hinwendung zum Christentum oder seine elektronische „Folk Rock“-Phase (vgl. Brown, 2014).
                    
            
            
               Korpus und Datenaufbereitung
               In dieser Arbeit wurde ein Korpus bestehend aus 452 Liedtexten mit einem Umfang von 133.045 Tokens untersucht, die Bob Dylan zwischen den Jahren 1962 und 2016 auf Studio-Alben veröffentlicht hat. Die Liedtexte und Metainformationen wie etwa Titel, Album und Jahr stammen von der Plattform 
                        LyricsWikia
                  . Da es sich bei 
                        LyricsWikia um ein community-gestütztes Projekt handelt, erfolgte vorab ein stichprobenartiger Abgleich einzelner Lieder mit den offiziellen Texten nach, 
                        
                        wobei keinerlei Abweichungen festgestellt werden konnten. 
                    
               Das Korpus wurde weiterhin mit Methoden der Computerlinguistik aufbereitet, insbesondere unter Verwendung des 
                        Python Natural Language Toolkits (NLTK). Die Verarbeitung des Korpus umfasst die grundlegende Lemmatisierung mit dem 
                        WordNet-Lemmatizer (Teil des NLTK) und eine Stoppwortbereinigung (NLTK-Stoppwortliste für Englisch mit eigener Erweiterung) sowie die Wortartenannotation mithilfe des 
                        Stanford Log-linear Part-of-Speech-Taggers.
                        
                        Da Dylan in seinen Texten häufig umgangssprachliche Formulierungen wie etwa verkürzte Gerundformen (bspw. „savin“, „swimmin“) verwendet, wurde für den POS-Tagger ein Modell verwendet, welches auf der Grundlage von Twitter-Texten trainiert wurde und gute Ergebnisse für Texte mit nicht-standardisiertem Vokabular und Slang liefert. 
                        
                    
            
            
               Korpusvergleich – Assoziationsmaße und Referenzkorpus
               
                  Assoziationsmaße
                  Ein etabliertes Verfahren, um aus einem Korpus spezifische Wörter zu extrahieren, ist ein direkter Korpusvergleich mit dem 
                            Log-Likelihood-Test, 
                            
                            der sich zum Vergleich von Korpora unterschiedlicher Größe besonders eignet (Rayson, Berridge, & Francis, 2004). Damit können Wörter, die im untersuchten Korpus mit einem signifikanten Frequenzunterschied zum Referenzkorpus auftreten, als Schlagworte betrachtet werden. Dies kann besonders aussagekräftige Ergebnisse liefern, wenn zusätzlich eine POS-Filterung erfolgt, womit sich beispielsweise signifikante Nomen oder Verben eines Korpus berechnen lassen. Darüber hinaus wurde eine Berechnung von N-Grammen in Form von Bi- und Trigrammen umgesetzt. Die berechneten N-Gramme lassen sich in der Web-App unter der Wahl eines Assoziationsmaßes, wie dem 
                            Chi Quadrat-Test, dem Jaccard-Test, dem Poisson-Stirling-Test, dem Likelihood Ratio-Test sowie dem Pointwise Mutual Information-Test anzeigen. Dabei liefert jedes Verfahren zur N-Gramm-Berechnung eigene spezifische Ergebnisse. Dieser Freiraum wird ganz bewusst erhalten, um die verschiedenen Facetten eines Texts, die ein Assoziationsmaße jeweils anzeigt, für die spätere Datenanalyse nutzen zu können.
                        
               
               
                  Referenzkorpus 
                  Als Referenzkorpus dient das mündliche Subkorpus des 
                            Open American National Corpus (OANC; American National Corpus Project),
                            
                            welches insgesamt 3.862.172 Tokens umfasst. Das Korpus enthält viele Belege aus der mündlichen Kommunikation 
                             
                            und eignet sich dadurch in besonderer Weise als Vergleichskorpus für Dylans Texte, die wie bereits beschrieben einen hohen Anteil umgangssprachlicher Formulierungen und Slang-Ausdrücke enthalten.
                        
                  Beim Korpusvergleich kann entweder das gesamte Dylan-Korpus mit dem Referenzkorpus verglichen werden, oder mit den jeweiligen Dylan-Subkorpora, also bspw. all seinen Texten aus den 1970er-Jahren oder aus der ersten Schaffensperiode „Becoming Bob Dylan“ (1960-1964). Ein Vergleich der einzelnen Dylan-Subkorpora zum Gesamtwerk ist ebenso möglich. Letztere Option wird z.B. genutzt, um anhand jeweils signifikanter Wörter die einzelnen Schaffensperioden nach Brown (2014) zu überprüfen und damit die grundsätzliche Eignung solch quantitativer Verfahren zur Identifikation thematischer Verschiebungen zu untersuchen. Die Ergebnisse dieses Korpusvergleichs sind, zusammen mit allen anderen Ergebnissen der angewandten Analyseverfahren, in einer interaktiven Webanwendung über unterschiedliche Visualisierungen (Balkendiagramm, 
                            treemap, wordcloud, Tabelle) für weitere Interpretationen zugänglich. Wie schon bei den Assoziationsmaßen, so gilt auch hier, dass jede Visualisierungsform eine bestimmte Perspektive auf die Berechnungsergebnisse eröffnet.
                        
               
            
         
         
            Ergebnisse
            Im direkten Vergleich des gesamten Dylan-Korpus (1962-2016) mit dem OANC-Referenzkorpus treten einige interessante, signifikant-häufige Wörter im Werk Dylans hervor. Die von Bob Dylan verwendeten Adjektive erzeugen in der Gesamtschau tendenziell eher eine bedrückende Stimmung (blind, weary, lonely, drunken, scared, restless, ragged). Bei den Substantiven mischen sich unter viele Personen- und Ortsnamen auch religiöse Begriffe (soul, heaven, devil, eden, prayer, paradise). Viele der übrigen Begriffe sind erwartungsgemäß typisch für Folk-Musik (levee, rooster, train), was sich wiederum durch die Wahl des Referenzkorpus, das verschiedenartige mündliche Textquellen enthält, erklären lässt (Rayson, Berridge, & Francis, 2004: 8).
                
            Die Analyse signifikant-häufiger Wörter für die einzelnen Schaffensphasen Dylans liefert Ergebnisse mit hoher Aussagekraft. So fällt etwa für die Phase „The Changing of the Guard" (1978-1981), in der sich Dylan dem Christentum hinwendet, auf, dass das Vokabular tatsächlich viele christliche Motive aufweist (lord, Jesus, devil, altar, faith, confession, grace, power, serve). Insgesamt nimmt der Anteil an „düsterem“ Vokabular in dieser Phase ab, verschwindet jedoch nicht komplett (bspw. 
                    shot, destruction). Der Anteil an hoffnungsvollen Wörtern nimmt hingegen zu (bspw. 
                    beginning, ready, arise, wake, thank). Bei den übrigen Schaffensphasen fallen die Ergebnisse jedoch mitunter wesentlich weniger deutlich aus.
                
            Ein differenziertes Bild ergibt sich für die N-Gramm-Analyse, was einerseits der Vielfalt an verfügbaren Methoden zur Berechnung 
                     
                    und andererseits den unterschiedlichen N-Gramm-Längen geschuldet ist. Die Ergebnisse für Bigramme mit Hilfe des 
                    Pointwise-Mutual-Information-Tests (PMI) erschienen dabei am geeignetsten, um die thematischen Schwerpunkte von Dylans Schaffensphasen nach 
                     
                    nachzuvollziehen. So findet das PMI-Verfahren im Subkorpus der Phase „The Changing of the Guard“ (1978-1981) Bigramme wie 
                    close prayer, name lucifer, jesus good, jesus bone oder arise upon, die eindeutig religiöse Bezüge in Dylans Texten dieser Phase veranschaulichen. Generell fällt jedoch die Dominanz von Refrain-Versen in den Liedern bedeutend ins Gewicht (z.B. 
                    knock heaven door), was die Qualität der Ergebnisse insbesondere bei den Trigrammen beeinflusst. 
                
         
         
            Diskussion
            Im Sinne einer Kritik der Digitalen Vernunft bleibt demnach festzuhalten, dass sich Methoden der computergestützten Textanalyse und des statistischen Korpusvergleichs grundsätzlich dafür eignen, einen inhaltlichen Gesamtüberblick zu einem Liedtext-Korpus zu erhalten. Es können damit diachrone Entwicklungen des Wortschatzes und Verlagerungen thematischer Schwerpunkte als grobe Tendenzen aufgezeigt werden, um das Bild des Gesamtwerks zu ergänzen. Ein solcher Ansatz eignet sich demnach gut für die initiale Thesengenerierung und kann in gewisser Weise die Funktion eines Empfehlungs- bzw. Hinweissystems für erklärungsbedürftige Stellen
                     in den Geisteswissenschaften übernehmen.
                
            Die Identifikation konkreter Schaffensperioden, ausschließlich auf Basis signifikant häufiger Wörter ist aber – zumindest für das Werk Dylans – nicht ohne Weiteres erfassbar. Bei den N-Grammen zeigt sich, dass im Falle von Dylans Texten methodenübergreifend und mit zunehmender N-Gramm-Länge meist keine brauchbaren Ergebnisse erzielt werden konnten. Dies ist ein Hinweis darauf, dass die hier präsentierten Analysemethoden, die für andere Textsorten wie bspw. Parlamentsprotokolle bereits erfolgreich eingesetzt werden konnten (vgl. Sippl et al. 2016), auf Liedtexte nur eingeschränkt anwendbar sind. Ein möglicher Kritikpunkt am hier beschriebenen Vorgehen mag zudem das verwendete OANC-Referenzkorpus sein, welches trotz hoher Anteile mündlicher Kommunikation doch nur beschränkt vergleichbar mit der Textsorte „Liedtext“ ist. Für künftige Vergleichsstudien böte sich ggf. ein Vergleich mehrerer unterschiedlicher Künstler und deren Liedtexte an, also bspw. Bob Dylan vs. Johnny Cash.
         
      
      
         
             http://lyrics.wikia.com, alle Hyperlinks dieses Dokuments wurden zuletzt abgerufen am 10.01.2018
             Verfügbar unter http://www.nltk.org/
             Filterung von Stoppwörtern, wie „hey“, „ah“, „yeah“, und Verkürzungen, wie „‘ve“, „‘s“ etc.
             https://www.colin-sippl.de/dylan (Klick auf den Analyse-Button rechts oben)
             Diesen Gedanken äußerte Hubertus Kohle auf der #DigiCampus-Tagung im Juni 2017 in München, vgl. https://twitter.com/8urghardt/status/876725916487036928.
         
         
            
               Bibliographie
               
                  American National Corpus Project (2015a): 
                        American National Corpus. Frequency Data. http://www.anc.org/data/anc-second-release/frequency-data/ [Letzter Zugriff 10. März 2017].
                    
               
                  American National Corpus Project (2015b): 
                        The Open American National Corpus (OANC). http://www.anc.org/ [Letzter Zugriff 10. März 2017].
                    
               
                  Brown, Donald (2014): 
                        Bob Dylan: American troubadour. Lanham, Md. [u.a.]: Rowman & Littlefield.
                    
               
                  Cott, Jonathan (2006): 
                        Bob Dylan, the essential interviews. New York: Wenner Books.
                    
               
                  Derczynski, Leon et al. (2013): "Twitter part-of-speech tagging for all: Overcoming sparse and noisy data", in: 
                        Proceedings of the Recent Advances in Natural Language Processing September, 198–206. http://www.derczynski.com/sheffield/papers/twitter_pos.pdf [Letzter Zugriff 9. März 2017].
                    
               
                  Dylan, Bob (2016): 
                        The lyrics: 1961-2012. New York: Simon & Schuster.
                    
               
                  Evert, Stefan (2005): "The Statistics of Word Cooccurrences, Word Pairs and Collocations", in: 
                        Unpublished doctoral dissertation Institut fur maschinelle Sprachverarbeitung Universitat Stuttgart 98: August 2004, 353. http://en.scientificcommons.org/19948039 [Letzter Zugriff 3. März 2017].
                    
               
                  Fell, Michael / Sporleder, Caroline (2014): "Lyrics-based Analysis and Classification of Music", in: 
                        International Conference on Computational Linguistics 25: 23–29, 620–631.
                    
               
                  Geisel, Sieglinde (2016): 
                        Bob Dylan - Literaturnobelpreisträger wider Willen. Deutschlandradio Kultur. http://www.deutschlandradiokultur.de/bob-dylan-literaturnobelpreistraeger-wider-willen.1005.de.html?dram:article_id=373494 [Letzter Zugriff 7. März 2017].
                    
               
                  Rayson, Paul / Garside, Roger (2000): "Comparing corpora using frequency profiling", in: 
                        Proceedings of the workshop on Comparing Corpora 1–6.
                    
               
                  Rayson, Paul / Berridge, Damon / Francis, Brian (2004): "Extending the Cochran rule for the comparison of word frequencies between corpora", in: 
                        JADT 2004: 7es Journées internationales d’Analyse statistique des Données Textuelles: 1–12.
                    
               
                  Schmidt, Mathias R. (1983): 
                        Bob Dylan und die sechziger Jahre: Aufbruch und Abkehr. Frankfurt am Main: Fischer Taschenbuch Verlag.
                    
               
                  Sippl, Colin / Burghardt, Manuel / Wolff, Christian / Mielke, Bettina (2016): Korpusbasierte Analyse österreichischer Parlamentsreden. In: 
                        Netzwerke: Tagungsband des 19. Int. Rechtsinformatik Symposions IRIS 2016: 25.- 7. Feb. 2016, Univ. Salzburg, S. 139-148.
               
               
                  Suzuki, Takafumi / Hosoya, Mai (2014): "Computational Stylistic Analysis of Popular Songs of Japanese Female Singer-songwriters", in: 
                        Digital Humanities Quarterly 8: 1, .
                    
               
                  Svenska Akademien (2016): 
                        Der Nobelpreis in Literatur des Jahres 2016.
                    
               
                  Taylor, Jeff / Israelson, Chad (2015): 
                        The Political World of Bob Dylan: Freedom and Justice, Power and Sin. New York: Palgrave Macmillan.
                    
               
                  Toutanova, Kristina / Klein, Dan / Manning, Christopher D (2003): "Feature-rich part-of-speech tagging with a cyclic dependency network", in: 
                        Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL ’03), 252–259. http://nlp.stanford.edu/~manning/papers/tagging.pdf [Letzter Zugriff 3. März 2017].
                    
               
                  Wissolik, Richard David / McGrath, Scott. / Colaianne, A. J. (1994): 
                        Bob Dylan’s words: a critical dictionary and commentary. Greensburg, PA: Eadmer Press.
                    
            
         
      
   


10703	2018	
      
         
            Introduktion
            Im Rahmen eines Softwareprojektes
                    , das sich mit der automatisierten Analyse von Märchen in deutscher Sprache befasst, hat sich die Notwendigkeit ergeben, eine formale Repräsentation von Märchen zu bestimmen, damit die einzelnen Komponente des Systems miteinander integriert werden können.
                
            Wir beschreiben in diesem Beitrag zum einen, welche Informationen in dieser formalen Repräsentation enthalten sind, und zum anderen, wie diese Informationen in XML bzw. Python konkret codiert werden.
         
         
            Kodierte Information
            Ein Märchen besteht im Sinne unseres Projektes aus den folgenden Bestandteilen:
            
               Eine Menge von Orten, an denen die Handlung spielt;
               Eine Menge von Charakteren, die an der Handlung beteiligt sind;
               Eine zeitliche Abfolge von Szenen, die jeweils an einem bestimmten Ort spielen und an denen jeweils eine Teilmenge der Märchencharaktere beteiligt ist;
               Jede Szene besteht ihrerseits aus einer zeitlichen Abfolge von Dialogakten zwischen den Märchencharakteren oder vom Erzähler zum Zuhörer. Zusammengenommen bilden diese Dialogakte den Märchentext.
            
            Im Folgenden werden die verschiedenen Bestandteile, sowie ihre Eigenschaften und Beziehungen untereinander, näher beschrieben.
            
               Orte, an denen das Märchen spielt, werden nur über ihren 
                    Typ (Attribut type) charakterisiert. Mögliche Ortstypen sind dabei z.B. Wald, Schloss oder Stall. Daneben existiert außerdem der Typ „Nirgendwo“ für Szenen ohne eindeutig bestimmbaren Ort (z. B. Abschnitte des Märchens, an denen nur der Erzähler beteiligt ist). Jeder Ort erhält eine spezifische ID der Form 
                    loc1, 
                    loc2 etc.
                
            
               Charaktere werden über eine Reihe von Eigenschaften beschrieben, welche zum einen inhärente demographische Eigenschaften (Name, Alter, Geschlecht, Typ), sowie zum anderen externe Eigenschaften (Einstellung, Propp-Archetyp – s. (Propp 1977)) beinhalten. Beim 
                    Namen (name) des Charakters handelt es sich um eine Zeichenkette, z.B. „Rapunzel.“ (Wird ein Charakter auf mehrere Arten gerufen, so wird die häufigste Bezeichnung gewählt.) 
                    Alter (age) des Charakters wird nicht in Zahlen, sondern in Stufen angegeben, da Märchen im Allgemeinen keine genauen Altersangaben enthalten; die möglichen Werte sind dabei „toddler“, „child“, „teenager“, „young adult“, „adult“ und „senior“. Das 
                    Geschlecht (gender) des Charakters wird den klassischen Vorstellungen folgend entweder mit „male“ oder „female“ angegeben. Zusätzlich gibt es den Wert „none“ für geschlechtlich unterspezifizierte Charaktere wie Tiere, Monster usw. Der 
                    Typ des Charakters unterscheidet z. B. zwischen „human“ oder „animal/monster“. Für „animal/monster“ unterscheiden wir zusätzlich nach 
                    Subtypen, z.B. für Tiere nach Größe, also „small“, „medium“ oder „big“, oder „witch“ und „demon“ für einen bestimmten Monstertyp. Eine binäre Feststellung der 
                    Einstellung bzw. Gesinnung des Charakters verortet diesen auf der Gut-/Böse-Achse: „evil“ oder „neutral“. Außerdem wird der 
                    Propp-Archetyp des Charakters angegeben: „hero“, „villain“ etc. (Propp, 1977). Jeder Charakter erhält eine spezifische ID von der Form 
                    ch1, 
                    ch2 usw. Außerdem gehören zu jedem Märchen zwei „Dummy“-Charaktere für Erzähler und Zuhörer, welche stets die IDs 
                    ch0 bzw. 
                    ch-1 und die Typen „narrator“ bzw. „listener“ zugewiesen bekommen. Dies ist nötig, um auch Passagen darstellen zu können, welche vom Erzähler gesprochen werden, der selbst ja kein eigentlicher Charakter der Handlung ist. Dies ist notwendig, um ein automatisches „Vorlesen“ des Märchens zu implementieren.
                
            
               Szenen werden im Hinblick auf Zeit, Ort, beteiligte Charaktere sowie Propp Funktionen (Propp, 1977) beschrieben. Der 
                    Zeitpunkt (time), zu dem die Szene spielt, wird anhand einer ID der Form 
                    t1, 
                    t2 usw. angegeben, wobei die IDs den linearen Ablauf der Zeit darstellen. Der 
                    Ort (location), an dem die Szene spielt, wird als String in Großbuchstaben angegeben, ausgewählt aus einer Liste mit Möglichkeiten. Der 
                    Übergang zur nächsten Szene (transition) wird ebenfalls codiert, indem das Bewegungsverb, das den Übergang von einem Ort zum anderen beschreibt, oder die Phrase, die stattdessen den Szenenwechsel einleitet, angegeben wird. Die an der Handlung der Szene beteiligten 
                    Charaktere werden mit ihren IDs angegeben, also z. B. 
                    ch2, 
                    ch3, 
                    ch5. Dabei werden alle Charaktere berücksichtigt, die in der Szene zugegen sind, auch wenn diese bspw. nicht sprechen. Die Propp-Funktionen und -Subfunktionen der Szene werden mit ihrem Symbol (nach der englischen Ausgabe Propp (1977)) angegeben, also z. B. A4 – „theft of daylight“. Jede Szene erhält eine spezifische ID der Form 
                    s1, 
                    s2 etc. Da die Märchenhandlung im Allgemeinen linear erzählt wird, ist der Index üblicherweise (aber nicht notwendigerweise) identisch mit demjenigen des Zeitpunkts der Szene, d. h. die Szene 
                    s1 wird üblicherweise zum Zeitpunkt 
                    t1 spielen usw. Jeder Szene sind 
                    Dialogakte untergeordnet, denen der zu dieser Szene gehörige Text entspricht.
                
            
               Dialogakte werden im Hinblick auf ihre Sprecher und Adressaten, ihren Inhalt sowie ihren Zeitpunkt beschrieben. Der 
                    Zeitpunkt (time), zu dem der Dialogakt geäußert wird, wird anhand einer ID angegeben, welche eine Spezifizierung der ID des Zeitpunkts der zugehörigen Szene darstellt. Spielt z. B. Szene 
                    s5 zum Zeitpunkt 
                    t5, so haben die zugehörigen Dialogakte die Zeitpunkte 
                    t5.1, 
                    t5.2 usw. Der 
                    Sprecher (speaker) des Dialogakts wird über seine ID angegeben. Der 
                    Adressat bzw. die 
                    Adressaten (receiver) des Dialogakts werden über eine Liste von Charakter-IDs angegeben, z.B. 
                    ch2, 
                    ch4, 
                    ch6. Passagen des Erzählers stellen dabei einen Spezialfall dar: Sie werden als Dialogakte des Erzählers mit dem Zuhörer bzw. Leser betrachtet, d. h. der „Dummy“-Charakter des Erzählers wird als Sprecher angegeben und der Dummy-Charakters des Zuhörers als Empfänger. Abgesehen davon werden sie behandelt wie Dialogakte zwischen Charakteren. Jeder Dialogakt erhält eine spezifische ID, die – unabhängig von der Szenestruktur – linear hochgezählt wird, also 
                    d1, 
                    d2 usw.
                
         
         
            XML-Repräsentation
            Die oben beschriebenen Informationen lassen sich im XML-Format darstellen. Dabei wird eine XML-Baumstruktur genutzt, um die Hierarchie der verschiedenen Objekte zu repräsentieren. Das Wurzelelement des Dokuments hat stets den Bezeichner Tale und die Attribute „title“ und „annotator“, welche Titel und den Namen des Annotators des jeweiligen Märchens enthalten:
            
               1: Struktur des Tale-Wurzelelements (Beispiel).
                ... 
               Diesem Element untergeordnet sind die Elemente Characters, Locations und Text. Das Characters-Element enthält Character-Subelemente, die jeweils die gesammelten Informationen für einen Charakter speichern:
            
            
               2: Struktur des Characters-Elements (Beispiel).
                
               Analog dazu enthält das Locations-Element untergeordnete Location-Elemente, die jeweils einen Ort codieren:
            
            
               3: Struktur des Locations-Elements (Beispiel).
                
               Das Text-Element enthält schließlich den eigentlichen Märchentext. Dieser ist auf die verschiedenen Szenen – repräsentiert durch Szene-Elemente – aufgeteilt, welche wiederum die verschiedenen Dialogakte (Dialog-Elemente) enthalten:
            
            
               4: Struktur des Text- und Szene-Elemente (Beispiel).
               
               ...
                Ach, du bist’s, alter Wasserpatscher, 
               ...
               
               Beim Entwurf des XML-Schemas wurde besonders Wert auf Übersichtlichkeit und Leserlichkeit gelegt. Trotz der Vielzahl der kodierten Informationen sind die resultierenden XML-Dateien daher vergleichsweise kompakt; so besteht die XML-Repräsentation des (vergleichsweise langen) Märchens „
                        Hänsel und Gretel“ bspw. nur aus 226 Zeilen.
                    
               Diese XML Repräsentation basiert auf und erweitert das Annotation Schema, das in (Scheidel & Declerck, 2010) beschrieben wird.
            
         
         
            Python-Repräsentation
            Auf der Grundlage der oben beschriebenen XML-Struktur kann eine Python-Klassenstruktur aufgebaut werden, die ein Märchen sowie seine einzelnen Teile als Python-Objekte repräsentiert.
            Neben einer Oberklasse Tale gibt es für jeden der oben beschriebenen Teile eine eigene Python-Klasse, d. h. die Klassen Location, Character, Scene und Dialogue. (Insgesamt bestehen die Dateien zur Märchen-Repräsentation aus 288 Zeilen Code.) Jede Klasse enthält dabei als Attribute die oben beschriebenen Eigenschaften, wobei diese auch Verweise auf andere Elemente darstellen können. So verweisen bspw. Dialogue-Objekte auf die Character-Objekte von Sprecher und Empfängern. Der Python-Code dient als Interface für drei Anwendungen. Erstens können Märchen aus bestehenden XML-Dateien eingelesen werden; zweitens können XML-Dateien anhand einer anderweitig (z. B. durch automatische Klassifizierung) erzeugten Python-Märchenstruktur generiert werden; und drittens kann anderer Python-Code auf die Märchen-Information zugreifen, was die Grundlage für Anwendungen wie Text-to-Speech oder Visualisierung bildet. Sowohl die XML Kodierung als auch die Python Objekte interagieren mit einer Märchen-Ontologie interagieren, die eine Erweiterung der in (Koleva et al., 2012) beschriebenen Ontologie ist.
            Somit haben wir eine formale Repräsentation von Märchen, die in verschiedenen Anwendungen zum Tragen kommen kann.
         
      
      
         
             Mit Beiträgen von Anastasija Aman, Stefan Grünewald, Matthias Lindemann, Lisa Schäfer, Natalia Skachkova.
         
         
            
               Bibliographie
               
                  Propp, Vladimir ; Scott, Laurence (Hrsg.): Morphology of the folktale. 2. überarbeitete Auflage. Austin, TX u.a., 1977
               
                  Antonia Scheidel and Thierry Declerck. 2010. Apftml - augmented proppian fairy tale markup language. In Sándor Dar´anyi and Piroska Lendvai, editors, First International AMICUS Workshop on Automated Motif Discovery in Cultural Heritage and Scientific Communication Texts. Szeged University.
               
                  Nikolina Koleva, Thierry Declerck, and Hans-Ulrich Krieger. 2012. An ontology-based iterative text processing strategy for detecting and recognizing characters in folktales. In Jan Christoph Meister, editor, Digital Humanities 2012 Conference Abstracts, pages 467–470, Hamburg, 7. University of Hamburg, Hamburg University Press
            
         
      
   


10716	2018	
      
         
            Einleitung
            Der Sammelauftrag der Deutschen Nationalbibliothek (DNB) beginnt 1913 und bezieht sich auf »lückenlos alle deutschen und deutschsprachigen Publikationen« (»Wir über uns«, 16.03.2017). Der DNB-Katalog ist natürlich längst digitalisiert und die Arbeit mit ihm mittlerweile sehr komfortabel, da der Datendienst der DNB unter http://www.dnb.de/datendienst vierteljährlich einen Komplettabzug der Katalogdaten im RDF-Format bereitstellt, unter der freien Lizenz CC0 1.0. Momentan (Stand vom 23.06.2017) enthält er 14 102 309 Datensätze, also Metadaten zu von der DNB gesammelten Medien. Bisher gibt es aus geisteswissenschaftlicher Sicht nur wenige Versuche, diese Quelle nutzbar zu machen (eine Ausnahme bilden etwa Häntzschel u. a. 2009). Wir präsentieren ein einfaches Framework, mit dem verschiedene Aspekte des DNB-Katalogs untersucht werden können, seine Entwicklung über die knapp 105 Jahre seit Bestehen der Nationalbibliothek (vgl. auch Schmidt 2017, der für die Library of Congress einen ähnlichen Ansatz vorgestellt hat). Wir konzentrieren uns dabei auf Romane als Untersuchungsobjekt, von denen in der DNB rund 180 000 als solche rubriziert sind (dies entspricht nicht der Gesamtanzahl an Romanen, denn Nachauflagen und Übersetzungen zählen dort mit hinein – außerdem fehlen auch einige Romane, da sie nicht entsprechend verschlagwortet worden sind. Dieser Vortrag ist methoden-, nicht vorderhand ergebniszentriert, wobei wir an zwei Anwendungsszenarien aus der Praxis der digitalen Literaturwissenschaft demonstrieren, wie Katalogmetadaten bei der Bearbeitung konkreter Forschungsfragen behilflich sein können bzw. diese überhaupt erst ermöglichen.
         
         
            Beschreibung des Frameworks
            Die Titeldaten der DNB werden in typischen Linked-Data-Formaten (RDF/XML, JSON-LD usw.) angeboten. Der übliche Ansatz mit solchen Daten zu arbeiten ist, diese in eine geeignete Datenbank (Triple-Store) einzuladen und Anfragen mit Hilfe der entsprechenden Anfragesprache (i. A. SPARQL) zu stellen. Prinzipiell sind auch andere Systeme (z. B. relationale Datenbank, Suchmaschine) geeignet. Dies ermöglicht sehr flexible Anfragen und die leichte Einbindung weiterer Datenquellen. Da die Größe der Daten (unkomprimiert ca. 21 GB) jedoch gewisse Anforderungen an die Hardware stellt und die Konfiguration und Optimierung der Datenbank aufwendig ist, haben wir uns für eine andere, kompakte und leichter nachzuvollziehende Lösung entschieden. Langfristiges Ziel ist jedoch die Bereitstellung einer fertig konfigurierten Arbeitsumgebung in Form eines Docker-Containers, in der die Daten in einer Datenbank ad hoc verfüg- und analysierbar sind.
            Der Titeldatensatz ist mit 14 102 309 Datensätzen und 227 212 707 Tripeln (»Fakten«) sehr umfangreich und enthält neben Angaben zu Büchern auch Angaben zu weiteren Medientypen wie etwa Zeitschriften. Neben den üblichen Metadatenfeldern wie Titel und Erscheinungsjahr ist bei Buchobjekten meist auch die Seitenanzahl sowie das Format vermerkt. Ganz im Sinne von Linked Data werden viele Angaben mit Hilfe von standardisierten Vokabularien (z. B. Dublin Core oder Bibo) beschrieben und ermöglichen so die Verlinkung mit weiteren Datensätzen. Insbesondere ermöglicht die Angabe der Autor*innen durch die numerische Kennung aus der Gemeinsamen Normdatei (GND) die Verknüpfung der Daten mit Wikidata, der (zukünftig) hinter Wikipedia stehenden Faktendatenbank. Wikidata verwendet ein auf Linked Data basierendes Datenmodell und ermöglicht, ähnlich wie Wikipedia, jedermann das Hinzufügen und Bearbeiten von Daten. Neben Angaben zu Städten und Ländern (z. B. Fläche, Einwohnerzahl) sind in Wikidata auch Daten zu zahlreichen Persönlichkeiten gespeichert, etwa deren Namen, Geburtsdaten, Berufe, Werke und, falls vorhanden, GND-Kennung (als Beispiel sei auf die Seite zu Johann Wolfgang von Goethe verwiesen: https://www.wikidata.org/wiki/Q5879).
            Unser Framework umfasst derzeit vier Schritte, die im Folgenden beschrieben werden:
            
               Vorverarbeitung und Konvertierung der Daten von RDF/XML zu JSON (rdf2json.py)
               RDF/XML wird von den üblichen Softwaretools im Allgemeinen nicht als Datenstrom verarbeitet, sondern im Hauptspeicher abgelegt und dann weiterverarbeitet. Aufgrund der Größe der Daten scheidet diese Möglichkeit aus. Da jedoch alle wesentlichen Daten zu einem Medium typischerweise innerhalb eines XML-Tags "rdf:Description" abgelegt sind, können wir die Daten auch mit Hilfe eines SAX-Parsers als XML verarbeiten. Wir extrahieren die für die Analyse wesentlichen Metadaten (z. B. dcterms:contributor, dcterms:language, dc:title, dcterms:extent, rdau:P60493) und speichern diese als JSON ab. JSON ist im Allgemeinen platzsparender als RDF/XML und kann leicht in Elasticsearch eingeladen werden, was ein geplanter nächster Schritt ist.
            
            
               Extraktion von Daten zu Autoren aus Wikidata (WKD-Toolkit)
               Unser Ziel ist die Anreicherung der Autorenangaben im DNB-Datensatz mit Informationen aus Wikidata, beispielsweise Geburtsdatum- und ‑ort, Beruf und Verweis auf einen etwa vorhandenen Artikel in Wikipedia. Da die Python-Softwarebibliothek zur Verarbeitung von Wikidata-Datensätzen veraltet ist, greifen wir auf das Java-basierte Wikidata Toolkit zurück. Nach Herunterladen des aktuell (14.08.2017) 16 GB großen komprimierten Wikidata-Datensatzes extrahieren wir in zwei Durchgängen zunächst alle Elemente mit einer GND-Kennung einschließlich ausgewählter Merkmale und ergänzen im zweiten Durchlauf die Werte der Merkmale. Das Ergebnis speichern wir im JSON-Format.
            
            
               Normalisierung und Anreicherung der Daten (json2json.py)
               Unser Python-Skript implementiert eine Pipeline, die alle in den vorherigen Schritten extrahierten Daten einliest und mit Hilfe der GND-Kennung verknüpft, Metadatenangaben (wie z. B. Seitenanzahlen) extrahiert, vereinfacht und normalisiert, Datensätze mit fehlenden Angaben filtert und schließlich die gewünschten Datenfelder spaltenbasiert ausgibt. Die Vereinfachung umfasst vor Allem das Entfernen von Namespace-Präfixen (etwa http://id.loc.gov/vocabulary/iso639-2/ bei der Angabe der Sprache); Seitenanzahlen werden mit Hilfe eines regulären Ausdrucks extrahiert, der die häufigsten Fälle abdeckt; Jahreszahlen ebenso; Verlagsnamen können mit Hilfe einer Normtabelle normiert werden (dies ist nötig, da die Schreibung dieser Namen innerhalb des Katalogs nicht standardisiert ist).
            
            
               Analyse der Daten (Shell-Skripte und -Tools wie awk, sort, datamash, gnuplot, ...)
               Die entstandenen Dateien im TSV-Format können mit den üblichen Unix-Kommandozeilen-Werkzeugen wie awk, sort, uniq etc. leicht verarbeitet und analysiert werden; Visualisierungen wurden mit gnuplot erzeugt. Alle Schritte sind im GitHub-Repository dokumentiert.
            
         
         
            Zeitliche Entwicklung über 105 Jahre DNB
            Abbildung 1 zeigt die zeitliche Verteilung einiger Subdatensätze des Katalogs. Von den etwa 14,1 Mio. Objekten im originalen DNB-Datensatz weisen etwa 8,3 Mio. extrahierbare Seitenanzahlen auf (59 %). Beschränken wir diese Anzahl auf ›Romane‹ (über das Datenfeld "rdau:P60493"), bleiben 353 498 übrig, von denen wiederum 316 518 Umfangsangaben aufweisen und 180 219 einen Verfasser, der mindestens einen Wikipedia-Eintrag (in egal welcher Sprache) besitzt. Dieses Datenset ist die Grundlage für die unten folgenden Anwendungsszenarien.
            
               
                  
                  Abbildung 1: Fünf verschieden qualifizierte Subdatensätze des DNB-Katalogs in zeitlicher Verteilung.
               
            
         
         
            Repräsentativität
            Als möglicher Plausibilitäts- bzw. Repräsentativitätstest kann das Auszählen derjenigen Romanciers dienen, die mit den meisten Romanen im Katalog vertreten sind. Da der DNB-Katalog Vollständigkeit anstrebt, kann ein entsprechendes Ranking etwas über vergangene Realitäten auf dem deutschsprachigen Buchmarkt aussagen (Tab. 1), und tatsächlich stehen die Verfasser*innen von Romanbestsellern im Unterhaltungsbereich ganz oben (die Anzahl der Bücher umfasst von der DNB mitgesammelte Neuauflagen, Konsalik hat also nicht über 2 000 Romane geschrieben).
            
               
                  Autor*in
                  Romane
               
               
                  Heinz G. Konsalik
                  2232
               
               
                  Marie Louise Fischer
                  1264
               
               
                  Gert Fritz Unger
                  1013
               
               
                  Georges Simenon
                  783
               
               
                  Utta Danella
                  778
               
               
                  Edgar Wallace
                  654
               
               
                  Hedwig Courths-Mahler
                  647
               
               
                  Eleanor Hibbert
                  635
               
               
                  Pearl S. Buck
                  596
               
               
                  Alistair MacLean
                  582
               
               
                  Stephen King
                  577
               
               
                  Georgette Heyer
                  576
               
               
                  Agatha Christie
                  574
               
               
                  Theodor Fontane
                  565
               
               
                  Hans Ernst
                  563
               
               
                  Lion Feuchtwanger
                  501
               
               
                  Erich Maria Remarque
                  419
               
               
                  Hans Hellmut Kirst
                  411
               
               
                  Johannes Mario Simmel
                  403
               
               
                  Hans Fallada
                  396
               
               
                  Heinrich Mann
                  394
               
               
                  Fjodor Dostojewski
                  390
               
               
                  Barbara Cartland
                  390
               
               
                  Nora Roberts
                  381
               
               
                  Graham Greene
                  375
               
               
                  A. J. Cronin
                  370
               
               
                  Vicki Baum
                  366
               
               
                  Thomas Mann
                  359
               
               
                  Robert Ludlum
                  358
               
               
                  Gerd Hafner
                  357
               
               
                  Dean Koontz
                  354
               
               
                  Heinrich Böll
                  340
               
               
                  Alexandra Cordes
                  325
               
               
                  John le Carré
                  322
               
               
                  Marion Zimmer Bradley
                  321
               
               
                  Jason Dark
                  317
               
               
                  Willi Heinrich
                  313
               
               
                  Ludwig Ganghofer
                  311
               
               
                  Jack London
                  309
               
               
                  Joseph Roth
                  307
               
               
                  Danielle Steel
                  299
               
               
                  Johanna Lindsey
                  288
               
               
                  Erle Stanley Gardner
                  287
               
               
                  Siegfried Lenz
                  279
               
               
                  Jules Verne
                  277
               
               
                  Rosamunde Pilcher
                  274
               
               
                  Franz Kafka
                  271
               
               
                  Ernest Hemingway
                  271
               
               
                  Taylor Caldwell
                  269
               
               
                  Dorothy L. Sayers
                  269
               
            
            Tabelle 1: Romanautor*innen geordnet nach Anzahl der Werke (inkl. Nachauflagen) im DNB-Katalog.
         
         
            Anwendungsfall 1: Buchtitel
            Die Verfügbarkeit großer digitalisierter Kataloge ermöglicht Large-Scale-Analysen bibliografischer Metadaten, etwa die Entwicklung von Romantiteln. Ein Vorläufer auf diesem Gebiet, Werner Bergengruens immer noch zu empfehlende Bibliothekarsfantasie »Titulus« von 1960, musste sich noch auf eine manuelle Sammlung des Autors stützen. Mittlerweile gibt es mit Franco Morettis Studie »Style Inc.« (2009) ein prominentes datengestütztes Beispiel (wobei sich Moretti bei seiner Analyse von um die 7 000 Romantiteln auf Fachbibliografien stützte, nicht auf Katalogdaten).
            Um einen ersten Einblick in das Vokabular von Romantiteln zu bekommen, seien in Tabelle 2 die am häufigsten vorkommenden Substantive aufgelistet.
            
               
                  Substantiv
                  Frequenz
               
               
                  Liebe
                  3117
               
               
                  Mann
                  1906
               
               
                  Frau
                  1686
               
               
                  Tod
                  1537
               
               
                  Nacht
                  1505
               
               
                  Leben
                  1496
               
               
                  Welt
                  1188
               
               
                  Haus
                  1158
               
               
                  Zeit
                  1037
               
               
                  Schatten
                  1029
               
            
            Tabelle 2: Häufigste Substantive in Romantiteln im gesamten DNB-Katalog.
            Überzeitliche Konzepte – Liebe, Tod usw. – dominieren das Feld. Und nebenbei bemerkt: Ein wenig erinnert diese Liste an Jan Böhmermanns satirischen Song »Menschen, Leben, Tanzen, Welt«, mit dem auf die Beliebig- und Austauschbarkeit kontemporärer deutschsprachiger Liedproduktion angespielt wird (vgl. Pandzko/Böhmermann 2017), ein Befund, der sich analog auch auf Romantitel projizieren ließe.
            Diese Anfragetechnik kann – wie beim Google Ngram Viewer – auf n-Gramme ausgedehnt werden, die Top-10 der häufigsten Trigramme findet sich in Tabelle 3.
            
               
                  Trigramm
                  Frequenz
               
               
                  Das Geheimnis der
                  238
               
               
                  Das Haus der
                  224
               
               
                  Der Mann der
                  189
               
               
                  Das Geheimnis des
                  175
               
               
                  Die Tochter des
                  160
               
               
                  Im Schatten des
                  128
               
               
                  Der Mann im
                  128
               
               
                  Das Lied der
                  125
               
               
                  Die Frau des
                  124
               
               
                  Die Reise nach
                  108
               
            
            Tabelle 3: Häufigste Trigramme in Romantiteln im DNB-Katalog.
            Ebenfalls analog zum Ngram Viewer lässt sich die zeitliche Entwicklung von n-Gramm-Frequenzen darstellen. Die unterschiedlichen Darstellungen in absoluten (Abb. 2) und relativen Zahlen (Abb. 3) kann etwa zeigen, dass sich zwischen Mitte der 1970er-Jahre und Mitte der 1990er-Jahre die Zahl an Romanen mit »Liebes«-Titeln zwar nahezu verdoppelt, dass sich diese Titel aber in relativen Zahlen nicht großartig vermehren.
            Für genauere Analysen auf Grundlage dieser Extraktions- und Visualisierungsmethoden stellt das von uns vorgestellte Framework eine ideale Basis dar.
            
               
                  
                  Abbildung 2: Vorkommen ausgewählter Wörter in Romantiteln im zeitlichen Verlauf (absolut).
               
            
            
               
                  
                  Abbildung 3: Vorkommen ausgewählter Wörter in Romantiteln im zeitlichen Verlauf (relativ).
               
            
         
         
            Anwendungsfall 2: Textumfang
            Unser zweites Anwendungsszenario betrifft die Erforschung des literarischen Textumfangs. Abbildung 4 zeigt die durchschnittliche Seitenanzahl von Romanen im Katalog der DNB.
            Als Zuarbeit zu einer Theorie des literarischen Textumfangs haben wir mit dem von uns hier vorgestellten Framework in einer umfangreicheren Studie untersucht, wie sich der Umfang von Romanen etwa auf die Kanonbildung auswirkt (längere Romane, speziell solche von mehr als 1 000 Seiten Umfang, haben es leichter, in Kanonlisten zu landen). Außerdem ist es uns gelungen zu zeigen, wie umfangreiche Romane die DNA von Verlagen bestimmen können (vgl. Fischer/Jäschke 2018).
            
               
                  
                  Abbildung 4: Entwicklung der mittleren Seitenanzahl pro Jahr seit 1913.
               
            
         
         
            Fazit
            Katalogdaten als Untersuchungsobjekt der quantifizierenden Literaturwissenschaften sind keine sich selbst erklärende Quelle, sondern ein über Jahrhunderte gewachsenes, überaus komplexes System. Die bibliothekarische Betreuung dieser Daten zielt nicht per se auf literaturwissenschaftliche Anwendungsfälle. Die Verschlagwortung kann lückenbehaftet sein, bestimmte Angaben wie etwa zum Textumfang können Fehler aufweisen. Die literaturwissenschaftliche Beschäftigung mit Katalogdaten setzt deren Explorier- und Kontrollierbarkeit voraus, wozu das hier vorgestellte Framework einen ersten Beitrag leisten soll. Zwei konkrete Anwendungsfälle sollten als Praxisbeispiele und ausdrücklich als Anreiz für weitere Szenarien dienen.
         
      
      
         
            
               Bibliographie
               Das 
                        Arbeitsrepositorium ist unter > zu finden.
                    
               
                  Bergengruen, Werner (1960): Titulus. Das ist: Miszellen, Kollektaneen u. fragmentar., mit gelegentl. Irrtümern durchsetzte Gedanken zur Naturgeschichte d. dt. Buchtitels oder unbetitelter Lebensroman e. Bibliotheksbeamten. Zürich: Verlag der Arche.
                    
               
                  DNB (2017): »Wir über uns«, Stand 16.03.2017. URL: >.
                    
               
                  Fischer, Frank; Jäschke, Robert (2018): Ein Quantum Literatur. Empirische Daten zu einer Theorie des literarischen Textumfangs. DFG-Symposium »Digitale Literaturwissenschaft«. Villa Vigoni, 9.–13. Oktober 2017. (Entsprechender Sammelband erscheint demnächst.)
                    
               
                  Häntzschel, Günter; Hummel, Adrian; Zedler, Jörg (2009): Deutschsprachige Buchkultur der 1950er Jahre. Fiktionale Literatur in Quellen, Analysen und Interpretationen. Wiesbaden: Harrassowitz 2009. URL: >.
                    
               
                  Moretti, Franco (2009): Style Inc. Reflections on Seven Thousand Titles (British Novels, 1740–1850). In: Critical Inquiry, Vol. 36, No. 1 (Autumn 2009), S. 134–158.
                    
               
                  Pandzko, Jim
                  ; 
                  Böhmermann, Jan (2017): Menschen Leben Tanzen Welt [Musikvideo]. In: Neo Magazin Royale, 05.04.2017. URL: >.
                    
               
                  Schmidt, Ben (2017): A brief visual history of MARC cataloging at the Library of Congress. In: Sapping Attention [Blog], 16.05.2017. URL: >.
                    
            
         
      
   


10752	2018	
      
         
            Introduction and Motivation
            The emergence of computational methods of text processing has created new paradigms of research in literary studies in recent years (Jockers & Underwood, 2016), for instance 
                    distant reading to find patterns and regularities (Moretti, 2005). Network analysis and extraction of information about relations between characters from literary texts is an example for distant reading methods. Such information can not only be helpful for better understanding of character interactions but can also facilitate the comparison of thereof in different texts.
                
            Existing tools of text analysis and network visualization such as Voyant
                     or Gephi
                     are either missing modules for character network analysis or require preliminary steps on data preprocessing from the user and therefore are not easy-to-use for some humanities scholars who lack programming skills. Interactive tools in addition often lack features to ensure reproducibility of results.
                
            We present our ongoing effort on closing this gap by developing a literary analysis reporting tool 
                    rCAT
               
                  
               , whose primary purpose is to provide an easy-to-use, stable, and reusable solution for automatic extraction of relational information from text and to characterize these relationships automatically to provide the user with deeper qualitative insight. We opt for implementation as a web-based reporting tool instead of an interactive tool for two reasons: (1) automatically generated reports in PDF format can serve as a stable foundation for discussion and can be reused in publications and visualizations easily, and (2) the results are clearly connected to the chosen input parameters such that reproducibility of results is ensured.
                
            As a use-case study, we apply 
                    rCAT to Johann Wolfgang von Goethe's epistolary novel 
                    Die Leiden des jungen Werthers. On the basis of this epistolary novel, we show that not only the network can be generated, but also the characteristic triangular relationship of the protagonists is easily identified. The goal is to automatically determine this triad in the original text and in the adaptations that have been published since the publication of 
                    Werther in 1774.
                
         
         
            
               Previous Work
                
            Previous research on social networks in literary fiction generally fall into one of the two categories: (1) works that explore methods for extracting and formalizing character networks (
                    cf., Elson et al. (2010), Agarwal et al. (2012, 2013), Park et al. (2012)), and (2) works that primarily focus on qualitative implications of network analysis (
                    cf., Rydberg-Cox (2011), Moretti (2011), Nalisnick & Baird (2013), Jayannavar et al. (2015)). It is common to address both tasks at the same time, as in Beveridge & Shan (2016), who introduce a number of formal measures for analyzing the centrality of the characters in 
                    Game of Thrones books, which results in both expected and surprising findings. 
                
            Building on graph theory extensively elaborated in the past fifty years (e.g., Bondy and Murty, 1976 or West, 2001), our work is similar to Beveridge & Shan (2016), in particular, in terms of the weighted degree measure, and to Park et al. (2012), in terms of distance measure for detecting closely related characters in a text.
         
         
            
               Methods
                
            In the following, we explain the different components in 
                    rCAT, which are available for text analysis. After that, we discuss the results based on a use-case study.
                    
            
            
               Character lists and character identification
               To detect character mentions in the text we use a fundamental named-entity recognition approach based on dictionaries. This approach is suitable for scholars who analyze texts they already know. Consequently, we opt for a transparent and simple character recognition procedure: The user provides a list of character names to be included in the analysis specifying a canonical name form and all variations thereof she would like to take into account (
                         e.g., “Lotte” is the canonical name and “Lotten”, “Lottens”, “Lottgen”, “Lottchen”, “Charlotten S.”. are its variants).
                     
            
            
               Relation detection and context words
               We define the closeness of relationship between two characters using a 
                    distance measure
                  dist
                  X
                  (p,q), where 
                    p and 
                    q are the strings corresponding to these characters and 
                    X is the number of tokens between them (Park et al., 2012). In addition, we introduce the 
                    context measure
                  cont
                  Y
                  (p,q), where 
                    p and 
                    q are the strings corresponding to these characters and 
                    Y is the number of tokens before the character 
                    p and after the character 
                    q. While the former measure allows for detecting those characters that are closely related to each other, the latter one enables a contextual analysis of their relationship.
                
            
            
               Network analysis
               We visualize the network of characters with an undirected graph 
                    G=(V,E), where 
                    V are the vertices, each vertex corresponding to one character, and each edge 
                    E=(V
                  i,
                  ,V
                  j
                  ) corresponding to relations between pairs of characters. We output the following measures for each character node: 
                    degree, 
                    edge weight, 
                    weighted degree and 
                    density. The degree is the number of edges occurring with a given vertex. The edge weight, 
                    w
                  i,j
                   ≥ 0, is defined as the number of interactions between the vertices V
                    i and V
                    j. The weighted degree is the sum of weights of the edges occurring with a vertex 
                    i. Density is the ratio of occurring edges between two vertices and all possible vertex pairs.
                    
            
            
               Word clouds
               Word clouds are an approach to visualize the vocabulary of a text. The size of one word corresponds to its frequency. We use two different kinds of word clouds: For each character in the character list, we show word clouds based on the context of a window size 
                        n. For each pair of characters occurring in the network, we present a word cloud based on the words between them as well as on the words found in the context. Both types of word clouds can be filtered to the specific word fields (words from specific domains) which is helpful in gaining a focused insight into the characters relations.
                    
            
            
               Word Field developments
               We plot the timeline of multiple predefined world fields (specified by word lists) in the text. This feature is helpful in representing how certain fields (
                      e.g., concepts, emotions) develop throughout the narrative (Kim et al., 2017).
                   
            
            
               Implementation
               The tool was developed using Python v.3.6 and the Flask
                     web development framework. The tool outputs a single PDF report. The resulting document contains information from the analysis modules described in the previous section. Network graphs included in the report are generated with 
                    graphviz. Additionally, the tool can generate a CSV file that can be used as input to Gephi. 
                    
            
         
         
            
               Use-case Demonstration
                
            For a use-case analysis, we apply 
                    rCAT to 
                    Die Leiden des jungen Werther by Johann Wolfgang Goethe with the following parameters: X=8, Y=5, stop words removed (previous work focused on this analysis without rCAT, 
                    cf. Murr, 2017).
                
            In Goethe's epistolary novel, the protagonist Werther describes his unhappy love for Lotte, who is engaged to Albert. The characteristic triangular relationship in the novel arises from this constellation (protagonist - beloved woman - antagonist). With 
                    rCAT we expect to identify and characterize this relationship. Figures 1 and 2 show a sample network analysis output (tables are shown only partly).
                
            The protagonist Werther shows a degree of 21, which is the number of characters with whom he interacts. The closest relationship measured by edge weight (Figure 2) is observed between Werther and Lotte (81 interactions). The antagonist Albert has a low degree of 3. However, his weighted degree is 36 (third highest after Werther and Lotte), which confirms his important role in the triangular relationship.
            
               
                  
                  Illustration 1: Degrees and weighted degrees for most important characters of Goethe’s Werther
               
            
            
               
                  
                  Illustration 2: Edge weights
               
            
            
               
                  
                  Illustration 3: Complete network of Goethe’s Werther
               
            
            Highlighted in red is the typical triangular relationship in Goethe’s novel, which corresponds to the three highest weighted degrees. In further steps, we will use 
                    rCAT to analyze the adaptations of Goethe's novel with a focus on this triad.
                
            To better characterize the edges, the tool outputs top-
                    n word clouds sorted by edge weight (
                    n is specified by the user) for character pairs and by degree for single characters. Figure 4 and 5 show examples of the word clouds for character pairs filtered to the words from the emotion domain.
                
            
               
                  
                  Illustration 4: Word clouds for Werther-Lotte
               
               
                  
                  Illustration 5: Werther-Albert
               
            
            The word clouds enable first conclusions about the relationships of the characters. Werther and Lotte's word cloud characterizes their ambivalent relationship. The key words "Leidenschaft" and "Freude" reflect Werther's love, whereas the mentions of "sterben" and "Verblendung" are characteristic of the unrequited love, which leads Werther into his "disease unto death". As Werther and Albert’s word cloud reveals, their relationship is dominated by the "Unruhe" that Werther feels through his adversary. 
            Additionally, the tool plots the development of the narrative (not bound to specific characters) based on the word fields, an example of which is shown on Figure 6. In this case we used words from the emotion domain (with emotion dictionaries by Klinger et al. (2016)).
            
               
                  
                  Illustration 6: Word field development for Goethe’s Werther
               
            
            The word field development can highlight the prevalence of individual emotion domains across the text. The accumulation of the negative emotion words (Wut,Trauer, Furcht) towards the end suggests, for example, that Goethe’s novel has no “happy ending”. The striking rash on “Freude”, however, captures the last happy hours Werther spends with Lotte in the second part of the narration before he kills himself.
            
               
                  Future Work
                    
               The next version of the tool will include a character-oriented word field development calculated and plotted for the main characters of the stories. In addition, future releases will include more analysis features and bulk file processing.
            
         
      
      
         
            
               
            
            
               
            
            
               
                  www.ims.uni-stuttgart.de/data/rcat
               
            
            
               
            
         
         
            
               Bibliographie
               
                  Agarwal, A. / Corvalan, A. / Jensen, J. / Rambow, O. (2012): “Social Network Analysis of Alice in Wonderland”, in: CLfL@ NAACL-HLT 88-96.
               
                  Agarwal, A. / Kotalwar, A. / Rambow, O. (2013): “Automatic Extraction of Social Networks from Literary Text. A Case Study on Alice in Wonderland”, in: IJCNLP 1202-1208.
               
                  Beveridge, A. / Shan, J., (2016): “Network of thrones”, in: Math Horizons, 23(4): 18-22.
               
                  Bondy, J.A. / Murty, U.S.R. (1976): Graph theory with applications (Vol. 290). London: Macmillan.
               
                  Burrows, J.F. (1987): “Word-patterns and story-shapes: The statistical analysis of narrative style”, in: Literary & Linguistic Computing, 2(2): 61-70.
               
                  Elson, D.K. / Dames, N. / McKeown, K.R. (2010): “Extracting social networks from literary fiction”, in: Proceedings of the 48th annual meeting of the association for computational linguistics 138-147. Association for Computational Linguistics.
               
                  Heuser, R., F. Moretti / E. Steiner (2016): The Emotions of London. Technical report. Stanford University. Pamphlets of the Stanford Literary Lab.
               
                  Jayannavar, P. / Agarwal, A. / Ju, M. and Rambow, O. (2015): “Validating Literary Theories Using Automatic Social Network Extraction”, in CLfL@ NAACL-HLT 32-41.
               
                  Jockers, M.L. / Underwood, T. (2016): “Text‐Mining the Humanities”, in: Schreibman, Susan / Siemens, Ray / Unsworth, John (eds.): A New Companion to Digital Humanities 291-306.
               
                  Kim, E. / Padó, S. / Klinger, R. (2017): “Investigating the Relationship between Literary Genres and Emotional Plot Development”, in: Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature 17-26.
               
                  Klinger, R. / Sulliya S.S. / Reiter N. (2016): “Automatic Emotion Detection for Quantitative Literary Studies -- A Case Study on Kafka's ‘Das Schloss’ and ‘Amerika’”, in: Digital Humanities (DH), Conference Abstracts, Kraków, Poland, 2016.
               
                  Michel, J.B. / Shen, Y.K. / Aiden, A.P. / Veres, A. / Gray, M.K. / Pickett, J.P. / Hoiberg, D. / Clancy, D. / Norvig, P. / Orwant, J. / Pinker, S. (2011): “Quantitative analysis of culture using millions of digitized books”, in: science, 331(6014) 176-182.
               
                  Moretti, F. (2005): Graphs, maps, trees: abstract models for a literary history. Verso.
               
                  Moretti, F. (2011). Network theory, plot analysis. Stanford Literary Lab Pamphlet Series 2. Available at: https://litlab.stanford.edu/LiteraryLabPamphlet2.pdf
               
                  Murr, S. / Barth, F. (2017): Digital Analysis of the Literary Reception of J.W. v. Goethe’s ‘Die Leiden des jungen Werthers’, in: Digital Humanities (DH), Conference Abstracts, Montreal, Canada 2017.
               
                  Nalisnick, E.T. / Baird, H.S. (2013): “Extracting sentiment networks from Shakespeare's plays”, in: Document Analysis and Recognition (ICDAR), 2013 12th International Conference on IEEE 758-762.
               
                  Park, G.M. / Kim, S.H. / Cho, H.G. (2013): “Structural analysis on social network constructed from characters in literature texts”, in: Journal of Computers, 8(9): 2442-2447.
               
                  Rydberg-Cox, J., (2011): “Social networks and the language of greek tragedy”, in: Journal of the Chicago Colloquium on Digital Humanities and Computer Science (Vol. 1, No. 3).
               
                  West, D.B. (2001): Introduction to graph theory (Vol. 2). Upper Saddle River: Prentice Hall.
            
         
      
   


10754	2018	
      
         
            1. Einleitung: Verfassungsvergleich als Spiegel staatlichen Wandels?
            Staatlich verfasste Gesellschaften sind komplex und differenziert (Mayntz 1997; Schimank 1999). Will man etwas über die „Identität“ von Staaten und deren Wandel erfahren, dann eignen sich Verfassungen, da diese spezfische Dokumentensorte soziologisch als kodifizietre Selbstbeschreibung von Gesellschaften verstanden werden kann (Boli-Bennett und Meyer 1978; Go 2003; Heintz und Schnabel 2006; Boli-Bennett 1979). Moderne Staaten produzieren aber nicht nur Unmengen an amtlichen Dokumenten, sondern sind darüber hinaus durch ihre konstitutionelle sowie rechtsstaatlich-bürokratische Verfasstheit grundsätzlich textlich strukturiert (Weber 1972). Für die historsiche Dokumentenanalyse spielen die Erstellung des Korpus und die Auswahl der Untersuchungsmethoden wichtige Rollen, um sowohl die Textlichkeit, als auch den Kontext angemessen zu berücksichtigen.
            In diesem Vorhaben wird mit Verfassungsdokumenten europäischer Staaten gearbeitet, anhand derer staatlicher Wandel von der ersten Verfassungsgebung bis heute sichtbar gemacht wird. Verfassungen beinhalten u.a. Vorstellungen darüber, wie die Gesellschaft beschaffen ist. Konkret wird der historisch-wissenssoziologischen Frage nach der sozialen Konstruktion des (Staats-)Bürgers nachgegangen. Denn historisch betrachtet reflektieren Verfassungen den sukzessiven Umbau von ständisch stratifizierten hin zu souveränen Bürgergesellschaften und damit reflektieren sie ebenso den Wandel des gesellschaftlichen Personals, das es in Form von Personenkategorien und Zugehörigkeitsdimensionen aus dem Material herauszuarbeiten gilt. Was aber genau unter „Verfassung“ verstanden wurde, wie sich die Staaten und ihr ‚Personal’ über dieses Dokument selbst beschreiben und welches Wissen in selbiges eingeht, variiert erheblich (Gosewinkel, Masing und Würschinger 2006; Vorländer 2007). Daher bedarf es eines geeignetne methodisch-analytischen Instrumentariums, um die aufgeworfene Fragen zu beantworten.
         
         
            2. Dokumentenanalyse im Schnittfeld von historischer Soziologie und Computerlinguistik 
            Um Verfassungen strukturell und inhaltlich untersuchen zu können, werden Ansätze der historischen Wissenssoziologie (Thelen 1999, 2002; Jepperson 1991) und der Computerlinguistik (z.B. Hausser 2014; Lobin 2010) miteinander verschränkt. Die Entwicklung dieses methodischen Werkzeugs zur „Dokumentenarbeit“ umfasst Verfahrensschritte der Datenerhebung, -aufbereitung und -auswertung, wobei hier vor allem auf methodologische Herausforderungen, d.h. die Korpuserstellung und die semi-automatische Analyse von Dokumentenstrukturen eingegangen wird. 
            Rechtstexte im Allgemeinen und Verfassungen im Besonderen, weisen eine Dokumentenlogik auf, die stark durch eine formale hierarchische Struktur gekennzeichnet ist. Bei dieser Dokumentenart ist daher davon auszugehen, dass der Struktur eine besonders sinnstiftende Bedeutung zukommt, die es vor allem bei vergleichenden Untersuchungen (synchron wie auch diachron) zu berücksichtigen gilt. Insofern sollte ein computerlinguistisches Verfahren die Strukturinformationen bspw. in welche (Sinn-)Abschnitte sich ein Dokument gliedert für den Vergleich nutzen. Diese Strukturauswertungen können dann wiederum mit statistischen Häufigkeits- und Ähnlichkeitsberechnungen von Worten innerhalb von Fließtexten – wie das u.a. die gängigen Vektorraummodelle (Manning, Raghavan und Schütze 2008; Salton, Wong und Yang 1975) oder insbesondere die derzeit populären „word embedding“ Modell (z.B. Mikolov et al. 2013) machen – kombiniert werden. 
            Bei der hierarchischen Struktur von Dokumenten anzusetzen bietet einen klaren Ausgangspunkt für die systematische Analyse von großen Textmengen und stiftet zugleich Orientierung im Feld der inhaltsanalytischen Methoden (Kuckartz 2012; Mayring 2015). Diese unterscheiden sich vor allem in Bezug auf ihre Anlage, d.h. entweder Häufigkeiten zählende oder hermeneutisch interpretierende Ausrichtung, und firmieren in den Sozialwissenschaften oftmals unter dem Label „Dokumentenanalyse“. Zwar verbindet alle diese Ansätze, dass sie sich durch eine ständige Korrespondenz von Forschungsfrage und Arbeit am Material auszeichnen und in der Regel mehrere Iterationen durchlaufen, bevor valide Ergebnisse vorliegen. Dennoch bringt vornehmlich die manuelle Bearbeitung von umfangreichen Textmengen, etwa in Form von Kodier- und Kategorisierschritten der 
                    Grounded Theory (Strauss und Corbin 1996), Probleme der methodisch kontrollierten Auswertung und damit der Reliabilität der Ergebnisse mit sich.
                
            Außerdem lassen sich über den strukturellen Zugang Fragen erschließen, die über den „reinen“ Inhalt hinausgehen, und die die Verwendung wie auch die Art und Weise in den Vordergrund rücken, in der Verfassungen im Zeitverlauf politisch unter Druck geraten, sich also aufgrund wechselnder politischer Machtverhältnisse wandeln. Damit werden die Relation von Dokument und (Entstehungs-)Kontext und besonders die Verfasser von Dokumenten und deren Konstruktion sozialer Wirklichkeit durch die schriftliche Fixierung gesellschaftlichen Wissens (Prior 2011) fokussiert.
            Von der Dokumentenstruktur auszugehen heißt, zunächst methodologisch zu fragen, inwieweit sich Dokumente formal wie auch inhaltlich ähneln. Das setzt wiederum voraus, dass sich Dokumente überhaupt vergleichen lassen und so einer Analyse etwaiger struktureller Ähnlichkeiten und spezifischer Unterschiede allererst zugänglich gemacht werden. Hier bieten computergestützte Verfahren einen produktiven Ausgangspunkt, um einerseits bestehende Methoden zu reflektieren und andererseits ein dann auch verallgemeinerbares Werkzeug zur Dokumentenanalyse von Rechtstexten zu entwickeln. Zur Beantwortung der formulierten Frage wird eine innovative, von uns eigens entwickelte Software vorgestellt, mit der sich Verfassungen in ihrer historischen Entwicklung vergleichen lassen. Hierdurch werden Impulse zur Generierung neuer methodischer Ansätze gegeben werden. 
         
         
            3. Arbeitsschritte: Vom Download zur Analysesoftware 
            Um mit der Auswertung der Dokumente beginnen zu können, muss das Korpus erstellt werden. Ein normales PDF beinhaltet in der Regel kaum explizite Strukturinformationen, lediglich einzelne Worte ließen sich automatisch erfassen, nicht aber die zugrunde liegenden Strukturen abbilden. Hierfür bedürfte es bspw. der Kennzeichnung von Überschriften, Absätzen oder inhaltlich unterscheidbaren Abschnitten. Das Dokument muss also mit weiteren Informationen in seiner Struktur beschrieben werden.
            Das konkrete methodische Vorgehen, das hier als „Dokumentenarbeit“ zur Korpuserstellung bezeichnet wird, gliedert sich in drei Schritte: das Zusammenstellen der Ausgangsdaten als HTML-Dateien, die Transformation der Ausgangsdaten in das XML-Format sowie die eigentliche Auszeichnung der Ausgangsdaten mit Metadaten zur Modellierung der Struktur. 
            Entgegen der Annahme, dass derart politisch relevante Dokumente wie Verfassungen als elektronische Ausgangsdaten vorliegen sollten, müssen diese zunächst hergestellt und in ein bearbeitbares Datenformat transformiert werden. Zwar finden sich aktuelle Verfassungen als elektronisch veröffentlichte Ressourcen bspw. in den Rechtsdatenbanken und -portalen der jeweiligen Staaten, im deutschen Fall bspw. „Juris“.Es existiert jedoch kein lückenloser, chronologischer Verlauf, aus dem sich alle Änderungen computergestützt entnehmen ließen. 
            Auf der Seite 
                    www.verfassungen.org lassen sich die meisten Verfassungen online (auf Deutsch) abrufen und downloaden. Zudem beinhalten die dortigen Dokumente farblich abgesetzte Änderungen in Textform und nicht etwa als Kommentar oder gesonderte Liste sowie jeweils Totalrevisionen als separate Dokumente. Diese Dokumente werden dann mit offiziell veröffentlichen Verfassungen abgeglichen, um gegebenenfalls inhaltliche Fehler aufzuspüren und zu beheben. Anschließend werden die Daten manuell bereinigt und standardisiert, d.h. nicht benötigte Beschreibungen der Autor*innen der Webseiten oder andere irrelevante Informationen werden entfernt. Diese Dokumente bilden sodann die Grundlage für das Korpus. Die einzelnen Verfassungen beinhalten eine Fülle an textlichen Ergänzungen, Streichungen und anderweitigen textlichen Veränderungen, die einerseits schwer zu identifizieren sind und andererseits nicht chronologisch sortiert, sondern der Texthierarchie folgend vorliegen. Aus diesen Gründen wird zuerst eine Ausgangsverfassung des Jahres 2011, also dem Ende des Untersuchungszeitraums, erstellt, um ausgehend davon jede weitere Änderung als eigenständige, nicht offizielle, „Phantom-Verfassung“ zu rekonstruieren. Durch diesen iterativen, historischen Rekonstruktionsprozess wird schließlich die Datengrundlage geschaffen. 
                
            Die Verfassungen liegen zunächst als HTML-Dokumente, mit einer sehr flachen Dokumentenstruktur vor. Die zu entwickelnde Analysesoftware benötigt jedoch ein Datenformat, das Metadaten mit Dokumentendaten assoziieren kann. Dem aktuellen Entwicklungsstand entsprechend verwenden wir ein XML-Format (Bubenhofer und Scharloth, 2015). 
            Deshalb wird im nächsten Schritt der HTML-Code mittels eines XSL-Skripts (Extensible Stylesheet Language) in das technische XML Format (vgl. XML Schema 2001; XQuery 2002; XSLT 1999) überführt und formatiert. Bei XSL bzw. XSLT handelt es sich um eine Programmiersprache zur Transformation (und Formatierung) von XML Derivaten – in unserem Fall die HTML-Dateien – in XML Dokumente. Das XML Format eignet sich in erster Linie dafür, die informationsarmen Ausgangsdaten mit Metadaten (z.B. Attribute, Codes oder Variablen) zur systematischen Beschreibung der Strukturen und der Inhalte anzureichern. Bspw. ließe sich das Ausgangsdatum „Herbert“ mit dem Attribut „Vorname“ verknüpfen und so systematisch alle Vornamen erschließen. 
            Die Umwandlung von HTML zu XML ist die Grundlage des Mappings der einzelnen Versionen auf einander. Hierfür wurde kein existierender Standard verwendet, sondern das Format so entwickelt, dass es die Struktur der Texte möglichst treu abbildet. Dafür sollen möglichst wenige Elemente verwendet und unnötig tiefe Einbettungen vermieden werden. 
            Jeder Version wird ein Vorspann vorangestellt () der den Titel () und das Datum der jeweiligen Version () enthält. Diesem Vorfeld folgt der eigentliche Text der Verfassung (). Dieser ist zunächst in Hauptteile gegliedert (). Diese können wiederum aus Sektionen () bestehen, welche die Artikel () der Verfassung enthalten. Die Artikel setzen sich aus Sätzen () und gegebenenfalls auch aus listenartigen Aufzählungen () zusammen. Letztere bestehen aus einer Reihe von Listenelementen ( ). 
            Schwesterknoten werden mit eins anfangend durchnummeriert (n). Ein Hauptteil mit n="0" ist eine Präambel. Diese enthalten keine Artikel, sondern eine Reihe von Sätzen () und gegebenenfalls Aufzählungen. Hauptteile, Sektionen und Artikel weisen jeweils ein Element für ihre Überschriften auf. Darüberhinaus enthalten nur Paragraphenelemente () Text. Eine Validierung gegen eine DTD findet nicht statt.
            Es ergibt sich folgendes Format:
            
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
              
             
             
             
             
             
             
             
             
             
            
            An dieser Stelle der Beschreibung der Ausgangsdaten im XML Format setzen wiederum qualitative Dokumentenarbeitsschritte ein, die sich an der analytischen Strategie des Kodierens und Kategorisierens anlehnen. In diesen Vorgang fließen einerseits Kontextinformationen ein, andererseits werden während des Arbeitsschritts wichtige empirische Beobachtungen gemacht, die in Form von Kodiermemos dokumentiert werden. So können die gewonnenen Informationen zu einem späteren Zeitpunkt für die Tiefenstrukturanalyse des Materials oder die Ausdifferenzierung der Analysesoftware genutzt werden.
            
               Mapping als Strukturvergleich
               Das Mapping der Strukturelemente der im Vergleich stehenden Versionen aufeinander wird automatisiert vollzogen, indem jedes Element einer strukturellen Ebene (Hauptteil, Sektion, Artikel) mit jeder anderen entsprechenden Ebene der Vergleichsversion abgeglichen wird. Dafür verwenden wir das gängige Cosinus-Maß, das TextÄhnlichkeit durch Modellierung im hochdimensionalen Vektorraum misst. Auf diese Weise wird eine Matrix von Ähnlichkeitswerten aufgebaut. 
               Währen des Aufbaus der Matrix wird die Anzahl der Berechnungen reduziert, indem, sobald eine Ähnlichkeit vom Wert 1 zu einem Element der Vergleichsversion gefunden wird, also eine perfekte Übereinstimmung vorliegt, das Elementpaar als unveränderte Übereinstimmung abgespeichert und die Elemente aus dem weiteren Vergleich ausgeschlossen werden. 
               Liegt keine genaue Übereinstimmung vor, wird getestet, ob die beiden zu vergleichenden Texte unterschiedlicher Länge sind. Ist dies der Fall, wird ferner geprüft, ob der kürzere der beiden Texte einen Teilstring des längeren bildet. In solchen Fällen werden die Texte einander als Änderungen (Erweiterungen oder Kürzungen) zugeordnet, abgespeichert und ebenfalls aus der weiteren Berechnung ausgeschlossen.
               Schließich bleibt eine Matrix der Ähnlichkeitswerte ausschließlich der Elemente übrig, für die keine Entsprechung gefunden werden konnte. 
               Die Paare, die die höchsten Cosinusähnlichkeiten (
               
                  
               
            
            
               Entwicklungsergebnisse
               Die Software zur Verfassungsanalyse ist in der Programmiersprache 
                        Python geschrieben. Im Zuge der ersten Entwicklungsphase lassen sich rein formal die hierarchische Struktur und die jeweiligen Abschnittslängen der Dokumente vergleichen und auch quantifizieren. 
                    
               In der vorliegenden Fassung des Werkzeugs können vier verschiedene Operationen ausgeführt werden: 
               1. Vergleichen 
               Für den Vergleich wird zunächst ein Land und ein Zeitraum ausgewählt für den die Änderungen ausgegeben werden sollen. Um die Suche weiter einzuschränken, wird zunächst gezeigt, wie viele Änderungen in welchen Hauptstücken in dem angegebenen Zeitraum stattgefunden haben. Nach der Auswahl eines Hauptteils werden die gefundenen Änderungen (in Sektionen und Artikeln), Tilgungen und Hinzufügungen ausgegeben (Abbildung 2).
               
                  
               
               2. Cosinusähnlichkeiten 
               Mit dieser Funktion (Abbildung 3) lassen sich die Cosinusähnlichkeiten ganzer Versionstexte untereinander berechnen und ausgeben.
               
                  
               
               3. Wortzählungen und Wortprofile
               Der Nutzer kann sich unter Angabe der Version, die von Interesse ist, die Aufkommenshäufigkeiten von Wörtern ausgeben lassen. Zudem lassen sich die Textstellen, die das gezählte Wort enthalten, zusammen mit einer Liste der Wörter ausgeben, die häufiger als ein definierter Schwellenwert (bspw. fünf Mal) in derselben textuellen Umgebung vorkommen (Abbildung 4).
               
                  
               
               Das Beispiel zeigt die politische Kernkategorie, den Bürger, in Version 7 der irländischen Verfassung. Die textuelle Umgebung ist durch Begriffe wie Gesetz (42 mal), Gerichtshof (37 mal) und Präsident (28 mal), die Hinweise dazu liefern in welchen Sinnzusammenhängen der Bürger theamtisiert wird, gekennzeichnet. Die Begriffe Person (26 mal) und Staat (25 mal) weisen darauf hin, dass es sich beim Bürger offenbar tatsächlich um eine Kategorisierung als Person handelt, die wiederum in irgendeiner Beziehung zum Staat steht. Dieser Zusammenhang, die Beziehung von Bürger und Staat kann nun mithilfe von (historischen) Kontextrecherchen und Literatur basierten Konzepten und Theorien genauer untersucht werden. 
               Zur Erstellung der Wortprofile, d.h. für die Anreicherung der Daten mit bspw. Lemmata, POS-Taggs und Dependenzrelationen wurde die Pipeline des DARIAH-DKPro-Wrapper
                         des NLP-Toolkits DKPro Core (vgl. Eckart de Castilho und Gurevych (2014)) benutzt. Das ermöglicht die Ausgabe der syntaktischen Relationen, die das gezählte Wort mit anderen Wörtern eingeht, zusammen mit ihren Häufigkeiten (Abbildung 5).
                    
               
                  
               
               Das verwendete Beispiel, die Personenkategorie Mitglied, kommt in der gewählten Verfassungsversion 78 mal vor. Die Informationen, wovon Mitglied das Subjekt ist oder inwiefern Mitglied als Genitivattribut oder Akkusativobjekt von bestimmten Termini vorkommt können u.a. dabei helfen, die Eigenschaften dieser Kategorie oder auch die Prozesse in die diese kategorie eingebunden sein kann, genauer zu bestimmen. So kann ein Mitglied hinzugefügt oder auch ernannt werden. In weiteren Betrachtungen kann dann herausgearbeitet werden, wozu ein Mitglied ernannt oder hinzugefügt werden kann. Das Genitivattribut der Kategorie König gibt bspw. Auskunft darüber, von welcher Bezugsgruppe diese Person überhaupt der König ist. Diese automatisiert verfügbaren Informationen tragen dazu bei, die kategoriale Wissensbestände der Verfassungsstaaten historisch-vergleichend zu untersuchen.
               4. Graphische Darstellungen 
               Nutzende können sich in der aktuellen Version die Auftretensverteilungen von Wörtern in einem anzugebenden Zeitraum als Graph ausgeben lassen. Dabei werden pro eingegebenes Wort ein Balkendiagramm sowie ein Diagramm generiert, das die Kurven aller angegebenen Wörter in einem Graph zugleich darstellt (Abbildung 6). 
               
                  
               
               In dem verwendeten Beispiel werden die Verkommenshäufkeiten von drei Personenkategorien – Gott, König und Bürger – in den Verfassungen der Niederlande für den Zeitraum 1815 bis 1948 dargestellt. Dabei fällt auf, dass der Bürger im Vergleich zum König als Repräsentation des Souveräns eine deutlich untergeordnete Rolle spielt und ab 1848 bis 1948 im Prinzip nicht vorkommt. Die Verfassungen spiegeln das politische System der Monarchie und nicht die Staatsbürgergesellschaft wieder. Ab 1848 nimmt auch das Vorkommen der Kategorie Gott signifikant ab. Während 1840 Gott noch 30 Mal vorkommt, verringert sich die Häufigkeit in den darauffolgenden 100 Jahren auf durchscnittlich sieben. Anahnd dieser Ergebnisse lassen sich ganz verschiedene Interpretationen und tiefergehende Analysen anschließen, um bspw. Erkenntniss über die religiöse Semantiken staatlicher Selbstbeschreibung (Gottesbezog, Gründungsmythen, Vorstellungen der Nation usw.) oder den Wandel des politischen Gemeinwesens und politischer Zugehörigkeit (wer gehört eigentlich dazu?) zu erlangen.
               Diese Funktion wird demnächst um weitere bereichert werden, um die Potentiale von Visualisierungen als darstellende Klammer des Strukturvergleichs, der Wortsuche und der syntaktischen Wortrelationen wie auch als analytisches Werkzeug (Lupton 2014) selbst zu sondieren.
            
         
         
            4. Zusammenfassung und Ausblick
            Derzeit kann die Software alle Änderungen – unabhängig von formalen Totalrevisionen innerhalb der historischen Verfassungsentwicklungen aufzeigen. So lässt sich bspw. feststellen, welche Teile besonders häufig geändert werden oder welche Teile bis heute unangetastet geblieben sind. 
            Die Vorteile dieser Software gegenüber frei im Internet zugänglichen Versionierungstools (github, gitlab o.ä.) liegen auf der Hand: Zwar bieten solche Programme relativ einfach die Möglichkeit Textänderungen nachzuverfolgen, das gezielte Nachvollziehen von der Änderungshistorie spezifischer Textabschnitte ist ungleich schwieriger. Darüber hinaus bieten die beschrieben Funktionalitäten viel weitgehendere Auswertungsszenarien, als das reine Mapping, das durch ein Versionierungstool angeboten wird.
            Beispielsweise kann mit dem Programm untersucht werden, welche neuen (normativen) Vorgaben Einzug in die Verfassung finden. Diese Änderungen in den Zeitreihen lassen sich dann ihrerseits im Zuge der weitergehenden Untersuchung historisch kontextualisieren und bspw. mit Blick auf staatlichen Wandel oder die Institutionalisierung bzw. Legitimierung neuer Werte, Normen, staatlicher Handlungsverpflichtungen und kultureller Leitideen (Meyer et al. 2005) interpretieren. Das Programm stellt das technische Werkzeug dafür dar, beide Ebenen, Mikro- und Makroebene, gleichermaßen zu betrachten, indem die Änderungshistorie einzelner Abschnitte ins Verhältnis zur Distanzsicht auf die gesamttextlichen Änderungen vieler Verfassungen gesetzt werden können.
            Künftig sollen auch Fallvergleiche zwischen verschiedenen europäischen Staaten möglich sein, bspw. indem für frei wählbare Textbereiche die Cosinusähnlichkeit berechnet wird. Dadurch wird u.a. die Herausforderung des Vergleichs verschiedener historischer Kontexte tangiert. Wie kann ein informationstheoretisches Modell aussehen, das verschiedene Vektorräume, die definierte temporäre Sequenzen umfassen, zusammenbringt und miteinander vergleicht? Wie können Okkurrenzen kategorisiert werden, wenn diese bspw. häufiger auftreten?
            Die dargestellte Form der Dokumentenarbeit macht Verfassungen nicht nur einer breiten Öffentlichkeit und vielfältigen wissenschaftlichen Erhebungen zugänglich. Vielmehr reflektiert sie Methoden der Dokumentenanalyse, indem sie der spezifischen Dokumentengattung „Verfassung“ besondere Aufmerksamkeit schenkt. Hermeneutisch-interpretative Verfahren versuchen Kontextwissen zumindest zu Beginn der Analyse weitestgehend auszublenden, wohingegen beim skizzierten Vorgehen eben dieses Wissen über die Dokumentenart, deren struktureller Aufbau sowie etwaige kulturell-historische Besonderheiten in die Auszeichung des Textes mit Metadaten für die computerbasierte Bearbeitung einfließt. 
            Insgesamt leistet die methodische Verschränkung von historischer Wissenssoziologie und Computerlinguistik als Dokumentenarbeit und Entwicklung einer Analysesoftware einen Beitrag zur Untersuchung der Ko-Fabrikation von Sprache und Verfassungsrecht in Europa, indem über einzelene Begriffe und Begriffskombinationen spezifische Wissensbestände und Semantiken in den Blick genommen werden können. Dieses Vorgehen kann dazu beitragen, neue Textkorpora zu erschließen und weitere gesellschaftliche Wissensbestände (bspw. Bibelversionen, Dramen usw.) zu erkunden. Diese Analysen ließen sich mit anderen methodisch ähnlichen, aber gegenständlich anders ausgerichteten Untersuchungen koppeln. Bspw. könnten Verfassungen in Beziehung zu Presseartikeln und den sich darin ablesbaren Diskursen gesetzt und miteinander verglichen werden.
         
      
      
         
             Im Zuge dieses Entwicklungsschrittes wurden u.a. folgende Tagger und Parser verwendet: Open NLP Segmenter, Mate Tools POS-Tagger, Mate Tools Lemmatizer, Open NLP Chunker, Mate Tools Morphological Analyzer, Hyphenation Annotator, CoreNLP Named Entity Recognizer, Mate Tools Dependency Parser. 
         
         
            
               Bibliographie
               
                  Boli-Bennett, John (1979): 
                        The Ideology of Expanding State Authority in National Constitutions, 1870-1970, in: Meyer, John W. / Michael Thomas Hannan (eds.): National development and the world system: educational, economic and political change. Chicago: University of Chicago Press 222-237. 
               
               
                  Boli-Bennett, John / John W. Meyer (1978): The ideology of childhood and the state: Rules distinguishing children in national constitutions, 1870-1970, in: 
                        American Sociological Review 43: 797–812
                        .
               
               
                  Bubenhofer, Noah / Joachim Scharloth (2015): Themenheft „Maschinelle Textanalyse“,
                         in: Zeitschrift für germanistische Linguistik, 43.1.
               
               
                  Eckart de Castilho, Richard / Gurevych, Iryna (2014): A broad-coverage collection of portable NLP components for building shareable analysis pipelines. In: 
                        Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT (OIAF4HLT) at COLING 2014, 1-11, Dublin, Ireland.
                    
               
                  Go, Julian (2003): A Globalizing Constitutionalism? Views from the Postcolony, 1945-2000, in: 
                        International Sociology 18.1: 71-95.
                    
               
                  Gosewinkel, Dieter / Johannes Masing / Andreas Würschinger (2006): 
                        Die Verfassungen in Europa 1789-1949. München: Beck
                        .
               
               
                  Hausser, Roland (2014): 
                        Foundations of Computational Linguistics: Human-Computer Communication in Natural Language
                        . Berlin: Springer.
                    
               
                  Heintz, Bettina / Annette Schnabel (2006): Verfassungen als Spiegel globaler Normen? Eine quantitative Analyse der Gleichberechtigungsartikel in nationalen Verfassungen, in: 
                        Koelner Zeitschrift für Soziologie und Sozialpsychologie, 58.4, 685-716.
                    
               
                  Jepperson, Ronald L (1991): 
                        Institutions, Institutional Effects, and Institutionalism, in: DiMaggio, Paul / Walter W. Powell (eds.): The New Institutionalism in Organizational Analysis. Chicago: University of Chicago Press 143-163.
               
               
                  Kuckartz, Udo (2012): 
                        Qualitative Inhaltsanalyse. Methoden, Praxis, Computerunterstützung. Weinheim / Basel: Beltz.
                    
               
                  Lobin, Henning (2010): 
                        Computerlinguistik und Texttechnologie. Paderborn / München: Fink.
                    
               
                  Lupton, Deborah (2014): 
                        Digital sociology. New York: Routledge.
                    
               
                  Manning, Christopher D. / Prabhakar Raghavan / Hinrich Schütze (2008): 
                        Introduction to information retrieval. New York: Cambridge University Press.
                    
               
                  Mayntz, Renate (1997): 
                        Soziale Dynamik und politische Steuerung: theoretische und methodologische Überlegungen. Frankfurt/Main: Campus.
                    
               
                  Mayring, Philipp (2015): 
                        Qualitative Inhaltsanalyse. Grundlagen, Techniken. Weinheim / Basel: Beltz.
                    
               
                  Meyer, John W. (2005): 
                        Die Weltgesellschaft und der Nationalstaat, in: ders., Weltkultur: wie die westlichen Prinzipien die Welt durchdringen. Frankfurt/Main: Suhrkamp 85-132.
               
               
                  Mikolov, Thomas / Wen-tau Yih / Geoffrey Zweig (2013): Linguistic Regularities in Continuous Space Word Representations, in:
                         Proceedings of the HLT-NAACL conference 746-752. 
                    
               
                  Prior, Lindsay (2011): 
                        Using documents in social research. Los Angeles: Sage.
                    
               
                  Salton, Gerard / Andrew Wong / Shungshu Yang (1975): A vector-space model for information retrieval, in: 
                        Journal of the American Society for Information Science 18: 613-620.
                    
               
                  Schimank, Uwe (1999): 
                        Funktionale Differenzierung und Systemintegration der modernen Gesellschaft: Soziale Integration. Opladen: Westdeutscher Verlag.
                    
               
                  Strauss, Anselm L. / Juliet M. Corbin (1996): 
                        Grounded theory: Grundlagen qualitativer Sozialforschung. Weinheim: Beltz.
                    
               
                  Thelen, Kathleen (1999): Historical Institutionalism in Comparative Politics, in: 
                        Annual Review of Political Science 2.1: 369-404.
                    
               
                  Thelen, Kathleen (2002): 
                        The explanatory power of historical institutionalism, in: Mayntz, Renate (eds.): Akteure-Mechanismen–Modelle. Zur Theoriefähigkeit makro-sozialer Analysen. Frankfurt / New York: Campus) 91-107.
               
               
                  Vorländer, Hans (2007): Europas multiple Konstitutionalismen, in: 
                        Zeitschrift für Staats- und Europawissenschaften 5.2: 160-180.
                    
               
                  Weber, Max (1972): 
                        Wirtschaft und Gesellschaft: Grundriss der verstehenden Soziologie. Tübingen: Mohr.
                    
               
                  „XML Schema”. World Wide Web Consortium (W3C)
                  http://www.w3c.org/XML/Schema, 
                        [letzter Zugriff] 10.09.2017).
                    
               
                  „XQuery 1.0: An XML Query Language“. World Wide Web Consortium (W3C) 
                  http://www.w3.org/TR/xquery
                         [letzter Zugriff 10.09.17]. 
                    
               
                  „XSL Transformation Version 1.0“. World Wide Web Consortium (W3C)
                  http://www.w3c.org/TR/xslt
                         [letzter Zugriff 10.09.17].
                    
            
         
      
   


10756	2018	
      
         
            
               Introduction
                
            
               Detecting direct speech in fiction allows gaining insight into an important element of its narrative structure. In literary studies, there are assumptions on the factors influencing the distribution of direct speech, like genre, period and aesthetic complexity.
                
            This paper aims to provide a detailed analysis of the use of direct speech across different time periods and domains. To create a reliable database for these analyses, we need to measure the usage of direct speech in a large and representative corpus. This task is more challenging than it may sound: While, nowadays, direct speech is often marked very explicitly by the use of quotes, this has not always been consistently the case. Many historical novels are not available in a well-edited form, meaning that there may be inconsistent use of quotation, or no quotation at all (Brunner, 2013). In this case, a more robust method for detecting direct speech is necessary.
            Our first contribution is therefore a deep learning-based method to detect direct speech using large amounts of rule-based, but slightly flawed, labelled data extracted from raw text.  This has multiple advantages over the use of manually annotated training data: First, manually annotating large amounts of text is very time-intensive and therefore costly. Furthermore, annotations for one type of texts may not be transferable to other types, leading to the necessity of new annotated data for new corpora. Being able to learn from the already existing weakly labelled data is therefore desirable, as this data can automatically be extracted for a new corpus.
            Our second contribution is the application of this approach on curated texts to gain insight in trends of direct speech distribution. On one hand we try to look for development of direct speech over time, analysing a large dataset of novels from the nineteenth century, on the other hand we focus on differences in genre comparing contemporary high and low brow literature.
         
         
            Related Work and Task Description
            
               There have been several previous approaches to direct speech detection applying machine learning methods.
                
            For example, Brunner (2013) tests rule-based and machine learning driven classification, as well as combinations of both, on German novels. She recommends using a pure machine learning approach (Random Forest), reaching an F1 score of 0.87.
            Scheible et al. (2016) employ a simple greedy algorithm and a semi-Markov model, showing that the latter outperforms the previous state-of-the-art by achieving a precision of 0.88.
            Although the results seem quite satisfying, these systems require a relatively large amount of labelled data for training. As stated above, this is problematic because of the need for expensive annotation and lack of transferability to other domains. Thus, our goal in this paper differs from that in previous work. We do not aim to set a new state-of-the-art in direct speech detection, but instead: 
            a) present a method that can leverage large amounts of weakly labelled data extracted from raw text, and 
            b) use this model for the analysis of different distributions of direct speech across genres or time-periods. 
            To the best of our knowledge, the second task has never been done on a large collection of texts.
         
         
            Corpus and Resources
            
               The following experiments are based on three German corpora. The first one is a large corpus containing 4600+ public domain novels including texts from the TextGrid digital libraryand Project Gutenberg. We will refer to this as the Corpus 
                    Public Domain, PD. The second one contains 800+ texts of current popular genres like romance, crime or science-fiction (Corpus 
                    Low Brow, LB). Finally, we use a corpus with 200 novels nominated for the 
                    German Book Prize or the 
                    Georg Büchner Prize (Corpus 
                    High Brow, HB).
                
            In order to train and evaluate our classifiers, we need to obtain labels specifying which parts of the texts contain direct speeches. To this end, we chose two strategies:
            For training our classifiers, we decided to extract weak labels using a simple rule based on quotation, implying everything written between quotation marks is direct speech. To yield high accuracy for this approach, it is necessary to use a well-edited collection of texts. Our PD corpus contains such a subset, which we refer to as our 
                    Kerncorpus. This 
                    Kerncorpus consists of 250 high and middle brow texts (those from the TextGrid digital library), has been manually edited and is assumed to have a mostly consistent use of quotation. 
                
            Using our quotation rule on the 
                    Kerncorpus resulted in a dataset where about 36% of tokens were marked as direct speech. In order to assess the quality of these weak labels, we gave 500 of the sentences to domain experts for manual correction. We found that there was an error-rate of about 3% in those sentences, mostly caused by nested direct speech or inscriptions being enclosed by quotation marks.
                
            For further evaluation, we chose to annotate a smaller subset of the corpus 
                    LB by hand. We selected 50 snippets from texts of low brow literature. This dataset, referred to as 
                    ALB, is relatively skewed towards text outside direct speech, with only about 18% of tokens in a direct speech.
                
         
         
            Experiments
            
               The following experiments use both labelled subsets described above, the large 
                    Kerncorpus
               
                    and the smaller 
                    ALB. For all experiments, quotation marks are removed from the texts. This is done to avoid training models that rely only on the formal style of qualifying direct speech, but also consider implicit signs like the use of first person verbs or speech words.
                
            We conducted experiments on two different levels, starting with a sentence classification task, which is then refined to detect direct speech on word-level.
            
               Sentence-Level Classification
               In our first classification task, documents are split into sentences and vectorised by storing each sentence in a bag-of-words representation. To create a baseline for measuring the advantage using deep learning for direct speech recognition, we compared the performance of traditional machine learning algorithms on our labelled datasets. Training and testing some of the most common machine learning classifiers to detect sentences containing at least one word of direct speech leads to an accuracy of 
                        0.85 using Logistic Regression; for more results see Table 1.
                    
               
                  Using the same setting and replacing machine learning with a combination of recurrent and convolutional neural networks (see Chollet 2017 and Goodfellow 2017) ended up with an accuracy of 
                        0.84.
                    
               
                  
                     
                  
               
               Since we noticed that three of our classifiers all ended up with about the same score, we decided to give the task to two human annotators to establish an upper bound. We selected 250 sentences for manual annotation and again removed all quotation marks. Both annotators ended up with an accuracy comparable to that of the best machine learning methods, 84% and 82.8% respectively. From this result we concluded that it is not expedient to further optimise the sentence classification task, as we had already reached human-level accuracy.
            
            
               Word-Level Classification
               Because of the results from the previous section, we decided to modify our task to a word-level prediction, which enables us to include more context by ignoring sentence boundaries and at the same time make more fine-grained predictions. In this second classification task, each word is to be classified separately as inside or outside a direct speech. As baseline for this task, we trained a Linear Chain Conditional Random Field (CRF) that was only given the word itself and its part-of-speech tag. This CRF stagnated at a comparably low accuracy of 0
                        .71 using cross-validation on the 
                        Kerncorpus. 
                    
               Since our goal was to provide the classifier with more context, we chose to use an architecture based on recurrent neural networks, which are able to deal with relatively large contexts. Our assumption here is that, for a good classification, we need context from both before and after the target word itself, as markers for direct speech can be found at the beginning or the end of the direct speech. We thus designed a two-branch network, visualised in Figure 1. This network receives as input a text-segment, specifically the target word in its context. The words of the input are then passed through an embedding layer and split into two parts, where the first part contains the context up to the target word and the second part contains the context following the target word. The target word itself is contained in both parts. Each part is passed through three separate LSTM-layers. In the future-branch, the context is passed through the layers in reverse, so that the target word is the last word to be read in both branches. The LSTM-layers in the past-branch are stateful and can therefore theoretically retain the entire context of the novel up to the target word. The outputs of the final LSTM-layer of both branches are concatenated. The final prediction is made based on this concatenation by a fully connected layer.
               In our best setup, we used 60 words before and after the target word as context. 
               Training on one half of the 
                        Kerncorpus and evaluating on the other half, this setup yielded an accuracy of 
                        0.83. Training on the full 
                        Kerncorpus and evaluating on the manually annotated 
                        ALB reached an even better accuracy of 
                        0.90.
                        
               
               
                  
                     
                     
                        Figure 1: Architecture of recurrent network to detect direct speech.
                            
                  
               
            
         
         
            Distribution of direct speech
            
               In the following experiments, we used the model based on the architecture described above. We trained this model on the 
                    Kerncorpus and used it to detect direct speech in the complete corpora 
                    PD, 
                    LB and 
                    HB. Here, we describe our findings on these corpora.
                
            
               Direct speech in 19th Century Fiction
               
                  
                  
                     Figure 2: Ratio of direct speech in German novels from 1800 – 1900.
                                    
               
               
                  Figure 2 shows the ratio of direct speech in German novels from 1800 till 1900 based on the texts from Corpus 
                        PD. The regression line indicates a decline of direct speech over time; at the same time, we can observe a decrease of variance. The strong variations between certain years, especially in the early 19th century, are caused by low numbers of provided texts (see Fig. 3). For instance, the peak in 1805 can be explained by the first publication of Denis Diderots ”Herrn Rameaus Neffe”, a philosophical dialogue-based novel.
                    
               
                  
                  
                     Figure 3: Number of provided novels per year.
                                    
               
            
            
               Distribution of direct speech in low and high brow literature
               
                  
                  
                     Figure 4: Ratio of direct speech in German low and high brow novels after 1945.
                                    
               
               
                  There is an assumption in literary studies that a huge amount of direct speech is an indicator of low brow fiction. Figure 4 shows the ratio of direct speech between Corpus 
                        LB and 
                        HB. While the mean usage of direct speech is nearly equal in both groups, the high brow literature is far more variable.
                    
               
                        This finding is contrary to the assumption mentioned above. We propose that, while there is no clear difference in the average use of direct speech between high and low brow literature, authors in high brow literature are far more flexible in choosing how much direct speech they use in their novels. Low brow literature, on the other hand, is expected to have a rather constant amount of dialogue.
                    
            
         
         
            Conclusion and Future Work
            
               In this paper, we introduced a neural network architecture that is able to learn the classification of direct speech by training on weakly labelled data. This network works purely on the raw text of a novel by taking into account a relatively large context. We also demonstrate that training on weakly labelled data leads to satisfying results.
                
            While an accuracy of 0.9 is remarkable, there is still need for optimisation. Recent developments in the performance of neural networks by adding an attention mechanism (see Rush 2015) could improve the results.
            We used our neural network to analyse the distribution of direct speech over time and genres. Besides algorithmic refinements, there is a lot of potential in adding more text to our corpus and refining metadata to allow more sophisticated research questions like differences between or development of direct speech in certain genres.
         
      
      
         
            
               
            
            
               https://gutenberg.spiegel.de
            
            
                We cannot use the remaining texts for either training or evaluation, as we do not have any reliable source of labels for these texts.
            
         
         
            
               Bibliography
               
                  Brunner, Annelen (2013): “Automatic recognition of speech, thought, and writing representation in German narrative texts”, in 
                        Literary and Linguistic Computing. Vol. 28 (2013).
               
               
                  Chollet, Francois (2017): “Deep Learning with Python”. Manning Publications. New York. (Preprint: https://www.manning.com/books/deep-learning-with-python)
                    
               
                  Goodfellow, Ian /  Bengio Yoshua / Courville,  Aaron (2016): “Deep Learning”. MIT Press. (URL: 
                        
                     http://www.deeplearningbook.org
                  )
                    
               
                  Rush, Alexander M. / Chopra, Sumit / Weston, Jason (2015): “A Neural Attention Model for Abstractive Sentence Summarization”. 
                        arXiv preprint arXiv:1509.00685.
               
               
                  Scheible, C., Klinger, R. & Padó, S. (2016): “Model Architectures for Quotation Detection”, in 
                        Proceedings of ACL (p./pp. 1736–1745).
               
            
         
      
   


10787	2018	
      
         
            LDA (Latent Dirichlet Allocation) Topic Modeling ist ein computergestütztes Verfahren zur semantischen Analyse digitaler Textsammlungen. Hierbei werden mit Hilfe eines probabilistischen Verfahrens aus Texten eine Reihe sogenannter “Topics” generiert: Gruppen semantisch ähnlicher Begriffe, die über mehrere Texte gemeinsam auftreten und im Modell als Wahrscheinlichkeitsverteilungen über die Gesamtheit des analysierten Vokabulars repräsentiert werden. Das heißt, daß zum Beispiel in einem Topic zum Thema Seefahrt nautische Begriffe besonders hohe Wahrscheinlichkeiten haben (Blei 2012, Steyvers und Griffiths 2006).
            
         In den letzten Jahren ist das Interesse an LDA als Verfahren für die Analyse literarischer Textcorpora auf Seiten der digitalen Geisteswissenschaften stark gestiegen. Im Kontrast zu diesem gesteigerten Interesse ist die Anwendung der Methode allerdings nicht wesentlich leichter geworden. Gängige Implementierungen des LDA-Algorithmus werden entweder über ein kommandozeilenbasiertes Java-Programm (MALLET von McCallum 2002) oder über Skripte in der Programmiersprache Python (Gensim von Rehurek und Sojka 2010) angesprochen. Die Aufbereitung der Daten vor dem Topic Modeling, das sog. “Preprocessing” und die Analyse der Ergebnisse hinterher geschieht dann zumindest in Teilen häufig unter Verwendung weiterer Programme bzw. Arbeitsumgebungen. Alles in allem erfordert die Durchführung einer LDA-basierten Inhaltsanalyse damit zur Zeit relativ umfangreiche technische Kenntnisse.
         Um den Zugang zu dieser Methode zu erleichtern entwickeln wir im Rahmen von DARIAH-DE (https://de.dariah.eu/) zur Zeit eine ausführlich dokumentierte Python-Programmbibliothek, die es ermöglichen soll, den gesamten Arbeitsprozess einer LDA-basierten Analyse in einer einzigen Umgebung durchzuführen (). Neben der Schaffung integrierter, flexibler Arbeitsabläufe, die vollständig in einer Programmiersprache und Umgebung stattfinden können,  wollen wir auch Forscherinnen und Forschern ohne vorherige Programmierkenntnisse eine Möglichkeit zu bieten, Topic Modeling als Verfahren kennen zu lernen und an eigenen Daten auszuprobieren.
            
         Um einen leichtgewichtigen Einstieg in diese Thematik zu bieten haben wir auf Basis unserer Programmbibliothek, der Python-nativen LDA-Implementierung von Allan Riddell (https://pypi.python.org/pypi/lda) und dem Python-Microframework “Flask” () einen sogenannten GUI-Demonstrator entwickelt (Abb. 1). Dabei handelt es sich um eine browserbasierte graphische Benutzeroberfläche für die DARIAH-Topics Bibliothek, mit der sich ein basaler Topic-Modeling Analysevorgang lokal, mit eigenen Daten, aber eben ohne jegliche Programmierkenntnisse durchführen lässt.
            
         Der GUI-Demonstrator übernimmt und erklärt hierbei exemplarisch alle Arbeitsschritte einer einfachen Analyse. Zunächst werden Textdateien über ein Auswahlmenü eingelesen und tokenisiert. Nutzerinnen und Nutzer können zur Reduktion des Vokabulars auf die Funktionswörter vorgeben, wie viele der häufigsten Wörter aus den Texten entfernt werden sollen, oder alternativ über ein weiteres Auswahlmenü eine externe Stopwortliste einbinden. Die Anzahl der zu berechnenden Topics und die Zahl der Iterationen, über die die Berechnung durchgeführt werden soll, ein Faktor, der die Qualität der Ergebnisse entscheidend beeinflusst, können ebenfalls über das Interface gesteuert werden. In der derzeitigen Form generiert das Programm als Output eine Tabelle mit den zehn am stärksten gewichteten Wörtern in jedem Topic, sowie ein Heatmap als Übersicht über die Verteilung der Topics über die Texte.
         Im Fokus der gegenwärtigen Weiterentwicklung steht die Gestaltung interaktiver Outputs mit Hilfe von Bokeh (), die einen flexibleren Zugriff auf eine größere Zahl von Aspekten der Modellierungsergebnisse ermöglichen sollen.
            
         Das Ziel dieser Entwicklung bleibt aber in erster Linie ein didaktisches: Der GUI-Demonstrator führt die  grundsätzlichen Möglichkeiten der Methode vor und informiert gleichzeitig über die Abläufe im Hintergrund, so dass der Schritt hin zur Verwendung der gleichen Funktionalitäten in einem vorbereiteten Notebook mit interaktiven Codeblöcken, das schnell an die spezifischen Bedürfnisse eine bestimmten Forschungsfrage angepasst werden kann, nur noch klein ist.
         
            
               
               Abbildung 1.: Screenshot
            
         
      
      
         
            
               Bibliographie
               
                  
                  Blei, David M.
                        (2012): „Probabilistic Topic Models“, in 
                        Communication of the ACM
                        55, Nr. 4 (2012): 77–84. doi:10.1145/2133806.2133826.
                    
               
                  McCallum, Andrew K.
                        (2002): MALLET : A Machine Learning for Language Toolkit.
                        
                     http://mallet.cs.umass.edu
                  .
                    
               
                  Rehurek, Radim/ Sojka, Petr
                        (2010): "Software framework for topic modelling with large corpora."
                        In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks.
                    
               
                  Steyvers, Mark/ Griffiths, Tom
                        (2006): „Probabilistic Topic Models“, in
                        Latent Semantic Analysis: A Road to Meaning, herausgegeben von T. Landauer, D. McNamara, S. Dennis, und W. Kintsch. Laurence Erlbaum.
                    
            
         
      
   


10795	2018	
      
         
            Einleitung
            Das Ziel dieses Tutorials ist es, den Teilnehmerinnen und Teilnehmern konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten. Es werden dabei keinerlei Vorkenntnisse zu maschinellem Lernen oder Programmierkenntnisse vorausgesetzt.
            Es gibt keinen Grund, den Ergebnissen von maschinellen Lernverfahren im Allgemeinen und NLP-Tools im Besonderen blind zu vertrauen. Durch die konkreten Einblicke in den "Maschinenraum" von maschinellen Lernverfahren wird den Teilnehmenden ermöglicht, das Potenzial und die Grenzen statistischer Textanalysewerkzeuge realistischer einzuschätzen. Mittelfristig hoffen wir dadurch, den immer wieder auftretenden Frustrationen beim Einsatz automatischer Verfahren für die Textanalyse und deren teilweise wenig zufriedenstellender Ergebnis-Daten zu begegnen, aber auch die Nutzung und Interpretation der Ergebnisse von maschinellen Lernverfahren (d.h. in erster Linie von automatisch erzeugten Annotationen) zu fördern. Zu deren adäquater Nutzung, etwa in hermeneutischen Interpretationsschritten, ist der Einblick in die Funktionsweise der maschinellen Methoden unerlässlich. Insbesondere ist die Art und Herkunft der Trainingsdaten für die Qualität der maschinell produzierten Daten von Bedeutung, wie wir im Tutorial deutlich machen werden.
            Neben einem Python-Programm für die automatische Annotierung von Entitätenreferenzen, mit und an dem während des Tutorials gearbeitet werden wird, stellen wir ein heterogenes, manuell annotiertes Korpus sowie die Routinen zur Evaluation und zum Vergleich von Annotationen zu Verfügung. Das Korpus enthält Entitätenreferenzen, die im "Center for Reflected Text Analytics" (CRETA)
                     in den letzten zwei Jahren annotiert wurden, und deckt Texte verschiedener Disziplinen und Sprachstufen ab.
                
         
         
            Entitätenreferenzen
            Als empirisches Phänomen befassen wir uns mit dem Konzept der Entität und ihrer Referenz. Das Konzept steht für verschiedene linguistische und semantische Kategorien, die im Rahmen der Digital Humanities von Interesse sind. Es ist bewusst weit gefasst und damit anschlussfähig für verschiedene Forschungsfragen aus den geistes- und sozialwissenschaftlichen Disziplinen. Auf diese Weise können unterschiedliche Perspektiven auf Entitäten berücksichtigt werden. Insgesamt werden in den ausgewählten Texten fünf verschiedene Entitätenklassen betrachtet: PER (Personen/Figuren), LOC (Orte), ORG (Organisationen), EVT (Ereignisse) und WRK (Werke).
            Unter Entitätenreferenzen verstehen wir Ausdrücke, die auf eine Entität in der realen oder fiktiven Welt referieren. Das sind zum einen Eigennamen (Named Entities, z.B. "Peter"), zum anderen Gattungsnamen (z.B. "der Bauer"), sofern diese sich auf eine konkrete Instanz der Gattung beziehen. Dabei wird als Referenzausdruck immer die maximale Nominalphrase (inkl. Artikel, Attribut) annotiert. Pronominale Entitätenreferenzen werden hingegen nicht annotiert.
            In 
                    literarischen Texten sind vor allem Figuren und Räume als grundlegende Kategorien der erzählten Welt von Interesse. Über die Annotation von Figurenreferenzen können u.a. Figurenkonstellationen und -relationen betrachtbar gemacht sowie Fragen zur Figurencharakterisierung oder Handlungsstruktur angeschlossen werden. Spätestens seit dem 
                    spatial turn rückt auch der Raum als relevante Entität der erzählten Welt in den Fokus. Als "semantischer Raum" (Lotmann, 1972) übernimmt er eine strukturierende Funktion und steht in Wechselwirkung mit Aspekten der Figur.
                
            In den 
                    Sozialwissenschaften sind politische Parteien und internationale Organisationen seit jeher zentrale Analyseobjekte der empirischen Sozialforschung. Die Annotation der Entitäten der Klassen ORG, PER und LOC in größeren Textkorpora ermöglicht vielfältige Anschlussuntersuchungen, unter anderem zur Sichtbarkeit oder Bewertung bestimmter Instanzen, beispielsweise der Europäischen Union.
                
         
         
            Textkorpus
            Die Grundlage für (überwachte) maschinelle Lernverfahren bilden Annotationen. Um die Annotierung von Entitätenreferenzen automatisieren zu können, bedarf es Textdaten, die die Vielfalt des Entitätenkonzepts abdecken. Bei diesem Tutorial werden wir auf Annotationen zurückgreifen, die im Rahmen von CRETA an der Universität Stuttgart entstanden sind (cf. Blessing et al., 2017; Reiter et al., 2017a). Das Korpus enthält literarische Texte aus zwei Sprachstufen des Deutschen (Neuhochdeutsch und Mittelhochdeutsch) sowie ein sozialwissenschaftliches Teilkorpus.
                    
            
            Der 
                    Parzival
                Wolframs von Eschenbach ist ein arthurischer Gralroman in mittelhochdeutscher Sprache, entstanden zwischen 1200 und 1210. Der 
                    Parzival zeichnet sich u.a. durch sein enormes Figureninventar und seine komplexen genealogischen Strukturen aus, wodurch er für Analysen zu Figurenrelationen von besonderem Interesse ist. Der Text ist in 16 Bücher unterteilt und umfasst knapp 25.0000 Verse.
                
            
               Johann Wolfgang von Goethes 
               Die Leiden des jungen Werthers ist ein Briefroman aus dem Jahr 1774. Unsere Annotationen sind an einer überarbeiteten Fassung von 1787 vorgenommen und umfassen die einleitenden Worte des fiktiven Herausgebers sowie die ersten Briefe von Werther an seinen Freund Wilhelm.
                
            Das 
                    Plenardebattenkorpus des deutschen Bundestages besteht aus den von Stenografinnen und Stenografen protokollierten Plenardebatten des Bundestages und umfasst 1.226 Sitzungen zwischen 1996 und 2015.
                     Unsere Annotationen beschränken sich auf Auszüge aus insgesamt vier Plenarprotokollen, die inhaltlich Debatten über die Europäische Union behandeln. Hierbei wurde pro Protokoll jeweils die gesamte Rede eines Politikers bzw. einer Politikerin annotiert.
                
         
         
            Ablauf
            Der Ablauf des Tutorials orientiert sich an sog. 
                    shared tasks aus der Computerlinguistik, wobei der Aspekt des Wettbewerbs im Tutorial vor allem spielerischen Charakter hat. Bei einem traditionellen 
                    shared task arbeiten die teilnehmenden Teams, oft auf Basis gleicher Daten, an Lösungen für eine einzelne gestellte Aufgabe. Solch eine definierte Aufgabe kann z.B. 
                    part of speech-tagging sein. Durch eine zeitgleiche Evaluation auf demselben Goldstandard können die entwickelten Systeme direkt verglichen werden. In unserem Tutorial setzen wir dieses Konzept live und vor Ort um.
                
            Zunächst diskutieren wir kurz die zugrundeliegenden Texte und deren Annotierung. Annotationsrichtlinien werden den Teilnehmerinnen und Teilnehmern im Vorfeld zur Verfügung gestellt. Im Rahmen der Einführung wird auch auf die konkrete Organisation der Annotationsarbeit eingegangen, so dass das Tutorial als Blaupause für zukünftige Tätigkeiten der Teilnehmenden in diesem und ähnlichen Arbeitsfeldern dienen kann.
            Die Teilnehmerinnen und Teilnehmer versuchen selbständig und unabhängig voneinander, eine Kombination aus maschinellen Lernverfahren, Merkmalsmenge und Parametersetzungen zu finden, die auf einem neuen, vom automatischen Lernverfahren ungesehenen Datensatz zu den Ergebnissen führt, die dem Goldstandard der manuellen Annotation am Ähnlichsten sind. Das bedeutet konkret, dass der Einfluss von berücksichtigten Features (z.B. Groß- und Kleinschreibung oder Wortlänge) auf die Erkennung von Entitätenreferenzen empirisch getestet werden kann. Dabei sind Intuitionen über die Daten und das annotierte Phänomen hilfreich, da simplem Durchprobieren aller möglichen Kombinationen (``brute force'') zeitlich Grenzen gesetzt sind.
            Wir verzichten bewusst auf eine graphische Benutzerschnittstelle (cf. Reiter et al., 2017b) -- stattdessen editieren die Teilnehmerinnen und Teilnehmer das (Python)-Programm direkt, nach einer Einführung und unter Anleitung. Vorkenntnisse in Python sind dabei nicht nötig: Das von uns zur Verfügung gestellte Programm ist so aufgebaut, dass auch Python-Neulinge relativ schnell die zu bearbeitenden Teile davon verstehen und damit experimentieren können. Wer bereits Erfahrung im Python-Programmieren hat, kann fortgeschrittene Funktionalitäten des Programms verwenden.
            Wie am Ende jedes maschinellen Lernprozesses wird auch bei uns abschließend eine Evaluation der automatisch generierten Annotationen durchgeführt. Hierfür werden den Teilnehmerinnen und Teilnehmern nach Ablauf einer begrenzten Zeit des Experimentierens und Testens (etwa 60 Minuten) die finalen, vorher unbekannten Testdaten zur Verfügung gestellt. Auf diese Daten werden die erstellten Modelle angewendet, um automatisch Annotationen zu erzeugen. Diese wiederum werden dann mit dem Goldstandard verglichen, wobei die verschiedenen Entitätenklassen sowie Teilkorpora getrennt evaluiert werden. Auch das Programm zur Evaluation stellen wir bereit.
         
         
            Lernziele
            Am hier verwendeten Beispiel der automatischen Annotation von Entitätenreferenzen demonstrieren wir, welche Schritte für die Automatisierung einer Textanalyseaufgabe mittels maschinellen Lernverfahren nötig sind und wie diese konkret implementiert werden können. Die Teilnehmerinnen und Teilnehmer bekommen einen zusammenhängenden Überblick von der manuellen Annotation ausgewählter Texte über die Feinjustierung der Lernverfahren bis zur Evaluation der Ergebnisse. Die vorgestellte Vorgehensweise für den gesamten Ablauf ist grundsätzlich auf ähnliche Projekte übertragbar.
            Das Tutorial schärft dabei das Verständnis für den Zusammenhang zwischen untersuchtem Konzept und den dafür relevanten Features, die in ein statistisches Lernverfahren einfließen. Durch Einblick in die technische Umsetzung bekommen die Teilnehmerinnen und Teilnehmer ein Verständnis für die Grenzen und Möglichkeiten der Automatisierung, das sie dazu befähigt, zum einen das Potenzial solcher Verfahren für eigene Vorhaben realistisch(er) einschätzen zu können, zum anderen aber auch Ergebnisse, die auf Basis solcher Verfahren erzielt wurden, angemessen hinterfragen und deuten zu können.
            
               Zeitplan
            
            
               Im Vorfeld der Veranstaltung: Installationsanweisungen und Online-Support
            
            
               Dauer in Minuten (ca.)
            
            
               10 Lecture
                        
                     Intro & Ablauf
                  
               
               15 Hands-On
                        
                     Test der Installation bei allen
                  
               
               50 Lecture
                        
                     Einführung in Korpus und Annotationen
                     Grundlagen maschinellen Lernens
                     Überblick über das Skript (where can you edit what?)
                                
                           Grundlagen Python Syntax
                           Bereitgestellte Features
                        
                     
                  
               
               15 Hands-On
                        
                     Erste Schritte
                  
               
               30 Kaffeepause
               60 Hands-On
                        
                     Hack
                  
               
               30 Evaluation & Preisverleihung
            
         
         
            Beitragende (Kontaktdaten und Forschungsinteressen)
            Der Workshop wird ausgerichtet von Mitarbeiterinnen und Mitarbeitern des "Center for Reflected Text Analytics" (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine "black box" sein, sondern auch für Nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird.
            
               Nils Reiter
               
                  nils.reiter@ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
                        Pfaffenwaldring 5b
                        70569 Stuttgart
                    
               Die Forschungsinteressen von Nils Reiter liegen generell in der Anwendung computerlinguistischer Methoden auf Fragen aus den Geistes- und Sozialwissenschaften. Insbesondere die Operationalisierung literarischer Forschungsfragen und die adäquate Interpretation von Ergebnissen ist dabei ein Schwerpunkt, neben der regelgeleiteten Annotation und damit zusammenhängenden Fragen.
            
            
               Nora Ketschik
               
                  nora.ketschik@ilw.uni-stuttgart.de
                  Institut für Literaturwissenschaft
                        Keplerstraße 17
                        70174 Stuttgart
                    
               Nora Ketschik ist Promotionsstudentin in der Abteilung für Germanistische Mediävistik. Im Rahmen des CRETA-Projekts nimmt sie Analysen narratologischer Kategorien (u.a. Figur, Raum) an ausgewählten mittelhochdeutschen Romanen vor und setzt sich dabei mit der Verwendung computergestützter Methoden für literaturwissenschaftliche Analysezwecke auseinander.
            
            
               Gerhard Kremer
               
                  gerhard.kremer@ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
                        Pfaffenwaldring 5b
                        70569 Stuttgart
                    
               Der Interessenschwerpunkt Gerhard Kremers ist der reflektierte Einsatz von Werkzeugen der Computerlinguistik für geistes- und sozialwissenschaftliche Fragestellungen. Damit zusammenhängend gehören die Entwicklung übertragbarer Arbeitsmethoden und die angepasste, nutzerfreundliche Bedienbarkeit automatischer linguistischer Analysetools zu seinen Forschungsthemen.
            
            
               Sarah Schulz
               
                  sarah.schulz@ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
                        Pfaffenwaldring 5b
                        70569 Stuttgart
                    
               Sarah Schulz beschäftigt sich überwiegend mit der automatischen Verarbeitung von Texten, die syntaktischen oder lexikalischen Eigenschaften aufweisen und damit vom 
                        Standard abweichen. Sie hat einen Hintergrund in sowohl Computerlinguistik als auch Theater-und Medienwissenschaften und Germanistik.
                    
            
         
         
            Zahl der möglichen Teilnehmerinnen und Teilnehmer
            Zwischen 15 und 25.
         
         
            Benötigte technische Ausstattung
            Es wird außer einem Beamer keine besondere technische Ausstattung benötigt. Es sollte sich um einen Raum handeln, in dem es möglich ist, den Teilnehmenden über die Schulter zu blicken und durch die Reihen zu gehen.
         
      
      
         
            
               www.creta.uni-stuttgart.de
            
             Aus urheberrechtlichen Gründen wird das Tutorial ohne das Teilkorpus zu Adornos ästhetischer Theorie stattfinden, das in den Publikationen erwähnt wird.
             Die Texte wurden im Rahmen des PolMine-Projekts verfügbar gemacht: http://polmine.sowi.uni-due.de/polmine/
         
         
            
               Bibliographie
               
                  Kuhn, Jonas / Reiter, Nils (2015): "A Plea for a Method-Driven Agenda in the Digital Humanities" in: 
                        Digital Humanities 2015: Conference Abstracts, Sydney.
                    
               
                  Reiter, Nils / Blessing, Andre / Echelmeyer, Nora / Koch, Steffen / Kremer, Gerhard / Murr, Sandra / Overbeck, Maximilian / Pichler, Axel (2017a): "CUTE: CRETA Unshared Task zu Entitätenreferenzen" in 
                        Konferenzabstracts DHd2017, Bern.
                    
               
                  Reiter, Nils / Kuhn, Jonas / Willand, Marcus (2017b): "To GUI or not to GUI?" in 
                        Proceedings of INFORMATIK 2017, Chemnitz.
                    
               
                  Blessing, Andre / Echelmeyer, Nora / John, Markus / Reiter, Nils (2017): "An end-to-end environment for research question-driven entity extraction and network analysis" in 
                        Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, Vancouver.
                    
               
                  Lotman, Juri (1972): 
                        Die Struktur literarischer Texte, München.
                    
            
         
      
   


10807	2018	
      
         
            Sentiment Analyse und Dramenanalyse
            Sentiment Analyse (SA) beschreibt eine Reihe von computergestützten Methoden zur Prädiktion der Polarität eines Texts, versucht also vereinfacht gesagt automatisiert herauszufinden, ob ein Text ein positives oder negatives Gefühl ausdrückt (Liu 2016). Darüber hinaus werden teilweise auch komplexere emotionale Kategorien (wie z.B. Zorn und Freude) betrachtet (Mohammad & Turney 2010). Zentrale Anwendungsfelder der SA sind bislang vor allem die Analyse von Online-Reviews (McGlohan, Glance & Reiter 2010) und Social Media-Daten (Kouloumpis, Wilson & Moore 2011). 
            Zur Analyse von literarischen Texten mittels SA-Techniken finden sich bislang nur wenige Studien, z.B. zu Märchen (Alm, Roth & Sproat 2005) und Romanen (Kakkonen & Kakkonen 2011; Elsner 2012; Jannidis et al. 2016). Auf größeren Textkorpora wurde getestet, inwiefern SA-Werte eines Textes und Emotionskurven von Texten zur Genreklassifikation verwendet werden können (Kim, Padó & Klinger 2017) und wie begriffsgeschichtliche Bedeutungsverschiebungen in literarischen Texten mithilfe von erweiterten SA-Methoden erforscht werden können (Buechel, Hellrich & Hahn 2017). In Dramentexten hat man bisher die Verteilung von emotionalen Kategorien (Mohammad 2011) oder die Entwicklung von Figurenbeziehungen (Nalisnick & Baird 2013) in Shakespeare-Dramen untersucht. Auch der vorliegende Beitrag beschäftigt sich mit dem Einsatz von SA im Bereich der Dramenanalyse. Es werden erstmals systematisch verschiedene Methoden der SA für Dramen getestet und evaluiert. Zudem wird exploriert, inwiefern bisher in der Literaturwissenschaft erforschte Aspekte von Dramen mithilfe der SA erfasst werden und inwiefern die SA auch für die Gewinnung neuer literaturwissenschaftlicher Erkenntnisse eingesetzt werden kann.
            Das im Rahmen dieser Studie verwendete Lessing-Korpus umfasst ein mit Strukturinformationen annotiertes Dramenkorpus mit 11 Dramen, bestehend aus insgesamt 8224 Einzelrepliken. Sämtliche Dramen wurden über die Plattform 
                    TextGrid
                bezogen, so dass alle im Rahmen dieses Beitrags entwickelten Tools auch auf andere 
                    TextGrid-Dramen anwendbar sind. Mit dem am besten evaluierten SA-Verfahren wurde eine webbasierte Anwendung zur Analyse und Visualisierung von Sentiment-Verteilungen und -Verläufen implementiert.
                
         
         
            Evaluation unterschiedlicher SA-Verfahren
            
               Lexikonsbasierte SA
               Innerhalb der SA unterscheidet man zwei wesentliche Ansätze: (1) die Nutzung maschinellen Lernens und (2) die Verwendung lexikonbasierter Verfahren. Für das erstgenannte Vorgehen ist typischerweise ein mit Sentiment-Informationen annotiertes Trainingskorpus notwendig (D‘Andrea et al. 2015), welches für die Dramenanalyse bislang nicht vorliegt. Aus diesem Grund werden in der vorliegenden Arbeit lexikonbasierte Verfahren eingesetzt. Ein Sentiment-Lexikon ist dabei eine Wortliste, in der für jedes Wort Sentiment-Informationen angegeben sind (Liu 2016: 10), also z.B. ob es positiv oder negativ konnotiert ist und in welchem Ausmaß (Polaritätsstärke). Ein derartiges Wort nennt man auch 
                        sentiment bearing word (SBW; Liu 2016: 189).
                    
            
            
               SA-Parameter
               Folgende SA-Optionen wurden in unterschiedlichen Kombinationen systematisch evaluiert: 
               
                  i) Lexika – Es wurden fünf zentrale Sentiment-Lexika für den deutschsprachigen Bereich herangezogen: 
                        SentiWortschatz (SentiWS; Remus, Quasthoff & Heyer 2010), die 
                        Berlin Affective Word List – Reloaded (Bawl-R; Vo et al. 2009), die deutsche Version des 
                        NRC Emotion-Association Lexicon (NRC, Mohammad & Turney 2010), ein Lexikon von Clematide & Klenner (2010; im folgenden CK genannt) und das 
                        German Polarity Clues (GPC; Waltinger 2010). SentiWS, Bawl-R und CK enthalten Polaritäten und Polaritätsstärken, das NRC und GPC nur Polaritätsangaben. Das NRC enthält des Weiteren Annotationen zu acht unterschiedlichen Emotionen (Zorn, Furcht, Erwartung, Freude, Vertrauen, Ekel, Traurigkeit, Überraschung).
                    
               
                  ii) Historisch-linguistische Varianten – Über ein Tool des Deutschen Text-Archivs von Jurish (2011) wurde die Option der Lexikon-Erweiterung mit historischen linguistischen Varianten der Originalwörter untersucht.
                    
               
                  iii) Stoppwortlisten – Analog zu Saif et al. (2014) wurde der Einfluss der Verwendung von insgesamt drei unterschiedlichen Stoppwortlisten auf die Qualität der SA untersucht. Grund hierfür ist, dass durch verschiedene Kombination der Verfahren Sentiment-tragende Stoppwörter entstehen. Neben herkömmlichen Stoppwörtern wurden dabei auch Listen mit hochfrequenten Wörtern des Korpus untersucht. Dadurch wird der Einfluss von Wörtern analysiert, die zwar als sentiment-tragend in SA-Lexika ausgezeichnet werden, aber aufgrund der häufigen Nutzung im Korpus ein ungleichmäßiges Sentiment-Gewicht erzeugen (z.B. Herr, Fräulein).
                    
               
                  iv) Lemmatisierung – Eine weitere untersuchte Verarbeitungsform für die SA ist die Lemmatisierung. Als Lemmatisierer werden der 
                        Pattern-Lemmatisierer (De Smedt & Daelemans 2012) der Python-Bibliothek 
                        textblob und der Python-Wrapper des 
                        treetagger-Tools (Schmid 1995) evaluiert. Viele SA-Lexika enthalten lediglich Grundformen. Aufgrund der Probleme und Schwierigkeiten der Lemmatisierung im Deutschen (Eger, Gleim & Mehler 2016) soll vergleichend untersucht werden, welcher Lemmatisierer die besten Ergebnisse in Kombination mit Lexika erzielt. Ferner enthalten einige SA-Lexika manuell angegebene flektierte Wortformen. Es wird somit auch die automatische Lemmatisierung mit der manuellen Erweiterung verglichen.
                    
            
            
               SA-Metriken
               Alle nachfolgenden Berechnungen wurden bezüglich aller kombinatorischen Möglichkeiten der soeben beschriebenen SA-Parameter durchgeführt. Dabei werden die jeweiligen SA-Metriken nach Term-Zähl-Methodik (Kennedy & Inkpen 2006) berechnet, d.h. ein Text wird hinsichtlich vorhandener SBWs untersucht, positive und negative Wörter ausgezählt und für einen Polaritätswert die positive von der negativen Zahl subtrahiert. SA-Metriken wurden auf folgenden Ebenen über die jeweils zugehörigen Texte kalkuliert: Drama, Akte, Szenen, Repliken sowie Sprecher und Sprecherbeziehungen pro Drama, Akt, Szene und Replik. Die Beziehungen zwischen den Figuren wurden nach einer Heuristik von Nalisnick & Baird (2013) berechnet. 
            
            
               Erstellung des Gold Standards
               Zur systematischen Evaluation der Prädiktionsleistung der verschiedenen SA-Ansätze wurde ein Evaluationskorpus bestehend aus 200 Repliken erstellt. Bei der Auswahl der Repliken wurde darauf geachtet, dass die dramenspezifische Verteilung berücksichtigt wird, längere Dramen sind also mit mehr Repliken vertreten. Ferner wurden nur solche Repliken aufgenommen, die mindestens 19 Wörter umfassen. Diese Länge entspricht etwa -25% des Mittelwerts des Gesamtkorpus und vermeidet damit die Selektion von zu kurzen Repliken. Es wurde insgesamt auf eine gleichmäßige Längenverteilung geachtet.
               Die Repliken wurden von insgesamt fünf Personen (4 weiblich, 1 männlich; alle jeweils mit Deutsch als Muttersprache) jeweils unabhängig voneinander bezüglich deren Polaritätswirkung bewertet. Die Polarität jeder Replik wurde jeweils sechswertig (sehr negativ, negativ, neutral, gemischt, positiv, sehr positiv) und binär (positiv, negativ) bewertet. Die Annotationen wurden bezüglich des Übereinstimmungsgrades analysiert. Dazu wurden das Übereinstimmungsmaß Fleiss‘ Kappa (Fleiss 1971) sowie der Durchschnittswert der prozentualen Übereinstimmung aller Annotatoren und Annotatorinnen berechnet (vgl. Tabelle 1).
               
                  
                  
                     Tabelle 1. 
                            Annotator agreement.
                        
               
               Man erkennt eine geringe Übereinstimmung für die Bewertungsskala mit sechsstufiger Polarität und eine moderate Übereinstimmung für die binäre Variante. Die Ergebnisse verhalten sich konform zu verwandten Studien bei der Interpretation literarischer Texte (Alm & Sproat 2005). Als finale Annotation für eine Replik wird die binäre Polarität gewählt, die die Mehrheit der Annotatoren und Annotatorinnen ausgewählt haben (Endresultat: 139 negativ, 61 positiv).
            
            
               Evaluationsmaße 
               Als Evaluationsmaße wurden Genauigkeit (accuracy), Recall, Precision und F-Werte (Gonçalves et al. 2013) herangezogen. Abb. 1 zeigt einen Ausschnitt aus den je fünf besten Kombinationen pro Lexikon, geordnet nach Genauigkeit.
                        
               
               
                  
                  
                     Abbildung 1: Ausschnitt aus der detaillierten Ergebnistabelle zur Evaluation der SA-Kombinationsmöglichkeiten.
                        
               
            
            
               Ergebnisse der Evaluation
               Nachfolgend erfolgt eine überblicksartige Zusammenstellung einiger zentraler Ergebnisse aus der Evaluation:
               
                  Eine explizite Lemmatisierung führt zu einer verbesserten Leistung. Beide Lemmatisierer erzielen dabei meist ähnliche Ergebnisse. Die Lexikonerweiterung durch historische Varianten macht die explizite Lemmatisierung jedoch weitestgehend unnötig, da hierbei auch eine grundlegende Lemmatisierung inkludiert ist. 
                  Es zeigt sich eine konsistente Verbesserung durch die Lexikonerweiterung mittels der Wort-Varianten aus dem Tool von Jurish (2011). 
                  Stoppwortlisten haben nur auf vereinzelte Lexika (GPC, CK) einen merklich positiven Einfluss. 
                  Lexika mit Polaritätsstärken sind meist besser als reine Term-Zähl-Verfahren desselben Lexikons. 
                  Das Lexikon, dass die höchsten Genauigkeiten für die SA erzielt, ist SentiWS 
                  Die beste Leistung (unter Analyse aller Metriken) erzielt das erweiterte SentiWS mit den Polaritätsstärken, lemmatisiert mittels Pattern-Lemmatisierer und ohne Stoppwortliste (Genauigkeit = 0,67; F-Wert = 0,64). Die Erkennungsrate ist besser als die random baseline von 0,576 aber schlechter als viele Erkennungsraten auf anderen Anwendungsgebieten der SA (Vinodhini & Chandrasekran 2012). 
               
               Aufgrund der Tatsache, dass hier ein verhältnismäßig simpler SA-Ansatz gewählt wurde und bereits menschliche Annotatoren und Annotatorinnen Schwierigkeiten mit der Polaritätsbestimmung haben, sind die Ergebnisse insgesamt durchaus positiv zu bewerten.
            
         
         
            Online-Tool
            Abschließend wurde auf Basis des besten SA-Ansatzes ein Web-Tool für die SA bei Dramen entwickelt. Dieses bietet interaktive Visualisierungen der Sentiment-Verteilungen und -Verläufe für alle berechneten Ebenen. Neben den SentiWS-Metriken wurden auch die Emotionskategorien des NRC integriert. Über das Tool kann man erste Fallstudien auf Dramen-, Akt-, Szenen-, Repliken-, Sprecher- und Sprecherbeziehungsebene durchführen. Die SA-Komponente ist online verfügbar.
                    
            
            Trotz der historischen Differenz stimmen die Ergebnisse der automatischen SA tendenziell mit dem überein, was man in der Dramengeschichte über Bewertungen von Figuren und deren Verhalten weiß. Zusätzlich ist aber ein wichtiger heuristischer Mehrwert zu beobachten: eine Analyse allein auf der Basis von Sentiment-Zuschreibungen führt dazu, dass man das Augenmerk gezielt auf Fakten des Textes richtet, die bisher nicht berücksichtigt wurden. 
            Im Folgenden einige Beispiele für die Bestätigung bekannter Ergebnisse und für Entscheidungen von Analysefragen: 
            
               Fallstudie: Minna von Barnhelm
               Die Analyse von Minna von Barnhelm zeigt, dass die negativen emotionalen Bewertungen insgesamt gegenüber den positiven deutlich überwiegen (vgl. Abb. 2). Dieser Befund bestätigt die bekannte Erkenntnis, dass Lessing das Schema des rührenden Lustspiels verwendet hat. Während die Komik im Stück eher das Ergebnis von Schlussprozessen ist, geht es auf der wörtlichen Ebene überwiegend um ernste Vorwürfe und drohenden Identitäts- und Beziehungsverlust.
               
                  
                  
                     Abbildung 2: Polaritätsverteilung im Drama – 
                            Minna von Barnhelm
                  
               
               Es ist verschiedentlich behauptet worden (Saße 1993), Minna und nicht Tellheim sei die lächerliche Figur des Stücks. Die Sympathielenkung auf der wörtlichen Ebene des Textes, die in der unten stehenden Sentimentverteilung pro Akt abgebildet ist, kann dazu herangezogen werden, diese Frage negativ zu bescheiden (vgl. Abb. 3). Es ist eine auffällige Abweichung der Polarität im zweiten Akt erkennbar. In diesem Akt tritt Minna von Barnhelm zum ersten Mal auf, Tellheim jedoch nicht.
               
                  
                  
                     Abbildung 3: Polaritätsverlauf pro Akt – 
                            Minna von Barnhelm
                  
               
            
            
               Fallstudie: Emilia Galotti
               Die letzte Visualisierung kann genutzt werden die Frage zu diskutieren, warum Emilia in Lessings Drama „Emilia Galotti“ sterben muss (vgl. Abb. 4). Auffällig ist hier die starke negative Bewertung Emilias im zweiten Akt. Entgegen bisheriger Interpretationen, in denen nur die Intrige des Prinzen und Marinelli dafür verantwortlich gemacht werden, dass Emilia um ihre Tugend fürchten und ihren Vater dazu bringen muss, sie umzubringen, wird dadurch die Abwertung allein durch die Avancen des Prinzen sichtbar, die später sowohl Emilias als auch für Odoardos Einschätzung der Ehrbarkeit Emilias in ihrem zukünftigen Leben bestimmen.
               
                  
                  
                     Abbildung 4: Polaritätsverlauf von Sprechern pro Akt – 
                            Emilia Galotti
                  
               
            
            
               Fazit 
               Insgesamt sind die ersten Analyse-Ergebnisse über das Web-Tool sehr vielversprechend. Dabei ist zu bedenken, dass über die Verwendung von SA-Lexika ein sehr einfacher SA-Ansatz gewählt wurde. Über ML- oder Hybrid-Ansätze können Besonderheiten der poetischen und veralteten Sprache möglicherweise besser beachtet werden. Ferner ist fraglich, ob eine Reduktion auf das sonst in der SA übliche binäre System positiv/negativ ausreichend ist für komplexe Interpretationen von Emotionen in Dramen.
               Durch Optimierung des SA-Verfahrens, Ausbau der Funktionen im Front-End und Erweiterung des Tools mit zusätzlichen Dramen sollen künftig Möglichkeiten und Nutzen der SA in der Dramenanalyse weiter exploriert werden.
            
         
      
      
         
             https://textgridrep.org/repository.html; Hinweis: alle im Beitrag erwähnte URLs wurden zuletzt am 12.1.2018 überprüft
             Die vollständige Tabelle ist online verfügbar unter https://drive.google.com/open?id=1cvyqiiLJ03XT1VNaWgSDoajeTE3wgeqxxr2PXp-VM4w
             http://lauchblatt.github.io/QuantitativeDramenanalyseDH2015/FrontEnd/sa_selection.html
         
         
            
               Bibliographie
               
                  Alm, Cecilia Ovesdotter / Sproat, Richard (2005): "Emotional sequencing and development in fairy tales.", in:
                         International Conference on Affective Computing and Intelligent Interaction 668-674.
                    
               
                  Alm, Cecilia Ovesdotter / Roth, Dan / Sproat, Richard (2005): "Emotions from text: machine learning for text-based emotion prediction.", in: 
                        Proceedings of the conference on human language technology and empirical methods in natural language processing 579-586.
                    
               
                  Buechel, Sven / Hellrich, Johannes / Hahn, Udo (2017): “The Course of Emotion in Three Centuries of German Text – A Methodological Framework.”, in: 
                        Digital Humanities 2017 176-179.
                    
               
                  Clematide, Simon / Klenner, Manfred (2010): "Evaluation and extension of a polarity lexicon for German.", in: 
                        Proceedings of the First Workshop on Computational Approaches to Subjectivity and Sentiment Analysis 7-13.
                    
               
                  D’Andrea, Alessia et al. (2015): "Approaches, tools and applications for sentiment analysis implementation.", in 
                        International Journal of Computer Applications 125.3: 26-33.
                    
               
                  De Smedt, Tom / Daelemans, Walter (2012): "Pattern for python.", in: 
                        Journal of Machine Learning Research 13: 2063-2067.
                    
               
                  Eger, Steffen / Gleim, Rüdiger / Mehler, Alexander. (2016). “Lemmatization and Morphological Tagging in German and Latin: A Comparison and a Survey of the State-of-the-art.”, in: 
                        LREC 1507–1513.
                    
               
                  Elsner, Micha (2012): "Character-based kernels for novelistic plot structure.", in: 
                        Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics 634-644.
                    
               
                  Fleiss, Joseph L. (1971): "Measuring nominal scale agreement among many raters.", in: 
                        Psychological bulletin 76.5: 378-382.
                    
               
                  Gonçalves, Pollyanna, et al. (2013): "Comparing and combining sentiment analysis methods.", in: 
                        Proceedings of the first ACM conference on Online social networks 27-33.
                    
               
                  Jannidis, Fotis, et al. (2016): "Analyzing Features for the Detection of Happy Endings in German Novels.", in: 
                        arXiv preprint arXiv:1611.09028
                    
               
                  Jurish, Bryan (2011): 
                        Finite-state canonicalization techniques for historical German. Diss. Universitätsbibliothek der Universität Potsdam.
                    
               
                  Kakkonen, Tuomo / Kakkonen, Gordana Galić (2011): "SentiProfiler: creating comparable visual profiles of sentimental content in texts.", in: 
                        Language Technologies for Digital Humanities and Cultural Heritage 62-67.
                    
               
                  Kennedy, Alistair / Inkpen, Diana (2006): "Sentiment classification of movie reviews using contextual valence shifters.", in: 
                        Computational intelligence 22.2: 110-125.
                    
               
                  Kim, Evgeny / Padó, Sebastian / Klinger, Roman (2017): “Investigating the relationship between Literary Genres and Emotional Plot Development.”, in: 
                        Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature 17–26.
                    
               
                  Kouloumpis, Efthymios / Wilson, Theresa / Moore, Johanna D.  (2011): "Twitter sentiment analysis: The good the bad and the omg!.", in: In 
                        Proceedings of the Fifth International Conference on Weblogs and Social Media 538-54.
                    
               
                  Liu, Bing (2016): 
                        Sentiment analysis: Mining opinions, sentiments, and emotions. New York: Cambridge University Press.
                    
               
                  McGlohon, Mary / Glance, Natalie S. / Reiter, Zach (2010) "Star Quality: Aggregating Reviews to Rank Products and Merchants.", in: 
                        Proceedings of the International Conference on Weblogs and Social Media (ICWSM-2010) 114-121.
                    
               
                  Mohammad, Saif (2011): "From once upon a time to happily ever after: Tracking emotions in novels and fairy tales.", in: 
                        Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities 105-114.
                    
               
                  Mohammad, Saif M. / Turney, Peter D. (2010): "Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon.", in: 
                        Proceedings of the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text 26-34.
                    
               
                  Nalisnick, Eric T. / Baird, Henry S. (2013): "Character-to-character sentiment analysis in shakespeare’s plays.“, in:
                         Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics 479–483.
                    
               
                  Remus, Robert / Quasthoff, Uwe / Gerhard, Heyer (2010): "SentiWS-A Publicly Available German-language Resource for Sentiment Analysis.", in: 
                        LREC 1168-1171.
                    
               
                  Saif, Hassan, et al. (2014): "On stopwords, filtering and data sparsity for sentiment analysis of twitter.", in: 
                        Proc. 9th Language Resources and Evaluation Conference (LREC) 810-817.
                    
               
                  Saße, Günter (1993): 
                        Liebe und Ehe: oder, wie sich die Spontaneität des Herzens zu den Normen der Gesellschaft verhält. Lessings Minna von Barnhelm. Tübingen: Niemeyer.
                    
               
                  Schmid, Helmut (1995): "Improvements in part-of-speech tagging with an application to German.", in: 
                        Proceedings of the acl sigdat-workshop.
                    
               
                  Vinodhini, G. / Chandrasekaran, R. M. (2012): "Sentiment analysis and opinion mining: a survey.", in: 
                        International Journal of Advanced Research in Computer Science and Software Engineering 2.6: 282-292.
                    
               
                  Võ, Melissa LH, et al. (2009): "The Berlin affective word list reloaded (BAWL-R) ", in: 
                        Behavior research methods 41.2: 534-538.
                    
               
                  Waltinger, Ulli (2010): "Sentiment Analysis Reloaded-A Comparative Study on Sentiment Polarity Identification Combining Machine Learning and Subjectivity Features.", in: 
                        Proceedings of the 6th International Conference on Web Information Systems and Technologies (WEBIST '10).
                    
            
         
      
   


10808	2018	
      
         
            Einleitung
            Der vorliegende Beitrag enthält methodische Überlegungen und Experimente zu “Zeta”, einem von John Burrows (2007) vorgeschlagenen Maß für die Distinktivität oder “keyness” von textuellen Merkmalen (Wortformen, Lemmata, etc.). Mit solchen Maßen werden Merkmale ermittelt, die für eine bestimmte Gruppe von Texten gegenüber einer Vergleichsgruppe charakteristisch sind.
            Das Exposé gibt einen Überblick zu solchen Maßen, bevor die Funktionsweise von Zeta erläutert wird. Aufbauend auf einer Neu-Implementierung in Python (“pyzeta”, https://github.com/cligs/pyzeta) und Vorarbeiten (Schöch im Druck) liegt der spezifische Forschungsbeitrag dann in den folgenden Schritten: erstens werden mehrere Varianten von Zeta vorgeschlagen und implementiert; zweitens werden Verfahren zum Vergleich und der Evaluation der Ergebnisse erprobt. Ziel ist es, Zeta in seiner Funktionsweise und in seiner Beziehung zu vergleichbaren Maßen besser zu verstehen und vorhandene Nachteile des Maßes durch gezielte Modifikationen zu beheben.
         
         
            Überblick und Stand der Forschung
            Die vergleichende, kontrastierende Analyse zweier Gruppen von Texten ist ein in den Sprach- und Literaturwissenschaften weit verbreitetes Verfahren. Entsprechend wurden zahlreiche Maße der Distinktivität oder “keyness” von Merkmalen entwickelt und für vielfältige Fragestellungen eingesetzt. Die grundlegende Annahme solcher Maße ist, dass ein Merkmal nicht schon durch seine reine Häufigkeit in einer Textgruppe für diese charakteristisch ist, sondern dass dies auch davon abhängt, wie häufig das Merkmal in einer Vergleichsgruppe ist. Diejenigen Merkmale bekommen einen besonders hohen Wert zugewiesen, die in der einen Gruppe sehr häufig sind und zugleich in der Vergleichsgruppe sehr selten sind (Scott 1997, 236). Man kann vier Arten von Verfahren unterscheiden: 
            
               Verfahren, welche erwartete und beobachtete Werte vergleichen (wie “log-likelihood-ratio”; siehe Rayson und Garside 2000);
               Verfahren, die eine Gewichtung der Häufigkeiten vornehmen (wie “tf-idf”, “term frequency / inverse document frequency”; siehe Robertson 2004);
               Statistische Hypothesentests, die Verteilungseigenschaften vergleichen (wie “Welch’s t-Test”; siehe Bortz und Schuster 2010); 
               Dispersionsmaße, die nicht die Häufigkeit, sondern den Grad der konsistenten Verwendung von Merkmalen in Beziehung setzen (wie “deviation of proportions”; Gries 2008). 
            
            Die praktische Bedeutung von Distinktivitätsmaßen ist daran erkennbar, dass Korpusanalyse-Software meist eine entsprechende Funktion anbietet, so “keyness” in WordCruncher (Scott 1997) oder “spécificity” in TXM (Heiden et al. 2012). Kilgariff 2004 und Lijfijt et al. 2014 sind wichtige Arbeiten zur Evaluation von Distinktivitätsmaßen. 
         
         
            Was ist Zeta?
            Das von John Burrows (2007) vorgeschlagene “Zeta” beruht auf einem Dispersionsmaß. Vor der Berechnung werden die Texte in kleinere Segmente gesplittet, wobei die Segmentlänge ein wichtiger Parameter ist. Dann wird für jedes Merkmal der Anteile der Segmente erhoben, in denen das Merkmal mindestens einmal vorkommt (die “document proportion”). Von diesem Anteil in der untersuchten Gruppe wird der entsprechende Anteil in der Vergleichsgruppe subtrahiert, woraus sich ein Zeta-Wert zwischen -1 und 1 ergibt.
            Ein Effekt dieser Berechnungsweise ist, dass Zeta Inhaltswörter als distinktive Wörter favorisiert, Funktionswörter sowie Eigennamen hingegen penalisiert. Daraus ergibt sich eine hohe Interpretierbarkeit der Ergebnisse, die Zeta im Vergleich zu anderen Maßen für die (digitalen) Literaturwissenschaften besonders attraktiv macht. Ein Nachteil ist, dass Merkmale durch die Subtraktion niemals einen Zeta-Wert bekommen können, der höher ist als ihre “document proportion” in der untersuchten Textgruppe, selbst wenn sie gegenüber der Vergleichsgruppe deutlich überrepräsentiert sind (Abbildung 1, Wörter in den roten Rahmen; Schöch im Druck).
            
               
                  
                  Abbildung 1: Scatterplot der Wörter in zwei Textgruppen (französische Komödien und Tragödien): “document proportions” der Wörter in zwei Textgruppen (x- und y-Achse) und resultierende Zeta-Werte (Distanz von der Diagonale).
               
               Eine bekannte Implementierung von Zeta existiert im stylo-Paket für Rin der Funktion "oppose()" (Eder et al. 2016). Abbildung 2 zeigt für ein Beispiel die Ergebnisdarstellung in der hier verwendeten “pyzeta”-Implementierung. 
            
            
               
                  
                  Abbildung 2: Positive und negative Keywords für französische Komödien (rechts) im Vergleich mit Tragödien (links). Zeta-Werte auf der horizontalen Achse.
               
            
            Anwendungsbeispiele von Zeta gibt es in der Shakespeare-Forschung (Craig und Kinney 2009), der modernen englischsprachigen Literatur (Hoover 2010; Weidman und O’Sullivan 2017) und der Romanistik (Schöch im Druck). In der zuletzt genannten Arbeit zum französischen Theater der Klassik und Aufklärung konnte nicht nur die erwartbare, klare Differenzierung von Komödien und Tragödien gezeigt werden. Vielmehr wurde auch die spezifische Verortung der Tragikomödien deutlich, die nicht als Mischform zwischen Komödien und Tragödien zu verstehen sind, sondern eine besondere Affinität zur Tragödie aufweisen (Abbildung 3). 
            
               
               Abbildung 3: Hauptkomponentenanalyse auf Grundlage der 50 Wörter, die für Komödien und Tragödien die höchsten Zeta-Werte erhalten. Komödien in rot, Tragödien in blau, Tragikomödien in grün. Quelle: Schöch im Druck.
            
         
         
            Varianten von Zeta
            Ausgehend von der ursprünglichen Formulierung von Zeta durch Burrows als Subtraktion der “document proportions” lassen sich mehrere Faktoren identifizieren, die zur Formulierung von Varianten von Zeta geeignet erscheinen: 
            
               Statt “document proportions” werden relative Häufigkeiten verwendet; 
               Statt der Subtraktion erfolgt eine Division;
               Statt nicht-transformierter Werte wird eine log2-Transformation der Werte vorgenommen.
            
            Die Kombination dieser Faktoren ergibt 8 Varianten von Zeta (Tabelle 1). 
         
         
            
               
               document proportions
               relative Häufigkeiten
            
            
               
               keine Transformation
               log2-Transformation
               keine Transformation
               log2-Transformation
            
            
               Subtraktion
               sd0
               sd2
               sr0
               sr2
            
            
               Division
               dd0
               dd2
               dr0
               dr2
            
         
         Tabelle 1: Übersicht über die getesteten Varianten von Zeta. Die Variante mit Label „sd0“ entspricht Burrows‘ Zeta. 
         Einige der Varianten sind mathematisch gut motivierbar und versprechen, den oben genannten Nachteil der begrenzten Werte für bestimmte Wörter auszugleichen und damit Zeta zu verbessern, es wurden aber alle implementiert und auf zwei Datensätzen evaluiert.
         
            Datensätze
            Es wurden zwei unterschiedliche Korpora verwendet. Erstens ein Korpus aus der textbox-Sammlung (Schöch et al. 2017), das Romane enthält, die zwischen 1880 und 1940 veröffentlicht wurden: jeweils 24 Texte aus Spanien und aus Lateinamerika (ca. 2,8 Millionen Tokens). Zweitens, ein Teil der Sammlung 
                    Théâtre classique (Fièvre 2007-2017) mit französischen Dramen: 134 Tragödien und 158 Komödien aus Klassik und Aufklärung (ca. 4,9 Millionen Tokens).
                
         
         
            Evaluation
            Die 8 Varianten führen zu unterschiedlichen Wortlisten, geordnet nach absteigenden Zeta-Werten. Vergleicht man den Beginn der Wortlisten für zwei Varianten, fällt auf, dass es wie erwartet zu Verschiebungen im Rang der distinktivsten Wörter kommt. 
            
               Ähnlichkeit der Varianten
               Um den Grad der Abweichung der Ergebnisse für alle Varianten zueinander auf der Grundlage längerer Wortlisten zu erheben, ist ein quantifizierendes Verfahren unerlässlich. Ein Ansatz ist, ein Clustering der Maße auf Basis der Zeta-Werte ihrer Wörter vorzunehmen (Abbildung 4). 
               
                  
                     
                     Abbildung 4: Dendrogramm auf Grundlage einer Cluster Analyse der Zeta-Werte für die 8 Zeta-Varianten (Théâtre-classique-Datensatz; 500 distinktive Wörter; Ward-Verfahren).
                  
                  Abbildung 4 zeigt, dass der wichtigste Faktor für die Unterschiedlichkeit der Varianten ist, ob subtrahiert oder dividiert wird (zwei Haupt-Cluster). Die beiden anderen Variablen spielen eine viel kleinere Rolle. (Die Ergebnisse weiterer Analysen, u.a. auf Basis der RBO-Ähnlichkeit (“ranked biased order”, Webber et al. 2010), werden aus Platzgründen hier nicht diskutiert.)
               
            
            
               Evaluation mit Klassifikationstask
               Unabhängig von den Beziehungen der Varianten zueinander stellt sich die Frage, welche der Varianten von Zeta besonders gut distinktive Wörter identifiziert. Dabei kann zur Evaluation nicht auf einen Goldstandard zurückgegriffen werden: eine händische Annotation der Wörter nach dem Grad ihrer Distinktivität ist nicht möglich, weil niemand das zugrunde liegende Korpus überblicken kann. Die Qualität eines Distinktivitätsmaßes kann aber evaluiert werden, indem es als Merkmalsselektor für einen Klassifikationstask verwendet wird.
               Wenn die durch Zeta am höchsten bewerteten Wörter als Features für einen Klassifikator verwendet werden, sollte dieser Klassifikator eine höhere Genauigkeit erreichen als bei einfacher Verwendung der häufigsten Wörter. Tatsächlich lässt sich dieser Effekt auf dem Korpus der spanisch-sprachigen Romane nachweisen (Tabelle 2). Zur Ermittlung einer Baseline wurde für die Klassifikation in spanische und lateinamerikanische Romane ein linearer SVM-Classifier auf den häufigsten 80, nach TF-IDF gewichteten Wörtern (ohne Stoppwörter) trainiert. Dieser Classifier erreichte lediglich eine Klassifikationsgüte (F1-Score) von 0.49, ist also nicht vom Zufall zu unterscheiden.
               Trainiert man stattdessen auf den 40 distinktivsten Wörtern nach Zeta (oder einer der Varianten), lassen sich Genauigkeiten von deutlich über 90% erzielen. Diese Genauigkeit kann nicht als tatsächliches Klassifikationsergebnis gesehen werden, da die distinktivsten Merkmale auf dem gesamten Korpus extrahiert wurden, ohne Aufteilung in Trainings- und Testdaten. Dennoch zeigt das Ergebnis, dass die von Zeta selektierten Merkmale tatsächlich sehr nützlich für eine Klassifikation sind. Zudem zeigen sich deutliche Unterschiede in der Performanz je nach verwendeter Variante: während mit “sd0” (=Burrows Zeta) 81% Genauigkeit erreicht wird, erhöht sich dieser Wert bei der Variante mit log2-Transformation, “sd2”, auf 98%. 
               
                  
                     baseline
                     sd0
                     sd2
                     sr0
                     sr2
                     dd0
                     dd2
                     dr0
                     dr2
                  
                  
                     0.49
                     0.81
                     0.98
                     0.48
                     0.83
                     0.79
                     0.85
                     0.75
                     0.79
                  
               
               
                  Tabelle 2: Klassifikationsergebnisse bei Verwendung einer linearen SVM, trainiert auf den 40 am höchsten gerankten Wörtern verschiedener Maße im Vergleich zur Baseline. Alle Werte sind der Durchschnitt einer dreifachen Kreuzvalidierung.
               
            
         
         
            Fazit
            Wichtigste Ergebnisse dieses Beitrags sind ein differenziertes Verständnis davon, wie Zeta im Kontext anderer Distinktivitätsmaße einzuordnen ist und wie bestimmte mathematischen Parameter sich auf die Ergebnislisten auswirken: als ein auf dem Grad der Dispersion der Merkmale beruhendes Maß, dessen entscheidende Eigenschaft die Subtraktion der Werte ist. Ein weiteres wesentliches Ergebnis sind die beiden vorgeschlagenen Strategien zum Vergleich und der Evaluation von Distinktivitätsmaßen, wenn eine direkte Evaluation auf Goldstandard-Daten nicht möglich ist.
            Nächste Schritte: Wir möchten als weitere Evaluationsstrategie künstliche Texte generieren, in denen wir kontrolliert einzelne Wörter mit unterschiedlich stark abweichender Verteilung einfügen. So können verschieden Zeta-Varianten direkt dahingehend evaluiert werden, wie gut sie diese Wörter korrekt identifizieren. Zudem möchten wir neben der “document proportion” von Zeta ein weiteres Dispersionsmaß, die von Gries (2008) vorgeschlagene “deviation of proportions” als Grundlage für eine weitere Zeta-Variante verwenden. Schließlich möchten wir untersuchen, ob die hohe Interpretierbarkeit des Original-Zeta bei den Varianten mit noch höherer Klassifikationsgüte erhalten bleibt.
            Eine separate Untersuchung ist in Vorbereitung zu zwei eng zusammenhängenden Fragen: wie sich unterschiedliche Segmentlängen einerseits auf die Ergebnisse auswirken, und wie sich die Ergebnisse verändern, wenn unterschiedlich lange Texte nicht mit allen Segmenten in die Berechnung eingehen, sondern aus jedem Einzeltext zufällig eine identische Anzahl von Segmenten gesampelt wird. 
            Übergeordnetes Ziel all dieser Arbeiten zu Zeta ist es letztlich weniger, ein perfektes Distinktivitätsmaß zu identifizieren, als ein justierbares Maß vorzuschlagen, bei dem in Abhängigkeit von Daten und Forschungsfragen dynamisch Parameter verändert und die resultierenden Verschiebungen in den Ergebnissen visualisiert werden können. 
         
      
      
         
            
               Bibliographie
               
                  Bortz, Jürgen, and Christof Schuster (2010). 
                        Statistik für Human- und Sozialwissenschaftler. 7. Auflage. Berlin: Springer.
                    
               
                  Burrows, John (2007). “All the Way Through: Testing for Authorship in Different Frequency Strata.” 
                        Literary and Linguistic Computing 22, no. 1: 27–47. doi:10.1093/llc/fqi067.
                    
               
                  Craig, Hugh, and Arthur F. Kinney, eds. (2009). 
                        Shakespeare, Computers, and the Mystery of Authorship. 1st ed. Cambridge University Press.
                    
               
                  Eder, Maciej, Mike Kestemont, and Jan Rybicki(2016). “Stylometry with R: A Package for Computational Text Analysis.” 
                        The R Journal 16, no. 1: 1–15.
                    
               
                  Fièvre, Paul, ed. (2007-2013). “Théâtre classique.” Paris: Université Paris-IV Sorbonne. http://www.theatre-classique.fr.
                    
               
                  Gries, Stefan Th. (2008). “Dispersions and Adjusted Frequencies in Corpora.” 
                        International Journal of Corpus Linguistics 13, no. 4: 403–37. doi:10.1075/ijcl.13.4.02gri.
                    
               
                  Heiden, Serge (2010). “The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme.” In 
                        24th Pacific Asia Conference on Language, Information and Computation - PACLIC24, edited by Ryo Otoguro, Kiyoshi Ishikawa, Hiroshi Umemoto, Kei Yoshimoto, and Yasunari Harada, 389–98. Sendai: Waseda University. https://halshs.archives-ouvertes.fr/halshs-00549764/en.
                    
               
                  Hoover, David L. (2010). “Teasing out Authorship and Style with T-Tests and Zeta.” In 
                        Digital Humanities Conference. London: ADHO. http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/html/ab-658.html.
                    
               
                  Kilgarriff, Adam (2001). “Comparing Corpora.” 
                        International Journal of Corpus Linguistics 6, no. 1: 97–133. doi:10.1075/ijcl.6.1.05kil.
                    
               
                  Lijffijt, Jefrey, Terttu Nevalainen, Tanja Säily, Panagiotis Papapetrou, Kai Puolamäki, and Heikki Mannila (2014). “Significance Testing of Word Frequencies in Corpora.” 
                        Digital Scholarship in the Humanities 31, no. 2: 374–97. doi:10.1093/llc/fqu064.
                    
               
                  Rayson, Paul, and R. Garside (2000). “Comparing Corpora Using Frequency Profiling.” In 
                        Proceedings of the Workshop on Comparing Corpora, 1–6. Hong Kong: ACM.
                    
               
                  Robertson, Stephen (2004). “Understanding Inverse Document Frequency: On Theoretical Arguments for IDF.” 
                        Journal of Documentation 60, no. 5 : 503–20.
                    
               
                  Schöch, Christof (im Druck). “Zeta für die kontrastive Analyse literarischer Texte. Theorie, Implementierung, Fallstudie.” In 
                        Quantitative Verfahren in der Literaturwissenschaft. Von einer Scientia Quantitatis zu den Digital Humanities, edited by Andrea Albrecht, Sandra Richter, Marcel Lepper, Marcus Willand, and Toni Bernhart. Berlin: de Gruyter. https://cligs.hypotheses.org/files/2017/09/Schoech_2017-preprint_Zeta-fuer-die-kontrastive-Analyse.pdf.
                    
               
                  Schöch, Christof, José Calvo Tello, Ulrike Henny-Krahmer, and Stefanie Popp (angenommen). “The CLiGS Textbox: Building and Using Collections of Literary Texts in Romance Languages Encoded in XML-TEI.” 
                        Journal of the Text Encoding Initiative http://cligs.hypotheses.org/files/2017/09/Schoech-et-al_2017_Textbox.pdf.
                    
               
                  Scott, Mike (1997). “PC Analysis of Key Words and Key Key Words.” 
                        System 25, no. 2: 233–45.
                    
               
                  Webber, William, Alistair Moffat, and Justin Zobel (2010). “A Similarity Measure for Indefinite Rankings.” 
                        ACM Trans. Inf. Syst. 28, no. 4: 20:1–20:38. doi:10.1145/1852102.1852106.
                    
            
         
      
   


10813	2018	
      
         
            I. Motivation 
            In den textbasierenden Geisteswissenschaften ist eine optimale Beschaffenheit der zu Grunde liegenden Texte eine wesentliche Bedingung für wissenschaftliches Arbeiten. Wie sehr sich die Bedeutung eines Textes schon durch scheinbar minimale Unterschiede wie die der Interpunktion verschieben kann, hat sich einer breiteren Öffentlichkeit zuletzt in der Diskussion über einen Punkt in einer Abschrift der amerikanischen Unabhängigkeitserklärung gezeigt (The Atlantic 7/2014).
            Während die historische Diskussion über digitale Editionen vielfach mit dokumentarischen Editionstypen bzw. der Frage nach dem Dokumentcharakter der Grundlagen einer Edition (Manuskripte etc.) und der Frage nach der Essenz des Textbegriffes beschäftigt ist (P. Sahle 2013), ergibt sich insbesondere für diejenigen historischen und altertumswissenschaftlichen Disziplinen, die weiterhin auf die klassische Form kritischer Texteditionen angewiesen sind, ein anderes Problem: Da sie traditionell in hohem Maße mit Texten beschäftigt sind, die notwendig als Interpretationen und Rekonstruktionen gelten müssen (West 1973,32), ist hier vor allem die Frage nach der Beschaffenheit und Begründung der zum Teil massiven editorischen Eingriffe nicht nur in die Lesart, sondern auch in den Umfang und die Zuschreibung von Texten, etwa im Falle sogenannter „Fragmente“, von vorrangiger Bedeutung. Der Workshop soll in diese allgemeine Problematik einführen und sich im Hinblick auf das Thema der Konferenz dem Bereich Kritik der digitalen Geisteswissenschaften (traditionelle Fächer und DH) zuordnen. Den Teilnehmenden soll dies praktisch anhand der Anwendung der Software eCOMPARATIO vermittelt werden. eCOMPARATIO bietet eine einfache Möglichkeit der Kollationierung verschiedener Varianten eines Textes und ermöglicht eine digitale, auf dem automatischen Textvergleich beruhende Form des kritischen Apparates.
            Dazu sind einzelne Use Cases ausgearbeitet worden (anhand der Fragmentsammlung der Vorsokratiker von Diels/Kranz [griechisch], der Res Gestae des Augustus [lateinisch], des Genfer Gelöbnisses [deutsch], der Gettysburg Address von Abraham Lincoln [englisch]), anhand derer den Teilnehmern die Funktionalitäten demonstriert werden.
         
         
            II. Die Software
            Im Projekt eCOMPARATIO, das von 2014 bis 2016 von der DFG gefördert wurde und in Leipzig am Lehrstuhl für Alte Geschichte in Kooperation mit dem Center of E-Humanities in History and Social Sciences (ICE) am Max-Weber-Kolleg für kultur- und sozialwissenschaftliche Studien durchgeführt wurde, ist vor dem Hintergrund dieser Problematik ein einfach zu bedienendes Tool für den Vergleich prinzipiell beliebig vieler und beliebig langer digitalisierter Editionen vorrangig griechischer und lateinischer, technisch gesehen aber auch sämtlicher anderer in einem UNICODE-Format zugänglicher Texte entstanden. Der Textvergleich arbeitet auf der Basis der Identifikation von Ungleichheiten. Hierbei reicht die Spanne der programmiertechnisch unterscheidbaren Differenzen von Ungleichheiten innerhalb von Wörtern und Buchstabenfolgen bis hin zu vertauschten Passagen. Im Unterschied zu anderen Vergleichsprogrammen benötigen Anwender weder eine Installation von Python, eine Levenshtein Bibliothek oder ein Java-Plugin, sondern erhalten eine für die Anwender ohne Vorkenntnisse sofort im Browser nutzbare und mit Copy-and-Paste einfach zu bedienende Oberfläche. Auch Fallbeispiele, eine Textdokumentation sowie Videoanleitungen stehen zur Verfügung. In zwei derzeit (2016-17) von der DFG und der Andrew W. Mellon Foundation geförderten Projekten an der Universität Leipzig in Kooperation mit Christopher Blackwell von der Furman University in Greenville, SC/ USA, erfolgt eine Einbindung des Vergleichstools in ein Interface (zu dem Protokoll Canonical Text Services (CTS) als Teil der CITE Architecture: http://cite-architecture.github.io/cts/), dessen Ziel nicht nur die Bereitstellung möglichst vieler digitalisierter Editionen einzelner Texte, sondern damit auch erweiterte Möglichkeiten des Textvergleiches, der Suche nach Parallelstellen und weiterer Formen des sog. „Text-Mining“ ist.
            
               Die Texteingabemaske
               Die eigenständige Version der Vergleichssoftware (unter http://ecomparatio.net/~khk/instanzen/ecompp/ ist ein Beispiel frei zum Gebrauch bereitgestellt) ermöglicht weiterhin eine browserbasierte oder auch offline verwendbare Anwendung für Texte nach dem Copy-and-Paste-Prinzip: Hier können beliebig viele Versionen eines Textes eingefügt werden.
            
            
               Die Darstellung
               Das Tool ermöglicht verschiedene Ausgaben des Vergleiches:
            
            
               
               Abbildung 1, Detail-Vergleich
            
            
               
               Abbildung 2, Synopse
            
            
               
               Abbildung 3, Buch-Darstellung mit digitalem Apparat zum Textvergleich
            
            
               Ausgabe/Export der Ergebnisse
               Die Ausgabe der Ergebnisse kann zur weiteren Integration in den Arbeitsprozess als TEI XML oder LaTeX Code erfolgen. Will man die Vergleichsdaten abfragen, so steht ein JSON Interface zur Verfügung, über das weitere Software angebunden werden kann. 
            
         
         
            III. Ziele und Zielgruppe
            Ziel des Workshops ist eine Einführung in die Anwendung der eCOMPARATIO Vergleichssoftware, auch in Abgrenzung zu bereits verfügbarer Software (collateX, Juxta) mit ähnlichen Anwendungsbereichen. Dazu soll zunächst eine Einführung in die Relevanz der Frage nach scheinbar marginalen textlichen Unterschieden anhand prägnanter Beispiele aus verschiedenen historischen Epochen und in verschiedenen wissenschaftlichen Diskurssprachen gegeben werden (anhand griechischer, lateinischer, englischer und deutscher Texte).
            Darüber hinaus soll den Teilnehmenden die Möglichkeit gegeben werden, auch Texte aus den eigenen Disziplinen oder beliebige im Internet verfügbare Versionen von Texten selbst miteinander zu vergleichen und sich so mit den Funktionen der Software vertraut zu machen.
            Zielgruppe des Workshops sind prinzipiell alle Interessierten aus dem Bereich der textbasierten Geisteswissenschaften. Dabei sind diejenigen, die, wie oben angesprochen, vor allem mit der Vielzahl digitalisierter Editionen arbeiten, ebenso angesprochen wie solche, die selbst an der Erstellung von Editionen etc. arbeiten, und für die eCOMPARATIO ein hilfreiches Mittel bei der Sichtung und Kollationierung von Textzeugnissen sein kann.
            Besondere technische Kenntnisse sind nicht erforderlich, da sich eCOMPARATIO bewusst an Anwender richtet, die entweder selbst an einer kritischen Edition arbeiten oder auf der Grundlage mehrerer kritischer Editionen wissenschaftliche Fragestellungen verfolgen.
         
         
            IV. Ablauf und Teilnehmerzahl
            Im Workshop soll die browsergestützte Version zur Anwendung kommen, ein Download der Software wird aber auch möglich sein. Neben der bereits beschriebenen Einführung in die wissenschaftliche Grundproblematik erfolgt zunächst eine kurze Präsentation eigener Ergebnisse im Zuge der Forschung mit eCOMPARATIO.
            Hauptsächlich soll im praktischen Teil den Teilnehmenden die Möglichkeit gegeben werden, auf ihren eigenen Rechnern in der browsergestützten Version selbst Vergleiche der für sie relevanten Texte vorzunehmen, die mitgebracht werden sollten oder vor Ort aus Onlinedatenbanken heruntergeladen werden können.
            Während des Workshops soll dann von Seiten der Organisatoren auf eventuelle individuelle Probleme und Schwierigkeiten eingegangen werden. Die Organisatoren des Workshops erhoffen sich hiervon einen eigenen Erkenntnisgewinn auch im Hinblick auf die Anwendungsmöglichkeiten und notwendige Ergänzungen im Hinblick auf die Arbeit gerade mit nicht-lateinischen oder nicht-griechischen Texten.
         
         
            V. Ablauf
            Die TeilnehmerInnen erhalten vorab eine detaillierte Anleitung (Handbuch eCOMPARATIO).
            Im eigentlichen Workshop werden die jeweiligen Arbeitsschritte von einem der Organisatoren live vorgeführt (dafür wird ein leistungsstarker Beamer benötigt). Die konkreten Inhalte orientieren sich dabei an den bisher von den Organisatoren ausgearbeiteten Use Cases (s.o.), die von den Organisatoren präsentiert werden.
            Während des Workshops werden wir bei auftretenden Fragen und Problemen den Teilnehmenden helfend zur Seite stehen, da sie auch die Möglichkeit haben sollen, anhand eigener Texte zu arbeiten. Um eine möglichst gute Betreuung der TeilnehmerInnen gewährleisten zu können, sollte die Teilnehmerzahl 25-30 nicht überschreiten.
         
         
            VI. Organisatoren
            Charlotte Schubert ist Althistorikerin, hat zu Themen der Mentalitätsgeschichte, Medizin- und Wissenschaftsgeschichte sowie zu verschiedenen Bereichen der griechischen Geschichte gearbeitet; seit 2006 verantwortliche Koordinatorin in verschiedenen DH-Projekten, die vom BMBF (eAQUA, eXChange), der DFG (eCOMPARATIO, CTS, Etablierung eines Open Access Online eJournals: Digital Classics Online), der VolkswagenStiftung (Digital Plato) und der Andrew W. Mellon Foundation (CTS) gefördert wurden und werden.
            Hannes Kahl ist Informatiker, Berufserfahrung aus diversen DH-Projekten, Entwickler von eCOMPARATIO, arbeitet an der Weiterentwicklung von CTS und an einer Dissertation zu dem Thema „Form und Formalisierung – mit Anwendung innerhalb automatischer Ermittlung von Buchstabenwerten aus digitalen Abbildungen griechischer Lettern innerhalb wissenschaftlicher Editionen sowie deren digitaler Formatierung“ (Betreuer: Prof. Ch. Schubert, Alte Geschichte/Universität Leipzig/ Prof. O. Arnold, Informatik/FH Erfurt).
            Friedrich Meins ist Althistoriker und hat eine Dissertation zum Thema „Literarische Kritik, rhetorische Theorie und historische Methode bei Dionysios von Halikarnassos“ geschrieben. Im Bereich der eHumanities hat er in den Projekten „eAQUA“ und „eAQUA Dissemination“ an der Uni Leipzig sowie im Projekt „eCOMPARATIO“ am ICE der Universität Erfurt mitgearbeitet. Derzeit arbeitet er im von der Andrew W. Mellon Foundation geförderten Kooperationsprojekt der Universität Leipzig und der Furman University (Greenville, SC/ USA) „Annotating and Editing With Canonical Text Services (CTS)“.
            Oliver Bräckel ist Althistoriker und arbeitet an einer Dissertation zu Politischen Flüchtlingen im Römischen Reich. Im Bereich der eHumanities hat er im Projekt „eCOMPARATIO“ am ICE der Universität Erfurt sowie im Projekt eXChange an der Uni Leipzig mitgearbeitet. Derzeit arbeitet er im von der Andrew W. Mellon Foundation geförderten Kooperationsprojekt der Universität Leipzig und der Furman University (Greenville, SC/ USA) „Annotating and Editing With Canonical Text Services (CTS)“.
         
      
      
         
            
               Bibliographie
               
                  A. Olheiser, Have We Been Reading the Declaration of Independence All Wrong?, 
                        
                            https://www.theatlantic.com/entertainment/archive/2014/07/typo-could-mean-weve- been-reading-the-declaration-of-independence-all-wrong/373915/
                         (letzter Zugang 7.7.2017).
                    
               
                  P. Sahle, Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels, Norderstedt 2013, 3 Bde. (Schriften des Instituts für Dokumentologie und Editorik Bd. 7).
               
                  M.L. West, Textual Criticism and Editorial Technique, Stuttgart 1973.
               
                  Links:
               
               The Atlantic: https://www.theatlantic.com/entertain
                        ment/archive/2014/07/typo-could-mean-weve-been-reading-the-declaration-of-independence-all-wrong/373915/
                        (letzter Zugang 7.7.2017)
                    
               http://www.eaqua.net
               
                  http://www.ecomparatio.net/
               
               
                  http://www.ecomparatio.net/~khk/instanzen/ecompp/ (letzter Zugang17.9.2017)
               http://digital-plato.org
               http://digital-classics-online.eu
               
                  http://cite-architecture.github.io/cts/
                    
            
         
      
   


10826	2018	
      
         Ziel dieses Posters ist es, anhand von 32 deutschsprachigen Dramen in die Netzwerkanalyse literarischer Texte einzuführen, eine didaktische Intervention für eine zwar mittlerweile etablierte Methode der literaturwissenschaftlichen Analyse, die aber nicht immer genügend reflektiert wird: Der Errechnung teils komplexer netzwerktheoretischer Maße entspricht nicht immer ein entsprechender Sprung zur Bedeutungsebene. Was bedeutet es zum Beispiel wirklich, dass die durchschnittliche Pfadlänge in Goethes »Faust. Der Tragödie erster Theil« genau 1,79 beträgt? Wenn man jedoch diesen Wert in Beziehung zu entsprechenden Werten anderer Stücke setzt, gewinnt er an komparatistischer Bedeutung. Die Anschaulichkeit der Wert und ihre spielerisch erfahrene Dimensionierung ermöglichen so die Einübung in die strukturalistische Betrachtung von Netzwerken am Beispiel von Dramen, wobei damit zugleich kulturelles Grundwissen über die Strukturation von Netzwerken – immerhin ubiquitäre Gegenstände der sozialen und technischen Welt – erworben werden kann. 
         Um den komparatischen Blick im Kontext der literaturwissenschaftlichen Netzwerkanalyse zu schulen, setzen wir mit unserem Poster auf einen Gamification-Ansatz. Anders als bei unserem ersten Experiment in dieser Richtung – der auf der DHd2016 präsentierten Android-App »Play(s)« (vgl. Göbel/Meiners 2016), in deren Mittelpunkt die spielerische Korrektur und Anreicherung unserer Korpusdaten stand –, handelt es sich diesmal um eine nicht-technische Anwendung, die auf spielerische Weise netzwerkanalytisches Datenmaterial explorierbar macht.
         Dabei wird das Posterformat in zweierlei Hinsicht bespielt: Das Poster ist einerseits eine Datenvisualisierung auf Grundlage eines selbst gepflegten größeren Dramenkorpus. Andererseits ist es ein in 32 Teile zerlegbares Dramenquartett, das spielerisch mit den Bedeutungshorizonten verschiedener netzwerktheoretischer Größen bekannt macht und ein Bewusstsein für komparatistische Möglichkeiten trainiert. Dieser Ansatz ist in den Geisteswissenschaften nicht neu, verwiesen sei etwa auf das architekturgeschichtliche Quartettspiel »Plattenbauten. Berliner Betonerzeugnisse« (Mangold u. a. 2001), in dem technische Daten verschiedener Plattenbautypen gegenübergestellt werden (vgl. auch Richter 2006).
         Die Didaxe des Dramenquartetts bezieht sich auf mehrere Dimensionen: eine literaturgeschichtliche, eine quantitative, eine netzwerktheoretische. Die 32 Stücke bilden einen Minimalkanon, der von der Zeit der Gottschedischen Theaterreformen bis in die Moderne reicht. Statt der lexikonartigen Beschreibung eines solchen Kanons (wie etwa im »Dramenlexikon des 18. Jahrhunderts«, Hollmer/Meier 2001), besteht das Beschreibungsinstrument hier in visuellen und quantitativen Werten, die Vergleichbarkeit herstellen – erst dieser Umstand vereint die verschiedenen Karten zu einem kompetitiven Spiel.
         Als visueller Catch der Quartettkarten dient eine Visualisierung des jeweiligen extrahierten sozialen Netzwerks (vgl. Fischer u. a. 2016). Die weiteren Informationen auf den Karten setzen sich aus (Kanonwissen präsentierenden) Metadaten (Autor*in – Titel – Untertitel – Genre – Jahr) und vor allem aus statischen und dynamischen Netzwerkdaten zusammen (Anzahl von Subgraphen – Netzwerkgröße – Netzwerkdichte – Clustering-Koeffizient – Durchschnittliche Pfadlänge – Höchster Degreewert und Name der entsprechenden Figur –), wie sie im Rahmen des dlina-Projekts berechnet wurden.
                 Das Deckblatt enthält eine Einführung zum Projekt und seinen Hintergründen sowie Kurzdefinitionen der auf den einzelnen Karten enthaltenen netzwerktheoretischen Maßzahlen, die damit nicht nur spielerisch erkundet, sondern auch konzeptuell verstanden werden können. 
            
         Das Poster wird mit unserer Python-Skriptsammlung ›dramavis‹ generiert, die in der neuen Version 0.4 eine entsprechende Funktion erhalten hat (Kittel/Fischer 2017). Für das Konferenzposter haben wir einen Fallback-Kanon zusammengestellt (Stücke von Johann Christoph und Luise Adelgunde Victorie Gottsched, von Gellert, J. E. Schlegel, Caroline Neuber, Klopstock, Lessing, Gerstenberg, Goethe, Lenz, Klinger, Schiller, Kotzebue, Kleist, Zacharias Werner, Müllner, Grillparzer, Grabbe, Büchner, Hebbel, Gustav Freytag, Anzengruber, Arno Holz, Wedekind, Schnitzler, Erich Mühsam). Über eine individualisierbare Kanon-Datei können aber auch eigene Quartette zusammengestellt werden, sodass sich etwa auch epochenspezifische Sets (Dramen der Aufklärung, Dramen der Klassik, Romantische vs. Klassische Dramen, Dramen des Sturm und Drang vs. Dramen des Naturalismus) oder gattungsspezifische Sets erstellen lassen. 
         Auf der Konferenz werden wir neben einem Poster, das das didaktisch-interventionistische Konzept veranschaulicht, auch diverse Quartett-Sets präsentieren. 
      
      
         
             Vgl. das Blog 
                        https://dlina.github.io/ und das Github-Repo 
                        https://github.com/dlina.
                    
         
         
            
               Bibliographie
               
                  Fischer, Frank / Göbel, Mathias / Kampkaspar, Dario / Kittel, Christopher / Trilcke, Peer (2016): “Distant-Reading Showcase. 200 Years of Literary Network Data at a Glance”, DHd2016, Leipzig. DOI: https://dx.doi.org/10.6084/m9.figshare.3101203.v1
                        >.
                    
               
                  Göbel, Matthias / Meiners, Hanna-Lena: Play(s): Crowdbasierte Anreicherung eines literarischen Volltext-Korpus”. DHd2016, Leipzig.
                    
               
                  Hollmer, Holmer / Meier, Albert (eds.) (2011): Dramenlexikon des 18. Jahrhunderts. München: C.H. Beck.
                    
               
                  Kittel Christopher / Fischer, Frank: “dramavis v0.4” (September 2017). Repo: https://github.com/lehkost/dramavis
                        >.
                    
               
                  Mangold, Cornelius u.a. (2001): 
                        Plattenbauten. Berliner Betonerzeugnisse. Ein Quartettspiel. Berlin
                    
               
                  Richter, Peter (2006): 
                        Der Plattenbau als Krisengebiet. Die architektonische und politische Transformation industriell errichteter Wohngebäude aus der DDR am Beispiel der Stadt Leinefelde. Hamburg: Univ., Diss.
                    
            
         
      
   


10843	2019	
      
         
            Fragen
            Wie gut lassen sich Gattungen und Untergattungen durch Maschinelles Lernen über eine längere Periode erkennen? Obwohl eine Reihe von Artikeln die Frage hauptsächlich für Englisch (Kessler, Numberg, und Schütze 1997; Petrenz und Webber 2011; Underwood 2014) und Deutsch (Hettinger et al. 2016) beantwortet hat, befasst sich wenig Forschung mit diesem Thema aus einer diachronischen Perspektive oder wird auf spanischen Texte angewendet (Henny-Krahmer 2018). Welche Gattungen sind leichter zu erkennen, welche komplizierter? Welche Algorithmen, Transformationen und Anzahl der lexikalischen Einheiten funktionieren am besten?
         
         
            Datensatz: CORDE 1475-1975
            Zur Beantwortung ob verschiedene Gattungen durch Maschinelles Lernen erkannt werden können, wurde das umfangreichste historische Korpus des Spanischen analysiert, CORDE. Dieses Korpus wurde von der Real Academia Española kompiliert (Rojo Sánchez 2010; Sánchez Sánchez und Domínguez Cintas 2007) und ist ein standard-Tool in der Hispanistik über das online Such-Interface (Kabatek und Pusch 2011). Für die Analyse wurden die Frequenzen der Tokens und die Metadaten jedes Texts an Forscher weitergegeben. Das Korpus beinhaltet ca. 300 Millionen Tokens (34.000 Texte) und die Texte sind mit expliziten Metadaten über Jahrhunderte, Länder und Gattungen markiert.
            Die Daten der mittelalterlichen Sektion des Korpus präsentieren mehrere Probleme (Rodríguez Molina und Octavio de Toledo y Huerta 2017), wie beispielsweise ausgeprägte Unausgewogenheit der Anzahl der Texten im Vergleich zu anderen Jahrhunderten oder schwankende philologische Qualität. Deswegen wurden für diese Analyse nur die Texte der letzten 500 Jahre des Korpus selektiert, die länger als 100 Tokens sind. Somit beinhaltet das analysierte Korpus über 22.000 Texte (über 244 Millionen Tokens). Die Metadaten unterscheiden:
            
               Fachtexte in Themen (Jura, Geschichte, Geisteswissenschaften…)
               Gattungen und Untergattungen (lyrischer Vers, kurzer dramatischer Vers…)
               oder Medien (journalistische Texte, Briefe…).
            
            Eine komplette Liste der Gattungen ist auf den Abbildungen zu finden.
         
         
            Methoden der Evaluation
            Die Klassifikation wurde binarisiert durchgeführt, d. h. jeder Text könnte zu jeder Gattung gehören oder nicht. Verschiedene Parameter wurden evaluiert:
            
               Transformation der lexikalischen Information: relative Frequenz, binäre Frequenz, z-scores, TF-IDF, logarithmierte relative Frequenz
               Algorithmen: k-Nearest Neighbors, Random Forest, Logistic Regression und Support Vector Machine
               Anzahl der Tokens: 10, 50, 100, 500, 1.000, 2.000, 3.000, 4.000, 5.000 und 6.000
            
            Das Korpus wurde für jede Gattung undersampled: die gleiche Anzahl an positiven wie an negativen Fällen wurden für jede Gattung gesamplet. Die Evaluation wurde mit Hilfe von Cross-Validation (10 folds) durchgeführt und der Mittelwert der F1 Scores berechnet. Der Code wird als Python Notebook über GitHub zugänglich sein.
         
         
            Ergebnisse und Diskussion
            Die höchsten F1 Scores der Kombinationen von Parametern für jede Gattung lagen zwischen 0,9 und 1,0 mit einem Mittelwert der verschiedenen Gattungen von 0,96 (Standardabweichung von 0,03). Diese sehr hohen Ergebnisse ähneln sich denen von Underwood (2014), der an einem sehr großen Datensatz forschte. Die häufigsten Parameter bei den besten Ergebnissen waren Logistic Regression (16 Fälle von 27), binäre Häufigkeit (16, was nicht zu erwarten war) und 6.000 MFW (9).
            Auf den nächsten Boxplots sind die 10 besten Kombinationen zu sehen. Jeder Punkt entspricht dem Mittelwert der F1 Scores der Cross-Validation der 10 besten Kombinationen von Parametern. Diese sind nach Gattung differenziert aufgelistet:
            
               
                  
               
            
            Folgende Gattungen wurden am besten erkannt: Theater (Vers und Prosa), Romane, lyrischer Vers, und Fachtexte über Naturwissenschaften und Kunst. Lyrische Prosa zeigt heftige Schwankungen, außerdem wurden die niedrigsten Ergebnisse von folgenden Gattungen erreicht: Autobiografie, narrativer Vers, Essay, lyrische Prosa, Prosa sowie Fachtexte über Gesellschaft, Geschichte und Geisteswissenschaften.
            Ein interessanter Aspekt ist, welche die allgemeinen Tendenzen der Parameter und dessen Kombinationen sind. Dafür eignet sich ein Facet Grid Scatter Plot mit den Algorithmen als Spalten und den Transformationen als Reihen (einzelne Punkte entsprechen den Mittelwert der F1 Scores pro Gattung):
            
               
                  
               
            
            Hinsichtlich der Transformation (Reihen) zeigen die relative und die logarithmierte Häufigkeit niedrigere Ergebnisse als TF-IDF, z-scores und die binäre Häufigkeit. Bei den Algorithmen (Spalten) ist KNN merklich schlechter als die anderen drei. Zuletzt ist noch zu erkennen, dass die Qualität der Ergebnisse bis zu einer Anzahl von 2.000 Tokens zunimmt, und mit Schwankungen bis 6.000 stabil bleibt. Ein interessanter Aspekt ist die Tatsache, dass spezifische Kombinationen (SVC + TF-IDF, binäre + Logistic Regression, relative + Random Forest) von Vorteil im Vergleich zu anderen sind.
         
      
      
         
            Bibliographie
            
               Henny-Krahmer, Ulrike (2018): “Exploration of Sentiments and Genre in Spanish American Novels.” In DH Conference. Mexico City: ADHO.
                
            
               Hettinger, Lena / Reger, Isabella / Jannidis, Fotis / Hotho, Andreas (2016): “Classification of Literary Subgenres.” In DHd Konferenz, 154–58. Leipzig: Universität Leipzig.
                
            
               Kabatek, Johannes / Pusch, Claus D. (2011): Spanische Sprachwissenschaft: eine Einführung. Tübingen: Narr.
                
            
               Kessler, Brett/ Numberg, Geoffrey / Schütze, Hinrich (1997): “Automatic Detection of Text Genre.” In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, 32–38. ACL ’98. Stroudsburg, PA, USA: Association for Computational Linguistics.
                
            
               Petrenz, Philipp / Webber, Bonnie (2011): “Stable Classification of Text Genres.” Computational Linguistics 37 (2): 385–93.
                
            
               Rodríguez Molina, Javier / Octavio de Toledo y Huerta, Álvaro Sebastián (2017): “La imprescindible distinción entre texto y testimonio: el CORDE y los criterios de fiabilidad lingüística.” Scriptum digital: revista de corpus diacrònics i edició digital en llengües iberoromàniques, no. 6: 5–68.
                
            
               Rojo Sánchez, Guillermo (2010): “Sobre codificación y explotación de corpus textuales: Otra comparación del Corpus del español con el CORDE y el CREA.” Lingüística, no. 24: 11–50.
                
            
               Sánchez Sánchez, Mercedes / Domínguez Cintas, Carlos (2007): “El banco de datos de la RAE: CREA y CORDE.” Per Abbat: boletín filológico de actualización académica y didáctica, no. 2: 137–48.
                
            
               Underwood, Ted (2014): “Understanding Genre in a Collection of a Million Volumes, Interim Report.”
				
				
         
      
   


10867	2019	
      
         
            Einleitung
            Obwohl sich infrastrukturell einiges getan hat, sieht ein typischer Operationsmodus der digitalen Literaturwissenschaft immer noch so aus, dass eine bestimmte Forschungsmethode auf ein oft nur ephemeres Korpus angewandt wird. Im besten Fall ist das Ergebnis 
                    irgendwie reproduzierbar, im schlechtesten Fall gar nicht. Im besten Fall gibt es ein offen zugängliches Korpus in einem Standardformat wie TEI, einer anderen Markup-Sprache oder zumindest als txt-Datei. Im schlechtesten Fall ist das Korpus gar nicht zugänglich, d. h., die Forschungsergebnisse müssen einfach hingenommen werden.
                
            Doch seit kurzem gibt es Anzeichen, dass sich dies ändert. Einige Digital-Humanities-Projekte stellen Schnittstellen zu stabilen Korpora zur Verfügung, über die man mannigfaltige Zugriffsmöglichkeiten bekommt und reproduzierbar arbeiten kann. Eines dieser Projekte ist DraCor, eine offene Plattform zur Dramenforschung, die in diesem Vortrag vorgestellt werden soll (zugänglich unter 
                    bzw. über die Repos und verschiedene Schnittstellen). DraCor transformiert vorliegende Textsammlungen zu ›Programmable Corpora‹ – ein neuer Begriff, den wir mit diesem Vortrag ins Spiel bringen möchten.
                
         
         
            Die Bausteine
            
               Vanillekorpora
               Ähnlich wie die COST Action zu europäischen Romanen (Schöch et al. 2018), versucht das DraCor-Projekt als Basis für eine digitale Komparatistik einen Stamm an multilingualen Dramenkorpora aufzubauen, die in basalem TEI kodiert sind. Ein selbst betriebenes russischsprachiges 
                    () und ein deutschsprachiges Korpus 
                    () dienen dabei als Einstieg. Diese Korpora sind, ähnlich wie die Sammlung »Théâtre classique« von Paul Fièvre, im weitesten Sinne als Vanillekorpora angelegt, die über das notwendige Markup hinaus zunächst kaum weitere spezielle Auszeichnungen enthalten, allerdings frei zur Verfügung stehen und damit fork- und erweiterbar sind. Zur Demonstration, dass auch andere, reicher kodierte Korpora dazugebunden und sofort alle bereits bestehenden Extraktions- und Visualisierungsmethoden der Plattform angeboten werden können, wurden das Shakespeare Folger Corpus sowie das schwedische Dramawebben-Korpus geforkt und angedockt 
                    (bzw. 
                        ). Dramenkorpora in weiteren Sprachen sollen folgen; einzige Voraussetzung dabei ist jeweils, dass diese in TEI vorliegen.
                    
               Die Vorteile von frei auf GitHub gehosteten Korpora liegen auf der Hand. Unabhängig von den letztlich durch die Plattform zur Verfügung gestellten Schnittstellen können die Korpora alternativ durch Klonen oder andere Downloadmethoden, etwa über den SVN-Wrapper von GitHub, direkt bezogen und individuell weiterverarbeitet werden. Ein offen zugängliches GitHub-Repositorium heißt auch, dass Pull Requests zur Fehlerkorrektur und Forks für Erweiterungen möglich und erwünscht sind.
            
            
               XML-Datenbank (eXist-db) und Frontend
               DraCor als Plattform setzt auf die eXist-Datenbank, um die TEI-Dateien zu verarbeiten und Funktionen zur Beforschung der Korpora zur Verfügung zu stellen. Das Frontend wurde mit ReactJS gebaut, ist responsiv und einfach erweiterbar. Der Schwerpunkt liegt aber nicht auf der GUI, sondern auf der API (vgl. generell zur Unterscheidung zwischen beiden Schnittstellenansätzen Bleier/Klug 2018).
            
            
               API und Entwicklungsumgebung
               Um dem Ideal und der Möglichkeit nahe zu kommen, auf einfache Weise »alle Methoden auf alle Texte« anwenden zu können (Frank/Ivanovic 2018), braucht es mehr als offene Korpora. Der zitierte Text von Frank/Ivanovic macht sich hinsichtlich dessen für SPARQL-Endpunkte stark; auch DraCor bietet einen solchen an, besitzt darüber hinaus aber eine reiche API, die über Swagger dokumentiert und erläutert wird 
                    (). In einem Teilbereich der Korpusphilologie, den Digital Scholarly Editions, hat die Diskussion um eine proaktivere Nutzung von APIs bereits begonnen (zur Vorgeschichte vgl. wiederum Bleier/Klug 2018), als Beispiel hierfür diene die Folger Digital Texts API 
                    (), über die man sich spezifische Querys zusammenbauen kann. Der Vorteil einer moderneren Lösung wie Swagger besteht darin, dass API-Querys live und direkt ausgeführt und die Outputs genauer kontrolliert und gesteuert werden können.
                    
               Ein einfaches Use-Case-Szenario sieht dann so aus, dass man etwa im RStudio mit zwei, drei Zeilen Code einen Blick in ein Korpus werfen kann, etwa über die zeitliche Entwicklung der Anzahl der Charaktere im russischen Drama zwischen 1740 und 1940, die in der Metadatentabelle festgehalten sind 
                    (). Diese Datei, beziehbar im JSON- oder CSV-Format, wird in eine Data.Table eingelesen, woraufhin die Werte zweier Spalten (Erscheinungsjahre und Number of Speakers) einfach über ggplot visualisiert werden können (Abb. 1).
                    
               
                  
                     
                     Abbildung 1: Anzahl der Charaktere pro Drama in chronologischer Ordnung (Quelle: RusDraCor).
                  Anhand dieses sehr simplen Beispiels zeigt sich dann recht deutlich, dass sich mit Puschkins an Shakespeare angelehntem historischen Drama »Boris Godunow« (1825), in dem Sprechakte von 79 Charakteren vorkommen, eine strukturelle Diversifizierung der russischen Dramenlandschaft Bahn bricht.
                    
               Die Möglichkeiten beschränken sich aber nicht darauf, vorgefertigte API-Funktionen zu benutzen. Neue Forschungsideen zeitigen immer auch neue Bedarfe an einfach bezieh- und reproduzierbaren Daten und Metriken; die API kann dementsprechend erweitert werden. Dies wird dadurch erleichtert, dass über Apache Ant die gesamte Entwicklungsumgebung auf dem eigenen System nachgebaut werden kann.
               Durch bereits implementierte Funktionen können neben Struktur- und Metadaten etwa auch Volltexte ohne Markup bezogen werden (auch Untermengen von Volltexten wie Regieanweisungen), etwa wenn Methoden wie die Stilometrie oder das Topic Modeling der Endzweck sind, also Methoden, die nach dem »bag of words«-Prinzip arbeiten, für das kein Markup vonnöten ist.
               Insgesamt wird durch den Aufbau und die Dokumentation offener APIs die bisher oft aufwendige Reproduzierbarkeit von Forschungsergebnissen erheblich erleichtert.
            
            
               Shiny App
               Ein Beispiel für die vielseitigen Nutzungsmöglichkeiten der DraCor-API ist die Shiny App, die Ivan Pozdniakov aufgesetzt hat 
                    (). Shiny ist ein auf R basierendes Framework, das es ermöglicht, interaktive Visualisierungen im Browser darzustellen. Die DraCor-Shiny-App tut genau dies und setzt dabei vollkommen auf die DraCor-API für den Datenbezug. So kann zu Lehr- und Forschungszwecken, aber auch zur einfacheren Datenkorrektur, auf Visualisierungen des aktuellen Datenbestandes zugegriffen werden.
                    
            
            
               Didaxe
               Das Markup oder andere Formalisierungen literarischer Texte sind nicht selbsterklärend. Zwar gibt es einige Standards, aber die jeweilige Operationalisierungslösung hängt von der Forschungsfrage ab. Allein das Extrahieren von Figurennetzwerkdaten ist auf viele Arten und Weisen möglich, was dazu führt, dass etwa alle von verschiedenen Forschungsgruppen extrahierten Netzwerke aus Shakespeares »Hamlet« zu leicht verschiedenen Ergebnissen kommen. Selbst für Dramen ist dies also schon ein nicht-trivialer Akt, von Romanen dann ganz zu schweigen (beispielhaft seien Grayson et al. 2016 genannt, die verschiedene Extraktionsmethoden für Romane durchtesten und die Ergebnisse vergleichen). Um diese Erkenntnis schon in der Lehre zu fördern, wurde das Tool »Easy Linavis« 
                    () entwickelt und in die DraCor-Toolchain integriert. Per Hand können Netzwerkdaten aus Texten extrahiert und dabei das Bewusstsein für die Kontingenz dieses Vorgangs geschärft werden, eine wichtige Vorstufe zur Operationalisierung.
                    
               Neben einem Ansatz zur Gamifizierung des TEI-Korrekturvorgangs (Göbel/Meiners 2016) haben wir für Lehrzwecke auch ein Dramenquartett entwickelt, um spielerisch das Verständnis von Netzwerkwerten zu trainieren (Fischer at al. 2018).
               Die aufgezählten, um die Plattform herumgruppierten didaktischen Mittel sind integraler Bestandteil des ganzen Projekts, da sie auf dessen Daten und Operationalisierungen aufsetzen. Wichtig dabei war die Erkenntnis, dass Daten mehrere Gestalten annehmen und für Forschung und Lehre gleichermaßen von Bedeutung sein können.
            
            
               Linked Open Data (LOD)
               Im TEI-Code sind PND- bzw. Wikidata-Identifier sowohl für Autor*innen als auch für die Werke hinterlegt. Auf diese Weise lassen sich verschiedene Realien, die außerhalb der eigenen Korpusarbeit liegen, hinzufügen. Eine automatisch erstellte Autor*innengalerie hat dabei noch eher illustrativen Charakter (de la Iglesia/Fischer 2016).
               Darüber hinaus kann man aber zum Beispiel feststellen, ob es nicht einen unbewussten regionalen Bias im Korpus gibt. Dafür lässt man sich über die Wikidata-Identifier die Verteilung der Geburts- und Sterbeorte der Autor*innen auf einer Karte anzeigen. So konnte dann für das deutschsprachige Korpus GerDraCor ausgeschlossen werden, dass es einen solchen Bias gibt, da sich die Orte relativ gleichmäßig über die (historisch) deutschsprachigen Gebiete verteilen (Göbel/Fischer 2015).
               Ebenso lässt sich über die Wikidata-ID der Stücke herausfinden, wo diese uraufgeführt worden sind (Beispiel-Query: 
                        ), d. h., Aspekte der Aufführungsgeschichte lassen sich zuschalten, obwohl diese gar nicht im Fokus des Kernprojekts liegen. Programmable Corpora verbinden sich also auch mit der Welt um sie herum, was sie u. a. von den nach innen gerichteten Workbenches der Korpuslinguistik unterscheidet.
                    
            
            
               Infrastruktur statt Rapid Prototyping
               Projekte wie DraCor versuchen nichts anderes als den digitalen Literaturwissenschaften eine verlässliche und ausbaufähige Infrastruktur zu geben, damit sie sich stärker auf eigentliche Forschungsfragen konzentrieren und reproduzierbare Ergebnisse hervorbringen können.
               Eine wichtige Folgerung für uns war, dass wir die Weiterentwicklung unserer seit vier Jahren entwickelten all-in-one Python-Skriptsammlung 
                        dramavis aufgeben und uns lieber der Arbeit an der API widmen. 
                        Dramavis (Kittel/Fischer 2014–2018 sowie Fischer et al. 2017) folgte dem in den Digital Humanities nicht untypischen Rapid Prototyping mit direkter Verarbeitung literarischer XML-Daten (Trilcke/Fischer 2018) und einer mittlerweile stark gewachsenen Codebasis, die alles auf einmal kann, deren Maintenance aber immer schwieriger geworden ist und oft genug von den eigentlichen Forschungsfragen weggeführt hat.
                    
            
         
         
            
               Fazit
                
            In Anlehnung an das Projekt »ProgrammableWeb« – das eine Datenbank von offenen APIs unterhält und dessen Slogan lautet: »APIs, Mashups and the Web as Platform« (zugänglich unter 
                    ) – schlagen wir für infrastrukturell-forschungsorientierte, offene, erweiterbare und LOD-freundliche Korpora den Begriff ›Programmable Corpora‹ vor.
                
            Programmable Corpora erleichtern es, Forschungsfragen auf viele Arten und Weisen um Korpora herum programmieren zu können. Es steht zu erwarten, dass sich infrastrukturelle Anstrengungen dieser Art für die gesamte Community auszahlen mit Effekten, wie sie John Womersley in seiner Präsentation auf der ICRI2018 in Wien aufgezählt hat: a) dramatically increase scientific reach; b) address research questions of long duration requiring pooled effort; c) promote collaboration, interdisciplinarity, interaction.
            Der Anschlussmöglichkeiten sind viele, egal ob man gar nicht programmieren möchte, sondern nur eine GEXF-Datei für Gephi benötigt, ob ein Korpus über seine Verbindungen zur Linked Open Data Cloud beforscht oder einfach aus R oder Python heraus bestimmte Daten bezogen werden sollen, ohne dass man sich mit dem Korpus und dessen Maintenance und Reproduzierbarkeit selbst kümmern muss (all dies bleibt natürlich aber eine Option). Programmable Corpora erleichtern die Entscheidung, auf welcher Ebene der eigene Forschungsprozess einsetzt.
         
      
      
         
            
               Bibliographie
               
                  Bleier, Roman / Klug, Helmut W. (2018): Discussing Interfaces in Digital Scholarly Editing. In: Digital Scholarly Editions as Interfaces. BoD, Norderstedt, S. V–XV. URL: 
               
               
                  de la Iglesia, Martin / Fischer, Frank (2016): The Facebook of German Playwrights. URL: 
               
               
                  Fischer, Frank / Dazord, Gilles / Göbel, Mathias / Kittel, Christopher / Trilcke, Peer (2017): Le drame comme réseau de relations. Une application de l‘analyse automatisée pour l’histoire littéraire du théâtre. In: Revue d'historiographie du théâtre. № 4. URL: 
               
               
                  Fischer, Frank / Kittel, Christopher / Milling, Carsten / Schultz, Anika / Trilcke, Peer / Wolf, Jana (2018): Dramenquartett – Eine didaktische Intervention. In: Konferenzabstracts zur DHd2018, Universität zu Köln. S. 397 f. DOI: 
               
               
                  Göbel, Mathias / Fischer, Frank (2015): The Birth and Death of German Playwrights. URL: 
               
               
                  Göbel, Mathias / Meiners, Hanna-Lena (2016): Play(s): Crowdbasierte Anreicherung eines literarischen Volltext-Korpus. In: Konferenzabstracts zur DHd2016, Bern/CH. S. 140–143. URL: http://www.dhd2016.de/abstracts/vortr%C3%A4ge-007.html
               
               
                  Grayson, Siobhán / Wade, Karen / Meaney, Gerardine / Greene, Derek (2016): The Sense and Sensibility of Different Sliding Windows in Constructing Co-Occurrence Networks from Literature. In: 2nd IFIP International Workshop on Computational History and Data-Driven Humanities. Trinity College Dublin 2016. PDF: 
               
               
                  Kittel, Christopher / Fischer, Frank (2014–2018): dramavis. Python-Skriptsammlung. Repo: 
               
               
                  Schöch, Christoph et al. (2018): Distant Reading for European Literary History. A COST Action [Poster]. In: DH2018: Book of Abstracts / Libro de resúmenes. Mexico: Red de Humanidades Digitales A. C. URL: https://dh2018.adho.org/en/?p=11345
               
               
                  Trilcke, Peer / Fischer, Frank (2018): Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen. In: Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden. Hrsg. von Martin Huber und Sybille Krämer (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3). DOI: 
               
            
         
      
   


10882	2019	
      
         
            Einleitung
            Das Ziel dieses Tutorials ist es, den Teilnehmerinnen und Teilnehmern konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten. Es werden dabei keinerlei Vorkenntnisse zu maschinellem Lernen oder Programmierkenntnisse vorausgesetzt.
            Es gibt keinen Grund, den Ergebnissen von maschinellen Lernverfahren im Allgemeinen und NLP-Tools im Besonderen blind zu vertrauen. Durch die konkreten Einblicke in den "Maschinenraum" von maschinellen Lernverfahren wird den Teilnehmenden ermöglicht, das Potenzial und die Grenzen statistischer Textanalysewerkzeuge realistischer einzuschätzen. Mittelfristig hoffen wir dadurch, den immer wieder auftretenden Frustrationen beim Einsatz automatischer Verfahren für die Textanalyse und deren teilweise wenig zufriedenstellender Ergebnis-Daten zu begegnen, aber auch die Nutzung und Interpretation der Ergebnisse von maschinellen Lernverfahren (d.h. in erster Linie von automatisch erzeugten Annotationen) zu fördern. Zu deren adäquater Nutzung, etwa in hermeneutischen Interpretationsschritten, ist der Einblick in die Funktionsweise der maschinellen Methoden unerlässlich. Insbesondere ist die Art und Herkunft der Trainingsdaten für die Qualität der maschinell produzierten Daten von Bedeutung, wie wir im Tutorial deutlich machen werden.
            Neben einem Python-Programm für die automatische Annotierung von Entitätenreferenzen, mit und an dem während des Tutorials gearbeitet werden wird, stellen wir ein heterogenes, manuell annotiertes Korpus sowie die Routinen zur Evaluation und zum Vergleich von Annotationen zu Verfügung. Das Korpus enthält Entitätenreferenzen, die im "Center for Reflected Text Analytics" 
                (CRETA) in den letzten drei Jahren annotiert wurden, und deckt Texte verschiedener Disziplinen und Sprachstufen ab.
                
         
         
            
               Entitätenreferenzen
                
            Als empirisches Phänomen befassen wir uns mit dem Konzept der Entität und ihrer Referenz. Das Konzept steht für verschiedene linguistische und semantische Kategorien, die im Rahmen der Digital Humanities von Interesse sind. Es ist bewusst weit gefasst und damit anschlussfähig für verschiedene Forschungsfragen aus den geistes- und sozialwissenschaftlichen Disziplinen. Auf diese Weise können unterschiedliche Perspektiven auf Entitäten berücksichtigt werden. Insgesamt werden in den ausgewählten Texten fünf verschiedene Entitätenklassen betrachtet: PER (Personen/Figuren), LOC (Orte), ORG (Organisationen), EVT (Ereignisse) und WRK (Werke).
            Unter Entitätenreferenzen verstehen wir Ausdrücke, die auf eine Entität in der realen oder fiktiven Welt referieren. Das sind zum einen Eigennamen (Named Entities, z.B. "Peter"), zum anderen Gattungsnamen (z.B. "der Bauer"), sofern diese sich auf eine konkrete Instanz der Gattung beziehen. Dabei wird als Referenzausdruck immer die maximale Nominalphrase (inkl. Artikel, Attribut) annotiert. Pronominale Entitätenreferenzen werden hingegen nicht annotiert.
            In 
                    literarischen Texten sind vor allem Figuren und Räume als grundlegende Kategorien der erzählten Welt von Interesse. Über die Annotation von Figurenreferenzen können u.a. Figurenkonstellationen und -relationen betrachtbar gemacht sowie Fragen zur Figurencharakterisierung oder Handlungsstruktur angeschlossen werden. Spätestens seit dem 
                    spatial turn rückt auch der Raum als relevante Entität der erzählten Welt in den Fokus. Als "semantischer Raum" (Lotmann, 1972) übernimmt er eine strukturierende Funktion und steht in Wechselwirkung mit Aspekten der Figur.
                
            In den 
                    Sozialwissenschaften sind politische Parteien und internationale Organisationen seit jeher zentrale Analyseobjekte der empirischen Sozialforschung. Die Annotation der Entitäten der Klassen ORG, PER und LOC in größeren Textkorpora ermöglicht vielfältige Anschlussuntersuchungen, unter anderem zur Sichtbarkeit oder Bewertung bestimmter Instanzen, beispielsweise der Europäischen Union.
                
         
         
            
               Textkorpus
                
            Die Grundlage für (überwachte) maschinelle Lernverfahren bilden Annotationen. Um die Annotierung von Entitätenreferenzen automatisieren zu können, bedarf es Textdaten, die die Vielfalt des Entitätenkonzepts abdecken. Bei diesem Tutorial werden wir auf Annotationen zurückgreifen, die im Rahmen von CRETA an der Universität Stuttgart entstanden sind (vgl. Blessing et al., 2017; Reiter et al., 2017a). Das Korpus enthält literarische Texte aus zwei Sprachstufen des Deutschen (Neuhochdeutsch und Mittelhochdeutsch) sowie ein sozialwissenschaftliches 
                Teilkorpus.
            
            Der 
                    Parzival
                Wolframs von Eschenbach ist ein arthurischer Gralroman in mittelhochdeutscher Sprache, entstanden zwischen 1200 und 1210. Der 
                    Parzival zeichnet sich u.a. durch sein enormes Figureninventar und seine komplexen genealogischen Strukturen aus, wodurch er für Analysen zu Figurenrelationen von besonderem Interesse ist. Der Text ist in 16 Bücher unterteilt und umfasst knapp 25.0000 Verse.
                
            
               Johann Wolfgang von Goethes 
               Die Leiden des jungen Werthers ist ein Briefroman aus dem Jahr 1774. Unsere Annotationen sind an einer überarbeiteten Fassung von 1787 vorgenommen und umfassen die einleitenden Worte des fiktiven Herausgebers sowie die ersten Briefe von Werther an seinen Freund Wilhelm.
                
            Das 
                    Plenardebattenkorpus des deutschen Bundestages besteht aus den von Stenografinnen und Stenografen protokollierten Plenardebatten des Bundestages und umfasst 1.226 Sitzungen zwischen 1996 und 
                    2015. Unsere Annotationen beschränken sich auf Auszüge aus insgesamt vier Plenarprotokollen, die inhaltlich Debatten über die Europäische Union behandeln. Hierbei wurde pro Protokoll jeweils die gesamte Rede eines Politikers bzw. einer Politikerin annotiert.
                
         
         
            
               Ablauf
                
            Der Ablauf des Tutorials orientiert sich an sog. 
                    shared tasks aus der Computerlinguistik, wobei der Aspekt des Wettbewerbs im Tutorial vor allem spielerischen Charakter hat. Bei einem traditionellen 
                    shared task arbeiten die teilnehmenden Teams, oft auf Basis gleicher Daten, an Lösungen für eine einzelne gestellte Aufgabe. Solch eine definierte Aufgabe kann z.B. 
                    part of speech-tagging sein. Durch eine zeitgleiche Evaluation auf demselben Goldstandard können die entwickelten Systeme direkt verglichen werden. In unserem Tutorial setzen wir dieses Konzept live und vor Ort um.
                
            Zunächst diskutieren wir kurz die zugrundeliegenden Texte und deren Annotierung. Annotationsrichtlinien werden den Teilnehmerinnen und Teilnehmern im Vorfeld zur Verfügung gestellt. Im Rahmen der Einführung wird auch auf die konkrete Organisation der Annotationsarbeit eingegangen, so dass das Tutorial als Blaupause für zukünftige Tätigkeiten der Teilnehmenden in diesem und ähnlichen Arbeitsfeldern dienen kann.
            Die Teilnehmerinnen und Teilnehmer versuchen selbständig und unabhängig voneinander, eine Kombination aus maschinellen Lernverfahren, Merkmalsmenge und Parametersetzungen zu finden, die auf einem neuen, vom automatischen Lernverfahren ungesehenen Datensatz zu den Ergebnissen führt, die dem Goldstandard der manuellen Annotation am Ähnlichsten sind. Das bedeutet konkret, dass der Einfluss von berücksichtigten Features (z.B. Groß- und Kleinschreibung oder Wortlänge) auf die Erkennung von Entitätenreferenzen empirisch getestet werden kann. Dabei sind Intuitionen über die Daten und das annotierte Phänomen hilfreich, da simplem Durchprobieren aller möglichen Kombinationen („brute force'“) zeitlich Grenzen gesetzt sind.
            Wir verzichten bewusst auf eine graphische Benutzerschnittstelle (vgl. Reiter et al., 2017b) – stattdessen editieren die Teilnehmerinnen und Teilnehmer das (Python)-Programm direkt, nach einer Einführung und unter Anleitung. Vorkenntnisse in Python sind dabei nicht nötig: Das von uns zur Verfügung gestellte Programm ist so aufgebaut, dass auch Python-Neulinge relativ schnell die zu bearbeitenden Teile davon verstehen und damit experimentieren können. Wer bereits Erfahrung im Python-Programmieren hat, kann fortgeschrittene Funktionalitäten des Programms verwenden.
            Wie am Ende jedes maschinellen Lernprozesses wird auch bei uns abschließend eine Evaluation der automatisch generierten Annotationen durchgeführt. Hierfür werden den Teilnehmerinnen und Teilnehmern nach Ablauf einer begrenzten Zeit des Experimentierens und Testens (etwa 60 Minuten) die finalen, vorher unbekannten Testdaten zur Verfügung gestellt. Auf diese Daten werden die erstellten Modelle angewendet, um automatisch Annotationen zu erzeugen. Diese wiederum werden dann mit dem Goldstandard verglichen, wobei die verschiedenen Entitätenklassen sowie Teilkorpora getrennt evaluiert werden. Auch das Programm zur Evaluation stellen wir bereit.
         
         
            
               Lernziele
                
            Am hier verwendeten Beispiel der automatischen Annotation von Entitätenreferenzen demonstrieren wir, welche Schritte für die Automatisierung einer Textanalyseaufgabe mittels maschinellen Lernverfahren nötig sind und wie diese konkret implementiert werden können. Die Teilnehmenden des Workshops bekommen einen zusammenhängenden Überblick von der manuellen Annotation ausgewählter Texte über die Feinjustierung der Lernverfahren bis zur Evaluation der Ergebnisse. Die vorgestellte Vorgehensweise für den gesamten Ablauf ist grundsätzlich auf ähnliche Projekte übertragbar.
            Das Tutorial schärft dabei das Verständnis für den Zusammenhang zwischen untersuchtem Konzept und den dafür relevanten Features, die in ein statistisches Lernverfahren einfließen. Durch Einblick in die technische Umsetzung bekommen die Teilnehmerinnen und Teilnehmer ein Verständnis für die Grenzen und Möglichkeiten der Automatisierung, das sie dazu befähigt, zum einen das Potenzial solcher Verfahren für eigene Vorhaben realistisch(er) einschätzen zu können, zum anderen aber auch Ergebnisse, die auf Basis solcher Verfahren erzielt wurden, angemessen hinterfragen und deuten zu können.
                    
            
            
               Zeitplan:
               (Dauer in Minuten, ca.)
                
            Im Vorfeld der Veranstaltung: Installationsanweisungen und Support
            
               (10) Lecture
                        
                     Intro & Ablauf
                  
               
               (15) Hands-On
                        
                     Test der Installation bei allen
                  
               
               (50) Lecture
                        
                     Einführung in Korpus und Annotationen
                     Grundlagen maschinellen Lernens
                     Überblick über das Skript (where can you edit what?)
                                
                           Grundlagen Python Syntax
                           Bereitgestellte Features
                        
                     
                  
               
               (15) Hands-On
                        
                     Erste Schritte
                  
               
               (30) Kaffeepause
               (60) Hands-On
                        
                     Hack
                  
               
               (30) Evaluation
            
         
         
            Beitragende (Kontaktdaten und Forschungsinteressen)
            Der Workshop wird ausgerichtet von Mitarbeitenden des "Center for Reflected Text Analytics" (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine "black box" sein, sondern auch für Nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird.
                    
            
            
               Gerhard Kremer
               
                  gerhard.kremer@ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
                        Pfaffenwaldring 5b
                        70569 Stuttgart
                    
               Der Interessenschwerpunkt Gerhard Kremers ist der reflektierte Einsatz von Werkzeugen der Computerlinguistik für geistes- und sozialwissenschaftliche Fragestellungen. Damit zusammenhängend gehören die Entwicklung übertragbarer Arbeitsmethoden und die angepasste, nutzerfreundliche Bedienbarkeit automatischer linguistischer Analysetools zu seinen Forschungsthemen.
                        
               
            
            
               Kerstin Jung
               
                  kerstin.jung
                  @ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
                        Pfaffenwaldring 5b
                        70569 Stuttgart
                    
               Kerstin Jungs Forschungsinteressen liegen im Bereich der Nachhaltigkeit von (computer)linguistischen Ressourcen und Abläufen sowie der Verlässlichkeitsbeschreibung von automatisch erzeugten Annotationen. Dabei verfolgt sie einen aufgabenbasierten Ansatz und arbeitet an der Schnittstelle zwischen Computerlinguistik und anderen sprach- und textverarbeitenden Disziplinen.
            
         
         
            
               Zahl der möglichen Teilnehmerinnen und Teilnehmer
                
            Zwischen 15 und 25.
         
         
            
               Benötigte technische Ausstattung
                
            Es wird außer einem Beamer keine besondere technische Ausstattung benötigt. Es sollte sich um einen Raum handeln, in dem es möglich ist, den Teilnehmenden über die Schulter zu blicken und durch die Reihen zu gehen.
         
      
      
         
            
               www.creta.uni-stuttgart.de
            
            
                    Aus urheberrechtlichen Gründen wird das Tutorial ohne das Teilkorpus zu Adornos ästhetischer Theorie stattfinden, das in den Publikationen erwähnt wird.
                
            
                    Die Texte wurden im Rahmen des PolMine-Projekts verfügbar gemacht: http://polmine.sowi.uni-due.de/polmine/
                
         
         
            
               Bibliographie
               
                  Kuhn, Jonas / Reiter, Nils (2015): "A Plea for a Method-Driven Agenda in the Digital Humanities" in: Digital Humanities 2015: Conference Abstracts, Sydney.
                    
               
                  Reiter, Nils / Blessing, André / Echelmeyer, Nora / Koch, Steffen / Kremer, Gerhard / Murr, Sandra / Overbeck, Maximilian / Pichler, Axel (2017a): "CUTE: CRETA Unshared Task zu Entitätenreferenzen" in Konferenzabstracts DHd2017, Bern.
                    
               
                  Reiter, Nils / Kuhn, Jonas / Willand, Marcus (2017b): "To GUI or not to GUI?" in Proceedings of INFORMATIK 2017, Chemnitz.
                    
               
                  Blessing, André / Echelmeyer, Nora / John, Markus / Reiter, Nils (2017): "An end-to-end environment for research question-driven entity extraction and network analysis" in Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, Vancouver.
                    
               
                  Lotman, Juri (1972): Die Struktur literarischer Texte, München.
                    
            
         
      
   


10885	2019	
      
         Das Zedlersche 
                Universal-Lexicon (17-1754) stellt mit seinen 68 Bänden eine wichtige Quelle für die Erforschung des 18. Jahrhunderts dar. Durch das Portal „Zedler-Lexikon.de“ wurde dieses „bedeutendste Monument des enzyklopädischen Schreibens im Zeitalter der Aufklärung“ (Schneider 2013, S. 9) in einem Kooperationsprojekt zwischen der Herzog August Bibliothek Wolfenbüttel und der Bayerischen Staatsbibliothek München von 2004 bis 2007 digital erschlossen (siehe: Dorn et al. 2008). Entsprechend wurde dieser Webauftritt zu einer wichtigen digitalen Quelle, was sich in hohen Zugriffszahlen widerspiegelt (Dorn et al. 2008, S. 100). Daher erschien es grundlegend angebracht, diesen digitalen Zugriff auf die analoge Quelle einer gründlichen Kritik zu unterziehen.
            
         Das vorgeschlagene Poster präsentiert die Ergebnisse dieser Quellenkritik. Bei einem Abgleich der Anzahl der digitalen Scanbilder mit der gedruckten Seitenzählung zeigte sich, dass teilweise bis zu 180 Seiten pro Band „fehlten“. Die fehlende Seitenzahl geht nach Überprüfung am Original jedoch auf Fehler im Buchdruck zurück und bildet ein deutliches Muster ab, dass die Tätigkeit der verschiedenen Redakteure (Jakob August Franckenstein 1731-32, Paul Daniel Longolius 1733-35 und Carl Günther Ludovici 1738-54) abbildet. 
         
            
         
         So zeigen deutliche Variationen in der Seitenzählung (Auslassung und Doppelzählung von Seiten) die anfänglichen Schwierigkeiten im verteilten Druck zwischen Halle, Leipzig und Delitzsch (1731-36) und die deutliche Professionalisierung des Drucks nach der Übernahme der Redaktion durch Carl Günther Ludovici ab 1739.
         Dies ist für die Forschung interessant, da die Entstehungsbedingungen trotz bereits beachtlicher Erfolge (Calov 2007, Haug 2007, Löffler 2007, Prodöhl 2005, Quedenbaum 1977) immer noch viele Fragen aufwerfen. Zusätzlich zeigt das Poster Ergebnisse einer Analyse von Metadaten, die mittels eines Python Scripts von Zedler-Lexikon.de abgerufen wurden:
         Kamen beispielsweise im ersten Band auf jeden Artikel nur etwa 0,3 Verweise, so stieg diese bis zum letzten Band auf rund 2 Verweise pro Artikel an. Darin zeigt sich ein zunehmender Anspruch an die Nutzerfreundlichkeit des Universal-Lexicon von Seiten der Redakteure. Weiters zeigt eine Untersuchung der Länge der einzelnen Artikel einen deutlichen Wandel vom knappen Konversationslexikon zur umfangreichen Enzyklopädie. Der Begriff »Enzyklopädie« bezeichnet in diesem Kontext enzyklopädische Lexika, eine Bedeutung, die sich ab der zweiten Hälfte des 18. Jahrhunderts durch die Vorbildwirkung der Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers herausbildete. Die ältere Bedeutung von »Enzyklopädie« als systematische Wissensordnung trifft auf das UL nicht zu. (Vergleiche hierzu ausführlich Dierse 1977).
         Klar erkennbar wird hier jedoch, dass die durchschnittliche Länge der Artikel pro Band von 1732 bis 1754 stetig zunimmt und um 1754 durchschnittlich die 8-fache Länge gegenüber 1732 erreicht. 
         
            
         
         Hier zeigt sich vor allem in den sehr langen Artikeln der letzten Bände (wie Wien mit 134 Seiten, Wolfische Philosophie mit 174 Seiten oder Zunft mit 54 Seiten) nicht nur der Anspruch, Themen nun in größerem Detailgrad darzustellen, sondern auch ein finanzielles Motiv der Herausgeber. So erschienen von 1747-1750 noch acht Bände zum Buchstaben „W“ und nochmal fünf Bände zum Buchstaben „Z“, wohl um die profitablen Zahlungen der Subskribenten noch möglichst lange zu nutzen.
         Die Zusammenführung dieser und weiterer erhobenen Metadaten mit der wechselvollen Geschichte des 
                Universal-Lexicon, wie sie die bisherige Forschungsliteratur rekonstruieren konnte, ermöglichte es, viele Ergebnisse der historischen Forschung weitgehend zu bestätigen. Gleichzeitig mahnen die Erkenntnisse damit zu einer kritischen Verwendung des 
                Universal-Lexicons, da dieses in den 23 Jahren seiner Entstehung großen inhaltlichen Wandlungsprozessen unterzogen war und daher Artikel aus dem Band A-Am (1732) anders bewertet werden müssen als Artikel aus dem Band Zm-Zz (1750) oder gar aus den vier Supplementbänden (1751-54).
            
         Der Einsatz digitaler Quellenkritik an Zedler-Lexikon.de führte in diesem Fall nicht nur zu einer positiven Bestätigung der digitalen Repräsentation des Werkes, sondern lieferte auch Erkenntnisse zum analogen Werk, die ohne die vorhergehende Digitalisierung und den Einsatz entsprechender Methoden nicht realisierbar gewesen wäre.
      
      
         
            
               Bibliographie
               
                  Calov, Carla (2007): Quellen zu Johann Heinrich Zedler und seinem Lexikon im Stadtarchiv Leipzig. In: Leipziger Jahrbuch zur Buchgeschichte 16, S. 203–244.
					
               
                  Dierse, Ulrich: Enzyklopädie. Zur Geschichte eines philosophischen und wissenschaftstheoretischen Begriffs. Band. 2. Bonn 1977.
					
               
                  Dorn, Nico / Oetjens, Lena / Schneider, Ulrich Johannes (2008): Die sachliche Erschließung von Zedlers "Universal-Lexicon". Einblicke in die Lexicographie des 18. Jahrhunderts. In: Das achtzehnte Jahrhundert 32 (1), S. 96–125.
					
               
                  Haug, Christine (2007): Das "Universal-Lexicon" des Leipziger Verlegers Johann Heinrich Zedler im politischen Konfliktfeld zwischen Sachsen und Preußen. In: Leipziger Jahrbuch zur Buchgeschichte 16, S. 301–331.
					
               
                  Jürgens, Hanco / Lüsebrink, Hans-Jürgen (2017): Enzyklopädismus und Ökonomie im Aufklärungszeitalter. Zur Einführung. In: Das achtzehnte Jahrhundert 41 (2), S. 197–202.
					
               
                  Löffler, Katrin (2007): Wer schrieb den Zedler? Eine Spurensuche. In: Leipziger Jahrbuch zur Buchgeschichte 16, S. 265–284.
					
               
                  Lohsträter, Kai / Schock, Flemming (Hg.) (2013): Die gesammelte Welt : Studien zu Zedlers "Universal-Lexicon" ; [Ergebnisse einer internationalen Tagung, die im November 2010 in der Herzog-August-Bibliothek Wolfenbüttel stattgefunden hat]. Wiesbaden: Harrassowitz (Schriften und Zeugnisse zur Buchgeschichte).
					
               
                  Prodöhl, Ines (2005): "Aus denen besten Scribenten." Zedlers "Universal Lexicon" im Spannungsfeld zeitgenössischer Lexikonproduktion. In: Das achtzehnte Jahrhundert 29 (1), S. 82–94.
					
               
                  Quedenbaum, Gerd (1977): Der Verleger und Buchhändler Johann Heinrich Zedler 1706-1751. Ein Buchunternehmer in den Zwängen seiner Zeit ; ein Beitrag zur Geschichte des deutschen Buchhandels im 18. Jahrhundert. Hildesheim u.a.: Olms.
					
               
                  Schneider, Ulrich Johannes (2013): Die Erfindung des allgemeinen Wissens : enzyklopädisches Schreiben im Zeitalter der Aufklärung. Berlin: Akademie-Verlag.
					
            
         
      
   


10901	2019	
      
         Das Projekt „Paleocoran“ untersucht Koran-Handschriftenfragmente aus dem siebten bis zehnten Jahrhundert aus der Amr ibn al-ʿĀs- Moschee in al-Fuṣtāt (Alt-Kairo), die heute in verschiedenen Sammlungen weltweit verteilt aufbewahrt werden. Paleocoran sammelt kodikologische (buchgeschichtliche) und paläographische (schriftgeschichtliche) Daten zu den Handschriftenfragmenten, um die zu rekonstruieren, welche Fragmente ursprünglich einen Kodex gebildet haben. Die digitale Rekonstruktion der Kodizes – einem Puzzle mit ca. 25.000 Teilen zu vergleichen – wird durch IIIF digital ermöglicht.
         Das von F. Déroche (Paris) und M. Marx (Potsdam) geleitete DFG-ANR-Projekt „Paleocoran“ greift inhaltlich und methodisch an das Akademievorhaben „Corpus Coranicum“ der BBAW an: Datensätze aus den Corpus-Coranicum-Datenbanken zu Handschriften und Koran-Textvarianten werden in einem System zur wortgenauen Textstellenverortung von Koranpassagen verwendet. Die in „Paleocoran“ und „Corpus Coranicum“ generierten philologischen Daten werden in einem web-basierten System auf Grundlage des PHP-Frameworks Laravel in einer MySQL-Datenbank aufgezeichnet.
         Für die Rekonstruktion der Korankodizes aus Alt-Kairo werden die Seiten der einzelnen Fragmente in der Datenbank erfasst und mit Textstellenkoordinaten (Sure-Vers-Wort) ausgezeichnet. Derzeit befinden sich ca. 1100 Koranhandschriften mit insgesamt über 25.000 Manuskriptseiten in der aus mehr als 40 Sammlungen Corpus Coranicum-Datenbank. Die hier vorgehaltenen Bilder werden über den IIIF-kompatiblen Bildserver digilib ausgeliefert.
         Anhand der Textstellenangaben wird der Text auf der Manuskriptseite mit dem Text der Koran-Druckausgabe Kairo 1924 nach Schreibvarianten, Textvarianten und Verszählung untersucht. In vielen Fällen enthalten die verschiedenfarbigen Tinten der Handschrift Vokalzeichen, die Textvarianten in die Handschriften eintragen. Auch diese werden als unterschiedliche Lesarten verzeichnet.
         Außerhalb der philologischen Daten stellen Illuminationen und Ornamente eine wichtiges Kennzeichen für Manuskriptfragmente dar, die ursprünglich zusammengehörten. Die Form und farbliche Gestaltung der funktionalen Ornamente (Verstrenner und Kapitelüberschriften). Form und Layout (z.B. Pflanzenornamente oder geometrische Muster) der Ornamente oder deren farbliche Gestaltung lassen dabei auf eine gemeinsame Herkunft der Fragmente schließen.
         Für den Vergleich und die Darstellung von Orthographiedifferenzen im Vergleich zum Text der Druckausgabe Kairo 1924 wurde im Rahmen des Paleocoran-Projekts die Programmierbibliothek „Rasmify“ entwickelt. Rasmify entfernt sämtliche buchstabendifferenzierenden Zeichen (Diakritika) und Vokalzeichen aus arabischen Zeichenketten, sodass nur das Konsonantenskelett (arabisch: rasm) der verarbeiteten Strings übrig bleibt. Dies ist wichtig, da die frühen Koranhandschriften sehr häufig Buchstaben undifferenziert schreiben. Durch die reduzierte Wortform wird es einfacher, Unterschiede zwischen einzelnen Koranhandschriftenfragmenten und der Kairiner Druckausgabe zu identifizieren. Das Programm „Rasmify“ wurde in PHP, Python 3 und JavaScript als freie Software veröffentlicht und kann einfach über die jeweiligen Abhängigkeitsverwaltungen composer, pip und npm nachgenutzt werden.
         Ähnliche Muster bei Abweichungen in Textvarianten (Lesarten), Schreibvarianten- und Verssegmentierung weisen darauf hin, dass die betreffenden Fragmente ursprünglich demselben Korankodex stammen. Bei genügend vorliegenden Indizien werden die einzelnen Handschriftenfragmente bzw. Teile der Fragmente einem virtuellen Kodex zugeordnet. Durch die den einzelnen Handschriftenseiten zugeordneten Textstellen werden dann die dem virtuellen Kodex zugeordneten Handschriftenseiten nach Textkoordinate sortiert, sodass letztendlich ein IIIF-Manifest für den virtuellen Kodex erstellt werden kann.
         Auf der Projektwebseite paleocoran.eu kann mittels des IIIF-Manifests der virtuell rekonstruierte Korankodex in seiner ursprünglichen Form im IIIF-Viewer Mirador digital abgebildet werden. Mirador bietet darüber hinaus Lichttischfunktionalitäten, sodass sowohl einzelne Seiten desselben virtuell rekonstruierten Korankodex als auch unterschiedliche virtuell rekonstruierte Korankodizes miteinander gezielt verglichen werden können.
         Zusätzlich werden Metadaten der zugeordneten Manuskriptfragmente sowie Metadaten zur kodikologischen und paleographischen Einordnung des rekonstruierten Kodex angezeigt. Weiterhin werden die Lesarten- und Orthographievarianten sowie Ornamente samt Wortkoordinate und zitierfähigem IIIF-Bildausschnitt angegeben, sodass Forschende die virtuelle Rekonstruktion nachvollziehen können.
         Zum aktuellen Zeitpunkt wurden 338 virtuell rekonstruierte Kodizes bzw. Kodexteile angelegt und über 1500 Lesartenvarianten sowie über 2500 Orthographieunterschiede identifiziert. Durch die Anbindung an das Corpus Coranicum Projekt ist die langfristige Sicherung und Nachnutzung gewährleistet. Der Launch der Paleocoran-Projektwebseite soll Ende 2018 erfolgen.
      
   


10925	2019	
      
         
            Beschreibung
            
               
               Briefeditionen sind ein Typus der digitalen Edition, in dem die Vorteile des digitalen Mediums bereits mit am intensivsten fruchtbar gemacht werden.
               1
                
               In der alltäglichen Arbeit des Edierens sowie der Software-Entwicklung richtet sich der Blick zum großen Teil meist auf den einzelnen Brief und seine Tiefenerschließung, weniger auf die Menge an Briefen eines Korrespondenzkorpus. Weiterführende quantitative Analysen auf Basis der Tiefenerschließung (vollständige Transkription, Modellierung in XML/TEI, Normdaten etc.) und mit digitalen Methoden, die gerade auch für korpusübergreifende Untersuchungen den Weg ebnen würden, sind traditionellerweise in Editionsprojekten (noch) nicht vorgesehen.
               2
                Mit dem 
               eintägigen 
               Workshop „Distant Letters“ möchten wir ein Panorama an quantitativ orientierten Analysemethoden und -praktiken für Daten digitaler Briefeditionen vorstellen, 
               vermitteln und diskutieren, um so neue Perspektiven 
               auf
                Briefkorpora 
               zu
                erproben.
               3
                
               Der Workshop gliedert sich in vier Abschnitte:
            
            
               Auswertung von Metadaten und Entitäten
               
                  
                  Auf der Grundlage von standardisiert kodierten Briefmetadaten in XML/TEI sollen mit der Abfragesprache XQuery zunächst Fragen formuliert werden wie: „Wie viel hat Sender A an Empfänger B insgesamt geschrieben? Wie viel in einem bestimmten Jahr?“ Im Anschluss sollen vergleichende Untersuchungen angestellt werden, denen Fragen wie „Wie viel hat Sender A an Empfänger B und Empfänger C geschrieben?“ oder „Wie gestaltet sich das Verhältnis von gesendeten und empfangenen Briefen in der Korrespondenz von A und B?“ zugrunde liegen. Auch Entitäten aus dem Brieftext können in die Untersuchung mit einbezogen werden („Wie häufig wird Person X im Verlauf der Korrespondenz erwähnt?“). Das Ergebnis von derlei Fragen sind statistische Werte, die, um sie interpretatorisch zugänglicher zu machen, weiter aufbereitet werden müssen, z.B. als Visualisierungen in Diagrammen, Kreisen und Kurven.
               
            
            
               Analyse linguistischer Merkmale
               
                  
                  Im zweiten Teil wendet sich die Untersuchung den Volltextdaten zu. In den Blick genommen werden dabei linguistische Merkmale auf Token-Ebene (z.B. Lemma und Wortart), die einfachen oder komplexen Abfragen (z.B. nach typischen Adjektiv-Anbindungen bestimmter Substantive, Häufungen einer bestimmten Wortart, festen Wendungen, Kollokationen) an den Text zugrunde gelegt werden können und so u.a. Aufschluss über inhaltliche und stilistische Gegebenheiten ermöglichen. Im Workshop werden Werkzeuge gezeigt und benutzt, die zum einen die automatische linguistische Analyse von Texten, z.B. deren Lemmatisierung und POS-Tagging, erlauben und zum anderen Möglichkeiten der Auswertung annotierter linguistischer Merkmale bieten, z.B. mittels leistungsstarker Suchanfragesprachen oder Möglichkeiten der Visualisierung. Genauer in den Blick genommen und z.T. benutzt werden TXM, Corpus Workbench, DTA und WebLicht.
                  4
               
            
            
               Topic Modeling
               
                  
                  Im dritten Teil des Workshops rücken die Inhalte der Briefkommunikation stärker in das Zentrum des Interesses, wenn Fragen aufgegriffen werden wie „Welche Themen werden 
                  behandelt und wie sind diese zeitlich verteilt?“ oder „Gibt es bestimmte Themen, die in einer bestimmten Personengruppe stärker verhandelt werden als in einer anderen?“. Analysiert wird dabei der Volltext der Briefe, zusätzlich können jedoch auch die Briefmetadaten in die Interpretation der Analyseergebnisse einfließen. Für die Modellierung 
                  der Topics
                   wird das Tool „Mallet“
                   
                  verwendet,
                  5
                   und es wird im Workshop gemeinsam ein Topic Model für ein Briefkorpus erstellt. Für die Auswertung in Kombination mit Metadaten und Visualisierungen wird der „Topic Modeling Workflow“ 
                  (TMW) verwendet.
                  6
                  
                  Diskutiert werden soll 
                  außerdem, wie sich die Konzepte ‚Topic‘ und ‚Thema‘ zueinander verhalten.
                  7
               
            
            
               Stilometrie
               
                  Im letzten Teil des Workshops soll mit Methoden und Tools der Stilometrie der Sprach- bzw. Schreibstil eines Briefkorpus genauer untersucht werden. Analysiert wird dabei erneut der Volltext, diesmal in orthografisch normalisierter Form. Mögliche Fragestellungen der Analyse sind: „Welche Rückschlüsse erlauben stilometrische Analysen hinsichtlich Sender und Empfänger der Briefe?“, „Korrelieren die stilistische Nähe bzw. Distanz mit Faktoren wie Zeit, Raum oder Empfänger?“. Auch Stilvergleiche werden beispielhaft auf Grundlage der Fragen „Ändert sich der Stil von Sender A in seinen Briefen an die Empfänger B und C?“ und „Variiert der Stil zwischen Geschäfts- und Privatkorrespondenz?“ unternommen. Für die stilometrischen Analysen nutzen wir das „Stylo“-Paket für R.8
                  
                  Auch für die stilistischen Analysen ist zu diskutieren, welches Konzept von Stil hinter den gewählten Methoden steht und wie es sich zu anderen Definitionen von Stil verhält.
                  9
               
            
         
         
            Ziele
            
               Ziel des Workshops ist es, ein Panorama quantitativer Analysemöglichkeiten für Briefkorpora vorzustellen, das eine Ergänzung zu den traditionellen ‚close reading‘-Verfahren in wissenschaftlichen Editionen darstellt und die Digitalität der Editionsdaten mit Methoden der Digital Humanities noch stärker für quellenimmanente Forschungsfragen fruchtbar macht. Die Teilnehmerinnen und Teilnehmer sollen den Workshop am Ende des Tages mit einem Set an Skripten und Tools verlassen und in der Lage sein, diese auf andere (ggf. eigene) Datensätze anzuwenden. Neben der Vermittlung von technischen Fertigkeiten ist die Diskussion der Methoden und Ergebnisse mit den Teilnehmerinnen und Teilnehmern fester Bestandteil des Workshops. 
                    
               Es soll dabei gemeinsam eruiert werden, auf welchen theoretischen Annahmen die Methoden jeweils basieren, wo ihre Stärken und Schwächen liegen und auch inwieweit die vermittelten Praktiken eine Chance haben könnten, zukünftig ein Bestandteil bei der Erstellung und Nutzung wissenschaftlicher digitaler Briefeditionen zu werden. 
            
         
         
            Daten
            Die Organisatorinnen und Organisatoren stellen XML/TEI und Plain Text Datensätze aus zwei verschiedenen Briefeditionen für den Workshop bereit: ca. 5500 Brieftexte und ebenso viele Metadatensätze aus „Jean Paul - Sämtliche Briefe digital“ (Bernauer / Miller / Neuber 2018) sowie ca. 400 Brieftexte und 3000 Metadatensätze der „edition humboldt digital“ (Ette 2017-). Darüber hinaus steht es den Teilnehmerinnen und Teilnehmer frei, ihre eigenen Datensets (XML/TEI-kodiert und Plain Text) zu verwenden.
         
         
            Teilnehmerzahl und Vorkenntnisse
            Die Anzahl der Teilnehmerinnen und Teilnehmer ist auf 25 begrenzt. Gewisse Grundkenntnisse in der Programmierung (z.B. XSLT/XQuery, Python, R) sind von Vorteil, die im Workshop verwendeten Skripte werden jedoch so vorbereitet, dass sich die Arbeit daran auf Modifikationen und Erweiterungen unter Anleitung der Lehrenden beschränkt. Im Vorfeld des Workshops werden Installationshinweise für die verwendeten Werkzeuge gegeben und die Übungsdaten zum Download bereitgestellt.
         
         
            Lehrende
            
               
               Stefan Dumont: Wissenschaftlicher Mitarbeiter bei der TELOTA-Initiative der Berlin-Brandenburgischen Akademie der Wissenschaften, dort u.a. zuständig für die „edition humboldt digital”. Wissenschaftlicher Koordinator des DFG-Projekts „correspSearch - Briefeditionen vernetzen“. Co-Convener der TEI Special Interest Group „Correspondence“. Expertise u.a. mit Standardisierung von Briefkodierung und -metadaten und X-Technologien.
                
            
               Susanne Haaf: Wissenschaftliche Mitarbeiterin im Projekt CLARIN-D an der Berlin-Brandenburgischen Akademie der Wissenschaften, u.a. beteiligt am Auf- und Ausbau des Deutschen Textarchivs. Doktorandin im Bereich korpusbasierter Untersuchung von Textsortenspezifika. Spezialisierung in Korpusaufbau, Korpuslinguistik, Standards für Text- und Metadaten (insbes. TEI) sowie Textedition.
                
            
               Ulrike Henny-Krahmer: Wissenschaftliche Mitarbeiterin im Projekt „Computergestützte Literarische Gattungsstilistik” (CLiGS) an der Universität Würzburg. Studium der Regionalwissenschaften Lateinamerika in Köln und Lissabon, Doktorandin in Digital Humanities mit dem Thema „Topic and Style in Subgenres of the Spanish American Novel (1830-1910)“.
                
            
               Benjamin Krautter: Wissenschaftlicher Mitarbeiter im Projekt “Quantitative Drama Analytics” (QuaDramA) an der Universität Stuttgart. Studium der Literaturwissenschaft (Germanistik) und Politikwissenschaft in Stuttgart und Seoul (Südkorea), Doktorand im Bereich Digital Literary Studies mit dem Thema “Quantitative Dramenanalyse - Operationalisierung aristotelischer Kategorien” (Arbeitstitel).
                
            
               Frederike Neuber: Wissenschaftliche Mitarbeiterin bei der TELOTA-Initiative der Berlin-Brandenburgischen Akademie der Wissenschaften, dort u.a. zuständig für die Briefedition “Jean Paul - Sämtliche Briefe digital”. Studium der Italianistik und Editionswissenschaft in Berlin und Rom, Doktorandin in Digital Humanities. Spezialisierung in Editionsphilologie, Datenmodellierung und Programmierung mit X-Technologien.
                
         
      
      
         
            
                Der webservice „correspSearch“ etwa illustriert die Bedeutung von standardisierter Metadatenerfassung mit Normdaten zur Vernetzbarkeit von Korrespondenzen, . 
            
                Vereinzelt werden quantitative Analysemethoden bereits auf Editionsdaten angewandt: Etwa wird im Kontext des Projekts “Mapping the Republic of Letters” (Stanford University 2013) zur Erschließung der Briefkommunikation und -verbreitung mit verschiedenen statistisch- und/oder netzwerkanalytisch-basierten Visualisierungen experimentiert; Andorfer (2017) erprobt Topic Modelling mit dem Korrespondenzkorpus Leo von Thun-Hohensteins.
            
                Nicht Teil dieses Panoramas ist die Netzwerkanalyse, auch wenn diese Form der Auswertung bzw. Visualisierung für Briefdatensätze oft die am naheliegendste scheint. Grundkompetenzen zur Netzwerkanalyse bzw. -visualisierung werden mittlerweile regelmäßig in Workshops vermittelt, z.B. im Rahmen der „Historical Network Research-Community“ (http://historicalnetworkresearch.org/). Der Fokus des Workshops richtet sich daher auf bisher weniger berücksichtigte Formen der Analyse von Briefkorpora.
            
               
                http://textometrie.ens-lyon.fr, , , .
            
               
               
            
            
               
               
            
            
                Zwar ist das Verfahren für die Ermittlung von Schlüsselwörtern und Themen entwickelt worden, je nach verwendetem Korpus ergeben sich aber auch andere Arten von Topics, z.B. sprachspezifische oder motivische. Vgl. dazu u.a. Rhody (2012) und Schöch (2017).
            
               
               
            
            
                Für einen Überblick zu verschiedenen Stilbegriffen in der Literatur- und Sprachwissenschaft siehe Herrmann et al. (2015).
         
         
            
               Bibliographie
               
                  
                  Andorfer, Peter (2017):
                  Turing Test für das Topic Modeling. Von Menschen und Maschinen erstellte inhaltliche Analysen der Korrespondenz von Leo von Thun-Hohenstein im Vergleich, 
                        in: Zeitschrift für digitale Geisteswissenschaften; doi: 
                        
                     10.17175/2017_002
                  [zuletzt abgerufen 7. Januar 2019].
                    
               
                  Bernauer, Markus / Miller, Norbert / Neuber, Frederike (eds.) (2018):
                  Jean Paul – Sämtliche Briefe digital. 
                        In der Fassung der von Eduard Berend herausgegebenen 3. Abteilung der Historisch-kritischen Ausgabe (1952-1964), im Auftrag der Berlin-Brandenburgischen Akademie der Wissenschaften überarbeitet und herausgegeben von Markus Bernauer, Norbert Miller und Frederike Neuber; 
                        
                     http://jeanpaul-edition.de
                   [letzter Zugriff 7. Januar 2019].
                    
               
                  
                  Burrows, John (2002):
                  Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship, 
                        in: Literary and Linguistic Computing 17/3, S. 267–287.
                    
               
                  Dumont, Stefan (2016):
                  correspSearch – Connecting Scholarly Editions of Letters, 
                        in: Journal of the Text Encoding Initiative [Online], Issue 10; 
                         [letzter Zugriff 7.Januar 2019].
                    
               
                  Eder, Maciej / Rybicki, Jan / Kestemont, Mike (2016):
                  Stylometry with R: A Package for Computational Text Analysis, 
                        in: The R Journal 8/1 (2016), S. 107–121.
                    
               
                  Ette Ottmar (eds.) (seit 2016):
                  edition humboldt digital. 
                        Berlin-Brandenburgische Akademie der Wissenschaften, Berlin. Version 3 vom 14.09.2018; 
                        
                        [letzter Zugriff 7. Januar 2019].
                    
               
                  Graham, Shawn / Weingart, Scott / Milligan, Ian (2012):
                  Getting Started with Topic Modeling and MALLET, 
                        in: The Programming Historian 1;
                        
                        [letzter Zugriff 7. Januar 2019].
                    
               
                  
                  Heiden, Serge (2010):
                  The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme, 
                        24th Pacific Asia Conference on Language, Information and Computation, Nov 2010, Sendai, Japan. Institute for Digital Enhancement of Cognitive Development, Waseda University, S.389–398. 
                    
               
                  
                  Herrmann, Berenike J. / van Dalen-Oskam, Karina / Schöch, Schöch (2015):
                  Revisiting Style, a Key Concept in Literary Studies, 
                        in: Journal of Literary Theory 9/1, S. 25–52.
                    
               
                  
                  Rhody, Lisa M. (2012):
                  Topic Modeling and Figurative Language, 
                        in:  Journal of Digital Humanities 2/1; 
                        
                        [letzter Zugriff 7. Januar 2019].
                    
               
                  
                  Schöch, Christof (2017):
                  Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama, 
                        in: Digital Humanities Quarterly 11/2; 
                        
                        [letzter Zugriff 7. Januar 2019].
                    
               
                  Walmsley, Priscilla (2009):
                  XQuery: Search Across a Variety of XML Data.
                        O’Reilly Media.
                    
            
         
      
   


10935	2019	
      
         Zu den Verfahren der digitalen Textanalyse, die in den vergangenen Jahren in den textbasierten digitalen Geisteswissenschaften etabliert wurden, gehört das LDA (Latent Dirichlet Allocation) Topic Modeling (Blei 2012; Steyvers und Griffiths 2006). Diese Methode eignet sich zur Analyse der Verteilung semantischer Wortgruppen in Textsammlungen, und kann sowohl für die computergestützte Textklassifikation als auch für die explorative Betrachtung der Inhalte eines Corpus herangezogen werden. Um dem zunehmenden Interesse am Topic Modeling von Seiten der DH-Community Rechnung zu tragen, entwickelt DARIAH-DE seit 2017, basierend auf den Python-Bibliotheken "LDA" von Allan Riddell und "DARIAHTopics" (Jannidis et al. 2017), den DARIAH-TopicsExplorer, eine Software, mit der interessierte Forschende Topic Modeling auf ihren eigenen Rechnern an ihren eigenen Texten ausprobieren können, und die den gesamten Analyseprozesses, vom unverarbeiteten Text bis zum Ergebnis, durch eine graphische Nutzeroberfläche (GUI) unterstützt.
         Der TopicsExplorer bietet nicht die Leistungsfähigkeit und vor allem die Flexibilität bisheriger Lösungen, die entweder, wie das weit verbreitete MALLET (McCallum 2002), als Kommandozeilenprogramme, oder aber, wie Gensim (Rehurek und Sojka 2010), als Bibliothek für eine Programmiersprache konzipiert wurden. Dafür kann er aber ohne Kenntnis irgendeiner Programmier- oder Skriptsprache eingesetzt werden und muss nicht einmal über die Kommandozeile aufgerufen werde. Damit schließt der TopicsExplorer eine wichtige Lücke: Forschende, die selbst nicht, oder noch nicht, programmieren, können sich hier einen Eindruck davon verschaffen, wie die Methode funktioniert und was sie theoretisch leisten kann. Das befähigt auf der einen Seite, Forschungsarbeiten, die auf Topic Modeling basieren, und ihre Ergebnisse informiert einzuschätzen. Auf der anderen Seite kann das Werkzeug für einfache Fragestellungen, die Topic Modeling erfordern, direkt eingesetzt werden, und bei komplizierteren Ansätzen eine informierte Entscheidung darüber ermöglichen, ob sich die Aneignung der notwendigen technischen Fähigkeiten für die Verwendung einer fortgeschrittenen Lösung für die jeweilige Forschungsfrage überhaupt lohnt.
         Der TopicsExplorer wurde in einer ersten Version 2017 und 2018 im Rahmen mehrerer Workshops und Konferenzen verschiedenen Gruppen von Forschenden und Studierenden vorgestellt. Die Erfahrungen aus dem Umgang mit Nutzerinnen und Nutzern in solchen Workshops und vor allem ihr direktes Feedback sind seither umfangreich in die Weiterentwicklung eingeflossen und die daraus resultierenden Änderungen gehen weit über die Beseitigung von Bugs und die Sicherstellung der nachhaltigen Funktionalität hinaus. Aus einem anfänglichen "GUI-Demonstrator" (Simmler et al. 2018), der noch die Installation einer Python-Bibliothek erforderte, auf einem lokalen Server lief und im Browser angezeigt wurde, ist eine vollwertige Stand-Alone-Software geworden, die nach dem Herunterladen ohne weitere Vorbereitung auf gängigen Windows-, MacOS- und Linux-Systemen gestartet werden kann. Die Visualisierungen können interaktiv manipuliert, und die Ergebnisse im csv-Format exportiert werden. Zahlreiche kleinere, von der Testcommunity gewünschte Features, wie z.B. eine Fortschrittsanzeige mit Abbruchbutton (Abb. 1), haben die Usability wesentlich verbessert.
         Zur Zeit wird eine Version in grundlegend überarbeitetem Design vorbereitet, deren Ziel es ist, auch komplizierteren, aus den Testcommunies heraus formulierten Ansprüchen an die Interaktivität der Software gerecht zu werden. Auf technischer Ebene wird dabei die Visualisierung der Ergebnisse statt mit der bisher verwendeten Python-Bibliothek "Bokeh" direkt in Javascript realisiert, um zusätzliche Flexibilität für die Umsetzung neuer Funktionalitäten zu gewinnen. Äußerlich wird es damit möglich, Ergebnisse auch nachträglich interaktiv umzusortieren und in mehreren Fenstern darzustellen. Mit dem Ziel, die User-Experience im explorativen Umgang mit den erzeugten Modellen zu verbessern, wird darüber hinaus ein völlig neues Visualisierungskonzept auf Basis der Vorschläge von Chaney und Blei (2012) umgesetzt. Dieses Konzept erlaubt es nicht nur, einzelne Dokumente im Corpus mitsamt den dazugehörigen Analyseergebnissen zu betrachten, es werden darüber hinaus automatisch andere Texte mit ähnlichen inhaltlichen Schwerpunkten vorgeschlagen (Abb. 2).
         Wir hoffen, dass der TopicsExplorer mit den angestrebten Verbesserungen und Erweiterungen dazu beiträgt, eine mittlerweile doch recht verbreitete Forschungsmethode aus der Nische derjenigen DH-Verfahren heraus zu holen, die nur von Programmiererinnen und Programmierern verwendet, verstanden und kritisch diskutiert werden. Die neuen Features, die für das kommende Release entwickelt werden, sollten dazu einen wesentlichen Beitrag leisten.
         
            
               
               
                  Abbildung 1. Fortschrittsanzeige für die laufende Modellierung im aktuellen TopicsExplorer
					
            
         
         
            
               
               
                  Abbildung 2. Übersicht für ein Dokument im Prototypen der Version 2
					
            
         
      
      
         
            
               Bibliographie
               
                  Blei, David M. (2012):
                  Probabilistic Topic Models, in: 
                        Communication of the ACM55, Nr. 4 (2012): 77–84. doi:10.1145/2133806.2133826.
                    
               
                  Chaney, Allison J.B. / Blei, David M. (2012):
                  The Visualizing Topic Models, in: 
                        Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media 419-422.
                    
               
                  Jannidis, Fotis/ Pielström, Steffen / Schöch, Christof / Vitt, Thorsten (2017):
                  Making topic modeling easy: a programming library in Python, in: 
                        Proceedings of the Digital Humanities 2017 Conference.
                    
               
                  McCallum, Andrew K. (2002): 
                  MALLET : A Machine Learning for Language Toolkit.
                        
                     http://mallet.cs.umass.edu.
                  
               
               
                  Rehurek, Radim/ Sojka, Petr (2010):
                  Software framework for topic modelling with large corpora. 
                        In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks.
                    
               
                  Simmler, Severin / Vitt, Thorsten / Pielström. Steffen (2018):
                  LDA Topic Modeling über eine graphisches Interface, in: 
                        Konferenzabstracts der 5. Tagung des Verbands Digital Humanities im deutschsprachigen Raum e.V. 428-429.
                    
               
                  Steyvers, Mark/ Griffiths, Tom (2006):
                  Probabilistic Topic Models, in:
                        Latent Semantic Analysis: A Road to Meaning, herausgegeben von T. Landauer, D. McNamara, S. Dennis, und W. Kintsch. Laurence Erlbaum.
                    
            
         
      
   


10940	2019	
      
         
            Die 
                Faustedition versammelt 573 Textzeugen mit ca. 730 datierbaren Objekten zu Goethes „Faust“ in einem digitalen Archiv. Für die Benutzer sollen sich die repräsentierten historischen Objekte nicht isoliert voneinander, sondern in einem sinnvollen Zusammenhang darstellen. Eine solcher Zusammenhang kann dadurch entstehen, dass die Objekte genetisch geordnet werden. Genetische Einordnung bedeutet zuallererst, Objekte zeitlich zu situieren. Dabei geht es teils um die Ermittlung von Kalenderdaten, teils um die Bestimmung des relativen zeitlichen Verhältnisses mehrerer Objekte. Die Ermittlung chronologischer Systeme ist eine Grundfrage in vielen Disziplinen. Je nach Sachlage können dabei etwa statistische Verfahren (z.B. Bayliss 2015 zur Archäologie) oder evolutionäre Modelle (Trovato 2014: 189–200 zur mediävistischen Philologie) eingesetzt werden; die Verwendung disziplinfremder Ansätze kann aber auch zu Problemen führen (Pereltsvaig/Lewis 2015 zur Glottochronologie). Beim derzeitigen Stand scheint es am aussichtsreichsten, das in einzelnen Disziplinen oder bei einzelnen Projekten geübte Vorgehen zu formalisieren, um potentiell generalisierbare Verfahren zu entwickeln.
            
         Besonders schwierig ist es, nicht bloß einzelne Objekte zu datieren, sondern eine große Menge in eine chronologische Ordnung zu bringen, wenn die Objekte genetisch voneinander abhängig sind, nur wenige absolute Daten zur Verfügung stehen und sonst nur lokale Anhaltspunkte für relative Chronologien gegeben sind (klassisches Beispiel: die antike Chronographie; Grafton 1993, Burgess/Witakowski 1999). Dies ist auch bei umfangreichen genetischen Handschriftendossiers neuzeitlicher Autoren und Werken mit komplexer Entstehungsgeschichte der Fall. Hier können einzelne Teilentwürfe in relativer zeitlicher Beziehung stehen, vereinzelt sind Datierungen verfügbar; doch die Rekonstruktion der 
                Makrogenese, d.h. der chronologischen Ordnung des Gesamtbestands (zum Begriff Van Hulle 2018: 47–48), kann sich als außerordentlich komplex erweisen.
            
         Und so liegen die Dinge auch bei „Faust“. Nur wenige der makrogenetischen Objekte sind genau datierbar; stattdessen gibt es eine große Menge relativer, aber strikt lokaler Chronologien für jeweils nur einige Objekte. Den bislang einzigen Versuch, Einzelaussagen zu aggregieren und alle Objekte in zeitlich-stemmatische Beziehung zueinander zu setzen, macht Fischer-Lamberg für zwei Akte des “Faust II”. Ihre Stemmata (Fischer-Lamberg 1955: 150–166) markieren die praktische Grenze dessen, was an Einzelinformationen mit menschlichen Mitteln aggregiert werden kann. Die Rekonstruktion einer (theoretisch beliebig großen) Makrogenese verlangt nach maschineller Verarbeitung und visueller Aufbereitung vorhandener Einzelinformationen.
         
            Datengrundlage
            Um dies zu ermöglichen, wurde der Informationsgehalt der verfügbaren einschlägigen Aussagen zur Datierung1 in XML erfasst:
                
            
               bibliographische Quelle
               datierter Textzeuge
               absolute Datierung
               relative Datierung
               (ungefähre) Gleichzeitigkeit von Textzeugen
            
            Eine 
                    Relative Datierung setzt Zeugen2 in eine zeitliche Reihenfolge. Es wird ausgesagt, dass ein Zeuge vor einem anderen entstanden ist.
                
            
               
                  
                  
                     Abbildung 1. Kodierte Aussage: Laut Fischer-Lamberg 1955: 160 (source), ist 2 III H.5 vor 2 III H.8 (temp-pre) (vereinfacht).
						
               
            
            Eine 
                    Absolute Datierung ordnet einen oder mehrere Textzeugen konkreten Datumsangaben zu. Tagesgenaue Datierungen bilden die Ausnahme; häufig sind Aussagen, dass ein Zeuge nicht vor oder nicht nach einem Zeitpunkt entstanden sei. Typisch sind unscharfe Angaben wie 1800/1801 oder Frühsommer. Solche Angaben werden nach dokumentierten Regeln auf Aussagen der Form nicht vor und nicht nach  mit jeweils normierten tagesgenauen Angaben abgebildet.
                
            
               
                  
                  
                     Abbildung 2. Laut Bohnenkamp 1994: 208, ist 2 III H.5 auf den Zeitraum vom 26. Februar zum 5. April 1825 zu datieren (vereinfacht).
						
               
            
         
         
            Modellierung als Graph
            Die verschiedenen Aussagen werden in einem gerichteten Graphen modelliert3: Die Zeugen bilden Knoten, die relativen Datierungen Kanten des Graphen. Zur Integration der absoluten Datierungen werden die Datumsangaben (Tage) ebenfalls als Knoten in den Graphen integriert: Aus einer Datierung wie 
                    2 III H.5 wurde nicht vor dem 26. Februar 1825 und nicht nach dem 5. April 1825 geschrieben wird so ein Teilgraph 
                    1825-05-25 → 2
                
               III
                
               H.5 → 1825-04-06, in dem die Knoten für Ereignisse (Daten oder Zeugen) und die Kanten für 
                    zeitlich vor stehen. Ergänzt wird der Graph durch »Zeitstrahlkanten«, die von jedem Datum zum nächstfolgenden führen.
                
            
               
                  
                  
                     Abbildung 3. Aus Einzelaussagen gebildeter Graph zu 2 III H.8, unmittelbar benachbarten Zeugen und Datierungen (vereinfacht).
						
               
            
            Aus diesem Graphen lassen sich Informationen ableiten, die sich erst aus dem Zusammenspiel der Einzelaussagen ergeben: Betrachtet man einen Zeugen 
                    z, so sind alle von 
                    z aus entlang gerichteter Kanten erreichbaren Zeugen (hier: 2 III H.1) nach 
                    z entstanden, und die von 
                    z aus erreichbaren Daten bilden Grenzen des 
                    terminus ante quem (des Zeitpunkts, vor welchem ein Zeuge entstand). Der Graph bietet damit die Grundlage für eine ungefähre Datierung auch derjenigen Zeugen, für die keine direkte absolute Datierung vorliegt.
                
            Ist die Gesamtheit der Aussagen nicht widerspruchsfrei, so ergeben sich Zyklen im Graphen. Dies induziert einen Teilgraphen, in dem (vgl. Abb. 4 mit relativen Datierungen einiger Handschriften) jeder Knoten von jedem anderen erreichbar ist (der Teilgraph ist 
                    stark zusammenhängend).
                
            
               
                  
                  
                     Abbildung 4. Relative Datierungen für einige Handschriften des 4. Akts. Erst die Entfernung aller rotgestrichelten Kanten führt zu einem zyklenfreien Graphen.
						
               
            
            Um den Graphen aus Abb. 4 zyklenfrei zu machen, ist die Entfernung von wenigstens drei Kanten notwendig (gestrichelt). Der komplette Makrogenesegraph enthält eine stark zusammenhängende Komponente mit 477 Dokumenten und 2136 Kanten – zu umfangreich, um die Konflikte manuell zu eliminieren.
         
         
            Konfliktbehandlung
            Eine möglichst kleine Menge von Kanten zu entfernen, um einen zyklenfreien Graphen zu erhalten, ist ein als 
                    Minimum Feedback Arc Set bekanntes, NP-vollständiges (Karp 1972) Problem der Graphentheorie; um eine optimale Lösung zu finden, ist die größte der stark zusammenhängenden Komponenten zu groß (und die aus graphentheoretischer Sicht optimale Lösung muss auch nicht die philologisch korrekte sein, es wäre nur diejenige, die die wenigsten Datierungsaussagen verwirft). Verwendet wird stattdessen eine Heuristik, z.Z. das Verfahren von Eades (1993, Implementierung von igraph4), wobei zuviel entfernte Kanten nach Möglichkeit wieder hinzugefügt werden.
                
            Entfernt man alle Konfliktkanten, so erhält man einen zyklenfreien gerichteten Graphen (DAG), der die Basis für die automatisierte Weiterverarbeitung ist. Dessen Knoten können in eine 
                    topologische Ordnung gebracht werden, d.h. eine Reihenfolge, die mit allen Kanten des Graphen konsistent ist (Manber 1989: 199).
                
            Um die aus einer Vielzahl teils widersprüchlicher Aussagen mechanisch gezogenen Schlüsse nachvollziehbar und verbesserbar zu machen, werden die Daten in einer Reihe verlinkter Darstellungen mit GraphViz (Gansner/North 2000) visualisiert. Die Grundlage bildet der Gesamtgraph mit allen Aussagen, in dem algorithmisch entfernte Aussagen (Konflikte) rotgestrichelt visualisiert werden.
            Zu jedem Zeugen gibt es einen Teilgraphen, der seine Nachbarn, die nächsten erreichbaren absoluten Datierungen und alle Aussagen dazwischen visualisiert. Darunter werden die Aussagen tabellarisch aufgelistet. Zu jeder (entfernten) Konfliktkante zeigt eine separate Visualisierung einen Pfad in der Gegenrichtung, mit dem die Kante in Konflikt stand, so dass der Konflikt erkennbar wird und zu den beteiligten Zeugen weiternavigiert werden kann.5 Daneben gibt es aufbereitete Darstellungen für jede Szene und jede Quelle.
                
            Anhand der Visualisierungen können die vorliegenden Reihenfolgeentscheidungen nachvollzogen, aber auch Datierungskontroversen und -lücken identifiziert undin den Quelldateien behoben werden:
            
               Aussagen können als 
                        zu ignorieren markiert werden, um sie bei der Bildung des Graphen auszuschließen.
                    
               Quellen können nach ihrer Zuverlässigkeit bewertet werden, um zu beeinflussen, wie leicht oder schwer entsprechende Kanten als Konfliktkanten entfernt werden.
               Eigene Erkenntnisse können mit entsprechend hohem Gewicht einbezogen werden.
            
            Die visualisierten Ergebnisse erfüllen so einen mehrfachen Zweck: Sie dienen zur systematischen Erschließung der einschlägigen Forschung, zur Klärung der genetischen Verhältnisse für den gesamten Faust und als Hilfsmittel zur Überprüfung, Vervollständigung und Verbesserung der Datenlage, d.h. zur Erweiterung des Forschungsstand. Darüber hinaus wird die ermittelte Reihenfolge in der Faustediton verwendet, um die Zeilensynopse sowie das Balkendiagramm zu sortieren.
         
         
            Ausblick
            Neben der manuellen Nachbearbeitung der Daten kommen zur Verbesserung des Verfahrens alternative Heuristiken für das Minimum-Feedback-Arc-Problem in Frage (z.B. Even et al. 1998). 
            
               Bei der vorgestellten Methode werden in der relativen Chronologie die Entstehungsintervalle nur in eine disjunkte vor-nach-Beziehung gesetzt. Möglich wäre es, hier zu prüfen, ob eine weitere Ausdifferenzierung der Beziehungen zwischen Entstehungsintervallen realisierbar ist, so dass Beziehungen der Form „wurde begonnen vor Fertigstellung von“ ausgedrückt werden können. Komplexere Relationen als eine einfache zeitliche Abfolge werden bereits von Allen (1983) in einem Graphmodell beschrieben, allerdings ist hier die globale Konfliktfreiheit ein Problem.
                
            Eine Alternative zu der oben beschriebenen Abbildung unscharfer Aussagen auf scharf begrenzte Intervalle ist etwa die Modellierung über Fuzzy-Mengen (vgl. z.B. Barro et al. 1994). Dies erfordert jedoch auch die Neudefinition der Relationen (Schockaert/De Cock 2008) und der darauf aufbauenden Verfahren etwa zur Konfliktauflösung.
            Der vorgestellte Ansatz basiert auf bereits vorhandenen, mit traditionellen philologischen Mitteln gewonnenen Datierungsaussagen. In Wissenbach/Pravida/Middell (2012) wird ein Verfahren vorgestellt, mit der kodierte, textinhärente Eigenschaften von Fassungen für die regelbasierte Bildung genetischer Hypothesen genutzt werden, um auf diesem Weg generelle Hypothesen zur Arbeitsweise des Autors zu verifizieren. 
                    Nachdem die genetische Analyse des Korpus nun weiter vorangeschritten ist, kann dieser Ansatz auf einer breiteren Datenbasis evaluiert werden.
                
         
      
      
         
            
               Zu den ausgewerteten Quellen siehe faustedition.net/macrogenesis/sources.
            
               Das Datenmodell der Faustedition sieht eine konzeptuelle Unterscheidung zwischen Zeugen und Inskriptionen (Niederschriften) vor: Die eigentlichen Objekte der Datierung sind Inskriptionen. Ein Zeuge kann eine oder mehrere unterschiedlich datierte Inskriptionen enthalten. Im folgenden wird der Einfachheit halber nur von Zeugen gesprochen.
            
               Mit der Graphbibliothek NetworkX (Hagberg/Schult/Swart 2008).
             http://igraph.org/
            
               Beispiel: faustedition.net/macrogenesis/H_P93--1825-01-01.
         
         
            
               Bibliographie
               
                  
                  Allen, James F. (1983):
                  Maintaining knowledge about temporal intervals, 
                        in: Communication of ACM 21(11): 832–843 doi: 
                        
                     10.1145/182.358434.
                  
               
               
                  Barro, Senén / Marín, Roque / Mira, José / Patón, Alfonso R. (1994):
                  A model and a language for the fuzzy representation and handling of time, 
                        in: Fuzzy Sets and Systems 61: 153–175. doi: 
                        
                     10.1016/0165-0114(94)90231-3.
                  
               
               
                  Bayliss, Alex (2015):
                  Quality in Bayesian chronological models in archaeology, 
                        in: World Archaeology 47(4): 677-700. doi:
                        
                     10.1080/00438243.2015.1067640
                  
               
               
                  Bohnenkamp, Anne (1994): 
                  ... das Hauptgeschäft nicht außer Augen lassend.
                        Die Paralipomena zu Goethes
                  Faust.
                         Frankfurt am Main / Leipzig: Insel.
               
               
                  Burgess, Richard W. / Witakowski, Witold (1999):
                  Studies in Eusebian and Post-Eusebian Chronography 
                        (= Historia. Einzelschriften; 135). Stuttgart: Steiner.
                    
               
                  Eades, Peter / Lin, Xue-Min / Tamassia, Roberto (1993):
                  A fast and effective heuristic for the feedback arc set problem, 
                        in: Information Processing Letters 47(6): 319–323. doi:
                        
                     10.1016/0020-0190(93)90079-O.
                  
               
               
                  Even, Guy / Naor, Joseph / Schieber, Baruch / Sudan, Madhu (1998):
                  Approximating Minimum Feedback Sets and Multicuts in Directed Graphs, 
                        in: Algorithmica 20(2): 151–174. doi:
                        
                     10.1007/PL00009191.
                  
               
               
                  Fischer-Lamberg, Renate (1955):
                  Untersuchungen zur Chronologie von Faust II 2 und 3. Diss. phil. (masch.), 
                        Humboldt-Universität Berlin.
                   
               
                  Gansner, Emden R. / North, Stephen C. (2000):
                  An open graph visualization system and its applications to software engineering, 
                        in: Software: Practice and Experience 30(11): 1203–1233. doi:
                        
                     10.1002/1097-024X(200009)30:113.0.CO;2-N.
                  
               
               
                  Grafton, Anthony (1993):
                  Joseph Scaliger. A Study in the History of Classical Scholarship. Vol. II: Historical Chronology
                        (= Oxford-Warburg Studies). Oxford: Clarendon.
                    
               
                  Hagberg, Aric A. / Schult, Daniel A. / Swart, Pieter J. (2008):
                  Exploring Network Structure, Dynamics, and Function using NetworkX, 
                        in: 
                        Varoquaux, Gael / Vaught, Travis / Millman, Jarrod (eds): 
                        Proceedings of the 7th Python in Science Conference (SciPy2008) in Pasadena, CA
                        
                        [letzter Zugriff 14. Oktober 2018].
                    
               
                  Karp, Richard M. (1972):
                  Reducibility Among Combinatorial Problems, 
                        in: 
                        Miller, Raymond Edward / Thatcher, James W. (eds.):
                  Complexity of Computer Computations. 
                        New York: Plenum 85–103. doi: 10.1007/978-3-540-68279-0_8.
                    
               
                  Manber, Udi (1989):
                  Introduction to Algorithms: A Creative Approach. 
                        Reading, MA: Addison-Wesley.
                    
               
                  Pereltsvaig, Asya / Lewis, Martin (2015):
                  The Indo-European Controversy. Facts and Fallacies in Historical Linguistics. 
                        Cambridge: Cambridge University Press.
                    
               
                  Schockaert, Steven / De Cock, Martine (2008):
                  Temporal Reasoning about Fuzzy Intervals, 
                        in: Artificial Intelligence 172(8): 1158–1193. doi:
                        
                     10.1016/j.artint.2008.01.001.
                  
               
               
                  Trovato, Paolo (2014):
                  Everything You Always Wanted to Know About Lachmann's Method. A Non-Standard Handbook of Genealogical Textual Criticism in the Age of Post-Structuralism, Cladistics, and Copy-Text. 
                        Padova: libreriauniversitaria.it
                    
               
                  Van Hulle, Dirk (2018):
                  Editing the Wake’s Genesis: Digital Genetic Criticism, 
                        in: 
                        Sartor, Genevieve (ed.):
                  James Joyce and Genetic Criticism. Genesic Fields 
                        (= European Joyce Studies; 28). Leiden, Boston: Brill Rodopi 37–54.
                    
               
                  Wissenbach, Moritz / Pravida, Dietmar / Middell, Gregor (2012):
                  Reasoning about Genesis or The Mechanical Philologist, 
                        in: 
                        Meister, Jan Christoph (ed.):
                  Digital Humanities 2012. Conference Abstracts. 
                        Hamburg: Hamburg University Press 418–422
                        
                        [letzter Zugriff 14. Oktober 2018].
                    
            
         
      
   


10941	2019	
      
         
            Die Digital Humanities haben sich im Verlauf der letzten zehn Jahre aus einem randständigen Thema an den deutschen Universitäten zu einem etablierten Ausbildungsbereich verwandelt. Die seit Jahren anhaltende Diskussion um konvergente Curricula zeugt von dieser Entwicklung (Sahle 2013). Digitale Editionen waren vor zehn Jahren im deutschsprachigen Raum selten anzutreffen. In ihren Ausprägungsformen waren sie noch sehr unterschiedlich und trugen den Charakter vereinzelter Leuchtturmprojekte, die die Grenzen neuer Verfahren in den Geisteswissenschaften ausloteten. Mittlerweile ist die “digitale Editorik” ein eigener Forschungsbereich. Während Fachkenntnisse in diesem Bereich vor zehn Jahren nur in außeruniversitären Sonderveranstaltungen wie Summer Schools und Workshops erworben werden konnten, gibt es heutzutage an einigen deutschsprachigen Universitäten regelmäßige Lehrveranstaltungen zum Thema, die im Kontext der bisher entstandenen Lehrstühle
            1
             der Digital Humanities verortet sind.
         
         
            Obwohl sich die universitäre Ausbildung in den letzten Jahren merklich und kontinuierlich verbessert hat, ist dennoch der Bedarf nach den Schools des Instituts für Dokumentologie und Editorik (IDE) ungebrochen; dies ist ein deutliches Zeichen, dass die Digital Humanities weiterhin stärker an den Universitäten verankert werden müssen. Daneben bieten die IDE-Schools ein gutes Angebot für InteressentInnen sowohl des außeruniversitären, als auch des postdoktoralen Sektors.
         
         
            Deshalb bleiben komprimierte Angebote jenseits von Studiengängen wie die Veranstaltungsreihe ESU Leipzig
            2
             oder eine Vielzahl vereinzelter Workshops oder Summer Schools
            3
             die einzige Möglichkeit, sich grundlegende Kompetenzen für die von individuellen Forschungsfragen angetriebene Arbeit in den Digital Humanities anzueignen. Daneben wurden auch verschiedene Online-Angebote für E-Learning entwickelt. Für den Bereich der digitalen Editorik seien hier beispielsweise Kurse bei #dariahTeach, Schulungsmaterialien von DiXiT und DARIAH-DE oder Dokumentationen auf Webseiten oder GitHub genannt.
            4
         
         
            Das IDE bietet seit 2008 regelmäßig einwöchige Schools an, die sich auf Themen rund um digitale Editionen konzentrieren. Bis 2018 wurden insgesamt 13 Schools in Wien (4), Köln (2), Chemnitz (2), Graz (2), Berlin (1), Weimar (1) und Rostock (1) mit insgesamt fast 300 TeilnehmerInnen durchgeführt.
            5
             Sie vermitteln wesentliche Kenntnisse für AnfängerInnen und Fortgeschrittene auf dem Gebiet der XML-basierten digitalen Editorik. In der Regel organisieren dabei lokale InteressentInnen die finanziellen und örtlichen Rahmenbedingungen und können grobe inhaltliche Vorgaben machen. Das IDE übernimmt die inhaltliche Ausgestaltung und die Auswahl des Lehrpersonals. Dabei wird in die Planung neuer Schools immer die Auswertung von Evaluationsbögen der vorangegangenen Schools einbezogen.
         
         Das IDE legt Wert darauf, dass die TeilnehmerInnen an ihren eigenen Editionsprojekten arbeiten, um die Motivation, die eigene Arbeit konsequent auf den neuen Methoden aufzubauen, zu erhöhen. Es hält jedoch auch eigene Übungsmaterialien bereit, um den Einstieg durch gemeinsames Erarbeiten der jeweils neuen Lernstoffe sowohl an der “Tafel” und zeitgleich am eigenen Arbeitsgerät zu erleichtern.
         
            Der erfolgreiche Besuch einer School wird stets durch ein Zertifikat bescheinigt, das in manchen Fällen als “credit points” in Studiengängen angerechnet werden konnte. Besonderes Augenmerk wird auf eine gute personelle Betreuung der TeilnehmerInnen durch zusätzliche TutorInnen und einen hohen Praxisanteil für Übungen gelegt. Die Kurse behandeln Basistechnologien wie XML, XSL, XQuery, Python, kontrollierte Vokabularien und Normdaten, editionsrelevante Kapitel der TEI, Metadaten, Text Mining, sowie allgemeine Webtechnologien wie HTML und JavaScript oder neuere Ansätze wie Graphentechnologien. Die konsequente Online-Bereitstellung von Vortragsfolien und Übungsaufgaben auf der Website des IDE ermöglicht auch nachträglich, sich Inhalte der Schools anzueignen und fügt das Angebot in die wachsende Zahl von online verfügbaren Tutorials ein (s.o.). Im Zuge der Schools wurden essentielle Technologien in Flyerform
            6
             kurz zusammenzufassen. Das auf den Schools vermittelte Wissen lässt sich so einerseits direkt nachnutzen, andererseits können diese Angebote auch zeitlich versetzt in andere Schools eingebunden werden.
         
         
            Das Poster wird die mit den Schools gewonnenen Erfahrungen der letzten Jahre zusammenfassen. Es vergleicht die IDE-Schools mit thematisch benachbarten Veranstaltungen,
            7
             analysiert Trends und Konstanten der curricularen Struktur, erhebt statistische Angaben über die BesucherInnen, präsentiert die Sicht der TeilnehmerInnen durch die Auswertung einer umfassenden Befragung und verortet das Angebot im Gesamtfeld der digitalen Editorik bzw. der Digital Humanities im Allgemeinen. Es leistet damit einen Beitrag für die Untersuchung der Vermittlungsformen und Lehrinhalte in der Ausbildungslandschaft der Digital Humanities außerhalb ordentlicher Studiengänge und die Auswirkungen dieser Ausbildungsformen auf Forschung und Karriere.
         
      
      
         
            
               https://dhd-blog.org/?p=6174
            
            
               http://www.culingtec.uni-leipzig.de/ESU_C_T/node/97
            
             Siehe z.B. Digital Humanities at Oxford Summer School (https://digital.humanities.ox.ac.uk/dhoxss/), Digital Humanities Summer Institute der University of Victoria (http://www.dhsi.org/courses.php).
             Siehe https://teach.dariah.eu/; zu den Schulungsmaterialien im Rahmen des Marie Sklodowska Curie Doktorandenprogramms DiXiT: http://dixit.uni-koeln.de/programme/materials/; zu den Lehrmaterialien von DARIAH-DE siehe exemplarisch “Digitale Textedition mit TEI” von Christof Schöch: https://de.dariah.eu/tei-tutorial; eine Workshop-Dokumentation unter https://www.lib.ncsu.edu/workshops/introduction-to-xml-and-digital-scholarly-editing-using-the-text-encoding-initiative-tei-1, ein Github-Repository unter https://github.com/slstandish/lrbs-scholarly-editing.
             Zur Dokumentation der Schools siehe https://www.i-d-e.de/aktivitaeten/schools/.
            
               https://www.i-d-e.de/publikationen/weitereschriften/xml-kurzreferenzen/.
             Zu digitalen Editionen siehe z.B. die Reihe „Edirom“ in Paderborn 2013-2018 (https://ess.uni-paderborn.de/) oder als Einzelveranstaltungen Madrid "Edición digital académica" 2015 (https://extension.uned.es/actividad/idactividad/9408) und 2016 (https://formacionpermanente.uned.es/tp_actividad/idactividad/8680, München “Digital Humanities” 2017 (https://dhmuc.hypotheses.org/summerschool-2017), Prag 2017 (https://praguebeast.hypotheses.org/program) und Grenoble 2018 (https://edeen.sciencesconf.org/).
         
         
            
               Bibliographie
               
                    Digital Humanities als Beruf. Fortschritte auf dem Weg zu einem Curriculum. Akten der DHd-Arbeitsgruppe 
                        
                     "
                  Referenzcurriculum Digital Humanities
                        
                     "
                  . Graz 2015.
                    
               Digital Humanities Course Registry. Dariah/Clarin 2014-2018. 
                        https://registries.clarin-dariah.eu/courses/
               
               
                  Fritze, Christiane / Rehbein, Malte (2012):
                  Hands-On Teaching Digital Humanities: A Didactic Analysis of a Summer School Course on Digital Editing, 
                        in: 
                        Hirsch, Brett D. (ed.): 
                  Digital Humanities Pedagogy: Practices, Principles and Politics [Online]. 
                        Cambridge: Open Book Publishers. 
                        
                     http://books.openedition.org/obp/1617
                  
               
               
                  Henny, Ulrike (2012): 
                  Digitale Editionen – Methoden und Technologien für Fortgeschrittene 
                        [Tagungsbericht zur IDE-School, Chemnitz 2012], in: 
                        H-Soz-Kult, 11.12.2012, 
                        www.hsozkult.de/conferencereport/id/tagungsberichte-4540
               
               
                  Locke, Brandon T. (2017): 
                  Digital Humanities Pedagogy as Essential Liberal Education: A Framework for Curriculum Development, 
                        in: DHQ 11.3 (2017). 
                        
                     http://www.digitalhumanities.org/dhq/vol/11/3/000303/000303.html
                  
               
               
                  Neuber, Frederike (2015): 
                  Spring in Graz – Sunshine and X-technologies 
                        [Bericht zur IDE-School Graz 2015], in: DiXiT Blog 26.4.2015. 
                        https://dixit.hypotheses.org/633
               
               
                  Sahle, Patrick (2008):
                  Digitale Editionen – Methodische und technische Grundfertigkeiten 
                        [Tagungsbericht zur IDE-School, Köln 2008], in: H-Soz-Kult, 21.11.2008, 
                        www.hsozkult.de/conferencereport/id/tagungsberichte-2353
               
               
                  Sahle, Patrick (2013): 
                  DH studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities. 
                        (= DARIAH-DE Working Papers Nr. 1). Göttingen: GOEDOC. 
                        http://nbn-resolving.de/urn.nbn.de.gbv:7-dariah-2013-1-5
               
            
         
      
   


10942	2019	
      
         
            Das OCR-Programm ocropy
            Die optische Zeichenerkennung (engl. Optical Character Recognition – OCR) von historischen Texten weißt oftmals niedrige Erkennungsraten auf. Mit einem gekonnten Preprozessing und ocropy (auch ocropus), einem modular aufgebauten Kommandozeilenprogramm auf Basis eines neuronalen long short-term memory Netzes, ist es möglich, deutlich bessere Ergebnisse zu erzielen. (Springmann 2015, S. 3; Vanderkam 2015) Ocropy ist in Python geschrieben und enthält u. a. Module zur Binarisierung (Erzeugung einer Rastergrafik), zur Segmentierung (Dokumentaufspaltung in Zeilen), zur Korrektur fehlerhafter Erkennungstexte, zum Training neuer Zeichen und natürlich zur Erkennung von Dokumenten (siehe Abbildung 1). Ein bedeutender Vorteil dabei ist, dass jedes Modul eine Reihe von nachvollziehbaren Einstellungsmöglichkeiten hat, um auf die individuellen Herausforderungen jedes Dokumentes einzugehen. Zusätzlich besteht die Möglichkeit ocropy auf die Erkennung einer bestimmten Schriftart, bzw. eines Zeichensatzes zu trainieren.
            
               
               Abbildung 1. Überblick zum Prozessablauf der Texterkennung mit den grundlegenden Software-Modulen
            
            Die Benutzung von ocropy als Kommandozeilenprogramms setzt jedoch den Umgang mit einer Consolen-Umgebung und eine grundlegende Kenntnis von Bash-Kommandos voraus: Für viele potenzielle NutzerInnen stellt dies eine erste Einstiegshürde dar, denn der NutzerInnenanteil von Linuxderivaten beträgt nur 3% (statista 2018), wobei die Gruppe an ShelluserInnen noch kleiner sein dürfte. Im Workshop wird diese Hürde abgebaut, indem alle Schritte „from zero to recognised textfile“ nachvollziehbar und zum Mitmachen aufzeigt wird. Insgesamt werden sechs Themengebiete behandelt, damit die TeilnehmerInnen des Workshops alle benötigten Informationen erhalten, um selbstständig Frakturschriften (oder andere Schriftarten) durch ocropy erkennen zu lassen.
         
         
            Ubuntu in der VirtualBox
            Für bisher ausschließliche NutzerInnen des Betriebssystem Windows oder Mac OS, ist es unverhältnismäßig, allein wegen ocropy Linux als Zweit- oder sogar Hauptsystem zu installieren. Durch die Verwendung einer 
                    VirtualBox und des Linux-Derivats 
                    Ubuntu kann dieser Schritt umgangen werden. Mit Hilfe einer virtuellen Maschine lässt sich ein Betriebssystem innerhalb eines anderen Betriebssystems emulieren. Das bringt den Vorteil mit sich, keine größeren Änderungen am System vornehmen zu müssen und die Software in einem geschützten virtuellen Rahmen testen zu können. Das Einrichten einer virtuellen Maschine ist daher für die meisten NutzerInnen das Fundament (und vielleicht auch der Einstieg) in Unix-basierte Entwicklerumgebungen. Dabei sind diverse kleinere Einstellungen zu beachten, vom Einschalten der Virtualisierung im BIOS bis hin zur Installation von gemeinsam genutzten Ordnern zwischen Host und Gast. Ubuntu als „Einstiegslinux“ eignet sich hervorragend für die ersten Schritte, da es eine hohe Benutzerfreundlichkeit aufweist und trotzdem alle wichtigen Features mitbringt, die benötigt werden.
                
         
         
            Repositorien für brauchbare Digitalisate
            OCR-Software erzielt bessere Ergebnisse mit hochauflösenden und fehlerfreien Digitalisaten. Bilddateien sollten mindestens eine Auflö
                  zentrale Verzeichnis digitalisierter Drucke
                oder das 
                    
                  Münchener DigitalisierungsZentrum
                bieten exzellente Anlaufstellen zur Beschaffung digitalisierter Drucke; aber auch Sammlungen wie das 
                    
                  Verzeichnis der im deutschen Sprachbereich erschienen Drucke des 16. - 19. Jahrhunderts
                der Universität- und Landesbibliothek Sachsen-Anhalt verfügen über frei zugängliche Digitalisate mit einer Auflösung bis zu 600 DPI. 
                
         
         
            Installation von ocropy
            Ocropy ist nicht in den nativen Quellen von den bekanntesten Linux-Derivaten enthalten, sondern muss von 
                    Github heruntergeladen und über ein Script installiert werden. Dabei ist die Version der Programmiersprache Python 2.7 zu beachten und die Abhängigkeiten einiger benötigter Module. Im Workshop wird die Installation begleitet und ein bereits auf Drucke des 18. Jahrhundert trainiertes Erkennungsmodul zur Verfügung gestellt.
                
         
         
            Preprocessing mit ScanTailor
            Eine Texterkennung ist nur so gut wie das Preprocessing des Digitalisates. Bilder, Initiale oder Flecken im Bild stören die Texterkennung und müssen entfernt werden. Darüber hinaus benötigt ocropy binarisierte (schwarz/weiß gerasterte) oder normalisierte Graustufenbilder zur Verarbeitung. Obwohl ocropy mit dem Modul ocropus-nlbin eine eigene Lösung zur Binarisierung von Bilddateien anbietet, hilft dies nicht in Bezug auf Nicht-Text-Elemente, wie Bilder oder schräge Spaltenlinien. Bearbeitungssoftware wie Gimp beinhaltet zwar alle benötigten Funktionen, ist jedoch in Bezug auf die serielle Verwendung bei Textdigitalisaten ineffizent. Im Workshop wird die Software 
                    ScanTailor als passgenaues Preprocessing-Tool zur Vorbereitung der Digitalisate favorisiert. ScanTailor ist wie dafür gemacht gescannte Texte in eine einheitliche Form zu bringen und beinhaltet (zum Teil vollständig automatisierte) Funktionen wie 
                
            
               der Aufsplittung von Spalten oder Seiten
               das Ausrichten der Seite
               des Auswählens des Inhalts
               der Möglichkeit Bereich zu füllen
               der Entzerrung gekrümmter Seiten und
               der Anpassung des Schwellwertes (threshold) bei der Binarisierung.
            
            Außerdem werden Hinweise zu den grundlegenden Eigenschaften eines guten Eingangsbildes gegeben, z. B. in Bezug auf Schwellwert oder DPI-Zahl.
         
         
            Entwicklung einer Pipeline zur Texterkennung
            Die ocropy-Module funktionieren am effizientesten innerhalb einer Pipeline. Ausgehend von der Konvertierung unpassender Dateiformate der Roh-Digitalisate bis hin zur Erstellung einer Korrektur-HTML für die Verbesserung der falsch erkannten Zeichen bietet die Linux-Shell zusammen mit ocropy und dem Programm 
                    ImageMagick alle benötigten Werkzeuge. So lassen sich auch große Mengen an Bilddateien stapelweise verarbeiten. In einem Script werden Befehle zur Bildkonvertierung, Zeilenauftrennung, Texterkennung und Textkonvertierung in Reihe geschaltet, um eine stapelhafte Verarbeitung zu ermöglichen. Der Workshop bietet zwei vorgefertigte Scripte zum Gebrauch an und erklärt ihren Ablauf, um eventuelle Anpassungen an die eigenen Bedürfnisse vornehmen zu können.
                
         
         
            Training unbekannter Schriftarten
            Die eigentliche Stärke von ocropy ist die Möglichkeit Erkennungsmodule für Schriftarten zu trainieren. Die dazu bereitgestellte Ground Truth Data bestimmt maßgeblich die Leistungsfähigkeit der Erkennungsmodule. Dabei stellt sich die Frage, wie eine gute Ground Truth im wörtlichen Sinne auszusehen hat? Wie „schmutzig“ dürfen die Daten sein? Sind abgeschnittene Serifen, fehlende Bögen oder i-Punkte ein Problem? Welche Zeichen sollten verwendet werden, um Abbreviationen oder Abkürzungszeichen zu kodieren? Darüber hinaus trainiert ocropy sich nicht permanent besser, sondern baut das neurale Netz zeitweise mit negativen Auswirkungen für die Erkennungsraten um (siehe Abbildung 2). Im Workshop wird ein Script zur Identifikation des besten Trainingsmoduls vorgestellt, um das Beste aus ocropy herauszuholen.
            
               
               
                  Abbildung 2. Trainingsprozess von ocropus-rtrain mit Ground Truth von Zedlers Universallexikon. training = Ground Truth anhand derer das Modul trainiert wurde, testing = unbekanne Ground Truth zum Test der Performance.
					
            
         
         
            Ablauf
            Der Workshop richtet sich vorrangig an Anfänger und leicht fortgeschrittene NutzerInnen im Umgang mit Linux und der Console. Es werden keine Vorkenntnisse in Bash oder Python benötigt und alle im Kurs vorgestellte Software, 
                    Scripte und Daten stehen frei zur Verfügung. Der Workshop möchte alle an Interessierten da abholen, wo sie stehen und versucht durch ein schrittweises Vorgehen an die Vorzüge der Consolen-Benutzung und kommandozeilenbasierte Software heranzuführen. Teilnehmer sollten ihr eigenes Notebook mitbringen, auf dem sie auch Administrator-Rechte besitzen. Des Weiteren wird ein Internetzugang benötigt, um fehlende Software oder Abhängigkeiten herunterladen zu können. Größere Softwarepakete (VirtualBox, Ubuntu) werden auch auf USB-Sticks zur Verfügung gestellt, sollten aber nach Möglichkeit vorher selbstständig heruntergeladen werden. Es können je nach Erfahrungsstand der TeilnehmerInnen mit Console und Linux 20 bis 25 Personen betreut werden. Der Workshop dauert drei bis vier Stunden.
                
         
      
      
         
            
               Bibliographie
               
                  ImageMagick (2018):
                  Convert, Edit, Or Compose Bitmap Images @ ImageMagick, 
                        URL: 
                        https://www.imagemagick.org/, [zuletzt besucht am 14.10.2018].
                    
               
                  MDZ (2018):
                  Münchner DigitalisierungsZentrum, 
                        Bayerische Staatsbibliothek, München, URL: 
                        https://www.digitale-sammlungen.de/, [zuletzt besucht am 12.10.2018].
                    
               
                  ocropy (2018):
                  Python-based tools for document analysis and OCR, 
                        URL: 
                        https://github.com/tmbdev/ocropy, [zuletzt besucht am 14.10.2018].
                    
               
                  ScanTailor (2018):
                  ScanTailor, 
                        http://scantailor.org/, [zuletzt besucht am 14.10.2018].
                    
               
                  Springman, Uwe (2015):
                  Ocrosis. A high accuracy OCR method to convert early printings into digital text, Center for Information and Language Processing (CIS), 
                        Ludwig-Maximilians-University, Munich, URL: 
                        http://cistern.cis.lmu.de/ocrocis/tutorial.pdfm [zuletzt besucht am 14.10.2018].
                    
               statista, Marktanteile der führenden Betriebssysteme in Deutschland von Januar 2009 bis Juli 2018, URL: https://de.statista.com/statistik/daten/studie/158102/umfrage/marktanteile-von-betriebssystemen-in-deutschland-seit-2009/, [zuletzt besucht am 10.10.2018].
               
                  Vanderkam, Dan (2015):
                  Extracting text from an image using Ocropus, 
                        URL: http://www.danvk.org/2015/01/09/extracting-text-from-an-image-using-ocropus.html, [zuletzt besucht am 10.10.2018].
               
                  VD (2018):
                  Digitale Sammlungen des 16. bis 19. Jahrhunderts, 
                        Universitäts- und Landesbibliothek Sachsen-Anhalt, Halle (Saale), URL: 
                        http://digitale.bibliothek.uni-halle.de/, [zuletzt besucht am 14.10.2018].
                    
               
                  VirtualBox (2018):
                  Oracle VM VirtualBox, 
                        URL: 
                        https://www.virtualbox.org/, [zuletzt besucht am 14.10.2018].
                    
               
                  Ubuntu (2018):
                  The leading operating system for PCs, IoT devices, servers and the cloud | Ubuntu, 
                        URL: 
                        https://www.ubuntu.com/, [zuletzt besucht am 14.10.2018].
                    
               
                  ZVDD (2018):
                  Zentrales Verzeichnis Digitalisierter Drucke, 
                        Georg August Universität Göttingen, Niedersächsische Staats- und Universitätsbibliothek Göttingen, Göttingen, URL: 
                        http://www.zvdd.de/, [zuletzt besucht am 12.10.2018].
                    
            
         
      
   


10943	2019	
      
         
            EINLEITUNG
            In den vergangenen 30 Jahren ist ein beträchtlicher Teil des in Deutschland gedruckten Materials aus der Zeit von 1500 bis ca. 1850 in mehreren, durch die Deutsche Forschungsgemeinschaft (DFG) geförderten Kampagnen in den Verzeichnissen der im deutschen Sprachraum erschienenen Drucke des 16.-18. Jahrhunderts (VD16, VD17, VD18) zunächst nachgewiesen und seit 2006 digitalisiert worden. Zusätzlich vorliegender Volltext wird mittlerweile auf breiter disziplinärer Front als Schlüssel zu einer ganzen Reihe von geistes- und kulturwissenschaftlichen Forschungsfragen gesehen und gilt zunehmend als elementare Voraussetzung für die Weiterentwicklung der transdisziplinär arbeitenden Digital Humanities. Deshalb werden bereits an verschiedenen Stellen OCR-Verfahren angewendet; viele dieser Unternehmungen haben allerdings noch sehr starken Projektcharakter. Die informationswissenschaftliche Auseinandersetzung mit OCR kann an der großen Zahl wissenschaftlicher Studien und Wettbewerbe ermessen werden, die Möglichkeiten zur Verbesserung der Textgenauigkeit sind in den letzten Jahrzehnten enorm gestiegen. Der Transfer der auf diesem Wege gewonnenen, oftmals sehr vielversprechenden Erkenntnisse in produktive Anwendungen ist jedoch häufig nicht gegeben: Es fehlt an leicht nachnutzbaren Anwendungen, die eine qualitativ hochwertige Massenvolltextdigitalisierung aller historischen Drucke aus dem Zeitraum des 16. bis 19. Jahrhundert ermöglichen. 
                    Auf dem DFG-Workshop „Verfahren zur Verbesserung von OCR-Ergebnissen“ (Deutsche Forschungsgemeinschaft 2014) im März 2014 formulierten Expertinnen und Experten daher folgende Desiderate um die Weiterentwicklung von OCR-Verfahren zu ermöglichen. Es bestehe eine dringende Notwendigkeit für freien Zugang zu historischen Textkorpora und lexikalischen Ressourcen zum Training von vorhandener Software zur Texterkennung bestehe. Ebenso müssen Open-Source-OCR-Engines zur Verbesserung der Textgenauigkeit weiterentwickelt werden, wie auch Anwendungen für die Nachkorrektur der automatisch erstellten Texte. Daneben sollten Workflow, Standards und Verfahren der Langzeitarchivierung mit Blick auf zukünftige Anforderungen an den OCR-Prozess optimiert werden. Als zentrales Ergebnis dieses Workshops stand fest, dass eine koordinierte Fördermaßnahme der DFG notwendig ist. Die „Koordinierte Förderinitiative zur Weiterentwicklung von Verfahren der Optical Character Recognition (OCR)“, kurz OCR-D, begann im September 2015 und versucht seitdem einen Lückenschluss zwischen Forschung und Praxiseinsatz, indem für die Entwicklungsbedarfe Lösungen erarbeitet und der aktuelle Forschungsstand zur OCR mit den Anforderungen aus der Praxis zusammengebracht werden. 
                
         
         
            ARBEITEN IM PROJEKT OCR-D
            Das Vorhaben hat zum Ziel, einerseits Verfahren zu beschreiben und Richtlinien zu erarbeiten, um einen optimalen Workflow sowie eine möglichst weitreichende Standardisierung von OCR-bezogenen Prozessen und Metadaten zu erzielen, andererseits die vollständige Transformation des schriftlichen deutschen Kulturerbes in digitale Forschungsdaten in (xml-strukturierter Volltext) konzeptionell vorzubereiten. Am Ende des Gesamtvorhabens (d.h. unter Einschluss der Modulprojektphase) sollte ein in allen Aspekten konsolidiertes Verfahren zur OCR-Verarbeitung von Digitalisaten des schriftlichen deutschen Kulturerbes stehen und eine Dokumentation, die Antworten auf die damit verbundenen technischen, informationswissenschaftlichen und organisatorischen Probleme und Herausforderungen gibt sowie Rahmenbedingungen formuliert.
            Das Projekt ist in zwei Phasen geteilt: In der ersten Phase hat das Koordinierungsgremium von OCR-D Bedarfe für die Weiterentwicklung von OCR-Technologien analysiert und sich intensiv mit den Möglichkeiten und Grenzen der Verfahren zur Text- und Strukturerkennung auseinandergesetzt. Zahlreiche Gespräche mit ExpertInnen aus Forschungseinrichtungen und Bibliotheken sowie Sichtung vorhandener Werkzeuge aber auch Betrachtung vorhandener Textsammlungen sowie aktueller und geplanter Digitalisierungsvorhaben mündeten in der Erkenntnis, dass der Lückenschluss zwischen Wissenschaft und Praxis das primäre Desiderat im Bereich der Textdigitalisierung darstellt. Zudem hat sich im Lauf der ersten Projektphase eine technologische Wende auf dem Gebiet der Zeichenerkennung vollzogen - an die Stelle traditioneller Verfahren der Mustererkennung, die auf einer Segmentierung von Textabschnitten in Zeilen, Wörter und schließlich einzelne Glyphen basieren, die anschließend aufgrund charakteristischer Merkmale (z.B. Steigung an Kanten) erkannt werden, ist eine zeilenorientierte Sequenzklassifizierung auf Basis statistischer Modelle, insbesondere verschiedener Arten neuronaler Netze (sog. 
                    Deep Learning), getreten. Grund für diesen Technologiewechsel ist die vielfach nachgewiesene Überlegenheit segmentierungsfreier Erkennungsverfahren bezüglich der resultierenden Textgenauigkeit. Diese Überlegenheit gilt insbesondere für schwierige, historische Vorlagen. Dieser Technologiewandel hat sich bisher nicht oder nur äußerst begrenzt auf die Digitalisierungspraxis ausgewirkt. Der Grund dafür liegt vor allem in den bisher bestehenden Hürden beim Einsatz verfügbarer OCR-Lösungen auf Basis neuronaler Netze. Ohne weitreichende projektspezifische Anpassungen ist ein produktiver Einsatz derzeit nicht möglich. Das betrifft unter anderem die Erstellung passender Erkennungsmodelle, die durch das Trainieren eines neuronalen Netzes auf Basis ausgewählter Ground-Truth-Daten generiert werden. Dafür sind zum einen hochqualitativer und umfangreicher Ground Truth aber auch Erfahrungen bzgl. freier Parameter wie z.B. Anzahl der Trainingsschritte, Lernrate, Modelltiefe unabdingbar. Aus OCR-D heraus ist daher ein Datenset mit Trainings- und Ground-Truth-Daten entstanden, welches für Trainings und Qualitätsanalysen im Vorhaben selber genutzt wird aber auch durch andere Forschungsprojekte nachgenutzt werden kann. Neben der Qualität der Zeichenerkennung sind es vor allem Umfang und Korrektheit der strukturellen Annotationen, die die Utilität eines Volltexts für wissenschaftliche Kontexte determinieren. Auch im Bereich der automatischen Layouterkennung (OLR) gab es innerhalb des bisherigen Projektzeitraums vielversprechende Forschungsergebnisse durch den Einsatz innovativer statistischer Verfahren. Der Übertrag in die Praxis in Form nachnutzbarer Software ist hier jedoch noch nicht gegeben. Kommerzielle OCR-Lösungen ignorieren diesen Bereich weitestgehend und bieten nur minimale Strukturinformationen auf Seitenebene (Text, Tabelle, Abbildung etc.) an. Tiefergehende strukturelle Auszeichnungen (Kapitelstruktur, Bildunterschriften, Inhaltsverzeichnisse) werden daher manuell erfasst und in METS/MODS repräsentiert. Eine Verknüpfung zwischen Struktur und Volltext findet, obwohl technisch möglich, in vielen Digitalisierungsvorhaben nicht statt. Für die philologische, editorische oder linguistische Wissenschaftspraxis bedeutet das eine massive Einschränkung die bspw. eine sinnvolle Transformation in hochstrukturierte Formate wie TEI verhindert. 
                
            Die Erkenntnisse dieser Bedarfsanalyse mündeten in einem OCR-D-Funktionsmodell, welches den Rahmen für die Modulprojekt-Ausschreibung der DFG im März 2017 bot. Vor diesem Hintergrund wurden acht Modulprojekte bewilligt die seit 2018 an Lösungen zur Bildvorverarbeitung, Layouterkennung, Textoptimierung (inkl. Nachkorrektur), zum Modelltraining und zur Langzeitarchivierung der OCR-Daten arbeitet. Die Entwicklungen schöpfen dabei das Potential innovativer Methoden für den gesamten Bereich der automatischen Texterkennung für die Massenvolltextdigitalisierung von historischen Drucken aus. Sie werden anschließend nahtlos in den OCR-D-Workflow zur optimierten OCR-basierten Texterfassung integriert. Das so entstehende OCR-D-Softwarepaket steht damit Kultureinrichtungen wie Forschenden für die automatische Texterkennung als Open-Source-Software zur Verfügung.
            Die meisten Arbeiten werden im Sommer 2019 abgeschlossen sein, aber bereits Anfang des Jahres wird die Alpha-Version einen Einblick in die zu erwartende Gesamtlösung bieten können.
         
         
            ZIEL DES WORKSHOPS
            Der Workshop soll neben der Vorstellung des Projektes und der Software die Gelegenheit bieten selber die Software zu testen und zugleich über Optimierungen und Anforderungen seitens der Wissenschaft an diese Technologien zu diskutieren. Teilnehmende erhalten somit einen exklusiven Einblick in die Entwicklungsarbeit und haben die Möglichkeit proaktiv auf die Arbeiten Einfluss zu nehmen, die Ihren späteren Forschungsalltag begleiten und verbessern soll.
                
         
         
            PROGRAMM
            Der Workshop gliedert sich in drei Abschnitte:
            
               Vorstellung des Projekts OCR-D, des Ground-Truth-Datensets und der Guidelines (30min)
               Demonstration der Eigenentwicklung und eines Test-Workflows (120min)
               Diskussion zu Anforderungen und Optimierungen aus Sicht der Digital Humanities (30min)
            
            Der erste Abschnitt stellt die Hintergründe zum Vorhaben vor und geht auf Besonderheiten der Volltextdigitalisierung von historischen Beständen ein. Anschließend wird das Trainings- und Ground-Truth-Datenset präsentiert, das im Rahmen von OCR-D auf- und weiter ausgebaut wird. Besonders die dazu entwickelten Guidelines geben Hinweise für eine spätere Nachnutzung und die Erstellung eigener Ground-Truth-Daten in anderen Projekten. Der Fokus des Workshops liegt auf dem zweiten Abschnitt, in welche der derzeitige Entwicklungsstand präsentiert wird. Die benötigten Test-Dateien werden auf 
                GitHub1 veröffentlicht. Abgerundet wird der Workshop durch eine Diskussionsrunde zu Anforderungen aus der Wissenschaft heraus an OCR-Techniken und die dafür eingesetzte Software.
                
         
         
            VORAUSSETZUNG
            Teilnehmende benötigen einen eigenen Laptop mit Internetanbindung und Ubuntu 18.04 als Betriebssystem. Alternativ kann auch Windows/Mac OSX mit der Software VirtualBox verwendet werden. Die VM wird den Teilnehmenden vom OCR-D-Projekt vor Ort zur Verfügung gestellt. Die Anzahl der Teilnehmenden ist auf 20-25 begrenzt. Python- und Linux-Kommandozeilen-Kenntnisse sind wünschenswert
         
      
      
         
             OCR-D Git-Hub: https://github.com/OCR-D/
         
         
            
               Bibliographie
               
                  Deutsche Forschungsgemeinschaft (2014):  
                        Workshop 
                        Verfahren zur Verbesserung von OCR-Ergebnissen. 
                        Protokoll zu den Ergebnissen und Empfehlungen des Workshops. 
                        
                     http://www.dfg.de/download/pdf/foerderung/programme/lis/140522_ergebnisprotokoll_ocr_workshop.pdf
                   [Zuletzt abgerufen 07.01.2019]
                    
            
         
      
   


10946	2019	
      
         Gesellschaften fügen sich aus Individuen zusammen. Das gilt auch für die Vergangenheit, aus der die Mehrzahl der Individuen nur schlecht bis gar nicht dokumentiert ist. Es hat sich deshalb ein eigenständiger historischer Forschungsbereich entwickelt, die “Prosopographie”, die sich der Aggregation von Einzelinformationen zu Individuen aus historischen Quellen und ihrer Auswertung widmet (Keats-Rohan 2007). Dieses Forschungsgebiet hat früh digitale Methoden eingesetzt. Der Beitrag widmet sich der Frage, ob die Methoden vergleichbar zu IIIF (International Image Interoperability Framework) in ein „International Proposography Interoperability Framework“ (IPIF) integriert werden können.
         Ein IPIF muss von den Personendatenbanken abweichen, die sich als kontrollierte Vokabularien und Referenzen für Linked Open Data in den Digital Humanities etabliert haben (GND/VIAF, deutsche-biographie), bzw. im Begriff sind, sich zu etablieren (wikidata). Diese berücksichtigen nämlich nicht den Vorgang, mit dem Informationen über eine Person aus historischen Quellen aggregiert werden. Der Ansatz weicht damit auch von der „personography“ der TEI ab, die, wie die Linked-Data-Ressourcen, eine Person mit einer Liste an Eigenschaften beschreiben. Ein IPIF muss dagegen ein Modell realisieren, für das Bradley/Short (2005) die Bezeichnung „Factoid“-Model eingeführt haben. Es geht von drei Informationseinheiten aus: Quelle, Individuum und Aussagen der Quelle über das Individuum. John Bradley hat das Modell mehreren Projekten des King’s College London zu Grunde gelegt (PASE, DPRR, CCEd). Auch das Persondendatenrepositorium (PDR) der Berlin-Brandenburgischen Akademie der Wissenschaften (Neumann et al. 2011) und Projekte, die die Software der BBAW weitergenutzt haben, verwenden das gleiche Modell, auch wenn das PDR nicht explizit auf Bradley referenziert. Ebenso verwendet das Repertorium Academicum Germanicum ein solches dreiteiliges Modell (Andresen 2008). 
         Das dreiteilige Modell impliziert auch, dass (auch widersprechende) Aussagen über dasselbe Individuum aus verschiedenen Quellen an verschiedenen Orten publiziert werden können. Es erscheint also als ein Paradebeispiel für das 
                Web of Data des W3C. Das 
                Web of Data ist die Fortführung der Semantic-Web-Aktivitäten des W3C. Es konzentriert sich auf die Öffnung von Datenbanken und erhebt insbesondere den Anspruch, individuelle kleine Datenmengen als RDF über das Semantic Web abfragbar zu machen. Technisch ist RDF, die Grundlage des 
                Web of Data, eine weit verbreitete und gut unterstützte Technologie. Es ist deshalb auch eine Technologie, mit deren Hilfe immer häufiger Maschinen auf prosopographische Datenbanken zugreifen können. Deshalb haben Bradley/Pasin (2015) eine CIDOC-CRM basierte Version des Factoid-Modells vorgeschlagen und entsprechende Ontologien veröffentlicht (Bradley 2017). Das Basismodell ist aber auch mit anderen Vokabularien realisiert worden: SNAP verwendet z.B. Vokabularien aus dem 
                Linking-Ancient-Wisdom-Projek
               1
            ). Das King’s Digital Lab hat jüngst mit Hilfe von 
                Ontop
            
               2
             die prosopographische Datenbank zur römischen Republik als LOD-Ressource incl. eines SPARQL-Endpoints 
                veröffentlicht.
               3
            
         
         Diese Strategie teilt jedoch das Problem vieler RDF-Ressourcen: Die technische Pflege eines SPARQL-Endpoints ist sehr anspruchsvoll. SPARQL-Endpoints sind hä
               4
             und Core 
            API
               5
             liegen auch Vorschläge vor, derartige API-Definition standardisiert zu beschreiben, so dass die Implementation von einschlägigen API-Anbietern und API-Konsumenten teilweise sogar automatisiert werden 
            kann.
               6
             Aus Sicht des Software-Engineering erscheint es also angemessen, auf eine eigene API-Definition statt auf einen SPARQL-Endpoint zurückzugreifen. Gleichzeitig wird es damit erschwert, Daten aus verschiedenen Datenquellen zu aggregieren, da für jeden Datenanbieter ein eigener API-Konsument programmiert werden müsste. Im Bereich der Bibliotheken hat sich deshalb für die Bereitstellung von Bildern von Büchern mit IIIF eine Kombination aus einem Datenstandard und einer Adressierungs-API durchgesetzt. Es ist an der Zeit, auch für personenbezogene Daten über einen solchen technischen Standard nachzudenken, der die Implementation von Anwendungen erleichtert und die Daten auch praktisch interoperabel macht.
            
         Ein solcher Standard muss von konkreten Anwendungsszenarien ausgehen. Sie können unter den Überschriften „Biographical Lexicon“, „Careers”, „Source Editing“, „Fact Checking“, „New Interpretation“, „Publish a Database”, „Integrate Other Databases“, „Analysis“, „Tool User“, „Tool Builder” zusammengefasst werden. Die Szenarien bilden sowohl Forschung mit prosopographischen Daten wie die Erzeugung solcher Daten ab. Zusätzlich achten die Szenarien darauf, nicht nur explizit prosopographische Workflows zu berücksichtigen, sondern schließen auch wissenschaftliches Edieren als Szenario mit ein, in dem der edierte Text als Beleg für eine Person betrachtet werden kann. In einem Workshop in Wien im Februar 2017 haben Forscher aus dem Themengebiet der Prosopographie religiöser Orden solche Anwendungsszenarien diskutiert und einen Entwurf für eine API entwickelt.
         Ein Ergebnis dieser Arbeit ist eine nach den Standards von OpenAPI beschriebenen Definition einer prosopographischen 
            API.
               7
             Die API baut auf dem dreiteiligen Factoid-Modell auf und erlaubt den Zugriff auf Personen, Aussagen, Quellen und ihr Aggregat, einem „Factoid“. Für alle diese Objekte gibt es eigene Pfade zur Suche und Ausgabe der Daten über die zu ihnen abgelegten IDs. Im Kern der API steht deshalb der Zugriff auf Factoide 
            (/factoid). Sie können individuell über bekannte IDs adressiert werden 
            (/factoid/id). Wichtiger sind aber inhaltliche Filtermöglichkeiten. Sie ergeben sich einfach aus den Eigenschaften des Factoids, als Aussage über eine Person. Die Parameter 
                s
            , 
            st und 
                f lassen also die Suche in den Inhalten der mit dem Factoid verknüpften Quellen 
                (source), Aussagen 
                (statement) und den Metadaten des Factoids selbst 
                (factoid) zu. Dabei ist der Standard eine Volltextsuche. Ebenso lassen sich die Quellen und Personen abfragen. Als Parameter können aber auch Identifikatoren für die einzelnen Informationsgruppen übergeben werden, also z.B. mit 
                /statement/?p_id=Placidus_Seiz alle Aussagen über die Person mit einem Identifikator „Placidus_Seiz“ in einem beliebigen Kontext. Die Anwendung liefert dann ein JSON-Objekt zurück, in dem diese Aussagen formalisiert sind. Zu jeder Aussage gehört eine ID, mit der Entwickler z.B. über die API überprüfen können, woher die jeweilige Aussage stammt.
            
         Als Rückgabewert der API-Definition sind JSON-Serialisierungen vorgesehen. Die Statements können Daten als Text (z.B. der Quelle) ebenso wie strukturiert als Graph enthalten. Die Graphen sollen den Spezifikationen von JSON-LD folgen. Damit können zwei Ziele erreicht werden: Erstens ist damit die Ausgabe der API direkt in Linked-Open-Data-Umgebungen nutzbar, kann prinzipiell auch in einer FROM-Klausel einer SPARQL-Abfrage integriert werden oder in Caching-Mechanismen wie im 2011 als Linked Data Middleware von Virtuoso vorgeschlagenen URI-Burner verwendet werden. Zweitens wird damit ein Standard verwendet, der die Referenzierung der verwendeten Vokabularien und ihre formale Beschreibung mit RDFS und OWL ermöglicht.
         Der Workshop in Wien hat als Kernproblem eines echten Datenaustausches die divergierenden Datenmodelle für die Einzelaussagen über die Individuen identifiziert. Während die Individuen selbst im Factoid-Modell keine beschreibenden Metadaten tragen und damit kaum Probleme beim Datenaustausch erzeugen, sind für die Aussagen über die Individuen je nach Projekt, Verwendungszweck und Forschungsdomäne eine Vielfalt von Vokabularien im Einsatz. Einen Ausweg aus dieser Situation bietet die 2017 gegründete 
            dataforhistory-Initiative.
               8
             Die Initiative arbeitet daran projekt- und domänenspezifische Modellierungen zu erleichtern, die zum CIDOC CRM kompatibel sind. Die derzeitige API-Definition sieht deshalb vor, dass die zurückgegebenen Daten eine Referenz auf ein Schema (in JSON-LD als 
                @context) enthalten müssen, das die verwendeten Klassen und Eigenschaften auf Definitionen im CIDOC CRM abbildet, der es der die API konsumierenden Anwendung erlaubt, die Daten als CIDOC CRM zu interpretieren und darauf aufbauende Operationen durchzuführen. Ergänzend dazu ist ein Parameter 
                format=json/cidoc-crm vorgesehen, bei dem die Transformation serverseitig stattfindet. Die Abbildung auf CIDOC CRM soll insbesondere die grundlegenden Suchoperationen ermöglichen, die Katerina Tzompanaki und Martin Doer 2012 formuliert haben und die im Projekt 
                researchspace
               9
             realisiert werden. Die API definiert die Objekteigenschaft 
                graph für die strukturierte Repräsentation der Daten über Personen.
            
         John Bradley und Michele Pasin haben 2015 eine OWL basierte Ontologie vorgestellt, in der eine „temporal entity documented“ (TED) als Ereignis (E4 und E5 im CIDOC-CRM) oder als eine zeitliche Einheit oder klare zeitliche Grenzen (E3: condition, state) modelliert sind. Das entspricht dem Stand der Diskussion über prosopographische Datenmodelle (Lind 1994, Andresen 2008, Tuominen / Hyvönen / Leskinen 2018).
         Nicht zuletzt der Erfolg von IIIF belegt, dass eine solche API aber auch Referenzimplementationen benötigt. Dabei ist entsprechend der oben beschriebenen Benutzungsszenarien sowohl an Ressourcen zu denken, die Daten bereitstellen, als auch an Anwendungen, die diese Daten konsumieren. Die Nachnutzung des „Archiveditors“, eines zunächst projektinternen Werkzeugs der BBAW, in anderen Projekten zeigt, dass dabei nicht nur an Datenextraktion und –anzeige sondern auch an Datengenerierung zu denken ist. Im Rahmen der Arbeit an der Personendatenbank der Österreichischen Akademie der Wissenschaften ist deutlich geworden, dass gerade automatische Informationsextraktion von „Personenrelationen“ (Schlögl et al. forthcoming, Schlögl et al. 2018) von einer solchen API profitieren kann. Die automatisch generierten Aussagen können als eigenständige Factoide in die Personendatenbanken eingehen. Die Metadaten des Factoids und die Referenz auf die verwendete Quelle stellen sicher, dass sie als automatisch generierte Daten identifizierbar bleiben. Der Vortrag wird Beispiele für Datenangebote aus dem Umfeld mittelalterlicher Urkunden (Register der Urkundenempfänger von Papsturkunden nach den Regesten von August Potthast, Daten aus monasterium.net) und Steuererhebungen (England) vorstellen, und Prototypen für Anwendungen benennen, welche die mit der API bereitgestellten Daten konsumieren können.
      
      
         
            
                http://lawd.info/ontology/
            
             https://github.com/ontop/ontop
            
               
                  http://romanrepublic.ac.uk/rdf
               , Dokumentation von John Bradley: 
                  http://romanrepublic.ac.uk/rdf/doc
               
            
             https://www.openapis.org/ 
             http://www.coreapi.org
             z.B. das Python-Framework Flask in Verbindung mit 
                  https://github.com/zalando/connexion
               , vgl. weitere Tools: 
                  https://swagger.io/tools/open-source/open-source-integrations/
               
            
             https://github.com/GVogeler/prosopogrAPhI
             http://dataforhistory.org
             https://www.researchspace.org/
         
         
            
               Bibliographie
               
                  Andresen, Suse (2008):
                  Das 'Repertorium Academicum Germanicum'. Überlegungen zu einer modellorientierten Datenbankstruktur und zur Aufbereitung prosopographischer Informationen der graduierten Gelehrten des Spätmittelalters, 
                        in: 
                        Sigrid Schmitt u. Michael Matheus (eds.):
                  Städtische Gesellschaft und Kirche im Spätmittelalter (Geschichtliche Landeskunde 62). Stuttgart: Steiner 17-26.
                    
               
                  Bradley, John (2017):
                  Factoids. A site that introduces Factoid Prosopograph, 
                        
                     http://factoid-dighum.kcl.ac.uk/
                   und 
                        
                     https://github.com/johnBradley501/FPO
                  
               
               
                  Bradley, John / Pasin, Michele (2015):
                  Factoid-based Prosopography and Computer Ontologies. Towards an integrated approach, 
                        in: DSH 30,1: 86-97.
                    
               
                  Bradley, John / Short, Harold (2005):
                  Texts into databases. The Evolving field of New-style Prosopography, 
                        in: LLC 20, suppl. 1: 3-24.
                    
               
                  CCEd: 
                  Clergy of the Church of England Database, King’s College London 
                        
                     http://theclergydatabase.org.uk/
                  
               
               
                  DPRR: 
                  Digital Prosopography of the Roman Republic, King’s College London 
                        
                     http://romanrepublic.ac.uk/
                  
               
               
                  Keats-Rohan, Katherine S.B. (ed.) (2007): 
                  Prosopography. Approaches and Applications. A Handbook 
                        (Prosopographica et genealogica 13). Oxford: P&G.
                    
               
                  Lind, Gunner (1994):
                  Data Structures for Computer Prosopography, 
                        in: Yesterday: Proceedings from the 6th International Conference of the Association of History and Computing, Odense 1991. Odense: University Press of Southern Denmark. 77-82.
                    
               
                  Neumann, Gerald / Körner, Fabian / Roeder, Torsten / Walkowski, Niels-Oliver (2011):
                  Personendaten-Repositorium, 
                        in: Berlin-Brandenburgische Akademie der Wissenschaften. Jahrbuch 2010: 320-326.
                    
               
                  PASE:
                  Prosopography of Anglo-Saxon England, King’s College London, URL: 
                        
                     http://www.pase.ac.uk/jsp/index.jsp
                  
               
               
                  Schlögl, Matthias / Katalin Lejtovicz (2018):
                  A Prosopographical Information System (APIS), 
                        in: 
                        Antske Fokkens / ter Braake Serge / Sluijter, Ronald / Arthur, Paul / Wandl-Vogt, Eveline (eds.): 
                  BD-2017. Biographical Data in a Digital World 2017. Proceedings of the Second Conference on Biographical Data in a Digital World 2017. 
                        Linz, Austria, November 6-7, 2017. Budapest: CEUR (CEUR Workshop Proceedings 2119): 53-58.
                    
               
                  Schlögl, Matthias / Lejtovicz, Katalin / Bernád, Ágoston Zénó / Kaiser, Maximilian / Rumpolt, Peter (2018):
                  Using deep learning to explore movement of people in a large corpus of biographies. 
                        Zenodo. 
                        
                     http://doi.org/10.5281/zenodo.1149023
                  
               
               
                  Tuominen, Jouni / Hyvönen, Eero / Leskinen, Petri (2018):
                  Bio CRM. A Data Model for Representing Biographical Data for Prosopographical Research, 
                        in: BD-2017. Biographical Data in a Digital World 2017, hg. v. Antske Fokkens, Serge ter Braake, Ronald Sluijter, Paul Arthur, Eveline Wandl-Vogt, Budapest: CEUR (CEUR Workshop Proceedings 2119): 59-66. 
                    
               
                  Tzompanaki, Katerina / Doerr Martin (2012):
                  Fundamental Categories and Relationships for intuitive querying CIDOC-CRM based repositories, 
                        Technical Report ICS-FORTH/TR-429, April 2012, 
                    
            
         
      
   


10957	2019	
      
         Geographische Angaben können in historischen Kontexten nicht als simple 2-dimensionale Daten (Längen- und Breitengrade) verstanden werden. Punkte die auf der Karte nur wenige Kilometer voneinander entfernt sind waren vor 500 Jahren wegen geographischer Hürden (Berge, Schluchten, Flüsse etc.) vielleicht ewig weit voneinander entfernt. Ähnliches gilt für politische Grenzen: Vor 30 Jahren waren Orte in Deutschland die heute wenige Autominuten voneinander entfernt liegen durch den eisernen Vorhang voneinander getrennt. Kamzelak (2018) hat es so formuliert: “Orte haben eine historisch-politische Dimension, die bei einer übergreifenden Registererfassung erst sichtbar zu einem Problem wird. [...] Für die Visualisierung von Briefen etwa sind historische Karten ein Desiderat; generell auch Geodaten für Flächen. Und alle mit Geodaten versehenen Einträge müssen mit einem Zeitstempel kombiniert sein, denn beispielsweise die Altstadt von Jerusalem ist eben heute nicht am selben Ort wie vor 2.000 Jahren.”.
         Trotzdem arbeitet eine Vielzahl an Digital Humanities Projekten auch heute noch mit 2-dimensionalen geographischen Angaben. Unserer eigenen Erfahrung nach liegt das Hauptsächlich an der Verfügbarkeit von Daten und Services. Moderne Punkt-Daten können einfach als geonames, openstreetmap etc. dump heruntergeladen werden bzw. über API Schnittstellen abgefragt werden. Für historische Daten existieren diese Services noch nicht vollumfänglich. Mit Pelagios gibt es ein Ökosystem an Services für Ortsdaten in der antiken Welt (http://commons.pelagios.org/), das im Zuge des “World-Historical Gazetteer” Projektes (http://whgazetteer.org/) auch in jüngere Zeiten ausgedehnt wird. Abgesehen von diesen notwendigen Initiativen und sehr wertvollen Datensätzen fehlen den Digital Humanities immer noch Polygondaten zu politischen Entitäten im Verlauf der Zeit.
         HistoGIS hat sich zum Ziel gesetzt genau diese Lücke zu füllen in dem:
         
            ein Repository historischer Polygondaten mit einheitlichen Metadaten aufgebaut und zum Download angeboten wird. Für dieses Repository werden zunächst schon existierende Polygondaten eingesammelt, aufbereitet und erst in einem zweiten Schritt für historisch wichtige Zeitspannen neue Polygone erstellt. Dabei setzt sich HistoGIS zunächst zum Ziel die Periode zwischen dem ausgehenden 18. Jhdt. und 1918 für das Gebiet der KuK Monarchie und des deutschen Bundes abzudecken.
            und RestAPI Services zur einfachen Anreicherung historischer Daten mit Hilfe des Repositories angeboten werden. Z.B.: Wo war Punkt X/Y zum Zeitpunkt 
                Z? Die API antwortet mit den Metadaten zu den einzelnen überlappenden Polygonen die für den Zeitraum (oder Zeitpunkt) Gültigkeit haben. Nehmen wir an: Ein Projekt hat Reiseberichte in seiner Datenbank. Eine Station war Bolzano 1910. Die Geokoordinaten (46.49067, 11.33982) wurden über Geonames gefunden. Schickt man diese mit dem Jahr an die API bekommt man die Metadaten für Bozen Stadt, An der Etsch, Tirol und Österreich-Ungarn 
                zurück.
                
         
         Eine Fokussierung auf die politischen Verwaltungseinheiten und ihre Grenzen erlaubt es in vielen Bereichen - zumindest für jüngere Zeiten - einen Mix aus modernen Daten/Methoden und historischen zu verwenden ohne historisch falsche Daten zu generieren. So können bei oben angeführten Beispiel Bozen die Google Maps API, Geonames oder Openstreetmap zur Geolokalisation verwendet werden (die Geokoordinaten von Bozen haben sich ja nicht geändert) und die HistoGIS API um die Eingliederung in die Verwaltungshierarchien zu verbessern (Bozen war 1910 Teil der KuK Monarchie). Dieses Vorgehen reduziert nicht nur den Aufwand für die Erstellung/Kuratierung der Daten drastisch, es ermöglicht es auch leichter Punkte denen schon Längen- und Breitengrade zugewiesen wurden politisch/historisch zu verorten.
         
            Datenmodell, Technische Grundlage und Workflow
            Die Modellierung historischer Verwaltungsräume hinsichtlich ihrer räumlichen und zeitlichen Ausdehnungen erfolgte bewusst in einer äußerst vereinfachten Art und Weise. Das mittels Python (bzw. GeoDjango) definierte und als Postgresql implementierte Datenmodell besteht in seinem Kern aus den drei Hauptklassen bzw. Tabellen “TempSpatial”, “Source” und “TempSpatialRel”
            Ein historischer Verwaltungsraum (Temporalized Spatial Entity oder eben “TempSpatial”) definiert sich über die im gesamten Datenset einzigartige Kombination der Eigenschaften zeitliche Ausdehnungen (“start_date” und “end_date”, Datumsfelder), räumliche Ausdehnung (“geom”; Multipolygon) sowie einem Feld “date_accuracy”, welches Auskunft über den Grad der Genauigkeit der angegebenen Datumswerte gibt. Ergänzt wird diese Klasse um die Eigenschaften “name” für, entsprechend den Projektkonventionen einen zeitgenössischen Namen der Entität, “alt_names” für alternative Namen sowie einem Feld “additional_data”, welches das Speichern arbiträrer weiterer Daten im JSON Format ermöglicht.
            Das Feld “administrativ_unit” zeigt auf eine Hilfsklasse (“SkosConcept”) für kontrolliertes Vokabular, welche in weiten Teilen das SKOS Datenmodell implementiert. 
            Die Quelle jeder Instanz der Klasse TempSpatial bzw. jeder im Datenset erfassten historischen Verwaltungseinheit wird mit Hilfe der Klasse “Source” beschrieben. Darin werden URLs zu verwendeten Daten anderer Projekte gespeichert wie auch eine Beschreibung der (weiteren) Datenerhebung und Kuration im Rahmen des Projektes sowie ein Zitationsvorschlag. Jedes Source Objekt ist außerdem auch mit einem ESRI-Shapefile verbunden welches Projektintern als primäres Datenformat dient. Mehr dazu im Abschnitt Workflow. 
            Die Modellierung beliebiger Relationen zwischen beliebigen historischen Verwaltungsräumen ist im Datenmodell durch die Klasse TempSpatialRelation grundgelegt. Hier können jeweils zwei TempSpatial Objekte (“instance_a” und “instance_b”) für eine Zeitspanne (“start_date” und “end_date”) in eine typisierte (Verweis auf die bereits erwähnte Hilfsklasse “SkosConcept”) Relation gebracht werden. Hierbei ist jedoch weniger an die in Gazetteern üblichen “part of” Beziehungen gedacht, sondern an Relationen wie beispielsweise “ist Vorgänger von” oder “wurde zusammengelegt mit”. Allerdings muss darauf hingewiesen werden, dass im derzeitigen Status des Projektes noch keine derartigen Beziehungen erfasst werden.
            Die Art und Weise wie eben erwähnte “part of” Beziehungen erfasst werden sollen, wurde im Projekt ausgiebig diskutierte, wobei hier neben formal- konzeptionellen Argumenten vor allem auch die konkreten Arbeitsvoraussetzungen und -bedingungen im Projekt berücksichtigt werden mussten. 
            Schlussendlich wurde eine explizite Modellierung hierarchischer Strukturen der erfassten Verwaltungseinheiten verzichtet, sprich im Datenmodell wird nicht ausdrücklich festgehalten, dass z.B. die Verwaltungseinheit A für einen gegebenen Zeitraum, Teil der Verwaltungseinheit B und diese Teil der Verwaltungseinheit D war. Dies erscheint deshalb als zulässig, weil im Projekt von der Prämisse ausgegangen wird, dass, zumindest für den für das Projekt primär interessante (Zeit)Raum, die Fläche der übergeordnete Einheit stets die Summe aller ihr untergeordneten Einheiten bildet. Dies ermöglicht es, dass part-of Beziehungen zwischen TempSpatial Objekten ‘on the fly’ mit Hilfe von spatial queries und unter Berücksichtigung der jeweiligen Start- und Enddatumswerten berechnet werden können. Und dies wiederum erleichtert einerseits die Arbeit der DatenkuratorInnen im Projekt, da diese keine expliziten Verbindungen pflegen müssen, andererseits ermöglicht dieses Flexibilität die Integration anderer Datensatz mit relativ geringem Aufwand.
            An dieser Stelle muss jedoch betont werden, dass die bis dato kuratierte Menge an Daten noch nicht ausreicht um konkrete Aussagen hinsichtlich der Belastbarkeit des hier vorgestellten Datenmodells treffen zu können. Erste Tests diesbezüglich fielen jedoch durchwegs positiv aus, sowohl was die Genauigkeit der spatial queries, vor allem aber auch was deren Performanz betrifft.
            Die technischen Komponenten des Projektes sind überschaubar. Als Storage Layer fungiert eine Postgresql Datenbank mit PostGIS erweiterung. Die Interaktion damit erfolgt über einen mittels dem Python basierten Webframework (Geo)Django implementierten Applikation Layer, wobei mit (Geo)Django, respektive django-rest-framework sowohl die HistoGIS-Webapplikation als auch ein entsprechender REST Webservice implementiert wurde bzw. wird.
            Die eigentlich Datenkuration erfolgt davon völlig unabhängig und unter Verwendung der open source Software Qgis. Bis dato wurden damit vorwiegend bereits existierende Daten harmonisiert und als ESRI shapefiles gespeichert. Das Schema dieser Dateien entspricht dabei weitgehend dem oben skizzierten Datenmodell. Als ‘fertig’ erachtete Datensätze (gezippte Shapefiles) werden dann über ein Webformular in HistoGIS als sogenannte “Source” objekte hochgeladen, entpackt, die Features der Shapfiles als TempSpatial Objekte gespeichert und mit dem Source Objekt verknüpft. 
            Neben der Kuration und Harmonisierung bereits bestehender “Vektordaten” werden im Projekt aber auch selbst Daten erzeugt. Dazu wählt ein Historiker im Team verwertbare (historische) Karten aus, welche idealerweise bereits digitalisiert (gescannt) sind. Die Datakuratorinnen georeferenziert diese Scans (geotiffs) und extrahieren die darin auffindbaren Informationen zu historischen Verwaltungsgrenzen als Vektordaten. 
         
         
            Kartenmaterial bis dato und Ausblick
            Bis dato befinden sich knapp 4000 Polygone in der Production Instanz des Systems. Das Anpassen und Einspielen schon vorhandener Polygone wurde mit Daten aus dem Census mosaic Projekt (https://censusmosaic.demog.berkeley.edu/) und dem HGis Archiv (http://www.hgis-germany.de/) begonnen. Damit können große Teile des 19. Jhdts für die Gebiete Österreich-Ungarns und des Deutschen Bundes (inkl. der jeweiligen Nachfolgeentitäten) bereits abgedeckt werden. Die Daten werden nicht nur technisch aufbereitet, sondern auch inhaltlich von einem Verwaltungshistoriker überprüft. HistoGIS implementiert dafür ein Ampelsystem. Vom HistoGIS-Team technisch wie inhaltlich überprüfte Daten werden mit Grün markiert, vom HistoGIS-Team ausgewählte Daten die noch nicht überprüft wurden mit Gelb und von Usern zur Verfügung gestellte, nicht überprüfte Daten mit Rot (momentan befinden sich lediglich gelbe und grüne Daten im System).
            Die Aufbereitung der Daten, wie auch die Entwicklung des technischen Systems schreitet erfreulicher Weise schneller voran als geplant. Es wurde deshalb erst kürzlich beschlossen in HistoGIS schon während der Projektlaufzeit auch Daten außerhalb der geplanten räumlich-zeitlichen Grenzen aufzunehmen.
            In unserer Präsentation werden wir vor allem das Datenmodell und die RestAPI Schnittstellen des Systems diskutieren und vorstellen.
         
      
      
         
            
                    Eine auf diese API aufbauende Abfragemaske findet sich hier: https://histogis.acdh.oeaw.ac.at/shapes/where-was/
                
            
                    Beispielhaft das Polygon für Tirol: https://histogis.acdh.oeaw.ac.at/shapes/shape/detail/3352
                
         
         
            
               Bibliographie
               
                  Kamzelak, Roland S. (2018): “Von der Raupe zum Schmetterling oder Wie fliegen lernen – Editionsphilologie zwischen Infrastruktur und Semantic Web.” In: Kamzelak, Roland S / Steyer, Timo (eds.): Digitale Metamorphose: Digital Humanities und Editionswissenschaft. (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 2). text/html Format. DOI: 10.17175/sb002_004 [letzter Zugriff 28. September 2018]
					
               
                  Nüssli, Marc-Antoine / Nüssli, Christos (2017): “A formal model for historical atlases and historical knowledge”, http://www.academia.edu/35853762 [letzter Zugriff 28. September 2018]
					
            
         
      
   


10966	2020	
      
         
            Einleitung
            Unter dem Begriff des 
                     Semantic Web (Berners-Lee, Hendler, Lassila 2001) werden Techniken, Standards und Methoden zusammengefasst, mit deren Hilfe im Internet verfügbare Daten der semantischen Verarbeitung durch Maschinen zugänglich gemacht werden können. Durch die Einführung und Nutzung von offenen Standards wie z. B. RDF (Schreiber & Raimond 2014) soll hierbei die Interoperabilität unterschiedlicher Datenquellen sichergestellt werden. Diese Standards beziehen sich auf die Art, wie Informationen repräsentiert werden und wie Verknüpfungen mit anderen Informationen hergestellt werden können. Daher wird oftmals auch der Begriff der 
                     Linked Data verwendet (Bizer, Heath, Berners-Lee 2009). In einer Visualisierung der Linked-Data-Cloud von 2017 (Freyberg 2017: 29) sind die Geisteswissenschaften als eigener Bereich nicht explizit aufgeführt, was die geringe Veröffentlichung geisteswissenschaftlicher semantischer Daten widerspiegelt bzw. vermuten lässt, wenngleich z. B. im Bereich der Graphentechnologien durchaus einige Projekte existieren (Kuczera 2017). 
                  
                   
            
               Metadaten als Basis literaturwissenschaftlicher Forschung
               Dabei sind solche Daten Basis vieler (literatur-)wissenschaftlicher Fragestellungen: Soll bspw. eine quantitative Textanalyse einer großen Anzahl von Romanen durchgeführt werden, müssen zunächst einmal die in Frage kommenden Werke ermittelt und ausgewählt werden. Die Erstellung solcher möglichst repräsentativen Samples ist allerdings ohne eine Kenntnis der gesamten Romanproduktion einer Epoche, der dort behandelten Themen und Motive und weiterer Angaben über die inhaltliche Ausgestaltung der zu betrachtenden Textproduktion nicht ohne Weiteres möglich.
               Hierbei helfen können Nachschlagewerke wie z. B. Fachbibliographien, in denen bibliographische Metadaten verzeichnet sind. Teilweise liegen solche Metadaten bereits als Linked Data vor, da Bibliothekskataloge (retro-)digitalisiert wurden. Diese Metadaten sind als Basis literaturhistorischer Arbeit jedoch häufig nicht ausreichend, da für eine zielgerichtete Auswahl relevanter Literatur oftmals mehr als die üblicherweise erschlossenen bibliographischen Angaben notwendig sind.
               Einen weiteren, großen Anteil an der prinzipiell verfügbaren Literatur haben jedoch auch Werke, die nicht digitalisiert, sondern nur in gedruckter Form vorliegen. Die 
                        Bibliographie du genre romanesque français 1751-1800 (Martin, Mylne, Frautschi 1977) fasst alle von den Autoren auffindbaren französischsprachigen Romane aus der zweiten Hälfte des 18. Jahrhunderts zusammen. Neben bibliographischen Daten zu Autoren, Werktiteln, Verlegern u. a. sind, soweit möglich, auch Angaben zu weiteren Auflagen (Reeditionen) und zum Inhalt der Werke zusammengetragen worden. Die Bibliographie enthält somit inhaltliche Informationen zu den einzelnen Romanen, die weit über eine Auflistung bibliographischer Metadaten hinausgehen. Solche Informationen sind wie o. g. notwendige Voraussetzung für die Erstellung repräsentativer Samples, u. a. zur weiteren literaturhistorischen Untersuchung der Textproduktion einer Sprache bzw. Epoche.
                     
            
            
               Zielsetzung
               Im Rahmen des hier präsentierten Vorhabens – einer Masterarbeit im Studiengang Digital Humanities an der Universität Trier – wurde die o. g. Bibliographie eingescannt und mittels 
                        Optical Character Recognition (OCR) in maschinenlesbaren Text umgewandelt. Auf dieser Grundlage wurden mithilfe eines Verfahrens des überwachten maschinellen Lernens die einzelnen Einträge extrahiert, in ein selbst entwickeltes semantisches Modell überführt und mit externen Daten verknüpft, sodass die Bibliographie nunmehr als RDF-Datensatz vorliegt und weiterverwendet werden kann. Zielsetzung der Arbeit war es, die in der Bibliographie enthaltenen Informationen unter Nutzung bibliographischer Standards und aktueller, verbreiteter Datenmodelle auf eine Art und Weise zu repräsentieren, die zukünftig weitere Verarbeitungen und Anreicherungen ermöglicht. Die so entstandene digitale Bibliographie kann darüber hinaus als Basis für buchwissenschaftliche, literaturhistorische und verwandte Forschungen dienen, da in ihr sowohl formale als auch inhaltliche Metadaten zur französischsprachigen Romanproduktion eines definierten Zeitraums enthalten sind.
                     
            
         
         
            Metadatenextraktion
            
               Ablauf
               Der Ablauf der Metadatenextraktion ist in Abbildung 1 dargestellt.
               
                  
                     
                     Abbildung 1: Ablauf der Metadatenextraktion
                  Nach dem Einscannen der gedruckten Vorlage, der OCR, der Vorverarbeitung (Korrektur von Fehlern, Entfernen von Vorwort und Abbildungen, einheitliche Zeichenkodierung etc.) wurden die einzelnen Jahreslisten der Bibliographie und innerhalb dieser die einzelnen Einträge/Romane durch XML-Markup voneinander getrennt (Segmentierung).
                     
               Anschließend wurde ein Trainingsset erstellt, mit welchem der verwendete Algorithmus trainiert werden konnte. Für die Trainingsdaten wurde aus jedem Jahrzehnt ein Jahr ausgewählt und die Metadaten der in diesem Jahr erschienenen Romane wurden manuell mit XML-Markup ausgezeichnet. Zur Evaluation der Modelle wurde ein Teil der Daten als Testset zurückgehalten.
               Das maschinelle Lernen verlief iterativ, sodass jeweils Modelle für unterschiedlich „tiefe“ Metadatenebenen gelernt wurden, da eine mehrstufige Anwendung mehrerer Modelle oftmals bessere Ergebnisse als die Verwendung eines einzigen Modells für die gesamten Daten erzielt (Kovacevic et al. 2011: 388) und simpler strukturierte Modelle weniger Trainingsdaten benötigen (Candeias 2011: 28). Ein erstes Modell wurde bspw. zur Bestimmung der Makrostruktur der Metadaten verwendet (Titel, Autor, Publikationsdetails etc.), weitere Modelle verfeinerten jeweils die Auszeichnung innerhalb einer dieser Gruppen (z. B. Differenzierung der Publikationsdetails: Ort, Verleger, Jahr, Format, Seitenangabe). Insgesamt wurden sechs Modelle trainiert, die durch stichprobenartige Analyse der erzeugten Daten sukzessive angepasst wurden, bis keine Verbesserungen mehr möglich waren. Das jeweils beste Modell einer Iteration wurde dann auf die restlichen, noch nicht im Trainings- bzw. Testset enthaltenen Jahreslisten angewendet.
            
            
               Algorithmus und Features
               Zur Modellbildung wurden 
                        Conditional Random Fields (CRF), ein Verfahren des überwachten maschinellen Lernens, verwendet (Lafferty, McCallum, Pereira 2001), das sich in den letzten Jahren zu einem wesentlichen Verfahren im Rahmen der Informationsextraktion entwickelt hat (vgl. z. B. Groza, Grimnes, Handschuh 2012). CRF kombinieren die Vorteile von 
                        Hidden-Markov-Modellen (HMM) und 
                        Support Vector Machines (SVM), zwei weiteren gut untersuchten Verfahren (Peng, McCallum 2004: 329).
                     
               Die in den Algorithmus eingespeisten Daten (hier: Wörter bzw. Token) werden als Sequenzen von Zuständen modelliert und auf Grundlage dieser beobachteten Zustände werden Label für die einzelnen Elemente vergeben. Im Gegensatz zu HMM berücksichtigen CRF jedoch mögliche Beziehungen der Elemente untereinander – im vorliegenden Fall also der Metadatenfelder bzw. der berücksichtigten Features. Da die Einträge der Bibliographie einem definierten Schema folgen (z. B. steht immer zuerst die Autorenangabe, dann folgt der Titel), ist dieser Algorithmus zur Modellierung der vorliegenden Daten besonders geeignet.
               
                  
                     
                  Damit ein CRF-Modell trainiert werden kann, müssen Features erhoben werden, die den Inhalt der einzelnen Metadatenfelder repräsentieren. Tabelle 1 gibt die genutzten Features wieder. Diese Features wurden nicht nur für das jeweilige Wort, sondern auch für das vorherige und das nachfolgende Wort erhoben. So kann im Modell bspw. gelernt werden, dass auf ein bestimmtes Wort stets eine Zahl folgt. 
                     
               Die genutzten Features wurden ausgehend von einer manuellen Analyse der Einträge in der Bibliographie und basierend auf den ausführlichen Erläuterungen der Autoren zur Sammlung und Strukturierung der Daten im Vorwort der Bibliographie ausgewählt. In der gedruckten Vorlage wurde Großschreibung bspw. zur Hervorhebung von Familiennamen verwendet und Angaben zum Inhalt eines Romans folgten fest definierten einleitenden Begriffen. 
               Eine ausführliche Evaluation unterschiedlicher Feature-Kombinationen fand im Rahmen der Arbeit nicht statt, da bereits die o. g. simplen Features zu ausreichend hoher Genauigkeit der Metadatenextraktion führten. Weitere Optimierungen hätten überdies vom eigentlichen Ziel der Arbeit weggeführt. Die zur Unterscheidung der einzelnen Metadatenfelder günstigsten Features wurden jedoch erhoben, um die Wirksamkeit und innere Struktur der gelernten Modelle zu überprüfen. Hierbei zeigte sich z. B., dass die einleitenden Wendungen zur inhaltlichen Beschreibung der Romane auch vom Algorithmus als solche gelernt und zur Auszeichung neuer Daten verwendet wurden. 
               Um auch weniger strukturierte Datengrundlagen als Bibliographien mit dem entwickelten Workflow verarbeiten zu können, bestünde hier ein möglicher, näher zu untersuchender Ansatzpunkt für eine genauere Analyse hilfreicher Features und die eventuelle Einführung weiterer Features.
                  
            
            
               Evaluation
               Das maschinelle Lernen wurde mithilfe der Programmiersprache 
                        Python und der dort verfügbaren Bibliothek 
                        sklearn-crfsuite
                   implementiert. Die Evaluation der Modelle geschah mit der zu 
                        sklearn-crfsuite kompatiblen Bibliothek für wissenschaftliche Programmierung
                         scikit-learn
                  .  In der folgenden Tabelle sind die gängigen Maße Precision, Recall und der F1-Score für die sechs gelernten Modelle angegeben.
                     
               
                  
                     
                  
               
               Für alle Metadatenfelder konnte eine sehr hohe Genauigkeit erreicht werden. Der so erzeugte Datensatz mit allen Einträgen aus der Bibliographie ist somit nahezu vollständig korrekt ausgezeichnet.
            
         
         
            Semantische Modellierung
            Zurzeit existiert kein einheitlicher, akzeptierter Standard, der in der Bibliothekswelt für die semantische Repräsentation bibliographischer Daten verwendet wird. Stattdessen orientieren sich diejenigen Bibliotheken, die bereits Linked Data zur Verfügung stellen, an unterschiedlichen Datenmodellen, Schemas und Ontologien. Es existieren jedoch Versuche, die bereits entwickelten Modelle in ein möglichst generisches und von vielen Bibliotheken nachnutzbares Modell zu integrieren (Suominen, Hyvönen 2017).
            
               Vorhandene Ontologien
               Vor allem die folgenden Datenmodelle sind für die semantische Modellierung der Metadaten aus der Bibliographie relevant, da sie entweder bereits weit verbreitet sind oder spezifische Elemente enthalten, die nachgenutzt werden können.
               
                  
                     FRBR: Functional Requirements for Bibliographic Records  und
                           RDA: Resource Description and Access (IFLA 2009)
                        
                  
                     DCTerms: 
                     Dublin Core Metadata Terms (Dublin Core Metadata Initiative 2012)
                        
                  
                     PRISM: Publishing Requirements for Industry Standard Metadata (IDEAlliance 2008)
                        
                  
                     SPAR
                     Ontologies (Peroni, Shotton 2018)
                        
               
               Die Entwicklung der SPAR-Ontologien wird von den Autoren u. a. damit begründet, dass bisherige Systeme uneinheitlich seien und deutliche Schwächen aufwiesen. PRISM und FRBR seien bspw. „top-level vocabularies rather than something specifically developed to characterise specific aspects of scholarly publishing“ (Peroni, Shotton 2018). Gleichzeitig benutzen die SPAR-Ontologien jedoch Elemente aus den anderen o. g. Vokabularen, um Redundanzen und doppelte Element-Definitionen zu vermeiden. In der hier beschriebenen Arbeit wurde daher ebenfalls versucht, aus den o. g. Datenmodellen vorrangig diejenigen Elemente zu verwenden, die bereits im Bibliothekswesen etabliert und nicht zu spezifisch, gleichzeitig aber ausreichend detailliert sind.
            
            
               Modellentwicklung
               Nach einer eingehenden Analyse der in der Bibliographie vorhandenen Metadaten wurden aus den o. g. Ontologien diejenigen Elemente zur weiteren Berücksichtigung ausgewählt, die zur möglichst genauen und eindeutigen Modellierung der einzelnen Einträge der Bibliographie (siehe Abbildung 2) benötigt werden. Hierbei wurde darauf geachtet, nicht bloß die einzelnen Romane mit ihren Metadaten abzubilden, sondern auch den Aufbau und die Struktur der Bibliographie an sich. Dadurch konnte das gesamte zu erzeugende Modell an den bereits im Linked-Data-Service der 
                        Bibliothèque nationale de France (BnF) vorhandenen Eintrag für die 
                        Bibliographie du genre romanesque français angebunden werden.
               
               
                  
                     
                     Abbildung 2: Beispieleinträge in der gedruckten Bibliographie
                  
               
               Durch die im Vorfeld bereits erfolgte Extraktion der einzelnen Metadatenfelder aus den OCR-Daten konnten diese schließlich direkt auf die entsprechenden Elemente in dem erstellten RDF-Modell abgebildet werden. Dies geschah überwiegend mithilfe der Programmiersprache 
                        Java und der dort verfügbaren Bibliothek 
                        Apache Jena
                  .
                     
            
            
               Verknüpfung mit anderen Ressourcen
               Um die Möglichkeit der Anreicherung der Daten mit Informationen aus externen Ressourcen beispielhaft darzustellen, wurden die Namen der Autoren der einzelnen Romane aus dem RDF-Modell extrahiert und mithilfe von Apache Jena an die API der 
                        Virtual 
                  International Authority File (VIAF) gesendet. Von dort wurden – sofern vorhanden – die VIAF-IDs extrahiert und dem RDF-Modell hinzugefügt. Weitere externe Ressourcen könnten auf ähnliche Weise integriert werden. Voraussetzung für die erfolgreiche Nutzung der API ist, dass die Einträge im RDF-Modell keine Schreibfehler oder OCR-Fehler aufweisen. Dies kommt allerdings relativ häufig vor (Gründe sind u. a.: kleine Schrift in der Vorlage, viele Eigennamen, kurze Wörter mit wenig Kontext) und ist eines der wesentliche Probleme des Datensatzes.
                     
            
         
         
            Fazit
            Sowohl die Extraktion der einzelnen Metadaten aus den OCR-Texten als auch die Erstellung und anschließende Überführung in ein RDF-Modell ließen sich mit gutem Erfolg umsetzen. Die Erkennungsgenauigkeit des CRF-Algorithmus war mit einem F1-Score von durchschnittlich 0,964 (0,908–0,997) außerordentlich hoch. Grund hierfür war sicherlich vor allem die bereits stark strukturierte Datengrundlage. Fehlende einheitliche Standards zur Repräsentation bibliographischer Metadaten und Fehler in den Textdaten sind jedoch Schwachstellen, die eine genauere Analyse und evtl. umfangreiche Bereinigung/Korrektur der zu repräsentierenden Daten nötig machen.
            Das vorgestellte Projekt hat durch die Kombination von modernen Verfahren zur Informationsextraktion und die Zusammenstellung von aktuellen Ontologien zur Repräsentation bibliographischer Metadaten einen für die Datengrundlage passenden Ansatz entwickelt, der als Standard-Workflow für ähnliche Projekte verwendet werden könnte und in solchen überprüft und verfeinert werden sollte. Denkbar wären z. B. die Digitalisierung und Metadatenextraktion weiterer Bibliographien, um den erzeugten Datenbestand zu ergänzen, zu erweitern oder anzureichern. Auch die Überprüfung des hier beschriebenen Vorgehens in verwandten Kontexten (andere Nachschlagewerke, andere Sprachen, andere Epochen) unter Nutzung weiterer oder anderer Features wäre sinnvoll. 
            Der Workflow und die Daten werden daher am 
                     Trier Center for Digital Humanities im Rahmen des von der Forschungsinitiative Rheinland-Pfalz geförderten Projektes „MiMoText – Mining and Modeling Text“ weiterverwendet und erweitert. Ziel ist hier der Aufbau eines „aus unterschiedlichen Quellen gespeisten Informationsnetzwerks für die Geisteswissenschaften, das durch die Bereitstellung als Linked Open Data nicht nur frei verfügbar und mit anderen Wissensressourcen des Semantic Web verknüpfbar ist, sondern auch neuartige und effiziente Zugriffsmöglichkeiten auf fachwissenschaftliche Informationen bietet“. Die beschriebene Arbeit liefert hierfür eine geeignete Grundlage.
                  
         
      
      
         
             Der Datensatz ist verfügbar unter 
                      (Lizenz: CC-BY).
                  
            
               
            
            
               
            
            
               
            
            
               
            
            
               
            
            
               https://kompetenzzentrum.uni-trier.de/de/projekte/projekte/m/
            
         
         
            
               Bibliographie
               
                  Berners-Lee, Tim / 
                  Hendler, 
                  James / 
                  Lassila, 
                  Ora (2001): 
                        "The Semantic Web", in: 
                        Scientific American 284.5: 29–37.
                     
               
                  Bizer, Christian / 
                  Heath,
                   Tom / 
                  Berners-Lee, 
                  Tim (2009): 
                        "Linked Data – The Story So Far", in: 
                        International Journal on Semantic Web and Information Systems 5.3: 1–22 
                        http://tomheath.com/papers/bizer-heath-berners-lee-ijswis-linked-data.pdf
                        [letzter Zugriff 03. Januar 2020]. 
                     
               
                  Candeias, Ricardo Pereira (2011): 
                        Metadata Extraction from Scholarly Articles. Master Thesis, Universidade Técnica de Lisboa. 
                         [letzter Zugriff 03. Januar 2020].
                     
               
                  Dublin Core Metadata Initiative (2012): 
                        DCMI Metadata Terms. DCMI Recommendation. 
                         [letzter Zugriff 03. Januar 2020],
                     
               
                  Freyberg, Linda (2017): 
                        "Density of Knowledge Organization Systems", in: 
                        Knowledge Organization for Digital Humanities. Proceedings of the 15th Conference on Knowledge Organization WissOrg '17 of the German Chapter of the International Society for Knowledge Organization (ISKO) 25–30.
                     
               
                  Groza, T. / Grimnes, A. / Handschuh, S. (2012): 
                        "Reference Information Extraction and Processing Using Conditional Random Fields", in: 
                        Information Technology and Libraries 31.2: 6–20.
                     
               
                  IDEAlliance – International Digital Enterprise Alliance (2008): 
                        The PRISM Namespace – Final 
                  http://www.prismstandard.org/specifications/2.0/PRISM_prism_namespace_2.0.pdf
                        [letzter Zugriff 03. Januar 2020].
                     
               
                  IFLA Study Group on the Functional Requirements for Bibliographic Records (2009): 
                        Functional Requirements for Bibliographic Records – Final Report. (IFLA Series on Bibliographic Control, Vol. 19) 
                         [letzter Zugriff 03. Januar 2020].
                     
               
                  Kovacevic, Aleksandar / Ivanovic, Dragan / Milosavljevic, Branko / Konjovic, Zora / Surla, Dusan (2011): 
                        "Automatic extraction of metadata from scientific publications for CRIS systems“, in: 
                        Program 45.4: 376–396.
                     
               
                  Kuczera, A. (2017): 
                        "Graphentechnologien in den Digitalen Geisteswissenschaften", in: 
                        ABI Technik, 37.3: 179–196.
                     
               
                  Lafferty, John D. / 
                  McCallum, 
                  Andrew / 
                  Pereira, 
                  Fernando C. N (2001): 
                        "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", in: 
                        Proceedings of the Eighteenth International Conference on Machine Learning (ICML '01) 282–289.
                     
               
                  Martin, Angus / 
                  Mylne,
                   Vivienne / 
                  Frautschi, 
                  Richard (1977): 
                        Bibliographie du genre romanesque français 1751-1800. London, Paris: Mansell, France expansion.
                     
               
                  Peng, Fuchun / McCallum, Andrew (2004): 
                        "Accurate Information Extraction from Research Papers using Conditional Random Fields", in: 
                        Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLTNAACL) 329–336 
                        [letzter Zugriff 03. Januar 2020].
                     
               
                  Peroni, Silvio / 
                  Shotton, 
                  David (2018): 
                        "The SPAR Ontologies", in: Vrandečić D. et al. (eds.): 
                        The Semantic Web – ISWC 2018. Lecture Notes in Computer Science. Cham: Springer 119–136.
                     
               
                  Schreiber, Guus / 
                  Raimond, 
                  Yves (2014): 
                        RDF 1.1 Primer. W3C Note. 
                         [letzter Zugriff 03. Januar 2020].
                     
               
                  Suominen, Osma / Hyvönen, Nina (2017): 
                        "From MARC silos to Linked Data silos?", in: 
                        o-bib. Das offene Bibliotheksjournal 4.2: 1–13.
                     
            
         
      
   


10968	2020	
      
         
            Die Infrastuktur und das Projekt
            Seit 2010 kooperieren das Wittgenstein Archiv der Universität Bergen und das Centrum für Informations- und Sprachverarbeitung der Ludwig-Maximilians Universität München in der Forschungsgruppe „Wittgenstein Advanced Search Group“ (WAST). Die Forschungsgruppe entwickelt Web-Frontends (FinderApps) und spezielle Suchwerkzeuge, die sich gut für die Forschung und Lehre im Bereich der Digital Humanities eignen. Ihre erste Suchmaschine, die FinderApp WiTTFind (wittfind.cis.lmu.de, siehe Abb. 1), die den von der UNESCO zum Weltkulturerbe (im Jahr 2017) erhobenen (Schmidt 2018) Nachlass von Ludwig Wittgenstein durchsucht, gewann im Jahre 2014 der EU-Open-Humanity Award. Der Preis zeichnet Gruppen aus, die herausragende Technologie im Bereich der Humanities entwickelt haben. Die in der Forschergruppe programmierte FinderApp WiTTFind erlaubt es, mit hochqualifizierten, computerlinguistisch orientierten Suchwerkzeugen Nachlasstrans-kriptionen zu durchsuchen. Die Transkriptionen entstammen der 
                    Bergen Normalized Edition, die die Grundlage der Wittgenstein Edition bildet. Neben den gefundenen Treffern der Suchmaschine, werden in den Suchergebnissen von WiTTFind die Faksimile-Extrakte aus den Originaldokumenten angezeigt. So kann der Nutzer die „Aura“ der gefundenen Textstelle im Original studieren und nicht nur den transkribierten Text sehen.
                
            
               
                  
                   Abbildung 1: WiTTFind (http://wittfind.cis.lmu.de)
               
            
            Damit derNutzer auch den seitenweisen Kontext des Suchtreffers im Original studieren kann, wurde am CIS eine weitere WEB-Applikation entwickelt, der doppelseitige Reader. Dieser Reader ermöglicht es, vom Suchtreffer direkt an die entsprechende Stelle im entsprechenden Dokument des Originals zu springen. Im doppelseitigen Lesemodus kann der Nutzer in den Faksimile des originalen Dokuments blättern. Eine symmetrische Autovervollständigung gibt während der Suchanfrage einen statistischen und lexikalischen Zugang zu den Wörtern, die in der Edition vorkommen. Im Zentrum der Suche steht die selbstprogrammierte C++ Suchmaschine wf, die mit Hilfe von Vollformlexika (WiTTlex), verbessertem POS-Tagging und weiteren Metainformationen regelbasiertes Suchen erlaubt. Zum Aufspüren semantisch ähnlicher Textpassagen in der Edition gibt es das NLP-Tool WiTTSim.
            Die thematisch getrennten Aufgaben innerhalb der Infrastruktur der WAST-Tools (siehe Abb. 2) werden über REST-API’s von einzelnen Microservices realisiert, deren zentrale Datenhaltung über eine mongo Datenbank realisiert wird. Die Oberflächen der FinderApps werden mit HTML5, Javascript und Bootstraptechniken für WEB-Browser programmiert und möglichst browserunabhängig gehalten. 
            
               
                  
                   Abbildung 2: Infrastruktur der WAST-Tools (http://gitlab.cis.lmu.de)
               
            
            Alle Programme, Schnittstellen und Entwicklungen werden dokumentiert (siehe Abb. 3) und Tutorials für Anschlussprojekte entwickelt. So ist gewährleistet, dass die Tools und Suchmaschinen nachhaltig verwendet und auch für die Forschung und Lehre eingesetzt werden können. Als Versionskontrollsystem wird git verwendet.
            
               
                  
                   Abbildung 3: Dokumentation der WAST-Tools: http://wittfind.cis.uni-muenchen.de/wast/infrastruktur/index.html
                  
               
            
            Bei der Entwicklung der Infrastruktur der WAST-Tools wurden die strengen Vorgaben des EU-Open-Humanity Awards eingehalten: Forderungen nach Open-Source, interdisziplinäre Öffnung und Nachhaltigkeit. Diese Offenheit ermöglichte es weitere FinderApps für andere Wissenschaftsbereiche zu implementieren: GoetheFind (Faust-I und Faust-II Edition, Deutsches Textarchiv Berlin (XML-TEIP5, DTA Basis Format)), HistoFind (Briefwechsel Erzherzog Leopold Wilhelms an Kaiser Ferdinand III. aus dem Reichsarchiv Stockholm; Kooperation mit Historikern) und den OdysseeReader (Schreibprozess der zur Logisch-Philosophischen-Abhandlung führte; Kooperation mit Philosophen).
            In diesem Workshop werden die verwendeten Softwaretechnologien und computerlinguistischen Methoden im konkreten Einsatz vorgestellt. Den Teilnehmer*innen wird ein Debian-10 Container mit allen notwendigen Programmen, Tools und Dokumentation der gesamten Softwareinfrastruktur zur Verfügung gestellt. Innerhalb dieses Containers können die Teilnehmer*innen die einzelnen Tools der WAST-Projektgruppe kennenlernen und bekommen von den Projektmitarbeiter*innen kleine Aufgaben gestellt, die sie dann mit ihnen bearbeiten. So können sie die Arbeitsweise der WAST Infrastruktur konkret kennenlernen.
         
         
            Im Workshop werden folgende Datenformate, Tools und Programmierkonzepte vorgestellt und geübt
            Gitlab Projektmanagement und Continuous Integration, XML TEI-P5 Edition CISWAB, Faksimilestrukturierung und Texterkennung, lexikalische Arbeit, WEB-Oberfläche der FinderApps und Einsatz mit Micorservices, doppelseitiger Faksimilereader mit MongoDB, NLP-Tools zur semantischen Ähnlichkeitssuche, Vorstellung und Programmierung einer regelbasierten Suchmaschine und die Erstellung eines Dokumentationssystems mit Sphinx.
            
               Voraussetzungen an die Kursteilnehmer*innen
               Programmierkenntnisse (Grundkenntnisse): LINUX (Arbeit mit der UNIX-Shell), Python, XML, HTML, git, javascript, POS-Tagging.
               Da beim Workshop einige Entwickler der WAST-Tools anwesend sein werden, gibt es die Möglichkeit auch vertieft in die jeweilige Thematik einzusteigen.
            
            
               Gitlab Projektmanagement und Continuous Integration (Hadersbeck, Still)
               Im gesamten Projekt wird als Versionierungssystem git verwendet. Die Projektrepositories werden auf zwei unterschiedlichen Rechnern ausgerollt: Dem preview-Server für Tests und einem Projektserver für die offizielle Onlineversion. Es wird das in der Praxis bewährte „git branching model“ kombiniert mit einer „continuous integration“ Technik eingesetzt. Mit einer Feedbackapp können Nutzer Fehler melden oder Implementierungswünsche äußern, die in Issues innerhalb der Projektrepositories bearbeitet werden.
            
            
               XML TEI-P5 Edition CISWAB (Hadersbeck)
               Als Datenbasis für das WiTTFind Projekt wird die „Bergen Nachlass Edition“ (BNE) verwendet, die sich an den Richtlinien der Text Encoding Initiative (TEI-P5) orientiert. Im Workshop werden die wichtigen TEI-XML-Elemente der BNE vorgestellt.
                  
            
            
               Faksimilestrukturierung und Erkennung (Eisterhues, Landes)
               Da in den FinderApps neben den gefunden Textstellen auch die zugehörigen Faksimileextrakte aus der Edition dargestellt werden, sind Kenntnisse der Bildkoordinaten der Textstellen nötig. Diese Koordinaten werden mit Hilfe einer Kette von Bildverarbeitungstools ermittelt. Da bei Manuskripten und bei manuellen Änderungen in Dokumenten die automatische Zeichenerkennung unbrauchbare Ergebnisse liefert, wurden eigene Strategien entwickelt, die die Informationen aus der BNE nutzen. Im Workshop werden die eingesetzten Tools und Optimierungsstrategien vorgestellt.
            
            
               Lexikalische Arbeit (Lokale Grammatiken, Semantik) (Röhrer)
               Zur lemmatisierten Suche, Partikelverberkennung und semantischen Wortfeldern wurden spezielle Projektlexika entwickelt (Röhrer 2017). Die Lexika enthalten alle Wörter der zu durchsuchenden Edition und sind mit grammatischen Angaben und zum Teil mit zusätzlichen semantischen Informationen versehen. Diese Lexika und ein nachgestelltes optimiertes Part-of-Speech Tagging ist die Grundlage für die computerlinguistischen Methoden, die bei der regelbasierten Suche im Nachlass von Ludwig Wittgenstein eingesetzt werden.
            
            
               Regelbasierte Suchmaschine (Babl)
               Im Zentrum der FinderApps steht die Suchmaschine wf, ein multithreaded C++ Programm, das viele Anfragemöglichkeiten zur Suche implementiert: Einwort und Mehrwortsuche (mit internem Rankingverfahren) und reguläre Ausdrücke kombiniert mit linguistischen Anfragen (Morphologische Eigenschaften, POS-Tags, semantische und syntaktische Tags). Für das Rankingverfahren wird für jeden Suchtreffer die Relevanz zur Suchanfrage berechnet. Die Qualität für jeden Suchtreffer, die Distanz zwischen den einzelnen Wörtern und unterschiedlichen Belohnungs- und Bestrafungsparametern, gehen in die Berechnung der Relevanz ein. Die Treffer werden dann nach dieser sortiert und auf der Website ausgegeben. Durch dieses neuartige Ranking kann nun auch nach verschiedenen Wörtern gesucht werden, die im Text nicht direkt hintereinander stehen müssen.
            
            
               NLP-Tool Semantische Ähnlichkeitssuche (Ullrich)
               Zur Extraktion von semantisch ähnlichen Bemerkungen wurde das Analysetool WiTTSim (Ullrich 2018) entwickelt, welches anhand von semantischen und syntaktischen Features ähnliche Texte identifiziert. Da die enorm hohe Anzahl von etwa 100.000 Features in Kombination mit den zu vergleichenden 54.000 Bemerkungen eine effiziente Suche unmöglich macht, wurde ein semantisches Clustering-Verfahren vorgeschaltet (Ullrich 2019), welches durch Dimensionsreduktion und Gruppierung der Texte die Rechenzeit der Ähnlichkeitssuche um den Faktor 100 beschleunigt.
            
            
               WEB-Oberfläche der FinderApps und Micorservices (Hadersbeck, Still)
               Zur Arbeit mit WiTTFind wird dem User eine WEB-basierte FinderApp zur Verfügung gestellt, die über REST-APIs und „internet microservices“ mit den WAST-Tools kommuniziert. HTML5, Javascript und Bootstrap-css erlauben den Aufbau der WEB-page, die nahezu browserunabhängig die Schnittstelle zum Anwender darstellt. 
            
            
               Doppelseitiger Faksimilereader und MongoDB (Lindinger)
               Der doppelseitiger Faksimilereader ist eine komplett eigenständige Anwendung mit Suchschlitz und Investigate Mode zur gleichzeitigen Betrachtung von Faksimile und Transkription. Außerdem gibt es zahlreiche weitere Features, die es den Nutzern sehr bequem erlauben, die gefunden Treffer der Suchmaschine im Kontext einer doppelseitigen Darstellung der Faksimile zu sehen und gleichzeitig durch die Dokumente der Forschungsdomäne zu blättern. Sämtliche Informationen bzgl. Edition und Faksimile sind in einer MongoDB gespeichert und werden über HTTP-Schnittstellen abgefragt.
            
            
               Dokumentationssystem Sphinx (Babl) (siehe Abb.2)
               Für jedes Teilprojekt der Wittgenstein Advanced Search Tools (WAST) wird im entsprechenden Gitlab Ordner eine README.md Datei erstellt, das in einer Dokumentation, die alle Projekte umspannt mithilfe der Software Sphinx zusammengefasst und online auf ansprechende Art und Weise darstellt. Die Dokumentation hilft, neuen Studierenden einen schnelleren Einstieg in das Projekt zu finden und ermöglicht es, das gesamte WAST-Projekt schnell nach bestimmten Fachbegriffen zu durchsuchen. 
            
         
         
            Programm des Workshops (ganztages Workshop)
            
               Überblick/Einführung/Vorstellungsrunde 
               Digitaler Zugang zum Nachlass von Ludwig Wittgenstein, das Projekt WAST (Dr. Max Hadersbeck)
               Fragen/ Diskussion/ gewünschte Schwerpunkte der Teilnehmer*innen des Workshops
            
            
               
                  WAST-Spezialthemen (jeweils ca. 15 Min. Theorie / 20 Min. Praxis)
               
                   Gitlab Projektmanagement und Continuous Integration mit git production / testing server (Hadersbeck, Still) 
      
                   XML TEI-P5 Edition CISWAB (Hadersbeck): Bergen Normalized Edition und xslt-Transformationen und Investigate-Mode von WiTTFind
                   Faksimilestrukturierung und OCR Erkennung (Eisterhues, Landes) 
                   Lexikalische Arbeit (Röhrer): Lemmatisierte Suche, Lexika, Lokale Grammatiken, Query Beispiele
                   WEB-Oberfläche der FinderApps und Microservices (Hadersbeck, Still): Flask server, Javascript
                   Doppelseitiger Faksimilereader und mongodb (Lindinger) 
                   NLP-Tool Semantische Ähnlichkeitssuche (Ullrich): NLP-Python Libraries, Funktionalitäten
                   Regelbasierte Suchmaschine (Babl): Programmierung C++, make/cmake, client-server Programmierung mit C++
                   Dokumentationssystem Sphinx (Babl): Markdown, Sphinx Installation, 2HTML, 2PDF
               
            
            
               Arbeitsgruppen: Diskussionen/Spezialfragen
               Je nach Interesse der Teilnehmer*innen unter der Leitung der einzelnen Dozent*innen.
            
         
         
            Kurzbiographie der Dozent*innen
            
               Florian Babl (CIS)
               Bachelorarbeit: Entwicklung eines Rankingverfahrens der Suchtreffer für die FinderApp WiTTfind im Nachlass Ludwig Wittgensteins 
               Forschungsschwerpunkte: verschiedene Rankingalgorithmen und ihre Funktionalität mit dem Ziel der Rankingverbesserung.
            
            
               Marcel Eisterhues (CIS)
               Forschungsschwerpunkte: Der momentane Forschungsschwerpunkt ist die automatische Seitensegmentierung von handgeschriebenen Texten.
            
            
               Max Hadersbeck (CIS)
               Projektleiter und Dozent am CIS
               Forschungsschwerpunkte: Digitaler Zugang zum Nachlass von Ludwig Wittgenstein, FinderApp WiTTFind, Wittgenstein Advanced Search Tools, Programmierung: C++, Python, XML
            
            
               Florian Landes (Kommission für bayerische Landesgeschichte bei der Bayerischen Akademie der Wissenschaften) 
               Bachelorarbeit: Optical Character Recognition (OCR) – Optische Zeichenerkennung (OZE) Ein Werkzeug zur Verknüpfung von digitaler Edition und Faksimile? Semiautomatische Ermittlung von Bildkoordinaten für WiTTFind
               Forschungsschwerpunkte: OCR, OZE, Bavarikonprojekt Ortsnamen des Regierungsbezirks Schwaben
                  
            
            
               Ines Röhrer (CIS)
               Masterarbeit: Lexikon, Syntax und Semantik - computerlinguistische Untersuchungen zum Nachlass Ludwig Wittgensteins
               Forschungsschwerpunkte: Digitales Speziallexikon WiTTLex für den Nachlass von Ludwig Wittgenstein
            
            
               Sebastian Still (CIS)
               Masterarbeit: Ludwig Wittgenstein: 100 Jahre Traktatus. Der Odyssee-Reader, ein web-basiertes Tool zur textgenetischen Suche im Traktatus
               Forschungsschwerpunkte: moderne Frontend Programmierung, NLP (Backend)
            
            
               Sabine Ullrich (CIS)
               Masterarbeit: Clustering zur Verbesserung der Performanz einer Ähnlichkeitssuche
               Forschungsschwerpunkte: Natural Language Processing, Data Mining, semantische Ähnlichkeitserkennung im Nachlass von Ludwig Wittgenstein
            
         
      
      
         
            
               Bibliographie
               
                  Babl, Florian (2019): 
                        Entwicklung eines Rankingverfahrens der Suchtreffer für die FinderApp WiTTFind im Nachlass Ludwig Wittgensteins. Bachelor‘s thesis. LMU.
                    
               
                  Landes, 
                  Florian (2019): 
                        Optical Character Recognition (OCR) – Optische Zeichenerkennung (OZE). Ein Werkzeug zur Verknüpfung von digitaler Edition und Faksimile? Semiautomatische Ermittlung von Bildkoordinaten für WiTTFind, Bachelorarbeit, LMU.
                    
               
                  Lindinger, Matthias (2013): 
                        Highlighting von Treffern des Suchmaschinentools 
                  WiTTFind im zugehörigen Faksimile. Bachelor‘s thesis, LMU.

               
                  Lindinger, 
                  Matthias (2015): 
                        Entwicklung eines WEB-basierten Faksimileviewers mit Highlighting von Suchmaschinen-Treffern und Anzeige der zugehörigen Texte in unterschiedlichen Editionsformaten. Master's thesis, LMU.
                    
               
                  Pichler, Alois (2017): 
                        Wittgenstein Archives at the University of Bergen (WAB): Open Access to Wittgenstein's Nachlass. XML based Interactive Dynamic Presentation (IDP) of WAB's Nachlass transcriptions. 16. Mai 2017. http://wab.uib.no/transform/wab.php?modus=opsjoner [letzter Zugriff 20.09.2019].
                    
               
                  Hadersbeck, 
                  Maximilian / 
                  Pichler, 
                  Alois / 
                  Fink, 
                  Florian /
                   Gjesdal, 
                  Øyvind L. (2014): „Wittgenstein's Nachlass: WiTTFind and Wittgenstein advanced search tools (WAST)“
                        , in: 
                        Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage, 91-96. ACM.
                    
               
                  Hadersbeck , 
                  Maximilian /
                   Pichler, 
                   Alois /
                   Bruder, Daniel / Schweter, Stefan (2016): 
                        New (re)search 
                  possibilities for Wittgenstein's Nachlass II: Advanced Search, Navigation and Feedback with the FinderApp WiTTFind. http://wab.uib.no/alois/Hadersbeck_Pichler%20Kirchberg2016.pdf [letzter Zugriff 20.09.2019].
                    
               
                  Röhrer, 
                  Ines / 
                  Ullrich, 
                  Sabine / 
                  Hadersbeck, 
                  Maximilian (2019): 
                        Weltkulturerbe international digital: Erweiterung der Wittgenstein Advanced Search Tools durch Semantisierung und neuronale maschinelle Übersetzung. multimedial multimodal. Abstracts zur Jahrestagung des Verbandes Digital Humanities im deutschsprachigen Raum, 25. - 29.03.2019 an den Universitäten zu Mainz und Frankfurt.
                    
               
                  Röhrer, 
                  Ines (2017): 
                        Musik und Ludwig Wittgenstein: Semantische Suche in seinem Nachlass. Bachelor‘s thesis, LMU.
                    
               
                  Schmidt, 
                  Alfred (2018): „Ludwig Wittgenstein’s Nachlass in the UNESCO Memory of the World register.“, in: 
                        Nordic Wittgenstein Review 7(2):209–213.
                    
               
                  Ullrich, 
                  Sabine /
                   Bruder, 
                  Daniel /
                   Hadersbeck, 
                  Maximilian (2018): Aufdecken von „versteckten" Einflüssen: Teil-Automatisierte Textgenetische Prozesse mit Methoden der Computerlinguistik und des Machine Learning. Kritik der digitalen Vernunft. Abstracts zur Jahrestagung des Verbandes Digital Humanities im deutschsprachigen Raum, 26.02.-02.03. 2018 an der Universi
                        tät zu Köln, veranstaltet vom Cologne Center for eHumanities (CceH).
                    
               
                  Ullrich, Sabine (2019):
                         Boosting Performance of a Similarity Detection System using State of the Art Clustering Algorithms. Master‘s thesis. LMU.
                    
            
         
      
   


10969	2020	
      
         
            Einleitung
            
  Seit 2010 kooperieren das Wittgenstein Archiv an der Universität Bergen (WAB, Alois Pichler) und das Centrum für Informations- und Sprachverarbeitung der Ludwig-Maximilians Universität München
  (CIS, Max Hadersbeck et. al.) in der Forschungsgruppe „Wittgenstein Advanced Search Tools” (WAST). Die WAST-Projektgruppe entwickelt die web-basierte 
  FinderApp WiTTFind (), die einen computerlinguistisch gestützten digitalen Zugang zu WABs Wittgenstein-Edition erlaubt. Nach einer kompletten Neuscannung des Nachlasses und intensiven Verhandlungen des WAB mit den Rechteinhabern, dürfen seit 2018 WABs Edition auf der WiTTFind-Webseite durchsucht und Faksimileextrakte dargestellt werden. Nun konnten wir uns einer zentralen Frage der Wittgensteinforscher widmen: Wo finden sich in seinem Nachlass semantisch ähnliche Bemerkungen und, retroperspektivisch betrachtet, wann fanden diese Änderungen statt? 

            Wir entwickelten das Analysetool WiTTSim (Ullrich, 2018), das semantisch ähnliche Bemerkungen in der Edition aufspürt, zusammen mit einem vorgeschalteten semantischem Clusterverfahren (Ullrich, 2019), welches die Rechenzeit der Ähnlichkeitssuche um den Faktor 100 verkürzte. Zur retroperspektivischen Analyse der Edition entwickelten wir ein zeitorientiertes, textgenetisches Datenmodell, das die Spielräume der Interpretation der bisher dokumentorientierten Edition auf zugelassene Lesarten reduziert.
            In unserem Vortrag stellen wir die Verfahren unserer Ähnlichkeitssuche mit vorgeschaltetem semantischen Clustering und ein neues mehr textgenetisch- als dokumentorientiertes Modell einer Edition vor, das im Web-Frontend des OdysseeReaders (www.odysseereader.wittfind.cis.lmu.de) implementiert ist und auch die Frage beantwortet: „Wann gibt es semantisch ähnliche Bemerkungen“.
         
         
            Die Datenbasis: Dokument- und Zeitorientierte Modelle
            Die bei uns verwendete Datenbasis BNE 2015- und IDP 2016-, die am Wittgensteinarchiv an der Universität Bergen (Pichler, WAB) erstellt werden, enthalten Faksimile und Transkriptionen (auf der Basis von XML-TEI-P5) des Nachlasses von Ludwig Wittgenstein. Dieser Nachlass umfasst ca. 20.000 Seiten, welche vom WAB in Dokumente und diese wiederum in logische Textabschnitte unterteilt sind. Jeder der 54.930 Textabschnitte – eine sogenannte Bemerkung – wird mit einer eindeutigen Bezeichnung, dem sogenannten Siglum, versehen und wird in unserer Ähnlichkeitssuche als einzelnes Textobjekt definiert und semantisch analysiert. 
            Betrachtet man die Annotationen der BNE unter dem Aspekt der Retroperspektive, taucht folgendes Problem auf: Die BNE liefert nur auf der Ebene der Bemerkungen Informationen über ihren Erstellungszeitpunkt bzw. -zeitrahmen. Die Änderungen auf Wort und Zeichenebene sind zwar akribisch annotiert, allerdings fehlt die zeitliche Information wann diese Änderungen vorgenommen wurden. Um textgenetische Metainformationen auf Wort- bzw. Zeichenebene in das “ordered hierarchy of content objects model data” (OHCO) einer XML-Edition, wie das der BNE zu integrieren, schlägt das TEI-P5 Konsortium Fragmentierungs-, Milestone oder Standoff-Markup Annotationen vor (Jörg Hornschemeyer, 2013), die am WAB bisher nicht durchgeführt wurden. Von Geisteswissenschaftlern, deren wissenschaftliches Kerngebiet im Allgemeinen weit entfernt von der XML-Programmierung liegt, würde großer programmtechnischer Editionsaufwand verlangt. Eine Folge ist, dass von „Nachverwertern“ der Edition zur Generierung der textlichen Varianten algorithmisches Ausmultiplizierten der annotierten Varianten implementiert wird, was z.B. in der Wittgenstein-Edition bei einzelnen Bemerkungen eine vierstellige Anzahl von Lesarten generiert. Betrachtet man die so automatisch generierten Lesarten, sind die meisten syntaktisch und semantisch falsch, was fatale Auswirkungen auf semantische Analysen der Textobjekte hat. Ohne zusätzliche, fein granulierte Metainformation in den annotierten Varianten sind die Spielräume der automatisierten Lesartengenerierung jedoch nicht einzugrenzen.
            Im Umfeld der Wittgensteinforschung gibt es eine Edition, die bis auf Zeichenebene zeitliche Informationen zur Textgenese liefert: Die Prototractatus-Tools (PTT 2016) von Martin Pilch (Pilch 2018). Sie dokumentieren den 
Nutzern Ludwig Wittgensteins Schreibprozess, beginnend mit einem leeren Notizbuch im Jahre 1915 und bis zum endgültigen Diktat des Ts-204 im Sommer 1918, das zu seiner einzigen philosophischen Veröffentlichung zu Lebzeiten, der „Logisch-Philosophischen Abhandlung“ führte. Leider konnten wir die Daten und Metainformationen der PTT-Edition in unserer FinderApp Infrastruktur nicht direkt analysieren, da unsere WiTTFind Infrastruktur zum einen auf das dokumentorientierte XML-TEI-P5 Datenformat aus Bergen zugeschnitten ist, und zum anderen die PTT-Edition im inkompatiblen Microsoft Word-97 Format vorliegt. Alle verfügbaren XML-TEI Importtools erfassten nur Bruchteile der Annotationen, sodass z.B. die Zeitinformationen der PTT überhaupt nicht erkannt und transformiert wurden. Um möglichst viel von der PTT-Textedition weiterzuverwenden, und damit der PTT-Hg. die Edition in seiner gewohnten Microsoft-Office Umgebung weiter optimieren kann, entwickelten wir eine mit Microsoft EXCEL leicht zu bedienende mehrdimensionale Tabellenstruktur. Die Editionsdaten und Metainformation der Word-97 Edition konnten wir größtenteils mit eigenen Programmen und Office-Macrotechniken transferieren. Zur Integration der Tabellen in die Infrastruktur unserer FinderApp verwendeten wir LibreOffice-Tools und selbst geschriebene Python Programme, die die Daten, sobald sie in das git-Repository des Projekts kopiert werden, mit Hilfe der continuous Integration automatisch transformieren und importieren. Zur Web-Präsentation werden sie an unsere neu entwickelte FinderApp, den 
OdysseeReader (siehe Abb. 1,
odysseereader.wittfind.cis.lmu.de), übergeben. Dieses Vorgehen trennt zwar das Daten- und Repräsentationsmodell, jedoch entwickelten wir ein positionsinvariantes Siglensystem, bestehend aus dem Tupel (Zeitstempel, Dokument, Seite, Zeile, Zeichenposition), das die beiden Modelle eineindeutig verknüpft. Diese bijektive Relation zwischen den beiden Modellen definiert dem Hg., wo er in seinem Datenmodell Änderungen vornehmen muss um sie an eine bestimmte Stelle, zu einem bestimmten Zeitpunkt im Repräsentationsmodell zu platzieren.

            
               
                  
                   Abbildung 1: Der OdysseeReader odysseereader.wittfind.cis.lmu.de
                  
               
            
         
         
            Ähnlichkeitssuche mit vorgeschaltetem Semantic Clustering
            Die Ähnlichkeitssuche WiTTSim berechnet mit Hilfe
computerlinguistischer Methoden für jede Bemerkung einen
„charakteristischen” Vektor, oder, intuitiv gesprochen: Man bestimmt
einen “Fingerabdruck”. Dieser automatisierte Prozess wird unabhängig
im Voraus berechnet, was spätere Prozesse vereinfacht und
beschleunigt. Dieser „Fingerabdruck“ beinhaltet linguistische
Informationen, wie beispielsweise Wörter, deutsche und englische
Synonyme (aus Germanet und Wordnet), Wortarten (Treetagger) und
Lemmata  (WiTTLex, Röhrer 2019). Diese Informationen werden in binäre Vektoren übersetzt, welche insgesamt etwa 115.000 Features umfassen. Zusätzlich zur Datenbasis wurden 471 Bemerkungen bereits gruppiert, also mit Ground Truth Labels versehen. Die Gruppen bestehen dabei aus 2-15 Bemerkungen und das gelabelte das Korpus umfasst 1.670 Bemerkungen, was ca. 3% des gesamten Nachlasses entspricht.

            Zur Semantischen Ähnlichkeitsberechnung ist allerdings eine Reduktion des Feature Raumes zwingend nötig, da die Vektoren mit so hoher Dimensionalität semantisch „weit voneinander entfernt“ sind und keine semantischen Gruppierungen auszumachen sind. Dieses Phänomen ist auch bekannt als 
Curse of Dimensionality. Daher werden die Vektoren zunächst auf eine angemessene Anzahl von Features skaliert, um sie anschließend clustern zu können. Verwendete Reduktionstechniken umfassen Singular Vector Decomposition (SVD), Principal Component Analysis (PCA), Sparse Random Projection (SRP) und Uniform Manifold Approximation and Projection (UMAP). Auf unseren Daten zeigte eine SVD Reduktion zu 1.600 Dimensionen die besten Ergebnisse, zusammen mit UMAP, welches darüber hinaus die Daten im zweidimensionalen Raum klar gruppiert. Letzteres erlaubt nur eine Zieldimension von 2 bis 100 Dimensionen, weshalb zum Erhalt der Varianz die maximale Dimensionsanzahl von 100 gewählt wurde, um einen bestmöglichen Erhalt der gespeicherten Information zu gewährleisten.

            Nach erfolgter Reduktion der Dimension können die Datenpunkte, also alle Bemerkungen, geclustert werden. Verwendete Clustering Techniken umfassen den klassischen K-Means Ansatz (Mac-Queen 1967, Ball and Hall 1956, Lloyd 1982, Steinhaus 1955), aber auch Dichte-basierte Ansätze wie Mean-Shift (Duda und Hart 1973) und DBSCAN (Ester et al. 1996), das statistische Gaussian Mixture Modell (Redner und Walker 1984) und das hierarchische Ward Clustering (Ward 1963). Beste Ergebnisse konnten mit einer Kombination von SVD und K-Means mit einer Anzahl an k=150 Clustern erzielt werden. Evaluiert wurde anhand der drei unüberwachten Metriken Silhouette Score, Davies Bouldin Index, und Calinski-Harabasz Index. Zusätzlich konnte durch die verfügbaren Ground Truth Labels auch der Recall berechnet werden, welcher in den Experimenten einen maximalen Wert von 1,0 erreicht. Dies zeigt, dass alle der gelabelten Daten richtig zugeordnet werden konnten. Wird eine Suchanfrage zum Auffinden ähnlicher Bemerkungen gestartet, muss nur der charakteristische Vektor der eingegebenen Bemerkung berechnet werden und das nächstliegende Cluster bestimmt werden. Letzteres erfolgt durch eine Bestimmung des am nächsten gelegenen Cluster Mittelpunkts (Zentroids). Anschließend werden die Abstände zu allen Bemerkungen des bestimmten Clusters gemessen, welche zuletzt dem Philologen zur genaueren Prüfung „gerankt“ vorgeschlagen werden.
         
         
            Zusammenfassung und Ausblick
            Unsere zeitgesteuerte textgenetische Edition kann von einem Wissenschaftler ohne XML Kenntnisse innerhalb einer Office Umgebung erstellt werden. Das continuous Integration System von git transferiert die Edition automatisch in unser WEB-basiertes Repräsentationssystem, den OdysseeReader. Über das von uns entwickelte eineindeutige Siglensystem verliert der Hg. niemals den klaren Zusammenhang zwischen Editions- und Präsentationsmodell.

            Das von uns entwickelte Ähnlichkeitstool mit vorgeschaltetem Semantic Clustering könnte auch zur Ähnlichkeitsbestimmung zwischen zwei gegebenen Texten verwendet werden: Der Nutzer könnte einen Text eingeben, und es werden potentiell ähnliche Textpassagen in einer Sammlung von Texten gesucht, die dann „gerankt“ nach Ähnlichkeiten in einer Art Hitliste ausgegeben werden. Eine derartige Sortierung nach Textähnlichkeiten könnte es dem Philologen zum Beispiel besonders erleichtern, potentielle Zitate, Einflüsse und Verweise eines Autors innerhalb seines Werkes und im Bezug auf die Literatur seiner Zeit aufzuspüren.
         
      
      
         
            
               Bibliographie
               
                  Ball, 
                  Geoffrey H.
                   / Hall 
                  David J. (1965): 
                        Isodata, a novel method of data analysis and pattern classification. Technical report, Stanford research inst Menlo Park CA.
                    
               
                  Duda, Richard
                   O. / Hart, 
                  Peter E. (1973): "Pattern analysis and scene classification." 
                        J. Wiley 1:73.
                    
               
                  Ester, 
                  Martin
                   / Kriegel 
                  Hans-Peter /
                   Sander, 
                  Jörg
                   / Xu, 
                  Xiaowei
                   et al. (1996): „A density-based algorithm for discovering clusters in large spatial databases with noise.“, in 
                        KDD, volume 96, pages 226–231.
                    
               Hadersbeck, Maximilian / Pichler, Alois / Fink, Florian / Gjesdal, Oyvind (2014): Wittgenstein’s Nachlass: WiTTFind and Wittgenstein Advanced Search Tools (WAST), DATeH, Madrid.
               
                  Hadersbeck, 
                  Maximilian
                   / Still, 
                  Sebastian
                   (2018): 
                  Investigating Wittgenstein’s Nachlass: WiTTFind, WiTTReader, OdysseeReader and Wittgenstein Advanced Search Tools, im Katalog zur Ausstellung „DIE TRACTATUS ODYSSEE“ S.127-137, Wittgenstein Initiative, Wien.
                    
               
                  Lloyd, 
                  Stuart P. (1982): „Least squares quantization in pcm“, in: 
                        IEEE transactions on information theory, 28(2):129–137.
                    
               
                  MacQueen, 
                  J. B. (1967): „Some methods for classification and analysis of multivariate observations.“, in: 
                        Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pages 281–297. Oakland, CA, USA, 1967
                    
               
                  Pichler, Alois / Krüger, Heinz W. / Smith, D. / Bruvik, Tone / Lindebjerg, Anne / Olstad, Vemund (Hrsg.) (2009): Wittgenstein Source Bergen Facsimile (BTE). Wittgenstein Source Bergen.
                    
               
                  Redner, 
                  Richard A. /
                   Walker, 
                  Homer F. (1984): Mixture densities, maximum likelihood and the em algorithm. SIAM review, 26(2):195–239.
                    
               
                  Röhrer, 
                  Ines / 
                  Ullrich, 
                  Sabine / 
                  Hadersbeck, 
                  Maximilian (2019): 
                        Weltkulturerbe international digital: Erweiterung der Wittgenstein Advanced Search Tools durch Semantisierung und neuronale maschinelle Übersetzung. multimedial multimodal. Abstracts zur Jahrestagung des Verbandes Digital Humanities im deutschsprachigen Raum, 25. - 29.03.2019 an den Universitäten zu Mainz und Frankfurt.
                    
               
                  Steinhaus, 
                  Hans (1955): Quelques applications des principes topologiques à la géométrie des corps convexes. Fund. Math, 41:284–290.
                    
               
                  Ullrich, Sabine /
                   Bruder, Daniel /
                   Hadersbeck, Maximilian (2018): “Aufdecken von “versteckten” Einflüssen: Teil-Automatisierte Textgenetische Prozesse mit Methoden der Computerlinguistik und des Machine Learning”, 5. Tagung Digital Humanities im deutschsprachigen Raum 26.2.-2.3. (Köln).
                    
               
                  Ullrich, Sabine (2019):
                         Boosting Performance of a Similarity Detection System using State of the Art Clustering Algorithms. Master‘s thesis. LMU.
                    
               
                  Pilch, Martin (2018): 
                        Frontverläufe im Prototractatus – Zur gedanklichen Entwicklung von Krakau bis Sokal (1914/1915), Wittgenstein-Studien 9 (S.101-154), Internationale Ludwig Wittgenstein Gesellschaft (ILWG).
                    
               
                  Still, Sebastian (2018): 
                        Ludwig Wittgenstein: 100 Jahre Traktatus. Der Odyssee-Reader, ein web-basiertes Tool zur text-genetischen Suche im Traktatus, Masterthesis, Ludwig-Maximilians-Universität München.
                    
               
                  Feldweg, Birgit /
                   Feldweg, Helmut (1997): „GermaNet - a Lexical-Semantic Net for German.", in: 
                        Proceedings of the ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications. Madrid.
                    
               
                  Henrich, Verena / Hinrichs, Erhard (2010): „GernEdiT - The GermaNet Editing Tool", in: 
                        Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC 2010). Valletta, Malta, pp. 2228-2235.
                    
               
                  Hörnschemeyer, Jörg / Thaller, Manfred / Förtsch, Reinhard (2017): 
                        Textgenetische Prozesse in Digitalen Editionen, Köln Universitäts- und Stadtbibliothek Köln 2017, https://www.worldcat.org/title/textgenetische-prozesse-in-digitalen-editionen/oclc/1002260195
                    
               
                  Schmidt, 
                  Alfred (2018): „Ludwig Wittgenstein’s Nachlass in the UNESCO Memory of the World register.“, in: 
                        Nordic Wittgenstein Review 7(2):209–213.
                    
               
                  UNESCO (2017): UNESCO-Weltdokumentenerbe - Zwei
  Neuaufnahmen. URL:
  https://www.unesco.at/presse/artikel/article/unesco-weltdokumentenerbe-zwei-neuaufnahmen/
  [letzter Zugriff 19. Juni 2018].

               
                  Ward, John H. (1963): „Hierarchical grouping to optimize an objective function." 
                        Journal of the American statistical association 58.301: 236-244.
                    
            
         
      
   


10974	2020	
      
         Punctuation is an important and cohesive device in all kinds of written discourse. Standard marks used to separate words, phrases, clauses and sentences for the purpose of cohesion. Already [2][5][1] pointed out that through punctuation marks, one can signal different information structures in written language. Regarding the translation of texts, we use such marks to identify the ends of sentences, closely related sentences or clauses, etc. This is why missing punctuation burdens the translations and forces the translator to go over the text several times to understand its meaning [10]. Understanding the uses and functions of punctuation marks, therefore, is extremely important for translators, as their purpose is to clarify the meaning of a particular construction within a text. On the other hand, modern poetry often disregarded such punctuations. Ever since Italian Futurism around 1900 spoke of the ‘parole in libertà’, i.e. the liberation of words from grammatical and syntactic limitations, modern poetry has hardly used punctuation. This lack of punctuation makes analysis, but also translation, more difficult. The only way to reconstruct this punctuation is by listening to the poems, i.e. by subsequently identifying sentence boundaries. However, this lack of punctuation can be found very often in modern and post-modern poetry, so the challenge is to recognize the phrase boundaries. We contribute in the paper an application towards the problem of identifying left-out punctuation in post-modern poetry, by proving that only a very simple type of punctuation - the semicolon - is needed to improve machine translation. This simple punctuation refers to phrase boundaries, the so-called “grammetrical units”, which Donald Wesling defined in his study “The Scissors of Meter” [11]. Such units must be identified in order to improve machine translation.
         The need for adding left-out punctuation becomes in case of
creating machine translations obvious with regards to the poem “bitte
verlassen sie diesen raum” (english: please leave this room) written
by the German poet Nicolai Kobus [6] (Text A):
          
         bitte verlassen sie diesen raum
         so wie sie ihn vorfinden möchten
         danke möchten sie diesen raum
         vorfinden wie sie ihn verlassen
         haben bitte räumen sie alles so
         vorgefundene als wären sie
         verlassen worden danke sie
         möchten doch nicht daß man
         sie so verlassen im raum vor
         findet bitte seien sie für einen so
         verlassen vorgefundenen raum
         dankbar [...]
          
         
            The challenge for the interpretation of this poem lies in the adequate identification of the line endings. These endings can only be identified correctly by listening to the poet's reading, which is possible because we got the audio version on the 
            lyrikline
             [7] (the world's largest corpus of spoken (post-) modern poetry which also features translations for many of the poems) webpage. This  is  the  reason,  why  the  manual translation, made by Catherine Hales, is able to translate these endings in a correct manner (Text B):
         
          
         please leave this room
         in the state in which you would like
         to find it thank you would you like
         to find this room in the state in which
         you have left it please clear out
         everything thus found as though you
         had been left thank you you would not
         like somebody to find you left
         abandoned in the room now
         would you please be grateful for
         a room a space found in such
         an abandoned state (...)
          
         In the human translation or the target poem, made by Hales, there is just a little difference. This difference is caused by the missing punctuation. And it can basically be explained by the fact that Hales has chosen a different line arrangement. In terms of content, however, her translation is reproduced correctly. Since there is no specific translation system trained with poem data with/without punctuation (small amounts of training data), we used a Google machine translation (GMT) system [3]. When we compare this (human) translation with the GMT system, we recognize the difficulty of recognizing the sentence boundaries within the poem without punctuation (Text C):
          
         please leave this room
         as they would like to find him
         Thank you for wanting this room
         find out how to leave him
         please have everything clear
         found as if they were
         Thank you
         you do not want that one
         So leave them in the room
         please find one for you
         leave found space (...)
          
         Obviously, this machine translation (MT) becomes much better if we add the full punctuation marks to the source text, when listening to the audio of the poem (Text D):
          
         please leave this room
         as you would like him to find
         Thank you. Do you want this room
         find how they leave him
         to have? Please clear everything up
         found as if they were
         been left. thank you
         Do not want that one
         So leave them in the room
         please, please be for one
         leave found space
         grateful. (...)
          
         Punctuation is an essential aspect of poetry translations, as it is for discourse analysis in general [8]. Punctuation “gives a semantic indication of the relationship between sentences and clauses, which may vary according to languages”, as well as to translations [4].
         
            A first step towards solving the problem of translation unpunctuated texts is the correct localization of the missing punctuation within such sentences and clauses. In the Google translation, which was completely without punctuation, we see that Google system translated every single line anew (Text C), ignoring the line-arrangement and the “enjambments”, when one phrase continues beyond the line, or continues from the previous line. This explains the translation error in the third line: Reading the line as a full sentence disregarding its character as an enjambment, the translation produces a full sentence (Thank you for wanting this room), which does not fit to the original (... danke. möchten sie diesen raum ...). However, this translation error will be improved if we add the missing punctuation to the machine translation, which could be identified as Text D.
         
         
            It is hard to translate automatically without having information about the sentence boundaries and the punctuation as a discourse unit for meaning demarcation. But to what extent punctuation information has to be recovered for the translation of post-modern poetry? Which kind of information do we need to improve machine translation? Do the questions have to be distinguished from the statements? Or is the simple marking of phrase boundaries already sufficient? To answer these questions, we analysed unpunctuated German poems. There are 234 german-speaking poets on the 
            lyrikline
             webpage reading a total of 2591 poems. A total of 733 German poems are translated to English which are used in this work. There are 98 German poems which do not contain any punctuation information. We analysed 120 poems in this work with a maximal punctuation information ratio of 0.05%. This process yields a total of 2924 lines out of which only 28 (0.009%) with punctuation information.
         
         The philological scholar of our project annotated the punctuation information manually by using text and audio information in the 120 poems, focusing on the intonation of poets reading their poems. In order to clarify the question which type of punctuation has to be added, we inserted two kinds of punctuation in the source text. In a first step, we focused on six different punctuation marks: full stop (.), comma (,), semicolon (;), colon (:), exclamation mark (!), and question mark (?). In a second step, we simplified this insertion by reducing these six marks to a single semicolon.
         The human reference translations are compared with the automatic translation of GMT system without/with consideration of punctuation information. The experiment consists of three tasks based on the GMT system:
         
            Task 1: Standard translations of original poems (without punctuation).
            Task 2: Translations with one level of punctuation information: replacement of all manually annotated punctuation information by one level of punctuation (;).
            Task 3: Translations with six punctuation information: consideration of the six manually annotated punctuation information (.,;:!?).
         
         The translation enhancement should be observable from improved translation quality scores. The results are calculated by bilingual evaluation understudy (BLEU) [9] score, which used for evaluating the quality of text by translation. The BLEU score of tasks 1, 2, and 3 are 0.256, 0.275, and 0.280, respectively. The results indicate that we need just one type of punctuation - semicolon - to improve the scoring for automatic translations of post-modern poetry.
         Every generic translation system is trained with data in which segments are defined by end points. It is astonishing that even the addition of a semicolon to segmental boundaries is sufficient to improve machine translation. This also explains the central problem: machine translation does not fail because of mixing up questions and statements, but because of mixing up segmental units and enjambements.
         In our future work, we plan to train a specific system on translating unpunctuated poetry in order to compare the results with manual translations. The fact that we add punctuation signs on the basis of oral representations of the poems is acceptable when it comes to audio poems, in which the oral representation is an essential part of the poem as a piece of art, closely connected to the written form.
      
      
         
            
               Bibliography
               
                  [1] Baker, M. (1994): In Other Words: A Course Book on Translation. London, New York: Routledge. 

               
                  [2] Halliday, M. A. K. (1985): An Introduction to Functional Grammar. London: Edward Arnold. 

               
                  [3] Han, S.: Free Google Translate API for Python. Available on https://pypi.org/project/googletrans/. Last accessed at 15. August 2019.

               
                  [4] Hosseini-Maasoum, S. M. / Mahdiyan, M. (2012): Punctuation in Translation: The Unseen Side of the Coin. Mediterranean journal of social sciences, 3(11):25–32. 

               
                  [5] Kirkman, J. (2006): Punctuation Matters: Advice on Punctuation for Scientific and Technical Writing. Routledge study guides. Routledge. 

               
                  [6] Kobus, N. (2006): Hard cover: Gedichte. Ardey Verlag, Münster. 

               
                  [7] Lyrikline Literaturwerkstatt Berlin: Lyrikline: listen to the poet. Available on www.lyrikline.org. Last accessed at 03. September 2019.

               
                  [8] Newmark, P. (1988): A Textbook of Translation. Prentice Hall. 

               
                  [9] Papineni, K. / Roukos, S. / Ward, T. / Zhu, W-J (2002): BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics. 

               
                  [10] Shiyab, S. M. (2017): Translation: Concepts and Critical Issues. Garant Publishers. 

               
                  [11] Wesling, D. (1996): The Scissors of Meter: Grammetrics and Reading. University of Michigan Press.

            
         
      
   


10976	2020	
      
         
            Einleitung
            Das Ziel dieses Tutorials ist es, den Teilnehmenden konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten. Es werden dabei keinerlei Vorkenntnisse zu maschinellem Lernen oder Programmierkenntnisse vorausgesetzt.
            Es gibt keinen Grund, den Ergebnissen von maschinellen Lernverfahren im Allgemeinen und NLP-Tools im Besonderen blind zu vertrauen. Durch die konkreten Einblicke in den "Maschinenraum" von maschinellen Lernverfahren wird den Teilnehmenden ermöglicht, das Potenzial und die Grenzen statistischer Textanalysewerkzeuge realistischer einzuschätzen. Mittelfristig hoffen wir dadurch, den immer wieder auftretenden Frustrationen beim Einsatz automatischer Verfahren für die Textanalyse und deren teilweise wenig zufriedenstellender Ergebnis-Daten zu begegnen, aber auch die Nutzung und Interpretation der Ergebnisse von maschinellen Lernverfahren (d.h. in erster Linie von automatisch erzeugten Annotationen) zu fördern. Zu deren adäquater Nutzung, etwa in hermeneutischen Interpretationsschritten, ist der Einblick in die Funktionsweise der maschinellen Methoden unerlässlich. Insbesondere ist die Art und Herkunft der Trainingsdaten für die Qualität der maschinell produzierten Daten von Bedeutung, wie wir im Tutorial deutlich machen werden.
            Neben einem Python-Programm für die automatische Annotierung von Entitätenreferenzen, mit und an dem während des Tutorials gearbeitet werden wird, stellen wir ein heterogenes, manuell annotiertes Korpus sowie die Routinen zur Evaluation und zum Vergleich von Annotationen zu Verfügung. Das Korpus enthält Entitätenreferenzen, die im "Center for Reflected Text Analytics" (CRETA) annotiert wurden, und deckt Texte verschiedener Disziplinen und Sprachstufen ab.
                
         
         
            
               Entitätenreferenzen
                
            Als empirisches Phänomen befassen wir uns mit dem Konzept der Entität und ihrer Referenz. Das Konzept steht für verschiedene linguistische und semantische Kategorien, die im Rahmen der Digital Humanities von Interesse sind. Es ist bewusst weit gefasst und damit anschlussfähig für verschiedene Forschungsfragen aus den geistes- und sozialwissenschaftlichen Disziplinen. Auf diese Weise können unterschiedliche Perspektiven auf Entitäten berücksichtigt werden. Insgesamt werden in den ausgewählten Texten fünf verschiedene Entitätenklassen betrachtet: PER (Personen/Figuren), LOC (Orte), ORG (Organisationen), EVT (Ereignisse) und WRK (Werke).
            Unter Entitätenreferenzen verstehen wir Ausdrücke, die auf eine Entität in der realen oder fiktiven Welt referieren. Das sind zum einen Eigennamen (Named Entities, z.B. "Peter"), zum anderen Gattungsnamen (z.B. "der Bauer"), sofern diese sich auf eine konkrete Instanz der Gattung beziehen. Dabei wird als Referenzausdruck immer die maximale Nominalphrase (inkl. Artikel, Attribut) annotiert. Pronominale Entitätenreferenzen werden hingegen nicht annotiert.
            In 
                    literarischen Texten sind vor allem Figuren und Räume als grundlegende Kategorien der erzählten Welt von Interesse. Über die Annotation von Figurenreferenzen können u.a. Figurenkonstellationen und -relationen betrachtbar gemacht sowie Fragen zur Figurencharakterisierung oder Handlungsstruktur angeschlossen werden. Spätestens seit dem 
                    spatial turn rückt auch der Raum als relevante Entität der erzählten Welt in den Fokus. Als "semantischer Raum" (Lotmann, 1972) übernimmt er eine strukturierende Funktion und steht in Wechselwirkung mit Aspekten der Figur.
                
            In den 
                    Sozialwissenschaften sind politische Parteien und internationale Organisationen seit jeher zentrale Analyseobjekte der empirischen Sozialforschung. Die Annotation der Entitäten der Klassen ORG, PER und LOC in größeren Textkorpora ermöglicht vielfältige Anschlussuntersuchungen, unter anderem zur Sichtbarkeit oder Bewertung bestimmter Instanzen, beispielsweise der Europäischen Union.
                
         
         
            
               Textkorpus
                
            Die Grundlage für (überwachte) maschinelle Lernverfahren bilden Annotationen. Um die Annotierung von Entitätenreferenzen automatisieren zu können, bedarf es Textdaten, die die Vielfalt des Entitätenkonzepts abdecken. Bei diesem Tutorial werden wir auf Annotationen zurückgreifen, die im Rahmen von CRETA an der Universität Stuttgart entstanden sind (vgl. Blessing et al., 2017; Reiter et al., 2017a). Das Korpus enthält literarische Texte aus zwei Sprachstufen des Deutschen (Neuhochdeutsch und Mittelhochdeutsch) sowie ein sozialwissenschaftliches Teilkorpus.
            
            Der 
                    Parzival
                Wolframs von Eschenbach ist ein arthurischer Gralroman in mittelhochdeutscher Sprache, entstanden zwischen 1200 und 1210. Der 
                    Parzival zeichnet sich u.a. durch sein enormes Figureninventar und seine komplexen genealogischen Strukturen aus, wodurch er für Analysen zu Figurenrelationen von besonderem Interesse ist. Der Text ist in 16 Bücher unterteilt und umfasst knapp 25.0000 Verse.
                
            
               Johann Wolfgang von Goethes 
               Die Leiden des jungen Werthers ist ein Briefroman aus dem Jahr 1774. Unsere Annotationen sind an einer überarbeiteten Fassung von 1787 vorgenommen und umfassen die einleitenden Worte des fiktiven Herausgebers sowie die ersten Briefe von Werther an seinen Freund Wilhelm.
                
            Das 
Plenardebattenkorpus des deutschen Bundestages besteht aus den von Stenografinnen und Stenografen protokollierten Plenardebatten des Bundestages und umfasst 1.226 Sitzungen zwischen 1996 und 2015. Unsere Annotationen beschränken sich auf Auszüge aus insgesamt vier Plenarprotokollen, die inhaltlich Debatten über die Europäische Union behandeln. Hierbei wurde pro Protokoll jeweils die gesamte Rede eines Politikers bzw. einer Politikerin annotiert.
                
         
         
            
               Ablauf
                
            Der Ablauf des Tutorials orientiert sich an sog. 
                    shared tasks aus der Computerlinguistik (s. a. Willand et al., 2019 zu dieser Form in den DH), wobei der Aspekt des Wettbewerbs im Tutorial vor allem spielerischen Charakter hat. Bei einem traditionellen 
                    shared task arbeiten die teilnehmenden Teams, oft auf Basis gleicher Daten, an Lösungen für eine einzelne gestellte Aufgabe. Solch eine definierte Aufgabe kann z.B. 
                    part of speech-tagging sein. Durch eine zeitgleiche Evaluation auf demselben Goldstandard können die entwickelten Systeme direkt verglichen werden. In unserem Tutorial setzen wir dieses Konzept live und vor Ort um.
                
            Zunächst diskutieren wir kurz die zugrundeliegenden Texte und deren Annotierung. Annotationsrichtlinien werden den Teilnehmerinnen und Teilnehmern im Vorfeld zur Verfügung gestellt. Im Rahmen der Einführung wird auch auf die konkrete Organisation der Annotationsarbeit eingegangen, so dass das Tutorial als Blaupause für zukünftige Tätigkeiten der Teilnehmenden in diesem und ähnlichen Arbeitsfeldern dienen kann.
            Die Teilnehmerinnen und Teilnehmer versuchen selbständig und unabhängig voneinander, eine Kombination aus maschinellen Lernverfahren, Merkmalsmenge und Parametersetzungen zu finden, die auf einem neuen, vom automatischen Lernverfahren ungesehenen Datensatz zu den Ergebnissen führt, die dem Goldstandard der manuellen Annotation am Ähnlichsten sind. Das bedeutet konkret, dass der Einfluss von berücksichtigten Features (z.B. Groß- und Kleinschreibung oder Wortlänge) auf die Erkennung von Entitätenreferenzen empirisch getestet werden kann. Dabei sind Intuitionen über die Daten und das annotierte Phänomen hilfreich, da simplem Durchprobieren aller möglichen Kombinationen („brute force“) zeitlich Grenzen gesetzt sind.Zusätzlich werden bei jedem Testlauf Informationen über die Entscheidungen protokolliert, um die Erklärbarkeit der Ergebnisse zu unterstützen. 
            Wir verzichten bewusst auf eine graphische Benutzerschnittstelle (vgl. Reiter et al., 2017b) – stattdessen editieren die Teilnehmerinnen und Teilnehmer das (Python)-Programm direkt, nach einer Einführung und unter Anleitung. Vorkenntnisse in Python sind dabei nicht nötig: Das von uns zur Verfügung gestellte Programm ist so aufgebaut, dass auch Python-Neulinge relativ schnell die zu bearbeitenden Teile davon verstehen und damit experimentieren können. Wer bereits Erfahrung im Python-Programmieren hat, kann fortgeschrittene Funktionalitäten des Programms verwenden.
            Wie am Ende jedes maschinellen Lernprozesses wird auch bei uns abschließend eine Evaluation der automatisch generierten Annotationen durchgeführt. Hierfür werden den Teilnehmerinnen und Teilnehmern nach Ablauf einer begrenzten Zeit des Experimentierens und Testens (etwa 60 Minuten) die finalen, vorher unbekannten Testdaten zur Verfügung gestellt. Auf diese Daten werden die erstellten Modelle angewendet, um automatisch Annotationen zu erzeugen. Diese wiederum werden dann mit dem Goldstandard verglichen, wobei die verschiedenen Entitätenklassen sowie Teilkorpora getrennt evaluiert werden. Auch das Programm zur Evaluation stellen wir bereit.
         
         
            
               Lernziele
                
            Am hier verwendeten Beispiel der automatischen Annotation von Entitätenreferenzen demonstrieren wir, welche Schritte für die Automatisierung einer Textanalyseaufgabe mittels maschinellen Lernverfahren nötig sind und wie diese konkret implementiert werden können. Die Teilnehmenden des Workshops bekommen einen zusammenhängenden Überblick von der manuellen Annotation ausgewählter Texte über die Feinjustierung der Lernverfahren bis zur Evaluation der Ergebnisse. Die vorgestellte Vorgehensweise für den gesamten Ablauf ist grundsätzlich auf ähnliche Projekte übertragbar.
            Das Tutorial schärft dabei das Verständnis für den Zusammenhang zwischen untersuchtem Konzept und den dafür relevanten Features, die in ein statistisches Lernverfahren einfließen. Durch Einblick in die technische Umsetzung bekommen die Teilnehmerinnen und Teilnehmer ein Verständnis für die Grenzen und Möglichkeiten der Automatisierung, das sie dazu befähigt, zum einen das Potenzial solcher Verfahren für eigene Vorhaben realistisch(er) einschätzen zu können, zum anderen aber auch Ergebnisse, die auf Basis solcher Verfahren erzielt wurden, angemessen hinterfragen und deuten zu können. 
         
         
            Abgrenzung zur Einreichung 
                    „Vom Phänomen zur Analyse – ein CRETA-Workshop zur reflektierten Operationalisierung in den DH“
                
            Neben diesem CRETA-Hackatorial befindet sich noch ein weiterer Workshop des Stuttgarter DH-Zentrums CRETA in Begutachtung. Auch wenn es eine gewisse Schnittmenge zwischen den Workshops gibt (Textgrundlagen, Anwendungsfälle), ist die jeweilige Zielsetzung grundsätzlich verschieden: Während es beim hier vorgestellten CRETA-Hackatorial um Verfahren des Maschinellen Lernens geht, konzentriert sich der parallel ausgearbeitete CRETA-Workshop auf den grundlegenderen Schritt der Operationalisierung – es geht also darum, Ansätze aufzuzeigen, wie ein Untersuchungsvorhaben oder theoretisches Konzept überhaupt für die computergestützte Analyse “vor- bzw. aufbereitet” werden kann. Beide Workshops ergänzen einander sinnvoll, was die Teilnahme an beiden oder an nur einem der Workshops möglich macht.
         
         
            Anhang 
            
               Zeitplan (Dauer in Minuten, ca.)
               Im Vorfeld der Veranstaltung: Installationsanweisungen und Support
               
                  (10) Lecture
    
                        Intro & Ablauf
                     
                  
                  (15) Hands-On
    
                        Test der Installation bei allen
                     
                  
                  (50) Lecture
    
                        Einführung in Korpus und Annotationen
                        Grundlagen maschinellen Lernens
                        Überblick über das Skript (where can you edit what?)
      
                              Grundlagen Python Syntax
                              Bereitgestellte Features
                           
                        
                     
                  
                  (15) Hands-On
    
                        Erste Schritte
                     
                  
                  (30) Kaffeepause
                  (60) Hands-On
    
                        Hack
                     
                  
                  (30) Evaluation
               
            
            
               
            
            
               Beitragende (Kontaktdaten und Forschungsinteressen)
               Der Workshop wird ausgerichtet von Mitarbeitenden des "Center for Reflected Text Analytics" (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine "black box" sein, sondern auch für Nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird.
  
                 
               
                  Gerhard Kremer
                  gerhard.kremer@ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
  Pfaffenwaldring 5b
  70569 Stuttgart
  
               Der Interessenschwerpunkt Gerhard Kremers ist der reflektierte Einsatz von Werkzeugen der Computerlinguistik für geistes- und sozialwissenschaftliche Fragestellungen. Damit zusammenhängend gehören die Entwicklung übertragbarer Arbeitsmethoden und die angepasste, nutzerfreundliche Bedienbarkeit automatischer linguistischer Analysetools zu seinen Forschungsthemen.
  
                 
               
                  Kerstin Jung
                  kerstin.jung@ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
    Pfaffenwaldring 5b
    70569 Stuttgart
  
               Kerstin Jungs Forschungsinteressen liegen im Bereich der Nachhaltigkeit von (computer)linguistischen Ressourcen und Abläufen sowie der Verlässlichkeitsbeschreibung von automatisch erzeugten Annotationen. Dabei verfolgt sie einen aufgabenbasierten Ansatz und arbeitet an der Schnittstelle zwischen Computerlinguistik und anderen sprach- und textverarbeitenden Disziplinen.
            
            
               
                  Zahl der möglichen Teilnehmerinnen und Teilnehmer
                    
               Zwischen 15 und 25. 
            
            
               
                  Benötigte technische Ausstattung
                    
               Es wird außer einem Beamer und ausreichend Stromanschlüssen für die Laptops der Teilnehmenden keine besondere technische Ausstattung benötigt. Es sollte sich um einen Raum handeln, in dem genügend Platz ist, durch die Reihen zu gehen und den Teilnehmenden über die Schulter zu blicken.
            
         
      
      
         
            
               www.creta.uni-stuttgart.de
            
             Aus urheberrechtlichen Gründen wird das Tutorial ohne das Teilkorpus zu Adornos ästhetischer Theorie stattfinden, das in den Publikationen erwähnt wird.
             Die Texte wurden im Rahmen des PolMine-Projekts verfügbar gemacht: http://polmine.sowi.uni-due.de/polmine/
         
         
            
               Bibliographie
               
                  Blessing, André / Echelmeyer, Nora / John, Markus / Reiter, Nils (2017): "An end-to-end environment for research question-driven entity extraction and network analysis" in 
Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, Vancouver.

               
                  Kuhn, Jonas / Reiter, Nils (2015): "A Plea for a Method-Driven Agenda in the Digital Humanities" in: 
  Digital Humanities 2015: Conference Abstracts, Sydney. 

               
                  Lotman, Juri (1972): 
  Die Struktur literarischer Texte, München.

               
                  Reiter, Nils / Blessing, André / Echelmeyer, Nora / Koch, Steffen / Kremer, Gerhard / Murr, Sandra / Overbeck, Maximilian / Pichler, Axel (2017a): "CUTE: CRETA Unshared Task zu Entitätenreferenzen" in 
  Konferenzabstracts DHd2017, Bern.

               
                  Reiter, Nils / Kuhn, Jonas / Willand, Marcus (2017b): "To GUI or not to GUI?" in 
  Proceedings of INFORMATIK 2017, Chemnitz.

               
                  Willand, Marcus / Gius, Evelyn / Reiter, Nils (2019): “Ein neues Format für die Digital Humanities: Shared Tasks. Zur Annotation narrativer Ebenen.” in 
  Abstracts of DHd: multimedial und multimodal, Frankfurt. 

            
         
      
   


10989	2020	
      
         
            Einleitung
            Der Workshop adressiert eine der großen Herausforderungen für Arbeiten in den Digital Humanities – die Operationalisierung geisteswissenschaftlicher Konzepte und Fragestellungen für computergestützte Methoden (vgl. Jannidis 2010, 109–132; Moretti 2013; Flanders, Jannidis 2015; Jacke 2014, 118–139). Während Geisteswissenschaftler vor allem mit komplexen, häufig textübergreifenden Phänomenen arbeiten und als relevant erachtete Kontexte der behandelten Themen heranziehen, ist die computergestützte Arbeit an identifizierbare Phänomene auf der Textoberfläche gebunden. Die hieraus erwachsende Diskrepanz zwischen Erwartungen und Ergebnissen gilt es über eine adäquate Operationalisierung, 
                    also eine Messbarmachung theoretischer Konzepte, zu überbrücken. Mit unserem Workshop wollen wir genau diese Schnittstelle in den Fokus rücken. Anhand dreier Anwendungsfälle zeigen wir auf, welche Herausforderungen sich aus dem Einsatz computergestützter Methoden für geisteswissenschaftliche Zwecke ergeben und wie mit ihnen umgegangen werden kann. In einem praktischen Teil haben die Teilnehmenden die Möglichkeit, selbst an der Operationalisierung eines Phänomens zu arbeiten; hierfür stellen wir Anwendungsfälle mit geeigneten Tools und Technik-„Baukästen“ zur Verfügung. Programmierkenntnisse werden dabei nicht vorausgesetzt. Ziel des Workshops ist es, das Bewusstsein für die Differenzen zwischen geisteswissenschaftlicher und computergestützter Arbeitsweise zu schärfen, typische Herausforderungen zu adressieren und Herangehensweisen zur Operationalisierung geisteswissenschaftlicher Phänomene aufzuzeigen. Denn nur durch die reflektierte Auseinandersetzung mit den Operationalisierungsannahmen kann ein angemessener (also reflektierter) Umgang mit den Ergebnissen gewährleistet werden.
                
         
         
            
               Use Cases
                
            Als Anwendungsfälle stellen wir drei unterschiedliche literatur- und sozialwissenschaftliche Phänomene vor, zu denen wir im Rahmen des Stuttgarter „Center for Reflected Text Analytics“ (CRETA)
umfangreiche Erfahrungen gesammelt haben. Die gewählten Beispiele decken verschiedene Aufgabentypen ab: Wir behandeln erstens die Extraktion bestimmter Instanzen aus einem Text, zweitens die Segmentierung eines Textes und drittens ein holistisches Textphänomen.
                
            
               
                  Entitäten und Entitätenreferenzen
                    
               Zum einen befassen wir uns mit dem Konzept der Entität und ihrer Referenz in literatur- und sozialwissenschaftlichen Texten (vgl. Reiter u.a. 2017, 19–22; Blessing u.a. 2017). Als Entitätenreferenzen gelten alle Ausdrücke, die auf eine Entität der realen oder fiktiven Welt referieren. Dazu zählen Personen/Figuren, Orte, Organisationen sowie Ereignisse, so dass das Konzept der Entität bewusst weit gefasst und für verschiedene Forschungsfragen anschlussfähig ist. Auf Entitäten kann auf verschiedene Weise referiert werden, u.a. über Eigen- und Gattungsnamen (z.B. “Angela Merkel”, “die Kanzlerin”). Um Entitäten in einem Text zu extrahieren, müssen folglich die Entitätenreferenzen annotiert und kookkurrente Ausdrücke aufgelöst werden. Die Herausforderungen bestehen vor allem in der Festlegung der Referenzausdrücke (welche Ausdrücke werden berücksichtigt?), in der Abgrenzung von Entitätenreferenzen gegenüber Generika sowie im Umgang mit Verschachtelungen, Metonymien und textspezifischen Besonderheiten. Am Beispiel zweier Textsorten (mhd. Artusroman und Bundestagsdebatten) stellen wir das Phänomen und Möglichkeiten der Umsetzung vor.
            
            
               
                  Erzählebenen
                    
               Des Weiteren beschäftigen wir uns mit der Annotation von Erzählebenen.
Hierbei geht es formal darum, einen Text in sinnvolle Segmente zu zerlegen, die seriell aneinandergereiht oder ineinander verschachtelt sein können. Auch wenn das narratologische Konzept ‘Erzählebene’ recht klar definiert erscheint, wird das Phänomen je nach theoretischer Grundlage unterschiedlich aufgefasst und analysiert (vgl. Genette 1988 [1983]; Ryan 1991). Um eine intersubjektive Annotation von Erzählebenen zu erreichen, gilt es deshalb zunächst, einen gemeinsamen Konsens zu theoretischen Grundannahmen zu finden. Ferner macht es die Operationalisierung von Erzählebenen notwendig, das vage Konzept akkurat zu formalisieren und distinktive Merkmale zu bestimmen, die das Phänomen sinnvoll abgrenzen können.
                    
            
            
               
                  “Wertherness”
                    
               Als dritten Anwendungsfall stellen wir die sog. “Wertherness” vor, womit eine Sammlung von Texteigenschaften gemeint ist, die Texte als “Wertheriaden” identifizieren können. Die Veröffentlichung von Goethes “Die Leiden des jungen Werthers” 1774 zog eine Reihe an literarischen Adaptationen nach sich, die sich durch verschiedene Bezugnahmen auf den Originaltext als sog. Wertheriaden ausweisen. Die Referenzen können dabei sowohl formaler (z.B. Briefroman, Dreiecksbeziehung) als auch inhaltlicher (z.B. Rolle der Natur, Verhältnis Subjekt-Gesellschaft) Art sein. Für eine computergestützte Analyse solcher Referenztexte müssen einerseits die einzelnen formalen und semantischen Kategorien operationalisiert und in den Texten identifiziert werden, andererseits ist zu untersuchen, welche Kriterien in bekannten Wertheriaden in Kombination miteinander auftreten.
            
         
         
            
               Ansätze zur Operationalisierung
                
            Im Workshop stellen wir zwei Ansätze zur Operationalisierung vor, die sich – in verschiedenen Phasen des Forschungsprozesses – sehr gut gegenseitig ergänzen. Der erste Ansatz besteht dabei in der Schärfung von 
                    Konzeptdefinitionen durch Annotationen und richtet sich an Menschen. Die Ergebnisse sind also keine Skripte oder Funktionen, sondern klare(re) Definitionen der fraglichen Konzepte, die von Menschen mit größerer intersubjektiver Übereinstimmung umgesetzt werden können, aber auch die theoretische Diskussion bereichern (vgl. Gius/Jacke, 2017; Pagel et al., 2018; Reiter et al., im Erscheinen). Daneben führt der Annotationsprozess auch zu einer intensiven und kritischen Beschäftigung mit dem Material und den textuellen Instanzen des Konzeptes und liefert damit auch Ideen für eine computergestützte Operationalisierung.
                
            Als zweiten Ansatz stellen wir die Idee vor, Zielphänomene 
                    indirekt zu operationalisieren. Hierbei werden pro Phänomen mehrere messbare Eigenschaften in den Blick genommen, die mit dem Zielkonzept verwandt, aber nicht deckungsgleich sind. Aufschlussreich ist dabei in erster Linie nicht die Inspektion einzelner Eigenschaften, sondern die Gesamtschau der verschiedenen Einflussfaktoren (vgl. “instrumental variables” in Sack, 2011; “indirekte Operationalisierung” in Reiter/Willand, 2018). Bei textbasierten Phänomenen können so insbesondere linguistische und strukturelle Eigenschaften betrachtet werden, die größtenteils mit großer Reliabilität automatisch extrahierbar sind.
                
         
         
            
               Ablauf
                
            In einem Theorieteil führen wir in die Problematik der Operationalisierung von geisteswissenschaftlichen Phänomenen für die computergestützte Analyse ein. Anhand der drei oben genannten Beispiele aus der CRETA-Praxis thematisieren wir die Problematik und stellen die Ansätze der Operationalisierung im Detail vor. Je nach Interesse kann anschließend einer dieser Anwendungsfälle ausgewählt und bearbeitet werden.
            Im praktischen Teil des Workshops haben die Teilnehmenden die Möglichkeit, beide Operationalisierungsansätze an ihrem gewählten Anwendungsfall zu erproben. Hierfür befassen sie sich zunächst mit dem Phänomen, indem sie es anhand eines Textauszugs manuell annotieren und parallel stichpunktartig die Richtlinien schärfen. In einer ersten Diskussionsrunde werden die verschiedenen Ergebnisse gesammelt und diskutiert. Zur Erprobung des zweiten Ansatzes stellen wir für jeden Anwendungsfall einen Operationalisierungs-„Baukasten“ vor. Dieser besteht aus einer Sammlung von Python-Skripten in einem Jupyter-Notebook, die auf das jeweilige Untersuchungsvorhaben zugeschnitten ist und den Teilnehmenden die Möglichkeit gibt, sich dem zu untersuchenden Phänomen über computergestützte Verfahren anzunähern. Die Teilnehmenden können in Kleingruppen in diesem Baukasten verschiedene Parameter einstellen sowie manuell Eigenschaften an- oder abwählen, wobei sie auf ihr Vorwissen über den Untersuchungsgegenstand aus der ersten Praxisrunde zurückgreifen (können). Nachdem die Teilnehmenden die Eigenschaften ausgewählt und ggf. parametrisiert haben, können sie die Ergebnisse visualisieren und mit den Texten abgleichen. Damit erhalten die Teilnehmenden ein direktes Feedback zu den ausgewählten Parametern und können prüfen, ob das Untersuchungsvorhaben mit den festgelegten Einstellungen angemessen umgesetzt wird. Der Baukasten ist zur iterativen Nutzung vorgesehen, so dass der Einfluss verschiedener verwandter Eigenschaften auf die Ausgaben sichtbar wird und die Teilnehmenden sich einer geeigneten technischen Umsetzung sukzessiv annähern können. In einer abschließenden Diskussion werden die Ergebnisse gesammelt und es wird ausgewertet, wie adäquat sich die jeweiligen Zielphänomene mittels der gewählten Annahmen haben abbilden lassen.

         
         
            
               Lernziele
                
            Ziel unseres Workshops ist es, die Teilnehmenden für die
Wichtigkeit der Operationalisierung in den Digital Humanities zu
sensibilisieren und ihnen Lösungsangebote vorzustellen. Durch die
interdisziplinäre Ausrichtung von DH-Arbeiten kommt der
Operationalisierung eine Schlüsselposition zu, indem diese eine Brücke
zwischen geisteswissenschaftlichem Phänomen und computergestützter
Umsetzung schlägt. Mit den gewählten Anwendungsfällen wollen wir den
Teilnehmenden ein “Repertoire” für die Operationalisierung
verschiedener Aufgabentypen mitgeben. Wir zeigen zum einen, dass die
Annotation eines Phänomens als Methode seiner Operationalisierung
dienen kann (vgl. Gius, Jacke 2017, 233–254); zum anderen führen wir
für textbasierte Phänomene eine approximative Operationalisierung ein
(vgl. Reiter/Willand, 2018). Beide Verfahrensweisen sind auf andere
Anwendungsfälle übertragbar. Gleichzeitig möchten wir deutlich machen,
dass es für jedes Untersuchungsvorhaben nicht nur eine, sondern
verschiedene Wege der Operationalisierung gibt. Die Spielräume, die
bei der Operationalisierung geisteswissenschaftlicher Fragestellungen
entstehen, machen es notwendig, Entscheidungen reflektiert zu treffen,
sie offenzulegen und ihren Einfluss auf die Ergebnisse als
Voraussetzung für eine angemessene Interpretation zu bedenken.
               
            
               Abgrenzung zum CRETA-Hackatorial “Maschinelles Lernen lernen”
               Neben diesem Workshop zur Operationalisierung wird noch ein weiterer Workshop des Stuttgarter DH-Zentrums CRETA während der diesjährigen DHd-Konferenz stattfinden (Gerhard Kremer, Kerstin Jung: “Maschinelles Lernen lernen: Ein CRETA-Hackatorial zur reflektierten automatischen Textanalyse”). Auch wenn es eine gewisse Schnittmenge zwischen den Workshops gibt (Textgrundlagen, Anwendungsfälle), ist die jeweilige Zielsetzung grundsätzlich verschieden: Während es beim CRETA-Hackatorial um Verfahren des Maschinellen Lernens geht, konzentriert sich der hier vorgestellte Workshop auf den grundsätzlicheren Schritt der Operationalisierung. Es geht also darum, Ansätze aufzuzeigen, wie ein Untersuchungsvorhaben oder theoretisches Konzept überhaupt für die computergestützte Analyse “vor- bzw. aufbereitet” werden kann. Beide Workshops ergänzen einander sinnvoll, was die Teilnahme an beiden oder an nur einem der Workshops möglich macht.
            
         
         
            
               Anhang
  
            
               
                  Zeitplan
    
               (insgesamt 3 Stunden + 30 Min. Pause)
               
                  Einführung und Ablauf (10 Min.)
                  Theoretischer Teil (insgesamt 40 Min.)
      
                        Erläuterung der Problemstellung 
                        Vorstellung der drei Anwendungsfälle
                     
                  
                  Praktischer Teil
      
                        Einführung in die Primärtexte und Tools, Ausgabe der skizzierten Guidelines (10 Min.)
                        Erste Praxisrunde (Kleingruppen): Manuelle Annotation eines Phänomens, parallele Erweiterung/Überarbeitung der Guidelines, iterativ (30-40 Min.)
	  - Kaffeepause (30 Min.) -
                        
                        Sammeln der Ergebnisse und Diskussion der Herangehensweisen (20 Min.)
                        Zweite Praxisrunde (Kleingruppen): Arbeit am Operationalisierungsbaukasten, Feedback über Ausgabedatei, iterativ (30-40 Min.)
                     
                  
                  Abschlussdiskussion: Sammeln der „Ergebnisse“, Diskussion der Erfahrungen und Lernziele (30 Min.)
               
            
            
               
                  Zahl der möglichen Teilnehmer
    
               Zwischen 15 und 25.
            
            
               
                  Angaben zur technischen Ausstattung
    
               Abgesehen von Beamer und ausreichend Steckdosen ist keine besondere technische Ausstattung erforderlich. Die Teilnehmenden arbeiten im praktischen Teil an ihrem eigenen PC. Informationen zu eventuellen Vorab-Installationen werden rechtzeitig mitgeteilt.
            
            
               
                  Beitragende
  
               Der Workshop wird von Mitarbeitenden des “Center for Reflected Text Analytics” (CRETA) der Universität Stuttgart veranstaltet, die bereits erfahrene Workshop-Leiter/-innen im DH-Bereich sind (DHd 2017, DH 2017, DHd 2018, ESU 2018, DHd 2019, HCH 2019). 
               Das BMBF-geförderte eHumanities-Zentrum CRETA ist auf die interdisziplinäre Zusammenarbeit von Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung ausgerichtet. Die übergreifende Zielsetzung besteht in der Erarbeitung systematischer und transparenter Workflows, in denen die Entwicklung komputationeller Modelle und Methoden kritisch reflektiert und adäquat auf die unterschiedlichen geistes- und sozialwissenschaftlichen Forschungsfragen angepasst wird.
                
               
                  Nora Ketschik
                  
                  nora.ketschik@ilw.uni-stuttgart.de
                  Universität Stuttgart
    Institut für Literaturwissenschaft, Abt. für Germ. Mediävistik
    Keplerstraße 17
    70174 Stuttgart
  
               Nora Ketschik ist Promotionsstudentin in der Abteilung für
  Germanistische Mediävistik. Im Rahmen von CRETA führt sie
  Netzwerkanalysen zu ausgewählten mittelhochdeutschen Romanen durch
  und setzt sich dabei kritisch mit der Verwendung computergestützter
  Methoden für literaturwissenschaftliche Analysezwecke
  auseinander.
                
               
                  Benjamin Krautter
                  
                  Benjamin.Krautter@ilw.uni-stuttgart.de
                  Keplerstraße 17
    70174 Stuttgart
  
               Benjamin Krautter ist Promotionsstudent in der Abteilung für
  Neuere Deutsche Literatur II und Mitarbeiter im Projekt QuaDramA -
  Quantitative Drama Analytics. Dort arbeitet er an der
  Operationalisierung Aristotelischer Kategorien für die quantitative
  Dramenanalyse. Er beschäftigt sich zudem mit der Integration
  quantitativer Methoden in literaturwissenschaftliche Fragestellungen
  (scalable reading).
  
                
               
                  Sandra Murr
                  
                  sandra.murr@ts.uni-stuttgart.de
                  Universität Stuttgart
  Institut für Literaturwissenschaft, Abt. für Neuere Deutsche Literatur I 
  Keplerstraße 17
  70174 Stuttgart
  
               Sandra Murr ist Promotionsstudentin in der Abteilung für Neuere Deutsche Literatur I. In CRETA arbeitet sie an der digitalen Analyse des “Wertheriaden-Korpus”, Texte, die in der Folge von Goethes “Werther” seit 1774 erschienen sind. Mittels computergestützter Verfahren wird sich mit der Frage auseinandergesetzt, anhand welcher charakteristischer Kriterien eine “Wertheriade” als solche definiert wird und wie sich entsprechende strukturelle und inhaltliche Kriterien operationalisieren, in den Texten automatisch identifizieren und reflektiert vergleichen lassen.
                  
               
                  Janis Pagel
                  
                  janis.pagel@ims.uni-stuttgart.de
                  Universität Stuttgart
  Institut für Maschinelle Sprachverarbeitung
  Pfaffenwaldring 5b
  70569 Stuttgart
  
               Janis Pagel ist Promotionsstudent am Institut für Maschinelle Sprachverarbeitung und Mitarbeiter im QuaDramA-Projekt. Er forscht zu Anwendungen von computerlinguistischen Methoden auf literaturwissenschaftliche Fragestellungen und innerhalb von CRETA hauptsächlich zu Koreferenzresolution für literarische Texte.
                
               
                  Nils Reiter
                  
                  nils.reiter@uni-koeln.de
                  Institut für Digital Humanities
    Universität zu Köln
    Albertus-Magnus-Platz
    50931 Köln
  
               Nils Reiter hat Computerlinguistik/Informatik an der Universität des Saarlandes studiert, wurde 2013 an der Uni Heidelberg promoviert und ist seit 2014 Post-Doc am Institut für Maschinelle Sprachverarbeitung. Seit seiner Promotion ist er im Bereich Digital Humanities unterwegs, mit einem besonderen Interesse an Fragen der Operationalisierung, und zwar sowohl im Hinblick auf Automatisierung wie auch auf manuelle Annotation. Er arbeitet dabei auch an praktischen Fragen der Kooperation zwischen Geistes- und Computerwissenschaftler*innen, und organisiert einen shared task zur Erkennung von Erzählebenen. Derzeit ist er Vertretungsprofessor für Sprachliche Informationsverarbeitung/Digital Humanities an der Universität zu Köln.
            
         
      
      
         
            
               www.creta.uni-stuttgart.de
            
            
    Bei der Umsetzung des Konzepts wurde auf Vorarbeiten des Shared Tasks “SANTA” (Systematic Analysis of Narrative Texts through Annotation) zurückgegriffen, 
    . Das Material ist veröffentlicht in Reiter u.a. (2019).
  
            
               
            
         
         
            
               Bibliographie
               
                  Blessing, André / Echelmeyer, Nora / John, Markus / Reiter, Nils (2017): „An end-to-end environment for research question-driven entity extraction and network analysis“ in 
  Proceedings of the Joint SIGHUM Workshop on
  Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, Vancouver.

               
                   Julia Flanders / Fotis Jannidis (2015):
  Knowledge Organization and Data Modeling in the Humanities, urn:nbn:de:bvb:20-opus-111270.

               
                   Gérard Genette (1988 [1983]: Narrative Discourse Revisited. (Translated by Jane E. Lewin), Ithaca. 
               
                   Marie-Laure Ryan (1991): Possible Worlds, Artificial Intelligence and Narrative Theory, Bloomington, Indianapolis.
               
                   Evelyn Gius / Janina Jacke (2017): The Hermeneutic Profit of Annotation. On Preventing and Fostering Disagreement in Literary Analysis, in: International Journal of Humanities and Arts Computing 11, S. 233–254.
               
                   Janina Jacke (2014): Is There a Context-Free Way of Understanding Texts? The Case of Structuralist Narratology, in: Journal of Literary Theory 8, S. 118–39.
               
                   Fotis Jannids (2010): Methoden der computergestützten Textanalyse, in: Methoden der literatur- und kulturwissenschaftlichen Textanalyse. Ansätze – Grundlagen – Modellanalysen, hg. v. Vera Nünning, Ansgar Nünning und Irina Bauder-Begerow, Stuttgart, Weimar, S. 109–132.
               
                   Franco Moretti (2013): “Operationalizing”: or, the function of measurement in modern literary theory, in: Literary Lab 6, S. 1–13.
               
                   Janis Pagel / Nils Reiter / Ina Rösiger / Sarah
Schulz (2018): A Unified Text Annotation Workflow for Diverse Goals, in: Proceedings of the Workshop for Annotation in Digital Humanities (annDH), hg. v. Sandra Kübler und Heike Zinsmeister, Sofia, Bulgaria, August 2018, S. 31–36.
               
                   Nils Reiter / Evelyn Gius / Marcus Willand
(Hrsg.) (2019): A Shared Task for the Digital Humanities. Special issue of Cultural Analytics. November 2019.
               
                   Nils Reiter / Marcus Willand (2018): Poetologischer Anspruch und dramatische Wirklichkeit: Indirekte Operationalisierung in der digitalen Dramenanalyse, in: Quantitative Ansätze in den Literatur- und Geisteswissenschaften: Systematische und historische Perspektiven, hg. v. Toni Bernhart, Marcus Willand, Sandra Richter und Andrea Albrecht, Stuttgart, S. 45–76.
               
                   Nils Reiter /   André Blessing / Nora Echelmeyer /
Gerhard Kremer / Steffen Koch / Sandra Murr / Maximilian Overbeck /
Axel Pichler (2017): CUTE: CRETA Unshared Task zu Entitätenreferenzen, in: DHd 2017 Bern, Conference Abstracts, S. 19–22.
               
                   Graham Alexander Sack (2011): Simulating Plot: Towards a Generative Model of Narrative Structure, in: Papers from the AAAI Fall Symposium (FS-11-03).
            
         
      
   


11000	2020	
      
         
            Einleitung
            In diesem Beitrag stellen wir eine Methode vor, um Informationen über Figurenrelationen in dramatischen Texten, die innerhalb der 
dramatis personae (Figurenverzeichnis) sprachlich kodiert sind, zu extrahieren und maschinenlesbar im TEI/XML vorzuhalten. Das Figurenverzeichnis kann als Paratext (Genette 1993) dem Nebentext zugerechnet werden, ist jedoch literaturwissenschaftlich, von Einführungswerken abgesehen, noch so gut wie nicht erschlossen. Das Figurenverzeichnis steht zwar unabhängig vom eigentlichen Text am Anfang, kann jedoch bereits Figuren- bzw. Textwissen vermitteln, indem die Figuren nach sozial-politischem Stand, Familienzugehörigkeit oder nach anderen Gruppierungen geordnet sind (vgl. Abbildung 1). Häufig lässt sich an der Positionierung eines Names im Figurenverzeichnis auch die Wichtigkeit der betreffenden Figur im Drama ablesen (Pangallo 2015, 91). Durch diese Strukturierung ist es teilweise möglich, schon vorab auf zentrale Konfliktpotentiale des Textes zu schließen (Jeßing 2015, 79–80). Darüberhinaus kann das Figurenverzeichnis laut Pfister und Asmuth auch der Ort erster auktorialer Bewertungen oder Hinweise sein und dient somit nicht nur der reinen Vorstellung der Figuren und ihrer Strukturen untereinander (Pfister 2001, 95; Asmuth 2016, 85).

            
               
                  
                   Abbildung 1: Figurenverzeichnis in Die Räuber (Friedrich Schiller, 1781)
               
            
            Das Verfahren – und dessen Implementierung in einem Python-Skript – ist auch für in Zukunft digitalisierte Dramen anwendbar, und wird von uns als quelloffene Software zur Verfügung gestellt. Es ist vergleichsweise einfach auf neue Sprachstufen oder Genres anpassbar und liefert – auch bei nicht-perfekten Ergebnissen – eine gute Vorlage. Eine Evaluation des Verfahrens erfolgt auf ungesehenen Testdaten. Außerdem veröffentlichen wir einen Datensatz mit extrahierten Figurenrelationen aus deutschsprachigen Dramen, die manuell validiert und korrigiert wurden. Diese Daten werden zur einfachen und breiten Nutzung im TEI-Format in das GerDraCor eingespeist. Schlussendlich beschreiben wir beispielhaft zwei Analyseszenarien in denen die Daten neue Einblicke bieten (können).

         
         
            Automatische Extraktion von Figurenrelationen
            Unsere Methode unterscheidet zwischen sieben Kategorien von Figurenrelationen (Tabelle 1). Ausschlaggebend für die Zuordnung zu einer der Kategorien sind Signalwörter wie “Vater”, “Kammerdiener”, “Geschwister” etc. Diese Signalwörter werden in einer kontextfreien Grammatik der entsprechenden Kategorie zugeordnet.
            
                Tabelle 1: Figurenrelationen
               
                  Relationen Label
                  gerichtet/ungerichtet
                  Beschreibung
               
               
                  parent_of
                  directed
                  Eine Figur ist Elternteil einer anderen
               
               
                  lover_of
                  directed
                  Liebesbeziehungen (unverheiratet)
               
               
                  related_with
                  directed
                  Familienbeziehungen (außer Eheleute)
               
               
                  associated_with
                  directed
                  Figuren, die miteinander anderweitig verbunden sind (z.B. Diener, Kindermädchen etc.)
               
               
                  siblings
                  undirected
                  Figuren, die mindestens ein gemeinsames Elternteil haben
               
               
                  spouses
                  undirected
                  verheiratete oder verlobte Figuren
               
               
                  friends
                  undirected
                  Freundschaftsbeziehungen
               
            
            Kontextfreie Grammatiken bezeichnen in der Informatik eine Sammlung aller syntaktisch korrekten Programme einer Programmiersprache (Böckenhauer und Hromkovič 2013, 177). Die formalisierte Art, in der die Grammatik alle Regeln einer Programmiersprache enthält, erlaubt es, automatisierte Syntaxanalysen von Programmen durchzuführen (Böckenhauer und Hromkovič 2013, 177). Die Regeln werden mit Hilfe zweier Alphabete beschrieben: Das Terminalalphabet enthält alle Wörter einer Sprache, wohingegen das Nichtterminalalphabet Variablen enthält, die vorgeben, auf welche Art und Weise die Wörter kombiniert werden können (Böckenhauer und Hromkovič 2013, 178).
            Wir nutzen eine solche Grammatik, um drei verschiedene Zeilenarten im Figurenverzeichnis zu unterscheiden, bei denen es sich um Nichtterminale handelt. Alle in den Sätzen vorkommenden Tokens sind Terminale, deren Kombination und Anzahl Aufschluss darüber gibt, um was für eine Art von Zeile es sich jeweils handelt. Auf diese Weise können auch zeilenübergreifende Relationen erkannt werden.
            Zu Beginn des Programmablaufs werden die in GerDraCor vorhandenen Figuren-IDs zusammen mit dem Figurenverzeichnis ausgelesen und gespeichert. Da wir die Beziehungen zwischen den Figuren ausschließlich anhand der Angaben im Figurenverzeichnis konstruieren, muss der Dramentext nicht extra eingelesen werden. Daraus ergibt sich die Beschränkung, dass jegliche Beziehungen, die nicht im Figureverzeichnis explizit gemacht werden, vom Programm auch nicht erkannt werden können. Es geht demnach ausschließlich darum, das Personenverzeichnis maschinenlesbar und -interpretierbar zu machen. So ignoriert das Programm beispielsweise auch alle Zeilen, die eine Gruppe von Figuren als Kollektiv einführt, da diese als “Nummern oder als anonyme Angehörige von Untergruppen” (Schlaffer 1972, 11) meistens keine eigenen Namen haben und auch keine explizit gemachten Beziehungen.
            
            Anschließend werden alle Tokens jeder Zeile des Figurenverzeichnisses daraufhin untersucht, ob es sich dabei um Figurennennungen oder Signalwörter handelt und die Grammatik einem Parser übergeben, der die Zeilen des Figurenverzeichnisses in Baumstrukturen überführt (Abbildung 2).
            
               
                  
                  Abbildung 2: Zwei reduzierte Baumstrukturen für Figuren aus Nathan der Weise. 
               
            
            Aus den erstellten Baumstrukturen werden einzelne Informationen ausgelesen, die grundlegend für die Erkennung der Figurenrelationen sind. Zuerst wird überprüft, wie viele IDs sich in einer Zeile befinden. Die erste oder einzige wird zur Erstellung späterer Relationen abgespeichert. Befindet sich in einer Zeile zusätzlich zu einer ID noch ein Signalwort für eine Figurenrelation, bezieht sich die Zeile in der Regel auf die vorangegangene, wie beispielsweise in 
                    Nathan der Weise:
                
            Sultan Saladin.
            Sittah, seine Schwester.
            Die zweite Zeile enthält neben dem Namen noch das Signalwort “Schwester”, das auf die Beziehungsart siblings hinweist, eine ungerichtete Relation. Da keine zweite Figurenbezeichnung in der Zeile vorkommt, entnimmt das Programm als zweiten Part für die Geschwisterbeziehung den Namen bzw. die daraus abgeleitete ID saladin aus der vorherigen Zeile:
            
            Wenn die beiden benötigten IDs für das Erstellen der Figurenrelation feststehen, wird die Art der Relation durch das Auslesen des Signalworts aus der Baumstruktur festgestellt. Danach werden daraus die Zeilen mit den Figurenrelationen erstellt und diese anschließend in die jeweilige TEI-Version des Textes geschrieben.
            Befindet sich in einer Zeile eine zweite Figuren-ID, bezieht sich die Zeile nicht auf eine vorangegangene, sondern stellt selbst den zweite Bezugspunkt der Relation. Das ist beispielsweise bei der Figur “Camillo Rota” in 
                    Emilia Galotti der Fall:
                
            Camillo Rota, einer von des Prinzen Räten.
            Die erste erkannte ID ist camillo_rota, die zweite der_prinz, abgeleitet aus “des Prinzen”. Die IDs werden in gerichtete Relationen mit aktivem und passivem Part überführt:
            
            Das Programm arbeitet dabei ausschließlich mit den IDs. Dafür ist es nicht nötig, dass Figurennamen explizit als Namen oder Adelstitel als Titel erkannt werden. Es geht ausschließlich darum aus den einzelnen Wörtern einer Zeile im Figurenverzeichnis Namen bzw. Namensteile und Titelangaben herauszufiltern, die den IDs entsprechen, um die Zeilen einer oder mehreren Figuren zuordnen zu können.
            Um auch IDs zu erkennen, die sich geringfügig von den Namensnennungen im Figurenverzeichnis unterscheiden, überprüft das Programm pro Wort eine Reihe an Varianten. So trennt es beispielsweise vom oben gennannten Wort “Prinzen” das Suffix ab und überprüft, ob ein Artikel Teil der ID ist. So kann “des Prinzen” der ID “der_prinz” zugeordnet werden. In manchen Fällen funktioniert diese Abwandlung aber nicht so reibungslos. In 
                    Der Eheteufel auf Reisen wird eine Figur im Figurenverzeichnis mit dem Namen “Gustel” eingeführt, wohingegen die ID „gustchen“ lautet. Die ID orientiert sich hier an der Namensform, die im Stück tatsächlich verwendet wird und nicht an der Bezeichnung im Figurenverzeichnis. Das führt dazu, dass das Programm die ID “gustchen” nicht dem Wort “Gustel” zuordnen kann, da sie sich zu stark unterscheiden.
                
         
         
            Evaluation
            Um die Methode zu evaluieren, wurden die automatisch erzeugten Relationen manuell nachkorrigiert und so ein Goldstandart erzeugt. Im Schnitt bearbeiteten die Korrektoren 12 Texte pro Stunde. Beim Abgleich der automatisch erzeugten Ergebnisse mit dem Goldstandart lag der Macro-Average-Recall Wert bei 0,3 (Standardabweichung: 0,3) und der Wert von Macro-Average-Precision bei 0,55 (Standardabweichung: 0,4), was einen Macro-Average-F-Score von 0,49 (Standardabweichung: 0,25) ergibt.
         
         
            Korpus
            GerDraCor ist ein deutsches Dramenkorpus, das nach TEI-P5 Standarts kodiert ist und im Dezember 2019 474 Dramen enthält, die im Zeitraum von 1730 bis 1940 veröffentlicht wurden (Fischer u. a. 2019). Es ist Teil des größeren DraCor (Fischer u. a. 2019), das als 
                    Programmable Corpus darauf ausgelegt ist, durch Community-Anstrengungen korrigiert und verbessert werden zu können (Fischer u. a. 2019, 195). Da auf einem Fork von GerDraCor gearbeitet wurde, können die automatisch erzeugten Figurenrelationen dem Korpus unproblematisch hinzugefügt werden. Zusätzlich wurden die Relationen, wie bereits beschrieben, manuell nachkorrigiert, um eine erhöhte Qualität für die Nachnutzung zu gewährleisten.
                
            Im Rahmen der manuellen Nachkorrektur wurden außerdem interessante Fälle identifiziert. So wird etwa eine Gruppe von Figuren in dem oben abgebildeten Figurenverzeichnis von Schillers 
                    Die Räuber als “Libertiner,
		    nachher Banditen” bezeichnet, wodurch
		    Informationen aus der späteren Handlung des
		    Stückes vorweggenommen werden. Diese Art der
		    Vorwegnahme findet sich außerdem in Stücken von
		    Grabbe
		    (Herzog Theodor von
		    Gothland, Panizza
		    (Das Liebeskonzil) und
		    Uhland
		    (Ludwig der Bayer). In Kaisers 
                    Stadt und Land hingegen wird mit der Zeile “Erster Bergmann, später Michael” keine Entwicklung in der Handlung, sondern eine Veränderung der Sprecherbezeichnung markiert. Vorwegnahmen mit Bezug auf veränderliche Beziehungen zwischen Figuren konnten nicht festgestellt werden.
                
         
         
            Analyseszenarien
            Wir stellen im folgenden zwei Analysen vor, in denen von den automatisch extrahierten Relationen Gebrauch gemacht wird, sowohl eine Einzeltext- als auch eine Korpusanalyse. Diese illustrieren Möglichkeiten, die Relationen in der Textanalyse zu berücksichtigen.
            Im ersten Beispiel betrachten wir Shakespeares 
                    Romeo and Juliet in der derzeit auf dracor.org verfügbaren Fassung. Zunächst können die Relationen visualisiert werden. Abbildung 3 zeigt das Figurennetzwerk nach Kopräsenz auf der linken und das Netzwerk, das sich aus den sozialen Beziehungen ergibt auf der rechten Seite. Zur besseren Lesbarkeit wurde ein geeigneter Layout-Algorithmus angewendet. Dabei ist zunächst interessant, dass die beiden Familien keineswegs unverbunden sind: Über Mercutio (Freund von Romeo) und Paris (Verlobter von Julia) sind beide mit dem Prinzen verbunden.
                
            
               
                  
                  Abbildung 3: Figurennetzwerke nach Kopräsenz (oben) und Relationen (unten). Zur besseren Übersichtlichkeit wurden die Figuren auf feste Positionen gesetzt, die oben und unten gleich sind. Bezeichnungen werden nur gezeigt wenn der Grad groß genug ist (oben) oder sie an einer Beziehung beteiligt sind (unten). 
               
            
            Auch wenn Abbildung 3 eine gewisse Symmetrie suggeriert, ist diese keineswegs gegeben wenn wir die Redeanteile nach Familien aufschlüsseln, wie es aus den Annotationen ebenfalls direkt möglich ist. Abbildung 4 zeigt die aggregierten Redeanteile der Figuren, wobei Figuren, die durch Verwandtschaft oder Arbeitsverhältnis zu einer der Familien gehören, zusammengefasst wurden (mit Ausnahme von Mercutio und Paris, die beide mit dem Prinzen verwandt sind). Es zeigt sich, dass Angehörige der Familie Capulet etwas weniger als doppelt so viele Wörter äußern als Angehörige der Familie Montague.
            
               
                  
                  Abbildung 4:  Redeanteile nach Familie 
               
            
            
               
                  
                  Abbildung 5: Verteilung der Relationen im Gesamtkorpus
               
            
            Betrachtet man das annotierte Gesamtkorpus stellt man fest, dass die Relationen ungleich verteilt sind. Während Ehen/Verlobungen, Elternschaft und sonstige Assoziationen relativ häufig vorkommen, spielen Geliebte, sonstige Verwandtschaften, Freundschaften und Geschwister eine vergleichsweise kleine Rolle.
            
            In Abbildung 6 sehen wir die Anzahl der Relationen bestimmter Typen
ins Verhältnis gesetzt zur Großgattung (Komödie/Tragödie). Dabei
wurden die Angaben auf den Titeln der Dramen übernommen und leicht
vereinheitlicht (z.B. Bürgerliches Trauerspiel → Tragödie oder Zauberlustspiel → Komödie). Dabei ist zu konstatieren, dass Median und erstes Quartil bei 0 für alle Dramen bei 0 liegen: Viele Dramen weisen keine Beziehungsdefinition auf (oder sie konnten nicht automatisch identifiziert werden, siehe Fußnote ). Größere oder signifikante Abweichungen zwischen den Gattungen gibt es nicht, egal welche Relation betrachtet wird. Lediglich die Relation spouses scheint im Figurenverzeichnis von Komödien häufiger genannt zu werden.
            
               
                  
                  Abbildung 6:  Anzahl typisierter Relationen nach Gattung 
               
            
            
               
                  
                  Abbildung 7: Anzahl typisierter Relationen nach Autor. Zur besseren Übersicht wurden nur Autoren berücksichtigt, die mindestens durch fünf Dramen vertreten sind
               
            
            Eine Verteilung der genannten Relationen nach Autor zeichnet jedoch ein anderes Bild (Abbildung 7). Bestimmte Autoren, vor allem Ludwig Anzengruber (1839-1889) und Johann Nestroy (1801-1862), haben klare Tendenzen dazu, mehr Relationen im Figurenverzeichnis zu nennen. Beide verfassen tendenziell Possen und Komödien.
         
         
            Fazit
            Mit den von uns bereitgestellten maschinenlesbaren Informationen ermöglichen wir Analysen dramatischer Figuren, die die als bekannt vorausgesetzten Informationen im Figurenverzeichnis mit berücksichtigen können. Neben den oben skizzierten Analysen können die Informationen auch in inhaltliche Analysen einfließen und etwa die soziale Nähe mit der Bühnennähe korrelieren o.ä.
            Kontextfreie Grammatiken haben sich hier – trotz der bekannten Schwächen im Bezug auf natürliche Sprache – als effizienter Formalismus herausgestellt, um die Figurenverzeichnisse maschinenlesbar zu machen. Wir halten dieses Verfahren für geeignet, um auch in anderen Kontexten mit semi-strukturierten Textdaten zu arbeiten, wo aufgrund der begrenzten Menge ein maschinelles Lernverfahren nur bedingt zum Einsatz kommen kann.
         
      
      
         
            
      Beispielsweise  spielt das Figurenverzeichnis im kürzlich erschienenen
      (Tonger-Erk, Werber, und Baum 2018), aber auch in (Genette 1993)
      quasi keine Rolle.
    
            
               https://dracor.org
            
            
      Für mehr     Informationen vergleiche (Schlaffer 1972, 11)
    
            
               
            
            
      Die konkreten Ergebnisse wurden auf den vollautomatisch erzeugten Relationen erzielt.
    
         
         
            
               Bibliographie
               
                   Asmuth, Bernhard.  (2016). 
  Einführung in die Dramenanalyse. Stuttgart: J.B. Metzler Verlag.

               
                   Böckenhauer, Hans-Joachim /  Juraj Hromkovič  (2013): 
  Formale Sprachen: Endliche Automaten, Grammatiken, lexikalische und syntaktische Analyse. Zürich: Springer.

               
                   Fischer, Frank / Ingo Börner / Mathias Göbel /
  Angelika Hechtl  / Christopher Kittel / Carsten Milling /  Peer
  Trilcke
  (2019):  „Programmable Corpora – Die digitale Literaturwissenschaft zwischen Forschung und Infrastruktur am Beispiel von DraCor“. In 
  Proceedings of DHd. 
  .

               
                   Genette, Gérard  (1993): 
  Palimpseste. Die Literatur auf zweiter Stufe. Frankfurt am Main: Suhrkamp.

               
                   Jeßing, Benedikt (2015): 
  Dramenanalyse. Eine Einführung. Berlin: Erich Schmidt Verlag.

               
                   Pangallo, Matteo (2015):  „‚I will keep and character that name‘: Dramatis Personae Lists in Early Modern Manuscript Plays“. 
  Early Theatre 18 (2): 87–118. .

               
                   Pfister, Manfred (2001): 
  Das Drama. München: Wilhelm Fink.

               
                   Schlaffer, Hannelore (1972):
  Dramenform und Klassenstruktur. Eine Analyse der dramatis persona "Volk". Stuttgart: J.B. Metzler Verlag.

               
                   Tonger-Erk, Lily  /  Nils Werber /  Constanze Baum (Hrsg.) (2018):  „Hauptsache Nebentext. Regiebemerkungen im Drama“. 
  Zeitschrift für Literaturwissenschaft und Linguistik 48 (3).

            
         
      
   


11009	2020	
      
         
            Einleitung
            Mit der Einführung des Konzepts des „Distant Reading“ von Moretti (2002) wurde in den digitalen Literaturwissenschaften in den letzten Jahren ein Trend angestoßen, den Einsatz von computergestützten quantitativen Methoden zur Analyse und Visualisierung von sehr großen Mengen von Texten zu explorieren. Während dieses Konzept und der Einsatz digitaler Methoden in den Literaturwissenschaften umstritten ist, sind computergestützte und quantitative Verfahren in den Musikwissenschaften schon länger etabliert und werden meist als statistische Musikwissenschaften bezeichnet (Nettheim, 1997). In Anlehnung an den Distant Reading-Begriff aus den Literaturwissenschaften wurde in den letzten Jahren versucht ähnliche Begriffe für die Musikwissenschaft einzuführen, um die computergestützte quantitative Analyse und Visualisierung von größeren Mengen an Musikstücken zu beschreiben. In der jüngsten Forschung findet man diesbezüglich die Begriffe: 
                    Distant Audition (Abdallah et al., 2017), 
                    Distant Listening (Cook, 2013) aber auch 
                    Distant Hearing (Burghardt, 2018). Diese Begriffe werden in Abgrenzung des jeweiligen Close-Konzepts, also 
                    Close Audition/Listening/Hearing betrachtet, womit die etablierte individuelle Analyse einzelner oder sehr weniger Stücke mittels hermeneutischer und qualitativer Methoden bezeichnet wird. Die genannten Begriffe sind nicht in gleicher Weise etabliert wie der Distant Reading-Begriff. Auch wird mit Distant Reading mittlerweile eine Vielzahl komplexer Methoden wie Sentiment Analysis und Topic Modeling beschrieben. Im Folgenden werden wir jedoch den Begriff Distant Hearing verwenden und bezeichnen damit die computergestützte quantitative Analyse und Visualisierung von mehreren Musikstücken. Wir berichten im vorliegenden Beitrag über den momentanen Stand der Entwicklung des neuen Tools 
                    BeyondTheNote, welches Konzepte des Distant Hearing integriert.
                
         
         
            Tools und Programme in der digitalen Musikwissenschaft
            Unabhängig von der Begriffsverwendung wurden einige Tools und Programme entwickelt, um die computergestützte Analyse im Sinne von Distant Hearing zu unterstützen. Nichtsdestotrotz liegen noch einige Mängel vor, die wir im Folgenden herausarbeiten, um die Entwicklung des neuen Tools 
                    BeyondTheNotes zu motivieren. Bereits in den 1990er Jahren wurde das 
                    Humdrum-Toolkit entwickelt (Huron, 1994; Huron, 2002). Es handelt sich dabei um eine programmiersprachen-unabhängige Sammlung von Kommandozeilen-Tools. Eines der bekanntesten und meistgenutzten Programm-Pakete ist 
                    music21 (Cuthbert & Ariza, 2010). Dies ist eine Python-Bibliothek, die Analyse-Möglichkeiten für Musikstücke bietet, die in digitalen Formaten symbolhaft repräsentierter Musik (z.B. MusicXML) vorliegen. In beiden Fällen sind jedoch fortgeschrittene Programmier- und IT-Kenntnisse notwendig, um die Tools zu verwenden. Speziell für HumDrum findet man aber auch Tools, die versuchen eine grafische Schnittstelle anzubieten, um leichter auf die Funktionen von HumDrum zuzugreifen (Taylor, 1996; Kornstädt, 1996). Die genannten Umsetzungen benötigen jedoch teils aufwendige Installationen und erhebliche Einarbeitungszeit. Wie jedoch Burghardt und Wolff (2014) in ihrem Aufsatz über Humanist-Computer Interaction schreiben, ist eine möglichst einfache Zugänglichkeit und eine geringe Schwelle bezüglich des technischen Vorwissens ein essenzielles Kriterium damit Tools in den Geisteswissenschaften breite Verwendung finden. Ferner wird argumentiert, dass auch Aspekte der Usability und User Experience besonders wichtig sind, um aufwendige Einarbeitungszeiten zu vermeiden. Ein leichter zugängliches Web-Tool ist das 
                    Digital Music Lab VIS (DML-VIS, Abdallah et al., 2017). Das Tool integriert auch Ideen des Konzepts von Distant Hearing und ermöglicht Analysen und Visualisierungen auf vorgefertigten Korpora. Dennoch fehlen einige Analysen wichtiger musikalischer Metriken und es ist auch nicht möglich eigenes Material zu analysieren.
                
            Tools und Programme, die speziell die Bedürfnisse von Geisteswissenschaftlern beachten sind bislang selten. Im Kontext der Digital Humanities findet man aktuell Arbeiten im Kontext von Jazz (Frieler et al., 2018), klassischer Musik (Condit-Schultz et al., 2018) und Volksmusik (Burghardt et al., 2015; Burghardt & Lamm, 2017). Vereinzelt bieten diese auch statistische Analysen an (Burghardt et al., 2015), sind aber insgesamt verstärkt fokussiert auf Retrieval-Aspekte.
         
         
            BeyondTheNotes
            BeyondTheNotes wurde mit dem Ziel entwickelt, die weiter oben genannten Probleme und Mängel bisheriger Software-Pakete aufzugreifen und sich an Bedürfnissen von Musikwissenschaftlern zu orientieren. BeyondTheNotes grenzt sich von bisherigen Tools ab indem die technischen Hürden bezüglich Programmierkenntnissen und aufwendigen Installationsverfahren umgangen werden, da BeyondTheNotes als leicht zugängliches Web-Tool geplant ist, das eine grafische Benutzeroberfläche bietet und in jedem gängigen Browser verwendet werden kann. Um Aspekten der Usability und User Experience gerecht zu werden, integrieren wir Methoden des User Centered Design-Prozesses. Als weitere Abgrenzung zu bisherigen Software-Paketen liegt der Fokus auf Distant Hearing und nicht auf der Einzelanalyse. Im DML-VIS fehlende Funktionen wie der Upload von eigenen Dateien oder die Analyse wichtiger musikalischer Metriken wurden integriert. Zielgruppe des Tools sind Musikwissenschaftler und Studierende mit Interesse an quantitativer computergestützter Musikanalyse.
            
               
               Abbildung 1: Logo von BeyondTheNotes
            
         
         
            Entwicklung
            Für die Entwicklung des Tools wurden Ideen des User Centered Design-Prozesses (UCD) (Vredenburg et al., 2002) integriert. Dabei wird versucht in iterativen Entwicklungszyklen potentielle Nutzer mit Methoden des Usability Engineerings so früh wie möglich in den Entwicklungsprozess einzubeziehen.
            Um den Anforderungen unserer Zielgruppen gerecht zu werden, fand gemäß UCD vor Entwicklungsbeginn eine Anforderungsanalyse statt. Diesbezüglich wurden eine Fokusgruppe mit Studierenden der Musikwissenschaft sowie zwei semi-strukturierte Interviews mit ausgebildeten Musikwissenschaftlern durchgeführt. Dadurch sollte Einblick in die Arbeitsweisen von Musikwissenschaftlern gewonnen und Bedürfnisse an ein computergestütztes Tool identifiziert werden. Die Ergebnisse werden im Folgenden zusammengefasst: 
            Die Teilnehmer unserer Anforderungsanalysen erläuterten, dass es kein festes methodisches Vorgehen bei der Analyse von Musikstücken gibt, jedoch steht im Mittelpunkt stets das Verfassen eines Textes. Für diesen Prozess werden statistische Visualisierungen als nützlich erachtet. Meist wird nur ein Stück oder eine überschaubar große Zahl analysiert. Größere Analysen finden für Genres und Komponisten. Als wichtige Features wurden die Analyse von eigenem Material sowie der Download der Ergebnisse genannt. Interessante Metriken für die Analyse sind aus Sicht unserer Teilnehmer Leittöne, Tonarten, Akkorde, Intervalle, der Tonumfang, Tonhöhen und jegliche Form von Motiven. Die Teilnehmer äußerten selten den konkreten potentiellen Einsatz und Nutzen eines Tools in ihrem Arbeitsworkflow und sehen den meisten Nutzen eines potentiellen Tools eher in der vielseitigen Exploration einer großen Menge an Ergebnisse. Die Ergebnisse der Anforderungsanalyse wurden in greifbare Features übertragen und die Mehrzahl dieser in das Tool eingearbeitet. In der Weiterentwicklung des Tools werden wir den UCD weiter aufgreifen indem z.B. größere Usability-Tests und Redesign-Phasen stattfinden und wir den Einsatz des Tools im konkreten Forschungsworkflow untersuchen. 
            Das Tool wurde in Python mit dem Framework Django implementiert. Für viele musikalische Analysen wurde im Back-End music21 (Cuthbert & Ariza, 2010) eingesetzt. Für die Visualisierung von Notenblätter und Statistiken wurde OpenSheetMusicDisplay, Zingchart und chartist.js genutzt. Andere wichtige Technologien für die Entwicklung schließen PostgreSQL, JavaScript und Jquery ein.
                
         
         
            Funktionen
            Die Funktionen des Tools gliedern sich in zwei Bereiche. Die Analyse von einem einzelnen Stück inklusive seiner Partitur („Individual Analysis“) und die statistische Analyse von einem oder mehreren Werken bezüglich der Verteilungen unterschiedlicher Metriken („Distant Hearing"; Abbildung 2).
            
               
               Abbildung 2: Start-Screen von BeyondTheNotes
            
            Nach Auswahl eines Bereichs kann der Nutzer eine oder mehrerer Dateien für die Analyse hochladen. Es werden alle gängigen Dateiformate symbolhaft repräsentierter Musik akzeptiert z.B. MusicXML, MEI, Midi, ABC usw. Alternativ wird zum Testen der music21-Korpus zur Verfügung gestellt. Es handelt sich dabei um ein freies, überschaubar großes Korpus, das unter anderem Werke von Mozart, Bach und Schubert enthält.
            Über eine Suchfunktion können die hochgeladenen Dateien und das bestehende Korpus gefiltert werden (Abbildung 3).
            
               
               Abbildung 3: Upload und Suche
            
            Wird die Individual Analysis gewählt, wird die Partitur des Stücks angezeigt (Abbildung 4). 
            
               
               Abbildung 4: Anzeige für die Auswahl der „Individual Analysis“
            
            Folgende Analysemöglichkeiten sind hier möglich:
            
               Die Akkordanalyse („Chords“): Hierbei wird die Partitur mit den Akkorden ersetzt und diese in römischen Ziffern oder ihren herkömmlichen Namen angezeigt (Abbildung 5) 
            
            
               
               Abbildung 5: Transformiertes Notenblatt nachdem die Akkordanalyse durchgeführt wurde
            
            
               Die Analyse des Tonumfangs („Ambitus“) (Abbildung 6)
            
            
               
               Abbildung 6: Tonumfang-Analyse (Ambitus)
            
            
               Die Analyse der Tonart: Hierbei werden die vier wahrscheinlichsten Tonarten mit ihren Wahrscheinlichkeitswerten angezeigt (Abbildung 7). Die Kalkulationen basieren auf music21.
            
            
               
               Abbildung 7: Tonart-Analyse
            
            Die Ergebnisse der Akkord- und Tonartanalyse können auch verknüpft werden. Der Nutzer kann eine der ermittelten Tonarten auswählen und je nachdem werden die Akkorde angepasst, wenn römische Ziffern zur Anzeige verwendet werden.
            Für die Distant Hearing-Funktionen muss der Nutzer zunächst die zu analysierenden Gruppen benennen. Es können dann beliebig viele Stücke der Suchleiste einer Gruppe hinzugefügt werden. Nach der Kalkulation der Daten werden fünf Visualisierungsbereiche angezeigt:
            
               Akkordanalyse: Über gepaarte Histogramme werden die Verteilungen der Akkorde in den einzelnen Gruppen dargestellt (Abbildung 8). Neben den Akkordverteilungen werden auch Akkord-Grundton- und Tongeschlechts-Verteilungen der Akkorde angezeigt.
            
            
               
               Abbildung 8: Akkordanalyse – Verteilungen von Akkorden für zwei Gruppen
            
            
               Tonhöhenanalyse: Über gepaarte Histogramme werden die Verteilungen der einzelnen Töne sortiert nach Tonname und Oktave angezeigt.
               Tondaueranalyse: Über gepaarte Histogramme werden die Verteilungen der Tondauern unterteilt in Noten und Pausen angezeigt (Abbildung 9).
            
            
               
               Abbildung 9: Tondaueranalyse – Verteilungen von Tondauern für zwei Gruppen
            
            
               Tonartanalysen: Hier werden über gepaarte Histogramme die Verteilung der Tonarten angezeigt. Auch wird ein Liniengraph angezeigt, der pro Gruppe die Wahrscheinlichkeiten für die einzelnen Tonarten angibt (Abbildung 10).
            
            
               
               Abbildung 10: Tonartanalyse – Liniendiagramm für die Wahrscheinlichkeiten verschiedener Tonarten mehrerer Stücke
            
            
               Tonumfanganalyse: Es wird ein Reichweitendiagramm pro Gruppe angezeigt, welches den Tonumfang pro Stück in Form von horizontalen Balkendiagrammen anzeigt (Abbildung 11). Für die Gesamtgruppe wird die Menge und die Verteilung der genutzten Halbtonschritte auch noch in Form eines Boxplots angezeigt.
            
            
               
               Abbildung 11: Tonumfanganalyse – Reichweitendiagramm für 4 Stücke, die der Gruppe Beethoven hinzugefügt wurden
            
            Alle Graphen sind dabei interaktiv und bieten weiterführende Informationen an, wenn der Mauszeiger über Elemente bewegt wird. Die Diagramme können auch zusammen mit ihren Legenden heruntergeladen werden. Ebenso können die gesammelten Daten zur Weiterverwendung in einem JSON-Format heruntergeladen werden. An zahlreichen Stellen wurden Tutorials und Erklärungen eingebaut, um die Nutzung zu erleichtern.
         
         
            Ausblick
            Die momentane erste Version des Tools ist frei verfügbar und kann über 
                    GitHub heruntergeladen und genutzt werden. Des Weiteren ist ein erster vorläufiger Prototyp auch online verfügbar.
                
            Wir befinden uns am Ende des ersten Entwicklungszyklus und planen momentan die Evaluation des Tools gemäß dem UCD-Prozess. Des Weiteren explorieren wir weiter zusammen mit Musikwissenschaftlern, ob die gelieferten Funktionen den Analyseprozess unterstützen können und wie das Tool konkret in den Forschungsworkflow integriert werden kann. Im gleichen Schritt wollen wir auch erste forschungsrelevante Einsatzbeispiele diskutieren. Als ein Bereich für mögliche Analysen wurde von den Teilnehmern unserer Anforderungsanalyse vor allem der Vergleich von Genres, Komponisten und eigens erstellten Sammlungen bezüglich gängiger musikalischer Metriken genannt (Akkorde, Tonumfang etc.). Als eine komplexere Forschungsidee wurde die Untersuchung von Variationen diskutiert. 
                    La Folia, ein spanisches Motiv aus dem 16. Jahrhundert wurde von zahlreichen Komponisten als Grundlage für Variationen genutzt (Hudson, 1973). Durch die Nutzung eines geeigneten Korpus kann mit BeyondTheNotes untersucht werden, ob die Variationen dieses Motivs sich mehr nach Komponist, Zeitraum oder Ursprungsland unterscheiden. Ebenso wollen wir in den kommenden Iterationen durch die enge Zusammenarbeit mit Musikwissenschaftlern das Konzept und den tatsächlichen Nutzen des Distant Hearing kritisch reflektieren.
                
         
      
      
         
            
      https://opensheetmusicdisplay.org/
    
            
      https://www.zingchart.com/
    
            
      https://gionkunz.github.io/chartist-js/
    
            
      Online verfügbar unter: https://github.com/Maxikilliane/DH_MusicAnalysis (Eine Installationsanleitung findet man im Repository)
    
            
      Online verfügbar unter: https://beyondthenotes.herokuapp.com/ 
    
         
         
            
               Bibliographie
               
                  Abdallah, Samer / Benetos, Emmanouil / Gold, Nicolas / Hargreaves, Steven / Weyde, Tillman / Wolff, Daniel (2017): “The Digital Music Lab: A Big Data Infrastructure for Digital Musicology”, in: 
                        Journal on Computing and Cultural Heritage (JOCCH) 10(1).
                    
               
                  Burghardt, Manuel (2018): “Digital Humanities in
Der Musikwissenschaft – Computergestützte Erschließungsstrategien Und Analyseansätze Für Handschriftliche Liedblätter” in: 
                        Bibliothek Forschung Und Praxis 42(2): 324–32.
                    
               
                  Burghardt, Manuel / Lamm, Lukas (2017): “Entwicklung Eines Music Information Retrieval-Tools Zur Melodic Similarity-Analyse Deutschsprachiger Volkslieder” in: Eibl, Maximilian / Gaedke Martin (eds.): 
                        INFORMATIK 2017. Bonn: Gesellschaft für Informatik 87–99.
                    
               
                  Burghardt, Manuel / Wolff, Christian (2014): “Humanist-Computer Interaction: Herausforderungen für die Digital Humanities aus Perspektive der Medieninformatik” in: 
                        DHd Workshop: Informatik und die Digital Humanities.
               
               
                  Burghardt, Manuel / Lamm, Lukas / Lechler, David / Schneider, Matthias / Semmelmann, Tobias (2015): "MusicXML Analyzer. Ein Analysewerkzeug für die computergestützte Identifikation von Melodie-Patterns" in: 
                        Hildesheimer Evaluierungs- und Retrievalworkshop 2015: 29-42.
                    
               
                  Condit-Schultz, Nathaniel / Ju, Yaolong / Fujinaga, Ichiro (2018): “A Flexible Approach to Automated Harmonic Analysis: Multiple Annotations of Chorales by Bach and Prætorius” in: 
                        19th International Society for Music Information Retrieval Conference 66–73.
                    
               
                  Cook, Nicholas (2013): 
                        Beyond the score: Music as performance. Oxford University Press.
                    
               
                  Cuthbert, Michael Scott / Christopher, Ariza (2010): “Music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data” in: 
                        11th International Society for Music Information Retrieval Conference (ISMIR 2010) 637–642.
                    
               
                  Frieler, Klaus / Hoger, Frank / Pfleiderer, Martin / Dixon, Simon (2018): “Two Web Applications for Exploring Melodic Patterns in Jazz Solos” in: 
                        19th International Society for Music Information Retrieval Conference 777–83.
                    
               
                  Hudson, Richard (1973): “The Folia Melodies” in: Acta Musicologica 45 (1): 98–119.
                    
               
                  Huron, David (1994): 
                        UNIX Tools for Music Research: The Humdrum Toolkit. Reference manual http://www.humdrum.org/Humdrum/manual07.html [letzter Zugriff 21. September 2019].
                    
               
                  Huron, David. (2002): “Music Information Processing Using the Humdrum Toolkit: Concepts, Examples, and Lessons” in: 
                        Computer Music Journal 26 (2): 11–26. 
                    
               
                  Kornstädt, Andreas (1996): “SCORE-to-Humdrum: A Graphical Environment for Musicological Analysis” in: 
                        Computing in Musicology 10: 105–22.
                    
               
                  Moretti, Franco (2002): “Conjectures on World Literature” in: 
                        New Left Review Jan / Feb: 54–68.
                    
               
                  Nettheim, Nigel (1997): “A Bibliography of Statistical Applications in Musicology” in: 
                        Musicology Australia 20(1): 94–106.
                    
               
                  Taylor, Michael (1996): 
                        Humdrum Graphical User Interface. Belfast, Queen’s University.
                    
               
                  Vredenburg, Karel / Mao, Ji-Ye / Smith, Paul W. / Carey, Tom (2002): “A Survey of User-Centered Design Practice” in: 
                        Proceedings of the SIGCHI Conference on Human Factors in Computing Systems 471–478.
                    
            
         
      
   


11019	2020	
      
         
            Einleitung
            Das schriftliche Kulturgut des deutschsprachigen Raums aus dem 16.–18. Jahrhundert wird schon seit Jahrzehnten in den Verzeichnissen der im deutschen Sprachraum erschienenen Drucke (VD) zusammengetragen. Ein signifikanter Anteil der verzeichneten Titel wurde der Forschung bereits durch die Bereitstellung von Volldigitalisaten oder einzelnen Schlüsselseiten leichter zugänglich gemacht. Die Verfügbarmachung von Volltexten ist dagegen noch ein Desiderat der Forschung. Das DFG-Projekt OCR-D nimmt sich seit Oktober 2015 im Rahmen der Koordinierten Förderinitiative zur Weiterentwicklung von Verfahren für die Optical Character Recognition (OCR) dieser Aufgabe an, indem es eine modular aufgebaute Open Source-Software entwickelt, deren Werkzeuge alle für die Texterkennung nötigen Schritte abdecken sollen. Der modulare Ansatz ermöglicht es, die technischen Abläufe und Parameter der Texterkennung stets nachzuvollziehen und maßgeschneiderte Workflows zu definieren, die jeweils optimale Ergebnisse für spezifische Titel aus dem Zeitraum des 16. bis 19. Jahrhunderts liefern. Zudem werden Antworten auf die damit verbundenen konzeptionellen, informationswissenschaftlichen und organisatorischen Fragen gefunden.
            Künftig sollen mithilfe der OCR-D-Software Volltexte generiert werden, die zum einen von Forschenden zur Recherche verwendet werden können. Zum anderen könnten diese zum Ausgangspunkt für Studien im Bereich der Digital Humanities (DH) werden, wobei auch auf diese Texte die textkritische Methode anzuwenden ist. Gerade bei einer automatisierten Weiterverarbeitung der erzeugten Volltexte ist es für Forschende unerlässlich, die Genese der von ihnen verwendeten Daten kritisch zu hinterfragen. Nur so können Eigenheiten der Daten, die Resultat von zuvor genutzten “Spielräumen” sind, von DH-Forschenden erkannt und in ihrem Umgang mit der Datengrundlage berücksichtigt werden. Nicht nur diese interpretatorischen Spielräume sind zu betrachten, sondern auch, welche konkreten Implementierungen den DH die gewünschten “Spielräume“ für die Erkenntnisgenerierung geben. Im Folgenden wird in vier Thesen eine notwendige Begrenzung der Spielräume vorgenommen. Diese Begrenzung ergibt sich aus dem Vergleich mit anderen Projekten und der heute gängigen Praxis. Ziel ist es, den Forderungen der DH nach qualitativ hochwertigen Volltexten gerecht zu werden. 
         
         
            Im Rückblick
            Das Projekt hat sich in den vergangenen vier Jahren mit verschiedenen Themen auf der DHd zur Diskussion gestellt (Boenig et al 2016; Boenig et al 2018; Baierer et al 2019). Zu Beginn standen methodische Fragen, wie die Textqualität erhöht werden kann. Dabei wurden statistische Methoden vorgestellt, die auf Basis eines Vergleichs von mindestens zwei erstellten Textfassungen entwickelt wurden. Im Rahmen des Themas “Kritik der digitalen Vernunft” wurden die DH befragt, wie in den Geisteswissenschaften Ergebnisse ohne Ground Truth und Referenzdaten gewonnen bzw. verifiziert werden. Diesem Desiderat begegnete das Projekt OCR-D mit dem Vorschlag von Transkriptionsrichtlinien für die Erfassung von Ground Truth-Daten und in der Folge mit der Definition von spezifischen Metadaten. Bei dem 2019 veranstalteten Workshop konnten Wissenschaftler und Wissenschaftlerinnen sowie Interessierte Einblicke in den OCR-D-Workflow erhalten. An Beispielen konnten die Möglichkeiten der Software demonstriert und getestet werden. Die Diskussion, Hinweise und Fragen wurden soweit wie möglich in OCR-D umgesetzt. 
                
         
         
            Thesen
            Das Ziel der prototypischen Implementierung des OCR-D-Workflows und damit der Generierung von Forschungsdaten, die sich durch eine erkennbare XML-Strukturierung sowie eine hohe Zeichen- und Textqualität auszeichnen, wird im ersten Quartal 2020 erreicht werden. Dies stellt jedoch nicht das Ende des Weges dar, sondern eher den Beginn der nun folgenden Volltexttransformation. Letztlich besteht die Aufgabe darin, ca. 1 Mio. frühneuzeitliche Titel mit ca. 250 Mio. Seiten, die zum Teil bereits als Bilddigitalisate vorliegen, zu Volltextdigitalisaten zu transformieren.
            1. Die Volltexttransformation der Bestände stellt eine Herausforderung für Bibliotheken und Archive dar. Die vorhandenen institutionellen und interinstitutionellen Vorgehensweisen und Konventionen sind möglichst zentral aufeinander abzustimmen, damit die Aufgabe in absehbarer Zeit gelöst wird.
            
            Es gibt bereits einige Projekte, in denen (Teil-)Bestände und Sammlungen volltextdigitalisiert wurden. Deren Nutzen für die DH wird jedoch v.a. durch zwei Faktoren begrenzt: Zum einen weisen die erstellten Volltexte aufgrund fehlender Standards bzw. Konventionen im Bereich von Text- und Strukturerkennung eine große Bandbreite in der Transkription der Texte und der Benennung von Textstrukturen auf, die deren automatisierte Auswertung und Bearbeitung durch die DH erschweren. Zum anderen gibt es bislang keine zentrale Anlaufstelle, die die Bereitstellung und auch die Erstellung von Volltexten steuert. Dadurch sind die existierenden Volltexte sowohl für die Forschung, als auch für die volltextdigitalisierenden Einrichtungen weniger sichtbar, was die Gefahr aufwändiger und teurer Doppelarbeiten erhöht.
                
            2. Die Volltexttransformation auf Basis von Erkennungssoftware, die neuronale Netze nutzt, setzt Trainingsdaten voraus. Diese fundamental wichtigen Daten sind systematisch aus vorhandenen Ressourcen zu gewinnen und aktiv zu erweitern.
            Mit ihren Förderinitiativen von 2010 und 2013 hat die DFG die Bedeutung der Forschungsdaten und des zugehörigen Managements erkannt. Heute sollten Projekte von Beginn an mit entsprechenden Forschungsdatenmangementplänen aufgesetzt und die entstehenden Daten in den zuvor bereitgestellten Repositorien verwahrt werden. Gerade bei der automatisierten Texterfassung im Rahmen von Editionsprojekten werden in der Regel aber nur die abschließend bearbeiteten und korrigierten Daten veröffentlicht. Eine Nachnutzung dieser Daten ist in vielfacher Hinsicht nur begrenzt möglich. Dabei spielt nicht nur das Format der Daten, sondern auch die Methodik der Datenerfassung eine entscheidende Rolle. Für die Nachnutzung ist eine Transformation dieser Daten nötig, die entweder von den Nutzenden zu leisten ist, oder von den bestandsverwaltenden Einrichtungen angeboten werden könnte. Um eine solche Transformation zu gewährleisten, sind sowohl Richtlinien als auch entsprechende Metadaten zu etablieren, damit vergleichbare und konsistente Daten bereitgestellt werden können.
            
            3. Die Volltexttransformation wird für einen Teil der Dokumente ein Prozess sein, der sich über einen größeren Zeitraum wiederholt.
            Digitale Daten müssen beständig gepflegt und aktualisiert werden. Dies haben auch Bibliotheken als Herausforderung der digitalen Transformation ihrer Bestände erkannt (vgl. Kempf 2015: 277–278). Werden lernende Systeme für die Text- und Strukturerkennung genutzt, können diese in absehbaren Intervallen verbessert werden. Denn die Verbesserung bestehender Algorithmen sowie die Nutzung zusätzlicher oder verbesserter Trainingsdaten führt auch zu besseren Ergebnissen in der Text- und Strukturerkennung, wie sich beispielsweise im GoogleBooks-Projekt zeigt. Diese wiederkehrende Prozessierung muss konzeptionell berücksichtigt werden.
                
            4. Die Volltexttransformation muss in ihrer Qualität von den Nutzenden beurteilbar sein.
            Bibliotheken geben den Nutzenden mit ihrem Bestand und dessen Erschließung ein Qualitätsversprechen. Die Nutzenden können sich auf die vorhandenen Daten verlassen und sie z.B. in Bibliographien verwenden. Das Volltextangebot aus der automatischen Texterkennung kann dagegen häufig nur unpräzise als “schmutzige OCR” bezeichnet werden. Diese pauschale Angabe ermöglicht den DH keine verlässliche Qualitätseinschätzung und führt dazu, dass Volltextbestände oft a priori als minderwertig eingeschätzt werden. Daher besteht die Gefahr, dass projektintern eine erneute Volltextdigitalisierung durchgeführt wird, die nicht immer sinnvoll ist, da die Erkennung teilweise nur durch eine Korrektur verbessert werden könnte. Oder es könnten im umgekehrten Fall auf Grund einer ungenauen bzw. zu groben Einschätzung aufwendige Korrekturen vorgenommen werden. In beiden Fällen werden finanzielle und personelle Ressourcen verschwendet.
                
         
         
            Lösungen und Desiderate des OCR-D-Projekts
            
               Zu 1: Die bisherigen umfassenden Bilddigitalisierungsarbeiten im VD17 wurden über einen Masterplan gesteuert, um die große Anzahl an Titeln effizient, in nachnutzbarer Form verarbeiten zu können und Doppelarbeiten zu vermeiden. Ein ähnliches Vorgehen, bei dem die zu prozessierenden Titel an interessierte Einrichtungen verteilt werden, dürfte auch für die Volltexttransformation der VD zielführend sein. Die Voraussetzungen und Rahmenbedingungen für diese Arbeiten wurden von dem OCR-D-Koordinierungsprojekt um die Jahreswende 2019/2020 durch eine Umfrage mit den VD-Bibliotheken zusammengetragen. OCR-D wird die mehrjährige Projekterfahrung im Austausch mit den verschiedenen Stakeholdern nutzen, um die Nachnutzbarkeit von Daten und Abläufen zu verbessern, sowohl mit technischer Dokumentation und Best Practices, als auch als Katalysator für einen ergebnisorientierten, inklusiven Diskurs zur Etablierung von Standards.
            
               Zu 2: Für die Transkription von Texten gibt es unzählige Richtlinien, die von verschiedenen Fächern, Arbeitskreisen und Forschungsprojekten entsprechend ihrer jeweiligen Anforderungen aufgestellt und wiederum an die spezifischen Erfordernisse bestimmter Transkriptionsprojekte angepasst wurden. Bei diesen Gruppen ist zum einen ein Bewusstsein dafür zu schaffen, ihre Transkriptionen auch mit Blick auf deren Nachnutzbarkeit durch andere Projekte anzufertigen. Zum anderen sind interdisziplinär erarbeitete und gültige Transkriptionsrichtlinien ein großes Desiderat der Forschung. Erste Impulse hierfür könnten große Fördergeber wie bspw. die DFG geben, indem Praxisrichtlinien geschaffen werden, die von Antragstellern zu beachten sind. Das OCR-D-Projekt ist zudem darum bemüht, seine auf Grundlage des DTA-Basisformats erstellten Transkriptionsrichtlinien interdisziplinär zur Nutzung durch weitere Projekte zu kommunizieren.
            
               Zu 3: Modelltraining mit tesstrain und okralact
            Das Projekt ocropy, die Python-Implementierung von Tom Breuels OCRopus-Projekt, brachte neben Werkzeugen für die Text- und Strukturerkennung auch Werkzeuge für das Erstellen von Ground Truth und das Trainieren neuer Modelle mit sich. Mit diesen Werkzeugen und einigen Anpassungen lassen sich auch die auf ocropy basierenden Weiterentwicklungen Calamari und Kraken trainieren. Insbesondere für tesseract, die mit Abstand am meisten genutzte Open Source OCR, gab es bis 2018 kaum Dokumentation oder Tooling für das Training. Daher wurde im Rahmen von OCR-D 
ocrd-train entwickelt, eine Makefile-basierte Lösung zum Trainieren von Tesseracts LSTM-Engine, das inzwischen unter dem Namen 
tesstrain vom Tesseract-Entwicklerteam gepflegt und weiterentwickelt wird. Die Aufrufe zum Training von Texterkennungsmodellen und insbesondere das Inventar an freien Parametern sind allerdings in hohem Maße engine-spezifisch, keineswegs trivial und erfordern zur optimalen Feinadjustierung manuelle Intervention. Daher entwickelt OCR-D seit 2019 das Werkzeug okralact, das über ein komfortables Webinterface und ein skalierbares Backend ein Training aller relevanter Open Source OCR-Engines mit einem einheitlichen Interface ermöglichen wird.
                
            
               Zu 4: Nachkorrektur und Qualitätsanalyse
            Innerhalb des OCR-D-Projektes beschäftigen sich zwei Projekte mit der automatischen, bzw. semi-automatischen Nachkorrektur von OCR-Texten. Das Hauptproblem dabei ist es, historische Schreibweisen und Druckfehler von OCR-Fehlern zu unterscheiden. Für moderne Texte würde eine reine Rechtschreiberkennung genügen, wie sie in jedem Textverarbeitungsprogramm verfügbar ist. Die Projekte kooperieren und haben verschiedene Verfahren entwickelt, basierend auf einem Fehler-Profiler, neuronalen Netzen oder endlichen Automaten. Als trainierbare Algorithmen werden sie, analog zur Struktur- und Texterkennung, mit mehr und besseren Trainingsdaten bessere Ergebnisse liefern. Was "besser" bedeutet ist noch Gegenstand der Forschung. OCR-D bringt sich in die Entwicklung ein und unterstützt tatkräftig Projekte wie dinglehopper (ein Werkzeug zur Fehlervisualisierung). Gerade im Bereich der Ground-Truth-freien Evaluation von Text und der Qualitätsanalyse von Strukturdaten gibt es noch große Lücken im Software-Portfolio, die zu schließen sich OCR-D auch weiterhin befleißigen wird.
                
         
         
            Ausblick
            Ab der ersten Jahreshälfte 2020 werden die entwickelten Software-Komponenten im OCR-D-Workflow verankert sein. Damit tritt diese Software immer mehr aus dem Projektstadium heraus und wird in den produktiven Einsatz überführt. Um kontinuierlich gute Erkennungsergebnisse mit dem aus fast vier Jahrhunderten stammenden Material zu erhalten, sind Optimierungen notwendig. Dabei wird stets darauf abgezielt, Forschungsdaten aus den digitalen Beständen der Bibliotheken zu erzeugen und nicht unstrukturierte Textdaten. So wird die Volltexttransformation in einem umfassenden Maße Grundlagen für datenzentrierte Digital Humanities schaffen.
         
      
      
         
            
      Im Kontext von OCR bezeichnet 
      Ground Truth manuell korrigierte, fehlerfreie Transkriptionen. Diese werden zum einen für das Training von OCR-Engines, zum anderen für die Evaluation der OCR-Ergebnisse benötigt.
    
            
      Der Gedanke folgt der neunten Empfehlung (“Establish an ‘OCR Service Bureau’”) aus dem Report von Smith und Cordell (2018).
    
            
      Vgl. bspw. die folgenden Projekte, die sich auf unterschiedlich große (Teil-)Bestände beziehen: Helmstedter Drucke Online:
      http://www.hab.de/de/home/wissenschaft/forschungsprofil-und-projekte/helmstedter-drucke-online.html;
      Über 14.000 preußische Drucke des 17. Jahrhunderts online verfügbar:
      https://blog.sbb.berlin/ueber-14-000-preussische-drucke-des-17-jahrhunderts-online-verfuegbar/;
      Projekt Digi20
      https://digi20.digitale-sammlungen.de/de/fs1/about/static.html
            
            
      Nachdem im Jahr 2010 der Aufbau von Infrastrukturen für Forschungsdaten von der DFG ausgeschrieben worden war, wurde drei Jahre später das Förderprogramm „Informationsinfrastrukturen für Forschungsdaten“ eingerichtet. Vgl. DFG 2019: 7.
    
            
      Zur aktuellen Situation des Datenmanagements und der Rolle, die Bibliotheken in diesem Bereich einnehmen (könnten), vgl. Neuroth et al 2019.
    
            
      Die Notwendigkeit einheitlicher Richtlinien wird besonders an Projekten wie “Venice Time Machine” deutlich, dessen bereits vorhandenen 8 TB an Daten aufgrund fehlender einheitlicher Richtlinien und Vorgehensweisen bei der Erfassung der Metadaten für die Forschung vermutlich wertlos sind. Vgl. Castelvecchi 2019: 607.
    
            
      Kempf geht davon aus, dass mit OCR-Software nie völlig fehlerfreie Volltexte generiert werden können. Vgl. Kempf 2015: 274.
    
            
      Während die OCR-Ergebnisse im Rahmen des GoogleBooks-Projekts zunächst insgesamt unbefriedigend, für gebrochene Schriften vollkommen unbrauchbar waren, konnten ab dem Jahr 2008 einzelne Frakturtexte in ausreichender Qualität prozessiert werden. In den letzten Jahren konnte die Erkennungsrate noch deutlich gesteigert werden. Vgl. Wikisource: Google Book Search.
    
            
      Bspw. gibt Google die Fehlerquote im Google Books pauschal mit 1,37 % an (vgl. Kempf 2015: 272). Diese für die wissenschaftliche Nutzung hohe Fehlerrate unterscheidet sich, bedingt durch die Vielfalt an Typen und Layouts sowie den großen Publikationszeitraum der digitalisierten Bücher, von Text zu Text deutlich.
    
            
               https://github.com/tesseract-ocr/tesstrain
            
            
               https://github.com/OCR-D/okralact
            
            
               https://github.com/qurator-spk/dinglehopper
            
         
         
            
               Bibliographie
               
                  Baierer, Konstantin / Boenig, Matthias / Hartmann, Volker / Hermann, Elisa / Neudecker, Clemens (2019): 
	„Vom gedruckten Werk zu elektronischem Volltext als Forschungsgrundlage“ (Workshop)
	(, S. 58).
      
               
                  Boenig, Matthias / Würzner, Kay-Michael / Binder, Arne / Springmann, Uwe  (2016): 
	„Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts.“ 
	Vortrag auf der DHd 2016, 7.12.03.2016 in Leipzig ().
      
               
                  Boenig, Matthias / Federbusch, Maria / Herrmann, Elisa / Neudecker, Clemens / Würzner, Kay-Michael (2018):
	 „Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?“.
	Vortrag auf der DHd2018, 28.02.2018 in Köln
	().
      
               
                  Castelvecchi, Davide (2019): 
      “Venice ‘Time Machine’ Project Suspended amid Data Row. Disagreements between International Partners Leave Plans to Digitize the Italian City’s History in Limbo” in: 
      Nature 574: 607.
      
               
                  DFG  (2019): 
	„Weiterentwicklung des Förderprogramms ‚Informationsinfrastrukturen für Forschungsdaten‘"
                   [6.3.2019 / 26.9.2019].
      
               
                  Kempf, Klaus (2015): 
      „Data Curation oder (Retro-)Digitalisierung ist mehr als die Produktion von Daten“ in: 
      o-bib 4: 268–278.
      
               
                  Neuroth, Heike / Rothfritz, Laura / Petras, Vivien / Kindling, Maxi  (2019): “Digitales Datenmanagement als neue Aufgabe für wissenschaftliche Bibliotheken” in: Bibliothek. Forschung und Praxis 43: 421–431.
               
                  Smith, David A. / Cordell, Ryan  (2018): 
	“A Research Agenda for Historical and Multilingual Optical Character Recognition”
                   [9.12.2019].
      
               
                  Wikipedia, Die freie Enzyklopädie  (2019): 
	„Google Books“.
	 [16.6.2019 / 25.9.2019].
      
               
                  Wikisource  (2019): 
	“Google Book Search”
                   [22.8.2019 / 26.9.2019].
      
            
         
      
   


11024	2020	
      
         Die Web-Applikation 
Schmankerl Time Machine
            
             wurde im Rahmen des Hackathons für offene Kulturdaten, „Coding da Vinci Süd 2019“ (Bergmann, 2019), von einem interdisziplinären Team aus Informatikern, Statistikern und Geisteswissenschaftlern entwickelt (Deck, 2018). Das Projekt basiert auf den digitalisierten Speisekarten Münchner Restaurants, die die 
Monacensia der Stadtbibliothek München für den Hackathon zur Verfügung gestellt hatte. Am Ende der sechswöchigen Sprintphase konnte der Prototyp reüssieren und wurde von der Jury mit dem Preis in der Kategorie „Most Technical“ bedacht (Lehr, 2019). Seitdem lädt die 
Schmankerl Time Machine zu einem lukullischen Streifzug durch die traditionsreiche Münchner Wirtshausgeschichte der vergangenen 150 Jahre ein. Einen ähnlichen Weg schlägt die Plattform „What’s on the Menu?“ ein, die auf dem Speisekartenbestand der 
New York Public Library basiert und die überwiegend US-amerikanische Gastronomie zwischen 1851 und 2008 abbildet. Andere interessante Bestände harren dagegen noch ihrer Digitalisierung aus.
         
         Die Applikation besitzt bereits jetzt ein großes Potenzial für eine breite Öffentlichkeit (Guyton, 2019; Kotteder, 2019) und zeigt damit exemplarisch, wie die 
Digital Humanities über den wissenschaftlichen Raum hinaus zu einer Beschäftigung mit kulturgeschichtlichen Daten anregen können. Das einzureichende Poster möchte die Idee hinter der Applikation, ihre technische Umsetzung und Funktionalität gleichermaßen wie die Nachhaltigkeitsstrategie sowie künftige Entwicklungsmöglichkeiten präsentieren.

         
            Daten und Datenaufbereitung
            375 Speisekarten mit 1.020 Seiten aus den Jahren 1855 bis 2008 wurden inklusive 
  Metadaten durch die 
  Monacensia bereitgestellt. Sie entstammen 144 Münchner Gaststätten, Restaurants, Cafés, Bars, Festzelten und -hallen, die regional größtenteils in den Stadtbezirken Altstadt-Lehel und Ludwigvorstadt-Isarvorstadt zu verorten sind. Aufgrund unterschiedlicher Schriftarten wurde in 
  Transkribus ein komplementärer Ansatz mit 
  Handwritten Text Recognition (HTR) und 
  Optical Character Recognition (OCR) verfolgt. Anschließend erfolgte eine manuelle Fehlerüberprüfung. Zusätzlich wurde ein 
  Tagset entworfen, um die konsistente Annotation von Mengenangaben und Preisen, Gerichten und deren Zusammensetzung zu gewährleisten. Für die kollaborative Projektarbeit, insbesondere die Datenorganisation und -analyse, wurde die Lehr- und Forschungsinfrastruktur 
  Digital Humanities Virtual Laboratory (DHVLab) eingesetzt, die seit 2016 an der IT-Gruppe Geisteswissenschaften der Ludwig-Maximilians-Universität München entwickelt wird (Klinke, 2018: 29–32).
  
         
         
            Technische Umsetzung und Funktionalitäten
            Folgende Frage stand im Vordergrund: Wie kann die enorme Vielfalt der Speisekarten auch von einer technisch wenig versierten Zielgruppe auf möglichst unterschiedliche Art und Weise exploriert werden? Um dies zu erreichen, wurde eine interaktive, responsive Web-Applikation mit der Open-Source-Umgebung 
R und den auf 
R basierenden Paketen 
Shiny und 
Tidyverse entwickelt, die auf Clientseite ergänzt wird durch 
HTML5, 
JavaScript und das 
Frontend-CSS-Framework Bootstrap. Eine Lokalität kann entweder über ein 
Dropdown-Menü oder eine dynamische Karte (basierend auf 
Leaflet und 
LocationIQ) ausgewählt werden (Abbildung 1). Zu jeder Lokalität werden weiterführende Informationen angeboten. Sofern digital vorhanden, wird auf alte Ansichten der Restaurants aus dem Münchner Stadtarchiv verlinkt.

            
               
               Abbildung 1: Auswahl einer Lokalität über eine dynamische Karte. Bildnachweis: 
  Schmankerl Time Machine; lizenziert unter CC BY-SA 4.0.
  
            
            Jede zu einer Lokalität gehörende Speisekarte kann beliebig gezoomt und verschoben werden. Zudem ist jede Annotation, und damit auch jedes Gericht, direkt anwählbar (Abbildung 2). Besonders „exquisite“ Speisen werden algorithmisch über das 0,65-Quantil ausfindig gemacht – und sogar komplette Menüs zusammengestellt; wobei nicht nur die Präferenzen der jeweiligen Nutzerin oder des jeweiligen Nutzers berücksichtigt werden, sondern auch ihr oder sein Budget (Abbildung 3). Ein virtueller Warenkorb unterstützt die Exploration des Fundus weiterhin: Durch die Verknüpfung mit der Rezeptdatenbank des Webportals 
Chefkoch.de können ausgewählte Gerichte nachgekocht werden; Zutatenliste inklusive.

            
               
               Abbildung 2: Speisekarte der Augustiner Gaststätte von 1959. Bildnachweis: 
  Schmankerl Time Machine; lizenziert unter CC BY-SA 4.0.
  
            
            Neben diesem spielerischen Zugang zu den Speisekarten kann die 
Schmankerl Time Machine als Ausgangspunkt für wissenschaftliche und gesellschaftliche Fragestellungen dienen:

            
               In welchem Jahr findet sich erstmals ein bestimmtes Gericht? Wie stellt sich die Preisentwicklung dar?
               Welche Strategien verfolgten die Restaurants, um ihre Kunden zu einer profitablen Speisenauswahl zu animieren?
               Finden sich in der Beschreibung der Gerichte Hinweise auf ein sich veränderndes Ernährungsbewusstsein?
            
            
               
               Abbildung 3: Nutzerpräferenzen-basierte Menüempfehlung. Bildnachweis: 
  Schmankerl Time Machine; lizenziert unter CC BY-SA 4.0.
  
            
            Diese Beispiele zeigen, wie vielfältig sich die Beschäftigung mit den hier erstmals dargebotenen Speisekarten gestalten kann (Roth und Rauchhaus, 2018). Um einen möglichst niederschwelligen Einstieg zu gewährleisten, werden 
Jupyter Notebooks in 
Python 3 zur Verfügung gestellt, die die Daten importieren, bereinigen und exemplarische Statistiken beinhalten. Hierfür werden gängige Bibliotheken im Bereich 
Data Science verwendet (etwa 
pandas, 
NumPy und 
Matplotlib).

         
         
            Nachhaltigkeitskonzept
            Gemäß den 
  FAIR-Prinzipien
  (Findable, Accessible, Interoperable, Re-usable) wurde ein umfassendes Nachhaltigkeitskonzept verfolgt. Der Quelltext der Applikation sowie die Skripte sind auf 
  GitLab verfügbar.
  Die Abbildungen der Speisekarten sowie die im Projekt entstandenen
  Forschungsdaten stehen im Repositorium der
  Ludwig-Maximilians-Universität München
  (Open Data LMU) unter einer offenen Lizenz (CC BY-SA 4.0) dauerhaft und mittels 
  DOI eindeutig referenzierbar zur Nachnutzung bereit. Die Beschreibung des Projekts erfolgt im Metadatenschema 
  DataCite unter Verwendung eines Best-Practice-Guides, der eine standardisierte Anreicherung der Metadaten unterstützt. Dies ermöglicht die Einbindung der Projektdaten in übergeordnete Forschungsdateninfrastrukturen (z. B. 
  GeRDI) und damit ihre leichtere Auffindbarkeit.
  
         
         
            Ausblick
            Bei der 
                    Schmankerl Time Machine handelt es sich um einen fortgeschrittenen Prototyp. Um sein Potenzial sowohl für wissenschaftliche Fragestellungen als auch für eine interessierte Öffentlichkeit zu vergrößern, ist die Integration weiterer Speisekartensammlungen und damit eine wesentliche Erweiterung des Datenbestands vorgesehen. Damit einhergehend wird darauf abgezielt, die Annotation der Karten – unter Einbezug der „Crowd“ – fortzuführen und um weitere Analysekategorien zu erweitern. In Kooperation mit der 
                    Monacensia ist zu diesem Zweck auch ein 
                    Edithaton geplant, bei dem Studierende der Ludwig-Maximilians-Universität München u. a. praktische Kenntnisse im Umgang mit 
                    Transkribus erhalten.
                
            Ein Beispiel, welche Forschungsfragen dadurch eröffnet werden können, stellt das Projekt „Menu Journeys“ dar, das Studierende der 
                    Berkeley School of Information 2015 angestoßen haben. In interaktiven Grafiken wird auf Basis des Speisekartenbestands der 
                    New York Public Library anschaulich dargestellt, wie sich etwa der durchschnittliche Preis eines Gerichts über die Jahrzehnte hinweg und in Relation zur Inflationsrate entwickelt hat. Analysen dieser Art wären auch für die Münchner Gastronomiegeschichte begrüßenswert. Die hier vorgestellte Web-Applikation bietet einen Ausgangspunkt für künftige Entwicklungen in diese Richtung.
                
         
      
      
         
            
      https://dhvlab.gwi.uni-muenchen.de/schmankerltimemachine/ (27.09.2019).
    
            
      http://menus.nypl.org/ (27.09.2019).
    
            
      Größere Bestände mit geographischem Schwerpunkt auf München befinden sich im Münchner Stadtarchiv sowie der Stadtbibliothek München.
    
            
      https://gitlab.com/cds19-team/cds19 (27.09.2019).
    
            
      https://doi.org/10.5282/ubm/data.146 (27.09.2019).
    
            
      Der DataCite-Best-Practice-Guide wurde im Rahmen des Projekts „eHumanities – interdisziplinär“ in Kooperation mit dem Leibniz-Rechenzentrum der Bayerischen Akademie der Wissenschaften entwickelt: https://doi.org/10.5281/zenodo.3559800.
    
            
      Eine Mitarbeit am Projekt „cds19“ ist nach Registrierung möglich: https://transkribus.eu/r/read/projects/ (27.09.2019).
    
            
      http://people.ischool.berkeley.edu/~carlos/menujourneys/ (18.12.2019).

         
         
            
               Bibliographie
               
                   Bergmann, Claudia  (2019): „Kultur-Hackathon Coding da Vinci Süd. Sterbende Jesuiten, visualisierte Theaterdaten und Wirtshausgeschichte zum Nachkochen“, auf: 
      Wikimedia Blog vom 27.05.2019, URL: 
      https://blog.wikimedia.de/2019/05/27/kultur-hackathon-coding-da-vinci-sued-sterbende-jesuiten-visualisierte-theaterdaten-und-wirtshausgeschichte-zum-nachkochen/.
    
               
                   Deck, Klaus-Georg  (2018): „Digital Humanities. Eine Herausforderung an die Informatik und an die Geisteswissenschaften“, in: Huber, Martin / Krämer, Sybille (Hrsg.): 
      Wie Digitalität die Geisteswissenschaften verändert. Neue Forschungsgegenstände und Methoden. Sonderband der Zeitschrift für digitale Geisteswissenschaften 3. DOI: 
      10.17175/sb003_002.
    
               
                   Guyton, Patrick (2019): „Zeitreise durchs kulinarische München“, in: 
      Der Tagesspiegel  vom 26.07.2019, URL: 
      https://www.tagesspiegel.de/gesellschaft/panorama/historische-speisekarten-zeitreise-durchs-kulinarische-muenchen/24843382.html.
    
               
                   Klinke, Harald (2018): „Datenanalyse in der Digitalen Kunstgeschichte. Neue Methoden in Forschung und Lehre und der Einsatz des DHVLab in der Lehre“, in: Ders. (Hrsg.): 
      #DigiCampus. Digitale Forschung und Lehre in den Geisteswissenschaften. München: Universitätsbibliothek der Ludwig-Maximilians-Universität, 19–34, DOI: 
      https://doi.org/10.5282/ubm/epub.42415.
    
               
                   Kotteder, Franz (2019): „Hat’s geschmeckt?“, in: 
      Süddeutsche Zeitung  vom 09.07.2019, URL:
      http://sz.de/1.4516782.
    
               
                   Lehr, Andrea (2019): „Hochkarätig. CDV Süd
      punktet mit Projekten auf hohem Niveau“. 
      Pressemitteilung vom 21.05.2019, URL: 
      https://codingdavinci.de/news/2019/05/21/cdvs-preisverleihung_2.html.
    
               
                   Roth, Tobias / Rauchhaus, Moritz (Hrsg.)  (2018): Wohl bekam’s! In hundert Menus durch die Weltgeschichte. Berlin: Verlag Das Kulturelle Gedächtnis.

            
         
      
   


11027	2020	
      
         
            This paper started out as a report on the state of the art in text classification, but over time it became much more a reflection on the pitfalls in modeling genre using classification. The start of our research was motivated by developments in text classification: Recent years have seen new approaches like gradient boosting and deep neural networks. Our initial goal was to inform about these approaches, which are seldom used yet in the digital humanities. But this proved to be only a starting point for a deeper exploration of genre structures of our collection of dime novels (‘Heftromane’, ‘Groschenromane’).
            
         Most research on genre classification has been looking into what you could call ‘high level classes’ like newspaper genres (news, editorials etc.; e.g. Frank and Bouckaert, 2006) or web genres (blog, personal website etc.; e.g. Eissen and Stein, 2004). Under this perspective all texts we are looking at belong to one genre: the novel. The subgenres are types of love stories like the doctor novel (‚Arztroman‘) or the country novel (‚Heimatroman‘) and types of adventure novels, mainly distinguished by the setting: the war novel (‚Kriegsroman‘) or the science fiction novel. These novels are cheap (‘dime novels’) and published in a booklet format and are usually distributed via magazine kiosks and not book shops (Stockinger 2018). From the very beginning it was clear to us, that they don’t contain a random collection of each genre. On the contrary, the crime novels for example are just a small and very specific subsection of crime novels in general. But nevertheless we assumed that genre is the main aspect to group novels - for publishers and readers.
         Our dataset consists of 11,600 dime novels from 12 different genres (see Fig.1). The genre label come from the four publishers who divide the market among themselves. (Bastei, Martin Kelter, Pabel Moewig and Cora). The corpus has been documented in previous studies such as Jannidis et. al. (2019a) and Jannidis et. al (2019b).
         
            
               
               Figure 1: Novels per Genre
            
            
            
            We have employed three groups of methods: traditional feature-based classifiers (Group A), modern feature-based classifiers (Group B) and deep learning (Group C). While Group A and B are based on document-term-matrix (20,000 most-frequent-words, tf-idf-weighted, stopwords removed, dimensionality reduction with LSI to 1000 features) as input, Group C works with unprocessed text. Named entities are removed completely. Hyperparameter optimization was done by sampling from the space of values recommended by the documentation of the libraries and (Olson et al. 2017) using Optuna (Akiba et al. 2019): In table 1 we report the best performance. We evaluated the performance of the deep learning approaches in advance on a smaller dataset, so that later only the best architecture had to be extensively tested (table 1). To increase speed initialized with pre-trained (wikipedia.de+30.000 novels) fasttext embeddings (Bojanowski et al. 2016). As a compromise between performance and speed we used the BiRNN architecture for all following experiments. 
         
             
         
            Table 1: Prestudy of deep learning architectures (4 subgenres, 800 novels) 
            
               
               Fasttext
               Flair
               CNN
               CNN+BiRNN
               BiRNN
               HATTN
            
            
               f1-score
               .886
               .931
               .925
               .935
               .923
               .926
            
            
               Time per epoch (seconds)
               
               288
               210
               190
               90
               215
            
            
               Time to converge (minutes)
               3
               48
               28
               25
               6
               21
            
         
         
            Table 2: Results of subgenre classification
            
            
               
               Multi. NaiveBayes
               
               Logistic Regression
               SVM (svc)
               K-Nearest Neighbors
            
            
               f1-score
               .932
               0.940
               0.948
               
                  0.915
                    
            
            
               
               XGBoost
               LightGBM
               CatBoost
               BiGRU
            
            
               f1-score
               *
               .878
               *
               .907
            
         
         
            
As was to be expected from the experience of previous studies on genre classification, the results were initially very good
(Jannidis et. al. 2019a). They decreased slightly (~ 2 %) when we added novels from the publisher “CORA”. With this addition our collection contains almost all dime novels published in recent years. Table 2 shows the classification results for this collection.

         
            The decrease of our F1-score alone wasn‘t a great surprise, as the addition of new data is expected to increase diversity within groups and complicates classification. But two observations were irritating: First, we noticed classification results were improved when we included stopwords. Usually removing stopwords improves classification performance (Toman et al. 2006; Gonzales and Quaresma 2014). As most  stopwords are typical function words which are used in stylometric research, this indicated that authorship information was used in the classification. Secondly, we noticed strong fluctuations between cross-validation folds, which seemed to indicate a very uneven class distribution.
            
         
To understand the first phenomenon better, we plotted the distribution of the authors across the genres  (see Fig. 2): Many authors write exclusively within a genre. The greatest overlap can be found in the genres 
Love and Family. 

         
            
               
               Figure 2: Inter-genre authorship
            
         
         
            So, indeed, the authorship information could be used to identify the genre of text, but not in all genres equally.
            
         In order to gain an insight into the influence of genre and publisher on the text form, we use Ivis (Szubert 2019) for unsupervised dimensionality reduction. The coloring of the data points according to publisher (figure 3) and genre (figure 4) shows the strong influence of these variables on the texts. It is also clear that Cora Verlag allows less variance among genres and thus becomes the most discriminatory factor. Figure 5 shows a detail of the previous plot, but focuses on microstructures. Theses structures indicate, that on this level genre and publisher are not enough to explain the distribution and that something else – author or series – comes into play. 
         
            
               
                  
                  Figure 3: Ivis dimension reduction based on 20.000 mfw. Colors indicate genre.
               
            
         
         
            
               
               Figure 4: Ivis dimension reduction based on 20.000 mfw. Colors indicate publishers.
            
            
            
               
               Figure 5: Detail of fig. 4, showing genres from publishing houses Bastei and Cora.
            
         
         
            
Obviously the variables publisher, author and series are influencing the distributions of our features, the words of the texts, and the variable, we want to predict, the genre. In a classical scientific model publisher, author and series would be called 
confounding variables, but in text
classification the role of confounding has been mostly overlooked,
probably because usually the main goal is prediction and not causal
inference (Landeiro / Culotta 2016). Confounding variables are those factors in statistical  models, that lead to false correlations or bias. For example, in an experiment that investigates the relationship between age of a person and the tendency to drive fast, the car would be a confounding variable. Because older people have probably a higher income and own faster cars. Something very similar is happening here. In the next section, we will apply a standard measure to control for confounding variables (restriction), while keeping the machine learning setup.

         We created a restricted setup with a clear separation of authors, series and publishers between training and test data (i.e. authors which were in the training data, were not included in the test data etc.), and tested the subgenres in an one-vs-rest scheme. Figure 6 shows the results of this setup with at least 30 different combinations of test and training data per genre and a sample size of 200 novels split in half for training and test data.
         
            
               
               Figure 6: Binary Classification of Genres (Logistic Regression). Strict: No shared authors, series or publishers in training and testing dataset. Random: random sample to compare performances. Historic novels are excluded due to insufficient data.
                        
               
            
            
            
            The performance of the ‘strict setups’ is lower, sometimes even below 50%. This behavior is the result of negative examples in the training data being more similar to the positive examples of the test data, for example in love and doctoral novels of Cora.
         
         Though now we control for confounding variables, it is less clear, what it implies for the genre model. It is not unusual in genre theory to conceptualize genre in an ideal way as independent of other factors like authorship, time, publisher etc. which corresponds to the ‘strict’ version of splitting train and test data.  But at the same time, these factors may be so intertwined with the genre features, that it is difficult, if not impossible to separate them at all (Hempfer 2010). Under this perspective our attempt to construct a ‘clean’ and strict model of genre, independent of publishers etc. is a misguided attempt.
         Looking back we now see that we started our research with some assumptions which seem to be unfounded for this part of the literary market which is dominated by four publishers: We assumed that the genre labels have the same function as in the rest of the literary market. But the small number of publishers seems to create a different situation. We assume now, that at least in some instances combinations of genre names with publisher names (loves stories from Cora vs. love stories from Bastei-Lübbe) describe the clusters best. To start to evaluate this hypothesis, we trained the corpus on label combinations: 1) Genre and Publisher, e.g. ‘Cora-Love’, 2) Genre and Series. Figure 6 shows, that in many, but not all cases these combinations achieve very good results, which indicates that a clear-cut set of features corresponds these combinations. In some genres the same is true for series, for example doctoral or horror, while in others the series have no clear feature set (erotic, love).
         
            
               
               Figure 7: Classification of series and publisher within a genre (one-vs-rest scheme). Points of single observations are colored by the series publishers.
            
            
            
            To explore this in more detail, we looked at those genres where the values for a randomized and a strict setup in figure 4 are markedly different, which we see as a sign of a heterogeneity of the genre which was masked in the random setup. In this experiment we classified each of these six genres, using different setups for the separation of training and test set in order to control for the confounding variables. For the love novels this shows for example, that separating cleanly between the authors didn‘t reduce the performance, while doing the same with the series results in a drastic drop (figure 7), showing again, that in this genre, the genre cohesion is quite low, while the publishers and even the series have distinctive features.
         
         Following up the indications for confounding variables we uncovered the complicated situation of genre in this subfield of the literary market. We succeeded to explore some of its substructures which haven’t been described yet in literary studies, though it has been always one of its topics that this kind of literature is a commodity (Nusser 1973, Nusser 1991, Nutz 1999, Stockinger 2018). It is quite astonishing that almost every genre behaves differently, but this may be the result of a decades-old competition between this small number of publishers.  Probably the different structures correspond to different strategies of each publisher. Bastei-Lübbe for example seems to follow a strategy where each series has a distinct profile, while Cora is focussing more on the publisher name as brand (Fig. 7 and Fig. 4) - though the clustering may also be influenced by the fact that Cora translates many novels from English. It would be an interesting follow-up-project, to find out, whether the readers of these genres know about these structures and how this knowledge directs their choices. Last but not least, we think that the strategies to control for known and unknown confounding variables in text classification, especially if it is done to understand existing structures and not so much to predict really new data, needs to be explored in more detail.
         
            Acknowledgements
            We like to thank Reviewer 2 for providing detailed and very informative feedback especially on the relation between data leakage and confounding variables as well as on the evaluation of dimension reduction techniques.
         
      
      
         
            
Our code can be found: 
            
            
For Multinomial Naive Bayes, Logistic Regression, SVM and K-NN we used the library Scikit-Learn (Pedregosa 2011). For the new gradient boosting approaches we used XGBoost (Chen and Guestrin 2016), LightGBM (Ke et al. 2017), CatBoost (Dorogush et al. 2017).

         
         
            
               Bibliographie
               
                  
                  Akiba, Takuya / Shotaro Sano / Toshihiko Yanase / Takeru Ohta /  Masanori Koyama  (2019): „Optuna: A Next-generation Hyperparameter Optimization Framework“. 
CoRR abs/1907.10902. http://arxiv.org/abs/1907.10902.

               
                  Bojanowski, Piotr / Edouard Grave /  Armand Joulin / Tomas Mikolov (2016): „Enriching Word Vectors with Subword Information“. 
CoRR abs/1607.04606. http://arxiv.org/abs/1607.04606.

               
                  Chen, Tianqi /  Carlos Guestrin (2016): „XGBoost: A Scalable Tree Boosting System“.
CoRR abs/1603.02754. http://arxiv.org/abs/1603.02754.

               
                  Dorogush, Anna Veronika /  Andrey Gulin / Gleb Gusev / Nikita Kazeev / Liudmila Ostroumova Prokhorenkova / Aleksandr Vorobev  (2017): „Fighting biases with dynamic boosting“. 
CoRR abs/1706.09516. http://arxiv.org/abs/1706.09516.

               
                  Eissen, Sven Meyer zu  /  Stein, Benno (2008):  „Retrieval models for genre classification“.
Scandinavian Journal of Information Systems.
               
               
                  Frank, Eibe  /  Remco R. Bouckaert (2006): „Naive Bayes for Text Classification with Unbalanced Classes“. In 
Knowledge Discovery in Databases: PKDD 2006
, herausgegeben von Johannes Fürnkranz, Tobias Scheffer, und Myra Spiliopoulou, 503–510. Berlin, Heidelberg: Springer Berlin Heidelberg.

               
                  Goncales, T.  /  Quaresma, P. (2014): Evaluating
preprocessing techniques in a Text Classification problem. In 
Information Processing & Management 50. Jg., Nr. 1, S. 104-112.

               
                  Hempfer, Klaus W. (2010): „Zum begrifflichen Status der Gattungsbegriffe: Von ‘Klassen’ zu ‘Familienähnlichkeiten’ und ‘Prototypen’.“ 
Zeitschrift Für Französische Sprache Und Literatur
120, 1: 14-32. .

               
                  Jannidis, Fotis,  /  Konle, Leonard  /  Leinen, Peter 
(2019a): Thematic Complexity. DH 2019 in Utrecht. Conference Abstracts.

               
                  Jannidis, Fotis  /  Konle, Leonard  /  Leinen, Peter (2019b): Makroanalytische Untersuchung von Heftromanen. DHd 2019. Conference Abstracts.

               
                  Kaufman, Shachar  /  Saharon Rosset  /  Claudia Perlich  /  Stitelman (2012): „Leakage in Data Mining: Formulation, Detection, and Avoidance“. 
ACM Trans. Knowl. Discov. Data 6 (4): 15:1–15:21.
.

               
                  Ke, Guolin  /  Qi Meng  /  Thomas Finley  /  Taifeng
Wang  /  Wei Chen  /  Weidong Ma  /  Qiwei Ye  /  Tie-Yan Liu (2017): „LightGBM: A Highly Efficient Gradient Boosting Decision Tree“. In
Advances in Neural Information Processing Systems 30
, herausgegeben von I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, und R. Garnett, 3146–3154. Curran Associates, Inc. 
.

               
                  Landeiro, V.  /  Culotta, A. (2016):  „Robust Text Classification in the Presence of Confounding Bias“. 
AAAI'16: Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, 186–193.

               
                  Nusser, Peter (1973):
Romane für die Unterschicht. Groschenhefte und ihre
Leser. Stuttgart: Metzler.

               
                  Nusser, Peter (1991):
Trivialliteratur.
Stuttgart: Metzler.

               
                  Nutz, Walter (1999):
Trivialliteratur und Populärkultur.
Opladen: Wiesbaden.

               
                  Olson, Randal S. / William La Cava / Zairah Mustahsan
/ Akshay Varik /  Jason H. Moore (2017):
„Data-driven Advice for Applying Machine Learning to Bioinformatics Problems“.
arXiv:1708.05070 [cs, q-bio, stat], August. http://arxiv.org/abs/1708.05070.
               
               
                  Pedregosa, F. / G. Varoquaux / A. Gramfort / V. Michel
/ B. Thirion / O. Grisel / M. Blondel u. a. (2011): „Scikit-learn: Machine Learning in Python“. 
Journal of Machine Learning Research 12: 2825–2830.

               
                  Ribeiro, Marco Tulio/  Sameer Singh / Carlos Guestrin (2016): „’Why Should I Trust You?‘: Explaining the Predictions of Any Classifier“. 
arXiv:1602.04938 [cs, stat], Februar. .

               
                  Stockinger, Claudia (2018): „Das All dort draußen zeigt uns, wer wir sind. Die Leseuniversen der Groschenhefte“. In Steffen Martus / Carlos Spoerhase (ed.): 
Gelesene Literatur: Populäre Literatur im Medienwandel.
Text und Kritik. edition text und kritik.

               
                  Szubert, Benjamin, et al.  (2019): “Structure-Preserving Visualisation of High Dimensional Single-Cell Datasets.”
Scientific Reports, vol. 9, no. 1, June, p. 8914, doi:
10.1038/s41598-019-45301-0.
               
               
                  Toman, M. / Tesar, R. / Jezek, K. (2006): “Influence in Word Normalization on Text Classification."
Proceedings of InSciT 4 (2006): 354-358.

            
         
      
   


11043	2020	
      
         Kochtraditionen, ob regional oder international, sind eine der herausragendsten Elemente der europäischen Kultur und ein wichtiger Bestandteil der europäischen Identität. Aber die Fragen nach ihrem Ursprung, den Einflüssen und ihrer Entwicklung sind nach wie vor unklar. In den letzten Jahrzehnten kam die Forschung zu zwei wichtigen Schlussfolgerungen, welche mittlerweile die Forschungsbestrebungen prägen: Erstens gibt es keine quantitativen Studien über den Ursprung und die Entstehung der regionalen Küche in Europa; zweitens sind erst ab dem Mittelalter Handschriften mit Tausenden von Kochrezepten überliefert, was wohl als die Geburt der modernen europäischen Küche angesehen werden kann (vgl. Flandrin & Hyman 1988, Laurioux 2005). Auf dem europäischen Kontinent stellen frühneuhochdeutsche, mittelfranzösische und mittellateinische Rezepte den größten Teil der kulinarischen Überlieferung dar, die mehr als 80 Manuskripte und etwa 8000 Rezepte umfasst. Das Projekt “Cooking Recipes of the Middle Ages” bereitet die Kochrezeptüberlieferung von Frankreich und dem deutschsprachigen Raum auf, um ihre Herkunft, ihre Beziehung und ihre Migration innerhalb Europas zu analysieren (vgl. thematisch ähnliche Studien mit unterschiedlicher Fokussetzung: Hieatt 1995 mit linguistischem Fokus, Flandrin 1984, Adamson 1995, Carlin 1989, Adamson 2002, Karg 2007 allgemein und van Winter 1989, Hyman 2005, Laurioux 2002 mit Fokus auf spezielle Gerichte). Die Partner, das Zentrum für Informationsmodellierung der Universität Graz und das Laboratoire CESR (Centre d’Etudes Supérieures de la Renaissance) der Universität Tours werden diese mehrsprachigen Texte mit Hilfe von digitalen Standards aufarbeiten und sie mit aktuellen quantitativen und qualitativen Forschungsmethoden untersuchen. Der Vergleich der französischen und deutschen Ernährungsgeschichte eignet sich besonders gut für diese Aufgabe, da Frankreich einen kulturell prägenden Einfluss auf die deutschsprachigen Völker hatte. 
         Kochrezepte sind kulturell aufgeladene, flüchtige Texte; die erhaltenen Niederschriften stellen daher nur ein punktuelles Zeugnis, eine individuelle Zubereitungsweise eines Gerichts (Hieatt 1985, 26) in Raum und Zeit dar. Das inhaltliche Verständnis dieser Rezepte, ihre möglichen Entstehungs- und Anwendungskontexte und ihre Überlieferung ist zudem kein einfacher Prozess, denn die Fachbegriffe, Zutaten, Utensilien, Verfahren und Bräuche der damaligen Zeit, die in den Rezepten eher öfter implizit als direkt genannt werden, sind auch für Sprach- und Geschichtswissenschaftler, die sich auf das Thema spezialisiert haben, immer wieder eine Herausforderung. Ihre Entwicklung sollte daher am besten diachron und räumlich analysiert werden, was mittlerweile mit digitalen geisteswissenschaftlichen Methoden verhältnismäßig leicht möglich ist – vorausgesetzt, die entsprechenden Daten liegen vor. Im aktuellen Projekt werden die historischen Texte auf mehreren Ebenen erschlossen: So werden die Texte nicht nur neu transkribiert und philologisch-editorisch bearbeitet (vgl. Klug, Kranich 2015), sondern auch in unterschiedlichen Wissensgebieten semantisch angereichert. Das schafft jene Spielräume, die nötig sind, um Analysen wie maschinengestützte Abgleiche von Zutaten, Kochprozessen oder Kochutensilien, die Suche nach Rezepttradition und -migration oder standardisierte philologische Vergleiche, wie z.B. Kollationierungen, durchzuführen. Die Basis unserer Daten sind customized TEI/XML-Dokumente mit einem zusätzlichen adaptierten Schema, das die semantische Annotation von Kochrezepten im Allgemeinen erleichtern soll. 
         Die Rezeptüberlieferung – in Form einzelner Rezeptsammlungen – wird mithilfe einer , die sich an den Handschriftenbeschreibungen renommierter Bibliotheken orientiert und die in Kooperation mit der Abteilung für Sondersammlungen der Grazer Universitätsbibliothek entstanden ist, räumlich und zeitlich fixiert. Besonderes Augenmerk wird dabei auf Informationen zur Handschriftenentstehung (materiell wie auch inhaltlich) und auf die Schriftbeschreibung bzw. den Schreibhandbefund der Rezeptsammlungen gelegt, wobei erstere Informationen meist den Katalogen entstammen, letztere aus der Arbeit mit den Texten kommen. Die Grundlage des Projekts ist die einheitliche Erfassung der überlieferten Texte durch eine hyperdiplomatische Neutranskription der historischen Quellen: Als Arbeitsumgebung fungiert Transkribus, wo das Textlayout automatisch erkannt und die Texte manuell mittels proprietären Codierungen erfasst werden. Mithilfe mehrerer Transformationsschritte wird aus den Rohdaten die Basis für die elektronische Quellenabbildung erstellt, die sich an germanistisch-editorischen Richtlinien orientiert. Die Quellentexttranskription verzeichnet dabei nicht nur das unterschiedliche Schriftzeicheninventar, sondern auch alle textstrukturierenden Elemente. Das gesamte Zeicheninventar ist in einer nach den Richtlinien der TEI erstellten Zeichenbeschreibung erfasst. Die Beschreibung stützt sich dabei auf die theoretischen Ergebnisse zur Beschreibung von Zeichen aus dem DigiPal-Projekt und verwendet außerdem die Zeichenidentifikatoren der Medieval Unicode Font Initiative (vgl. Böhm, Klug 2020). Die so produzierten Daten sind nicht nur der Ausgangspunkt für die wissenschaftlichen Fragestellungen im Projekt, sondern bieten eine solide Grundlage für viele weitere Forschungsfragen aus Germanistik/Linguistik, Paläographie usw. Diese Erarbeitungsstufe wird nach editorischen Richtlinien normalisiert und gibt im Rahmen des Webauftritts in Form einer Text-Bild-Synopse detaillierten Einblick in die historische Quelle.
            
         Aus den Transkriptionsdaten wird außerdem eine auf Zeichenebene normalisierte, in Sinneinheiten untergliederte Textfassung geschaffen, in der die semantischen Informationen annotiert werden. Diese Sinneinheiten umfassen neben dem eigentlichen Rezept und Rezepttitel Eingangs- und Schlussformeln, Handlungsanweisungen, Küchen- und Serviertipps, Hinweise auf medizinische und religiöse Aspekte und selbstverständlich Zutaten, Gerichte und Küchenutensilien. 
         Den Kern der digitalen Forschungsstrategie bildet das Semantic Web beziehungsweise die Anbindung und Integration unserer Daten an Linked Open Data. Wir sind innerhalb der Geisteswissenschaften in der vorteilhaften Position, dass sich unser Projekt zu einem großen Teil mit Lebensmittelzutaten befasst, d.h. mit Tieren, Pflanzen und Pilzen. Das sind Forschungsgebiete, in denen sich bereits eine signifikante Menge an relevanten Ontologien etabliert haben und die gut an die Linked Open Data Cloud, einschließlich der allgemeinen Wissensdatenbanken Wikidata und DBPedia angeschlossen sind. Ontologien werden, wenn auch mit unterschiedlichen Schwerpunkten und Granularität der Daten, außerdem bereits erfolgreich für die Repräsentation von Kochrezepten (Hoehndorf & Lange 2018, Sam et al. 2014, Ribeiro et al. 2006) und in deren Analyse eingesetzt (Chow & Grüniger 2019, Jovanovic et al. 2015, Vadivu & Waheeta Hopper 2010). In unserem digitalen Forschungsansatz setzen wir zwar teilweise auch auf Textähnlichkeiten, der größte Teil unserer Analyse basiert jedoch auf dem Vorkommen von Zutaten, Kochprozessen bzw. Zubereitungshinweisen und Kochutensilien. Weitere Entitäten, die wir für die Analyse der Rezepte heranziehen sind Serviervorschläge sowie medizinische, kulturelle und religiöse Aspekte in den Texten. Die Annotation dieser Entitäten gestaltet sich schon aufgrund ihrer schieren Menge in historischen Kochrezepten als sehr komplex. Neben den Möglichkeiten zuvor unbekannte Beziehungen zwischen den Quellen und deren Entitäten zu finden, war das Arbeiten außerhalb von Sprachbarrieren ein Hauptargument für die Entscheidung, Semantic Web-Technologien in den Mittelpunkt des Projekts zu stellen. 
            
         Durch die Verwendung von 
                Konzepten, im Sinne einer Idee oder eines mentalen Bildes und nicht eines 
                Begriffes, versuchen wir, historische und sprachliche Grenzen zu überwinden. Ein konkretes Beispiel für diese Diskrepanz zwischen Begriff und mentaler Vorstellung liefert uns die österreichische / süddeutsche Variante für Kartoffel: "Erdapfel" ("erdaphel" im Frühneuhochdeutschen) wird etwa in einem Manuskript aus der Zeit um 1488 erwähnt, lange bevor die Kartoffel von Südamerika nach Europa importiert wurde, was uns zeigt, dass das Konzept von "Erdapfel" ein anderes gewesen sein muss (wahrscheinlich jede Art von Rübe) als das heutige Konzept des Erdapfels. Wie oben bereits erwähnt, war es also nötig einen Workflow zu finden, der nicht nur die philologische, sondern auch die semantische Komplexität der Rezepte widerspiegelt. Während die phrasenartigen Informationseinheiten manuell annotiert werden, erfolgt die Annotation auf Wortebene semiautomatisch, indem die Texte mithilfe von XSLT- und Python-Skripten und individuellen Vokabularien, vorgehalten als CSV Dateien, angereichert werden, die alle darauf ausgerichtet sind, die Varianz der historischen Sprachstufen auszugleichen. Für die frühneuhochdeutschen Texte stand uns aus einem früheren Projekt eine Liste mittelalterlicher Pflanzennamen und ihrer Übersetzungen in modernes Englisch und Deutsch sowie ihrer mittelalterlichen Variantendiktionen zur Verfügung. Dies gab uns die Möglichkeit, mit Hilfe der von OpenRefine bereitgestellten Reconciliation Service API, einen teilautomatisierten Prozess zur Annotation von Wikidata-Konzepten zu starten. Die daraus resultierenden Daten bildeten den Grundstock für die zuvor genannten Vokabularien. Ähnliche Listen wurden von den Projektpartnern in Frankreich erstellt, die mithilfe des von den französischen Kollegen entwickelten Tools “Heterotoki” in einem kollaborativen Arbeitsschritt konsolidiert werden können. Sobald jeder Begriff mit einem Konzept verbunden ist, werden diese Konzepte verwendet, um die Zutaten innerhalb der eigentlichen Rezepttexte in den TEI-Dokumenten anzureichern. Ein entscheidender Faktor dieses semiautomatischen Prozesses bleibt jedoch die menschliche Interpretation der angereicherten Einheiten und die Entscheidung für ein konkretes bereits bestehendes Konzept bzw. die Erstellung eines neuen Konzepts in Wikidata.
            
         Wir befinden uns derzeit mitten in dieser semantischen Annotationsphase. Ist diese abgeschlossen, bieten sich mannigfaltige Analysemethoden an. Sobald die Einheiten der einzelnen Rezepte mit Konzepten ausgestattet sind, kann die Analyse des Projekts übereinstimmende oder abweichende Essgewohnheiten, Textmigration sowie den Einfluss der Nachbarländer auf die jeweilige Küche aufzeigen. Die Implementierung von Ontologien aus den Naturwissenschaften wie FoodOn oder SNOMED ermöglicht es uns, Verbindungen von historischen Essgewohnheiten zu modernen Konzepten von Lebensmitteln herzustellen und neues Wissen für den Bereich der Ernährungsgeschichte zu generieren. Die Ontologiedaten werden zusammen mit den Entitäten in einem Triplestore gespeichert und können mit Hilfe von SPARQL Queries befragt werden. Die Ergebnisse dienen als Grundlage für eine räumliche und zeitliche Visualisierung der Daten.
            
          Die Speicherung, Analyse und Dissemination der Projektdaten erfolgt über das vom Zentrum für Informationsmodellierung in Graz entwickelte Repository GAMS (Geisteswissenschaftliches Asset Management). Innerhalb dieser auf Langzeitarchivierung ausgerichteten Infrastruktur wird auf den Triplestore “Blazegraph” über einen Webservice zur Speicherung und Abfrage von RDF-Triples zugegriffen.
            
      
      
         
            
               https://transkribus.eu/Transkribus/
            
            
      Describing Handwriting I-VII; http://www.digipal.eu/blog
            
            
               https://folk.uib.no/hnooh/mufi/
            
            
      Für eine Übersicht an Ontologien in diesen Bereichen siehe: 
      http://www.ontobee.org/, 
      http://aims.fao.org/, 
      https://ndb.nal.usda.gov/ndb/, 
      https://agclass.nal.usda.gov/about.shtml, 
      http://zbw.eu/stw/version/latest/thsys/70498/about.de.html.
      
	Alle sind an die Linked Open Data Cloud angeschlossen indem eine oder mehrere Serialisierungen in OWL und/oder RDF(S) vorliegen. 
      
            
            
               http://medieval-plants.org
            
            
               https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation-Service-API
            
            
               https://github.com/ponchio/heterotoki
            
            
               http://foodon.org/
            
            
               https://browser.ihtsdotools.org/
            
            
               https://gams.uni-graz.at
            
            
               https://www.blazegraph.com/
            
         
         
            
               Bibliographie
               
                  Adamson, M. W. (Ed.)  (1995): Food in the
      Middle Ages. A Book of Essays. New York, London:
      Garland. Adamson, M. W. (Ed.). (2002). Regional Cuisines of
      Medieval Europe: A Book of Essays. New York, London: Routledge.
      
               
                  Amoia, M. / Martínez, J.M.M.  (2019): SaCoCo Diachronic Corpus [WWW Document]. URL 
	http://fedora.clarin-d.uni-saarland.de/sacoco/ (accessed 1.7.20).
      
               
                  Böhm, A.  /  Klug, H.: Quellenorientierte
      Aufbereitung historischer Texte im Rahmen digitaler Editionen:
      Das Problem der Transkription in mediävistischen
      Editionsprojekten. In: [Titel steht noch nicht fest] Hrsg. von
      Ingrid Bennewitz und Martin Fischer (= Bamberger
      interdisziplinäre Mittelalterstudien.) [in Vorbereitung]
      
               
                  Carlin, M. /  Rosenthal, J. T. (Eds.)
      (1998): Food and Eating in Medieval Europe. London: Hambledon
      Press.
      
               
                  Chow, A. E. / Ninger, M. G.  (o. J.): 
	Multimodal Event Recognition with an Ontology For Cooking Recipes. 12.
      
               
                  Dooley, D. M. / Griffiths, E. J. / Gosal, G. S. / Buttigieg, P. L. / Hoehndorf, R. / Lange, M. C.,  / …  / Hsiao,
	W. W. L.  (2018): FoodOn: A harmonized food ontology to increase global food traceability, quality control and data integration. 
	Npj Science of Food, 2(1), 23. 
	https://doi.org/10.1038/s41538-018-0032-6
               
               
                  Flandrin, J.-L. (1984): «Internationalisme,
      nationalisme et régionalisme dans la cuisine des XIVe et XVe
      siècles: le témoignage des livres de cuisine». In Manger et
      boire au Moyen âge. Actes du Colloque de Nice (15-17 octobre
      1982). (pt. 2, p. 75-91). Paris.
      
               
                  Flandrin, J.-L.  /  Hyman, P. (1988):
      “Regional tastes and cuisines: Problems, documents, and
      discourses on food in Southern France in the 16th and 17th
      centuries”. Food and Foodways 1-3, p. 221-251.
      
               
                  Gloning, T.,  (2000): Monumenta Culinaria et Diaetetica Historica. Corpus of culinary & dietetic texts of Europe from the Middle Ages to 1800. Corpus älterer deutscher Kochbücher und Ernährungslehren [WWW Document]. URL 
	http://www.staff.uni-giessen.de/gloning/kobu.htm (accessed 1.7.20).
      
               
                  Hammad, R. /  Hassouna, M.  (2011): 
	Multi-Language Semantic Search Engine. 6.
      
               
                  Hieatt, C.  (1995): Sorting through the
	Titles of Medieval Dishes: What Is, or Is Not, a “Blanc
	manger”. In M. W. Adamson (Ed.), Food in the Middle Ages. A
	Book of Essays. (pp. 25-43). New York, London: Garland.
      
               
                  Hyman, P.  /  M.  (2005): «Les associations de saveurs dans les livres de cuisine français du XVIe siècle». In 
	Le Désir et le Goût. Une autre histoire (XIIIe-XVIIIe siècles). Actes du colloque international à la mémoire de Jean-Louis Flandrin (Saint-Denis, septembre 2003). Dir. Odile Redon, Line Sallman et Sylvie Steinberg. (p. 135-150). Saint-Denis: Presses Universitaires de Vincennes.
      
               
                  Karg, S. (Ed.) (2007): Medieval Food
      Traditions in Northern Europe. Copenhagen: National Museum of
      Denmark.
      
               
                  Klug, H. W. /  Kranich, K.  (2015): “Das Edieren von handschriftlichen Kochrezepttexten am Weg ins digitale Zeitalter. Zur Neuedition des Tegernseer Wirtschaftsbuches.” In T. Bein (Ed.), Vom Nutzen der Editionen. Zur Bedeutung moderner Editorik für die Erforschung von Literatur- und Kulturgeschichte. (pp. 121-137). Berlin, Boston: De Gruyter.
               
                  Lamé, M. /  Pittet, P. (2018):
      Heterotoki: Non-st / ructured and heterogeneous terminology alignment for Digital Humanities data producers. 12.
      
               
                  Laurioux, B.  (2005): «Les voyageurs et la gastronomie en Europe à la fin du Moyen âge ». In Le Désir et le Goût. Une autre histoire (XIIIe-XVIIIe siècles), Actes du colloque international à la mémoire de Jean-Louis Flandrin (Saint-Denis, septembre 2003). Dir. Odile Redon, Line Sallman et Sylvie Steinberg. (p. 99-117). Saint-Denis, Presses Universitaires de Vincennes.
               
                  Ribeiro, R. / Batista, F. / Pardal, J. P.  / Mamede, N. J. /  Pinto, H. S.  (2006): Cooking an Ontology. In J. Euzenat & J. Domingue (Hrsg.), 
	Artificial Intelligence: Methodology, Systems, and Applications (S. 213–221). Springer Berlin Heidelberg.
      
               
                  Sam, M. / Krisnadhi, A. A. / Wang, C. / Gallagher, J. /  Hitzler, P. (2014): 
	An Ontology Design Pattern for Cooking Recipes – Classroom Created.
      
               
                  Vadivu, G. / Hopper, S. W.  (2010): Semantic Linking and Querying of Natural Food, Chemicals and Diseases. 
	International Journal of Computer Applications, 
	11(4), 35–38. 
	https://doi.org/10.5120/1567-2093
               
               
                  van Winter, J. M. (1989):
	“Kochen und Essen im Mittelalter.” In B. Herrmann
	(Ed.). Mensch und Umwelt im
	Mittelalter. (pp. 88-100). Frankfurt am Main: Fischer
	Taschenbuch Verl.
      
            
         
      
   


11044	2020	
      
         
            Opaque – digitale Arbeitsumgebung für die Humanities
            Im Rahmen der DHd 2020 Spielräume möchten wir unsere in aktiver Entwicklung befindliche Webanwendung Opaque vorstellen. Anspruch ist es, Opaque als Arbeitsumgebung für DH-Projekte zu etablieren. Die Entwicklung der Webanwendung, deren Funktionen sukzessive erweitert werden sollen, wird im Rahmen des DFG-geförderten Sonderforschungsbereichs (SFB) 1288 "Praktiken des Vergleichens" im Teilprojekt (TP) INF "Dateninfrastruktur und Digital Humanities" durchgeführt.
            Das TP INF betreut das Forschungsdatenmanagement des SFB und unterstützt dessen Wissenschaftler*innen darüber hinaus bei der Planung, Konzeptionierung und Durchführung von Forschungsprojekten unter Zuhilfenahme digitaler Methoden. Diese beiden Bereiche sollen in Opaque synthetisiert werden. Aufbauend auf den Erfahrungen der Kooperationen entwickeln wir Opaque zur Bündelung und Automatisierung der erprobten Workflows und Best Practices. Eine besondere Schwierigkeit ist hierbei die Heterogenität und Komplexität von Forschungsdaten in den Geisteswissenschaften. Um dieser Schwierigkeit zu begegnen orientiert sich unsere Etablierung von Best Practices an den verschiedenen Stadien des Data Life Cycle, bestehend aus Planung/Beratung, Sammlung, Datenorganisation, Datenanalyse, Dissemination und Nachnutzung, und hat zum Ziel, für alle diese Stadien Best Practices zu entwickeln oder implementieren und so den Forscher*innen verfügbar zu machen. Die einzelnen in Opaque verfügbaren Funktionen werden durch etablierte Open Source-Lösungen realisiert, die durch die modulare Konstruktion der Webanwendung nicht nur gut erweitert sondern auch beständig auf dem neuesten Stand gehalten werden können, sowie reproduzierbare Routinen gewährleisten. Der Fokus auf Nachnutzung bestehender Software ermöglicht es uns, ein breites Spektrum an Funktionalitäten in Opaque zu integrieren.
            
               Opaque: Die Webanwendung
               Opaque bündelt verschiedene Werkzeuge und Services, die Geisteswissenschaftler*innen Methoden der DH an die Hand geben und somit deren verschiedene individuelle Forschungsprozesse unterstützen können. Mittels Opaque können Forschende digitalisiert vorliegende Quellen einer 
Optical Character Recognition (OCR) unterziehen. Die daraus resultierenden Textdateien können anschließend als Datengrundlage zum 
Natural Language Processing (NLP) weiterverwendet werden. Die Texte werden hierbei automatisiert verschiedenen linguistischen Annotationen unterzogen. Die via NLP prozessierten Daten können in der Webanwendung anschließend als Corpora zusammengefasst und mittels eines 
Information Retrieval System durch komplexe Suchanfragen analysiert werden. Der Funktionsumfang der Webanwendung wird zudem anhand der Bedarfe der Forschenden sukzessive erweitert.

               Die Funktionsschwerpunkte von Opaque unterscheiden sich von anderen deutschen DH-Softwareentwicklungen. Hervorzuheben sind 
TextGrid, 
FuD und 
CQPweb, die einen ähnlichen Anspruch als virtuelle Forschungsumgebung verfolgen. Im Unterschied zu Opaque legen 
TextGrid und 
FuD ihre Schwerpunkte auf händische Datenaufbereitung und nachhaltige Speicherung via integrierter Publikationsplattformen, wohingegen 
CQPweb ein Werkzeug zur
Korpusanalyse darstellt, dessen Query
Processor in Opaque übernommen wurde. Opaque
soll demgegenüber keine Publikationsplattform
integrieren, sondern eine automatisierte
Aufbereitung und Informationsanreicherung von
Forschungsdaten mit anschließender Analyse
ermöglichen. Die aufbereiteten Daten und
Analyseergebnisse können mittels
Exportfunktionen anhand gängiger Standards in
offene Dateiformate exportiert und
anschließend auf eigens gewählten
Publikationsplattformen veröffentlicht
werden. Die bereits in Opaque integrierten und
beständig auf dem neuesten Stand gehaltenen
Funktionen im Bereich des NLP und der OCR
grenzen die Plattform von den genannten
bestehenden Lösungen ab.
                  , 
                  
               
               Da Opaque plattformunabhängig konzipiert ist, können die verschiedenen Funktionen von den Wissenschaftler*innen auf beliebigen Endgeräten ohne vorangehende Einrichtung genutzt werden. Alle Funktionen wie z.B. OCR werden innerhalb der Cloud-Infrastruktur ausgeführt, so dass Nutzer*innen selbst keine leistungsfähigen Endgeräte benötigen.
            
            
               Nutzerorientiertes Design
               Die in Opaque implementierten Funktionen und Workflows orientieren sich an den aus unserer Zusammenarbeit im SFB hervorgegangenen Erfahrungen, etablierter Best Practices sowie Vorgaben und Standards des Forschungsdatenmanagements.
               Dies führt nicht nur zu besseren Ergebnissen für die Forscher*innen, sondern auch zu einer besseren Datenorganisation mittels anerkannter Standards.
               Durch eine Gegenüberstellung soll auf dem Poster anhand der verschiedenen Stadien des Data Life Cycle veranschaulicht werden, wie sich Arbeitsprozesse und -schritte durch die Einführung von Opaque verändert haben. Prägnante Beispiele für diese Gegenüberstellung sind Datensammlung und Datenanalyse. Mit Hilfe der Webanwendung können Forscher*innen eigene Quellen und Texte einem OCR-Prozess unterziehen und die Ergebnisse zeitnah selbstständig hinsichtlich der Güte der Texterkennung evaluieren. Diese Automatisierung der Prozesse in Verbindung mit der intuitiven Bedienoberfläche tragen zu einer erhöhten Autonomie der Forschenden bei. Gleichzeitig macht die Echtzeitverfolgung der Jobstatus die Prozessabläufe transparent und nachvollziehbar. Gespräche, die vorher technischer und organisatorischer Natur waren, können nun gezielter für inhaltliche Diskussionen und Planung der Forschung genutzt werden.
               Bezüglich der Qualität der Eingabedateien (z.B. Scans) offerieren
wir Hinweise zur bestmöglichen Digitalisierung von Ausgangsmaterialien
und orientieren uns an gängigen Standards zur Speicherung und
Veröffentlichung von Forschungsdaten (z.B. FAIR), um deren Nachnutzung
zu gewährleisten. Dies schließt neben den Forschungsdaten auch die
Nachhaltung und Bereitstellung von für den Forschungsprozess genutzter
Software in den jeweils genutzten Versionen mit ein, um die
Reproduzierbarkeit von Forschungsergebnissen sicherzustellen.
                      
            
            
               Implementierung
               Die Umsetzung beruht auf 
                        Free Open Source Software und Python. Auf dem Poster werden die Vorteile von Linux Containern in einem skalierbaren Docker-Rechencluster, wie z.B. eine einfache Verwaltung verschiedener Softwareversionen – insbesondere wichtig um Forschungsdaten reproduzieren zu können –, vorgestellt und die einzelnen im Folgenden aufgeführten Module der Plattform näher beleuchtet.
                    
               
                  
                     Webanwendung: Die Webanwendung dient als Schnittstelle zwischen Nutzer*innen und Recheninfrastruktur. Hier können Datenaufbereitungen in Form von Jobs gestartet und in Echtzeit verfolgt werden, dabei werden die Jobs automatisch auf das zugrundeliegende Rechencluster verteilt. Das Webinterface bietet außerdem die Möglichkeit über ein 
                            Information Retrieval System Auswertungen durchzuführen.
                        
                  
                     Daemon: Agiert im Hintergrund, um die von den Nutzer*innen durch die Webanwendung abgesetzten Befehle und Services umzusetzen bzw. zu verwalten.
                        
                  
                     Datenbank: Die Datenbank speichert alle Metadaten, die während der Nutzung der Webanwendung anfallen. Als Datenbanksystem wird 
                            PostgreSQL benutzt.
                        
                  
                     Netzwerkspeicher: Speichert die von den Nutzer*innen hochgeladenen Dateien sowie die daraus generierten Resultate. Die Netzwerkspeicherlösung garantiert den Servern des Cloud-Rechenclusters gleichermaßen Zugriff auf die zu bearbeitenden Dateien.
                        
                  
                     Services: OCR und NLP-Dienste werden mittels der state of the art Software 
                            Tesseract OCR und 
                            spaCy realisiert. Die Korpusanalyse erfolgt durch eine Anbindung an den 
                            CQP query processor der IMS Open Corpus Workbench. Jede Ausführung eines Dienstes ist mit einem Job assoziiert, der in einem eigens dafür erstellten Container bearbeitet wird.
                        
               
               Ein zusätzliches Hands-On von Opaque soll zu einem Erfahrungsaustausch einladen.
            
         
      
      
         
            Eintrag des offiziellen DARIAH-Wikis schildert, dass gängige Funktionen wie ein Lemmatisierer nicht mehr nachinstalliert werden können.
            Der Abschlussbericht des Projekts TextGrid aus dem Jahr 2012 schildert die Implementierung einer OCR Funktion mittels OCRopus, welche in den aktuellen Versionen nicht mehr zu finden ist.
         
      
   


11059	2020	
      
         Die kulinarische Tradition ist eine der prägendsten Elemente der europäischen Kultur und sie stellt einen großen Teil der nationalen Identitäten dar. In den letzten Jahrzehnten kam die Forschung zu zwei wichtigen Schlussfolgerungen in Bezug auf dieses Thema: Erstens, es gibt keine quantitativen Studien über die Herkunft und die Bildung von regionalen Küchen in Europa. Zweitens, im Mittelalter entstehen wesentliche Quellen: Manuskripte mit tausenden von Kochrezepten. Damit kann das Mittelalter als die Wiege der modernen europäischen Küche angesehen werden. Auf dem europäischen Kontinent bilden lateinische, mittelfranzösische und frühneuhochdeutsche Rezepte den Großteil der kulinarischen Überlieferung.
         Das vorliegende internationale Projekt (ANR-17-CE27-0019-01, fwf I 3614) zielt darauf ab, die interkulturelle Forschung der mittelalterlichen Kochrezepte und deren Wechselbeziehung mithilfe eines interdisziplinären Ansatzes zu verwirklichen. Das Projekt nimmt die Kochrezeptüberlieferung von Frankreich und den deutschsprachigen Ländern als Korpus – dieses umfasst mehr als 80 Manuskripte und an die 8000 Rezepte – und untersucht sie in Hinblick auf ihre Entstehung, ihre Beziehung untereinander und ihre Migration durch Europa. Der Vergleich der französischen und deutschen Kulinargeschichte eignet sich besonders für diese Aufgabe, da Frankreich seit jeher einen kulturell prägenden Einfluss auf deutschsprachigen Völker hatte!
         Die Partner, das Zentrum für Informationsmodellierung der Universität Graz und das Laboratoire CESR (Centre d’Etudes Supérieures de la Renaissance) der Universität Tours werden diese mehrsprachigen Texte nach modernen Standards aufarbeiten und sie mit aktuellen quantitativen und qualitativen Forschungsmethoden untersuchen. Für eine computergestützte Analyse werden die Rezeptsammlungen und die darin enthaltenen Texte und deren Metadaten in TEI/XML (Digitale Transkription und Edition) modelliert und mit Semantic Web Technologien analysiert (Digitale Annotation und Datenvisualisierung). Die Daten werden einer Langzeitarchivierungsinfrastruktur (GAMS, Zentrum für Informationsmodellierung Graz) zugeführt, in der sie weiter erforscht werden können. Alle Rezepte werden mithilfe von Vokabularien für Zutaten, Kochprozesse und Kochutensilien sowie kulturhistorisch relevanten Metadaten (z. B. in Bezug auf religiöse, kulturelle oder medizinische Aspekte) angereichert. Aufgrund dieser Informationen wird das Projekt über die Sprachgrenzen hinweg konkurrierende oder abweichende Essgewohnheiten, Textmigration sowie den gegenseitigen Einfluss der Nachbarländer auf ihre jeweilige Küche zu Tage fördern. Für die Analyse der deutschsprachigen Texte werden außerdem NLP-Methoden für historische Sprachstufen herangezogen, um Textverwandtschaften innerhalb dieser Überlieferung untersuchen zu können. Die Forschungsdaten und die Auswertungsergebnisse werden die Grundlage für eine räumliche und zeitliche Visualisierung und statistische Auswertung bilden, die neue Ansätze zur Interpretation des historischen und kulturellen Vermögens fördern wird.
         Die im Projekt erarbeiteten Workflows und Daten werden ganz im Sinne des Open Science Gedanken und den FAIR-Prinzipien für die Nachnutzung zur Verfügung gestellt:
         Der Transkriptionsworkflow und die Transkriptionsprinzipien (Theorie und Praxis, in Kooperation mit KONDE) können zur Gänze nachgenutzt werden. Da die Manuskripte mit Transkribus
                 transkribiert wurden, steht ein trainiertes HTR-Modell zur Verfügung mit dem eine automatische Handschriftenerkennung von ähnlichen Texten denkbar ist. Das Annotationsvokabular (Zutaten, Speisen, Küchengeräte, Zubereitungsweisen) wird samt der zugewiesenen semantischen Wikidata-Konzepte zur Verfügung gestellt und stellt somit eine essentielle Basis für die Forschung im Bereich Kulinarhistorik dar. Die Konzepte in Wikidata werden falls vorhanden kontrolliert und gegebenenfalls mit weiteren Daten (wie etwa Links zu relevanten Ontologien wie FoodO oder SNOMED) von unserer Seite angereichert. Noch nicht vorhandene Konzepte werden von uns neu erstellt und mit allen nötigen Daten (Statements) versehen. Die Nutzung von Wikidata verfolgt neben praktischen Überlegungen hauptsächlich das Ziel, die im Projekt gewonnenen Daten auf einfache Art und Weise für die Community bereitzustellen und eine weitere Bearbeitung dieser Daten zu ermöglichen. Überdies hinaus werden von uns auch die Annotationsskripte (Python und XSLT) für die Übertragung der Annotationsvokabularien nach TEI/XML zum Download angeboten. 
            
         Die überlieferten Texte werden durch eine hyperdiplomatische Neutranskription der historischen Quellen einheitlich erfasst und stehen als TEI/XML ebenfalls zur weiteren Nutzung zur Verfügung. Die Quellentexttranskription verzeichnet dabei nicht nur das unterschiedliche Schriftzeicheninventar, sondern auch alle textstrukturierenden Elemente. Das gesamte Zeicheninventar ist in einer nach den Richtlinien der TEI erstellten Zeichenbeschreibung erfasst. Die Beschreibung stützt sich dabei auf die theoretischen Ergebnisse zur Beschreibung von Zeichen aus dem DigiPal-Projekt und verwendet außerdem die Zeichenidentifikatoren der Medieval Unicode Font Initiative (vgl. Böhm, Klug 2020). Die so produzierten Daten sind nicht nur der Ausgangspunkt für die wissenschaftlichen Fragestellungen im Projekt, sondern bieten eine solide Grundlage für viele weitere Forschungsfragen aus Germanistik/Linguistik, Paläographie usw. Die Textdaten werden für eine Nutzung durch NLP Tools auch als Plaintext angeboten und die Handschriftenabbildungen sind je nach Nutzungsbedingungen der Bibliotheken frei verfügbar. 
            
         Darüber hinaus wird aus dem CoReMA-Projekt heraus ein Modell für die Integration weiterer Texte in die Forschungsumgebung bereitgestellt. Das Projekt soll fachliche Impulse für alle betroffenen Disziplinen der mittelalterlichen und frühneuzeitlichen Geschichte, Kulinargeschichte, Digitale Edition und Digital Humanities liefern.
      
      
         
            
               http://www.digitale-edition.at/
            
            
               https://transkribus.eu/Transkribus/
            
            
               https://www.wikidata.org
            
            
               http://foodon.org/
            
            
               https://browser.ihtsdotools.org/
            
            
      Describing Handwriting I-VII; http://www.digipal.eu/blog
            
            
               https://folk.uib.no/hnooh/mufi/
            
         
         
            
               Bibliographie
               
                  Adamson, M. W. (Ed.)  (1995): Food in the Middle Ages. A Book of Essays. New York, London: Garland. Adamson, M. W. (Ed.). (2002). Regional Cuisines of Medieval Europe: A Book of Essays. New York, London: Routledge.
      
               
                  Amoia, M., Martínez, J.M.M.  (2019): SaCoCo Diachronic Corpus [WWW Document]. URL 
	http://fedora.clarin-d.uni-saarland.de/sacoco/
	(accessed 1.7.20).
      
               
                  Böhm, A. & Klug, H.: Quellenorientierte Aufbereitung historischer Texte im Rahmen digitaler Editionen: Das Problem der Transkription in mediävistischen Editionsprojekten. In: [Titel steht noch nicht fest] Hrsg. von Ingrid Bennewitz und Martin Fischer (= Bamberger interdisziplinäre Mittelalterstudien.) [in Vorbereitung]
      
               
                  Carlin, M., & Rosenthal,
	J. T. (Eds.).  (1998): Food and Eating in Medieval Europe. London: Hambledon Press.
      
               
                  Flandrin, J.-L.  (1984): «Internationalisme, nationalisme et régionalisme dans la cuisine des XIVe et XVe siècles: le témoignage des livres de cuisine». In Manger et boire au Moyen âge. Actes du Colloque de Nice (15-17 octobre 1982). (pt. 2, p. 75-91). Paris.
      
               
                  Flandrin, J.-L. & Hyman, P.  (1988): “Regional tastes and cuisines: Problems, documents, and discourses on food in Southern France in the 16th and 17th centuries”. Food and Foodways 1-3, p. 221-251.
      
               
                  Gloning, T.,  (2000): Monumenta Culinaria et Diaetetica Historica. Corpus of culinary & dietetic texts of Europe from the Middle Ages to 1800. Corpus älterer deutscher Kochbücher und Ernährungslehren [WWW Document]. URL http://www.staff.uni-giessen.de/gloning/kobu.htm
	(accessed 1.7.20).
      
               
                  Hieatt, C.  (1995): Sorting through the Titles of Medieval Dishes: What Is, or Is Not, a “Blanc manger”. In M. W. Adamson (Ed.), Food in the Middle Ages. A Book of Essays. (pp. 25-43). New York, London: Garland.
      
               
                  Hyman, P. & M.  (2005). «Les associations de saveurs dans les livres de cuisine français du XVIe siècle». In 
	Le Désir et le Goût. Une autre histoire (XIIIe-XVIIIe siècles). Actes du colloque international à la mémoire de Jean-Louis Flandrin
	(Saint-Denis, septembre 2003). Dir. Odile Redon, Line Sallman et Sylvie Steinberg. (p. 135-150). Saint-Denis: Presses Universitaires de Vincennes.
      
               
                  Karg, S. (Ed.).  (2007): Medieval Food Traditions in Northern Europe. Copenhagen: National Museum of Denmark.
      
               
                  Klug, H. W., & Kranich, K. (2015): “Das Edieren von handschriftlichen Kochrezepttexten am Weg ins digitale Zeitalter. Zur Neuedition des Tegernseer Wirtschaftsbuches.” In T. Bein (Ed.), Vom Nutzen der Editionen. Zur Bedeutung moderner Editorik für die Erforschung von Literatur- und Kulturgeschichte. (pp. 121-137). Berlin, Boston: De Gruyter.
      
               
                  Laurioux, B. (2005): «Les voyageurs et la gastronomie en Europe à la fin du Moyen âge ». In Le Désir et le Goût. Une autre histoire (XIIIe-XVIIIe siècles), Actes du colloque international à la mémoire de Jean-Louis Flandrin (Saint-Denis, septembre 2003). Dir. Odile Redon, Line Sallman et Sylvie Steinberg. (p. 99-117). Saint-Denis, Presses Universitaires de Vincennes.
      
               
                  van Winter, J. M.  (1989). “Kochen und Essen im Mittelalter.” In
      B. Herrmann (Ed.). Mensch und Umwelt im
      Mittelalter. (pp. 88-100). Frankfurt am Main: Fischer
      Taschenbuch Verl.
      
            
         
      
   


11068	2020	
      
         
            Einleitung
            Die Idee des Distant Reading (Moretti, 2002) ist davon geprägt, durch den Einsatz von Methoden der computergestützten Textanalyse und Textvisualisierung große Mengen an Literatur zu explorieren, um Einsichten zu gewinnen, die mit herkömmlichen Methoden nicht möglich sind. Der Einsatz von Distant Reading wird dabei mittlerweile auch außerhalb der Literaturwissenschaften untersucht wie z.B. in den Religionswissenschaften (Pfahler et al., 2018). Im folgenden Beitrag wird ein Projekt vorgestellt, in dem der Einsatz und Nutzen von Distant Reading in ersten Analysen auf einer größeren Menge deutschsprachiger Songtexte exploriert wird. Ziel des Projekts ist es, mittels Distant Reading Unterschiede in gängigen Genres populärer Musik herauszukristallisieren. 
         
         
            Verwandte Arbeiten 
            Im Bereich des Text Mining wird die
Analyse von Songtexten vor allem im Kontext von Retrieval- und
Recommender-Aufgaben betrieben. Ziel ist meist die automatische
Klassifikation und Vorhersage verschiedener Kategorien, z.B. dem Genre
(Fell & Sporleder, 2014; De Sousa et al., 2016). Außerhalb dieses
Arbeitsgebiets findet man in Bereichen der Kultur- und
Literaturwissenschaften sowie der Psychologie Studien mit Songtexten
als Untersuchungsgegenstand (Cole, 1971; Kuhn,
1999). Forschungsinteressen umfassen dabei Analysen spezifischen
Musikern
(Beatles, West & Martindale, 1996; Whissel, 1996, 
Bob Dylan, Whissel, 2008; Körner, 2012), Epochen (Pettijohn & Sacco, 2009), Emotionen (Napier & Shamir, 2018) oder Erfolg (Riedemann, 2012). Im Bereich der computergestützten Korpus-Analyse findet man vereinzelt Projekte für den englischsprachigen Bereich. Dabei werden beispielsweise quantitative und qualitative Methoden verknüpft, um Stil und historische Eigenheiten zu analysieren (Werner, 2012), Annotations- und Akquisemöglichkeiten von Korpora exploriert (Kreyer & Mukherjee, 2009) oder N-Gramme untersucht (Nishina, 2017). Die Analyse von deutschsprachigen Texten ist jedoch bislang selten und findet vor allem im Bereich von regionalem Rap statt (Hess-Lüttich, 2009) sowie eher qualitativ und hermeneutisch (Stiegler, 2009).
                
         
         
            Korpus-Erstellung
            Als Plattform für die Akquise der Songtexte wurde 
LyricWiki
                gewählt. Ausgehend von aktuellen Umfragen zu den populärsten Genres in Deutschland werden die folgenden vier Genres betrachtet: 
Pop, Rock, Schlager und
 Rap/Hip Hop. Für die Auswahl der Songs wurden manuell durch Analyse der deutschen Charts seit den 60er Jahren eine angemessene Anzahl der wichtigsten deutschsprachigen Genre-Vertreter aufgestellt. Dieser Schritt ist (auch) subjektiv geprägt, der Fokus auf berühmte und „typische“ Vertreter der einzelnen Genres erlaubt jedoch trotzdem erste Analysen. Kritisch sei jedoch anzumerken, dass die Grenzen der Genres für einzelne Interpreten und Songs nicht immer eindeutig sind, insbesondere was Rock, Pop und Schlager betrifft. Wir haben versucht, für das vorliegende Korpus eine Auswahl mit möglichst eindeutigen Zuordnungen zu treffen.
            
            Für jeden gewählten Interpreten wurden über ein Skript alle Songtexte mit Metadaten von LyricWiki akquiriert. Die Akquise des Korpus wurde mittels eines frei verfügbaren angepassten ruby-Skripts durchgeführt.
                
            Abbildung 1 illustriert Eckdaten zum Gesamtkorpus und den Künstlern. In der Spalte „Bekannte Vertreter“ werden einige Künstler beispielhaft angegeben.
            
               
               Abbildung 1: Korpus-Zusammensetzung
            
            Abbildung 2 zeigt die Songverteilung im zeitlichen Verlauf und Genre-Kontext auf.
            
               
               Abbildung 2: Genre und zeitlicher Verlauf des Korpus
            
            Im Bereich des Preprocessing wurden Stoppwörter entfernt und alle Wörter zu Normalisierungszwecken in Kleinschreibung gebracht. 
         
         
            Methoden und Ergebnisse
            Für die allgemeine Textanalyse und das Topic Modeling wurden alle Analysen mittels 
R und unterschiedlichen Bibliotheken wie dem 
NLP
               - und 
topicmodels-package durchgeführt. Die Sentiment Analysis wurde mit 
Python und 
SentiWS (Remus et al., 2010) implementiert.

            
               Allgemeine Textanalyse
               Die Repetition von besonders bedeutenden Wörtern ist ein gängiges Stilmittel bei der Gestaltung von Songtexten. Aus diesem Grund betrachten wir die Analyse der häufigsten Wörter von Songtexten als besonders aufschlussreich. Die folgenden Bilder (Abbildung 3-6) illustrieren die 10 häufigsten Wörter (Most Frequently Used Words; MFWs) der einzelnen Genres.
               
                  
                  Abbildung 3: MFWs für Pop
               
               
                  
                  Abbildung 4: MFWs für Rap
               
               
                  
                  Abbildung 5: MFWs für Rock
               
               
                  
                  Abbildung 6: MFWs für Schlager
               
               Man erkennt, dass es drei Wörter gibt, die in allen vier Genres gleichmäßig stark vertreten sind: „Welt“, „Leben“ und „Zeit“. Diese Konzepte sind demnach konsistenter Inhalt deutschsprachiger Liedtexte unabhängig vom Genre. Die größte Differenzierung zeigen die Genres Rap, in dem Terme der Umgangs- und Jugendsprache enthalten sind, aber auch thematische Schwerpunkte deutlich werden („Geld“) sowie das Genre Schlager, das vor allem von emotionalen Termen wie „Liebe“, „Herz“ oder „Glück“ dominiert wird.
            
            
               Sentiment Analysis
               
                  Sentiment Analysis ist die Methodik zur computergestützten Analyse von Sentiments in Texten, also ob und in welchem Ausmaß Wörter eines Textes eher positiv oder negativ konnotiert ist (Liu, 2016). In den Digital Humanities werden häufig lexikonbasierte Methoden zur Bestimmung von Sentiment-Werten eingesetzt (Mohammad, 2011; Nalisnick & Baird, 2013). Dabei wird durch Summenbildung von Sentiment-Werten von Wörtern die Gesamtpolarität einer Texteinheit ermittelt. Wir verwenden dabei das etablierte Sentiment-Lexikon 
                        SentiWS (Remus et al., 2010). Abbildung 7 illustriert einige Ergebnisse:
                    
               
                  
                  Abbildung 7: Ergebnisse – Sentiment Analysis
               
               Man erkennt, dass für alle 4 Genres insbesondere Varianten von Liebe einen erheblichen Beitrag zur positivien Polarität leisten. Rap grenzt sich deutlich mit für das Genre typischen Themen ab, ausgedrückt durch Wörter wie „reich“ und mit Slang („hart“, „alter“). Alle Genres weisen insgesamt auf eine negative Polarität hin. Entgegen der naiven Intuition sind die Genres „Rap“ und „Rock“ dabei noch am positivsten (gemessen an den normalisierten Werten) bewertet. Erste Analysen machen jedoch auch Probleme der lexikonbasierten Sentiment-Analyse deutlich. Die Wörter „wein“ (weinen) und „feuer“ (das Feuer) sind in SentiWS als negativ markiert, haben aber in unseren Texten oft eher positive Konnotationen. Bei dem Wort „wein“ dann, wenn dieses durch die Normalisierung von „der Wein“ hergeleitet wird. In zukünftigen Arbeiten wollen wir mit einem domänenspezifischen Lexikon arbeiten, das für die jeweilige Anwendungsdomäne optimiert ist.
            
            
               Topic Modeling
               Topic Modeling ist eine Methode, um den Anteil verschiedener Themen in Dokumenten zu analysieren. Ein Thema ist dabei ein selbst definiertes Label für eine Liste von Wörtern, die besonders häufig zusammen auftreten. Als Algorithmus wurde Latent Dirichlet Allocation (LDA) gewählt (Blei et al., 2003). Das Topic Modeling wurde separat für die einzelnen Genres durchgeführt, um Unterschiede und Gemeinsamkeiten zu untersuchen. Wir sind momentan noch am Anfang der Analyse der einzelnen Topics, aber neben Differenzen werden auch Topics gefunden, die ähnliche Konzepte widerspiegeln. Folgende Visualisierungen geben die Wortlisten wider, die wir jeweils als das Topic „Liebe“ in den einzelnen Genres benannt haben. Die Wortgröße gibt die Häufigkeit des Wortes im jeweiligen Sub-Korpus wider (Abbildung 8).
               
                  
                  Abbildung 8: Wortlisten für das Topic „Liebe“
               
               Auffällig ist, dass insbesondere bei Rap familiäre Begriffe wie „Mama“, „Vater“ oder auch „Bruder“ Bestandteil des Topics sind, was traditionellerweise ein häufiger Schwerpunkt im Rap-Genre ist.
            
         
         
            Ausblick
            In unseren zukünftigen Arbeiten wollen wir insbesondere das Korpus systematisch vergrößern und verbessern. Momentane Probleme sind z.B. die Ungleichverteilung in der Menge bezüglich der Genres aber auch ein Fokus auf eher aktuelle Künstler. Wenngleich wir schon erste Eigenheiten der Genres feststellen konnten, wollen wir Methoden wie Sentiment Analysis und Topic Modeling noch weiter explorieren, indem wir beispielsweise die Varianz der Sentiments untersuchen. Des Weiteren wollen wir unsere Arbeit aber auch auf andere Textanalyse-Möglichkeiten wie Kollokationsprofile von Keywords, Named Entity Recognition und Stilometrie ausweiten. Durch die Zusammenarbeit mit Musik- und Literaturwissenschaftlern wollen wir in Zukunft auch explorieren, welche weiteren Forschungsfragen mit Hilfe größerer Korpora und Distant Reading-Methoden beantwortet werden können.
         
      
      
         
            
      https://lyrics.fandom.com/wiki/LyricWiki
    
            
      https://de.statista.com/statistik/daten/studie/171224/umfrage/beliebteste-musikrichtungen/
    
            
      Das Korpus kann auf Anfrage per Mail erhalten werden.
    
            
      https://gist.github.com/siavashs/3556469
    
            
      https://cran.r-project.org/web/packages/NLP/index.html
    
            
      https://cran.r-project.org/web/packages/topicmodels/index.html
    
         
         
            
               Bibliographie
               
                  Blei, David M. / Andrew, Y. Ng / Michael, I. Jordan (2003): "Latent dirichlet allocation", in 
                        Journal of machine Learning research 3: 993-1022.
                    
               
                  Cole, Richard R. (1971): "Top songs in the sixties: A content analysis of popular lyrics", in: 
                        American Behavioral Scientist 14 (3): 389-400.
                    
               
                  De Sousa, Jefferson Martins / Eanes Torres, Pereira / Luciana Ribeiro, Veloso (2016): "A robust music genre classification approach for global and regional music datasets evaluation", in:
                         IEEE International Conference on Digital Signal Processing (DSP).
               
               
                  Fell, Michael / Caroline Sporleder (2014): "Lyrics-based analysis and classification of music", in: 
                        Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics.
               
               
                  Hess-Lüttich, Ernest WB. (2009): "Rap-Rhetorik. Eine semiolinguistische Analyse schweizerischer rap-lyrics", in: 
                        Ars Semeiotica 32.
               
               
                  Körner, Stefan (2012): "Bob, Pop, Bibel-die Spuren der Bibel in den Songtexten Bob Dylans." 
                        Pastoraltheologie 101 (12): 503-521.
                    
               
                  Kreyer, Rolf / Joybrato Mukherjee (2007): "The style of pop song lyrics: A corpus-linguistic pilot study." in: 
                        Anglia-Zeitschrift für englische Philologie 125 (1): 31-58.
                    
               
                  Kuhn, Elisabeth D. (1999): "‘I just want to make love to you’–Seductive strategies in blues lyrics", in: 
                        Journal of pragmatics 31 (4): 525-534.
                    
               
                  Liu, Bing (2016): 
                        Sentiment analysis: Mining opinions, sentiments, and emotions. New York: Cambridge University Press.
                    
               
                  Mohammad, Saif (2011): "From once upon a time to happily ever after: Tracking emotions in novels and fairy tales.", in: 
                        Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities 105-114.
                    
               
                  Moretti, Franco (2002): “Conjectures on World Literature” in: 
                        New Left Review Jan / Feb: 54–68.
                    
               
                  Napier, Kathleen / Lior, Shamir (2018): "Quantitative Sentiment Analysis of Lyrics in Popular Music", in: 
                        Journal of Popular Music Studies 30 (4): 161-176.
                    
               
                  Nalisnick, Eric T. / Baird, Henry S. (2013): "Character-to-character sentiment analysis in shakespeare’s plays.“, in
                        : Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics 479–483.
                    
               
                  Nishina, Yasunori (2017): "A study of pop songs based on the billboard corpus." in: 
                        Int. J. Lang. Linguist 4 (2): 125-134.
                    
               
                  Pettijohn, Terry F. / Donald F. Sacco Jr. (2009): "The language of lyrics: An analysis of popular Billboard songs across conditions of social and economic threat", in: 
                        Journal of Language and Social Psychology 28(3): 297-311.
                    
               
                  Pfahler, Lukas / Elwert, Frederik / Tabti, Samira / Morik, Katharina / Krech, Volker (2018): "Versuche zum distant reading religiöser Online-Foren“: in 
                        Book of Abstracts, DHd 2018.
               
               
                  Remus, Robert / Quasthoff, Uwe / Gerhard, Heyer (2010): "SentiWS-A Publicly Available German-language Resource for Sentiment Analysis.", in: 
                        LREC: 1168-1171.
                    
               
                  Riedemann, Frank (2012): "Computergestützte Analyse und Hit-Songwriting.", in: 
                        Black box pop. Analysen populärer Musik: 43-56.
                    
               
                  Stiegler, Christian (2009): 
                        Nur ein Wort. Dissertation, Universität Wien.
                    
               
                  Werner, Valentin (2012): “Love is all around: A corpus-based study of pop lyrics.” in: 
                        Corpora 7 (1): 19–50.
                    
               
                  West, Alan / Colin Martindale (1996): "Creative trends in the content of Beatles lyrics" 
                        Popular Music & Society 20 (4): 103-125.
                    
               
                  Whissell, Cynthia (1996): "Traditional and emotional stylometric analysis of the songs of Beatles Paul McCartney and John Lennon", in: 
                        Computers and the Humanities 30 (3): 257-265.
                    
               
                  Whissell, Cynthia (2008): "Emotional fluctuations in Bob Dylan's lyrics measured by the Dictionary of Affect accompany events and phases in his life", in
                        : Psychological reports 102 (2): 469-483.
                    
            
         
      
   


11076	2020	
      
         
             Prosopographical Interoperability – State of the Art
            In einem wegweisenden Artikel zu Prosopographie aus dem Jahr 1971 schreibt Lawrence Stone (Stone, 1971) “The method employed is to establish a universe to be studied, and then to ask a set of uniform questions...”. Dieses 
    Erstellen des Universums ist einer der Reize der Prosopographie, stellt das Feld aber auch vor besondere Herausforderungen. Ein 
    Universum besteht gemeinhin aus Millionen von Objekten und noch mehr Relationen zwischen diesen Objekten. Projekte, die sich mit prosopographischer Forschung beschäftigen, können oder würden deshalb von weiterverwendbaren Daten besonders profitieren. Ihr Daten
    universum dreht sich nicht nur um für ihre Forschung zentrale Daten (die oft neu erstellt oder überprüft werden), sondern auch um angebundene Daten: Geburtsorte von Personen, Orte in denen sich Institutionen befinden, Lehrer von Personen in der Kerngruppe etc.
    
            Eine logische Konsequenz daraus ist es, zumindest für periphere Daten der eigenen Prosopographie LOD-Daten nachzunutzen. Um dies ohne großen Aufwand tun zu können, bräuchte es kompatible Ontologien/Datenmodelle. In den letzten Jahren wurden mehrere Versuche unternommen, für das Feld der Prosopographie einheitliche Datenmodelle vorzuschlagen (Fokkens and ter Braake, 2018; Tuominen et al., 2017), ohne dass es schon zu einem Konsens gekommen wäre.
            Das Poster wird über den Stand einer Initiative berichten, die seit vergangenem Jahr an der Definition einer RESTful API arbeitet, welche die Veröffentlichung von maschinenlesbaren prosopographischen Daten so erleichtern soll, dass typische Anfragen performant und für Softwareentwickler einfach zu realisieren sind. Dieses “International Prosopography Interoperability Format” (IPIF) hat als Kern die Definition in einer RESTful API, die in OpenAPI beschrieben ist.
            
            Das dort vorgeschlagene Datenmodell deckt zum einen die Notwendigkeit ab, zwischen Person, Quelle und Quelleninterpretation zu unterscheiden (“Factoid”-Modell, Bradley & Short 2005), zum anderen vereinfacht es den Zugriff auf Informationen über eine Person in klassischen Benutzungssenzarien der Prosopographie. In einem “Statement” über eine Person können verbale Beschreibungen oder Quellenzitate ebenso wie strukturierte Informationen enthalten sein. Um die prosopographische Benutzung zu erleichtern, lassen sich die strukturierten Informationen im Modell von IPIF als datierbare Ereignisse verstehen, wenn sie mit einer Property “date” versehen sind. Sie können aber für reine Identifikationszwecke auch einfache Eigenschaften (Name, Geschlecht) abbilden. Properties “relatesToPerson” und “isMemberOf” bedienen ein drittes zentrales Szenario der Nutzung von prosopographischen Daten, nämlich Beziehungen zu anderen Personen. Schließlich sind Ortsangaben zu einer Person mit der Property “place” möglich. Mit diesen Angaben ermöglicht IPIF Anzeigen wie die des DARIAH-Cosmotools und Ego-Netzwerke wie z.B. in der Deutschen Biographie.
    
         
         
             Proof of concepts
            Von Beginn an war eine Grundidee des
  Unterfangens, die Praxistauglichkeit des Datenmodells und der API
  Definition möglichst früh zu testen. Zu diesem Zweck wurden seit
  2018 mehrere “Proof of Concept” Applikationen erstellt. Das Poster
  wird den aktuellen Entwicklungsstand dieser Proof of Concept
  Applikationen darstellen.
         
         
             APIS (Austrian Prosopographical Information System)
            APIS ist ein Web-basiertes System zur
  Arbeit an prosopographischen Daten (Schlögl/Lejtovicz 2018). Es
  bietet Webformulare, aber auch API-Schnittstellen zu den
  Daten. Aufbauend auf die schon vorhandenen APIs wurde ein Renderer
  erstellt, der vorhandene Daten in das IPIF Format
  überführt. Zusätzlich wurden als parallele API die IPIF endpoints
  implementiert und somit compliance level 1 laut API Definition (GET
  requests inklusive Filter) erreicht.
         
         
            API Wrapper
            Eines der Anwendungsszenarien von IPIF
  ist die Suche schon vorhandener Identifier über mehrere Referenz
  Ressourcen hinweg (vgl. Vogeler et al 2020 forthcoming). Um die
  Anwendbarkeit von IPIF für dieses Szenario zu testen, wurde eine
  einfache Applikation erstellt, die als Middle-Layer zwischen der
  IPIF API auf der einen Seite und dem Wikidata SPARQL Endpoint und
  der Lobid GND API auf der anderen Seite fungiert. Die Applikation
  übersetzt dabei Anfragen an die API in eine wikidata kompatible
  SPARQL query bzw. einen Lobid kompatiblen GET request und überführt
  die Antworten in das IPIF format. Die Applikation macht sich für
  diese Übersetzung die Django-Templating Engine zu Nutze und ist
  damit auch für andere APIs einfach konfigurierbar.
         
         
            Papilotte
            Auf der in OpenAPI veröffentlichten Spezifikation aufbauend lässt sich über Frameworks wie das von Zalando als Open Source Software bereitgestellte “Connexion” schnell ein Stand-Alone-Server in Python schreiben, der über flexible Konnektoren den Zugriff auf unterschiedlichste interne oder externe Datenquellen ermöglicht und diese IPIF-konform bereitstellt. Ein solcher Server wurde mit Beispieldaten aus dem Monasterium.net-Projekt erstellt.
            
         
         
            Papi-Cosmotool
            Das DARIAH-Cosmotool bietet eine prototypische Oberfläche für eine prosopographische Datenbank an. Es enthält eine biographische (“Zeitleiste”), eine textuelle (“Ereignis-Detail”) und ein geographische (“Kartendarstellung”) Ansicht. Die Datenanzeige wird mit einem Quellenverweis ergänzt. Als Test für die Verwendbarkeit der API-Definition wird von Sebastian Stoff am Zentrum für Informationsmodellierung eine JavaScript basierte Anwendung erarbeitet, die vergleichbare Funktionalitäten bietet.
            
         
         
            JSON-LD
            Schließlich arbeiten wir an einer Integration des JSON-Outputs der API-Definition in das Semantic Web. Dafür soll eine context.json-Datei bereitgestellt werden, die in den Resultsets der API-Anfragen gültige RDF-Aussagen identifiziert.
            
         
      
      
         
            
               https://github.com/GVogeler/prosopogrAPhI
            
            
               https://cosmotool.de.dariah.eu/
            
            
      z.B. https://www.deutsche-biographie.de/graph?id=sfz53095
            
            
               https://github.com/gvasold/papilotte, mit einer Beispielinstallation: 
      https://ginko.uni-graz.at/illurk/api/ui/
            
            
               http://glossa.uni-graz.at/gamsdev/stoffse/erla/mapp/map/, Code von Sebastian Stoff.
    
            
      Ein erster Entwurf ist unter https://github.com/GVogeler/prosopogrAPhI/blob/master/context.json einsehbar.
    
         
         
            
               Bibliographie
               
                  Bradley, John / Short, Harold
                   (2005): “Texts into databases. The Evolving field of New-style Prosopography.“ in: 
                  LLC
                   20, suppl. 1: 3-24.
               
               
                  Fokkens, A. / ter Braake, S. 
                  (2018): Connecting People Across Borders: a Repository for Biographical Data Models, in: Proceedings of the Second Conference on Biographical Data in a Digital World 2017. Linz, Austria, November 6-7, 2017. CEUR Workshop Proceedings: 83–92.
               
               
                  Stone, Lawrence (1971): "Prosopography." in: 
Daedalus
                   100: 46–79.
               
               
                  Schlögl, Matthias / Katalin Lejtovicz
                  . (2018). “A Prosopographical Information System (APIS).“ In: Antske Fokkens, ter Braake, Serge, Sluijter, Ronald, Arthur, Paul, and Wandl-Vogt, Eveline (eds.). 
                  BD-2017. Biographical Data in a Digital World 2017. Proceedings of the Second Conference on Biographical Data in a Digital World 2017
                  . Linz, Austria, November 6-7, 2017. Budapest: CEUR (CEUR Workshop Proceedings 2119): 53-58.
               
               
                  Tuominen, Jouni / Hyvönen, Eero / Leskinen, Petri 
                  (2018): “Bio CRM. A Data Model for Representing Biographical Data for Prosopographical Research.” In: 
                  BD-2017. Biographical Data in a Digital World 2017
                  , ed. by Antske Fokkens, Serge ter Braake, Ronald Sluijter, Paul Arthur, Eveline Wandl-Vogt, Budapest: CEUR (CEUR Workshop Proceedings 2119): 59-66.
               
               
                  Vogeler, Georg / Vasold, Gunter / Schlögl, Matthias 
                  (2020 forthcoming): Data exchange in practice: Towards a prosopographical API. In: BD2019, Workshop on Biographical Data in Occasion of the RANLP 2019, Varna, ed. by Antske Fokkens et al.
               
            
         
      
   


11079	2020	
      
         
            Einleitung
            Digitale Analyseverfahren verändern immer intensiver die Forschungsweise der GeisteswissenschaftlerInnen und mit dem wachsenden Spielraum der Methoden wächst auch die Anzahl an Fragen, die sich vor allem an den Grad der Genauigkeit und wissenschaftliche Relevanz dieser Methoden richtet. Das Topic Modeling gewinnt als eine Methode für automatische Erkennung von versteckten thematischen Strukturen in großen Textmengen (Blei 2012: 8) immer mehr an Beliebtheit, erweckt aber auch Unsicherheiten. Daher beschäftigt sich diese Arbeit mit den Möglichkeiten und Problemen des Topic Modeling am Beispiel von Briefen und stellt unter anderem die Fragen, 1) wie Topic Modeling in der Analyse von Briefkorpora eingesetzt werden kann und 2) wie die Qualität der Ergebnisse dieses Prozesses beeinflusst werden kann. 
         
         
            Forschungsmaterial
            Das Forschungsmaterial besteht aus Briefen des Grazer Sprachwissenschaftlers Hugo Schuchardt (1842-1927). Die umfangreiche und mehrsprachige Korrespondenz dieses schon seinerzeit sehr geschätzten Wissenschaftlers ist seit 2007 Teil des Digitalisierung-Projektes 
                    Hugo Schuchardt Archiv (Hurch 2019). Für die Topic-Modeling-Analyse werden 2261 Briefdateien im TEI-Format in Betracht gezogen, da die restlichen zurzeit noch in keinem entsprechenden Format vorhanden sind. Der Vorteil einer solchen Methode ist es aber, dass das gleiche Modell jederzeit auf eine erweiterte Menge an Daten anwendbar ist. Eine Besonderheit dieses Korpus ist, dass Schuchardt in mehreren Sprachen korrespondiert hat, von denen hier elf repräsentiert sind (Abbildung 1). Daher wird das Modell für einzelne Sprachen separat angewendet. Dies ist insofern eine Herausforderung, weil 1) Vorgänge den jeweiligen Sprachen angepasst werden müssen (wie etwa die Lemmatisierung), 2) der Textumfang bei vielen Sprachen nicht ausreichend ist und daher nicht auf alle Sprachen effektiv angewendet werden kann und 3) die verschiedenen Ergebnisse pro Sprache verglichen werden sollten. Ein weiteres Problem für das Topic Modeling ist die große Diskrepanz in den Textlängen der einzelnen Dateien (Abbildung 2), da die Korrespondenz auch kürzere Formen wie Postkarten und Telegramme beinhaltet. So enthalten etwa die kürzesten deutschsprachigen Dateien etwa drei Tokens, die längste jedoch 3947. Dies ist aber ein Zustand, den viele Briefkorpora in der Realität begegnen, da wir als ForscherInnen selten einem ‚idealen‘ Korpus gegenüberstehen. Die Auseinandersetzung mit solchen Problemen ist ein fester Bestandteil unserer Arbeit.
                
            
               
                Abbildung 1: Anteil der einzelnen Sprachen im Briefkorpus
            
            
               
                Abbildung 2: Menge der deutschsprachigen Briefdateien nach ihrer Anzahl der Tokens
            
         
         
            Methode
            Für die Beantwortung der Forschungsfragen war zuerst die Literaturrecherche nötig, und zwar erstens zum Topic Modeling, zweitens zur Textsorte Brief und drittens zu dieser Korrespondenz. Um eine genauere Vorstellung zum Forschungsstand des Topic Modeling zu bekommen, wurden wissenschaftliche Aufsätze und Anwendungsbeispiele in Betracht gezogen, wie etwa Blei 2010, Jagarlamudi/Daumé 2010, Boyd-Graber/Blei 2012, Riddell 2015, Vulić et al. 2015, Bock et al. 2016, Andorfer 2017, Fechner/Weiß 2017, Schöch 2017, Murakami et al. 2017 und Arora et al. 2018. Zudem wird am genannten Korpus Topic Modeling mit Hilfe der Programmiersprache 
                    Python (Python Software Foundation 2001-2019), der Software MALLET (McCallum 2002-2019) und der Anweisungen der Jupyter-Notebooks von DARIAH-DE (DARIAH-DE 2019) vollzogen. Darüber hinaus werden verschiedene Tools zur Vorverarbeitung evaluiert – z. B. 
                    spaCy (Explosion AI 2019) und DTA::CAB (Berlin-Brandenburgische Akademie der Wissenschaften 2011-2018) für die Lemmatisierung – sowie verschiedene Tools und Parameter für die Topic-Modellierung – z. B. 
                    Topics Explorer (DARIAH-DE 2018) – und die daraus resultierenden Ergebnisse und Erfahrungen verglichen. 
                
         
         
            Ergebnisse
            Obwohl es sich um ein laufendes Projekt handelt, gibt es bereits einige relevante Ergebnisse und Schlussfolgerungen. 
            1) Die Vorverarbeitung stellt einen wichtigen Schritt in der Topic-Modellierung dar und beeinflusst die Ergebnisse. Dabei spielen nicht nur die eingesetzten Tools eine Rolle, sondern auch die gewählte Vorgehensweise.
            2) Die Lemmatisierung, auf die beim Topic Modeling oft verzichtet wird, ermöglicht mehr semantische Differenz in den Topics. 
            3) Der unterschiedliche Textumfang von einzelnen VerfasserInnen kann zu falschen Ergebnissen führen, wenn die Topics pro VerfasserIn analysiert werden.
            4) Entscheidungen über Parameter wie Optimierungsintervall, Topic- und Iterations-Anzahl können die Ergebnisse beeinträchtigen und müssen immer projektspezifisch getestet werden, bis ein sinnvolles Resultat vorliegt. Das ‚Sinnvolle‘ zu erkennen ist eine Herausforderung, die fachwissenschaftliches Verständnis verlangt. 
            Die Inkonsistenz der Topics und manchmal verwirrende Ergebnisse zeigen, dass die naive Anwendung eines Topic-Modeling-Tools nicht immer befriedigend sein kann. Intensivere Beschäftigung mit den einzelnen Schritten und Ergebnissen kann sich jedoch positiv auf den Erfolg der Analyse auswirken. Die weitere Arbeit wird zeigen, ob und welchen Mehrwert Topic Modeling bei der Analyse der Schuchardt-Korrespondenz leisten kann, die durch 
                    close reading nicht erreicht werden können. 
                
         
      
      
         
            
               Bibliographie
               
                  Andorfer, Peter (2017): 
"Turing Test für das Topic Modeling. Von Menschen und Maschinen
erstellte inhaltliche Analysen der Korrespondenz von Leo von
Thun-Hohenstein im Vergleich", in: 
Zeitschrift für digitale Geisteswissenschaften 2. http://zfdg.de/2017_002 [letzter Zugriff 27. September 2019].

               
                  Arora, Sanjeev / Ge, Rong; Halpern, Yoni / Mimno, David / Moitra, Ankur / Sontag, David / Wu, Yichen / Zhu, Michael (2018): 
  "Learning topic models - provably and efficiently",  in: 
  Communications of the ACM 61 / 4: 85–93. 10.1145/3186262.

               
                  Berlin-Brandenburgische Akademie der Wissenschaften (ed.) (2011-2018): 
  Das DTA-Basisformat. http://www.deutschestextarchiv.de/doku/basisformat/ [letzter Zugriff 27. September 2019].

               
                  Blei, David M. (2010): 
 "Introduction to Probabilistic Topic Models", in: 
 Semantic Scholar.
 https://pdfs.semanticscholar.org/5f10/38ad42ed8a4428e395c96d57f83d201ef3b3.pdf
 [letzter Zugriff 27. September 2019].

               
                  Blei, David M. (2012): 
  "Topic Modeling and Digital Humanities", in: 
  Journal of Digital Humanities 2 / 1: 8–11.
  http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/ [letzter Zugriff 27. September 2019].

               
                  Bock, Sina / Du, Keli / Huber, Michael / Pernes, Stefan / Pielström, Steffen (2016): 
  Der Einsatz quantitativer Textanalyse in den Geisteswissenschaften. Bericht über den Stand der Forschung. (= DARIAH-DE working papers 18). Göttingen: GOEDOC – Dokumenten- und Publikationsserver der Georg-August-Universität Göttingen. http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2016-18.pdf [letzter Zugriff 27. September 2019].

               
                  Boyd-Graber, Jordan / Blei, David (2012): 
  Multilingual Topic Models for Unaligned Text. http://arxiv.org/pdf/1205.2657v1 [letzter Zugriff 27. September 2019].

               
                  DARIAH-DE (2018): 
  Topics Explorer. V. 2.0.1. https://github.com/DARIAH-DE/TopicsExplorer [letzter Zugriff 27. September 2019].

               
                  DARIAH-DE (2019): 
  DARIAH Topics. Easy Topic Modeling in Python. V. 2.0.1. https://github.com/DARIAH-DE/Topics [letzter Zugriff 27. September 2019].

               
                  Explosion AI (2019): 
  spaCy. V. 2.1.6. https://github.com/explosion/spaCy [letzter Zugriff 27. September 2019].

               
                  Fechner, Martin / Weiß, Andreas (2017): 
  "Einsatz von Topic Modeling in den Geschichtswissenschaften: Wissensbestände des 19. Jahrhunderts", in: 
  Zeitschrift für digitale Geisteswissenschaften 2. http://zfdg.de/2017_005 [letzter Zugriff 27. September 2019].

               
                  Hurch, Bernhard (2019): 
  "Hugo Schuchardt Archiv". Institut für Romanistik, Karl-Franzens-Universität Graz (ed.). https://schuchardt.uni-graz.at [letzter Zugriff 27. September 2019].

               
                  Jagarlamudi, Jagadeesh / Daumé, Hal (2010): 
  "Extracting Multilingual Topics from Unaligned Comparable Corpora",  in: Gurrin, Cathal (ed.): 
  Advances in information
  retrieval. Proceedings 444–456. (= Lecture notes in computer science 5993). Berlin / Heidelberg / New York: Springer.

               
                  McCallum, Andrew Kachites (2002-2019): 
  MALLET. A Machine Learning for Language Toolkit. V. 2.0.8. http://mallet.cs.umass.edu [letzter Zugriff 27. September 2019].

               
                  Murakami, Akira / Thompson, Paul / Hunston, Susan / Vajn, Dominik (2017): 
  "‘What is this corpus about?’: using topic modelling to explore a
  specialised corpus", in: 
  Corpora 12 / 2: 243–277. https://www.euppublishing.com/doi/10.3366/cor.2017.0118 [letzter Zugriff 27. September 2019].

               
                  Python Software Foundation (2001-2019): 
  Python. V. 3.7.4. https://github.com/python [letzter Zugriff 27. September 2019].

               
                  Riddell, Allen (2015): 
  Text Analysis with Topic Models for the Humanities and Social Sciences — Text Analysis with Topic Models for the Humanities and Social Sciences. DARIAH-DE Initiative (ed.). https://liferay.de.dariah.eu/tatom/ [letzter Zugriff 27. September 2019].

               
                  Schöch, Christof (2017): 
  "Topic Modeling Genre. An Exploration of French Classical and
  Enlightenment Drama",
  in:  Digital Humanities Quarterly 11 / 2. http://www.digitalhumanities.org/dhq/vol/11/2/000291/000291.html [letzter Zugriff 27. September 2019].

               
                  Vulić, Ivan / Smet, Wim de / Tang, Jie / Moens, Marie-Francine (2015): 
  "Probabilistic topic modeling in multilingual settings. An overview
  of its methodology and applications",  in: 
  Information Processing & Management 51 / 1: 111–147. https://www.sciencedirect.com/science/article/pii/S0306457314000739 [letzter Zugriff 27. September 2019].

            
         
      
   


11705	2022	
        
            Mixed methods are firmly established in digital humanities (DH) scholarship. While this approach is understood as a research design adopted from social sciences, mixed methods seem to be an umbrella term for defining DH’s methodological framework in general (Sá Pereira 2019; Herrmann 2017). The use of computational procedures in DH is often regarded as a combination of quantitative and qualitative methods. Further conceptual pairs, for example 
                close and 
                distant reading, go hand in hand with this. However, mixed methods research rests on two premises (Uprichard and Dawney 2019, 20).
                
                     This paper connects directly to Uprichard & Dawney’s research approach, which already addresses the problem of integration as well as data diffraction in mixed-methods approaches in the social sciences.
                 First, the research design indicates that the complexity of an epistemic object is addressed by the plurality of methods used (Fieldling 2012, 127). Second, research data obtained by mixed methods can be integrated. In other words, results of different methodological settings can be put into a coherent narrative. So far integration within mixed methods research is often discussed in a realm of technical challenges concerning data settings, standards and ontologies. Although data integration addresses epistemological and social issues of conformity and interoperability of research data for a global DH community. 
            
            In this short presentation, I argue that integration provides one device to explore questions of difference and diversification within DH scholarship. Therefore, I investigate promises, constraints and pitfalls of the “integration”-narrative, which seems to be deeply enfolded in mixed methods research. The focus of attention will be on compatibilities as well as forms of inferences, which gain relevance manufacturing of knowledge within mixed methods research (Knorr Cetina 1981; Kuhn 1994). In order to tackle these questions, I discuss “data diffraction” (Uprichard and Dawney 2019, 26) – a counternarrative presented by Uprichard and Dawney – as one complementary aim for dealing with different data settings resulting from mixed methods research. What new perspectives open up if we speak of data diffraction instead of data integration? 
            The term 
                diffraction, which was originally introduced by Donna Haraway and Karen Barad for epistemological endeavors, initially describes optical interference patterns that arise when two waves are superimposed (Haraway 1992, 300; Barad 2007, 91). Contrary to an holistic idea of integrating parts under a whole, diffraction is about the productive maintenance of differences. Exploring the narrative of data diffraction, this short presentation brings into sharper relief latent integration mechanisms on the one hand, and explore possible alternatives on the other (Drucker 2021, 2; Liu 2020, 130). Beyond or complementary to integration, how could methods or data relate to each other? What if, we explicitly describe diffractions, that is incommensurabilities and dissonances, of methods and data? What would this mean for international collaboration? 
            
            Two examples are shortly discussed in this presentation. The first example brings into focus data integration in the context of mixed methods through ontologies as formal models. Ontologies enable to store and query mixed research data. Therefore, ontologies promise a semantic interoperability that allows different data sets to be integrated with each other (Pidd and Rogers, 2018). But how are different data settings handled? What possibilities do OWL and RDF schemas offer to describe leftovers and surplus of research data? In this context, I speculate about possibilities for data diffraction. One scenario here is ontology hijacking (Eide and Smith-Ore 2019, 188). 
            The second example dwells on existing mixed methods approaches from the literary studies, digital stylometry in particular. “DH style studies may be a natural environment for the mixed-methods-paradigm”, as Herrmann has phrased it (Herrmann 2017). In digital stylometry, for instance, authorship attribution with Burrows’ Delta algorithm, agglomerative cluster analysis as well as principal component analysis are widely used (Karsdorp et al. 2021, 248f.). Using the literary category of style, I examine how integration and diffraction might differently enact and constitute style as an object of inquiry within mixed methods research. In doing so, I engage a critical reading of two python scripts from digital stylometry studies. Where does data integration or diffraction actually take place in concrete terms?
        
        
            
                
                    Bibliography
                    
                        Barad, K. (2007). 
                        Meeting the Universe Halfway: Quantum Physics and the Entanglement of Matter and Meaning. Durham: Duke Univ. Press.
                    
                    
                        Drucker, J. (2021). Viewpoint: Hetero-ontologies and taxonomies in the wild. 
                        Art Libraries Journal 46 (2), 36–39. 
                        https://doi.org/10.1017/alj.2021.2.
                    
                    
                        Eide, Ø. and Smith-Ore, C.E. (2019). Ontologies and data modeling. In Flanders, J. and Jannidis, F. (eds), 
                        The shape of data in the digital humanities. Modeling texts and text-based resources. London/New York, Routledge, pp.178–196.
                    
                    
                        Fielding, N. G. (2012). Triangulation and Mixed Methods Designs. 
                        Journal of mixed methods research 6 (2): 124–136. 
                        https://doi.org/10.1177/1558689812437101.
                    
                    
                        Haraway, D. (1992). The Promises of Monsters: A Regenerative Politics for Inappriopriate/d Others. In Grossberg, L., Nelson, C. and Treichler, P. A. (eds), 
                        Cultural Studies. New York, NY, London: Routledge, pp. 295–337.
                    
                    
                        Herrmann, B. (2017). In a Test Bed with Kafka. Introducing a Mixed-Method Approach to Digital Stylistics. 
                        Digital Humanities Quarterly 11, (4). 
                        http://www.digitalhumanities.org/dhq/vol/11/4/000341/000341.html.
                    
                    
                        Karsdorp, F., Kestemont, M. and Ridell, A. (2021). 
                        Humanities Data Analysis: Case Studies with Python. Princeton: Princeton University Press.
                    
                    
                        Knorr Cetina, K. (1981). 
                        The Manufacture of Knowledge: An Essay on the Constructivist and Contextual Nature of Science. 1. Edition. Oxford: Pergamon Press.
                    
                    
                        Kuhn, T. (1994). 
                        The Structure of Scientific Revolutions. Chicago: Chicago Univ. Press, 1994.
                    
                    
                        Liu, A. (2020). Toward a Diversity Stack: Digital Humanities and Diversity as Technical Problem. 
                        PMLA 135, (1): 130–51. 
                    
                    
                        Pidd, M. and Rogers, K. (2018). Why Use an Ontology? Mixed Methods Produce Mixed Data. October 18, 2018, 
                        https://talkinghumanities.blogs.sas.ac.uk/2018/10/18/why-use-an-ontology-mixed-methods-produce-mixed-data/ (accessed 20 April 2022).
                    
                    
                        Sá Pereira, M. P. (2019). Mixed Methodological Digital Humanities. In Gold, M. K. and Klein, L. F. (eds), Debates in the Digital Humanities 2019. 5. Minneapolis: University of Minnesota Press, 2019. 
                    
                    
                        Uprichard, E. and Dawney, L. (2019). Data Diffraction: Challenging Data Integration in Mixed Methods Research. Journal of mixed methods research 13, (1): 19–32. 
                        https://doi.org/10.1177/1558689816674650
                    
                
            
        
    


11712	2022	
        
            This study tests the use of a novel computational approach, one that analyses changes in shape of historical artefacts across time, in a new context. Previously developed by the authors and tested upon Western art, in particular ancient Greek pottery, this methodology is here applied for the first time to East-Asian art, in particular Chinese vases [1]. The East-Asian perspective is crucial in understanding the adaptability of the approach to different geographical regions and time periods, contributing to the construction of a global history of shape evolution and design progression over time. 
            The study of shapes and styles as embodying the cultural concerns of a particular historical moment has been at the center of several disciplines including art history and archaeology. It has captured the interest of scholars since the eighteenth century when Johann Joachim Winckelmann devised his categorisation of style, focusing particularly on Greek and Roman art [2]. In more recent times, George Kubler proposed new ways of historical sequencing of form based on continuous change across time [3]. In Chinese art, surveys of the development of pottery over time have also been conducted, most recently by Ye Zhemin叶喆民 [4].
            The current research inscribes itself within this intellectual tradition yet propose a new way of quantifying changes in shape and exploring connections between objects: a computational technique. A few studies have attempted to move in this direction although they have been restricted by the technology, and materials, namely photographs [5, 6]. This paper employs a new methodology and material, 3D scans of historical artefacts, therefore providing one of the first case studies of corpus research on 3D digitised objects. 
            The approach has been tested on a case study of four Chinese vases of the Beaker type, deriving from late Ming and early Qing Dynasties (1620-1683). These objects are held in the Ashmolean Museum in Oxford, U.K., under ascension numbers EA1978.799, EA1978.798, EA1971.22, and EA1978.1903. These were chosen because of the transformation in shape of beaker vases between the late-Ming and early-Qing Dynasties (1620-1683), due to changing tastes in this period of dynastic transition. This has captured much scholarly interest. Some scholars, such as Geng Baochang 耿宝昌, Zhu Jun朱军 and Xu Jingjing徐菁菁 have examined the changing shapes of vases in this period, noting that appraisers were required to memorise shapes when inspecting and identifying ceramic vases [7, 8, 9]. Other scholars, such as Soame Jenyns and Margaret Medley, applied a topological method of visual analysis of ceramic vases, leading to a revision in their dating [10, 11]. A combination of quantitative methods and topological techniques were used by Ji Dongge
                纪东歌 and Yu Haiyang 于海洋, both of whom were interested in the historical and societal influences over shape design and patterns in the two dynasties [12, 13]. This paper proposes a new, quantitative approach to undertake the study of shapes and forms. 
            
            To analyse the dataset, the vases were captured in three dimensions using photogrammetry, from which a 3D model was built. From the mesh of each model, a random sample of vertices of 1000 points was extracted. The vases were roto-translated and centered so that the orientations were standardised across models. These models were compared by relying on metrics that measured the distance between their distribution of points. In this study, an approximation of the Wasserstein metric, known as the Sinkhorn distance, was used. The benefit of the Wasserstein metric for this comparative approach lies in its capacity to synthetise into one ‘number’ the dissimilarity between two distributions (shapes): the greater the difference, the greater the cost (value) to reposition the points. A pre-existing suite was deployed to implement the algorithm [14, 15]. The Sinkhorn distances are the final output of the analysis. The comparison produced is a series of pairwise distances that can be used to assess the relative closeness or similarity between shapes.
            This study has outlined the usefulness of this new computational approach for quantifying changes in East-Asian pottery. The method can be scaled to large datasets of 3D objects scans where changes can be computed automatically, without the need for human intervention. As museums and cultural institutions move to digitise their collections in three dimensions, this approach opens new possibilities for the large-scale study of form across time and geographical locations.
        
        
            
                
                    Bibliography
                    [1] Pala, G. and Costiner, L. (2022). “Tracing Changes in Shape of Historical Artefacts across Time using 3D Scans: A New Computational Approach”, 
                        Journal of Open Humanities Data, forthcoming. 
                    
                    [2] Winckelmann, J. (1764). 
                        Johann Winckelmanns, […] Geschichte der Kunst des Alterthums. Dresden: In der Waltherischen Hof-Buchhandlung. The English translation is Winckelmann, J. (2006), 
                        The History of the Art of Antiquity. Los Angeles: Getty Research Institute.
                    
                    [3] Kubler, G. (1962). 
                        The shape of time: Remarks on the history of things. New Haven: Yale University Press.
                    
                    [4] Zhemin, Y., 叶喆民 (2011), 
                        Zhongguo taoci shi
                  中国陶瓷史 [
                        History of Chinese Pottery and Porcelain]. Beijing: SDX Joint Publishing Company. 
                    
                    [5] Liming, G., L. Hongjie and J. Wilcock (1989). “The Analysis of Ancient Chinese Pottery and Porcelain Shapes: a Study of Classical Profiles from the Yangshao Culture to the Qing Dynasty Using Computerised Profile Data Reduction, Cluster Analysis and Fuzzy Boundary Discrimination”, in Rahtz, S. (ed.),
                         Computer Applications and Quantitative Methods in Archaeology. CAA89 (BAR International Series 548). Oxford: B.A.R., pp. 362-374.
                    
                    [6] Liying W., and B. Marwick (2020). “Standardization of ceramic shape: A case study of Iron Age pottery from northeastern Taiwan”. 
                        Journal of Archaeological Science: Report, Vol. 33, pp. 1-11. 
                    
                    [7] Baochang, G. 耿宝昌 (1993).
                         Ming Qing ciqi jianding 
                  明清瓷器鉴定 [Ming and Qing Porcelain on Inspection]. Beijing: The Palace Museum.
                    
                    [8] Jun Z. 朱军 (2002). “
                        Mingmo Qingchu qinghau huagu jianding 
                  明末清初青花花觚鉴定 [A late Ming and early Qing dynasty blue and white goblet identification]”, 
                        Wenwu Shijie 
                  文物世界 4, pp. 38-42.
                    
                    [9] Jingjing, X 徐菁菁 (2017). “
                        Mingqing cigu yuanliu ji tezheng
                  明清瓷觚源流及特征 [Sources and Characteristics of Ming and Qing beaker vases].” 
                        Yishupin 
                  艺术品, 11, pp. 66-73.
                    
                    [10] Medley, M. (1987). “The Ming-Qing Transition in Chinese Porcelain”, 
                        Arts Asiatiques 42, pp. 65-76.
                    
                    [11] Jenyns,S. (1955). “The Wares of the Transitional Period Between the Ming and Ch’ing, 1620-1683”, 
                        Archives of the Chinese Art Society of Americas 9, pp. 20-42.
                    
                    [12] Dongge, J. (2012). 纪东歌, “
                        Qingchuqi Jingdezhen Jinian ciqi fenqi yanjiu 
                  清初期景德镇纪年瓷器分期研究[A staging study of early Qing dynasty Jingdezhen chronological porcelain]”, 
                        Zhongguo yishu yanjiu yuan 
                  中国艺术研究院.
                    
                    [13] Haiyang, Y. (2012) 于海洋, “
                        Mingqing guxing ciqi yanjiu 
                  明清觚形瓷器研究 [A study of Ming and Qing beaker vases]”, PhD diss., Jilin Daxue 吉林大学[Jilin University].
                    
                    [14] Point Cloud Utils (pcu) - A Python library for common, 
                        https://github.com/fwilliams/point-cloud-utils.
                    
                    [15] Cuturi, M. (2013). 
                        Sinkhorn distances: Lightspeed computation of optimal transport. Advances in Neural Information Processing Systems, 26, pp. 2292-2300.
                    
                
            
        
    


11713	2022	
        
            
                Introduction
                Fuzzy string matching is a common challenge of linking data in many digital humanities projects, which often deal with noisy, historical, or non-standard text (Olieman et al., 2017). Named entities (in particular place names) are often present under a variety of forms, which can range from regional spelling differences to cross-linguistic or diachronic variation, sometimes due to a change in the political and cultural context, to lack of standardization, or to a process of linguistic standardization. In working with digitized materials, an additional, artificial layer of variation can occur, introduced by optical character recognition errors (Butler et al., 2017; Coll Ardanuy and Sporleder, 2017; De Wilde and Hengchen, 2017; van Strien et al., 2020).
                Several studies have warned of the importance of fuzzy string matching for entity linking, especially in noisy and non-standard text (Coll Ardanuy et al., 2020; De Wilde and Hengchen, 2017; Hachey et al., 2013). However, to date, most entity linking systems rely on either exact or partially overlapping string matching. This is due to the high computation time required by most fuzzy string matching approaches, such as Levenshtein distance (Santos et al., 2018a). In this tutorial, we will introduce DeezyMatch (Hosseini et al., 2020), an open-source, user-friendly Python library for fuzzy string matching and candidate ranking for entity linking that has been developed in the Living with Machines project (https://livingwithmachines.ac.uk/). DeezyMatch builds and expands on Santos et al. (2018b), an approach to fuzzy string matching that uses a deep learning architecture to classify pairs of toponyms as either potentially referring to the same entity or not. DeezyMatch is a tool that integrates recent deep learning advances, and has been specifically designed to be flexible, user-friendly, and fast, and therefore ready to be used in real entity linking scenarios.
                In this tutorial, we will show how DeezyMatch can be used to mitigate the problem of name variation in noisy, historical, or non-standard data. We will show how to create string pair datasets that can be used to train and test a DeezyMatch model, and how DeezyMatch models can be used to retrieve candidate entities from a gazetteer or knowledge base. By way of motivation, we will provide and discuss some real digital humanities examples which require fuzzy string matching and will show how DeezyMatch can be used to tackle them. During our tutorial, we will focus on the following case studies:
                
                    
                        Case study 1: We will show how a DeezyMatch model can be created from token-level alignments of OCRed text and their manual corrections. We will use the aligned tokens generated in van Strien et al. (2020) using a corpus of OCRed newspaper texts (from the National Library of Australia Trove digitized newspaper collection) that are aligned with human corrections performed by volunteers (Evershed and Fitch, 2014). We will show how to train a DeezyMatch model that learns OCR transformations from newspaper data and will show how it can be used to find a match for a given OCRed query from a pool of potential candidates from a specific knowledge base.
                    
                    
                        Case study 2: We will show how to create DeezyMatch models that are trained on name variations of places, which will enable us to find the best entry in a gazetteer, for a given query. As an example, we will show how these models can be used to consolidate data about names of heritage locations in Arabic speaking countries, like in the Heritage Gazetteer of Libya (https://slsgazetteer.org/). Currently, the high level of spelling variation in Arabic placenames (across time and transcriptions) makes it difficult to consolidate data that lies in different archives and collections, which at the moment rely on perfect string matching to find connections. We will show how DeezyMatch can be used to more easily associate a heritage location to a number of variant names, thus improving accuracy of data and metadata, and facilitating alignment with other knowledge bases such as Wikidata or Geonames.
                    
                
                This is a hands-on tutorial: participants will be shown how to train a DeezyMatch model and use it for candidate ranking. We will allocate time at the end for discussion, including how to adapt DeezyMatch to different digital humanities projects in different languages and time periods.
                We will build on the experience gained on providing two different tutorials on DeezyMatch in the past:
                
                    December 2020: "Linking and Enriching GeoData through Test and Play: a tutorial on DeezyMatch", as part of the 
                        LinkedPasts conference (Mariona Coll Ardanuy, Kasra Hosseini, Katherine McDonough, and Federico Nanni), followed by a round table. It was held virtually, and there were around 40 participants. Link to the tutorial: 
                        https://github.com/LinkedPasts/LaNC-workshop/tree/main/deezymatch
                    
                    July 2021: "Best practices in collaborative coding and on using GitFlow for data science research", as part of the 
                        Digital Humanities & Research Software Engineering virtual summer school, hosted by the Alan Turing Institute. It was held virtually, and there were 25 participants. The focus wat not so much on fuzzy string matching but on collaborative coding. Link to the tutorial: 
                        https://github.com/alan-turing-institute/DH-RSE-Summer-School/tree/main/Day%201/gitflow
                    
                
            
            
                Outline
                This is a half-day tutorial which will cover the following core content:
                
                    
                        Part 1: Introduction to DeezyMatch and motivation [60 min]
                        
                            [10 min] Introduction to fuzzy string matching and entity linking
                            [30 min] Description of case studies and data obtaining and preparation
                            [20 min] Overview of DeezyMatch
                        
                    
                    
                        Part 2: Interactive hands-on session [1h20 min]
                        
                            [10 min] Demo 1: candidate ranking using a pre-trained model
                            [20 min] Hands-on exercise
                            [10 min] Touch base
                            [10 min] Demo 2 and hands-on session: DeezyMatch training and candidate ranking
                            [20 min] Hands-on exercise
                            [10 min] Touch base
                        
                    
                    
                        Part 3: Discussion and feedback [40 min]
                        
                            [20 min] How to adapt DeezyMatch for your project
                            [20 min] Questions
                        
                    
                
            
            
                Instructors
                
                    
                        Mariona Coll Ardanuy: Mariona is a computational linguist at the Alan Turing Institute in the Living with Machines project. Her research interests lay in the intersection between the humanities and language technology.
                    
                    
                        Kasra Hosseini: Kasra is a Research Data Scientist at The Alan Turing Institute. He is interested in (artificially) intelligent systems, machine learning, and data analysis and visualisation.
                    
                    
                        Federico Nanni: Federico is a Research Data Scientist at The Alan Turing Institute. He is a historian by training and works exploring the intersections between digital humanities, computational social science, and natural language processing.
                    
                    
                        Valeria Vitale: Valeria Vitale is a researcher in the field of digital cultural heritage. She works at the Alan Turing Institute as Research Associate on the Machines Reading Maps project.
                    
                
            
            
                Target audience
                Based on past experience, we believe the number of participants should be 20 at most. Participants should have some experience in programming in Python and running scripts, and ideally be interested in entity linking or fuzzy string matching.
            
            
                Funding statement
                This work was supported by Living with Machines (AHRC grant AH/S01179X/1) and The Alan Turing Institute (EPSRC grant EP/N510129/1). The Living with Machines project, funded by the UK Research and Innovation (UKRI) Strategic Priority Fund, is a multidisciplinary collaboration delivered by the Arts and Humanities Research Council (AHRC), with the Alan Turing Institute, the British Library and the Universities of Cambridge, East Anglia, Exeter, and Queen Mary University of London.
            
        
        
            
                
                    Bibliography
                    
                        Butler, J. O., Donaldson, C. E., Taylor, J. E., and Gregory, I. N. (2017). Alts, Abbreviations, and AKAs: historical onomastic variation and automated named entity recognition. 
                        Journal of Map & Geography Libraries, 
                        13(1), 58-81.
                    
                    
                        Coll Ardanuy, M., Hosseini, K., McDonough, K., Krause, A., van Strien, D., and Nanni, F. (2020). A deep learning approach to geographical candidate selection through toponym matching. In 
                        Proceedings of the 28th International Conference on Advances in Geographic Information Systems (pp. 385-388).
                    
                    
                        Coll Ardanuy, M., and Sporleder, C. (2017). Toponym disambiguation in historical documents using semantic and geographic features. In 
                        Proceedings of the 2nd International Conference on Digital Access to Textual Cultural Heritage (pp. 175-180).
                    
                    
                        De Wilde, M., and Hengchen, S. (2017). Semantic enrichment of a multilingual archive with linked open data. 
                        Digital Humanities Quarterly.
                    
                    
                        Evershed, J., & Fitch, K. (2014). Correcting noisy OCR: Context beats confusion. In 
                        Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage (pp. 45-51).
                    
                    
                        Hachey, B., Radford, W., Nothman, J., Honnibal, M., & Curran, J. R. (2013). Evaluating entity linking with Wikipedia. 
                        Artificial intelligence, 
                        194, 130-150.
                    
                    
                        Hosseini, K., Nanni, F., and Coll Ardanuy, M. (2020). DeezyMatch: A flexible deep learning approach to fuzzy string matching. In 
                        Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations (pp. 62-69).
                    
                    
                        Olieman, A., Beelen, K., van Lange, M., Kamps, J., and Marx, M. (2017). Good applications for crummy entity linkers? the case of corpus selection in digital humanities. In 
                        Proceedings of the 13th International Conference on Semantic Systems (pp. 81-88).
                    
                    
                        Santos, R., Murrieta-Flores, P., and Martins, B. (2018). Learning to combine multiple string similarity metrics for effective toponym matching. 
                        International journal of digital earth, 
                        11(9), 913-938.
                    
                    
                        Santos, R., Murrieta-Flores, P., Calado, P., and Martins, B. (2018). Toponym matching through deep neural networks. 
                        International Journal of Geographical Information Science, 
                        32(2), 324-348.
                    
                    
                        Van Strien, D., Beelen, K., Ardanuy, M. C., Hosseini, K., McGillivray, B., and Colavizza, G. (2020). Assessing the impact of OCR quality on downstream NLP tasks. 
                        Special Session on Artificial Intelligence and Digital Heritage: Challenges and Opportunities, in
                         Proceedings of the 12th International Conference on Agents and Artificial Intelligence (pp. 484-496)
                    
                
            
        
    


11719	2022	
        
            
                In traditional, paper-based archives, finding aids afford basic means for discovery, but tend to return results shaped by researchers’ preexisting knowledge and queries. With the digitization of archives, new analysis and visualization methods allow researchers to textually map large corpora and not only pinpoint specific materials, but also open new ways of navigation.
                
                    
                        ENDNOTES
                    
                    
                         See Graham, S., Milligan, I., and Weingart, S. (2015), 
                        Exploring Big Historical Data: The Historian's Macroscope
                        , Imperial College Press, London; Morrissey, R. (2015), “Archives of connection: ‘Whole network’ analysis and social history,” 
                        Historical Methods
                        , vol. 48, no. 2, pp. 67–79; Duff, W., and Haskell, J. (2015), “New uses for old records: A rhizomatic approach to archival access,” 
                        American Archivist
                        , vol. 78, no. 1, pp. 38-58; Putnam, L. (2016), “The transnational and the text-searchable: Digitized sources and the shadows they cast,” 
                        American Historical Review
                        , vol. 121, no. 2, pp. 377–402; Edelstein, D., Findlen, P., Ceserani, G., Winterer, C., and Coleman, N. (2017), “Historical research in a digital age: Reflections from the Mapping the Republic of Letters project,” 
                        American Historical Review
                        , vol. 122, no. 2, pp. 400–424; and Hoekstra, R. and Koolen, M. (2018), “Data scopes for digital history research,” 
                        Historical Methods: A Journal of Quantitative and Interdisciplinary History
                        , vol. 52, no. 2, pp. 79–94.
                    
                
                However, effective use of computational tools including natural language processing (NLP), named entity recognition (NER), and knowledge graphing often requires considerable technical sophistication, forming an access barrier for many researchers.
                
                    
                         Piotrowski, M. (2012), 
                        Natural Language Processing for Historical Texts
                        , Synthesis Lectures on Human Language Technologies, vol. 17, Morgan & Claypool Publishers, San Rafael; Marrero, M., Urbano, J., Sánchez-Cuadrado, S., Morato, J., and Gómez-Berbís, J.M. (2013), “Named Entity Recognition: fallacies, challenges and opportunities,” 
                        Computer Standards and Interfaces
                        , vol. 35, no. 5, pp. 482–489; Shahin, S. (2016), “When scale meets depth: Integrating Natural Language Processing and textual analysis for studying digital corpora,” 
                        Communication Methods and Measures
                        , vol. 10, no. 1, pp. 28–50; Srinivasa-Desikan, B. (2018), 
                        Natural Language Processing and Computational Linguistics: A Practical Guide to Text Analysis with Python, Gensim, spaCy, and Keras
                        , Packt Publishing, Birmingham and Mumbai; Ehrmann, M., Romanello, M., Flückiger, A. and Clematide, S. (2020), “Named Entity Recognition and linking on historical newspapers,” in Arampatzis, A. et al. (eds.), 
                        Experimental IR Meets Multilinguality, Multimodality, and Interaction
                        , Springer International, Cham, pp. 288–310.
                    
                
            
            
                To help bridge this technical gap, we have been developing a toolset, currently funded by an NEH-ODH Digital Humanities Advancement Grant, that integrates large-scale text processing and data visualization capabilities into the open-source 
                
                    Omeka
                
                
                    
                        
                            We developed
                            Archiviz
                            for the Omeka Classic version; technical and user support for our plugin will be available in February 2023 in the
                            
                                Omeka plugin library
                            
                            . Additional availability is planned through github and a Docker container for ease of use.
                        
                    
                
                content management platform. With the tool, users can upload batches of documents, perform NER on them, and manipulate an interactive graph that displays extracted entities (people, places, organizations, etc.) from the collection and how they interconnect in its component documents. The toolset’s components
                
                    
                        
                            The primary tools and plug-ins used to construct Archiviz include Harvard’s
                            
                                Elasticsearch
                            
                            Omeka plugin for enhanced search capabilities, Google 
                            
                                Tesseract
                            
                            for OCR, 
                            
                                spaCy
                            
                            ’s NLP functionality, particularly its NER functionality, with current development focusing on the integration of the Science Museum Group’s
                            
                                Heritage Connector
                            
                            for better entity identification, linking, and disambiguation. The graphing of this information is primarily performed with the force-directed graphing of
                            
                                d3.js
                            
                            .
                        
                    
                
                were all specifically developed for (and tested by) non-technical researchers and community advocates, with all computational work taking place in a simple graphical user interface.
            
            
                Our 
                Archiviz
                toolset produces social network-style visualizations that connect results on the basis of the important people, places, organizations, and other entities mentioned. By applying NER to a collection, users can see the full breadth of their research subject at a glance, and explore connections they may not have known exist.
                
                    
                        
                            For a similar application, see Tumbe, C. (2019), "Corpus linguistics, newspaper archives and historical research methods," 
                            Journal of Management History
                            , vol. 25, no. 4, pp. 533–549.
                        
                    
                
                What our implementation offers that is new, is the efficient display of interrelationships between large numbers of nodes (named entities) combined with the possibility of rapid navigation to search results (documents). In addition, the interface moves beyond traditional query-based archival searching, “flattening” out a collection to show results beyond a researcher’s interests or knowledge.
            
            
                
            
            
                We are continuing to refine
                Archiviz
                 in ways intended to be intuitive, usable, and readily adoptable by users from a variety of different backgrounds. A primary design goal of the project has been to make it accessible not just to relatively tech-savvy academics, but more importantly, communities beyond the academy. It is our hope that the tool may particularly benefit disadvantaged communities—which in the United States have often faced pressure from gentrification and urban redevelopment, with consequent coercive displacement from historical neighborhoods. Residents in such areas often lack the infrastructure to collect, preserve, and interpret local history. As such, we have partnered with various community groups and activists throughout the process to ensure that the tool can be useful to them. Most ambitiously, in 2019 we hosted a “Community Researcher Workshop” for Atlanta-based librarians, archivists, community organizers, nonprofit staffers, and students to explore the
                
                    Mayor Ivan Allen Digital Archive
                
                that has served as our first test corpus, using a prototype of our toolset.
                
                    
                        
                             On the issues here, see Flinn, A., Stevens, M., and Shepherd, E. (2009), “Whose memories, whose archives? Independent community archives, autonomy and the mainstream,” 
                            Archival Science
                            , vol. 9, no. 71, pp. 71–86.
                        
                    
                
            
            
                User testing of the interface during this workshop produced very positive feedback, with participants calling the platform “incredibly useful,” with the “potential to break down traditional barriers of why people are hesitant to use archives.” A GLAM (Gallery, Library, Archive, Museum) researcher noted that it “is something almost any archive or library could utilize to their advantage,” while a community advocate stated that she “wanted the information for myself, but also…to share with my fellow residents.” With an additional two years of grant-funded development since this workshop, we are excited to share our work with the DH community. While we have integrated much of the feedback from the workshop, future plans include capabilities to process a wider spectrum of digital, textualized media—documents, video and audio converted to text, and even images identified and categorized with computer vision making our tool relevant for a more diverse array of communities and collections.
                
                    
                        
                            Wiriyathammabhum, P., Summers-Stay, D., Fermüller, C., and Aloimonos, Y. (2017), “Computer vision and Natural Language Processing: Recent approaches in multimedia and robotics,” 
                            ACM Computing Surveys
                            , vol. 49, no. 4, pp. 1–44.
                        
                    
                
            
        
        
            
                
                    Bibliography
                    
                         See Graham, S., Milligan, I., and Weingart, S. (2015), 
                        Exploring Big Historical Data: The Historian's Macroscope
                        , Imperial College Press, London; Morrissey, R. (2015), “Archives of connection: ‘Whole network’ analysis and social history,” 
                        Historical Methods
                        , vol. 48, no. 2, pp. 67–79; Duff, W., and Haskell, J. (2015), “New uses for old records: A rhizomatic approach to archival access,” 
                        American Archivist
                        , vol. 78, no. 1, pp. 38-58; Putnam, L. (2016), “The transnational and the text-searchable: Digitized sources and the shadows they cast,” 
                        American Historical Review
                        , vol. 121, no. 2, pp. 377–402; Edelstein, D., Findlen, P., Ceserani, G., Winterer, C., and Coleman, N. (2017), “Historical research in a digital age: Reflections from the Mapping the Republic of Letters project,” 
                        American Historical Review
                        , vol. 122, no. 2, pp. 400–424; and Hoekstra, R. and Koolen, M. (2018), “Data scopes for digital history research,” 
                        Historical Methods: A Journal of Quantitative and Interdisciplinary History
                        , vol. 52, no. 2, pp. 79–94.
                    
                    
                         Piotrowski, M. (2012), 
                        Natural Language Processing for Historical Texts
                        , Synthesis Lectures on Human Language Technologies, vol. 17, Morgan & Claypool Publishers, San Rafael; Marrero, M., Urbano, J., Sánchez-Cuadrado, S., Morato, J., and Gómez-Berbís, J.M. (2013), “Named Entity Recognition: fallacies, challenges and opportunities,” 
                        Computer Standards and Interfaces
                        , vol. 35, no. 5, pp. 482–489; Shahin, S. (2016), “When scale meets depth: Integrating Natural Language Processing and textual analysis for studying digital corpora,” 
                        Communication Methods and Measures
                        , vol. 10, no. 1, pp. 28–50; Srinivasa-Desikan, B. (2018), 
                        Natural Language Processing and Computational Linguistics: A Practical Guide to Text Analysis with Python, Gensim, spaCy, and Keras
                        , Packt Publishing, Birmingham and Mumbai; Ehrmann, M., Romanello, M., Flückiger, A. and Clematide, S. (2020), “Named Entity Recognition and linking on historical newspapers,” in Arampatzis, A. et al. (eds.), 
                        Experimental IR Meets Multilinguality, Multimodality, and Interaction
                        , Springer International, Cham, pp. 288–310.
                    
                    
                        We developed
                        Archiviz
                        for the Omeka Classic version; technical and user support for our plugin will be available in February 2023 in the
                        
                            Omeka plugin library
                        
                        . Additional availability is planned through github and a Docker container for ease of use.
                    
                    
                        The primary tools and plug-ins used to construct Archiviz include Harvard’s
                        
                            Elasticsearch
                        
                        Omeka plugin for enhanced search capabilities, Google 
                        
                            Tesseract
                        
                        for OCR, 
                        
                            spaCy
                        
                        ’s NLP functionality, particularly its NER functionality, with current development focusing on the integration of the Science Museum Group’s
                        
                            Heritage Connector
                        
                        for better entity identification, linking, and disambiguation. The graphing of this information is primarily performed with the force-directed graphing of
                        
                            d3.js
                        
                        .
                    
                    
                        For a similar application, see Tumbe, C. (2019), "Corpus linguistics, newspaper archives and historical research methods," 
                        Journal of Management History
                        , vol. 25, no. 4, pp. 533–549.
                    
                    
                         On the issues here, see Flinn, A., Stevens, M., and Shepherd, E. (2009), “Whose memories, whose archives? Independent community archives, autonomy and the mainstream,” 
                        Archival Science
                        , vol. 9, no. 71, pp. 71–86.
                    
                    
                        Wiriyathammabhum, P., Summers-Stay, D., Fermüller, C., and Aloimonos, Y. (2017), “Computer vision and Natural Language Processing: Recent approaches in multimedia and robotics,” 
                        ACM Computing Surveys
                        , vol. 49, no. 4, pp. 1–44.
                    
                
             -->
        
    


11728	2022	
        
            Reconstructing past population configurations is no easy task, even when detailed demographic information exists such as birth, death and marriage records (e.g. Bailey et al. 2020, Efremova 2016). Reconstruction is necessary to be able to disambiguate and link people mentioned across various historic sources. The best way to do it is by hand, but that is prohibitively labor intensive. For automatic methods, name and spelling variations and uncertain relations between mentioned names present large problems (see e.g. Idrissou et al. 2018). Missing information does, too: demographic records tend not to include information on migrations into or out of the city in question, in our case Amsterdam.
            Methods that are being tried tend to reconstruct individuals on the basis of detected name identifications and their relations to other mentioned names (e.g. Bloothooft et al 2015, Bailey et al. 2020). While this seems promising, it fails to include other information we may make use of: demographic and biological statistics (e.g. Störmer 2018, Alter and Clark 2010). Using the latter, we should be able to identify a person mentioned with one name in a certain source with the same person mentioned in another by only a nickname without being dependent on related names being mentioned as well.
            We explore employing a temporal, iterative agent interaction model, similar to those used in biological population evolution models (e.g. Toni and Stumpf, 2010, Marchetti et al 2017,Levin et al 1997). We set up a Python actor model using 
                
                    MESA
                 (Kazil et al. 2020) that can iterate over time and has access to all birth, death and marriage ‘events’ that are known from the archival records. We iterate over a 50-year sample in the dataset (for Amsterdam data is available from ca. 1600 through 1940, we use 1750-1800 as a test case).
            
            Each year, all names mentioned in ‘events’ in that year become actors (in the model sense). So if there is a birth record, the name of the child will be ‘born’ in that year, and the parents become actors that have children. The actors then decide how they ended up existing in that year: are they representations of actors that already exist in the current model because of a mention in a previous year? Are they new migrations into the city? Are they births of new citizens, or perhaps deaths of previously known or unknown actors? Do they (re)marry, should they have been married before that year? Actors make these decisions based on the event, their existing relations within the currently running model, and probability.
            The decisions the actors make are based on statistics of the population in question combined with biological statistics and limitations. For example, it is very unlikely that people in the dataset live longer than 130 years, but very young children also die often, so mortality probability changes over an actor’s lifespan. But we also know that mortality rates are not constant over the whole timespan – certain years may have seen disasters, increasing mortality (see e.g. Aberth 2013, Jensen 2019), and others may have seen immigration waves, increasing population size without a higher birth rate. Other basic probabilities also affect the model: e.g. women rarely have children before age 11 or after age 60, and unmarried women are more likely to be childless than married ones.
            Running this model we end up with a temporally changing network that we can evaluate in different ways in order to approximate historic reality. First, we can use general demographic knowledge to evaluate it, for example known population sizes (e.g. Lourens and Lucassen 1997, Paping 2014, Frijhoff et al 2006 etc.), mortality rates, birth rate estimations etc. (e.g. Heathcote 2001). Secondly, we can use network properties (e.g. population spikiness, average number of children, average age etc.) to evaluate. A third way is to deduce how ‘solid’ each actor’s life is, recording e.g. whether they have parents, whether the parents have the same last name as themselves, whether they have any relations to others in the model at all, whether they have only one or multiple mentions in the records etc. A fourth way is to compare them to known prosopographic data (we use ECARTICO). We test each of these methods and combine them into a meta-evaluation method.
            Re-running the model many times, resulting in different models, we evaluate each in order to select the one best approximating reality and will discuss our findings.
        
        
            
                
                    Bibliography
                    
                        Aberth, John (2013): 
                        From the Brink of the Apocalypse: Confronting Famine, War, Plague and Death in the Later Middle Ages. 2nd Edition 2013, First Published 2010
                    
                    
                        Alter, G., & Clark, G. (2010). 
                        The demographic transition and human capital. In S. Broadberry & K. O'Rourke: The Cambridge Economic History of Modern Europe, pp. 43-69. Cambridge: Cambridge University Press. doi:10.1017/CBO9780511794834.004
                    
                    
                        Bayley, Martha J., Connor Cole, Morgan Henderson and Catherine Massey (2020): 
                        How Well Do Automated Linking Methods Perform? Lessons from US Historical Data. Journal of Economic Literature Vol.. 58, NO. 4, December 2020 (pp. 997-1044)
                    
                    
                        Bloothooft, Gerrit, Peter Christen, Kees Mandemakers and Marijn Schraagen (2015): 
                        Population Reconstruction. Springer
                    
                    
                        Efremova, I. (2016). 
                        Mining social structures from genealogical data. Technische Universiteit Eindhoven.
                    
                    
                        Frijhoff, W., M. Prak and M. Hel (2006):
                         Geschiedenis van Amsterdam, II-2, Zelfbewuste stadstaat 1650-1813. Bijdragen en mededelingen betreffende de geschiedenis der Nederlanden 121(3):500 DOI:10.18352/bmgn-lchr.6475
                    
                    
                        Heathcote C., Higgins T. (2001) A Regression Model of Mortality, with Application to the Netherlands. In: Tabeau E., van den Berg Jeths A., Heathcote C. (eds) Forecasting Mortality in Developed Countries. European Studies of Population, vol 9. Springer, Dordrecht. https://doi.org/10.1007/0-306-47562-6_3
                    
                    
                        Idrissou, A., Zamborlini, V., Latronico, C., van Harmelen, F., & van den Heuvel, C. M. J. M. (2018). 
                        Amsterdamers from the Golden Age to the Information Age via Lenticular Lenses: Short paper.
                    
                    
                        Jensen, L.E. (2019): 
                        'Disaster upon Disaster Inflicted on the Dutch'. Singing about Disasters in the Netherlands, 1600-1900. Bijdragen en Mededelingen Betreffende de Geschiedenis der Nederlanden, 134, 2, (2019), pp. 45-70 https://doi.org/10.18352/bmgn-lchr.10449
                    
                    
                        Kazil, Jackie, David Masad, and Andrew Crooks (2020): 
                        Utilizing Python for Agent-Based Modeling: The Mesa Framework. In Robert Thomson, Halil Bisgin, Cristopher Dancy,Ayaz Hyder and Muhammad Hussain: Social, Cultural, and Behavioral Modeling. Springer International Publishing
                    
                    
                        Levin, Simon A., Bryan Grenfell, Alan Hastings, Alan S. Perelson (1997): 
                        Mathematical and Computational Challenges in Population Biology and Ecosystems Science. Science 1997 Vol 275, Issue 5298 pp. 334-343 DOI: 10.1126/science.275.5298.334
                    
                    
                        Lourens, Piet, and Jan Lucassen (1997): 
                        Inwonertallen van Nederlandse steden ca. 1300-1800. Amsterdam: Vereniging Het Nederlandsch Economisch-Historisch Archief..
                    
                    
                        Marchetti, Luca, Corrado Priami and Vo Hong Tanh (2017): 
                        Simulation Algorithms for Computational Systems Biology. 2017 Springer 331963111X 
                    
                    
                        Newton, G., & Bennett, R. (2020). 
                        Record-linkage of entrepreneurs in the England and Wales Censuses 1851-91 using BBCE and I-CeM. https://doi.org/10.17863/CAM.50178
                    
                    
                        Paping, Richard (2014): 
                        General Dutch Population development 1400-1850. 1st ESHD conference, Italy
                    
                    
                        Störmer, Charlotte, Corry Gellatly, Anita Boele, and Tine De Moor (2017): 
                        Long-Term Trends in Marriage Timing and the Impact of Migration, the Netherlands (1650-1899). Historical Life Course Studies 6 (December):40-68. https://doi.org/10.51964/hlcs9327.
                    
                    
                        Toni, Tine, Stumpf, Michael P.H (2010): 
                        Simulation-based model selection for dynamical systems in systems and population biology. Bioinformatics, Volume 26, Issue 1, 1 January 2010, Pages 104–110, https://doi.org/10.1093/bioinformatics/btp619.
                    
                
            
        
    


11742	2022	
        
            Gale Digital Scholar Lab was developed in 2018 to fulfil requests for a platform for text mining primary source documents without necessarily having to learn to code in Python or R. Based on beta-testing user interviews, it was determined that some of the most significant barriers to entry into the field of text-based digital humanities data mining include not knowing how or where to start in order to build a DH project, not having time to gather a significant data set or to clean and organize data for analysis, and having limited institutional infrastructure and support for projects that include text mining methodologies. In designing the DS Lab, the goal was to provide a scaffolded experience for users new to the field of digital humanities, while offering options for extensibility for researchers with established projects. This included providing pathways for research, teaching and learning by both students, faculty, and librarians.
            The DS Lab has been iteratively developed since its first release, with updates including tool enhancements and support for pedagogical use of the platform, and more recently a platform migration, workflow tweaks and improved accessibility. The DS Lab integrates six GUI-based tools for conducting text analysis of primary source archives and user-uploaded plaintext documents. These tools comprise Named Entity Recognition, Sentiment Analysis, Ngrams, Parts of Speech Tagging, Topic Modeling and Clustering. Recognizing that quality of OCR text is key in achieving meaningful analysis outputs, the DS Lab also presents options for text cleaning as part of the curation process. Import and export of text and metadata are also supported.
            To orient users who are new to the field of DH to the workflow and outcomes, the platform incorporates an extensive Learning Center with contextual help documentation including brief recorded videos, images, text, and sample projects. Similarly, for teachers who are looking for ideas or additional support in the platform, there are draft syllabi, outline learning objectives, and downloadable project outlines.
            This 10-minute talk will focus on describing the process and challenges of developing the DS Lab interface, meeting the often-competing demands of balancing developer time, the scope of individual project sprints, and projected cost. Consideration will be given to the workflows which were successful as well as those that needed to be adapted or scrapped altogether. It will discuss using personas to design the features and functionality in the platform, and the advantages and drawbacks of doing so. The development of the Learning Centre is a case in point, since its development drew on a range of internal and external expertise such as academic advisors, curriculum developers, and UX designers as well as in-house software and content engineers, metadata and content architects and the product and archives team. This collaborative undertaking took considerable management to balance expectations and outcomes, and to ensure that communication flowed clearly. These considerations are not unique to the Gale Digital Scholar Lab development project but can be extrapolated to other similar DH projects. The intent of the talk is to highlight how the lessons learned by the Gale development team and external stakeholders can be used to provide guidance for others.
        
        
            
                
                    Bibliography
                    Besette, Lee. (2012). “Challenges in Digital Humanities.” 
                        Inside Higher Ed.
                        https://www.insidehighered.com/blogs/college-ready-writing/challenges-digital-humanities. Accessed 16 March 2022.
                    Campese, C., Thiago Bertolini, d. S., Lorena Pereira, d. C., & Janaina Mascarenhas, H. C. 
                        (2019). 
                        User stories method and assistive technology product development: A new approach to requirements elicitation. Cambridge: Cambridge University Press. doi:http://dx.doi.org/10.1017/dsi.2019.385
                    
                    Coutu, Diane. (2015). “Why Teams Don’t Work.” 
                        Harvard Business Review. https://hbr.org/2009/05/why-teams-dont-work. Accessed 25 Apr. 2022.
                    
                    Currier, Brent D. (2017). “They Think all of this is new: Leveraging Librarians’ Project Management Skills for the Digital Humanities.” 
                        College & Undergraduate Libraries 24, 270-289. 
                        https://doi.org/10.1080/10691316.2017.1347541
                    
                    Dingsøyr, Torgeir, et al. (2018). “Coordinating Knowledge Work in Multiteam Programs: Findings From a Large-Scale Agile Development Program.” 
                        Project Management Journal, vol. 49, no. 6. 64–77, doi:10.1177/8756972818798980.
                    
                    Gratton, Lynda, and Tamara J. Erickson. (2016). “Eight Ways to Build Collaborative Teams.” 
                        Harvard Business Review. hbr.org/2007/11/eight-ways-to-build-collaborative-teams. Accessed 25 Apr. 2022.
                    
                    Jenkins, Nick. (2008) 
                        A Software Testing Primer An Introduction to Software Testing. San Francisco: Creative Commons.
                    
                    Nielsen, Jakob. (nd). “Why You Only Need to Test with 5 Users.” 
                        Nielsen Norman Group. 
                        www.nngroup.com, 
                        https://www.nngroup.com/articles/why-you-only-need-to-test-with-5-users/. Accessed 30 Apr. 2022.
                    
                    Siemens, L. (2011). The Balance between On-line and In-person Interactions: Methods for the Development of Digital Humanities Collaboration. 
                        Digital Studies/le Champ Numérique, 
                        2(1). DOI: 
                        http://doi.org/10.16995/dscn.259. Accessed 16 March 2022.
                    
                
            
        
    


11743	2022	
        
            
                1. Research question
            
            In Computational Literary Studies texts are typically pre-processed with Natural Language Processing (NLP) tools. However, due to historical and/or aesthetic characteristics, literary texts sometimes deviate notably from the data the tools are trained on. Due to this difference in domain, the performance of the tools drops (Scheible et al., 2011; Rayson et al., 2007; Herrmann, 2018; Bamman, 2020). Instead of considering this to be a problem, the ‘erroneousness’ of the tools could provide a computational understanding of the ‘deviance of literary texts’; produced errors might reveal something about the characteristics of literature.
            In the following, we report on a 
                Tool Misuse experiment on German lyric poetry – a genre that is usually associated with a high degree of deviance (Müller-Zettelmann, 2000: 100; Zymner, 2019: 29–30) – in which we develop a pipeline that provokes tokenization, lemmatization and POS tagging 'errors' of NLP tools and typologises these 'errors' in a rule-based way. 
            
            
            2. Operationalization
         
            
                
                Pipeline for error typing of the corpora.
            
            Since gold standard annotations are not available for our scenario, we base our evaluation on the assumption that correctly produced lemmas can be found in dictionaries of German language. Based on the 
                TextGridRepository, we build a canon-based corpus of 'prototypical' German-language poetry comprising 5,144 poems. For comparison, we use a prose corpus of 100 German-language novels from the 19th century, compiled from the TextGridRepository and 
                Project Gutenberg. As dictionaries we use ‘GermaNet‘ (Hamp & Feldweg, 1997; Henrich & Hinrichs, 2010) and the ‘Digitales Wörterbuch der deutschen Sprache‘ (Klein & Geyken, 2010). To ensure that the resulting errors are not tagger specific, we use several NLP tools for tokenization, lemmatization and POS tagging of the corpora (fig. 01) and consider all content word types as potential errors that are lemmatized by at least two tools and for which none of the produced lemmas are found in the dictionaries (fig. 02, column "pFail").
            
            Our 'error pipeline' prefers recall over precision, thus it produces only circumstantial evidence of potential errors. A larger number of false positives is to be expected, because we process out-of-vocabulary words of the dictionaries.
            
                
                    
                        
                        Poetry
                        Poetry
                        Prose
                        Prose
                    
                    
                        
                        all
                        pFail
                        All
                        pFail
                    
                    
                        Types
                        70,422
                        24,244
                        263,042
                        115,785
                    
                
                Number of word types (NOUN, VERB, ADJECTIVE) for the entire corpus ("all") and for the sets with potential errors ("pFail").
            
            
                3. Analysis
            
            Based on manual inspections of the pFail set, we postulate 13 error types described in figure 03. For each type we formulate a rule
                
                     For the rules see: 
                        https://gitup.uni-potsdam.de/sluytergaeth/poetry_as_error
                    
                 which is then applied to the pFail set following the order of the error types listed below. Multiple typings are not possible.
            
            
                
                Description of error types.
            
            
            4. Results
         
            
                
                Relative frequency for the types of potential errors for the two pFail sets.
            
            53.33 % of the word types in the pFail set for poetry and 59.88 % of the word types in the pFail set for prose are identified. PUNC and SHORT are predominantly sub-word level characters, mostly noise which appears to a comparable extent in poetry and prose. ORTH_SZ reflects the effect of Historical Orthography which a normalisation step could remedy.
            The ten remaining types can be combined into three groups:
            
                COMP_DASH, COMP, PART_ ADJECTIVE, PREFIXED gather 
                    Creative Lexis, i.e. word formation mechanisms (composition, derivation); these are often out-of-vocabulary words and therefore pipeline errors, not tool errors. In poetry, 45.25 % of the "pFail" set can be assigned to this group, in prose 57.09 %. 
                
                As expected, the pipeline produces a higher error rate for poetry (0.62 %) than for prose (0.02 %) for ORTH_UPPER, which identifies a characteristic of 
                    Lyric Typography (capitalizing first letters in lines). 
                
                The error rate of 
                    Prosodic Deformation consisting of ELISION_APO, ELISION_SIMPLE, ELISION_END, EPITHESIS and CONTRACT is also higher for poetry than for prose (6.62 % compared to 1.93 %). We assume that the deformations are due to the addition or deletion of vowels for metric reasons.
                
            
            5. Outlook
            Our pipeline identifies 
                Prosodic Deformation, 
                Lyric Typography and 
                Creative Lexis as typical sources of error when processing poetry with NLP tools. However, our pipeline needs to be optimized: too many potential errors are, as in the case of 
                Creative Lexis, in fact not tool errors but pipeline errors. Additionally, our rule-based typology is only able to describe 53.33 % of the pFail set. This reveals two areas for follow-up research: the pipeline could be improved on to decrease the number of pipeline errors and the rule-based typologisation procedure could be optimized against our baseline.
            
        
        
            
                
                    Bibliography
                    
                        Bamman, D. (2020). LitBank: Born-Literary Natural Language Processing. [Preprint]. https://people.ischool.berkeley.edu/~dbamman/pubs/pdf/Bamman_DH_Debates_CompHum.pdf [Last accessed November 16, 2021].
                    
                    
                        Braam, H. (2019). 
                        Die berühmtesten deutschen Gedichte. Auf der Grundlage von 300 Gedichtsammlungen. Stuttgart: 2. Aufl., Kröner.
                    
                    
                        Hamp, B. and Feldweg, H. (1997). GermaNet - a Lexical-Semantic Net for German. In 
                        Proceedings of the ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications. Madrid, Spain, pp. 9–15. https://aclanthology.org/W97-0802.pdf [Last accessed November 16, 2021]. 
                    
                    
                        Henrich, V. and Hinrichs, E. (2010). GernEdiT - The GermaNet Editing Tool. In 
                        Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC 2010), Valletta, Malta, pp. 2228-35. http://www.lrec-conf.org/proceedings/lrec2010/pdf/264_Paper.pdf [Last accessed November 16, 2021].
                    
                    
                        Herrmann, J. B. (2018). Praktische Tagger-Kritik. Zur Evaluation des PoS-Tagging des Deutschen Textarchivs. In 
                        DHd2018: Kritik der digitalen Vernunft. Book of Abstracts. Cologne, Germany, pp. 287-90. https://zenodo.org/record/3684897#.YO_x1W5CTOQ [Last accessed November 16, 2021].
                    
                    
                        Honnibal, M. et al. (2020). spaCy: Industrial-strength Natural Language Processing in Python. Zenodo. https://doi.org/10.5281/zenodo.1212303 [Last accessed November 16, 2021]. 
                    
                    
                        Klein, W. and Geyken, A. (2010). Das ‘Digitale Wörterbuch der Deutschen Sprache DWDS’, in: Lexicographica 26: 79–96.
                    
                    
                        Müller-Zettelmann, E. (2000). 
                        Lyrik und Metalyrik. Theorie einer Gattung und ihrer Selbstbespiegelung anhand von Beispielen aus der englisch- und deutschsprachigen Dichtkunst. Heidelberg, Germany, Winter.
                    
                    
                        Qi, P. et al. (2018). Universal dependency parsing from scratch, in: 
                        Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, Brussels, Belgium, pp. 160-70.
                    
                    
                        Rayson, P. et al. (2007). Tagging the bard: Evaluating the accuracy of a modern POS tagger on Early Modern English corpora. In 
                        Proceedings of Corpus Linguistics (CL2007). https://eprints.lancs.ac.uk/id/eprint/13011/1/192_Paper.pdf [Last accessed November 16, 2021]. 
                    
                    
                        Schmid, H. (1994). Probabilistic part-of speech Tagging using decision trees. In 
                        Proceedings of International Conference on New Methods in Language Processing, Manchester, UK, pp. 154-62.
                    
                    
                        Schmid, H. (1995). Improvements in Part-of-Speech Tagging with an Application to German. In 
                        Proceedings of the ACL SIGDAT-Workshop, Dublin, Ireland, 13-25. https://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/data/tree-tagger2.pdf [Last accessed November 16, 2021]. 
                    
                    
                        Schmid, H. (2019). Deep learning-based morphological taggers and lemmatizers for annotating historical texts. 
                        In Proceedings of the 3rd international conference on digital access to textual cultural heritage, Brussels, Belgium, 133-37.
                    
                    
                        Scheible, S. et al. (2011). A gold standard corpus of Early Modern German. In: 
                        Proceedings of the 5th Linguistic Annotation Workshop, pp. 124–28. https://dl.acm.org/doi/abs/10.5555/2018966.2018981 [Last accessed November 16, 2021]. 
                    
                    
                        Zymner, R. (2019). Begriffe der Lyrikologie. In: Hildebrandt, Claudia et al. (eds.) Lyrisches Ich, Textsubjekt, Sprecher? (= Grundfragen der Lyrikologie, Bd. 1). Berlin, Germany: De Gruyter 25–50.
                    
                
            
        
    


11747	2022	
        
            
                Abstract
                This workshop will introduce participants to the HathiTrust Research Center’s Extracted Features Dataset, and demo new data fields and functionality introduced in the latest version, 2.0. Generated from the over 17 million volumes (over 60% still in copyright) in the HathiTrust Digital Library, the EF 2.0 Dataset supports text and data mining in this corpus while still being distributed as open, restriction-free data. This tutorial will introduce the EF 2.0 Dataset, the key concepts behind its creation, and hands-on research use cases for the Dataset using IPython notebooks. 
                Index Terms—digital libraries, text and data mining, HathiTrust, HTRC Extracted Features Dataset, collections as data 
            
            
                Introduction
                Digital libraries are in a state of flux: as institutional collections have grown, their purposes have been transformed from “preservation and access” into “big data” repositories. “The Santa Barbara Statement on Collections as Data” identifies foundational principles for this new focus of digital libraries, encouraging library providers and researchers to view these vast digital collections as sources of data for computational research (Santa Barbara Statement on Collections as Data, 2019). This approach opens new possibilities of inquiry and discovery, but also carries with it the weight of copyright limitations and implications; questions of bias, completeness and representation in data; and additional requirements for new skills, sophisticated infrastructures, and increased funding from stakeholders at all levels.
                Having foreseen some of these issues, the HathiTrust Research Center (HTRC) developed an approach to “non-consumptive research,” a framework that supports computational research of digital items in the HathiTrust Digital Library (HTDL) while complying with copyright limitations on data access. Similarly, through its Extracted Features (EF) Dataset, HTRC supports an approach to computational research that seeks to minimize skill and infrastructure requirements while also aligning with the broader considerations mentioned above. First released in 2015 with version 0.2, the EF Dataset blends MARC-derived, volume-level metadata with computationally generated, page-level metadata and data (Capitanu et al., 2015). The EF Dataset is open (available under a Creative Commons Attribution 4.0 International License), and includes data for each volume in the HathiTrust Digital Library (HTDL) in a bag-of-words format, supporting a multitude of computational use cases.
                This workshop will focus on introducing the context and application of the EF Dataset version 2.0, for the first time published in a linked-data-compliant format that includes new and updated data fields, a revised structure, and meaningful uniform resource identifiers (URIs) (Jett, et al., 2020). Attendees will learn about the data from which the EF Dataset is generated, the motivation for the dataset’s creation, its structure and format, and its potential applications and limitations. Additionally, workshop attendees will work hands-on programmatically with EF 2.0 data using IPython notebooks, and will use the HTRC FeatureReader Python library, developed specifically to facilitate use of the EF Dataset (Organisciak and Capitanu, 2016).
            
            
                Workshop Description
                
                    Overview 
                    Section In this three-hour workshop, attendees will first be introduced to HathiTrust, the data of the HathiTrust Digital Library, and the HathiTrust Research Center, followed by context and motivation of the Extracted Features Dataset, the specifics features (or “fields”) included in the dataset, its format, and a discussion of its potential uses and limitations. Finally, attendees will have a chance to work directly with Python code to analyze HTRC EF data and to use the HTRC FeatureReader Python library. Hands-on work will also present new linked data fields and their potential application for research with the EF Dataset. A more detailed breakdown of the workshops modules follows:
                    Section 1: Intro to HathiTrust Digital Library and HTRC
                    
                        What is HathiTrust?
                        What is/isn’t the HathiTrust Digital Library?
                        What is the HathiTrust Research Center?
                    
                    Section 2: Context and motivation for the HTRC EF Dataset
                    
                        Non-consumptive research
                        What is in the data?
                        Data models and analysis techniques
                    
                    Section 3: Ethical considerations of text datasets
                    
                        Bias in libraries, datasets, data, and algorithms
                    
                    Section 4: Getting and Exploring EF data
                    
                        Hands on with EF data, Python notebooks and the HTRC FeatureReader library
                    
                    The workshop will be a mix of presentation, discussion, and hands-on activity, with an emphasis on open discussion. Our discussion on the ethics of dataset construction, big data and algorithmic analysis will highlight work from Katherine Bode (2020), Catherine D’Ignazio and Lauren Klein (2020), Mimi Ọnuọha (2021) and Roopika Risam (2019).
                
                
                    Audience 
                    The workshop is open to all, and is targeted at digital library and digital humanities scholars of all levels, library and information professionals, and anyone interested in computational research using HathiTrust Digital Library data. Attendees will develop an understanding of HathiTrust Digital Library, the HathiTrust Research Center’s services, tools and platforms, selected ethical issues associated with digital libraries, dataset and data analysis, and the Extracted Features model, its suitability for various text and data mining applications, and hands-on familiarity with using the dataset for exploratory data analysis. The workshop can accommodate 10 - 40 attendees.
                    This workshop builds on years of HTRC workshops ranging from general introductions to text and data mining to more advanced work with HTRC’s tools, services, and data. This workshop will provide an in-depth focus on the Extracted Features Dataset version 2.0, demo new ways for exploring and using the dataset and also engage with the ethics of datasets, data and algorithms.
                    The general objectives of this workshop are to introduce the HathiTrust context, motivation for, and development and release of the Extracted Features Dataset, and to familiarize participants with the data format, its potential applications, and the latest additions in the 2.0 version. Topics of instruction and potential discussion will include:
                    • How does the Extracted Features Dataset help make the HTDL more accessible for text and data mining?
                    • What is the EF Dataset model and the structure of its files?
                    • What research use cases or exploratory data analysis can be supported using HTRC EF data, especially using the new features of the 2.0 dataset”? 
                    • What tools are available for working with EF data, and hands-on experience using them in Python notebooks? 
                    Our goal is for attendees to leave this workshop with a general understanding of the utility of derived datasets and to be comfortable beginning exploratory data analysis using the EF Dataset. 
                    In addition to more HTRC-centric learning objectives, hands-on activities will have added bonuses of an introduction to common cultural analytics tasks in Python, and the associated software libraries used for such tasks, including Pandas, NLTK and Gensim.
                
                
                    Instructor Biographical Information 
                    Ryan Dubnicek is a Digital Humanities Specialist with HTRC, where he works on external and internal research support and outreach and education. He has a Master of Science in Library and Information Science from the University of Illinois at Urbana-Champaign. 
                    Jennifer Christie is an Associate UX Specialist at the HathiTrust Research Center. She is interested in user-centered interaction design and front-end web development. Her research is grounded in qualitative and quantitative assessments of HTRC’s user base, as well as assisting with outreach and education efforts to engage with HTRC’s growing user community.
                    The remaining listed authors have contributed to the development of this curriculum and its associated instructional materials and concepts, but will not be part of the instructional team.
                
            
            
                Acknowledgements
                The curriculum presented in this workshop was developed with support from HathiTrust, University of Illinois, Indiana University, and the Institute of Museum and Library Services, award number RE-00-15-0112-15. Additionally, former Associate Director of Outreach and Education with HTRC, Eleanor Koehl, contributed heavily to the development and design of the teaching materials and activities.
            
        
        
            
                
                    Bibliography
                    Bode, K. (2020). Why You Can’t Model Away Bias. Modern Language Quarterly, 81(1): 95–124 doi:
                        10.1215/00267929-7933102.
                    
                    D’Ignazio, C. and Klein, L. (2020). ‘What Gets Counted Counts’. Data Feminism 
                        https://data-feminism.mitpress.mit.edu/pub/h1w0nbqp/release/3 (accessed 10 December 2021).
                    
                    Jett, J., Capitanu, B., Kudeki, D., Cole, T., Hu, Y., Organisciak, P., Underwood, T., Dickson Koehl, E., Dubnicek, R. and Downie, J. S. (2020). The HathiTrust Research Center Extracted Features Dataset (2.0) HathiTrust Research Center doi:
                        10.13012/R2TE-C227. 
                        https://wiki.htrc.illinois.edu/pages/viewpage.action?pageId=79069329 (accessed 2 June 2022).
                    
                    Organisciak, P. and Capitanu, B. (2016). Text Mining in Python through the HTRC Feature Reader. Programming Historian 
                        https://programminghistorian.org/en/lessons/text-mining-with-extracted-features (accessed 2 June 2022).
                    
                    Risam, R. (2019). The Stakes of Postcolonial Digital Humanities. New Digital Worlds. (Postcolonial Digital Humanities in Theory, Praxis, and Pedagogy). Northwestern University Press, pp. 23–46 doi:
                        10.2307/j.ctv7tq4hg.5. 
                        http://www.jstor.org/stable/j.ctv7tq4hg.5 (accessed 10 December 2021).
                    
                    Partners Always Already Computational - Collections as Data 
                        https://collectionsasdata.github.io/partners/ (accessed 30 November 2021).
                    
                    The Library of Missing Datasets — MIMI ONUOHA MIMI  ONUOHA 
                        https://mimionuoha.com/the-library-of-missing-datasets (accessed 10 December 2021).
                    
                    The Santa Barbara Statement on Collections as Data Always Already Computational - Collections as Data 
                        https://collectionsasdata.github.io/statement/ (accessed 30 November 2021).
                    
                
            
        
    


11762	2022	
        
            
                Archive of the Digital Present for Online Literary Performance in Canada (COVID-19 Pandemic Period) is a research and development project that arises out of the need to address foundational, practical and theoretical research questions about the impact of the recent (and ongoing) COVID-19 pandemic, and attendant social disruptions and restrictions, upon literary communities in Canada through the study of organised literary events as they have occurred online since March 2020.
            
            The papers that constitute this panel focus on the design and development work pursued in building a searchable, open access database and directory – The Archive of the Digital Present (ADP) – to allow scholars, literary practitioners, and the public to gain knowledge about the nature and significance of literary events (online, hybrid, and in-person) that have occurred during the pandemic period, through the collection and structuring of metadata, and, in some cases, with direction to audiovisual (AV) documentation of the events themselves as they were held using platforms such as Zoom and YouTube.
            Our papers explain key facets of development by presenting approaches to (1) data collection and structuring, (2) stack development, (3) data visualisation, and (4) front end design, that have emerged through the process of community and user-oriented design research and development used to create the ADP.
            
                Finding and Structuring Metadata about Pandemic Literary Events 
                (Jason Camlot)
            
            Public readings represent a significant form of literary communication, dissemination, circulation and community-building. The study of literary performances, events and activities through audiovisual media documentation, digital images (posters) and textual records, raises important new questions about literary work as it acts 
                in situ among artists and audiences (Camlot, Fong and Shearer).  Such materials and the events they document reveal unique traces of sociality and affective response in literary exchange, foreground tonal and performative aspects of cultural transmission, document formations of literary community in action, and highlight the mediated nature of such events as they first occurred and as we subsequently access them through archived recordings. Focusing on the presentation of digital documentation and records of online events of the COVID-19 pandemic period, the ADP is designed to help us understand the impact of pandemic disruptions on literary communities in Canada. 
            
            The ADP project necessarily began with questions about the data we were seeking to collect.  In February 2021 we performed a preliminary analysis of online and social media postings for listings of literary events hosted in Canada.  This revealed 77 discrete organisers of over one thousand (1,011, to be exact) literary events between 20 March 2020 - 31 December 2020.  This list served as the starting point for an expanded catalogue of events, and for team discussions about the nature and number of metadata fields we would use. We proceeded by adapting extant categories of the SpokenWeb metadata schema that has been designed for the description of historical literary audio recordings. This allowed us to repurpose the backend of the Swallow Metadata Ingest System (
                Swallow), built for metadata management of historical research collections, through the development of a crosswalk that best serves the goals of data collection for ADP. Storing the metadata as unstructured JSON, 
                Swallow makes it possible to quickly generate and modify a cataloguing interface, and changes in metadata schemas, and this flexibility has proven useful for the iterative design and feedback process we have pursued in ADP UX development. Data fields we have shaped for this project include categories related to Title, Creator/Contributor, Language, Production Context, Genre, Duration, Date, Location, Online Platform, and Contents, among others.
            
            This section of our presentation presents our ongoing methods of discovering events to be included in the ADP database, explains the rationale of our selection of metadata categories and our approach to structuring those fields, and raises some of the philosophical and ontological questions that have arisen in the process of abstracting the complex and mediated literary activities of the pandemic period into categories of searchable data. Our methods of data collection extended beyond the analysis of social media notices of events to the identification and research of key organisations involved in hosting events, and to crowdsourcing calls within diverse literary communities.  Our selection of the specific fields identified were based on feedback from researchers and practitioners about the kinds of information they would seek from a database such as this, and on our goals in data visualisation, discussed below. Philosophical and ontological questions raised by our data determination and collection process included questions about the nature of an event, the roles of participants in relation to categories such as creator and contributor, and generic categories that include or preclude the overarching category of the “literary” event, and the status of such events as entities within a data structure.
            
                Stack Development and the Rationale of a Headless CMS 
                (Ben Joseph, Francisco Berrizbeitia)
            
            In this section we present the software stack chosen to develop the solution and the rationale behind it. The right technical stack is, to a great extent, the key to a project’s success, while the wrong choice of web application development technologies may be a reason for failure. Our stack had to manage the trade-offs for processing heavy loads and maintaining a low latency with high responsiveness. 
            The data was collected using 
                Swallow (Camlot et al. 2020), an open-source metadata management system developed by the SpokenWeb partnership. Interacting with this backend system, cataloguers collected and compiled the information of the different events. This corpus of data is then exported as whole and ingested by the Strapi headless content management system (
                https://strapi.io/) as shown in 
                Figure 1. MeiliSearch (https://www.meilisearch.com) provides advanced search functionalities and Strapi allows for digital asset management while providing a backend to the presentation layer. The presentation layer was developed using AngularJS (
                https://angularjs.org/).  The component-based structure of Angular makes specific sections of code highly reusable across our application and for future frontend developments. The AngularJS layer interacts with the data sources via a GraphQL (https://graphql.org/) thus ensuring data consistency.
            
            
                
            
            
                Figure 1
                . Software stack schema and information flow. 
            
            
                Visualizing Time, Place and Relations of Literary Activity in a Pandemic 
                (Tomasz Neugebauer, Sukesh Gandham)
            
            A user survey identified the host location of the event as one of the more important elements of event description, along with event type, contributors, links to recordings, and the titles of texts read.  The most important metadata elements for navigation of content were identified as: names of participants, names of organizers and event, titles of texts read/performed/discussed, and date/time of event. This lead to an initial front page design for the ADP site that included three visualizations of the data: A) Timeline: a spiral histogram (Condegram) showing the number of events that took place on specific dates during the pandemic period B) Connections: a network graph showing adjacency relations between contributors and events, using hierarchical edge bundling based on the events’ organizations, and C) Places: geographical map showing number of events that took place in each of the events’ host cities.  These visualizations touched on all the important metadata elements identified in the user surveys, except for the titles of texts read/performed.  
            During the user-centered design process, a usability session evaluating some proposed wireframes for the site with scholars from the Spokenweb network, event organizers, practitioners and students confirmed that there is interest and enthusiasm for the data visualizations on the interface, especially for any interactive functionalities of these.  To keep the visualizations interactive, we decided to treat them as tools that navigate into the data.  For example, clicking on a name of a contributor or event in the edge bundling visualization leads the user to the browse view of the search results in the dataset for that name.  Similarly, clicking on a bar for a specific date on the condegram, or the city name on the map visualization, leads the user to the browse view of events that took place on that date or that were hosted in that city.  
            For the implementation of these visualizations, keeping complexity in check, we wanted the same code base for all three of them.  We also wanted a solution that is based on open-source code that is free from license restrictions on usage and sharing, which eliminated solutions such as amCharts (
                https://www.amcharts.com/). We chose the D3.js (
                https://d3js.org/) JavaScript library as our code base to generate Scalable Vector Graphics (SVG) that we present with custom HTML5 and Cascading Style Sheets (CSS). We added the Bootstrap library (
                https://getbootstrap.com/docs/3.4/) for responsive functionality. The D3 Gallery (
                https://observablehq.com/@d3/gallery) served as an excellent starting place for choosing the closest existing examples to build on.  We used Python to transform the ADP JSON data from our Swallow Metadata Management System into data shapes that are required for the three visualizations using D3.   
            
            
                User-Oriented Design and Aesthetics for a Pandemic-Period Website 
                (Alexandre Bustamante)
            
            The avenue for building the front-end of ADP was, from an early phase, directed as a user-centred project. After the design was initiated, discussions evolved for experimenting with the lines of a Participatory Design (PD) approach. A user-centred methodology is considered executed "on behalf of users" while PD approaches design "with the users" (Spinuzzi 2005, 165), We chose this option to have ensure greater involvement of the stakeholders of the ADP in all development phases of the front-end design. We organized a series of workshops, PD activities and user-experience research surveys distributed to invited participants and the direct team of the front-end development, with the project designer playing a facilitating rather than individual authorial role. The workshops adopted established design methods (Martin 2012) such as personas and user journeys for delineating stakeholder needs. Card sorting, tree-testing and first-click tests were used to create and confirm an informational architecture (figure 2). We adapted to the added challenge of conducting workshops remotely due to the pandemic. This led to the exploration of available tools, such as 
                Miro Boards (
                https://miro.com), 
                Google Jamboards (
                https://jamboard.google.com/) and 
                Optimal Workshop (
                https://www.optimalworkshop.com). 
            
            
                
            
            
                Figure 2 - Results from an online workshop, with automated interpretation of a card sorting exercise compiled by the user-experience tool 
                Optimal Workshop
                . Screenshot by the authors.
            
            The initial outcome of the PD process was the creation of a three-level information architecture for the prototype of the front-end design: starting at the first level with an overall glimpse at the directory of events on the home page which presents content through visualizations and browsing functions. From the homepage, the user moves to a second level where content is presented on either a dashboard or in the form of lists in pre-established categories, displaying more information and details and introducing different filters for the content. Finally, the third and final level of the information architecture is detailed access to the metadata, which can be visualized, exported or shared. The front-end design allows a search function to be performed at all three levels, for direct access to the directory of events.
            
                
            
            Figure 3 - Latest front-end design for the ADP website. A glimpse of the landing page of the Archive. Screenshot by the authors.
            Once the information architecture was established, the participatory approach to design was expanded to inform the final look of the prototype (figure 3). Team members and stakeholders were invited to guide the visual design by reflecting on their perception of the pandemic and providing keywords that capture the experience of living through this period. The final prototype was then designed to reflect on these shared perceptions, drawing from participant’s contributions to inspire a mood board for the design work, which directed the design decisions to achieve a final result that aims to be characteristic of its particular time.
        
        
            
                
                    Bibliography
                    
                        AngularJS - HTML enhanced for web apps! (n.d.). AngularJS. 
                        
                            https://github.com/angular/angular.js
                         (accessed 12 September 2021).
                    
                    
                        Camlot, Jason. (2013). The Sound of Canadian Modernisms: The Sir George Williams University Poetry Series, 1966-1974.  
                        Journal of Canadian Studies / Revue d’études canadiennes, 46(3): 28-59.
                    
                    
                        Camlot, Jason , Neugebauer, Tomasz and Berrizbeitia, Francisco. (2020). Dynamic Systems for Humanities Audio Collections : The Theory and Rationale of Swallow.
                         DH2020 (Digital Humanities 2020 Virtual Conference), 23 July 2020, Ottawa, Canada.
                         https://spectrum.library.concordia.ca/id/eprint/987014/ (accessed 10 April 2022).
                    
                    
                        Fong, Deanna, and Karis Shearer. (2018). Gender, Affective Labour, and Community Building Through Literary Audio Artifacts. 
                        SpokenWebBlog. 
                        https://spokenweb.ca/spokenweblog/ (accessed 20 April 2022).
                    
                    
                        GraphQL: A query language for your API. (n.d.). GraphQL. 
                        
                            https://graphql.org/
                         (accessed 9 December 2021).
                    
                    
                        Martin, Bella, and Bruce M Hanington. (2012). 
                        Universal Methods of Design: 100 Ways to Research Complex Problems, Develop Innovative Ideas, and Design Effective Solutions. Beverly, MA: Rockport.
                    
                    
                        MongoDB database. (n.d.). MondoDB Github Repository. 
                        
                            https://github.com/mongodb/mongo
                         (accessed 12 September 2021).
                    
                    
                        Spinuzzi, Clay. (2005). The Methodology of Participatory Design. 
                        Technical Communication, 52(2): 163–74.
                    
                    
                        Strapi: Open source Node.js headless CMS to easily build customisable APIs. (n.d.). Strapi Github Repository. 
                        
                            https://github.com/strapi/strapi
                         (accessed 9 December 2021).
                    
                
            
        
    


11769	2022	
        
            
                Introduction
                Since the 2003 UNESCO convention stated the importance of preserving intangible cultural heritage (ICH), numerous efforts were undertaken to expand traditional cultural heritage to encompass immaterial aspects. However, the boundaries of what should be intended as ICH are not set: the intangible element can be found in such areas as the aesthetics (e.g. performing arts), epistemology (style, semiotics) and transmission (oral history) of culture.
                Treating martial arts as cultural heritage potentially incorporates all these categories of immateriality 
                    (Hou, Picca, Egloff, & Adamou,
                    2021). The Hong Kong Martial Arts Living Archive (HKMALA) 
                    (Chao, Delbridge, Kenderdine,
                    Nicholson, & Shaw, 
                    2018), as a testimony of arts, styles and methods that are passed down onto small communities, is a prominent example of multiple degrees of ICH occurring together. Whilst gathering a mass of audio-visual and motion capture content to visually preserve endangered Southern Chinese cultures (see Figure 
                    1), it offers an opportunity to turn it into structured knowledge, being originally organized around its past run of exhibitions.
                
                We present an ongoing endeavor to extract and formalize Hakka martial arts knowledge out of HKMALA content into a knowledge graph of linked datasets, thus offering a ground truth for future research questions on the understanding of cultural contact across martial arts communities. 
                
                    
                
                Figure 1: 
                    Overview of HKMALA content types.
                
            
            
                A basic ontological framework for martial arts
                Barring sports or military contexts, no known ontology presents a unified theory of the martial discipline, to form the basis of such a knowledge organization of archival content. As an initial task, this project built one such framework. This is modelled as an ontology network grounded on foundational ontologies, rather than cultural heritage models, but whose modularity reflects the intent to highlight the aesthetic, epistemic and social traits of martial arts (Figure 
                    2). The identification of which traits to be considered culturally relevant will subsequently be delegated to inference models (rule systems, reasoners) combining domain ontologies with cultural ones like CIDOC-CRM and ArCo 
                    (Carriero et
                    al.,
                    2019). See 
                    (Adamou, Hou, Picca, Egloff, Kenderdine, 
                    2021) for details on our ontology engineering work.
                
                
                    
                
                Figure 2: 
                    Documented ontology at 
                    https://crossings.github.io/ont/
                
            
            
                Instantiation on the HKMALA content
                A knowledge organization of HKMALA involves (1) annotating media content of technique demonstrations, MoCap segments and interviews to masters, and (2) indexing these media so that they can be consumed using computational methods like ontology-based data access and semantic query languages like SPARQL. The above ontologies offer a basic framework for a martial arts data domain, yet do not specifically model HKMALA’s Hakka Kung Fu domain: therefore, an instantiation effort is required. This is accomplished, as per Figure 
                    3, by:
                
                
                    
                
                Figure 3: 
                    Pipeline of HKMALA knowledge organization.
                
                
                    Constructing the Hakka martial arts dataset from selected sources.
                    Semantically annotating HKMALA media content using terminology lifted from the dataset in question.
                    Iteratively refining our martial arts ontologies to satisfy the versatility required by the media annotation schema.
                    Continuously publishing the knowledge graph resulting from datasets (1,2) as FAIR data 
                        (Groth & Dumontier, 
                        2020).
                    
                
                To generate the dataset that instantiates the Martial Arts ontology (1), we performed data extraction and reengineering from the following sources:
                a) texts of past HKMALA exhibition panels and captions; b) glossaries in the literature on Southern Chinese martial arts referenced by said texts; c) manifest files containing tabular data to assemble exhibitions out of the archive content; d) transcripts of interviews to masters. 
                Entity extraction from the texts was performed using the Stanford named entity tagger. Part of these sources had previously been used to bootstrap the ontology itself, therefore the terms not used then that denoted instances were included in the dataset. 
                Semantic media annotation (2) is performed using the ELAN toolkit, a multi-tiered video annotation software that allows custom terminology and stores annotation in the open format EAF (
                    Sloetjes & Seibert,
                    2016), which can be exported to JSON-LD and is therefore interoperable with the standards underneath knowledge graphs. Our tiered organization of EAF reflects the three dimensions of cultural heritage that constitute the ontology modules: for example, a video of a Southern Praying Mantis master demonstrating a technique will be annotated along a layer for the aesthetic dimension (e.g. movement, stance and body parts involved), one for the epistemic dimension (technique, style and symbolic reference such as the mantis itself), and one for the social/transmission dimension (the master’s identity).
                
                The scrutiny and annotation of HKMALA media files brought to light a need to refine the original ontological framework (3) to accommodate the expressivity of the knowledge encoded in the medium. For example, a master explaining what qualities are developed by a training exercise on what parts of the body required that an n-ary relation should be materialized as a 
                    Development class. Similarly, distinguishing techniques where the energy flow (
                    qi ) comes from the point of contact (external) or from the attacker’s body (internal) hinted that a class 
                    FlowTransmissionType should be formalized.
                
                The dataset and annotations (4) are published in formats compliant with the Linked Data standards (i.e. Turtle, N-Triples and JSON-LD), both on GitHub
                    
                         CROSSINGS knowledge graph sources, 
                            https://github.com/CROSSINGS/kg
                        
                     and on Zenodo
                    
                         DOI for data citation: 
                            https://doi.org/10.5281/zenodo.5886867
                        
                    
                     and following a rolling release / continuous update model. The choice of representation formats and publishing channels was driven by the need to a) ensure data FAIRness and the ability to load them onto any triple store for querying; b) ease of availability for programming libraries to download and use the data in client code, as exemplified in the concluding section.
                
            
            
                Conclusions
                Aiming at creating a gold standard for building knowledge graphs on martial arts, we offer data and annotations for a subset of the HKMALA content that was originally made publicly available
                    
                         Hakka Kung Fu portal, 
                            http://www.hakkakungfu.com/
                        
                    ,
                     with a view on extending it to the entire archive in the future.
                
                The next step is to align our datasets with the open data cloud: although full third-party coverage is not expected for our domain, Wikidata offers limited authority coverage for some masters, techniques and most importantly geographical and ethnological coordinates.
                The project will provide a way to programmatically consume public HKMALA media through an extension of the DHTK Python library, an ontology- based computational model for Humanities data access 
                    (Picca & Egloff, 
                    2017).
                
            
            
                Acknowledgement 
                This work was supported by CROSSINGS - Computational Interoperability for Intangible and Tangible Cultural Heritage, a project in Collaborative Research on Science and Society (CROSS 2021). The authors also acknowledge the 
                    Hong Kong Martial Arts Living Archive, a research collaboration between the International Guoshu Association, City University of Hong Kong, and the Laboratory of Experimental Museology at EPFL.
                
            
        
        
            
                
                    Bibliography
                    Adamou, A., Hou, Y., Picca, D., Egloff, M., Kenderdine, S. (2021). 
                        Ontology- mediated cultural contact detection through motion and style in Southern Chinese martial arts. In Semantic web and ontology design for cultural heritage (swodch 2021) (Vol. 2949). CEUR-WS.org. Retrieved from 
                        
                        
                            http://ceur-ws.org/Vol-2949#short2
                        
                    
                    Carriero, V. A., Gangemi, A., Mancinelli, M. L., Marinucci, L., Nuzzolese, A. G.,Presutti, V., & Veninata, C. (2019). 
                        ArCo ontology network and LOD on italian cultural heritage. In A. Poggi (Ed.), Proceedings of the first international workshop on open data and ontologies for cultural heritage, odoch@caise 2019 (Vol. 2375, pp. 97–102). CEUR-WS.org. Retrieved from 
                        
                        
                            http://ceur-ws.org/Vol-2375/short3.pdf
                        
                    
                    Chao, H., Delbridge, M., Kenderdine, S., Nicholson, L., & Shaw, J. (2018). Kapturing 
                        Kung Fu: Future proofing the Hong Kong martial arts living 
                        
                        archive. In Digital echoes (pp. 249–264). Springer.
                    
                    Groth, P., & Dumontier, M. (2020). 
                        Introduction - FAIR data, systems and analysis. Data Sci., 3 (1), 1–2. Retrieved from 
                        
                            https://doi.org/10.3233/
                        
                        
                        
                            ds-200029
                        doi: 10.3233/ds-200029
                    
                    Hou, Y., Picca, D., Egloff, M., & Adamou, A. (2021). 
                        Digitizing intangible cultural heritage embodied: state of the art. Journal on Computing and 
                        Cultural Heritage. (To appear)
                    
                    Picca, D., & Egloff, M. (2017). 
                        DHTK: the digital humanities toolkit. In A. Adamou, E. Daga, & L. Isaksen (Eds.), Proceedings of the second workshop on humanities in the semantic web (whise II
                        ) (Vol. 2014, pp. 81–86). CEUR-WS.org. Retrieved from 
                        
                            http://ceur-ws.org/Vol-2014/
                        
                        
                        
                            paper-09.pdf
                        
                    
                    Sloetjes, H., & Seibert, O. (2016). 
                        New facets of the multimedia annotation tool ELAN. In M. Eder & J. Rybicki (Eds.), 11th annual international conference of the alliance of digital humanities organizations, DH 2016 (pp. 888–889). ADHO. Retrieved from 
                        
                            http://dh2016.adho.org/abstracts/
                        
                        
                            161
                        
                    
                
            
        
    


11824	2022	
        
            
                Panel abstract
            
            The social media platform Reddit understands itself as a “home to thousands of communities”, where every used can find their community (
                https://www.redditinc.com). As researchers in humanities, we find that the submissions and comments posted to Reddit’s subreddits do indeed comprise authentic digital human interaction by groups of people that are in some cases prototypical communities and in other cases merely chance encounters of users who find themselves oriented towards the same virtual space. The collective communicative acts of Reddit users can be positioned in the tradition of computer-mediated communication (CMC) – as one key site of digitalised communication, shaped partially by the affordances provided by the platform, and uniquely available to researchers not just in terms of their linguistic content, but also their multimodal context and discursive structure. Importantly, however, Reddit (sub-)communities are not necessarily subject to identical communicative patterns – within each community, user types and even individual users communicate following particular patterns or even idiosyncratically. 
            
            Our recently formed interdisciplinary network, copRe (
                communicative 
                practices on 
                Reddit – copre.org), is dedicated to exploring Reddit discourse(s) from different theoretical perspectives, but all with the aim to contribute to the understanding of Reddit’s own communicative culture as well as the exploration of digital practices more generally. 
            
            Specifically, our panel at the DH2022 conference explores aspects of digital culture and participatory culture, manifest in the communicative acts of different (sub-)communities on the online social platform Reddit. These subreddit-communities and the digital genres they give rise to are sites of linguistic innovation as well as of new debating practices – from the combative far-right subreddit r/The_Donald to the more harmonious r/changemyview. They let us gain insights into individual and group identities, as on r/Mountaineering, and they raise question of methodology, such as the understanding of text length as both a challenge for research and a motivated choice of text authors and the employment of mixed-methods to gain insights that are both driven by big data as well as by in-depth understanding of individual and collective communicative acts.
            
                Language innovation and diffusion online.
                Lisa Donlan (University of Manchester)
                Who are the innovators of lexical terms online? What are the community roles of the early adopters who successfully diffuse linguistic innovations?
                In offline communities, the weak-tie theory of language change envisions the innovators of linguistic forms as peripheral to a community while early adopters are the community's central members. However, the only study to explore the applicability of the theory in an online Community of Practice (CoFP) was grounded in an unusual linguistic context. My research addresses this gap in the literature by using a mixed-methods approach to analyse the status of the innovators and early adopters of four community-salient innovative linguistic forms which diffused through an online music-orientated CofP, Popheads.
                Contrary to expectations, three of the four forms studied were innovated by non-peripheral members who scored highly across multiple markers of status. This departure from previous findings may be related to the fact that linguistic creativity is highly valued in many virtual contexts. Consequently, high-status members may perceive linguistic innovation as desirable behaviour online. 
                This research also found that identifying the hierarchical structures that underpin a community leads to more precise descriptions of the characteristics of early adopters. Specifically, it has been possible to conclude that early adopters are prolific contributors, whose posts are successful at generating discussion, and who are on inbound trajectories in the community. Therefore, to speak of an early-adopter as being 'central' or 'high-status' is, I argue, ultimately too vague and fails to acknowledge the multidimensional nature of status. 
            
            
                Functions of text length on Reddit
                Aatu Liimatta (University of Helsinki)
                In corpus-linguistic studies, text length is typically seen as a potential confounding factor (see e.g. Liimatta, 2020), largely because its effects have been difficult to study using even the largest traditional corpora. However, like any other linguistic choice, the length of a text is also a choice made by the writer or speaker: it is also affected by the communicative purpose of the text and the limitations and affordances of the communicative situation.
                Fortunately, large social media datasets with a range of text lengths have allowed us to approach this previously unassailable topic. Reddit is particularly interesting in terms of text length, since the length of a Reddit comment is free to vary according to the commenter’s needs. Recent studies have shown that Reddit comment length is linked to the distribution of functional linguistic features: for instance, simple information-seeking comments tend to be very short, whereas narrative registers appear to favor longer comments on average (Liimatta, forthc.).
                In order to further explore the role and functions of comment length on Reddit, I analyze a number of subreddits in terms of both the distribution of comment lengths and the distribution of functional linguistic features across comment lengths. To do this, I make use of a large-scale dataset of Reddit comments and a simple but powerful pooling-based computational methodology.
            
            
                Register variation in Reddit comments - A multidimensional analysis
                Hanna Mahler, Kyla McConnell, Axel Bohmann, Gustavo Maccori Kozma, Rafaela Tosin (University of Freiburg)
                Researchers are increasingly becoming interested in the many opportunities that Reddit provides for linguistic analysis. In this large-scale natural language processing project, we focus on register variation within Reddit comments (inspired by Liimatta 2016, 2020).
                We analyze Biber’s (1998) linguistic features for register analysis, as well as platform-specific features, on all Reddit comments since 2005, using the Pushshift Reddit Corpus (Baumgartner et al. 2020). We are using this feature annotation to implement a short-text MDA (Clarke & Grieve 2019), a version of Biber’s (1988) multi-dimensional analysis, to find out which dimensions describe the linguistic variation found on the platform and whether the topical "subreddits" can be described as different registers. Our method also promises to serve as a useful tool for analysing other topics such as adaptation of linguistic norms or register diversification over time.
                Our study therefore adds to the state of knowledge in several ways:
                1. We regard a single comment as one text (with features extracted on the sentence level), which allows us to accurately locate linguistic variation within individual users.
                2. We train a tagger specifically to overcome previous difficulties of tagging social media data (e.g. Banga & Mehndiratta 2017), based on data from Behzad & Zeldes (2020) and Gessler et al. (2020).
                3. The feature extraction script, a refined and elaborated version of Biber's (1988) initial features, is written in Python and will be made openly available.
                4. Our long-term goal is to develop an MDA solution that captures variation within and among all (English) subreddits.
            
            
                Combating the Far-/Alt-Right on Reddit: Lessons from r/AgainstHateSubreddits
                Adrienne Massanari (American University, Washington D.C.)
                Reddit embodies a carnivalesque spirit, often reflecting a kind of geek masculinity (Kendall, 2011) that champions both niche, technical prowess and clever humor (Massanari, 2015). At the same time, communities engaging in far-right rhetoric, such as the now-banned (and widely popular) r/The_Donald, have flourished in part because the platform relies almost exclusively on volunteer labor to moderate and grow communities (Matias, 2019). Shifting the responsibility and risk of moderating onto unpaid individuals allows Reddit to remain a “lean” organization with few employees, but also creates a kind of plausible deniability when it comes to so-called “alt-right” subreddits.
                In response to the growing threat that these subreddits present, and the lack of response from Reddit administrators, activists on the platform have created their own communities focused on highlighting hate speech pervasive on the platform. One such example is r/AgainstHateSubreddits, which is dedicated to exposing subreddits that may superficially conform to Reddit’s few rules, but also engage in transphobic, misogynistic, Islamophobic, and racist rhetoric. Through a critical discourse analysis (Fairclough, 2013) of popular postings on the subreddit, I explore how this community challenges Reddit’s politics and offers an ethical counterpoint to the toxic geek masculinity that pervades much of the platform. Drawing on work from platform studies (Bucher, 2018; Gillespie, 2010) and design justice (Costanza-Chock, 2020; D'Ignazio & Klein, 2020), I argue that Reddit’s governance, design, and platform policies work implicitly welcome and mainstream far-right communities, but that spaces like r/AgainstHateSubreddits provide critical forms of resistance and community for activists.
            
            
                Share my view: Harmonious debating culture on r/changemyview
                Thomas C. Messerli (University of Basel), Daria Dayter (University of Tampere)
                In current times, digital discourses are often understood in terms of polarization. Public lay metadiscourses are full of references to social bubbles and disparate parts of society, whereas academic scholars give a lot of focus to binary categories such as information/disinformation, truth/post-truth or outrage culture. Within this context, the debating culture on the subreddit r/ChangeMyView (CMV) stands out because it encourages what we could term persuasibility – the capacity or willingness of someone to change their opinion when encountering new information. While some work has been done on the specific strategies that commenters use to achieve the task at hand, i.e. to change the original poster’s (OP) view, little attention has been paid to the question how prepared OPs actually are to change their mind and how this “malleability of opinion” (Tan et al. 2016: 621) is discursively constructed. From this perspective, original posts – 
                    submissions in Reddit terminology – are firstly performances of persuasibility, and secondly access points to persuasible-persuasive pairings, in which the subreddit community enacts its codified and tacit norms. In order to explore these pairings, we make use of the CMV corpus we have compiled and specifically compare submissions, delta-awarded comments, i.e. those comments that have changed the OP’s view, and the OP’s responses to delta-awarded comments. We do this comparison itself with a mixed-methods approach that is grounded in qualitative annotation of persuasibility in a sample of r/changemyview threads and scaled up to the corpus using corpus linguistic methods.
                
            
            
                “Science has no business in the mountains”: Stance-taking and expert knowledge on r/Mountaineering
                Sven Leuckert (TU Dresden)
                Stance-taking, as popularised in pragmatics and sociolinguistics by Du Bois (2007), refers to “the speaker’s (or writer)’s relationship to (a) the topic of discussion, (b) the interlocutor or audience, and (c) the talk (or writing) itself” (Kiesling et al. 2018: 684). On social media, stance-taking plays an important role in the discursive construction of relationships and may be employed as a gatekeeping device. In this talk, I focus on strategies of stance-taking as it is linked to the expression of expert knowledge on the subreddit r/Mountaineering. On this subreddit, stance-taking represents a dominant tool to establish who can be considered an expert and, hence, part of the knowledgeable in-group.
                In this talk, I explore which specific linguistic phenomena are employed by users of the subreddit to express stance in situations where expertise in mountaineering is in focus. After an initial manual assessment of recurring phenomena on the basis of randomly selected threads, a quantitative approach inspired by Kiesling et al.’s (2018) annotation scheme is used to establish the bigger picture of how stance-taking is employed as a gatekeeping device on r/Mountaineering. For this study, the entirety of r/Mountaineering from 2012 to August 2021 has been scraped and is taken into consideration. In sum, the findings suggest that, while quantitative methods are a useful addition in the investigation of stance on Reddit, they can only be complementary to an in-depth study of stance-taking phenomena in their discursive context.
            
        
        
            
                
                    Bibliography
                    
                        Banga, R. and Mehndiratta, P. (2017). Tagging Efficiency Analysis on Part of Speech Taggers: International Conference on Information Technology (ICIT: 264–267.
                    
                    
                        Baumgartner, J., Zannettou, S., Keegan, B., Squire, M. and Blackburn, J. (2020). The Pushshift Reddit Dataset: Proceedings of the International AAAI Conference on Web and Social Media, 14th edn.
                    
                    
                        Behzad, S. and Zeldes, A. (2020). A Cross-Genre Ensemble Approach to Robust Reddit Part of Speech Tagging. http://arxiv.org/pdf/2004.14312v1.
                    
                    
                        Biber, D. (1988). Variation Across Speech and Writing. Cambridge University Press.
                    
                    
                        Bucher, T. (2018). If...then : algorithmic power and politics. Oxford University Press.
                    
                    
                        Clarke, I. and Grieve, J. (2019). Stylistic Variation on the Donald Trump Twitter Account: A Linguistic Analysis of Tweets Posted between 2009 and 2018. PloS one, 14(9), e0222062.
                    
                    
                        Costanza-Chock, S. (2020). Design justice: Community-led practices to build the worlds we need. The MIT Press.
                    
                    
                        D'Ignazio, C. and Klein, L. F. (2020). Data feminism
                        . The MIT Press.
                    
                    
                        Dayter, D, & Messerli, T. C. (2021). Persuasive language and features of formality on the r/ChangeMyView subreddit. Internet Pragmatics, 5(1): 165–195. 
                        
                            https://doi.org/10.1075/ip.00072.day
                        
                    
                    
                        Du Bois, J. W. (2007). The stance triangle. In Engelbretson, R. (Ed.), Stancetaking in Discourse. John Benjamins, pp. 139–182
                    
                    
                        Fairclough, N. (2013). Critical discourse analysis: The critical study of language. Routledge.
                    
                    
                        Gessler, L., Peng, S., Liu, Y., Zhu, Y., Behzad, S. and Zeldes, A. (2020). AMALGUM – A Free, Balanced, Multilayer English Web Corpus: Proceedings of The 12th Language Resources and Evaluation Conference: 5267–5275.
                    
                    
                        Gillespie, T. (2010). The politics of ‘platforms’. New Media & Society, 12(3): 347–364.
                    
                    
                        Kendall, L. (2011). ‘White and nerdy’: Computers, race, and the nerd stereotype. The Journal of Popular Culture, 44(3): 505–524.
                    
                    
                        Kiesling, S. F, Pavalanathan, U., Fitzpatrick, J., Han, X. and Eisenstein, J. (2018). Interactional Stancetaking in Online Forums. Computational Linguistics 44(4): 683-718
                    
                    
                        Liimatta, A. (2016). Exploring Register Variation on Reddit: A Mulit-Dimensional Study
                        . Master thesis, University of Helsinki.
                    
                    
                        Liimatta, A. (2020). Using lengthwise scaling to compare feature frequencies across text lengths on Reddit. In Rüdiger, S. and Dayter, D. (Eds.), Corpus Approaches to Social Media
                        . John Benjamins, pp. 111–130.
                    
                    
                        Liimatta, A. (forthc.). Register variation across text lengths: Evidence from social media. International Journal of Corpus Linguistics.
                    
                    
                        Massanari, A. L. (2015). Participatory culture, community, and play: Learning from reddit
                        . Peter Lang.
                    
                    
                        Matias, J. N. (2019). The Civic Labor of Volunteer Moderators Online. Social Media + Society, 5(2). https://doi.org/10.1177/2056305119836778
                    
                    
                        Tan, C., Niculae, V., Danescu-Niculescu-Mizil, C. and Lee, L. (2016). Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions: Proceedings of the 25th International Conference on World Wide Web: 613–624.
                        
                             https://doi.org/10.1145/2872427.2883081
                        
                    
                
            
        
    


11838	2022	
        
            
                The International Prosopography Interchange Format 
                This presentation intends to describe ongoing work at the Austrian Centre for Digital Humanities and Cultural Heritage (ACDH-CH) on the International Prosopographical Interchange Format (IPIF). It presents IPIF’s design, and explores various conceptual and technical challenges arising from its implementation. 
                IPIF is an API definition and data model enabling the sharing and querying of prosopographical data. The original IPIF paper (Vogeler et al. 2019) recognises the power of semantic web tools (RDF, OWL, SPARQL), but also highlights their shortcomings for an interoperable format, chiefly the absence of a standard data model and the complexity of SPARQL. (See Bradley 2020.) Such difficulties circumscribe uses such as ‘light-weight’ data access and querying (as opposed to complex data processing). Accordingly, IPIF opts for a simple RESTful API. 
                In addition to a reference implementation, Papilotte (Vasold 2019), IPIF is intended to be implemented on top of existing projects, allowing access to data in a standardised format and providing the facilities for federated queries and as a data discovery tool. (Vogeler et al. 2019) 
            
            
                The Data Model and API 
                To achieve maximum interoperability, IPIF adopts a version of the ‘factoid model’ (Bradley 2005). This model separates 
                    statements made about a person from the abstract idea of a 
                    person, instead attributing statements to a researcher’s interpretation of a historical 
                    source (Figure 1). This enables contradictory statements, made by distinct sources, to be recorded. As Baillie (2021) argues, this is not always desirable, some historical sources being more trustworthy than others; nevertheless, it is a requirement for an open ecosystem without any single ‘guiding hand’ that contradiction be permitted. 
                
                FIGURE 1: THE IPIF DATA MODEL 
                
                    
                        
                    
                
                The API — described in OpenAPI — is a RESTful interface, allowing direct access to the four IPIF entity types (Factoid, Source, Person, Statement), and querying of statements through graph-like traversals (Figure 2). It returns JSON-LD for each entity, comprising content and/or metadata, and embeds references to related entities. 
                The choice of Statement fields represents a highly pragmatic decision. These fields, allowing a label and a URI, were chosen to match the most obvious use-cases in prosopographical data; the 
                    statementType field allows arbitrary extension beyond these standard statement types. 
                
                
                    FIGURE 2: API EXAMPLES 
                
                
                    
                        
                    
                
            
            
                Implementations, issues and works-in-progress 
                Since the original definition of the standard, IPIF endpoints have been implemented on top of several existing frameworks, including the ACDH-CH’s APIS platform (a Django- based platform for prosopographical projects) and as an eXist-DB module for serving TEI- XML personography data (typically from digital scholarly editions). IPIF client libraries for Python and JavaScript, allowing federated queries and data aggregation across several endpoints, have also been developed. Building these tools has afforded a wealth of practical experience, highlighting the strengths of the format and the difficulties involved in its implementation. This has led to several pragmatic decisions regarding the data model, API and the semantics of querying. 
                
                    A 
                        label field was introduced for Persons, allowing use cases such as autocompletes. The theoretically correct modelling — as a ‘naming statement’ — required too many additional requests to retrieve the appropriate information. 
                    
                    Persons and Sources allow multiple URIs (interpreted as owl:sameAs). Strictly, these should be Statements (i.e. non-definitive assertion that one Person or Source is the same as another: see Zedlitz 2009); but reconciling data from multiple endpoints is considerably easier when this information is a ‘meta’ field of a Person entity.
                    IPIF requires a Source for each Factoid. In many projects, data that would comprise an IPIF Statement is given with no source (e.g. name, death, profession of a person”). In this case, the source is taken to be the project itself. 
                    
                        TEI-XML personographies can express a factoid model by applying the @source and @resp attributes to the sub-elements of a  entry (see Schwarz 2020), 
                        but this is optional. Pragmatically, we suggest using the metadata of the TEI document as fallback (@source, @resp, ancestor::TEI/teiHeader/ titleStmt/editor etc.).
                    
                    
                        To avoid ambiguity in combining statement filters in Person queries (does person/?place=Graz&role=professor mean “a professor in Graz” or “a professor, located in Graz for any reason”?), we defined a default behaviour (both conditions apply to the 
                            same Statement) and an optional parameter independentStatements=true to allow ‘or’ conditions. 
                        
                        In this presentation, we will argue that such decisions are justified by the requirements of interoperability; and that our experiences in developing IPIF thus far contribute usefully to debates surrounded data interoperability in the digital humanities.
                    
                
            
        
        
            
                
                    Bibliography
                    Baillie, James. “Alternative Database Structures for Prosopographical Research”. 
                        International Journal of Humanities and Arts Computing 15, Nr. 1–2 (October 2021): 117– 32. 
                        https://doi.org/10.3366/ijhac.2021.0265. 
                    
                    Bradley, John Douglas. “A Prosopography as Linked Open Data: Some Implications from DPRR”. 
                        Digital Humanities Quarterly 014, Nr. 2 (29 July 2020). 
                        http://digitalhumanities.org/ dhq/vol/14/2/000475/000475.html. 
                    
                    Brradley, John, and Harold Short. “Texts into Databases: The Evolving Field of New-Style Prosopography”. 
                        Literary and Linguistic Computing 20, Nr. Suppl (1 January 2005): 3–24. 
                        https://doi.org/10.1093/llc/fqi022. 
                    
                    Pasin, Michele, and John Bradley. “Factoid-Based Prosopography and Computer Ontologies: Towards an Integrated Approach”. 
                        Literary and Linguistic Computing 30, Nr. 1 (1 April 2015): 86–97. 
                        https://doi.org/10.1093/llc/fqt037. 
                    
                    Vogeler, Georg, Matthias Schlögl, and Gunter Vasold. “Data Exchange in Practice: Towards a Prosopographical API (Preprint)”. Paper given at BD2019 in co-location with RANLP 2019 in Varna (2019). 
                        https://doi.org/10.17613/YW4H-5F09. 
                    
                    Schwartz, Daniel L, Nathan P Gibson, and Katayoun Torabi. “Modeling a Born-Digital Factoid Prosopography Using the TEI and Linked Data”. 
                        Journal of the Text Encoding Initiative, 2020, 37. 
                    
                    Vasold, Gunter: Papilotte. A flexible and extensible IPIF server. 
                        https://github.com/ gvasold/papilotte (2019) 
                    
                    Zedlitz, Jasper. “Gedbas4all – New Data Model for Genealogy”. GenWiki, 2009. 
                        http:// wiki-en.genealogy.net/Gedbas4all/Article = English version of Zedlitz, Jasper. „Gedbas4all – neues Datenmodell für die Genealogie“. 
                        Computergenealogie, Nr. 04 (2009). 
                    
                
            
        
    


11841	2022	
        
            From Shibuya ward with its intense nightlife in 
                After Dark and the commercial area of Kichijōji in 
                Sputnik Sweetheart, to Setagaya’s highway in 
                1Q84, Bunkyo's Rikugien Garden in 
                Norwegian Wood, Aoyama Cemetery in 
                South of the Border, West of the Sun, and the Pacific Hotel near Shinagawa station in 
                The Wind-Up Bird Chronicle – Haruki Murakami, arguably the most famous Japanese contemporary writer, has left his mark on the capital of Japan both within and outside of fiction. In his fourteen novels, Tokyo’s thousand faces appear in great detail, even though they are filtered through a wide variety of characters’ perspectives and reconceptualized by magical realism. Regardless of the literary space abstraction, Seymour Chatman’s theory affirms that literary characters are cognitively experienced in the same way a reader would get to know a person in real life; and Akhil Gupta’s research argues the existence of a link between spatial perception and cultural identity. This would suggest that the spatial conceptualization performed by literary characters entail a 
                forma vivendi ascribable to a certain cultural environment. But is it really that simple? Does an identifiable Tokyo's 
                genius loci emerge from Murakami’s novels? In the wake of Computational Criticism, we will use code-writing to help us get a glimpse of the characters’ cognitive mapping processes and thus delve into a literary-cultural analysis, bearing in mind that imaginative literature is the result of layers of mediation and re-presentation causing straightforward cultural seeping-through to be questioned. In order to do this, we present a Python script able to extract spatial data concerning the cognitive mapping of Murakami’s fictional figures. 
            
             In fact, in the last few years, literary spatiality has emerged both from a theoretical and a digital perspective. In light of this, the results given by the software application will be discussed in order to reflect on Tokyo as an example of Japanese urbanisation, and to identify pros and cons of digitally-assisted interpretive acts regarding spatiality. Lastly, in treating Murakami’s fictional spatiality with digital tools, particular attention will be given to recent criticism of distant reading and corpus selection. 
             From a technical perspective, the presented script will use Natural Language Processing to tokenize, tag, and parse a selected corpus from of Murakami's novels to eventually perform Named Entity Recognition. The script will identify when movement verbs present fictional characters as subjects, leading to the extraction of the starting points and destinations of some of their itineraries. Thus, it will be possible to identify meaningful landmarks and itineraries of some characters, leading to the schematization of their cognitive maps. 
             The spatial structures extracted by digital means will be interpreted in view of cultural differences that Eastern and Western societies present as far as living in metropolitan areas is concerned. Spurious results (counterfactual spatial indications, and phraseological expressions, among others) will also be considered from a technical perspective. Lastly, with respect to the multilingual DH approach, the analysis will be conducted onto the English corpus but a few observations the original text in Japanese language will be presented, thus suggesting new possible research paths.
        
        
            
                
                    Bibliography
                    
                        Algee-Hewitt, M.
                         (2017). Canon/Archive: Studies in Quantitative Formalism from the Stanford Literary Lab, N+1 Foundation
                    
                    
                        Bode K.
                         (2018). A World of Fiction: Digital Collections and the Future of Literary History, University of Michigan Press.
                    
                    
                        Bode, K.
                         (2020). 
                        «Why You Can
                        ’
                        t Model Away Bias»
                        , Modern Language Quarterly, 81, 95-124
                    
                    
                        Bushell, S.
                         (2020). Reading and Mapping Fiction: Spatialising the Literary Text, Cambridge University Press
                    
                    
                        Chatman, S.
                         (2019). Story and Discourse: Narrative Structure in Fiction and Film, Cornell University Press
                    
                    
                        Cooper, D. and Gregory, I.
                         (2011) «
                        Mapping the English Lake District: A Literary GIS»
                        , Transactions of the Institute of British Geographers 36, no. 1, 89–108
                    
                    
                        Cooper, D., Donaldson, C., and Murrieta-Flores, P.
                         (2016). Literary Mapping in the Digital Age, Burlington
                    
                    
                        Gitelman L.
                         (2013). Raw Data Is an Oxymoron, MIT Press
                    
                    
                        Gupta, A. and Ferguson, J.
                         (1992). «
                        Space, Identity, and the Politics of Difference»
                        , in Cultural Anthropology, Vol. 7, No. 1, pp. 6-23
                    
                    
                        Herman, D.
                         (2002). Story logic. Problems and possibilities of narrative, University of Nebraska Press.
                    
                    
                        Heuser, R., Moretti, M., and Steiner, E.
                         (2016). 
                        «The Emotions of London»
                        , Pamphlets of the Stan- ford Literary Lab
                    
                    
                        Hillman, J. 
                        (1992). The Thought of the Heart and the Soul of the World, Spring
                    
                    
                        Loper, E., Klein, E., Bird, S
                        . (2009). Natural Language Processing with Python, O’Reilly Media
                    
                    
                        Marie-Laure Ryan, et al.
                        (2016). Narrating Space/spatializing Narrative: Where Narrative Theory and Geography Meet, Ohio State University Press
                    
                    
                        Moretti F.
                        (1997). Atlas of the European Novel, Einaudi
                    
                    
                        Moretti, F.
                         (2013). Distant Reading, Verso
                    
                    
                        Nan Z, D.
                         (2019).
                        «The Computational Case against Computational Literary Studies»
                        , in Critical Inquiry, 45
                    
                    
                        Schmid, C., Karaman, O., Hanakata, N. C., Kallenberger, P., Kockelkorn, A., Sawyer, L., Streule, M., & Wong, K. P. 
                        (2018). Towards a new vocabulary of urbanisation processes: A comparative approach, Urban Studies, 55(1), 19–52
                    
                    
                        Sorensen, A.
                         (1999). 
                        «Land Readjustment, Urban Planning and Urban Sprawl in the Tokyo Metropolitan Area»
                        , Urban Studies, 36(13), 2333–2360
                    
                    
                        Strecher, M. C.
                         (1999). 
                        «Magical Realism and the Search for Identity in the Fiction of Murakami Haruki»
                        , The Journal of Japanese Studies , Vol. 25, No. 2, pp. 263-298
                    
                    
                        Strecher, M. C.
                         (2021). Dances with Sheep: The Quest for Identity in the Fiction of Murakami Haruki, University of Michigan Press
                    
                    
                        Suter, R.
                         (2008). The Japanization of modernity: Murakami Haruki between Japan and the United States, Harvard University Asia Center
                    
                    
                        Tally, R.
                         (2013). Spatiality, Routledge
                    
                    
                        Underwood, T.
                         (2019). Distant Horizons: Digital Evidence and Literary Change, University of Chicago Press
                    
                    
                        Vasiliev, Y
                        . (2020). Natural Language Processing with Python and SpaCy: A Practical Intro- duction, No Starch Pres
                    
                    
                        Westphal B.
                         (2009). Geocriticism: Real and Fictional Spaces, Armando editore
                    
                    
                        Wilkens, M.
                         (2021). «
                        “Too isolated, too insular”: American Literature and the World»
                        , The journal of Cultural analytics
                    
                
            
        
    


11846	2022	
        
            What do the digital humanities look like outside of the library and the classroom? This paper presents a possible site of digital humanities pedagogy within the realm of open source. 
            In 2021, myself and a team built out and began to populate 
                Sourcegraph Learn (
                https://github.com/sourcegraph/learn), an open source and open access web project that provides educational resources for software developers. A project with only a few contributors on a singular team is not necessarily in the spirit of open source, so during the month of October the team worked with the momentum of Hacktoberfest, the yearly celebration of open source, to invite and encourage more contributors to become involved in our project. We created a Hacktoberfest-specific readme.md file with instructions, added relevant tags, and created issues specific to programming languages and error messages. 
            
            Software developers were invited to contribute troubleshooting guides to the project. These guides would go into depth regarding common error messages that programmers encounter. For example, titles of these technical tutorials included “How to troubleshoot Java ArrayIndexOutOfBoundsException,” or “How to troubleshoot Python AttributeError.” Each tutorial included the full text of the error message, a way to replicate the error message, and two or three ways that a developer could recover from the error message. The troubleshooting guides are all available under the “troubleshooting” tag of the Sourcegraph Learn site (
                https://learn.sourcegraph.com/tags/troubleshooting). 
            
            Altogether, over 40 Git issues were created soliciting contributions, nearly 40 pull requests were opened by prospective contributors, and over 30 high-quality pull requests were merged from contributors across 5 continents. These community-based efforts represent a global, distributed outcome of employing pedagogical techniques such as student-centered learning through encouraging practitioners to share what they have learned with others. 
            This paper will present the approach to documentation, templatizing Git issues, and collaborative code reviews in order to demonstrate how others may be able to set up open-source humanities computing projects to encourage public contributors. In particular, I will draw from the body of work related to crowdsourcing in relation to museums and archives, including Mia Ridge’s edited collection 
                Crowdsourcing our Cultural Heritage (2014), Melissa Terras’s “Crowdsourcing in the Digital Humanities” (2015), and the Getty Museum’s and Folger Library’s respective Zooniverse projects to encourage crowdsourcing. Additionally, this paper will explore how engaging in this space may benefit both open source and the humanities. 
            
            Following the presentation, the author would be interested in hearing from other researchers who may have accepted open-source contributors, and how others may have scaled their approach to digital pedagogy through open source. 
        
        
            
                
                    Bibliography
                    Deines, N. (2018). “Six Lessons Learned from Our First Crowdsourcing Project in the Digital Humanities.” 
                        Getty Iris Blog, 
                        https://blogs.getty.edu/iris/six-lessons-learned-from-our-first-crowdsourcing-project-in-the-digital-humanities/. 
                    
                    Folger Library. 
                        Shakespeare’s World. 
                        https://www.zooniverse.org/projects/zooniverse/shakespeares-world. 
                    
                    Ridge, M. (2014). 
                        Crowdsourcing Our Cultural Heritage. Ashgate.
                    
                    Terras, M. (2016). “Crowdsourcing in the Digital Humanities.” 
                        A New Companion to Digital Humanities, edited by Schreibman, Siemens, Unsworth, Wiley-Blackwell, pp. 420-439.
                    
                
            
        
    


11851	2022	
        
            
                Introduction
                Symbols are an essential part of cultures as means to express ideas, values, traditions and as instantiations of belief systems (Kroeber and Kluckhohn, 1952; Brislin, 1976). Unsurprisingly, thus, symbols form the basis of a variety of comparative cultural studies such as evoked concepts in jewellery and ornaments (Zavvāri* and Chitsāziyān, 2021), rituals, mottos, and icons (Manners, 2011), symbolism of trees, dragons, and tree of life (Rival, 2020; Yuan and Sun, 2021; Reno, 1977).
                Guided by Martinho (2018), who argues for a shift in cultural studies towards quantitative approaches, and Zepetnek (1999), who adapted comparative literature methodology to identify parallels between cultures, we propose a computational approach that uses symbols for quantified comparative cultural analyses. Leveraging information contained in HyperReal (Sartini et al., 2021), a novel database of symbolism, we define two quantitative measurements of cultural similarity which we apply to its data.
                Focussing on a set of cultural contexts from the continent of Asia, and using the defined similarity measures, we address two research questions:
                
                    Does symbolism provide a useful basis for quantitative cultural comparisons? That is, to what extent does it reproduce expected similarities, and does it have the potential to highlight new, unexpected connections?
                    Do cultures tend to be more similar in terms of their symbols or in terms of the symbolic meanings that their symbols refer to?
                
                For RQ1, we analyse the values of our similarity measures and the clusters of cultures they induce. Additionally, we contrast the similarities of Asian cultural contexts among themselves with two European cultural contexts: Christian and Greco-Roman.
                For RQ2, we analyse how similarity values change when applied to either only symbols or only symbolic meanings, as they exist independently in HyperReal.
            
            
                Linking symbols
                Symbolic knowledge has previously been modelled in a semantic web context by (Sartini and Gangemi, 2021) and (Sartini et al., 2021) resulting in the creation of HyperReal, a multi-cultural knowledge graph containing more than 40,000 symbolic meaning relationships (simulations), following the Simulation Ontology schema
                    
                         https://w3id.org/simulation/docs
                    . In this ontology, symbols (simulacra) are linked to their symbolic meanings (reality counterpart) through an n-ary relationship class Simulation. Simulations are also linked to one or more cultural contexts. Figure 1 shows the example of a mirror (simulacrum), that, in the Japanese context, symbolises the sun (reality counterpart) using HyperReal’s structure.
                
                
                    
                    Mirror-sun simulation example
                
            
            
                Data selection and extraction
                From HyperReal, we selected the 15 unambiguously Asian contexts with the highest number of simulations: Ainu, Buddhist, Cambodian, Chinese, Hindu, Indic, Jain, Japanese, Kalmyk, Mongolian, Philippine, Taoist, Tibetan, Vietnamese, Zoroastrian. This set comprises various types of cultural contexts, such as nationalistic (Chinese) or religious-philosophical (Buddhist), and includes intricate relationships (e.g., Chinese and Taoist). Anticipating that these aspects would emerge from our quantitative analyses themselves, we treat all contexts as equivalent and perform direct comparisons. After the selection, we extracted the subgraphs containing the simulations associated with each context along with the labels for their simulacra and reality counterparts.
            
            
                Measuring similarity
                
                    Semantic approach
                    Being embodied by linguistic expression allows us to measure symbols' and their symbolic meanings’ semantic similarity, for which we use the spaCy
                        
                             https://spacy.io/
                         and Wiki2Vec (Yamada et al., 2020) Python implementations.
                        
                             We measure cosine similarities between vectors of symbols and symbolic meanings generated using the mentioned models.
                         We then use the Jaccard similarity metric (Jaccard, 1901) to aggregate sets of semantic similarities for a given pair of cultures. Additionally, we apply weighting according to 
                        symbolic impact and 
                        symbolic referencing, where we define 
                        symbolic impact as the number of symbolic meanings associated with a symbol in a specific context and 
                        symbolic referencing as the number of times a symbolic meaning is denoted by a symbol in a specific context.
                    
                
                
                    Structural approach
                    We use graph edit distance (Hagberg et al., 2008) to compute the structural similarity
                        
                             Similarity = 1-graph edit distance
                         of the extracted cultural contexts’ subgraphs. This measurement provides an interface into the similarities of how cultures structurally organise symbols, agnostic of the semantics of symbols, and is thus complementary to the semantic approach.
                    
                
            
            
                Results
                As displayed by Figure 2, our measurements lead to an intricate overall picture of cultural similarities. Whereas, for instance, a larger culture like Buddhist is similar to Chinese, Hindu, Japanese and Taoist; smaller ones like Ainu or Kalmyk are distant, especially semantically, from most other cultures. 
                Semantic similarity generally seems to be the more conservative, and therefore more often intuitively correct, although counterexamples exist: Jain and Indic, two relatively close cultural contexts, are structurally similar but not semantically so. This underlines the complementary nature of both measurements and is mirrored by the clusters induced from the similarity matrices (Figure 3).
                Here, too, groupings of cultures are mostly according to intuition although it is clear that quantitative measures require being supplemented with other sources of information. Then again, as exemplified by the Greco-Roman and Christian cultures, which distinguish themselves from these Asian cultural contexts, connections emerge that are worth further investigation.
                
                    
                    Heatmap of the semantic (left) and structural (right) semantic matrices
                
                
                    
                    Hierarchical clustering induced by semantic (left) and structural (right) similarity matrices. Colours indicate clusters
                
                Regarding RQ2, the investigated cultures, on average, have slightly higher similarities in terms of their symbols than their symbolic meanings. Additionally, Figure 4 shows that some cultures tend to be moderately more similar according to one of the two measurements, such as the Chinese-Jain or the Zoroastrian-Ainu pairs. Thus, cultures seem to not be more similar in terms of either symbols or symbolic meanings, but these have complementary effects to explaining cultural similarity.
                
                    
                    Similarity matrix given by the semantics of symbols themselves (left) and symbols‘ meanings (right)
                
            
            
                Conclusions
                With this work, we initiate quantitative methods for investigations into the similarities of cultures based on symbolism. We provide evidence for their usefulness as a complement to established comparative cultural studies and predict that situating our findings within this field will facilitate new discussions. To this end, future work should also apply the methodology proposed here to larger global sets of cultures to put the similarities within the set of Asian cultures considered here into perspective.
            
        
        
            
                
                    Bibliography
                    
                        Brislin, R. W. (1976). Comparative research methodology: Cross-cultural studies. 
                        International Journal of Psychology, 
                        11(3): 215–29 doi:10.1080/00207597608247359.
                    
                    
                        Hagberg, A., Swart, P. and Chult, D. (2008). Exploring Network Structure, Dynamics, and Function Using NetworkX.
                    
                    
                        Jaccard, P. (1901). Etude de la distribution florale dans une portion des Alpes et du Jura. 
                        Bulletin de La Societe Vaudoise Des Sciences Naturelles, 
                        37: 547–79 doi:10.5169/seals-266450.
                    
                    
                        Kroeber, A. L. and Kluckhohn, C. (1952). Culture: a critical review of concepts and definitions. 
                        Papers. Peabody Museum of Archaeology & Ethnology, Harvard University, 
                        47(1): viii, 223–viii, 223.
                    
                    
                        Manners, I. (2011). Symbolism in European integration. 
                        Comparative European Politics, 
                        9(3): 243–68 doi:10.1057/cep.2010.11.
                    
                    
                        Martinho, T. D. (2018). Researching Culture through Big Data: Computational Engineering and the Human and Social Sciences. 
                        Social Sciences, 
                        7(12). Multidisciplinary Digital Publishing Institute: 264 doi:10.3390/socsci7120264.
                    
                    
                        Reno, S. J. (1977). Religious Symbolism: A Plea for a Comparative Approach. 
                        Folklore, 
                        88(1). [Folklore Enterprises, Ltd., Taylor & Francis, Ltd.]: 76–85.
                    
                    
                        Rival, L. (ed). (2020). 
                        The Social Life of Trees: Anthropological Perspectives on Tree Symbolism. London: Routledge doi:10.4324/9781003136040.
                    
                    
                        Sartini, B., van Erp, M. and Gangemi, A. (2021). Marriage is a Peach and a Chalice: Modelling Cultural Symbolism on the Semantic Web. 
                        Proceedings of the 11th on Knowledge Capture Conference. (K-CAP ’21). New York, NY, USA: Association for Computing Machinery, pp. 201–08 doi:10.1145/3460210.3493552. https://doi.org/10.1145/3460210.3493552 (accessed 9 December 2021).
                    
                    
                        Sartini, B. and Gangemi, A. (2021). Towards the unchaining of symbolism from knowledge graphs: how symbolic relationships can link cultures. 
                        Book of Extended Abstracts of the 10th National AIUCD Conference. Pisa, pp. 576–80.
                    
                    
                        Yamada, I., Asai, A., Sakuma, J., Shindo, H., Takeda, H., Takefuji, Y. and Matsumoto, Y. (2020). Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia. 
                        ArXiv:1812.06280 [Cs] http://arxiv.org/abs/1812.06280 (accessed 11 December 2021).
                    
                    
                        Yuan, L. and Sun, Y. (2021). A Comparative Study Between Chinese and Western Dragon Culture in Cross-Cultural Communication. Atlantis Press, pp. 74–77 doi:10.2991/assehr.k.210121.015. https://www.atlantis-press.com/proceedings/sschd-20/125951577 (accessed 24 November 2021).
                    
                    
                        Zavvāri*, M. and Chitsāziyān, A. (2021). A Comparative Study on the Symbolism in Turkmen and Baluch Ornaments in Iran. 
                        Journal of Iranian Handicrafts, 
                        3(2). دانشگاه کاشان: 121–30.
                    
                    
                        Zepetnek, S. T. D. (1999). From Comparative Literature Today toward Comparative Cultural Studies. doi:10.7771/1481-4374.1041.
                    
                
            
        
    


11871	2022	
        
            
                Introduction
                An important element in digital 3D reconstruction, in the fields of archeology, art and architecture history, is the subsequent visualization of the result (Messemer, 2016). The standardization of the documentation and publication is seen as the most important priority across the board (Cieslik, 2020). Widely established 3D repositories with integrated 3D visualization such as Sketchfab
                    (https://sketchfab.com/) belong to a commercial offer, while 3D viewers introduced by scientific institutions like Kompakkt 
                    (https://kompakkt.de/home)
                    or by other research projects like patrimonium.net (Dworak, Kuroczyński, 2016) have still not provided approved and applied standards for the documentation and publication of 3D models in the field of hypothetical 3D reconstruction of art and architecture.
                
            
            
                Project assumptions
                Against this background, the project “DFG 3D-Viewer - Infrastructure for digital 3D reconstructions” was launched, which the goal is to provide an offer of permanent infrastructure for decentralized web-based display of models in the DFG 3D-Viewer and in suitable Virtual Research Environments (VRE), accompanied by low threshold interface usage
                    (http://dfg-viewer.de/en/dfg-3d-viewer).
                    The work presented here concerns the results of the first phase of the project including the definition of documentation standards for web-based 3D publication of the digital reconstruction models and the development of the web-based 3D-viewer for digital datasets as a minimal-effort plugin. Proposed solution considers a generic approach with adaptability and reusability (Münster, 2019), respects FAIR principles
                    (https://www.go-fair.org/fair-principles/) and follows existing DFG (German Research Foundation) standards
                    (https://www.dfg.de/en/research_funding/principles_dfg_funding/good_scientific_practice/index.html).
                
            
            
                Minimal documentation standard
                Analysis of documented metadata of the chosen commercial and institutional 3D repositories formed the basis for the definition of a scheme for documentation (Fig. 1). The developed data set was discussed among the community in the form of a survey, which significantly advanced the work towards establishing a standard. It also allowed to emerge documentation-related functionalities of the viewer, such as automatic rendering of the preview images or the displaying the information about model geometry (3D metadata) in the viewer window. The documentation scheme was implemented in a new prototypical 3D repository created in WissKI-based VRE (http://wiss-ki.eu/), which has already been successfully used in several projects of digital reconstructions at the University of Applied Sciences Mainz (Kuroczyński et al., 2022; https://www.new-synagogue-breslau-3d.hs-mainz.de). The data model in the repository uses the CIDOC Conceptual Reference Model (https://www.cidoc-crm.org/) as an ontology. The fundamental research on data modelling was carried out along the community in order to concerns about different combinations of classes and properties to describe the same aspects of documentation.
                            
                                
                        Fig.1
               The comparison of metadata sets in chosen institutional and commercial 3D web-based repositories (©2021, Hochschule Mainz).
                            
            
            
                Framework architecture of the 3D Viewer
                Comparing present 3D viewer solutions, it was decided to take the following properties into account: support for 2D & 3D objects, variety of source formats, support for complex objects, modern technology based, suitable for hand-modeled and laser-scanned objects, 3D world operations, level of detail (LoD) as models representations, compression of 3D objects, 3D metadata, utilities/tools, documentation.
                It appears that only a few 3D viewers fulfill more than half of the requirements. In fact, some of the analyzed applications support 2D/3D objects and a variety of formats, but some are still missing (PLY, XYZ, DAE) (Champion, Rahaman, 2020). These technologies are optimized for hand-modeled objects, while others only for laser-scanned ones. Three of them allow 3D world operations and support 3D metadata, nevertheless none of them supports 3D compression.
                            
                                
                        Fig.2
               Comparison of functionalities of the most competitive 3D viewers on the market (©2021, Hochschule Mainz).
                            
            
                The architecture of the DFG 3D-Viewer is developed considering existing web-based 3D viewers (Champion, Rahaman, 2020; Fernie at al., 2020). Conducted research compares existing infrastructures and viewers (Fig. 2), as well as the concept of a modular architecture for the DFG 3D-Viewer. It concludes that the framework for the scientific 3D infrastructure (considering documentation and publication) should be cross-browser, platform independent and based on modern, promising and long-term supported technology. The viewer should allow viewing of 3D models with textures, stored in the most common formats used nowadays , i.e. OBJ, DAE, FBX, JSON (Cieslik, 2020). It should be also capable of loading 2D images (JPG, PNG, TIFF) (Cieslik, 2020), 3D metadata and provide 3D world operations on models (Fernie at al., 2020). Solution should be integrable out of the box, open source and client-only in order to distribute workload away from the server and minimize the requirements for repository providers to support the DFG 3D-Viewer.
                The developed framework is based on the existing 3D library - three.js. Implementation was prepared in modern and interchangeable programming languages and technologies as JavaScript, PHP or Python. Architecture is optimized to be technology-independent and can be easily exchanged for any other client-side viewer. The solution is suitable for complex, hand-modeled, laser-scanned objects and 3D metadata as well. The viewer is extended to meet the requirements of the specialist community, including the possibility of displaying highly complex geometries and multiple data formats (inter alia IFC and FBX) (Fernie at al., 2020). Moreover, uploading 3D data triggers automatic unattended compression, based on Draco algorithm, and encoding into the glTF format which is optimized for web-based visualization (Fig. 3).
                            
                                
                        Fig.3
               Rendered entity in the 3D Repository with visualized 3D model in the DFG 3D-Viewer (©2022, Hochschule Mainz).
                            
            
            
                Further research
                The next stage of the project is the implementation of the developed modular DFG 3D-Viewer in various academic institutions' repositories, which will be realized in the next two years. The final solution will be available as minimal-effort plugin (set of scripts) for any environment that supports JavaScript, PHP and Python. The datasets from decentralised library repositories will be indexed and displayed in centralized browser web service. As a result, users are provided with a uniform interface for viewing digitised media. The project serves also for further fundamental research conducted by two PhDs in work in the topic of the scientific validation of published 3D reconstruction data and also visualization of the uncertainty on the published 3D models. 
            
        
        
            
                
                    Bibliography
                    
                        Champion, E. and Rahaman, H.
                         (2020). Survey of 3D digital heritage repositories and platforms, 
                        Virtual Archaeology Review,
                        11(23)
                        :1.
                    
                    
                        
                            https://www.cidoc-crm.org/
                        
                         (accessed 09 December 2021)
                    
                    
                        Cieslik, E.
                        (2020). 
                        3D Digitization in Cultural Heritage Institutions Guidebook
                        . Baltimore: Dr. Samuel D. Harris National Museum of Dentistry.
                    
                    
                        
                            https://www.dfg.de/en/research_funding/principles_dfg_funding/good_scientific_practice/index.html
                        
                         (accessed 11 April 2022)
                    
                    
                        
                            http://dfg-viewer.de/en/
                        
                         (accessed 10 December 2021)
                    
                    
                        Dworak, D., Kuroczyński, P
                        . (2016) Virtual Reconstruction 3.0 – New Approach of Web-based Visualisation and Documentation of Lost Cultural Heritage.
                        Proceedings of 6th International Conference EuroMed
                        , Cyprus: Springer International Publishing LNCS Series, pp. 292 – 306.
                    
                    
                        
                            https://www.go-fair.org/fair-principles/
                        
                         (accessed 10 December .2021)
                    
                    
                        Fernie, K. et al.
                         (2020).
                        3D content in Europeana task force
                        , Hague: Europeana Network Association.
                    
                    
                        
                            https://kompakkt.de/home
                        
                         (accessed on 10 December 2021).
                    
                    
                        Kuroczyński, P.
                        (2017). Virtual Research Environment for Digital 3D Reconstructions: Standards, Thresholds and Prospects. In: Frischer, B., Guidi, G., Börner, W., (Hg.) 
                        Cultural Heritage and New Technologies 2016 Proceedings
                        , 
                        Studies in Digital Heritage,
                         Open Access Journal, Vol. 1, No. 2, pp. 456 – 476.
                    
                    
                        Kuroczyński, P., Bajena, I., Große, P., Jara, K., Wnęk K.
                        (2022) Digital Reconstruction of the New Synagogue in Breslau: New Approaches to Object-Oriented Research. In Niebling, F., Münster, S. (eds.),  
                        Proceedings of the Conference on Research and Education in Urban History in the Age of Digital Libraries & Digital Encounters with Cultural Heritage
                        , Springer, January 2022.
                    
                    
                        Messemer, H.
                        (2016) The Beginnings of Digital Visualisation of Historical Architecture in the Academic Field. In: Hoppe, S. and Breitling, S. (eds.),  V
                        irtual Palaces, Part II. Lost Palaces and their Afterlife. Virtual Reconstruction between Science and Media
                        , München: PALATIUM, pp. 21-54.
                    
                    
                        Münster, S.
                         (2019) Digital Cultural Heritage as Scholarly Field – Topics, Researchers and Perspectives from a bibliometric point of view In: 
                        Journal of Computing and Cultural Heritage
                        12(3)
                        : 22-49.
                    
                    
                        
                            https://www.new-synagogue-breslau-3d.hs-mainz.de
                        
                         (accessed on 08 December 2021)
                    
                    
                        https://www.patrimonium.net
                         (accessed on 10 December 2021)
                    
                    
                        
                            https://sketchfab.com/
                        
                        (accessed on 19 April 2022)
                    
                    
                        
                            http://wiss-ki.eu/
                        
                         (accessed on 09 December 2021)
                    
                
            
        
    


11872	2022	
        
            
                Introduction
                Nature plays a particular role in the Romantics’ worldview. This concept is characterised by a shift in emphasis from nature as a passive principle (cf. natura naturata by Schelling, 1994) to nature as a subject, an active, creative principle (natura naturans ibid.). The unifying force between these states is the spirit (‘Weltseele’). This leads to the dissolving of established boundaries and oppositions between a human being and nature in romanticist texts (cf. Wanning, 2005). Romantics transform the vertical world order into a horizontal one and create a worldview where human being forms a unity with the world around them. Among other, these transformations manifest themselves in the anthropomorphisation of nature.
                Our approach is therefore to find out how nature is represented in Romanticist texts by analysing whether it is depicted as an active entity and related to the concept of human being, both in terms of anthropomorphism and the possible semantic projection as a holistic concept. While we acknowledge that nature is a complex concept with blurred boundaries, in this contribution we would like to demonstrate how one can gain insights with a relatively simple approach focused on nature as ‘nature’ and comparing Romanticism to other epochs.
            
            
                Data
                For our analysis, we compiled two corpora. Our main corpus contains 90 novels (10,511,420 tokens) from 1780 until 1850, coinciding with the epoch of German Romanticism. A comparative corpus with 102 novels (10,423,812 tokens) from Realism (56 novels) and Naturalism (46 novels) published from 1880 until 1900 was taken from the d-Prose corpus (Gius et al., 2020). This corpus was selected for a contrastive analysis since, in Naturalism and Realism, writers saw their conception of the world as the antithesis of Romanticism.
            
            
                Methods
                For exploring nature’s agency in the sociological sense, i.e., the ability to act independently, we took the grammatical position of ‘nature’ as a proxy and parsed both corpora using spaCy (Honnibal et al., 2021). The subsequent analysis of the possible anthropomorphic representation of nature in Romanticism and Naturalism is based on bigram collocations of ‘nature’ identified with NLTK (Bird et al., 2009). The strength of association between collocates was measured by log-likelihood for collocations occurring more than three times. As an exploration of the semantic dimensions of nature, we finally constructed word embedding models for nature and human beings in Romanticism and Naturalism using word2vec in gensim (Mikolov et al. 2013; Rehurek, 2021) and visualised them with the t-SNE-algorithm (Maaten and Hinton, 2008) implemented by scikit-learn (Pedregosa et al., 2011).
            
            
                Selected Results
                We now sketch some key results from our project. Table 1 shows the most frequent verbs in sentences where ‘nature’ is used in the subject and thus, from a grammatical perspective, in an active position. In both cases, nature seems to be conceptualised as acting human-like. However, the proportion of sentences with ‘nature’ as subject is higher in Romanticism with 0,38% (i.e., 1,382 sentences out of 363,318) against 0,06% (305 sentences out of 545,023).
                
                    
                        
                    
                    Table 1: Most frequent verbs in sentences with ‘nature’ as subject in Romanticism and Naturalism/Realism
                
                Interestingly, we found more human-related words in bigrams in the Naturalist’s subset than in Romanticism (see highlighted words in Table 2). However, the semantics of these words refer rather to human nature in the sense of character traits than to nature.
                
                    
                        
                    
                    Table 2: Collocations with ‘nature’ in Romanticism and Naturalism
                
                The word2vec analysis (see Table 3) seems to confirm this. In the compared datasets, different concepts of ‘nature’ are contextualised. While the Romantics aestheticise 
                    nature, Naturalists refer to ‘nature’ as 
                    human characteristics and thus as 
                    human nature.
                
                
                    
                        
                    
                    Table 3: Similar word vectors to the keyword ‘nature’
                
                Finally, the t-SNE-visualisations with the projections of the vectors ‘human being’ and ‘nature’ into the same two-dimensional space make the different conceptions of Romanticism and Naturalism visible (cf. Fig. 1 and 2).
                
                    
                        
                    
                    Figure 1: The t-SNE projection of the word vectors ‘nature’ and ‘human being’ (Romanticism)
                
                
                    
                        
                    
                    Figure 2: The t-SNE projection of the word vectors ‘nature’ and ‘human being’ (Naturalism)
                
                While the–more compact–cluster of the ‘nature’ vector in Romanticism is built by words belonging to the semantic field of human being or aesthetical, metaphysical concepts, in Naturalism, it is more directed towards human-related, often moral or character-related, properties. The ‘human being’ clusters, on the contrary, address humans in their social roles in both epochs. Moreover, the ‘nature’ cluster for Romanticism seems to point at the ‘Universalpoesie’ in Romanticism with its ideal of merging the fields of life, science, art and nature.
            
        
        
            
                
                    Bibliography
                    
                        Bird, S., Klein, E. and Loper, E. (2009). 
                        Natural Language Processing with Python. Beijing; Cambridge [Mass.]: O’Reilly.
                    
                    
                        Gius, E., Guhr, S. and Adelmann, B. (2020). d-Prose 1870-1920 Zenodo doi:10.5281/zenodo.4315209. https://zenodo.org/record/4315209 (accessed 7 April 2022).
                    
                    
                        Honnibal, M., Montani, I. and Van Landeghem, S. (2021). 
                        SpaCy · Industrial-Strength Natural Language Processing in Python. Python and Cython Berlin: Explosion https://spacy.io/ (accessed 7 April 2022).
                    
                    
                        Maaten, L. van der and Hinton, G. (2008). Visualizing Data using t-SNE. 
                        Journal of Machine Learning Research, 
                        9(86): 2579–605.
                    
                    
                        Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. and Dean, J. (2013). Distributed Representations of Words and Phrases and their Compositionality. 
                        Advances in Neural Information Processing Systems, vol. 26. Curran Associates, Inc. https://papers.nips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html (accessed 7 April 2022).
                    
                    
                        Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., et al. (2011). Scikit-learn: Machine Learning in Python. 
                        The Journal of Machine Learning Research, 
                        12: 2825–30.
                    
                    
                        Řehůřek, R. (2021). 
                        Gensim: Topic Modelling for Humans. https://radimrehurek.com/gensim/models/word2vec.html (accessed 7 April 2022).
                    
                    
                        Schelling, F. W. J. von (1994). 
                        Ideen zu einer Philosophie der Natur: (1797). (Ed.) Durner, M. (Historisch-Kritische Ausgabe). Stuttgart: Frommann-Holzboog.
                    
                    
                        Wanning, B. (2005). 
                        Die Fiktionalität der Natur: Studien zum Naturbegriff in Erzähltexten der Romantik und des Realismus. Berlin: Weidler.
                    
                
            
        
    


11874	2022	
        
            Recently, the relevance of sound for the meaning of words in a language has increasingly gotten scholarly attention. Works on sonic iconicity in different languages all over the world are gaining importance, regarding both the importance of sound in bound language (lyrics), and in daily spoken language. This research has provided serious evidence suggesting that the sound of a word already gives us information about its meaning, or at least, about the overall associations attached to it. In other words, phonemes already seem to carry semantic information (Auracher, 2020). Whereas a number of studies have investigated phonosemantics concerning single phonemes in a general sense, a noticeable research gap exists in the context of poetry or verse drama. Since it became the first modern mass culture, the early modern verse drama appears to constitute an especially crucial corpus for this field of research. 
            The short presentation format was chosen to present the ongoing work of my research including first hints on the use of phonosemantic relations in Spanish early modern theatre (16
                th and 17
                th century). As a basis for researching such relations, in a first step, we created a corpus from the existing digital text repositories, collecting over 500 plays by different playwrights such as Pedro Calderón de la Barca, Lope de Vega, Tirso de Molina, Sor Juana Inés de la Cruz, Mira de Amescua, and many others. The second step included the development of a Python script, enabling the analysis of the phonic structure of every single verse line. The program identifies the number of syllables, the rhythmical patterns, the rhymes, identifies between stressed and unstressed vowels, and creates a csv-file to collect all the created data. As a first result, a phonologic transcriptor and a syllabic analysis tool have already been published as Python libraries (see Sanz Lázaro, 
                fonemas and 
                silabeador). These scripts will be presented briefly, taking into account the existing research on automatic verse analysis in Spanish (González-Blanco, Remón, de la Rosa).
            
            In order to analyze these phonetic data on a large scale, a number of shorter Python scripts were developed. Using the libraries pandas and matplotlib, the goal of research is to answer the following questions: Do the different playwrights have preferences for different rhythmical patterns? Do these rhythmical patterns appear randomly throughout the texts? Or is it possible to establish a pattern between negative and positive emotions and the occurrence of certain rhythmical structures? To what extent are rhythmical patterns related to other phonosemantic phenomena? Is there an interrelation between rhyming structures and different rhythms or a connection between the stressed vowels and the rhythmical patterns?
            As it aims to evoke very strong emotions in its spectators, the early modern theatre is often referred to as an affect machinery. This aspect of Spanish Golden Age theatre is usually reduced to visual aspects: the use of baroque theatre machinery, special effects, and the overwhelming costumes of the professional actors.
            In contrast, this research shows, that an important medium to achieve this goal can be found in the sound of the verses themselves. Therefore, this presentation has a threefold aim:
            
                to present the new Python scripts and libraries created for the automatic verse analysis;
                to discuss phonosemantic relations and the digital methods to analyze them;
                to provide new insights into the importance of sound effects in the affect machinery of the early modern theatre.
            
            Thus, the presentation will show important advances in the automatic analysis of metrical texts, which can easily be transferred to texts from other epochs and, with slight adaptations, also to other romance languages like Italian.
        
        
            
                
                    Bibliography
                    
                        Auracher, J., Menninghaus, W. and Scharinger, M. (2020). Sound Predicts Meaning: Cross-Modal Associations Between Formant Frequency and Emotional Tone in Stanzas. 
                        Cognitive Science, 
                        44:10: e12906. 
                        https://doi.org/10.1111/cogs.12906.
                    
                    
                        Bird, S., Klein, E. and Loper, E. (2009). 
                        Natural Language Processing with Python. Sebastopol, CA: O’Really.
                    
                    
                        De la Rosa, J., Pérez, Á., Hernández, L., Ros, S. and González-Blanco, E. (2020). Rantanplan, fast and accurate syllabification and scansion of spanish poetry. 
                        Procesamiento del Lenguaje Natural
                        65, 83-90.
                    
                    
                        Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: A Package for Computational Text Analysis. 
                        The R Journal
                        8,1: 107–21.
                    
                    
                        González-Blanco, E. Pérez, Á., Ros, S. 
                        PoetryLab​. An Open Source Toolkit for the Analysis of Spanish Poetry Corpora.
                    
                    
                        Karsdorp, F., Kestemont, M. and Riddell, A. (2021). 
                        Humanities Data Analysis: Case Studies with Python. Princeton, Oxford: Princeton University Press. 
                        
                            https://press.princeton.edu/books/hardcover/9780691172361/humanities-data-analysis
                        .
                    
                    
                        Manning, C., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S. and McClosky, D. (2014). The Stanford CoreNLP Natural Language Processing Toolkit. In 
                        Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 55–60. Baltimore, Maryland: Association for Computational Linguistics. 
                        https://doi.org/10.3115/v1/P14-5010.
                    
                    
                        Marco, G. De La Rosa, J., Gonzalo, J. Ros, S., González-Blanco, E. (2021). Automated Metric Analysis of Spanish Poetry: Two Complementary Approaches. 
                        IEEE Access
                        9, 51734-51746.
                    
                    
                        McKinney, W. 
                        Python for Data Analysis, Sebastopol, CA: O’Really.
                    
                    
                        Plecháč, P. (2018). Versification and authorship attribution. A pilot study on Czech, German, Spanish and English poetry. 
                        Studia Metrica et Poetica, 
                        5, 2: 30-54.
                    
                    
                        Remón, GM., Gonzalo, J. (2021). Escansión automática de poesía española sin silabación. 
                        Procesamiento del Lenguaje Natural
                        66: 77-87.
                    
                    
                        Sanz Lázaro, F. (2021). 
                        Fonemas. A Python phonologic transcription library for Spanish. Version 1.0.3. fsanzl, Okt. 2021. Software. 
                        GitHub, 
                        https://github.com/fsanzl/fonemas
                    
                    
                        Sanz Lázaro, F. (2021). 
                        Silabeador: A Python library for syllabic division and stress detection for Spanish. Version 1.0.5, fsanzl, Okt. 2021. Software. 
                        GitHub, 
                        https://github.com/fsanzl/silabeador
                        .
                    
                
            
        
    


11875	2022	
        
            Digital literary text analysis is increasingly becoming an integral part of literary studies. However, many tools designed for performing such analysis remain inaccessible to researchers without significant coding and computing skills. Voyant Tools was designed in part to address this gap. Spyral Notebooks are an extension of Voyant Tools and allow researchers to expand upon their findings from Voyant in a notebook environment. Unlike other notebook environments, Spyral Notebooks are accessible without downloading any programs or advanced set-up. Spyral Notebooks are available in an entirely online format. To use Spyral Notebooks, one needs only a connection to the Internet. The notebooks are easily adaptable, shareable, and editable. 
            Spyral is a notebook development environment that is integrated into Voyant Tools. Notebook environments can be thought of as both extensions of traditional research notebooks and as novel tools that integrate documentation, active analysis and presentation of results. At their core, notebooks are made up of three types of blocks or cells that a user can add or delete in a sequence. 
            
                There are text cells that can contain headings and other text elements found in word processors or browser editors (usually based in HTML) for typing unstructured text. Depending on the notebook environment, the text blocks can be simple or more sophisticated. Spyral Notebooks use HTML for text and offer an in- browser WYSIWYG HTML editor for the text blocks. 
                There are code cells where the user inputs code, be it Python, the Wolfram language used in Mathematica, or JavaScript, which is used in Spyral. The code cells can be run in sequence or individually as you debug your code. Code cells can contain as much or as little code as the user desires. 
                There are output cells which produce the output of the code you input in the associated code cell. It is important to recognize that the output of the code is dependent on what you have instructed the computer to do; that is, it is not a printout of the code cell but the results of running your code. You thus have to instruct the computer to print out the desired results. 
            
            In our tutorial we introduce participants to Spyral Notebooks. We illustrate how to create a corpus for textual analysis from Voyant Tools or directly in Spyral Notebooks. After walking through the basic mechanisms for using Spyral Notebooks including saving, editing, and sharing notebooks, we move on to more specific features available in Spyral. Participants will learn how to enhance the capabilities of Voyant and go deeper with their textual analysis using Spyral. Finally we provide participants with several tutorial notebooks designed to highlight some of Spyral’s advanced features such as categories for use in sentiment analysis. 
            Spyral Notebooks are a welcome addition to the field of digital humanities as they provide an accessible notebook environment specifically designed for literary text analysis. Spyral Notebooks are thoughtfully designed to serve researchers with limited coding skills who want to take their analysis from Voyant one step further. We especially envisage Spyral proving useful for digital humanities instructors. Spyral provides a useful platform for student work, allowing students to embed their analysis from Voyant, perform more complex analysis using JavaScript, and annotate their code with their thought processes.
        
    


11880	2022	
        
            
                Pitchfork.com has published music reviews, news, interviews, feature stories since 1995. Growing out of 1990s zine culture, 
                Pitchfork took advantage of the affordances of the internet early and has since become the self-proclaimed “most trusted voice in music.” In that time, they have reviewed over 23,000 albums by more than 10,000 artists. When it began, 
                Pitchfork was known for its attention to alternative/punk/indie rock and, though it has increased its coverage of hip-hop, pop, and more mainstream music over the years, it remains a source of information about unconventional and emerging artists across genres.
            
            Though they often cite an extraordinary number of genres, sub-genres, and sub-sub-genres in their reporting, they stick to a very conservative list of nine official genre tags for most of their reviews.
            
                
            
            
                Figure 1: Reviews by genre in 
                Pitchfork
                . Note that one review may have several genres, each of which would be counted in this graph. The “Null” category describes a small subset of reviews that had no genre tags.
            
            Debates over the function of genre are common in music as well as literature, film, and culture studies generally (Sanneh, 2021; Lena, 2012; Brackett, 2016). For an outlet like 
                Pitchfork, they function, at least in part, to set expectations for readers. Yet, given the scale and the breadth of the types of music covered by 
                Pitchfork, these nine highly unevenly distributed tags are somewhat uninformative. While the genre tags alone may do little to set expectations, the reviews themselves do this work in at least one highly characteristic way: by comparing the artist under review to other artists who share similar qualities. Fortunately for us, 
                Pitchfork does this in an easily minable fashion by maintaining individual pages for several thousand artists, and then using links to these pages when the artists are mentioned in reviews.
            
            
                
            
            Figure 2
            Figure 2 is a screenshot from a review. Clicking on “The Meters” or “Dr. John”, indicated by red underlining, will take you to the individual pages for each respective artist. We scraped all of the reviews and relevant metadata, including the artist links. With that data set, we created networks of artists where edges are drawn between any artists who have links in the same review.
            
                
            
            
                Figure 3. Nodes are artists; edges reflect co-presence in reviews. Node size shows edge count. Color shows detected community.
            
            Tools like Gephi can depict sub-groups in a network like the one we created via community detection (calculated in Gephi using the Louvain method (Blondel et al, 2008)). However, there are important limitations to this method. Because it is non-deterministic, the precise membership of each group, and even the total number of groups, can vary each time the algorithm is run. To work around this, we introduce a method we call “metamodularity”. We simply ran Louvain community detection on the artist network 10,000 times.
                
                     We used the NetworkX and Community libraries in Python. Some artists had no links, because they were never mentioned in an article with anyone else. To avoid very small communities and improve the legibility of results, we also filtered out 34 artists not connected to the main network. This left us with 7,524 unique artists.
                 Though there are many other possibilities that would achieve similar ends, including examining the communities detected during the “passes” from which the Louvain method constructs its final groups, our approach has several key advantages: It is simple to run, easy to understand, and eschews a non-deterministic approximation in favor of data about the probability of particular groupings.
            
            Using this method, we can show how often any two artists were sorted into the same group. For instance, the jazz musician Alice Coltrane was grouped with John Coltrane 10,000 times, with Sly and the Family Stone 4,939 times, and with Guns n’ Roses one time. This gives us a more reliable and comprehensible picture of the level of connection between the artists.
            
                
            
            
                Figure 4: Showing the top artists (sorted by number of total connections with other artists) who group with Bon Iver at increasing thresholds of connectedness
            
            Using the results from this method, we examined every group of at least 25 artists at six different thresholds and renamed the groups to reflect our assessment of the underlying artistic community. For instance, we called the rightmost group in Figure 4 “00’s Indie Rock”.
            It is worth underscoring that our group names are subjective. Nonetheless, they help to show the relationship between groups at different tiers. The Sankey diagram, Figure 5, makes clear the branching of closely-knit artist communities from larger, more loosely connected groups.
            
                
            
            Figure 5
            The richness of these results points to many interesting findings about the operation of genre in 
                Pitchfork; we will mention just one here. Some of the genres suggested by Figure 5 are incredibly specific—e.g., our names for groupings of metal artists include classic, goth, punk, grüv, and crossover. One notable exception is a group of predominately African American musicians, which is remarkably large and stable up until the 9,000 threshold. The artists within it range from instrumental hip hop composer Flying Lotus to Motown legend Marvin Gaye to trap rapper Young Thug to R&B singer-songwriter Sade to funk innovators Funkadelic to crossover hip hop star Cardi B. This is a far more capacious group along aesthetic, market, and even historic grounds than we see in, e.g., the heavy metal clusters at the same metamodularity threshold. This may reflect real-world connections, since many artists in this group have worked together in various ways, perhaps more often than metal bands have. Or this may reflect a structural difference in the way that writers at 
                Pitchfork have covered Black artists relative to their reviews of white musicians, particularly in the early years of the publication. In any case, it is a noteworthy difference in the shape of genre in this corpus.
            
            This finding will be one of three concluding points in the talk. We will also discuss how, in this dataset, metamodularity based solely on artists’ connections seems to demonstrate Carolyn Miller’s description of genre as “social action” rather than some kind of top-down, taxonomic structure (Miller, 1984). We will also reflect on the potential use of metamodularity as a more broadly useful method for understanding (and depicting) network structures.
        
        
            
                
                    Bibliography
                    
                        Blondel, V., Guillaume, J., Lambiotte, R. and Lefebvre, E. (2008). Fast Unfolding of Communities in Large Networks. 
                        Journal of Statistical Mechanics: Theory and Experiment
                        . 2008: 10. 9.
                    
                    
                        Brackett, D. (2016).
                        Categorizing Sound: Genre and 20th Century Popular Music
                        . Oakland: University of California Press.
                    
                    
                        Lena, J. (2012).
                        Banding Together: How Communities Create Genres in Popular Music
                        . Princeton: Princeton University Press.
                    
                    
                        Miller, C. (1984). Genre as Social Action. 
                        Quarterly Journal of Speech
                        . Vol 70, 2; 151-76.
                    
                    
                        Sanneh, K. (2021). 
                        Major Labels: A History of Popular Music in Seven Genres
                        . New York: Penguin Press.
                    
                
            
        
    


11888	2022	
        
            
                With the increased demand in industry for people with programming skills and the visibility of Digital Humanities in academia, there has been a rise in humanities and other non-computer science students looking to learn programming (Hughes, 2020). However, programming course availability and quality at humanities departments vary greatly (see e.g Abrahams, 2010; Lyon & Magana, 2020) and students risk losing motivation if they cannot see the connection to their own studies with numerical rather than textual content and practical applications (Forte & Guzdial, 2005; Ramsay, 2012; Kokensbarger et al., 2018; Öhman, 2019). 
            
            
                Some research already exists on what makes a programming course successful in terms of high pass rates (see e.g. Nikula et al., 2011), however, in this paper we focus on examining what practices convince students themselves that a course was useful as a measure of success. In order to examine this aspect and to try to create a few guidelines for successful Python courses specifically for students of humanities-related subjects, we examined student course feedback from three courses: an undergraduate course 
                Python Programming for Digital Humanities
                (~150 students, early 20s, online) at Waseda University, Japan and the 
                NLP for Linguists
                 and 
                Working with Text
                 courses, partially based on the 
                Applied Language Technology
                 online tutorials (Hiippala, 2021), at the University of Helsinki, Finland (~30 students each, early to late 20s, hybrid style) that are open both for undergraduate and graduate students. The main contents of these courses are fairly similar and so are both the complaints and praise. We compare difficulties students face, explore what keeps them motivated, and analyze their feedback holistically while looking for common denominators of what works: 
            
            
                “These courses have been the high point of the spring semester, and all of the exercises have been motivating and satisfying to complete.”
                  (Student. Finland)
            
            
                As is common with humanities-focused programming courses there is a significant skill gap between students (see e.g. Öhman, 2019) and this too shows in the evaluations. In 2022 the question “What was the level of this week’s content?” was asked of students after their first week of the “Python Programming for Digital Humanities” course. With 107 respondents, the distribution of responses shows normal distribution with most students (69%) feeling that the level was just right, and the rest almost evenly split between “I struggled a bit” and “Too easy” (Table 1).
            
            
                
            
            
                Table 1. Results of poll regarding content difficulty
            
            
                In the Finnish data, the question “Was the level of the course appropriate?” the answers have the greatest spread as well, suggesting that ideally perhaps students should either be placed in different groups based on initial skill level or that some students could really benefit from a pre-introduction course or extra tutoring where they could gain confidence in the most basic of programming concepts. This is especially true in the light that there does not seem to be a correlation between struggling students and below average final grades as long as the students do not drop out. A correlation matrix of evaluation questions and results (Figure 1.) show that there are high correlations between how students experience the speed of the course and the workload and the level of the course. If one of these parameters are adjusted it will likely affect the others, e.g., if the speed that new topics are introduced is slowed down, the workload will feel more manageable and the contents easier to digest.
            
            
                We also recommend that programming courses such as these are best balanced by using scaffolding methods to keep students in the zone of proximal development (Chaiklin et al, 2003; Vygotsky, 1987). In practice this means telling the students the outline of what they are going to learn first to enable independent learning at the students’ own pace later and not making the contents too easy, but also providing students with the tools to do their own trouble-shooting and advanced learning by introducing StackOverflow and other such tools early on. Additionally, the content should be humanities focused and practical. This could mean introducing the NLTK library (Bird & Loper, 2004) right after teaching the basics (data types & loops) as a way to demonstrate usefulness. 
            
            Overall, student satisfaction ratings are very high for both courses as are enrollment numbers. Ramsay (2012) referred to teaching humanities students programming as raising an army of hacker-scholars, and it certainly seems that the interest is there from the student side once they get past the initial hurdle of enrolling in the course.
            
                
            
            Figure 1. Correlation matrix of feedback responses
        
        
            
                
                    Bibliography
                    
                        Abrahams,  D.  A.
                        (2010).  Technology adoption in higher education:  A  framework for identifying and prioritizing issues and barriers to the adoption of instructional technology. 
                        Journal of Applied Research in Higher Education
                         2, 2, 34–49.
                    
                    
                        Bird, S. G., & Loper, E.
                         (2004). 
                        NLTK: the natural language toolkit
                        . Association for Computational Linguistics.
                    
                    
                        Chaiklin, S.
                        (2003). The zone of proximal development in Vygotsky’s analysis of learning and instruction. Vygotsky’s educational theory in cultural context 1,  39–64.
                    
                    
                        Forte, A., and Guzdial, M. (
                        2005). Motivation and nonmajors in computer science: Identifying discrete audiences for introductory courses.
                         IEEE Transactions on Education
                         48:2, 248–253.
                    
                    
                        Hiippala, T. (
                        2021). Applied Language Technology: NLP for the Humanities. 
                        In Proceedings of the Fifth Workshop on Teaching NLP
                         (pp. 46-48).
                    
                    
                        Hughes, O
                        . (2020). 
                        Developer training sees spike in demand as more people learn to cod
                        e. TechRepublic. Retrieved November 29, 2021, from https://www.techrepublic.com/article/the-economic-outlook-is-uncertain-so-more-people-want-to-become-developers/ 
                    
                    
                        Kokensparger,  B., and Peyou,  W.
                        (2018). Programming for the humanities: A whirlwind tour of assignments.  In
                         Proceedings of the 49th ACM Technical Symposium on Computer Science Education,
                         SIGCSE’18, ACM, pp. 1050–1050.
                    
                    
                        Lyon, J.A., and J. Magana, A
                        . (2020). 
                        Computational thinking in higher education: A review of the literature
                        . Computer Applications in Engineering Education.
                    
                    
                        Öhman, E.S
                        . (2019). Teaching Computational Methods to Humanities Students. In 
                        Digital Humanities in the Nordic Countries Proceedings of the Digital Humanities in the Nordic Countries 4th Conference
                        . CEUR-WS.org.
                    
                    
                        Nikula, U., Gotel, O. and Kasurinen, J. (
                        2011). A motivation guided holistic rehabilitation of the first programming course. 
                        ACM Transactions on Computing Education
                         (TOCE), 11(4), pp.1-38.
                    
                    
                        Ramsay,  S
                        . (2012).  Programming with humanists:  Reflections on raising an army of hacker-scholars in the digital humanities. 
                        Digital Humanities Pedagogy:  Practices, Principles, and Politics
                        , 217–41.
                    
                    
                        Vygotsky, L.
                        (1987). Zone of proximal development. 
                        Mind in society: The development of higher psychological processes
                         5291, 157.
                    
                
            
        
    


11901	2022	
        
            
                Aim of the workshop
                In the half-day workshop, DraCor (https://dracor.org), an open platform for researching plays in different languages, will be introduced using practical examples from digital drama analysis. At the center of DraCor are so-called 'Programmable Corpora'. By this we mean infrastructurally research-oriented, open, extensible, Linked-Open-Data-friendly full-text corpora, which should make it possible to address diverse research questions from the field of digital literary studies in a low-threshold way using corpora in a data-based, traceable, and reproducible way (Fischer et al. 2019).
                The workshop aims at people who
                - work or would like to work with literary texts and in particular with dramas and would like to create their own corpora for this purpose or reuse already existing corpora;
                - want to learn methods of digital drama analysis (network analysis, stylometry) or want to try them out on the basis of the Programmable Corpora approach;
                - are interested in the possibilities of researching literary texts using Linked Open Data (LOD).
                There will be a presentation of the concept of 'Programmable Corpora' as well as a demonstration of the exemplary implementation in the DraCor platform including a presentation of all components. Hands-on tutorials will give participants a practical introduction to creating and curating their own drama corpora for analysis with DraCor. Another part introduces the use of the DraCor API as well as the Python library PyDraCor by means of practical examples on the methods stylometry and network analysis. The Application Programming Interface (API) allows customized direct access to specific parts of the corpora. The possibilities for cross-corpus queries and inclusion of information from the Linked Open Data cloud using SPARQL will also be explored.
            
            
                The Concept of Programmable Corpora
                The core of DraCor consists of corpora of dramas in eleven languages (German, Russian, French, English, Italian, Swedish, Spanish, Ancient Greek, Alsatian, Latin, Bashkir and Tatar) as well as two additional author corpora (Shakespeare, Calderón), to which the platform offers a variety of possible research accesses: The dramas are encoded as XML files according to the TEI guidelines and are freely available under an open license via GitHub at https://github.com/dracor-org. They can be loaded from there, transformed or enriched by oneself if necessary, and reused for further research with any tools. 
                In addition to this "classical" modus operandi of corpus-based research, however, DraCor as an open digital ecosystem offers further interfaces and connected tools (network visualizations, Shiny App, Easy Linavis). Fundamental to this is the DraCor REST API (https://dracor.org/doc/api), which provides functions for retrieving data in different formats (TEI, JSON, plaintext, RDF, GEXF, GraphML) as well as some built-in analysis functionalities (e.g. on network metrics). The API can be used to retrieve not only structural and metadata, but also the full texts without further markup, so that methods such as stylometric analysis or topic modeling can be applied without any further intermediate step to remove markup. The DraCor API is documented in the OpenAPI standard and can be used in an interactive documentation implemented using Swagger UI (https://dracor.org/documentation/api) directly from the web browser. 
                API libraries are available for the Python (PyDraCor: https://github.com/dracor-org/pydracor) and R (rdracor: https://github.com/dracor-org/rdracor) programming languages, which allow the API functionalities to be integrated quickly and adapted to the respective programming language. For complex queries, a SPARQL endpoint (https://dracor.org/sparql) is available on the platform. This allows both cross-corpus and combined queries (federated queries), in which DraCor can be queried simultaneously with other resources available as LOD, such as Wikidata.
            
            
                Digital Drama Analysis with DraCor
                Corpus-based analyses of drama, usually using quantitative methods, have emerged as a distinct subfield of Computational Literary Studies (CLS) in recent years (cf. Willand et al. 2017; Reiter 2021). In this context, the provision of jointly curated and open resources such as DraCor has proven productive also for related disciplines such as computational linguistics (cf. for example Pagel, Reiter 2020).
                Methods operating at the word level have focused, for example, on authorship attribution (Schöch 2014) or genre classification with topic modeling (Schöch 2017). Currently, promising reconceptualizations of stylometric measures such as the measure Zeta are being developed and applied (Schöch 2018). Furthermore, on the basis of structurally annotated corpora, targeted analyses of, for example of stage directions can be performed, operating with POS information or semantic fields (Trilcke et al. 2020). 
                In the field of structural analysis, drama corpora were studied early on using network analytic approaches, beginning with the work of Stiller, Nettle, Dunbar (2003) and continuing, for example, with Moretti (2011). Typological work on, for example, the concept of Small Worlds (Trilcke et al. 2016) stands here alongside approaches to the quantitative classification of figure types (Fischer et al. 2018). 
                Although semantic technologies are now an integral part of the spectrum of methods in the digital humanities, they have rarely been applied in corpus-based CLS (on prose, for example, Frank and Ivanovic 2018; Dittrich 2017). However, the collection of metadata as Linked Data and the connection to external reference resources, especially Wikidata, allow for far-reaching query possibilities and can be profitably used for the analysis of literary corpora. For example, the DraCor corpus data does not contain detailed information on authors and performance locations. However, since the unique Wikidata identifiers are stored for the individual pieces, this information can be retrieved via federated queries in SPARQL and displayed in various visualization forms, such as a map.
            
            
                Learning objectives and timeline of the workshop
                In the first part of the workshop the concept of 'Programmable Corpora' will be introduced and discussed. Afterwards, the platform DraCor and its components will be introduced, with short practical phases during which the participants can directly try out the presented components and tools. In particular, the different possibilities for the reference and analysis of corpus data will be tested. One focus will be on the use of the API. The API functionalities will be explained with the help of interactive documentation and can be tested extensively by the participants. This will be followed by a short overview of corpus creation and the specifics of TEI encoding as used in DraCor.
                The second part of the workshop will consist of work phases in which three topics can be explored in more depth:
                (1) corpus creation and curation with DraCor: Participants will delve into TEI coding of dramas through hands-on exercises and learn how to set up a local instance of the platform using Docker, customize it if necessary, and populate it with their own corpora.
                (2) Drama analysis with DraCor API and Python: Using Jupyter notebooks with extensively documented Python code, participants will be introduced to methods of digital drama analysis using the DraCor API. The notebooks should also enable participants who have no previous experience in programming with Python to follow the individual analysis steps and to adapt them themselves in the sense of a literate programming approach. The notebooks implement concrete research questions on drama analysis, for example on the literary-historical development of network-analytical measures or on the quantitative dominance of characters.
                (3) Drama Analysis with Linked Data: The focus will be on practical analyses made possible from connecting DraCor to the Linked Open Data Cloud. The workshop will provide a brief crash course in the SPARQL query language, followed by joint queries of DraCor and Wikidata and visualization of the results.
            
            
                Organizational matters
                Number of possible participants: 25
                The Workshop will be held via Zoom. Software to be installed on local machines (Oxygen XML editor, Docker, ...) will be announced in advance. Materials will be made available on GitHub; Jupyter notebooks will be posted at (https://github.com/dracor-org/dracor-notebooks).
            
            
                Contributors / Contact details
                Ingo Börner (ingo.boerner@uni-potsdam.de) works as a research assistant in the project "CLSInfra" at the University of Potsdam on the further development of DraCor. His work focuses on data modeling and Linked Open Data.
                Frank Fischer (fr.fischer@fu-berlin.de) is Professor at the Freie Universität Berlin. His involvement with digital drama analysis goes back to the Digital Literary Network Analysis DLINA project (https://dlina.github.io), from which DraCor emerged.
                Peer Trilcke (trilcke@uni-potsdam.de) is Professor of modern German literature at the University of Potsdam. His work focuses on the research-based development of infrastructures for literary corpora and the quantitative analysis of literary texts.
                Carsten Milling (cmil@hashtable.de) is a web developer and is responsible for the development of the DraCor platform in the project "CLSInfra" at the University of Potsdam. 
                Henny Sluyter-Gäthje (sluytergaeth@uni-potsdam.de) is a research assistant at the Chair of 19th Century German Literature at the University of Potsdam. She holds a Master of Science in Cognitive Systems with a focus on computational linguistics and works on algorithmic processing of literary texts.
            
            
                Funding
                DraCor is currently being further developed within the EU Horizon 2020 funded project "CLSInfra" (grant number: 101004984, https://cordis.europa.eu/project/id/101004984).
            
        
        
            
                
                    Bibliography
                    
                        Dittrich, Andreas (2017): "Intra-Connecting an Exemplary Literary Corpus with Semantic Web Technologies for Exploratory Literary Studies" in: Bański, Piotr et al. (Hg.): 
                        Proceedings of the Workshop on Challenges in the Management of Large Corpora and Big Data and Natural Language Processing (CMLC-5+BigNLP) 2017. Mannheim: Institut für Deutsche Sprache. https://nbn-resolving.org/urn:nbn:de:bsz:mh39-62441.
                    
                    
                        Fischer, Frank / Trilcke, Peer / Kittel, Christopher / Milling, Carsten / Skorinkin, Daniil (2018): "To catch a protagonist: Quantitative dominance relations in german-language drama (1730–1930)" in: 
                        Digital Humanities 2018. Conference Abstracts. Mexico City: El Colegio de México / Universidad Nacional Autónoma de México / Red de Humanidades Digitales 193–201. 
                    
                    
                        Fischer, Frank / Börner, Ingo / Göbel, Mathias / Hechtl, Angelika / Kittel, Christopher / Milling, Carsten / Trilcke, Peer (2019): "Programmable Corpora: Die digitale Literaturwissenschaft zwischen Forschung und Infrastruktur am Beispiel von DraCor " in: 
                        DHd2019: »Digital Humanities: multimedial & multimodal«. Book of Abstracts. Mainz/Frankfurt a. M.: Johannes Gutenberg Universität Mainz / Goethe Universität Frankfurt, 194–197.
                    
                    
                        Frank, Andrew / Ivanovic, Christine (2018): "Building Literary Corpora for Computational Literary Analysis – A Prototype to Bridge the Gap between CL and DH" in: Calzolari, Nicoletta et al. (Hg.): 
                        Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). Miyazaki, Japan: European Language Resources Association.
                    
                    
                        Moretti, Franco (2011): "Network Theory, Plot Analysis " in: 
                        Stanford Literary Lab Pamphlets 2. http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf [letzter Zugriff 13.7.2021].
                    
                    
                        Pagel, Janis / Reiter, Nils (2020): "GerDraCor-Coref: A Coreference Corpus for Dramatic Texts in German" in: 
                        Proceedings of the Language Resources and Evaluation Conference (LREC). Marseille 55-64 http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.7.pdf [Letzter Zugriff: 15.7.2021].
                    
                    
                        Reiter, Nils (2021): "Möglichkeiten Quantitativer Dramenanalyse" in: 
                        Comparatio. Zeitschrift für Vergleichende Literaturwissenschaft 12(2): 39–52.
                    
                    
                        Schöch, Christof (2017): "Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama" in: 
                        Digital Humanities Quarterly 11, Nr. 2 http://www.digitalhumanities.org/dhq/vol/11/2/000291/000291.html [Letzter Zugriff: 15.7.2021].
                    
                    
                        Schöch, Christof (2018): "Zeta für die kontrastive Analyse literarischer Texte. Theorie, Implementierung, Fallstudie" in: Bernhart, Toni et al. (Hg): 
                        Quantitative Ansätze in den Literatur- und Geisteswissenschaften. Systematische und historische Perspektiven. Berlin: de Gruyter 77–94 doi:10.1515/9783110523300-004.
                    
                    
                        Schöch, Christof (2014): "Corneille, Molière et les autres. Stilometrische Analysen zu Autorschaft und Gattungszugehörigkeit im französischen Theater der Klassik" in: Schneider, Lars / Schöch, Christof (Hg.): 
                        Literaturwissenschaft im digitalen Medienwandel. Beihefte zu Phin 7 http://web.fu-berlin.de/phin/beiheft7/b7t08.pdf [Letzter Zugriff: 15.07.2021].
                    
                    
                        Stiller, James / Nettle, Daniel / Dunbar, Robin I. M. (2003): "The Small World of Shakespeare's Plays" in: 
                        Human Nature 14: 397–408.
                    
                    
                        Trilcke, Peer / Fischer, Frank / Göbel, Mathias / Kampkaspar, Dario / Kittel, Christopher (2016): "Theatre Plays as ›Small Worlds‹? Network Data on the History and Typology of German Drama, 1730-1930" in: 
                        Digital Humanities 2016. Conference Abstracts. Jagiellonian University & Pedagogical University, Kraków 385-387 https://dh2016.adho.org/abstracts/static/dh2016_abstracts.pdf [Letzter Zugriff: 15.07.2021].
                    
                    
                        Trilcke, Peer / Kittel, Christopher / Reiter, Nils / Maximova, Daria / Fischer, Frank (2020): "Opening the Stage. A Quantitative Look at Stage Directions in German Drama" in: 
                        Digital Humanities 2020. Conference Abstracts. Ottawa: University of Ottawa https://dh2020.adho.org/wp-content/uploads/2020/07/337_Opening​the​Stage​A​Quantitative​Look​at​Stage​Directions​in​German​Drama​.html [Letzter Zugriff: 15.07.2021].
                    
                    
                        Willand, Marcus / Trilcke, Peer / Schöch, Christof / Rißler-Pipka, Nanette / Reiter, Nils / Fischer, Frank (2017): "Aktuelle Herausforderungen der Digitalen Dramenanalyse" in: 
                        DHd 2017. Digitale Nachhaltigkeit. Konferenzabstracts. Bern: Universität Bern 175–180 doi:10.5281/zenodo.3684825.
                    
                
            
        
    


11907	2022	
        
            
                Introduction
                In literary history and historiography, places and spaces play an important role – not least in the context of the ‘spatial turn’ (Lafon, 1997; Piatti et al., 2009; Dennerlein, 2009; Weber, 2014). In literary works, narrative locations are particularly relevant, but places of publication as well as further spatial dimensions can also be taken into account (Curran 2018; Burrows et al. 2016). Our contribution presents how we obtained spatial statements from three different information sources and combined them in a knowledge network based on the Linked Open Data (LOD) paradigm (Berners-Lee, 2006; Hooland und Verbough, 2014; Hitzler, 2021).
            
            
                Project context
                The aim of the project Mining and Modeling Text is to establish an information network for the humanities built from various sources.
                         See https://mimotext.uni-trier.de/en/.
                     This aim is closely linked to the finding that, considering the steadily growing digital cultural heritage, the acquisition of knowledge from large amounts of text and data can no longer be handled by individuals. In representing knowledge as LOD, we see untapped potential that we are exploring in the current project phase on the French Enlightenment novel (Delon/Malandain, 1996; Mylne, 1981).
                
            
            
                Creating spatial statements
                Information on spatial statements relevant to our domain is extracted from three different types of sources: 1. bibliographic metadata, 2. primary sources, 3. scholarly publications. After focusing on thematic statements in an earlier phase of our project (Schöch et. al., 2022; Röttgermann et al., 2022), we are currently addressing spatial statements.
            
            
                Mining
                Bibliographic metadata: For our domain, the Bibliographie du genre romanesque français 1751-1800 (BGRF, Mylne et al., 1977) is central, as it defines the population of about 2000 French Enlightenment novels. The BGRF has been extensively analysed and modeled (Lüschow 2020) and contains rich metadata (including places of publication, narrative locations, narrative form, characters, themes, style).
                Primary sources: The Collection of Eighteenth-Century French Novels (Röttgermann, 2021) is analysed via SpaCy’s (Honnibal und Montani, 2017) named entity recognition and reconciliation pipeline supported by OpenRefine (Huynh, [2012] 2010). Our pipeline requires human intervention (Hinzmann et al., 2022) concerning the challenges of ambiguity, fictionality and historicity (Heuser et al., 2016; Jockers, 2016; Nielsen, 2016).
                
                    
                
                Figure 1: Reconciliation of narrative locations with OpenRefine
            
            
                Modeling
                Our approach relies on importing both the text strings as found in the information source (green) and the abstract spatial items (blue) into our Wikibase instance. Combined with the fact that we reference all statements uniquely via the "stated in" property (orange), this ensures a high degree of verifiability of our data (see fig. 2).
                
                    
                
                Figure 2: ‘Narrative locations’ of Diderots La religieuse from NER and BGRF data
                Our spatial vocabulary was built up incrementally and provides items (=spatial concepts) for 7 properties, of which 5 are currently mapped with Wikidata (see fig. 3).
                    
                        
                             See for the vocabulary: https://github.com/MiMoText/vocabularies/blob/main/spatial_vocabulary.tsv.
                        
                    
                
                
                    
                
                Figure 3: Ontology of ‘spatial statements’ within the domain of literary history/historiography
            
            
                Infrastructure
                For the provision of data, we follow Open Science principles, such as the publication of FAIR data in open access as well as the use of open source software – in particular Wikibase (see fig.4).
                    
                         Generally, see Suber 2012 and Wilkinson et al. 2016; related to the project, see Röttgermann and Schöch 2020 and Schöch, 2021. We expect the Wikibase instance to become publicly available in mid-2022: https://www.mimotext.uni-trier.de/en
                     We created a custom bot using the Python library Pywikibot to import and update the RDF triples into our Wikibase instance from TSV files.
                    
                        
                             See https://github.com/MiMoText/Wikibase-Bot.
                        
                    
                
                
                    
                
                Fig. 4: Local Wikibase instance
            
            
                Spatial Querying
                Having all the spatial triples stored in our Wikibase allows us to query and visualize them using the DockerWikibaseQueryService interface. We can gain an overview of the entire set of metadata (see fig. 5, query 1), see places of publication appearing and disappearing over time (see fig. 6, query 2) or explore narrative locations linked to specific thematic concepts such as ‘miracle’ (see fig. 7, query 3). Via 'federated queries' (see fig. 8, query 4), information from other knowledge bases (here Wikidata) can be used.
                
                    
                
                Figure 5: Overview of the most frequent places of publication
                
                    
                
                Figure 6: Places of publication over time
                
                    
                
                Figure 7: Narrative locations linked to the thematic concept ‘miracle’ (excerpt)
                
                    
                
                Figure 8: Cluster of dominant publication places based on a federated query (Wikidata)
            
            
                Conclusion
                We showcase key aspects of spatial information extraction and modeling in our project. Our presentation will show how we link spatial statements from three sources into a multilingual knowledge network. Triples on publication dates, themes, locations and authors can be combined and be differentiated by their source, something which allows new perspectives for literary history, book history and other domains. Future work concerns extracting and adding statements from scholarly publications to the Wikibase instance.
            
            
                Appendix
                
                    Query 1: Items (novels) and their place of publication (fig. 5)
                    
                        
                    
                
                
                    Query 2: Places of publication over time (fig. 6)
                    
                        
                    
                
                
                    Query 3: Narrative location of novels with theme “miracle” (fig. 7)
                    
                        
                    
                
                
                    Query 4: Places of publication with geocoordinate location via federated query (fig. 8)
                    
                        
                    
                
            
        
        
            
                
                    Bibliography
                    
                        Berners-Lee, T. (2006). 
                        Linked Data – Design Issues. https://www.w3.org/DesignIssues/LinkedData.html.
                    
                    
                        Burrows, S. et al. (2016). Mapping Print, Connecting Cultures. 
                        Library & Information History, 32(4), pp. 259–71. 10.1080/17583489.2016.1220781.
                    
                    
                        Curran, M.
                         (2018). 
                        The French Book Trade in Enlightenment Europe I: Selling Enlightenment. London: Bloomsbury Publishing.
                    
                    
                        Delon, M. and Malandain, P. (1996). 
                        Littérature française du XVIIIe siècle. Paris: Presses universitaires de France. https://gallica.bnf.fr/ark:/12148/bpt6k48060529.
                    
                    
                        Dennerlein, K. (2009). 
                        Narratologie Des Raumes. Berlin, New York: De Gruyter.
                    
                    
                        Heuser, R., Algee-Hewitt, M. and Lockhart, A. (2016). Mapping the Emotions of London in Fiction, 1700–1900: A Crowdsourcing Experiment. In 
                        Literary Mapping in the Digital Age. London: Routledge.
                    
                    
                        Hitzler, P. (2021). A Review of the Semantic Web Field. 
                        Commun. ACM. 10.1145/3397512.
                    
                    
                        Honnibal, M. and Montani, I. (2017). 
                        SpaCy 2: Natural Language Understanding with Bloom Embeddings, Convolutional Neural Networks and Incremental Parsing.
                    
                    
                        Hooland, S. van. and Verborgh, R. (2014). 
                        Linked Data for Libraries, Archives and Museums: How to Clean, Link and Publish Your Metadata. Facet Publishing.
                    
                    
                        Huynh, D. (2010). 
                        OpenRefine. OpenRefine. https://github.com/OpenRefine/OpenRefine.
                    
                    
                        Jockers, M. L. (2016). The Ancient World in Nineteenth-Century Fiction; or, Correlating Theme, Geography, and Sentiment in the Nineteenth Century Literary Imagination
                        . Digital Humanities Quarterly, 010(2).
                    
                    
                        Lafon, H. (1997). 
                        Espaces Romanesques Du XVIIIe Siècle, 1670-1820: De Madame de Villedieu à Nodier. 1. éd. Paris: Presses Universitaires de France.
                    
                    
                        Martin, A., Mylne, V. and Frautschi, R. L. (1977). 
                        Bibliographie du genre romanesque français, 1751-1800. London: Mansell.
                    
                    
                        Mylne, V. (1981). 
                        The Eighteenth-Century French Novel: Techniques of Illusion. 2nd Edition. Cambridge, New York: Cambridge University Press.
                    
                    
                        Nielsen, F. A. (2016). Literature, Geolocation and Wikidata. In 
                        Wiki@ICWSM. Proceedings of the International AAAI Conference on Web and Social Media. Cologne, pp. 61–4. https://ojs.aaai.org/index.php/ICWSM/article/view/14833.
                    
                    
                        Röttgermann, J. (ed.) (2021). Collection de romans français du dix-huitième siècle (1750-1800) / Eighteenth-Century French Novels (1750-1800) [dataset]. 
                        Release v0.2.0. 10.5281/zenodo.5040855.
                    
                    
                        Schöch, C. et al. (2022). Smart Modelling for Digital Literary History. 
                        IJHAC: International Journal of Humanities and Arts Computing [Special Issue on Linked Open Data], 16(1), pp. 78–93. https://doi.org/10.3366/ijhac.2022.0278.
                    
                    
                        Suber, P. (2012). 
                        Open Access. Cambridge, Mass: The MIT Press.
                    
                    
                        Weber, A.-K. (2014). 
                        Mapping Literature: Spatial Data Modelling and Automated Cartographic Visualisation of Fictional Spaces. Zurich: ETH Zurich. 10.3929/ETHZ-A-010106067.
                    
                    
                        Wilkinson, M. D. et al. (2016). The FAIR Guiding Principles for Scientific Data Management and Stewardship. 
                        Scientific Data, 3(1), p. 160018. 10.1038/sdata.2016.18.
                    
                
            
        
    


11928	2022	
        
            Verbs have not been a prominent object of investigation in the digital humanities, although we do find computational literary studies on acoustic phenomena in novels (Katsma 2014) or narrativity of stage instructions in modernist drama (Trilcke et al. 2020) that both focus on the use of verbs in the respective corpora. Philosophical discussions about verbs often refer only to certain classes that are considered philosophically relevant in a given context: illocutionary verbs (Green 2021), verbs expressing propositional attitudes (Nelson 2019), or intensional transitive verbs (Forbes 2020). Our approach is both more comprehensive and more specific. We look at finite verbs regardless of their semantic classification (in this we follow Langer 1927). At the same time, our interest is ‘philological’ in that we aim to understand how verbs contribute to the meaning and interpretation of historical philosophical texts (Kahn 2003 follows a similar approach, however limited to only one verb, ‘to be’). Whereas most philosophical enquiry focuses on nouns as the linguistic side of relevant concepts like reason, duty, taste, etc., the study of verbs that function as predicates and thus relate these concepts can complete the picture of how philosophical judgements and arguments are made.
            We present first results of an investigation of verb usage in Kant’s major critical writings. This corpus has the advantage that these texts present a unified system, so that we can ignore their diachronic dimension. And yet they allow for a contrastive analysis, because the three philosophical subdisciplines theoretical philosophy, practical philosophy and aesthetics are clearly mirrored in the structure of this corpus. We exclude minor writings published after the first edition of the
                Critique of Pure Reason (1781), since they either cannot be clearly assigned to one of the three subdisciplines of philosophy under investigation or belong to subdisciplines such as philosophy of history, political philosophy or philosophy of religionwhich form only a small part of Kant’s overall critical system. The goal of our analysis consists in the identification of verbs that are typical for the respective subcorpus and philosophical subdiscipline. We aim to show that Kant’s usage of verbs differs depending on the philosophical subdiscipline the respective text belongs to.
            
            The corpus consists of the main writings of Kant’s critical philosophy and is divided into three subcorpora (tab. 1): 1) 
                Prolegomena, 
                Metaphysical Foundations of Natural Science, and the second edition of the 
                Critique of Pure Reason (the inclusion of both editions would have introduced a lack of balance in the dataset) in theoretical philosophy, 2) 
                Foundations of the Metaphysics of Morals, 
                Critique of Practical Reason and 
                Metaphysics of Morals for practical philosophy and 3) the 
                Critique of Judgment for aesthetics and teleology.
            
            
                
                    Assembled Subcorpora
                    Works
                    N tokens
                
                
                    
                        Theoretical Phil.
                        (3 works)
                    
                    Prolegomena
                    52,588
                
                
                    Metaphysical Foundations of Natural Science
                    41,962
                
                
                    Critique of Pure Reason
                    216,587
                
                
                    Total
                    311,137
                
                
                    
                        Practical Phil.
                        (3 works)
                    
                    Foundations of the Metaphysics of Morals
                    32,958
                
                
                    Critique of Practical Reason
                    67,090
                
                
                    Metaphysics of Morals
                    106,423
                
                
                    Total
                    206,471
                
                
                    
                        Aesth. / Tel.
                        (1 work)
                    
                    Critique of Judgment
                    127,939
                
            
            Tab. 1 number of tokens per work and subcorpus
            The digital edition we used employs modernised orthography which increases the reliability of verb identification through POS-tagging. The texts were tagged and lemmatized with the Stanza POS tagger (Qi et al. 2020) with some manual post processing. We then calculated the key verbs (Culpeper/Demmen 2015) for each subcorpus in contrast to the complete critical writings as reference corpus. We used log-likelihood ratio (Dunning 1993) as keyness measure which can handle the differences in the subcorpus sizes and results in a list of verbs that are used significantly more often in a subcorpus than would be expected from a hypothetical equal distribution
            
                
                    Results
                
                With the help of our domain knowledge, we can state that Kant’s verb usage shows clear differences across the three subdisciplines. Moreover, we can identify areas within the respective subdiscipline in which verbs make a substantial semantic contribution to Kant’s philosophical language.
                In practical philosophy, many of the high-ranking verbs belong to the semantic field of law (including the moral law, i. e. the Categorical Imperative): to acquire (
                    erwerben), to obligate (
                    verpflichten), to force (
                    zwingen). Others are generic terms for actions (
                    handeln, machen, tun). Only one verb denotes an emotion (to love, 
                    lieben). In theoretical philosophy, the two most high-ranking verbs are associated with the faculty of sensibility: to give, 
                    geben, associated with what is given in sensibility, and to intuit (
                    anschauen). Others seem to belong to natural philosophy (
                    erfüllen, to fill, e. g. space), to move (
                    bewegen), to begin (
                    anfangen), to change (
                    verändern). Some are what we could call ‘generic ontological verbs’, to take place (
                    stattfinden), to exist
                     (existieren). Only one verb is connected to a pertinent epistemic activity, to construe 
                    (konstruieren). In aesthetics and teleology, i. e. in 
                    Critique of Judgment, verbs that express an activity are more prominent: judging (
                    urteilen, 
                    beurteilen) plays, of course, an eminent role as do verbs that denote an aesthetic response (to please
                    , gefallen, to entertain (
                    unterhalten), the communicative force of an aesthetic judgment (to require
                    , ansinnen) or the act of communication itself (to communicate
                    , mitteilen).
                
                Further research will be required to investigate the syntactic diversity of Kant’s use of verbs (finite verb forms compared to participles or infinitives) and its relation to 18th century German in general. Moreover, the collection of typical verb-noun collocations as 
                    Recht erwerben (to aquire a right) will be a useful step.
                
                Our corpus, code and data will be published under free licenses.
            
        
        
            
                
                    Bibliography
                    
                        Culpeper, Jonathan / Demmen, Jane (2015). "Keywords." In: Biber, Douglas / Reppen, Randi (eds.): The Cambridge Handbook of English Corpus Linguistics. Cambridge: Cambridge University Press. 90–105. (
                        doi:10.1017/
                        CBO9781139764377.006)
                    
                    
                        Dunning, Ted (1993). "Accurate methods for the statistics of surprise and coincidence." In: Computational Linguistics 19 (1), S. 61–74.
                    
                    
                        Forbes, Graeme (2020), "Intensional Transitive Verbs", 
                        The Stanford Encyclopedia of Philosophy (
                        ). 
                    
                    
                        Green, Mitchell (2021), “Speech Acts”, in: 
                        The Stanford Encyclopedia of Philosophy, (
                        
                            https://plato.stanford.edu/­­­archives/fall2021/entries/speech-acts/
                        ).
                    
                    
                        Kahn, Charles H. (2003) The verb "be" in ancient Greek, Indianapolis: Hackett.
                    
                    
                        Katsma, Holst (2014). "Loudness in the Novel." (= 
                        Stanford Literary Lab, Pamphlet Nr. 7 [September 2014].)
                    
                    
                        Langer, Susanne K. (1927). "A Logical Study of Verbs." In: 
                        The Journal of Philosophy. Band 24, Heft 5 (März 1927), S. 120–129. (
                        
                            doi:10.2307/2015082
                        )
                    
                    
                        Nelson, Michael (2019) "Propositional Attitude Reports", 
                        The Stanford Encyclopedia of Philosophy, (
                        ).
                    
                    
                        Qi, Peng / Zhang, Yuhao / Zhang, Yuhui / Bolton, Jason / Manning, Christopher D. (2020).
                         
                        
                            Stanza: A Python Natural Language Processing Toolkit for Many Human Languages.
                        In Association for Computational Linguistics (ACL) System Demonstrations. 2020.
                    
                    
                        Trilcke, Peer / Kittel, Christopher / Reiter, Nils / Maximova, Daria / Fischer, Frank (2020). "Opening the Stage: A Quantitative Look at Stage Directions in German Drama." In: 
                        DH2020: »carrefours/intersections«. 22–24. Juli 2020. Conference Abstracts, University of Ottawa.
                    
                
            
        
    


11938	2022	
        
            
                Introduction
                Invarsson (2021) suggests a digital epistemology that is to be “understood as an attempt to do digital humanities without being committed to digital tools and objects”. While it is an intriguing idea to leave digital tools and methods out of the equation in order to discern the true epistemological core of DH, we believe that it is equally possible to argue that the use of specific tools virtually shapes and influences the epistemology of DH, or as Nietzsche (1882) put it: “Unser Schreibzeug arbeitet mit an unseren Gedanken
                    
                        
                            Translation: “Our writing tools [in Nietzsche’s case: his new typewriter] shape our thoughts”.
                        
                    ”. Today, Nietzsche’s observations on writing tools can be easily extended to all kinds of research tools, allowing us to ask questions about the epistemological implications of tools for the digital humanities (see Dalbello, 2011; Drucker, 2002; Ramsay & Rockwell, 2012). As there are manifold, rather diverse tools that are used in DH, there is a certain tradition for tool directories that systematically list and categorize different tools. One of the most popular directories is TAPoR
                    
                        
                            TAPoR: https://tapor.ca/home
                        
                    , which has steadily evolved and by now includes more than 1,600 tools. 
                
                The TAPoR list of tools has been used lately to extract and analyze tools mentioned in DH abstracts (Barbot et al. 2019, Fischer & Moranville, 2020b) and tutorials (Fischer & Moranville, 2020a). The motivation of these analyses is primarily to identify relevant and widely used tools in order to make them sustainably available via infrastructures like the 
                    Social Sciences & Humanities Open Marketplace
                    
                        
                             SSH Open Marketplace: https://marketplace.sshopencloud.eu/
                        
                    . While the previous studies so far have only looked at comparatively small corpora, we suggest to enhance the scope of DH tool studies by using a large corpus of DH journal articles (
                    Computers and the Humanities, 
                    Digital Humanities Quarterly, 
                    Literary and Linguistic Computing/
                    Digital Scholarship in the Humanities). The corpus comprises 3,737 articles and covers a time span from 1966-2020, which allows for diachronic analyses of tool usage in DH. In addition to using a larger corpus, we also propose an approach to automatically increase the size of the TAPoR tool list.
                
            
            
                Experiments with the TAPoR tool list
                For our experiments we followed the approach described by Barbot et al. 2019 and also used the TaPOR directory to derive queries to find tool occurrences in our corpus. All in all, we found 319 different tools being mentioned throughout the corpus
                    
                        
                            For a complete list see https://docs.google.com/spreadsheets/d/1BtDVo_2A6a1cLPQCZ8CriNcSGHz3UuVc2mxTsM-ZCxo/edit?usp=sharing
                        
                    . Figure 1 shows the most frequent 25 tools in the overall corpus.
                
                
                    
                    Top 25 tools mentioned in the corpus.
                
                It is noticeable that among the top 25 DH tools we find many tools for text and data analysis, but also a large proportion of high-level programming languages. This is certainly a bias of the early days of humanities computing. Taking a closer look at the diachronic development reveals the natural rise and fall of programming languages, with the steady rise of 
                    Python in the DH since the beginning of the 2010s being particularly prominent (see Figure 2)
                    
                        
                             An interactive version of the plots in Figures 2+3 alongside with more plots can be found here: https://bbrause.github.io/tools-in-dh/
                        
                    .
                
                
                    
                    The rise and fall of programming languages in DH.
                
                To provide some more high-level insights, we also did a cooccurrence analysis of the most frequent 150 tools, i.e. tools that are mentioned together in an article (see Figure 3). Such cooccurrence analyses could yield insights into typical tool combinations and more complex workflows, for instance the use of 
                    Brat to annotate 
                    Twitter data and the use of 
                    SentiStrength to perform sentiment analyses of tweets.
                
                All in all, Figures 1-3 show some strong potential to analyze tools to find out more about their epistemological implications of DH. However, the predominance of outdated programming languages and the absence of state-of-the-art tools such as 
                    spaCy or transformer architectures clearly shows large gaps in the TAPoR list.
                
                
                    
                    UMAP 2D projection of the top 150 most-frequent tools and their nPMI cooccurrence scores (point size indicates numbers of occurrences).
                
            
            
                Query expansion experiments by means of tool embeddings
                
                    To fill these gaps, we conducted a second experiment in which we used BERT embeddings to expand our list of tools, as proposed by Wevers & Koolen (2020). We created embeddings for the 25 most frequent tools (see Figure 1) and looked for their nearest neighbors, i.e. words that have similar embedding vectors as the 25 most frequent tools (see Figure 4). 
                
                
                    
                    Ten nearest neighbors for the embeddings of “python”, “rstudio” and “fortran”, ranked by their cosine similarity.
                
                This approach allows us to identify tools that are not listed in the TAPoR directory directly, but that are mentioned in similar article contexts as the TAPoR-listed tools. Obviously, not all the nearest neighbors identified in this way are actual tools, but if we rank the results according to the number of nearest neighborhoods for the top 25 tools, there are indeed many promising results in the higher ranks
                    
                        
                             The nearest neighbors for each of the 25 most frequent tools as well as a ranked overall list is available here: https://docs.google.com/spreadsheets/d/1iipWwyk7wVcaSzzpEq-W5_vjTe2FO-dmjk_IGjMitEc/edit?usp=sharing
                        
                    . To give just one example: 
                    XSLT (Extensible Stylesheet Language Transformations) has a fairly high score of 16, which means it was in the nearest neighbors of 16 of the 25 most frequent tools from the initial list. In the top ranks we find many other promising tool candidates, such as 
                    RDF, 
                    SGML, 
                    mySQL and also more generic concepts, such as 
                    NLP and 
                    parsing, which could be interpreted as tool super categories.
                
            
            
                Conclusion and next steps
                The experiments by Barbot et al. 2019 and Fischer & Moranville, 2020a/b as well as our follow-up experiments with a larger corpus of texts demonstrate that the empirical analysis of tool mentions in DH publications can be used to discern patterns in the diachronic use of different types of tools. This allows us to explore the effects of tools as rapidly evolving epistemological frameworks in the DH. At the same time, it became clear that the static list of tools as provided by TAPoR has obvious gaps, as the tool landscape is evolving swiftly. We therefore plan to include further directories in follow-up studies, including 
                    ProgrammingHistorian
                    
                         Programming Historian: https://programminghistorian.org/en/lessons/
                    , 
                    forTEXT
                    
                         forTEXT: https://fortext.net/
                    , 
                    DigiHum
                    
                         DigiHum: https://digihum.de/tools/
                    , 
                    DMI (Digital Methods Initiative)
                    
                         DMI: https://wiki.digitalmethods.net/Dmi/ToolDatabase
                    , 
                    DH Toychest
                    
                         DH Toychest: http://dhresourcesforprojectbuilding.pbworks.com/w/page/69244319/Digita
                    , etc. In this article, we illustrated the benefits of an embeddings-based approach to further expand these static lists of tools. Our next steps will be to extend our corpus to also include articles from neighboring disciplines, such as computational linguistics, computational social sciences, information science and others. We also plan to expand the nearest neighbor search beyond the limit of the 25 most frequent tools and to filter the results list manually, to identify reasonable tools. 
                
            
        
        
            
                
                    Bibliography
                    
                        Barbot, L., Fischer, F., Moranville, Y. & Pozdniakov, I. (2019). Which DH tools are actually used in research? Published via weltliteratur.net – A Black Market for the Digital Humanities, https://weltliteratur.net/dh-tools-used-in-research/
                    
                    Bush, V. (1945). As we may think. The Atlantic Monthly, 176(1), 101-108.
                    
                        Dalbello, M. (2011). A genealogy of digital humanities. Journal of Documentation.
                    
                    Drucker, J. (2002), “Theory as praxis: the poetics of electronic textuality”, Modernism/Modernity, Vol. 9, November, pp. 683-91. 
                    
                        Fischer, F. & Moranville, Y. (2020a). DH tools mentioned in "The Programming Historian"? Published via weltliteratur.net – A Black Market for the Digital Humanities, https://weltliteratur.net/dh-tools-programming-historian/
                    
                    
                        Fischer, F. & Moranville, Y. (2020b). Tools mentioned in DH2020 abstracts. ublished via weltliteratur.net – A Black Market for the Digital Humanities, https://weltliteratur.net/tools-mentioned-in-dh2020-abstracts/.
                    
                    
                        Ingvarsson, J. (2020). Digital Epistemology: An Introduction. In Towards a Digital Epistemology (pp. 1-28). Palgrave Macmillan, Cham.
                    
                    
                        Nietzsche, F. (1882). Letter 202. An Heinrich Köselitz in Venedig (Typoskript). Nietzsche Source – Digital Critical Edition (eKGWB): http://www.nietzschesource.org/#eKGWB/BVN-1882,202
                    
                    
                        Ramsay, S., & Rockwell, G. (2012). Developing things: Notes toward an epistemology of building in the digital humanities. Debates in the digital humanities, 75-84.
                    
                    
                        Wevers, M., & Koolen, M. (2020). Digital begriffsgeschichte: Tracing semantic change using word embeddings. Historical Methods: A Journal of Quantitative and Interdisciplinary History, 53(4), 226-243.
                    
                
            
        
    


11949	2022	
        
            
                Introduction
                Picture books are a main source for pre-schoolers to learn about the wider world; it is important for children to see themselves in books and be aware of differences (Johnston and Bainbridge, 2017; Latima, 2020). Male dominance in children’s books, such as the dominance of male characters has long been problematic (Gooden and Gooden, 2001; Kim, 2016; Terras, 2018), it is important therefore to consider gender (as a protected characteristic) when considering diversity in the Chinese children’s book market. 
                Picture books in different countries reflect diverse cultural preferences (Saxby & Winch, 1987; Wee et al., 2015). With a large, growing children’s picture book market (Johnson, 2018), China has translated children’s titles from countries including US, UK, Japan, etc (Li et al., 2020). This study examines the diversity in gender and popular themes in Chinese children’s picture books, by analysing the book authorship, titles, and blurbs of 2,000 bestselling children’s picture books from Dangdang, the major Chinese online bookseller. It provides a general reflection of topics in children’s books from different countries, including Asian, Western countries and other regions. 
            
            
                Method
                
                    Data Corpus
                    We conducted an experimental approach from publicly available information online. Metadata from 2,000 best-selling pre-school picture books were scraped from Dangdang.com, using Python. All data, including book title, blurbs, author introductions were collected on 24
                        th September 2020 and filtered by sales, the earliest title was published in 2003. Data was allocated to four separate corpora regarding original language and region of publication including Chinese local, East Asian (Japan, South Korea, etc), English (US, UK, Canada, etc) European, and multiregional books (French, Germany, South Africa, etc).
                    
                
                
                    Data Analysis
                    This study firstly matched all authors with their sex and nationality by analysing authors’ introductions, as well as searching for official authorial information online, then compiling a statistical breakdown. Secondly, we tokenized book titles and blurbs using the Jieba package in Python, calculated the top 1,000 word list for each corpus. We then identified gendered words, classified them into five groups: pronouns (he/she); gender roles (mum/dad); nouns (princess/witch); animals (cow/cock); name of character (Carmela/Tintin); calculated frequency and compared them. 
                    Thirdly, we adopted topic modelling, a text mining method for understanding contents of a corpus through a group of topics (Heidarysafa et al., 2019), to investigate how book topics differ. We used Bertopic, which supports English and Chinese, and tested different parameters for each corpus. We finally modified the number of topics to 3 and 30 topic words, with a value for each word showing its centrality to the topic. 
                    All data was collected in Chinese with Chinese-style narratives. We analysed data in Chinese then translated the results into English, as presented below. Finally, we tried to match characters’ names with their original versions and present English names (if there were any).
                
            
            
                Results
                
                    Comparison of Authors
                    China had imported picture books from 34 countries and areas, with a male dominance among authors (see in Fig1&2). There are 519 East Asian titles, 730 English titles and 332 multilingual titles. 
                    
                        
                        
                        
                    
                    Fig 1 Nationality of Authors
                    
                        
                    
                    Fig 2 Gender of Authors
                        
                             There are some titles were written by a group of editors, it is difficult to confirm their sex information, so we classified them into the none group.
                        
                    
                
                
                    Gender representation in books 
                    A gendered keyword list compared the gender representation in books. There are more male characters in all corpora, however, mum/mom weighed more than dad in all books. Chinese and Asian titles have fewer gendered words, while more characters were portrayed in English books, correspondingly more pronouns are used. 
                    
                        
                    
                    * Numbers in every blank respectively represent the number of different words in this type and the total frequency of all words in this type.
                
                
                    Topic clustering between corpora of different regions
                    Topics from each dataset were presented in word clouds (Fig 3-14). Chinese culture, children’s education and books resulting from corporate franchises are three main topics in Chinese local titles. Similar topics can be found in East Asian titles, but there are more local topics. Big brands such as Disney, family education and animal stories are important aspects in English titles. 
                    
                        
                        
                    
                    
                        
                        
                    
                
            
            
                Discussion
                The data-driven analysis of book authors and blurbs indicates publishing, purchasing, and reading preferences and showed an overall male dominance the Chinese children’s picture book market. Chinese and East Asian titles emphasise cultural contents and are more education-oriented, while books from Western countries portray more characters and focus on storytelling and children’s emotions. However, this study is a partial reflection of the Chinese children’s book market due to limited data collected for popular titles. All translated titles were chosen under the publishing censorship in China, and they might not be proper representatives of diverse picture books. Further studies can expand the dataset, include more languages and book genres, as well as adopting other methods such as network analysis.
            
        
        
            
                
                    Bibliography
                    
                        Gooden, A. M. and Gooden, M. A. (2001). Gender Representation in Notable Children’s Picture Books: 1995-1999. 
                        Sex Roles, 
                        45(1/2), pp. 89–101. 
                        http://dx.doi.org/10.1023/A:1013064418674.
                    
                    
                        Johnston. and Bainbridge, J. (2017). 
                        Reading Diversity through Canadian Picture Books: Preservice Teachers Explore Issues of Identity, Ideology, and Pedagogy. University of Toronto Press. 
                        10.3138/9781442666412.
                    
                    
                        Johnson. (2018a). 
                        China’s Children’s Book Market: Big Numbers and Local Talent. 
                        Publishing Perspectives. 
                        https://publishingperspectives.com/2018/11/china-childrens-book-market-big-numbers-local-talent/ (accessed 25 October 2021).
                    
                    
                        Johnson. (2018b). 
                        Why Children’s Book Publishing in China Is Growing So Fast. 
                        Publishing Perspectives. 
                        https://publishingperspectives.com/2018/03/why-chinas-childrens-book-industry-is-growing-so-fast/ (accessed 25 October 2021).
                    
                    
                        Kim, S. J. (2016). Expanding the Horizons for Critical Literacy in a Bilingual Preschool Classroom: Children’s Responses in Discussions with Gender-Themed Picture Books. 
                        International Journal of Early Childhood, 
                        48(3), pp. 311–27. 
                        http://dx.doi.org/10.1007/s13158-016-0171-3.
                    
                    
                        Li, Yi., Hangxizi, S. and Li, Yongning. (2020). Is the Chinese Children’s Mainstream Book Market Inclusive Enough? A Data Analysis of Children’s Bestsellers on Dangdang.Com. 
                        Publishing Research Quarterly, 
                        36(1), pp. 129–44. 
                        http://dx.doi.org/10.1007/s12109-019-09706-z.
                    
                    
                        Saxby, H. M. and Winch, G. (1987). 
                        Give Them Wings: The Experience of Children’s Literature. South Melbourne: Macmillan Co. of Australia.
                    
                    
                        Wee, S.-J., Park, S. and Choi, J. S. (2015). Korean Culture as Portrayed in Young Children’s Picture Books: The Pursuit of Cultural Authenticity. 
                        Children’s Literature in Education, 
                        46(1), pp. 70–87. 
                        10.1007/s10583-014-9224-0.
                    
                
            
        
    


11951	2022	
        
            This poster comes from the AHRC-funded project 
                Dunham’s Data: Katherine Dunham and Digital Methods for Dance Historical Inquiry. The overarching project explores 
                the kinds of questions and problems that make the analysis and visualization of data meaningful for dance history, through the case study of 20th century African American choreographer Katherine Dunham, who toured the globe extensively, picking up performers and gathering culturally specific movement for her repertory as she went (Bench and Elswit 2020; Bench and Elswit 2022). Drawing on data-informed research in theatre history (Varela 2021; Miller 2016) and feminist and anti-racist approaches to data (D’Ignazio and Klein 2020; Johnson 2018), our manually-curated core project datasets represent Dunham’s everyday itinerary of over 5000 days spent between 1947-1960 on every continent but Antarctica, the almost 200 dancers, drummers, and singers who travelled with her, and the almost 200 interconnected elements of repertory that they performed. These are currently being expanded to 1938-63, covering the majority of Dunham’s stage career, and all of her domestic and international touring.
            
            Reflecting the 2022 conference theme “Responding to Asian Diversity,” we will present digital visualizations based on data collected regarding the time Dunham spent touring in the Asia-Pacific region from 1956-1958, in particular highlighting research findings related to: 1) the sites of performances and other travel to 21 cities across Australia, New Zealand, Philippines, Singapore, Malaya, Hong Kong, Korea, and Japan, 2) the performers who toured with her and those who joined her company en route (including two Australians, a New Zealander, and eight Filipinos), and 3) the existing repertory they brought with them to perform in these locations, as well as new repertory inspired by their time in the region.
            The company’s international touring brought them into contact with a broad range of rhythms, gestures, and referents, which then circulated onward as Dunham toured. During this period, the company began to perform a number based on the Maori haka, which was later combined with Baby San and Planting Rice, pieces that referenced travels to Japan, Korea, and the Philippines, to form Eastern Suite. We employ 
                spatial, network, and computational analyses as they are used in performing arts research (Bollen and Holledge 2011; Balme 2019) to better understand how Dunham’s choreography materializes the influence of the many geographic places that infused her diasporic imagination, and trace the flows of performers working together over time and space as a dynamic collective, and how their embodied knowledge supports the creation and transmission of Dunham’s repertory. The analyses and visualizations displayed on this poster use a combination of Python/Pandas, Matplotlib, Seaborn, Gephi, Leaflet, and NetworkX. Based on Dunham’s program notes for her choreography, we geolocated every repertory work (accurate to the degree Dunham described) and assigned the map a 2D color palette in such a way that repertory associated with locations of inspiration near each other will have similar colors, which we use to represent these both on the map and off as a stacked bar chart by year.  We then seek to understand the multi-directional force of inspiration by connecting timelines of Dunham’s places visited and of repertory inspired by place as a bipartite graph. This is further complicated by joining three datasets to examine the correlations of Dunham’s travel itinerary by means of a temporal punch card, the trajectories of each company member through the company, and the passports they each carried. Together, these 
                analyses make traceable potential ripples of Dunham’s influence in the many locations the company visited, including Dunham’s long term impact in the Australian entertainment landscape (Bollen 2020), and the ways in which her presence is narrated in the development of Japanese Butoh (Michio 2019).
            
            
                Because scholars generally consider Dunham’s artistic and political project to be one of tracing resonances and retentions of Africanist elements in diasporic 
                movement practices
                 throughout the Caribbean and Americas (Clark 1994; Manning 2004; Das 2017), they have not fully accounted for Asia as a site of inspiration for her choreographic work, and how her influence may have extended throughout the area as performers joined and left the company while touring, as well as the impact of her depiction of African-diasporic practices on local audiences. This poster connects with current scholarship on the African diaspora beyond the Black Atlantic (Gilroy 1993) toward various Black internationalisms that “have never been contained within the holy trinity of Europe, Africa, and the Americas” (Patterson and Kelley 2000, 32) and offers an opportunity to illuminate Dunham as part of transnational creative flows between the Asia-Pacific region and the Afro-Caribbean and Americas.
            
        
        
            
                
                    Bibliography
                    
                        Balme, C. (2019). The Globalization of Theatre 1870–1930: The Theatrical Networks of Maurice E. Bandmann
                        . Cambridge: Cambridge University Press.
                    
                    
                        Bench, H. and Elswit, K. (2020). Katherine Dunham’s Global Method and the Embodied Politics of Dance’s Everyday, Theatre Survey, 61(3): 305-30.
                    
                    
                        Bench, H. and Elswit, K. (2022). Visceral Data for Dance Histories: Katherine Dunham’s People, Places, and Pieces, TDR, 66(1): 39-62.
                    
                    
                        Bollen, J. (2020). Touring Variety in the Asia Pacific Region, 1946–1975
                        . London: Palgrave.
                    
                    
                        Bollen, J. and Holledge, J. (2011). Hidden Dramas: Cartographic Revelations in the World of Theatre Studies, The Cartographic Journal, 48(4): 226-36.
                    
                    
                        Caplan, D. (2016). Reassessing Obscurity: The Case for Big Data in Theatre History. Theatre Journal, 68 (4): 555-573.
                    
                    
                        Clark, V.
                         (1994). Performing the Memory of Difference in Afro-Caribbean Dance: Katherine Dunham’s Choreography, 1938-87. In Fabre, G and O’Meally, R. G. (eds), History and Memory in African-American Culture. New York: Oxford University Press. 188-204.
                    
                    
                        Das, J. D.
                         (2017). Katherine Dunham: Dance and the African Diaspora. New York: Oxford University Press.
                    
                    
                        D’Ignazio, C. and Klein, L. F.
                         (2020). Data Feminism. Cambridge, MA: The MIT Press.
                    
                    
                        Johnson, J. M.
                         (2018). Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads, Social Text, 36(4):57–79. 
                    
                    
                        Gilroy, P.
                         (1993). The Black Atlantic: Modernity and Double Consciousness. Cambridge, MA: Harvard University Press.
                    
                    
                        Manning, S. 
                        (2004). Modern Dance, Negro Dance: Race in Motion. Minneapolis: University of Minnesota Press.
                    
                    
                        Michio, A.
                         (2019). From Vodou to Butoh: Hijikata Tatsumi, Katherine Dunham, and the Trans-Pacific Remaking of Blackness. In Baird, B. and Candelario, R. (eds), The Routledge Companion to Butoh Performance. New York: Routledge.
                    
                    
                        Patterson, T. R. and Kelley, R. D. G.
                         (2000). Unfinished Migrations: Reflections on the African Diaspora and the Making of the Modern World, African Studies Review, 43(1): 11-4.
                    
                    
                        Varela, M. E.
                         (2021). Theater as Data: Computational Journeys into Theater Research. Ann Arbor: University of Michigan Press.
                    
                
            
        
    
11984	2022	
        
            The AGODA project
                
                    
                        https://github.com/mpuren/agoda
                    
                 (Puren and Vernus, 2021) is one of five pilot projects supported by the DataLab of the Bibliothèque nationale de France. It aims to create an online platform facilitating the exploration and use of the parliamentary debates of the Chamber of Deputies published in the 
                Journal officiel from 1881 to 1940. In the framework of the DataLab, we are working on a test sub-corpus, namely the parliamentary cycle from 1889 to 1893, to test our hypotheses on a smaller dataset.
            
            Over the past sixty years, a great deal of work has been done on parliamentary debates (Chester and Bowring, 1962; Franklin and Norton, 1993). It is indeed a valuable source for historians (Marnot, 2000; Ouellet and Roussel-Beaulieu, 2003; Ihalainen, 2016; Lemercier, 2021), political scientists (Van Dijk, 2010), sociologists (Cheng, 2015) or linguists (de Galembert et al., 2013; Hirst et al., 2014; Rheault et al., 2016). Access to digitised and ocerised debates thus seems to have a positive effect on the number of historical works using these documents (Mela et al., 2022). The same effect can be observed for other disciplines using contemporary debates (Fišer et al., 2018; Fišer et al., 2020). AGODA is thus part of a wider movement to facilitate the use and analysis of parliamentary data, following the example of ParlaClarin (Fišer and Lenardič, 2018) and ParlaMint (Erjavec et al., 2022a; Erjavec et al., 2022b), which propose to produce comparable and multilingual Parliamentary Proceedings Corpora according to the XML-TEI standard. Naomi Truan has also produced a corpus of parliamentary debates encoded in XML-TEI (Truan, 2016; Truan and Romary, 2021). The production of this type of resource facilitates the publication of works exploiting this data to better understand French political discourse (Diwersy et al., 2018; Blaette et al., 2020; Diwersy and Luxardo, 2020).
            Between 1881 and 1899, 2596 issues of the 
                Journal Officiel were published (50791 JPG images). The debates are also in TXT format but put online without extensive post-correction: the quality of the OCR is not sufficient to provide a satisfactory online browsing experience, and it could have a negative impact on the analyses performed on these texts (van Strien, 2020). Therefore, we chose to ocerise the text, to obtain a better-quality result. We use the PERO OCR (Kodym and Hradiš, 2021; Kohút and Hradiš, 2021; Kišš et al., 2021) based solution developed by the SODUCO project
                
                    
                        https://soduco.github.io/
                    
                . This tool, still in private alpha version, has been used to prepare the data in (Abadie et al., 2022) that will be accessible via Zenodo.
            
            Ocerised texts are obtained in JSON format; we are developing Python scripts to convert this output into an XML file corresponding to the chosen TEI model. This model is formalised with an adapted XML schema, created using an ODD (Rahtz and Burnard, 2013). We chose to use the ODD created by ParlaClarin (Erjavec and Pančur, 2021) which can be easily adapted to annotate historical parliamentary debates. In the case of France, the rules for transcribing debates were set in the 19th century; thus, the recordings of today's debates are very similar to those produced during the Third Republic. The TEI-encoded corpus will be stored in an eXist-db database, and it will be visualised using the TEI Publisher application, which can transform the source data into HTML web pages. The parliamentary debates will thus be made available to online users as a digital edition and integrated into an application context.
            We will also present the first analyses we have carried out on this corpus with "bag-of-words" techniques - these being not too sensitive to the quality of the OCR. We first used topic modelling, an unsupervised learning method that allows us to discover the latent semantic structures of a corpus of texts, without using semantic and lexical resources (Blei et al., 2003). This method is well suited to study parliamentary debates (Bourgeois et al., 2022).
            
                
            
            Distribution of four different topics over time
            Alternatively, we can use word embeddings to reduce the dimension of the original space from several tens of thousands of forms to a hundred axes, and then apply classical data science tools such as clustering or correlation analysis on the reduced space (Mikolov et al., 2013). Word embedding has thus shown its interest in the study of parliamentary debates (Rheault and Cochrane, 2020). We used a continuous bag-of-words model for dimension reduction and an unsupervised classification algorithm - in this case DBSCAN - to group words into clusters.
            
                
            
            t-SNE projection of the centroïds of the clusters
        
        
            
                
                    Bibliography
                    
                        Abadie, N., Carlinet, E., Chazalon, J., Dumenieu, B. (2022). A Benchmark of Named Entity Recognition Approaches in Historical Documents. Application to 19th Century French Directories. DAS 2022 15th IAPR International Workshop on Document Analysis Systems. La Rochelle, France. May 22-25, 2022.
                    
                    
                        Blaette, A., Gehlhar, S. and Leonhardt, C. (2020). The Europeanization of Parliamentary Debates on Migration in Austria, France, Germany, and the Netherlands. Proceedings of the Second ParlaCLARIN Workshop. Marseille, France: European Language Resources Association, pp. 66–74 
                        https://aclanthology.org/2020.parlaclarin-1.12 (accessed 21 April 2022).
                    
                    
                        Blei, D. M., Ng, A. Y. and Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning Research, 3: 993–1022.
                    
                    
                        Cheng, J. E. (2015). Islamophobia, Muslimophobia or racism? Parliamentary discourses on Islam and Muslims in debates on the minaret ban in Switzerland. Discourse & Society, 
                        26(5). SAGE Publications.
                    
                    
                        Chester, D. N. and Bowring, N. (1962). Questions in Parliament. Oxford: Clarendon Press.
                    
                    
                        Diwersy, S., Frontini, F. and Luxardo, G. (2018). The Parliamentary Debates as a Resource for the Textometric Study of the French Political Discourse. Proceedings of the ParlaCLARIN@LREC2018 Workshop. Miyazaki, Japan 
                        https://hal.archives-ouvertes.fr/hal-01832649 (accessed 21 April 2022).
                    
                    
                        Diwersy, S. and Luxardo, G. (2020). Querying a large annotated corpus of parliamentary debates. LREC, ParlaCLARIN Workshop. (Proceedings of the Second ParlaCLARIN Workshop). Marseille, France 
                        https://hal.archives-ouvertes.fr/hal-03317717 (accessed 21 April 2022).
                    
                    
                        Erjavec, T. and Pančur A. (2021) Parla-CLARIN: A TEI Schema for Corpora of Parliamentary Proceedings 
                        https://clarin-eric.github.io/parla-clarin/ (accessed 21 April 2022).
                    
                    
                        Erjavec, T., Pančur A. and Kopp M. (2022a). ParlaMint: Comparable Parliamentary Corpora. GLSL CLARIN ERIC 
                        https://github.com/clarin-eric/ParlaMint (accessed 21 April 2022).
                    
                    
                        Erjavec, T., Ogrodniczuk, M., Osenova, P., Ljubešić, N., Simov, K., Pančur, A., Rudolf, M., et al. (2022b). The ParlaMint corpora of parliamentary proceedings. Language Resources and Evaluation doi:10.1007/s10579-021-09574-0. 
                    
                    
                        Fišer, D., Eskevich, M. and Jong, F. de (eds). (2018). Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). Paris: European Language Resources Association (ELRA).
                    
                    
                        Fišer, D., Eskevich, M. and Jong, F. de (eds). (2020). Proceedings of the Second ParlaCLARIN Workshop. Marseille: European Language Resources Association (ELRA).
                    
                    
                        Fišer, D. and Lenardič, J. (2018). CLARIN resources for parliamentary discourse research. Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association (ELRA), pp. 2–7.
                    
                    
                        Galembert, C. de, Rozenberg, O., Vigour, C. (eds) (2013). Faire parler le parlement: méthodes et enjeux de l’analyse des débats parlementaires pour les sciences sociales. Paris: LGDL-Lextenso.
                    
                    
                        Hirst, G., Feng, V., Cochrane, C. and Naderi, N. (2014). Argumentation, Ideology, and Issue Framing in Parliamentary Discourse. In Cabrio E, Villata S. and Wyner A. S. (eds), Proceedings of the Workshop on Frontiers and Connections between Argumentation Theory and Natural Language Processing, Forlì-Cesena, Italy, July 21-25, 2014, CEUR-WS.org, 
                        http://ceur-ws.org/Vol-1341/paper6.pdf (accessed 26 April 2022). 
                    
                    
                        Ihalainen, P., Ilie, C. and Palonen, K. (2018). Parliament and Parliamentarism: A Comparative History of a European Concept, New York, Oxford: Berghahn.
                    
                    
                        Ilie, C. (2010). European Parliaments under Scrutiny: Discourse Strategies and Interaction Practices. Amsterdam; Philadelphia: John Benjamins.
                    
                    
                        Kišš, M., Beneš, K. and Hradiš, M. (2021). AT-ST: Self-training Adaptation Strategy for OCR in Domains with Limited Transcriptions. In Lladós, J., Lopresti, D., Uchida, S. (eds) Document Analysis and Recognition – ICDAR 2021. ICDAR 2021. Lecture Notes in Computer Science, vol 12824. Cham: Springer, 
                        https://doi.org/10.1007/978-3-030-86337-1_31 (accessed 26 April 2022).
                    
                    
                        Kodym, O. and Hradiš, M. (2021). Page Layout Analysis System for Unconstrained Historic Documents. Document Analysis and Recognition – ICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 5–10, 2021, Proceedings, Part II. Berlin, Heidelberg: Springer-Verlag, pp. 492–506.
                    
                    
                        Kohút, J. and Hradiš, M. (2021). TS-Net: OCR Trained to Switch Between Text Transcription Styles. In Lladós, J., Lopresti, D., Uchida, S. (eds) Document Analysis and Recognition – ICDAR 2021. ICDAR 2021. Lecture Notes in Computer Science, vol 12824. Cham: Springer, 
                        https://doi.org/10.1007/978-3-030-86337-1_32 (accessed 26 April 2022). 
                    
                    
                        La Mela, M., Norén, F., and Hyvönen, E. (2022). Digital parliamentary data in action (DiPaDA 2022), workshop co-located with the 6th Digital Humanities in the Nordic and Baltic countries conference (DHNB 2022), 
                        https://dhnb.eu/conferences/dhnb2022/workshops/dipada/ (accessed 26 April 2022).
                    
                    
                        Lemercier, C. (2021). Un catholique libéral dans le débat parlementaire sur le travail des enfants dans l’industrie (1840). Parlement[s], Revue d’histoire politique, 
                        33(1): 195–206.
                    
                    
                        Marnot, B. (2000). Les ingénieurs au Parlement sous la IIIe République. Paris: CNRS Editions.
                    
                    
                        Mikolov, T., Chen, K., Corrado, G. and Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. ArXiv:1301.3781 [Cs] 
                        http://arxiv.org/abs/1301.3781 (accessed 26 April 2022). 
                    
                    
                        Ouellet, J. and Roussel-Beaulieu, F. (2003). Les débats parlementaires au service de l’histoire politique. Bulletin d’histoire politique, 
                        11(3). Bulletin d’histoire politique: 23–40 doi:10.7202/1060736ar.
                    
                    
                        Puren, M. and Vernus, P. (2021). AGODA : Analyse sémantique et Graphes relationnels pour l’Ouverture et l’étude des Débats à l’Assemblée nationale. Inauguration Du BnF DataLab. Paris, France 
                        https://hal.archives-ouvertes.fr/hal-03382765 (accessed 26 April 2022). 
                    
                    
                        Rahtz, S. and Burnard, L. (2013). Reviewing the TEI ODD system. Proceedings of the 2013 ACM Symposium on Document Engineering. (DocEng ’13). New York, NY, USA: Association for Computing Machinery, pp. 193–96.
                    
                    
                        Rheault, L., Beelen, K., Cochrane, C. and Hirst, G. (2016). Measuring Emotion in Parliamentary Debates with Automated Textual Analysis. PLOS ONE, 
                        11(12). Public Library of Science: e0168843 doi:10.1371/journal.pone.0168843.
                    
                    
                        Rheault, L. and Cochrane, C. (2020). Word Embeddings for the Analysis of Ideological Placement in Parliamentary Corpora. Political Analysis, 
                        28(1). Cambridge University Press: 112–33 doi:10.1017/pan.2019.26.
                    
                    
                        Strien, D. A. van, Beelen, K., Ardanuy, M. C., Hosseini, K., McGillivray, B. and Colavizza, G. (2020). Assessing the Impact of OCR Quality on Downstream NLP Tasks. ICAART doi:10.5220/0009169004840496.
                    
                    
                        Study of parliament group (GB), F., Mark N. and Norton, P. (1993). Parliamentary Questions. Oxford: Clarendon Press.
                    
                    
                        Truan, N. (2019). Débats parlementaires sur l'Europe à l'Assemblée nationale (2002-2012) [Corpus]. ORTOLANG (Open Resources and TOols for LANGuage) - www.ortolang.fr, v1.1, 
                        https://hdl.handle.net/11403/fr-parl/v1.1 (accessed 21 April 2022c).
                    
                    
                        Truan, N. and Romary, L. (2021). Building, Encoding, and Annotating a Corpus of Parliamentary Debates in XML-TEI: A Cross-Linguistic Account. Journal of the Text Encoding Initiative 
                        https://halshs.archives-ouvertes.fr/halshs-03097333 (accessed 21 April 2022).
                    
                
            
        
    


11985	2022	
        
            Over the last decades, several breakthroughs have made the dream to automatically transcribe thousands of handwritten documents a reality (Causer et al., 2018; Sánchez et al., 2017; Seaward, 2017; Yin et al., 2013). For example, software like 
                Transkribus (Kahle et al., 2017) and 
                eScriptorium (Stokes et al., 2021) provide non-specialist users with simple environments to conduct transcription campaigns relying on efficient HTR
                
                    
                         HTR stands for Handwritten Text Recognition.
                    
                 engines. While transposing scriptures from a piece of paper onto a text editor used to require effort and concentration, it is now possible to imagine simply pressing a button and letting your computer work while you start preparing your next cup of tea. A few minutes later, your drink is ready, and so is the transcription of the two thousand pages you needed. As automatic transcription software is about to produce huge volumes of data (Clanuwat et al., 2019; Camps, 2021. See also the Vietnamica project
                
                    
                        Vietnamica is a research project undertaken jointly by the École Pratique des Hautes Études, the Institute of Hán-Nôm Studies, the Social Sciences Academy of Viêt Nam and the National University of Viêt Nam (Faculty of Humanities and Social Sciences). See 
                        
                            https://vietnamica.online/
                        
                    
                .), it seems crucial to think about how we can interact with the resulting files with maximum efficiency.
            
            In response to previous similar initiatives (Carius, 2020), we would like to present an end-to-end workflow revolving around the use of various automatic techniques to go from a set of digital images to the actual publication of a text edition. Such techniques include, on top of HTR, information extraction tools
                
                    
                         Rosa Stern defined information extraction as a task consisting of extracting and structuring, in semantic classes, the specific information elements contained in non-structured data for automatic processing, such as coreference resolution, relationship extraction, and named entity recognition (Stern, 2013, p. 59).
                    
                 and an open source and ready-to-use environment for publication. Moreover, we aim to make this framework as simple and generic as possible: it is independent from the transcription engine, and potentially compatible with any language, writing system, and any type of document (Balogh and Griffiths, 2020. See also the TEI Special Interest Group for East Asian/Japanese
                
                    
                         See 
                        
                            https://tei-c.org/Activities/SIG/EastAsian/
                        
                         and 
                        
                            https://wiki.tei-c.org/index.php/SIG:East_Asian
                        
                    
                ).
            
            Several key principles ensure the coherence of the workflow: transparency and availability of the data at each step and the use of a fully standardized format like TEI XML as the cornerstone to store all the available information. Other XML standards like ALTO
                
                    
                         See the Analyzed Layout and Text Object (ALTO) 4.2 schema specifications at 
                        
                            https://www.loc.gov/standards/alto/news.html#4-2-released
                        
                    
                 or PAGE (Pletschacher & Antonacopoulos, 2010) are commonly used by transcription software to export the output, but we advocate for a change of paradigm in order to give more importance to TEI earlier in the workflow (Scheithauer et al., 2020). The TEI guidelines define a set of elements to document this type of data, namely “sourceDoc” and its children
                
                    
                         See 
                        
                            https://tei-c.org/release/doc/tei-p5-doc/en/html/ref-sourceDoc.html
                        
                    
                . Leveraging TEI from the start is essential to connect the metadata of the images
                
                    
                        Including when the images are distributed within the IIIF framework .
                    
                 and documents, the text and layout information generated during the transcription, and any further editorial layer added to the raw transcription.
            
            
                
                    
                        
                            
                            
                        
                    
                
                
                    
                        Fig. 1: TEI as a threefold structure.
                    
                
            
            We imagine a configuration capable of processing a large family of TEI customizations as long as the file follows a structure (Fig. 1) in which:
            
                “teiHeader” stores the metadata,
                “sourceDoc” the raw transcription, and
                “body” the interpreted logical structure along with the editorial layers
                    
                        
                             Logical structure reconstruction can be performed semi-automatically (see the pipeline built for the LECTAUREP project called “LEPIDEMO”, 
                            
                                https://github.com/lectaurep/lepidemo
                            
                            ), or automatically with tools such as GROBID (
                            
                                https://github.com/kermitt2/grobid
                            
                            ).
                        
                    .
                
            
            We thus aggregate two phases in the digitization lifecycle which are often disconnected.
            Editorial operations can include preprocessing tasks such as post-HTR corrections (spell-checking) and text normalization, as well as information extraction (text mining). When the volume of data increases, extracting and linking named entities with indexes quickly risks becoming a laborious task. Instead, natural language processing tools can automate the process (Ehrmann et al., 2020; Frontini et al., 2015) all the while relying on the analysis of the sentences and words within their context. We developed 
                Semantic@, a proof of concept utilizing deep learning models, to extract named entities which are then cycled back into the TEI tree (Fig. 2). The extraction of named entities (i.e. names of people, places, or dates, etc.) is a crucial step before disambiguation which further permits to build links with open general or domain-specific knowledge bases. These steps allow for later explorations of the text with data mining technologies.
            
            
                
                    
                        
                            
                            
                        
                    
                
                
                    
                        Fig. 2: Virtuous circle for the enriched TEI document.
                    
                
            
            Once all the layers of an edition are connected into the same TEI file, edited documents can be posted online with softwares like 
                TEI Publisher (Turska et al., 2016; Chiffoleau et al., 2021). It provides a fully customizable environment where templates generate “views” based on the content of the XML files. With the aforementioned TEI structure, we propose an edition template containing:
            
            
                a flat representation of the transcription,
                an imitative representation of the transcription based on SVG
                    
                        
                             An XML-based markup language, see the Scalable Vector Graphics (SVG) 2 recommandations at
                             
                            
                                https://www.w3.org/TR/SVG2/
                            
                             ; we wish to point at the fact that working with SVG when displaying transcriptions allows us to deal with different writing systems and languages.
                        
                     integrating the layout of the pages,
                
                a diplomatic edition of the source document, based on the content of the body element, and
                a facsimile, using the IIIF protocol (Fig. 3).
            
            
                
                    
                        
                            
                            
                        
                    
                
                
                    
                        Fig. 3: A mock-up showing the four different views potentially available in an application like TEI-Publisher.
                    
                
            
            We would like to take the opportunity of presenting a short paper during the DH2022 international conference to subject our framework (Fig. 4) -and its robustness to different writing systems- to the scrutiny of the DH community. In particular, we believe that our proposition addresses challenges raised by Open Science, primarily the necessity to gain better control over every step within complex pipelines that involve various tools, thus facilitating reproducibility. A paradigm revolving around a pivotal element, like a TEI file grouping the different results, frees us from the constraint of a linear progression by maintaining multiple entry points in the workflow.
            
                
                    
                        
                            
                            
                        
                    
                
                
                    
                        Fig. 4: Simplifying the workflow by using TEI from the beginning.
                    
                
            
        
        
            
                
                    Bibliography
                    
                        Balogh, D. and Griffiths, A. (2020). DHARMA Encoding Guide for Diplomatic Editions EFEO ; Humboldt-Universität (Berlin) ; CEAIS - Centre d’Études de l’Inde et de l’Asie du Sud report 
                        https://halshs.archives-ouvertes.fr/halshs-02888186 (accessed 10 December 2021).
                    
                    
                        Camps, J.-B. (2021). Gallic(orpor)a: Extraction, annotation et diffusion de l’information textuelle et visuelle en diachronie longue Paper presented at the Inauguration du BnF DataLab, Paris 
                        https://www.academia.edu/58990010/Gallic_​orpor_​a_​Extraction_​annotation_​et_​diffusion_​de_​l_​information_​textuelle_​et_​visuelle_​en_​diachronie_​longue (accessed 9 December 2021).
                    
                    
                        Carius, J.-C. (2020). Plateforme d’éditions enrichies à l’INHA : Premier point d’étape d’un projet en cours d’élaboration Billet 
                        Numérique et recherche en histoire de l’art
                        https://numrha.hypotheses.org/1107 (accessed 8 December 2021).
                    
                    
                        Causer, T., Grint, K., Sichani, A.-M. and Terras, M. (2018). ‘Making such bargain’’: Transcribe Bentham and the quality and cost-effectiveness of crowdsourced transcription. 
                        Digital Scholarship in the Humanities doi:
                        10.1093/llc/fqx064. 
                        https://academic.oup.com/dsh/advance-article/doi/10.1093/llc/fqx064/4810663 (accessed 11 June 2018).
                    
                    
                        Chagué, A. and Scheithauer, H. (2021). 
                        LEPIDEMO, a Pipeline Demonstrator for LECTAUREP to Go from EScriptorium to TEI-Publisher. Jupyter Notebook doi:
                        10.5072/zenodo.977657. 
                        https://github.com/lectaurep/lepidemo (accessed 10 December 2021).
                    
                    
                        Chiffoleau, F., Baillot, A. and Ovide, M. (2021). A TEI-based publication pipeline for historical egodocuments -the DAHN project. 
                        Next Gen TEI, 2021 - TEI Conference and Members’ Meeting. Virtual, United States 
                        https://hal.archives-ouvertes.fr/hal-03451421 (accessed 10 December 2021).
                    
                    
                        Clanuwat, T., Lamb, A. and Kitamoto, A. (2019). KuroNet: Pre-Modern Japanese Kuzushiji Character Recognition with Deep Learning. 
                        ArXiv:1910.09433 [Cs]
                        http://arxiv.org/abs/1910.09433 (accessed 8 December 2021).
                    
                    
                        e-editiones (2021). 
                        Eeditiones/Tei-Publisher-App. XQuery e-editiones.org 
                        https://github.com/eeditiones/tei-publisher-app (accessed 10 December 2021).
                    
                    
                        Ehrmann, M., Romanello, M., Flückiger, A. and Clematide, S. (2020). Extended Overview of CLEF HIPE 2020: Named Entity Processing on Historical Newspapers. 
                        CLEF 2020 Working Notes. Conference and Labs of the Evaluation Forum. [online event]: Zenodo doi:
                        10.5281/ZENODO.4117566. 
                        https://zenodo.org/record/4117566 (accessed 10 December 2021).
                    
                    
                        Frontini, F., Brando, C. and Ganascia, J.-G. (2015). Semantic Web Based Named Entity Linking for Digital Humanities and Heritage Texts. In Zucker, A., Draelants, I., Zucker, C. F. and Monnin, A. (eds), 
                        First International Workshop Semantic Web for Scientific Heritage at the 12th ESWC 2015 Conference. Portorož, Slovenia: Arnaud Zucker and Isabelle Draelants and Catherine Faron Zucker and Alexandre Monnin 
                        https://hal.archives-ouvertes.fr/hal-01203358 (accessed 10 December 2021).
                    
                    
                        Kahle, P., Colutto, S., Hackl, G. and Mühlberger, G. (2017). Transkribus - A Service Platform for Transcription, Recognition and Retrieval of Historical Documents. 
                        14th IAPR International Conference on Document Analysis and Recognition (ICDAR), vol. 04. pp. 19–24 doi:
                        10.1109/ICDAR.2017.307.
                    
                    
                        Kiessling, B. (2021). 
                        Mittagessen/Kraken. Python 
                        https://github.com/mittagessen/kraken (accessed 10 December 2021).
                    
                    
                        Lopez, P. (2008). 
                        GROBID. Java 
                        https://github.com/kermitt2/grobid (accessed 10 December 2021).
                    
                    
                        Pletschacher, S. and Antonacopoulos, A. (2010). The PAGE (Page Analysis and Ground-truth Elements) format framework. pp. 257–60 doi:
                        10.1109/ICPR.2010.72.
                    
                    
                        Sánchez, J. A., Romero, V., Toselli, A. H., Villegas, M. and Vidal, E. (2017). ICDAR2017 Competition on Handwritten Text Recognition on the READ Dataset. IEEE Computer Society, pp. 1383–88 doi:
                        10.1109/ICDAR.2017.226. 
                        https://www.computer.org/csdl/proceedings-article/icdar/2017/3586b383/12OmNy4IEXJ (accessed 9 December 2021).
                    
                    
                        Scheithauer, H., Chagué, A., Gabay, S., Romary, L., Janes, J. and Jahan, C. (2021). From page to content – which TEI representation for HTR output?. 
                        Next Gen TEI, 2021 - TEI Conference and Members’ Meeting. Weaton (virtual), United States 
                        https://hal.archives-ouvertes.fr/hal-03380807 (accessed 7 December 2021).
                    
                    
                        Seaward, L. (2017). Project Update – teaching a computer to READ Bentham 
                        UCL Transcribe Bentham
                        http://blogs.ucl.ac.uk/transcribe-bentham/2017/06/09/project-update-teaching-a-computer-to-read-bentham/ (accessed 4 June 2018).
                    
                    
                        Stern, R. (2013). Identification automatique d’entités pour l’enrichissement de contenus textuels Université Paris-Diderot - Paris VII phdthesis 
                        https://tel.archives-ouvertes.fr/tel-00939420 (accessed 10 December 2021).
                    
                    
                        Stokes, P. A., Kiessling, B., Ezra, D. S. B., Tissot, R. and Gargem, E. H. (2021a). The eScriptorium VRE for Manuscript Cultures. 
                        Classics@ Journal. [online] 
                        https://classics-at.chs.harvard.edu/classics18-stokes-kiessling-stokl-ben-ezra-tissot-gargem/ (accessed 30 November 2021).
                    
                    
                        Stokes, P. A., Kiessling, B., Ezra, D. S. B., Tissot, R. and Gargem, E. H. (2021b). The eScriptorium VRE for Manuscript Cultures. 
                        Classics@ Journal. [online] 
                        https://classics-at.chs.harvard.edu/classics18-stokes-kiessling-stokl-ben-ezra-tissot-gargem/ (accessed 30 November 2021).
                    
                    
                        Terriel, L. (2021). 
                        Semantic@. Python 
                        https://github.com/Lucaterre/semanticat (accessed 10 December 2021).
                    
                    
                        Tissot, R. (2021). 
                        Scripta/EScriptorium. Python 
                        https://gitlab.com/scripta/escriptorium/-/tree/v0.10.2a (accessed 10 December 2021).
                    
                    
                        Turska, M., Cummings, J. and Rahtz, S. (2016). Challenging the Myth of Presentation in Digital Editions. 
                        Journal of the Text Encoding Initiative(Issue 9). Text Encoding Initiative Consortium doi:
                        10.4000/jtei.1453. 
                        https://journals.openedition.org/jtei/1453 (accessed 10 December 2021).
                    
                    
                        Yin, F., Wang, Q.-F., Zhang, X.-Y. and Liu, C.-L. (2013). ICDAR 2013 Chinese Handwriting Recognition Competition. IEEE Computer Society, pp. 1464–70 doi:
                        10.1109/ICDAR.2013.218. 
                        https://www.computer.org/csdl/proceedings-article/icdar/2013/06628856/12OmNxEBzcq (accessed 9 December 2021).
                    
                    
                        
                            https://gitlab.com/scripta/escriptorium/-/tree/v0.10.2a
                        
                    
                
            
        
    


11988	2022	
        
            Research problem statement
            This paper documents the intersections of language and the public sphere through a cross-sectional model of comments on public Facebook (FB) pages of selected newspapers from 2015-2019. The analysis examines language use in discussions of current events, differences between national and regional newspapers, and social media insights into the conduct of public discourse. A common observation in the Philippines is, while English is used for official documents, tertiary education, and national broadsheets, oral discussions tend to involve Filipino or regional languages (Gonzalez, 1998). Class and political differences are also heavily associated with language use and media preferences (Kusaka 2017). Social media, with its informal written language, yet socially and politically relevant content, may thus offer insights into contemporary language use and public engagement with media.
            Methodology
            Comments on public FB pages of selected national and regional newspapers (
                Manila Bulletin, 
                Manila Times, 
                Philippine Daily Inquirer, 
                Philippine Star, 
                Cebu Daily News, 
                Mindanews, 
                Mindanao Times, 
                Sun Star Cebu, 
                Sun Star Davao, and 
                The Freeman) were captured through 
                Facepager (Jünger & Keyling 2019). The corpus is taken from a separate project on Muslim identities in the Philippines and consists of data from selected months in 2015, 2017, and 2019.
            
            While language identification is a common task in natural language processing, most of the available software casts this as a multi-class classification task, where only a single language can be assigned to a textual document. In the present case, involving code-switching as a signature feature, multiple languages can be simultaneously present in a document, thus turning this task effectively into a multilabel classification problem. We finetuned a bespoke multilabel classifier on top of a pretrained BERT (Devlin et al., 2019) feature extractor (the ‘bert-base-multilingual-uncased’ model). We only considered messages where the target language(s) could be identified and divided these into a train, validation and test set of 10,000 social media messages (containing 7,304, and 2 x 913 instances respectively). We manually annotated for the presence/absence of Tagalog/Filipino, Cebuano, and English respectively. We compared the performance of this SOTA approach to a simpler baseline, consisting of a conventional multitarget classifier in the form of a random forest (RF) (Pedregosa et al., 2011) of 16,156 manually annotated entries (with a train, validation and test set of 12,998 entries, and 2 x 2,294 instances), trained on top of a TF-IDF representation of a vocabulary character n-grams (for 2 ≤ n ≤ 6) (see details Table 2 below). Finally, we applied the BERT language detector (with the weights that optimized the validation performance) to the unseen data.
            
                
            
            
                Table 1 – Number of entries per language in Test, Train and Validation Sets
            
            
                
            
            
                Table 2 – Test Accuracies
            
            Findings
            Both classifiers were able to demonstrate similar general trends: Tagalog/Filipino entries were the most common overall (Figures 1 and 2). Regionally, more diverse language use is noticeable. Cebuano comments were prominent in at least two Cebu-based newspapers, while the Mindanao Times and Sun Star Davao featured Tagalog/Filipino as the most-used language, followed by English and Cebuano (Figures 3 and 4), suggesting the usage of Tagalog/Filipino even where Visayan languages are prominent. Despite the presence of monolingual English-language newspapers, the fact that current events are 
                written about and responded to by a multilingual Philippine public sphere is often obscured. Social media thus offers opportunities not only for re-thinking monolingual norms in media, but may also act as a forum for revitalizing written forms of regional languages, while acting as a potent corpus and resource for codeswitching and informal written language for automatic language identifiers. 
            
            
                
            
            Fig. 1 – Language Use (BERT)
            
                
            
            Fig. 2 – Language Use (RF)
            
                
            
            Fig. 3 – Languages per Newspaper (BERT)
            
                
            
            Fig. 4 – Languages per Newspaper (RF)
        
        
            
                
                    Bibliography
                    
                        Devlin, J., Chang, M.-W., Lee, K. and Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 
                        Proceedings of NAACL-HLT, Minneapolis, MN, June 2019, pp. 4171–4186.
                    
                    
                        Gonzalez, A. (1998). The Language Planning Situation in the Philippines. 
                        Journal of Multilingual and Multicultural Development, 
                        19(5 & 6): 487–525.
                    
                    
                        Jünger, J. and Keyling, T. (2019). 
                        Facepager: An application for automated data retrieval on the web.
                         
                        
                            https://github.com/strohne/Facepager/
                         (accessed April 7 2022).
                    
                    
                        Kusaka, W. (2017). 
                        Moral Politics in the Philippines. Singapore/Japan: NUS Press & Kyoto University Press.
                    
                    
                        Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Bertrand, T., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., David Cournapeau, Brucher, M., Perro, M., and Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. 
                        Journal of Machine Learning Research, 
                        12: 2825–2830.
                    
                
            
        
    


11991	2022	
        
            Historians have since long stressed the political and cultural functions of memory and heritage for societies. The more born-digital data have a past of their own, the more memory has also become a topic of interest for data scientists (Au Yeung and Jatowt 2011, Keegan and Brubaker 2015, Graus et.al. 2018, West and Leskovec 2021). Stringently connected to memory but much less studied in data science is our experience of time (Jatowt et.al. 2015 and Van Eijnatten and Huijnen 2021 being notable exceptions). The influential theory of François Hartog, for example, explains the memory boom from the establishment of a ‘time regime of the present’ at the end of the 1980s (Hartog 2015). Aleida Assmann goes against Hartog’s subsequent assertion that an increasingly shorter present is all we are left with (Assmann 2020, 139). Instead, she puts forward her notion of cultural memory to replace ideas of temporal ruptures with a model that ‘emphasizes the ineluctable entanglement of [past, present and future]’ (Assmann 2020, 195-6). 
            Departing from this theory, this study proposes a computational approach to study the relation between the present and the past in twentieth century newspapers by analysing trends in phrases of the format ‘n years ago’. Obviously, there is a plethora of ways in language can evoke the past. However, there are two arguments that justify singling out ‘n years ago’. First, ‘n years ago’ is, as an expression, ubiquitous and syntactically stable in Dutch newspaper language throughout the studied period of the twentieth century. Second, unlike explicit references to years, events or persons in the past, ‘n years ago’ intricately ties the past to the present. The phrase presents the past—and keeps it present—
                as something useful for the present. This notion of ‘useful past’ or ‘present past’ (Paul 2015, 25-27) forms part of Assmann’s critique of Hartog’s theory of temporal orders. 
            
            
                Questions and method
                The questions that are at the center of this study are how trends in references to present pasts relate to named philosophies of time experience, but also which past remains present and how these trends change over time. It takes newspapers as data, because of the vital role they as the ‘first rough draft of history’ have played in memory culture throughout the twentieth century. This study is based on the digitized versions of the most important nation-wide and regional newspapers the Dutch National Library holds and has made available.
                    
                        
                            
                                www.delpher.nl/kranten
                            
                            . 
                        
                     The preliminary results presented here are based on the example of the national newspaper Telegraaf (1893-1989) of 10,000 documents (articles and advertisements) per year.
                    
                        
                            For the final paper, these titles will be complemented with other available newspaper titles with similar century-spanning scopes. Subsequently, all results will be aggregated to come to an encompassing picture of the present past in Dutch newspapers by means of references to ‘n years ago’. The Telegraaf was chosen as an example here, because it shows trends that are paradigmatic for most other newspapers.
                        
                     These documents have been rid of duplicates and cleaned with the help of Python’s NLTK package.
                    
                        
                            
                                https://github.com/PimHuijnen/looking_back_newspapers
                            
                            . Cleaning in this script is mostly restricted to the removal of punctuation and caps. Stop words are not removed to guarantee that the Dutch word for the English indefinite article ‘a’ (‘een’), which is part of any standard stop word list, remains part of the data. Similarly, the removal of numbers is no part of preprocessing to allow for phrases like ’10 years ago’(even though Dutch linguistic convention formally does not allow for numbers in running text).
                        
                    
                
            
            
                Analysis and results
                The analysis is done with Python scripts in Jupyter Notebooks and consists of three subsequent steps: 
                The first step is the extraction of a list of the most common trigrams ending with ‘year(s) ago’
                    
                        
                             The Dutch equivalent of ‘year(s) ago’ is for both one and more years written in the singular form ‘jaar geleden’.
                        
                     from the cleaned and sampled dataset. This list is sorted by decade and by frequency to get an idea of the years that newspapers most often use in the phrase ‘n years ago’ throughout the twentieth century. This indicates that single digit years (one – nine) make up the most common phrases of ‘n years ago’, along with decades (ten, twenty, etc.) and one hundred.
                
                
                    Table 1: Most common trigrams ending in ‘year(s) ago’ with their English translation from a sample of 10,000 documents from the national newspaper De Telegraaf per decade per million trigrams, 1890-1980.
                
                
                    
                        92
                        92
                        32
                        32
                        32
                        32
                        32
                        32
                        32
                        32
                        32
                        32
                    
                    
                        Trigram
                        English translation
                        1890
                        1900
                        1910
                        1920
                        1930
                        1940
                        1950
                        1960
                        1970
                        1980
                    
                    
                        
                            Een jaar geleden
                        
                        One year ago
                        1,95
                        4,02
                        4,62
                        8,81
                        21,77
                        13,23
                        16,06
                        19,51
                        17,90
                        16,16
                    
                    
                        
                            Twee jaar geleden
                        
                        Two years ago
                        1,70
                        2,10
                        2,70
                        3,18
                        7,51
                        5,60
                        12,67
                        22,43
                        27,79
                        31,44
                    
                    
                        
                            Twintig jaar geleden
                        
                        Twenty years ago
                        1,27
                        0,50
                        0,59
                        2,34
                        3,28
                        4,24
                        3,39
                        3,34
                        4,07
                        6,08
                    
                    
                        
                            Paar jaar geleden
                        
                        Few years ago
                        1,10
                        1,26
                        1,06
                        1,50
                        1,53
                        3,39
                        2,74
                        7,05
                        9,22
                        10,01
                    
                    
                        
                            Vier jaar geleden
                        
                        Four years ago
                        0,76
                        0,17
                        0,53
                        1,20
                        2,48
                        1,36
                        3,39
                        7,37
                        10,85
                        11,99
                    
                    
                        
                            Drie jaar geleden
                        
                        Three years ago
                        0,68
                        1,26
                        1,06
                        1,86
                        3,54
                        2,88
                        6,22
                        11,08
                        14,50
                        14,75
                    
                    
                        
                            Dertig jaar geleden
                        
                        Thirty years ago
                        0,59
                        0,42
                        0,26
                        0,72
                        1,86
                        2,71
                        2,42
                        2,92
                        1,89
                        3,09
                    
                    
                        
                            Tien jaar geleden
                        
                        Ten years ago
                        0,51
                        1,01
                        0,79
                        1,80
                        4,16
                        4,24
                        5,25
                        9,54
                        12,39
                        15,42
                    
                    
                        
                            Honderd jaar geleden
                        
                        One hundred years ago
                        0,42
                        0,50
                        0,73
                        1,14
                        2,08
                        2,54
                        1,86
                        2,86
                        2,02
                        1,98
                    
                    
                        
                            Acht jaar geleden
                        
                        
                            Eight years ago
                        
                        0,42
                        0,25
                        0,20
                        0,60
                        0,91
                        0,85
                        1,05
                        2,97
                        3,20
                        3,59
                    
                
                In a second step, the most common references of ‘n years ago’ are plotted, relative to one another, over time. Figures 1-3 show the trajectories of all single years together (figure 1), of all decades from ten to one hundred (figure 2) and of the single years one, ten, one hundred and two hundred (figure 3). These figures show that Dutch newspapers started to look back in time by the use of the phrase ‘n years ago’ since the 1930s. Once they did, the use of some variations of this phrase strongly gained traction, particularly in reference to the near past (one to ten years ago). 
                The final step of the analysis is to look at the actual years that the phrase ‘n years ago’ referred to throughout the twentieth century. Do newspapers tend to look back at specific years, as in the end of the Second World War being ‘five years ago’ in 1950? Or are ‘notable’ years submerged in what is here called ‘everyday memory culture’? The latter seems to be the case if these years are calculated for ‘n years ago’, where n stands for the years from one to twenty and decades from twenty to two hundred. Figure 5 shows that ‘n years ago’ tends to evoke the recent past itself above any particular year. No single year really stands out.
                    
                         Given the nature of this method, the decline in the frequency of years after 1975 that Figure 5 shows is as necessary as it is meaningless. There is, after all, increasingly less data (that ends in 1989) on which these numbers can be based. 
                    
                
            
            
                Conclusion
                In the light of Hartog’s theory of an all-encompassing present, one would have expected a steady decrease of references to the past and the future. This study shows the opposite and substantiates Assmann’s contention that interest for the past returns, particularly in the second half of the twentieth century, in the form of memory culture. With the growing popularity of the studied phrase, Dutch newspapers became mediators of a form of memory culture than can be seen as ‘latent’ or ‘everyday’ in that it emphasizes recurring events rather than returning to a specific thing in the past. This is, particularly, true for the phrases ‘two years ago’ and ‘four years ago’ (Figure 4), the spikes in the diagrams of which indicate references to important sport (Olympics, European and World Championship soccer) and political events (national parliamentary elections).
                The use of the phrase ‘n years ago’ is by no means the only, nor the most important manifestation of memory culture. This limits the explanatory power of this study. In contrast with official and cultivated forms of memory culture, however, it does shed light on the latent, almost oblique, everyday invocation of the past that forms just as much part of that culture.
                
                    Figure 1: Frequency over time of ‘n years ago’, where n stands for one to ten, per million words in a sample of 10,000 documents per year of the national newspaper 
                    De Telegraaf
                    , 1893-1989. For similar diagrams (figures 1-4) for other newspapers, see: https://github.com/PimHuijnen/looking_back_newspapers/tree/main/Data.
                
                
                    
                
                
                    Figure 2: Frequency over time of ‘n years ago’, where n stands for decades from ten to one hundred, per million words in a sample of 10,000 documents per year of the national newspaper 
                    De Telegraaf
                    , 1893-1989.
                
                
                    
                
                
                    Figure 3: Frequency over time of ‘n years ago’, where n stands for one, ten, one hundred and two hundred, per million words in a sample of 10,000 documents per year of the national newspaper 
                    De Telegraaf
                    , 1893-1989.
                
                
                    
                
                
                    Figure 4: Frequency over time of ‘four years ago’ per million words in a sample of 10,000 documents per year of the national newspaper 
                    De Telegraaf
                    , 1893-1989.
                
                
                    
                
                
                    Figure 5: Frequency over time of dates (in years) that are referred to via the phrase ‘n years ago’, where n stands for years from one to twenty and decades from twenty to two hundred, per million words in a sample of 10,000 documents per year of the national newspaper 
                    De Telegraaf
                    , 1893-1989.
                
                
                    
                
            
        
        
            
                
                    Bibliography
                    
                        Assmann, Aleida (2020). 
                        Is Time out of Joint? On the Rise and Fall of the Modern Time Regime. Ithaca: Cornell University Press.
                    
                    
                        Au Yeung, Ching-man and Jatowt, Adam (2011). 
                        Studying how the Past is Remembered: Towards Computational History through Lare Scale Text Mining. 
                        Proceedings of the 20
                        th
                         ACM International Conference on Information and Knowledge Management, 1231-1240. DOI: 
                        https://doi.org/10.1145/2063576.2063755. 
                    
                    
                        Eijnatten, Joris van and Pim Huijnen (2021). Something Happened to the Future: Reconstructing Temporalities in Dutch Parliamentary Debate, 1814-2018. 
                        Contributions to the History of Concepts, 16: 52-82. DOI: 
                        https://doi.org/10.3167/choc.2021.160204. 
                    
                    
                        Graus, David, Daan Odijk and Maarten de Rijke (2018). The Birth of Collective Memories: Analyzing Emerging Entities in Text Streams. 
                        Journal of the Association for Information Science and Technology, 69: 773-786. DOI: 
                        http://dx.doi.org/10.1002/asi.24004. 
                    
                    
                        Hartog, François (2015). 
                        Regimes of Historicity: Presentism and Experiences of Time. New York: Columbia University Press.
                    
                    
                        Jatowt, Adam et.al. (2015). Mapping Temporal Horizons: Analysis of Collective Future and Past related Attention in Twitter. 
                        WW ’15: Proceedings of the 24
                        th
                         International Conference on World Wide Web, 484-494. DOI: 
                        http://dx.doi.org/10.1145/2736277.2741632. 
                    
                    
                        Keegan, Brian C. and Jed R. Brubaker (2015). ”Is” to “Was”: Coordination and Commemoration in Posthumous Activity on Wikipedia Biographies. 
                        Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work & Social Computing, 533-546. DOI: 
                        https://doi.org/10.1145/2675133.2675238. 
                    
                    
                        Paul, Herman (2015). 
                        Key Issues in Historical Theory. New York and London: Routledge.
                    
                    
                        West, Robert, Jure Leskovec and Christopher Potts (2021). Postmortem Memory of Public Figures in News and Social Media. 
                        Proceedings of the National Academy of Science 118. DOI: 
                        https://doi.org/10.1073/pnas.2106152118. 
                    
                
            
        
    


11996	2022	
        
            
                Introduction 
                Between 2018 and 2020, the digitization project TASTEN, funded by the German
                    government, digitized 3200 piano rolls for self-playing pianos preserved at the
                    Musical Instruments Museum at Leipzig University (MIMUL). A piano roll is a historic
                    music storage media (MSM) coding movement impulses through holes punched into
                    paper (Focht, 2020). When played, a pneumatic system uses this code to create
                    sounds at runtime.
                    For musicologists, such piano rolls are of high value. First, they are the only source of
                    musical performances by famous pianists in times preceding sound recording
                    technologies. Secondly, for researchers they offer a rich repository to study the
                    music-making practice and musical interpretation around 1900. We expand our prior
                    source catalogue to include their technical predecessors: Cardboard and metal plates,
                    which also use punched holes as code (Focht, 2021). They share the issue of being
                    fragile – after decades of usage and storage – making it important to digitize them
                    for preservation, also.
                
            
            
                Related Work 
                We found a lack of publications dealing with challenges, issues, and processing of MSM in general. This may be due to most of such projects being on a small scale in
                    private settings (pianola.co.nz, 2021; IAMMP, 2021). Scientific publications are
                    presented by Debrunner (2013), who developed a scanner capable of reading
                    information directly from piano rolls and dealing with paper based distortion issues,
                    for a single format (Welte piano rolls). Shi et al. (2019) built an online database of almost 500 digitized
                    piano rolls offering representations including images, audios, and MIDI files, focusing
                    only on the same format.
                    No published scientific or private projects are to be found concerning metal or
                    cardboard plates and their digitization.
                
                
                    
                
                Figure 1: Pipeline of the digitizing of piano rolls and plates through different processing steps and manual input of a Musicologist
            
            
                Digitizing MSM
                While prior projects show great results in digitizing piano rolls, processes capable of
                    dealing with multiple formats of piano rolls and plates in general are non-existent.
                    Furthermore, digitizing is only the first step for musicologists. This data allows them
                    to answer research questions by reading and hearing the encoded works which are
                    difficult to impossible to read. Additionally, these processes allow for distant reading
                    analysis and comparative approaches.
                    We propose a workflow (Figure~1) capable of digitizing all 3,200 piano rolls of 30
                    different formats available in the MIMUL. Currently, work is done to include more
                    than 25 formats of 438 plates.
                    The workflow begins with a conservator-led cleaning process to protect the objects
                    and the researchers. Damages which would lead to the destruction of the object
                    were documented. Next, metadata like weight, measurements, format, title,
                    composer, and performer was extracted to be included in our research tool
                    musiXplora. For the actual digitizing, a scanning company was commissioned, for which we constructed an unwinding mechanism, making it possible to create a single
                    scan of the piano rolls (as 300dpi .tif images, resulting in up to 5.000x550.000 pixels and up to 5GB).
                    While the prior project (TASTEN – 2018-2020) generated scans of piano rolls, the
                    current DISKOS project focuses more deeply on musicologists’ research questions.
                    Examples would be “How did composers play their own compositions on the piano?”,
                    or “Can the computer use this digital knowledge to identify which pianist played a
                    piano roll of unknown origin?” Also, exploration and visual analysis of the objects can
                    be offered through distant reading visualization systems embedded in the
                    musiXplora.
                
            
            
                Technical Details
                Starting with the preprocessing of the backlit images the actual process labels
                    connected components (musical notes) on the image and applies filtering to keep
                    only the components representing notes. Calculating the distance from each hole to
                    the edge of the medium and using mean shift clustering we can assign each note to
                    its respective track. We finally apply corrections to account for empty tracks and
                    distortions and calculate the position and width of each hole. Combining this
                    information with an expert created mapping of tracks to MIDI notes, we can then
                    generate a MIDI file accurately representing the information on the medium.
                    Relevance for Musicology
                    These files allow users to work interactively with the music and help musicologists in
                    their work with these sources. Furthermore, this offers an enhanced experience for
                    museum visitors, by making it possible to voice the digitized piano rolls and plates
                    using the digital representation of keyboard instruments created during TASTEN.
                    Hence, historic media can be experienced on historic instruments even if the physical
                    instruments would not have been interoperable.
                    For the musicologists, these results offer a way to open up previously unreadable
                    sources. Besides close reading approaches, the generated data allows for distant
                    reading (visualization) methodology and novel research questions like examining
                    which schools of interpretation and playing techniques are represented and how they
                    have spread between performers. 
                    Further, these results are also important for educational and playful aspects like
                    listening to these historical virtuous interpretations without risking the media and
                    instruments.
                
            
            
                Limitations
                As we are in the first of three project years, some challenges still exist: The image
                    processing results are highly dependable on a suitable preprocessing, which differs
                    quite a lot even between different formats of the same media type. Metal plates in
                    particular are operated under pressure during playback. While the playback
                    instruments themselves are constructed to negate this deformation, a plain
                    photography of the media does not lead to correct results and needs a semi-manual
                    correction process. In general, we are content with our results for cardboard plates
                    and piano rolls of specific formats, but are aware of needed improvements.
                
            
            
                Conclusion
                Even with the best preservation techniques, most materials deteriorate from
                    humidity and stress through time. Hence, valuable information stored on analog
                    media is prone to damage and even loss. Such information includes original
                    recordings of famous virtuosos like Edvard Grieg, not only valuable as music
                    recording, but also important for musicologists interested in analyzing playing
                    techniques and differences between musical notation and interpretation by their
                    composers. To allow digital processing of such media, digitization is mandatory. We
                    present a pipeline taking image sources of circular and linear played storage media
                    and creating digital representations in form of MIDI files, which then can be analyzed,
                    further processed, edited, or even played through modern and historical instruments.
                
            
        
        
            
                
                    Bibliography
                    
                        pianola.co.nz
                        (2021): Saving the Music of Yesterday. 
                        http://www.pianola.co.nz/public/index.php (Accessed: 01 January 2022)
                        
               
                        Comaniciu, D., & Meer, P. 
                        (2002). Mean shift: A robust approach toward feature space analysis. 
                        IEEE Transactions on Pattern Analysis and Machine Intelligence
                        , 24(5), 603–619. https://doi.org/10.1109/34.1000236
                        
               
                        Debrunner, D.
                         (2013). Von der Welte-Rolle zu parametrisierbaren Wiedergabe auf synthetischen Instrumenten und MIDI-fähigen Selbstspielklavieren. 
                        In C. E. Hänggi & Köpp (Eds.), Recording the Soul of Music: Welte-Künstlerrollen für Orgel und Klavier als authentische Interpretationsdokumente.
                        
               
                        Focht, J.
                         (2020). 
                        MusiXplora
                        . https://musixplora.de/mxp/2002522 (Accessed 01 January 2022)
                        
               
                        Focht, J.
                         (2021). 
                        MusiXplora.
                        https://musixplora.de/mxp/2003518 (Accessed 01 January 2022)
                        
               
                        IAMMP
                        (2021). 
                        International Association of Mechanical Music Preservationalists
                        . http://www.iammp.org/ (Accessed 01 January 2022)
                        
               
                        Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.
                         (2011). Scikit-learn: Machine Learning in Python. 
                        Journal of Machine Learning Research
                        , 12, 2825–2830.
                        
               
                        Shi, Z., Sapp, C. S., Arul, K., McBride, J., & Smith III, J. O.
                         (2019). Supra: Digitizing the Stanford University Piano Roll Archive. 
                        In A. Flexer, G. Peeters, J. Urbano, & A. Volk (Chairs), ISMIR, Delft
                        .
                        
               
                        van der Walt, S., L. Schönberger, J., Nunez-Iglesias, J., Boulogne, F., D. Warner, J., Yager, N., Gouillart, E., Yu, T.
                         (2014). scikit-image: Image processing in Python.
                        PeerJ
                         2:e453 https://doi.org/10.7717/peerj.453
                        
               
                        The ImageMagick Development Team.
                        (2021). 
                        ImageMagick
                        . Retrieved from https://imagemagick.org (Accessed 01 January 2022)
                    
                
            
        
    


12003	2022	
        
            There has been an abundance of research on the lexical (Bergel et al., 2016) and prosodic (Anttila and Heuser, 2016) features of poetic texts. Some recent attempts such as Šeļa et al. (2020) combine the two feature sets to model the association between poetic meter and meaning. In this project, we ask how the relative importance of lexical and prosodic features vary across different forms and genres. We pose this question by classifying different categories of English poetry with lexical and prosodic features separately. Then we compare the results of the parallel experiments, to ask which categories have a stronger lexical or prosodic character.
            We collected all 37,700 English poems that are tagged with a certain “genre” from the Chadwyck-Healey Literature Collections (http://collections.chadwyck.com). (Note that our use of the term “genre” is drawn from our source; scholars might well characterize some of these genres as “forms” or “modes.”) Since many of the 44 total genres only have a few cases, we kept the poems of the top 8 genres only, which consist of 30,704 (81.44%) poems. Next, we trained a lexical classifier and a prosodic classifier and extracted features for each of them. For the lexical classifier, the features were extracted by transforming the texts into an array of word frequencies, and a grid search was conducted to select the algorithm and number of features used for classification. Optimization is achieved when the random forest algorithm and top 500 features are used. As for the prosodic classifier, we used the Python library “Poesy” (https://github.com/quadrismegistus/poesy) to obtain “foot type”, “feet number”, and “rhyme style” of each poem. Once again, the best performance is achieved when the random forest algorithm is used.
            We then conducted classification experiments based on a random train-test split (70% training vs 30% testing) and compared the performances of the two classifiers on an 8-genre classification task and a 2-class classification task, where we classified each of the top 8 forms against all others (e.g., “sonnet” vs “non-sonnet”, “ballad” vs “non-ballad”).
            
                
            
            Figure 1. Confusion matrix of models trained with different feature sets (8-genre classification)
            In the 8-genre classification experiment, while the lexical and the prosodic classifier have similar overall performances, their results on different genres significantly differ: the lexical classifier classifies ballads and metrical psalms very well, but easily confuses heroic couplets with sonnets and epigrams, while the prosodic classifier classifies heroic couplets very well but easily confuses ballads and metrical psalms with lyrics.
            
                
            
            Figure 2. F1 score of the classification experiments of different genres trained with different feature sets (2-class classification)
            The results of 2-class classification demonstrate a similar pattern: sonnets and heroic couplets are better distinguished from other forms when using classifiers trained with prosodic features, while ballads and metrical psalms are better distinguished from other forms when using classifiers trained with lexical features. These results preliminarily suggest that different genres are distinguished with different features: while ballads and metrical psalms are distinguished by the diction they use, sonnets and heroic couplets are defined by prosodic features.
            We also examined the performance of both classifiers on poems by different authors. Here, we used the “hold-out-one” strategy: we selected all poems by authors with at least 30 poems in the dataset and trained the classifiers with all poems except those written by an author, and tested the results on the poems written by that author.
            
                
            
            Figure 3. Performances of classifiers on poems of different poets
            In figure 3, most authors of translated works are in the bottom of the figure (their works cannot be easily classified by prosodic features). We also see preliminary evidence that there might be a correlation between poetic prominence and prosodic regularity: the prominent poets mostly appear in the upper half (their works can be well classified by prosodic features). It is likely that the prominent poets continuously stick to certain prosody conventions for each genre, while the translated poems use very different prosody in the same genre, which is understandable as prosodic pattens are easily lost in translation. The accuracy of classification with lexical features does not show a consistent pattern. However, two influential early modern authors, Spencer and Shakespeare, appear in the top-right corner (their works can be very well classified by both lexical and prosodic features), perhaps indicating that they set the standard for following authors in both the diction and prosody used in different genres.
            To further explore these observations, in the future we will further investigate the relationships between the accuracy of prediction and factors such as the date of the poem, the poets’ canonicity, and their nationality. Additionally, we will also take word order into consideration and use N-gram in supplement of single-word frequencies.
            The findings above already tell us a lot of things about the history of English poetry like the roles of lexical and prosodic features in poetry genre classification, and how such roles differ in different genres. However, the most important potential contribution of this project is to distinguish the concepts of “genre” and “form”: if some poetic categories are best identified by prosody, and others by “content” (represented by lexical features), it might be possible to disentangle “form” from other aspects of “genre.” For example, it is observed that heroic couplets and epigrams use similar diction, what distinguish them as two “genres” is their “forms”; in contrast, ballads and lyrics are in similar “forms”, and they are considered as different “genres” because of the words they use.
            This could reinforce the argument of King (2021) that “genre” operates on a larger scale than “form”. Furthermore, in addition to understanding how the transformation of poetic “forms” was related to narratives of culture (Martin, 2012), insights can also be gained on the evolution of the roles of “content” and “form” in defining “genres” of poems by different poets and in different periods and locations. While various follow-up experiments need to be done, there is preliminary evidence that the works of prominent poets tend to be close to a prosodic prototype for a genre.
        
        
            
                
                    Bibliography
                    
                        Anttila, A. and Heuser, R. (2016). Phonological and Metrical Variation across Genres. In Hansson, G. Ó., Farris-Trimble, A., McMullin, K. and Pulleyblank D. (eds), 
                        Proceedings of the 2015 Annual Meetings on Phonology. Washington, D.C.: Linguistic Society of America. https://doi.org/10.3765/amp.v3i0.3679
                    
                    
                        Bergel, G., Howe, C. J. and Windram, H. F. (2016). Lines of Succession in an English Ballad Tradition: The Publishing History and Textual Descent of The Wandering Jew’s Chronicle. 
                        Digital Scholarship in the Humanities, 
                        31(3), 540–562.
                    
                    
                        King, R. S. (2021). The Scale of Genre. 
                        New Literary History, 
                        52(2), 261–284. https://doi.org/10.1353/nlh.2021.0012
                    
                    
                        Martin, M. (2012). 
                        The Rise and Fall of Meter: Poetry and English National Culture, 1860–
                        1930. Princeton: Princeton University Press.
                    
                    
                        Šeļa, A., Orekhov, B. and Leibov, R. (2020). Weak Genres: Modeling Association Between Poetic Meter and Meaning in Russian Poetry. In Karsdorp, F., McGillivray, B., Nerghes, A. and Wevers, M. (eds), 
                        Proceedings of the Workshop on Computational Humanities Research (CHR 2020). CEUR Workshop Proceedings, pp. 12–31.
                    
                
            
        
    


12014	2022	
        
            
                Introduction
                Broadly defined, a corpus is a collection of machine-readable texts that are somewhat representative of a particular reality of scholarly interest (McEnery et al. 2006: 5, Xiao 2012: 147). Corpus creation has been part of the research practices of linguists and philologists for decades, and it has recently entered the computer sciences via the mixture field of natural language processing (NLP). Corpora have become a key resource in the development and evaluation of computer systems that deal with language. As these approaches from NLP are being re-discovered, applied, and enriched within the computational humanities, the making of these corpora and their transformation into structured or plain digital texts is of vital importance. Just in the literary domain, there are arguably thousands of corpora available to download or query. In a comprehensive survey, Xiao (2010) describes over a hundred well-known and highly influential corpora in English and other languages. Smaller corpora for understudied or endangered languages have also recently appeared (see Scannell 2007, Ostler 2008, Cox 2011). Notably, only five corpora in these surveys contained poetry and only one of them was annotated with relevant poetic features. As newer poetic corpora with rich annotations are becoming available, we need a proper tool to uniformly access them.
            
            
                Averell
                Among the characteristics that should guide the building of a corpus (McEnery and Wilson, 2001; Gries and Berez, 2015), two are commonly desired: machine readability and availability to researchers. Unfortunately, even when corpora is made fully available in electronic format, it is often the case that scholars struggle to find a proper way to address their research questions using the ready-made corpora (see e.g., Xiao, 2010). In this sense, Averell is a tool that tries to lower the barrier for researchers interested in the study of multilingual poetry corpora. It provides a unified interface to query, manage, download, and merge corpora of poetic nature in multiple languages based on features relevant for poetry scansion and meter analysis. At its core, Averell is a Python library that connects existing annotated corpora in either JSON, XML, or TEI formats, and makes them available into rich CSV and JSON-lines formats that can be later converted into semantic RDF according to the POSTDATA network of ontologies (González-Blanco et al., 2020). Averell exposes a consistent programming application interface to integrate its functionalities into larger software projects, and it is also packaged as a command line tool for its direct use from the terminal.
            
            
                Granularity
                Averell is structured around two key aspects: the catalogue and its granularity. Each corpus defines a granularity level at which its documents can be split. All corpora support splitting by poems and lines (verses), but a line can also be split into words, and then syllables, for which metrical patterns might be provided. In some cases, stanzas, a set of structural and often semantical units within the poem, are also available. Extra information such as the lengths of verses, the amount of lines per stanza, or the type of rhymes is also added when available. This granular annotation allows scholars to merge different corpora and extract sets of poems that meet specific criteria. For example, a corpus of hendecasyllabic safic verses, or poems for a specific period only at the level of the stanza. Instances of the use of Averell to carry out studies in poetry already exist. De la Rosa et al. (2020, 2021) used Averell to create training and validation datasets to fine-tune transformers-based models and to create rule-based systems to a metrical prediction task.
            
            
                Catalogue
                The current catalogue in Averell (Table 1) contains corpora in Czech, English, French, Italian, Portuguese, and Spanish. A total of 12 corpora with 3,847,739 verses are available to download and remix, with different levels of granularity but all of them annotated to a certain extent.
                Since corpora have different sizes, formats, and metrical information, we pre-processed each corpus looking for common metadata tags and structures. We then created reusable parsers to extract the relevant information exposed by Averell. The result is a JSON-lines structure capable of capturing the common details of the different corpora. From this common intermediate format, Averell is able to produce data in formats suitable for analysis such as CSV, Parquet, XML TEI, and even POSTDATA RDF triplets.
            
            
                Conclusions
                In this work, we have introduced the tool Averell for the management and remixing of annotated poetic corporar in a multilingual setting. We have described its structure and showcased a few of its uses in existing scholarly work. We hope to enrich the tool supporting more formats, better interoperability, a larger catalogue, and an easy-to-use web interface.
                Table 1. Corpora available and granularity levels for each
                
                    
                        51
                        141
                        51
                    
                    
                        Name
                        Description
                        Granularity
                    
                    
                        Disco v2.1 and v3
                        The Diachronic Spanish Sonnet Corpus (DISCO) Spanish 15th and the 19th centuries sonnets corpus
                        stanza
                            line
                        
                    
                    
                        Sonetos siglo de oro
                        Spanish 16th and the 17th centuries sonnets corpus (Miguel de Cervantes Virtual Library)
                        stanza
                            line
                        
                    
                    
                        ADSO 100
                        Spanish Golden age sonnet corpus
                        stanza
                            line
                        
                    
                    
                        Poesía Lírica Castellana del Siglo de Oro
                         Golden Age Castilian lyric poetry corpus 
                        stanza
                            line
                            word
                            syllable
                        
                    
                    
                        Gongocorpus
                        Luis de Gongora poetry corpus 
                        stanza
                            line
                            word
                            syllable
                        
                    
                    
                        Eighteenth-Century
                            Poetry Archive
                        
                        English Eighteenth Century poetry corpus
                        stanza
                            line
                            word
                        
                    
                    
                        For Better For Verse
                        University of Virginia poetry corpus
                        Stanza
                            line
                        
                    
                    
                        Métrique en Ligne
                        Université de Caen Normandie (CRISCO) french poetry corpus
                        stanza
                            line
                        
                    
                    
                        Biblioteca Italiana
                        Italian Medioevo to Novecento poetry corpus
                        stanza 
                    
                    
                        Corpus of Czech Verse
                        Corpus of Czech poetry of the 19th and of the beginning of the 20th centuries
                        stanza
                            line
                            word
                        
                    
                    
                        Stichotheque Portuguese
                        Stichotheque project portuguese poetry copus
                        stanza
                            line
                        
                    
                
            
        
        
            
                
                    Bibliography
                    
                        Cox, C. (2011). 
                        Corpus Linguistics and Language Documentation: Challenges for Collaboration. Brill doi:
                        10.1163/9789401206884_013. 
                         (accessed 28 April 2022).
                    
                    
                        De la 
                        Rosa, J., Pérez, Á., Hernández, L., Ros, S. and González-Blanco, E. (2020). Rantanplan, Fast and Accurate Syllabification and Scansion of Spanish Poetry. 
                        Procesamiento del Lenguaje Natural, 
                        65(0): 83–90.
                    
                    
                        De la 
                        Rosa, J., Pérez, Á., Sisto, M. de, Hernández, L., Díaz, A., Ros, S. and González-Blanco, E. (2021). Transformers analyzing poetry: multilingual metrical pattern prediction with transfomer-based language models. 
                        Neural Computing and Applications doi:
                        10.1007/s00521-021-06692-2. 
                         (accessed 28 April 2022).
                    
                    
                        González-Blanco, E., Ros Muñoz, S., De la Rosa, J., Pérez Pozo, Á., Hernández, L., De Sisto, M., Díaz, A., Khalil, O., Rodríguez, J. L. and Leguina, L. (2020). Towards an Ontology for European Poetry doi:
                        10.5281/zenodo.4299645. 
                         (accessed 28 April 2022).
                    
                    
                        Gries, S. Th. (2009). What is Corpus Linguistics?. 
                        Language and Linguistics Compass, 
                        3(5): 1225–41 doi:
                        10.1111/j.1749-818X.2009.00149.x.
                    
                    
                        McEnery, T. and Wilson, A. (2001). 
                        Corpus Linguistics: An Introduction. Edinburgh University Press.
                    
                    
                        Ostler, N. (2008). Corpora of less studied languages. 
                        Corpus Linguistics: An International Handbook, 
                        1. Walter de Gruyter Berlin: 457–83.
                    
                    
                        Scannell, K. P. (2007). The Crњbadсn Project: Corpus building for under-resourced languages. 
                        Cahiers Du Cental, 
                        5. Citeseer: 1.
                    
                    
                        Xiao, R. (2010). Corpus creation. In Indurkhya, N. and Damerau, F. (eds), 
                        The Handbook of Natural Language Processing. 2nd edition. CRC PRESS-TAYLOR & FRANCIS GROUP, pp. 147–65.
                    
                    Corpus-Based Language Studies: An Advanced Resource Book 
                        Routledge & CRC Press
                         (accessed 28 April 2022).
                    
                
            
        
    


12022	2022	
        
            
                Background
                Within the tradition of speculative fiction, many sub-genres of literature engage with world-building and speculation about the future of humanity, civilization, and our institutions. While the term “Afrofuturism” is commonly used to describe the ways in which artists and creators of the African Diaspora engage with the intersections of race and technology in their works, the less contested term "Black Fantastic" more accurately reflects transcultural iterations of world-building (Iton, 2008). While there is an active scholarly community studying the Black Fantastic (Third Stone, 2021), the use of computational methods in this study is inhibited by a lack of digital collections of Black Fantastic literature. Though the rise of digital libraries has led to new studies and insights into forms and histories of literary genres (Schöch, 2017; Underwood, 2016; Gittel, 2021; Wilkens, 2016), before genre-specific texts can be studied algorithmically, they must first be identified amidst the massive holdings of digital libraries, such as those of the HathiTrust. This project outlines efforts to use text similarity methods to uncover Black Fantastic texts that may be hidden thanks to incomplete metadata or to cataloging practices not conducive to fine-grained genre identification.
            
            
                Methods & Analysis
                Catalog searches in the HathiTrust Digital Library (HTDL) of previously-identified Black Fantastic titles revealed a preliminary list of 13 unique titles. Because metadata records for these volumes do not include “black fantastic” or “afrofuturism,” common library catalog tools would not uncover these known items nor other potential Black Fantastic works. This challenge presented an opportunity to deploy technical methods, specifically text-similarity clustering, to reveal volumes of interest potentially hidden within the HTDL. We started by collecting samples of Black-authored fiction and manually labeling those identified as belonging to the Black Fantastic genre, then analyzed the lexical features that best distinguish these from both general fiction and Black-authored non-Fantastic fiction.
                We randomly sampled 100 volumes each of Black-authored and general fiction, then used HTRC Extracted Features data to aggregate word usage in each volume. Our first attempt at distinguishing Black Fantastic texts necessarily relied on a very limited, 13-volume sample set. We aggregated frequently-used words most closely associated with each genre and generated a Latent Dirichlet Allocation (LDA) topic model (Blei, et al., 2003; Rehurek and Sojka, 2011) to illustrate the different sets of characteristic words for each sub-genre. Figure 1 shows these words for each text class.
                
                    
                
                (a) General fiction (b) Black-authored fiction
                
                    
                
                (c) Black Fantastic fiction
                Figure 1. Topic model representation of distinctive words in each sub-genre of the sample. We categorized each genre with a 4-topic model. Words that most distinguish topics from the general fiction category include “duke”, “car”, “dollars”, “president”, and others, whereas topics from Black-authored fiction rely most on words such as “preacher”, “church”, religious”, “worship”, “encampments”. Moreover, with the addition of words distinct to Black-authored fiction, the Black Fantastic topical signal relies more on words such as “politer”, “bodkin”, “sundered”, “libels”, “amble”, “oracular”, etc.
                Next, we use distinct word sets to develop a classification model for the three sub-genres. We experimented with a hierarchical classification system, first attempting to distinguish Black-authored from general fiction, and then to identify the Black Fantastic sub-genre from among all Black-authored fiction. We used the Stochastic Gradient Descent (SGD) algorithm (Bottou, 2012; Pedregosa, et al., 2011) for our classification models based on Term Frequency-Inverse Document Frequency (TF-IDF) features. Figure 2 shows the confusion matrix for each classification of the test set. As we can see, the classification of Black-authored fiction works well, with 90% accuracy on average, but the Black Fantastic classification is less effective, with 67% accuracy and 75% recall. This further implies that our choice of distinctive tokens as a feature to differentiate fictional sub-genres indeed works, but to a lesser degree for detection of the Black Fantastic as a genre. These errors might result from our limited Black Fantastic training set. Beginning with more Black Fantastic texts, and identifying their most distinctive words, might improve the classification model.
                
                    
                
                Figure 2. Confusion matrices for a hierarchical classification model. The left-hand matrix represents classification into two collections: general and Black-authored fiction; and the right-hand represents the classification of the Black-authored set into Black Fantastic and general Black-authored works.
            
            
                Future Work
                Our next phase will rely on an enriched Black Fantastic dataset in order to improve our classification accuracy. We will also experiment with different features, such as full text (rather than our bag-of-words approach). In spite of limited results for our ultimate goal of discovering new Black Fantastic texts, our success with this model for distinguishing related categories gives reason for optimism given more robust data.
                Note: This submission is a companion piece to another one focused on the pedagogical aspects of the “Black Fantastic” project.
            
        
        
            
                
                    Bibliography
                    
                        Blei, D. M., Ng, A. Y. and Jordan, M. I. (2003). Latent dirichlet allocation. The Journal of Machine Learning Research, 3(null): 993–1022.
                    
                    
                        Bottou, L. (2012). Stochastic Gradient Descent Tricks. In Montavon, G., Orr, G. B. and Müller, K.-R. (eds), Neural Networks: Tricks of the Trade: Second Edition. (Lecture Notes in Computer Science). Berlin, Heidelberg: Springer, pp. 421–36 doi:
                        10.1007/978-3-642-35289-8_25. 
                        https://doi.org/10.1007/978-3-642-35289-8_25 (accessed 1 June 2022).
                    
                    
                        Gittel, B. (2021). An Institutional Perspective on Genres: Generic Subtitles in German Literature from 1500-2020. Journal of Cultural Analytics, 6(1). Department of Languages, Literatures, and Cultures: 22086 doi:
                        10.22148/001c.22086.
                    
                    
                        Iton, R. (2008). In Search of the Black Fantastic: Politics and Popular Culture in the Post-Civil Rights Era. (Transgressing Boundaries: Studies in Black Politics and Black Communities). New York: Oxford University Press doi:
                        10.1093/acprof:oso/9780195178463.001.0001. 
                        https://oxford.universitypressscholarship.com/10.1093/acprof:oso/9780195178463.001.0001/acprof-9780195178463 (accessed 1 June 2022).
                    
                    
                        Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., et al. (2011). Scikit-learn: Machine Learning in Python. The Journal of Machine Learning Research, 12(null): 2825–30.
                    
                    
                        Rehurek, R., and Sojka, P. (2011) "Gensim–python framework for vector space modelling." NLP Centre, Faculty of Informatics, Masaryk University, Brno, Czech Republic 3, no. 2.
                    
                    
                        Schöch, C. (2017). Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama. Digital Humanities Quarterly, 011(2).
                    
                    
                        Underwood, T. (2016). Genre Theory and Historicism. Journal of Cultural Analytics, 2(2). Department of Languages, Literatures, and Cultures: 11063 doi:
                        10.22148/16.008.
                    
                    
                        Wilkens, M. (2016). Genre, Computation, and the Varieties of Twentieth-Century U.S. Fiction. Journal of Cultural Analytics, 2(2). Department of Languages, Literatures, and Cultures: 11065 doi:
                        10.22148/16.009.
                    
                    
                        About This Journal | Third Stone | Rochester Institute of Technology 
                        https://scholarwor
                            ks.rit.edu/thirdstone/about.html
                         (accessed 10 December 2021).
                    
                
            
        
    


12029	2022	
        
            
                A combined approach to the relevance of text passages
                In this contribution, we want to outline insights that arise from combining two distinct approaches to literary texts that analyse the relevance of specific text passages. We have been working on the identification of the 
                    narrativity represented in literary texts as well as on the 
                    quotation of the texts in research to identify passages especially relevant from a hermeneutical perspective. This now allows us to explore whether the structures and patterns that are emerging from these two approaches can be related to each other in a meaningful way.
                
                
                    Narrativity as textual relevance criterion
                    Our identification of narrativity of literary prose texts is based on the annotation of events. By considering the core features of events in narrative theory (i.e., being a state, a process in time and change of state) we classify each verbal phrase in a text as change of state, process event, stative event or non-event.
                        
                            For a detailed explanation of the manual annotation on which the data used in this contribution is based cf. Vauth and Gius, 2021.
                        
                        ,
                        
                            For a description of classification of events as well as the automation approach and results cf. Vauth et al., 2021. 
                         To enable measuring narrativity, this categorical scaling is transposed into a numerical scaling reflecting the degree of narrativity of the event categories. In accordance with a narrative theory understanding of events, we determine the narrativity of the annotation categories with the values 7 (change of state), 5 (process event), 2 (stative event), and 0 (non-event). By additionally smoothing the narrativity value we are able to model the narrativity of a text as a graph.
                        
                            For a discussion of the adequacy of this implementation for literary studies, especially with regard to intersubjectivity, cf. Gius and Vauth, 2022.
                         Figure 1 shows the narrativity graph for the novella 
                        Die Judenbuche by Annette von Droste-Hülshoff which serves as an example for our approach.
                    
                    
                        
                            
                            Figure 1. Narrativity score
                        
                    
                
                
                    Key passages: quotation as textual relevance criterion
                    
                        We consider key passages as parts of a literary text that are especially relevant for interpretation and can differ in length from only a few words to one or more paragraphs. To learn which parts of a text are more relevant than others, we rely on expert knowledge, which we obtain from numerous interpretations of a literary work containing quoted passages. This is a new approach in text and literature studies, that has not been theoretically founded yet; though the term “Schlüsselstelle” (key passage) and equivalents are used regularly in text interpretations in German language.
                        
                            For more details on key passages and the aim of the project cf. Arnold and Fiechter, 2022.
                        
                    
                    
                        For this study, we limit ourselves to a quantitative view. We have analysed 44 interpretations of 
                        Die Judenbuche
                        , all in German language, published between 1995 and 2015 and identified quoted passages with a Python tool for quotation detection in fictional texts.
                        
                            For a detailed explanation, cf. Arnold and Jäschke (accepted). Source code available at: 
                                .
                            
                        
                         Figure 2 
                        visualises the identified quotations over the course of the text; the histogram shows quotation frequency and the graph the smoothed frequency for each verbal phrase identified during the event annotation (cf. Section 1.1). Notably, the beginning and the end are quoted most frequently, together with three other frequently quoted passages. 
                    
                    
                        
                            
                            Figure 2. Quotation frequency
                        
                    
                
            
            
                Combining the approaches: frequently quoted (key) passages and narrativity
                By combining exploration of narrativity and quotation frequency (cf. Figure 3) we can explore whether a passage is referred to as one that is mainly interesting for the storyline or for the interpretation. Passages with a high narrativity score are particularly important for the plot and the comprehension of the plot, while passages with a low narrativity value more often contain dialogue or narrator comments in which interpretation proposals are already made that are taken up in literary studies texts. For passages with a medium narrativity value, potential interdependencies are difficult to determine on the basis of only one text, but we are aiming to obtain more detailed knowledge on this in the future, including also non-frequency based analyses of references. Also, the beginning and the end of the text seem to be quoted in a different manner. Here quotation frequency and narrativity seem to be connected only loosely. Instead, these borders of the text seem to be used mostly to provide a framework for interpretations, in which the interpreters select the most interesting passages for their intent. 
                
                    
                        
                        Figure 3. Narrativity score and quotation frequency 
                    
                
            
            
                Outlook
                While these findings point out how the quotation of text passages may relate to their narrativity, they should be evaluated against a broader corpus of texts. There, the classification of functions of 
                    quotations is the most interesting aspect. We assume that plot-orientated 
                    quotations in the secondary literature correlate with higher narrativity, whereas passages quoted in order to develop a more comprehensive interpretation of the text display less narrativity. For evaluating this, we plan to combine our automated analysis of narrativity with the automated detection of key passages.
                
            
        
        
            
                
                    Bibliography
                    
                        Arnold, F. and Fiechter, B. (2022). Lesen, was wirklich wichtig ist: Die Identifikation von Schlüsselstellen durch ein neues Instrument zur Zitatanalyse. 
                        DHd2022: Konferenzabstracts. 
                         (accessed 11 April 2022).
                    
                    
                        Arnold, F. and Jäschke, R. (accepted). Lotte and Annette: A Framework for Finding and Exploring Key Passages in Literary Works. 
                        Proceedings of the Workshop on Natural Language Processing for Digital Humanities at ICON 2021.
                    
                    
                        Gius, E. and Vauth, M. (2022). Inter Annotator Agreement und Intersubjektivität. 
                        DHd2022: Konferenzabstracts. 
                         (accessed 11 April 2022).
                    
                    
                        Vauth, M. and Gius, E. (2021). Richtlinien für die Annotation narratologischer Ereigniskonzepte. 
                        Zenodo.
                         
                         (accessed 11 April 2022).
                    
                    
                        Vauth, M., Hatzel, H. O., Gius, E. and Biemann, C. (2021). Automated Event Annotation in Literary Texts. 
                        CHR 2021: Computational Humanities Research Conference. Amsterdam, pp. 333–45.
                         
                         (accessed 11 April 2022).
                    
                
            
        
    


12048	2022	
        
            
                Theoretical background
                Periodization is one of the fundamental topics of literary studies. As Rene Wellek puts it in one of the most notorious and important books of the last century theory of literature: “the concept of period is certainly one of the main instruments of historical knowledge”, meaning, of course, literary-historical knowledge. 
                    (Wellek, 1956: 268). And yet is one of the most controversial and debated:
                
                It is virtually impossible to divide periods according to dates for, as [Jurij] Lotman points out, human culture is a dynamic system. Attempts to locate stages of cultural development within strict temporal boundaries contradict that dynamism. 
                    (Bassnett, 2013: 41)
                
                How it comes that we can hypostatize the dynamic nature of cultural systems, superimposing on them a scalar chronology? How can we say, as Jameson puts it, that Ulysses is something that happened in 1922 
                    (Jameson, 1971: 313)? Following Meneghelli 
                    (Meneghelli, 2013), we can individuate at least 4 critical issues, or even aporias in literary periodization:
                
                
                    Historical categories are related to cultural and social phenomena and overlap and interact in complex ways, determining 
                        anisochronies and 
                        dischronies;
                    
                    Most if not all literary-historical categories have an ontological and trans-historical status and meaning embedded in them (take for instance Romanticism or Modernism);
                    Historical categories interact with and are dependent on geospatial ones, resulting in a multiplicity of asynchronous periodizations;
                    The notion of a historical category in literature is strictly associated with the canonical corpus of texts that are considered representative of a period, and the “dialectics between these two poles, period and canon, are complex and manifold” (Meneghelli, 2013: 3).
                
                This last point is particularly relevant: literary periodization is usually the product of a process of generalization and synthesis, within a historical and social horizon, of the small-scale critical and interpretive practices that characterize the study of literary texts. Being bound to idiosyncratic hermeneutical practices and to the “epistemology of close reading”, periodization suffers from all the pitfalls and limitations of that approach. In the last two decades, the landscape of literary and cultural studies has been enriched by a methodological perspective that is based on a quantitative approach. Among the various disciplinary labels that identify this current in studies, the most common is distant reading, introduced by Franco Moretti in his work on World Literature 
                    (Moretti, 2000) and subsequently extended to denote (even retroactively) the entire tradition of quantitative literary studies 
                    (Moretti, 2013; Underwood, 2019; Piper, 2018; Jockers, 2013). In this framework literary texts are elements of a population whose synchronic and diachronic characteristics should be empirically investigated on a molar scale, adopting statistical-probabilistic and computational methods. This would require the move from 
                    interpretation to 
                    explanation as the primary methodology of scholarly inquiry in the cultural and literary domains 
                    (Ciotti, 2021). 
                
                The possible contribution of a distant reading approach to the literary periodization problem, has been previously explored by some important studies, like 
                    (Jockers, 2013: chap. 6), 
                    (Piper, 2018: chap. 4), 
                    (Underwood, 2019) for English narrative fiction, all based on a supervised classification approach; while 
                    (Jannidis and Lauer, 2014) for German literature adopted a stylometric method. This paper explores the results of a mainly explorative and unsupervised analysis of a corpus of 19
                    th and 20th century Italian narrative fiction.
                
            
            
                The corpus and the methodology
                The main research hypothesis is if adopting computational quantitative methods, it’s possible to identify time-based groupings in a set of texts distributed over a long historical period. Related to this there is the question of whether those eventual groupings are aligned with traditional historical periodization. The corpus consists of 660 Italian novels and short novels written between 1810 and 2000 of different aesthetic "levels", canonization status, genre, dimension, and authors’ gender. Admittedly, this corpus is still far from being adequate and well balanced, mainly due to the uneven chronological distribution, but it is sufficient for an exploratory inquiry.
                I wanted to test if the corpus can be clustered in a chronological sensible way using an algorithmic approach without presuming any prior categorization: this explains the preference for unsupervised methods in this research. In particular, I have adopted two different approaches, based on different assumptions, analytical techniques and features selection:
                
                    bootstrap consensus network, following (Eder, 2017) that applies phylogenetic consensus networks method to MFW based clustering;
                    lexicon-based text analysis and subsequent K-Means clustering of the results.
                
                Fort the first experiment I have adopted the popular stylometric R package 
                    Stylo
                    (Eder et al., 2016) to generate the consensus network dataset, conflating ten hierarchical clusterings generated comparing from 100 to 1000 most frequent words. The resulting output dataset is imported into the network analysis tool Gephi 
                    (Bastian et al., 2009) and each node is associated with an attribute that specifies its decade of composition. Then modularity is calculated with a resolution set at 4, resulting in 10 modules, and the final layout is generated applying the layout algorithm Force Atlas 2.
                
                The second approach is based on the text analysis of the corpus with the tool 
                    Linguistic Inquiry and Word Count (
                    LIWC) 
                    (Pennebaker et al., 2015). For each text, LIWC produces a vector containing the relative frequencies of various words-classes (E.g.: "Affect Words", "Cognitive Processes", "Perceptual Processes", "Biological Processes"....). The final output is a low dimensional document matrix, to which I have applied a K-Means clustering process, adopting the Python implementation of the algorithm in the 
                    SciKit-Learn library 
                    (Pedregosa et al., 2011). The choice of the number of clusters has been done evaluating 20 different models with 
                    Elbow method and 
                    Silhouette Score tests, which both indicated an optimal value of 4 clusters.
                
            
            
                Results and future directions
                The results of the stylometric approach represented in Fig.1 shows that the texts clustered in time sensible way are only those written in the second half of the 20
                    th century (that in the network have a blue color tone), while texts written in the first and second half of the 19
                    th century e in the first half of 20
                    th are not clearly separated, with the exception of the island in the upper right part of the graph, which is mostly composed of canonical Italian modernist texts.
                
                
                    
                    Consensus Network of the corpus: red 1810-1860; green 1860-1900; yellow 1900-1940; blue 1940-2010
                
                This overall result is confirmed by the K-Means approach. For my analysis I have produced two matrices using the LIWC dictionary for Italian 
                    (Agosti and Rellini, 2007):
                
                
                    a matrix with all LIWC lexical categories;
                    a matrix limited to the following word categories: Verbs, Pronouns, orthographic signs of represented speech, and categories related to emotional and cognitive activity.
                
                In this way I can test an additional hypothesis very common in literary-historical and critical scholarship: the linguistic sphere of the cognitive and emotional dimension is a characteristic feature of the evolution of narrative along the nineteenth and twentieth centuries, namely it’s a characteristic of the transition to the Modernism. While the K-Means clustering of the first matrix has very little relation to chronology, the second one provides clear indicators of temporal segmentation (Figure 2). Therefore, we can say that the incidence of the lexicon related to the sphere of thought/consciousness/emotivity is a signal of an evolutional pattern in the Italian novel. Anyway, also in this case the more clearly time-based cluster is that of the text written in the second half of the 20
                    th century.
                
                
                    
                    K-Means clusters in the document matrix restricted to the cognitive/emotional lexicon
                
                In conclusion, the first analysis in general confirms for Italian literature the limited role of the time dimension for clustering texts on a purely stylometric base already observed by 
                    (Jockers, 2013: chap. 6), as compared to authorship and even author gender. Instead, there is some evidence that quantitative empirical analysis partially confirms the relevance of cognitive/emotional lexicon in the evolution of literature. In this direction, I think that a fruitful development of the research will require a more effective way to identify the presence of cognitive/emotional attitudes in texts. To this end, we are training a streamlined Italian BERT language model to identify the relevant textual blocks, and the provisional results are promising.
                
            
        
        
            
                
                    Bibliography
                    
                        Agosti, A. and Rellini, A. (2007). The Italian LIWC dictionary. 
                        Austin, TX: LIWC. Net.
                    
                    
                        Bassnett, S. (2013). 
                        Translation Studies. London: Routledge.
                    
                    
                        Bastian, M., Heymann, S. and Jacomy, M. (2009). Gephi - The Open Graph Viz Platform https://gephi.org/ (accessed 26 November 2018).
                    
                    
                        Ciotti, F. (2021). Distant reading in literary studies: a methodology in quest of theory. 
                        TESTO & SENSO(23).
                    
                    
                        Eder, M. (2017). Visualization in stylometry: Cluster analysis using networks. 
                        Digital Scholarship in the Humanities, 
                        32(1): 50–64 doi:10.1093/llc/fqv061.
                    
                    
                        Eder, M., Rybicki, J. and Kestemont, M. (2016). Stylometry with R: A Package for Computational Text Analysis. 
                        The R Journal, 
                        8(1): 107–21.
                    
                    
                        Jameson, Fredric. (1971). 
                        Marxism and Form : Twentieth-Century Dialectical Theories of Literature. Princeton (N.J.): Princeton University Press.
                    
                    
                        Jannidis, F. and Lauer, G. (2014). Burrows’s Delta and Its Use in German Literary History. In Erlin, M. and Tatlock, L. (eds), 
                        Distant Readings. Topologies of German Culture in the Long Nineteenth Century. Rochester: Camden House, pp. 29–54 gerhardlauer.de/index.php/download_file/view/335/1/.
                    
                    
                        Jockers, M. L. (2013). 
                        Macroanalysis: Digital Methods and Literary History. (Topics in the Digital Humanities). University of Illinois Press 
                    
                    
                        helli, D. (2013). Periodization, Comparative Literature, and Italian Modernism. 
                        CLCWeb: Comparative Literature and Culture, 
                        15(7) doi:10.7771/1481-4374.2386. https://docs.lib.purdue.edu/clcweb/vol15/iss7/12 (accessed 8 December 2021).
                    
                    
                        Moretti, F. (2000). Conjectures on World Literature. 
                        The New Left Review http://newleftreview.org/A2094.
                    
                    
                        Moretti, F. (2013). 
                        Distant Reading. London: Verso http://www.amazon.de/Distant-Reading-Franco-Moretti/dp/1781680841.
                    
                    
                        Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., et al. (2011). Scikit-learn: Machine Learning in Python. 
                        Journal of Machine Learning Research, 
                        12: 2825–30.
                    
                    
                        Pennebaker, J. W., Boyd, R. L., Jordan, K. and Blackburn, K. (2015). The Development and Psychometric Properties of LIWC2015. University of Texas at Austin doi:10.15781/T29G6Z. http://hdl.handle.net/2152/31333 (accessed 8 December 2021).
                    
                    
                        Piper, A. (2018). 
                        Enumerations: Data and Literary Study. Chicago ; London: The University of Chicago Press.
                    
                    
                        Underwood, T. (2019). 
                        Distant Horizons: Digital Evidence and Literary Change. Chicago: The University of Chicago Press.
                    
                    
                        Wellek, R., Warren, Austin,, (1956). 
                        Theory of Literature. New York: Harcourt, Brace & World.
                    
                
            
        
    


