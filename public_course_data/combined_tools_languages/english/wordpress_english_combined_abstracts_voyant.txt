
“Making the Digital Humanities More Open”, a NEH ODH Digital Humanities Start-Up Grant Project, is creating a free and easy-to-use tool that enables end-users with a variety of disabilities and abilities to access online humanities resources, allowing digital humanities projects to share the products of their text-based projects with this often-neglected audience of readers. During the year previous to DH 2013, our team will design and deploy a WordPress-based accessibility tool that will create braille content for end-users who are blind or low vision. Specifically, we plan to extend the use of Anthologize — a free and open source plug-in for WordPress that currently translates any RSS text into PDF, ePub, HTML, or TEI — to include the conversion of text to braille. As a result, we will not only make it easy for digital research content creators to convert a text into braille, thereby extending humanities content to hundreds of thousands of visually disabled readers, but we will also experiment with making braille available visually through the WordPress interface. This tool will also make it possible to translate the textual content of an Omeka archive into braille provided the site — like most that use Omeka — publishes an RSS feed; we’ll have conducted initial tests of the tool by translating the existing BrailleSC oral histories into braille, and then we will reach out to other Omeka- and WordPress-based humanities computing projects to ask for their cooperation and collaboration in translating their content.

Over the last several decades, scholars have developed standards for how best to create, organize, present, and preserve digital information so that future generations of teachers, students, scholars, and librarians may still use it. What has remained neglected for the most part, however, are the needs of people with disabilities (Abou-Zahra). As a result, many of the otherwise most valuable digital resources are useless for people who are blind or have low vision; the barriers to participation are varied and include such obstacles as the high price of specialized software and hardware, the advanced expertise that such software and hardware often requires, and design choices that can prevent end-users with sensory disabilities from taking full advantage of online resources (Fox).

For public humanities practitioners, including the many local museums, small organizations, and individual scholars, our project provides an entrance into digital humanities communities that might otherwise be obfuscated by the high costs of technology adoption, customization, and deployment. Accessibility is important because disabled users need to be able to participate fully in humanities research and teaching. In providing accessibility tools to disabled communities, we enrich their individual research and learning efforts beyond the formal educational process. As the insights of scholars working in disability studies in the humanities have shown, creating tools for individuals with disabilities improves digital environments for all users (Williams). Our work aims to increase participation by all people in experiencing and creating scholarly digital projects.

The deliverables of the project, all completed by May 2013, will be a public GitHub repository of the code, a documentation guide, a pilot test of using a local LibLouis library with WordPress, a pilot test of using a remote LibLouis library with WordPress and Anthologize, and a white paper explaining what we’ve learned about various options for creating online braille documents. A demo of the technical components of our work will be part of our poster presentation, but we will emphasize the theoretical takeaways of the project, modeling the ways in which digital humanities projects should be designed and implemented with the needs of disabled users in mind. We will discuss what we have learned about accessibility for both digital humanities projects and digital writing in general, and explore both the direct impact of this tool and the significance of opening digital humanities work to a wider audience of participants. Our poster will be of interest to anyone interested in the theory behind how we code and design digital humanities tools and site, designing for universal and accessible use, and accessing, preserving, and working with the cultural histories of people interacting with the braille form of reading and writing.

References
Abou-Zahra, S. (ed). (2011). Evaluating Web Sites for Accessibility: Overview. Web Accessibility Initiative. World Wide Web Consortium. http://www.w3.org/WAI.
Fox, S. Americans Living with Disability and Their Technology Profile. Pew Research Center’s Internet & American Life Project. http://pewinternet.org/Reports/2011/Disability.aspx. (accessed 21 January 2011).
Williams, G. H. (2012). Disability, Design, and the Digital Humanities. In Gold, M. K. (ed.), Debates in the Digital Humanities. Minneapolis: University of Minnesota Press. 202-212.
The PressForward Initiative1 at the Roy Rosenzweig Center for History and New Media (RRCHNM) has developed a methodology and a technology to expedite scholarly communications and nurture and expand communities of practice on the open web. Over the past two years we have produced our own open source WordPress plugin"2 to facilitate aggregating, curating, and disseminating scholarly content with a process that fosters community and resource-sharing among its users. Simultaneously, we experiment with multiple processes for surfacing, selecting, and circulating digital humanities work and gray literature outside formal traditional publication models.

This poster profiles the workflows and content of two digital humanities publications that have adapted technology developed by PressForward to suit their unique editorial and community needs: Digital Humanities Now3, produced by RRCHNM and dh+lib4, a publication hosted by the Association of College and Research Libraries' Digital Humanities Discussion Group. Visualizing and documenting intellectual and technical schemas for the plugins with diagrams and onsite demonstrations, this presentation exposes the philosophies and methodologies behind aggregating and curating scholarly work and learned expertise made available on the open web. Illustrating multiple workflows, layouts, and interfaces, this poster presents the scalable, replicable, and adaptable potential of the PressForward Plugin for niche scholarly communities eager to tailor their own hubs for communication and collaboration.

As a technology, the PressForward Plugin provides a smoothly integrated editorial process for the aggregation, review, discussion, and republication of external web content within the WordPress dashboard. PressForward aggregates content via RSS feeds, functions as a seamless feed reader, and allows users and groups to mark and discuss individual items before modifying or reproducing them for republication. This poster will document how two publications currently tailor the scope, structure, and flow of the plugin, gesturing toward the potential for replication and modification.

The largest case study, Digital Humanities Now (DHNow), is a principle test case for the plugin and for streamlining workflows that facilitate volunteers' nomination of scholarship on the open web. Drawing from a growing digital humanities community, volunteer editors-at-large sign up to survey over 1,000 potential items each week from more than 650 RSS feeds and nominate material they believe is salient to DHNow's readership. Nominations are considered for republication in DHNow as either Editor's Choice or News items. Next, rotating editors-in-chief -- faculty and graduate assistants at RRCHNM -- select, prepare, and publish links to nominated content, improving its visibility by directing attention back to the original site of publication. DHNow's streamlined processes for managing large numbers of novice and seasoned community volunteers also experiments with automated methods that include computer learning algorithms for filtering through large amounts of content.

dh+lib, published by an existing professional group, aims to give increased presence and voice to librarians interested in, or already knowledgeable about, DH. dh+lib publishes content in two streams. First, original content--posts, essays, and other work--is published directly to the site biweekly or monthly. Second, more regular, content, appears as the dh+lib Review. To produce dh+lib Review, dh+lib relies on the PressForward plugin to facilitate the nomination of content from approximately 167 RSS feeds. Modeled after DHNow, dh+lib's workflow also relies on volunteer editors-at-large, who sign up for weekly shifts to sift through the feeds to select material of interest to the community. dh+lib has also developed an additional layer of editorial intervention, where items selected for publication are written up as short "review" pieces. These pieces contextualize the nominated content, often pulling in other sources, links, and commentary to frame a project, resource, or post.

By volunteering for DHNow or dh+lib, community editors-at-large develop conversance in relevant trends and issues. While DHNow broadcasts a wide scope of work and provides an easy entry point for new practitioners to begin tracking the field, the published dh+lib Review provides librarians with weekly updates of useful and timely content, as selected by their peers. Additional examples will include Global Perspectives on Digital History5, a collaboration between RRCHNM and his.net, that has adapted the PressForward methodology to monitor and distribute material from a smaller, but multilingual source base.

Committed to the ongoing experimentation of DHNow and GPDH, PressForward also encourages adaptation of the tool by others and welcomes collaborations such as with dh+lib.  Exposing the production of these digital humanities publications through documentation, workflow diagrams, and guides for getting started, we encourage viewers to consider how the PressForward model might improve the scholarly communication and collaboration of their own communities of interest on the open web.

References
1. PressForward: Discover, Curate, and Distribute Scholarship the Web Way. PressForward. Roy Rosenzweig Center for History and New Media, n.d. Web. 30 Oct. 2013. pressforward.org

2. The PressForward Plugin. PressForward. Roy Rosenzweig Center for History and New Media, n.d. Web. 30 Oct. 2013. pressforward.org/the-pressforward-plugin/

3. Digital Humanities Now. Digital Humanities Now. Roy Rosenzweig Center for History and New Media, n.d. Web. 01 Nov. 2013. digitalhumanitiesnow.org

4. dh+lib. Association of College and Research Libraries, n.d. Web. 01 Nov. 2013. acrl.ala.org/dh/

5. Global Perspectives on Digital History. N.p., n.d. Web. 01 Nov. 2013. gpdh.org
Getting an XML-based website up and running can be a repetitive, time consuming and tedious task. It usually also requires someone with good technical knowledge to bring all of the pieces together into a complete package. Kiln is a tool that allows a non-technical user to quickly and easily create a working web application that provides base functionality for publishing, indexing and searching XML source content.

The user is free to focus on developing the content, and can easily assess the way in which content is used and review changes to see how they are reflected in the published output. Importantly this ease of use does not preclude the customisation of the existing functionality, nor the addition of new tools, processes and outputs.

Kiln, previously known as xMod, is an open source multi-platform framework for building and deploying complex websites whose source content is primarily in TEI/XML. It brings together various independent software components into an integrated whole that provides the infrastructure and base functionality for such sites.

Kiln is developed and maintained by a team at the Department of Digital Humanities, King’s College London. Over the past years and over several versions, Kiln has been used to generate more than 50 websites for digital humanities research projects which have very different source materials and customised functionality.

The main Kiln components are:

Apache Cocoon web development framework for XML processing.
Jetty web application server.
Apache Solr, a searching platform for indexing, searching and browsing of contents.
Sesame, a framework for processing RDF data.
In a production web server context, Kiln integrates with other web publishing tools to support images, maps and other data sources, like relational databases. It can equally easily be used to provide a full web application solution, or as a backend providing content to be surfaced by some other technology, such as Django or WordPress.

Kiln has been developed around the concept of the separation of roles, allowing people with different backgrounds, knowledge and skills to work simultaneously on the same project without interfering with one another’s work. The parts of the system used by developers, designers and content editors are distinct. For example, the templating engine handles the general structure of an output document in individual files, with reference to separate sources which supply the individual content elements. Templating is also achieved using a lightweight syntax that allows frontend development to take place without any knowledge of XSLT.Kiln has two competing design goals: to support the development of unique, complex web applications; and to provide an out-of-the-box system suitable for a single non-technical person to publish a TEI-based site. The former demands not only the customisability of every component, but also the avoidance of any technical magic that makes one way easier at the cost of another way being harder. The latter requires a large amount of built-in behaviour that can be easily tweaked in isolation, and excellent documentation. Kiln’s documentation includes a tutorial showing how to customise each of the major elements of a site, as required beyond the provided defaults.

In comparison with other publishing tools, such as XTF, SADE and TE- ICHI, Kiln offers some advantages. It is mature and flexible, as it has been in active development for 10 years since its initial version, and has been used in many research projects. It is standalone (beyond an installation of the Java language), requiring no other software for any of its functionality, and it provides a working site, including a faceted search, with no more than placing TEI files in a particular directory.

The proposed poster will provide a diagrammatic overview of a Kiln project structure, and an accompanying interactive demo will show the system in action, from creating a new project through to displaying content as a website.

References
1. kcl-ddh.github.io/kiln/

2. A list of some of these projects is available at kiln.readthedocs.org/en/latest/ projects.html.

3. cocoon.apache.org/2.1/

4. www.eclipse.org/jetty/

5. lucene.apache.org/solr/

6. www.openrdf.org/

7. xtf.cdlib.org/

8. www.bbaw.de/telota/software/sade/sade-1

9. www.teichi.org/
The Letters of 1916 Project is the first crowd-sourced digital humanities project in Ireland. The project pivots around one of the most important events in twentieth-century Irish history -- the 1916 Easter Rising -- in which a small group of Irish Volunteers rebelled against the British Army on 24 April, Easter Monday 1916. The Rising was quickly quashed by the British, but the executions of its leaders several weeks later made martyrs of them, and set in motion a series of events that resulted in Irish independence from the United Kingdom in 1922.

The Letters of 1916 project focuses on this tumultuous period. Its goal is to create a crowd-sourced digital collection of letters written for a six-month period before and after the Easter Rising (1 November 1915 – 31 October 1916).

The project includes letters held at institutions (in Ireland and abroad), alongside those in private collections. The collection criteria is extremely broad: it includes any letter written to or from someone in Ireland, or that contains Irish subject matter. 

The ultimate goal is to create a new online resource that will add a novel and heretofore underutilised viewpoint to the events of the period, a confidential and intimate perspective that will form the basis for a revision of our understanding of life in Ireland in the early 20th Century. 

Letters 1916 integrates four distinct theoretical and methodological approaches:

The creation of a thematic research collection
A crowd-sourced public humanities project
Text analysis
Social media for dissemination and outreach
This paper will explore phase I of the project: the sourcing, gathering, and transcribing/encoding of collection. It will also document how the project is being used in the classroom at the Masters level. 

Phase II will transform the transcribed/encoded letters, along with their accompanying images, into a thematic research collection where users will be able to do full text searching, text analysis, view correspondences, view letters temporally and spatially, and engage with thematic exhibitions. The collection is being viewed as a major new resource and as such, will ultimately be deposited at a cultural heritage institution for long-term preservation. 

In her 2009 article Rose Holley made the distinction between social engagement projects, which she defined as ‘giving the public the ability to communicate with us and each other’ via such activities as tagging, commenting, rating, and reviewing, and crowdsourcing which uses social engagement techniques to help a group of people achieve a shared, usually significant, and large goal by working collaboratively together as a group. Crowdsourcing also usually entails a greater level of effort, time and intellectual input from an individual than just socially engaging

The project stops short, however, of a collective editing project. Phase 1 of the project was launched in September 2013 to a great deal of press.[1] The database was pre-populated with some 300 letters from cultural heritage institutions. To date, the project has 1500 letters in its workflow (about 800 publicly available). 

The project, thus far, uses two distinct crowdsourcing methodologies according to the typology developed by Carletti, et al: a) correction and transcription; and b)  complementing collection. Building on the immense success of the Europeana World War 1 Road shows, we invited the public to bring their letters in for scanning at the launch. This model is being replicated, thus far in Ireland and the UK. Moreover, the software allows the pubic to upload their letters from home, create basic metadata, and provide the project with descriptive information. 

At the time of this writing, there are approximately 30 privately-held collections (or letters from small institutions we would have never have thought to reach out to) for letters from this period. The size of individual deposits range from one letter to 90– a correspondence between the donor’s grandmother and grandfather who happened to be courting during 1916. 

The public is also invited to transcribe previously uploaded letters. The response has been so overwhelming that the project team is frequently can barely keep up with demand. The engagement, encouragement, and reaching out to volunteers is a major aspect of the project to be discussed. At present the site does not utilise some of the more traditional features of crowdsourced projects (such as badges and icons) to engage its volunteers. However, other methods have been implemented, such as a volunteer forum, featuring contributors both on the project site and in the more traditional forms of media. 

The project has benefited immensely from previous crowdsourced projects, from its methodological approaches, to its workflow and software. The project utilises WordPress for its front end, and for its content management system a version of Omeka that was modified by University of Iowa libraries (DIY History) to support crowdsourcing projects. It utilises George Mason’s Scripto tool for transcribing, and the Transcribing Bentham’s TEI/XML toolbar for light encoding. In a domain space that has constantly called for the creation of tools, the ability to fairly rapidly and without a great deal of bespoke programming to string together these diverse tools into one unified web presence, may signal a golden age of tool development for our field. 

The Transcribing Bentham toolbar has been an interesting feature which many transcribers have utilised with little instruction. The Transcribing Bentham project designed the toolbar to hide ‘much of the complexity of TEI markup from the transcriber’ (Moyle, 353). An analysis of the markup used in this environment reveals an especially intuitive use of tagging, something the TEI is rarely praised for. Despite the caveat that the toolbar only allows the encoding of 15 out of the TEI’s hundreds of tags, the ease in which individuals with absolutely no experience in textual editing, AML,  or text encoding have been utilizing it may point to new ways to create a more intuitive encoding interface. A downside to the reduced set of tags available is that users, in trying to make sense of the tags available to them, use certain tags, such as <lb> (line break), so ubiquitously that it almost amounts to tag abuse. 

This project is pitting, in many respects, methods typical of thematic research collections-- including the creation of a carefully curated dataset with fully transcribed and proofed documents, and hand-crafted markup and metadata created in accordance with the Text Encoding Initiative Guidelines-- against the less painstaking creation of a full text dataset amenable to text analysis and visualisation as a method of discovery and analysis. Ultimately, in Phase II of the project, we intend to offer readers both views – the traditional reading environment of the thematic research collection along with the analytics available via text analysis and visualisations. But the decisions we make at this stage which will be detailed in this talk will impact what is possible in later phases.  

The social media dimension of finding and engaging  our audience demonstrates just how significantly the environment in which digital humanists operate has changed over the past decade. An analysis of the first month’s tweets reveal an ever expanding network of cultural institutions, individuals, organisations, and traditional media who have passed the message of the project along. A twitter-feed is a feature of the project’s home page and discussions amongst the project team reveal a shifting notion of how to position the project in a web of similar initiatives in Ireland coming broadly under the rubric of The Decade of Commemoration.[2] The project has already been positioned not as a silo, but as an aggregator embracing multiple communities of holders of these precious objects. We share back with individuals and institutions photographs we take, and will eventually share the fully transcribed/encoded texts files. But as we near the centenary of the Easter Rising, we are aware of our responsibility to, one the one hand, maintain the integrity of the project while, on the other, opening our resource more fully to sister initiatives, while respecting copyright and intellectual property promises to our donators.

References
Carletti, Laura, Gabriella Giannachi,  Dominic Price, Derek McAuley, (2012), ‘Digital Humanities and Crowdsourcing’.MW2012: Museums and the Web 2013. Online

Causer, Tim, Justin Tonra, Valerie Walace (2012). ‘Transcription Maximized; Expense Minimized? Crowdsourcing and Editing The Collected Works of Jeremy Bentham’. Literary and Linguistic Computing. 27/2 2012. 119-137.

Dunn, Stuart and Mark Hedges. ‘Crowd-Sourcing Scoping Study: Engaging the Crowd with Humanities Research’. Centre for e-Research, Department of Digital Humanities, King’s College London. Online. 

Holley, Rose (2010). ‘Crowdsourcing: How and Why Should Libraries Do It’. D-Lib Magazine. March-April 2010. 16:3/4. Online

Moyle, Martin, Justin Tonra, Valerie Wallace (2011). ‘Manuscript Transcription by Crowdsourcing; Transcribing Bentham’. Liber Quarterly 20:3/4. March 2011.  

Palmer, Carol (2004). ‘Thematic Research Collections’.Companion to Digital Humanities’.  Blackwell. Online. 

Robinson, Peter. (2010). ‘Editing Without Walls’. Literature Compass. Special issue on Scholarly Editing in the Twenty-first Century. 57-61.

Please see letters1916.ie for full press coverage

This shorthand refers to a decade of significant historical events in Ireland 1913-1923 in Ireland, beginning with the Dublin Lockout in 1913 and ending with Irish independence, followed by a Civil War a decade later.


    
        
            
                Challenges of an XML-based Open-Access Journal: Digital Humanities Quarterly
                
                    
                        Flanders
                        Julia
                    
                    Northeastern University, United States of America
                    j.flanders@neu.edu
                
                
                    
                        Piez
                        Wendell
                    
                    Piez Consulting Services
                    wapiez@wendellpiez.com
                
                
                    
                        Walsh
                        John
                    
                    Indiana University
                    jawalsh@indiana.edu
                
                
                    
                        Terras
                        Melissa
                    
                    University College London
                    m.terras@ucl.ac.uk
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    open access
                    digital publishing
                    XML
                    TEI
                
                
                    project design
                    organization
                    management
                    publishing and delivery systems
                    xml
                    copyright
                    licensing
                    and Open Access
                    standards and interoperability
                    English
                
            
        
    
    
        
            
                Background and Technical Infrastructure
                
                    DHQ’s technical design was constrained by a set of higher-level goals and needs. As an early open-access journal of digital humanities, the journal had an opportunity to participate in the curation of an important segment of the scholarly record in the field. This meant that it was more than usually important that the article data be stored and curated in a manner that would maximize the potential for future reuse. In addition to mandating the use of open standards, this aim also strongly indicated that the data should be represented in a semantically rich format. Of equal concern was the need for flexibility and the ability to experiment with both the underlying data and the publication interface, throughout the life of the journal, without constraint from the publication system. Both of these considerations moved the journal in the direction of XML, which would give us the ability to represent any semantic features of the journal articles we might find necessary for either formatting or subsequent research. It would also permit us to design a journal publication system, using open-source components, that could be closely adapted to the 
                    DHQ data. At the journal’s founding, several alternative publishing platforms were proposed (including the Open Journal System), but none were XML-based and none offered the opportunity for open-ended experimentation that we needed.
                
                
                    DHQ’s technical infrastructure is a standard XML publishing pipeline built using components that are familiar in the digital humanities. Submissions are received and managed through OJS through the copyediting stage, at which point articles are converted to basic TEI using OxGarage (http://www.tei-c.org/oxgarage/). Further encoding and metadata are added by hand, and items from the articles’ bibliographies are entered into a centralized bibliographic system that is also XML-based. All journal content is maintained under version control using Subversion. The journal’s organizational information concerning volumes, issues, and tables of contents is represented in XML using a locally defined schema. The journal uses Cocoon, an XML/XSLT pipelining tool, to process the XML components and generate the user interface. 
                
            
            
                
                    DHQ’s Evolving Data and Interface
                
                As noted above, 
                    DHQ’s approach to the representation of its article data has from the start been shaped by an emphasis on long-term data curation and a desire to accommodate experimentation. The specific encoding practices have evolved significantly during the journal’s lifetime. The first schema developed for the journal was deliberately homegrown and was designed based on an initial informal survey of article submissions and articles published in other venues. Following this initial period of experimentation and bottom-up schema development, once the schema had settled into a somewhat stable form we expressed it as a TEI customization and did retrospective conversion on the existing data to bring it into conformance with the new schema. At several subsequent points, significant new features have been added to the journal’s encoding: for example, explicit representation of revision sites within articles (for authorial changes that go beyond simple correction of typographical errors), enhancements to the display of images through a gallery feature, and adaptation of the encoding of bibliographic data to a centralized bibliographic management system.
                
                These changes to the data have typically been driven by emerging functional requirements, such as the need to show where an article has been revised or the requirements of the special issue on comics as scholarship. However, they also respond to a broader set of requirements that this data should represent the intellectual contours of scholarship rather than simply interface. For example, the encoding of revision notes retains the text of the original version, identifies the site of the revision, and supports an explanatory note by the author describing the reason for the revision. Although 
                    DHQ’s current display uses this data in a simple manner to permit the reader to read the original or revised version, the data would support more advanced study of revision across the journal. Similarly, although our current display uses the encoding of quoted material and accompanying citations in very straightforward ways, the same data could readily be used to generate a visualization showing most commonly quoted passages, quotations that commonly occur in the same articles, and similar analyses of the research discourse. The underlying data and architecture lend themselves to incremental expansion.
                
            
            
                Analysis
                The approach 
                    DHQ has taken offers several significant advantages and also some corresponding disadvantages. The most important advantages are
                
                 • The autonomy the journal has to control all aspects of its own data modeling and interface.
                 • The high value of the resulting data, from a historiographic perspective.
                 • The ease of long-term curation of the data, including continuing evolution of our modeling decisions.
                 • The ease of long-term evolution of the publication infrastructure, including migration to other XML-based systems as needed.
                 • The scalability of a template-based infrastructure: with the system in place, each article requires no incremental work in styling or design; all effort goes towards consistent representation of semantically valued features.
                These advantages all carry a burden of cost and effort: autonomy and control necessarily entail responsibility for maintaining appropriate levels of expertise and undertaking the labor necessary to build and revise technical systems. Because our article work flow includes some hand encoding in TEI, our managing editors need to be better trained and more expert than if they were simply formatting articles in Word and exporting PDF. However, there are also some less obvious tradeoffs. 
                    DHQ’s publication model gains its efficiencies and scalability through an emphasis on uniform handling of repeated features, but this means that it is comparatively difficult to accommodate individual authorial requests for special handling. These entail not only extra effort at the time of publication but also the long-term prospect of special attention during the future data curation activities and updates to the interface and publication system. Authors familiar with content management systems such as WordPress or Scalar are accustomed to being able to exercise a significant level of control over the formatting and behavior of their text and accompanying media such as images and video. Long-term data curation is a less visible feature of such publishing systems. 
                
                Even more interesting and challenging are the special cases that entail semantically distinctive features. Although such submissions are rare, they have provided some valuable test cases in which the data being represented is not a straightforward ‘article’ but some other rhetorical mode: commented program code, dynamic HTML that provokes reader interaction, an article in the form of a comic book. In handling these cases, 
                    DHQ has sought to find ways to accommodate the distinctive form of the original piece while also giving it a proxy presence within the standard 
                    DHQ XML archive, so that its content can be searched and analyzed as part of the larger 
                    DHQ corpus of DH scholarship. As these cases accumulate, the editors seek to identify repeated needs that could become part of the regular 
                    DHQ feature set. 
                
                In the full version of this paper, we will consider in greater detail the role of authorial design in digital humanities publication, and the possible convergences between XML-based systems like 
                    DHQ and content-management based systems like Scalar.
                
            
            
                Future Directions
                
                    DHQ is now completing a multiyear project to centralize its bibliography, and the next step will be to develop interface features that exploit this data. We are also in the planning stages of a project to explore internationalization of the journal through a series of special issues dedicated to individual languages. In both cases, these amplifications of the journal represent natural extensions of the journal’s existing architecture, and although both are substantial projects, they are made feasible by the investment already made in strongly modeled data and an extensible publication infrastructure. In the fuller version of this paper, we will discuss both of these developments in greater detail.
                
            
        
    



    
        
            
                Ivanhoe: A Platform for Textual Play
                
                    
                        Chawla
                        Swati
                    
                    University of Virginia, United States of America
                    sc2wt@virginia.edu
                
                
                    
                        Ferguson
                        Andrew
                    
                    University of Virginia, United States of America
                    af3pj@virginia.edu
                
                
                    
                        Grayburn
                        Jennifer
                    
                    University of Virginia, United States of America
                    jng3au@virginia.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Poster
                
                
                    pedagogy
                    storytelling
                    collaboration
                    content management
                    theory
                
                
                    interface and user experience design
                    software design and development
                    digital humanities - pedagogy and curriculum
                    creative and performing arts
                    including writing
                    social media
                    English
                
            
        
    
    
        
            As members of the 2014–2015 Praxis Program cohort at the UVA Scholars’ Lab, we set ourselves the task this year of updating Ivanhoe—the digital platform for roleplaying and textual intervention first formulated by Jerome McGann, Johanna Drucker, and Bethany Nowviskie. In its initial conception, the idea of Ivanhoe as a space for creative interaction and intervention grew out of ‘dissatisfaction with the limitations inherent in received forms of interpretation’; the players ‘wanted to develop a more imaginative form of critical methodology, a form closer in spirit and method to original works of poetry and literature’ (McGann and Drucker, 2003).
            This concretized thought experiment has seemed ideal for use in classroom curricula (McGann et al., 2004) that draw increasingly on notions of play and games for pedagogical outcomes (Willett et al., 2009). Play, generally, tends to be defined by what it is not: play is not work. Yet, over the past century, scholars and instructors have recognized and embraced play as a vital component of the learning process (Patton, 2014). As a result, games and play have been theorized and applied to curricula as a way to engage students in high school and college. While theory behind play education seems well-established, however, there is a strong disconnect between that theory—what we as educators believe we are doing—and what college students understand and internalize. In fact, by requiring such interactions, it seems that we eliminate the spontaneity and self-motivation so effective in play and ultimately induce student confusion, frustration, and anxiety (Ludewig and Ludewig-Rohwer, 2013). As we evaluated our goals for the purpose and design of Ivanhoe, we found ourselves wrestling with the classic question of digital classroom tools: How to make a program that students engage with by choice, rather than by force?
            In response to this question, we find ourselves investigating how textual intervention plays out in communities with long-running histories of sustained roleplay: tabletop and paper gaming, alternate history-making, fan fiction sites, and others. This has moved us away from concentrating on designing for classroom use alone (with its implicit inequalities of available resources, educational opportunity, and so on), and towards a more flexible tool designed to allow for textual creativity by a wide range of interactive, interpretive communities. By focusing on accessibility and ease of use across platforms—improving the desktop version on which Ivanhoe presently resides, but also making better use of WordPress’s mobile functionality—we aim to produce an updated Ivanhoe capable of fostering textual play as something sought out as its own end, rather than for the purpose of earning badges or engaging in other methods of gamification.
            At present, we envision such a resource as building on the presently available WordPress theme designed by the previous year’s Praxis group, keeping its emphasis on sustained roleplay and mutual commentary. However, we also look to draw on social media tools such as Yikyak and Snapchat that value anonymity, ephemerality, and asynchronous response time. This flexibility provides space for players to negotiate their roles and the conditions of their participation in a variety of emergent communities of play. As time allows, we hope also to incorporate improved functionality, with the capacity to incorporate not only a variety of media but also scripts, stylesheets, and other interactive elements—while maintaining a simple learning curve so that the platform can be used by players of any computing experience level.
            At the DH 2015 conference, we propose to demonstrate Ivanhoe in both static and dynamic formats—the former as a demonstration of sessions already played within Ivanhoe, showing the capabilities of the platform; the latter as a interactive play session that will run for the duration of the conference. Through this new build, we aim to realize the playful spontaneity inherent in the early Ivanhoe sessions of McGann, Drucker, Nowviskie, et al., while opening up the possibility of participation beyond the small groups of the classroom to a vastly expanded base of users.
        
        
            
                
                    Bibliography
                    
                        Ivanhoe. (n.d.). http://ivanhoe.scholarslab.org.
                    
                    
                        Ludewig, A. and Ludewig-Rohwer, I. (2013). Does Web-Based Role-Play Establish a High-Quality Learning Environment? Design versus Evaluation. 
                        Issues in Educational Research,
                        23(2): 164–79.
                    
                    
                        McGann, J. (in collaboration with J. Drucker and B. Nowviskie). (2004). Ivanhoe: Education in a New Key. 
                        Romantic Circles. December, http://www.rc.umd.edu/pedagogies/commons/innovations/IVANHOE.html.
                    
                    
                        McGann, J. and Drucker, J. (2003). The Ivanhoe Game. http://www2.iath.virginia.edu/jjm2f/old/IGamehtm.html. 
                    
                    
                        Nowviskie, B. (2009). Sketching Ivanhoe. June 5. http://nowviskie.org/2009/sketching-ivanhoe/. 
                    
                    
                        Patton, R. M. (2014). Games That Art Educators Play: Games in the Historical and Cultural Context of Art Education. 
                        Studies in Art Education,
                        55(1): 241–52. 
                    
                    
                        Pill, S. (2014). Game Play: What Does It Mean for Pedagogy to Think Like a Game Developer? 
                        Journal of Physical Education, Recreation &amp; Dance,
                        85(1) (January): 9–15.
                    
                    
                        Rao, D. and Stupans, I. (2012). Exploring the Potential of Role Play in Higher Education: Development of a Typology and Teacher Guidelines. 
                        Innovations in Education and Teaching International,
                        49(4) (November): 427–36.
                    
                    
                        Willett, R., Robinson, M. and Marsh, J. (eds). (2009). 
                        Play, Creativity, and Digital Cultures. Routledge, New York.
                    
                    
                        Wills, S., Leigh, E. and Ip, A. (2011). 
                        The Power of Role-Based E-Learning: Designing and Moderating Online Role Play. Routledge, New York. 
                    
                
            
        
    



    
        
            
                Mapping the Futures of Higher Education: Teaching, Learning and Research in the Age of Google
                
                    
                        Tagliaferri
                        Lisa
                    
                    The Graduate Center (CUNY), The Futures Initiative, United States of America
                    ltagliaferri@gc.cuny.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Short Paper
                
                
                    higher education
                    pedagogy
                    digital humanities
                    gis
                
                
                    geospatial analysis
                    interfaces and technology
                    digital humanities - institutional support
                    digital humanities - pedagogy and curriculum
                    English
                
            
        
    
    
        
            This poster will present an analysis of relationships and knowledge-sharing in the first Futures Initiative course, Mapping the Futures of Higher Education: Teaching, Learning, and Research in the Age of Google, offered at the Graduate Center of the City University of New York (CUNY) in spring 2015. Graduate students enrolled in this course are from widely different backgrounds and teach courses throughout New York City across levels and disciplines. In order to facilitate a collaborative, peer-driven learning environment among this diverse group, the Futures Initiative has deployed a sophisticated web-based technology. 
            The Mapping the Futures website (http://futures.gc.cuny.edu) utilizes a specialized version of Commons In A Box (CBOX), a software project based on WordPress and developed by CUNY and the Graduate Center. As an open-source platform, CBOX offers community and collaboration tools in a single installation package, with low barriers to entry for students, who have various levels of technical proficiency. The site is equipped with multi-site functionality in addition to groups and forums to allow each student to engage online with his or her own course, as well as the larger network of collaborators across campuses. Within the first few weeks, the site’s network includes over 200 users, 20 sites, and 15 groups, many of which are highly active. 
            This poster will be exploring and evaluating this online cross-campus community through data visualizations and analysis. As a Futures Initiative fellow with a background in both the humanities and the computer sciences, as well as an educational background both at CUNY and abroad, I will be exploring the efficacy of the course through its online community network and suggest ways in which aspects of its collaborative and digital pedagogy model may be expanded upon by the global higher education community. I will also consider how the Mapping the Futures site may be a scalable and adaptable digital model, working without geospatial limitations, to facilitate peer-driven and collaborative learning.
            The classroom sessions, led by Futures Initiative director and pioneering digital humanist Cathy N. Davidson and former Graduate Center president William Kelly, are designed to support graduate student-instructors to innovate in their own classrooms. As an interdisciplinary course, with students from humanities as well as STEM fields, it employs a number of digital and collaborative pedagogic approaches to re-envision the methodologies and structure of higher education in the 21st century, many of which are being housed on the Mapping the Futures site. In an effort to harness CUNY’s unique networked structure, in which graduate students teach at a wide array of colleges throughout the city, the course and its site leverage a multiplier effect through an intrinsic embedded flow that can disperse throughout the system. Two professors working with a dozen graduate students and 368 undergraduate students can create profound effects toward refactoring pedagogical assumptions and innovating both in the classroom and online. This first course will proliferate by fivefold through subsequent iterations in the coming semester, thus expanding the student outreach and prompting profound reverberations throughout the system. 
            The physical network of CUNY, the largest urban university in the United States, is echoed in the course’s site, with its linked network of course websites, discussions, documents, and reflections. Students are developing geographic information systems as part of their coursework, approaching it from various skill levels and through diverse methods. This visualization work, which joins identity with a map, is an important means of cultivating the community across campuses. This provides several instantiations of the students’ interconnectedness across disciplines and campuses, through virtual, physical, networked, and temporal mapping. The virtual map is symbolic of the online site that hosts this GIS work, providing a platform to connect students across geospatial realities.
            As a public-facing course intending to encourage iterations and cross-institutional collaboration, all meeting sessions are also posted on the Mapping the Futures website as blogs, videos, and publicly editable online documents. This transparency encourages evaluation, research, and community engagement. The inaugural Futures Initiative course will be completed along with its final public presentation in May 2015. CUNY and the Futures Initiative will be creating additional instances of this course over time, beginning with five more courses in 2015–2016, to foster a wider spread of innovative pedagogies within higher education that develops from the bottom up and offers support from the top down.
        
    


        
            Defining the “state of the art” in Digital Humanities (DH) is a really challenging task, given the range of contents that this tag covers. One of the most successful efforts in this sense has been the international blogging event known as “DayofDH” or “A Day in the Life of the Digital Humanities” project, promoted and sponsored by centerNet (
                http://www.dhcenternet.org/), which has put together digital humanists from around the world to document once a year what they do (Rockwell et al., 2012). The websites of DayofDH were hosted in North America until 2015, when it was coordinated in Europe by LINHD (
                http://linhd.uned.es), the Digital Innovation Lab, at UNED in Madrid. Participants belong to several countries around the world.
            
            The relevance of DH in non-English speaking countries has been quick and important in the last decade, and especially important in the Spanish-speaking world (Spence and González-Blanco, 2014; González-Blanco, 2013; Del Rio Riande, 2014a; Del Rio Riande, 2014b; Galina et al., 2015). Technological projects for humanities have existed in the Spanish world for many years; however, the discipline called “Digital Humanities” arose in 2011 with the first meeting that originated the Spanish Digital Humanities Association, HDH. This relevance is reflected in the creation of a parallel version of the DayofDH in Spanish, the “DíaHD”, which was hosted by the UNAM in Mexico in 2013 and 2014 and converged in the last initiative at UNED transforming both blogging events into a bilingual version of the Day. 
            Although there have been general studies about the information on participation in those events (Priani et al., 2014), there has not been an automated data analysis using NLP (Natural Language Processing) or Big Data tools to extract and classify the relevant information gathered in blogs (Webb et al., 2004). More technical details about these aspects can be found in (Tobarra et al., 2014b).
            According to this, the main goal of this paper is to develop a dashboard that allows us to get more insight about interest topics and leaderships of this community during the period of time in which this event has been developed. With the “dashboard” word, we mean the analysis and presentation of results, not a tool. In this sense, the topic characterization process deals with the detection of the most relevant topics which are employed in the publication tools of these kinds of virtual communities (Tobarra et al., 2014a).
            In order to achieve our aforementioned objectives, this work is focused on the datasets corresponding to four years of DayofDH (2012, 2013, 2014 and 2015 editions), and the Spanish version of the event in DíaHD 2013. This work strives at showing the evolution and trends in the last four years in order to give account of the presence of the Hispanic communities in the field. The information of the Spanish 2014 edition has been discarded, as it is not any more available online due to technical problems at the organizing institution. All editions of DayofDH employ WordPress, which has an associated SQL database, including several general tables and a specific set of tables per blog, defined in the project and common to all editions. The CMS is combined with the Buddypress social plugin, which lets users register, create communities and forums and interact among them. For the last edition of the Day, LINHD included also the bilingual plugin WPML to make it available the possibility of including translation in Spanish and English. This feature was, however, just used for the general website and its blog entries.
            The data employed in this proposal has been obtained by using web scraping techniques (Fredheim, 2014) in the DayofDH websites for the previously mentioned editions. In particular, humanists’ blog data, and their associated posts in the website have been gathered in this phase. All information scrapped from blogs is public and accessible from the Internet and, also, they have been anonymized for ethical issues. For validation purposes, the conceptual information about the database schemas have been compared with the extracted dataset, concluding the extraction process has been satisfactory.
            Since the data obtained are huge enough to be efficiently processed, the use of big data techniques have been considered for this work (
                http://social-metrics.org/analyzing-big-data-with-python-pandas/). In order to achieve the main goals of the project, all the information related to the textual content in DayofDH have been processed, so that the most significant tokens are selected. Then, these tokens have been characterized by two parameters; first, it has been used the direct frequency which characterize if a token is used regularly in all DayofDH blogs. Secondly, the inverse frequency of the token that give information of how significant the token is in the context of digital humanities in a semantic way.
            
            These parameters have been used to observe the interest and evolution of the characterized tokens along the time, either in a global and individual way. The interest of the global analysis is to find how the knowledge has evolved during the years of the study. From the point of view of a personal analysis, the interest is to build individual profiles that show the main interest of the researchers in the humanist community. Finally, the leadership relations have also been explored by using disease propagation techniques in the generated social network, taken into account the different editions of the DayofDH. For instance, Fig. 1 shows the social network according to the amount of authors’ participations.
            
                
            
            Fig. 1. Social network generated for 2012-2015 editions of DayofDH
            The resulting graphics and visualizations (Tobarra et al., 2014b) let users make a quick idea of how the DH focus has been moving and distributing across the time through the different Academies in the different countries, but also how topics and interests change from one country to another and it is strongly related to their perspectives and disciplines, which are not independent from their origin (as an example, see Figs. 2 and 3). This approach will enlighten future studies on DH perspectives with real and precise data on the current state-of-the-art on DH perception and its evolution. Data of the years 2009, 2010 and 2011 are not used at the moment, as the same information is not available through web scraping. They will be incorporated to this study as a future work.
            
                
            
            Fig. 2. Interest topics for 2012-2015 editions of DayofDH
            
                
            
            Fig. 3. Interest topics for 2013 edition of DíaHD
            
                Acknowledgements
            
            This work is part of next research projects funded by MINECO and leaded by the professor Elena González-Blanco; Research Europe Action EUIN2013-50630: Digital index of European poetry (DIREPO) and FFI2014-57961-R; and the Digital Humanities Innovation Lab: Digital edition, Linked data, and Research virtual environment for Humanities, in addition to the European project ERC-2015-STG-679528 POSTDATA. Authors would also like to acknowledge the support of the research project (2014I/PPRO/031) from UNED and Banco Santander, and a local project (2014-027-UNED-PROY). Furthermore, we thank both the Region of Madrid for the support of E-Madrid Network of Excellence (S2013-ICE2715) and the Spanish Government for the support of SNOLA Network of Excellence (TIN2015-71669-REDT).
        
        
            
                
                    Bibliography
                    
                        Fredheim, R. (2014). Web-scraping: The basics. Available online at 
                        http://www.r-bloggers.com/web-scraping-the-basics/ (Accessed 9th February, 2016).
                    
                    
                        Galina, I., González-Blanco, E. and Rio Riande, G. del (2015). Se habla español. Formando comunidades digitales en el mundo de habla hispana 
                        (in Spanish). 
                        Abstracts of the HDH 2015 Conference. Madrid, Spain. Available online at 
                        http://hdh2015.linhd.es/ebook/hdh15-galina.xhtml (Accessed 9th February, 2016).
                    
                    
                        González-Blanco, E. (2013). Actualidad de las humanidades digitales y un ejemplo de ensamblaje poético en la red: ReMetCa 
                        (in Spanish). 
                        Cuadernos Hispanoamericanos, 
                        761: 53–67.
                    
                    
                        Priani, E., Spence, P., Galina, I., González-Blanco, E., Alves, D., Barrón Tovar, J. F., Godínez Bustos, M. A. and Paixão De Sousa, M. C. (2015). Las humanidades digitales en español y portugués. Un estudio de caso: DíaHD/DiaHD 
                        (in Spanish). 
                        Anuario Americanista Europeo, 12: 5–18.
                    
                    
                        Rio Riande, G. del (2014a). ¿De qué hablamos cuando hablamos de humanidades digitales?. 
                        Abstracts of the AAHD Conference. Culturas, Tecnologías, Saberes. Buenos Aires, Argentina.
                    
                    
                        Rio Riande, G. del (2014b). ¿De qué hablamos cuando hablamos de Humanidades Digitales?. Available online at 
                        http://blogs.unlp.edu.ar/didacticaytic/2015/05/04/de-que-hablamos-cuando-hablamos-de-humanidades-digitales/ (Accessed 9th February, 2016).
                    
                    
                        Rockwell, G., Organisciak, P., Meredith-Lobay, M., Ranaweera, K., Ruecker, S. and Nyhan, J. (2012). The design of an international social media event: A day in the life of the digital humanities. 
                        Digital Humanities Quarterly, 
                        6(2). 
                    
                    
                        Spence, P. and González-Blanco, E. (2014). A Historical Perspective on the Digital Humanities in Spain. 
                        The Status Quo of Digital Humanities in Europe, H-Soz-Kult.
                    
                    
                        Tobarra, Ll., Robles-Gómez, A., Ros, S., Hernández, R. and Caminero, A. C. (2014). Analyzing the students’ behavior and relevant topics in virtual learning communities. 
                        Computers in Human Behavior (CHB), 31: 659–69.
                    
                    
                        Tobarra, Ll., Ros, S., Hernández, R., Robles-Gómez, A., Caminero, A. C. and Pastor, R. (2014). An integrated analytic dashboard for virtual evaluation laboratories and collaborative forums. 
                        Tecnologias Aplicadas a la Ensenanza de la Electronica (Technologies Applied to Electronics Teaching) (TAEE), 2014 XI. Bilbao, Spain, pp. 1–6.
                    
                    
                        Webb, E., Jones, A., Barker, P. and van Schaik, P. (2004). Using e-learning dialogues in higher education. 
                        Innovations in Education and Teaching International, 
                        41(1): 93–103.
                    
                
            
        
    


        
            
                Background
                Online education has been advocated as the ultimate way of democratizing knowledge, but recent research indicates that there are reasons for concern. As the Allen & Seaman 2014 report underlines, 66% of higher education institutions report that online education remains critical to their long-term strategy while 74% of chief academic officers consider the learning outcomes for online courses to be ‘as good as or better’ than traditional face-to-face courses. But “despite this confidence in online education, researchers continue to report ‘compromised quality in online courses’ as one of the concerns of faculty, administration, and the general public” (Kidder, 2015; Selingo, 2014). In the landscape of online teaching, MOOCs (Massive Open Online Courses) have received much attention in both academic and popular publications  (Bayne and Ross, 2015; Bulfin et al., 2014; Clara and Barbera, 2013) despite the fact that they are not representative of the diverse modalities of online teaching.
                Siemens (2012) makes a useful distinction between xMOOCs (behaviorist MOOCs) and cMOOCs (connectivist MOOCs). The former emphasizes “a more traditional learning approach through video presentations and short quizzes and testing” with a focus on “knowledge duplication”, whereas the latter focus on “knowledge creation” (Siemens, 2012). Along the same lines, Ozturk recently reported that new variations of MOOCs have emerged becoming more market oriented “aligning with instructivist, cognitive, and behaviourist pedagogy” (Ozturk, 2015). Moreover, the financial model of the MOOCs raises questions about the audience for and motivations behind this method of teaching (Ozturk, 2015; Manjoo, 2015).
                Conscious of this present situation, the #dariahTeach project (funded by an Erasmus+ Strategic Partnership) is developing a  network based in seven partner countries exploring the production, dissemination, and promotion of high quality, dynamic, extensible, localisable, and integrated educational materials for the digital humanities specifically tailored for third level education. It is adopting a cMOOC philosophy which focuses on ‘creation, creativity, autonomy, and social networked learning’ (Siemens, 2012) to provide pedagogical content that can be easily integrated into diverse teaching and learning situations. 
                A key consideration in the design of the platform is interoperability between courses/modules (and units within those modules) since DH draws on a wealth of methods and tools from a variety of disciplines. Moreover, it is envisioned that these modules will be used beyond the DH community as the societal impact of a culturally-driven digital transition grows opening up new ways of collaborating on productive theory and critical thinking (Hayles, 2012). Thus a goal of #dariahTeach is to develop rich educational materials that 1) instructors in the growing number of Digital Humanities programmes can use as appropriate to their own institutional settings and learning outcomes; 2) instructors in other disciplines can draw on and; 3)  students who are not at institutions that have DH expertise can use to develop the skills and methods, as well as understand the theoretical basis, to engage in digital humanities and humanities research.
                The project team is currently developing the infrastructure and design of the modules based on the production of five modules: Introduction to Digital Humanities, Text Encoding, AudioVisual Media and Multimodal Literacies, Retrodigitizing Dictionaries, and Ontologies and Knowledge Management. This paper will present the results of preliminary research carried out through an extensive study of user requirements, as well as desk research on module and platform design informed by a workshop in Belgrade funded by the Digital Research Infrastructure for the Arts and Humanities (DARIAH) on developing open educational materials. 
            
            
                Analysis of User Requirements
                The design and the implementation of a successful platform-based learning environment  melds concepts from psychology, education, and human-computer interaction. Poor interface design can become a serious obstacle to the learning outcome, as it may slow the process down and impose cognitive obstacles. To this end, a qualitative analysis and interpretation of online teaching practices and recommendations in the DH domain and the elicitation of corresponding user requirements was based on a series of semi-structured interviews with experienced instructors of online courses within Europe.
                Findings of the user requirement process are a key component of the development of the #dariahTeach platform. These indicate that the platform needs to cater for the following needs: be adaptable to different learning methodologies; allow for persistent roles; provide an API or advanced forms of web services so that new unforeseen components can be added to the environment; support  ad hoc groupings and grouping of materials across modules and units; allow for both synchronous and asynchronous collaboration and communication and enable user customization.
            
            
                Module Design
                #dariahTeach modules are designed as building blocks tailored to the exigencies of teaching situations in different educational and cultural contexts, allowing  for localization and adaptation (via translation, subtitles, domain-specific examples etc.). By offering examples of and encouraging further adaptation of training materials to specific linguistic/cultural contexts, #dariahTeach will dispel any notion that the use of ICT methods leads to abstract representations of culturally impoverished outputs. 
                It is important to stress two levels of translatability of module design: 1) translatability and adaptability of the language of instruction; and b) selectability, translatability and adaptability of primary sources and materials that are used in instruction. This means that an English-language module on Text Encoding, for instance, is localizable both in terms of the instructional narrative, as well as the kind of texts that are used to exemplify the taught principles and methods of text modeling: different genres (poetry, prose, drama) but also language (Latin, Greek, Serbian, Dutch etc.) 
                Our “Introduction to DH” module will also not attempt to impose a single pedagogical narrative on what is a constantly evolving and highly diverse, interdisciplinary field. Instead, our Introduction to DH is based on a micromodular, polycentric approach: a collection of mutually-linked, cross-referenced, metadata-rich short videos that shed light on DH as a community of practice from multiple perspectives without creating a false sense of uniformity.
            
            
                Platform Design
                Modules will be made available via an online portal/ web application based on existing solutions. This paper will explore the decision tree in adopting a solution including whether to use a well-established Content Management Systems (eg Drupal, WordPress, Joomla) with Learning Management System plugins and appropriate customizations or the use of a customizable Learning Management System (such as  Moodle or Blackboard). Considerations feeding into the decision tree include the platform being open source, freely available, well documented and customizable with plugin development support;   support for multilinguality; an embedded xml editor; collaboration and interaction functionalities (eg chats, forums and wikis); test and assessment functionalities; extended search functionalities for available metadata (mapped to Dublin Core and LOM to facilitate sharing and support interoperability and reusability (Roy et al., 2010); and copyright attribution and licence management functionalities. 
            
            
                Conclusion
                The paper will conclude with longer-term prospects for the project. Oversight of #dariahTeach will be maintained after the grant has ended by a General Editor and Editorial Board under the oversight of the DARAH’s Research and Education Competency Centre.
            
        
        
            
                
                    Bibliography
                    
                        Allen, I. E. and Seaman, J. (2014). 
                        Grade Change – Tracking Online Education in the United States, Babson Survey Research Group and the Sloan Consortium, LLC. http://www.onlinelearningsurvey.com/reports/gradechange.pdf
                    
                    
                        Bayne, S. and Ross, J. (2015). MOOC Pedagogy. In Kim, P. (ed.) 
                        Massive Open Online Courses: The MOOC Revolution. Oxford: Routledge.
                    
                    
                        Bulfin, S., Pangrazio, L. and Selwyn, N. (2014). Making ‘MOOCs’: The construction of a new higher education within news media discourse. 
                        International Review of Research in Open and Distance Learning, 15(5): 209-305.
                    
                    
                        Clarà, M. and Barberà, E. (2013). Learning online: massive open online courses (MOOCs), connectivism, and cultural psychology. 
                        Distance Education 34(1): 129-36. 
                    
                    
                        Ferguson, R. and Sharples, M. (2014). 
                        Innovative Pedagogy at Massive Scale: Teaching and Learning in MOOCs. Open Learning and Teaching in Educational Communities. Springer.
                    
                    
                        Hayles, N. K. (2012). 
                        How We Think. Digital Media and Contemporary Technogenesis. Chicago University Press.
                    
                    
                        Kidder, L. C. (2015). The Multifaceted Endeavor of Online Teaching: The Need for a New Lens”. In Hokanson, B., Clinton, G., Tracey, M. (Eds.) 
                        The Design of Learning Experience Creating the Future of Educational Technology. Springer, pp. 77-91.
                    
                    
                        Manjoo, F. (2015, 16 September). ‘Udacity Says It Can Teach Tech Skills to Millions, and Fast’. 
                        The New York Times,  http://nyti.ms/1ihbcp7
                    
                    
                        Ozturk, H. T. (2015).
                        Examining Value Change in MOOCs in the Scope of Connectivism and Open Educational Resources Movement”. 
                        International Review of Research in Open and Distributed Learning 16/5, Creative Commons 4.0.
                    
                    
                    Peters, D. (2014).
                        Interface Design for Learning: Design Strategies for Learning Experiences. San Francisco: New Riders.
                    
                    
                        Roy, D., Sarkar S. and Ghose S. (2010). A Comparative Study of Learning Object Metadata, Learning Material Repositories, Metadata Annotation and an Automatic Metadata Annotation Tool. In Joshi, M., Boley, H., Akerkar, R. (eds.). 
                        Advances in Semantic Computing 2: 103-26.
                    
                    
                        Selingo, J. J. (2014). “Demystifying the MOOC”. 
                        The New York Times, 
                        http://nyti.ms/1u6MYCL
                    
                    
                        Siemens, G. (2012). MOOCs are really a platform, In idem 
                        Elearnspace blog, 
                        http://www.elearnspace.org/blog/2012/07/25/moocs-are-really-a-platform/
                    
                
            
        
    


        
            Euchronie est un projet collaboratif francophone s’inscrivant dans le domaine des humanités numériques. Partant du constat qu’une abondante production de contenu sur le passé est autopubliée sur le web francophone, le projet vise créer une base de données en centralisant, collectant, indexant et hiérarchisant dès leur parution cette multiplicité de contenus.
                
                     Notes historiographiques, synthèses méthodologiques, premières versions d’articles en cours de rédaction, hypothèses d’un chapitre de thèse encore à écrire, analyses d’archives en ligne, tutoriels concernant de nouveaux outils, expression de doutes épistémologiques, recensions d’ouvrages récents ou de classiques, analyses de productions filmiques ou multimédias, prises de position dans l’espace public, micro-essais d’ego histoire, vidéos, podcasts audio…
                 Contemporains de l’essor de la blogosphère, puis des humanités numériques, de l’autoproduction dans le domaine musical tout autant que de la diffusion de la photographie amateur en ligne, ces contenus ont pour double spécificité d’émaner de chercheur.e.s et d’être autopubliés sur le web.
                
                     Doctorant.e.s aussi bien que professeur.e.s d’université, enseignant.e.s du secondaire, amateurs et amatrices éclairé.e.s faiant usage des méthodes de la discipline.
                 
             
            La base de données ainsi créée prend la forme d’un site web hébergé par Huma-Num, la très grande infrastructure de recherche numérique pour les SHS. Pour parvenir à créer rapidement un corpus d’étude, la collecte de données est facilitée par l’utilisation du plug-in PressForward pour WordPress. Libre et gratuit, aisé à prendre en main par des chercheurs néophytes en humanités numériques, il autorise la recopie partielle de pages produites sur différents sites web et réseaux sociaux, mais aussi de vidéos et de fichiers audio. Dans le même temps, l’agrégation des contenus s’accompagne d’une éditorialisation fondée sur la hiérarchisation des données : nous proposons un nouvel agencement basé sur l’ajout de métadonnées. Strictement normées, ces dernières permettent une catégorisation fine (chronologie, thématique et aire géographique) qui constitue l’une des principales valeurs ajoutées du projet.
            Pour développer le contenu de la base de données, chaque membre du comité éditorial d’Euchronie est chargé de la veille d’un ensemble de sites défini en fonction de son domaine de spécialité. Pour chaque actualité publiée, le responsable éditorial s’assure qu’il s’agit bien de l’expression d’un point de vue, sans prise de position partisane : il s’agit de la dimension qualitative de la sélection. D’autre part, le responsable éditorial n’est pas amené à faire une sélection sur des critères plus restrictifs tels que l’intérêt historiographique du contenu ou une quelconque adéquation avec une ligne éditoriale que la plateforme souhaiterait défendre. Au contraire, la volonté à la base de la création de cette plateforme est d’être un lieu de rencontre entre différents types de discours sur le passé. À ce titre, l’aspect collaboratif du projet est primordial : dans la mesure où l’indexation est avant tout pensée comme une interaction sociale, les usagers de la plateforme sont aussi invités à proposer l’agrégation de billets ou sites qu’ils et elles ont identifiés. Enfin, un billet de synthèse est publié chaque semaine sur le blog du projet. Dans celui-ci, chaque membre du comité éditorial y présente brièvement les trois meilleurs contenus de la semaine. 
            Le site web permet ainsi de rendre compte d’une écriture de l’histoire de moins en moins logocentrée et de plus en plus intermédiale, c’est-à-dire écrite et visuelle, parlée et sonore, interactive et collaborative. Euchronie est ainsi un outil créé par et pour des spécialistes, tout autant qu’un point d’entrée pour les passionnés du passé.
            En phase de développement, le projet dispose d’un 
                financement assuré par plusieurs laboratoires de recherche en histoire et information-communication. Au 1er novembre 2015, son comité éditorial se compose de 
                19 membres séniors et juniors en France, Suisse et Canada, tandis que la première version de son corpus agrège 
                100 références, tant au format textuel que vidéo et audio. La mise en ligne du site est prévu pour l’année 2016 et sera donc visualisable lors du colloque. Un 
                blog Hypotheses regroupe l’ensemble des informations relatives au projet.
            
            La création de notre poster doit permettre de présenter l’outil et d’échanger autour des problématiques liées à la construction d’un corpus dans une perspective de Digital Public History. Nous désirons notamment rencontre les développeurs de PressForward pour partager nos retours et comparer notre projet avec Digital History Now.
        
    


        
             The Exhibitium ProjectGeneration of knowledge about temporary art exhibitions for a multivalent reuse was the topic of the proposal presented to the 2014 competition organized by the BBVA Foundation for projects in the field of Digital Humanities, resulting selected from over 250 submissions. The project website is available at: http://exhibitium.com. The Exhibitium Project began in January 2015 and will end in December 2016, so currently we are completing the first phase., awarded by the BBVA Foundation, is a data-driven project developed by an international consortium of research groups.They are: iArtHis_Lab (http://www.iarthislab.es) and Khaos (http://khaos.uma.es) at the University of Málaga; Techne, ingeniería del conocimiento y del producto (http://www.ugr.es/~tep028/quienes_somos_es.php) at the University of Granada; and CulturePlex at the University of Western Ontario (http://www.cultureplex.ca). One of its main objectives is to build a prototype that will serve as a base to produce a platform for the recording and exploitation of data about art-exhibitions available on the Internet.Specifically, the ultimate Exhibition’s purpose is to extract unprecedented and strategic knowledge about temporary art exhibitions through the use of a variety of data mining techniques. Therefore, our proposal aims to expose the methods, procedures and decision-making processes that have governed the technological implementation of this prototype, especially with regard to the reuse of WordPress (WP) as a development framework.
            According to the project's purpose, it was necessary to create a device that, to the extent possible, could capture in automated way information on art exhibitions from any Internet source. Consequently, the inquiry into the possibilities of web mining strategies emerged as a priority from the early stages. Taking into account the high expressiveness and flexibility of linguistic structures usually used in the description of art exhibitions, our project opted for a mixed platform which combines the potential modeling system based on textual indicators, the heuristic means that characterize some methods -such as the Bayesian classification- and the human supervision provided by a well trained team of editors.
          
          1. General overview. WordPress as a framework of the Expofinder system 
            As Baumgartner et al. (2009: 1) established, the web data extraction task is usually divided into five different functions: (1) the web interaction, which mainly comprises the navigation through predetermined web pages containing the information sought; (2) the extraction of the searched data by means of a software that identifies, extracts and transforms them into a structured format; (3) the setting of a specific calendar that enable to perform automatically the extraction tasks in regular sequence; (4) the processing of the captured data, which includes filtering, transformation -if applicable-, refining and integration; and (5) the delivery of the resulting structured data to a variety of data analysis-based systems.
            Assuming this distribution as the most convenient for our purposes, we decided to include them in the Exhibitium’s architecture grouped into two large blocks.
            A. A block consisting of an automated capture system of information robust enough to ensure the reliability of the collected data.
            B. A second block made up by the set of elements necessary to store the data, including functions for filtering, cleaning, management, structuring and description. This block also incorporates a system to export the collected data to those platforms that will process and analyze them during the second phase of the project.
            Block A was called Beagle, and block B became known as ExpoFinder. Both blocks work in a coordinated manner, so that what is extracted by Beagle is put at the disposal of ExpoFinder. The two blocks are part of an unified system configured by a cyclic algorithm: Beagle captures, ExpoFinder analyzes and approves the captured information, the team of editors validates or discards what ExpoFinder has previously approved, and Beagle recaptures again (see figure 1).
            
                
                Figure1. Beagle + ExpoFinder. Operating plan (simplified)
            
            Regarding the software, after preliminary versions based on own developments, it was decided that the most interesting option between the free software and open source solutions currently available (as the openness philosophy is a sine qua non requirement of this project) would be to use WP as framework of the system.Although, in reality, according to the Tom McFarlin’s statement in his popular page «tuts +» (http://tutsplus.com/), it is more a foundation that a framework. And maybe he is right: a framework consists of a set of conventions as well as libraries and tools that allow us to easily start working on an application. In short, it provides the means by which an application can be built from scratch, from the database schema to the front end. However, a foundation allows to «extend» an existing application. WP has its own well-defined internal mechanisms, and the foundation simply expands its operation or takes advantage of it for their own benefit. The main benefits that the use of WP as framework offers for our project can be synthesized in the following items: a database with a flexible and very solid organizational structure; a layer of a core application with numerous hooks which allow to maximize its functionality; and a high easy management system to carry out tasks on the two sides (server and client), assuming both administrator and user roles.The advantages that a robust mechanism as such provided by WP offers for the maintenance of a security system (essential in any development accessible through Internet), or the substantial savings in time and resources involved in a CRUD structure records management -which is both sufficiently malleable to suit any need and rigid enough to follow canonical deployment patterns (such as the «nonce» safeguards in the capture forms), are weighty arguments when opting for the use of one framework or another. Thus, for the implementation of the Beagle-ExpoFinder system we took advantages of the predefined data base, the available APIs and the set of data visualization templates to build solutions using an application that is already fully functional.
            We used WP without adaptations, that is, as it can be downloaded from the Internet. All the functionality of our application lies, then, on the code itself that constitutes WP, so it is not supported on variant versions (forks) of the original program. Hence, any improvement provided by the computing community will be directly usable by our project without further adaptations. As part of the requirements of the development of the Beagle-ExpoFinder system (B + E), from the beginning it was considered that the programming work did not constitute a «tailored suit» for the Exhibitium project. On the contrary, we expect that this work can be useful in other projects with a small number of modifications or by using configuration files or other similar systems. For that reason, our choice was to implement B + E by means of a «WP theme», solution that easily allows us to readapt the software to different purposes.
          
          
          2. Beagle and Expofinder development and technical features 
            Beagle captures – as it has been said- by automated means web data concerning temporary art exhibitions from any source of information, and includes a filtering mechanism. The automated capture process uses the tools of WP API, particularly WPCron. Likewise, the frequency of the process is configured according with the options offered by ExpoFinder to the system administrator. Beagle employs two statistical complementary functions to «predict» the degree of the adequacy of the captured information to the ExpoFinder preconditions:Even though this document is not largest enough to expose in detail the list of selected preconditions of significant terms used by Beagle in order to filter the captured information, we want to emphasize that this is a «weighted» relationship of lexemes in which each root term is assigned a total «weight» in the set (1 to 3). When the entire process is complete, the absolute amount of the sum corresponding to the found terms is weighted with the relative values (relating to the length of the text where they have been detected) to assign a positive or negative evaluation to the whole information. 1. One of this is based on the intersection of a set of «positive» and «negative» keywords with a proportional weight assigned to each one, which is also based on the shortest path algorithm of Bellman-Ford;The Bellman-Ford algorithm (or Bellman-Ford-Moore) calculates the shortest paths from a single source vertex to all other vertices in a weighted digraph. It is slower than Dijkstra's algorithm for the same problem, but more versatile, since it is suitable to deal with graphs using negative numbers for edge weights. ExpoFinder takes advantage of it in its weighting mechanism, useful for us because we work with lists of lexemes for words used as «positive» or «negative» markers. 2. The other is defined by its heuristic nature; it employs a naive Bayesian classifierIn machine learning terminology, the «naive Bayesian» constitutes a family of simple probabilistic classifiers based on the application of Bayes’ theorem about the hypothesis of independence between variables. Widely studied since the 1950s, it began to be used since the beginning of the next decade as a taxonomy method capable of self-optimization in the recovery community text. We use the frequency of occurrence of a given lexeme as a trigger, so that ExpoFinder can contribute to the semi-automated selection of relevant information from the experience gained. It is not a pure discriminative mechanism, but an auxiliary tool that has proven to be useful for application operators. to guide the «human» editor during the task of discriminating whether or not an information captured by Beagle is valid. The latter is able to improve their efficiency through continuous learning processes (each discarding or acceptance made by the «human» editor refines the system «perceptiveness» (see figures 2 and 3).
            
                
                Figure 2. Screenshot. Exhibitions list (fragment). See the Bayesian classifier indicator
            
            ExpoFinder also includes a control system (QC) that identifies the mistakes and failures, which are also associated with the human editor who made them, so that he/she can perform the appropriate corrections (see figures 3 and 4).
            
                
                Figure 3. Screenshot. Automated evaluation of efficiency
            
            
                 
                Figure 4. Screenshot. Quality control (QC). Resume
            
            In its current state of development, the Beagle-ExpoFinder system captures and selects daily about 100 references from more than 12,000 web sources of information. Its error rate during the recording and validation processes is about 3.9%, below the 5% initially considered as permissible.
          
        
        
            
                
                    Bibliography
                    
                        Baumgartner, R. et al. (2009). Web Data Extraction System. In 
                        Encyclopedia of Database Systems. Springer-Verlag.
                    
                    
                        Kokkoras, F. et al. (2013). DEiXTo: A Web Data Extraction Suite. 
                        Proceedings of 6th Balkan Conference in Informatics (BCI 2013), Nueva York: ACM, pp. 9-12. 
                    
                    
                        Pree, W. (1994). 
                        Design Patterns for Object-Oriented Software Development. Reading, Massachussets, USA: Addison-Wesley, ACM Press Books. 
                    
                    
                        Raposo, J. et al. (2002). The Wargo System: Semi-Automatic Wrapper Generation in Presence of Complex Data Access Modes. 
                        Database and Expert Systems Applications. 
                        Proceedings, 13th International Workshop, IEEE, pp. 313-17.
                    
                
            
        
    

This poster session will introduce DH Box, a browser-based platform that provides access to a variety of difficult-to-install digital humanities tools. Currently under development at The Graduate Center, CUNY, and funded by generous assistance from an NEH Startup Grant, DH Box is designed with particular attention to digital humanities pedagogy. Because teaching DH requires infrastructure of various kinds, including computing resources, IT support, and teachers with specialized training, student access to the digital humanities is unevenly distributed and highly dependent on local institutional conditions. Faculty with DH skills are often called upon to teach classes and workshops in adverse circumstances, contending with limited access to labs or other computing resources, restrictive IT policies that prevent new software from being installed, and platform fragmentation on student-owned machines. As a cloud-based platform, DH Box was conceived to address some of these concerns by providing a set of digital humanities tools through a unified computing environment accessed through the browser.

DH Box is not a service, nor is it a standalone application that runs on local computers. Rather, it is software that can be set up by teachers or institutions on cloud-based infrastructure such as Amazon Web Services or DigitalOcean. DH Box is an open-source project developed by and for teachers and researchers in the humanities, and as such is intended to provide an alternative to proprietary services that prevent access to user data. DH Box also makes an intervention in terms of design, UX, and usability. The platform uses a virtualization technology called Docker to quickly create and remove digital working environments, providing students with access to either shared work spaces or individualized virtual machines. Each of these environments contains a set of tools and utilities frequently used in the digital humanities, including IPy-thon Notebooks, the Natural Language Toolkit, MALLET, Omeka, and WordPress. DH Box's tab-based browsing interface makes it possible to easily switch between these utilities, the command line, and a text editor. In addition to providing DH tools in a unified computing environment, the platform offers resources in partnership with institutions like the British Library, making digitized texts available for student exploration. By integrating Git Lit, a tool for downloading text corpora developed in partnership with Columbia University, DH Box provides access not only to digital humanities tools, but also to materials for analysis and experimentation.

Even as it aims to circumvent institutional barriers to DH scholarship, DH Box addresses broader questions of access to DH tools. The necessity for specialized knowledge--use of the command line, importing packages and modules, configuring a working envi-ronment--may deter students from pursuing research questions using these novel methods. Making these tools more accessible, on the other hand, should help encourage a new generation of DH scholars. By allowing teachers and students to bypass the difficult process of installation and configuration, the DH Box team hopes to give them room to focus on exploration and experimentation. During the DH Box poster presentation, team members will be available to discuss decisions of design as well as future use cases.

Resources
For more information about DH Box, please refer to: Dhbox.org | @DH_Box | https://github.com/DH-Box
Description of Session Topic
How can DH regional communities best be cultivated and sustained? This panel explores US-based digital humanities collectives that foster active communities of practice. Regional consortia are a growing phenomenon in US digital humanities, offering opportunities for loosely organized groups of DHers to share workshops, events, datasets, conferences, and news with one another. Such arrangements have the benefits of addressing institutional barriers to DH work and enabling sharing that can help address, at least in a small way, funding and infrastructural inequities that can make it hard for newcomers to begin DH work. According to John Theibault, regional consortia “can be distinguished from, on the one hand, state and national digital humanities groups that organize conferences and edit journals and require paid membership, as well as digital humanities centers located in a single institution or formally constituted groups with explicit criteria for admission, and, on the other hand, not visibly organized interactions in active digital humanities regions, even if those interactions are frequent in practice” (2016). Theibault identified 11 self-organized regional consortia not affiliated with a larger organization such as EADH, mostly in the US. Such consortia can serve an important need in connecting researchers from a range of institutions and building community more expansive than a campus and more localized than a national or international scholarly organization. As Rebecca Frost Davis and Bryan Alexander have pointed out, “largescale multi-institutional projects aimed at building resources and pooling expertise . . . [can be] constructed to match the needs of both small liberal arts colleges and large research institutions.” The building out of regional consortia, then, can help broaden the impact and audience for digital humanities work while simultaneously addressing infrastructural needs and allowing institutions to share complementary strengths.

This panel will consider how a range of regional DH groups have organized their communities, many of them through the free WordPress-based platform Commons In A Box, to build active and lasting communities of DH practice. Presentations will cover the contours and plans of specific regional communities as well as the guidance such examples may offer to academic communities just beginning to organize themselves. Attention will be paid to shared commonalities across regional organizations as well as distinctive areas of focus. Included in the panel will be a discussion of software tools that communities can use to build regional DH organizations, with a particular focus on the Commons In A Box software used by many of the groups represented on the panel. The panel will also delve into the challenges of organizing and sustaining a regional consortium, including rewarding the volunteer labor that such organizations depend upon, raising awareness of the consortium, and developing an appropriate governance model.

Though this panel focuses on US-based regional organizations, it is hoped that the panel will stir conversations on an international level about shared models of community sustenance and infrastructure. Out of this exchange, the panel hopes to build an informal network of regional DH consortia that can serve as the basis for the ongoing development of DH communities.

CBOX Development and NYCDH
Matthew K. Gold
This presentation will discuss the design, implementation, and future plans for Commons In A Box (CBOX), with a particular focus on how it is being used in the NYCDH community to foster community among New York City DH researchers and practitioners. In NYCDH, a CBOX site has been used to create groups around discrete topics such as “Digital Art History,” “Librarians In DH,” and the “Digital Antiquity Working Group.” Each of these groups were started by members of the site and are used, along with larger public groups such as “Announcements,” to foster connections across a range of NYC institutions. In recent years, the site has facilitated the planning and implementation of an annual conference that includes a week of workshops on DH subjects offered across the NYC area by a range of institutions, and an annual prize for the best graduate student DH project. The site includes a shared calendar that is populated by members and member institutions, so that it provides a quick sense of DH events in the region.

Commons In A Box is a free software project supported by a grant from the Alfred P. Sloan Foundation and maintained by the Graduate Center of the City University of New York. Commons In A Box has two elements: a plugin manager and a default theme. The plugin manager enforces version dependencies between a collection of WordPress plugins, ensuring that all such plugins work seamlessly and without conflict—a technical innovation for the WordPress plugin ecosystem. For the end-user, the experience of installing Commons In A Box is extremely smooth; upon activating CBOX, the plugin manager automatically performs what would otherwise be dozens of manual steps, including plugin installation, activation, and configuration. The CBOX theme, which is based on the design of the CUNY Academic Commons, can also be installed; when activated, it creates a Commons space that features image sliders, community elements such as lists of members and recently updated blogs in the sidebar, and group-related functionality such as “reply-byemail” group forum functionality that has been crucial to community building on the CUNY Academic Commons. The result, and the key technical innovation of the CBOX project, is the transformation of a complicated set-up and customization procedure into an easy, streamlined installation process. CBOX is currently used by a number of organizations, including the Modern Language Association, NYCDH, Texas DH, Virginia DH, and Florida DH, to build Commons spaces for DH communities.

Under the aegis of an NEH Implementation Grant, Commons In A Box - OpenLab is a new version of CBOX dedicated specifically to teaching and learning, called CBOX-OL. In this presentation, we will describe how integrating a suite of teaching-centered digital tools for content sharing and annotation into the core code of CBOX and bundling it with a core set of DH pedagogical tools ensuring that CBOX-OL communities meet accessibility standards. We will also partner with OER initiatives at partner institutions to create ways of sharing open-access humanities content within and across Commons environments that promote and reinforce sound citation practices. Finally, we have secured a commitment from Reclaim Hosting, a popular hosting service for educational institutions and digital humanities projects, to integrate CBOX-OL into its suite of easily installable software packages. Included in the new version of CBOX will be a set of teaching-related plugins as well as a suite of DH-related WordPress tools in the areas of scholarly communication, such as Braille, which translates English text into Braille; Anthologize, which enables bloggers to create eBooks from their posts; DiRT Tools, which helps community members identify the tools from the DiRT Directory they use; and PressForward, which can be used by teachers to collect content, and then to assign the selection, evaluation, and re-publication of that content with an introduction as an activity to encourage students to connect their classroom discussion with ongoing contemporary debates. All of these plugins will benefit the digital humanities humanities via their potential to enable open learning and exchange, emphasizing scholarship as an open, process-based activity, rather than a closed, solitary one.

Commons spaces don’t build themselves; rather, they are cultivated and energized by participation and leadership from members. Technical development lays the foundation on which such work can happen, but the growth, evolution, and sustainability of such platforms require vision, commitment, and resourcefulness from members of the communities where they launch. This presentation will close with a consideration of the ongoing personal, institutional, individual, consortial, and infrastructural support needed to foster and maintain flourishing communities of active practitioners.

The CUNY Academic Commons
Micki Kaufman and Lisa Rhody
The CUNY Academic Commons is an academic network built at the City University of New York (CUNY) by and for faculty, graduate students, administrators, staff, postdocs, and alumni across the 24-campus system. The site serves to foster community and promote scholarship across our twenty-four campuses. With over 7,293 members and well into its seventh year, the Commons continues to evolve as a vibrant space where members connect, create, collaborate, and explore.

As the site has grown and matured, user outreach has become an increasing focus. In addition to webinars and one-to-one engagement with users and administrators of Commons groups, the team has most recently established Commons Faculty Fellowships to assist faculty in setting up Commons sites and customizing the platform to provide the right teaching tool for class needs. In conjunction with CUNY campus Teaching and Learning Centers, these outreach efforts are helping to promote the benefits of the Commons as a teaching tool across the 24 campuses.

A significant focus of the Commons development has been on personalization of the user experience. With the addition of attractive, customizable public profiles, members can now create beautiful online portfolios for their work. The high-impact header section provides an elegant profile synopsis and collapses as one scrolls down the page. Social media icons and website links help connect user profiles to a range of services on the web. Our sophisticated profile builder makes it easy to create an online CV using freeform and specialized widgets designed to highlight positions, education, publications, and interests, and the new RSS feed widget helps members include excerpts of recent posts. Likewise, the addition of Quick Links gives user profiles, blogs and groups customizable shortened links, easy to remember and ideal for business cards and CVs. Using the cuny.is/ URL shortener helps Commons users to personalize their site and to more effectively communicate their relationship to CUNY.

The Commons allows users to create and join as many groups and websites as they want. This allows users to administrate departments, teach classes, discuss and share resources on a selected topic, and connect with people sharing common interests on the Commons. Features of the Commons include the ability to have groups be public, private, or hidden and can include a host of functions including discussion forums, file storage and document collaboration, and rich email integration that allows a more email-driven, ‘listserv-like’ interaction with the Commons. Sites created by Commons users take many forms including personal blogs, research projects, department, class, event or conference sites, journals, reviews and news commentaries, and photo blogs, and are configurable using hundreds of themes and plugins for a wide range of visual presentations. Users also have free access to the Commons community’s WordPress Help group to get help building their sites and managing accessibility.

The Commons also now includes Social Paper, a networked writing environment that enables students to compose and share all forms of their written work across classes, disciplines, semesters, and publics. Likewise, students can browse and comment on the papers of their peers. Unlike many learning management systems or course blogs, Social Paper gives students full control over the sharing settings of each individual piece of writing. Students may choose to share a paper with a professor, a class, a writing group, the public at large, or alternately, keep it private as part of their personal, in-progress, reflective writing portfolio. Additionally, while composing, students can post comments on their writing with questions mentioning other users in order to solicit peer feedback or interest. By giving students a centralized space to manage the totality of their writing, students can easily change privacy settings as they mature as writers and thinkers, develop audiences for their growing body of work, and reflectively build off prior writing.

As the Commons continues to grow, and as its development team continues to release updated versions of Commons In A Box, the site will endeavor to continue serving the needs of faculty, students, administrators and alumni across the 24 CUNY campuses -- and in the process exemplifying how an academic social network can be sustained across a university system with multiple campuses.

Texas Digital Humanities Consortium
Lisa Spiro
Even as digital humanists at Texas colleges and universities are creating significant digital humanities projects, we face common challenges, such as finding collaborators, learning new skills and developing educational programs. In the fall of 2013, Lisa Spiro (Rice), Cameron Buckner (University of Houston), and Laura Mandell (Texas A&M) discussed establishing a statewide digital humanities consortium to connect digital humanists across the state and foster collaborations. The University of Houston hosted the first Texas Digital Humanities Consortium (TxDHC) conference, co-sponsored by Rice and Texas A&M, in April of 2014. At the conference, we held an open business meeting to plan the TxDHC, which attracted 19 participants from colleges and universities across the state. We discussed common needs, including to support faculty, graduate, and undergraduate training in DH; to build community so that members are aware of projects, opportunities and potential collaborators across the state; and to gain access to infrastructure (such as Omeka) for projects (Spiro 2014). To meet these needs, we planned to develop a website, hold an annual peer-reviewed conference, and provide informal opportunities to interact, such as by publicizing visiting speakers at our home institutions. We also explored creating internship opportunities for graduate students and advocating for DH. Rather than establishing formal structures, we decided to operate as a “coalition of the willing,” with decision-making by consensus. To gather additional input, TxDHC used an online survey, which had 14 respondents between April and September of 2014. Respondents ranked the following as the leading “high” priorities: foster networking (92.9%); identify researchers in Texas with common interests (71.4%); and facilitate collaborative research (64.3%).

Informed by the input received from the meeting and survey, Spiro set up a founding steering committee (SC) for TxDHC, inviting representatives from Texas universities and colleges to serve. Current SC members include Spiro (chair), Jennifer Hecker (University of Texas), Laura Mandell, Rafia Mirza (UT-Arlington), Laurel Stvan (UT-Arlington), Toniesha Taylor (Prairie View A&M), Andrew Torget (University of North Texas), and Dillon Wackerman (SMU); representatives from Southwestern University and University of Houston have also served on the committee.The Steering Committee meets via video conferencing several times each year to discuss the ongoing development of consortium and events such as conferences and webinars; we also communicate fairly regularly through email. Our mission is to “ promote digital research in the humanities disciplines and facilitate interaction amongst researchers working in the digital humanities both within the state, nationally, and internationally” by connecting people, facilitating training and knowledge sharing, and raising the visibility of DH work. Membership in TxDHC is open to anyone who sets up a profile on the organization’s website. Currently there are 58 “active” members and 116 members signed up for announcements.

This presentation will explore TxDHC’s history, initiatives, challenges and future plans. TxDHC’s core activities focus on building community across the state, including through:

•    Our website. Texas A&M’s Initiative for Digital Humanities, Media, and Culture (thanks particularly to the work of former staff member Matthew Christy) installed and hosts the TxDHC’s website ( http://www.txdhc.org/ ), using Commons in a Box to encourage collaboration. The website includes member profiles, groups focused on topics such as Training and Metadata Standards, and a calendar. We have also implemented the DiRT Tools plugin to identify tools used by TxDHC members ( http://www.txdhc.org/tool/ ).

•    State-wide    conferences. TxDHC has

sponsored two multi-day conferences: its inaugural conference at the University of Houston in 2014 and its second conference at UT Arlington in 2015. While these conferences boasted strong programs, recruiting Texas institutions to host the event has proven challenging, especially since TxDHC lacks resources beyond endorsing the conference, publicizing it, and enlisting steering committee members to assist with it. In 2016, TxDHC shifted to a strategy of partnering with other Texas digital humanities/ digital library conferences to hold post-conference events, avoiding duplication of efforts, making it more convenient for people to attend both events, and taking advantage of cross-publicity. Recently we organized    a    one-day    hybrid

unconference/mini-conference following the Texas Conference on Digital Libraries in Austin and a THATCamp following Digital Frontiers in Houston. The mini-conference combined the best of THATCamps and more formal conferences by giving speakers fifteen to twenty minutes to set the context, then devoting the rest of the hour to discussion. While attendance at THATCamp Digital Frontiers was fairly small, the event connected participants from Rice, the University of Houston, UT Arlington, UT Austin, and local museums and explored topics such as creating an introduction to DH course and reviving DH projects.

•    Webinars: TxDHC runs occasional web-based workshops on topics such as OpenRefine, DPLA,

and using tools like Omeka or Hypothes.is in the classroom. To encourage interaction and build community, we employ a video conferencing platform so that participants can see each other’s faces, and we try to set aside time at the end of each event for information sharing.

TxDHC faces several challenges:

•    Raising awareness of its existence. At the

recent Digital Frontiers conference, it became clear that many attendees didn’t know about TxDHC. The organization is primarily publicized through its events, website, word of mouth, and Twitter, but more outreach to DH groups across the state is needed.

•    Accomplishing its vision with few resources.

Lacking a budget or staff, TxDHC depends on the time and commitment of its hard-working volunteers, particularly its steering committee members. Of course, these volunteers face competing demands on their time, so consortium work is fit in as it can be.

•    Keeping the website updated and encouraging people to make full use of it. Ideally, members of TxDHC would add events to the calendar and participate more actively in online groups, but those hopes haven’t yet materialized. Focused outreach may increase participation.

•    Developing a governance model. Initially we operated without clearly defined roles, which allowed for flexibility but also meant that members didn’t necessarily get recognized for their contributions and weren’t tied to particular responsibilities. Moreover, we would like to develop a more coherent and transparent method for bringing new members onto the steering committee and rolling senior members off. In December of 2015, members of the steering committee came together for a daylong retreat at Rice University, where we discussed the need to spread out the work and ensure that all are recognized for their contributions and sketched out more defined roles. We are working on bylaws to formalize the organization’s operations and are in the process of implementing plans made at the retreat. At the same time, many SC members are sympathetic to the lightweight organizational approach taken by NYCDH, which several Steering Committee members learned about during a recent presentation by Alex Gil and Kimon Keramidas at Digital Frontiers 2016. Focusing on partnerships and fostering communication across Texas DH organizations seems like a sound strategy for a small, allvolunteer organization like TxDHC.

• Dealing with geographical distance: Since Texas is the second largest state in the US, it can be difficult to connect people across such a significant distance. To cope with this distance, we have organized state-wide events and online gatherings, but we also hope to encourage members to use the website to promote events and activities within a particular city or region within Texas.

By working together, regional consortia can learn from each other, explore developing common infrastructure (as we have already benefited from CUNY’s work on Commons in a Box), and facilitate broader collaborations around research and teaching. Florida Digital Humanities Consortium

Scot French
My presentation will explore the opportunities and challenges involved in creating a geographically extended, self-governing regional consortium (FLDH/Florida Digital Humanities Consortium) that depends, for its intellectual labor and technology support infrastructure, on academic institutional sponsorship while promoting free, open access to academic networks and DH resources.

Founded at THATCamp Florida 2014, FLDH’s mission “is to provide a platform for studying and discussing digital tools, methods, and pedagogies as well as for educating teachers, faculty, and the public about the multiple, interdisciplinary ways humanities research and computing impact our world. It meets annually to identify issues of interest and to set goals for future collaboration and digital humanities research.” At present the group has 12 institutional members, ranging from large research institutions to small liberal arts colleges: University of Florida (Gainesville), University of Central Florida (Orlando), Florida State University (Tallahassee), University of South Florida (Tampa), University of Miami, Florida International University (Miami), Rollins College (Winter Park), New College of Florida (Sarasota), Florida Southern College (Lakeland), Eckerd College (St. Petersburg), and the Florida Humanities Council. Inspired by NYCDH and the CUNY Academic Commons, group organizers adopted the Commons In a Box (CBOX) academic commons social networking platform as a virtual space in which to foster community and share resources.

An internal grant from the University of Central Florida funded an organizational meeting in Orlando at which representatives of participating schools and the Florida Humanities Council established an Executive Council with two representatives from each institution. A five-member elected Steering Committee drafted a mission statement and set of by-laws, established short- and long-term goals, and began planning for two major initiatives: Hosting HASTAC17 in Orlando (scheduled for November 2017) and submitting a proposal to host a southeast regional preconference workshop on visualization tools through the National Endowment for the Humanities’ Institutes for Advanced Topics in the Humanities. Working on these initiatives has placed new demands on FLDH’s leadership structure, and prompted a shift from volunteer initiative to a more active chair and working group/subcommittee organizational framework.

To date, all of our discussions about how best to build and organize our statewide/regional consortium have been internal to the group. We seek to expand the conversation to representatives of similar groups, particularly those using the CBOX platform developed by Matthew K. Gold and his project team at CUNY. Out of this exchange we hope to build an informal network of regional DH consortia for purposes of information sharing that can be expanded and formalized as needed.

This paper/presentation will raise issues of general interest to the DH community and of particular concern to members of the FLDH Executive Council (which I chair) and Steering Committee (on which I serve as a founding member).

•    Leadership/Self-Governance    Structures.
What sorts of leadership/self-governance structures do regional DH consortia employ to ensure    broad-based    institutional

representation, inclusion of diverse views, and transparency in setting group agendas and policies?

•    Academic Commons Activity. What strategies or best practices might regional consortia adopt to build community and generate sustained activity on CBOX or other academic commons platforms? How can participating institutions most effectively contribute to the development of CBOX or similar platforms, with an eye

toward enhanced functionality and customization?

•    Staffing/Program Support. Is it feasible for participating institutions to “share” staff in support of mutually beneficial program and projects? What roles might digital humanities center staff, graduate research assistants, or postdoctoral research/ teaching fellows play in fostering a regionwide DH community and providing training for interested faculty and students? Is state, national, international, or foundation funding available in support of such regional collaborations?

•    Service/”Invisible Work.” To what extent, if at all, is the volunteer work of consortia officers (such as FLDH Executive Council/Steering Committee members) recognized as valued service within their respective institutions? Could regional consortia have a role in documenting and validating the contributions of active members seeking promotion and tenure or other paths to career advancement?

•    Geographic Distance/Virtual vs. Face-to-Face Meetings. Are webinars and Google Hangouts an adequate substitute for face-to-face meetings and workshops? How might regional consortia spanning large geographic areas (such as Florida and Texas) create more funded opportunities for travel to consortia-sponsored conferences/meetings? What options are available both within and across member institutions for fundraising in support of travel?

As chair of the FLDH Executive Council and a member of the Steering Committee I will conduct an informal member survey to solicit other potential topics for discussion during this session. Ideas generated by this panel will be shared with the Executive Council and serve as the basis for discussion and action at an FLDH organizational meeting during the Orlando HASTAC conference in November 2017.

SD|DH: Building and Strengthening DH Teaching and Learning through a Regional Network
Erin Glass and Jessica Pressman
How can DH regional networks work to spread resources to underserved student populations, foster digital literacy and confidence in educators without DH institutional support, and strengthen local, humanistic forms of social advocacy? This presentation will focus on the development and future plans of the San Diego Digital Humanities (SD|DH) regional network, a collective comprised of faculty (and some staff and graduate students) from seven different local higher education institutions ranging from a R1 university to community colleges. While San Diego is not prominently known for humanities research—let alone DH—we recognize that its location on the border of Mexico, and the diverse student body of its institutions, make it an exceptionally rich site for socially-engaged, participatory forms of DH activity. By pooling resources, expertise, experience, perspectives, and moral support, SD|DH embraces the power of diversity as our chief value.

SD|DH has been working together for the last three years to share experiences and resources, plan events, apply for funding, and seek collegial support for our endeavors. We work from different institutional situations and relationships to digital humanities: a campus pursuing a grass-roots and ground-up approach to digital humanities research and teaching but lacking funding for institutional infrastructure, a campus administration wanting to implement digital humanities from a top-down structure, and a campus successfully implementing digital humanities projects within a specific department but lacking the leadership to build out from there. We have been successful in different ways at different campuses, but we now have a full-blown DH initiative at SDSU and an emerging program at USD, and we continue to use our individual campus efforts to bolster the region

Our first goal in forming SD|DH was to assess the barriers to implementing DH across a wide spectrum of institutions and diverse student populations, and develop work-around strategies for implementing DH by drawing upon the resources of multiple institutions within a single region. From the start, these efforts have been voluntary, but we were able to expand our efforts and visibility with the generous support of a National Endowment for the Humanities Level I Digital Start Up Grant for our project “Building and Strengthening Digital Humanities through a Regional Network.” This grant supported a series of workshops that focused on distributing DH to institutions and student populations usually left out of DH. It also helped encourage educators to explore new DH techniques for teaching within the safety net of a supportive community. Educators engaged in a wide range of DH methods such as text analysis and game design as well as DH-inspired techniques to creative re-imagine uses of everyday software and tools for pedagogical purposes. We met several times throughout the year to discuss challenges and successes, share expertise, and brainstorm new possible projects.

In this presentation, we will discuss our methods for developing and facilitating these workshops and pedagogical engagements as well as some of the relevant issues concerning coordination, labor, administrative support, technical resources, and transportation. We will also share lessons learned throughout our experience thus far and suggest protocols that could potentially be re-used and repurposed by other institutions. Finally, we will argue for the need to collectively and creatively communicate the value of multi-institutional collaboration to administrators in order to increase administrators’ receptivity to these efforts going forward.

In addition to discussing projects already carried out, we would like to share our plans for moving forward. As part of deepening and expanding our community’s ties, SD|DH is working towards launching a multi-institutional digital commons powered by Commons in a Box (CBOX) hosted at UCSD. While some SD|DH campuses could ostensibly run and host their own CBOX we have decided to experiment with creating a single commons for our community so that all participating SD|DH institutions can take advantage of the many affordances of CBOX. We are planning to use this Commons in multiple ways. First, we will use it to facilitate communications for the SD|DH group as a list serv, events calendar, member directory, and showcase of SD|DH projects. However, we will also invite all students, faculty, and staff of SD|DH institutions to use the Commons for networking across institutions, building websites related to research and teaching (for any discipline), and creating interest groups that cut across disciplines and institutions. Finally, we also hope to implement Social Paper, a collaborative writing tool developed at The CUNY Graduate Center for CBOX through the generous support of a NEH Digital Start Up Grant. We are currently discussing the possibility of designing multi-institutional synchronous courses that will engage students in thinking through the role of collaboration in their respective disciplines by collaborating with students outside of their institutions through Social Paper.

Bibliography
Alexander, B. and Frost Davis, R. (2012) "Should Liberal

Arts Campuses Do Digital Humanities? Process and

Products in the Small College World." Debates in the Digital Humanities, Ed. Matthew K. Gold. Minneapolis: University of Minnesota Press.

Spiro, L. (2014) “Creating the Texas Digital Humanities Consortium.” Digital Scholarship in the Humanities. April 23,    2014.

https://digitalscholarship.wordpress.com/2014/04/23 /creating-the-texas-digital-humanities-conso rtium/.

Theibault, J.    (2016) “Regional Digital Humanities

Consortia: An Emerging Formalization of Informal Network Ties? [Poster].” In Digital Humanities 2016: Conference Abstracts, 902-3. Krakow: Jagiellonian University    & Pedagogical University. h

ttp://dh2016.adho.org/abstracts/176.
Hermeneutica
In Hermeneutica, Geoffrey Rockwell and Stéfan Sinclair (2016) argue for an approach to the digital humanities that deemphasizes the tool and positivist notions of proof. Their proposed approach, also called Hermeneutica, champions tool accessibility over tool sophistication. Similarly, scholarly play is legitimated as a useful step in developing research questions and as a means to reconsider established notions within literary disciplines. The aim of Hermeneutica as a methodology seems to be the generation of interesting humanistic questions as much as the resolution of open questions.

Rockwell and Sinclair demonstrate the difference between Hermeneutica and typical DH approaches by quoting from Gary Wong's 2009 blog post:

[Typical DH] takes the worst part of the scientific papers (really really long sets of tabular data in the body of the text) and the worst part of papers from the humanities (really really complicated language where simple language would have done) and puts it in one. If this is what the cooperation of computational text analysis and traditional literary analysis yield, I

am scared.

Because Hermeneutica attempts to join the best parts of these fields, it has the potential to turn DH into a discipline that is more useful for the vast majority of non-DH humanists. It could be the means of accelerating the mainstreaming of DH methods and bringing us to the eventual point where all humanities are digital—a destination Claire Clivaz described succinctly (DARIAH, 2016).

Voyant
One feature that distinguishes Hermeneutica from many other DH approaches is its companion set of tools meant to demonstrate its application. Voyant Tools, now referred to simply as Voyant, is a web-based, modular suite of tools meant to be “worth thinking with” (Rockwell and Sinclair, 2016: 10, original emphasis). The goal is to accommodate playful exploration of text and sharing of corpora across the web. It is not designed as an industrial-grade text analysis tool, but as a “toy” that allows scholars to uncover new questions and gain new appreciation of texts.

Current limitations of Hermeneutica
A fundamental component of Hermeneutica is that the scholar views text through the lens of Voyant (or other computational text analysis tools), and then synthesizes that experience with their prior knowledge of the text and its milieu. A problem that Voyant addresses, but does not solve, is that many scholars who know the most about specific texts lack the technological skills that would be considered pre-novice in DH circles. Voyant allows everyone with a text and a browser to explore word frequencies, collocations, etc., but it presupposes that the text is available and clean enough for use. In order for Hermeneutica to appeal to non-DH humanities scholars, these issues of text availability and the lack of user skill must first be addressed.

On the issue of text availability, it is not often that scholars wish to analyze text that is rare or missing. More often they are interested in text that is protected by various copyright laws, which prohibit posting the text to public websites such as Voyant. Thankfully, in the Unites States at least, Google Books' recent court victory (Stohr, 2016) now permits scholars to publish online the analysis results derived from copyrighted texts, so long as the original text is not recoverable by the user. To this end Rockwell and Sinclair developed Voyant 2's “non-consumptive” mode which restricts access to tools that allow full-text views.

While such developments represent Rockwell and Sinclair's amenability to meet the ever-evolving needs of Hermeneuticans, accommodating users' lack of technology skill is beyond the scope of their involvement. For example, it is not reasonable to expect the Voyant developers to be concerned over issues of text acquisition or text preparation. Rather, those con-cerns—while critical to expanding the pool of potential Hermeneuticans—are issues of local implementation. Similarly, it makes sense that Voyant would offer the ability to link to a corpus after uploading the text, but uploading the text and keeping track of various versions of corpora is beyond the scope of Voyant. A local practice of adding some structure around the

Voyant suite ought to make Hermeneutica useful to a far greater audience than it is now.

Scaffolding
In the field of instructional design, such structure is called scaffolding. Specifically, scaffolding refers to the process of providing learners adequate introduction and examples before allowing them to attempt a task on their own (Bruner, 1978). For scaffolded Her-meneutica, DH-savvy professionals can work to acquire, clean, and upload text to Voyant (and other tools), and then provide public listings of the resulting corpora.

Examples of scaffolded Hermeneutica
We have implemented this scaffolded Hermeneu-tica approach in our Office of Digital Humanities beginning with the Cormac McCarthy Corpus Project (CMCP). The CMCP includes 13 Voyant corpora of McCarthy's 10 novels: one for the complete works, one for each novel, and two for novels (The Orchard Keeper and The Road) where the narration has been segregated from the dialogue. But the linchpin of scaffolded Hermeneutica is the CMCP's publicly-accessible website that organizes these Voyant corpora. The website is built on WordPress with the Pods content management plugin, and contains information on McCarthy's work, descriptions of Voyant (and other tools), and listings of links to the Voyant corpora. An essential feature of the website's structure is the ability to accommodate revisions to the current corpora as well as the addition of other tools in the future. Already, there is a non-Voyant sentence structure search tool attached as a beta-testing option.

A rough version of the Cormac McCarthy Corpus Project was presented at the 2015 conference of the Cormac McCarthy Society. The reaction to these tools being available for public use was strongly positive. One attendee referred to the website as “a game-changer.”

The same scaffolded Hermeneutica is being implemented on two other projects: Machado a longa distancia and The Modernist Short Fiction Project. Preliminary demonstrations of the approach have yielded similar reactions to what we observed with the CMCP. Non-DH scholars become excited rather than anxious when the digital analysis tools are scaffolded to provide them ready access. In fact, these demonstrations turn into play sessions where non-DH scholars repeatedly request for certain words to be added to the frequency charts and other Voyant panels.

Hermeneutica and Voyant represent the greatest potential for growth in DH not because they are the most technologically or theoretically advanced developments, but because they are the most accessible to non-DH scholars. Still, they don't quite reach the ground level of technology skills possessed by most researchers in the humanities. The scaffolded Herme-neutica approach proposed in this paper seems to span that gap to make Hermeneutica more accessible.

Bibliography
Bruner, J. S. (1978). “The role of dialogue in language acquisition.” In Sinclair, A., Jarvelle, R., J., and W. J.M. Levelt (eds), The Child's Concept of Language. New York: Springer-Verlag.

DARIAH (2016). My Digital Humanities - Part 1. YouTube.

https://www.youtube.com/watch?v=I8aRtHW3b6g

(accessed 1 November 2016).

Rockwell, G. and Sinclair, S. (2016). Hermeneutica. Cambridge: MIT Press.

Stohr, G. (2016). Google Book Project Can Proceed as Supreme Court Spurns Appeal. Bloomberg Politics. http://www.bloomberg.com/politics/articles/2016-04-18/google-book-project-can-proceed-as-top-u-s-court-spurns-appeal (accessed 1 November 2016).

Conclusion

        
            Introduction
            Ideophones, sometimes called “mimetics” (Akita, 2009) or “expressives” (Diffloth, 1976) are expressions that communicate sensory aspects of the physical word such as sound (i.e., onomatopoeia), movement, color, etc., or cognitive/emotional states (e.g., “ta-da” in English). Although most linguistics description of and inquiry into ideophones have focused on vocal expressions, gestures are integrated with ideophonic utterances in some languages. The analysis of these gestures and their symbolism may augment scholars’ understanding of the target language including how native speakers mentally represent their environment.
            In this paper, we describe a web-based tool, 
                Quechua Real Words, used by ideophonic linguists at [institution] to catalog and study multimedia representations of gestured ideophones as performed by native speakers of Pastaza Quichua. Research based on this tool is opening new understanding of the target language’s aesthetics, especially regarding the non-arbitrariness of gestured signs. We also discuss the relationship between this tool and other digital humanities efforts.
            
             Project Background
            The indigenous people of eastern Ecuador speak Pastaza Quichua (PQ), a dialect of Northern Quechua. Descended from the language of the Incan civilization, Quechua is still spoken by as many as 10 million people in the Andes region stretching from Ecuador in the north to Argentina in the south. In 2015 [second author], a linguistic professor at [institution] led a group of student researchers who spent one semester in Ecuador recording and appreciating the indigenous culture and language. This team videoed over a hundred hours of interviews with PQ speakers, including thousands of examples of PQ ideophonic gestures.
            The team returned to [institution] baffled at the scope of archival work that lay between their raw footage and their research goals. In consultation with their [institution]’s Office of Digital Humanities, they constructed a WordPress-based website that facilitated their archival activities, accelerated their research, and opened their work to a global audience.
             The Website
            
                Quechua Real Words uses custom content types within WordPress and simple data entry forms that allow students and professors with little computer experience to record ideophones and link entries to specific segments of recorded videos. The project’s footage is hosted on YouTube for simplicity and accessibility, and the data entry form only requests the video segment’s URL and start and stop times. 
            
            
                
            
            
                Quechua Real Words ideophone recording form.
            
            The entry form includes two other important features: First, the researcher may classify each ideophone by one or more “sensory modality” (e.g., color, haptic, movement, etc.). Second, each scholar—be they professor or student—may add their name to the list of the entry’s contributors.
            Once an ideophone is saved, it immediately appears on two indexes: the list of all ideophones, and the list of ideophones by modality. The first index allows researchers to look up specific ideophones, while the second promotes synthetic exploration where relationships between apparently unrelated ideophones can be made clear.
            Each ideophone page displays the pronunciation (in IPA format), definition and other information one would expect from a traditional dictionary entry. It also shows a text description of the ideophone’s paralinguistic qualities and one or more videos of native speakers expressing the ideophone in candid conversation. These videos are segments of longer YouTube videos, and the segments may be looped, paused, and replayed. (Such functionality is not native to YouTube’s standard embedded player, so the site’s video player is a custom JavaScript that connects to YouTube’s published API.) 
            
                
            
            An ideophone page from 
                Quechua Real Words.
            
            As insisted on by the supervising professor, each ideophone page displays a “How to Cite” section with a citation in the Linguistics Society of America’s preferred format. To recognize the collaborative nature of the website, the credited parties in the citation include everyone who contributed to the entry, even students. 
             Research Potential
            During the first two years of its existence, [first author] used 
                Quechua Real Words for research published in a special issue of the 
                Canadian Journal of Linguistics ([second author], 2017), in three presentations at international conferences ([second author], 2015a; [second author], 2015b; [second author], 2014), and in two invited book chapters ([second author], in press; [second author], in press). Additionally, the website’s content will inform an upcoming monograph ([second author], in preparation).
            
            These publications focus on contextually-rich methods of understanding PQ ideophones, comparing specific gestures and intonations between speakers and contexts, and discovering how the ideophones are integrated with—rather than distinct from—the language’s verbal aspects. As Akita and Tsujimura (2016) point out, the goal is to seek typological generalizations for ideophones rather than consider them in isolation. [Second author] seeks to extend these integrative studies and semantic generalizations beyond the vocal utterances into the gestured space.
             Quechua Real Words as a Model for DH Collaboration
            When [second author] proposed this website to the Office of Digital Humanities, [she/he] had little notion that it would lead to such a level of scholarly productivity. It was only as [she/he] saw how the site could function that [she/he] began to grasp its potential. Similarly, [first author], the digital humanists who crafted the website, overlooked its potential because, quite frankly, the technology behind 
                Quechua Real Words is rudimentary for most DH centers. 
            
            Perhaps [first author]’s estimation was clouded by the fact that DH as a field has favored text-based literary analysis over multimedia research. Despite the work of the ARTeFACT project (Coartney &amp; Wiesner, 2009) and a few others who have considered digital analysis of performing arts, DH has contributed much less to the analysis of video interactions, such as these ideophones, than it has to the analysis of written text. Garrard, Haigh, and de Jager (2011) demonstrate the status-quo for dealing with nonverbal communication in DH research: “…the recording and representation of various types of paralinguistic feature in transcription is somewhat idiosyncratic, and thus unreliable, suggesting that they should be removed in the interests of consistency.”
            This lack of emphasis on paralinguistic and nonverbal communication is in spite of those features’ apparent value. “The nonverbal channel carries important information about emotional expressions… Systems that combine multiple modalities usually outperform single-modality systems in recognizing emotional” (Truong, Westerhof, Lamers, &amp; de Jong, 2014). Unfortunately, even Truong et al. restricted their valuation of nonverbal channels to prosodic qualities such as timing and rhythm; they did not address issues of body language or gestures.
            Regardless of why [first author] overlooked the website’s potential, [she/he] has since changed how [she/he] evaluates potential collaborative DH projects. [She/He] now focuses on evaluating the use of the tools, websites, and other resources [she/he] would develop 
                relative to the target discipline rather than relative to the state of the art within DH. This new approach has already proven fruitful (first author, 2017).
            
             Future Plans
            While [second author] continues to leverage 
                Quechua Real Words for [her/his] scholarship, [first author] has combed the DH literature to discover methods of extending the site’s capacity. One DH project that could contribute guidance to this project is the work of Paquette-Bigras and Forest (2014) who attempted to build a descriptive vocabulary for dance movements. A similar effort to construct a vocabulary for describing non-vocal expressions may reveal yet-unnoticed relationships between expressive gestures. This would require intense, non-automated markup of the gestures, but the 
                Quechua Real Words website and the student-involved structure of [second author]’s courses would be facilitative. Such detailed modeling of the gestures would extend the modality-based clustering currently available on the website to include form-based clustering of the gestures.
            
            Additionally, we are working with [institution’s library] to add 
                Quechua Real Words to their federated search databases. This will increase the site’s discoverability by scholars and students throughout the world.
            
        
        
            
                
                    Bibliography
                    Akita, K. 2009. 
                        A grammar of sound-symbolic words in Japanese: Theoretical approaches to iconic and indexical properties of mimetics. PhD Dissertation. Kobe University.
                    
                    Akita, K. &amp; Tsujimura, N. 2016. “Mimetics”. In T. Kageyama and H. Kishimoto (eds), 
                        Handbook of Japanese Lexicon and Word Formation, 133–160. Berlin: Gruyter De Mouton.
                    
                    Coartney, J. S. &amp; Wiesnet, S. L. (2009). Performance as digital text: Capturing signals and secret messages in a media-rich experience. 
                        Literary and Linguistic Computing, 24(2), pp. 153–160. https://doi.org/10.1093/llc/fqp012
                    
                    Diffloth, G. 1976. “Expressives in Semai” 
                        Oceanic Linguistics Special Publications
                    
                    No. 13, 
                        Austroasiatic Studies Part I, pp. 249-264
                    
                    Garrad, P., Haigh, A., &amp; de Jager, C. (2011). Techniques for transcribers: assessing and improving consistency in transcripts of spoken language. Literary and Linguistic Computing, 26(4), pp. 389–405. https://doi.org/10.1093/llc/fqr018
                    Paquette-Bigras, E. &amp; Forest, D. (2014). A Vocabulary of the Aesthetic Experience for Modern Dance Archives. Paper presented at DH 2014, Lausanne, Switzerland.
                    Truong, K. P., Westerhof, G. J., Lamers, S. M. A., de Jong, F. (2014). Towards modeling expressed emotions in oral history interviews: Using verbal and nonverbal signals to track personal narratives. Literary and Linguistic Computing, 29(4), pp. 621–636. https://doi.org/10.1093/llc/fqu041
                    [The following references will be added following double-blind review:]
                        [first author], 2017
                        [second author], 2014
                        [second author], 2015a
                        [second author], 2015b
                        [second author], 2017
                        [second author], in press
                        [second author], in press
                        [second author], in preparation
                    
                
            
        
    

        
            
                
            
            “ISIDORE” es un buscador creado por una infraestructura francesa de investigación: la TGIR Huma-Num. No solamente ofrece una plataforma de búsqueda, sino que también normaliza y enriquece los datos y metadatos que cosecha, integrándolos en la Web semántica.
            Desde hace dos años, la plataforma “ISIDORE” lanzada en diciembre de 2010 puede enriquecer e indizar metadatos y recursos digitales en Ciencias Humanas y Sociales (CHS) en 3 idiomas: francés, inglés y español. Esta posibilidad es un gran avance para “ISIDORE” y abre perspectivas de colaboración científica en distintos continentes. Conforme a los principios de ciencia abierta y respetuosa de los principios “FAIR”, este enfoque permite el intercambio cada vez más estrecho de numerosos datos integrados en la Web de datos.
            De hecho, ahora cuenta con más de 5 millones de recursos digitales (libros, revistas científicas, artículos científicos, anuncios y programas de eventos, convocatorias, blogs, mapas, archivos, documentos audiovisuales, etc.) interconectados mediante referenciales, indizados por un motor de búsqueda. Estos datos enriquecidos son accesibles en tres formas : un portal web (http://www.rechercheisidore.fr/), una API (http://www.rechercheisidore.fr/api) y un acceso unificado (http://www.rechercheisidore.fr/sparql) en una óptica de metadatos abiertos según el formalismo RDF. De hecho “Isidore” promueve el uso de estándares interoperables. 
            Así, “ISIDORE” es capaz de cosechar corpus y bases de datos en español y en inglés, pero ofrece también enriquecimientos multilingües enlazados entre sí mediante las posibilidades ofrecidas por el linked data. Para lograrlo, “ISIDORE” utiliza las alineaciones de los conceptos entre tesauros y vocabularios disponibles en la web semántica como los Registros de Autoridad y Referencia de Materia de la Biblioteca Nacional de España (http://datos.bne.es/temas) para los datos en español, o bien los encabezamientos de materias del referencial de la Biblioteca del Congreso de EEUU (Library of Congress Subject Headings – LCSH, http://id.loc.gov/authorities/subjects.html) para los datos en inglès. De esta manera, los conceptos de estos dos referenciales mayores están alineados en parte con los conceptos del referencial francès Rameau de la BnF (Biblioteca Nacional Francesa).
            Junto con tesauros multilingües ya integrados en “ISIDORE” (como Pactols, Lexvo, GeoEthno, GEMET, etc.), y con el sistema de categorización/clasificación también multilingüe (categorías del sistema francés de archivos abiertos HAL-SHS y del sistema “Calenda” de anuncios de eventos científicos y convocatorias del CLEO-CNRS), “ISIDORE” es capaz de proponer un sistema de enriquecimientos/clasificación en 3 idiomas con la posibilidad de cambiar de idioma durante la búsqueda en la interfaz del portal www.rechercheisidore.fr y de la interfaz para tableta/smartphone (http://m.rechercheisidore.fr/?lang=es).
            Esta característica permite al investigador no-francófono de tener acceso a nuevos datos con enriquecimientos, enlaces y clasificaciones en tres idiomas, permitiéndole medir, por ejemplo, el interés de fuentes en idioma francés sugeridas por “ISIDORE” en la interfaz (bien sea en inglés o en español). 
            De momento, casi 220 000 documentos en español se encuentran en “ISIDORE” y la plataforma contempla cosechar aún más en el futuro.
            En paralelo, otros desarrollos que hacen de “ISIDORE” una herramienta cada vez más personalizada, han venido completando sus funcionalidades y abriendo perspectivas. Es el caso del widget “IMoCO”, ISIDORE Motor Constructor que permite crear sólo en unos clics, una interfaz de consulta personalizada de los recursos disponibles en la plataforma “ISIDORE” (por ejemplo recursos específicos sobre un tema). Así, “IMoCO” está diseñado para los usuarios que deseen incluir en su sitio Web el buscador “ISIDORE” haciendo una simple copia/pega de un código HTML. Simple y neutral, se adapta a la mayoría de los sitios Web. Además, IMoCO puede ser totalmente personalizado con sus estilos CSS. También el widget multiligue WordPress “ISIDORE suggestions” (https://fr.wordpress.org/plugins/isidore-suggestions/) permite al usuario de blogs WordPress conseguir sugerencias de documentos presentes en “ISIDORE”. Estas sugerencias se hacen basadas en palabras claves asociadas al artículo que el usuario esté leyendo. Es posible afinar su búsqueda, subrayando el contenido del artículo consultado o seleccionando una o varias disciplinas.
            Con este poster, quisiéramos mostrar todas las posibilidades que ofrece actualmente “ISIDORE” para el mundo hispánico en CHS, con lo que nos permite también contemplar colaboraciones fructuosas que contribuirán sin duda a alimentar esta plataforma y, al final, a enriquecer las búsquedas de investigadores o estudiantes francófonos de “ISIDORE” que tendrían acceso a más recursos en español, así como las investigaciones de usuarios hispanohablantes y angloparlantes. También tener la oportunidad de presentar este póster en el cuadro del congreso DH en México permitiría intercambiar con usuarios potenciales sobre sus necesidades, y alrededor de los futuros desarrollos de la plataforma.
        
    

        
            Humanities Commons is an open-source, open-access not-for-profit social network and scholarly communication platform founded by the Modern Language Association and supported by a collective of scholarly organizations. Scholars and practitioners across the humanities and around the globe can create a professional profile, discuss common interests with colleagues, develop new publications, and share their work with other scholars and with the world. 
            Humanities Commons grew out of the MLA’s experiences with its January 2013 launch of MLA Commons; the earlier platform was designed to serve the needs of MLA members by providing a range of types of open, networked communication. Early adopters, however, exhibited a strong desire to collaborate with scholars in fields other than those represented by the MLA. At the same time, the MLA was approached by several other ACLS member societies seeking similar networked communication solutions. Further, increasing concerns among scholars about the future disposition of commercial scholarly networks, given the sale of both Mendeley and SSRN to Elsevier and the problematic profit models being developed by ResearchGate and Academia.edu, revealed a strong desire for a sustainable not-for-profit alternative.
            Given its successful prior work in the area, the MLA was well-positioned to explore the development of a federated platform that might be jointly supported by multiple scholarly societies, bringing together proprietary membership-oriented spaces with a range of fully public functions. With the support of the Andrew W. Mellon Foundation, the MLA met with a group of societies to discuss the possibility and then designed a pilot project to test the technical assumptions behind the federated network. Working with three partner organizations — the Association for Jewish Studies; the Association for Slavic, East European, and Eurasian Studies; and the College Art Association — the MLA launched Humanities Commons in beta in December 2016. 
            The network currently comprises four primary functions: 
            
                a profile system permitting humanities practitioners to create a professional presence in a non-for-profit online space where they can easily connect with others in their fields; 
                an open-access repository that allows members to archive and share the many products of their work, and to notify other members of their availability;
                a community platform, permitting members to join groups, share ideas, and discuss common interests;
                a publishing platform, permitting individuals or groups to create articles, books, teaching materials, Web sites, and blogs, to make their research public and to seek feedback on work in progress. 
            
            The network is built on the Commons In A Box (CBOX) platform, developed by the CUNY Graduate Center; CBOX is in turn based on WordPress and BuddyPress, which bring together a flexible publishing engine with rich social networking capabilities. The network’s repository system is Fedora/Solr-based, with a WordPress front-end, developed in collaboration with the Columbia University Libraries and with the support of the National Endowment for the Humanities. Additionally, the network uses a federated authentication and identity management system, primarily based on COmanage and other Internet2-based systems, that communicates with the membership databases of participating scholarly organizations, thus allowing members to access all the organizations to which they belong through a single sign-on mechanism. 
            As of mid-April 2018, Humanities Commons has over 13,500 members who are actively developing their professional profiles. In order for the network to thrive, however, it must develop in a sustainable fashion. The planning and development for Humanities Commons were undertaken by the MLA as a service to the profession, as well as to its sister societies, with the goal of providing an open-source, scholar-governed alternative to the available commercial services. That development was partially supported through grant funding, as noted above, but grant funding is not a business model; funders expect a project such as this to develop a sustainability plan to ensure its future. Humanities Commons is thus working toward collective action by and shared services for scholarly societies and other kinds of scholarly organizations who want to work together to provide a rich scholarly communication infrastructure for their members and for the profession at large.
            This poster presentation will include an active demo of Humanities Commons as well as discussions of its platform, its community, its sustainability plan, and its development roadmap. We want to encourage members of the ADHO community to join the network and connect with one another across the conference and throughout the year. We also want to invite active participation among ADHO members and constituent organizations in the network’s development process.
        
    

        
            
                INTRODUCTION
                Integrating the digital humanities (DH) into undergraduate level higher education programs has often been a difficult and ambiguous process. Faculty sometimes struggle to create syllabi that incorporate technologies but that do not require constant redesign as technologies evolve. Institutions may lack systems to connect students with faculty and staff who are interested in collaborative research, and collaboration beyond one’s own institution can be complicated or inaccessible for students. These are real challenges; as institutions increasingly develop DH courses and degrees, the impact on undergraduate students is diverse, ranging in minimal involvement, to career-altering. So, what should the role of the undergraduate in DH be, and how can we address these challenges? For the past three years I have explored these questions. This exploration has led to helping redesign and teach the foundational seminar for Hope College’s Mellon Scholars DH Program, as well as co-founding and chairing the Undergraduate Network for Research in the Humanities (UNRH), an undergraduate-led organization with the mission of reimagining the undergraduate role in DH through the establishment of a network of digital humanists who present research, collaborate, and share ideas. On the basis of these experiences as an alumna of Hope’s DH Program and UNRH Chair, I have been considering the ways in which faculty, staff, and institutions might support undergraduate DH researchers. My work has culminated in a series of models, programs, and initiatives that address the need for fostering the next generation of digital humanists in the classroom, at the institution, and beyond.
            
            
                METHOD
                
                    2.1-CLASSROOM The first challenge I consistently identified in DH courses was an incohesive structure that treated the digital and the humanities as separate units rather than an interconnected academic space. Secondly, seminar themes grounded in particular technologies had to be redesigned frequently as these technologies evolved or became outdated. This was the case for the year-long introductory seminar for Hope’s DH Program. Each year students felt that the seminar was two unrelated courses, one focusing on a particular area in the humanities, the other, teaching technologies like GitHub and data analysis. The course was a noble attempt but ultimately inconsistent, incohesive, and not a truly interdisciplinary approach to DH. I set about designing a seminar model that was adaptable to new technologies yet still focused on an intersectional theme. I consulted with educators at conferences and researched seminar formats at other institutions, but unsurprisingly there was a wide range of approaches that seldom emphasized independent research quite like Hope’s program. Thus, I grounded the seminar model in that very aspect: a chronological approach to independent research in the humanities. Over course of four units students engage with the evolution of humanities-based research and with the research process from beginning to end. During the first unit, students work in the archives, practice cataloging primary sources with tools like Zotero, develop strong but focused research questions, and discuss literature to answer the ever-present question “What is DH?” The second unit follows the progression in humanities-based research, moving from sources like libraries and datasets into the first examples of DH: text analysis. Students curate their own text-based datasets, analyze and visualize them, present them with Omeka, and discuss research project methodologies of source compilation and argumentation. The third unit it titled: CCP-Collaboration, Communication, &amp; Presentation. It involves group research collaboration and finalizing research projects through effective communication and presentation. Students complete writing workshops in which they must adapt a piece of writing for different audiences and styles, from conference abstracts to blogs and tweets; they also practice oral and web presentation skills. The final unit addresses advanced topics and tools which require students to focus on race, gender, sexuality, politics, and socioeconomic status. Students learn that equity and accessibility are paramount when creating public scholarship, digital or otherwise, and they are exposed to a survey of technologies in efforts to broaden their concept of what form research can take. The outcome of this course should be a comprehensive and diverse approach to humanities-based research projects through the chronological progression that research in the humanities has followed.
                
                
                    2.2-INSTITUTION For collaborative research, students and faculty alike find it challenging to make necessary connections with one another in the four short years that students have on campus. My solution is Bin(d)r: the Baccalaureate Interdisciplinary Network for (Digital) Research. Stemming from an initial idea of a physical binder with pages featuring the profiles of faculty, staff, and students interested in collaborative research, Bin(d)r: is ideally implemented as a searchable database of anyone on campus with research interests and skills. It is like Tinder for academics. All faculty and staff interested in collaborating simply create a profile on a site with tools like WordPress’s “Ultimate Member” Plugin. Students are invited to create profiles if they are interested in research. By including specific research interests and skills, faculty and students can get “matched” in a timely manner. Bin(d)r: has parentheses around “digital” because this tool does not have to be exclusively for digital projects, but it would provide an extra level of support for digital projects, connecting computer science students with humanities faculty, for example. Bin(d)r: is capable of being entirely free, low maintenance, highly interdisciplinary, and ultimately a tool for encouraging undergraduate research. Furthermore, if the digital Bin(d)r: takes off at numerous institutions, searching others’ databases would foster cross-institutional collaboration.
                
                While considering the institutional level, I would also argue that institutions must make space to hear the voices of their students. I propose that institutions establish a quarterly forum for undergraduates, faculty, and administrators to gather and discuss how the institution can better support students. Academic institutions are designed first and foremost to educate their students, so I assert that students have the right to tell institutions how they can improve, and institutions have the responsibility to listen. Simply creating space for dialogue is empowering.
                
                    2.3-BEYOND I also argue that empowering undergraduate researchers means providing agency, accreditation, and opportunities to join a community. Because DH is emerging at different rates across the globe, many students never meet other students engaging in their work. Furthermore, exposure to different methodologies, technologies, and project ideas has a profound impact. Faculty and staff gain this exposure at academic conferences and within their departments. UNRH aims to give this space and community to students, too.
                
                Our method of creating UNRH relied heavily upon initial organization, forming a Steering Committee, review system, and website. The format of our conference was meticulously designed. We created a “speed-dating” session for rapid introductions and elevator pitch practice, a formal project presentation session, informal poster-style presentation sessions, a keynote address, and workshop sessions. These workshops include technology tutorials, panel discussions about different students’ roles and experiences at their institutions, and design-thinking sessions to address the needs and concerns of students striving to develop DH projects.
                Beyond the conference we have been developing an online network space in which students create profiles and can share project updates, articles, conference opportunities, and requests for peer review. In essence, each of our decisions was an effort to create space and flexibility for students to answer for themselves the question of what the undergraduate role in DH can be.
            
            
                RESULTS
                
                    3.1-CLASSROOM The feedback from my students who experienced my seminar model have been positive. The survey results indicate that the seminar has largely met the learning outcome goals, and students indicated increases in confidence and preparedness in conducting independent research (approximately 30% average increase) and using new technologies (approximately 37% average increase) according to a seven-point scale. Those who indicated having less prior experience (1-4) had an average increase of about 33% in independent research and about 39% in technology use. I plan to track program retention rates in the coming years to hopefully see improvements as the sophomore students navigate from the structured seminar into the independent research spaces of their junior and senior years.
                
                
                    3.2-INSTITUTION Bin(d)r: has not yet been implemented but is in development for implementation at Hope College in the coming year.
                
                
                    3.3-BEYOND The results of our efforts exceeded expectations. Since our first conference in 2015, we have accepted over 50 projects, involving over 80 undergraduates from 31 institutions all across the United States, Canada, Nigeria, and Pakistan. According to in-person comments and our post-conference evaluations, students have felt empowered, encouraged, and independent in their research. Moreover, students were amazed at what they learned and accomplished by interacting with undergraduates from other institutions.
                
                Through our initial design and modifications over the years, we feel confident in the model for an organization and conference that grants agency to undergraduates, and space to understand their own roles. Now in my third year as Project Manager/Chair, when I consider again the undergraduate role in DH, I think of students as connected learners and independent researchers pursuing their own interests while learning from peers and mentors alike. Within and beyond this space, each student must determine her role for herself.
                Instructors, institutions, and organizations, invest in these students, for they are the next generation of digital humanists.
            
        
    
Our lightning talk offers solutions to some shortcomings in communication about DH projects and undertakings on university campuses, particularly through the development of institutional DH websites. By an “institutional DH website,” we mean a community website, hosted by a given university or institution, that is explicitly devoted to the advancement, support, and promotion of DH work collectively. A number of previous attempts to establish institutional DH websites have failed, and there is a growing need to understand how we can sustainably create and maintain such sites in a way that meets the diverse needs of DH scholars. To this end, we offer an alternative approach for creating such communal sites that is designed for specific communities. More than merely providing a definition of DH and a set of resources for those interested in the field, institutional DH websites can beneficially act as community hubs for DH practitioners by showcasing live projects and encouraging interdisciplinary collaboration. While there is no one-size-fits-all solution, an open development process can help scholars and DH staff who face long-standing DH challenges around methodological innovation, data reproducibility, reinvention of the wheel, and the balance of technical and humanistic priorities. In particular, we offer a user-focused development process for DH websites, which emphasizes the identification and enhancement of human networks and communities of practice. At the most basic level, user-focused design starts with a needs assessment of the website’s primary audience and is refined through attention to typical user needs and exemplary uses throughout the project’s lifecycle in order to maintain an active user community. This stands in contrast to the design of institutional DH sites as a means of cataloguing the services or offerings at a specific institution. At the structural level, user-focused design for institutional DH sites foregrounds open access and accessibility by thinking about these concerns throughout the design process (rather than as a last-minute add-on). The two speakers have designed institutional DH websites at the University of Virginia and Bucknell University, each employing a different platform (Drupal and WordPress); nonetheless, both sites promote similar design philosophies. The talk will model how institutions can create similar sites designed for their own communities with an eye toward developing appropriate use cases and sustainability practices.

        
            The problem of political repression in the Soviet Union, despite the interest of a number of researchers, remains insufficiently studied. Some aspects of the implementation of repression or the life of the repressed were left outside of the research focus. For example, the problems of repression against the Volga Germans during the Second World War remain on the sidelines of research due to the following reasons. Firstly, these are not popular topics for modern Russia, and secondly, access to sources is rather difficult today. There are not so many documentary and oral sources, numerous of those people who were repressed left for Germany or stayed in endangered villages in the Urals and Siberia, so it is really difficult to gather materials for the research. At the same time, it is customary for the historical discourse of modern Russia to include the 1940s in the discourse of the Second World War winners (Golovashina, 2017). The great construction of unity among the winners leaves no place for dialogue on traumatic experiences, which includes repression on political and ethnic grounds.
             As a result, the historiography of repressions against the Volga Germans is quite fragmentary. And today, scientist researcher should give an answer not only to the questions of the academic community but rather to the questions of the repressed and their descendants, who are asked in an interview the only question «For what?».
            This project is particularly relevant in terms of preserving the memory of the repressed Germans. Most of the Germans were repressed between 1941 and 1946 when the USSR entered the war with Nazi Germany. There was virtually no real investigation or trial, in accordance with personal directive by I. Stalin (Merten, 2015). The only reason for deportation was the ethnicity of Germans.
            The purpose of this study is to identify the characteristics of the repressive practices of the USSR government regarding the ethnic group Volga Germans, as well as the analysis of everyday life of repressed people in special settlements and concentration camps in the Molotov region (currently the territory of the Perm region). Also, the study analyses memories from the perspective of the trauma phenomenon and the role/ability of victims and witnesses. The chronological framework of the study covers mainly the period of the Second World War and the 1950s, when the liberalization of the regime in the USSR occurs.
            The study is based on the approaches of historical anthropology, as well as methods of digital humanities and oral history. In addition, important aspects of the study are the consideration of these events, as well as the memories of the repressed adults and children in terms of the theory of historical memory and the phenomenon of historical trauma and secondary historical trauma. 
            The study uses the methods of descriptive statistics, information modelling, and content analysis. In addition, an electronic map is being developed, where German deportation routes, special settlements and concentration camps on the territory of Nyroblag will be visualized (the system of camps in the Molotov region, which contained political repressions of Russian, Ukrainian, Jewish, German origin, as well as representatives of other minorities). All objects of the electronic map were equipped with a historical information, which is compiled on the basis of archival documents and interviews.
            In the process of the research, there was one ethnographic expedition to the north, to the old villages, where we managed to collect unique materials, interview the repressed, as well as the children and grandchildren of the repressed Germans.
            The method of realization of the study comes, first of all, from the idea of synthesizing sources of various origins (such as official documents, diaries, correspondence, interviews, etc.). Initially, a dictionary with lemmatized word forms is created in several subject groups, all other words are entered into the stop list. Then analyze the statistics on the words count (by interviewing groups: victims and witnesses), this option can help to understand the overall context of the problem. Also, it could be helpful for the understanding the difference between victim’s and witness’s vocabulary that we use for describing of the repression’s experience. The information obtained is divided into four main blocks: events, place names, daily life, and political situation. For conducting content analysis, the software MAXQDA-12 is used. 
            The electronic map contains more than 150 objects, which are accompanied by historical information. In the future, the map will be further replenished. However, the main special settlements of the repressed and blocks of concentration camps have already been marked. Special settlements that have disappeared from the surface of the Earth are also partially introduced, they were found thanks to the help of local residents and children of the repressed. Also, the main routes of the repressed movement from the Volga region to the Molotov region are created. For maps creation we used the web-service Carto.com. The final point of the project will be the publication of maps, photo and video content in an open portal created on the basis of WordPress, which will partially solve the problem of commemorative practices in modern Russia about traumatic scenes in the history of the 20th century (Povroznik, 2015). Thus, an important step towards public rehabilitation of the repressed the Germans will be done. As mentioned above, self-awareness is still preserved as repressed, not only among those who went through it, but also their children and grandchildren (Assman, 2016).
        
        
            
                
                    Bibliography
                    
                        Golovashina O.V. The victory of the image: identification risks of commemorative practices/ Головашина О.В. Победа образа: идентификационные риски коммеморативных практик// Studia Humanitatis. 2017. № 1. URL: http://st-hum.ru/sites/st-hum.ru/files/pdf/golovashina.pdf (date of application: 06.03.2019).
                    
                    
                        Ulrich Merten. (2015)Voices from the Gulag: The Oppression of the German Minority in the Soviet Union, American Historical Society of Germans from Russia, Lincoln, Nebraska.
                    
                    
                        Povroznik N. (2015) Ideology of the Web 2.0 on service of the Historic Cultural Heritage Preservation // The Strategies of Modern Science Development: Proceedings of the IX International scientific–practical conference. North Charleston, USA, 16-17 October 2015. North Charleston: CreateSpace. Pp. 17-19.
                    
                    
                        Assman A. (2016) Shadows of Trauma: Memory and the Politics of Postwar Identity. Fordham University Press, New York.
                    
                
            
        
    

        
            Despite the thousands of digital projects launched during the past 20 years, experts warn of a new “digital dark age” (Cerf, 2015; Davis, 2016) as our ability to produce digital information continues to outpace our capacity to preserve and access that knowledge for the long term, even (or especially) when using content management systems (Montoya, 2016).
            
                Project Endings is a collaboration between the Humanities Faculty and the University Library which aims to provide practical solutions to issues attendant on ending a project and archiving the digital products of research, including not only data but also interactive applications and web-based publications. 
                Project Endings endeavours to align the aims of faculty researchers producing projects and the archivists who will eventually be responsible for curating their work.
            
            The project divides digital projects into five primary components: data, products, processing, documentation, and release management. We aim at longevity primarily for data and products, but believe that this goal requires careful attention to processing, documentation and release management. We are developing preservation principles for all of these factors, using practice-based methods (Holmes, 2017; Arneil and Holmes, 2017; Holmes and Takeda, 2018), diagnostic tools (Holmes and Takeda, 2017), and scholarly research as listed in the project bibliography at 
                .
            
            The project conducted a survey on the 
                LimeSurvey platform consisting of 30 questions 
                 to discover how project leaders dealt with the issues of long-term sustainability for each of the five primary components. We promoted the survey to Canadian and international professional communities and received 128 responses. 25 detailed interviews were run with a sample of the respondents to get more information on the issues raised by the survey results.
            
            Results of the survey show that concerns about longevity for digital humanities projects are not exaggerated. 57% of survey respondents did not consider an endpoint for their project, despite the fact that project management principles include declarations of goals, timelines, and milestones (Zanduis and Stellingwerf, 2013). In the light of this, perhaps it is not surprising that 54% did not have long-term preservation plans. These findings suggest that many researchers do not distinguish between products generated to exploit the features of the processing environment and products generated to survive after active work on the project ends or independent of development work in the project. Furthermore, only 32% considered “benchmarks for assessing progress” and 41% included precise timelines in their plans.
            In a group of projects that were for the most part (74%) less than 10 years old and 58% still in progress, 22% reported that project outputs stopped working due to software obsolescence. This is in a field of projects in which 74% started with born-digital data. If a failure occurs during the active life of the project it might be repairable, but repair is much less likely if the project has ended.
            The value of using a standardized data model is not universally recognized, with 14% of survey respondents not using one at all and 26% making up their own. Although a home-made data model is by definition not standardized, it may still be viable for a long time if well documented. 60% claimed to have a clearly documented data model, but 90% of those that had documentation considered it to be partial or inadequate, so it appears that a project’s data model is well documented in only about 50% of cases.
            HTML is the most popular standard output for DH projects (68% of respondents used it), despite the continued popularity of PDF (45%), XML (38%), and various binary media formats (&gt;65%). Javascript is considered by many (30%) to be a major technology in their project. HTML and Javascript are robust long-term (Holmes, 2017), but if they are produced in a project only on-the-fly by a content management system (CMS) or database, then the longevity of the output is dependent on that of the CMS or database. 34% of the respondents used WordPress or Drupal, 31% used PHP/SQL databases, 38% used XML/XSLT/XQuery systems, and 41% used “other” software services and libraries. Some projects used more than one of these.
            Lack of ongoing funding was cited by 38% of respondents as the main obstacle to long term preservation. Perhaps more surprisingly, 33% of respondents rated either lack of expertise or bad technology choices as their main obstacle, which may explain the results reported above regarding software obsolescence. Early results from the interviews suggest that CMS and other software libraries and services are the likeliest sources of software failure over time. We hope that further analysis of the interviews will tell us whether a more expert assessment of software and output choices would have mitigated the issue of lack of ongoing funding.
            While a reassuringly high 42% of respondents reported that university services were responsible for long-term maintenance of the project’s work, an alarming 45% reported that this responsibility fell to the Principal Investigator or nobody, demonstrating either significant vulnerability or great confidence.
            Our survey results suggest that there is a limited use of project management (“What is PRINCE2?”, 2018; Sedlmayer et al., 2015) and software lifespan principles in DH projects. Results further suggest that there is a need for an improved understanding by researchers of specific attributes of a project which are likely to facilitate long-term viability of the project data, outputs and documentation at minimal cost for those charged with preservation. Blurring the lines between data, processing, outputs and the management of those components over time can result in vulnerabilities for long term preservability which may not be apparent until it is too late.
            With all of this in mind 
                Project Endings is working on a suite of recommendations that will provide guidance on project structure and management with long term viability as the goal. We are offering an online interactive questionnaire that assesses the long-term viability of each component in a project and provides recommendations for improving the prospects for long-term survival. Behind each question is the empirical evidence provided by survey/interview participants as well as the combined experience of the 
                Project Endings team. The questionnaire is intended primarily to be a thought-provoking activity for project leaders and principal investigators. An early draft of the questionnaire is available at 
                .
            
        
        
            
                
                    Bibliography
                    
                        Arneil, S. and Holmes, M. (2017). Archiving form and function: preserving a 2003 digital project. Brighton, U.K.
                    
                    
                        Cerf, V. (2015). Google’s Vint Cerf warns of ‘digital Dark Age’ 
                        .
                    
                    
                        Davis, R. C. (2016). Die Hard: The Impossible, Absolutely Essential Task of Saving the Web for Scholars. Skidmore College, Saratoga Springs, U.S.A. 
                        .
                    
                    
                        Holmes, M. (2017). Selecting Technologies for Long-Term Survival. Victoria, Canada 
                        .
                    
                    
                        Holmes, M. and Takeda, J. (2017). Beyond Validation: Using Programmed Diagnostics to Learn About, Monitor, and Successfully Complete Your DH Project. Montreal, Canada 
                        .
                    
                    
                        Holmes, M. and Takeda, J. (2018). Why do I need four search engines?. Tokyo, Japan 
                        .
                    
                    
                        Montoya, R. D. (2016). Advocating for Sustainability: Scaling-Down Library Digital Infrastructure. 
                        Journal of Library Administration, 
                        2016(56:5): 603–20 doi:10.1080/01930826.2016.1186969.
                    
                    
                        Sedlmayer, M., Coesmans, P., Fuster, M., Schreiner, J. G., Gonçalves, M., Huynink, S., Jaques, T., et al. (eds). (2015). 
                        Individual Competence Baseline for Project, Programme &amp; Portfolio Management. International Project Management Association 
                        .
                    
                    
                        Zanduis, A. and Stellingwerf, R. (2013). 
                        ISO21500: Guidance on Project Management – a Pocket Guide. Van Haring Publishing 
                        .
                    
                    What is PRINCE2? 
                        Projects in Controlled Environments
                        .
                    
                
            
        
    

        
            Europe has a long and rich tradition as a centre of research and teaching in the arts and humanities. However, the huge digital transformation that affects the arts and humanities research landscape all over the world requires that we set up sustainable research infrastructures, new and refined techniques, state-of-the-art methods and an expanded skills base. Responding to these challenges, the Digital Research Infrastructure for Arts and Humanities (DARIAH) was launched as a pan-European network and research infrastructure. After expansion and consolidation, which involved DARIAH’s inclusion in the ESFRI roadmap, DARIAH became a European Research Infrastructure Consortium (ERIC) in 2014.
            The Horizon 2020 funded project DESIR (DARIAH ERIC Sustainability Refined) sets out to 
                strengthen the sustainability of DARIAH and help establish it as a reliable long-term partner within our communities. Sustaining existing digital expertise, tools, resources in Europe in the context of DESIR involves a goal-oriented set of measures in order to first, maintain, expand and develop DARIAH in its capacities as an organisation and technical research infrastructure; secondly, to engage its members further, as well as measure and increase their trust in DARIAH; thirdly, to expand the network in order to integrate new regions and communities.
            
            The DESIR consortium is composed of core DARIAH members, representatives from potential new DARIAH members and external technical experts.
            The sustainability of a research infrastructure is the 
                capacity to remain operative, effective and competitive over its expected lifetime. In DESIR, this definition is translated into an evolving 6-dimensional process, divided into the following challenges:
            
            
                Dissemination
                Growth
                Technology
                Robustness
                Trust
                Education
            
            With our poster, we would like to show how the project helps sustaining DARIAH.
            Within DESIR, dissemination is the ability to communicate DARIAH’s strategy and benefits effectively within the DARIAH community and in new areas, spreading out to new communities. Through the 
                international workshops
                
                    
                        https://dbe.hypotheses.org/
                    
                 held at Stanford University and at the Library of Congress, DARIAH has been introduced to many non-European DH scholars. These events were an important first step to foster international cooperation between US and European colleagues as well as a catalyst for ongoing collaborations in the future. A third workshop took place in Canberra at the Australian Research Data Commons in March 2019.
            
            DARIAH has currently 17 members from all over Europe. Nevertheless, efforts should be made to include as many countries as possible to bring in and scale, to a European level, even more state-of-the-art DH activities.
            Six candidates ready for building strong national consortia have been identified, enabling a 
                substantial expansion of DARIAH’s country coverage. Additionally, thematic workshops are organised in each country as well as tailored training measures.
            
            DESIR widens the research infrastructure in core areas which are vital for DARIAH’s sustainability but are not yet covered by the existing set-up. As DARIAH expands across Europe, continuously enhancing and further developing the ERIC exceeds DARIAH’s internal technological capacities. Two notable results were achieved so far: firstly, the publication of a 
                technical reference
                
                    
                        https://github.com/eurise-network/technical-reference
                    
                 as a result of a workshop organised in October 2017 with CESSDA and CLARIN. It’s a collection of basic guidelines and references for development and maintenance of infrastructure services within DARIAH and beyond, addressing an ongoing issue for research infrastructures, namely software sustainability. Secondly, the organisation of a Code Sprint
                
                    
                        https://desircodesprint.sciencesconf.org/
                    
                , focusing on 
                bibliographical and citation metadata, which helped shaping DARIAH’s profile in four technology areas (visualisation, text analytic services, entity-based search and scholarly content management). Another Code sprint is expected to take place in Summer 2019.
            
            Another output is the implementation of a 
                centralized helpdesk. This helpdesk is hosted by CLARIN-D and the solution of integration within the existing DARIAH website was the creation of a WordPress plugin. This plugin is used to connect our website with the OTRS
                
                    
                        https://community.otrs.com/
                    
                 server and allows the creation of issues easily by users unfamiliar with OTRS.
            
            Sustaining a research infrastructure involves also two important aspects: trust and education. For DARIAH, it is 
                crucial to increase trust and confidence from its users. In DESIR we develop recommendations and strategies accordingly, targeting new cross-disciplinary communities, based on the results of a survey and interviews addressed to the scientific community, with different levels of approach - national, institutional and individual.
            
            In addition, education is a key area and the project contributes to the ongoing discussions about the role and modalities of training and education in the development, consolidation and sustainability of digital research infrastructures. We believe that investing time and efforts into training and educating users is a way of securing the 
                social sustainability of a research infrastructure.
            
        
    

        
            A paramount challenge in present-day humanities knowledge production is to communicate research results in ways that aligns with increasingly digital research workflows. The OpenMethods metablog aims to explore and deliver a solution for this need in a Digital Humanities (henceforth DH) context. It provides a platform to bring together all formats of openly available digital publications. To this end the OpenMethods metablog provides a convenient and easy way for DH experts from around the globe to select, propose, curate, and highlight online published content. Suitable online content may be proposed by Community Volunteers. The OpenMethods platform is intentionally interdisciplinary and multilingual to facilitate a timely disclosure and spread of knowledge and to raise peer recognition for the related research results. The group of DH experts, known as the OpenMethods Editorial Team, currently comprises 23 editors from 11 countries. 
            The platform has been developed in partnership with the DARIAH community since it is an offspring of the DARIAH “Humanities at Scale” project (Engelhardt et al. 2017). 
            OpenMethods offers a collaborative model of open scholarly communication that is growing out of community practices and needs in DH (e.g. Spiro 2012). OpenMethods goes beyond traditional paper-based communication practices in several ways, as explained below.
            1. OpenMethods is dedicated to the critical discussion of DH tools and methods. Digital tools and methods are genuine research outputs whose academic recognition is still lagging behind monographs and journal articles that serve as the highest value currency in current academia (Schreibman, Mandell, and Olsen (2011)). In addition to spreading the knowledge and raising peer recognition for existing digital resources, a core mission of the project is also to facilitate the culture of reuse of these materials. 
            2. OpenMethods is inclusive with a variety of content types like blog posts, videos, preprints, podcasts, etc. These are becoming increasingly important aspects of scholarly work (Dacos and Mounier (2010)) as they are not only accelerating discussions within and outside of academia but are also flexible enough to follow the dynamic and multimodal nature of DH methodology discourse.
            3. One of the key aim of the platform is helping researchers in navigating through the rich and dynamically evolving DH landscape to find the most relevant tools and methods for their research. This is achieved via a novel form of expert community review. As an enrichment of the preselected valuable Open Access publications, successful nominations are categorised with tags based on TaDiRAH and a brief introduction in English is added to each post in which one of our Editors explains the relevance of the republished content to the DH practices. The multilingual character of the platform allows for the representation of multiple languages and cultures in the DH discourse yet currently this possibility is not exploited to its full extent.
            4. The platform not only propagates the culture of reuse but has been built on reused tools itself. It is based on a WordPress CMS and the PressForward plugin. It enables us to create a simple workflow for our experts: almost all steps of their work (content nomination, discussion, review, publishing, keeping track of published content) can be undertaken within this plugin. Besides, we are constantly seeking ways to put DH tools in service of more effective content discovery and enrichment. For this purpose, we have created plugins to achieve interoperability with the entity recognition NERD service and the research discovery platform ISIDORE.
            Our goal is to reach and engage the widest possible range of DH communities, from scholars taking the first steps towards going digital to DH experts who are shaping specific research areas as representatives for particular methods.
            In achieving its goals OpenMethods faces many difficulties. In our poster we wish to highlight a number of these challenges:
            
                Reaching a critical number of readership as a result of focused outreach strategy (like via our Twitter channel, presenting the platform on DH conferences).
                Finding solutions for long-term incentivisation of the editors, ensuring that editors are recognised for their contributions, and sustaining a viable pool of reviewers.
                Establishing bidirectional exchange between traditional journal publishing and novel components in scholarly communication such as blogging.
            
            Our poster presentation will allow us to solicit the widest possible feedback from DH communities. To this end it will not only explain the aims and strategies of OpenMethods, but will also include an interactive demo. We also wish to encourage the conference attendees to join and expand the OpenMethods network, explore its potentials for advancing their own research methods and participate in the development of the platform.
        
        
            
                
                    Bibliography
                    
                        Dacos, M., and Mounier, P. (2010). 
                        Les carnets de recherche en ligne, espace d’une conversation scientifique décentrée. Albin Michel.
                    
                    
                        Schreibman, S., Mandell,L., and Olsen, S. (2011). Introduction. 
                        Profession
                        (79): pp.123–35. DOI: 10.1632/prof.2011.2011.1.123.
                    
                    
                        Spiro, L. (2012). „This Is Why We Fight“: Defining the Values of the Digital Humanities. In Gold, Matthew K.(eds), 
                        Debates in the Digital Humanities. Minneapolis 2012, pp. 16-35.
                    
                
            
        
    

        
            
                Brief Summary 
                In this long paper the main principles, insights, and results from the development of the PARTHENOS Training Suite will be presented to the international Digital Humanities community in order to stimulate the discussion of Digital Humanities pedagogy and to stipulate the further uptake of these resources.
                PARTHENOS develops educational resources that focus on fundamental, interdisciplinary knowledge and skills that facilitate successful engagement and use of digital research infrastructures for digital humanities and cultural heritage scholarship in an increasingly complex, networked, and open environment. These resources are brought to the target groups and audiences in appropriate ways based on didactic and practical insights, using up-to-date means of communicating knowledge and information.
            
            
                Background
                Digital Research Infrastructures play an increasing role in the Humanities and in Cultural Heritage Studies (ESF 2011; Benardou, Champion, Dallas and Hughes (ed.) 2017). While some definitions of research infrastructures focus resources and tools provided, that is their “hard”, rather technical aspects, more and more their “soft” aspects, that is their function as networks of knowledge and people come into focus, with both aspects being in the service of aggregating resources to make us better connected and more informed (Edmond and Garnett 2017). Digital research infrastructures are relatively recent additions to the humanities and cultural heritage landscape; therefore, their transformative presence needs to be embedded in university curricula and beyond because the ongoing digital transformation of research affects not only (future) researchers, but also cultural heritage practitioners in the sense of lifelong learning. Those who use digital research infrastructures for their research and those who contribute to their development and their extension alike, need to acquire additional theoretical knowledge and practical skills to fully harvest the fruits of their resource-intensive development and sustenance and to make use of their potentials to advance research. This need for a broad implementation of data skills into curriculum frameworks and training for all disciplines was only recently underlined by the European Commission Expert Group on FAIR data (EU 2018).
                Before the start of PARTHENOS, a cluster project that consists of important European eHumanities and eHeritage Research Infrastructures and institutions and is funded by the European Commission, eHumanities and eHeritage infrastructures already offered training events and contributed to the development, extension, and sustenance of platforms for online training materials or course overviews, e.g. #dariahteach or the DH Course Registry. However, many of these training materials and events are highly project specific or thematically or methodologically specialised and require prior knowledge that most potential new users often lack (Edmond et al. 2016). This knowledge gap not only prevents them from successfully engaging with the material, it often forms a barrier that prevents them from taking interest into infrastructural themes in the first place. PARTHENOS aimed to fill these gaps by developing educational resources that focus on fundamental, interdisciplinary knowledge and skills that facilitate successful engagement and use of digital research infrastructures for digital humanities and cultural heritage scholarship in an increasingly complex, networked, and open environment. These resources are brought to the target groups and audiences in appropriate ways based on didactic and practical insights, using up-to-date means of communicating knowledge and information (Spiecker et al. 2017, Edmond and Garnett 2017, Wuttke et al. 2019).
            
            
                Developing PARTHENOS Training 
                The development of the cross-disciplinary PARTHENOS training materials is spearheaded by the PARTHENOS Training Team (lead by Trinity College Dublin, Dr Jennifer Edmond) with additional input from the User Requirements Team (lead by CLARIN, Steven Krauwer). Conforming to the results of the analysis of the user requirements carried out in the first phase of the project, PARTHENOS Training focuses on online-materials for self-study and for reuse by educators for teaching purposes in different contexts (synchronous and asynchronous) using the PARTHENOS Training Suite as its main carrier. Additionally, the training-portfolio encompasses (virtual) events to test the educational materials against users, such as the PARTHENOS eHumanities and eHeritage Webinar Series.
                PARTHENOS Training develops on the basis of the wealth of knowledge inherent in the project through its project partners introductory, cross-disciplinary learning resources that address infrastructural metatopics and are suited to foster the skills and knowledge that empower and unleash the potentials of eHumanities and eHeritage research(ers). The focus is on knowledge transfer about the roles, functions, and potentials of eHumanities and eHeritage Research Infrastructures for researchers, practitioners, developers, data and computing centre staff, policy makers, and managers. During the first phase of PARTHENOS Training the focus was on more generic levels of information. The first modules were centred around the creation of general knowledge, skills, and abilities that foster the understanding what eHumanities and eHeritage Research Infrastructures are, how they can be of benefit for different communities, and what kind of knowledge and skills are needed to successfully work with them. After an intensive assessment exercise, the focus was shifted in the second phase to more specialized areas, especially highlighting how outputs and products of Research Infrastructures and particularly of PARTHENOS can help users to navigate successfully through the increasingly complex Digital Humanities and Cultural Heritage research landscape. The modules created in the second phase address topics such as research data management, creating and assessing research impact, the use of community standards and ontologies, and how to develop research questions using Digital Humanities methods and tools from a broader Humanities and Cultural Heritage perspective. Additionally, all the materials of the PARTHENOS eHumanities and eHeritage Webinar Series (Drenth and Wuttke 2018) can be accessed via the PARTHENOS Training Suite for reuse.
            
            
                Guiding Principles and Modes of Delivery
                The educational resources produced by PARTHENOS address two levels of user needs: the ‘need to know about’ (awareness-raising) and the need to know how’ (skills building) and are presented online mainly via the PARTHENOS Training Suite Website, a WordPress-based eLearning platform. In contrast to print-based learning and teaching resources, the modules of the PARTHENOS Training Suite make use of up-to-date means of communicating knowledge and information, and technologies, allowing multimedia content and multimodal forms of acquisition and delivery of knowledge and skills by learners and teachers. Within the modules the range of materials consists of video-lectures, interviews, short (animated) explanatory clips, presentation slides, exercises and explanations of basic principles, case studies, collections of further links and reading suggestions, and brochures. The Training Suite consists of several modules that allow flexible access. The users are guided through the modules in a linear fashion for better orientation, but can also access specific points of interest via the side navigation menu. Through this they have full control over the learning process and can chose which materials they want to explore in more detail as well as different modes of mediation.
                PARTHENOS provides its training materials as Open Educational Resources (OER). Thus, it takes up one of the foundational principles of the culture of the Digital Humanities that is based on sharing and reuse. It also adheres to the principles of co-creation during the development process, another principle of the Digital Humanities (“collaboration as creation”, Burdick et al. 2016, p. 84). There is an ongoing process of exchange with the PARTHENOS partners and beyond about the further development of the training materials. This intensive exchange aims to ensure the uptake of new developments in the ever-changing complex world of digital infrastructures and eResearch methods and tools in the Humanities and Cultural Heritage field. It ensures the direct integration of research communities into the development process (bottom-up approach) and enhances the innovation potential and quality. 
                PARTHENOS partners not only to develop materials, but also to facilitate their uptake, either by inclusion into other educational contexts (training modules devised by others, incorporation into in situ and virtual training events, and lately a try out at an HEI), but also by using our partners’ networks to spread the news about available new resources. For PARTHENOS training is an ideal means of communicating project outcomes and to reach its disciplinary and geographically diverse communities. It is thus closely connected to the project’s outreach and dissemination activities. For example, to extend the reach of the PARTHENOS Training materials, a short movie featuring two aliens who talk about standards (promoting the Standardization Survival Kit developed by PARTHENOS), taking a rather light stance to a seemingly difficult topic, and reflecting the linguistic diversity of the potential users by voices in different languages, has been developed. In this context, PARTHENOS has also launched in 2018 the highly successful “PARTHENOS eHumanities and eHeritage Webinar Series” (Drenth and Wuttke 2018). In this series of online seminars, international experts from PARTHENOS and beyond took the learners on a journey along the research life cycle, highlighting how using and contributing to eHumanities and eHeritage research infrastructures empowers research(ers) (Wuttke 2019).  
            
            
                Outline and Proposal for a Long Paper
                In this long paper the main principles, insights, and results from the development of the PARTHENOS Training Suite will be presented to the international Digital Humanities community in order to stimulate the discussion of Digital Humanities pedagogy and to stipulate the further uptake of these resources.     
            
            
                Acknowledgements:
                PARTHENOS Training is a cross PARTHENOS effort and includes input from external experts. We especially express our gratitude to past and present members of PARTHENOS Training that are not authors of this abstract: Elizabeth Burr, Stefanie Läpke, Rebecca Sierig (all University of Leipzig), Helen Goulis (Academy of Athens), Jenny Oltersdorf (University of Applied Sciences Potsdam).
            
        
        
            
                
                    Bibliography
                    
                        Benardou, A., Champion, E., Dallas, C., Hughes, L. (eds.) (2017). 
                        Cultural Heritage Infrastructures in Digital Humanities. Digital Research in the Arts and Humanities. London, New York: Routledge.
                    
                    
                        Burdick, A., Drucker, J., Lunenfeld, P., Presner, T., Schnapp, J. (2016). 
                        Digital Humanities. Cambridge, MA: MIT Press.
                    
                    
                        Drenth, P., Wuttke, U. (2018). Successful PARTHENOS e-Humanities and eHeritage Series concluded. PARTHENOS News Item, 28.05.2018. Online: 
                        http://www.parthenos-project.eu/successful-parthenos-ehumanities-and-eheritage-webinar-series-concluded.
                    
                    
                        Edmond, J., Garnett, V. (2017). Soft Skills in hard Places: The changing place of DH training in European Research Infrastructures. Pre-print as presented at the DH Benelux Conference, 2017. Online: 
                        http://www.tara.tcd.ie/handle/2262/85444.
                    
                    
                        Edmond, J., Garnett, V., Burr, E., Läpke, S., Oltersdorf, J., Goulis, H. (2016). 
                        Initial Training Plan (PARTHENOS Deliverable 7.1), May 2016. DOI: 
                        https://doi.org/10.5281/zenodo.2551469. 
                    
                    
                        ESF (2011). 
                        Research Infrastructures in the Digital Humanities. Science Policy Briefing, 42.
                    
                    
                        European Commission Expert Group on FAIR Data (2018). 
                        Turning FAIR into reality: Final Report and Action Plan from the European Commission Expert Group on FAIR Data. Online: 
                        
                            https://ec.europa.eu/info/publications/turning-fair-reality_en
                        . 
                    
                    
                        Spiecker, C., Oltersdorf, J., Wuttke, U., Edmond, J., Garnett, V., Läpke, S. (2017). 
                        Report on Training and Education Activities and Updated Planning (PARTHENOS Deliverable 7.2), April 2018, DOI: 
                        https://doi.org/10.5281/zenodo.2551475. 
                    
                    
                        Wuttke, U. (2019). The “PARTHENOS eHumanities and eHeritage Webinar Series”: Webinars as a means to deliver successful research infrastructure training, in: 
                        Liber Quarterly 29(1): 1-35. DOI: 
                        http://doi.org/10.18352/lq.10257. 
                    
                    
                        Wuttke, U., Rothfritz, L., Edmond, J., Garnett, V., Uiterwaal, F., Annisius, M. (2019). 
                        Final Report on Training and Education Activities (PARTHENOS D7.3), January 2019. DOI: 
                        https://doi.org/10.5281/zenodo.2575417.
                    
                
            
        
    

        
            This workshop will go over how to complete an initial Linux server setup for use with the web. We will go over security, firewalls, HTTPS, and high availability. Administering one’s own server rather than relying on managed web hosting empowers researchers, teachers, and students by providing them with complete control over their web assets. The resulting setup can be used for webapps, static sites like Jekyll and Hugo, or more robust sites like WordPress, Omeka, Scalar, and Drupal. These will be ready for use with domain names. In addition to providing an entry point to the web, servers can also enable teams of researchers and students to collaborate on programming projects or access shared data. 
        
    



        
            
                This presentation will be given by the editors of a special issue of 
                Reviews in DH
                 focused on Digital Humanities Pedagogy in the global context. Inspired by recent efforts to validate DH pedagogy through formal publication, such as the edited collection
                 Digital Pedagogy in the Humanities
                , and journals such as 
                Hybrid Pedagogy
                 and 
                The Journal of Interactive Technology and Pedagogy, 
                we aim to bring awareness to the rich content created by students in higher education. In this presentation we will highlight the projects reviewed in the special issue, and we intend to include the voices of both the creators and the reviewers in order to make transparent the labor involved in building this publication. We hope to acknowledge the shape and infrastructure of pedagogy in the context of Global South(s) that often fall outside the normative forms reflected in these US-centric journals. 
            
             
            
                In this issue we focus on projects that demonstrate the power of community-engaged, research-based digital humanities across contexts. The intention is to showcase a range, both in terms of tools and scale, as well as a diversity in the production process from a variety of institutions across the globe. Some of the projects in this issue deploy open source and open access digital tools for project building such as Omeka and WordPress. These are accessible for most undergraduate students entering the field while also being accessible in the Global South where proprietary tools are cost prohibitive even when they are available within regions outside of the developed world. Participants adding content and updating infrastructure to sustain these digital spaces carefully over time raises questions about project sustainability. Given the realities of the Covid 19 pandemic, climate vulnerability, and the onslaught of disasters that threaten the Global South from the Caribbean to South Asia, we hope that these projects inspire creators and future DH teachers and practitioners to think more deliberately about preserving the digital record. 
            
             
            
                Being acutely aware that DH and its pedagogy(ies) are shaped by the infrastructures, limitations, and affordances of our local contexts, we highlight projects that exemplify the chaotic yet productive potential of digitality, while also being alive to its various discontents. We believe that this special issue of 
                Reviews in DH
                 exemplifies how DH pedagogy must at its core focus on two key facets: empathy and engagement. With a keen eye toward the relational and not only geographical definitions of Global DH, this issue eschews authoritatively defining “DH Pedagogy.” Instead, the projects are illustrative of the complex genealogies and overlaps between cultural connotations of digital pedagogy, often made invisible in normative DH conversations.
            
             
            The richness of the student projects in this issue has pushed the presenters to call for illuminating the often-invisible labor of digital humanities project development, even within resource-rich institutions. Many of the critiques raised in the reviews found in this journal issue, could be potentially addressed through a page on process in these projects, or a blog of reflection where project creators can lay bare the experimental and transformative nature of DH work in these specific circumstances. Perhaps this self-reflexive model of project development may become best practice through course sites where both instructors and students think through work together, or even with the inclusion of course syllabi as a menu option, elucidating the intellectual framework within which iterative student learnings emerge.  
             
            In this presentation we will walk the audience through the process of working on this special issue across international time zones. We will discuss the main points of synchronicity and tension that arose, as well as the challenges and rewards of identifying projects and reviewers that would accurately represent the wide range of possibilities to inspire pedagogues working around the globe. 
        
    



        
            The Advanced Research Consortium (ar-c.org) is a hub of humanities research nodes focused on aggregating and peer reviewing digital archives, collections, and research resources.  Each node hosts an online “finding aid” which serves as a centralized research space for exploring traditional scholarship such as journal articles, digital collections (Early English Books Online and Eighteenth-Century Collections Online, for example), and scholarly digital resources that ARC peer-reviews, using the same process as journals and university publishers in the humanities. These nodes aggregate millions of digital artifacts from across the digital humanities spectrum, from proprietary projects such as JSTOR and Adam Matthew Digital to independent digital humanities projects like the London Stage Database and the Lili Elbe Digital Archive, as well as collections from major libraries throughout North America and Europe.
            ARC has a long history. Its roots lie in Jerome McGann’s founding of NINES (Networked Infrastructure for Nineteenth-Century Electronic Scholarship) in 2003. The initial Steering Committee was led by Jerome McGann and Bethany Nowviskie and included Morris Eaves, Neil Fraistat, Steven Jones, Laura Mandell, Kenneth Price, and Martha Nell Smith, all of whom had created digital archives for nineteenth-century studies. Their resources would be peer-reviewed and made findable, at the level of objects in the archive, through NINES.org. The goal was to de-silo these projects, to render them all searchable alongside proprietary resources. NINES fit its nodes and peer review process into traditional humanities field structures to render the peer-review process legible to traditional colleagues: renowned experts served on the editorial boards, rendering them as illustrious as any board for a university press. Through this merging of traditional scholarly apparatus with ever-evolving digital work, NINES, and later ARC, translated digital humanities work into familiar structures for tenure and promotion, and made them visible for teaching and research. 
            ARC is a response to the need for aggregating and providing peer review to communities of scholars beyond those specializing in nineteenth-century studies; it supported the development of medieval, eighteenth-century, and modernist nodes. Later in ARC’s history, thanks to the efforts of Michigan State University, ARC began creating nodes for libraries with special collections that wanted to make their holdings searchable alongside relevant archives and journal publications. MSU’s Studies in Radicalism (SiRO) led the way in rethinking ARC’s traditional, period-centric structure.
            While conservative on the front end, ARC’s original backend was technologically advanced, thanks to the prescience of (now) Dean Nowviskie:
                
                    
                        See also Jerome McGann, Bethany Nowviskie, “NINES: a federated model for integrating digital scholarship”,
                         Electronic Book Review
                        , 31 January 2012, an earlier form of which is available here: https://nines.org/about/wp-content/uploads/2011/12/9swhitepaper.pdf.
                    
                 the metadata was rdf xml in format which was loaded into SoLR with a Lucene search engine (Nowviskie, 2007). This metadata format (
                
                    Submitting RDF
                ) made it possible to add URIs to create Linked Open Data and for ARC to organize search returns beyond traditional disciplines.
            
            ARC was launched when content management systems and cultural heritage platforms were in their infancy. To maintain robust and flexible metadata and search capabilities, we are sunsetting our legacy COLLEX software and moving to WordPress to allow nodes more control over their web presence. These WordPress instances will interface with an updated backend: the Corpora Dataset Studio, developed by ARC technology director Bryan Tarpley. Corpora uses mongoDB as well as elasticsearch and neo4j databases and has been designed with data visualization and Linked Open Data in mind. In collaboration with Linked Infrastructure for Networked Cultural Scholarship (
                
                    LINCS
                , Susan Brown, PI), Tarpley is recreating ARC’s legacy visual search interface, BigDIVA, in the form of a Rich Prospect Linked Open Data Viewer. Susan Brown has argued that Linked Open Data can demonstrate that categories such as disciplines, gender, and nationality can be revealed as “categorically provisional” through forging crosswalks between traditional categories and new, anti-disciplinary, intersectional, decolonizing, and queer modes of categorization (Brown, 2020). The new LOD viewer will allow users to dynamically organize the ARC catalog in their own ways, using categories and queries that make sense to them. 
            
            ARC is now working with special interest groups – from the American Antiquarian Society to the Canadian Writing Research Collaboratory and Escalator in South Africa – to create new nodes, and we are actively recruiting nodes and peer reviewing projects that reconsider the boundaries of traditional humanities subjects. ARC seeks partnerships with other groups who are in the same situation as the original NINES steering committee: they would like their projects to be searchable along with other digital projects and resources. To this end, we are developing nodes focused on disability studies, early modern drama, music studies, and more. 
            If ARC can support such groups, it will live up to that very tendentious but invigorating hope expressed in the original 
                Digital Humanities Manifesto composed by the DH group at UCLA: “
                Traditional Humanities is balkanized by nation, language, method, and media…. [W]e imagine different constellations (not just disciplinary constellations, but also other configurations of producing knowledge that can be team- and project-based, collaborative, open-ended, globally-oriented, engaging for new audiences and institutions)”(Digital Humanities Manifesto, 2008).
                
                    
                        The Digital Humanities Manifesto 2.0, dated 5/29/2009, is described and available for download: 
                        
                            http://www.toddpresner.com/?p=7
                        
                        . The original manifesto, dated 12/15/2008, is no longer available: email mandell at tamu dot edu for a saved copy.
                    
                
                 ARC and Corpora – to be released open source next year, and usable on a laptop – aspire to constitute a “diversity stack” (Liu, 2020)
            
            Establishing nodes related to special communities and interests can constellate the scholarly research universe in un- or even anti-disciplinary ways. Scholars everywhere will be able to get involved in these communities, to join the editorial boards, contribute projects for peer review, attend ARC workshops, etc. Insofar as “de-disciplining” requires de-colonizing as well, ARC is expanding Corpora’s capacities to read non-Latin script and make it possible to better accommodate “expanding the notion of evidence” to include “fragments of events, depicted in periodicals, testimony, pamphlets and even poetry, in the archive” (Risam, 2015).
            The authors present these two modes of de-disciplining ARC, viz., 1) supporting emergent (anti-)disciplines in co-developing an infrastructure free of colonial archiving constraints, from the ground up; and 2) transforming the ARC catalog from Enlightenment-style index (Pasanek and Wellmon, 2015) into a Linked Open Data viewer organized by SPARQ-L queries, in order to elicit ideas from the audience about how ARC can further support spontaneous constellations of intellectual community that may contribute to the evolution of university disciplines in the Humanities.
        
        
            
                
                    Bibliography
                    
                        Brown, S. (2020). Categorically provisional. 
                        PMLA
                        , 135(1): 165-174.
                    
                    
                        Liu, A. (2020). Toward a diversity stack: Digital humanities and diversity as technical problem. 
                        PMLA
                        , 135(1): 130-151. 
                    
                    
                        Nowviskie, B. (2007). A scholar’s guide to research, collaboration, and publication in NINES. 
                        Romanticism and Victorianism on the Net
                        , 
                        
                            https://doi.org/10.7202/016707ar
                        
                        . 
                    
                    
                        Pasanek, B. and Wellmon, C. (2015). The Enlightenment index. 
                        The Eighteenth Century
                        , 56(3): 359-382. 
                    
                    
                        Risam, R. (2015). Revising history and re-authouring the left in the postcolonial digital archive. 
                        Left History
                         18(2): 35-46.
                    
                
            
        
    



        
            1. The SpokenWeb Project: A Brief Overview:
            Michael O’Driscoll, Professor, Department of English and Film Studies, UAlberta
            This panel proposes to draw from the experience of the SpokenWeb Project team at the University of Alberta, an institutional partner in an exciting multidisciplinary research consortium now in the fourth year of seven-year SSHRC Partnership Grant. The project seeks to establish a networked archive of digital, literary audio drawn from the institutional and community collections managed by partner universities and academic libraries across Canada. The collections comprise thousands of analogue recordings (primarily reel to reel and cassette tapes) dating from the 1960s to the 1990s that, in an aggregated, searchable digital form, constitute a valuable cultural, research, and learning resource. In the vast majority of cases, these audio objects have rested unheard and uncatalogued in dusty corners and are only now meeting their future audience imagined into existence some half a century ago. 
            Integral to the project is the development of protocols not only for preservation and access that will encourage research and pedagogical innovations in this developing field at the intersection of the digital humanities, library science, literary analysis, and sound studies, but also the establishment of fundamental principles and practices for collections processing, metadata schemes, system interoperability, data analytics, and other technical matters necessary to the success of this innovative collaboration. SpokenWeb is also developing best practices in a multidisciplinary environment, ensuring high quality training and experiential learning for new scholars, and making inroads in community partnership development. 
            The University of Alberta SpokenWeb team is at a crucial juncture in the project’s lifecycle: we are about to move from a focus on collection development to a focus on public access and community engagement. This diverse panel will speak from a range of disciplinary perspectives, and draws on both emerging and established scholars on the UAlberta SpokenWeb team. Central to our presentations is the question of how the curation of durational digital objects that feature human speech across a variety of audiotextual literary genres presents pressing questions and unique challenges in a digital environment. In order to address a representative range of issues, panelists will speak to matters of digital media collection development, background archival research, timestamping and the production of structural metadata, rights management and curatorial ethics, and designing a public collections portal. After several years of project development, we’ve learned much, including the fact that is a lot more to learn. 
            
            2. Sound Studies and the Repository Environment
            Sean Luyk, Digital Projects Librarian, University of Alberta Library, Co-Investigator, SpokenWeb Alberta
            Institutional repositories provide foundational infrastructure for digital humanities projects, including long-term preservation, the provision of (open) access, resource description, and intellectual property rights management. Projects that work to curate audiovisual content, however, present significant challenges for these scholarly communications services, as they are typically not designed with the affordances of durational media in mind. Using the SpokenWeb Alberta project as a case study, this presentation will discuss the use of institutional audiovisual repositories for sound studies projects, and the considerations and challenges involved for libraries and archives supporting this work. Topics discussed will include intellectual property rights management, collections workflows, audiovisual resource description, and media preservation, as they relate to the delivery of scholarly communications services for digital humanities projects.
            
         
            3. Anomalies and Archaeology in the Archive: researching radio episodes
            Ariel Kroon, PhD, SpokenWeb Research Assistant, University of Alberta
            This section will introduce the audience to the archival research necessary to locate, confirm, and assure quality control for the audio being digitized. I will walk through the archiving process with focus first on a larger collection (UAlberta Archival radio show recordings, aired on campus radio network CKUA), then narrowing in on a single episode recording and detail the type of archaeological work required in order to identify its content and speakers, as well as the challenges of recording metadata for a new audio type that the project had not yet encountered, including the creation of appropriate language for metadata use, the labelling of weird and interesting speech acts, musical interludes, and surprising audio events. I will also discuss the challenges of tracking down related materials from a thirty-year-old radio show held in the collection of a library that no longer exists, which served a program (Radio and Television) that was shut down years ago.
            
         
            4. Introduction to Timestamping: Methods, Research Process and Ethics
            Zach Morrison, PhD Student, SpokenWeb Research Assistant, University of Alberta
            By timestamping literary audio performances, SpokenWeb researchers transform these lengthy and often unwieldy objects into navigable digital files amenable to future critical engagement. Timestamps index the speakers, literary works and topics that appear in each sonic artifact, allowing scholars to easily locate and listen to the spans of audio relevant to their research interests. They therefore improve the accessibility of SpokenWeb’s extensive archive and facilitate the activation of archival objects in the present. In this section, I will detail the various steps of the time-stamping process using a 1979 poetry performance by Fred Wah, a Canadian poet of mixed Swedish, Irish-Scots and Chinese heritage, as a case study. Drawing upon Wah’s performance, I will discuss not only the methods and types of research that are necessary to appropriately label literary audio events, but also the difficulties encountered when attempting to describe spans of audio that do not neatly fit into the syntax of SpokenWeb’s style guide. Crucial to this discussion will be the ethics of timestamping, as I will attend to the omissions effected by the timestamp itself, the small laughters, loaded pauses, and other sonic excrescences that exceed its limited descriptive space, and the potential for capturing these minor events as SpokenWeb’s practices continue to evolve.
            
         
            5. Audio Rights Management and Ethical Curation
            Michael O’Driscoll, Member, SpokenWeb Governing Board; Professor, Department of English and Film Studies, UAlberta
            The SpokenWeb Project is committed to the development and dissemination of digital audio objects drawn from analogue collections of literary performance located at institutions across Canada. In almost all cases, these recordings have never been publicly available, and their migration to an, ideally, open digital environment carries an extraordinary legal and ethical weight. The Project’s cross-institutional Rights Management Task Force (of which I am a member) is responsible for advising and supporting community and institutional partners in developing relationships, practices, and mechanisms that ensure open access to recordings that respect Canadian copyright law, rights holders, and performers. The legalities of rights management in the case of audiotextual performance are murky at best, and while the law seems to favour the recordist over the content creator, the ethical implications of curating such archived events stress a nonetheless heightened responsibility. The SpokenWeb collection includes everything from public events such as literary readings, classroom lectures, and panel presentations to more private events such as informal interviews, casual coffee-table recordings, and the sometimes revealing and even heated discussions that constitute the paratextual elements of exchanges captured on tape. Given the age of the recordings, the sometimes vulnerable identities of the speakers, and the ongoing interests of the (often still living) creative artists involved, curating these collections requires a delicate balance between the prerogatives of rights management and the responsibilities of ethical curation that devolves, often, to the specificity of the collections and audio objects under our stewardship.
            
         
            6. Lightening the Load: Minimalist Web Design and Development for Scholars
            Chelsea Miya, PhD, Postdoctoral Fellow, SpokenWeb, University of Alberta
            Tejas Ambarani, MDES Student, SpokenWeb Research Assistant, University of Alberta
            Knowledge dissemination, as a key component of scholarly work, increasingly occurs through online channels. When showcasing your research on the web, it is tempting to rely on tools like WordPress, Wix, and Squarespace, which promise glossy, ready-made builds at your fingertips. However, as Alex Gil points out, these user-friendly apps have “disconnected” researchers from the “material conditions of their own knowledge production” (Gil). Not only that, these tools are often server-heavy, stacking multiple programs on top of one another, and as such need to be continually updated. Drawing on the minimalist computing philosophy of Gil, Jentery Sayers, and others, we will use SpokenWeb UAlberta as a test-case for how to go “back to the basics” (Sayers) and build a website from scratch using basic programming languages. In our paper, we will explain how to implement minimal computing principles in both the front and back-end of development, creating a website that is self-sufficient, low-maintenance, and accessible. In addition to the underlying architecture, we will also describe how to approach the look and feel of a website with restraint, using less intrusive elements like flat colours and vector-based illustrations to cut out “excess” and foreground “just content” (Sayers). We will finally explain how to be more selective about features without compromising user experience and walk-through the benefits of using personas and scenarios to guide the design process.
        
      
        
            
                
                    Bibliography
                    Sayers, Jentery. 
                        Minimal Definitions. https://jntry.work/mindefinitions/.
                    
                    Gil, Alex. “The User, the Learner and the Machines We Make.” GO: DH. http://go-dh.github.io/mincomp/thoughts/2015/05/21/user-vs-learner/
                
            
        
    

