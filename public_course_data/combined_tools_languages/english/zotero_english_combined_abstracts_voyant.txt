
At a recent meeting, the head of Intersect, the New South Wales eResearch agency, argued that, since academics are not professional software engineers, eResearch software might be prototyped by academics but development should then be turned over to professionals. I believe this IT-centric viewpoint is based on flawed assumptions about the aims of Humanities computing, and that the potential benefits in good design, documentation and QA (assuming IT-managed projects to be superior in this respect) are generally far outweighed by the loss of flexibility which requirements gathering and structured development impose. An engineering approach may be suitable for well-defined deliverables (with commensurate funding), but research-driven and lightly funded projects will generally be far more productive under the control of a Scholar Programmer (Reside 2011, Welsh 2011) who can optimise the outcomes as opportunities present and rely on the goodwill of colleagues for bug identification and testing. Such an approach does not preclude the involvement of professional programmers in the development process (beyond a certain small scale such involvement is essential) nor equal partnership with IT professionals bringing multi-disciplinary perspectives.

I will use as example our development of Heurist, a database abstraction which allows non-technical researchers to rapidly build and incrementally modify — using a (fairly) simple web interface and researcher-centric concepts — complex multi-user web database applications with many entity types and rich relationships. Effective use of Heurist depends not on technical skill but on the decomposition of research data into a clear conceptual structure — an exercise which imposes a valuable sanity check on the quality of a researcher's conceptual model — and then allows them to pass almost directly to a fully functioning database, bypassing the plumbing of implementation (including programmer-centric abstractions such as UML and application frameworks). Over 30 projects are now running in Heurist, from individual research to public resources such as the Dictionary of Sydney (http://dictionaryofsydney.org), and it has also been used in undergraduate classes. Yet Heurist has encountered opposition from software engineers who believe it could be ‘faster’ or ‘better’ if rewritten (in a variety of alternative technologies, mostly post-dating its design), ignoring the fact that it is stable, extensible and fulfils the needs of its users better than any alternative they can propose.

Over the last few years, there have been significant shifts in data modelling towards object-relational models, XML databases, triple stores and so called NoSQL databases. Digital Humanists have embraced these techniques with varying degrees of success. However these technologies lie outside the knowledge or skills of most Humanities researchers — Excel, Access and Filemaker remain the bread and butter of many who venture into the digital domain; more ambitious projects may commission or develop custom applications backended on MySQL or PostGres. Some of these (eg. Kora) evolve into capable adaptable systems, or are conceived as such from the outset (eg. Zotero and Omeka).

While Heurist uses MySQL as its backend, it uses it to build an agnostic datastore along NoSQL principles. The majority of the code is then devoted to managing a user view of the structure based on entities, attributes and relationships which requires no understanding of the underlying methods. The structure is itself stored within the database, allowing on-the-fly modifications and providing a fully internally documented database (which can be easily — one click — exported to a fully documented XML dump of both the database structure and its content for archiving or transfer to another system). A key strength of this approach is that database structure can be developed incrementally and modified throughout the life of a project as the researcher's understanding of the domain (and software) evolves, without the need for programmers or the loss of existing data.

In this paper I will review the design principles underpinning Heurist and show how its flexible and user-modifiable approach to data structure allows the database to evolve with a scholar's needs. Heurist development has been research-driven and informal, responding to the needs of the many projects that use it, while developing new capabilities as generic tools rather than project-specific additions. I will particularly focus on the advantages of a pragmatic approach to development, driven by a Scholar-Programmer and based on evolving user needs, rapid prototyping and 'permanent beta'. I will contrast this with an 'engineering' approach based on pinning down a set of user requirements and rigorous development and testing within a framework which — even if notionally using an Agile program development methodology — discourages revision and innovation in favour of QA.

My argument will be supported by looking specifically at two mobile applications which were developed as core functions of Heurist. These applications were never envisaged in the original design of the system, but have been added with minor effort, extending the functionality of the system for all users.

First, as part of research into community engagement for the Dictionary of Sydney (dictionaryofsydney.org) we developed a mobile heritage tour application which runs off the Dictionary's underlying Heurist database. The application adopts TourML (TAP Into Museums 2012) but unlike standalone tour applications it is simply designed as a viewer with offline data caching, and can therefore run off any Heurist database containing appropriate data. No additional database design or programming was required to support data entry, storage of TourML data or the import of schemas or data, since these are already an intrinsic part of the Heurist model. With the addition of an appropriate XSL template to transform Heurist's native XML output, we also now have a web-based tour view which will work across all Heurist databases.

My second example is the handling of data schemas for the recently funded FAIMS (Federated Archaeological Information Management System) research infrastructure project funded by the Australian government's NeCTAR program. With trivial programming we were able to add a function to Heurist which exports record schemas as W3C standard XForms which can be loaded onto tablet devices for use in the field. Data is collected using these forms with Open Data Kit (Open Data Kit 2012) and synced back into the Heurist database from which the forms were exported. The field application is thus simply an extension of the Heurist database, rather than a separate application, allowing any Heurist database to become 'mobile' on a tablet — archive and library information gathering are an obvious spin-off acquired at no extra cost. This paradigm is further extended by exposing the database on the web, allowing any other Heurist database to import and reuse the schemas. Using this capability we were able to set up a schema clearinghouse for the FAIMS project and populate it with an initial set of schemas in a matter of hours. The other major components of the FAIMS project are following a formal engineering methodology and will provide useful comparative material by the time this paper is presented.

Through this paper I hope to stimulate a discussion in two areas:

First, on the appropriateness of the concept of the Scholar Programmer, picking up on recent debates on the need for Digital Humanists to embrace programming skills. I will propose an alternative concept of Scholar Analyst — a scholar who might or might not have programming skills, but mostly brings conceptual skills informed by a knowledge of what can be achieved (without necessarily being technically capable, even if time permitted). In that sense the Scholar Analyst is an Architect, who understands what different materials can achieve without being able to so much as lay a brick. I am not convinced that the Architect needs to be able to lay bricks, although they must know the limits of brick structures and of the bricklaying process.

Secondly I hope to stimulate some discussion about the dangers of development for, rather than by, the Academy. While this may not be an issue of concern to current projects with momentum, the centralisation of resources in relatively well-funded and science-dominated eResearch agencies and IT centres plays to the dominance of engineering-driven approaches and loss of capacity to develop applications aligned with Humanities research needs.

References
Date, C. J. (2004). An Introduction to Database Systems. Boston: Pearson/Addison Wesley. 8th edn.
Open Data Kit (2012). Magnifying human resources through technology. http://opendatakit.org/ Accessed 30/10/12
Reside, D. (2011). On Ant-Lions and Scholar-Programmers. http://mediacommons.futureofthebook.org/alt-ac/pieces/ant-lions-and-scholar-programmers Accessed 30/10/2012
TAP Into Museums (2012). TourML Overview. http://tapintomuseums.org/TourML Accessed 30/10/12
Welsh, T. (2011). To Code, or Not to Code. http://hastac.org/blogs/twelsh/code-or-not-code. Accessed 30/10/2012
Any tool should be useful in the expected way, but a truly great tool lends itself to uses you never expected.

Eric Raymond, The Cathedral and the Bazaar
Eric Raymond’s Cathedral and the Bazaar describes a series of lessons about the importance of sharing code with users learned during the development of Linux. Raymond emphasizes that non-technical users and third-party developers are capable of doing far more than just finding and fixing bugs—freely distributing code encourages users to take the software and develop new and unexpected things with it. “The next best thing to having good ideas,” argues Raymond, “is recognizing good ideas from your users. Sometimes the latter is better.” (2000) This kind of engagement with users tends to be the exception, not the rule, though, and even when developers are interested in establishing a cycle of feedback with users, that cycle has to be nurtured and maintained. Software development usually does not take place in a vacuum; software is developed for particular users and use cases. How people use software, and the ways in which they can share those uses, can be myriad. Developers are interested in learning how to recognize and nurture those uses, but this often proves difficult. Our panel will examine this complicated issue.

“Building” as a hermeneutic has gained increased attention and scrutiny among the Digital Humanities community. Ramsay (2011) argues that “the Digital Humanities is about building things” and is central to its “methodologization.” Sample (2012) emphasizes the importance of building as work. In particular, Sample espouses collaborative construction as a group effort where each contribution takes place in dialogue with other contributions, and creative analysis as a way to learn through creation. An emphasis on building necessitates an equal emphasis on builder, and as Gina Trampani (2011) argues, nurturing a beneficial user-contributor community that allows a variety of users, regardless of existing skills, to benefit from a hermeneutic of building. Accordingly, we are interested in modeling the communal approaches to building that bridge developer, researcher, and student.

This panel will bring together developers and users to explore the symbiotic relationships built during the life cycle of a software project, to discuss the ways in which open-source Digital Humanities projects should work to build both tools and user/developer communities. The project that we are using as a testbed for this examination is Neatline, a set of a geo-temporal tools built by the Scholars’ Lab at the University of Virginia for use in the Omeka content management system. During the months leading up to the conference, panel participants will work closely together to build and document their working relationships, all the while working to improve Neatline and implement it in productive ways. The panel will elaborate on problems and solutions for collaboration among developers and users they encountered, and suggest ways to turn users into contributors while better attuning a software development team to the needs of its users. Of particular attention to the panel will be the way in which the tool is used for multiple purposes, including research and teaching, and how such uses impact the feedback loop.

Panel Organization & Participants
We propose to conduct a panel featuring users and developers of the Neatline suite. Each participant will open the panel with a 5 minute statement describing their particular experience over the course of their collaboration, followed by a group discussion that addresses several questions. All participants are excited to participate in this panel.

Participants
Jeremy Boggs is Design Architect for Digital Research and Scholarship at the University of Virginia Library. Boggs will discuss methods for getting outside users more easily involved in the development process for Neatline. He will focus on the tools used and documentation developed during the group’s collaboration effort.

Amy Earhart is Assistant Professor of English at Texas A&M University. Earhart will discuss how her undergraduate students used Neatline to map Malcolm X’s New York, pointing to particular areas of tension between pedagogical models of digital humanities tools and the feedback loop. She will offer potential ways to eliminate such issues.

Wayne Graham is Head of Research and Development for Digital Research and Scholarship at the University of Virginia Library. Graham will discuss the day-to-day management of Neatline development, and in particular his strategies for balancing user needs and contributions with the priorities of the core Neatline development team.

T. Mills Kelly is Associate Professor of History at George Mason University and Associate Director of the Roy Rosenzweig Center for History and New Media. Kelly will discuss the use of Neatline in his historical methods course, “Dead in Virginia.” He will focus on the aspects of the user experience that seemed to influence student learning in the course.

David McClure is Web Applications Developer for Digital Research and Scholarship at the University of Virginia Library, and is lead developer on Neatline. McClure will talk about his perspective as a lead developer on Neatline.

Shawn Moore is a doctoral student at Texas A&M University and is a fellow for the Initiative for Digital Humanities, Media, and Culture (IDHMC). Moore will talk about the process of transitioning from a user of Neatline to a contributing developer during his ongoing dissertation project on Margaret Cavendish, Duchess of Newcastle (1623-1673).

Eric Rochester is Senior Developer for Digital Research and Scholarship at the University of Virginia Library. Rochester will discuss the tenuous balance between choosing the best tools and languages for a project with getting and encouraging outside contributions to a project.

Questions
What benefits will a software feedback loop provide to both user and developer?
What tools and methods did the group find most helpful during the process?
Discuss the impact of non-specialist users, such as students, on the feedback loop.
How can open-source projects create inclusive communities that invite contributions from people with skill-sets and backgrounds that are underrepresented in the open-source community?
In what ways does nurturing an outside user/developer community contribute to the use and sustainability of a Digital Humanities project?
What were the most challenging aspects of this collaboration?
Discuss future models of the feedback loop based on what you have learned in this model.
References
Bryant, T. (2006). Social software in academia. Educause Quarterly 29(2): 61.
Clement, T., and D. Reside (2011). Off the Tracks: Laying New Lines for Digital Humanities Scholars. Results of an NEH Workshop, Maryland Institute for Technology in the Humanities. http://mith.umd.edu/offthetracks/.
Cohen, D. J. (2008). Creating Scholarly Tools and Resources for the Digital Ecosystem: Building Connections in the Zotero Project. First Monday 13(8) http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/2233/2017.
Easley, D., and J. Kleinberg (2010). Networks, Crowds, and Markets: Reasoning about a Highly Connected World. http://www.cs.cornell.edu/home/kleinber/networks-book/
Fogel, K. (2005). Producing Open Source Software: How to Run a Successful Free Software Project. Sebastopol, CA: O’Reilly.
Klein, L. F. (2012). A Report Has Come Here. http://lmc.gatech.edu/~lklein7/?p=86.
McPherson, T. (2010). Scaling Vectors: Thoughts on the Future of Scholarly Communication. Journal of Electronic Publishing 13(2) http://hdl.handle.net/2027/spo.3336451.0013.208.
Neatline. http://neatline.org
Nowviskie, B. (ed.) (2011). #Alt-Academy: Alternative Academic Careers for Humanities Scholars. MediaCommons Press. http://mediacommons.futureofthebook.org/alt-ac/
Omeka. http://omeka.org.
Perspectives on Free and Open Source Software. (2010). Cambridge: The MIT Press.
Ramsay, S. (2011). On Building. http://stephenramsay.us/text/2011/01/11/on-building.html (accessed 11 January 2011).
Ramsay, S.. (2011). Who’s In and Who’s Out?http://stephenramsay.us/text/2011/01/08/whos-in-and-whos-out.html (accessed 8 January 2011).
Ramsay, S.. (2012). Programming with Humanists: Reflections on Raising an Army of Hacker-Scholars in the Digital Humanities. In Hirsch, B. D. (ed.), Teaching Digital Humanities: Principles, Practices, Politics. Ann Arbor: University of Michigan Press.
Ramsay, S., and G. Rockwell (2012). Developing Things: Notes Toward an Epistemology of Building in the Digital Humanities In Gold, M. (ed.), Debates in the Digital Humanities. Minneapolis: University of Minnesota Press. 75-84.
Raymond, E. S. (2000). The Cathedral and the Bazaar. Sebastopol, CA: O’Reilly.
Sample, M. (2012). Building and Sharing (When You’re Supposed to be Teaching). Journal of Digital Humanities. 1(1) http://journalofdigitalhumanities.org/1-1/building-and-sharing-when-youre-supposed-to-be-teaching-by-mark-sample/.
Trapani, G. Designers, Women, and Hostility in Open Source. http://smarterware.org/7550/designers-women-and-hostility-in-open-source (accessed 23 March 2011).
This paper describes an experimental approach to designing and teaching an introductory digital humanities course for graduate students. In 2011 Kevin Kee was asked to create and teach a class as part of a new interdisciplinary Humanities Ph.D. program. The graduate students taking the course would be largely unfamiliar with the digital humanities.

Kee began his preparation for the course by asking several questions. The first was "what should an introductory digital humanities course attempt to accomplish"? As he searched for answers, another important question emerged: "How could digital methods be used to design and deliver the course?" In this paper, Kee and Spencer Roberts, a Research Assistant who worked with Kee, describe first their method for researching and designing the course. They then sketch the structure and content of the course that resulted from their research. Finally, they provide examples of student responses to the material and methods covered in the course (including Roberts's perspective as a graduate student). The collected responses and their reflections on the process suggest particular ways in which future courses of this kind might be designed, implemented, and improved. Most importantly, they found that an effective way to design and teach an introductory digital humanities course is to think about the discipline through discussions about its topics and to think with the discipline by using digital tools and methods in the classroom.

Overviews of digital humanities course offerings have been conducted throughout the past fifteen years. In 1999, Willard McCarty and Matthew Kirschenbaum identified only fourteen institutions that offered courses in humanities computing. In 2006, Melissa Terras conducted another survey of digital humanities curriculum, and in 2011, Lisa Spiro undertook to collect and analyze syllabi from digital humanities courses. Of these previous surveys, Spiro’s was the most comprehensive; she collected over 134 syllabi from various levels of study in the digital humanities. Although Spiro’s work parallels and was helpful to that of Kee and Roberts, the latter were unaware of her project when they began, and had no way to replicate her research method. As a result, Kee and Roberts drew on the results of their own analyses while designing the course.

For Kee and Roberts' research, Roberts designed a method by which syllabi were converted into sets of data representing reading lists, assignments, assessment methods, and digital tools used. Commonly occurring items from within those sets were highlighted and identified as items deemed important by the statistical consensus of instructors represented in their sample. For example, their results showed that seven authors of digital humanities-related articles and books appeared on reading lists at a significantly higher frequency than others; the data also showed that other instructors found these authors most useful. Kee drew on these results when deciding on readings for his course list.

Topics covered in the course included text encoding and markup, distant reading, building, mapping, modelling and simulating, playing and gaming, teaching, and collaborating. Each of these was paired with a practical application, usually drawn from a modified version of William Turkel’s "Method". For example, students learned the theory of text markup and were asked to create pages on the course wiki using the basic wiki standard markup. Franco Moretti’s theory of distant reading made more sense for students once they experimented with text mining and analysis tools such as Voyant. Kee’s assessment strategy required students to use blogs and Twitter to comment on the theories and tools they encountered; Kee also encouraged them to participate in scholarly discourse that occurs on the Web. Although most of the students studied history, Kee aimed to create an environment in which the digital humanities were understood as both theory and practice that could be incorporated into any humanities discipline.

Because Kee and Roberts hoped to learn from the experiences of graduate students new to the digital humanities, Kee built feedback mechanisms into the course assignments, and asked students to reflect on the course before and after completion. Nearly all of the students were challenged by the dual responsibility of learning theory and skills simultaneously. Although some students were relieved to finish experimenting, others were pleased with their progress and the opportunities for future research. One student commented, “Not only do I now have some new tools to use while I’m doing research… I’m also more open-minded towards using them in the first place and really trying to engage with them, rather than brushing them off.” While students readily adopted some of the tools, such as Zotero and Evernote, they found more complex tools such as DevonThink or Voyant required a level of commitment and time they did not want to make. In short, these students were not willing to commit to a new, digital research method at a time when they were simultaneously taking graduate courses rooted in conventional research methods. For some students, however, patience led to late or accidental discoveries that improved their methods; in at least one case, a student who was skeptical throughout the course became an enthusiastic supporter of digital methods and now avidly attends DH conferences and events. At the conclusion of the course, most students were open to the various theories and approaches used in the digital humanities, and were enthusiastic about trying new tools and experimenting with new methods that might improve their research and scholarship.

From the outset of the project, Kee and Roberts understood that they were asking questions for which there were several feasible answers. Some graduate level digital humanities courses focus on topics within the digital humanities; others primarily train students to develop digital skills using computational tools. Kee's approach was to combine these two approaches into one course that provided opportunity for theoretical discussion while also showcasing practical applications, so that students could see the potential beneDits of digital humanities methods without having to master sophisticated tools. The research method used to build the course syllabus employed the same theories and tools that were later discussed in the course, creating an iterative loop through which student feedback and developments in the discipline can be incorporated into future versions. Already there are new tools to improve the collection and analysis of digital humanities syllabi, and new methods being explored by instructors. Through the experimental approach described in this paper, Kee and Roberts have found that thinking about and thinking with the discipline, a method that many digital scholars employ in their research, is also an effective way to design a course, and appeals to students who are new to the discipline, fostering enthusiasm for its use in their own often conventional humanities scholarship. The authors hope that this approach contributes to the growing conversation about teaching digital humanities, while also reflecting and adapting to the dynamic topics within the field.
Introduction
Rankings of academic articles and journals have been used in most disciplines, although concerns and objections about their use have been raised, particularly when they affect appointments, promotions and research grants. In addition, journal rankings may not represent real research outcomes, since low-ranking journals can still contain good work. Arts and humanities scholars have raised additional concerns about whether the various rankings accommodate differences in cultures, regions and languages. Di Leo (2010) wrote that “journal ranking is not very useful in academic philosophy and in the humanities in general” and one reason is the “high level of sub-disciplinary specialization”. Additionally, Di Leo notes there is “little accreditation and even less funding” in the humanities when compared with business and sciences.

In a Nature article entitled, “Rank injustice”, Lawrence (2002) notes that the “Impact factor causes damaging competition between journals since some of the accepted papers are chosen for their beneficial effects on the impact factor, rather than for their scientific quality”. Another concern is the effect on new fields of research. McMahon told The Chronicle of Higher Education, “Film studies and media studies — they were decimated in the metric because their journals are not as old as the literary journals. None of the film journals received a high rating, which is extraordinary” (quoted by Howard 2008).

Although the Australian government dropped rankings after complaints that they were being used “inappropriately”, it will still offer a profile of journal publications that provides an “indication of how often a journal was chosen as the forum of publication by academics in a given field” (Rowbotham 2011). Despite concerns over rankings, educators and researchers agree there should be a quality management system. By publishing their results, researchers are not just talking to themselves. Research outcomes are for public use, and others should be able to study and measure them. However, the questions are how can we measure the research efforts and their impact, and can we get an early indication of research work that is capturing the research community’s attention. A second question is whether measures appropriate for one research area also can be applied to publications in a different area. In this study, we seek initial insights on these questions by using data from a social media site to measure a real-time impact of articles in the digital humanities.

Research Community Article Rating (RCAR)
Citation analysis is a well-known metric to measure scientific impact and has helped in highlighting significant work. However, citations suffer from delays that could span months or even years. Bollen, et al., (2009) concluded that “the notion of scientific impact is a multi-dimensional construct that cannot be adequately measured by any single indicator”. Terras (2012) found that digital presence in social media helped to disseminate research articles, and “open access makes an article even more accessed”.

An alternative approach to citation analysis is to use data from online scholarly social networks (Priem and Hemminger 2010). Scholarly communities have used social reference management (SRM) systems to store, share and discover scholarly references (Farooq et al 2007). Some well-known examples are Zotero [1] , Mendeley (Henning and Reichelt, 2008) and CiteULike [2] . These SRM systems have the potential to influence and measure scientific impact (Priem et al., 2012). Alhoori and Furuta (2011) found that SRM is having a significant effect on the current activities of researchers and digital libraries. Accordingly, researchers are currently studying metrics that are based on SRM data and other social tools. For example, Altmetrics [3] was defined as “the creation and study of new metrics based on the social web for analyzing and informing scholarship”. PLOS proposed article-level metrics (ALM) [4] that are a comprehensive set of research impact indicators that include usage, citations, social bookmarking, dissemination activity, media, blog coverage, discussion activity and ratings.

Tenopir and King (2000) estimated that scientific articles published in the United States are read about 900 times each. Who are the researchers reading an article? Does knowing who these researchers are influence the article’s impact? Rudner, et al., (2002) used a readership survey to determine the researchers’ needs and interests. Eason, et al. (2000) analyzed the behavior of journal readers using logs.

There is a difference between how many times an article has been cited and how many times it has been viewed or downloaded. A citation means that an author has probably read the article, although this is not guaranteed. With respect to article views, there are several viewing scenarios such as intended clicks, unintended clicks or even a web crawler. Therefore, the number of views has hidden influential factors. To eliminate the hidden-factors effect, we selected the articles that researchers had added to an academic social media site. In this study, we ranked readers based on their education level. For example, a professor had a higher rank than a PhD student, who in turn had a higher rank than an undergraduate student.

Zotero’s readership statistics were not available to the public, and in CiteULike, the most cited articles in Literary and Linguistic Computing (LLC) were shared by few users. Therefore, we were unable to use either system’s data. Instead, we obtained our data from Mendeley, using its API [5] . We measured the research community article rating (RCAR)using the following equation:


RCAR uses the following quantities:


Citations, Readership and RCAR
We looked at seven digital humanities journals that were added to Mendeley and also mentioned in Wikipedia [6] . Of the seven journals, Google Scholar had an h5-index for only two journals: Digital Creativity (h5-index = 7) and LLC (h5-index = 13). We calculated the RCAR and compared the top-cited LLC articles with Mendeley readerships, as shown in Table 1. The number of citations were significantly higher than the number of Mendeley readerships for LLC (p-value>0.05).


Table 1:
Google citations, Mendeley readerships and RCAR for LLC

We investigated how the digital humanities discipline is different from other disciplines. We compared LLC with a journal from a different area of research, Library Trends, which had a similar h5-index. Library Trends received more citations and readerships than LLC. Three of its top articles also had more Mendeley readerships than citations, whereas LLC only had one such case. However, there was no significant difference between Library Trends citations and readerships. Next, we tested the Journal of the American Society for Information Science and Technology (JASIST) and the Journal of Librarianship and Information Science (JOLIS). We found that JASIST and JOLIS readerships of articles published in 2012 were higher than the citations with significance difference. This indicates that computer, information, and library scientists are more active in academic social media site than digital humanities researchers. By active we mean that they share and add newly published articles to their digital library.

Citations and altmetrics
In order to better understand different socially-based measures, we compared LLC articles using altmetrics and citations. We used an implementation of altmetrics called Altmetric that gave “each article a score that measures the quantity and quality of attention it has received from Twitter, Facebook, science blogs, mainstream news outlets and more sources”. We found that most of the articles that received social media attention were published during the last two years. A number of articles that were published four or more years ago were exceptions to this finding. These older articles had received at least four citations, as shown in Table 2. We also found similar correlations with articles in Digital Creativity.

Finally, we compared readerships and Altmetric. We found no significant difference between LLC citations of articles published in 2012 and readerships. However, we found a significant difference between Altmetric and citations (p<0.05) for articles that were published in 2012. This shows that the researchers who are interested in digital humanities are more active in general social media sites (e.g. Twitter, Facebook) than academic social media sites (e.g. Mendeley).


Table 2:
Altmetric score and citations to LLC articles

In this paper we describe a new multi-dimensional approach that can measure in real-time the impact of digital humanities research using academic social media site. We found that RCAR and altmetrics can quantify an early impact of articles gaining scholarly attention. In the future, we plan to conduct interviews with humanities scholars, to better understand how these observations reflect their needs and the standards in their fields.

References
Alhoori, H., and R. Furuta (2011). Understanding the Dynamic Scholarly Research Needs and Behavior as Applied to Social Reference Management. TPDL’11 Proceedings of the 15th international conference on Theory and practice of digital libraries. Berlin Heidelberg: Springer. 169–178.
Bollen, J., H. Van de Sompel, A. Hagberg, and R. Chute (2009). A principal component analysis of 39 scientific impact measures. PloS one. 4(6). e6022.
Eason, K., S. Richardson, and L. Yu (2000). Patterns of use of electronic journals. Journal of Documentation. 56(5): 477–504.
Farooq, U., Y. Song, J. M. Carroll, and C. L. Giles (2007). Social Bookmarking for Scholarly Digital Libraries. IEEE Internet Computing. 11(6): 29–35.
Henning, V., and J. Reichelt (2008). Mendeley — A Last.fm For Research? In 2008 IEEE Fourth International Conference on eScience. IEEE. 327–328.
Howard, J. (2008). New Ratings of Humanities Journals Do More Than Rank They Rankle. The Chronicle of Higher Education. http://chronicle.com/article/New-Ratings-of-Humanities/29072 (Accessed 5 March 2013).
Lawrence, P. A. (2002). Rank injustice. Nature. 415(6874): 835–6.
Di Leo, J. (2010) Against Rank. Inside Higher Ed. http://www.insidehighered.com/views/2010/06/21/dileo (Accessed 5 March 2012).
Priem, J., and B. M. Hemminger (2010). Scientometrics 2.0: Toward new metrics of scholarly impact on the social Web. First Monday. 15(7).
Priem, J., C. Parra, H. Piwowar, P. Groth, and A. Waagmeester (2012). Uncovering impacts: a case study in using altmetrics tools. Workshop on the Semantic Publishing (SePublica 2012) at the 9th Extended Semantic Web Conference.
Rowbotham, J. (2011). End of an ERA: journal rankings dropped. The Australian. http://www.theaustralian.com.au/higher-education/end-of-an-era-journal-rankings-dropped/story-e6frgcjx-1226065864847 (Accessed 5 March 2013).
Rudner, L.M., J. S. Gellmann, and M. Miller-Whitehead (2002). Who Is Reading On-line Education Journals? Why? And What Are They Reading? D-Lib Magazine. 8(12).
Tenopir, C., and D. W. King (2000). Towards Electronic Journals: Realities for Scientists, Librarians, and Publishers. Washington, D.C.: Special Libraries Association.
Terras, M. (2012). The impact of social media on the dissemination of research: results of an experiment. Journal of Digital Humanities. 1(3): 30–38.
Introduction
A frequently claimed hallmark of Digital Humanities is its emphasis on collaborative work and joint publication (see, for example, Moulin, Nyhan et al 2012; Koh 2012; and Deegan and McCarty 2012). Is there any mismatch between the way that the field describes itself and what we find when we examine the evidence of publication patterns and practices? Furthermore, have publication patterns changed since the first journal of the field, Computers and the Humanities, was established in 1966? Has joint publication become more or less common or have the proportions of jointly published articles remained the same? Also, how do such patterns compare with other disciplines of the Humanities?

Research context
To the best of our knowledge the empirical evidence of publication practices of Digital Humanities scholars has not, until our research, been systematically investigated. In order to make a first contribution towards addressing this gap in the research literature we focused our research on publication patterns in the leading Digital Humanities journals since 1966. We hope to extend our analysis to other Digital Humanities journals (for example Computing in the Humanities Working Papers), identify and analyse publications in non-specialist Digital Humanities journals and contributions to e.g. book collections in a future iteration of this research. We began by harvesting all bibliographical metadata from Computers and the Humanities (1966-2004), Literary and Linguistic Computing (1986-2011) and Digital Humanities Quarterly (2007-2011) and then analysed it in order to explore the following questions:

What can we observe about patterns of joint publication?
What percentage of articles per issue and per journal was jointly published?
What kinds of joint publication patterns existed? What percentage of joint publications had 2 authors? What percentage had 3, 4, 5 or more?
How many authors contributed to more than one joint publication?
Of those who published jointly what patterns can be observed? Did they tend to contribute to papers authored by two or more authors?
Did authors tend to publish with predominately the same people over their careers or do we see a number of shifting constellations?
What percentage of people published only one article in the journals listed above and based on this do we see large portions of people dropping in and out of the field at particular times?
Methodology
Using Zotero we extracted bibliographical metadata from Computers and the Humanities (Chum)(1966–2004); Literary and Linguistic Computing(LLC) (1986-2011); and Digital Humanities Quarterly (DHQ) (2007-2011) and then exported this to Excel for initial viewing. As far as possible the data was cleaned and regularised e.g. a canonical form of personal names chosen where slight differences existed such as E.G. Wills and Edward G. Wills. The cleaned data were then imported into an SQL database and sorted into groups based on the number of authors. The annual observed frequencies of papers with n authors were then calculated. For each group, a linear regression was calculated in order to determine within a given journal whether the incidence of n-authored papers had changed over time.

For each journal the data were also processed so that dual-authored papers could be analysed using a connectivity index (Bell et al 2002)to determine the extent to which the pool of authors contributing to a given journal were interconnected. A connectivity index was constructed both on a journal-wide basis and on a per-author basis, allowing the distribution of ‘well-connected’ authors to be compared within and between journals.

Findings related to Chum
The diagram below shows the key findings in relation to Chum; space will not allow the findings in relation to LLC and DHQ to be presented, and all three journals to be compared and contrasted, so this will be done in the full paper. In relation to Chum, contrary to what one might expect given digital humanities emphasis on collaborative work, the highest incidence is of single-authored papers, but the frequency of this is variable. We should also remember that this does not necessarily mean that the research was done by a single scholar — all we can say is that publications were predominately by a single scholar. Considering trends over the lifetime of Chum we see that single authored papers are flat, whereas frequency of 2 and 3 authored papers increases over time. The strength of this association was examined using regression analysis. For Chum, the observed frequency of 2 and 3 authored papers increased over time, and this was significant at the 1% level. The frequency of 5 authored papers decreased over the observed time period, and this relationship was significant at the 5% level. However the overall number of 5 authored papers was low throughout.


Frequency of n-authored papers, by year

Findings related to LLC
The same analysis was conducted for LLC (graphs to be shown in the full presentation). Again, the most common form of contribution was of sole-authored papers. However, regression analysis highlighted some notable trends. The frequency of sole-authored papers was found to be decreasing over time, and this relationship was significant at the 5% level. As with Chum, the frequency of 3-authored papers was found to be increasing, significant at the 1% level. The frequency of papers with 2, 4 and 5 authors showed some increase over time, but these relationships were not significant.

Conclusion
In relation to Chum we have found that single-author publications were predominant for much of the lifetime of the journal. However, this does not necessarily mean that Digital Humanities does not have a higher occurrence of joint publications than other disciplines of the Humanities. Therefore, it is important to not only examine the empirical evidence that exists for publication practices in the Digital Humanities since 1966 at the aggregate level but to attempt also to situate such findings in a wider comparative context.

It should be noted that authorship as reflected in publication credits does not necessarily reflect actual contribution to research: a range of alternative practices might occur, from papers that only carry a single name despite substantial contributions from others, to papers that include ‘gift’ attributions to persons who have had little or no input(Cronin et al 2003).

The stereotype of the Humanities lone scholar is well known, even if it is increasingly recognised as being an impoverished model(Bulger et al 2011). The rate of publication and productivity in the Humanities has been looked at by Muffo et al(1987); Ramsden(1994); Wanner, Lewis & Gregorio (1981) and Stone (1982) examined, inter alia, “the way humanities scholars work and the materials of their research”. Changing publication patterns in the Humanities in Flanders and Belgium have been analysed by Engels et al who found that in the period 2000-09 “The overall growth rate in number of publications is over 62.1%, but varies across disciplines between 7.5 and 172.9%. Publication output grew faster in the Social Sciences than in the Humanities.”(2012). In 2003 Kyvik found that in Norwegian Universities co-Authorship has become more common but it is difficult to determine from the article to what degree this applies to the Humanities. Lariviere et al used data from the CD-ROM versions of the Science Citation Index, Social Sciences Citation Index and the Arts & Humanities Citation Index from 1980 to 2002, to argue that “contrary to a widely held belief, researchers in the social sciences and the humanities do not form a homogeneous category. In fact, collaborative activities of researchers in the social sciences are more comparable to those of researchers in the [natural sciences and engineering] than in the humanities”(2006).

In essence, looking to the scholarly literature in both Digital Humanities and the wider Humanities context relatively few studies have been undertaken based on the empirical data that exists about joint publication patterns. This short paper will take a first step towards remedying this.

References
Bell, M., M. Blake, P. Boyle, O. Duke-Williams, P. Rees, J. Stillwell, and G. Hugo (2002). Cross-national comparison of internal migration: issues and measures. Journal of the Royal Statistical Society: Series A (Statistics in Society) 165: 435–464. doi: 10.1111/1467-985X.t01-1-00247
Bulger, Monica, et al. Research Information Network (2011). Reinventing research? Information practices in the humanities. Research Information Network, UK.
Cronin, B., D. Shaw, and K. La Barre (2003). A cast of thousands: Coauthorship and subauthorship collaboration in the 20th century as manifested in the scholarly journal literature of psychology and philosophy. Journal of the American Society for Information Science 54 855–871. doi: 10.1002/asi.10278
Deegan, M., and W. McCarty (2012.) Collaborative research in the digital humanities: a volume in honour of Harold Short, on the occasion of his 65th birthday and his retirement, September 2010. Farnham: Ashgate.
Engels, T. C. E., T. L. B. Ossenblook, and E. H. J. Spruyt (2012). Changing publication patterns in the Social Sciences and Humanities, 2000-2009. Scientometrics.
Koh, A. (2012.) The Challenges of Digital Scholarship. ProfHacker, http://chronicle.com/blogs/profhacker/the-challenges-of-digital-scholarship/38103 (accessed 25 January 2012).
Moulin, C., J. Nyhan, et al. (2011) ESF: Science Policy Briefing. Research Infrastructures in the Digital Humanities
Kyvik, S. (2003). Changing trends in publishing behaviour among university faculty, 1980-2000. Scientometrics, 58:1.
Lariviere, V., Y. Gingras, and Archambault, E. (2006). Collaboration networks: A comparative analysis of the natural science, social sciences and the humanities. Scientometrics, 68(3): 519-533.
Muffo, J. A., S.V. Mead, and A.E. Bayer (1987). Using faculty publication rates for comparing “peer” institutions. Research in Higher Education, 27.2 163-175.
Ramsden, P. (1994). Describing and explaining research productivity. Higher Education, 28(2): 207-226.
Stone, S. (1982). Humanities Scholars: Information Needs and Uses. Journal of Documentation, 38(4): 292–313.
Wanner, R. A., L.S. Lewis, and D.I. Gregorio (1981). Research Productivity in Academia: A Comparative Study of the Sciences, Social Sciences and Humanities. Sociology of Education, 54: 238-253.
Given that the nature of the research work involves computers and a variety of skills and expertise, Digital Humanities (DH) researchers are working in teams with Humanists, Computer Scientists, other academics, undergraduate and graduate students, computer programmers and developers, librarians and others within their institutions and beyond (Williford et al., 2012). These projects’ scope and scale also requires larger budgets than is typically associated with Humanities research (Siemens, 2009, Siemens et al., 2011a). As a result, Digital Humanists must developed meta-methodological skills, including teamwork and project management, to support the important and necessary methodological and technological ones in order to achieve project success. Programs such as the University of Victoria’s Digital Humanities Summer Institute’s Large Project Management Workshop (www.dhsi.org), MITH’s Digital Humanities Winter Institute (MITH 2012), University of Virginia’s Praxis Program (Scholars' Lab, 2011) and internships with libraries and DH centres (Conway et al. 2010) are contributing to the formal skill development in these meta-methodological areas while DH teams themselves are reflecting on the importance and nature of collaboration skills developed directly through experience (Liu, et al. 2007; Ruecker, et al. 2007; Ruecker, et al. 2008; Siemens, et al. 2010).
While McCarty (2005) articulates a “intellectual and disciplinary map” for DH (118), which has become an important and agreed upon description of the discipline, no corresponding framework of these necessary meta-methodological skills exists that can guide the preparation of Digital Humanists to be as “comfortable writing code as they are managing teams and budgets” (Scholars' Lab 2011), either within a traditional academic post or an alternative academic one (The Praxis Program at the Scholars' Lab 2012). By drawing upon exemplary DH projects which exhibit individual components of this framework, this paper contributes to larger discussions by suggesting those important meta-methodological skills, knowledge and tasks, whose absences may impact a digital project’s success and long-term sustainability. It ties together the many discussions about undergraduate and graduate DH training and education happening simultaneously across many various forums.
As can be seen in Figure 1, these meta-methodological skills involve more than collaboration and project management skills and include those typically associated with academic entrepreneurship. In their recent report on DH project sustainability, Maron and Loy (2011b) characterize digital resource projects as “small start-up businesses” (32) and use entrepreneurial language, such as empowered leadership, creation of a strong value proposition, cost management, revenue strategies and others to describe the ways in which case study DH projects have managed the impact of the current economic crisis on them. Further, their definition of sustainability, “the ability to generate or gain access to the resources — financial or otherwise — needed to protect and increase the value of the content or service for those who use it” draws further attention to skills, such as management, leadership, budget management, creation of alternative revenue streams, knowledge about users and their needs, and others (Maron, et al. 2011b, 10; Shane 2004). This view has been reinforced in a recent report that reviewed the Digging into Data program. Using the table metaphor, the authors likened project management expertise as the fourth leg, along side data management, domain, and analytical expertise (Williford, et al. 2012). Many digital projects are incorporating aspects of these skills, knowledge and tasks already.
For example, several projects are taking active steps to identify and understand users and their needs and then to ensure that the digital resources address these and allow the user to do more or do it faster or differently than before, while ensuring that knowledge of these resources reach the users (Warwick, et al. 2006; Siemens, et al. 2011b; Ithaka S&R, 2009; Shane 2004; Cohen, et al. 2005). To this end, the LARIAH report (2006) recommends that digital projects consult users widely, often, in a variety of forms and at the different development stages to ensure that a digital resource is in fact used consistently show “a deep understanding and respect for the value their resource contributes to those who use it” (Maron et al., 2009, pg. 7). One way to approach this is have users directly involved in the resource’s development as is the case with TEI-C (nd-b) and Zotero (nd). In other cases, a digital resource can identified new users and extended its services to them, as can be seen with NINES’ move beyond the 19th century to develop a supporting resource for the 18th (18thConnect; Bromley 2011). Alternatively, the digital resource may broaden offerings with new value added services, such as community websites, discussion lists, book and journal distributions and others, in response to users’ requests (Iter 2011). Social media, such as twitter, youtube, and Wikipedia, can also be used to both gather information about users and promote the resource itself. For example, The Modernist Versions project combined an #yearofulysses, webpage, digital version release of Ulysses, and Ulysses Art Competition as a way to generate knowledge (and perhaps even excitement) about the initiative (The Modernist Versions Project). Alternatively, the TEI-C used a viral marketing experiment to understand the size and geographical distribution of its community of practice as it increased awareness of the guidelines (Siemens, et al. 2011b). Digital resource logos embedded in other projects can be important for generating awareness of a resource and its potential uses (TEI-C, nd-a; Zotero). At a basic level, projects must be pro-active in planning effective ways to promote themselves (Guiliano 2012).
As various granting agencies are increasingly prioritizing funding for project development rather than the maintenance and ongoing operations and resource improvements (National Endowment for the Humanities, 2010, Maron et al., 2009), digital projects must develop alternative revenue and funding models that allow them to move from grant funding to “a longer term plan for ongoing growth and development” and to even survive changes in the funding environment (Maron, et al. 2011a, 4; 2011b). These models might include cooperative advertising and click-through ads (Internet Shakespeare Editions, 2010), subscriptions (Iter 2011) and smart phone apps (iHistory Tours 2010b). In order to ensure appropriate levels of resources can be gathered, digital projects must also understand methods to budget the costs need to “cover the costs of the tasks essential to the development, support, maintenance and growth of their projects” (Maron, et al. 2009, 15; Guthrie, et al. 2008; Scholars' Lab, 2011). The latter becomes particularly important since there are expectations that digital resources will continue and be updated and improved with changes in scholarship and technology (Kretzschmar Jr. 2009; Ithaka S&R 2009). Digital projects can then combine both revenue and cost management into long term sustainability plans, often required by funders (National Endowment for the Humanities 2010).
While financial resources are important, long term sustainability and development of digital projects also relies heavily on leadership (Maron, et al. 2011a; Maron, et al. 2009; Shane 2004; Siemens, 2009). These leaders have “a certain passion and tireless attention to setting and achieving goals” (Maron, et al. 2009, 7). This leadership is multifaceted and includes the supervision of human resources, such as paid staff, volunteers and collaborators, and cost and revenue management as well as strategic planning (Ithaka S&R 2009; Shane 2004; National Endowment for the Humanities Office of Digital Humanities 2010). As an example, Transcribe Bentham publishes regular project updates as motivators to its volunteer transcribers (Transcribe Bentham 2011). In addition, these project leaders need to continue to educate colleagues, administration and granting agencies so that these individuals understand the value of DH and the level of support needed for continued growth and development, including financial, in-kind and human resources, recognition and others (Ithaka S&R 2009; Siemens, 2010). The project leader for the 40-year old Thesaurus Linguae Graecae considers “it her job to ‘educate’ current and incoming administrators about her project” (Ithaka S&R 2009, 2). Finally, a strong leader understands when outside expertise and partnerships are necessary to ensure project success (Maron, et al. 2009). The Niagara 1812 iphone app development team included not only writers, researchers, and software developers, but also the nGen-Niagara Interactive media Generator and Brock Business Consulting Group, providing expertise that is not typically available in a History department (iHistory Tours, 2010a).
 
Figure 1:
Meta-methodological Commons1
Given the nature of digital projects, academics, particularly those from the Humanities, need to adopt these important meta-methodological skills and knowledge in addition to content and methodological ones to ensure project success (Ithaka S&R 2009; Cohen, et al. 2009; Cohen, et al. 2005). By outlining the range of meta-methodological skills, this framework has the potential to strengthen the positive work already ongoing to develop these skills and knowledge and to contribute to the discussion about the important skills that Digital Humanists need to create successful, useful and used projects (LeBlanc 2011; Leon 2011; McCarty 2011; Rovira 2011; Spiro 2011; Cohen, et al. 2005; Scholars' Lab 2011). This paper also contributes to the larger discussion of regarding undergraduate and graduate training and education in Humanities, DH, and beyond and provides a context for thinking about the additional skills needed for both faculty and alternative academic positions (Thaller, et al. 2012; Scholarly Communication Institute 2012; The Praxis Program at the Scholars' Lab 2012; Sample 2012; Pannapacker 2012; Williford, et al. 2012). Finally, it is designed to enable those who work in such teams or who work to support these individuals to recognise and develop the necessary meta-methodological skills and knowledge that lead to project success.
References
18thconnect What Is 18thconnect? http://www.18thconnect.org/18th_about/what_is.html (accessed October 13, 2011).
Bromley, A. (2011). Nines Project Enhances Tools for Digital Research in the Humanities. UVa Today.
Cohen, D. J., N. Fraistat, M. G. Kirschenbaum, and T. Scheinfeldt (2009). Tools for Data-Driven Scholarship: Past, Present and Future, Ellicott City, Maryland.
Cohen, D. J., and R. Rosenzweig (2005). Digital History: A Guide to Gathering, Preserving, and Presenting the Past on the Web http://www.chnm.gmu.edu/digitalhistory/audience/index.php (accessed October 28, 2011).
Conway, P., N. Fraistat, P. Galloway, K. Kraus, et al. (2010). Digital Humanities Internships: Creating a Model Ischool-Digital Humanities Center Partnership. Digital Humanities 2010. London, UK.
Guiliano, J. (2012). Neh Project Director’s Meeting: Lessons for Promoting Your Project. http://mith.umd.edu/neh-project-directors-meeting-lessons-for-first-time-pis/ (accessed
Guthrie, K., R. Griffiths, and N. Maron (2008). Sustainability and Revenue Models for Online Academic Resources: An Ithaka Report, New York: Ithaka.
Ihistory Tours (2010a). About the Team. http://www.ihistorytours.com/about/ (accessed October 28, 2011).
Ihistory Tours (2010b). Niagara 1812. http://www.ihistorytours.com/ (accessed October 13, 2011).
Internet Shakespeare Editions (2010). Support the Internet Shakespeare Editions.http://internetshakespeare.uvic.ca/Foyer/donate.html (accessed February 2, 2011).
Iter (2011). Iter: Gateway to the Middle Ages & Renaissance. http://www.itergateway.org/ (accessed October 13, 2011).
Ithaka S&R (2009). Sustaining Digital Resources: A Briefing Paper for Leaders of Projects with Scholarly Content.http://sca.jiscinvolve.org/wp/files/2009/10/sca_bp_projects_scholarly_content_sep09_v1-02.pdf (accessed October 12, 2011).
Kretzschmar Jr., W. A. (2009). Large-Scale Humanities Computing Projects: Snakes Eating Tails, or Every End Is a New Beginning? Digital Humanities Quarterly, 3 (2).
Leblanc, M. (2011). Re: [Humanist] 25.382 Intro Topics and Texts.http://www.digitalhumanities.org/humanist/Archives/Current/Humanist.vol25.txt (accessed October 17, 2011).
Leon, S. M. (2011). Project Management for Humanists: Preparing Future Primary Investigators.http://mediacommons.futureofthebook.org/alt-ac/pieces/project-management-humanists (accessed June 24, 2011).
Liu, Y., and J. Smith (2007). Aligning the Agendas of Humanities and Computer Science Research: A Risk/Reward Analysis. SDH-SEMI 2007. Saskatoon, SK.
Maron, N., and M. Loy (2011a). Funding for Sustainability: How Funders' Practices Influence the Future of Digital Resources, Ithaca, NY: Ithaka S&R.
Maron, N., and M. Loy (2011b). Revenue, Recession, Reliance: Revisiting the Sca/Ithaka S&R Case Studies in Sustainability. Ithaca, NY: Ithaka S& R.
Maron, N., K. K. Smith, and M. Loy (2009). Sustaining Digital Resources: An on-the-Ground View of Projects Today, Ithaca, NY: Ithaka S&R.
Mccarty, W. (2005). Humanities Computing, New York: Palgrave MacMillan.
McCarty, W. (2011). [Humanist] 25.370 Intro Topics and Texts.http://www.digitalhumanities.org/humanist/Archives/Current/Humanist.vol25.txt (accessed October 17, 2011).
Mith (2012). Welcome to Dhwi. http://mith.umd.edu/dhwi/ (accessed
National Endowment for the Humanities (2010). Digital Humanities Implementation Grants.http://www.neh.gov/grants/guidelines/digitalhumanitiesimplementation.html (accessed October 12, 2011).
National Endowment for the Humanities Office of Digital Humanities (2010). Summary Findings of Neh Digital Humanities Start-up Grants (2007-2010). Washington, D.C.: National Endowment for the Humanities.
Pannapacker, W. (2012). No Dh, No Interview. http://chronicle.com/article/No-DH-No-Interview/132959/ (accessed
Rovira, J. (2011). Re: [Humanist] 25.370 Intro Topics and Text.http://www.digitalhumanities.org/humanist/Archives/Current/Humanist.vol25.txt (accessed October 17, 2011).
Ruecker, S., and M. Radzikowska (2007). The Iterative Design of a Project Charter for Interdisciplinary Research.DIS 2008. Cape Town, South Africa.
Ruecker, S., M. Radzikowska, and S. Sinclair (2008). Hackfests, Designfests, and Writingfests: The Role of Intense Periods of Face-to-Face Collaboration in International Research Teams. Digital Humanities 2008. Oulu, Finland.
Sample, M. (2012). Digital Humanities at Mla 2013. http://www.samplereality.com/2012/10/17/digital-humanities-at-mla-2013/ (accessed
Scholarly Communication Institute (2012). Landscape of Alternate-Curriculum Graduate Training Programs.https://docs.google.com/document/d/1NVPiPHeOWbvMBBpFiNXRsqsjzVZ2t2E6mtxb-p89iSQ/edit (accessed
Scholars' Lab (2011). The Praxis Program at the Scholars' Lab. http://praxis.scholarslab.org/ (accessed September 12, 2011).
Shane, S. (2004). Academic Entrepreneurship: University Spinoffs and Wealth Creation, Cheltenham, UK, Edward Elgar.
Siemens, L. (2009). 'It's a Team If You Use "Reply All": An Exploration of Research Teams in Digital Humanities Environments. Literary & Linguistic Computing. 24(2). 225-233.
Siemens, L. (2010). Developing Academic Capacity in Digital Humanities: Thoughts from the Canadian Community.Digital Humanities 2010. London, UK.
Siemens, L., R. Cunningham, W. Duff, and C. Warwick (2011a). A Tale of Two Cities: Implications of the Similarities and Differences in Collaborative Approaches within the Digital Libraries and Digital Humanities Communities Literary & Linguistic Computing, 26(3). 335-348.
Siemens, L., and Inke Research Group (2010). Understanding Long Term Collaboration: Reflections on Year 1 and Before INKE 2010. The Hague, Netherlands.
Siemens, L., R. G. Siemens, and H. Wen (2011b). "The Apex of Hipster Xml Geekdom: Tei-Encoded Dylan and Understanding the Scope of an Evolving Community of Practice. Journal of the TEI Encoding Initiative, 1(1).
Spiro, L. (2011). Knowing and Doing: Understanding the Digital Humanities Curriculum.http://digitalscholarship.files.wordpress.com/2011/06/spirodheducationpresentation2011-4.pdf (accessed October 17, 2011).
Tei-C (nd-a). Tei Badges. http://www.tei-c.org/About/Badges/ (accessed October 14, 2011).
Tei-C (nd-b). Tei Workgroups. http://www.tei-c.org/Activities/Workgroups/index.xml (accessed October 14, 2011).
Thaller, M., P. Sahle, F. Clavaud, T. Clement, et al. (2012). Digital Humanities as a University Degree: The Status Quo and Beyond. Digital Humanities 2012. Hamburg, Germany.
The Modernist Versions Project (nd). It’s All About You(Lysses). http://web.uvic.ca/~mvp1922/you/ (accessed
The Praxis Program at the Scholars' Lab (2012). About Praxis. http://praxis.scholarslab.org/about.html (accessed
Transcribe Bentham (2011). Transcribe Bentham. http://www.ucl.ac.uk/transcribe-bentham/ (accessed October 28, 2011).
Warwick, C., M. Terras, P. Hunginton, N. Pappa, et al. (2006). The Lairah Project: Log Analysis of Digital Resources in the Arts and Humanities, Final Report to the Arts and Humanities Research Council, London: University College.
Williford, C., and C. Henry (2012). One Culture: Computationally Intensive Research in the Humanities and Social Sciences: A Report on the Experiences of First Respondents to the Digging into Data Challenge, Council on Library and Information Resources.
Zotero (nd). Get Involved with Zotero. http://www.zotero.org/getinvolved/ (accessed October 14, 2011).
Notes
1. The meta-methodological commons is centred around McCarty’s (2005) articulation of the Digital Humanities Methodological Commons.
“When found, make a note of” (Dickens 1848, 149). In 1849, William Thoms took this rule as the motto of his new journalNotes and Queries, observing that following this rule for any length of time will result in “a good deal of matter in various forms, shapes and sizes . . . [in] countless boxes and drawers, and pigeon-holes of such things, which want looking over, and would well repay the trouble” (Thoms 1849, 1–2). Thoms could have been describing the offices of a contemporary documentary editing project, except that these days the “pigeon-holes” include shared hard drives. Good documentary editors follow this rule scrupulously, and as their projects stretch on into multiple decades, they amass a rich storehouse of notes, which would indeed repay the trouble of those who would look over them. Yet few have this opportunity, as only a small fraction of the content of these notes are ever made available.
Our aim in the Editors’ Notes project (http://editorsnotes.org/) is much the same as that of Thoms in 1849: to provide a “medium by which much valuable information may become a sort of common property among those who can appreciate and use it” (Thoms 1849, 2). Much recent work in the digital humanities has focused on exploring ways to use networked computing to change scholarly practice. Tools like Zotero encourage open sharing of bibliographic data (Cohen 2008). Sharing of scholarly annotations has also been widely explored, and the Open Annotation Collaboration is working on developing standards and tools for making annotations interoperable across multiple tools (Hunter et al. 2010). Various projects have experimented with widening participation in humanist scholarship through “crowdsourcing” (Causer, Tonra, and Wallace 2012). Other projects have sought to banish the stereotype of the “lone scholar” through experiments in collaborative authorship of edited volumes (Dougherty and Nawrotzki 2012). And considerable effort has been made to increase access to the products of humanist scholarship through the creation of open access journals, monographs, and scholarly editions, most of these building upon a well-established humanities practice of digital editing and publishing.
These various projects address many aspects of the humanities research process. But notes have been curiously overlooked. Reference management tools like Zotero do enable shared note taking on individual documents, but this functionality is secondary to the management and sharing of bibliographic data. Other projects primarily focus either on managing research “inputs” such as annotations and transcriptions of source documents, or on “outputs”—finished scholarly products—whether these are books, databases, or virtual environments. With Editors’ Notes we are addressing the space in-between: the writing, organization, and linking of working notes, which are relevant to source documents but not necessarily tied to any specific document, and which may or may not become a formal finished product.
While not yet widely explored in the humanities, this space is one that has recently received much attention in the sciences. Both the National Institutes of Health and the National Science Foundation have instituted data sharing policies that encourage the researchers they fund to make
 
Figure 1.
Editing one section of a note on "Sanger and the Third ICPP."
available to other researchers the final research data underpinning their published work. “Open notebook science” presents a more radical vision in which not just the final research data, but all data generated during every stage of the research process is captured and made publicly available, either in “real time” or at the conclusion of a project. Advocates of this radically transparent approach to scientific practice expect it to result in more verifiable and reproducible research results, more efficient management and re-use of data on both the local and global levels, and new forms of algorithmic and “crowdsourced” research (Velden and Lagoze 2009).
The case for open notebook science rests upon the recognition that data from failed or incomplete experiments are potentially as important as those from successful ones. The problem is that current models of scientific publishing provide few incentives to publish such data, nor are there places to put it. Historians face similar problems, especially those engaged in long-running projects like documentary editions that may involve dozens of researchers working over decades. It is typical for a researcher to spend hours upon hours researching a topic only to find that she has duplicated work done years earlier and stowed away in a file cabinet or on a floppy disk.
William Thoms recognized back in 1849 that a major benefit of sharing working notes would be to induce researchers to “look over their own collections” and, by allowing others access, improve their own chances of finding past work (Thoms 1849, 2). In other words, a researcher need not be motivated by scholarly altruism to share her work. Yet Thoms believed that were sharing of notes to become cheap and frequent, then researchers would not hesitate to give help, not only to others engaged in similar lines of research, but also to “those who are going different ways, and only meet at the crossings” (Thoms 1849, 2). As this research commons grew, so would the opportunities for such crossings, and the net result would be more efficient research at the global level as well as the local.
Thoms’ vision is echoed in efforts by the scientific community to create publishing models that enable both finer-grained publication units (Mons and Velterop 2009; Groth, Gibson, and Velterop 2010; Mons et al. 2011) and new attribution practices (Nature Genetics 2007; Nature Genetics 2008; Giardine et al. 2011). These efforts recognize that citing published work helps drive scientific publication. They aim to expand the universe of citable work beyond the canonical research paper to units as small as individual statements. In doing so they hope to make visible the great iceberg of scientific work of which published papers are only the tip.
Scholarly footnotes such as those produced by documentary editors can be viewed as a form of nanopublication. One of the goals of the Editors’ Notes project is to give the status of individual publications to footnotes and the working notes that led to those footnotes. These notes can be complemented by machine-readable “factoids” (Bradley and Short 2005) about people, places, organizations, and events, drawn from open-access linked datasets (Heath and Bizer 2011). Scholars can assess and improve the quality of these factoids, connecting assertions to bibliographic descriptions of evidential resources and publishing “gold standard” datasets that meet their high standards (Shaw and Buckland 2011). The scholars’ notes provide context for the otherwise bare factoids, documenting why and to what extent they have chosen to accept them and the conclusions they have thus drawn.
Mons and Velterop (2009) make a distinction between “curated” and “observational” statements in scientific discourse. “Curated” statements take the form of records in trusted scientific databases. For example, a database recording known protein interactions may contain statements about these interactions along with metadata describing their context, conditions and provenance. In contrast, “observational” statements are factual statements such as “malaria is transmitted by mosquitos” that have not been formally recorded in any database but nevertheless are commonly asserted. A goal of nanopublication is to build knowledge bases that transform observational statements into curated ones.
History and the humanities mostly lack the databases of curated statements that exists in the sciences. The closest equivalents might be prosopographical or genealogical databases (Bradley and Short 2005; Church of Jesus Christ of Latter-day Saints 2012) or digital historical gazetteers (Elliott and Gillies 2011). These are exceptions that prove the rule, however, and the vast majority of factual statements in history and the humanities remain at the observational level.
To facilitate the shift from closed, personal or project-specific notes to openly shared notes, we’ve had to address a number of challenges. Editorial projects take varying approaches to structuring their research workflow, dividing labor among editors and student assistants, and standardizing on naming and citation practices. We have attempted to accommodate these varying work practices while creating opportunities for standardization across projects where it is desired. Our efforts to accommodate existing practices align with the broader objective of not disrupting ongoing research by integrating with research tools already in use. For example, Editors’ Notes integrates with the Zotero bibliographic data management platform, allowing researchers to access their existing bibliographic databases (Shaw, Buckland, and Golden 2012).
A major challenge has been developing a data model for notes that is flexible enough to accommodate a variety of working styles (Figure 2). We have tried to support fine-grained addressing and indexing of notes, allowing researchers to search for and link to notes taken on a single source document as it relates to one narrow topic. At the same time, we have sought to develop ways that researchers can work with aggregations of these small “atoms” in ways that feel natural to them. For example, notes taken while researching “the status of birth control in India in the 1930s” might reference dozens of documents encompass several more specific topics such as the Indian birth control activist Dhanvanti Handoo Rama Rau, birth control clinics, and the Bombay Municipal Corporation. Researchers can work with these notes in the context of the broader research task, or they can pull together all the notes about Rama Rau, whether or not these were taken in the course of researching “the status of birth control in India in the 1930s.”
 
Figure 2.
Part of the Editors' Notes data model. Notes, sections of notes, and topic summaries may cite document. Notes, sections of notes, documents, and document annotations are linked to the topics to which they relate.
Another ongoing challenge has been the question of how to bridge the gap between note-taking practices in scholarly research projects and those of curators of special collections and archives. The Joseph A. Labadie special collection of radical history at the University of Michigan helped us explore this question by providing thousands of notes created by Agnes Inglis, the first curator of the collection. The subject matter of these notes overlapped with that of the editorial projects involved, but these “curator’s notes” turned out to be useful less for their content per se, than for the metadata infrastructure (network of relationships among names and other topics) they produced. This realization helped catalyze our ongoing experimentation with incorporating linked data from libraries and archives.
Acknowledgements
We are grateful to the Andrew W. Mellon Foundation for funding “Editorial Practices and the Web” (http://ecai.org/mellon2010) and for the cooperation and feedback of our colleagues at the Emma Goldman Papers, the Margaret Sanger Papers, the Elizabeth Cady Stanton and Susan B. Anthony Papers, and the Joseph A. Labadie Collection.
References
Bradley, J., and H. Short (2005). Texts into Databases: The Evolving Field of New-style Prosopography. Literary and Linguistic Computing. 20 (Suppl). 3–24. doi:10.1093/llc/fqi022.
Causer, T., J. Tonra and V. Wallace (2012). Transcription Maximized; Expense Minimized? Crowdsourcing and Editing. The Collected Works of Jeremy Bentham. Literary & Linguistic Computing 27 (2). 119–137. doi:10.1093/llc/fqs004.
Church of Jesus Christ of Latter-day Saints (2012). FamilySearch. https://familysearch.org/.
Cohen, D. J. (2008). Creating Scholarly Tools and Resources for the Digital Ecosystem: Building Connections in the Zotero Project. First Monday. 13(8). http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/viewArticle/2233/2017.
Dickens, C. (1848). Dombey and Son. Boston: Bradbury and Guild. http://books.google.com/books?id=3r1yo6lx3BsC.
Dougherty, J., and K. Nawrotzki (eds). (2012). Writing History in the Digital Age. Trinity College web-book edition.http://writinghistory.trincoll.edu/.
Elliott, T., and S. Gillies (2011). Pleiades: an un-GIS for Ancient Geography. In Digital Humanities. held June 19-22 in Stanford, California. http://dh2011abstracts.stanford.edu/xtf/view?docId=tei/ab-192.xml.
Giardine, B., J. Borg, D. R. Higgs, K. R. Peterson, S. Philipsen, D. Maglott, B. K. Singleton, D. J. Anstee, A. Nazli Basak, B. Clark, F. C. Costa, P. Faustino, H. Fedosyuk, A. E. Felice, A. Francina, R. Galanello, M. V. E. Gallivan, M. Georgitsi, R. J. Gibbons, P. C. Giordano, C. L. Harteveld, J. D. Hoyer, M. Jarvis, P. Joly, E. Kanavakis, P. Kollia, S. Menzel, W. Miller, K. Moradkhani, J. Old, A. Papachatzopoulou, M. N. Papadakis, P. Papadopoulos, S. Pavlovic, L. Perseu, M. Radmilovic, C. Riemer, S. Satta, I. Schrijver, M. Stojiljkovic, S. Lay Thein, J. Traeger-Synodinos, R. Tully, T. Wada, J. S. Waye, C. Wiemann, B. Zukic, D. H. K. Chui, H. Wajcman, R. C. Hardison, and G. P. Patrinos. (2011). Nature Genetics 43:295–301. doi:10.1038/ng.785.
Groth, P., A. Gibson and J. Velterop (2010). The Anatomy of a Nanopublication. Information Services and Use. 30 (1-2). 51–56. doi:10.3233/ISU-2010-0613.
Heath, T., and C. Bizer (2011). Linked Data: Evolving the Web into a Global Data Space. Morgan & Claypool. doi:10.2200/S00334ED1V01Y201102WBE001.
Hunter, J., T. Cole, R. Sanderson, and H. Van de Sompel (2010). The Open Annotation Collaboration: A Data Model to Support Sharing and Interoperability of Scholarly Annotations. Paper presented at Digital Humanities, London, July 7–10.http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/html/ab-860.html.
Mons, B. and J. Velterop (2009). Nano-Publication in the e-Science Era. In Clark, T., Luciano, J. S., Marshall, M. S., Prud'hommeaux, E., and Stephens, S. (eds). Proceedings of the Workshop on Semantic Web Applications in Scientific Discourse (SWASD 2009). http://ceur-ws.org/Vol-523/.
Mons, B., H. van Haagen, C. Chichester, P.-B. Hoen, J. T. den Dunnen, G. van Ommen, E. van Mulligen, B. Singh, R. Hooft, M. Roos, J. Hammond, B. Kiesel, B. Giardine, J. Velterop, P. Groth, and E. Schultes. (2011). The Value of Data. Nature Genetics 43:281–283. doi:10.1038/ng0411-281.
Nature Genetics. (2007). Compete, Collaborate, Compel. 39(8):931. doi:10.1038/ng0807-931.
Nature Genetics. (2008). Human Variome Microattribution Reviews. 40(1):1. doi:10.1038/ng0108-1.
Shaw, R. and M. Buckland (2011). “Editorial Control over Linked Data.” Proceedings of the American Society for Information Science and Technology 48. doi:10.1002/meet.2011.14504801296.
Shaw, R., M. Buckland, and P. Golden (2012). Integrating Collaborative Bibliography and Research. Proceedings of the American Society for Information Science and Technology 49. doi:10.1002/meet.14504901245
Thoms, W. J. (1849). Notes and Queries. Notes and Queries s1-I(1):1–3. http://nq.oxfordjournals.org/content/s1-I/1/1.full.pdf+html.
Velden, T., and C. Lagoze (2009). Communicating Chemistry. Nature Chemistry 1:673–678. doi:10.1038/nchem.448.
A number of recent initiatives within the DH community promote the design, development, and implementation of digital tools aimed at speeding up, clarifying, or otherwise improving the research practices of humanities scholars. This year, the One Week | One Tool (OWOT) summer institute, funded by the National Endowment for the Humanities, resulted in the creation of Serendip-o-matic, a serendipity engine for digital research. This tool relies on users to feed it a selection of text or citations in order to create a list of keywords, which it then uses to find related information. The documents returned are taken from the Digital Public Library of America (DPLA), Europeana, and Flickr1. The participants of the 2013 OWOT initiative are not alone in their quest to design a digital tool geared toward enhancing the chance encounter with information, resources, ideas, research materials, and even people. Tim Sherratt, the manager of Trove at the National Library of Australia, often includes an element of chance in the tools he designs for use in the humanities. For instance, in his tool Trove News Bot, Sheratt (2013) allows users to interact with a Twitter stream by sending tweets with directions (such as #luckydip), which will return random results from the National Archives of Australia’s digital collection2. Similar tools have been developed that introduce serendipity into the collections of the DPLA and the British Library.
One motivation for the development of digital tools aimed at enhancing serendipity in digital environments comes out of the need to redesign and recreate the complexity of the research environment found in library stacks and archival collections. It is often argued that this complexity may be lost in digital environments, which are highly predictable and primarily based on keyword search. To what extent serendipity is reduced in digital search is debatable. Nonetheless, this perception of loss directly affects how scholars, and in particular humanities scholars, adopt and use digital tools. A study of historians’ research practices suggests that these scholars are skeptical of conducting their research exclusively in digital environments because they lack the ability to encounter key resources (primary and secondary materials) that could have a major impact on their research findings 3. In this study, the authors also found that historians were willing to experiment with digital tools, if these could recreate opportunities for encountering information. Hence, scholars perceive the discovery of resources, browsing, and chance encountering as central elements of their research practice that can, and need to, be supported online.
Outside of academia, a number of tools have emerged that try to introduce serendipity into the online experience. What is less clear from the literature is how to best support this process, as a wide range of approaches have been suggested ranging from interactions in social media 4, exploration in non-search related digital environments, and information search in digital environments 5. The approach most commonly taken is to introduce serendipity into the online information search experience; this is often done by introducing some element of randomness and thereby reducing the predictability of search results. An example of this approach is BananaSlug, which returns random results to a search query. Other approaches include reversing or modifying the ranking in which search results are presented online 6. This would draw attention to a different set of items because users commonly tend to investigate only the first and perhaps second pages of search results. All of these approaches aim at broadening “the search space, promoting encounters with items that might not, under existing algorithms, come to the attention of the user”. While the majority of digital tools aimed at promoting serendipity have emerged outside of the humanities, a series of tools have recently been developed with humanists in mind. These tools have garnered considerable attention in the field, but it remains unclear what element of serendipity they support. Part of the problem is the fact that the concept of serendipity is elusive 7 and difficult to pinpoint. Reducing it to the introduction of randomness, however, does not seem to be the best way to move forward, even though it is the one most commonly utilized. A second problem, and perhaps more concerning, is that scholars need to first understand that serendipity is not a one-dimensional concept but, rather, includes a number of related facets, which need to inform tool design and implementation. The present paper critically examines four DH tools that encourage serendipitous results and attempts to place these within current models of serendipity:
Serendip-o-matic (http://serendip-o-matic.com/)
Trove News Bot (https://github.com/wragge/trovenewsbot)
Mechanical Curator (http://mechanicalcurator.tumblr.com/)
DP.LA Bot (https://github.com/wragge/trovenewsbot).
As a basis for this examination, we have established the main facets of serendipity obtained from the extensive literature in Library and Information Science (LIS). Through this comparative study, we aim to accomplish two goals. First, there is a gap in understanding exactly what aspects of serendipity digital tools support. By merging the literature in LIS with tool design in DH, we hope to create greater clarity as to what aspects have been supported. Second, the results of the study will determine what future developments are needed to better support the work of humanists in digital environments.
Interviews with 20 history scholars inform the first phase of this study. These scholars indicated a desire for serendipitous encounters with material to remain a part of their research process after the integration of digital texts to their work. After discovering that historians were seeking new methods of information acquisition online, further interviews were conducted with DH scholars to see what methods they were using to browse information. The results of these two sets of qualitative data will be discussed and used to demonstrate a need for a serendipity tool within the DH community.
The second phase of this research is an in-depth exploration of the four information-discovery tools listed above. These tools will be examined in terms of Erdelez’s (2004) model of information encountering outlined below 8. After analyzing each tool carefully, follow-up interviews will be conducted with the creators of each tool to discuss their intentions for and reflections upon, the use of the tool by humanist scholars.
A wide range of models of serendipity have been developed relying on very different data sets and assumptions. Erdelez (2004) developed one of the first models and emphasized the experience of information encountering (IE), which she defined as a type of opportunistic acquisition of information. Erdelez’s (2004) utilized an experimental setting, where participants were asked to look for information related to a foreground problem and the researcher observed how they would react to information related to a background problem. As part of her model, Erdelez (2000) identified five elements:
noticing: the perception of encountered information;
stopping: the interruption of the initial information seeking activity;
examining: the assessment of usefulness of the encountered information;
capturing: the extraction and saving of the encountered information for future use;
and returning: the reconnection with the initial information seeking task.
In Erdelez’s (2004) model, a person is primarily focusing on the information needs related to a foreground problem. However, cues related to another problem, a background problem, may catch the person’s attention. If the person notices the cues and stops to examine the newly encountered information, then there is an opportunity for discovering unexpected resources. It is this process of noticing, examining, and capturing that digital tools try to emulated or support.
Each of the four tools reflects one or more aspects of the serendipitous process as outlined by Erdelez, (see Table 1).
  Noticing Stopping Examining Capturing Returning
Serendip-o-matic     ✓   ✓    
Trove News Bot   ✓   ✓   ✓   ✓  
Mechanical Curator   ✓   ✓   ✓ some  
DP.LA Bot ✓ ✓ some    
The tools listed above, with the exclusion of Serendip-o-matic, select materials randomly and then present these to followers on Twitter. Randomness, as we know, does not necessarily mean that serendipity will occur. These tools all provide links to places that users can go to receive extraneous materials in the hopes that something of interest will come their way.
Interestingly, the capturing element of these tools seems to be largely disregarded. Considering the DH community is acutely aware of the need to instantly capture digital documents and the associated metadata with citation tools (Zotero), none of the examined tools includes this element in their framework. This leads the authors to conclude that future design could focus on this element of capturing information, and could introduce a method that allows for the saving of documents so users can retrace their footsteps after returning to the initial task or foreground problem. Our critical analysis of various DH tools and how they support serendipity provides opportunity to further enhance these tools as well as a means to design additional tools that can impact the research practices of humanities scholars. 
References

1. CHMN. (2013). Serendip-o-matic: Let your sources surprise you. One Week | One Tool. Retrieved October 31, 2013, from serendip-o-matic.com/about
2. Sherratt, T. (2013). Conversations with Collections. discontents. Retrieved October 31, 2013, from discontents.com.au
3. Martin, K., & Quan-Haase, A. (2013). Are e-books substituting print books? Tradition, serendipity, and spportunity in the adoption and use of e-books for historical research and teaching. Journal of the American Society for Information Science and Technology. 64(5), 1016-1028.
4. Bogers, T., & Björneborn, L. (2013). Micro-serendipity: Meaningful coincidences in everyday life shared on Twitter. In Proceedings of iConference (pp. 196–208).
5. Quan-Haase, A., Burkell, J., & Rubin, V. L. (n.d.). The role of serendipity in digital environments. In Encyclopedia of Information Science and Technology. IGI Global.
6. Jansen, B. J., Spink, A., & Saracevic, T. (2000). Real life, real users, and real needs: A study and analysis of user queries and on the web. Information Processing & Management, 36(2), 207–227.
7. Merton, R. K. (2004). The travels and adventures of serendipity: a study in sociological semantics and the sociology of science. Princeton, N.J: Princeton University Press.
8. Erdelez, S. (2004). Investigation of information encountering in the controlled research environment. Information Processing & Management, 40(6), 1013–1025. doi:10.1016/j.ipm.2004.02.002
9. Erdelez, S. (2000). Towards understanding information encountering on the Web. In Proceedings of the 63rd annual meeting of the American Society for Information Science (pp. 363–371). Medford, N.J.: Information Today.

By DH 2014, Global Outlook::Digital Humanities (GO::DH) will be 18 months old. Old enough to have experienced its first growing pains but also old enough to have a sense of the opportunities that exist for promoting the globalisation of Digital Humanities research. This paper discusses the past and future of the Special Interest Group (SIG), concentrating particularly on what is generalisable: the lessons we have learned about collaborating in a multilingual, multiregional context (although this makes it in some sense a project report, the lessons are themselves highly relevant to and have implications for the community as a whole).

The basic premise behind GO::DH is both exciting and frightening. It is a community for Digital Humanities scholars around the world that encourages them to discover new work and new colleagues in regions and disciplines they might never have considered before. But in asking scholars to do this, it also asks them to work along some of our most controversial lines of division: language, culture, nationality, history, and income level. In a field that is famously and self-consciously “nice” (Scheinfeldt 2010), these are the places where our self-conception has been tested (and called into question) most vigorously (see especially Fiormonte 2012 on language and nationality; Babalola 2012 discusses some of the challenges that divide the use of the digital in High Income vs. Low Income Economies).

The English language and (arguably at least) Anglo-American disciplinary and rhetorical norms dominate the practice of our profession. This places non-native speakers of English and scholars working outside the Anglo-American academic context at an immediate disadvantage. In addition, as Fiormonte suggests, it probably also has led to the relative scarcity of such scholars in the disciplines'gatekeeping positions.

The interest in technology that defines our field, moreover, creates divisions the moment our collaborations attempt to cross the boundaries that distinguish high, mid, and low income economies (O’Donnell 2012a). As Babalola has shown, the kind of basic infrastructure that Digital Humanities scholars in High Income Economies take for granted either does not exist or can be disproportionately difficult and expensive to access in mid- and especially low-income economies. As she demonstrates, moreover, this problem is about more than download speeds or CPUs: many of our core approaches, assumptions, and methods of dissemination (from the outsized importance of conference presentations in our discipline to the use of crowdsourcing) imply an access to resources common only in High Income economies.

And finally there is the spectre of colonialism and development politics. Any project that brings scholars from high-income economies into close contact with scholars from mid and low income economies is going to run into questions of intention, history, and politics. Can such collaborations be collaborations of equals, in which all participants both teach and learn? Or must they inevitably resolve themselves into the more unequal relationship of donor and recipient? The initial impetus for GO::DH arose among scholars working in High Income economies who wondered about their lack of contact with scholars working in other regions (O’Donnell 2012b). Initially, this caused suspicion among scholars who live in or work with those in mid and low-income regions. What was the motivation for interest from the high-income scholars? To what extent would this new organisation be able to avoid replicating the status quo in the field at large, where those with resources determine the course of the collective effort.

Despite our initial fears about what could go wrong in such an endeavour, the first year of GO::DH's existence has been remarkably productive and relatively smooth. While there have been some misunderstandings (some of which are discussed in the other papers in the panel), there have been remarkably few problems. The SIG has successfully managed to integrate multi-lingualism into its discussion-list, which several threads being carried out in languages other than English. It has provided a framework for a remarkable number of projects and working groups—from Around the World of DH to the second Caribbean ThaTCamp, to the first Global DH conference (planned for this coming Spring in Mexico in association with RedHD. And it has even led to the formation of new groups, such as the proposed Portuguese-language DH organisation.

The techniques we have used in building this community, capturing the good will of its constituents while avoiding some of the more obvious potential problems offers wider lessons for the DH community. In this talk I will discuss some of the specific techniques—from ad hoc translation to the collaborative development of by-laws and executive positions that we have used to successfully build GO::DH over the last year into the relatively stable community it has now become.

Works cited
Babalola, Titilola. (2012). “The Digital Humanities and Digital Literacy: A Review of Digital Culture in Nigeria”. Lethbridge, Alta.

Fiormonte, Domenico. (2012). “Towards a Cultural Critique of the Digital Humanities.” Historische Sozialforschung / Historical Social Research 37 (3) (September): 59–76.

O’Donnell, Daniel Paul. (2012a). “In a Rich Man’s World: Global DH?” Dpod Blog. November 2. dpod.kakelbont.ca/2012/11/02/in-a-rich-mans-world-global-dh.

O’Donnell, Daniel Paul. (2012b). “Global Outlook :: Digital Humanities”. Lethbridge: Alliance of Digital Humanities Organisations. ubuntuone.com/187LiVZpJKwFNaRV0lZJeD.

Scheinfeldt, Tom. (2010). “Why Digital Humanities Is ‘Nice’.” Found History. May 26. www.foundhistory.org/2010/05/26/why-digital-humanities-is-%e2%80%9cnice%e2%80%9d.

Thinking with an Accent
Barbara Bordalejo, University of Saskatchewan

 

This paper explores the reasons behind various degrees of impact and development in Digital Humanities in the context of different countries, languages and cultures. It calls attention to the enormous gap between low, middle and high-income countries and offers avenues to continue a change that has already begun. Firstly, the paper considers where is Digital Humanities located as a discipline and how this might influence the phenomenon of inclusion/exclusion (particularly those that refer to language). Secondly, it focuses on practical problems in countries of low and middle-income. Thirdly, it suggests practical ways to construct a field that is truly inclusive and allows wider participation.

Locating the Digital Humanities
Matthew Kirschenbaum, in his article “What is Digital Humanities and What is Doing in English Departments?,” puts forward several possible reasons that explain why English Departments are one of the main spaces in which Digital Humanities is cultured. As Kirschenbaum explains it, the phenomenon is related to text as a relatively easy object to encode (“...by far the most tractable data type for computers to manipulate.”), the relationship between computers and composition, the relationship to editorial theory and the work by Jerome McGann in the 1980s, the advent of electronic literature, “the openness of English Departments to cultural studies,” and the development of e-readers, which have finally made it possible to have digital texts in the same form as we have digital music.

It seems that English and History are the main areas in which Digital Humanities posts are advertised. However, Roopika Risam (roopikarisam.com/2013/09/15/where-have-all-the-dh-jobs-gone) has pointed out that most of the recent DH jobs “go hand-in-hand with Rhetoric and Composition and literature positions.” In practical terms, this means that there is some substance to the Kirschembaum claim about Digital Humanities being easily located within English Departments (generally responsible for the teaching of composition and solid in the teaching of literature). If we, temporarily, accept this premise as true (that the English Department is a place in which we can find an important cluster of Digital Humanists) we have to consider the consequences of this: English Departments might have the natural tendency of hiring English Native speakers. An examination of the structure of the Alliance for Digital Humanities Organizations offers an insight on the distribution of power in Digital Humanities: in the steering committee, of 27 positions only Elisabeth Burr, Christoph Meister, Masashiro Shimoda, Oyvind Eide and Edward Vanhoutte are not British, North American (or more generally, native English speakers, as is Paul Arthur, from the Australasian Association for Digital Humanities). This is an observation of fact and not meant as a criticism of ADHO. In paper and pixels, the policy is of inclusion. ADHO has a committee in multilingualism and multiculturalism. However, those who come from the fringes either know or suspect that equity cannot be reached by decree (if this were possible, we would only have to pass laws forbidding poverty and the issue would be solved).

The Myth of Openness in DH
The state of affairs is surprising because it is widely believed among the high ranking digital humanists that the discipline boasts “...a culture that values collaboration, openness, nonhierarchical relations, and agility” and so “might be an instrument for real resistance or reform (59). Notably, this statement by Kirschembaum is also supported by Burdick et al. (“...however heterogeneous, the Digital Humanities is unified by its emphasis on making, connecting, interpreting, and collaborating” (24)), among others. The widespread belief that these values are at the core of Digital Humanities as a discipline, and that just by virtue of such values it is open and welcoming to all, may make us unable to see that the discipline fails to meet these standards.

Global Outlook :: Digital Humanities
Through GO::DH we have been exploring these and other issues and it is becoming clear that the discipline has a clear center and well defined margins. In April 2013, Frédéric Clavert published a note entitled “The Digital Humanities multicultural revolution that did not happen yet” (www.clavert.net/the-digital-humanities-multicultural-revolution-did-not-happen-yet). Domenico Fiormonte published the link and started a thread in which scholars from very different backgrounds weighed in on their own positions (listserv.uleth.ca/pipermail/globaloutlookdh-l/2013-April/000308.html). Clavert considers the disparity between the number of Anglo-American reviewers for the DH conference and the clear interest in the Francophone community. What defines the field as Anglophone, according to Clavert, is that the “...Digital Humanities, though claiming to be new and revolutionary, are structured in a very classical way for an academic field, where those who master the English language and the English speaking and impact factor based academic journals are the most visible (and the most quoted).” His statements were received with equal amounts of disbelief and approval. As if to confirm Clavert’s position, Craig Bellamy tried to dismiss this issue (listserv.uleth.ca/pipermail/globaloutlookdh-l/2013-April/000309.html). Although the exchange was academic and polite, it brought to light various issues we must face in a global context:

There are projects and initiatives being developed of which we are not aware because they are buried in a non-English context.
There are cultural factors that affect communication, of which one of the most important ones is the perceived imperialism of the imposition of English as lingua franca.
This same linguistic profiling is instrumental in the process of exclusion to which non-native speakers are subject because of the lack of native abilities.
At least, Fiormonte will agree with me in saying that a specific agenda to exclude non-native speakers from Digital Humanities is unnecessary: our linguistic limitations already prevent us from being considered central. Fiormonte’s example of Dino Buzzetti’s rise as an authority in the field (something that occurred not because of his excellent work, or the thirty years of publications in English and Italian, but rather because of the support of recognized scholars) painfully shows the lack of openness of this supposedly all-embracing field. As Fiormonte says: .“..it's not enough to have good ideas, work in the Northern [h]emisphere and write them in English: you need good sponsors and authoritative venues (listserv.uleth.ca/pipermail/globaloutlookdh-l/2013-May/000329.html).

Of course, the question here is how can this problem be solved. It is clear that we cannot both blame the establishment and ask for a solution coming from it. As I stated before, this is not a problem of ill will, but rather a misunderstanding of the determining factors that create these problems.

Minding the Gap
As a member of the executive of GO::DH, I consider that my role is to identify our problems to generate solutions to them. By organizing Spanish language THATCamps and delivering DH lectures in Argentina and Mexico, I have come to understand that to level the field we need not only to translate texts that currently are only available in English (the Cátedra Datos at the Universidad de Buenos Aires does exactly that: a team of seven people produce translations of up to date texts required as an introduction to Digital Humanities), but we also need to understand the reluctance of scholars to subject themselves to English as the exclusive language for communication and we have to allow for non-native standars to be considered when submissions to conferences or journals are made by non-native speakers. We should not forget that those who speak or think with a foreign accent, are able, at least, to speak another language.

The Possibilities and Pitfalls of Global Digital Humanities
Roopika Risam, Salem State University

 

Over the past two years, much has been made of the role of cultural critique in the digital humanities, particularly around silences and absences of race, gender, sexuality, and so forth in the digital humanities (Liu 2012; McPherson 2012; #transformDH Collective 2011; Lothian and Phillips 2013; Bailey 2011). Yet, these conversations have taken shape through a United States-centric frame of reference that often elides the larger picture of the digital humanities: its global frame. Taking up the global scope of the digital humanities, however, is to take up imbalances of power that operate in colonialist frames, visible in the dominance of United States and Western European voices within the digital humanities community writ large. Indeed, it requires heightened attention to cultural critique through a postcolonialist framework.

Reactions within the digital humanities community to cultural critique (Whitson 2012, Reid 2011) renders such critiques as problems. Indeed, the problem, as the narrative goes, lies not in gaps within the digital humanities but with the practitioners who dare to raise these issues. In this talk, I will examine the "problem" of the global in the digital humanities. I begin by outlining the stakes of attending to global participation in the digital humanities. These stakes are both intangible and tangible and include radically reimagining loci of the digital humanities beyond the current map that locates the United States and Western Europe at the center, strategizing models for global partnerships outside of neocolonial frames, and developing resources for fostering a truly global digital humanities.

As the stakes imply, attending to the global within the digital humanities requires a two-pronged approach that accounts for the complexities of both theory and praxis. Engaging these concerns, my talk provides a case study of theoretical and practical approaches: Global Outlook :: Digital Humanities (GO::DH) and Postcolonial Digital Humanities (DHPoco). GO::DH is a special interest group (SIG) of the Alliance of Digital Humanities Organizations. GO::DH is dedicated to hacking barriers that prohibit collaboration across both disciplines and geographies (Global Outlook :: Digital Humanities 2013). Barriers include telecommunications, financial resources, human labor, and language (O'Donnell). Focusing on stated goals of "discovery, community-building, research, and advocacy," GO::DH works to foster communication and collaboration on a global scale. DHPoco is both a movement and emergent subfield of the digital humanities, invested in decolonizing digital spaces, making space for colonial critique and anti-colonial thought in the digital humanities, and writing alternative genealogies of the digital humanities (DHPoco 2013). By examining the work of GO::DH and DHPoco, I make the case for continued attention to and development of theoretical and organizational spaces for fostering the global digital humanities, as well as its benefits to the digital humanities community as a whole.

References
Moya Bailey (2011), "All Digital Humanists Are White, All Nerds are Men, but Some of Us Are Brave," Journal of Digital Humanities 1.1.

Alan Liu (2012), "Where Is Cultural Criticism in the Digital Humanities" Debates in the Digital Humanities. Minneapolis: University of Minnesota Press.

Tara McPherson (2012), "Why Are the Digital Humanities So White?" Debates in the Digital Humanities. Minneapolis: University of Minnesota Press.

Daniel O'Donnell (2012), "In a Rich Man's World: Global DH?," 2 November 2012, dpod.kakelbont.ca/2012/11/02/in-a-rich-mans-world-global-dh

Global Outlook (2013) : Digital Humanities, "About,"www.globaloutlookdh.org/about

Alexis Lothian and Amanda Phillips (2013), "Can the Digital Humanities Mean Transformative Critique?" Journal of E-Media Studies 3.1.

Alex Reid, "Alan Liu, Cultural Criticism, the Digital Humanities, and Problem Solving?"

Roopika Risam and Adeline Koh (2013), "Mission Statement," Postcolonial Digital Humanities, dhpoco.org/mission-statement-postcolonial-digital-humanities/#transformDH Collective, "A Call to Action," 26 October 2011 www.hastac.org/blogs/amanda-phillips/2011/10/26/transformdh-call-action-following-asa-2011 Roger Whitson, "Does DH Really Need to Be Transformed?," 8 January 2012, www.rogerwhitson.net/?p=1358

Global Challenges, Local Interpretations. An analytical perspective about DH in Spain
Paul Spence (King’s College London) and Elena Gonzalez-Blanco (UNED, Spain)

 

Digital Humanities has a long history in the Spanish-speaking world, with landmark projects like Admyte and BOOST/Philobiblon emerging in the 1970s and 1980s (and later the Miguel de Cervantes Digital Library), followed by years of isolated research projects (in Spain, the focus for this paper, these have often had a strong philological focus, but also encompass bibliographic studies, multimedia and other forms of digital scholarship) and a number of experiences in teaching, including the now defunct online Masters programme in Digital Humanities at the University of Castilla-La Mancha, Spain, which ran with some success for a few years. But as is common with non-Anglophone traditions, the rich history in digital humanities in Spain is under-represented internationally, and in this particular case has not even achieved a consistent institutional presence in Spain. 2013 was a milestone in the history of digital humanities in the Spanish language (Baraibar 2013), and Spain saw a number of events and initiatives,iv including the inaugural conference (‘Digital Humanities: challenges, achievements and future perspectives’) in July 2013 of the newly-formed association Humanidades Digitales Hispánicas (HDH, ‘Hispanic Digital Humanities in English’). HDH joins a broader articulation of Hispanophone digital humanities organisations, which started with the Mexican association RedHD, and reflects a broader flowering of initiatives in Spain, which however do not conform a homogenous whole.

A survey of digital humanities in Spain
The term ‘humanidades digitales’ is a trending topic in Spain, and in this paper we explore its manifestations, its tensions and its challenges. A wide-ranging history of digital humanities in Spain is much needed, both to communicate the historic and disciplinary depth of the field in the country and to help give substance to the development of the field in Spain itself. What is needed as a prelude to this, however, is stable documentation of the field as it currently stands, and what we have done is to carry out a broad survey of digital humanities activities in Spain with a view to making it available for further research by others. In our research, we have surveyed conferences, publications, official and unofficial websites and blogs in Spain, in addition to broader international resources.

Digital humanities in Spain still suffers from relative invisibility at an international level in digital humanities, but the evidence changes in different settings – for example, Spanish representation is relatively prominent in the results of the ‘Who are you Digital Humanists?’ survey carried out by researchers from OpenEdition, with 40 respondents (compared to 49 in the UK, which has a population 30% higher). In part this is due to more general issues with how digital humanities is defined and represented internationally, but self-identification is a major issue – many researchers are interested in the digital humanities without necessarily feeling themselves to be represented by the label. While there is a relatively strong theoretical tradition often connected to conventional humanities disciplines (digital philology, digital art) or information science, there is not a strong history of tool-building that is more prevalent in other regional contexts.

Communities, definitions, labels
Domain-specific communities have played important role in developing awareness of digital scholarship, although they may not self-identify as digital humanities entities – some of the greatest progress has been made in groups such as TC/12, a research project with major funding to explore texts and research tools in Spanish Golden Age theatre, or CHARTA, an international research network which both provides guidelines for editing Spanish archival texts from the twelfth to nineteenth centuries, although in some cases the engagement with technology is uneven or still under negotiation (Spence et al 2012). Sometimes digital humanities finds its expression more comfortably in MediaLabs, as is the case of the MediaLab of Salamanca, which recently organised a series of seminars, or broader scholarly initiatives in social and human sciences, such as GRINUgr, which examines digital humanities from the prism of digital culture.

If in 2006 Isabelle Leibrandt could ask if Digital Humanities was a science fiction or an imminent reality (Leibrandt 2006), no-one could argue of its existence as such now. The question, and not only in a Spanish context of course, is what is it? Is it just a convenient label which allows each person to project their own fantasies, as Olivier le Deuff puts it (Le Deuff 2012), or is it a set of fully-formed academic practices? Labels may not be particular illuminating here, but the pattern in Spain, where any label has been used at all, is to use the term ‘informática humanística’, roughly equivalent to ‘humanities computing’ in English, and which is probably most closely influenced by the Italian usage ‘informatica umanistica’. Unlike in Italian, where the historic term has persisted (as evidenced in the name of the Italian association, evidence of the use of the term largely disappears in around 2008, when we gradually see the emergence of ‘humanidades digitales’, an almost direct translation of ‘digital humanities’.

In a blog on the relationship of the digital humanities to information science, Luis Rodriguez-Yunta asks why we use the term ‘digital humanities’ and not just ‘digital scholarship’. In his own response, he points to the academic, social and cultural demand for accessible and humanities-focused sources/documentation, but also, crucially, the role of the humanities in defence of the human, implying a ‘humanisation’ of technology.

Spain has suffered especially badly during the global economic crisis, with exceptionally high unemployment figures and drastic cut in funding, which has in turn heightened the sense of crisis in the academy, where the humanities are perceived as being especially vulnerable to criticism, and some have perceived this crisis as an opportunity to redraw traditional disciplinary lines. The Digital humanities have a strong background in the philological tradition in Spain – indeed the two initial seminars which led to the creation of the HDH association in Deusto and La Coruña with strong philological characters. But there are also strong voices for a recalibration of the humanities, which in the words of José Manuel Lucía, can use digital technologies to recuperate a social space it gradually lost in the twentieth century (Lucía, 2012).

Focus for the future
The HDH conference in July 2013 was the focus for a number of key debates affecting Spanish digital humanities at this time, including the role of teaching and systems of academic value and credit. The HDH association has filled an important void in Spain, providing formal structures for digital humanities, offering an organisational focus and functioning as a mechanism to lobby national academic institutions responsible for the formal evaluation process. We will end our paper by exploring the crucial role of HDH and other initiatives in helping the digital humanities to establish itself in Spain, and describe efforts to create an academic centre of digital humanities in Spain, which up until now has not existed, with a focus on research, teaching and general support for digital humanities practitioners.

References
Baraibar, Álvaro (2013) ‘Buenos tiempos para las Humanidades Digitales en español’ Blog dhd2013.filos.unam.mx/porvistadeojos/2013/05/20/buenos-tiempos-para-las-humanidades-digitales-en-espanol

Dacos, Marin (2013) ‘La stratégie du Sauna finlandais’ Blog blog.homo-numericus.net/article11138.html

Galina Russell, Isabel (2011) ‘¿Qué son las Humanidades Digitales?’ in Revista Digital Universitaria Vol. 12, No.7, www.revista.unam.mx/vol.12/num7/art68/index.html

González-Blanco, Elena (forthcoming). ‘Las Humanidades Digitales vistas desde España’

Le Deuff, Olivier. (2012) Humanisme numérique et littératies, Semen n° 34, p.117-134

Leibrandt Isabel “Humanidades, ciencia ficción o realidad inminente?” www.ucm.es/info/especulo/numero33/humadigi.html

Lucía, José Manuel (2012). Elogio del texto, Madrid, Fórcola

Romero, Esteban‘Humanidades Digitales en investigación, docencia y universidad’ presentation at estebanromero.com/2013/10/humanidades-digitales-en-investigacion-docencia-y-universidad

Spence, P., Isasi Martinéz, C., Pierazzo, E. & Vincente Miguel, I. (2012) Nuevas perspectivas para la edición y el estudio de documentos hispánicos antiguos. Sánchez-Prieto Borja, P. & Torrens Alvarez, M. J. (eds.). Bern: Peter Lang

Spence, Paul (forthcoming). Report on first Digital Humanities conference in Spain (forthcoming in Japanese)

Zotero group for ‘humanidades digitales’ https://www.zotero.org/groups/humanidades_digitales

www.admyte.com

bancroft.berkeley.edu/philobiblon/history_en.html iii www.cervantesvirtual.com

hd.paulspence.org/recursos/hh-dd-es/

hdh2013.humanidadesdigitales.org

www.humanidadesdigitales.com

humanidadesdigitales.net

tc12.uv.es

www.charta.es

medialab.usal.es

grinugr.org

Associazione per l’informatica umanistica e la cultura digitale, AIUCD www.umanisticadigitale.it
1. Introduction
Today we have more information at our fingertips than at any other time in human history. The problem is no longer finding information, the problem is being overwhelmed with the amount of information. This is no different in the realm of the digital humanities.  Information on people, projects, resources, methods, and tools exists in quantity everywhere we look, and yet we still have difficulty finding what we need. This paper will describe a transatlantic effort on the part of DiRT in the United States and DARIAH in Europe to construct a taxonomy of scholarly methods, that can be used not only to organize single collections of DH information and resources but also to allow these collections to interface with each other, creating a web of linked data that can be effectively searched for information across distributed collections. DiRT and DARIAH are not trying to impose a restrictive, monolithic scheme on DH; rather, our goal is to construct a lightweight, basic taxonomy of higher order goals and first-order methods that can be easily expanded in all directions by linking lower order techniques to multiple goals and/or methods to create machine-readable paths among the various resources. In building this taxonomy, we heavily rely on input and feedback from the digital humanities community. Still, at least for the intended use cases, we believe a stable taxonomy has advantages over more open, folksonomy-based solutions.

The taxonomy as it exists now is based upon three primary sources: 1) the arts-humanities.net taxonomy of tools of DH projects, tools, centers, and other resources, especially as it has been expanded by digital.humanities@oxford in the UK and DRAPIer in Ireland; 2) the DiRT collection of digital research tools, re-launched under Project Bamboo in the US but now continuing on after the end of that project; and 3) the DARIAH ‘Doing Digital Humanities’ Zotero bibliography of literature on all facets of DH. These resources were studied and distilled into their essential parts, producing a simplified taxonomy of two levels: 8 top-level goals that are broadly based on the steps of the scholarly research process and a number of general methods under these goals that are typically used by scholars to achieve these research goals. The updating of the taxonomy and the definition of the types of relationships to be described in the resulting ontology will be carried out by a joint working group in the DARIAH-EU and the NeDiMAH projects in Europe, which will conduct large scale desk and field research into scholarly practice to determine how best to describe the relationships between and among the goals, methods, and techniques of scholarly practice.  The future expansion of this organizational system will not be as a hierarchical taxonomy but, instead, as a linked ontology as lower-level techniques are attached to one or more methods, linking all the existing entities in the ontology together. The projects and collections that use this schema will play an important role here: as resources are added to these collections and linked to the taxonomy, the resulting ontology will grow in complexity.  This complexity will be more help than hindrance precisely because it will be a machine-actionable complexity.  Computers will traverse this web of relationships for us, only bringing back results that are closely related to our needs.

This may seem excessively optimistic, but this paper will support these claims by describing three very different types of resources that have used and expanded the taxonomy not only to improve the findability within their own collections but, more importantly, to link to each other in a machine-actionable way. These resources are the DiRT directory of digital tools, the DARIAH ‘Doing Digital Humanities’ bibliography, and the DARIAH-DE service-oriented project portal. A brief description of each of these collections and how they will profit from this taxonomy/ontology follows.

2. DiRT
DiRT (Digital Research Tools, http://dirt.projectbamboo.org) is a longstanding US-based directory for scholars interested in digital tools, which provides basic information about software that can facilitate the research process at different stages. The classification of tools by category has always been fundamental to DiRT: in its earlier incarnation as a wiki, wiki pages each corresponded to a category of tools, and the tools were presented in a list on the page. In 2011, DiRT was rebuilt using the Drupal content management system, which allowed information about each tool to be stored in a structured manner that enables faceted search and browsing. While users can now create complex queries on DiRT (e.g. using operating system and price to narrow their results), tool categories remain the primary way of navigating the site.

With support from the Andrew W. Mellon Foundation, DiRT is currently undergoing a new phase of development, with the goal of making information about digital tools available outside the DiRT directory itself. Since its inception, DiRT has used its own ad-hoc list of categories. All tools must belong to at least one category, though these categories can be supplemented with user-generated tags. The shortcomings of DiRT’s categories list can be illustrated through the example of OCR tools-- some are classified as “transcription”, others as “conversion”, and while neither is ideal, both are a reasonable approximation given the other options. Replacing DiRT’s former categories with the taxonomy will improve the consistency and quality of the data, and also provide a shared facet that can connect DiRT’s tool data with information provided by other projects, once DiRT’s contents are made available using RDF.

3. 'Doing Digital Humanities' bibliography
Another resource directly connected to the taxonomy is DARIAH-DE's ‘Doing Digital Humanities’ bibliography. The bibliography can be accessed on Zotero (www.zotero.org/groups/113737) or on the DARIAH-DE portal (https://de.dariah.eu/bibliography). Like DiRT, the bibliography is one of the seed activities for the taxonomy at the same time as being one of the already defined use cases, representing the application domain of making medium-sized collections of bibliographic references discoverable. This Zotero-based bibliography offers suggestions for introductory readings as well as more in-depth coverage of research literature in various areas of digital research, teaching and infrastructure planning in and for the humanities. The bibliography is carefully curated collaboratively, is freely accessible, currently has around 800 entries, and is being updated continuously.

Right now, the bibliography is already divided into thematic collections based on the "goals" defined in the taxonomy. Each collection, hence, covers one prototypical aspect or goal of the research process in the humanities as it is practiced with digital tools, methods and data. In addition, all entries in the bibliography are discoverable through keywords covering, on the one hand, typical research methods and activities in the humanities, and on the other hand, a wide range of objects of research. The current closed list of keyword represents an early draft version of the taxonomy described here.

Once a first stable version of the taxonomy is available, the bibliography's keyword implementation will be updated. Sharing a keyword system with other projects will make it easier for users to find related resources. And the public documentation of the taxonomy, including concise scope notes for all methods and techniques, will make the bibliography's keyword-based search more transparent and increase its usability.

4. DARIAH-DE portal
A third use case aims to examine the taxonomy as a functional structure for DARIAH-DE’s service-oriented website, the DARIAH-DE-Portal. Launched in a first version in May 2013, it will receive a makeover in the early stage of the upcoming German DARIAH II project scheduled for March 2014 that is based on the taxonomy.

The website is designed to offer a wide range of services concerning Digital Humanities in Germany and addresses both researchers who already work digitally and those seeking information or advice. The services provided are as heterogeneous as the DH landscape. They cover informational aspects on specific research projects, information on DH Centers, Bachelor/Master Programmes and tools as well as their documentation, tutorials and teaching materials. Services offered by DARIAH-DE (like the embedded bibliography mentioned above, the DARIAH-DE Working Papers, or hosting services and a developer’s portal) are complemented by external resources like blogs and a DH-calendar (a cooperation with calenda.org being currently on its way).

The variety of this content leads to multi-purpose requirements that enable a flexible access to information relevant to individual users. This use case meets that challenge by implementing the taxonomy in RDF, thus interlinking content and making it multi-purpose. In that way, the taxonomy will function as a ‘meta-service’ that meets the interests of an active and interlinked community, that visualizes Digital Humanities and promotes its results.

 5. Conclusion
The purpose of this talk is not to convince the audience that we in DiRT and DARIAH have all the right answers.  Instead, it is to continue a conversation about the importance of ontologies for managing the over-abundance of DH information, present our own work on this problem and our approach to gathering and incorporating community feedback, in hopes of spurring further work in this area.

References
Anderson, Sheila; Tobias Blanke; Stuart Dunn. (2010). Methodological Commons: Arts and Humanities E-Science Fundamentals. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 368, no. 1925 (2010): 3779 –3796. http://rsta.royalsocietypublishing.org/content/368/1925/3779.abstract.

Benardou, Agiatis, Panos Constantopoulos, Costis Dallas, and Dimitris Gavrilis. Understanding the Information Requirements of Arts and Humanities Scholarship. International Journal of Digital Curation 5, no. 1 (June 22, 2010): 18–33. doi:10.2218/ijdc.v5i1.141.

Borgman, Christine (2010). Scholarship in the Digital Age : Information, Infrastructure, and the Internet. Cambridge: MIT Press.

CLIR (Commission on Cyberinfrastructure for the Humanities and Social Sciences). (2006). Our Cultural Commonwealth: The Report of the American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities and Social Sciences. New York: American Council of Learned Societies, 2006.

Gasteiner, Martin, and Peter Haber, eds. (2010). Digitale Arbeitstechniken für die Geistes- und Kulturwissenschaften. Vienna: UTB. http://www.utb-shop.de/digitale-arbeitstechniken.html.

Reiche, Ruth; Rainer Becker; Michael Bender; Matthew Munson; Stefan Schmunk; Christof Schöch. (2014). Verfahren der Digital Humanities in den Geistes- und Kulturwissenschaften. DARIAH-DE Working Papers Nr. 4. Göttingen: DARIAH-DE (to appear). Preprint: https://dev2.dariah.eu/wiki/download/attachments/2295542/M223_DH-Verfahren.pdf.

Siemens, Ray; John Unsworth; Susan Schreibman, eds. (2004). A Companion to Digital Humanities. Hardcover. Oxford: Blackwell. http://www.digitalhumanities.org/companion/.

Unsworth, John. (2000). Scholarly Primitives: What Methods Do Humanities Researchers Have in Common, and How Might Our Tools Reflect This? London: King’s College London. http://www3.isrl.illinois.edu/~unsworth/Kings.5-00/primitives.html.
This paper presents findings of an Andrew W. Mellon Foundation-funded project conducted at Penn State University in the period April 2012-June 2013. The project explored scholarly workflow of the Penn State faculty across the sciences, humanities, and social sciences, focusing on the integration of digital technologies at all stages of a research lifecycle—from collecting and analyzing data, over managing and storing research materials, to writing up and sharing research findings. The study also examined scholars’ attitudes towards the use digital technologies in their research practice, as well as the level of institutional support available to them in developing and implementing digital research skills. This paper harvests a comparative multidisciplinary perspective of our study in order to explore specificities of humanities scholars’ digital workflow, providing a ground to identify and develop a software and service architecture that supports those practices. Therefore, while focusing on current findings, the paper briefly highlights the future trajectory of our study, as well as planned next steps regarding technological initiatives aimed at addressing management of digital scholarly workflow in humanities scholarship.
The study was comprised of two research phases, each of which focused on a specific set of research questions and goals. The first phase included a web-based survey that consisted of twenty-five questions, which, in addition to demographic information, included queries about data searching, storing, citing, sharing, and archiving practices, as well as about scholars’ experiences in using digital research tools and resources. A total of 196 faculty (59% female / 41% male) completed the survey, most of them tenured faculty, with fixed-term (non-tenure track) faculty, and tenure-track faculty following. The Humanities tended to have older respondents (over 40 years of age), while the sciences and social sciences faculty skewed lower in age.
The second phase of the study included a set of face-to-face ethnographic interviews. A total of twenty-three scholars volunteered to participate in the interviews, and they were equally divided along the lines of disciplinary profiles, academic ranks, and gender: 13 were faculty in the humanities and social sciences (HSS) and 10 in the sciences; 11 were tenure-track and 12 tenured faculty; 13 were female and 10 men. The interviews were semi-structured and, on average, lasted an hour. The interviewees were audio-recorded and then transcribed by a professional transcriptionist. The interview transcripts were first coded into broader categories (nodes) by two independent coders.
We then proceeded with focused coding, where the categories into which the data were originally coded had additionally been refined for relevant patterns, themes, and topics.
The results of our study show that digital technologies have different roles and levels of integration at various phases of scholarly workflow. For instance, digital tools are actively used for finding, storing, and archiving research materials. This finding is true across disciplines, although certain disciplinary differences can be traced. For instance, while the majority of respondents across disciplines (92%) actively store research materials important to them, humanities scholars reported the highest percentage of lost and inaccessible research files; predominantly (27%), inaccessible files resulted from failing to migrate research materials from obsolete to contemporary digital formats. Similarly, while searching for information electronically is a standard, daily practice of our respondents regardless of their disciplinary background and/or level of technical proficiency, humanities scholars commonly prioritize the Penn State library catalog as their search and access points, while scholars in the sciences prioritize Google Scholar. Our results also show, however, that across disciplines, the path towards finding information commonly starts with Google Search and Google Scholar, especially for scholars engaged in discovery search, which reaffirms results of other recent studies indicating the increasing prevalence of commercial over academic services for scholars’ information search (see: Nicholas et al., 2011; Kortekaas, 2012)
The results of our study further show that, in the phases of data collecting and analysis, the use of digital technologies significantly differs across disciplines. Our respondents in the science commonly noted that their work would be impossible without digital technologies, and scholars in the social sciences indicated digital tools and methods becoming ‘a new normal’ in their data gathering and analysis practice. Contrary to this, respondents in the humanities, with a few exceptions, implied the lack of digital technology use in those phases of their research process. Parallel with this, however, they indicated awareness of digital tools and methods that could facilitate their analytical practice, suggesting the lack of available training and time as key impediments to developing literacies needed for mastering those tools.
Disciplinary differences were evident in the activities of data sharing and communication, particularly in the use of social media. With regard to data sharing, two thirds (63%) of scholars in the sciences indicated that they actively share their research data, while a nearly identical percentage of the humanities scholars (69%) indicated opposite practice. Yet we found that in addition to disciplinary differences, differences in academic standing also influence data sharing practices of our respondents, with tenure- track faculty being more protective of their data than tenured scholars. We further observed widespread use of digital technologies in scholarly communication across disciplines, with a noticeable difference being frequent social media use among the humanities scholars, and nearly non-existent use among respondents in the sciences.
Annotating and reflecting emerged as research phases where the use of digital technologies is most idiosyncratic, that is, based on scholars’ personal preferences rather than the level of technical skills or availability of digital tools. With regard to citation, the use of citation management programs was somewhat higher in the sciences than in the HSS (55 % vs. 30 %), but the overall level of digital technology use in this research activity was lower than in other phases of the research workflow.
Conceptually, our results illustrate various ways in which integration of digital tools in one phase of the research processes influences other segments of the workflow. For example, scholars’ full reorientation on electronic search and access produces an abundance of collected materials, requiring adjustments in researchers’ storing, organizing, and archiving practices. As some of our respondents observed, integration of digital tools into their search activities resulted in a complete breakdown of their systems for organizing information, developed for print-based materials. Therefore, while implementation of digital tools into one phase of the workflow might be rewarding, it might also become a challenge in other phases of the workflow. This is particularly relevant in the perspective of tool development, implying that digital research tools should be designed to support a continuous research workflow instead of separate and disconnected activities.
Our findings also suggest that in a workflow of a digital scholar technical rather than traditional methodological expertise shapes interconnectedness among phases of the workflow. In our study, greater level of workflow interconnectedness was observed among scholars in the sciences, who tend to be more technologically savvy than scholars in the humanities and social sciences. This, as well as our previously mentioned study findings, indicates a significant scope of disciplinary differences with regard to the use of digital technologies in scholarly work. Broadly conceived, these disciplinary differences can be conceptualized as inherent and acquired. As an example of inherent disciplinary differences we could understand data privacy requirements, which widely differ across disciplines and, as our findings show, significantly determine the type and level of digital technology use. Acquired differences on the other hand can be observed in a set of habits and assumptions rooted in a particular community of practice. Technical architecture of digital research tools needs to support specific disciplinary needs in ways that address both inherent and acquired disciplinary differences. Data storage and management, for instance, has been identified as a dire problem across disciplines, but with distinctive disciplinary needs.
The next phase of our study (2014-2016) will be devoted to developing a digital research tool for humanities scholarship using Zotero as a test platform, in collaboration with George Mason University. Based on the results of the first phase of our study, we will focus on unifying several phases of the research workflow, and facilitating elements such as better integration of finding and archiving into the scholar’s online path. Discovery must be better finessed for the end user, and search and retrieval should be fully integrated into an interface that also allows annotation, organization, and archiving of research materials. Also, since the loss of information among the humanities scholars is significant, there is a need to build into the research workflow easy strategies for users to self-archive their work in storage services that are inherent to the individual or the institution. Optimizations to connect the institutional repository within Zotero, as well as expose references and metadata within uploaded PDFs will be explored. 



    
        
            
                How about Tools for the whole range of scholarly activities?
                
                    
                        Bradley
                        John
                    
                    Department of Digital Humanities, King's College London, United Kingdom
                    john.bradley@kcl.ac.uk
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    digital tools in the humanities
                    user experience
                    tools for cognition
                
                
                    digital humanities - nature and significance
                    interface and user experience design
                    software design and development
                    user studies / user needs
                    other
                    English
                
            
        
    
    
        
            The production of digital tools and the debate about how they fit with the humanities has a long history. Much of it has the character of frustration. Many digital tools have been built, but almost all them have had little or no effect upon the humanities as a whole. This has been a theme of mine for many years (see Bradley 2005; 2009), but the phenomenon has certainly been noted by many others. Edwards reports that Martin Mueller conducted a distant reading of the titles of monographs and articles in scholarly journals to measure mainstream interest in what he terms ‘literary informatics’ and concluded that it remains a niche activity with ‘virtually no impact on major disciplinary trends’ (Edwards, 2012, 216). The 2005 Summit on Digital Tools at the University of Virginia reports, ‘Only about six percent of humanist scholars go beyond general purpose information technology and use digital resources and more complex digital tools in their scholarship’, and I suspect that no one thinks that the number is much higher today. More recently, in an article titled ‘Building Better Digital Humanities Tools,’ Gibbs and Owens note that ‘despite significant investments in the development of digital humanities tools, the use of these tools has remained a fringe element in humanities scholarship’ (Gibbs and Owens, 2012, abstract). 
            Gibbs and Owen present a User Experience (UX) perspective drawn largely from interviews of humanist scholars, and tool builders will definitely benefit from many of the observations that they make. However, I believe that the challenge is more fundamental than trying to find out what potential users say they want. If we are looking for a sea change in humanities scholarship driven by the uptake of our tools, we should, instead, remember two innovators who both achieved one through their innovations and who understood the limitations of the UX user-oriented perspective. Henry Ford is reputed to have said, ‘If I had asked people what they wanted, they would have said faster horses’, and Steve Jobs claimed, ‘It’s really hard to design products by focus groups. A lot of times, people don’t know what they want until you show it to them’ (Ott, 2013). Perhaps a perspective on why digital tools for humanists are not being taken up needs to go outside of a set of (albeit valuable) UX observations.
            So, if the UX perspective is not the whole story, what else is there? Here I take a different approach, centered on the way many DH toolmakers think of the word ‘tool’, and attempt to broaden this idea of tool to fit it better with more of what goes on in the humanities, and therefore to make digital tools more influential there. My contention is that we need a more fundamental rethinking of tools and their function.
            I start with Ramsay and Rockwell’s observation that ‘Digital artifacts like tools could . . . be considered as “telescopes for the mind” that show us something in a new light’ (Ramsay and Rockwell, 2012, 79). The striking thing about this metaphor is that it draws parallels with a particular kind of tool—the scientific instrument. One way of seeing the assumptions that come loaded with this categorization is to look into the history of the conception of the telescope itself as a tool within science: something Malet (2005) reviews in his discussion of the early conceptualisation of telescopes in the 16th and 17th centuries. As a tool for ‘knowing’ and ‘discovery’—as something that revealed problems rather than solving them, opening discussions rather than closing them (238)—Malet claims that it became evident that, like this kind of humanities-oriented digital tool, the telescope needed to find a theoretical basis for what it did. Malet tells us of the evolution of this thinking over the 16th and 17th centuries—beginning with a view of the telescope as being essentially theoretically transparent: first as a kind of intimate extension of the eye (Malet, 2005, 245) or a kind of prosthesis for human vision (260) but eventually, starting with Kepler, to see the need for a theoretical basis that drew on optics and perspective, as to why materials seen through the telescope could be taken to have veracity (261). Of particular relevance to us is that Malet makes evident the separation of a theoretical basis for the telescope (drawn from optics and perspective) from a theoretical basis for what it revealed (such as the moons around Jupiter, or the lunar craters).
                1 How did these two very different research domains successfully connect with the telescope? DH toolmakers need a similar discussion: How does the theoretical basis of text analysis tools actually successfully connect and enrich discussions in the very different theoretical world of modern literary criticism?
            
             Although viewing humanities digital tools as analogues of scientific instruments is one paradigm, there are others, and we see alternatives in James Feibleman’s (1967) classification scheme for (nondigital) tools. The closest category he has to Malet’s 
                tools for knowing is his ‘tools for receptors’: the class of tools that ‘like spectacles, telescopes and vibrating membranes[,] extend the receptors’ in humans, allowing us to observe things that otherwise we cannot, or only with difficulty, experience. But Feibleman reminds us that there is a more familiar use of the word 
                tool than this—tools like spades or bicycles that ‘extend the effectors’, the parts of humans that allow us to make things, things like holes in the ground or trips to the grocery store (Feibleman, 1967, 330). These kind of tools in fact fit more naturally with the more conventional use of the word 
                tool, as is implied with ‘woodworking tools’ or ‘gardening tools’. Once this is noticed, we can see, perhaps, that Steve Jobs’ early classification of computers as ‘bicycles for the mind’ (see Popova, 2011) involves a very different sense of the potential of digital tools than we find in Ramsay and Rockwell’s seemingly parallel phrase ‘telescopes for the mind’. 
            
            The DH has had a significant history of tool making of this kind, too. One thinks of TuStep (2014) (for the making of sophisticated academic publications), for example, and Zotero (for the making of bibliographies), and we can see in the significant uptake of Zotero that the right tool, solving the right problem, can have a significant impact. The consideration of this kind of tool opens up a different perspective on issues, including on questions such as the UX’s 
                ease of use. Are woodworking tools, or a good violin, ‘easy to use’? Do they produce results easily? If not, why is this 
                not a reason that they have failed to find a user community? In fact, these tools help us to better understand the place of expertise in tool use, and its significance in the process of producing, in the hands of a master, good things from tools—an idea one can see in TuStep as well.
            
            A third category of tools that brings significant relevant insights is ‘tools for cognition’—tools that are meant to help us think better. An extended and useful description of the idea of cognitive tools (although for the related field of tools to facilitate learning) appears in Kim and Reeves (2007), and they, in turn, make frequent reference to the idea of cognitive tools from its original definition by Netchine-Grynberg (1995). Tools for cognition are interesting because they combine characteristics of both Feibleman’s tools for receptors and effectors in potentially powerful ways. Whereas tools for effectors allow us to better execute ideas we already have in our head to produce something external to it, and tools for receptors take things outside our heads and put new ideas in our heads, tools for cognition—because the product they produce are ideas in our heads, too—form a kind of positive feedback loop, perhaps of the kind characterised by Jerome McGann as 
                autopoietic systems (McGann, 2004, 200).
            
            Kim and Reeves use mathematical notation as an example of a tool for cognition. Mathematical notation not only serves to communicate ideas (tool for effectors), but is used by the person working on a problem to help him or her struggle with his or her research: scientists put mathematical fragments in their notebooks to help them think. Perhaps mathematics is not the right tool for humanities scholarship, but scholarly writing can act as a tool for cognition, too: many scholars write notes for this same purpose. Indeed, my Pliny environment (Bradley, 2008) is meant to explore how a tool to support the writing of fragments, as scholarly notes, might help humanities research.
            Scholarship in the humanities involves an interconnection of tasks that could benefit from both Feibleman’s tools for receptors and effectors, and in the task of working with the materials, involving cogitative work that tools for cognition could support. Perhaps a discussion that engages with these three kind of activities might move us better to understanding where digital tools could fit with the humanities?
            Note
            1. For various reasons, this issue is even more evident in the history of the microscope. See Hacking (1981).
        
        
            
                
                    Bibliography
                    
                        Bradley, J. (2005). What You (Fore)See Is What You Get: Thinking about Usage Paradigms for Computer Assisted Text Analysis. 
                        Text Technology,
                        14(2): 1–19, http://texttechnology.mcmaster.ca/pdf/vol14_2/bradley14-2.pdf.
                    
                    
                        Bradley, J. (2008). Pliny: A Model for Digital Support of Scholarship. 
                        Journal of Digital Information,
                        9(1 [26])
                        , http://journals.tdl.org/jodi/article/view/209/198.
                    
                    
                        Bradley, J. (2009). What the Developer Saw: An Outsider’s View of Annotation, Interpretation and Scholarship. In Siemens R. and Shawver, G. (eds), 
                        New Paths for Computing Humanists: A Volume Celebrating and Recognizing Ian Lancashire. Digital Studies / Le champ numérique, 1(1) (13 May), http://www.digitalstudies.org/ojs/index.php/digital_studies/article/view/143/202.
                    
                    
                        Edwards, C. (2012). The Digital Humanities and Its Users. In Matthew, G. (ed.), 
                        Debates in the Digital Humanities. Minneapolis: University of Minnesota Press, pp. 213–32.
                    
                    
                        Feibleman, J. K. (1967). The Philosophy of Tools. 
                        Social Forces, 
                        45(3) (March): 329–37, DOI:10.2307/2575191, http://www.jstor.org/stable/257519.
                    
                    
                        Gibbs, F. and Owens, T. (2012). Building Better Digital Humanities Tools: Toward Broader Audiences and User-Centered Designs. 
                        Digital Humanities Quarterly, 
                        6(2), http://www.digitalhumanities.org/dhq/vol/6/2/000136/000136.html.
                    
                    
                        Hacking, I. (1981). Do We See through a Microscope? 
                        Pacific Philosophical Quarterly,
                        62: 305–22.
                    
                    
                        Kim, B. and Reeves, T. (2007). Reframing Research on Learning with Technology: In Search of the Meaning of Cognitive Tools. 
                        Instructional Science,
                        35: 207–56, DOI:10.1007/s11251-006-9005-2.
                    
                    
                        Malet, A. (2005). Early Conceptualizations of the Telescope as an Optical Instrument. 
                        Early Science and Medicine,
                        10(2): 237–62, http://www.jstor.org/stable/4130312.
                    
                    
                        McGann, J. (2004). Marking Texts of Many Dimension. In Schreibman, S., Siemens, R. and Unsworth, J. (eds), 
                        A Companion to Digital Humanities. Oxford: Blackwell Publishing, pp. 198–217.
                    
                    
                        Netchine-Grynberg, G. (1995). The Functionality of Cognition According to Cassirer, Meyerson, Vygotsky, and Wallon: Toward the Roots of the Concept of Cognitive Tool’. In Lubek, I., Hezewijkvan, R., Pheterson, G. and Tolman, C. W. (eds), 
                        Trends and Issues in Theoretical Psychology. New York: Springer, pp. 207–13.
                    
                    
                        Ott, G. C. (2013). 
                        Why Steve Jobs Didn’t Listen to His Customers. http://www.helpscout.net/blog/why-steve-jobs-never-listened-to-his-customers/.
                    
                    
                        Popova, M. (2011). 
                        Steve Jobs on Why Computers Are Like a Bicycle for the Mind. Brain Pickings, http://www.brainpickings.org/2011/12/21/steve-jobs-bicycle-for-the-mind-1990/.
                    
                    
                        Ramsay, S. and Rockwell, G. (2012). Developing Things: Notes toward an Epistemology of Building in the Digital Humanities. In Gold, M. (ed.), 
                        Debates in the Digital Humanities. Minneapolis: University of Minnesota Press, pp. 75–84.
                    
                    
                        Summit. (2006). 
                        Summit on Digital Tools for the Humanities: A Report on the Summit on Digital Tools. University of Virginia, Charlottesville, VA, 28–30 September 2005.
                    
                    
                        TuStep. (2014). TuStep Text Processing Tools. http://www.tustep.uni-tuebingen.de/tustep_eng.html.
                    
                
            
        
    



    
        
            
                Libraries and Digital Humanities Special Interest Group
                
                    
                        Borovsky
                        Zoe
                    
                    UCLA, U.S.A.
                    zoe@library.ucla.edu
                
                
                    
                        Courtney
                        Angela
                    
                    Indiana University Libraries, U.S.A.
                    ancourtn@indiana.edu
                
                
                    
                        Galina
                        Isabel
                    
                    Universidad Nacional Autónoma de México
                    igalina@unam.mx
                
                
                    
                        Gehrke
                        Stefanie
                    
                    Biblissima, France
                    stefanie.gehrke@biblissima-condorcet.fr
                
                
                    
                        Stensrud Høsøien
                        Hege
                    
                    National Library, Norway
                    hege.hosoien@nb.no
                
                
                    
                        Potvin
                        Sarah
                    
                    Texas A&amp;M University Libraries, U.S.A.
                    spotvin@library.tamu.edu
                
                
                    
                        Stäcker
                        Thomas
                    
                    Herzog August Library, Germany
                    staecker@hab.de
                
                
                    
                        Worthey
                        Glen
                    
                    Stanford University Libraries, U.S.A.
                    gworthey@stanford.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Poster
                
                
                    Libraries
                    Digital Libraries
                    Librarians
                
                
                    digital humanities - institutional support
                    GLAM: galleries
                    libraries
                    archives
                    museums
                    English
                
            
        
    
    
        
            SIG Conveners
             Zoe Borovsky, UCLA Libraries, USA
             Angela Courtney, Indiana University Libraries, USA
             Isabel Galina
                , Universidad Nacional Autónoma de México
            
             Stefanie Gehrke, Biblissima, France
             Hege Stensrud Høsøien, National Library, Norway
             Sarah Potvin, Texas A&amp;M University Libraries, USA
             Thomas Stäcker, Herzog August Library, Germany
             Glen Worthey, Stanford University Libraries, USA
            We propose a poster introducing the ADHO Special Interest Group for Libraries and Digital Humanities that allows for both presentation and live discussion. The poster will present the main objectives motivating the SIG proposal, illustrate its history as an international effort, and afford the conveners an opportunity to circulate and discuss the SIG proposal face-to-face with supporters and potential members among international conference attendees. For those DH attendees not directly involved in library work, the poster will be primarily an introduction to the importance of, and issues related to, DH work in libraries internationally.
            Mission and Rationale
            The ADHO 
                Libraries and DH SIG aims to foster collaboration and communication among librarians and other scholars doing library-related digital humanities work. By establishing this SIG, ADHO serves its mission and provides the connective tissue between ADHO organizations and emerging DH initiatives within organizations of professional librarians such as the Association of College and Research Libraries (part of the American Library Association) and the International Federation of Library Associations and Institutions (IFLA). We believe that creating these connections will lead toward more informed ‘dual citizenship’—that is, of librarians connected to, participating in, and supported by both library- and DH-oriented professional organizations, in league with fellow DH-oriented librarians around the world. By fostering such dual citizenship, librarians and libraries will be well positioned to identify opportunities to initiate and engage with digital humanities projects and scholarship, as well as to collectively address the challenges we face. These challenges include, for example, (a) advocating for financial support, release time, training opportunities, technical infrastructure, and resources to undertake their digital humanities projects; (b) addressing the changing notions of ‘service’ and ‘research’ in light of the collaborative nature of much digital humanities work; and (c) promoting a culture of digital humanities scholarship in the library.
            
             The mission of the ADHO 
                Libraries and DH SIG will be to
            
             • Offer advice and support to these new and emergent associations of librarians engaged in digital humanities pursuits—either projects of their own making or in collaboration with nonlibrarian digital humanists.
             • Advocate for initiatives of interest and benefit to libraries as well as digital humanities (e.g., the ‘Collaborators’ Bill of Rights’ white paper, the Digital Public Library of America, the ‘Best Practices for TEI in Libraries’ guidelines, and other tools and best practices related to library-based DH).
             • Document how librarians and library-based units meet these challenges.
             • Provide information about available resources and opportunities (e.g., training, funding) that encourage collaboration between DH scholars in a variety of roles, especially in libraries.
             • Showcase the work of librarians engaged in the digital humanities.
             • Promote librarians’ perspectives and skills to the rest of the DH community.
            An initial goal of the SIG will center on developing working relationships within and across international library-oriented organizations such as the ACRL DH Interest Group, the Digital Library Federation, the TEI in Libraries Special Interest Group, the Society for American Archivists, the Association for Information Science and Technology, the International Federation of Library Associations and Institutions, and others.
            Activities
            In terms of concrete activities, the SIG would, when possible, coordinate or collaborate with librarian organizations to accomplish the following: 
             • Researching and documenting existing librarian-led DH projects and DH organizations in which librarians are active partners (e.g., the TEI Consortium SIG on Libraries).
             • Organizing conference sessions—ones for librarians at DH conferences, and others focused on DH at conferences intended primarily for librarians (such as ALA, ACRL, ARLIS, DLF, etc.).
             • Organizing workshops, training events, and conference sessions aimed at promoting librarians’ involvement in the general DH community and showcasing library-led projects. 
            Membership is open to anyone with an interest in the issue. Currently, 130 people have actively expressed an interest in joining this SIG, but we believe the potential membership to be substantially greater worldwide. One concrete benefit of presenting a poster at DH2015 will be community outreach, with the aim of reaching these potential members.
            Note
            A related public Zotero group on the topic of DH in libraries is maintained at https://www.zotero.org/groups/adho_library_sig. 
        
    


        
            
                Background
                Digital Harlem (DigitalHarlem.org), winner of the 
                    American Historical Association's 2009 
                    Roy Rosenzweig Prize for Innovation in Digital History, was a bespoke php/js/MySQL application with 34 tables and over 9,000 lines of code. While bespoke programming may under some circumstances offer the shortest path to a particular outcome, a fixed database structure and bespoke codebase pose problems for sustainability (the codebase will require ongoing maintenance and retention of support for code which is no longer current), for transferability (knowledge and development work which is not directly transferrable to other projects) and for evolutionary change (modification can involve significant rewriting and programming expertise). For Digital Harlem both evolving requirements and external changes have forced the excavation and re-learning of code long after development funding ceased, an experience which will be common for any project which seeks longevity beyond short-term project grants.
                
            
            
                Conversion
                In 2014/2015 we converted the Digital Harlem database to Heurist (HeuristNetwork.org). This allowed us to unlock the previously inflexible data structures and rigid interface, and refine the data model, enabling the project to start a campaign of data entry for a new research grant and focus. Where analysis of the data was previously restricted to three rather limiting search forms for People, Places and Events, the new version (Figure 1) opens up a full range of built-in data management functions and user-defined searches/filters, including multi-level faceted search. Search results can now be saved and visualised with maps, timelines, network diagrams and user-defined reports, as well as file and printed output. 
                
                
                    
                    Figure 1. Digital Harlem - the standard Heurist interface (used by the research team)
                
                The original public website was subsequently reimplemented, with minor external changes, as a reskinned view of the database running natively within Heurist (Figure 2). We moved significant elements of the interface - search forms, base maps, popup content, buttons – out of custom code into data. These data can be easily edited by the research team allowing them to extend the interface without technical assistance. Fixed form-based searches were replaced with saved faceted searches which can be added to or modified without programming.
                
                    
                    Figure 2. Digital Harlem - the reimplemented public interface with faceted search
                
            
            
                Adaptability
                The public interface is easily adaptable to other projects requiring a customised public search, mapping and timeline interface for richly linked entities. It is not tied to specific types of entity or relationship, as almost all customisation other than visual appearance occurs within the database content. The interface is built from reusable widgets in a responsive framework, using less than 1,000 lines of html, css, php and js code. New widgets can be added for additional types of interaction, although many projects will find the existing widgets adequate.
            
            
                Sustainability
                Heurist databases retain the inherent medium-term sustainability of an Open Source MySQL database at the backend, but reinforce this sustainability through the adoption of an identical structure across many diverse projects. The use of a single, well-documented database structure across all projects promotes the transfer of expertise and leverages the effort of code development - when someone requests Zotero synchronisation or the generation of GEPHI network files, the code can be written just once and every database inherits the capability. The same goes for any maintenance required to keep pace with the changing web environment and for bug fixing. Standard documented SQL queries, which can be run from any programming language, will work across all databases. A complete, fully documented XML data archive can be generated in a couple of clicks.
            
            
                Conclusion
                In this poster we will outline the sustainability and development benefits of the new implementation of Digital Harlem. By adopting an adaptable codebase (Heurist) which can run many heterogeneous projects we have leveraged development effort and benefits across Digital Harlem and several dozen other projects. For new projects, common data structures can be imported with a few mouse clicks from a clearinghouse of projects, adapted to specific needs and republished for use by others. Existing public interfaces can, with slightly more effort, be repurposed for new projects. A stable well-documented underlying data format also allows independent code development in a variety of languages.
                If there is a final takeaway, it is that - while there will always be a place for bespoke, one-off code as a vehicle for experimentation - for the majority of projects, re-use of a shared codebase and body of expertise (where practical) will be more cost-effective, require less technical development and provide a better chance of longevity. Heurist offers one such generic solution which has allowed Digital Harlem to escape its self-imposed straightjacket of bespoke data structure and code; new data structures, research tools or public interfaces can simply be added without reengineering the system. Digial Harlem builds on the work of many preceding projects, and future projects can build on this experience without the cost of reinvention.
            
        
    


        
            
                    Introduction
                    This paper introduces the TEI P5 XML – EpiDoc corpus of inscriptions on stone for ancient Sicily, I.Sicily. The project is one of the first attempts to generate a substantial regional corpus in EpiDoc. The project is confronting a number of challenges that may be of wider interest to the digital epigraphy community, including those of unique identifiers, linked data, museum collections, mapping, and data conversion and integration, and these will be briefly outlined in the paper which will concentrate on the conversion and technical development of the project.
                
            
                    Technical Background of I.Sicily
                    I.Sicily is an online, open access, digital corpus of the inscriptions on stone from ancient Sicily.
                         The corpus will be mounted at 
                             by the time of DH2016, but is currently on a development server.
                         The corpus aims to include all texts inscribed on stone, in any language, between approximately the seventh century BC and the seventh century AD. The corpus currently contains records for over 2,500 texts, and when complete is likely to contain c. 4,000. The corpus is built upon a conversion from a legacy dataset of metadata in MS Access to EpiDoc TEI XML. The XML records are held in an eXist database for xQuery access, and additionally indexed for full-text search using SOLR/Lucene. The corpus and related information (museum list, bibliography) are published as Linked Data, and are manipulated through a RESTful API. The records are queried and viewed through a web interface built with AngularJS and jQuery javascript components. Mapping is provided in the browser by the Google Maps API, and ZPR (Zoom, Pan, Rotate) image- viewing is provided by the IIIP image server.
                    
                     At the time of writing, the main conversion routine is being refined, and the epigraphic texts are being collated for incorporation into the records. An ancillary database of museum collections in Sicily has been constructed and bibliography is held in a Zotero library. Extensive search facilities will be provided, including map-based and bibliographic searching. Individual inscriptions and individual museums will both be provided with URIs, as will personal names and individuals; places will be referenced using Pleiades, epigraphic types, materials, and supports using the EAGLE vocabularies.
                
            
                    The motivations for I.Sicily 
                    The existing epigraphic landscape in Sicily is extremely diverse in two primary regards: on the one hand, the island has a very mixed cultural and linguistic make-up, meaning that the epigraphic material is itself extremely varied, with extensive use throughout antiquity of both Greek and Latin, as well as Oscan, Punic, Sikel, and Hebrew
                         Recent overview of much of the linguistic tradition in Tribulato 2012; and of the epigraphic material in Gulletta 1999.; on the other hand, the publication of this material has a very uneven record and despite an excellent pre-twentieth century tradition, the existing corpora are far from complete and the ability of key journals such as SEG or AE to keep pace with local publication has been limited. A limited number of museum-based corpora have been published in recent decades (for Catania, Palermo, Messina, and Termini Imerese, as well as the material from Lipari), but this has not greatly improved the overall situation. The combination of these two factors already means that locating, identifying, or working with a Sicilian inscription, or its publication record, is extremely challenging for anyone without extensive experience of the material. I.Sicily has been conceived in the hope of improving the situation in all these areas.
                    
                
            
                    Multilingualism 
                    Sicily is traditionally described as a ‘melting pot’, the ‘crossroads of the Mediterranean’. The situation created by basic technologies such as Unicode and TEI P5 EpiDoc XML mean that there is now no reason not to be language agnostic in the inclusion of material. The opportunities and possibilities offered by these technologies are considerable, since, for example, searching can be made language specific or language neutral. One obvious area where Sicilian studies are currently hampered by this disciplinary partitioning is in the study of onomastics. The Lexicon of Greek Personal Names records most instances of Greek names for the island, but Sicily is no less rich in non-Greek names (Latin and others), and at present there is no onomasticon for the island.
                         See 
                            
                         Simply by the marking-up and indexing of all names in the island’s inscriptions, I.Sicily will have generated a powerful tool for future study.
                    
                
            
                    Identification and Bibliography 
                    The PHI database of Greek inscriptions has a rich record of Greek texts, but again is text only and limited in outputs.
                         See 
                            
                         SEG references are available for 733 inscriptions on stone and AE references for 328 (data taken from the I.Sicily database and based upon comprehensive manual trawls of SEG and AE). One major aim of I.Sicily, therefore, is to generate unique identifiers for each inscription - the I.Sicily number, in the form ISic 1234 maintained as URIs, of the form: 
                        . I.Sicily is well placed to do this since its initial dataset is primarily a bibliographic concordance of the lapidary inscriptions of Sicily. One of the associated outputs of the project will therefore be an online bibliography for Sicilian epigraphy, and an online Zotero library has already been created with over 700 records which are referenced in the EpiDoc. A locally cached version of the bibliography will be presented at the I.Sicily site to facilitate detailed bibliographic searching (including the identification of inscriptions by publication) and to allow the generation of customised concordances.
                    
                
            
                    Location, location, location
                    I.Sicily is actively generating rich geo-data for the individual inscriptions, both for the original findspot/provenance and the current location (whether museum-based, on-site, or elsewhere), and we aim to provide map-based searching for inscriptions, as well as text-based searching by ancient and modern place-names. In addition to full listing wherever possible of both ancient and modern place names for epigraphic provenance, we are working to provide detailed location information for each find-spot and current location, through a combination of library and map-based research and the use of autopsy and GIS recording. At present geo-data is being recorded in two forms, both through the use of explicit geographical locations in the form of longitude and latitude records in decimal degree form, and through the use of Pleiades URI references wherever possible.
                         See 
                            
                         We are committed to the long-term use of Pleiades as our primary reference for ancient places, and to that end we aim to update and improve the Pleiades data for Sicilian locations, in particular name data and sub-locations, in conjunction with the editing of the I.Sicily records.
                    
                
            
                    Translations 
                    The creation and availability of translations is a major goal of the EAGLE project and its collaborators, and I.Sicily is no less committed to that ambition.
                         See Orlandi et al. 2014: Part II. Translations are very rarely available for any of the published Sicilian inscriptions. It is obvious that the inclusion of translations will make the material much more accessible to a wider audience both of students and the general public. Equally, provision of translations will add to the value of the database as a resource for museums and others curating the inscriptions recorded in the database. To that end, a long-term ambition of I.Sicily is to include translations wherever possible in both English and Italian. We see this as one obvious area where public contribution (‘crowd-sourcing’) will be invaluable.
                    
                
            
                    Limitations and future ambitions 
                    The scale of the enterprise, and the available resources, mean that in its current form the project has limited itself to inscriptions engraved on stone (the coverage of rupestral inscriptions/graffiti and of inscriptions painted on stone/plaster is regrettably uneven). However, there is no reason in principle not to extend coverage in future to include inscriptions on other materials. Similarly, although the current project does not include a programme to mark up linguistic features of the texts, the commitment to the long-term maintenance of the corpus and the open availability of the underlying XML records means that such a project would be entirely possible in the future. 
                    It is our long-term ambition that I.Sicily might become the default location for the publication and dissemination of Sicilian inscriptions; in the shorter term, we hope that it will serve as valuable portal in the world of Sicilian epigraphy and of ancient world open linked data, greatly improving the accessibility of Sicilian epigraphy and so enriching the study of the ‘crossroads of the Mediterranean’.
                
        
        
            
                
                    Bibliography
                    
                        Gulletta, M. I. (ed.) (1999), 
                        Sicilia Epigraphica. Atti del convegno internazionale, Erice, 15-18 ottobre 1998, 2 vols.
                    
                    
                        Orlandi, S., Santucci, R., Casarosa, V. and Liuzzo, P. M. (eds.) (2014). 
                        Information Technologies for Epigraphy and Cultural Heritage, Proceedings of the First EAGLE International Conference, Rome 2014, Sapienza Università Editrice. Published online at: http://www.eagle-network.eu/wp-content/uploads/2015/01/Paris-Conference-Proceedings.pdf (accessed 5 March 2016)
                    
                    
                        Prag, J. R. W. (2002). Epigraphy by numbers: Latin and the epigraphic culture in Sicily. In Cooley, A. E. (ed.), Becoming Roman, Writing Latin?: Literacy And Epigraphy In The Roman West (Journal of Roman Archaeology Supplementary Series), 
                        Journal of Roman Archaeology, pp. 15-31. 
                    
                    
                        Tribulato, O. (ed.) (2012). 
                        Language and linguistic contact in ancient Sicily, Cambridge: Cambridge University Press.
                    
                
            
        
    


        
            
                Starting Point
                The project objective of 
                    Digitizing 
                    Early 
                    Farming 
                    Cultures (DEFC) is the standardization and integration of research data from sites and finds from the Neolithic and Copper Age (7000–3000 BC) located in Greece and Western Anatolia. These datasets are based on digital and analog resources of research projects of the research group Anatolian Aegean Prehistoric Phenomena (AAPP) at the Institute for Oriental and European Archaeology (OREA) of the Austrian Academy of Sciences.
                
                Greece and Western Anatolia are two neighbouring and archaeologically closely related regions. They are, however, usually studied in isolation from each other and have therefore developed different terminologies and chronologies. Direct results of this de facto separation are not only huge amounts of fragmented research data but also several different models and standards for ordering and describing more or less the same kind of data. To pose and answer archaeological research questions concerning the whole territory, the information must be harmonized.
                The aim of the DEFC project is now to harmonize the existing data, to digitize analog resources and make metadata available to facilitate access and reuse this data. To achieve those goals an archaeological data management system is needed.
            
            
                Data model and Application
                The particular requirements to the data model are to reflect the high granularity of the archaeological data structure which correlates on different levels to the excavation process workflow, geographical location, chronological periodization and at the same time to keep the complex relationships between the data objects. After an evaluation of already existing solutions for managing (archaeological) data (e.g. Microsoft Access, Arches Project) it turned out that those were not comprehensive enough for modeling and capturing the very heterogeneous datasets the DEFC project is confronted with. Therefore the development of a more customizable application to collect, standardize, analyze and visualize archaeological data was necessary.
                To meet the needs of researchers a clear conceptual data model based on archaeological objects relationships has been defined with the following main model classes:
                
                    Site (location where research took place/observations were made)
                    Research Event (project and type of archaeological research that was carried out)
                    Area (particular part of the site, defined by its geolocation, period, as well as its type)
                    Finds (artefacts, animal and plant remains found)
                    Interpretation (archaeologist's interpretation of areas/finds etc.)
                
                Each of those classes is defined through several properties, most of them linked to a carefully curated set of controlled vocabulary.
                
                    
                  Figure 1. Simplified data model
                
                The DEFC-App is based on the Python web framework Django. As one of the application's design principles is to keep things as simple as possible, the application tries to leverage Django´s built-in generic functionality as far as possible. The application's web interface is based on Bootstrap. Client-side scripting, which is needed for a better user guidance and enabling a more responsive data querying and presentation, is implemented with JavaScript, jQuery, Tablesorter and Leaflet.
                
                    
                  Figure 2. Site details page
                
                
                    
                  Figure 3. Create new Research Event page
                
                
                    
                  Figure 4. Referencing Geonames
                
            
            
                Development and Upcoming tasks
                The project aims to integrate open access resources by using Web APIs and Linked Data practices. At the time being Geonames referencing is implemented for the archaeological locations and provided via a user interface. Hence the fetched Geonames IDs are stored within the database to later be linked to the Pelagios project.
                The bibliographic data formerly stored in proprietary formats MS-Access and AskSam was imported to a Zotero library and linked to the DEFC-Database so that every reference record in DEFC redirects to a Zotero library record, where the entire bibliography can also be explored.
                To make the published data available ‘open access’ for further reuse in research, a REST-API (Django REST framework) was implemented along with the web user interface for querying and exporting data.
                The outlook of the project is to turn the data into Linked Open Data and make it available via a SPARQL endpoint. Moreover, the thesaurus consisting of hierarchically structured archaeological data units (respectively the aforementioned controlled vocabulary) has been partially mapped to the CIDOC CRM ontology and will later be mapped to the SKOS schema. This will, overall, enhance the quality of the RDF data in the future.
            
            
                Conclusion
                The development of the DEFC-App and its underlying data model could be understood as a very common use case in the broad field of digital humanities as it involves a tight cooperation between archaeologists, data analysts and developers. 
            
        
        
            
                
                    Bibliography
                    
                        Arches project. [Online] Available from: 
                        http://archesproject.org/ [Accessed 4 March 2016].
                    
                    Christian Bach. 
                        Tablesorter. [Online] Available from: 
                        http://tablesorter.com/docs/ [Accessed 4 March 2016].
                    
                    
                        DEFC-App. [Online] Available from: 
                        http://defc.digital-humanities.at/ [Accessed 4 March 2016].
                    
                    Django Software Foundation and individual contributors. 
                        Django. [Online] Available from: 
                        https://www.djangoproject.com/ [Accessed 4 March 2016].
                    
                    
                        Django REST framework. [Online] Available from: 
                        http://www.django-rest-framework.org/ [Accessed 4 March 2016].
                    
                    
                        Geonames. [Online] Available from: 
                        http://www.geonames.org/ [Accessed 4 March 2016].
                    
                    
                        Pelagios: Enable Linked Ancient Geodata In Open Systems. [Online] Available from: 
                        http://pelagios-project.blogspot.co.at/p/about-pelagios.html [Accessed 4 March 2016].
                    
                    Vladimir Agafonkin. 
                        Leaflet. [Online] Available from: 
                        http://leafletjs.com/ [Accessed 4 March 2016].
                    
                    
                        Zotero. [Online] Available from: 
                        https://www.zotero.org/ [Accessed 4 March 2016].
                    
                
            
        
    

Introduction
Existing studies of anglophone digital humanities (DH) curricula have examined course syllabi (Terras 2006; Spiro 2011) and the development of programs at specific institutions (Rockwell 1999; Siemens 2001; Sinclair 2001; Unsworth 2001; Unsworth and Butler 2001; Drucker, Unsworth, and Laue 2002; Sinclair & Gouglas 2002; McCarty 2012; Smith 2014). This study adds to the literature on teaching and learning by presenting a survey of formal degree and certificate programs in anglophone DH.

While these programs represent only part of the entire DH curricula, they are important in several respects: First, they reflect intentional groupings of courses, concepts, skills, methods, and techniques, purporting to represent the field in its broadest strokes. Second, these programs include explicit learning outcomes, and their requirements form one picture of what all DHers are expected to know upon graduation. Third, formal programs organize teaching, research, and professional development in the field; they are channels through which material and symbolic capital flow and thereby shape the field itself. Finally, these programs, their requirements, and coursework are one way—perhaps the primary way— in which prospective students encounter the field and make choices about whether to enroll in a DH program and, if so, which one.

In addition to helping define the field, a study of DH programs also has the capacity to comment on pedagogical discussions in the literature. Hockey, for example, has long wondered whether programming should be taught in the field (1986) and asks, “How far can the need for analytical and critical thinking in the humanities be reconciled with the practical orientation of much work in humanities computing?” (2001). Also skeptical of mere technological skills, Mahony and Pierazzo (2002) argue for teaching methodologies or “ways of thinking” in DH, and Clement examines multiliteracies in DH (e.g., critical thinking, commitment, community, and play), which help to push the field beyond “training” to a more humanistic pursuit (2012, 372). Others have called on DH to engage more fully in critical reflection, especially in relation to technology and the role of the humanities in higher education (Brier 2012, Liu 2012, Walzer 2012).

These and other concerns point to longstanding questions about the proper balance of skills and reflection in DH. While a study of DH programs cannot address the value of critical reflection, it can report on its presence (or absence). These findings, together with our more general observations about DH activities, give pause to consider what is represented in, emphasized by, and omitted from the field at its most explicit levels of educational training. Methodology

We compiled a list of 37 DH programs active in 2015, drawn from public listings (UCLA Center for Digital Humanities 2015; Clement 2015), background literature, and web searches (e.g., “digital humanities masters”). In addition to degrees and certificates, we included minors and concentrations in which humanities content was the central focus, and omitted digital arts and media programs in which this was not the case. Because our sources and searches are all English-language, it limits what we can say about global DH.

We recorded the URL and basic information (e.g., title, level, location) about each program and looked up descriptions of any required courses in the institution’s course catalog. To analyze topics addressed in these programs, we applied the Taxonomy of Digital Research Activities in the Humanities (TaDiRAH 2014), which attempts to capture the “scholarly primitives” of the field (Perkins et al. 2014). TaDiRAH contains forty activities terms organized into eight parent terms (‘Capture’, ‘Creation’, ‘Enrichment’, ‘Analysis’, ‘Interpretation’, ‘Storage’,    ‘Dissemination’, and ‘Meta-Activities’).

TaDiRAH was chosen for its basis in the literature on “scholarly primitives” (Unsworth 2000), as well as three earlier sources (an arts-humanities.net taxonomy, DIRT categories and tags, and a Zotero bibliography) and community feedback and revision.

We applied terms to program/course descriptions independently and then tested intercoder agreement, which was extremely low. We attribute this to the many terms in TaDiRAH, complexity of program/course description language, questions of scope (i.e., using a broader or narrower term), and general vagueness. We did find discussing our codings helpful and, in doing so, were able to agree. Accordingly, each of us read and coded every program/course description and discussed them until we reached consensus. Often, this involved pointing to specific language in the descriptions and referencing TaDiRAH definitions or notes from previous meetings when interpretations were discussed.

Findings and discussion

The number of DH programs has risen sharply over time, beginning in 1991 and growing steadily by several programs each year since 2008 (see Figure 1).


Figure 1. Growth of digital humanities programs in this study

Geography

Most of the programs studied here were located in the US (22 programs, 60%), followed by Canada (6 programs, 16%), the UK (5 programs, 14%), Ireland (3 programs, 8%), and Australia (1 program, 3%). We note that these programs are all located in Anglophone countries and that TaDiRAH, too, originates from this context, which necessarily limits what we can say about DH programs from a global perspective.

Structure

Less than half of these DH programs grant degrees: some at the level of bachelor’s (8%), most at the level of master’s (22%), and some at the doctoral level (8%) (Figure 2). The majority of these programs are certificates, minors, or specializations—certificates being more common at the graduate level and nearly one-third of all programs studied here.


Degree Type (group) | Other | Doctoral | Masters | Bachelors

Certificate

Figure 2. Digital humanities programs in this study (by degree and level)

We also examined special requirements of these programs, finding that half require some form of independent research (see Figure 3), and half require a final deliverable, referred to variously as a capstone, dissertation, portfolio, or thesis (see Figure 4). About one-quarter of these programs require fieldwork, often an internship (see Figure 5).

)%


Degree Type (group)

| Doctoral

None    22%    3% BI    24%    T    ■ Mas,efs

leClfled    HI    H    ■ Bachelors

0% 5%    10% 15% 20% 25% 30% 35% 40% 45% 50%    ■ Certificate

% of Programs

Figure 3. Independent research requirements of digital


Figure 4. Final deliverable required by digital humanities programs in this study


I Doctoral I Master's I Bachelor's

Certificate

0%    10%    20%    30%    40%    50%    60%    70%    80%

Figure 5. Fieldwork requirements of digital humanities programs in this study

Location & disciplinarity

Most of these programs are housed in colleges/schools of arts and humanities, but about one-third    are outside of traditional

schools/departments, in centers, initiatives, and, in one case, jointly with the library (see Figure 6). Most DH concentrations and specializations are located within English departments, commensurate with Kirschenbaum’s claim that DH’s “professional apparatus...is probably more rooted in English than any other departmental home” (2010, 55).

College or School

Center

Department

Institute

3

Initiative

i

Library & Colleges

i

Studio

1

0    2    4    6    8    10    12    14

Number of Programs

Figure 6. Institutional location of digital humanities programs in this study

Elective courses for these programs span myriad departments and disciplines, from humanities departments (art history, classics, history, philosophy, religion, and various languages) to along with computer science, education, information and library science, design, media, and technology.

DH activities

We analyzed our TaDiRAH codings in two ways: overall term frequency (see Figure 7) and weighted frequency across programs (see Figure 8). To compute weighted frequencies, each of the eight parent terms were given a weight of 1, which was divided equally among the subterms in each area. These subterm weights were summed to show how much of an area is represented, regardless of its size.

Analysis and meta-activities (e.g., ‘Community building’, ‘Project management’, ‘Teaching/Learning’) make up the largest share of activities, along with creation (e.g., ‘Designing’, ‘Programming’, ‘Writing’). It is worth noting that ‘Writing’ is one of the most frequent terms (11 programs), but this activity certainly occurs elsewhere and is probably undercounted because it was not explicitly mentioned in program descriptions. The same may be true for other activities.

Parent Terms

Terms

This Study

•Capture

Total

|13

•Capture

I3

Conversion

1

Data Recognition

1

Discovering

1

•Interpretation

Total

■ 27

Gathering

|3

•Interpretation

1

Imaging

|2

Contextualizing

|3

Recording

1

Modeling

|3

Transcribing

1

Theorizing

■ 20

’Creation

Total

35

•Storage

Total 111

•Creation

P

•Storage

Designing

|10

Archiving

I3

Programming

|7

Identifying

Translation

Organizing

|2

Web Development

|2

Preservation

I6

Writing

|11

•Dissemination

Total

■ 24

•Enrichment

Total

I4

•Dissemination

I 5

•Enrichment

1

Collaboration

|7

Annotating

|3

Commenting

Cleanup

Communicating

|3

Editing

Crowdsourcing

1

•Analysis

Total

Publishing

I4

•Analysis

1 7

Sharing

I4

Content Analysis

1 6

’Meta-Activities

Total

51

Network Analysis

P

’Meta-Activities

Relational Analysis

|2

Meta Assessing

■ 14

Spatial Analysis

1 7

Meta: Community Building

1 5

Structural Analysis

1 6

Meta: Give Overview

■ 20

Stylistic Analysis

1 6

Meta: Project Management

1 6

Visualization

|8

Meta: Teaching/Learning

I6    1

Figure 7. TaDiRAH term coding frequency (grouped)


Figure 8. Digital humanities programs in this study and their required courses (by area)

Enrichment and storage terms (e.g., ‘Archiving’, ‘Organizing’, ‘Preservation’) were generally sparse (only 1.9% of all codings), but we suspect these activities do occur in DH programs and courses—in fact, they are assumed in broader activities such as thematic research collections, content management systems, and even dissemination. Generally, there seems to be less emphasis on content (‘Capture’, ‘Enrichment’, and ‘Storage’ terms) and more focus on platforms and tools (‘Analysis’ and ‘Meta-Activities’ terms) in the programs studied here—or at least, those may be more marketable to prospective students and institutions, two important audiences of program webpages.

Theory and critical reflection

To analyze theory and critical reflection, we focused our analysis on two terms: ‘Theorizing’ and ‘Meta: GiveOverview’, which we used to code theoretical or historical introductions to DH itself. We found that all programs studied here included some mention of theory or historical/systematic overview (see Figure 9). Our codings, of course, do not reveal anything further about the character of this reflection, whether it is the type of critical reflection called for in the literature, or how it interfaces with skills and techniques in these programs.

We plan to publish our data and visualizations publicly for researchers, students, and those developing curriculum: http://bit.ly/DHprograms. We believe it provides a baseline of field growth, areas, structure, and learning experiences, which can be used to measure changes in future, in addition to providing a data-driven perspective on the field today.

In that respect, we hope this study gives the community pause to consider how DH is described, represented, and taught. If there are common expectations not reflected here, perhaps we could be more explicit about those, at least in building our taxonomies and describing our formal programs and required courses. Conversely, if there are activities that seem overrepresented here, we might consider


Institution Degree

Program

Theorizing

Courses

I!

«0 4)

15

1    i

2    o Û. Ü

Australian National University Minor (undergraduate)

• •

• •

Brigham Young University Minor (undergraduate)

•

Brock University multiple

•

Carleton University MA

• •

• •

CUNY Graduate Center MA (program track)

• •

• •

Farleigh Dickinson University Minor (undergraduate)

•

Illinois Institute of Technology BS

•

King's College London BA

MA

• •

•

• •

•

Loyola University Chicago MA

•

Michigan State University Certificate (graduate)

Specialization (undergraduate)

•

•

•

•

National University of Ireland Maynooth MA

PhD

•

•

• •

•

North Carolina State University Certificate (graduate)

•

Pratt Institute Certificate (graduate)

• •

•

Rutgers University Certificate

• •

• •

Stanford University Certificate (graduate)

•

Texas A&M University Certificate (graduate)

•

•

Texas Tech University Certificate (graduate)

•

Trinity College Dublin MPhil

•

• •

UCLA Certificate (graduate)

Minor (undergraduate)

•

•

•

University of Alberta MA

• •

•

University of California, Santa Barbara Specialization (undergraduate)

•

University of Georgia Concentration (graduate)

Concentration (undergraduate)

•

•

•

University of Iowa Certificate (graduate)

• •

• •

University of Nebraska-Lincoln Certificate (graduate)

•

• •

University of North Carolina at Chapel Hill Certificate (graduate)

•

•

University of Victoria Certificate (graduate)

• •

• •

University of Washington Certificate (graduate)

•

•

Western University Canada Minor (undergraduate)

•

Figure 9. Theory in digital humanities programs in this study

Further directions

why those activities are prized in the field (and which are not) and whether this is the picture of DH we wish to present publicly.

Bibliography
Brier, S. (2012). “Where's the Pedagogy? The Role of Teaching and Learning in the Digital Humanities.” In Debates in the Digital Humanities, edited by Matthew K.

Gold, 390-401. Minneapolis: Univ Of Minnesota Press.

Clement, T. (2012). “Multiliteracies in the Undergraduate Digital Humanities Curriculum.” In Digital Humanities Pedagogy: Practices, Principles and Politics, edited by Brett D. Hirsch, 365-88. Open Book Publishers. http://www.openbookpublishers.com/prod-uct/161/digital-humanities-pedagogy--practices--prin-ciples-and-politics.

Clement, T. (2015). “Digital Humanities Inflected Undergraduate Programs.” Tanyaclement.org. January 8,

2015. http://tanyaclement.org/2009/11/04/digital-humanities-inflected-undergraduate-programs-2/.

Drucker, J., Unsworth, J., and Laue, A. (2002). “Final Report for Digital Humanities Curriculum Seminar.” Media Studies Program, College of Arts and Science: University of Virginia. http://www.iath.virginia.edu/hcs/dhcs/.

Hockey, S. (1986). “Workshop on Teaching Computers and Humanities Courses.” Literary & Linguistic Computing 1 (4): 228-29.

Hockey, S. (2001). “Towards a Curriculum for Humanities Computing: Theoretical Goals and Practical Outcomes.”

The Humanities Computing Curriculum / The Computing Curriculum in the Arts and Humanities Conference. Malaspina University College, Nanaimo, British Columbia.

Kirschenbaum, M. G. (2010). “What Is Digital Humanities and What's It Doing in English Departments?” ADE Bulletin 150: 55-61.

Liu, A. 2012. “Where Is Cultural Criticism in the Digital Humanities?” In Debates in the Digital Humanities, edited by Matthew K. Gold, 490-509. Minneapolis, Minn.: Univ Of Minnesota Press. http://dhdebates.gc.cuny.edu/de-bates/text/20.

Mahony, S., and Pierazzo, E. (2012). “Teaching Skills or Teaching Methodology.” In Digital Humanities Pedagogy: Practices, Principles and Politics, edited by Brett D. Hirsch, 215-25. Open Book Publishers. http://www.openbookpublishers.com/prod-uct/161/digital-humanities-pedagogy--practices--prin-ciples-and-politics.

McCarty, W. (2012). “The PhD in Digital Humanities.” In Digital Humanities Pedagogy: Practices, Principles and

Politics, edited by Brett D. Hirsch. Open Book Publishers. http://www.openbookpublishers.com/prod-uct/161/digital-humanities-pedagogy--practices--prin-ciples-and-politics.

Perkins, J., Dombrowski, Q., Borek, L., and Schoch, C.

(2014). “Project Report: Building Bridges to the Future of a Distributed Network: From DiRT Categories to TaDiRAH, a Methods Taxonomy for Digital Humanities.”

In Proceedings of the International Conference on Dublin Core and Metadata Applications 2014, 181-83. Austin, Texas.

Rockwell, G. (1999). “Is Humanities Computing and Academic Discipline?” presented at An Interdisciplinary Seminar Series, Institute for Advanced Technology in the Humanities, University of Virginia, November 12.

Siemens, R. (2001). “The Humanities Computing Curriculum / The Computing Curriculum in the Arts and Humanities: Presenters and Presentation Abstracts.” November 9-10, 2001. https://web.ar-

chive.org/web/20051220181036/http://web.mala.bc.c

a/siemensr/HCCurriculum/abstracts.htm#Hockey.

Sinclair, S. (2001). “Report from the Humanities Computing Curriculum Conference,” Humanist Discussion Group. November 16, 2001. http://dhhumanist.org/Ar-chives/Virginia/v15/0351.html.

Sinclair, S., and Gouglas, S. W. 2002. “Theory into Practice A Case Study of the Humanities Computing Master of Arts Programme at the University of Alberta.” Arts and Humanities in Higher Education 1 (2): 167-83.

doi:10.1177/1474022202001002004.

Smith, D. (2014). “Advocating for a Digital Humanities Curriculum: Design and Implementation.” Presented at Digital Humanities 2014. Lausanne, Switzerland. http://dharchive.org/paper/DH2014/Paper-665.xml.

Spiro, L. (2011). “Knowing and Doing: Understanding the Digital Humanities Curriculum.” Presented at Digital Humanities 2011. Stanford University.

TaDiRAH. (2014). “TaDiRAH - Taxonomy of Digital Research Activities in the Humanities.” GitHub. May 13, 2014. https://github.com/dhtaxonomy/TaDiRAH.

Terras, M. (2006). “Disciplined: Using Educational Studies

to Analyse ‘Humanities Computing.'” Literary and Linguistic Computing 21 (2): 229-46.

doi:10.1093/llc/fql022.

UCLA Center for Digital Humanities. (2015). “Digital Humanities Programs and Organizations.” January 8, 2015. https://web.ar-

chive.org/web/20150108203540/http://www.cdh.ucl

a.edu/resources/us-dh-academic-programs.html.

Unsworth, J. 2000. “Scholarly Primitives: What Methods Do Humanities Researchers Have in Common, and How Might Our Tools Reflect This?” Presented at Symposium on Humanities Computing: Formal Methods, Experi

mental Practice, King's College London. http://peo-

ple.brandeis.edu/~unsworth/Kings.5-00/primi-

tives.html.

Unsworth, J., (2001). “A Masters Degree in Digital Humanities at the University of Virginia.” Presented at 2001

Congress of the Social Sciences and Humanities. Univer-

site Laval, Quebec, Canada. http://www3.isrl. illi-nois.edu/~unsworth/laval.html.

Unsworth, J., and Butler, T. (2001). “A Masters Degree in Digital Humanities at the University of Virginia.” Pres-nted at ACH-ALLC 2001, New York University, June 1316, 2001.

Waltzer, L. (2012). “Digital Humanities and the ‘Ugly Stepchildren’ of American Higher Education.” In Debates in the Digital Humanities, edited by Matthew K. Gold, 33549. Minneapolis: Univ Of Minnesota Press.
Introduction

Pedagogy in the Digital Humanities is now leaving its “bracketed” state - a term used by HIRSCH 2012 to emphasise the fact that this dimension was not given the consideration its practical importance deserves. As programmes and courses are created on a larger scale and increasingly drive institutional strategies, also in Europe (see Sahle, 2013 and the DARIAH Digital Humanities Course Registry), it becomes essential to make comparisons and shared reflections possible.

Since 2014 all students of Greek and Latin languages and literatures at the Université Paris-Ouest Nanterre (France) have been enrolling in a Master programme entitled “Humanités classiques et humani-tés numériques.” Each semester features a fully fledged course of Digital Humanities: it is therefore an experiment in embedding Digital Humanities into an existing discipline, or rather into the array of disciplines which constitute the field of Classical studies around its philological backbone.

The aim of this poster is to share the approach I take in designing and teaching these courses, and to reflect on what this experience suggests about digital educational models, in Classics and beyond.

The poster will have three components, devoted to situating, describing and comparing the courses. Context and History

I will set out the conditions in which the curriculum was reformed (which involves both national and local contexts), the specific problems encountered (as the heterogeneous levels and motivations of the students, the relationships with the other courses, the available technical options, or the recent introduction of podcasting and distance learning), as well as the rationale and methods which shape the courses, including its main sources of inspiration in the Digital

Humanities community, whether online syllabi or publications like Jockers (2014) and Rockwell and Sinclair (2016).

Overview of the Courses

The courses alternately take the form of more traditional classes and collaborative or personal projects. Across the two years, their contents include theoretical and historical insights, while concentrating on hands-on experience: digital literacy elements are gradually integrated as students go from traditional scholarly editing recreated in Markdown and HTML to critical editing in TEI XML (the focus of year 1) and, beyond text and editing, discover computer-assisted analytical and visualisation methods with the Voyant Tools software environment and then work in a literate programming framework (For which the canonical reference is Knuth, 1984) implemented in R Markdown (the focus of year 2, see Figure 1).


Figure 1: Text analysis in RStudio

The principles of the courses will be expounded: favouring active participation, learning-by-doing and flipped classroom teaching; insisting on the critical, reflexive dimension of digital procedures; promoting free resources like TEI by Example (Van den Branden, Terras, and Vanhoutte) and The Programming Historian (Crymble et al), as well as data reuse; developing an open publication culture through the Classiques et numériques blog maintained by the students (see Figure 2) or a shared Zotero group library; creating an awareness of the surrounding Digital Humanities communities; fostering actual collaboration, both between the students and with other projects or programmes - to date, with another MA specialised in Web design on an online edition prototype, with the Pelagios Commons project on the annotation of place names and with the Sunoikisis

Digital Classics network in its effort to collectively define a core syllabus.


Figure 2: Classiques et numériques, the blog of the MA

Sahle, P. (2013). “DH Studieren! Auf Dem Weg Zu Einem Kern- Und Referenzcurriculum Der Digital Humanities.” DARIAH-DE Working Papers, 1. http://web-

doc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-1.pdf.

Van den Branden, R., Terras, M. and Vanhoutte, E. (n.d.)

“TEI by Example.” accessed 1 November 2016.

http://teibyexample.org/.

Comparing Models

Finally, drawing on this experience I will address several aspects of the current development of Di-gital Humanities pedagogy: as a separate entreprise or within established disciplines, with or without infrastructural, collegial or cross-departmental support, in various time formats, with different modes of external collaboration, etc. To sketch this broader typology, I will compare this French series of courses with other models, using in particular the data contributed to the aforementioned Digital Humanities Course Registry.

The poster will be in English, but I will naturally interact with the audience of the poster sessions both in English and French.

Bibliography

Crymble, A., Gibbs, F., Hegel, A., McDaniel, C., Milligan, I.,

Taparata, E., Visconti, A. and Wieringa, J. (eds.) (n.d.)

. The Programming Historian. http://programmin-ghistorian.org/.

Hirsch, B. (ed.) (2012). Digital Humanities Pedagogy: Practices, Principles and Politics. Open Book Publishers. http://www.openbookpublishers.com/reader/161.

Jockers, M. (2014). Text Analysis with R for Students of Literature. New York: Springer.

Knuth, D. (1984). “Literate Programming.” The Computer

Journal, 27(2): 97-111. http://comjnl.oxfordjour-

nals.org/content/27/2/97.short.

Rockwell, G. and Sinclair, S. (1984). Hermeneutica. Computer-Assisted Interpretation in the Humanities. Cambridge, Massachusetts: MIT Press.
In 2015, an initiative was started to set up a Dutch speaking DH+Lib community in the Netherlands and Belgium, based on the example of the American communal space of librarians, archivists, LIS graduate students, and information specialists to discuss topics ‘Where the Digital Humanities and Libraries meet’. At the initial meeting it became apparent that most participants were there to learn more about digital humanities and were not (yet) in the situation where they were able to offer expertise on the subject. On the administrative level, the directors of the libraries participating in the consortium of Dutch academic libraries (UKB) also expressed the wish that librarians become more fluent in DH.

A year later, the National Library of the Netherlands (Koninklijke Bibliotheek), and the University Library of the Vrije Universiteit Amsterdam again concluded that librarians at their institutes who wanted to get involved in DH needed more training to adequately support researchers and students in this field. Therefore both institutes joined forces to develop a set of clinics on DH for librarians. The two institutes were later joined by the Leiden University Libraries. We see this as the ideal opportunity to provide these educative sessions not only to our own librarians, but also to the academic librarians of other Dutch research libraries. In essence, we want to teach our country’s librarians the ins and outs of DH in order for them to take up their natural role of facilitating and supporting research and ideally become the research partner needed in DH projects.

The aim of these clinics is to provide basic methodological competencies and technical skills in DH, for a diverse group of librarians, consisting of both subject and technical librarians with basic technical skills. The content of these sessions should enable them to provide services to researchers and students, identify remaining gaps in knowledge or skills that they could address by self-directed learning and (perhaps) to automate their daily library work. We are not setting out to turn them into programmers or data crunchers, but want to boost their knowledge level to where they feel comfortable providing information about DH projects, follow the literature and research, follow online tutorials and hopefully take up the challenge of finishing this professional development by engaging with the DH community.

In order to design this curriculum we follow a four step approach with a Working Out Loud-principle (Williams, 2010):

1.    Desk research about what being a DH librarian entails (e.g. Hartsell-Gundy et al., 2015; Mulligan, 2016; also see the Zotero library of the LIBER Digital Humanities working group);

2.    Identify possible subjects, based on experience, a comparison of existing teaching material related to DH (e.g. The Programming Historian, the Digital Scholarship Training Programme at the British Library and Columbia University's Developing Librarian project) and the TaDiRAH taxonomy of research activities;

3.    Get feedback from researchers on possible subjects, based on the knowledge and skills they feel librarians need;

4.    Get feedback from librarians on possible subjects, based on already known gaps in their knowledge and skills.

With these in hand, we will design the curriculum of clinics, based on the method of 'constructive alignment' (Biggs et al., 2011), to make sure that the intended learning objectives and the teaching/learning activities stay aligned.

Our plan is to organize a maximum of 6 clinics, each one full day. Each day starts with one or more lectures by researchers, that address the conceptual knowledge needed. The afternoon sessions will be devoted to the hands-on training of skills, following the Library Carpentry model as closely as possible. By having researchers provide the lecture sessions, we hope to fuel the enthusiasm of the librarians with the inspiration of direct contact with researchers and to provide access to a network within and across universities. With these clinics, we hope to initiate a stream of DH activities in Dutch universities, making access to support easier for new digital scholars.

The poster at DH2017 will present the curriculum, its position in the international context and offer the lessons learned from both the design process and the first clinics. We welcome discussion about our efforts and the possibilities of applying this in other contexts. Bibliography

ACRL Digital Humanities Interest Group. (2012-) dh+lib

. http://acrl.ala.org/dh/ (accessed 31 March 2017g). Baker, J. (2014) British Library Digital Scholarship Training Programme: a round-up of resources you can use -

Digital scholarship blog. 30 October 2014. http://blogs.bl.uk/digital-scholarship/2014/10/brit-ish-library-digital-scholarship-training-programme-round-up-of-resources-you-can-use.html (accessed 31 March 2017c).

Biggs, J. B., Tang, C. and Society for Research into

Higher Education (2011). Teaching for Quality Learning at University: What the Student Does. Philadelphia, Pa.]; Maidenhead, Berkshire, England; New York: McGraw-Hill/Society for Research into Higher Education : Open University Press.

DARIAH (n.d.) TaDiRAH - Taxonomy of Digital Research

Activities in the Humanities

http://tadirah.dariah.eu/vocab/index.php (accessed 31

March 2017).

Hartsell-Gundy, A., Braunstein, L. and Golomb, L.

(2015). Digital Humanities in the Library: Challenges and Opportunities for Subject Specialists. Chicago: Association of College and Research Libraries, a division of the American Library Association.

Library Carpentry. (n.d.) Library Carpentry - Software skills for library professionals http://librarycar-pentry.github.io/ (accessed 31 March 2017d).

Mulligan, R. (2016). SPEC Kit 350: Supporting Digital Scholarship. Association of Research Libraries http://publi-cations.arl.org/Supporting-Digital-Scholarship-SPEC-

Kit-350.

The Programming Historian (2012) The Programming Historian. http://programminghistorian.org/ (accessed 31 March 2017e).

Wilms, L. (2017). Zotero | Groups > LIBER Digital Humanities working group https://www.zotero.org/groups/li-ber_digital_humanities_working_group (accessed 31

March 2017f).

Williams, B. (2010). When will we Work Out Loud? Soon! TheBrycesWrite

https://thebryceswrite.com/2010/11/29/when-will-we-work-out-loud-soon/ (accessed 31 March 2017).

The Humanities and History Team at Columbia University. (2013) “The Developing Librarian Project”. dh+lib.

1 June 2013. http://acrl.ala.org/dh/2013/07/01/the-developing-librarian-project/ (accessed 31 March 2017b).
This poster describes the development and uses of Archivlo, an application for improving the archival research workflow and enabling a more collaborative digital research community. In recent years, digital history has emerged as a vibrant subfield of the digital humanities community (see Robertson, 2016; and Weingart, 2016).Currently, the majority of digital history projects rely on digitized corpuses or community compiled datasets. However, the archival materials used in these projects represent only a small fraction of the archival sources that scholars currently utilize in their research.

Moreover, the proliferation of digital cameras and scanners has resulted in a wealth of archival material for scholars, but this digitized archival data is usually scattered across hard drives. To organize this data, scholars currently either keep notes or re-purpose bibliographic software.

Data management software provide some solutions to dealing with this abundance of material (such as Devonthink, Evernote, Zotero, and most recently, Tropy) but individual scholars often must invest a great deal of energy and time replicating the organizational structure of the archives to make sense of their research. This siloed approach to archival research makes finding information about archival collections or other scholars working in the archives difficult. Ar-chivlo is designed to solve these problems, and create a more coherent workflow for organizing archival data.

This poster will outline the development and design of Archivlo, from the early idea stages to our initial beta model. Archivlo is currently in progress, and the poster will share our experience building a web-based application, as well as designing a user interface that privileges data interoperability and flexibility. Archi-vlo is written in Python and Angular, and is fully opensource on Github. To access archival data, Archivlo utilizes archives' APIs and web page annotations to allow researchers to find collections. Users are able to save their archival collection research in their profile, and indicate whether they have worked in these archives or are interested in using the archive. This functionality adds efficiencies to how scholars locate and keep track of their archival research. Users can also export their records to multiple file formats, as well as other data management software, such as Zotero andDevon-think.

Additionally, Archivlo enables users to share their lists of visited and interested in archives, which we believe will help scholars share information about archives and potentially even form collaborations. For archivists, Archivlo can also provide data on user interest vis-a-vis usage of their archival collections. We believe our experience with Archivlo will be of interest to other digital humanities developers and project managers, as well as digital humanists who work with archival collections.

Previous efforts to encourage digital collaboration among researchers in archives have, with a few exceptions, largely faltered, with most of these projects requiring a high technical literacy to contribute to a database or extensive time to transcribe records (see Mostern and Arksey, 2016. Moreover, these efforts to construct large databases of archival data have been forced, through copyright restrictions, to limit their scope to material that is either from prior to the early twentieth century or born digital materials). Instead of requiring large resources to digitize materials or standardize collections, Archivlo presents an alternative solution to this problem - focusing on how scholars work with archives to enable more digital and collaborative research. We believe Archivlo will encourage more productive data management practices among scholars, and reduce inefficiencies in the archival research workflow. Much of Archivlo's goals remain experimental, and the opportunity to present our work at DH 2017 would help us share our progress and consider future directions for the tool.

Ultimately, we hope that Archivlo can help further the digital humanities ethos of digital collaboration, and present one solution for using tools to help foster digital research communities.

Bibliography

Mostern, R., and Arksey, M. (2016) “Don't Just Build It,

They Probably Won't Come: Data Sharing and the Social Life of Data in the Historical Quantitative Social Sciences”, International Journal of Humanities and Arts Computing, Volume 10 Issue 2, 205-224.

Robertson, S. (2016) “The Differences between Digital Humanities and Digital History” in Debates in Digital Humanities 2016, ed. Matthew K. Gold and Lauren F. Klein, (University of Minnesota Press, 2016), 289-307

Weingart, S. (2016) “Acceptances to DH2016 (pt. 1)”,

March 22, 2016, <https://scottbot.net/acceptances-

to-dh2016-pt-1/>.
This panel brings together three projects that explore and analyze annotated texts in digital environments: The Archeology of Reading in Early Modern Europe, The Winthrop Family on the Page, and Derrida’s

Margins: . All three projects seek to give users the experience of how another person read their books (see Notes). While each is fascinating in its own right, collectively these projects span centuries and provide a powerful and instructive lens on data modeling, interinstitutional collaboration, and interoperability.

The connections between these projects, both on a scholarly and an institutional level provide a powerful case study in creating distinct but interoperable digital humanities projects and resources. Both Winthrop and Derrida’s Margins are being built in the Center for Digital Humanities at Princeton University, while AOR is a collaboration between Johns Hopkins, University College London, and Princeton. AOR and the Winthrop project share a co-PI, Anthony Grafton, and two books owned and heavily annotated by John Dee are in the Winthrop library and thus came to New England. However, while the AOR project looks at a single renaissance annotator (Gabriel Harvey in Phase I- the project is heavily indebted to the methods and questions outlined in Jardine and Grafton, 1990-, and John Dee in Phase II), the Winthrop project looks at generators of annotators (men and women) making it as much a prosopographical project as a study of annotation. Like AOR, Derrida’s Margins focuses on a single scholar’s interaction with his library, but Jacques Derrida’s annotation practices differ dramatically from Harvey or Dee, and French copyright law poses serious challenges to representing the book pages online.

All three projects make use of the International Image Interoperability Framework (http://iiif.io/), but differ on the rest of their underlying data structures. AOR uses a custom XML schema to encode a critical edition of the annotations. Winthrop will use a custom prosopographical-bibliographical relational database, implemented in Django. While most of these relationships will be attested through annotation, the system will also record other connections (including purchasing records, books referenced in letters, etc). The annotation data model will most likely be created using a graph database, given the multiple types of annotations and the ways they can be expressed. Derrida’s Margins will be another custom relational database, built in Django, but will be fully bi-lingual, allowing users to search and browse in English and French. The Winthrop and Derrida database will also be exposed as Linked Open Data.

All three projects are committed to interoperability, with a key goal being the ability to search across the three projects once they are complete. The projects are in close contact the IIIF Editors and the W3C Web Annotation Working Group to ensure adherence to and input on best practices and emerging standards.

The Archaeology of Reading in Early Modern Europe
Earle Havens, Matthew Symonds,
Anthony Grafton
The Archaeology of Reading in Early Modern Europe (AOR), is an international collaboration among the Sheridan Libraries at Johns Hopkins University, the Centre for Editing Lives and Letters (CELL) at UCL, and the Center for Digital Humanities at Princeton University Library, with funding from the Andrew W. Mellon Foundation. It is also an interdisciplinary collaboration between historians, librarians, software engineers and data scientists.

While the body of scholarship on the history of early modern reading practices has burgeoned during the past several decades—guided in large part by the initial scholarly work of our project partners, the late Professor Lisa Jardine and Professor Anthony Grafton—as a collective body of knowledge the history of reading has nonetheless remained limited to isolated, partial, and impressionistic studies of single texts read by single annotators.

As researchers, we conduct this work in the conspicuous absence of comparative evidence of the larger range of early modern historical reading practices, strategies, and agendas. Scholars also find it physically impossible to effectively penetrate the dynamic array of information preserved in annotated books for the purpose of systematic analysis owing to their sheer density of content, in many instances, relative to the original texts on which the annotations comment.

During the original planning workshop on annotated books that led to the formation of the project, leading scholars, librarians, curators, and technologists agreed collectively that these constraints imposed upon the study of early modern annotated texts in their original analog form could only be overcome satisfactorily when annotations are treated as data sets that can be mined and analyzed effectively in more versatile, enriched, and readily searchable digital forms.

The AOR team has elected to focus on a distinct and roughly contemporary dyad of clearly identified early modern readers: Gabriel Harvey in Phase 1 (20142016) and John Dee in Phase 2 (2016-2018). While the identities of a large majority of early modern annotators remains unknown in extant collections, this focus on known readers will enable the project team to analyze and more precisely situate the processes of reading and annotation within their respective historical contexts.

In order to make these sources more accessible to analysis, the AOR technology team - based at Johns Hopkins University's Digital Research and Curation

Center - is working closely with the International Image Interoperability Framework (IIIF) protocol and community to develop features and use cases that will enhance AOR within this larger framework.

IIIF features a set of protocols, application programming interfaces (APIs), and shared technologies for the presentation of web-based images. In the case of AOR, these images are digital surrogates of rare book materials containing manuscript annotations by Gabriel Harvey and John Dee, which users interface through AOR's adapted version of the Mirador (version 2.0) viewer. The technical infrastructure for AOR includes a data archive, an image server, a IIIF image service, a corresponding IIIF presentation service, and the IIIF-compliant Mirador viewer.

The data archive for this project provides the framework for long-term access to, and preservation of, all project content: an important contribution to Digital Humanities more generally, insofar as this issue has not yet been unaddressed within IIIF. The technology team has defined an archival data model that can be mapped to other data models for data access and presentation over time. Another layer of the infrastructure that has been developed in Phase 1 of AOR consists of an image server that accesses content from within the data archive. Currently, the team is utilizing a commercial FSI image server, though comparable image server resources (e.g. djatoka) may also be used.

While AOR has been deployed using the versatile Mirador 2.0 API, the AOR technology team has developed IIIF endpoints for any available image service and presentation service, so long as they can be accessed by an IIIF-compliant image viewer. All AOR data can be accessed through our Mirador2 IIIF-com-plaint viewer, which has been specifically enhanced to meet the use requirements identified by the team for both current and future users. Over the course of AOR Phase 1, the technology and scholarly teams have worked together closely to define and implement a set of use cases related to image viewing and manipulation, transcription viewing, and dynamic, query-building search capabilities.

The current AOR viewer is the culmination of this iterative development process, which presents users with a wide-ranging set of functionalities aimed at enabling new forms of research on the history of reading practices. One new method is informed by the use of insights gleaned from the data generated throughout Phase 1 to formulate new research questions for the humanities team to investigate.

By creating a corpus of important and representative annotated texts with searchable transcriptions and translations, we can begin to compare and fully analyze early modern reading, and place that mass of research material within a broader historical context. In so doing, we could also approach—not in isolation but as a dynamic, internally and institutionally complimentary, research team—the traditionally subjective study of reading in a demonstrably empirical, comparative, and systematic way.

All visible interventions by readers in the books -marginal notes, underlining, marks and symbols, et cetera - have been marked up according to a non-TEI XML schema developed by humanities and technology teams working in close cooperation. The Phase 1 corpus of thirteen books owned and annotated by Gabriel Harvey has generated 3,355 XML files, each representing a single page of an annotated book. The data contained within these XML files has allowed us to map the language Harvey used in his own annotations and highlighted in the original printed texts, to find interesting correlations, and to use those correlations to form new investigations into the history of reading, Harvey's own biography, and the wider history of ideas in the early modern period.

Derrida's Margins: Annotations from the Personal Library of Jacques Derrida
Katie Chenoweth
This paper will give an overview of Phase One of the Derrida's Margins project currently underway at the Center for Digital Humanities at Princeton University (CDH).

Derrida's Margins is a longterm project that aims to create a website and online research tool for annotations from the Library of Jacques Derrida, an archival collection housed at Princeton University Library's Rare Books and Special Collections that was first opened to the public in March 2016. Phase One of Derrida's Margins focuses on annotations related to Derrida's landmark 1967 work De la grammatologie, first translated into English in 1976 as Of Grammatol-ogy (hereafter OG, see Spivak's 2016 translation. This corpus will serve as a pilot data set for future work, allowing us to establish protocols, workflow, and a relational database model.

Jacques Derrida is one of the major figures of twentieth-century thought, and his personal library represents a major intellectual archive. The Derrida Library, consisting of about 19,000 published books and other materials, represents a lifetime of reading. But for Derrida, the act of reading was not a passive process: he engaged — even grappled — with what he read, covering pages with notes and cross-references, inserting other handwritten materials, quoting and adapting what he read into what he wrote. As Derrida himself said in an interview later in his life, his books bear "traces of the violence of pencil strokes, exclamation points, arrows and underlining” (for details related to Princeton's acquisition of the Derrida Library, see the article posted by Princeton University Libraries, 2015).

It was in OG that Derrida first articulated a new style of critical reading, which would become the foundation of the philosophy of “deconstruction.” Our online research tool will enable scholars to study the development of this philosophy in an unprecedented way by providing comprehensive digital access to the material annotations, marginalia, bookmarks, and other notes from Derrida's library that correspond to each quotation and citation in OG. Beyond making Derrida's annotations available digitally for the first time, this project seeks to enable researchers to understand the relationship between Derrida's published writing and his reading practices. As we move beyond OG in future phases, the website will also allow researchers to gain new insights into Derrida's library and his published writing as networks of texts, citations, and annotations.

OG was chosen as the pilot text not only because it is a foundational text for deconstructive reading, but for two additional reasons: 1) it is among the most widely read of Derrida’s works and, according to Google Scholar, by far the most frequently cited, constituting more than 10% of the total citations for Derrida as of November 1, 2016; 2) 2017 will mark the fiftieth anniversary of the initial publication OG, and a number of scholarly conferences and events are scheduled to discuss the text and its legacy.

In Phase One, we began by identifying all instances of citation (quotations, references, footnotes, etc.) in OG that lead us back to books and other reading materials from Derrida’s library. Each cited work from OG is entered into a Zotero library, which will eventually form the basis of a public bibliography for OG. We then located these references in Derrida’s copy (or, often, multiple copies—different editions, translations, etc.) of each work. Each instance of citation is given a tag in the Zotero library that indicates the source page from OG, the type of citation, the page number(s) in the cited text, whether or not the volume is present in Derrida’s Library at Princeton, and the presence or absence of annotation (here construed as a mark of any kind, verbal or non-verbal). Next, we will transcribe all marginal annotations and other markings, limiting ourselves to tagged pages, i.e., those that are explicitly referenced by Derrida in OG. Digital images and transcriptions of these annotated pages will form the basis of the website, allowing users to go “behind the scenes” of Derrida's reading practices.

Given that Derrida's work is widely read in the United States, France, and around the world by scholars from numerous disciplines in the humanities and social sciences, we anticipate that the audience for this project will be broad and international. The more targeted audience would be specialists of Derrida and deconstruction, as well as researchers in philosophy, literary studies, and intellectual history.

One major risk this project faces is the question of French copyright restrictions. Due to the fact that the majority of books owned by Jacques Derrida are copyrighted works whose copyright is still in force, there is a question regarding the legality of posting digital images of sections of those books on our website. Despite the fact that the copies of these works in JD's library have been annotated by the philosopher's hand, the works themselves are the intellectual property of their respective publishing houses in France. Pending clarification by the Office of the University Counsel regarding the amount of risk we are at liberty to take in this respect, we will make a decision as to the amount of text from those works that it is acceptable for us to display on our public website.

This project is intended as the first phase in a longterm project. Team members in Phase One will create a manual detailing their work process and workflow to facilitate and standardize the project going forward. At the end of our pilot phase with the CDH, we will have the following

The main outcomes and deliverables of this project are the following:

•    Bibliography of De la grammatologie, made available as public Zotero library

•    Digitization of all relevant pages from the Derrida library

•    Customized grayed-out images, if necessary

•    Annotations transcribed, translated, and tagged

•    Custom designed relational database to record and analyze Derrida's annotation practices and the anchor text they reference

•    Bi-lingual (French and English) Web portal created in Django to allow users to search and browse the annotations

•    Transcriber's manual

The Winthrop Family on the Page
Anthony Grafton, Jennifer Rampling, Christian Flow
“The Winthrop Family on the Page” will employ an extensive database of bibliographic information, biographical data, and marginal annotations to allow digital exploration of early modern lives and learned practices. Centering on the surviving library holdings of the storied Winthrop family, whose representatives included such prominent figures as John Winthrop, a founder and governor of the Massachusetts Bay colony, the final product will be a web platform affording a dynamic sense of how colonial readers interacted with texts and fellow readers. Users will be encouraged to follow the story along multiple axes, both dia-chronically—as family members communicated across decades, even centuries, in the margins of their shared books—and synchronically, as single readers followed references from one text to the other, leaving a bread-crumb trail of notation along the way. The experience will be further enriched by inclusion of information on how the Winthrops' books surface in other historical sources they left us, including journals and correspondence, ensuring once again vivid access not just to the texts, but to the people who read them.

The bulk of the project's database will consist in entries for some 300 books formerly owned by the Winthrops and currently held at the New York Society Library. Aside from bibliographical information for all of these books, the database will include a second module treating the marginalia they reveal. Here, the project will log the location of all manuscript notes in the collection; in addition, for a select, smaller subset of 50-60 books, high-quality digital images will allow users to view the marginal material directly. Several representative notes from the same sub-set will be translated and transcribed along with relevant anchor text, and all of the collected data will be encoded for search. Users will be able to come to the collection with queries about everything from annotations of a particular era, to those of a particular person, to those presenting a specific sign (e.g., the manicule) or handling a specific theme.

The defining characteristic of the project is its deft exhibition of a very particular source-base. The rich surviving Winthrop holdings, coupled with the opportunities for collection and display afforded by a digital platform, offer a rare opportunity to excavate and reassemble an Early Modern book collection, giving a concrete frame to the “intellectual space” within which the family lived and thought. Still more exciting is the chance to fill that space with dialogue: the project's careful curation, transcription, and (when necessary) translation of the marginalia that the Winthrops recorded as they read allows users to follow their intellectual journey between books and between generations. Those generations spanned not just time but space: because the Winthrops were a colonial family, their New England library had its roots and its first readers in Europe. Users of the collection are therefore positioned to pose and answer questions about how Early Modern knowledge made the transition from the Old to the New World. How, we wonder, might our understanding of the Salem Witch Trials be affected when we take into account the fact that Wait Still Winthrop, the Chief Magistrate of Massachusetts, could well have perused the marginal annotations of his European ancestor, Adam Winthrop, in the witchcraft section of the family library? How did reading and annotation practices originally cultivated in England filter into the learned arsenal of later colonial readers?

Queries like these are made actionable by the labors of a highly-skilled team: specialties in book history and annotation, in ecclesiastical history, in alchemical practice, and in the classical tradition will be amply represented. The project itself, which will offer direct access to an array of understudied material, is meant to have wide interdisciplinary appeal. Situated at the nexus of digital humanities and history of the book, straddling Europe and the colonies, the domains of intellectual history and learned practice, it will resonate with historians of Europe, America, and the Atlantic World; book historians; librarians; historians of science, medicine, and religion; and users outside the Academy. And in future years the appeal will only grow more broad-based, for project members hope to build outward from the corpus on which the 2016-17 work is founded. The more material at hand, the richer the web of connections users will be able to make between books and readers. There are several possible modes of expansion: (1) full imaging, transcription and translation of all books in the NYSL corpus (that is, including those beyond the subset currently targeted for the pilot-phase of the project); (2) inclusion of further Winthrop books housed at other major repositories, including items from the eighteenth and nineteenth centuries (when the Winthrops added hundreds of new books to the library, including a collection of almanacs now housed at the Houghton Library, and the bulk of the collection of Winthrop books at Allegheny College); (3) development into a broader study of Colonial reading practices and knowledge networks, involving other family collections of annotated books, such as the Mather Family Library, now held at the American Antiquarian Society; the library of Thomas Prince, now at the Boston Public Library; and the library of James Logan, now at the Library Company of Philadelphia.

Notes
1. There are a number of projects dedicated to recording annotations or marginalia, notably the

crowd sourced Annotated Books Online and Book Traces. The three projects discussed in this panel proposal differ in that they are specifically grouped by annotator and want to answer specific questions about the worldview and read patterns of known annotators. They are also designed to facilitate more complex queries. These projects are also addressing a different question from other annotation projects such as Annotation Studio which are designed for modern readers to annotate digital texts.

Bibliography
Derrida, J. (1967), De la grammatologie (Paris: Editions du Seuil).

Derrida, J., (2016) Of Grammatology, trans. Gayatari Chakravorty Spivak (Baltimore: Johns Hopkins University Press).

Jardine, L., and Grafton, A. (1990)“‘Studied for Action': How Gabriel Harvey Read His Livy,” Past & Present, no.

129: 30-78.

Princeton University Libraries (2015). Princeton University Library Acquires Jacques Derrida's Personal Library.

31 March, http://library.princeton.edu/news/2015-03-31/princeton-university-library-acquires-iacques-der-

ridas-personal-library

        
            Arts and Humanities research has to address new challenges raised by the increasing amount of digital sources, contents and tools. New digital practices and protocols, new digital methodologies and services, new software and databases, offer a completely renewed framework for research, and encourage the emergence of a next generation of digitally-aware scholars. 
            Digital infrastructures, such as PARTHENOS, aim at supporting and accompanying the rise of this new generation of scholars by offering innovative solutions to connect digital tools and contents to Arts and Humanities researchers’ needs. PARTHENOS has thus acknowledged the growing importance to develop a data-centered strategy for the management of scientific data (European Commission, 2010), and is currently developing the Standardization Survival Kit (“SSK”) to help Arts and Humanities scholars understand the crucial role that proper data modelling and standards have to play in making digital contents sustainable, interoperable and reusable. 
            Accompanied by a live demo of the website
                
                     The beta-version of the website can be found here: 
                        
                            https://ssk-application.parthenos.d4science.org/ssk/#/scenarios
                        
                    
                , the poster will be composed of three parts: introducing the Standardization Survival Kit or “SSK”, using the SSK, customizing the SSK.
            
            Even if it is not obvious that the Arts and Humanities would be well-suited to taking up the technological prerequisites of standardization, it is yet essential that standardization takes a crucial role in the management of Arts and Humanities data. In this framework, this poster will present the Standardization Survival Kit, an overlay platform dedicated to promote a wider use of standards within Arts and Humanities. This comprehensive interface aims at providing documentation and resources concerning standards (especially authoritative references for each standard such as sources, Standard Development Organizations), and at covering three types of activities related to the deployment and use of standards in the Arts and Humanities scholarship: documenting existing standards by providing reference materials, supporting the adoption of standards, and communicating with all Arts and Humanities research communities.
            The SSK is designed as a comprehensive interface for guiding Arts and Humanities scholars through all available resources (collected within a dedicated Zotero library
                
                    
                        
                            https://www.zotero.org/groups/427927/parthenos-wp4
                        
                    
                ), on the basis of reference scenarios identified since the beginning of the project (PARTHENOS, 2016). The interface intends to provide a single entry point for both novice and advanced scholars in the domain of digital methods, so that they can have quick access to the information needed for managing digital content, or applying the appropriate method in a scholarly context. Users will be able to explore the platform according to their needs, thanks to precise research criteria: disciplines, standards, research activities and research objects. The poster will show how an Arts and Humanities scholar can navigate the Standardization Survival Kit website, by taking the example of an actual reference scenario. A live demo of the interface will also accompany the presentation, so that those interested in the poster will be able to search the website according to their needs.
            
            To stress the importance of standards for Arts and Humanities scholarly work, let us take the example of a sociologist who is a novice in digital methods, but who wants to disseminate a collection of field survey data online, so that they could be used by other researchers in the long-term. By browsing in the SSK, she or he will find a standardized scenario that could be perfectly suited to her or his needs: “Encode and modelize field surveys for their online dissemination”. The poster will follow this researcher exploring this reference scenario, and going through its nine steps
                
                     1. Obtain the informed consent of the participants, 2. Collect and Classify, 3. Select and digitize, 4. Anonymize, 5. Convert into sustainable formats, 6. Transcrive the interviews, 7. Add metadata, 8. Contextualize the research, 9. Disseminate and archive.
                 with the associated resources. Let us take some of the scenario’s steps as examples:
            
            
                the fourth step “Anonymize” offers a curated and up-to-date list of resources to help the researcher respect ethical practices and adopt proven techniques for anonymizing the collected data.
                the second and sixth steps stress on the importance of using tested standard - such as EAD to “Collect and classify” the data, and TEI to “Transcribe the interviews” -, highlight the importance of proper data modelling before disseminating them, and give access to appropriate resources on the subject.
            
            More advanced users will also be able to edit the scenarios themselves, by submitting new resources or adding new steps. They can also create new scenarios. The SSK scenarios and steps can be easily extended, reused and customized, thanks to their flexible data model in TEI
                
                    
                        
                            https://github.com/ParthenosWP4/SSK/spec
                        
                    
                . A dedicated interface in the Standardization Survival Kit will enable users to make suggestions, automatically converted in TEI according to the appropriate schema. The poster will present this interface and the associated functionalities. And for those who will be eager to test it, a live demo will be provided.
            
        
        
            
                
                    Bibliography
                    
                        Romary, L., Banski, P., Bowers, J., Degl’Innocenti, E., Ďurčo, M., Giacomi, R., Illmayer, K., et al. (2017). 
                        Report on Standardization (Draft). Technical Report Inria https://hal.inria.fr/hal-01560563 (accessed 27 April 2018).
                    
                    
                        Romary, L., Degl’Innocenti, E., Illmayer, K., Joffres, A., Kraikamp, E., Larrousse, N., Ogrodniczuk, M., Puren, M., Riondet, C. and Seillier, D. (2016). 
                        Standardization Survival Kit (Draft). Research Report Inria https://hal.inria.fr/hal-01513531 (accessed 27 April 2018).
                    
                    (2018). 
                        SSK: Development of the Standardization Survival Kit. XSLT ParthenosWP4 https://github.com/ParthenosWP4/SSK (accessed 26 April 2018).
                    
                    Riding the Wave. How Europe can gain from the rising tide of scientific data, 
                        FOSTER FACILITATE OPEN SCIENCE TRAINING FOR EUROPEAN RESEARCH https://www.fosteropenscience.eu/content/riding-wave-how-europe-can-gain-rising-tide-scientific-data (accessed 26 April 2018a).
                    
                    Standard Survival Kit https://ssk-application.parthenos.d4science.org/ssk/#/ (accessed 26 April 2018b).
                
            
        
    

        
            
                Introduction
                What is the meaning of scale in historical writings and migration narratives? Can digital tools and methods assist the detection of scale-related patterns in these categories of documents? May this enquiry be formalised into a system for scale analysis in texts? To address these questions, the paper combines theoretical background from historical, historiographical, linguistic and literary studies with digital tools and methods for text analysis and visualisation. The project is in an early phase; theoretical hypotheses and preliminary experiments are presented.
            
            
                Methodology
                Two types of corpora were considered: (1) historiographical - history writings mingling micro and global perspectives; (2) historical - migration narratives (autobiography). The first, in which variations of scale are clearly present, will serve to develop a prototype. The second, where representations of scale are more difficult to assess, will be used to test the approach.
                
                    Corpora
                    Although recent research in “global microhistory” (Trivellato, 2011) draws attention to the variable scale representation in history, the question of how this phenomenon is expressed through language in historians’ discourse is less studied. Research enquiries may be related to: topics distribution pertaining to scale (local to global, micro to macro); “story” versus ”study” distinctions (Kracauer, 2014: 122); epistemological explorations (Boudon, 1991). Corpus (1) samples: Brook (2009), Rothschild (2013), Wills (2001).
                    Corpus (2) is intended to East-West migration narratives, e.g. Kaminer (2011), Kassabova (2009), Verboczy (2017). Potential queries: representation of space and its scale-related particularities, e.g. the intimate, symbolic meaning, inspired by Bachelard (1957), of the old and new “home” (interior objects, house, street, city, country, continent) and its connections to geo-historical or cultural spaces, and a certain sense of belonging. Other elements could be considered: relations, names, events, time references.
                
                
                    Approach
                    The aim is to bridge “distant” and “close” reading, using zooming metaphor as an interpretative tool (Armaselu and Heuvel, 2017). Thus, a corpus/text can be explored via the hypothetical schema:
                     Level1: topic_X (obj_1, obj_2, …, obj_n)
                     Level2: topic_X.1 (obj_1.1, obj_1.2, …), topic X.2 (obj_2.1, obj_2.2, …), …
                     Level3: topic_X.1.1 (obj_1.1.1, obj_1.1.2, …), topic_X1.2, …, topic_X2.1, etc. 
                    Where, ‘obj_topic[.subtopic]’ represents a whole/section/fragment of a document associated to a topic and a scale-related logic. The system will allow zooming-in/out the different topics, traversing the conceptual space, e.g. from general to specific, and accessing the corresponding objects. One of the challenges is that the levels hierarchy and the degree of granularity may not be unique but depend on different “perspectives”. Corpus (1) can imply different viewpoints and objects grouped by topics on levels 1, 2: (a) world history – 17
                        th, 18
                        th century; (b) world history – trade routes, slavery; (c) world history – Europe, Asia, America. Some fragments generalise on world history, others discuss world trade routes between Europe, America and Asia, others narrow down to family history or paintings description. Like in a kaleidoscope, by rotating the device (changing the “magnifying-glass”), new patterns can emerge.
                    
                
            
            
                Proof of Concept (PoC)
                The PoC phase (in progress) will test these hypotheses on corpus (1). Two experiments on Brook (2009) are presented below.
                
                    
                
                Fig. 1. 
                    Vermeer’s Hat. Zotero - Paper Machines (topic modelling by subcollection/chapters)
                
                Figure 1 illustrates Paper Machines topics for each chapter. It is assumed that by combining these groupings with an analysis of the contexts where the corresponding words appear, e.g. co-occurrences, lexical chains, paths in a lexical-semantic hierarchy, a scale-related model of the text can be derived. Its levels may reflect how knowledge is organised, from synthesising, manipulating abstractions, through intermediate descriptions, to in-detail accounts referring to particular facts, persons, objects or quotations of sources.
                Figure 2 shows a visualisation via Z-editor (Armaselu, 2010). The scalable layout in chapter 2 (created manually) is explored by zooming through the European hatters history in the fifteenth and sixteenth century, the opening of the beaver pelts Canadian supply and Champlain’s fight with the Mohawks, the customs of wearing a hat and the rules of courtship in seventeenth century Netherlands, and, Vermeer’s painting, 
                    Officer and Laughing Girl, illustrating these practices.
                
                
                    
                
                Fig. 2. 
                    Vermeer’s Hat. Z-editor (zoomable text)
                
                Tools/methods currently under testing: topic modelling (MALLET), textometry (TXM), lexical-semantic resources (WordNet), Named Entity Recognition (GATE), lexical chains and text structure (Morris and Hirst, 1991), visualisation (graphs, textual zooming). The PoC outcome will consist of insight into the advantages/limitations of these tools/methods in building a prototype for scale analysis.
            
            
                Conclusion
                The paper presents theoretical points and experiments for a system dedicated to scale analysis in historical/historiographical texts. By a combined approach, evoking the metaphors of the magnifying glass and the kaleidoscope, the system may allow both scale-related patterns detection and perspective change.
                References
                
                    Armaselu (Vasilescu) F. (2010).Ph.D. Thesis, 
                    
                        Le livre sous la loupe : Nouvelles formes d'écriture électronique
                    , Papyrus, University of MontrealInstitutionalRepository.
                
                
                    Armaselu, F. and Heuvel, C. van den. (2017). 
                    
                        "Metaphors in Digital Hermeneutics: Zooming through Literary, Didactic and Historical Representations of Imaginary and Existing Cities"
                    , In 
                    Digital Humanities Quarterly (DHQ), Volume 11, Number 3.
                
                
                    Bachelard, G. (1957). 
                    La poétique de l’espace, PUF.
                
                
                    Boudon, P. (1991). 
                    De l’architecture à l'épistémologie. La question de l’échelle, PUF.
                
                
                    Brook, T. (2009).
                    Vermeer’s Hat.The Seventh Century and the Dawn of the Global World, Profile Books.
                
                
                    Kaminer, W. (2011). 
                    Russian Disco, Translated by Michael Hulse, Ebury Press.
                
                
                    Kassabova, K. (2009). 
                    Street Without a Name: Childhood and Other Misadventures in Bulgaria. New York: Skyhorse Publishing.
                
                
                    Kracauer, S. (2014). 
                    History. The Last Things Before The Last, Markus Wiener Publisher.
                
                
                    Morris, J. and Hirst, G. (1991). 
                    
                        "Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text"
                    , In 
                    Computational Linguistics, Volume 17, Number 1, Association for Computational Linguistics. 
                
                
                    Rothschild, E. (2013). 
                    The Inner Life of Empires. An Eighteenth Century History, Princeton University Press, 2011, paperback 2013.
                
                
                    Trivellato, F. (2011). 
                    
                        “Is There a Future for Italian Microhistory in the Age of Global History?”
                    , 
                    California Italian Studies, 2(1).
                
                
                    Verboczy, A. (2017). 
                    Rhapsody in Quebec. On the Path of an Immigrant Child. Translated by Casey Roberts. Montréal: Baraka Books.
                
                
                    Wills Jr., J. E. (2001). 1688. 
                    A Global History, New York, London: W.W. Norton &amp; Company.
                
                Tools
                GATE - General Architecture for Text Engineering, 
                    https://gate.ac.uk/. 
                
                MALLET - MAchine Learning for LanguagE Toolkit, 
                    http://mallet.cs.umass.edu/topics.php. 
                
                Paper Machines - 
                    http://papermachines.org/. 
                
                TXM – Textométrie project, 
                    http://textometrie.ens-lyon.fr/?lang=en. 
                
                Z-editor - 
                    http://www.zoomimagine.com. 
                
                WordNet - 
                    https://wordnet.princeton.edu/.
                
            
        
    

        
            
                INTRODUCTION
                Integrating the digital humanities (DH) into undergraduate level higher education programs has often been a difficult and ambiguous process. Faculty sometimes struggle to create syllabi that incorporate technologies but that do not require constant redesign as technologies evolve. Institutions may lack systems to connect students with faculty and staff who are interested in collaborative research, and collaboration beyond one’s own institution can be complicated or inaccessible for students. These are real challenges; as institutions increasingly develop DH courses and degrees, the impact on undergraduate students is diverse, ranging in minimal involvement, to career-altering. So, what should the role of the undergraduate in DH be, and how can we address these challenges? For the past three years I have explored these questions. This exploration has led to helping redesign and teach the foundational seminar for Hope College’s Mellon Scholars DH Program, as well as co-founding and chairing the Undergraduate Network for Research in the Humanities (UNRH), an undergraduate-led organization with the mission of reimagining the undergraduate role in DH through the establishment of a network of digital humanists who present research, collaborate, and share ideas. On the basis of these experiences as an alumna of Hope’s DH Program and UNRH Chair, I have been considering the ways in which faculty, staff, and institutions might support undergraduate DH researchers. My work has culminated in a series of models, programs, and initiatives that address the need for fostering the next generation of digital humanists in the classroom, at the institution, and beyond.
            
            
                METHOD
                
                    2.1-CLASSROOM The first challenge I consistently identified in DH courses was an incohesive structure that treated the digital and the humanities as separate units rather than an interconnected academic space. Secondly, seminar themes grounded in particular technologies had to be redesigned frequently as these technologies evolved or became outdated. This was the case for the year-long introductory seminar for Hope’s DH Program. Each year students felt that the seminar was two unrelated courses, one focusing on a particular area in the humanities, the other, teaching technologies like GitHub and data analysis. The course was a noble attempt but ultimately inconsistent, incohesive, and not a truly interdisciplinary approach to DH. I set about designing a seminar model that was adaptable to new technologies yet still focused on an intersectional theme. I consulted with educators at conferences and researched seminar formats at other institutions, but unsurprisingly there was a wide range of approaches that seldom emphasized independent research quite like Hope’s program. Thus, I grounded the seminar model in that very aspect: a chronological approach to independent research in the humanities. Over course of four units students engage with the evolution of humanities-based research and with the research process from beginning to end. During the first unit, students work in the archives, practice cataloging primary sources with tools like Zotero, develop strong but focused research questions, and discuss literature to answer the ever-present question “What is DH?” The second unit follows the progression in humanities-based research, moving from sources like libraries and datasets into the first examples of DH: text analysis. Students curate their own text-based datasets, analyze and visualize them, present them with Omeka, and discuss research project methodologies of source compilation and argumentation. The third unit it titled: CCP-Collaboration, Communication, &amp; Presentation. It involves group research collaboration and finalizing research projects through effective communication and presentation. Students complete writing workshops in which they must adapt a piece of writing for different audiences and styles, from conference abstracts to blogs and tweets; they also practice oral and web presentation skills. The final unit addresses advanced topics and tools which require students to focus on race, gender, sexuality, politics, and socioeconomic status. Students learn that equity and accessibility are paramount when creating public scholarship, digital or otherwise, and they are exposed to a survey of technologies in efforts to broaden their concept of what form research can take. The outcome of this course should be a comprehensive and diverse approach to humanities-based research projects through the chronological progression that research in the humanities has followed.
                
                
                    2.2-INSTITUTION For collaborative research, students and faculty alike find it challenging to make necessary connections with one another in the four short years that students have on campus. My solution is Bin(d)r: the Baccalaureate Interdisciplinary Network for (Digital) Research. Stemming from an initial idea of a physical binder with pages featuring the profiles of faculty, staff, and students interested in collaborative research, Bin(d)r: is ideally implemented as a searchable database of anyone on campus with research interests and skills. It is like Tinder for academics. All faculty and staff interested in collaborating simply create a profile on a site with tools like WordPress’s “Ultimate Member” Plugin. Students are invited to create profiles if they are interested in research. By including specific research interests and skills, faculty and students can get “matched” in a timely manner. Bin(d)r: has parentheses around “digital” because this tool does not have to be exclusively for digital projects, but it would provide an extra level of support for digital projects, connecting computer science students with humanities faculty, for example. Bin(d)r: is capable of being entirely free, low maintenance, highly interdisciplinary, and ultimately a tool for encouraging undergraduate research. Furthermore, if the digital Bin(d)r: takes off at numerous institutions, searching others’ databases would foster cross-institutional collaboration.
                
                While considering the institutional level, I would also argue that institutions must make space to hear the voices of their students. I propose that institutions establish a quarterly forum for undergraduates, faculty, and administrators to gather and discuss how the institution can better support students. Academic institutions are designed first and foremost to educate their students, so I assert that students have the right to tell institutions how they can improve, and institutions have the responsibility to listen. Simply creating space for dialogue is empowering.
                
                    2.3-BEYOND I also argue that empowering undergraduate researchers means providing agency, accreditation, and opportunities to join a community. Because DH is emerging at different rates across the globe, many students never meet other students engaging in their work. Furthermore, exposure to different methodologies, technologies, and project ideas has a profound impact. Faculty and staff gain this exposure at academic conferences and within their departments. UNRH aims to give this space and community to students, too.
                
                Our method of creating UNRH relied heavily upon initial organization, forming a Steering Committee, review system, and website. The format of our conference was meticulously designed. We created a “speed-dating” session for rapid introductions and elevator pitch practice, a formal project presentation session, informal poster-style presentation sessions, a keynote address, and workshop sessions. These workshops include technology tutorials, panel discussions about different students’ roles and experiences at their institutions, and design-thinking sessions to address the needs and concerns of students striving to develop DH projects.
                Beyond the conference we have been developing an online network space in which students create profiles and can share project updates, articles, conference opportunities, and requests for peer review. In essence, each of our decisions was an effort to create space and flexibility for students to answer for themselves the question of what the undergraduate role in DH can be.
            
            
                RESULTS
                
                    3.1-CLASSROOM The feedback from my students who experienced my seminar model have been positive. The survey results indicate that the seminar has largely met the learning outcome goals, and students indicated increases in confidence and preparedness in conducting independent research (approximately 30% average increase) and using new technologies (approximately 37% average increase) according to a seven-point scale. Those who indicated having less prior experience (1-4) had an average increase of about 33% in independent research and about 39% in technology use. I plan to track program retention rates in the coming years to hopefully see improvements as the sophomore students navigate from the structured seminar into the independent research spaces of their junior and senior years.
                
                
                    3.2-INSTITUTION Bin(d)r: has not yet been implemented but is in development for implementation at Hope College in the coming year.
                
                
                    3.3-BEYOND The results of our efforts exceeded expectations. Since our first conference in 2015, we have accepted over 50 projects, involving over 80 undergraduates from 31 institutions all across the United States, Canada, Nigeria, and Pakistan. According to in-person comments and our post-conference evaluations, students have felt empowered, encouraged, and independent in their research. Moreover, students were amazed at what they learned and accomplished by interacting with undergraduates from other institutions.
                
                Through our initial design and modifications over the years, we feel confident in the model for an organization and conference that grants agency to undergraduates, and space to understand their own roles. Now in my third year as Project Manager/Chair, when I consider again the undergraduate role in DH, I think of students as connected learners and independent researchers pursuing their own interests while learning from peers and mentors alike. Within and beyond this space, each student must determine her role for herself.
                Instructors, institutions, and organizations, invest in these students, for they are the next generation of digital humanists.
            
        
    

        
            The CWRC-Writer XML/RDF editor is the centerpiece of the Canadian Writing Research Collaboratory (CWRC) platform for the production, hosting, and dissemination of digital humanities scholarship. In development since 2011 and launched with the platform in 2016, the browser-based editor has reached maturity and stability. Well prior to this, the team had begun strategizing towards sustainability. We outline this strategy while highlighting features of the editor.
            Compared to some outcomes of digital dumanities tool building – such as gaining new insights into one’s own research – the effort of turning a tool into a sustainable, generalized service is less glamorous, more laborious, and less acknowledged. Tool-building is considered part and parcel of the scholarly work of DH 
                (Schreibman and Hanlon, 2010) and is beginning to be recognized by academic reward systems. Yet scant support and rewards accrue once software is up and running. This situation has changed little over the years, despite increasing concern regarding digital infrastructure sustainability generally 
                (Eghbal, 2016; Maron and Pickle, 2014) and attention to “care and repair” within DH 
                (Nowviskie, 2015; Sayers). Like all software, DH tools require maintenance, enhancement and updates, which is to say, continued funding and expertise. 
            
            Pursuing uptake seems like a natural approach to the sustainability dilemma, since:
            
                it is easier to demonstrate the success of a tool and to justify further resource allocation in light of increases in use; and
                adopters of a tool are invested in its survival and might put resources towards sustainability.
            
            However, uptake is no guarantee of sustainability. As observed by Cameron Neylon, many scholarly infrastructures are public goods, and “Finding sustainability models to support them is a challenge due to free-loading, where someone who does not contribute to the support of an infrastructure nonetheless gains the benefit of it” 
                (2017: 3). Nevertheless, unused tools are poorly positioned to request continued funding or support.
            
            The uptake or adoption of existing DH software by new users is far from guaranteed, even if it fulfills a need that it is well-documented in the research community where it originates. Fred Gibbs and Trevor Owens crystallize the ways in which tool uptake is hindered by multiple factors 
                (2012). Significant problems include:
            
            
                
                    managing expectations, while also scaling up functionality from local to more general needs (Koeser and Hicks, 2018);
                
                limited 
                    learning resources (examples, user documentation);
                
                unintuitive or complex 
                    user interfaces that discourage novice users;
                
                lack of support for 
                    standards and interoperability.
                
                community building
            
            Together with more mundane but important activities like code maintenance, stable hosting, and systems administration, these factors create challenges that can prove fatal to promising technologies. Some are proclaimed at digital humanities conferences but seldom heard from again, while others like Paper Machines 
                (Guldi and Johnson-Roberson, 2012) show immense promise but do not develop into fully robust tools. Even mature tools with uptake from a wide range of users, such as Gephi, live quite precarious lives 
                (Jacomy, 2018). In short, the challenges of sustaining tools are manifold. We use the above points as a rubric for reflecting on CWRC-Writer’s engagement with the challenges of uptake.
            
            
                Scaling features and expectations 
                The modular CWRC-Writer exists in several types of installation to suit users from novices to technical experts: 
                
                    CWRC-Writer: available to researchers within the CWRC platform, where it is integrated with an Islandora repository, 
                    Git-Writer (
                        cwrc-writer.cwrc.ca): uses GitHub’s file storage, versioning, and authentication to allow anyone to edit GitHub-hosted XML documents (Fig. 1).
                    
                    Installations by third parties in other software stacks.
                        
                    
                
                
                    
                
                Fig. 1. Git-Writer document loading interface
                To support a wide variety of users, CWRC-Writer provides these core features:
                
                    an interface that renders XML in a human-readable layout using CSS (Fig. 2);
                    XML tagging, with or without tags showing, with validation and error identification;
                    raw/source XML editing for experts;
                    entities tagging in XML and/or Web Annotation RDF with built-in authority lookups.
                
                Members of the DH community, as well as literary and cultural studies scholars using XML for their texts, were involved from the beginning in the design of the tool. The user group comprises both power-users – researchers with decades of experience in markup – and novice or occasional users with little familiarity with DH. CWRC-Writer was designed from the outset as a light-weight editor to allow novices to tag XML documents and link them to named entity authorities, such as the Virtual International Authority File, in a manner that would avoid the steep learning curve associated with other, more complex editing tools 
                    (Brown, 2015). 
                
                This lightweight usage is our main use case. CWRC-Writer does not aim to replace a full-featured XML editor for heavy-duty markup or transformations. The complexity of managing XML through an HTML front-end mean that major restructuring, for instance, is very tricky. To ensure that available affordances are aligned with the needs of the users, CWRC-Writer offers three different editing modes: 
                
                    A default combined XML &amp; RDF mode creates both XML tags and Web Annotations identifying entities in the same span of text; 
                        external named entity identifiers are mapped onto the equivalent tags within supported XML schemas - which include established 
                        
                            TEI customizations
                        
                         and other schemas employed by CWRC-supported projects. 
                    
                    RDF-only mode for Web Annotations that leave the body of the XML file untouched.
                    XML-only mode for tagging without adding any Web Annotations. 
                
                
                    
                
                Fig. 2. CWRC-Writer document showing application of CSS
                In conjunction with particular user communities, we are extending CWRC-Writer functionality based on a document’s schema declaration; for example, for EpiDoc files, a popup editor for translations will allow users to create or tag a translation while viewing it side-by-side with the original. To support transcription, side-by-side display of the XML and images allows transcribers to view the scanned manuscript within the tool (Fig. 3).
                
                    
                
                
                    Fig. 3. Editing interface (XML &amp; RDF mode) with side-by-side display of manuscript scan
                    
                
            
            
                Learning resources
                We mitigate the challenge of a new interface by providing extensive, searchable 
                    
                        user documentation
                     (produced with the DITA documentation standard) and 
                    
                        tutorial videos
                    , as well as virtual office hours for real-time support. Learning to apply markup is a major challenge for the uninitiated, so there are sandbox templates for fooling around. Projects can create customized document templates that can be used to kickstart content creation and editing. These can provide highly detailed instructions, in order to promote consistency and best practices. 
                
            
            
                A user-friendly interface 
                From 2012 on, CWRC-Writer has undergone successive rounds of user testing, which have informed feature development and UX improvements. Two extensive rounds of survey-based user-testing were conducted before 2016, followed by numerous informal consultations and feedback from users and workshop participants. CWRC-Writer code is available in GitHub and a ticketing template allows adopters to submit both feature requests and bug reports. Formal announcement of the GitHub version in 2019 will be followed by another round of systematic user testing. 
            
            
                Standards and interoperability 
                CWRC-Writer editor adheres to the standards for both markup and Web Annotation. An integrated XML validator allows users to validate against the declared schema as they work on the document. TEI is supported in all version of the editor. RDF annotations adhere to the Web Annotation Data model, a W3C Recommendation that is being widely adopted within DH and in the scholarly publishing community as a standard for annotation data.
            
            
                Promoting a community of users 
                In addition to passive adopters, who employ CWRC-Writer as made available through CWRC or GitHub, we have projects joining CWRC primarily thanks to its integration of the editor with other tools. There is growing interest from members of the DH community considering it for use in TEI editing projects, as components of library-based DH tool suites, or for teaching XML. The Center of Digital Humanities Research at Texas A&amp;M has produced a containerized version and has installed it on top of Fedora 4 as part of a larger toolkit. Bucknell University is installing a version of the Git-Writer to support diverse local DH projects, and other institutional installations are planned. External partners were consulted for the development of Git-Writer, and the code is configurable, modular, and well documented in order to permit installation in a range of software environments. Users currently cohere around specific projects. We hope a broader CWRC-Writer community will develop as numbers grow, and be joined by a community of developers familiar with and willing to contribute to upkeep. However, the experience of other projects indicates that this is a major challenge.
            
            
                Future developments
                CWRC-Writer has for several years now, since its launch within CWRC, been thinking hard about how to promote uptake and long-term sustainability. Our development roadmap is constructed around current and oncoming user needs. We will continue to adapt our strategy in response to insights gained from further user testing and feedback from the community following the launch of the Git-Writer to the DH community.
            
        
        
            
                
                    Bibliography
                    
                        Brown, S. (2015). Remediating the Editor. 
                        Interdisciplinary Science Reviews, 
                        40(1): 78–94 doi:10.1179/0308018814Z.000000000106.
                    
                    
                        Eghbal, N. (2016). 
                        Roads and Bridges: The Unseen Labor Behind Our Digital Infrastructure. Ford Foundation https://fordfoundcontent.blob.core.windows.net/media/2976/roads-and-bridges-the-unseen-labor-behind-our-digital-infrastructure.pdf.
                    
                    
                        Gibbs, F. and Owens, T. (2012). Building Better Digital Humanities Tools: Toward broader audiences and user-centered designs. 
                        Digital Humanities Quarterly, 
                        006(2).
                    
                    
                        Guldi, J. and Johnson-Roberson, C. (2012). 
                        Paper Machines. http://papermachines.org/.
                    
                    
                        Jacomy, M. (2018). Is Gephi obsolete? Situation and perspectives. 
                        Gephi Blog https://gephi.wordpress.com/2018/11/01/is-gephi-obsolete-situation-and-perspectives/ (accessed 27 November 2018).
                    
                    
                        Koeser, R. S. and Hicks, Benamin (2018). Bridging Digital Humanities Internal and Open Source Software Projects through Reusable Building Blocks. Mexico City, Mexico https://dh2018.adho.org/en/bridging-digital-humanities-internal-and-open-source-software-projects-through-reusable-building-blocks/ (accessed 27 November 2018).
                    
                    
                        Maron, N. L. and Pickle, S. (2014). 
                        Sustaining the Digital Humanities Host Institution Support beyond the Start-Up Phase. ITHAKA.
                    
                    
                        Neylon, C. (2017). Sustaining Scholarly Infrastructures through Collective Action: The Lessons that Olson can Teach us. 
                        KULA: Knowledge Creation, Dissemination, and Preservation Studies, 
                        1(1): 3 doi:10.5334/kula.7.
                    
                    
                        Nowviskie, B. (2015). On capacity and care 
                        Bethany Nowviskie http://nowviskie.org/2015/on-capacity-and-care/ (accessed 27 November 2018).
                    
                    
                        Sayers, J. From Make or Break to Care and Repair 
                        MLab in the Humanities. http://maker.uvic.ca/inke16/ (accessed 27 November 2018).
                    
                    
                         (2015). Remediating the Editor. 
                        Interdisciplinary Science Reviews
                        40, (1): 78–94 doi:10.1179/0308018814Z.000000000106.
                    
                    
                        Eghbal, N. (2016). 
                        Roads and Bridges: The Unseen Labor Behind Our Digital Infrastructure. Ford Foundation https://fordfoundcontent.blob.core.windows.net/media/2976/roads-and-bridges-the-unseen-labor-behind-our-digital-infrastructure.pdf.
                    
                    
                        Gibbs, F. and Owens, T. (2012). Building Better Digital Humanities Tools: Toward broader audiences and user-centered designs. 
                        Digital Humanities Quarterly, 
                        006(2).
                    
                    
                        Guldi, J. and Johnson-Roberson, C. (2012). 
                        Paper Machines. http://papermachines.org/.
                    
                    
                        Jacomy, M. (2018). Is Gephi obsolete? Situation and perspectives. 
                        Gephi Blog https://gephi.wordpress.com/2018/11/01/is-gephi-obsolete-situation-and-perspectives/ (accessed 27 November 2018).
                    
                    
                        Koeser, R. S. and Hicks, Benamin (2018). Bridging Digital Humanities Internal and Open Source Software Projects through Reusable Building Blocks. Mexico City, Mexico https://dh2018.adho.org/en/bridging-digital-humanities-internal-and-open-source-software-projects-through-reusable-building-blocks/ (accessed 27 November 2018).
                    
                    
                        Maron, N. L. and Pickle, S. (2014). 
                        Sustaining the Digital Humanities Host Institution Support beyond the Start-Up Phase. ITHAKA.
                    
                    
                        Neylon, C. (2017). Sustaining Scholarly Infrastructures through Collective Action: The Lessons that Olson can Teach us. 
                        KULA: Knowledge Creation, Dissemination, and Preservation Studies, 
                        1(1): 3 doi:10.5334/kula.7.
                    
                    
                        Nowviskie, B. (2015). On capacity and care 
                        Bethany Nowviskie http://nowviskie.org/2015/on-capacity-and-care/ (accessed 27 November 2018).
                    
                    
                        Sayers, J. From Make or Break to Care and Repair 
                        MLab in the Humanities. http://maker.uvic.ca/inke16/ (accessed 27 November 2018).
                    
                    
                        Schreibman, S. and Hanlon, A. M. (2010). Determining Value for Digital Humanities Tools: Report on a Survey of Tool Developers. 
                        Digital Humanities Quarterly, 
                        004(2).
                    
                
            
        
    

        
            
                Introduction
                The digital humanities (DH) community has reached an inflection point. Conceptual issues related to DH are now routinely discussed, and a significant body of literature about DH tools and methods exists, but very little is said about the challenges of maintaining the projects and tools that result from DH activity. The truth is that many teams are struggling with decades of accumulated technical debt, and the natural process of technological entropy. Lack of openness about the problems of sustaining and, when appropriate, archiving DH projects is natural: our problems expose weaknesses at the heart of our community, make us feel insecure when comparing ourselves to other teams, prompt us to question our relationship with funding agencies, and raise questions about the sustainability of our core activities. But we need to discuss the issues so we can increase our understanding, share best practices, and advocate for change. Long-term technical maintenance can be daunting, but good planning, carefully considered processes, transparent and healthy relationships with administrators and IT departments, and some common sense can resolve most issues.
                Our panel brings together four DH software engineering teams and initiatives, based in the United Kingdom, United States, and Canada, with responsibility for over 350 projects built over two decades. The panel aims to ‘clear the air’, by openly discussing the problems we face and detailing the security, maintenance, archiving, and sustainability solutions we have put in place to resolve them. The wide range of issues across the international DH community mean that we can only initiate a conversation, but we hope our commitment to full disclosure, coupled with a degree of geographical breadth, will help set the tone for a new era of collaboration and information sharing. Our long-term goal is to foster a culture of sustainability across the DH community, and respect for those engaged in the essential work of maintenance and repair.
                Our four panelists will describe the work of two DH centres and two archiving and sustainability projects, detailing the wide range of approaches they have adopted to manage technical but also operational (human resources, administrative) and financial complexities. Common themes are apparent across the four teams, related to forward planning and consideration of application and data life-cycle management, but differences exist too. Sometimes differences relate to technical philosophy (a decision to support heterogenous technologies, or a decision to focus on a more defined toolset); sometimes differences relate to the exigencies of local funding and operational realities (permanent versus fixed-term HR contracts, availability of internal or external maintenance funding). In all cases, however, our technical development is informed by a firm belief – resulting from hard experience – that maintenance and repair are integral to the art of making. Our panel members believe the next phase in the evolution of DH will require greater attention to the practical, epistemological, and methodological imperatives of maintenance and repair.
            
            
                Paper One: We Need to Maintain 100 Projects, Without Funding? Really? A Pragmatic Approach to Archiving and Maintenance
                King’s Digital Lab (KDL) was launched in November 2015. It was established to improve Digital Humanities (DH) software engineering process and quality, and ensure digital research is scalable and sustainable. It works in partnership with a DH department, and other departments across the Faculty of Arts &amp; Humanities at King’s College London, and comprises a Director, Project Manager, Systems Manager, and a team of 10 analysts, UI/UX designers, and engineers. The team inherited ~100 projects when it was established, built using heterogeneous technologies over two decades. The vast majority of those projects had no funding for maintenance and were running outdated operating systems and software as a result. Several had experienced minor hacks, and the infrastructure they used was approaching end of life. The Lab ‘estate’, although representing a significant corpus of high-quality DH research attracting ~250,000 unique users per year, constituted significant risk to DH at our university, and security risk to the wider university network.
                Over the last three years the team have undertaken a full audit of its projects and integrated archiving, sustainability, and research data management to its Software Development Lifecycle (SDLC). An internal process triaged the projects according to security risk, scholarly value, cultural and cultural heritage value, ‘brand’ value, and maintenance cost. Principal Investigators (PIs) were contacted and given a range of options to consider, from archiving their site to upgrading it and placing it under a Service Level Agreement (SLA), usually of between 2 and 5 years. Approximately 50 sites (most of which were of relatively low scholarly value, or merely proof of concept quality) were quickly archived, with the remainder being upgraded and moved to SLAs progressively over 18 months. A major infrastructure upgrade was implemented over the same period, providing 50TB of disk space, ~1TB RAM, and enterprise backup systems capable of supporting significant growth. The process of discussing projects with PIs and finding funding to upgrade and maintain projects was often difficult. The Faculty supported every project led by one of their staff members, and offered cost-recovery rates to all other projects, but many PIs had difficulty reconciling themselves to the need for ongoing maintenance and support or found it difficult to gain support from their administrators (even at cost-recovery level).
                At the time of writing all but a handful of projects have been resolved, but the process highlighted significant gaps in understanding across the DH community along with policy issues related to funding of DH projects. Archiving and sustainability issues have been reduced to an acceptable level in the Lab, but risks remain (as they do for any similar digital team), and ongoing management will be required. Archiving and sustainability are now integral to the Lab’s software engineering process, from requirements elicitation during concept development to archiving or ongoing maintenance in the post-funding period. The Lab’s processes are aligned, in turn, to the University’s Research Data Management (RDM) process and an effort is being made to align technical design with the national web archive.
            
            
                Paper Two: "Running God Knows How Many Versions of PHP": The Challenges of Successfully Sustaining Digital Projects
                In 2019, the Roy Rosenzweig Center for History and New Media is celebrating its 25th year, including a long legacy of both creating and sustaining digital history projects. Founded on the principle of democratizing history, RRCHNM is committed to creating open-source software and sustainable digital projects; millions of people use center-designed software such as Zotero, Omeka, and Tropy, while tens of millions visit the center's various project websites every year. Nor is this engagement limited to the center's most recent accomplishments; one of the center's older projects, the twenty-year-old History Matters, is still receiving over three million website hits annually.
                While this widespread and continuous engagement with center software and websites is a testament to the intellectual value of the center's digital scholarship, it also intensifies the sustainability pressure inherent in all digital projects. The center's earliest projects involved bespoke code which requires considerable labor either to maintain or to later transition to a content management system, e.g. Omeka, which was built in part to streamline the creation and management of the center's digital projects. Even projects running on a CMS still require upkeep, as new versions of the system or its components are released; external links break when websites restructure or eliminate content; and older technologies such as Adobe Flash are deprecated. Furthermore, the servers of which these projects live eventually reach the end of their hardware's lifespans and the projects must be migrated to new servers to survive.
                These technological challenges can be further exacerbated by logistical conditions that arise in a environment where collaborations happen across departments and institutions, projects have temporally limited funding support, and project team members are either soft-funded staff positions or graduate students who are—usually, though not always—transient members of an institution. While transdisciplinary and cross-institutional collaboration, grant funding, and graduate student involvement are generally seen as positive aspects of the digital humanities as a field, they lead to logistical challenges with respect to sustainability. The decentralization of knowledge across institutions and teams, as well as personnel discontinuity over time, leads to challenges in documenting and maintaining contextual knowledge around projects as they age, often requiring personnel to "reinvent the wheel" during the already challenging and un(der)funded process of technologically updating old projects.
                Despite these challenges, RRCHNM has sustained its digital projects over the course of its existence and—thanks to the institutional support it receives from its associated department and college, as well as the leadership of its previous and current directors—is well positioned to continue sustaining its digital projects for the foreseeable future. This paper will discuss both the institutional conditions that enable RRCHNM to sustain its digital projects as well as the technical and logistical challenges that it must overcome when continuously sustaining a wide variety of digital projects created by hundreds of people over the course of two and a half decades.
            
            
                Paper Three: Ruthless Principles for Digital Longevity
                
                    Project Endings is a SSHRC funded collaboration which aims to provide practical solutions to issues attendant on ending a project and archiving the digital products of research, including not only data but also interactive applications and web-based publications. Endings is a collaboration between the Humanities Faculty and the Library at the University of Victoria, and endeavours to align the aims of faculty researchers producing projects and the archivists who will eventually be responsible for curating their work.
                
                Using both practice-based methods and scholarly research, Endings is already producing recommended approaches (Holmes 2017; Arneil &amp; Holmes 2017; Holmes &amp; Takeda 2018) and diagnostic tools (Holmes &amp; Takeda 2017) that will assist scholars in ensuring that their project will be completed, archivable, functional, and available well in to the future.
                The project has conducted a survey with 128 project leaders, and conducted 28 follow-up interviews to gain insight in to the practical issues faced by DH scholars. Simultaneously it has been actively working on 'ending' several existing in-house projects (The Diary of Robert Graves, Le mariage sous L'Ancien Régime, The Map of Early Modern London, and a number of others) using the a set of principles developed from our work. These principles focus on reducing technological overhead and applying software development best practices to the planning and construction of a project’s digital outputs. Our methodology is based on paring back the range of technologies used to the absolute minimum (HTML, CSS and JavaScript), and building completely static web materials with no dependence on any server-side technologies.
                The Endings project divides digital projects into five primary components: data, products, processing, documentation, and release management. We aim at longevity primarily for data and products, but believe that this goal requires careful attention to processing, documentation and release management. We are developing preservation principles for of these factors, and this presentation will discuss key components of the principles along with their justification and practicality.
                Many of these principles are uncontroversial. For instance, principle 1.1, “Data is stored only in formats which conform to open standards and which are amenable to processing (TEI XML, GML, ODF, TXT)” would not be surprising to anyone. Others are more demanding and are likely to meet strong resistance from some members of a project team; programmers may be unsettled by the demand that there be “no dependence on external libraries: no JQuery, no AngularJS, no Bootstrap,” or puzzled by the requirement that “every page contains all the components it needs, so that it will function without the rest of the site if necessary, even though this means duplicating information across the site.” This “ruthless” set of maxims can make rapid development and deployment more difficult, but the principle of “hard now, easy later” is the only real guarantee of digital longevity for projects which, while they may be curated, are never likely to be actively maintained over the long term.
            
            
                Paper Four: Balancing Innovation &amp; Persistence in Digital Scholarly Publications
                In the past few years, several university presses have been awarded funding by the Andrew W. Mellow Foundation to meet the needs of digital humanists and social scientists who are pushing the bounds of traditional print publishing practices and seeking to output their arguments in a form that matches their methodologies. These multimodal, multilinear, open-access, web-based publications follow a parallel editorial and production workflow as traditional scholarly monographs, and as peer-reviewed scholarly works, carry the same weight in consideration of tenure and promotion for the scholars who create them. Stanford University Press is pushing farther than others by allowing authors to choose their own web-based platforms or builds rather than offering ones designed in-house. This openness introduces challenges for preservation and persistence of the publication. But rather than limit the potential for innovative expression, SUP is investing in exploring possibilities in digital preservation for complex interactive works.
                This program has seen the publication of four unique projects to date that could not possibly be rendered as print books but whose place in the scholarly record is just as important to their fields of study. It is important then that they can endure the specific threats that web-based digital content faces. The publisher needs to mitigate the potential fragility of these formats by keeping a handle on the complexity, but they must also support the innovative and courageous strides scholars are making as they rightly challenge the politics and limitations of traditional print publishing model. Essentially, they must balance innovation with durability.
                To intercept the dangers to persistence introduced in digital formats, SUP implemented a careful multi-phase set of guidelines for preservation and archiving. These start with pre-production technical recommendations that encourage but do not force authors to choose platforms and design elements that are durable and archivable. They also initiate a three-pronged preservation strategy during production and immediately after initial publication that can ensure viable access to and experience of the work once inevitable obsolescence creeps in to the live hosted product. These include documentation and digital repository deposit, web archiving, and emulation.
                While devoting resources not usually deployed in a university press context, SUP is also talking to authors before, during, and after the development and publication of their work and learning that expectations for their works’ longevity vary significantly. While most acknowledge and even embrace the risks and ephemerality of the digital, they also put trust in the Press to ensure the kind of persistence associated with traditional publications. Some expect to share the responsibility of persistence and others would be satisfied with a time stamp that covers academic milestones related to tenure and publishing expectations. A candid dialogue about existing and developing preservation practices in such programs will hopefully both assure scholars that publishers are aggressively pursuing preservation as part of the responsibility of publication, and also invite DH authors to share their expectations and concerns with the challenges digital presentation formats present.
            
        
        
            
                
                    Bibliography
                    
                        Arneil, S. and Holmes, M. (2017). Archiving form and function: preserving a 2003 digital project. Paper presented at the DPASSH Conference 2017: Digital Preservation for Social Sciences and Humanities, Brighton.
                    
                    
                        Ciula, A., Nyhan, J. and Moulin, C. (2013). ESF Science Policy briefing on research infrastructures in the digital humanities: landscapes, ecosystems and cultures
                        . Lexicon Philosophicum, 1: 277–87.
                    
                    
                        Hodder, I. (2014). The Entanglements of Humans and Things: A Long-Term View. New 
                        Literary History, 45(1): 19–36.
                    
                    
                        Holmes, M. and Takeda, J. (2017). Beyond Validation: Using Programmed Diagnostics to Learn About, Monitor, and Successfully Complete Your DH Project. Paper presented at the Digital Humanities 2017, Montreal https://dh2017.adho.org/abstracts/140/140.pdf.
                    
                    
                        Holmes, M. and Takeda, J. (2018). Why do I need four search engines? Paper presented at the Japanese Association for Digital Humanities Conference, Tokyo https://conf2018.jadh.org/files/Proceedings_JADH2018.pdf#page=58.
                    
                    
                        Holmes, M. (2017). Selecting Technologies for Long-Term Survival. Paper presented at the SHARP Conference 2017: Technologies of the Book, Victoria, BC https://github.com/projectEndings/Endings/raw/master/presentations/SHARP_2017/mdh_sharp_2017.pdf.
                    
                    
                        Maxwell, J. W., Bordini, A. and Shamash, K. (2017). Reassembling Scholarly Communications: An Evaluation of the Andrew W. Mellon Foundation’s Monograph Initiative (Final Report, May 2016). 
                        The Journal of Electronic Publishing, 20(1) doi:10.3998/3336451.0020.101. http://hdl.handle.net/2027/spo.3336451.0020.101 (accessed 26 April 2019).
                    
                    
                        Jackson, S. J. (2014). Rethinking Repair. In Gillespie, T., Boczkowski, P. J. and Foot, K. A. (eds), 
                        Media Technologies. The MIT Press, pp. 221–40.
                    
                    
                        King’s Digital Lab (2019). KDL’s pragmatic approach to managing 100 Digital Humanities projects, and more... King’s Digital Lab https://www.kdl.kcl.ac.uk/our-work/archiving-sustainability/ (accessed 26 April 2019).
                    
                    
                        Mulliken, J. (2018). 3 Approaches to the Preservation of Interactive Scholarly Works. SupDigital http://blog.supdigital.org/3-approaches-to-the-preservation-of-interactive-scholarly-works/ (accessed 26 April 2019).
                    
                    
                        Nicholson, C. (2018). Keeping the lights on. 
                        Research Europe: 13.
                    
                    
                        Nowviskie, B. (2015). Digital Humanities in the Anthropocene. 
                        Digital Scholarship in the Humanities, 30(suppl_1): i4–15 doi:10.1093/llc/fqv015.
                    
                    
                        Rusbridge, C. (2007). Arts and Humanities Data Service decision. 
                        Digital Curation Centre http://www.dcc.ac.uk/news/arts-and-humanities-data-service-decision (accessed 26 April 2019).
                    
                    
                        Russell, A. and Vinsel, L. (2016). Hail the Maintainers. 
                        Aeon https://aeon.co/essays/innovation-is-overvalued-maintenance-often-matters-more (accessed 9 June 2017).
                    
                    
                        Smithies, J., Westling, C., Sichani, A.-M., Mellen, P. and Ciula, A. (2019). Managing 100 Digital Humanities Projects: Digital Scholarship &amp; Archiving in King’s Digital Lab. 
                        Digital Humanities Quarterly, 12(4).
                    
                    
                        Smithies, J. (2017). 
                        The Digital Humanities and the Digital Modern. Basingstoke: Palgrave Macmillan https://www.palgrave.com/gb/book/9781137499431.
                    
                    
                        Smithies, J., Sichani, A.-M. and Westling, C. (2017). Preserving 30 years of Digital Humanities Work: The Experience of King’s College London Digital Lab. Paper presented at the DPASSH Conference 2017: Digital Preservation for Social Sciences and Humanities, Brighton. 
                    
                    
                        Smithies, J. (2019). The Continuum Approach to Career Development: Research Software Careers in King’s Digital Lab. 
                        King’s Digital Lab https://www.kdl.kcl.ac.uk/blog/rse-career-development/ (accessed 26 April 2019).
                    
                    
                        Straumsheim, C. (2015). Researchers, university press directors emboldened by Mellon foundation interest in academic publishing. 
                        Inside Higher Ed https://www.insidehighered.com/news/2015/02/25/researchers-university-press-directors-emboldened-mellon-foundation-interest (accessed 26 April 2019).
                    
                    
                        Waters, D. J. (2016). Monograph Publishing in the Digital Age. 
                        The Andrew W. Mellon Foundation https://mellon.org/resources/shared-experiences-blog/monograph-publishing-digital-age/ (accessed 26 April 2019).
                    
                
            
        
    

        
            “To write is to produce a mark that is a kind of machine, legible in a scribe
                ’s absence.” Jacques Derrida, 
                Signature, Event, Context (1982)
            
            Dissemination is an activity or practice; it is the process of grafting text into other contexts—or, to put it simply, 
                spreading the word (Derrida, 1982). In this sense, the act of annotating a text is already a sort of dissemination. Indeed, when beginning work on the website returntocinder.com, a published index of margin notes from the works of Jacques Derrida, the authors regarded annotation as “additive, useful, social, a means to collaborate with a text, and a ‘meta conversation running in the margin’” (Kalir and Garcia, 2021: 7). Returntocinder.com and its companion application, Databyss, affirm the social usefulness of the margin note, believing that the publication of notes, ideas, and quotations in a hypertext format can accelerate these conversations, producing new notes, ideas, quotations and digital projects.
            
             Inspired by Ted Nelson’s original theoretical work in hypertext and his writing application, Xanadu, the authors understood that “non-sequential writing” could benefit from developing software and thus algorithms that process text (Nelson, 1987). This processing, as Kalir and Garcia note, is an act of annotation in the computer science sense, meaning “labeling data—images, text, and audio—for the purposes of identifying, categorizing, and training machines and algorithms” (Ibid.). That is to say, generating metadata is a form of annotation. The supposed juxtaposition between that of the writer writing in the margins and that of the algorithm (programmer) categorizing, databasing and hyperlinking, is exactly what Derrida’s dissemination combines into a single concept. Both writer and developer are performing the same action of annotation/dissemination because the act of margin notes is already an act of labeling, an act of creating grammar, searchability, and organization, while the act of automatic indexing, databasing and hyperlinking is already an act of grafting new texts in the (hypertextual) margins. The margin note is already a minimal program and the hypertext program is a margin note; together, they facilitate the practice of digital dissemination through annotation.
             The Databyss Foundation (the non-profit company that has created returntocinder.com and databyss.org) began, as mentioned, as a project to build an index of several of Derrida's works and publish it in a way that was true to his concept of dissemination. The data was originally collected “manually” in a long word processor document, but because Derrida sees every written thought as a graft, ready to connect to another graft, it didn
                ’t make sense to publish this index as a single web page or even as a set of linear pages. Instead, we wrote the document using a grammar that would allow us to process and publish each note separately. Each paragraph began with a short code indicating the book from which the note was taken, followed by a page number. Notes were grouped under bolded headings that indicated a motif common to the notes below. We then wrote a parser script that broke the linear document into separate database entries, linking each to a table containing all of the sources and to another table that contained the motif headings.
            
             The result was returntocinder.com, an online database of margin notes (quasi-quotations) from several of Derrida
                ’s works that is searchable and navigable by way of an index of keywords and sources. After its initial launch, we discovered that one could easily add other authors to the database structure we had designed. This was quite exciting, as it meant that the 
                carrier of ideas could, in a sense, have a life separate from the ideas that originally inspired them. Since its initial publication five years ago, returntocinder.com has grown to include notes from texts by over 50 authors and receives hundreds of visitors every day.
            
             As more people used the site, researchers approached us hoping to build a similar resource for their work. Rather than suggest that they write a long document using our grammar and run it through our parser, we set about on a second project: The Databyss App. This web-based application codifies the grammar we used to generate returntocinder.com into a word processor-like interface. The short codes indicating sources and the bold headings indicating motifs are recognized by the app as you write them and linked to useful index pages that show other notes with the same tag(s). These links or tags are browsable and searchable in a sidebar to the left of the editor. We also set out to create a workspace that caters to humanities workers (researchers and students), with additional features like the ability to pull PDF highlights and annotations straight into your workflow, and search book and journal catalogs to generate Zotero-like citations. Future modules will facilitate interoperability with other annotation and bibliography applications such as Zotero, Hypothes.is and Readwise. When users share some or all of their Databyss notes, all the links remain intact and the workspace interface (i.e all the search options in the sidebar) is fully functional for the reader, so there is no need for a separate parsing and publishing step. True to Derrida
                ’s concept of the written mark, these Databyss notes become self-disseminating machines.
            
             In our presentation, we will first cover the theoretical foundations of our work and situate it within the context of prior hypertext resources, such as David Kolb's “Socrates in the Labyrinth" and Eric Steinhart's “Fragments of the Dionysian Body.” We will then demonstrate how the current features of the Databyss application can be used for humanities research and introduce some use cases already proposed or practiced on Databyss (a Critical Race Theory resource, a film studies curriculum, notes for an academic podcast, student reading assignments, and the distribution of a professor
                ’s lecture notes). Finally, we will discuss the ideas generated by our present work for the future of the application and foundation.
            
        
        
            
                
                    Bibliography
                    
                        Derrida, Jacques. (1982). Signature Event Context: 
                        Margins in Philosophy. Translated by Alan Bass. Chicago: University of Chicago Press.
                    
                    
                        Kalir, Remi H., and Antero Garcia
                        . (2021). 
                        Annotation
                        . Cambridge: MIT Press.
                    
                    
                        Nelson, Theodor H
                        . (1987).
                         Computer lib: Dream machines
                        . Redmond: Tempus Books of Microsoft Press.
                    
                
            
        
    



        
            
                Introduction
                
                    The study of historical figures is of great significance in the field of history. To investigate historical figures with digital humanity methods, the first step is to identify the names of people in texts. Not only is the person's name recognized from the text, but the person's name has to be linked to a knowledge base for reference. This is because the same person's name may refer to different people or other entities. This task is called named entity disambiguation (NED). The problem of historical figures with the same name is particularly serious when studying Manchu and Mongolian historical figures in the Qing Dynasty. A typical example is that the main persons (Cherin-Dorji, Yung-Te, Siang-Lin and Gui-Xiang) involved in the disaster report of the Kharkha Four Leagues 
                    
                    (Ting-ting, 2016)
                    have the same name as other historical figures. Fig. 1 shows that there are 10 Siang-Lins in the Ming-Qing Archives Name Authority Database (MQANAD).
                
                
                    
                
                
                    Fig. 1
                     There are ten people named Siang-Lin in the MQANAD database. Siang-Lin in this text refers to this historical figure with ID 0079695
                
                
                    Therefore, we select 
                    Qing Shi-Lu
                    (
                    QSL
                    )
                    
                     and MQANAD
                    
                    (Liu, 2015)
                     as the target text and the reference knowledge base for developing our NED system, respectively. 
                    QSL
                     is the imperial annals of the Qing emperors, with a total of about 58 million characters, written in classical Chinese.
                
                
                    In 
                    
                    (Tsai et al., 2020)
                    , several procedures were proposed to automatically generate labeled data from 
                    Ming Shi-Lu
                     (
                    MSL
                    ) for training a NED model. In this work, we improve 
                    
                    (Tsai et al., 2020)
                     in two folds. First, we propose a new way of expressing person names in both texts and profiles. Second, we modify the original procedures to improve the quality of the generated labeling data.
                
            
            
                Method
                As mentioned earlier, an NED 
                    
                    (Cheng et al., 2019
                    ;
                    
                    Huang et al., 2015) model can link each mention to the correct profile in the reference knowledge base. 
                    An official’s profile is shown in Table 1, 
                    we can see that it contains the official’s attributes such as name, birth/death year, biography, resume, relatives, etc. The resume field is composed of all titles the official had held. We search 
                    QSL
                     for all mentions of these officials’ names. These mentions with the paragraphs containing them are used as our dataset. Table 2 shows an example instance of the search results.
                
                
                    
                
                
                    Table 1
                     An official 
                    Wang An-Kuo’s
                     profile
                
                
                    
                
                
                    Table 2
                     An example instance
                
                We formulate this NED task as a text classification problem. Given an instance whose name field is m and a profile whose name field is also m, classify them as positive (matched) or negative (not matched). Matched means the person mentioned in the instance’s paragraph field is just the official in the profile.
                
                    Tsai et al. was the first work to propose the procedures that compare instances and profiles to automatically generate labeled data to train the model 
                    (Tsai et al., 2020). In this work, we use all the procedures in 
                    (Tsai et al., 2020). These procedures based on the rules help us identify part of the data pairs. Then we deal with those data pairs that cannot be identified by the rules by using BERT as a binary classifier. With training data acquired from previous procedures, we make the classifier learn the relationships between instances and profiles and identify them through the context besides using rule-based procedures. In addition, we also propose methods to improve the quality of the labeled data, as described in the following paragraphs.
                
                First, we replace the officials’ names mentioned in personal profiles and instances with the symbol [unused_token], as shown in Fig. 2. This allows our model to focus on more information from the context and can be more robust to various person names.
                
                    
                
                
                    Fig. 2
                     Cherin-Dorji, the official’s name mentioned in personal profiles and instances, is replaced with the symbol [unused_token] 
                    
                    (Hucker, 2008, 
                    
                    Zhang et al., 2017)
                
                Second, we have found many examples in the text where a person’s name is the same as a location name. For example, Guilin (桂林) can be the name of a city or a minister’s name. Therefore, we use a self-developed NER 
                    (Lin et al., 2020) system to process the texts. We use Flair as the NER model for this paper, the training data is from 
                    MSL and the test data is 
                    QSL's 50 manually annotated data with F1 score 0.81. We revise 
                    (Tsai et al., 2020)’s method to correct an instance from positive to negative, an example is shown in Fig. 3.
                
                
                    
                
                
                    Fig. 3
                     Guilin (桂林) is considered a location name by our NER system, therefore, it is generated as a negative instance
                
                Lastly, since the text processed by 
                    (Tsai et al., 2020) is the Ming Dynasty, the text processed by this study is the Qing Dynasty, and the contents of 
                    MSL and 
                    QSL are slightly different, we only retain the first condition of Procedure 1.
                
                
                    
                
                
                    Fig
                    .
                     4
                     Procedure one is modified for writing-style difference between 
                    MSL
                     and 
                    QSL
                
                
                    
                
                
                    Fig. 5
                     Our BERT-based NED model
                
                After automatically labeling the training data, we use BERT 
                    (Vaswani et al., 2017, 
                    Devlin et al., 2019), the state-of-the-art natural language understanding model, to perform text classification. We use the model pre-trained on the Chinese Wikipedia. As shown in Fig. 5, for each pair of profile and instance, we concatenate the cls symbol, the profile, the sep symbol, the instance as the input. The BERT model will output all label probabilities at the output position corresponding to the cls symbol.
                
                We use the manually labeled data set of 
                    (Tsai et al., 2020) for performance evaluation. The results show that our NED model achieves an accuracy of 90%, which is 16% higher than the model proposed in 
                    (Tsai et al., 2020).
                
                Finally, we conduct an ablation study. If we remove NER, performance will drop by 3%. If we do not do unused token replacement, performance will drop by 13%. If we do neither, the performance drops by 16%.
            
            
                Conclusion
                We have refined the approach of automatically generating labeled data for training an NED model. To be more specific, we employ our own NER system to eliminate the location names incorrectly labeled as positive instances and use the unused token symbol to enhance the robustness of our model. Results show that our refinement can improve the performance by 16% and our NED model 
                    will help us investigate historical figures in the Qing dynasty.
                
            
        
        
            
                
                    Bibliography
                    
                        Cheng, J. et al. (2019). Entity Linking for Chinese Short Texts Based on BERT and Entity Name Embeddings. 
                    
                    
                        Devlin, J. et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs]. 
                    
                    
                        Hucker, C.O. (2008) A Dictionary of Official Titles in Imperial China. Peking University Press.
                    
                    
                        Huang, H., Heck, L. and Ji, H. (2015). Leveraging Deep Neural Networks and Knowledge Graphs for Entity Disambiguation’. arXiv:1504.07678 [cs].
                    
                    
                        Institute of History and Philology, Academia Sinica (1984) Scripta Sinica. http://hanchi.ihp.sinica.edu.tw/ihp/hanji.htm. 
                    
                    
                        Liu, C. (2015) Ming-Qing Archives Name Authority Database.
                    
                    
                        Ting-ting, L. (2016). 清季蒙古親王車林多爾濟史事鉤沉—以喀爾喀四盟報災案為中心[History of Mongolian Prince Cherin-Dorji in the Qing Dynasty-A Disaster Report by the Four Leagues of Khalkha]. Studies of Chinese Frontier Ethnic Groups.
                    
                    
                        Tsai, R.T.-H. et al. (2020). Automatic Labeled Data Generation for Person Named Entity Disambiguation on the Ming Shilu. DH.
                    
                    
                        Vaswani, A. et al. (2017). Attention Is All You Need. arXiv:1706.03762 [cs].
                    
                    
                        Lin, B.Y. et al. (2020). TriggerNER: Learning with Entity Triggers as Explanations for Named Entity Recognition. arXiv:2004.07493 [cs]. 
                    
                    
                        Zhang, Y. et al. (2017). 明代職官中英辭典 Chinese-English Dictionary of Ming Government Official Titles.
                    
                
            
        
    



        
            
                Overview of the tutorial
                The first issue of the 
                    Journal of Digital History (JDH) went out in October 2021. A joint-venture with De Gruyter Publishingq group, the JDH is open access (no fees for authors) and encourages open data. Its distinctiveness is that it implements the concept of multilayered articles, that is based on a long-lasting history of thinking on the future of academic writings 
                    (Darnton). This implementation is based on a specifically designed infrastructure based on open source software, with Jupyter notebooks at its heart. JDH’s articles are composed of three layers: a narrative layer (the presentation of the results of a research), an hermeneutics layer (exposing methodologies, uses of digital tools, and code), and a data layer (the dataset itself).
                
                The tutorial will last 4 hours and will be divided into two parts. The first part (Two hours, including a break) will show the attendees how to set up their writing environment (
                    
                        https://journalofdigitalhistory.org/en/guidelines
                    ). This includes: installing jupyter notebooks and several extensions (nbextensions, cite2c), linking their writing environment to their Zotero account, setting up a github repository (optional, but strongly recommended).
                
                After the familiarization with the environment, the second part will focus on how to write an article and preview it on the JDH front-end  (
                    
                        https://journalofdigitalhistory.org/en/notebook-viewer-form
                     - for this interface, using a github repo is mandatory).
                
                For both parts, it is mandatory that attendees come with their own computer with administrator’s rights. It is strongly recommended that they have their own data and have a draft article (whatever the journal they intend to publish in) using this dataset in view.
            
            
                Learning outcomes
                
                    understanding the concept of multilayered article,
                    setting up a writing environment that allows interaction with a dataset through code,
                    testing their article in the JDH viewer.
                
                Beyond the use of the Journal of Digital History, we aim at showing concretely an alternative way to publish in the digital era, a topic that is fully belonging to Digital Humanities 
                    (Fitzpatrick; Vitali-Rosati et Sinatra).
                
            
            
                Timeline
                Introduction (5 minutes)
                The Journal of Digital History and the concept of multilayered article (10 minutes)
                
                    First part
                    Setting up a writing environment (1) (40 minutes)
                    
                        installation of jupyter notebooks / lab
                        installation of nbextensions / cite2c
                    
                    break (10 minutes)
                    Setting up your writing environment (2) (50 minutes)
                    
                        linking your jupyter installation to Zotero through cite2c
                        using GitHub (optional)
                    
                    Overview of the main functionalities of notebooks
                    break (10 minutes)
                
                
                    Second part
                    Presenting the guidelines of the JDH (including the specific JDH tagging system, 20 minutes)
                    Presenting the Jupyter Notebook JDH template (10 minutes)
                    Creating and syncing your repo on GitHub (10 minutes)
                    Preparing your bibliography on Zotero (10 minutes)
                    break (10 minutes)
                    Writing session (45 minutes)
                    
                        Including article preview on the JDH.
                    
                
                
                    Wrap up (15 minutes)
                
            
            
                Workshop instructors and leaders
                
                    Frédéric Clavert is assistant professor in contemporary history and managing editor of the Journal of Digital History. His research are recently focusing on collective memory and social media, as well as on the changing relationships between historians and their primary sources in the digital era.
                
                
                    Elisabeth Guérard is working on the JDH project as an application developer.
                
                
                    Mirjam Pfeiffer is a User Experience and User Interaction Designer, working full time on the Journal of Digital History.
                
            
            
                Target audience
                We target researchers, mostly but not exclusively historians, who have an experience in writing code to exploit their data but have not yet found a satisfactory way to expose their methods, tools, code and data.
                We expect a number of participants around 10 on site. In November 2021, we held a workshop at the French conference DHNord in Lille that aroused some interest, with a smaller audience than the DH conferences’. Aside from that workshop, we also organised several workshops online (Nebraska Lincoln, University of Sussex). Evaluating how many people will attend online is more difficult.
                The JDH’s team hopes that this tutorial will give them the occasion to meet and deepen their links to the Asian DH community.
            
        
        
            
                
                    Bibliography
                    Darnton, R. (1999). “The New Age of the Book”. 
                        New York Times.
                    
                    Fitzpatrick, K. (2011). Planned Obsolescence: Publishing, Technology, and the Future of the Academy. New York: New York University Press.
                    Vitali-Rosati, M. and Sinatra, M. eds. 
                        Pratiques de l’édition numérique, Montréal: Presses de l’Université de Montréal, 2014.
                    
                
            
        
    

