
The Scene
A raucous party on the eve of the Battle of Bosworth field in 1485, wine flows by the gallon, the table bends under the weight of the meat and breadcrumbs fly across the table as Richard III rises painfully to address his loyal peers:

Now is the summer of our sweet content,
Made overcast winter by these Tudor clouds.
And I that am not shaped for black-faced war,
I that am rudely cast and want true majesty,
Am forced to fight, To set sweet England free.
I pray to Heaven we fare well,
And all who fight us go to Hell
(transcription of the audio recording (British Broadcasting Corporation 2009), checked against (Curtis 2001)).
Thus starts Rowan Atkinson’s fabulously funny alternate history of Britain, the History of the Black Adder (Atkinson et al. 1983), one of the hallmark BBC sitcoms of the 1980s.

Fast forward through the show, right to the closing credits: “Written by Richard Curtis and Rowan Atkinson with additional dialogue by William Shakespeare”. And, indeed, even Richard III’s short speech contains at least three near-verbatim citations from Shakespeare’s play of the same name (and actually a fourth and a fifth, but more on that later):

Now is the Winter of our Discontent
Made glorious Summer by this Son of Yorke: […]
But I, that am not shap'd for sportiue trickes, […]
I, that am Rudely stampt, and want loues Maiesty, […]
And that so lamely and vnfashionable,
That dogges barke at me, as I halt by them [1]
All would be good if the citations were indeed verbatim, but they are not — the wit of Richard’s speech and indeed of much of Blackadder’s attraction depends on twisting citations, creating tension between original and new wordings. Blackadder like much of postmodern British Sitcom consciously sees history as a “cluttered patchwork of questionable stories which have been re-written, re-evaluated and ridiculed“ ((Roberts 2012), pos. 26, Kindle edition), using allusions precisely to enhance the sense of a subjective, patchwork-like history.

Enter Agents for Actors
Agents for Actors (AfA) (https://github.com/mwkuster/agents-for-actors) is an experimental, LGPL-licensed Open Source “framework for distributed microservices for text linking and visualization” that the author has developed to calculate precisely the types of “twisted” citations that we are seeing. It takes some inspiration from W. Artes’ Bachelor thesis (Artes 2012) on Similarity Search (cf. also (Hedges et al. 2012)), that the author has supervised, especially on the choice of NGram models for comparison, but is an independent implementation that deviates in the way the NGram model is built (cf. below). AfA is linked to TextGrid’s Text-Text-Image-Link Editor (Selig, Küster, and Conner 2012).

Afa identifies allusions between texts and their presumed sources and gives exact provenance information as XPointers (Küster et al. 2011). As an additional spin we make the comparison of Black Adder’s modern orthography transcript against the original-spelling First Folio, transcribed by Trevor Howard-Hill (http://ota.ox.ac.uk/id/3014).

AfA is extendable to other similarity models and measures and could be adapted for new visualization frontends. Given the computational complexity it embraces multicore architectures and parallelizes computations wherever possible. AfA situates itself between the macro-vision of big data digital humanities (e.g. (Jockers 2012) and the forthcoming (Jockers 2013)) and the micro-vision of the classical, manually encoded critical apparatus.

Implementation
AfA is implemented in the functional language Clojure (Hickey 2010) (Emerick, Carper, and Grand 2012; Halloway and Bedra 2012) on top of the Java Virtual Machine (JVM). Clojure thrives on immutable functional data structures (Okasaki 1999) for heavily multithreaded applications. Mutable operations are largely under control of Clojure’s Software Transactional Memory (STM).

AfA uses two Clojure paradigms to parallelize activities:

Futures: non-blocking threads that are transparently managed to parallelize calculations
Agents: asynchronous, used to interact with the visualization layer
The code basis itself consists of a number of individual functions for interacting with files, calculating similarity measures, handling visualization etc. They can be regrouped flexibly.

Handling XML
AfA currently expects both source and target to be encoded in XML. Both therefore contain parts such as the TEI header, cast lists or stage directions that without filtering generate noise when searching for references. Still, we need to preserve the exact pointers to source and target fragments in the underlying XML files to guarantee traceability.

With mutable data structures this objective would be difficult to attain without costly copying operations of XML structures in memory. The immutable abstraction of Zippers (Huet 1997) offers a solution by having the XML structures only once in memory, the algorithm operating on pointers to individual elements (“locations”). Furthermore, unlike e.g. XML DOMs Zippers are generic abstractions for tree structures, not only XML. AfA can hence be adapted to any form of structured data sources.

Measuring similarity
The AfA framework allows for multiple models and measures. At is simplest, the comparison is done using NGram models (cf. (Manning and Schuetze 1999)), that are used for fuzzy text comparison, e.g. in (Kestemont, Daelemans, and De Pauw 2010) and (Bernholz and Pytlik Zillig 2011), not to mention in Google-style big-data (Google 2010).

The tests presented in this paper are done with a variation of the NGram model, combining NGrams of words, that move in a slider over the text, creating chunks of size C, with NGrams of letters for the actual comparison of those chunks.

In the following example C=5. This way, the phrase “Now is the summer of our sweet content” into four chunks of five words each:


Each of these chunks is compared with a chunk from the supposed source material, built with the same algorithm, so that the totality of compared chunks is the Cartesian product NxM, N being the number of chunks in the source and M that in the target. Each of these pairs of chunks is compared using an NGram of ngram-count characters to smoothen over differences in spelling. The respective confidence level for a hit is calculated using the maximum dice-coefficient (cf. (Manning and Schuetze 1999), table 8.7) applied to this combination of chunks: (apply max (map (fn [[t1 t2]] (dice-coefficient (ngrams ngram-count t1) (ngrams ngram-count t2))) (for [chunk1 chunk-seq1 chunk2 chunk-seq2] [chunk1 chunk2]))) The algorithm works for other measures returning similarities normalized to [0,1].

Applying this admittedly computationally expensive algorithm to Richard’s speech with C=6, N=4 and a minimum confidence level of 0.65 identifies in Shakespeare’s complete First Folio precisely the expected links (XPointers referring to http://ota.ox.ac.uk/id/3014):


The algorithm cannot identify the fourth allusion, though, that contrasts the concepts of “overcast winter” with “glorious Summer”.

Dressing up
For visualization AfA uses Neo4j, an increasingly popular Open Source Non-SQL graph database (Neo4j.org 2012). Neo4j centres around two Topic Map (ISO/IEC 2002) like concepts, nodes and relationships. Both have unique identifiers and can have arbitrary properties besides. Relationships must have a start and an end node.

With neocons (Klishin 2012) Neo4j has an intuitive Clojure interface that permits to store and query the graph. Neo4j’s admin interface also has one of the more innovative graph UIs available in an off-the-shelf database.


Intertextuality and a bit of theory
This screenshot shows the individual phrases of Richard’s speech in Blackadder with their links to their source (“part-of”) and the phrases they cite (“cites”), giving a good visualization of the intertextuality (Kristeva 1969) (Barthes and Marty 1980; Barthes 1968) (Allen 2011) that animates much of Atkinson’s comedy.

Note that AfA does not “interprete” the links. Statistical methods rarely have an obvious interpretation, their epistemological openness and continuity being an asset for postmodern texts in a world of “stylistic and discursive heterogeneity without a norm” (Jameson 1991). If anything, Genette’s theory of hypertextualité (Genette 1982) might give us an adequate terminological apparatus.

Outlook
Three of the citations in Richard’s speech AfA has identified for us, the fourth we have discussed — but there is a fifth. Films are more than dialogue; intertextuality can just as well be construed without words. When Atkinson’s Richard is halfway through his speech we hear a dog barking — Richard is so ugly that “dogges barke at me”. Also, there is open linked data out there that situates Blackadder in context, here linking manually to Freebase movie data with some of Atkinson’s other films (for handling Freebase data cf. (Redmond and Wilson 2012), chapter Neo4j/Big data):


In the end an approach focussed on a narrow understanding of intertextuality will not suffice for audiovisual media; it must evolve into a network including knowledge and symbols (Peirce 1998), contextual, textual and non-textual. Now we can only manually add nodes establish these links, but there may be another untold history ahead.

References
Allen, G. (2011). Intertextuality. 2nd edn. Abdingdon: Routledge.
Artes, W. (2012). In Marc Wilhelm Küster, (ed). “Konzeption Und Entwicklung Eines Plug-in Basierenden SOAP-Services Für Die Ähnlichkeitsanalyse Im Text-Text-Link-Editor.” Worms: University of Applied Sciences Worms.
Atkinson, R., B. Blessed, E. Gray, R. East, T. McInnerny, and R. Robinson (1983). In Shardlow, M. The Black Adder. http://www.bbc.co.uk/programmes/p006cx8w.
Barthes, R. (1968). “La Mort De l'Auteur.” Manteia (4e trimestre).
Barthes, R., and E. Marty (1980). Oeuvres Complètes: 1974-1980.
Bernholz, C. D., and B. L. Pytlik Zillig (2011). “Comparing Nearly Identical Treaty Texts: a Note on the Treaty of Fort Laramie with Sioux, Etc., 1851 And Levenshtein's Edit Distance Metric..” Llc 26.1: 5-16. doi:10.1093/llc/fqq016.
British Broadcasting Corporation. (2009). The Black Adder. Unabridged. AudioGO Ltd.
Curtis, R.. (2001). Blackadder: the Whole Damn Dynasty: 1485-1917. Re-issue. Penguin Books, Limited (UK).
Emerick, C., B. Carper, and C. Grand (2012). “Clojure Programming — Practical LISP for the Java World.” O'Reilly 2012.
Gérard, G. (1982). Palimpsestes. La Littérature Au Second Degré. Paris: Éditions du seuil.
Google. (2010). Google Books NGram Viewer.
Halloway, S., and A. Bedra (2012). Programming Clojure. Second Edition. Pragmatic Bookshelf.
Hedges, M., A. Jordanous, S. Dunn, C. Roueche, M.W. Küster, T. Selig, M. Bittorf, and W. Artes (2012). “New Models for Collaborative Textual Scholarship.” In, 1-6. doi:10.1109/DEST.2012.6227933.
Hickey, R. (2010). “Clojure.”
Huet, G. (1997). “The Zipper.” Journal of Functional Programming 7 (5) (September).
ISO/IEC. (2002). ISO/IEC 13250: Information Technology — SGML Applications — Topic Maps. {ISO}.
Jameson, F. (1991). Postmodernism or the Cultural Logic of Late Capitalism. Durham: Duke University Press.
Jockers, M. L. (2012). “Computing and Visualizing the 19th-Century Literary Genome.” In Hamburg. http://www.dh2012.uni-hamburg.de/conference/programme/abstracts/computing-and-visualizing-the-19th-century-literary-genome/.
Jockers, M. L. (2013). Macroanalysis: Digital Methods and Literary History (Topics in the Digital Humanities). 1st edn. University of Illinois Press. http://www.matthewjockers.net/macroanalysisbook/.
Kestemont, M., W. Daelemans, and G. De Pauw (2010). “Weigh Your Words - Memory-Based Lemmatization for Middle Dutch..” Llc 25.3 287-301. doi:10.1093/llc/fqq011.
Klishin, M. (2012). “Neocons.” Clojureneo4j.Info. Accessed October 29. http://clojureneo4j.info/.
Kristeva, J. (1969). Semeiotike. Recherches Pour Une Sémanalyse. Éditions du seuil.
Küster, M. W., C. Ludwig, Y. Al-hajj, and T. Selig (2011). “TextGrid Provenance Tools for Digital Humanities Ecosystems.” In, 317-323. doi:10.1109/DEST.2011.5936615.
Manning, C. D., and H. Schuetze (1999). Foundations of Statistical Natural Language Processing. 1st edn. Cambridge, MA: MIT Press.
Neo4j.org. (2012). “Neo4j.” Neo4j.org. http://neo4j.org/.
Okasaki, C. (1999). Purely Functional Data Structures. Cambridge University Press.
Peirce, C. S. (1998). “What Is a Sign?.” In The Essential Peirce. 1893-1913, ed. Peirce Edition Project. 2 Bloomington.
Redmond, E., and J. R. Wilson (2012). Seven Databases in Seven Weeks: a Guide to Modern Databases and the NoSQL Movement. Pragmatic Bookshelf.
Roberts, J. F. (2012). The True History of the Black Adder: the Unadulterated Tale of the Creation of a Comedy Legend. Preface Publishing.
Selig, T., M.W. Küster, and E. Conner (2012). “Semantically Connecting Text Fragments — Text-Text-Link-Editor (Poster).” In, ed. Jan Christoph Meister, 518—520. Hamburg. http://www.dh2012.uni-hamburg.de/wp-content/uploads/2012/07/HamburgUP_dh2012_BoA.pdf.
Notes
1. The Tragedy of Richard the Third, cited after Shakespeare’s First Folio as transcribed in http://ota.ox.ac.uk/id/3014
Overview
Bibliopedia, which recently completed an NEH Digital Humanities Start-Up Grant, performs data-mining and cross-referencing of scholarly literature to create a humanities-centered collaboratory. Currently a working prototype, Bibliopedia can search resources including JSTOR and Library of Congress for metadata about scholarly articles and books, examine the articles and books for citations, then present the results in a publicly accessible database. Bibliopedia is designed to work with all humanities scholarship. It will also allow users to create browsable and customizable bibliographies of all the works cited by each article and book. Most importantly, it uses semantic web technology to enable automated textual analysis, data extraction, cross-referencing, and visualizations of the relationships among texts and authors. Using existing open source software, it extracts citation data from existing plain text resources and transforms them into linked open data. This process makes the information easily accessible to the wider scholarly and linked data communities, enables network visualizations of the scholarly landscape. This presentation will cover the details of the Bibliopedia system to show others how they can replicate it. We will also offer to all interested academic parties our existing installation and hosting platform for their experimentation. In particular, we will present our Drupal-based semantic wiki, which features a full web services API, and our custom citation crawler.

Linked open data, one of the core technologies of the semantic web, promotes open sharing of digital scholarly research while it encourages further, potentially unexpected uses. Bibliopedia's method for incorporating linked open data (via RDFa) requires only minimal technical expertise to reproduce. One of the central components of Bibliopedia is the Drupal content management sytem (CMS), which as of version 7 exposes data via RDF/RDFa as part of its core functionality. This functionality, moreover, is not limited to Drupal. For example, Omeka, another CMS developed at the Roy Rosenzweig Center for History and New Media, George Mason University, has some limited support for linked data through its DublinCoreExtended plugin. Bibliopedia demonstrates the power and flexibility of Drupal's approach to linked data while providing more general lessons for digital humanists who seek to incorporate this technology into their projects.

Project Details
Bibliopedia will aid humanities researchers of all levels of expertise by making simple the currently difficult tasks of discovering new scholarly works and the relationships among them. It will create an a scholarly community to verify and elaborate cross-referenced, linked bibliographic data through easy-to-use wiki pages. Scholarly literature will become browsable not only backwards in time, but also forwards, something that is currently impossible.

The semantic web is transforming the Internet from a collection of pages and data readable only by humans to one that machines can understand and process. Semantic web technology promises the ability automatically to determine meaning and then infer connections among different elements, thereby vastly improving search capabilities, discovery of new information, and the overall usefulness of the Internet. Just as information accessible only to humans comprises the great majority of the general Internet, so too is data about scholarly literature locked away in text that computers cannot process without great difficulty. At best, search engines for repositories such as JSTOR permit researchers to query author name, journal titles, and keywords, but once a work is found, the search stops. No connections among works are found precisely because machines cannot currently read that data. Although Google Scholar attempts to show citations of articles, its usefulness is highly limited because it does not make clear the relationships among articles, present very limited metadata about each article (if any), fails to provide for community elaboration or correction, and includes only works that are publicly available. Yet despite its limitations, Google Scholar stands as a significant technological advance beyond keyword-based search engines such as those provided by JSTOR and Project Muse.

Bibliopedia will, by aggregating data from as many sources as possible, converting citations into semantic web format, and then cross-referencing an ever-growing database of scholarly works, be able not only to overcome many of the limitations of Google Scholar and become a powerful research tool in its own right, but also to make a valuable contribution to the growing semantic web. Introducing high quality metadata about humanities scholarship to the semantic web will enable others in the semantic web/linked data world to process that data in new, unexpected ways that will accrue further benefits to the scholarly community. For example, the standards underlying the semantic web make data visualization and automated inferences about relationships trivially easy rather than the complex problems such tasks currently present. Bibliopedia will, then, through the innovation of placing metadata about scholarly literature into a linked data format, open up a vast range of possible future innovations and analyses based on that data, which is currently locked away and readable only by select humans.

Another virtue of a linked data format is that it will help resolve many of the challenges inherent in metadata, some will inevitably remain. Rather than attempt to solve this incredibly complex problem through automation alone, then, Bibliopedia will, in the process of displaying its results for human consumption, also provide for human feedback in the form of correction and elaboration. A common disadvantage of fully automated text analysis and data extraction tools such as Google Books, Google Scholar, and other digital research tools is that their automatic parsers have errors in their metadata that they do not allow subject matter experts to repair. Bibliopedia will pursue the goal of unifying that information into an environment that not only displays the information efficiently, but actively encourages crowd-sourcing metadata on books, articles, and publications of all kinds. In thus opening data up to revision by the scholarly community, Bibliopedia can build on the strong work of mature data silos, improve overall data quality, and provide the academic community at large a continuously evolving research tool.

There currently exists a multitude of projects and tools designed to work with book metadata, cross-reference scholarly articles (localized to the sciences), or create user communities around a chosen interest. Further, some of the most important trends currently revising the ways we use technology are social media, collaboration, and data aggregation. By incorporating the benefits realizable from each of these trends, Bibliopedia will create a powerful tool for scholarly research at all levels. None of the existing tools, however, focus on scholarship for the humanities, nor do they present the information in the linked data format necessary to the semantic web.
Introduction
This paper will present the findings from a PhD case study into the use and users of the British Library Nineteenth Century Newspapers Collection (BNCN). [1] Using data gathered from web analytics and user surveys, it will show that although digitization provides clear benefits to users who operate in an information-rich environment, these benefits are distributed unequally. I will therefore present an alternative geographical visualization based upon the location of subscribing institutions rather than individual users. This, combined with university rankings data and relative poverty measures, backs up the main argument of this paper: that the subscription-based model of digitization severely undermines the rhetorical embracement of universal access, and instead reinforces existing divides between information-rich and information-poor communities.

Addressing the Digital Divide
Web analytics services such as Google Analytics [2] provide map overlays that automatically visualize user locations. This important data provides web analysts with important insight into their user base. These automated mapping tools lack, though, a consideration of how user location is influenced by variations in access to digitized resource. The internet, and by extension, digitized collections, are generally viewed as an opportunity to widen participation and improve education (Norris 2001, 7; Bell 2005), but this paper will demonstrate that they can provide these benefits unevenly across society. Norris identifies that the digital divide is multifaceted; it is a global and democratic divide, but also a social divide between different groups in society (Norris 2001, 4).

This is important when considered alongside a common problem in the literature: increased quantities of digitized content have brought with them inflated expectation levels. Everett, for instance, wrongly conflates the digitization of large collections with the concept of universal access: “the problem for the twenty-first century scholar will be to limit inquiry to a manageable subset of data; because all scholars will have immediate access to all archives in the world” (Everett 2005). Commercial reality, though, makes this utopian outlook seem naïve. The social and professional environment in which users operate remains vital in deciding access to digitized content. This mirrors the wider context of the digital divide, which is increasingly manifested as an indicator of the differentiated uptake of important digital resources (Hargittai & Walejko 2008; Hassani 2006; Norris 2001; Castells 2002). Existing research into this “second-level digital divide” (Hassani 2006) suggests that those from higher socioeconomic backgrounds generally benefit most from technological developments. As a result, mapping the location of individual users may show nothing more than high levels of connectivity in a particular demographic (Hassani 2006, 251). Similarly, in academia it appears that a ‘digitized divide’ could emerge between those with access to digitized content in large quantities and those without, one strongly related to social and geographical status.

Methods
While Geographical Information Systems (GIS) have the potential to answer innovative research questions (Bodenhamer et al. 2010), the automated nature of default web analytics visualizations does not allow us to interrogate this problem.

In order to study the impact of BNCN, web data was analysed for a period of one calendar year using reports generated by Google Analytics, including the referral analysis (Madsen 2010) [3] which influenced this paper; the results demonstrated that many users located outside Europe and the USA were in fact linked to institutions within these two regions. A survey was also mounted, which showed that many BNCN users still found access difficult; indeed some were forced to travel in order to access the collection digitally. The GIS was therefore created to test concerns about potential inequalities in access.

The visualization is based upon a list of institutions with current subscriptions to BNCN, including educational institutions, UK public libraries, and national libraries. This list was collated using online subscriber lists, [4] manual searching and referrer lists derived from web analytics. These were then mapped to a web-based GIS using Google Maps Fusion Tables, [5] and combined with demographic information and university ranking data. A separate layer was created for English public libraries which combined access information with UK Government measures of relative deprivation, population, and public spend on libraries.

Findings
The findings demonstrate that the divide in access correlates strongly with the status of a university: more highly ranked institutions were more likely to have current subscriptions. Additionally, English public library authorities were far more likely to have access if they were in less deprived regions, or served a relatively large population. This backs up qualitative data from the survey; some respondents were worried about the impact of working at institutions without appropriate subscriptions, and the prospect of losing access when fixed-term academic posts expire. This mirrors Hargittai’s assertion that “the societal position that users inhabit influences aspects of their digital media use such as the technical equipment to which they have access” (Hargittai 2008, 940). As digitized content proliferates, the expectation that scholars will use it also increases. The idea of democratized access to digital content (Bell 2005) can therefore become a damaging myth for those left behind. Unequal access to resources has been a longstanding problem for researchers, but the rhetorical shift towards universal access has ignored it. Technological inequality never entirely disappears, as Castells points out: “as one source of technological inequality seems to be diminishing, another one emerges” (Castells 2002, 256).

Second, rather than facilitating the disintermediation of information, the current glut of commercially digitized content increases the importance of library services in relation to access. In the wake of the Google Books project, Roush questioned “the ‘value proposition’ they [libraries] offer in a digital future” (Roush 2005)., These findings suggest that this value proposition will centre on the library’s ability to supply relevant subscriptions to its users in a timely manner, for as long as access is too costly to maintain at an individual level.

Conclusion
The study of web impact would benefit from a more realistic appraisal of the digital divide in relation to digitized content. Commercial digitization, while an effective way to fund projects, has implications for scholars working outside information-rich institutions, or indeed outside institutional frameworks entirely. Similar case studies should therefore be done with other digitized resources to discover whether these patterns are replicated elsewhere. Additionally, we must consider how project developers and library services can work to address the inequalities discovered by this project. In keeping with the conference’s theme, this paper provides an analysis of how far digitization truly provides researchers with freedom to explore the content being produced.

References
Bell, D. (2005). The Bookless Future: What the Internet is Doing to Scholarship. The New Republic. http://www.tnr.com/article/books-and-arts/the-bookless-future (accessed March 27, 2012).
Bodenhamer, D. J., J. Corrigan, and T. M. Harris (eds.) (2010). The Spatial Humanities: GIS and the Future of Humanities Scholarship, Bloomington: Indiana University Press.
Castells, M. (2002). The Internet Galaxy: Reflections on the Internet, Business, and Society, Oxford: Oxford University Press.
Everett, G. (2005). Electronic Resources for Victorian Researchers — 2005 and Beyond. Victorian Literature and Culture. 33. 601–614.
Hargittai, E. (2008). The Digital Reproduction of Inequality. In Grusky, D. (ed). Social Stratification. Boulder, CO: Westview Press.
Hargittai, E. & G. Walejko. (2008). The Participation Divide: Content Creation and Sharing in the Digital Age. Information, Communication & Society. 11(2). 239–256.
Hassani, S. N. (2006). Locating Digital Divides at Home, Work, and Everywhere Else. Poetics. 34(4-5).
Madsen, C. (2010). What is Referrer Analysis? Toolkit for the Impact of Digitised Scholarly Resources (TIDSR). http://microsites.oii.ox.ac.uk/tidsr/kb/45/what-referrer-analysis (accessed June 11, 2012).
Norris, P. (2001). Digital Divide: Civic Engagement, Information Poverty and the Internet Worldwide, Cambridge: Cambridge University Press.
Roush, W. (2005). The Infinite Library. Technology Review. 108(5) 54–59.
Notes
1. newspapers.bl.uk/blcs/

2. www.google.com/analytics/

3. See the Toolkit for the Impact of Digitized Scholarly Resources: http://microsites.oii.ox.ac.uk/tidsr/.

4. http://gdc.gale.com/products/19th-century-british-library-newspapers-part-i-and-part-ii/evaluate/customer-list/ and http://www.bl.uk/reshelp/findhelprestype/news/database.pdf

5. https://developers.google.com/maps/
Free and open source, Juxta Commons (juxtacommons.org) is an online workspace for comparing multiple witnesses to a single textual work, privately storing collations, and sharing visualizations.

Originally offered as a downloadable Java-based application developed by the Applied Research in 'Patacriticism group at the University of Virginia in 2005, Juxta was taken up by NINES (http://nines.org) and transformed into a web service. This open-source API modularizes the sequence of steps required for digital collation and offers more options for working with XML documents. In addition, the NINES R&D team created the interface for Juxta Commons, a destination site for collation on the web.

Juxta Commons offers three visualizations of the differences between a group of texts: the heat map (an overlay of the texts with differences highlighted by color), the side-by-side view (two texts visualized with lines connecting the sites of difference) and the histogram (a global view of the text illustrating the portions with the most change across versions). It is also compatible with TEI Parallel Segmentation, so that users can upload their encoded files and make use of Juxta’s visualizations, or those with text or XML files can export their collations as a digital critical apparatus.

The interface streamlines the workflow of the original desktop application, and allows the user to edit their sources, filter the XML content included in a given collation, and share their results in a number of different ways. Even though the tool provides sophisticated options for working with texts (XML and TXT), our goal in designing Juxta Commons was to allow teachers, scholars, programmers and any other users curious about variants between documents the ability to create, collate and share their findings with others without requiring any extra software downloads or logins.

In taking Juxta to the web, NINES has transformed the tool into a rhetorical as well as analytical tool, enabling initial discoveries to be interrogated, explored, and ultimately offered as a new kind of evidence for scholarly arguments. Instead of relying upon footnotes or links in a digital edition, a scholar can embed their collations within the argument itself, targeting sites of interest, and encouraging readers to explore the full visualizations on their own.

Even as it presents some exciting opportunities for students of book history and textual criticism, Juxta Commons also broadens the scope of interest in collation to the many different kinds of texts on the web. Speeches and transcriptions, Wikipedia article versions, news releases, Google Books and HATHI Trust texts: all these require the kind of authentication and analysis that Juxta Commons makes possible.

Please visit the site juxtasoftware.org to learn more, and to find sample sets to spark your creativity.
The importance of scientific visualization for basic and applied research has been recognized as an importance aspect of scientific practise in many disciplines. Recent research trends in the Humanities in general and in Digital Humanities in particular are no exception in this respect (Culy and Lyding, 2010). The goal of the present paper is threefold: (i) to survey different types of scientific visualizations needed for language data, (ii) to describe a set of web applications that have been implemented in the context of the CLARIN–D project [1] , and (iii) to demonstrate the added value of visualization.
CLARIN offers language resources on a large scale, with text corpora often exceeding 100 million words, with spoken and multi–modal data recorded and annotated at different tiers, and with structured language resources of high complexity. In all instances, the querying of such resources will result in new data sets of considerable quantity and complexity. These results are typically rendered in raw data formats that are not conducive to direct inspection by the user. This lack of readability provides a major obstacle for Humanities scholars who are not accustomed to perusing large amounts of data in such a raw, digital form. To overcome this impasse, it is crucial to render data sets in a form that is cognitively more accessible and that highlights the central characteristics of the data in an intuitive fashion.
One area of language–related research where visualization is particularly useful concerns the domain of language variation and language diachrony. The web application CiNaViz (short for City Name Visualization) [2] has access to names and geographical coordinates of 1.162.040 geographical locations all over Europe. With its query interface, researchers can search for specific distributions of city names and visualize them on a map. As an example of language variation, Figure 1 shows on the left the distribution of city names ending with bach (red), beck (blue) and bek (green) in Central Europe. One can see that there are clear separators between the three variations, where the separator between bach and beck follows the so called Benrath Line which divides the nothern from the southern dialects in Germany. The map on the right hand side of Figure 1 shows locations containing the substring schwab. It is evident that these locations are not located in the region called Schwaben (Swabia) today (the region around Stuttgart/Tübingen/Ulm, marked on the map by a blue ellipse). This is because the Swabian people were relocated during the Middle Ages from their original places of residence.
 
Figure 1:
Visualization of language variation and diachrony
The use of language data is, of course, not only relevant for linguistic and philological research. The social sciences also draw on language data for empirical investigations of various kinds. One area where visualization can help is in tracking the dynamics of culture as reflected in language use. Michel et al. 2010 have coined the term culturomics for this new terrain of digital humanities at the interface of humanities and social sciences. One area discussed by Michel et al. concerns the tracking of celebrity names over time by frequency of mention in the Google Books corpus. Such data are, of course, of immediate relevance for historians, sociologists, as well as researchers in media and cultural studies. While Michel et al. based their visualizations on a very large, closed data set, we have applied the same type of techniques to a much smaller and dynamically updated corpus of news articles harvested from the online news feeds of major German newspapers and magazines.
Our WhoIsInTheNews [3] web application consists of two parts: (i) a web crawler, which downloads German news feeds everyday and extracts the contained named entities with the help of a chain of WebLicht web services [4] . (ii) A graphical user interface to the stored named entities which allows the user to analyze and visualize the appearances of named entities over time and geographical diffusion. For a morphologically rich language like German, linguistic– preprocessing of the raw data is necessary and is performed by a WebLicht workflow which consists of the following automatic annotation steps: tokenization, part of speech tagging and named entity recognition.
Figure 2 shows the occurrences of the names Romney (red) and Santorum (blue) in German newsfeeds over a 12 month timeframe, from November 2011 until October 2012. The visualization captures in a concise way the dynamics of the German news coverage of the two leading candidates in the Republican primaries for the 2012 U.S. presidential election. Despite the sometimes unexpected victories by Santorum in several primaries such as Minnesota, Missouri and Colorado, Romney had a consistently higher coverage in the German media, with Santorum dropping out of the German news alltogether shortly after abandoning his campaign on April 10. The WhoIsInTheNews application shows that visualization techniques can be used not just to plot unrelated career paths of celebrieties as in the case of Michel et al., but also to track the interdependence of such paths in textual materials harvested from online sources in a continuous and incremental fashion.
 
Figure 2:
Tracking of celebrity names over time
The WhoIsInTheNews web application also supports the visualization of named entities that refer to geographical locations. Figure 3 shows the distribution of the 1000 most frequent city names, referenced in the German news feeds harvested over the November 2011 to October 2012 timeframe. Not surprisingly, the density of locations is highest among European cities, followed by the Middle East, the east coast of the United States and the Pacific Rim. Equally noteworthy are the omissions: much of the Midwestern states of the United States, the vast Russian territory outside St. Petersburg and Moscow, as well as much of Africa and South America.
 
Figure 3:
Tracking the 1000 most frequent city names
Technical realization in CLARIN–D
All visualizations described in this abstract are embedded in the CLARIN–D infrastructure. More specifically, they are implemented as Web 2.0 Ajax driven web applications which make use of annotation web services included in WebLicht [5] , a Servic Oriented Architecture for the orchestration of RESTstyle web services (Hinrichs et al. 2010 and Dima et al. 2012).
 
Figure 4:
Embedding web services into web applications
Conclusion and Outlook
Space limitations do not allow us to present the full range of visualization web applications and their significance for Digital Humanities research that are offered in the CLARIN–D infrastructure. For an overview of the visualization tools offered in the CLARIN–D project, we refer interested readers to the WebLicht tools suite (http://weblicht.sfs.uni-tuebingen.de). Therefore, we concentrated on those visualizations that are of relevance to disciplines beyond Linguistics.
The easy web availability of the WebLicht tools is a crucial advantage over existing visualization tools, which typically require expertise in software installation and customization beyond the competence of ordinary digital humanities users. We therefore view web availability as a crucial advantage over existing solutions based on geographical information systems.
References
Culy, C., and V. Lyding (2010). Visualizations for exploratory corpus and text analysis, in Proceedings of the 2nd International Conference on Corpus Linguistics CILC–10, May 13–15, 2010, A Coruña, Spain, pp. 257–268.
Dima, E., E. Hinrichs, M. Hinrichs, A. Kislev, T. Trippel, and T. Zastrow (2012). Integration of WebLicht into the CLARIN Infrastructure. Proceedings of the joint CLARIN–D/DARIAH Workshop "Service–oriented Architectures (SOAs) for the Humanities: Solutions and Impacts" at Digital Humanities Conference 2012. Hamburg. 17–23.
Hinrichs, E., M. Hinrichs, and T. Zastrow (2010). WebLicht: Web–Based LRT Services for German. In: Proceedings of the Systems Demonstrations at the 48th Annual Meeting of the Association for Computational Linguistics (ACL–2010). Uppsala, Schweden. 25–29.
Michel, J. B. et al. and The Google Books Team (2010). Quantitative analysis of culture using millions of digitized books. Science 331, 176 – 82. (doi:10.1126/science. 1199644)
Notes
1. http://www.clarin-d.de
2. CiNaViz is freely available as a web application (see http://weblicht.sfs.uni-tuebingen.de/CityViz/). The Java code of CiNaViz is available under the GPL v. 3 generally used within CLARIN–D
3. WhoIsInTheNews can be accessed at http://weblicht.sfs.uni-tuebingen.de/ne/. The source code of WhoIsInTheNews is free available under GPL 3. The only IPR restriction concerns the particular data set of news articles, harvested from the news feeds of German newspapers and further processed by the WhoIsInTheNews application. Scholars who want to utilize WhoIsInTheNews for their own data sets and offer this web application on their own server, are free to do so under GPL.
4. https://weblicht.sfs.uni-tuebingen.de
5. The WebLicht acronym stands for Web based Linguistic Chaining Tool (https://weblicht.sfs.uni-tuebingen.de)
Musical scores are the central resource for music research, and research involving the capture, transmission, and analysis of these resources is a unique and largely untapped area in the Digital Humanities. For hundreds of years before the invention of audio recording music scores were the only format capable of capturing and transmitting sounds from one musician to the next. Our project, the Single Interface for Music Score Searching and Analysis (SIMSSA) targets digitized (scanned) music scores, and seeks to provide tools for searching and retrieving these resources. We seek to replicate the successes of similar initiatives for textual materials, like the HathiTrust or Google Books, in bringing large collections of musical materials to anyone with an internet connection. We have made the first steps towards this goal by developing a number of prototype systems, and have been actively seeking partnerships with music researchers and libraries.
An unprecedented number of musical scores are being made available as libraries digitize their collections. Nevertheless, there are two major challenges to using them. One is that the digitization efforts are distributed: all across the world, many different libraries, archives, and museums are digitizing their collections of music scores, both printed and manuscript, but no standards exist currently to unify these collections so that these digital scores can be easily found. The other challenge is that it is virtually impossible to perform content-based search or analysis of online scores — a sharp contrast with the situation for digitized texts. There is simply no reliable optical music recognition (OMR) software that can achieve results comparable to the optical character recognition (OCR) software that institutions use to make their text collections searchable. Until digital page images of musical scores can be converted into computer-readable format using OMR, the full potential of search, analysis, and retrieval of digital music collections is cannot be realized.
We currently have two research teams in place, developing tools to support our efforts. One team is concentrating on finding scores available as digital images on the Internet. This task requires crawling the Internet, automatically discovering digitized books, articles, and facsimiles of music sources. Each digital image is analyzed to determine whether it contains printed music. This type of large-scale music document analysis has never been attempted before, thus new and efficient algorithms need to be developed. Our preliminary study involving 659 page images, resulted in 98.7% recall; missing only 3 pages containing musical scores (false negatives). We are aware of large sites that contain music scores among the millions of books already digitized, such as Google Books, Internet Archive, HathiTrust, and the Bibliothèque nationale de France. When our system determines that there are music scores in a book, we will index the information so that in the future, each digital object will be easily locatable. In other words, we will automatically create a catalogue of digitized scores so that researchers can use a central resource to search hundreds of independent websites containing scores — much like web crawlers do today.
Our second team is working on developing content-based analysis tools for performing large-scale OMR. Current OMR tools are highly limited in scope — they can only work with a subset of music notation types, and are restricted to operating as a desktop application. We are currently developing new, web-based OMR tools that will allow us to operate large, flexible OMR systems through a web browser. We are also developing new methods of “crowdsourcing,” allowing us to distribute the steps of the OMR process to a wide, global audience. This work represents a significant advance in the state-of-the-art for OMR systems.
The outcomes of the SIMSSA project will prompt further exploration into large-scale digitization, transcription, retrieval, and analysis of music documents. The larger agenda behind SIMSSA is to make all musical documents available in electronic format to the wider public — an ambitious goal, but one that has some precedent in the library and musicological domain. To achieve this goal, we recognize that there are both technological and intellectual issues that need to be addressed.
The technological outcome of the SIMSSA project will be the development of powerful software tools that are accessible and usable by our constituent communities. Through the creation of web crawling and music image indexing systems, we hope to unlock the contents of existing digital music page images and convert them into searchable and analyzable data. The creation of an advanced, online toolkit for optical music recognition will also assist researchers in performing their own content analysis, both for improving the results of our indexing system, as well as operating on their own document collections.
Intellectually, the SIMSSA project will open up the possibility of performing search and analysis on the world’s musical collections, which will create new avenues of exploration in music theory and history. Currently researchers are limited to small personal data sets or to music that they have transcribed and analysed by hand. Researchers need tools that provide the ability to search across thousands of documents. This will promote discoveries about the nature of music that would have taken years or even lifetimes to do manually.
We also hope to tackle some of the issues surrounding the creation of the digital scholarly edition for music. While the digital edition represents several important advances over the traditional print edition by virtue of being in a dynamic, interactive environment, it also presents some difficulties in how these musical editions are represented at the computational level, and the amount of complexity involved in making these editions a reality. The large-scale nature of the SIMSSA project presents a unique opportunity for practitioners in musicology, library science, and computer science to develop standards, tools, and best practices for creating the digital scholarly edition.
The SIMSSA project is well into its second year of operation and we have already presented a number of prototype projects and software packages useful for both our project, and the digital humanities in general. We have developed the Diva.js image viewer, a web-based software package that allows users to quickly and efficiently view extremely high-resolution (multi-gigabyte) document images on the web. We have demonstrated both the Liber Usualis prototype for performing musical search and retrieval, and the Salzinnes Antiphonal prototype that combines high-quality scholarly information from the CANTUS website with a unique exploration interface that allows users to see this information in situ with the original page images.
We are continuing to develop a number of important relationships with the large, national libraries (British Library, Library of Congress, Bibliotheque Nationale de France, Bayerische Staatsbibliothek) who will provide access to their digital collections. We are also working closely with the Music Encoding Initiative, an international group of scholars, technologists, and librarians based at the University of Virginia who are developing standards and best practices for the digital music edition.
Musical scores is a unique and, as-yet, untapped area for digital humanities research. We have a very limited understanding of how people use and interact with vast amounts of musical information at their fingertips, since there are no large-scale initiatives that offer this. To address this we are actively creating a community of technologists, musicologists, librarians, and other interested parties to begin to uncover the many questions that must be answered, and to explore the new areas of research that will emerge from this work.
Notes
1. http://ddmal.music.mcgill.ca/diva/
2. http://ddmal.music.mcgill.ca/liber/
3. http://ddmal.music.mcgill.ca/salzinnes/
4. http://www.music-encoding.org/
I. Overview
From 1680 to the present day, the Comédie-Française (CF), France’s national theater troupe, has kept daily records of its repertory, box office receipts, and expenses, as well as additional information on set and costume design, actors’ roles, and other matters. This wealth of information is a vital resource for theater scholars, literary historians, and those interested in the political, social, and cultural history of France more generally. Students of French theater and literature have long been interested in the performance practices and institutional functioning of the troupe that held a monopoly on the public performance of works by Molière, Racine, Corneille, and Voltaire in Paris before 1789. Historians have come to realize that the public theaters of Paris, especially the CF, vividly reflected the mounting social and political tensions of the time. Because the CF combined the rituals and concerns of the court, the ideas of the philosophes, and the everyday actions of working class Parisians in the same space, some have argued that the troupe played a central role in the negotiation of French national culture. And yet, the workings of the CF have been difficult to analyze. Access to the CF’s archives in Paris has historically been extremely limited and the sheer volume of information quickly overwhelms traditional humanities research methods.
As an international collaboration between Hyperstudio — MIT’s digital humanities research lab, MIT’s department of history, and the Bibliotheque-Musee de la Comédie-Française, the Comédie-Française Registers Project (CFRP) seeks to provide access to and new ways to analyze such culturally significant data. In addition to creating an online database containing each daily receipt register of the Comédie-Française from 1680 to 1793, the CFRP also features a suite of interactive search and data visualizations tools, which allow for both filtering and complex analysis of information according to a set of parameters. Being able to apply different parameters, filter data, and see the results dynamically generated within a set of visualizations allows scholars to discover patterns and ask new kinds of research questions not possible without the tool. In this short paper, we will discuss the relationship between original sources, tool creation, and new research questions arising from visualization, in order to ask how quantitative analysis at unprecedented “levels of abstraction” (Witmore, 2012) might contribute to existing methodologies for historical research.
II. Current Implementation and Research Questions
Funded by grants from the Office of the Dean of MIT’s School of Humanities, Arts, and Social Sciences and a number of other internal MIT sources, the CFRP has already made substantial progress on a number of levels. We have created a database of high quality digital copies of thirteen seasons of the daily registers (1780-1793) and created a number of search and interactive visualization tools based on this initial data set. Our custom faceted browser allows users to filter and view archival documents according to a number of parameters, including play title, author, genre, year, number of tickets sold, ticket price, location of seats within the theater, and whether a particular showing was a premiere, a first run, revival, or revue. This search tool is also directly integrated with a range of data visualization tools, which are manipulated either by changing parameters in the faceted browser or the visualization tools directly. These visualizations encompass layered histograms, heat maps, parallel axis graphs, and flexibly scaled timelines.
In experimenting with potential combinations of such parameters through the faceted browser, our team has already generated a number of initial research questions which we believe could reveal telling patterns through data visualization. Rather than simply using data visualization to demonstrate pre-existing conclusions, our methodology emphasizes the process of discovery in research, allowing interesting questions and patterns to emerge from the scholar’s dynamic interaction with the faceted browser tool:
•        Like most theatrical spaces of the time, the physical structure of the theater where the CF performed was highly stratified according to class, with tickets in the loges costing significantly more than those in the parterre. Each register contains information on the kinds of tickets sold for each performance, along with a diagram of the theatrical space. Thus, pairing the number of tickets sold in each seating section with the title of the play performed could reveal the popularity of certain titles or genres with particular demographics. We are also currently developing an interactive version of the theater diagram which would allow the user to toggle between different titles or individual performances, and see the relative socioeconomic makeup of specific audiences through a map of the physical space.
•        Given that the CF registers range from 1680 to 1793, researchers can use the CFRP tools to study the effect of important political and cultural events like the French Revolution or the death of a king on the popularity of certain genres and authors, as well as general ticket sales. In terms of the latter question, experiments with our parallel axis graph have revealed large gaps in ticket sales in mid 1774, reflecting the death of King Louis XV.
•        Researchers could also use this data to track how the popularity of emerging plays interacted with more established works. Thus, visualizations of the data could begin to ask such questions as: Were the great seventeenth-century tragedians Racine and Corneille diminished by the success of Voltaire’s tragedies in the eighteenth century? Did the success of Voltairean themes indicate a new public sensibility in the Age of Enlightenment? Did audiences in the decades leading up to the Revolution prefer comic playwrights or tragic ones, and what might this tell us about pre-Revolutionary political culture?
According to the French theater historian Christian Biet, simply having the ability to answer such macro-level questions about the Comédie-Française could have a “transformational impact on the study of eighteenth-century French theatre.” But we also believe that our project will be of interest to those outside this specialized field. Because CFRP’s approach emphasizes dynamic interaction with visualization tools as a means to generate new research questions, we believe that our project could provide new insights and approaches to any scholar interested in the use of data analysis for historical research.
III. Conclusion and Future Directions
These major questions will receive new and more precise answers as we continue to create visualization tools for the analysis of CFRP data. As more scholars begin to explore this rich database, we believe that other interesting lines of inquiry will emerge, prompting us to tweak existing tools and create entirely new ones which support the needs of these domain experts. However, while we believe that simply having the technical ability to search through large amounts of documents at different levels of granularity, compare parameters, and control scales of analysis in data visualization is important in generating new scholarly insights, we also believe that such large-scale data crunching must be put into conversation with historians’ existing practices of close reading. Following Dan Cohen’s call in a recent essay on Google’s n-grams to not fetishize the macroscopic in data visualization, the CFRP toolset is meant to facilitate a scholarly process of toggling between macro- and micro- scales: moving seamlessly “from distant reading to close reading, from the bird’s eye view to the actual texts” (Cohen, 2010). By allowing scholars the ability to control the levels and parameters with which they engage with archival documents, the CFRP helps generate new questions for historians, while also relying on their expert knowledge to discern patterns and anomalies in the data. The principal justification for the CFRP, therefore, is that it will not only make data contained in the registers immediately accessible to anyone without access to the original sources in Paris, but also permit users to interpret data in ways which merge quantitative and qualitative approaches.
References
Witmore, M. (2012). Text: A Massively Addressable Object. In Gold, M. K. (ed.), Debates in the Digital Humanities.Minneapolis: University of Minnesota Press. 324-327.
Cohen, D. (2010). Initial Thoughts on the Google Books Ngram Viewer and Datasets. Dan Cohen’s Digital Humanities Blog. http://www.dancohen.org/2010/12/19/initial-thoughts-on-the-google-books-ngram-viewer-and-datasets/ (accessed 11 November 2012).
In the past two decades or so, national libraries have been digitising millions of pages of books, newspapers, magazines and other text based collections. In this digital age, the research landscape is changing rapidly, with scholars able to ask new types of questions and answer them in novel ways by working with a wide variety of materials and in new collaborative modes.
As a national library, the British Library (BL) holds over 150 million items dating as far back as 2000 BC and is responding to this climate by realigning its services and structure, including the creation of a new digital scholarship department, but there are still fundamental changes the Library needs to make in order to allow researchers to fully exploit digital resources. We are passionate about working with researchers and scholars so that they can use our digital collection to create new knowledge.
The National Library of the Netherlands (KB) has planned to have digitised and OCRed its entire collection of books, periodicals and newspapers from 1470 onwards by the year 2030. But already in 2013, 10% of this enormous task will be completed, resulting in 73 million digitised pages, either from the KB itself or via public-private partnerships as Google Books and ProQuest. Many are already available via various websites (e.g. kranten.kb.nl, statengeneraaldigitaal.nl, anp.kb.nl,earlydutchbooksonline.nl) and we are working on a single entry point to (re)search all sets simultaneously.
Of course, as an institution that serves the community, these (vast) digitisation projects are not done for ourselves and we make everything we digitise publicly available. Unfortunately, as a library that is not connected to a university — with researchers of their own — a place for us in the Digital Humanities landscape is not as naturally formed as that of a university library. But we do have interesting material and want to get our data out there and have it used by researchers, the general public or anyone who is interested in large corpora of text. But how can we best achieve this?
What does the Digital Humanities community need from us? What do we have to do be as institutions to serve researchers who want to get their hands dirty with this data? How would you like to access this data? Would you prefer a lab with support from material experts and programmers? Would you rather have quantity over quality? Which formats should we offer?
This poster will present the KB as the National Library of the Netherlands, and the collections we(currently)have, but also our efforts to make our data available as complete sets by setting up a Data Services team that focuses on the questions raised by these new activities. This poster will also present the British Library, the data and services it provides and the BL Labs project which is designed to achieve the transformational steps that will change the way the Library provides access to its digital collections and enable scholars to research entire collections rather than just individual items.
We hope to stir up a discussion with the digital humanists at DH2013 to ascertain whether our work is going into the right direction, and what researchers need and expect from us as national libraries. How can we help you?
1. Introduction

The Princeton Prosody Archive (PPA) is a full-text searchable database of nearly 10,000 digitized texts – comprising 800 million words – on prosody published in English between 1750 and 1923. During the 2012-2013 academic year, a grant from the Mellon Foundation supported the completion of the PPA’s first phase. This poster will reflect on the outcomes of the start-up stage, as well as some of the challenges and opportunities the PPA anticipates as the digital collection expands. Conference participants are encouraged to visit prosody.princeton.edu to access the Archive’s beta-site.
2. Prosody and Historical Poetics

In the nineteenth century, “prosody” – which refers to both pronunciation and the technicalities of versification – was codified as the fourth section of the grammar book, after “orthography,” “etymology,” and “syntax.” By the early twentieth century, “prosody” referred primarily to versification. In recent years, scholars of English literature have begun questioning the uniformity of poetic terminology, recognizing terms such as “prosody,” “meter, “tone,” or “rhythm” as culturally determined and fundamentally unstable concepts that have shifted through the centuries. By turning to historical texts, they are tracing how inherited notions of poetic form developed over time, and in turn, painting a more accurate picture of the evolution of English-language discourse on poetics.
As the field of historical poetics has grown, so too has our access to nineteenth-century materials through online archives. The majority of these digital resources, however, are primarily focused on prose works, and thus both technological and scholarly innovation has been made in the field of prose. The PPA is filling the gap as the only digital archive dedicated to the study of poetics, writ large, and allowing scholars to practice the kind of broad-view historical research the field demands. The PPA aggregates foundational texts in the history of poetics, reviews of these texts, debates about poetics in the public press, and grammar books and poetic handbooks that present contrary definitions and views of poetics so that “big questions” about literary movements and culture can be posed. With this large data-set, we can now ask: How did the changing science of linguistics and increased impulse toward education impact discussions of poetry over time? How often were particular poets used as examples in poetic pedagogy? How, when, and why did certain poetic terms and genres came in and out of use? 
3. Methodology

The PPA partnered with Google Books and HathiTrust in 2011, and the collection is currently composed of works digitized by Google and Hathi.[1] In 2013, the PPA began to develop a beta-site, which, though still under development, allows users to browse, search, and correct its content. To best serve its user community, the PPA functions as a freely-available, user-friendly repository, a trusted scholarly reference source, and a creative workspace that enhances traditional scholarly practices and pedagogies while enabling new ones.
3.1 Curation: Google Books and the HathiTrust Digital Library

The PPA’s initial corpus was selected from the holdings of the HathiTrust Digital Library. We began by gathering every out-of-copyright text referred to by prosody scholar T.V.F. Brogan in his annotated bibliography English Versification, 1570-1980 that had been digitized.[2] Though the availability of Hathi’s digital facsimiles and transcriptions is incredibly valuable, some aspects of their digitization and description present serious technical obstacles to the kinds of analysis the PPA intends to support. The most obvious is that the transcriptions were prepared by a range of Optical Character Recognition (OCR) systems, and few (if any) were hand corrected. Most were digitized as part of the Google Books program, whose OCR tools are not tailored to the vocabulary, orthographic conventions, or typefaces of eighteenth and nineteenth century texts. They were generally unable to capture indentation, italicization, or other formatting, variations in font size, or diacritical marks, not to mention musical notation or non-standard marks.
3.2 OCR and Diacritic Correction: Representing Scansion

When dealing with texts on prosody and versification, accurate representation of diacritics and typographical marks is particularly important. How do you render musical annotation, scansion, line spacing, or iambic markings, for example, into plain text? Because of the focus on notation and the transmission of concepts and terms, particular care must be taken to ensure that these issues do not interfere with (or silently distort) scholarly analysis. To that end, we are developing a model for encoding scansion – the non-textual elements such as musical notation, macron, breve, or other diacritics, including non-standard marks created by the many scholars who attempted to invent prosodic systems in English. Moreover, we will employ the kind of OCR that retains document coordinates for individual characters whose position on the page often conveys important information.
3.3 Metadata Correction: Scholarly Re-use and Linking Data

The metadata we ingest from HathiTrust also presents challenges. One of the PPA’s goals is to allow researchers to trace the development of prosodic discourse across time and place, and the ability to support this functionality depends on consistent and reliable metadata. While the HathiTrust provides the Machine-Readable Cataloging (MARC) records that have been supplied by contributing libraries, the fields indicating the place and date of publication are free text and vary widely in their conventions of encoding. In the PPA’s start-up phase, we developed an application that assembles text and metadata from the HathiTrust Digital Library, performs some initial automated correction, and loads the text and metadata into a Drupal 7 installation, where it can be browsed, searched, and corrected by scholars working with the Archive. Corrections to metadata can be credited to registered and authenticated users, and metadata fields can now even be versioned, using Drupal 7’s native revision control. In this initial phase, however, these corrections are essentially locked in the Drupal data store; they cannot be returned to the HathiTrust Digital Library or conveniently shared with other scholars working with the same HathiTrust volumes in other contexts. Going forward, the PPA will explore possibilities for enacting a workflow on its own metadata, engaging in the correction of HathiTrust metadata and connecting those corrections to linked data resources by working with the Maryland Institute for Technology in the Humanities and the Foreign Literatures in America project.
3.4 Connecting Prosody Networks: Topic Modeling and Visualization

Topic modeling, and specifically Latent Dirichlet allocation (LDA) has received attention in the digital humanities community over the past several years, in part because it is an unsupervised method – it does not require expensive training material or elaborate encodings – and also because it is relatively robust against textual errors. We have begun experimenting with LDA, not only to return a set of “topics” (which are simply distributions over the vocabulary) that often characterize the semantic and thematic composition of the PPA’s corpus in compelling ways, but also as a means by which we can identify mistranscription, special characters, and even musical notation. We also plan to begin experimenting with visualization tools in the following ways: 1) Plotting temporal and geographical metadata; tools such as Google Earth, MIT’s SIMILE, and Leaflet offer practical and intuitive ways to allow users to navigate temporally and geographically situated data sets interactively – for example, to view a three-dimensional chart on a globe indicating the relative prominence of cities as places of publication while moving a time slider through several centuries; 2) Mapping the documents in the corpus by its topical or lexical spaces; here, each document is represented as a point in a high-dimensional space, where the dimensions of the space are features such as counts or frequencies of individual words or n-grams, or the percentage of words allocated to a particular topic in a topic model; 3) tracking discursive networks by quotation identification and citation extraction; for example, the quotation of exemplars could be represented as a bimodal network, with nodes representing both volumes in the archive and lines of verse, and with edges from the former to the latter indicating instances of quotation.
3.5 Sharing Results

The PPA is committed to providing models so that other digital humanists struggling with the question of how to organize and present their own Hathi collections (in their research or in the classroom). Though these scholars might not be subject area experts in prosody or historical poetics, we would like to provide enough information that we might navigate unspecialized visitors through the corpus and share ideas about how they might build similar archives themselves. 
Notes

[1]  We negotiated a Google Distribution Agreement between the Princeton University Library, Princeton Counsel, and HathiTrust that allowed us to access, download, and host all of this data on our own servers.  A spreadsheet of all Archive monographs is available online at “Princeton Prosody Archive Database.” The PPA’s four collections can also be accessed through the HathiTrust site. See: 1) “Brogan's English Versification, 1570-1980” (578 works); 2) “Prosody Archive” (1,308 works); 3) “PPA Subject Search” (6,991 works); and 4) “Graphically/Typographically Unique“ (26 works set aside as possessing especially complex page images that would be misread by OCR).
[2] Brogan, Terry V. F. English Versification, 1570-1980: A Reference Guide with a Global Appendix. Baltimore: Johns Hopkins University Press, 1981.
1. Panel Proposal
The remediation of historical books and newspapers into the digital environment receives ample attention in today’s academic and library communities, and has become a core feature of major digital humanities projects that have enabled innovative methods of inquiry and new scholarly discoveries. However, despite the establishment of pioneering digital resources such as the Modernist Journals Project and the Modernist Magazines Project, the design and research potential of electronic collections of modernist historical magazines remains an understudied and under-theorized topic.

This panel aims to fill this gap by presenting a series of approaches, methodologies and possibilities inherent in the creation of robust digital collections of 20-th century magazines of the arts. We bring together designers of three collections – the Blue Mountain Project (Princeton University), the AAC- Austrian Academy Corpus (Austrian Academy of Sciences), and Digital Archive of Belgian Neo-Avant-garde Periodicals (or DABNAP, at the Royal Academy of Fine Arts Antwerp) – for a conversation addressing both the conceptual underpinnings as well as the practical applications of their work. This international panel will continue a lively discussion started at a conference at Princeton University in October 2013 on remediating avant-garde magazines. Our goal at DH 2014 will be to illustrate a variety of avenues available for digital curation of historical magazine collections, and to move toward a set of shared guidelines for representing this rich material in the electronic environment and facilitating advanced research.

A discussion of the theoretical and practical issues involved in remediating modernist and avant- garde periodicals is both important and timely. In recent years, as the field of Periodical Studies has cohered as subset of print culture scholarship, researchers have started looking at historical periodicals in new ways and discovering that they fundamentally challenge our conventional understandings of modern culture. The recent publication of the Oxford Critical and Cultural History of Modernist Magazines, a hefty three-volume set of essays that discusses over 500 magazines from Europe and the Americas, underscores the relevance of periodicals for today’s research on modern and modernist culture. If, as Sean Latham and Robert Scholes assert in “The Rise of Periodical Studies,” the flourishing of this field has been enabled and invigorated by the affordances of the digital environment – what is the response of the Digital Humanities community to the continued evolution of Periodical Studies? What are our obligations as designers of collections of digital historical periodicals? How must we build resources that embody, allow, and promote the sophisticated new avenues of scholarly engagement made possible by electronic tools and platforms?

The panelists’ answers to these questions will reflect the scope, methodology, and focus of their projects: Crombez’s DABNAP is concerned with magazine as a performance and art text, Biber’s paper illustrates the corpus linguistics approach, and Wulfman and Ermolaev maintain a Periodical Studies perspective. Each will touch upon the intellectual and technological insights that emerge when we remediate the arts magazine, such as: representation of aesthetic, material, and social features, questions regarding materiality (format, typography, paper, binding, etc.), tracing printing and distribution history, data modeling, linguistic analysis, semantic enrichment and tagging, entity and name recognition, interface design, and application and tool-building.

This panel promises to stimulate a coherent discussion that will be informative to a broad range of digital humanists. By presenting a cross-section of types of collections, the panel will demonstrate the current state of innovative projects on 20-th century arts magazines, and help forge the way for future work.

All panelists have agreed to participate.

2. Paper 1: Magazines of Magazines. Corpus Research Applications for New Digital Editions of Historical Magazines (Hanno Biber)
A magazine can be regarded as a specific container of texts. A corpus can be regarded as a structured and complex collection of texts. The simple questions of how to structure these complex texts in text corpora and how to integrate magazines in corpora and to make them accessible for research purposes is particularly challenging. This paper will present corpus research applications for innovative digital editions of magazines. Therefore, three research aspects have to be considered. First, the methodological parameters of corpus research have to be determined and the applicability of corpus linguistics for the question of designing and building digital text editions of historical magazines has to be examined. Second, the research potential of scholarly digital editions of magazines in the context of a digital humanities framework has to be described. And third, a practical corpus- based methodological approach of magazine studies has to be given by investigating the specific textual qualities of the magazines and the specific language use in the magazines’ texts, which have been annotated and made accessible with the help of corpus linguistics as well as innovative interface design and graphic design principles. The study will present several aspects of these research questions illustrated by text examples and discuss the methodological implications of such corpus-based investigations into the use of language in texts in particular. Literary magazines in particular have specific properties that can be recognized and registered by means of a corpus-based study of language. Therefore the literary magazines subsection of the “AAC-Austrian Academy Corpus” will be used as examples for this type of research. This resource is an ideal basis for a corpus linguistic exploration into the study of literary magazines. Corpus-based text studies can be regarded also as instruments of textual critique, whereby the corpus-based approach allows various ways of philological research and text analysis.

The framework of the “AAC-Austrian Academy Corpus” offers a research platform for corpus linguistics in a very broad sense as well as for corpus-based magazine studies in particular. So far, two model digital editions of magazines have been developed within the AAC for the purpose of analysing large amounts of literary texts with a methodological approach offered by corpus linguistics. The “AAC-FACKEL”[6] and the “Brenner online”[7] editions are two examples of how corpus research initiatives can help to develop new applications in the field of digital humanities, in particular for language studies in the context of literature research as well as for historical studies and for broader issues related to cultural studies. The “AAC-Austrian Academy Corpus” has been established as a corpus research initiative concerned with exploring electronic text corpora and conducting research in the fields of corpus linguistics, text analysis and digital text corpora. More than 500 million running words of text have been scanned, converted into machine-readable text and carefully annotated with structural mark-up. The texts that have been integrated into the AAC are German language texts of historical and cultural significance. The historical period covered is ranging from 1848 to 1989, a period showing historical changes with remarkable influences on the language and the language use. In the context of the subset of the AAC digital editions the language of satire written by Karl Kraus and his use of language in his magazine is of particular interest for a study on language use and in particular for a study on language use based upon the principles of corpus linguistics. Apart from the digital editions of the “Brenner online” and the “AAC-FACKEL”, which has been annotated to a large extent, the AAC magazines corpus holdings provide a great number of reliable resources and interesting corpus based approaches for investigations into the properties of these texts. Several other magazines have been digitized making use of XML-related standards. Both the „AAC-FACKEL“ and “Brenner online” offer fully searchable online editions of the journal with various indexes and search tools in a web interface, where all pages of the original are available as digital texts and as facsimile images. The „AAC-FACKEL“ and “Brenner online” allow new methods of scholarly research and philological analysis of texts that are of crucial importance for the history and study of not only the language but also of the status and the properties of the magazine. The edition interface of the AAC digital editions has several sophisticated search mechanisms and indexes as well as five individual frames synchronized within one single window. The philological principles of scholarly digital editions within the “AAC-Austrian Academy Corpus” are determined by the conviction that the methods of corpus research enable valuable text resources and research tools for linguists and literary scholars. The resource is an interesting text basis for corpus linguistic explorations whereby a corpus-based approach allows new ways of philological research and analysis of magazines.

3. Paper 2: The Blue Mountain Project and the Language of Avant-Garde Magazines (Wulfman and Ermolaev)
Introduction
This paper initiates a dialog with Hanno Biber’s contribution to this panel by suggesting that a magazine is not only a container of texts but a text itself. As Carolyn Ulrich, one of the authors of the influential book, The Little Magazine: A History and a Bibliography, wrote to her co-author Frederick Hoffman, “a magazine is a tricky individual”: tricky in its identity and tricky in its individuality. What is a magazine? Is it a particular issue (a manifestation at a particular point in time – a copy, an issuance, etc.), or is it a title with some sort of identity over time (expressed through editorial consistency, name continuity, etc.)?

To ask these questions is to engage Biber’s research approach to the magazine – specifically, “the applicability of corpus linguistics [to] the question of designing and building digital text editions of historical magazines.” While the methods of corpus linguistics are, without a doubt, of crucial importance to the study of historical magazines, other disciplinary methodologies can compliment lexical analysis by highlighting additional key aspects of this diverse and rich material.

In their 2010 book, Modernism in the Magazines: An Introduction, Wulfman and Scholes critique the conventional distinctions that have, historically, been applied to magazines, especially generic labels such as “little”, “mass”, “literary”, “avant-garde”, etc. They suggest employing a more analytic approach instead, one which entails identifying a set of characteristics that contribute to our understanding of magazines and then clustering them in different ways, much as linguists characterize speech sounds by clustering features into phonemes. The Blue Mountain Project at Princeton University is continuing this line of inquiry, and in this paper we describe our development of a language of magazines, and of a technological infrastructure and set of applications to represent this complex language in a robust digital resource.

An Ontology of Historical Magazines
The Blue Mountain Project, based in the Princeton University Library and funded by the National Endowment for the Humanities, was launched in 2012 as a freely-available electronic resource for art, music, and literary periodicals published in Europe and the United States between 1850-1923. In the first 2-year grant cycle, Blue Mountain is making 34 titles (approximately 95,000 pages) available in French, German, English, Italian, Spanish, Czech, Russian, Polish, Finnish, and Danish.

In the first part of this paper, we present our research on an ontology of historical magazines – that is, a set of concepts with which knowledge of magazines is represented, expressed in a vocabulary that denotes the types, properties and interrelationships of those concepts – that has become the core intellectual infrastructure of the Blue Mountain Project. This ontology differs from the terms of descriptive bibliography developed in the library community such as MARC (MAchine-Readable Cataloging), and from the critical terminology traditionally used by scholars and critics. The purpose of our ontology is to provide a framework on which researchers and scholars may encode and describe historical magazines.

To be sure, some of the concepts do come from the language of descriptive bibliography: title, editor, contributor, and so forth; but others are more nuanced than descriptive bibliography usually expresses. Issuance, for example, is often a complex phenomenon, especially for magazines published internationally, making the concept of an original copy from which a digital edition is derived a vexed one and requiring scholars to rethink narratives of publication. Other concepts, like circulation and price, are tied to data that are difficult to obtain; still others, like readership, are complex concepts whose sub-concepts must be teased apart and identified before they may be used meaningfully.

Other terms in the Blue Mountain ontology come from the languages of typography, book history, and graphic design, and are inspired by Jerome McGann’s notion of the “bibliographic code.” In order to discuss the relationship of content elements in a magazine, for example – such as the relationship of advertisements to articles – one must have a common language for expressing layout. Simple text transcription of a magazine's contents is insufficient for many kinds of research; thus our ontology is based on an understanding of the historical language of page composition (columns, paragraphs, various forms of headings, publication metadata, and so on) that is vital to the useful encoding of magazine structure and the analysis of a magazine's meaning.

Advanced Applications: Blue Mountaineer and Blue Mountain Springs
In the second part of our paper, we present an experimental architecture for encoding and expressing the language of magazines. The Blue Mountain Project has been designed to support a variety of research uses, beyond the now-standard modes of full-text searching and page browsing. To support those uses, we create high- resolution digital images and provide robust library-standard metadata including title, issue and constituent-level MODS (Metadata Object Description Schema) and METS/ALTO (Metadata Encoding and Transmission Standard/Analyzed Layout and Text Object) records. In our next phase of work, however, we plan to expose this highly structured data for mining and analysis by means of two new modules: Blue Mountaineer and Blue Mountain Springs.

Blue Mountaineer
The Blue Mountaineer is a set of web applications we will design for exploring Blue Mountain content through visualizations, topic modeling, and other forms of data mining and retrieval. It is intended to showcase the power and utility of a rich XML-encoded database while providing researchers and students with tools they can use in their own investigations. Examples of the type of research Blue Mountaineer will enable include the following:

Social Network Analysis: Users will be able to explore the complex publication webs with tools that enable them to plot graphs of relations among titles, authors, artists, languages, and nationalities using Blue Mountain's rich metadata. They will, for example, be able to see all the issues in which Tristan Tzara and Francis Picabia appeared together.

Data-Driven Timelines and Clusters: Blue Mountain will employ off-the-shelf natural-language processing software such as Apache OpenNLP and the Stanford Named Entity Recognizer, as well as Princeton's own WordNet, to perform first-order named-entity detection on its corpus of magazines. The results of these analyses will be encoded in Blue Mountain's TEI transcriptions, which will make it possible for researchers to discover and visualize sequences and relationships buried deep in the textual data itself. They will be able to compare how two authors use a particular term, for example, or see how the work of a particular artist relates to particular advertisers.

Topic Modeling: The aggregation and clustering of content zones encoded in METS/ALTO make it possible for topic modeling algorithms to perform much more fine-grained analysis of magazines and newspapers than they can perform using unzoned, "dirty-OCR'ed" texts. Researchers will be able to study and compare the abstract topics occurring in the work of two authors, for example, or to compare the topics in a magazine's articles with those in its advertising.

Blue Mountain Springs
Information scientists and digital humanities researchers often want to bypass reader-oriented interfaces and access full-text data directly and programmatically for use with external analytic tools. The Blue Mountain Springs module will make Blue Mountain an abundant source of clean data by providing an application programming interface (API) to Blue Mountain's metadata and machine-readable full-text transcriptions. Blue Mountain Springs will support traditional metadata harvesting via OAI-PMH[14], as well as the more elaborate aggregations supported by OAI-ORE[15]. It will enable software clients to access the plain text of Blue Mountain's materials through web-addressable text streams that can be piped directly into visualization and analysis applications.

4. Paper 3: The Document as Event. Analyzing Artist Networks through a Digital Archive of Avant-garde Periodicals (Crombez)
Introduction
In this panel presentation, I would like to highlight the ideas, problems and outcomes of DABNAP, which stands for the ‘Digital Archive of Belgian Neo-Avant-garde Periodicals.’ A considerable collection of forty artist periodicals from the 1950s to the 1980s is being digitized, in order to examine the underlying network of artists and artist groups.

My main interest will be the issue of semantic enrichment of the source documents. To various degrees, semantic enrichment is already common to many scholarly digitization projects. Think, for instance, of marking up personal names and locations in a TEI-encoded document. But for large-scale projects, manual semantic enrichment is often unfeasable. Can automatic procedures, such as named entity recognition (NER), help to mine art periodicals? Furtherwise, can we imagine and develop software that detects not merely names, but also artistic information in vast collections of text?

In order to answer these ambitious questions, I will first develop a new conceptual model (centered on ‘the document as event’), and then highlight the actual context in which this model will be put in practice.

The Document as Event
The archive of back issues from LIFE Magazine (spanning five decades) was a showcase project for Google Books when the initiative started in 2004. Apart from access through browsing or through the ubiquitous search box, the internet company uses basic text analysis to help users navigate the archive. The interface presents a cloud of words and expressions that are characteristic for the issue that the user is currently browsing. For the February 1937 issue, the web archive is happy to inform the user that “Leon Trotsky,” “Reichstag,” “Studebaker,” and “Kleenex” are among the common terms and phrases. But it is obviously blind to the question whether these names belong to people, organizations, places, or brands.

Current digital archives, then, create at least as many problems as they solve. Users prefer to direct their questions to a simple search box. But the limitations of this model for search are obvious to academic users, and have recently led Google itself to introduce the ‘Knowledge Graph’, enriching search results with semantic metadata. The Knowledge Graph or, more generically, semantic enrichment, shows the future direction of digital text collections.

In order to deal with this new conceptual reality behind current and future digitization projects, let me introduce a new conceptual model to think about such document collections.

I would like to conceptualize the document as event, in order to transfer something of the dynamics of the event onto the document, which is commonly conceived of rather statically. A periodical is occasioned by artistic events, such as the publication of new literary works, the exhibition of visual art, or the presentation of a new theatrical performance. However, as a document, it can also be considered to be an ‘event’ in itself. The text functions as a linguistic meeting space for a wide diversity of named entities. This includes names of artists, writers, dramatists, performers, directors and critics; names of museums, galleries, theatres, companies and schools; and titles of books, art works and theatrical productions. In other words, the concept of the document as event serves as a bridge between the literary and art-historical approach to sources as autonomous documents (and hence all too easily viewed as unconnected), and the linguistic approach to sources as text (which all too easily flattens the document).

The DABNAP Collection
The artistic renewal in Belgium since the 1950s, sustained by small groups of artists, led to a first generation of postwar artist periodicals. Titles such as Le surréalisme révolutionnaire, Cobra, De Tafelronde, Het Cahier and Gard Sivik proved decisive for the formation of the Belgian neo-avant-garde in literature and the visual arts.

During the 1960s and the 1970s, happening and socially engaged art (inspired particularly by the Provo movement) took over and gave a new orientation to artist periodicals. Examples include Happening News, Revo, Anar, Milky Way, Total’s, and, on the side of literature, Labris, Yang, Bok, MEP, Heibel, Boemerang, and many others. Finally, the 1970s and 1980s saw the rise of punk-inspired zines, including Force Mental.

The challenges and difficulties of this project lie in dealing with non-standard formats, types of paper, typography, and non-paper inserts. Paper sizes range from the ludicrously large (A2) to the very small (half of A5). Printing techniques include offset, mimeograph, screen printing and photocopy – resulting in extremely diverse kinds of lettering and typography, which often confuses the OCR software that is used to extract text from the scanned pages.

Apart from the technical difficulties of scanning and extracting text from the heterogeneous source documents, further difficulties of the DABNAP project include interface design and handling copyright issues, which will briefly be discussed in the presentation. The main and final difficulty (or rather, ambition) of DABNAP is to process of extracting complex information about artistic events from the text. This will require, first, to expand common procedures for named entity recognition with techniques for recognizing titles and events. In other words, which means are appropriate, and which linguistic tools have to be developed, for the task of recognizing meaningful relationships between names (e.g., that a certain director is the author of a theatre production)?

References
Both large-scale digitization of books (such as Google Books and the Open Content Alliance) as well as small, curated collections (such as ArchBook http://archbook.ischool.utoronto.ca) are transforming reading and research practices in literary studies and book history. Ryan Cordell’s research using data from the Library of Congress “Chronicling America” project is a prime example of new scholarly engagement with historical newspapers (see Cordell’s “Uncovering Reprinting Networks in Nineteenth-Century American Newspapers,” http://www.viraltexts.org).

Modernist Journals Project: http://modjourn.org; Modernist Magazines Project: http://www.modernistmagazines.com.

The 19th century periodical has been more heavily studied; see especially the project, nineteenth-century serials edition (http://www.ncse.ac.uk) and the essays: James Mussell and Suzanne Paylor,“Mapping the ‘Mighty Maze’: The Nineteenth-Century Serials Edition,” Nineteen: Interdisciplinary Studies in Nineteenth- Century Studies, 1 (2005) and Mussell, Paylor, M. Deegan and K. Sutherland, “Editions and Archives: Textual Editing and the Nineteenth-Century Serials Edition (ncse),” in Text Editing, Print, and the Digital World (Farnham: Ashgate, 2009), 137-158.

See Sean Latham and Robert Scholes, “The Rise of Periodical Studies,” PMLA, Vol. 121, No. 2 (Mar., 2006), pp. 517-531. In this article, Latham and Scholes discuss the context of North American academia.

The Oxford Critical and Cultural History of Modernist Magazines: Volume I: Britain and Ireland 1880-1955 (ed. Peter Brooker and Andrew Thacker, Oxford University Press, 2009); Volume II: North America 1894-1960 (ed. Brooker and Thacker, 2012); Volume III: Europe 1880 - 1940 (ed. Peter Brooker, Sascha Bru, Andrew Thacker, Christian Weikop, 2013).

AAC-Austrian Academy Corpus: AAC-FACKEL. Online Version: »Die Fackel. Herausgeber: Karl Kraus, Wien 1899- 1936«. AAC Digital Edition No 1, (Editors-in-chief: Hanno Biber, Evelyn Breiteneder, Heinrich Kabas, Karlheinz Mörth; Graphic Design: Anne Burdick) http://www.aac.ac.at/fackel.

AAC-Austrian Academy Corpus und Brenner-Archiv: BRENNER ONLINE. Online Version: »Der Brenner. Herausgeber: Ludwig Ficker, Innsbruck 1910-1954«. AAC Digital Edition No 2, (Editors-in-chief: Hanno Biber, Evelyn Breiteneder, Heinrich Kabas, Karlheinz Mörth; Graphic Design: Anne Burdick) http://www.aac.ac.at/brenner.

Robert Scholes and Clifford Wulfman, Modernism in the Magazines: An Introduction (New Haven, CT: Yale University Press, 2010)

Modernist Journals Project: http://modjourn.org; Modernist Magazines Project: http://www.modernistmagazines.com.

Jerome McGann, The Textual Condition (Princeton, NJ: Princeton University Press, 1991)

http://opennlp.apache.org

stanford.edu/software/CRF-NER.shtml

http://wordnet.princeton.edu

http://www.openarchives.org/pmh

http://www.openarchives.org/ore
The Open Philology Project (OPP) at the University of Leipzig aspires to re-assert the value of philology in its broadest sense and has been designed with the hope that it can contribute to any historical language that survives within the human record. It includes three different yet interdependent tasks:

(1) Open Greek and Latin Project (OGL): OGL is currently collecting and scanning editions of classical texts in an effort to build the largest and most comprehensive open-source library of classical philology to date, concurrently contributing to the expansion of Google Books. Where existing corpora of Greek and Latin have generally included one edition of a work, the OGL corpus is designed to manage multiple, copyright-free editions and translations.

The digitization workflow involves OCR, correction and encoding in EpiDoc-compliant XML. The large volume of data we aim to generate requires significant computational power and task management, thus entreating a partnership with two Data Entry companies who carry out each operation under the supervision of the Leipzig team. While performed by our contractors, OCR correction is facilitated and partly automated thanks to a proofreading tool jointly developed by Leipzig, Mount Allison University and the CNR (Bruce Robertson of Mount Allison University, Canada, and Federico Boschetti of the CNR, Italy) Works currently under conversion include, amongst others, the Patrologia Latina, the Patrologia Graeca, the Commentaria in Aristotelem Graeca.

Moreover, Leipzig has established international collaborations aiming at creating open-source, curated collections and electronic editions of Greek and Latin literature. Editorial projects include the Digital Fragmenta Historicorum Graecorumproject, Digital Athenaeus, and Bibliotheca Aeschylea. Furthermore, collaborations with Croatia, Bulgaria and Georgia will yield machine-actionable versions of translations of classical literature in these languages, thus opening-up research into less-explored textual heritage.

(2) Historical Languages e-Learning Project (eLP): the development of dynamic textbooks that use richly annotated corpora to teach the vocabulary and grammar of texts that learners have chosen to read, and at the same time engage users in collaboratively producing new annotated data. eLPis developing computationally customized learning materials for historical languages, beginning with Ancient Greek. The text selected for the pilot is the Pentecontaetia, part of Thucydides' History of the Peloponnesian War. Users learn through active engagement with the text and through the contribution of their own annotations. Future work will extend the system to accommodate other corpora.

At the core of eLP lies increasing the accessibility and enjoyability of the morphosyntactic and semantic annotation of text (e.g. treebanking), including that deriving from the OGL corpus. The creation of such a richly annotated and searchable text repository will serve a variety of purposes, including research in philology, Natural Language Processing (NLP), historical linguistics, and second language acquisition (SLA).

The production of automated queries to support this dynamic, customized, and localized interface relies upon the backend storage of complex textual data. The chosen graph model meets the broad requirements of the e-Learning application while retaining features of the real world objects represented by the data. The absence of schemas within graph databases enables extensibility, while maintaining a stable experience for users through the use of REST APIs.3 The web interface takes the data and adapts its presentation to individual needs and access devices. HTML5, CSS3, and responsive technologies provide an appropriate experience to users regardless of how they access the system, while templating systems allow for resources that are structurally accessible via any first language.

(3) Open Publications and Data Revenue Models: OPP is establishing a new model of scholarly publication in a born digital environment. Such a task is accomplished through Perseids, which is a collaborative platform for annotating TEI XML documents in Classics, including inscriptions and manuscripts. The main publication model within the OPP is the Leipzig Open Fragmentary Texts Series, whose goal is to establish open editions of ancient works that survive through quotations and re-uses in later texts. Such editions are fundamentally hypertexts and the effort is to produce a dynamic infrastructure for a full representation of relationships between sources, quotations, and annotations about them.

With open data meaning by definition free access for all users, the OPP team has already begun thinking of ways for it to be financially sustainable for years to come. The team intends to devise business models to sustain and maintain distributed open source learning and discourse. The core principle is to move away from charging for monopoly access to data, to charging instead for services that allow users to identify, analyze and then contribute to increasingly complex open data, with services for faculties, students and for the interested public set at recognized and affordable price points.

References
Bruce Robertson of Mount Allison University, Canada, and Federico Boschetti of the CNR, Italy.

Digital Fragmenta Historicorum Graecorum (DFHG): www.dh.uni-leipzig.de/wo/open-philology-project/the-leipzig-open-fragmentary-texts-series-lofts/digital-fragmenta-historicorum-graecorum-dfhg-project; Digital Athenaeus (with the University of Nebraska and the Perseus Digital Library), Bibliotheca Aeschylea (with researchers based in Leipzig and in Italy).

For more information about REST APIs, see: en.wikipedia.org/wiki/Representational_state_transfer (Accessed: 4 March 2014).


    
        
            
                Creating Time Capsules for Colonial Botanical Drugs in the Early Modern Low Countries
                
                    
                        Zervanou
                        Kalliopi
                    
                    Utrecht University, The Netherlands
                    k.a.zervanou@uu.nl
                
                
                    
                        Klein
                        Wouter
                    
                    Utrecht University, The Netherlands
                    W.Klein@uu.nl
                
                
                    
                        Van den Hooff
                        Peter
                    
                    Utrecht University, The Netherlands
                    P.C.vandenHooff@uu.nl
                
                
                    
                        Bron
                        Marc
                    
                    Utrecht University, The Netherlands
                    mbron@yahoo-inc.com
                
                
                    
                        Wiering
                        Frans
                    
                    Utrecht University, The Netherlands
                    F.Wiering@uu.nl
                
                
                    
                        Pieters
                        Toine
                    
                    Utrecht University, The Netherlands
                    T.Pieters@uu.nl
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Short Paper
                
                
                    history of botany
                    history of pharmacy
                    ontologies
                    linked data
                
                
                    historical studies
                    metadata
                    natural language processing
                    ontologies
                    digitisation
                    resource creation
                    and discovery
                    knowledge representation
                    xml
                    internet / world wide web
                    semantic web
                    linking and annotation
                    English
                
            
        
    
    
        
            
                The History of Medicinal Plants from the New World Case Study
                As proof of concept for semantic interoperability, our work focuses on cultural heritage data relating to the history of medicinal plants in the Low Countries roughly from the moment that natural drug components from the New World started penetrating Europe until the introduction of chemical and synthetic drugs in the nineteenth century (i.e., 1550–1850). The resulting system aims at enabling a historian to investigate the trajectories of colonial drug components in the Low Countries, and the patterns of correspondence, trade, and knowledge exchange that lie underneath it. Our case study is meant to provide the aggregated data required for new information to surface, which an individual researcher is unable to produce by means of traditional historical research alone.
                The factors leading to the adoption of a given drug component are revealing of the dynamics of the medical market at that historical period of time. Recent research showed that drug adoption has only partly to do with therapeutic qualities. Equally important are non-medical factors such as public acceptance, marketing, and availability (Pieters, 2004; Gijswijt-Hofstra et al., 2002; Friedrich and Müller-Jahncke, 2009). A special focus on the trajectories of individual drugs has proven itself a valuable research method in this respect. For colonial botanical drugs in the early modern period, several publications have paved the way for this novel approach in the history of tropical medicine (e.g., Vöttiner-Pletz [1990] on guaiacum, used to treat syphilis; Foust [1992] on rhubarb; Jarcho [1993] on Peruvian bark).
                Another interesting aspect of our case study relates to the circulation of knowledge and goods in the early modern period with particular attention to exotic botany and pharmacy in the Netherlands (as in, e.g., Wilson, 2000; Schiebinger and Swan, 2005; Cook, 2007; Dauser et al., 2008; Dupré and Lüthy, 2011; Snelders, 2012; Van Gelder, 2012). Knowledge about the vicissitudes of individual drugs across time and place may enrich current debates about the circulation of knowledge and goods. Moreover, sequenced data about colonial drugs may reveal trajectory patterns and the mechanisms of trade and exchange in the early modern period. 
                These aspects of our case study make it thus an excellent ground for digital research methods experimentation and a resource for other humanities areas, such as pharmacy, ethno-botany, philosophy, and science of the Enlightenment period, and socio-economic studies of the respective colonial trade period. 
            
            
                Creating Time Capsules
                    2
                
                Our data sources are of two main categories: structured database data, referring to linguistic, pharmaceutical, and botanical data, such as the Dutch Chronological Dictionary and the Economic Botany Database; and unstructured free text corpora collections, such as pharmacopoeias, medical books, and other Early Dutch text corpora. The complete list of our current data sources is illustrated in Table 1.
                
                    
                    Figure 1. Time capsule architecture overview.
                
                An important consideration and challenge in our database data lie in their various formats (relational database, Excel tables, etc.) and in our text data (OCR errors and transcribed data in Latin or Early Dutch). Most importantly, a serious issue lies in the spelling and semantic variation of both drug component as well as plant names and in the ambiguity of their taxonomic classification, which introduces fuzziness into our data classification. For example, a drug ingredient may or may not originate from a certain plant, which may or may not belong to a given plant taxonomy class and may or may not correspond to a given contemporary name.
                For the semantic integration of our data, we adopt a linked-data approach whereby our data sources are first converted into RDF and then linked to our Historical Drug Components Ontology, so as to enrich existing information of the respective KB, as illustrated in Figure 1. In order to address the issues associated with variation, we incorporate historical lexicon resources, and we further enrich these by application of automatic spelling variation detection (Reynaert, 2014). The issue of uncertainty in our data classification is currently addressed by adopting a fuzzy classification approach, whereby an ambiguous instance may be classified into more categories—e.g., a name can be an instance of both a plant concept and a drug component concept. Finally, our structured data sources are used as a resource in detecting relevant information about drug components and their potential use in text documents. The resulting semantic annotations in these historical text corpora are in turn also converted into RDF and automatically linked to our structured sources, so that original historical written evidence is provided to the researchers. 
                
                    
                        
                            Structured Data
                        
                    
                    
                        
                            Botanical, pharmaceutical, historical, and image resources
                        
                    
                    
                        
                            Ontology of Historical Drug Components
                            
                                Historical drug components information found in historical pharmaceutical sources
                            
                        
                        National Museum for the History of Pharmacy
                    
                    
                        
                            Economic Botany Database
                            
                                Metadata of objects in the Economic Botany Collection
                            
                        
                        Naturalis Biodiversity Center
                    
                    
                        
                            BRAHMS
                            
                                Metadata for 1.2 million records of plant collections
                            
                        
                        Naturalis Biodiversity Center
                    
                    
                        
                            Snippendaalcatalogus
                            
                                Inventory of plants of the Snippendaal’s 1646 Amsterdam botanic garden
                            
                        
                        
                            Hortus Botanicus Amsterdam
                        
                    
                    
                        
                            IrisBG
                            
                                Current information about the plants of the Amsterdam botanic garden, including pictures
                            
                        
                        
                            Hortus Botanicus Amsterdam
                            http://dehortus.gardenexplorer.org/
                        
                    
                    
                        
                            Dutch Nature Images Collection
                            
                                Images of flora and fauna in the Netherlands and the Dutch Antilles
                            
                        
                        
                            
                                Netherlands Institute for Sound and Vision—Stichting Natuurbeelden
                            
                            http://www.natuurbeelden.nl/
                        
                    
                    
                        
                            RADAR
                            
                                Geographical and research data about botanical macro remains collected during archaeological excavations on Dutch territory
                            
                        
                        Cultural Heritage Agency (RCE)
                    
                    
                        Lexical resources
                    
                    
                        
                            PLAND
                            
                                Plant names in various Dutch dialects, including sources, dates, locations, and name distributions
                            
                        
                        
                            Meertens Institute
                            http://www.meertens.knaw.nl/pland/
                        
                    
                    
                        
                            Chronological Dictionary
                            
                                Historical/etymological Dutch dictionary including first observed sources and dates 
                            
                        
                        
                            Meertens Institute
                            http://dbnl.org/tekst/sijs002chro01_01/
                        
                    
                    
                        
                            GiGaNT
                            
                                Diachronic Dutch lexicon (6th century–present), including spelling variations, proper names, and morpho-syntactic information
                            
                        
                        
                            Institute for Dutch Lexicology (INL)
                            http://www.inl.nl/onderzoek-a-onderwijs/projecten/gigant
                        
                    
                    
                        Free-text data
                    
                    
                        
                            Looted Letters
                            
                                Transcriptions and metadata of c. 3,500 letters, taken as loot from Dutch ships during the Anglo-Dutch wars (1652–1805)
                            
                        
                        
                            Meertens Institute
                            http://www.gekaaptebrieven.nl/
                        
                    
                    
                        
                            Letters as Loot
                            
                                Linguistically analysed subset of c. 1,000 letters from the Looted Letters collection
                            
                        
                        
                            Institute for Dutch Lexicology (INL)
                            http://brievenalsbuit.inl.nl/
                        
                    
                    
                        
                            CKCC corpus
                            
                                Transcriptions and metadata of c. 20,000 letters from the correspondence of 17th-century scholars in the Netherlands
                            
                        
                        
                            Huygens ING (and partners)
                            http://ckcc.huygens.knaw.nl/
                        
                    
                    
                        
                            DBNL corpus
                            
                                Subset of DBNL 16th- to 19th-century texts, including literary, medical, biographical, and other texts 
                            
                        
                        
                            DBNL–National Library of the Netherlands (KB)
                            http://dbnl.org/index.php
                        
                    
                    
                        
                            Pharmacopoeias
                            
                                Collection of scanned images of apothecary data
                            
                        
                        Google Books
                    
                
                Apart from data aggregation, our time capsules should allow for information re-contextualisation. This issue is partially addressed by our semantic data integration, thus providing the researcher with different but related information sources and aspects (botanical, linguistic, historical). Another important aspect in contextualisation lies in making explicit the relation of data to space and time, thus allowing the virtual spatio-temporal ‘reconstruction’ of the knowledge transfer trajectories. For this purpose, we exploit a combination of spatio-temporal information in our data sources with spatio-temporal reasoning using OWL-Time (W3C, 2006) and SOWL (Batsakis and Petrakis, 2011), an ontology framework for representing and reasoning over spatio-temporal information in OWL. 
                In the current version of our system, our structured and free-text sources are being processed and integrated, while a demo interface is gradually developed for querying the aggregated data. Our next steps include the development of an exploratory search interface and visualisations of data overviews. A particular challenge lies in the development of a user-friendly web interface for querying our RDF data using SPARQL (W3C, 2013). Future work finally includes the application of our system to a different case study and different types of data (text and images), so as to test its portability and its functionality in a different domain.
                Notes
                1. From the early ’70s until his death in 1987, Warhol selected items from correspondence, newspapers, gifts, photographs, and other material and preserved them in sealed boxes, which he marked with a date or title. These so-called time capsules provide a unique view into Warhol’s private world, as well as an enlightening window on the interrelatedness of culture, media, politics, economics, and science in the 1970s and 1980s. 
                2. http://www.timecapsule.nu/.
            
        
        
            
                
                    Bibliography
                    
                        Batsakis, S. and Petrakis, E. G. M. (2011). SOWL: A Framework for Handling Spatio-Temporal Information in OWL 2.0. In Bassiliades, N., Governatori, G. and Paschke, A. (eds), 
                        5th International Symposium on Rules: Research Based and Industry Focused (RuleML-2011), LNCS, vol. 6826. Springer, pp. 242–49.
                    
                    
                        Bauer, C., Rosensprung, F., Lajtos, S., Boch, L., Poncin, P. and Herben-Leffring. C.
                         (2005). Deliverable D15.1MDS1: Analysis of Current Audiovisual Documentation Models, Mapping of Current Standards, http://prestospace.org/project/
                        deliverables/D15-1_Analysis_AV_documentation_models.pdf,
                    
                    
                        Cook, H. J. (2007).
                         Matters of Exchange: Commerce, Medicine, and Science in the Dutch Golden Age. Yale University Press, New Haven, CT.
                    
                    
                        Dauser, R., Hächler, S., Kempe, M., Mauelshagen, F. and Stuber, M. (2008). 
                        Wissen im Netz: Botanik und Pflanzentransfer in europäischen Korrespondenznetzen des 18. Jahrhunderts. Akademie Verlag, Berlin.
                    
                    
                        Doerr, M. (2003). The CIDOC Conceptual Reference Module: An Ontological Approach to Semantic Interoperability of Metadata. 
                        AI,
                        24(3): 75.
                    
                    
                        Dupré, S. and Lüthy C. (2011). 
                        Silent Messengers: The Circulation of Material Objects of Knowledge in the Early Modern Low Countries. LIT Verlag, Berlin.
                    
                    
                        Foust, C. (1992). 
                        Rhubarb: The Wondrous Drug. Princeton University Press, Princeton, NJ.
                    
                    
                        Friedrich, C. and Müller-Jahncke, W.-D. (eds). (2009). 
                        Arzneimittelkarrieren: zur wechselvollen Geschichte ausgewählter Medikamente: die Vorträge der Pharmaziehistorischen Biennale in Husum vom 25–28 April 2008. Stuttgart: Wissenschaftliche Verlagsgesellschaft.
                    
                    
                        Gijswijt-Hofstra, M., Van Heteren, G. M. and Tansey, E. M. (eds). (2002). 
                        Biographies of Remedies: Drugs, Medicines and Contraceptives in Dutch and Anglo-American Healing Cultures. Clio medica 66. Rodopi, Amsterdam.
                    
                    
                        Jarcho, S. (1993). 
                        Quinine’s Predecessor: Francesco Torti and the Early History of Cinchona. Johns Hopkins University Press, Baltimore, MD.
                    
                    
                        Library of Congress. (2002). Encoded Archival Description (EAD), version 2002. Encoded Archival Description Working Group: Society of American Archivists, Network Development and MARC Standards Office, Library of Congress, http://www.loc.gov/ead/.
                    
                    
                        Library of Congress. (2010). MARC Standards. Network Development and MARC Standards Office, Library of Congress, http://www.loc.gov/marc/index.html.
                    
                    
                        Pieters, T. (2004). Historische trajecten in de farmacie: medicijnen tussen confectie en maatwerk. Inaugural lecture. Hilversum. 
                    
                    
                        Reynaert, M. (2014). TICCLops: Text-Induced Corpus Clean-Up as Online Processing System. In 
                        Proceedings of COLING 2014 System Demonstrations, Dublin, Ireland, pp. 52–56.
                    
                    
                        Schiebinger, L. and Swan, C. (2005). Colonial Botany: Science, Commerce, and Politics in the Early Modern World. University of Pennsylvania Press, Philadelphia.
                    
                    
                        Snelders, S. (2012). Vrijbuiters van de heelkunde: op zoek naar medische kennis in de tropen 1600–1800. Atlas, Amsterdam.
                    
                    
                        TEI Consortium (eds). (2014). TEI P5: Guidelines for Electronic Text Encoding and Interchange. Version 2.7.0.: 16 September 2014. TEI Consortium, http://www.tei-c.org/Guidelines/P5/.
                    
                    
                        Van Gelder, E. (2012). Bloeiende kennis: groene ontdekkingen in de Gouden Eeuw. Verloren, Hilversum.
                    
                    
                        Van Gompel, M. and Reynaert, M. (2013). FoLiA: A Practical XML Format for Linguistic Annotation: A Descriptive and Comparative Study. 
                        Computational Linguistics in the Netherlands Journal,
                        3: 63–81.
                    
                    
                        Vöttiner-Pletz, P. (1990). Lignum sanctum: zur therapeutischen Verwendung des Guajak vom 16. bis zum 20. Jahrhundert. Govi-Verlag, Frankfurt am Main.
                    
                    
                        Wilson, R. (2000). Pious Traders in Medicine: A German Pharmaceutical Network in Eighteenth-Century North America. Pennsylvania State University Press, University Park.
                    
                    
                        W3C. (2006). Time Ontology in OWL. W3C Working Draft, 27 September 2006, http://www.w3.org/TR/owl-time/.
                    
                    
                        W3C. (2013). SPARQL 1.1 Overview. W3C Recommendation, 21 March 2013, http://www.w3.org/TR/2013/REC-sparql11-overview-20130321/.
                    
                
            
        
    



    
        
            
                Exploration of Billions of Words of the HathiTrust Corpus with Bookworm: HathiTrust + Bookworm Project
                
                    
                        Auvil
                        Loretta
                    
                    University of Illinois, United States of America
                    lauvil@illinois.edu
                
                
                    
                        Aiden
                        Erez Lieberman
                    
                    Baylor College of Medicine and Rice University, United States of America
                    erez@erez.com
                
                
                    
                        Downie
                        J. Stephen
                    
                    University of Illinois, United States of America
                    jdownie@illinois.edu
                
                
                    
                        Schmidt
                        Benjamin
                    
                    Northeastern University, United States of America
                    bmschmidt@gmail.com
                
                
                    
                        Bhattacharyya
                        Sayan
                    
                    University of Illinois, United States of America
                    sayan@illinois.edu
                
                
                    
                        Organisciak
                        Piotr
                    
                    University of Illinois, United States of America
                    organis2@illinois.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Poster
                
                
                    text analysis
                    visualization
                    HTRC
                
                
                    databases &amp; dbms
                    information retrieval
                    text analysis
                    content analysis
                    visualisation
                    English
                
            
        
    
    
        
            Humanities scholars are traditionally concerned with close reading of a relatively small number of texts. Yet, as new textual resources such as the Google Books project and the HathiTrust Digital Library (HTDL) emerge, there is an increasing need for tools that analyze textual resources at scale. HTDL is the largest nonprofit digital book collection in the world, containing a total of 13,026,050 volumes in over 100 languages. The goal of this project is to integrate the HTDL corpus, processed at the HathiTrust Research Center (HTRC) (Downie et al., 2012), with the Bookworm platform for text analysis, developed at the Cultural Observatory. 
             Bookworm is an open-source platform that enables real-time analysis of repositories of digitized texts. Bookworm greatly extends the type of analysis that was popularized by the Google Ngrams Viewer (Michel et al., 2011), making it possible to slice and dice the data in an arbitrary corpus, in real time, using a greatly enhanced set of content-based and metadata-based features. 
             This poster will demonstrate initial results of this project (HT+BW)—in particular, a functional Bookworm interface displaying text data from HTRC. The HT+BW will greatly increase the value of the HTRC because it will assist humanities scholars and students in their effort to delve deeper into the HathiTrust corpus and to explore more complex, multifaceted research questions. At the same time, this project will continue to develop Bookworm as an open-source platform tool, not only for HTRC, but also as a potential portal for all libraries with extensive digital content. This collaboration includes the University of Illinois, Indiana University, Rice University, Baylor College of Medicine, and Northeastern University. 
            
                Implementing Analytics at Scale
            
            One of our goals for this collaboration is to implement a greatly enhanced open-source version of the Bookworm text analysis and visualization tool designed to assist scholars to meet the challenges posed by the massive scale of the HT corpus. The enhanced Bookworm will enable many important new features—for instance, enabling scholars to better customize sets of text for their personal analyses (HTRC worksets), and to identify new HTRC texts to add to their corpora in real time. We will also be improving the APIs used by Bookworm to leverage a Solr index, an index used by many libraries and digital archives. 
            
                Identify Valuable Metadata Formats for Humanities Scholars
            
            The effort to curate and deploy metadata is essential to any digital library effort, especially given the painstaking effort of cataloging by librarians. We have identified certain metadata fields that will be useful for examination of HTRC data. For instance, many raw MARC fields (year of publication, country, language) have been added. Some fields can be easily computed: page counts and word counts. Still other fields, such as author gender, can be recovered with high reliability by analysis of author names and comparison with external data repositories. We expect that the following metadata fields will be integrated into the HathiTrust Bookworm: Class, Subclass, Fiction, Genre, Language, Issuance, Author Gender, Page Count, Word Count, Publication Country, and Publication State. Thus, by combining the HathiTrust data with Bookworm analytics, scholars of English literature can study word frequencies in English novels, regional historians can limit their search to publishers from particular places, and historians of science can compare chemistry texts to those in biology. The use of facets can serve as an easy means for testing hypotheses that could previously have been probed only with extensive research. See Figure 1 showing the usage of the term ‘freedom’ with facets of ‘Genre’ in ‘government publication’ and ‘periodical’.
            
                
            
            Figure 1. The HTRC Bookworm showing the usage of the term ‘freedom’ when faceting ‘Genre’ by ‘government publication’ and by ‘periodical’ for the Non-Google digitized Public Domain corpus from HathiTrust.
             Textual data at a massive scale shifts the landscape of possibilities for the analysis of text corpora, allowing exploration to expand from the syntactic questions that linguistic corpora have excelled at answering, to capturing subtle cultural trends that underlie changes in the usage frequency of words or phrases. The goal of the HT+BW project is to create a tool that can help scholars realize this enormous potential.
        
        
            
                
                    Bibliography
                    
                        Downie, J. S., Plale, B., Kowalczyk, S., MacDonald, R. H., Poole, M. S. and Unsworth, J. M. (2012). HathiTrust Research Center: Expanding the Frontiers of Large-Scale Text Analytics. 
                        Proceeding of 2nd Annual Conference of the Japanese Association for Digital Humanities, 15–17 September 2012, University of Tokyo.
                    
                    
                        Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Pickett, J. P., . . . and Aiden, E. L. (2011). Quantitative Analysis of Culture Using Millions of Digitized Books. 
                        Science, 
                        331(6014): 176–82. 
                        
                    
                
            
        
    



    
        
            
                Medium data method for cultural studies: the case of gender studies in Russian National Corpus.
                
                    
                        Bonch-Osmolovskaya
                        Anastasia
                    
                    National Research Unversity Higher School of Economics Moscow, Russian Federation
                    abonch@gmail.com
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    dataset corpus cultural studies culturomics
                
                
                    corpora and corpus activities
                    lexicography
                    cultural studies
                    data mining / text mining
                    English
                
            
        
    
    
        
            The main aim of the paper is to show how the data of the Russian National Corpus can be used to explore nonlinguistic objects. The National Corpus is created as a well-balanced, marked-up text collection оf almost 300 million documents of different ages (18th–21st centuries), followed with rich metadata. Being one of the best national linguistics corpora it can also be regarded as a store of cultural memory and social reflection. Then the quantitative analysis of the change of frequency of some conceptually important lexeme or qualitative analysis of its various contexts in different periods can lead us to some new analysis of cultural trends and social processes, focusing primarily on their reflection in social consciousness.
            It should be emphasized that though some lexical change can be explained by extralinguistic reasons, the study is based on texts that are not thematically connected with this reason. For example, we observe an abrupt increase of the use of lexeme ‘woman’ in the second half of the 19th century, which surely should be associated with the first wave of emancipation in Russian society. But these are not manifestos, but mostly novels that are being analysed. The frequency increase for ‘woman’ in the 1850s signals a change in readers’ interest. This change and its connection with social processes and events are the objects of investigation with the methods of corpus analysis, distant reading, and culturomics.
            The paper doesn’t claim to make some profound sociocultural research, but the main focus will be on methodology. I will introduce the medium data approach and will argue that it can be used for cultural research, and that it allows solving some of the well-known problems of big-data analyses in textual data. First I will observe the culturomics approach and the benefits of National Corpus use instead of Google Books. Second I will concentrate on one case of medium data cultural research: gender naming in the 19th and 20th centuries in Russia and the Soviet Union. I will show how context analyses can shed light on unexpected data emission, and how competition of lexemes reflects changes of social consciousness.
            Culturomics was first declared as a new scientific method in a 2010 paper in 
                Science magazine titled ‘Quantitative Analyses of Culture Using Millions of Digitized Books’ by a group of scientists headed by Jan-Baptiste Michelle. Though very influential and inspiring, the method also has been widely criticized. The two main problems of the method mentioned most often are the trivial results and the dirty, nonreliable data. The methodological problems of culturomics research seem to be the other side of its benefits: the big textual data only allows comparing the queries known in advance. We can observe the effect of censorship by the decrease in Mark Chagall mentions, but we have no instrument to compare the change of semantic context of Chagall’s mentions that accompanied the beginning of content restrictions. That means that we can notice how the political changes are reflected in culture, but we still lack the instrument to explore these processes thoroughly.
            
            In contrast to the influential trend of Big Data, I propose a concept of nedium data. Medium data is the amount of data that allows for quantitative and qualitative studies. The main characteristics of the medium data are
             • The reliability of sources, which metadata can be filtered manually.
             • The sufficiency of the data amount for reliable statistical measures.
             • The possibility of additional semantic mark-up.
            The medium data concept serves to oppose the current practices where computational methods tend to ignore the complexity of the humanitarian sphere. I argue that the quality of the research can benefit much from the contamination of statistical and computational methods, with expert manual analyses possible only with very pure, precise data of not a tremendous amount. Although the primary data filtration will in this case be a matter of the researcher’s responsibility, this situation really doesn’t differ much from any natural science case, when the researcher has to provide the specific conditions of the experiment.
            Though the Russian Natural Corpus is much smaller than the Google Books corpus, its dataset surely has some advantages. First, every document has rich metadata. This allows counting not only the edition date but also the creation date, which can be quite important for studying the Soviet period, as its published texts can demonstrate a significant lexical bias due to censorship. Second, the mistakes of object recognition are very rare in RNC. The morphological mark-up, which plays an important role in a morphologically rich language such as Russian, is diverse and multivariate in RNC, while it is limited to POS-tagging in Google n-grams. The complex morphological mark-up gives an opportunity to make distant queries, which are targeted to represent syntactic relations. For example, the query that consists of the verb ‘to steal’ plus the noun in the accusative extracts the change of consumption needs for different periods (what people steal in different times is what they actually need). Their comparison can inspire some new level of social and cultural research. The textual collection and the markup of the Russian National Corpus thus give us an example of the medium dataset that can be used not only for language investigation but also for social research. 
            How can we benefit from a medium dataset? The research can focus not only on frequency change but also on qualitative context variation. It is not the words themselves but the semantic concepts (synsets, that are different in every period) that are being studied. Medium data allow disambiguation of semantic polysemy, which is usually impossible with big datasets and sometimes can cause damaged results. The semantically close contexts can be merged into classes that enable more transparent and still reliable analyses. The most significant difference between culturomics and medium data can be formulated as follows. Culturomics research results in an overall graphic that often demonstrates quite trivial dependencies. The medium data method allows for treating the graphics not as a diagnosis but as symptom, which serves as an impulse for further research.
            Finally I will focus on one model case of the medium data method. I will compare the frequency of the words ‘man’ and ‘woman’ in the 19th and 20th centuries. The comparison of two frequency graphs shows that the word ‘woman’ is much more frequent than the word ‘man’ in the both centuries. Is it that women are more often written about? Or are men referred to with some other lexical means? If we compare the two words ‘muzhik’ and ‘baba’, which in the 19th century are used as gender terms of low class, we get the opposite picture: males are more frequent than the females. The answer can be drawn out of the context analyses. The reference of males in general is rarely direct in the 19th century, but mostly implicit together with some specific lexical means, characterizing age, social, or professional status. This contrasts much with female references, for which the gender idea is much more important than the social occupation. I will also follow the changes in male and female word usage that take place in Soviet and post-Soviet epochs. 
        
    



    
        
            
                Text + Creation + Partnership: Whatever Happened to the Best Laid Plans of EEBO-TCP?
                
                    
                        Popham
                        Michael
                    
                    University of Oxford, United Kingdom
                    michael.popham@bodleian.ox.ac.uk
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    Text
                    Partnerships
                    Open
                    Sustainability
                    Reuse
                
                
                    encoding - theory and practice
                    digital humanities - nature and significance
                    literary studies
                    project design
                    organization
                    management
                    digitisation
                    resource creation
                    and discovery
                    copyright
                    licensing
                    and Open Access
                    English
                
            
        
    
    
        
            When the Early English Books Online Text Creation Partnership (EEBO-TCP) was proposed by Mark Sandler of University of Michigan in 1999, the intention was to raise sufficient funds to produce accurate, marked-up, full-text transcriptions of 25,000 titles from ProQuest’s EEBO database of images. The main partners in the collaboration were the library at the University of Michigan, the Bodleian Library at the University of Oxford, and ProQuest—which, in return for making the images available to the EEBO-TCP partners, would receive a five-year exclusivity period to exploit the full-text data. Once that period elapsed, these 25,000 carefully produced texts would be released into the public domain for use by anyone in the global scholarly community and beyond. That was the dream. This paper will describe the practical realities and challenges that were faced to make this dream come true.
            On 1 January 2015, the 25,000 electronic texts produced by Phase I of EEBO-TCP were released into the public domain. They represented the concerted efforts of a large number of individuals employed at both Michigan and Oxford—more than 20 editors who, between them, had contributed over 100 person-years of work towards the objectives of this $9,000,000+ endeavour. In terms of scale and scope of its ambition and its outputs, the Early English Books Online Text Creation Partnership should be recognized as a seminal project in the development of the digital humanities.
            Perhaps one of the most interesting aspects of the EEBO-TCP was the mixed nature of its underlying funding model. The anticipated cost of $9M over five years was felt to be too substantial to appeal to any single foundation or funding body. Moreover, the formal collaboration with the commercial e-publisher, ProQuest, complicated matters further and, not unreasonably, raised questions about whether this could ever be a collaboration of equals. 
            In the United States, colleagues at Michigan enthusiastically promoted the benefits of individual institutions joining the EEBO-TCP—putting heavy emphasis on the benefits of partnership. Contribution levels were adjusted to allow institutions of different sizes to join the Text Creation Partnership on an equal footing; they could choose to contribute via a single lump sum (of $50,000 on average) or by five equal annual $10,000 payments. In return for their commitment to the work of EEBO-TCP, these institutions would gain immediate access to the textual resources as they were created—via both the commercial EEBO interface offered by ProQuest and also via a more tailored platform, built on the DLXS system developed at Michigan. They would also be contributing to the production of a corpus of essential texts, published in England between 1473 and 1700, which would subsequently be made available for the benefit of all. Despite increasingly constrained budgets, almost 150 US institutions paid to support the work of EEBO-TCP.
            In the United Kingdom, we were fortunate to have the Jisc (then known as the Joint Information Systems Committee), a nationally funded service dedicated to acquiring or funding the production of content, tools, and services that would be of widespread benefit to the UK’s scholarly community. The Jisc immediately saw the merits of EEBO-TCP’s innovative approach and committed a single contribution of £1,000,000 on behalf of the UK academic community. This decision also served the Jisc, as they were involved in negotiating the relicensing of ProQuest’s EEBO database to UK universities, and the added value of full-text searching for 25,000 of these important items considerably increased its appeal to library budget holders.
            The initial successes in fundraising meant that the production work of EEBO-TCP was able to begin in 2000. A keying specification was developed, and text conversion companies were invited to tender for the work. Titles were selected in light of suggestions from an international editorial board and also colleagues at the growing number of EEBO-TCP partner institutions. The digital images of the texts were sent to the chosen keying companies, and the results were subject to robust quality assurance and markup enhancement by trained teams of digital editors based at Michigan and Oxford. Texts that did not achieve the desired quality threshold were returned for rekeying, whilst those that met the standards were fed into a delivery workflow that ensured their timely appearance in ProQuest’s products, and also the TCP’s own delivery platforms. Everything was going to plan.
            But even the best-laid plans need to account for unanticipated issues and obstacles, and this paper will share the lessons learned from this major international collaborative endeavour.
            For example, whilst the overall production workflow worked extremely well—thanks to the careful oversight of key individuals at Michigan—there is no doubt that fundraising a work-in-progress raises some additional challenges; yet had we not adopted this approach, it is probably unlikely that we would ever have secured a significant majority of the necessary funding before beginning the work. In fact, having outputs that we could 
                show to potential partners as the work moved along in many cases helped secure their commitment and enabled them to clearly understand what we were aiming to do. Even so, the hard work of attempting to raise funds to achieve the target of $9,000,000, whilst spending a proportion of that money each month on text production, resulted in the work taking longer than anyone had originally envisaged. We met our production target of 25,000 texts—but it took nearly four years longer than we had planned!
            
            In the course of our work, new questions began to emerge that we had not anticipated at the outset. In 2000, nobody asked us about the employment practices and ethical standards of the keying companies selected to work on EEBO-TCP material. By 2007, some institutions that were thinking about committing to EEBO-TCP, and even end-users of the materials, wanted reassurances that the digital data had been produced in an ethically acceptable environment. Moreover, with the growing awareness of the Google Books Library programme, some users began to question the legitimacy of ProQuest’s five-year exclusivity period to the full-text data, and understandably they wanted clarity on when that five-year embargo would elapse. EEBO-TCP had agreed that ProQuest’s exclusivity period would start from the end of the year in which production was completed (originally anticipated to be 2005), but because production was necessarily extended until 2009, the texts could not be released into the public domain until 2015.
            As the work of EEBO-TCP neared its end, other questions were also raised. The first and most rewarding for us as a project, was the request to carry on doing what we were doing: to produce 
                more texts. Whilst this was a clear demonstration that many people valued the work of EEBO-TCP, it also raised new questions about retaining and redefining our production methods and workflows, whether we could continue with the same funding model, how to select further texts, and so forth. It was tremendously rewarding to have the Jisc commit an additional £1,000,000 to EEBO-TCP ‘Phase II’ without hesitation—but this was 2008–2009, and we have undoubtedly been directly affected by the consequences of the global recession ever since.
            
            Perhaps some of the biggest questions about the 25,000 texts produced by EEBO-TCP (‘Phase I’, as it is now known) are around what we meant—and what we now understand by—the term ‘public domain’. Back in 1999 we blithely assumed that we would simply release this corpus of material into the intellectual wilds, and that ‘the community’ would assume responsibility for their ongoing maintenance and enhancement. Nowadays, we are constantly asked to consider the sustainability of digital resources—and to define what this might mean, who will do the work, and most importantly, how it will be resourced? 
            At the time of writing this abstract, the texts from EEBO-TCP have not yet been released. That will not happen until 1 January 2015. Several leading individuals and groups from around the globe have already expressed an interest in working with some or all of the corpus; for example, they have put forward ideas for how the materials can be enhanced with additional markup, or corrections crowdsourced from a community of volunteers, or their contents integrated into scholarly editions. But we do not know which, if any, of these things will happen—or if the texts will be taken up and used in wholly unexpected ways by communities with which we have yet to engage. However, by the time of the DH conference in 2015, we will be in a position to reflect on both the 15-year build-up to the release of one of the most important collections of digital texts yet to be created, and to summarize what has happened in the six months since their release. Will they have been taken up and used in new and exciting ways, been picked up by just a few people, or been resolutely ignored? 
            Whatever the impact of the release of the 25,000 EEBO-TCP Phase I texts on 1 January 2015, there will certainly be important lessons to be learned by the global digital humanities community. 
        
    



    
        
            
                Traces of Lives in Digital Archives: Life Writing, Marginalia and Google Books
                
                    
                        Barnett
                        Tully
                    
                    Flinders University, Australia
                    tully.barnett@flinders.edu.au
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    book digitisation
                    digital reading
                    marginalia
                
                
                    archives
                    repositories
                    sustainability and preservation
                    literary studies
                    digitisation - theory and practice
                    English
                
            
        
    
    
        
            The first decade of the ambitious Google Books project has produced a great deal of controversy as well as a valuable resource. The project has been consistently challenged by authors’ and publishers’ groups mounting legal action. The rhetoric that Google uses to justify the project frequently concerns fair use, access, marginal voices, a so-called global library, and the role of access to information in supporting a strong democracy. 
            In the process of digitization, the books are transformed considerably. The act of reading texts on Google Books is complicated by the many figures crowding the text and the act itself. In addition to the author/s of the work in question, there may be, scattered throughout the text, traces of a person who has gifted the book, of the borrowers whose trace is marked by library loan stamps, and there are the multiple readers who have left physical marks of their reading upon the page. This marginalia can be found in the form of underlines in the text and marks in the margins—from simple ‘X’s, to detailed comments of varying degrees of legibility, to questions asked of the text and doodles drawn on the page. Also present in the text, on occasion, and complicating reception, is the ghostly figure of the book scanner whose labour is made visible by spectral fingers caught in the scan and numerous other kinds of scanning errors that, given the haste of the project, make it through quality control and into the online space.
            
                
            
            But the Google Books reader does not leave a trace. I, reading the text on the screen, make no marks upon the text to leave for others. Although my digital trace may be recorded in the IP number, I cannot leave a trace of my reading for the next reader to examine or build upon. I may transcribe the text. I may now download (selected) books to read on a plethora of devices. I may use the clip tool to take or even share a passage with my social network. But I cannot alter the book as it appears in the archive. While I can annotate—publically or privately—a Kindle book, leaving traces in the form of underlines signifying highlights and notes, I cannot annotate a Google book. The object is frozen for all time in its scanned state (or states, since frequently scans exist from a number of different editions or copies of a book). 
            The presence of the reader on the page of the printed book has been a marginal concern for the field of book history for scholars from manuscript culture and before to the recent obsession with David Foster Wallace’s cacophonous presence on the pages of his library, now acquired by the Harry Ransom Centre at the University of Texas. Marginalia tell us something about the reader’s response to the work, and it tends to be the marginal notes of literary giants that receives attention from scholars (Nabokov, Kerouac, Wilde).
            The presence of life narrative voices in the archive, and the material conditions they reflect, has significant ramifications for how we see the texts. How then do we approach the case of a published family history that has been housed for over a century in a public library and in that time has been heavily annotated with additions, comments, responses, notes, and interjections? To pursue this train of thought, I want to use as a case study a life narrative texts in the Google Books archive. 
                Memoir of the Farrar Family was published in 1847 by Timothy Farrar (1788–1874). The book exists in multiple online formats. It is available as full text from Archive.org as well as from Google Books. It is available for purchase on Amazon, with a caveat that the work comes from a scan and may have marks and imperfections. The book is referred to on many genealogical websites, which tells us something about the sort of text that it is. The inside cover says, ‘A Discourse occasioned by the centennial anniversary of Hon. Timothy Farrar, LL. Delivered at Hollis, N.H. July 11 1847 by Timothy Farrar Clary. Printed by Request. Andover: Printed by William H. Wardwell. 1847’.
            
            Google’s bibliographic information for the book tells us that it was originally digitized on 18 September 2007 from a text located at the University of Wisconsin–Madison. What makes this case so curious is that the book as it exists in the Google Books archive is riddled with the markings of a later reader. The handwriting is persistent throughout the pages of the short work, and the marginalia take the form not of active reading, of someone underlining the work as they go, but of a reader supplementing and correcting the text’s facts.
            This paper considers the implications of Google Books on the life writing it digitizes and on the notion of the library/archive and the interplay of voices in the digitization not only of a polished and published family history but also of the voices added to the text in less formal ways. The paper uses frameworks from the history of the book discourse, marginalia studies, and new media studies to consider the residual marks of other readers in ephemera and marginalia on the page and uses as case studies two works of life writing from the 19th century to examine the implications of autobiographical voices unbound by Google Books and the intellectual labour/vandalism represented by the works’ many readers. I apply Alexander Galloway’s view of the interface as ‘a generative friction between different formats’ (2012, 31) and look to media archaeology and Lisa Gitelman’s work on the history of documents for a means of understanding the layers at work in reading an annotated digitized work of life writing, a genre in which the traces of readers and digitization procedures are more keenly felt. 
            
                
            
        
        
            
                
                    Bibliography
                    
                        Borgman, C. L. (2000). From Gutenberg to the Global Information Infrastructure: Access to Information in the Networked World. MIT Press, Cambridge, MA.
                    
                    
                        Brockmeier, J. (2010). After the Archive: Remapping Memory. Culture &amp; Psychology, 
                        16(1) (March): 5–35.
                    
                    
                        Darnton, R. (2011). Six Reasons Why Google Books Failed. New York Review of Books, 28 March, http://www.nybooks.com/blogs/nyrblog/2011/mar/28/six-reasons-google-books-failed/.
                    
                    
                        Esposito, J. (2011). The Terrible Price of Free: On E-Reading Jane Austen via Google’s E-books. Scholarly Kitchen, 14 March, http://scholarlykitchen.sspnet.org/2011/03/14/the-terrible-price-of-free-on-e-reading-jane-austen-via-googles-e-books/.
                    
                    
                        Galloway, A. R. (2012). The Interface Effect. Polity, Cambridge. 
                    
                    
                        Gitelman, L. 2014. Paper Knowledge: Toward a Media History of Documents. Duke University Press, Durham, NC.
                    
                    
                        Jackson, H. (2001). Marginalia: Readers Writing in Books. Yale University Press, New Haven, CT.
                    
                    
                        Kirschenbaum, M. (2013) Bookscapes: Books in Electronic Space, http://mkirschenbaum.files.wordpress.com/2013/01/bookscapes.pdf.
                    
                    
                        Lessig, L. (2010). For the Love of Culture. New Republic, 26 January, http://www.newrepublic.com/article/the-love-culture.
                    
                    
                        Nunberg, G. (2009). Google’s Book Search: A Disaster for Scholars. Chronicle Review, http://chronicle.com/article/Googles-Book-Search-A/48245/.
                    
                    
                        Pressman, J. (2013). The Literary and/as the Digital Humanities. Digital Humanities Quarterly, 
                        7(1).
                    
                    
                        Sheringham, M. (2005). Memory and the Archive in Contemporary Life-Writing. French Studies, 
                        59(1): 47–53.
                    
                    
                        Smith, S. and Watson, J. (2010). Reading Autobiography: A Guide for Interpreting Life Narratives. 2nd ed. University of Minnesota Press, Minneapolis.
                    
                    
                        Swanstrom, L. (2011). From Gutenberg to Google: Electronic Representations of Literary Texts; Digitize This Book! The Politics of New Media, or Why We Need Open Access Now; Poetic Acts and New Media. American Literature, 
                        83(2): 466–68.
                    
                    
                        Wilson, K. 2015. The Art of Google Books. http://theartofgooglebooks.tumblr.com/.
                    
                
            
        
    



    
        
            
                Visualizing Japanese Language Change During the Past Century
                
                    
                        Hodošček
                        Bor
                    
                    Osaka University, Japan
                    bor@lang.osaka-u.ac.jp
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Poster
                
                
                    diachronic corpus
                    Japanese language change
                    genre
                    register
                    cooccurrence networks
                
                
                    corpora and corpus activities
                    metadata
                    stylistics and stylometry
                    linguistics
                    genre-specific studies: prose
                    poetry
                    drama
                    networks
                    relationships
                    graphs
                    data mining / text mining
                    English
                
            
        
    
    
        
            This study introduces an online system for the visualization and analysis of over a century (1874–2008) of Japanese language change. A comprehensive account of register variation in contemporary Japanese has recently become possible with the public availability of the Balanced Corpus of Contemporary Written Japanese (BCCWJ), a 100-million-word corpus that contains a wide variety of written Japanese collected and curated by the National Institute for Japanese Language and Linguistics. Increasingly, too, public releases of new corpora that record various genres of modern (Meiji-era) Japanese writing have paved the way to enabling more comprehensive diachronic analysis (analysis of language development and evolution through time) of Japanese. Still, especially compared to recent efforts in English, which include large book digitalization projects such as the Google Books corpus (Michel et al., 2011), as well as more curated historical corpora such as the Corpus of Historical American English (COHA) or the register-balanced Corpus of Contemporary American English (COCA) (Davies, 2010; 2011), the available resources and research tools for investigating diachronic language change as well as register variation in Japanese lack in two respects: balanced representation of registers throughout time, and unified and sophisticated search interfaces.
            We combine the use of corpus metadata and annotations with textual features to model language change through time and between different registers from the following six corpora:
            • The Balanced Corpus of Contemporary Written Japanese (c. 1975–2008).
            • The Sun corpus (c. 1895–1925).
            • The Meiroku Zasshi corpus (c. 1874–1875).
            • The Kindai Josei Zasshi corpus (c. 1894–1925).
            • The Kokumin no Tomo corpus (c. 1887–1888).
            • A subset of the Aozora Bunko (c. 1890s–).
            All text is first converted into a unified structured format that includes structural information (paragraphs, headings, titles, lists, etc.) as well as other information (spoken text, quotations, etc.), where available, from the different textual or XML encodings of the corpora. Next, we process sentences into morpheme tokens using the morphological analyzer MeCab and, depending on the time period, the modern or contemporary version of the UniDic morphological dictionary. A unique property of both variants of UniDic is their organization of word tokens under lemma that cover the many orthographic variants observed in Japanese writing. Taking the basic lemma, word orthography, and POS triplets as a base, we construct co-occurrence networks between all words occurring in the same sentence or paragraph. This co-occurrence network is constructed so that we are able to generate sub-networks that match some metadata query, such as year and NDC code, which can then be used to compare with other sub-networks. The query and visualization interface thus provides a timeline for choosing specific time-related subsets from the corpora, as well as a visual way of selecting from other metadata, including categorical (gender, media type, etc.) and hierarchical (NDC, topic, etc.) information, which allows the user to further constrain the scope of investigation into language change to some register within a chosen time period or to instead focus on the differences between registers by comparing between two or more different registers within a set time period.
        
        
            
                
                    Bibliography
                    
                        Davies, M. (2010). The Corpus of Historical American English: 400 Million Words, 1810–2009. http://corpus.byu.edu/coha/ (accessed 1 November 2014).
                    
                    
                        Davies, M. (2011). Google Books (American English) Corpus (155 Billion Words, 1810–2009). http://googlebooks.byu.edu/ (accessed 1 November 2014).
                    
                    
                        Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Pickett, J. P., . . . Orwant, J., et al. (2011). Quantitative Analysis of Culture Using Millions of Digitized Books. 
                        Science,
                        331(6014): 176–82.
                    
                
            
        
    


        
            
                Grimm's 
                    Kinder- und Hausmärchen: Intratextuality and Intertextuality
                
                Described as 'a great monument to European literature' (David and David, 1964; 180), Jacob and Wilhelm Grimm’s masterpiece 
                    Kinder- und Hausmärchen (hereafter KHM) has captured adult and child imagination for 200 years. International cinema, literature and folklore have borrowed and adapted the Brothers’ fairy tales in multifarious ways, inspiring themes and characters in numerous cultures and languages. While commonly and erroneously considered the fathers of the genre, the fairy tales were not original to the Brothers. In fact, Jacob and Wilhelm collected and adapted their stories from earlier works, some of them dating back to as far as the seventh century BC, and made numerous changes to their own collection (David and David, 1964, p. 183), producing seven distinct editions between 1812 and 1857. In these four decades of writing and rewriting, the fairy tales changed in number, style and content in accordance with historical, social and literary influences. And yet, how did forty years of revisions not confuse the tradition and transmission amongst followers and readers? Indeed, some fairy tales were changed almost beyond recognition. What makes them so timeless and memorable? What is it that immortalises these tales? An answer to this question can be found in the 
                    motifs the Brothers borrowed from earlier traditions and disseminated by way of their famous collection.
                
                Motifs, defined by Prince's 
                    Dictionary of Narratology (2003) as '[...] minimal thematic unit[s]', pervade the Grimm collection and are stable elements interlacing the seven editions of the KHM. The puss with the boots (
                    Der gestiefelte Kater) and the concept of the breadcrumb trail (originating in the 
                    Hänsel und Gretel tale) are both examples of motifs, and they recur not only throughout the Grimm editions, but also over time and space. The occurrence and repetition of motifs within the Grimm collection is a form of 
                    intratextuality, a term used to describe the internal relations within a text or an author and, in our case, within the KHM editions. But a motif may also appear in other authors across traditions and languages, thus creating 
                    intertextual relations, relations that the KHM may have with other texts. 
                
            
            
                Related work and computational opportunities
                The breadcrumb trail generated by these motifs in literary history, and internationally spread through the Brothers Grimm, has been extensively studied by folklorists, historians and literary critics. Akin to memes, motifs are a form of information transfer and reuse, which opens up numerous opportunities for computational research in cultural evolution and transmission. Interestingly, however, the study of motifs has not yet fully explored the affordances of digital methods. Many authoritative volumes and ontologies have been published in print, such as the well-known 
                    Enzyklopädie des Märchens, or the 
                    Estonian Folktales and the 
                    Catalogue of Portuguese Folktales, but only a few digital projects or digital editions of these print sources exist. One such digital initiative is the 
                    Aarne-Thompson's Motif-Index
                    
                        
                            The Aarne-Thompson Motif-Index can be accessed at: http://www.ruthenia.ru/folklore/thompson/ (accessed 18 October 2015).
                        
                    , a crucial contribution to the field, often used as a reference system for the production of folktale catalogues. 
                
                The situation, however, is different for fairy tales, inasmuch as digital copies of many folktale collections are freely available from Google Books or the Internet Archive,
                    
                         For example, the 1550-1553 Venetian collection 
                            Le piacevoli notti by Giovanni Francesco Straparola, at: https://goo.gl/fAA0J6 (accessed 21 October 2015).
                        
                     or from online collections, such as the Nederlandse VolksverhalenBank initiative
                    
                         Available at: http://www.verhalenbank.nl (accessed 1 January 2016).
                     or the Satorbase project
                    
                         Available at: http://satorbase.org/ (accessed 1 January 2016).
                    , fostering intertextual research never before possible.
                    
                         The increasing availability of digital and digitised assets allows us to access information more easily and to potentially uncover previously unknown or unchartered territory.
                     Indeed, we can now leverage hyperlinks and APIs in order to automatically retrieve specific and previously inaccessible information across the web, and to connect existing resources for comparative studies. Moreover, no effort has yet addressed the cross-cultural relations of fairy tales, giving way to opportunities for interdisciplinary, multilingual and big data research.
                
            
            
                Our project
                The new project
                    
                         Starting in October 2015 and running until December 2018.
                     described in this paper is one such opportunity, whereby an international and interdisciplinary team of computer scientists and humanists is semi-automatically crawling digitised texts and the web to produce a multilingual motif index that uses the Grimm collection as its base reference.
                    
                         The team does not include but consults folklorists. We start with the Grimm collection as we already have clean data to work from.
                     More specifically, we combine knowledge acquired from existing print and digital resources with the deployment of the Google Search
                    
                         Available at: https://developers.google.com/custom-search/ (accessed 26 October 2015).
                     and Google Books APIs
                    
                         Available at: https://developers.google.com/books/?hl=en (accessed 26 October 2015).
                     in order to automatically retrieve as many motifs across the web in as many languages as possible, and hence to explore the intratextual and intertextual relations that characterise the motifs' hosting texts. The end goal is twofold; on the one hand, we provide a comprehensive reference resource for scholars in the field and interested citizens alike and concurrently revise the Aarne-Thompson Index; on the other, by testing state-of-the-art text reuse and retrieval algorithms on a sample of these diverse and large datasets, we are able to refine our methods in order to accommodate further web-scale queries and thus sharpen our understanding of why and how motifs changed. 
                
            
            
                Methodology
                The case studies we are working with to address our research questions are three Brothers Grimm tales: 
                    Snow White, 
                    Puss in Boots and 
                    The Fisherman and his Wife. These were chosen on the basis of their differing degree of popularity in order to better understand how transmission affects popularisation. 
                
                Our research starts with digital and clean copies of the Grimm texts, downloaded and catalogued from TextGrid
                    
                         Available at: https://textgridrep.org/browse/-/browse/nxvg_0 (accessed 26 October 2015).
                     and Wikisource
                    
                         Available at: https://de.wikisource.org/wiki/Kinder-_und_Haus-M%C3%A4rchen_Band_1_%281819%29 (accessed 26 October 2015).
                    . Next, our international team of researchers and student assistants collects digitally available translations and/or editions of the three tales in multiple languages
                    
                         eTRAP is currently a team of twelve people from seven nationalities speaking eleven different languages.
                     and manually enters them into a database, where information about the web source, the tales, the language, the work and the author is stored.
                    
                         An example may be of use in clarifying the point. Grimm's 
                            Snow White corresponds to Pushkin's Сказка о Мертвой Царевне и о Семи Богатырях (
                            The Tale of the Dead Princess and the Seven Knights in English). The two tales differ in many points, including the title of the tale. In Pushkin the princess is protected by seven 
                            knights (семь богатырей) whereas in the Grimm tale it is seven 
                            dwarves. Despite the differences, the motifs of the beautiful princess and of her seven protectors link the two stories. To hyperlink and map these versions and their differences, we use a combination of Thompson identifiers for tales, VIAF identifiers for authors and works, and customised identifiers where existing ones do not apply. This semi-automatic approach allows us to populate our database with both content and metadata, and establish relations between the different versions.
                        
                     Once this manually-compiled dataset is complete, we deploy the TRACER suite of text reuse algorithms (Büchler, 2013) to trace additional motifs in other digital libraries or corpora. At the same time, we use the Google Search and the Google Books APIs to search for motifs at a much larger scale, effectively crawling the web. 
                
            
            
                Impact
                Like the KHM, we believe this project appeals to a wide and diverse audience not only because of its subject matter, but also because of its international and interdisciplinary character. Our international group operates at the intersection of Computer Science and the Humanities in the arena that is Digital Humanities. This project is unique insofar as each and every member of the team can contribute a piece of his or her own culture, adding a personal and familiar touch to this joint endeavour. By exploring these different cultures, we aim to establish fruitful collaborations and, in so doing, broaden the boundaries of the Digital Humanities. 
                Furthermore, we believe that this project fully engages humanists in the digital process of tracing texts through space and time. Following the motif trail back in time allows humanists to investigate lines of transmission of folktales and to potentially uncover additional trails through which other documents or stories travelled. At the same time, it enables the computer scientists in the team to identify any shortcomings in our algorithms and to better understand what to automatically feature when tracing this type of information in a digital ecosystem.
            
        
        
            
                
                    Bibliography
                    Büchler, M. (2013). 
                        Informationstechnische Aspekte des Historical Text Re-use. Ph.D thesis, University of Leipzig.
                    
                    David, A. and David, M. E.  (1964). A Literary Approach to the Brothers Grimm. 
                        Journal of the Folklore Institute, 1(3): 180-96. http://www.jstor.org/stable/3813902 (accessed 26 July 2015).
                    
                    Prince, G. (2003). 
                        Dictionary of Narratology. Revised Edition. University of Nebraska Press, Lincoln and London.
                    
                
            
        
    


        
            
                Chronological corpus – definition 
                The notion of chronological corpus has appeared so far only in publications of limited circulation (cf. Pawłowski, 2006), whereas it is practically absent from mainstream literature on corpus linguistics and digital humanities. This is somewhat surprising given that the research of so called “lexical series” has been conducted in the past (cf. Brunet, 1981; Salem, 1987) and that Google has been for some years developing the tools 
                    Ngram Viewer and 
                    Google Trends, which allow sequential analysis of Google Books resources and of users' Internet queries (cf. Michel et al. 2011). 
                
                In order to understand what chronological corpus is one must grasp the basic distinction between synchrony and diachrony at the heart of structural linguistics. Synchrony is a study of language at a certain moment in time, where "moment" can last even several decades, provided there are no apparent changes in grammar, vocabulary and pronunciation. As for diachrony, it exposes evolution of language taking place in the process of its development, usually over long periods, spanning even several centuries. However, corpus research supported by NLP tools enables a far more flexible approach to the time variable, since text samples consistent in terms of their typography, spelling and grammar can be annotated with their exact publication dates. Scientific description then focusses on the dynamics of the frequency change of specific lexemes (or other segments) in time, rather than on the evolution of word forms beginning with a hypothetical proto–language until the present state. 
                An important issue to be resolved is that of naming such an approach, whereby the chosen term would instate it in the domain of corpus linguistics. While social sciences and psychology use the term of "longitudinal" research, its equivalent in linguistics could be ‘microdiachrony’. Having said that, the term’s association with traditional diachrony and history of language could prove misleading. This is why it is more advisable to speak of 
                    chronological analysis and 
                    chronological corpora, where, similarly to diachrony, both terms contain a reference to time – gr. 
                    chronos.
                
                Consequently, 
                    chronological corpus should be understood as a sequence of text samples consistent in terms of their spelling and grammar, corresponding to subsequent points on the axis of time (e.g. weeks, months etc.). Such a corpus would make it possible to explore the dynamics of the change of lexeme frequencies in successive periods using the method of time series analysis (cf. Gottman, 1981; Cryer, 1981; Pawłowski, 1998, 2001). A chronological corpus would differ from a diachronic one in that in the former texts are evenly spread in time and word forms remain unchanged, while in the latter it is the opposite: word forms must evolve to become object of interest and time spans between measurements may be of any length. It needs to be emphasised that the annotation of the time variable may be ignored (if such is the premise of the conducted research); a chronological corpus would then be treated as a synchronic one.
                
            
            
                Chronological corpora 
                There exist corpora that are intrinsically arranged with regard to the time variable, e.g. literary outputs of some authors presented as Shakespearean Corpus, Corpus Thomisticum or Corpus Platonicum. If the date of creation of every piece of data in the corpus is known, the development of the text's stylostatistical properties over time can be established. If text chronology remains partly unknown, stylometric research can help to put undated works in the correct order (cf. Lutosławski, 1897). 
                But there is also another set of text corpora that seem to be naturally predestined to sequential analysis. Contrary to other genres or functional styles, 
                    press and media texts necessarily incorporate the date or even the time of their creation. This is of course little surprising, since media must comment upon actual events. Publication dates in the headers or footers of printed newspapers may be thus regarded as “time markup” that is invaluable as a Gutenberg era contribution to the sequential corpus research in the digital universe. 
                
            
            
                ChronoPress – features and origin 
                At present ChronoPress corpus contains texts representing Polish press of the post-war totalitarian period (1945-1954). Future extensions are planned to cover the entire period of the country's sovereignty (1918–2018). As the flow of press information was abundant even in the early post-war period, representative method of sampling has been applied. It has also been assumed that the minimum time spans, visible from the user level, are limited to subsequent months (the volume of text corresponding to weeks was too small to guarantee a sufficient level of representativeness). Apart from that it should be pointed out that the pace of public discourse in the times of printed press was slower than it has been in the digital world, so that months as time units can be regarded as sufficient to trace relevant events or processes. 
                In order to obtain reliable values of lexeme frequencies the number of words per month has been established as ca 120.000-140.000. Since sample size has been fixed to ca 250-300 words of continuous text, every month is represented by 480 samples and every year by 5760 samples (ca 1,5 million words per year). Equal numbers of words per month are important to guarantee that time series and other parameters generated from ChronoPress remain statistically unbiased with regard to the sample size. Selected newspapers represent a relatively broad spectrum of public post-war discourse of the totalitarian state. Particular titles to be included in the corpus have been selected according to two criteria: newspaper circulation (only titles with the highest values and national range have been considered) and target groups, as defined by the ideology of the time (first of all the working class, peasantry, army, youth, and, to a lesser extent, "intelligentsia"). The corpus is freely available through an interactive user interface. 
                Data have been thoroughly sampled from Polish dailies and weeklies. Preparatory stages of analysis consist of: sample selection, scanning, OCR, markup with metainformation (XML scheme) and text curation. The corpus has been annotated morphosyntactically using Morfeusz tool which had been designed for the Polish National Corpus. Metadata (Fig.1) include newspapers’ titles, articles’ titles, authors’ names, exposition, and data support. 
                Fig. 1 Example of sample annotation with metadata 
                
                    
                
                ChronoPress offers a variety of online linguistic and exploratory tools, such as concordances, frequency lists, word profiles and word maps. Its specific feature is, however, the possibility to generate time series. The analysis of lexeme frequency values displayed on an axis of time allows users to discover and explore the dynamics of events and phenomena represented in daily press over long periods of time. 
                Fig. 2 Moving average of the frequency of the lexeme 
                    machine (maszyna). It 
                    supports the view that despite the systemic inefficiency of communist economy technological progress was promoted in post-war Poland.
                
                
                    
                
            
            
                Goals of the presentation 
                ChronoPress allows to conduct a wide range of research in linguistics, contemporary history, cultural anthropology, media studies and communication science. The perspective adopted here is oriented towards the analysis of dynamic processes occurring in time rather than towards static states of language reflecting extralinguistic reality. The goal of the presentation is to provide an overview of the available exploratory techniques for chronological corpora, highlighting their explanatory power and limitations. In particular the following issues will be addressed and exemplified with empirical evidence from the ChronoPress corpus: 
                
                    Manual and automated trend detection, modelling and interpretation 
                    Long-term change in frequency of selected lexemes will be analysed as the expression of dominating topics of interest in public discourse, but also of social change imperceptible from the point of view on an average. Illustration material will include salient and informative examples generated from the ChronoPress database (
                        inter alia lexemes from the semantic fields 
                        war, 
                        health, 
                        technology). Since trend analysis requires some conceptualisation of time, various approaches to time will be presented and discussed (linear or circular, anthropological, political and astronomical).
                    
                
                
                    Detection of events 
                    Events are understood here as sudden changes, such as natural or technological catastrophes, wars, accidents etc. They manifest themselves as sudden rises in the lexeme frequency. This task will be illustrated with some examples generated from the ChronoPress corpus and from other online sources. 
                
                
                    Testing the efficiency of statistical tools 
                    The utility of the autocorrelation function (for univariate series) and of the cross-correlation function (for multivariate series) in the analysis of chronological corpora will be verified. 
                
                
                    Comparison of ChronoPress and online resources 
                    Informational potential and functionalities of the ChronoPress corpus will be compared with other online research tools, such as Google Ngram Viewer, Hansard Corpus and Polish National Corpus. 
                
                
                    Word profiles analyses 
                    Collocations and semantic profiles of selected lexemes in subsequent years will be generated (using Log Likelihood Ratio) and compared. 
                
                
                    Automatic detection and annotation of named entities 
                    Proper names and other named entities in Polish can be detected and annotated using Liner 2. Spatial distribution of toponyms identified by Liner 2 will be automatically displayed on a map and further analysed. Both tools have been created in the framework of the Clarin-PL project (http://clarin-pl.eu/en/home-page/).
                
            
        
        
            
                
                    Bibliography
                    Brunet, E. (1981). 
                        Le vocabulaire français. De 1789 à nos jours. Paris–Genève: Slatkine, Champion. 
                    
                    Cryer, J. (1986). 
                        Time series analysis. Boston: Duxbury Press. 
                    
                    Gottman, J. M. (1981). Time-series analysis: a comprehensive introduction for social scientists. Cambridge, London etc.: Cambridge University Press. 
                    Lutosławski, W. (1897). 
                        The origin and growth of Plato’s logic. London, New York, Bombay: Longmans, Green and Co. 
                    
                    Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., The Google Books Team, Pickett, J. P., Hoiberg, D., Clancy, D., Norvig, P., Orwant, J., Pinker, S., Nowak, M.A., Aiden, E. L. (2011). Quantitative Analysis of Culture Using Millions of Digitized Books. 
                        Science 14 (2011), Vol. 331: 176–182. 
                    
                    Pawłowski, A. (1998). 
                        Séries temporelles en linguistique. Avec application à l'attribution de textes: Romain Gary et Émile Ajar. Paris, Genève: Champion-Slatkine. 
                    
                    Pawłowski, A. (2001). 
                        Metody kwantytatywne w sekwencyjnej analizie tekstu [Quantitative methods in sequential text analysis]. Warszawa: Uniwersytet Warszawski, Katedra Lingwistyki Formalnej. 
                    
                    Pawłowski, A. (2006). Chronological analysis of textual data from the Wrocław Corpus of Polish. 
                        Poznań Studies in Contemporary Linguistics (PSiCL) 41, 2006: 9-29. 
                    
                    Salem, A. (1987). 
                        Pratique des segments répétés. Essai de statistique textuelle. Paris: Klincksieck.
                    
                
            
        
    


        
            
                Introduction
                The dynamics of language change over time are most evident in the lexicon component of natural languages. In particular, the gradual semantic changes words may undergo have a strong effect on the comprehension of historical texts by modern readers. Yet, efforts to automatically detect and trace this lexical evolution are scarce. Our study follows the work of Kim et al. (2014) who detected lexico-semantic changes in English texts over the 20
                    th century via a series of neural network language models. Our models were trained on the German part of the 
                    Google Books Ngram
                    
                         An 
                            n-gram is a sequence of 
                            n words plus information on their frequency/probability of occurrence for a given corpus. The available version of the corpus does not consist of running text, but of 
                            n-grams instead. 
                        
                     corpus (Michel, et al., 2011; Lin et al., 2012), which covers over 657k German books. Such models have the particular advantage that they can be queried for the semantic similarity of arbitrary words. We tested this query option by sampling nouns from 
                    Des Knaben Wunderhorn (Arnim and Brentano, 1806-1808), a collection of German folk poems and songs from the German Romantic period. The choice of this volume is merely motivated by our interest in the literary period it belongs to. We detected interesting semantic changes between 1798 (often taken as the starting point for the German Romantic period) and 2009 (last year in the Google corpus). 
                
            
            
                Methods
                Using the specific contexts in which words appear in order to determine the (distributional) meaning of words is an old idea from linguistic structuralism (Firth, 1957). For a long time, this appealing approach could not have been seriously investigated due to the lack of suitably large corpora and adequate computational power to deal with distributional patterns of words on a larger scale. Thus, only few studies on automatically detecting semantic change have been conducted up until now, with a clear focus on the high-volume data provided by Google Books. This collection is widely popular due to its immediate availability and enormous coverage despite well-known problems stemming from both the quality of optical character recognition (OCR) and the sampling strategies used to compile it (Pechenick 
                    et al., 2015).
                    
                         The 
                            Deutsches Textarchiv (DTA) can be considered as a counter example, at least, as far as the quality of OCR is concerned. Yet, DTA suffers from tremendous size limitations in comparison with the (German portion of the) Google corpus, since this corpus for historic German texts contains only about 2.4k texts (
                            http://www.deutschestextarchiv.de/list).
                        
                    
                
                Early approaches towards modeling lexico-semantic change patterns used frequency and bi-gram co-occurrence data (Gulordava and Baroni, 2011), as well as (context-based) classifiers (Mihalcea and Nastase, 2012). Riedl et al. (2014) built distributional thesauri to cluster similar word senses. All of these approaches detected lexico-semantic changes between multiple pre-determined periods. In contrast, neural network language models can be used to detect changes between arbitrary points in time, thus offering a longitudinal perspective (Kim et al., 2014; Kulkarni et al., 2015). In our experiments, we use a skip-gram model, a simplified neural network that is trained to predict plausible contexts for a given word, thereby generating (computationally less expensive) low-dimensional vector space representations of a lexicon (Mikolov et al., 2013). Despite their simplicity, neural network language models are a state-of-the-art approach, with details concerning ideal implementation solutions and training scenarios still being under dispute (Baroni et al., 2014; Schnabel et al., 2015).
                
            
            
                Experiment
                We trained our models on 5-grams spanning the years 1748 to 2009, using a uniform sampling size of 1M 5-grams per year; the first 50 years were used for initialization only. Test words for high-lighting semantic change patterns were selected from 
                    Des Knaben Wunderhorn by identifying the ten most frequent nouns, i.e. 
                    Gott [‘god’], 
                    Herr [‘lord, mister’], 
                    Liebe [‘love’], 
                    Tag [‘day’], 
                    Frau [‘woman, miss’], 
                    Mutter [‘mother’], 
                    Herz [‘heart’], 
                    Wein [‘wine’], 
                    Nacht [‘night’] and 
                    Mann [‘man’]. For each of these ten nouns we selected the three words most similar to them (according to the cosine of their respective vector representations) during 1799 and 1808 and between 2000 and 2009, tracking how the similarity of these words developed between 1798 and 2009. The programs used for our experiments and resulting data are publically available via GitHub.
                    
                        
                            https://github.com/hellrich/dh2016
                        
                    
                
            
            
                Results
                The cosine similarity between the 1798 and the 2009 vector representation of the ten test words is rather high, ranging from 0.72 for 
                    Mann to 0.84 for 
                    Wein, thus showing only minor semantic changes. Manual interpretation of their most similar words revealed an interesting change for 
                    Herz (see Fig. 1) that is nowadays more similar to other anatomical terms (such as 
                    Gehirn [‘brain’], 
                    Lunge [‘lung’], or 
                    Ohr [‘ear’]) and less likely to be used metaphorically (such as indicated by 
                    erschrecke [‘frighten’], or 
                    Gemüth [archaic for ‘mind’]). As this change predates Google Books’ tendency to overrepresent scientific texts (at least for English, cf. Pechenick et al., 2015) this finding can be assumed to be an example of true lexico-semantic change. The example also demonstrates a need for a metric incorporating frequency information and normalization of input, since 
                    Gemüth is an archaic form for 
                    Gemüt non-conformant with modern German spelling conventions, although it is rated as currently similar to 
                    Herz.
                
                Fig. 1 Lexical semantics of 
                    Herz [‘heart’] as expressed by its similarity with six other words; similarity-axis not depicting whole range of possible values (0–1)
                
                
                    
                
            
            
                Conclusion
                This research note has gathered preliminary evidence for the feasibility of corpus-driven studies into German diachronic semantics. We advocate a computational, neural network-based approach where the evolution of lexico-semantic changes is traced by similarities of distributional patterns in the context of words over time.
                Looking backwards for semantic changes is, however, constrained by the quality and quantity of linguistic data available. While the primary corpus we use for determining semantic evolution patterns, the Google Books Ngram corpus, is remarkably large, it suffers from a idiosyncratic sampling policy, as well as OCR shortcomings and even more advanced issues, such as the absent normalization of historic orthographic variants. Other historic corpora dealing with the latter quality issues (such as the 
                    Deutsches Textarchiv) are plagued by their comparatively minuscule size. 
                
                Future research in Digital Humanities, besides dealing with these issues, will exploit the similarity data in order to make proper use of them under a humanities’ perspective and, thus, hopefully determine the added value of such computational results. This can be achieved by incorporating complementary types of data (e.g. historical, economic ones) to render additional evidences to change patterns. Since this is a huge and complex task, we plan to make our similarity data publically available on a website, together with an easy-to-use interface, as a humanities tool for comparative, diachronic lexico-semantic studies, with several user-adjustable parameters (e.g. different grain sizes of time intervals, alternative ranking metrics, 
                    etc.). From a methodological perspective, we plan to focus our research on protocols for training models covering long timespans, a metric to measure the quality of historic language models (probably including the need for a manual evaluation) and a way to include frequency information–a word which is no longer used cannot be said to be unchanged in its semantics. Such a system would ideally be tested by an in-depth study of the semantics of carefully selected words, including a comparison with prior, hermeneutically guided work in the humanities as a rich, yet completely informal background theory.
                
            
            
                Funding
                This work was supported by the DFG-founded Research Training Group "The Romantic Model. Variation - Scope - Relevance" [grant GRK 2041/1].
            
        
        
            
                
                    Bibliography
                    
                        Arnim, A. von and Brentano, C. (1806-1808). 
                        Des Knaben Wunderhorn
                         1(3), (Annotated TCF version provided by the Deutsches Textarchiv).
                    
                    
                        Baroni, M., Dinu, G. and Kruszewski, G. (2015). Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. 
                        Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,1: 238–47.
                    
                    
                        Firth, J. R. (1957). A synopsis of linguistic theory, 1930-1955. 
                        Studies in Linguistic Analysis, pp. 1–32.
                    
                    
                        Gulordava, K. and Baroni, M. (2011). A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus
                        
                            .
                        
                        Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics @EMNLP 2011, pp. 67–71.
                    
                    
                        Kim, Y., et al. (2014). Temporal analysis of language through neural language models
                        . 
                        Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pp. 61–65.
                    
                    
                        Kulkarni, V., et al. (2015). Statistically significant detection of linguistic change. 
                        Proceedings of the 24th International Conference on World Wide Web, pp. 625–35.
                    
                    
                        Lin, Y., et al. (2012). Syntactic annotations for the Google Books Ngram Corpus
                        . 
                        Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 169–74.
                    
                    
                        Michel, J.B., et al. (2011). Quantitative analysis of culture using millions of digitized books
                        . 
                        Science, 331(6014): 176–82.
                    
                    
                        Mihalcea, R. and Nastase, V. (2012). Word epoch disambiguation: Finding how words change over time
                        . 
                        Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, 2: 259–63.
                    
                    
                        Mikolov, T., et al. (2013). Distributed representations of words and phrases and their compositionality. 
                        Advances in Neural Information Processing Systems 26 (NIPS2013), pp. 3111–119.
                    
                    
                        Pechenick, E. A., Danforth, C. M. and Dodds, P. S. (2015). Characterizing the Google Books Corpus: Strong limits to inferences of socio-cultural and linguistic evolution. 
                        PLoS ONE 10(10): e0137041.
                    
                    
                        Riedl, M., Steuer, R. and Biemann, C. (2014). Distributed distributional similarities of Google Books over the centuries
                        . 
                        Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC’14), pp. 1401–405.
                    
                    
                        Schnabel, T., et al. (2015). Evaluation methods for unsupervised word embeddings. 
                        Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP ’15), pp. 298–307.
                    
                
            
        
    


        
            
                
                Summit work of the Spanish Golden Age and forefather of the so called picaresque novel, The Life of Lazarillo de Tormes and of His Fortunes and Adversities (henceforth: the Lazarillo still remains an anonymous text (Rico, 2011). The 400 years of attributions have left us an enormous, nearly intractable, amount of bibliography that must be reviewed and studied. Paradoxically, scholars, instead of shying away from this mystery, are still adding new proposals to the pool of candidate authors, although some of them use modern and less explored methods (mostly computational) that were not available a decade ago.
            
            
                Chronologically, the Hieronymite Friar José de Sigüenza was the first to propose a possible author: Friar Juan de Ortega. Father Sigüenza’s Historia de la Orden de San Jerónimo gathers his finding of a manuscript of the Lazarillo in the cell of Juan de Ortega. Although a draft was indeed found in the Friar’s cell, the circulation of handwritten copies was a common practice during the Spanish Golden Age (Botrel and Salaün, 1974). The claim that Father Ortega was the author is hard to sustain as the draft does not seem to be enough proof. Diego Hurtado de Mendoza was proposed a couple of years later, in 1607. His candidacy as the author of the Lazarillo was proposed by Valerio Andrés Taxandro. Over the years other scholars contributed to the diffusion of Mendoza being the author, and the attribution proved to be extremely popular. For almost three centuries book catalogues all over Europe recorded Hurtado de Mendoza as the author of theLazarillo. In 2010 Mercedes Agulló provided documentary proof to support the authorship, although some dispute the validity of such evidence (Agulló y Cobo, 2011). Other humanists were also proposed. The reformist Juan de Valdés was defended by Morel-Fatio and Manuel J. Asensio, but in view of the lack of solid evidence, this candidacy was abandoned in favor of his brother Alfonso. Rosa Navarro Durán supported Alfonso de Valdés, as she found connections between the works that influenced both Valdés’ work and the Lazarillo. The lack of direct comparisons has been strongly criticized.
            
            
                In the last decade, a couple of names have taken center stage. Juan Luis Vives was proposed and devotedly defended by Francisco Calero. Following the same precepts as for Alfonso de Valdés’ candidacy, the fact that all Vives’ known works were written in Latin makes the attribution lose some credibility. In 2008, after abandoning the authorship of Francisco Cervantes de Salazar, José Luis Madrigal proposed Juan Arce de Otálora. Using existing corpora such as Google Books and CORDE, Madrigal employed basic computational analysis based on the counting of words in order to find correlations between the style of authors in the corpora and the style of the author of the Lazarillo.
            
            
                Computational approaches to authorship attribution are usually not considered to be enough proof to state the final truth in the disputed authorship of an anonymous text. However, the use of modern authorship attribution techniques might help to reduce the pool of candidates and contribute with evidence to support a specific possible author. After building the pool of candidates, we collected each candidate’s available texts. Half of them lacked modern editions, which we solved building and using our own crowdsourcing OCR reviewing tool. The corpus we created counts a total of 50 works in a 90 year period surrounding the publication of the first known edition of the Lazarillo in 1554. All the major candidates for the authorship of this book were included, as well as some authors who had not been considered previously to add robustness to our analysis. The rules we followed for regularizing the spelling of old Spanish were borrowed from Ocasar’s system (Ocasar, 2014). Features from the text were then extracted following the winners of several editions of the authorship attribution competition known as PAN at CLEF, which establishes the state-of-the-art in attribution techniques (Stamatatos et al., 2015). The final set of features were composed by several distributions: functions words, the 300 most common words (BOW), the 3000 most common character 3-grams (CNG), punctuation signs, the 30 most common parts of speech; tf-idf for a maximum of 1000 word bi- and tri-grams, and for a maximum of 1000 character {2,4}-grams; and average sentence length, sentence length variation, and sentence lexical diversity. A combined feature vector with all the abovementioned features was also included.
            
            
                Once the texts were in digital format, we explored the dataset through distance-based methods, such as Burrows’ Delta and its variations (Burrows, 2003), which outperformed any other. For compression-based methods we applied Cilibrasi and Vitanyi’s NCD with BZIP2, RAR and PPM (Cilibrasi and Vitanyi, 2005). We tested PCA and Linear Discriminant Analysis with different settings for number of chunks per work, components and features (Burrows and Hassall, 2002). Our final approach was comprised of three steps: first we used unsupervised learning to reduce the pool of candidates. Then, applying supervised learning, we ranked the possible authors. Finally, only six of these candidates were fed into an ensemble algorithm for “unmasking” the most likely author. As for unsupervised learning techniques, we obtained that Ridge, Bernoulli, multinomial, and nearest centroid had the best performance for our total feature vector, BOW, and CNG. In supervised learning the best results were provided by SVM and maximum entropy models, for the same feature sets. This allowed us to reduce the pool of candidates for the unmasking method proposed by Moshe Koppel and Jonathan Schler since it is a computationally expensive method (Koppel and Schler, 2004). The results were consistent for all methods and in line to what we first obtained applying Burrows’ Delta. We found that the most likely author seems to be Juan Arce de Otálora, closely followed by Alfonso de Valdés. Unfortunately, although supporting previous hypotheses about the authorship of the Lazarillo and providing with evidence in the case of Valdés, the method also stated that no certain attribution could be made with the given corpus.
            
        
        
            
                
                    Bibliography
                    
                        Agulló y Cobo, M.
                        (2011). A vueltas con el autor del Lazarillo. Un par de vueltas más . Lemir: Revista De Literatura Española Medieval Y Del Renacimiento, 
                        15: 217-34.
                    
                    
                        Anónimo (2011).
                        Lazarillo de Tormes.
                        In F. Rico (ed.) Madrid: Real Academia Española-Galaxia Gutenberg-Círculo de Lectores .
                    
                    
                        Asensio, M. J. (1959). La intención religiosa del Lazarillo de Tormes y Juan de Valdés. 
                        Hispanic Review, pp. 78-102, ISSN 0018-2176.
                    
                    
                        Botrel, J. and Salaün, S. (1974). Creación y público en la literatura española.  Editorial Castalia.
                    
                    
                        Burrows, J. (2003). Questions of Authorship: Attribution and Beyond.
                        Computers and the Humanities, 37(1): 5.
                    
                    
                        Burrows, J. F. and Hassall, A. J. (1988). Anna Boleyn and the authenticity of Fielding's feminine narratives.
                        Eighteenth Century Studies, pp. 427-53.
                    
                    
                        Cilibrasi, R. and Vitanyi, P. (2005). Clustering by compression. 
                        Information Theory, IEEE Transactions On, 51(4): 1523-45.
                    
                    
                        De la Concha, V. (1972). La intención religiosa del Lazarillo. 
                        Revista De Filología Española, 55(3): 243-77, ISSN 1988-8538.
                    
                    
                        Koppel, M. and Schler, J. (2004). Authorship verification as a one-class classification problem.
                        Proceedings of the 21st International Conference on Machine Learning, 2004.
                    
                    
                        Ocasar, J. L. (2014). La atribución del Lazarillo a Arce de Otálora. Una perspectiva geneticista sobre los estudios de autoría, 2014.
                    
                    
                        Stamatatos, E., Daelemans, W., Verhoeven, B., Potthast, M., Stein, B., Juola, P. and López-López, A. (2015). Overview of the Author Identification Task at PAN 2015. 
                        CLEF 2015 Labs and Workshops, Notebook Papers. CEUR Workshop Proceedings, 1391: 31, ISSN 1613-0073.
                    
                
            
        
    


        
            
                The problems at hand
                In the context of the current onslaught cultural artefacts in the Middle East face from the iconoclasts of the Islamic State, from the institutional neglect of states and elites, and from poverty and war, digital preservation efforts promise some relief as well as potential counter narratives. They might also be the only resolve for future education and rebuilding efforts once the wars in Syria, Iraq or Yemen come to an end; and while the digitisation of Archaelogical artefacts has recently received some attention from well-funded international and national organisations, particularly vulnerable collections of texts in libraries, archives, and private homes are destroyed without the world having known about their existence in the first place.
                    
                         For a good example of crowd-sourced conservation efforts targetted at the Armenian communities of the Ottoman Empire see the 
                            Houshamadyan project, which was established by Elke Hartmann and Vahé Tachjian in Berlin in 2010 and launched an “Open Digital Archive” in 2015. Other digitisation projects worth mentioning are the 
                            Yemen Manuscript Digitisation Project (University of Oregon, Princeton University, Freie Universität Berlin) and the recent “Million Image Database Project” of the 
                            Digital Archaeology Institute (UNESCO, University of Oxford, government of the UAE) that aims at delivering 5000 3D cameras to the MENA region in spring 2016.
                        
                    
                
                Early Arabic periodicals, such as Butrus al-Bustānī’s 
                    al-Jinān (Beirut, 1876–86), Yaʿqūb Ṣarrūf, Fāris Nimr, and Shāhīn Makāriyūs’ 
                    al-Muqtaṭaf (Beirut and Cairo, 1876–1952), Muḥammad Kurd ʿAlī’s 
                    al-Muqtabas (Cairo and Damascus, 1906–18/19) or Rashīd Riḍā’s 
                    al-Manār (Cairo, 1898–1941) are at the core of the Arabic renaissance (
                    al-nahḍa), Arab nationalism, and the Islamic reform movement. These better known and—at the time—widely popular journals do not face the ultimate danger of their last copy being destroyed. Yet, copies are distributed throughout libraries and institutions worldwide. This makes it almost impossible to trace discourses across journals and with the demolition and closure of libraries in the Middle East, they are increasingly accessible to the affluent Western researcher only.
                    
                         In many instances libraries hold incomplete collections and only single copies. This, for instance, has caused even scholars working on individual journals to miss the fact that the very journal they were concerned with appeared in at least two different editions (e.g. (Glaß, 2004) see (Grallert, 2013; Grallert, 2014)).
                    
                
                Digitisation seemingly offers an “easy” remedy to the problem of access and some large-scale scanning projects, such as 
                    Hathitrust,
                    
                         It must be noted that the US-based HathiTrust does not provide public or open access to its collections even to material deemed in the public domain under extremely strict US copyright laws to users outside the USA. Citing the absence of editors able to read many of the languages written in non-Latin scripts, HathiTrust tends to be extra cautious with the material of interest to us and restricts access by default to US-IPs. These restrictions can be lifted on a case-by-case basis, which requires at least an English email conversation and prevents access to the collection for many of the communities who produced these cultural artefacts; see 
                            https://www.hathitrust.org/access_use for the access policies.
                        
                     the 
                    British Library’s “Endangered Archives Programme” (EAP), 
                    MenaDoc or 
                    Institut du Monde Arabe produced digital facsimiles of numerous Arabic periodicals. But due to the state of Arabic OCR and the particular difficulties of low-quality fonts, inks, and paper employed at the turn of the twentieth century, these texts can only reliably be digitised by human transcription (c.f. Märgner and El Abed, 2012).
                    
                         For the abominable state of Arabic OCR even for well-funded corporations and projects, try searching inside Arabic works on Google Books or HathiTrust.
                     Funds for transcribing the tens to hundreds of thousands of pages of an average mundane periodical are simply not available, despite of their cultural significance and unlike for valuable manuscripts and high-brow literature. Consequently, we still have not a single digital scholarly edition of any of these journals.
                
                On the other hand, gray online-libraries of Arabic literature, namely 
                    
                        al-Maktaba al-Shāmila
                    , 
                    
                        Mishkāt
                    , 
                    
                        Ṣayd al-Fawāʾid
                     or 
                    
                        al-Waraq
                    , provide access to a vast body of, mostly classical, Arabic texts including transcriptions of unknown provenance, editorial principals, and quality for some of the mentioned periodicals. In addition, these gray “editions” lack information linking the digital representation to material originals, namely bibliographic meta-data and page breaks, which makes them almost impossible to employ for scholarly research.
                
            
            
                Our proposed solution
                With the 
                    GitHub-hosted TEI edition of 
                        Majallat al-Muqtabas
                    
                    
                         For a history of Muḥammad Kurd ʿAlī’s journal 
                            al-Muqtabas (The Digest) see (Seikaly, 1981) and the readme.md of the project’s 
                            GitHub repository.
                        
                     we want to show that through re-purposing well-established open software and by bridging the gap between immensely popular, but non-academic (and, at least under US copyright laws, occasionally illegal) online libraries of volunteers and academic scanning efforts as well as editorial expertise, one can produce scholarly editions that offer solutions for most of the above-mentioned problems—including the absence of expensive infrastructure: We use 
                    digital texts from 
                        shamela.ws
                    , transform them into TEI XML, add light structural mark-up for articles, sections, authors, and bibliographic metadata, and link each page to facsimiles provided through 
                    EAP and 
                    HathiTrust; the latter step, in the process of which we also make first corrections to the transcription, though trivial, is the most labour-intensive, given that page breaks are commonly ignored by 
                    shamela.ws’s anonymous transcribers. The digital edition (TEI, markdown, and a web-display) is then hosted as a GitHub repository with a 
                    CC BY-SA 4.0 licence for reading, contribution, and re-use.
                    
                         The text of 
                            al-Muqtabas itself is in the public domain even under the most restrictive definitions (i.e. in the USA); the anonymous original transcribers at 
                            shamela.ws do not claim copyright; and we only link to publicly accessible facsimile’s without copying or downloading them.
                        
                    
                
                We argue that by linking facsimiles to the digital text, every reader can validate the quality of the transcription against the original we can remove the greatest limitation of crowd-sourced or gray transcriptions and the main source of disciplinary contempt among historians and scholars of the Middle East. Improvements of the transcription and mark-up can be crowd-sourced with clear attribution of authorship and version control using .git and GitHub’s core functionality. Such an approach as proposed by Christian Wittern (2013) has recently seen a number of concurrent practical implementations such as project 
                    GITenberg led by Seth Woodworth, Jonathan Reeve’s 
                    Git-lit, and others.
                
                In addition to the TEI XML files we provide structured bibliographic metadata for every article in 
                    al-Muqtabas (currently as BibTeX). The TEI edition will be referencable down to the word level for scholarly citations, annotation layers, as well as web-applications through a documented and persistent URI scheme.
                
                In order to contribute to the improvement of Arabic OCR algorithms, we will provide corrected transcriptions of the facsimile pages as ground truth to interested research projects starting with 
                    transkribus.eu.
                
                To ease access for human readers (the main projected audience of our edition) and the correction process, we also provide a 
                    basic web-display that adheres to the principles of 
                    GO::DH’s Minimal Computing Working group. This web-display is implemented through an adaptation of the 
                    TEI Boilerplate XSLT stylesheets to the needs of Arabic texts and the parallel display of facsimiles and the transcription. Based solely on XSLT 1 and CSS, it runs in most internet browsers and can be downloaded, distributed and run locally without any internet connection—an absolute necessity for societies outside the global North.
                
                
                    
                    Figure 1: The web-display of 
                        Digital Muqtabas based on TEI Boilerplate.
                    
                
                Finally, by sharing all our code, we hope to facilitate similar projects and digital editions of further periodicals. For this purpose, we successfully tested adapting the code to 
                    ʿAbd al-Qādir al-Iskandarānī’s monthly journal 
                        al-Ḥaqāʾiq (1910–12, Damascus)
                    
                    
                         On the history of 
                            al-Ḥaqāʾiq and some of its quarrels with 
                            al-Muqtabas see (Commins, 1990:118–22).
                        
                     in February 2016.
                
            
            
                Conclusion
                The paper will discuss the challenges cultural artefacts, and particularly texts, face in the Middle East. We will propose a solution to some of these problems based on the principles of openness, simplicity, and adherence to scholarly and technical standards. Applying these principles, our edition of 
                    Majallat al-Muqtabas improves already existing digital artefacts and makes them accessible for reading and re-use to the scholarly community as well as the general public. Finally, we will discuss the particular challenges and experiences of this still very young project (since October 2015).
                
            
        
        
            
                
                    Bibliography
                    
                        Commins, D. (1990). 
                        Islamic Reform: Politics and Social Change in Late Ottoman Syria. Oxford: Oxford University Press.
                    
                    
                        Glaß, D. (2004). 
                        Der Muqtaṭaf Und Seine Öffentlichkeit. Aufklärung, Räsonnement Und Meinungsstreit in Der Frühen Arabischen Zeitschriftenkommunikation. Würzburg: Ergon Verlag.
                    
                    
                        Grallert, T. (2013). The puzzle continues: al-Muqtaṭaf was printed in two different and unmarked editions,
                        Sitzextase, http://tillgrallert.github.io/blog/2013/08/19/the-puzzle-continues/ (accessed 6 February 2016).
                    
                    
                        Grallert, T. (2014). The puzzle continues II: in addition to al-kabīr and al-ṣaghīr, al-Muqtaṭaf published slightly different editions in Beirut and Kairo,
                        Sitzextase, http://tillgrallert.github.io/blog/2014/01/19/the-puzzle-continues-2/ (accessed 6 February 2016).
                    
                    
                        Märgner, V. and El Abed, H.  (Eds) (2012). 
                        Guide to OCR for Arabic Scripts. London: Springer http://link.springer.com/book/10.1007/978-1-4471-4072-6.
                    
                    
                        Seikaly, S. (1981). Damascene Intellectual Life in the Opening Years of the 20th Century: Muhammad Kurd ʿAli and Al-Muqtabas. In Buheiry, M. R. (ed), 
                        Intellectual Life In The Arab East, 1890-1939. Beirut: American University Of Beirut, pp. 125–53.
                    
                    
                        Wittern, C. (2013). Beyond TEI: Returning the Text to the Reader. 
                        Journal of the Text Encoding Initiative, 4: Selected Papers from the 2011 TEI Conference. http://jtei.revues.org/691.
                    
                
            
        
    


        
            Dutch language has been described extensively in the comprehensive historical dictionaries of the Institute for Dutch lexicology. These dictionaries (Oudernederlands Woordenboek, Dictionary of Old Dutch, ca. 500-1200; Vroegmiddelnederlands Woordenboek, Dictionary of Early Middle Dutch, 1200-1300 ; Middelnederlandsch Woordenboek; MNW, Dictionary of Middle Dutch, ~1250-550; Woordenboek der Nederlandsche Dictionary of the Dutch Language, 1500-976) cover over 15 centuries of Dutch and are as such a perfect guide to understanding historical language. The dictionaries also provide the core material for the diachronic computational lexicon of Dutch (GiGaNT), that can be used to support search in historical texts by users without (expert) knowledge of historical spelling variation: when searching for 
                slager (‘butcher’) the user also gets the morphological and spelling variants like 
                slagers, slagher(s), slaeger(s) slaegher(s) or 
                slegher(s).  However, when a user wants to study the history of the butcher’s trade, it is not immediately obvious from the way these traditional dictionaries are structured that one has also to look for 
                vleeschhouwer or 
                beenhouwer or 
                beenhakker. And it is only after reading the complete articles that a user learns that 
                vleeschouwer can also mean ‘executioner’, and 
                slager ‘a person who slays so.’, be it though that in the case of 
                vleeschhouwer the meaning 
                ‘executioner´ is derived from vleeschhouwer ‘butcher’, while 
                slager in contemporary meaning ‘butcher’ is derived from the meaning 
                ‘a person who slays so’.
            
            In this contribution we describe the first results of our work on the development of a diachronic semantic lexicon of Dutch. The lexicon aims to enhance text accessibility and to foster research in the development of concepts, by interrelating attested word forms and semantic units (concepts), and tracing semantic developments in time. In the lexicon, the diachronic onomasiology, i.e. the change in naming of concepts and the diachronic semasiology, i.e. the change in meaning of words, will be recorded in a way suitable for use by humans and computers. The onomasiological part of the lexicon is meant to enhance recall in text retrieval by providing different verbal expressions of a concept or related concepts (slager → beenhouwer, beenhakker, vleeshouwer; boer → landman). The diachronic semasiological component (which charts semantic change), aims to enhance precision by enabling the user to take semantic change into account; the oldest meaning of 
                appel for example is ‘a fruit’ (so 
                appel is also used for pears, plums etc.). 
                
            
            We describe the structure of the diachronic semantic lexicon and procedures for the acquisition and aggregation of content. The INL historical dictionaries will be the main source of the lexicon, as these dictionaries describe the Dutch lexicon from the 6
                th to the 20
                th century and cover most of the basic vocabulary of this period. Word sense descriptions are illustrated by dated quotations, which constitute a first step towards dating a concept. The temporal distribution of quotations pertaining to different senses gives a first picture of the diachronic development of the sense inventory of a headword. The fact that many words in the historical dictionaries are defined (partly) by synonym definitions and contemporary semantic (near)-equivalents enables us to extract an initial set of semantic relations. 
            
            Information from other sources is not disregarded. For contemporary Dutch, several lexical resources cataloguing semantic relationships are available. This includes traditional synonym dictionaries like Brouwers “Het Juiste woord” and more recent initiatives such as Open Dutch Wordnet (Vossen). For some specific domains, thesauri with a diachronic component are in development (eg. the 
                HISCO (
                http://historyofwork.iisg.nl/index.php)). 
            
            Besides lexical sources, diachronic corpus material
                
                    Corpora: DBNL (digital library of Dutch literature, 
                        http://www.dbnl.nl), digitized newspaper collections at the Dutch Royal Library, and other collections digitized by the Royal Library (
                        http://www.delpher.nl).
                    
                 and corpus-based methods are no less essential to the development and verification of the relevance of the lexicon content. This includes: i) corpus based analysis of semantic change at the “type”-level, using distributional methods. Here, the fact that our starting point is defined by the set of quotation dates per word sense provides an interesting perspective. ii) research into the application of token-based distributional methods to the interlinking of historical corpora and lexical resources.
            
        
        
            
                
                    Bibliography
                    
                        Fellbaum, C. ed. (1999). 
                        WORDNET. An Electronic Lexical Database. London: The MIT Press.
                    
                    
                        Geeraerts, D., et al. (1994). 
                        The Structure of Lexical Variation. Meaning, Naming, and Context. Berlin/New York: Mouton de Gruyter.
                    
                    
                        Geeraerts, D. (1997). 
                        Diachronic Prototype Semantics. A Contribution to Historical Lexicology. Oxford: Clarendon Press.
                    
                    
                        Geeraerts, D. (2010). 
                        Theories of Lexical Semantics. Oxford/New York: Oxford University Press.
                    
                    
                        Gulordava, K. and Baroni, M. (2011). A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus.
                         Proceedings of the EMNLP 2011 Geometrical Models for Natural Language Semantics (GEMS 2011) Workshop, pp. 67-71.
                    
                    
                        Heylen, K., et al. (2015). Monitoring polysemy: Word space models as a tool for large-scale lexical semantic analysis. 
                        Lingua, 157: 153-72.
                    
                    
                        Kay, C. J. and Chase, T. J. P. (1987). Constructing a Thesaurus database. 
                        Literary and Linguistic computing, 2(3): 161-63.
                    
                    
                        Laurence, S. and Margolis, E. (1999). Concepts and Cognitive Science. In Margolis, E. and Laurence, S., 
                        Concepts. Core Readings. Cambridge (US)/London: The MIT Press, pp. 3-81.
                    
                    
                        Sijs, N. van der (2001). 
                        Etymologie in het digitale tijdperk. Een chronologisch woordenboek als praktijkvoorbeeld. Ph.D. thesis, Universiteit Leiden.
                    
                    
                        Vanhove, M. ed. (2008). 
                        From Polysemy to Semantic Change. Towards a typology of lexical semantic associations. Amsterdam/Philadelphia: John Benjamins Publishing Company.
                    
                    
                        Vossen, P. ed. (1998). 
                        EuroWordNet: A mulitlingual database with lexical semantic networks. Reprinted from 
                        Computer and the Humanities, Vol. 32, Nos. 2-3, 1998. Dordrecht/Boston/London: Kluwer Academic Publishers.
                    
                
            
        
    


        
            
                Introduction: Digital musicology
                Computer-based methods in musicology have been around at least since the 1980s
                    
                         The popular series „Computing in Musicology“ started around 1985. For an overview of all volumes of the series cf. http://www.ccarh.org/publications/books/cm/; Note: All URLs mentioned in this text were last checked on March 3, 2016.
                    . Besides the creation of digital editions (cf. Kepper et al., 2014; Veit, 2015), scholars in this area of study have also been interested in quantitative approaches for musicological analyses (cf. Müllensiefen and Frieler, 2004; Vigilanti, 2007). Such quantitative analyses rely on music information retrieval (MIR) systems, which can be used to search collections of songs according to different musicological parameters. There are many examples for existing MIR systems, all with specific strengths and weaknesses. Among the main downsides of such systems are:
                
                
                    
                        Usability problems, i.e. tools are cumbersome to use, as they oftentimes only provide a command-line interface and also require some basic programming skills to utilize them; example: Humdrum
                        
                             http://www.humdrum.org/
                        
                    
                    
                        Restricted scope of querying, i.e. tools can only be used to search for musical incipits; examples: RISM
                        
                             https://opac.rism.info/
                        , HymnQuest
                        
                             http://hymnquest.com/
                        
                    
                    
                        Restricted song collection, i.e. tools can only be used for specific collections of music files; various examples of MIR tools for specific collections are described in Typke et al. (2005)
                    
                
                A particularly promising MIR tool can be found in Peachnote
                    
                         http://www.peachnote.com/
                     (Viro, 2011), which uses optical music recognition (OMR) software to index more than one million sheets from the Petrucci Music Library
                    
                         http://imslp.org/
                    , aiming to provide a search interface for musicology which can be seen as an analog of the Google Books Ngram Viewer
                    
                         https://books.google.com/ngrams
                    . Despite many existing software solutions, we believe that accurate OMR is still a major challenge in digital musicology. At the same time, there are numerous databases
                    
                         http://www.musicxml.com/music-in-musicxml/
                     at hand, that provide machine-readable music documents, fully annotated with MusicXML (Good, 2001) markup. 
                
                On this account, we designed MusicXML Analyzer, a generic MIR system that is trying to overcome the weaknesses of existing MIR tools, and that allows for the analysis of arbitrary documents encoded in MusicXML format. 
            
            
                MusicXML Analyzer: Basic functionality and implementation details
                MusicXML Analyzer can be used to analyze songs in a quantitative manner, and to search for specific melodic patterns in a collection of songs. The results of the analyses are rendered as virtual scores and can be viewed in any recent web browser. In addition, the queries and the results can be played as a synthesized audio file; all analyses can also be exported as PDF or CSV files.
                The tool comprises three main components: (1) the upload function, (2) the analysis function, and (3) the search function. After one or more files in MusicXML format have been uploaded via an intuitive drag-and-drop dialog, the analysis component parses the data and calculates basic frequencies; the results are stored in an SQL database and can be displayed in a dashboard view (cf. Fig. 1). 
                
                    
                    Figure 1: Snippet from the dashboard view, showing basic frequencies for a corpus of MusicXML documents.
                
                The dashboard displays the following information, either for an individual song, or for a corpus of multiple songs:
                
                    Overall statistics for single notes, rests and measures 
                    Types of instruments used in the song (if described in the MusicXML data)
                    Frequency distribution for single notes, intervals, keys, note durations and meters 
                
                Via a dedicated search function, a corpus of MusicXML documents can be queried for melodic patterns on different levels of information:
                
                    Search for a sound sequence; example: c’, c’, g’, g’
                    Search for a rhythmic pattern; example: eighth note, eighth note, quarter note, quarter note
                    Search for melodic patterns, i.e. a combination of sound sequence and rhythm; example: c’ / eighth note, c’ / eighth note, g’ / quarter note, g’ / quarter note
                
                Search queries can be entered via a virtual staff that was realized with the VexFlow library
                    
                         http://www.vexflow.com/
                     (cf. Fig. 2). Once a search pattern has been entered, it can also be played as a synthesized Midi sequence, which was realized with the Midi.js library
                    
                         http://mudcu.be/midi-js
                    . 
                
                
                    
                    Figure 2: Interface for entering queries to identify tonal, rhythmic, or melodic patterns in a corpus of MusicXML documents.
                
                After a query has been submitted, all results – i.e. the songs that contain the search pattern – are displayed in a list view. The list contains the name of the song and also the number of total occurrences of the search pattern in that song. By clicking on one of the song items in the list, a virtual score is rendered for the whole song; the search pattern is highlighted whenever it occurs in that virtual score (cf. Fig. 3). The whole song can be played directly in the web browser, or downloaded for further analyses as a PDF (realized with the jsPDF library
                    
                         https://parall.ax/products/jspdf
                    ).
                
                
                    
                    Figure 3: Virtual score rendering of a document from the results list; the search pattern is highlighted in red color.
                
                MusicXML Analyzer was implemented by means of standard web technologies (HTML, CSS, JavaScript, PHP), in particular by utilizing the following libraries and frameworks: Laravel
                    
                         http://laravel.com/
                    , jQuery
                    
                         https://jquery.com/
                    , D3.js
                    
                         http://d3js.org/
                    , Bootstrap
                    
                         http://getbootstrap.com/
                    , Typed.js
                    
                         http://www.mattboldt.com/demos/typed-js/
                    , Dropzone.js
                    
                         http://www.dropzonejs.com/
                    . 
                
                A short demo video that showcases the main functionality of the tool is available at
                
                    
                        https://dl.dropboxusercontent.com/u/4194636/MusicXML-Analyzer.mp4
                    
                
                A fully functional online demo
                    
                         Due to some technical limitations of our server environment, the initial access to the online demo may take a few seconds to wake up the server from 
                            idle mode.
                        
                     of MusicXML Analyzer is available at
                
                
                    
                        http://music-xml-analyzer.herokuapp.com/
                    
                
                MusicXML Analyzer can also be downloaded and modified (according to the MIT open source license) from GitHub:
                
                    
                        
                            https://github.com/freakimkaefig/Music-XML-Analyzer
                        
                    
                
            
            
                Future directions
                In its current implementation, MusicXML Analyzer performs an exact match search, i.e. only documents which have the exact same value in their MusicXML markup will be found by the search function. We are planning to implement a more sophisticated melodic similarity algorithm (cf. Grachten et al., 2002; Miura and Shioya, 2003) that allows for the configuration of different similarity thresholds. 
                At the same time, we are adapting MusicXML Analyzer for a recent project on a large corpus of German folksongs. Besides monophonic melodies, this collection of folksongs also contains machine-readable metadata (region, date, etc.) and lyrics. Accordingly, we are trying to enhance the search features of MusicXML Analyzer in a way it can not only search songs for melodic patterns, but also for metadata parameters and keywords from the lyrics. Such an enhanced MIR system could be used to analyze the following research questions:
                
                    Are there characteristic melodic and linguistic patterns for German folksongs, from a diachronic perspective as well as from a regional perspective?
                    Are there melodic-linguistic collocations, i.e. do certain melodic patterns co-occur with certain keywords or phrases?
                
            
        
        
            
                
                    Bibliography
                    
                        Good, M. (2001). MusicXML for Notation and Analysis. In Hewlett, W. B. and Selfridge-Field, E. (eds.), 
                        The Virtual Score: Representation, Retrieval, Restoration. Cambridge (MA) and London (UK): MIT Press, pp. 113–24.
                    
                    
                        Grachten, M. A., Josep, L. and Mántaras R. L. (2002). A comparison of different approaches to melodic similarity. 
                        Proceedings of the 2nd International Conference in Music and Artificial Intelligence (ICMAI) 2002.
                    
                    
                        Kepper, J., Schreiter, S. and Veit, J. (2014). ‚Freischütz‘ analog oder digital – Editionsformen im Spannungsfeld von Wissenschaft und Praxis. 
                        Editio, 28: 127–50.
                    
                    
                        Miura, T. and Shioya, I. (2003). Similarity among melodies for music information retrieval. 
                        Proceedings of the 12th International Conference on Information and Knowledge Management (CIKM) 2003.
                    
                    
                        Müllensiefen, D. and Frieler, K. (2004). Optimizing Measures Of Melodic Similarity For The Exploration Of A Large Folk Song Database. 
                        Proceedings of the 5th International Conference on Music Information Retrieval (ISMIR) 2004, pp. 274–80.
                    
                    
                        Typke, R., Wiering, F. and Veltkamp, R. C. (2005). A survey of music information retrieval systems. 
                        Proceedings of the 6th International Conference on Music Information Retrieval (ISMIR) 2005, pp. 153–60.
                    
                    
                        Veit, J. (2015). Music notation beyond paper. On developing digital humanities tools for music editing. 
                        Forschungsforum Paderborn, 18: 40-48. 
                    
                    
                        Viglianti, R. (2007). MusicXML: An XML Based Approach to Musicological Analysis. Digital Humanities 2007: Conference Abstracts, pp. 235–37.
                    
                    
                        Viro, V. (2011). Peachnote: Music Score Search and Analysis Platform. 
                        Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR) 2011, pp. 359-62. 
                    
                
            
        
    


        
            Digital data is fuel for data-intensive science. Access, re-use and sharing of this data, however, while required by academic ethos and good practice, are often highly restricted by legal frameworks. In particular, the areas of law that can affect research data are: intellectual property (copyright and database rights) and personal data protection. These issues are particularly relevant in the field of Digital Humanities, which study various aspects of human activities in general, and their creative and social aspects in particular. In fact, most of the research data in digital humanities are within the scope of either intellectual property or data protection law, which means that they cannot be freely accessed, re-used and shared without a permission of the right holder or the data subject’s consent. 
            Moreover, research funding agencies require more and more often that the results (and underlying data) of research projects that they fund be made available in Open Access. Open sharing of research data and outcomes is often perceived nowadays as an ethical obligation in contemporary science, but it cannot be done in a satisfactory way without addressing legal concerns (such as appropriate licensing and rights clearance). 
            Legal issues are increasingly being taken into account in the preparation phases of many research projects. Scientists who do not consider legal issues in their research activities may be exposed to certain legal risks. Existing statutory exceptions for research rarely provide for enough relief (even though lobbying efforts are being made to extend their scope). In short, modern science in general, and Digital Humanities in particular, are more concerned with legal issues than ever before. 
            The purpose of the multiple paper session we propose is to emphasize the problem, discuss various technological and organizational solutions, as well as future legal challenges that DH researches will have to face. 
            Three papers by authors with both legal training and hands-on experience with D-SSH research data management will be presented. The first one compares organizational and technical solutions adopted in the field of DH and Social Sciences. The second discusses research data licensing, and presents existing tools that help researchers in the process. The third paper examines legal and ethical aspects of stylometry and authorship attribution research.
            
                “One Does Not Simply Share Data”. Organisational and Technical Remedies to Legal Constraints in Research Data Sharing -- building bridges between Digital Humanities and the Social Sciences. 
                by Pawel Kamocki (IDS Mannheim/Paris Descartes/WWU Münster), Katharina Kinder-Kurlanda (GESIS) and Marc Kupietz (IDS Mannheim)
                Within the Social Sciences there exists a long tradition of data sharing, which is facilitated by infrastructure institutions such as the GESIS Data Archive that has been providing survey data to researchers since the 1960s. More recently various technical solutions aiming to grant secure and user-friendly access to data requiring special protection have been emerging in the field. 
                The DH also have a well-established tradition of research data sharing. In linguistics, for example, digital text collections have also been published since the 60s (e.g. the Brown Corpus or the Mannheimer Korpus). First software solutions to share the data and to make it accessible to other researchers emerged in the late 1980s and started to boom with the appearance of the WWW in the early 1990s. 
                
                    Legal barriers to data sharing 
                    Legal issues have long been identified as barriers to research data sharing. They can be divided into two categories: those related to intellectual property rights and those related to privacy laws. 
                    Intellectual property rights — such as copyright and the database right — grant the rights holders certain exclusive rights (monopolies), i.e., rights to exclude others from the use of their property. For example, in order to copy and distribute a copyright-protected work or a database, one normally has to obtain permission from the rights holder, usually in an agreement known as a license. This highly affects DH — disciplines fueled by digital data issued from human creative activities, which normally qualify for copyright protection. 
                    It is essential to understand that intellectual property is similar to “traditional” (i.e. corporeal) property. Therefore, it can be said that most research data in DH in fact belong to a third party (author or publisher). The right to property is a fundamental freedom, which overrides freedom of research. As a consequence, statutory research exceptions are rarely enough to allow use of copyright-protected data in research projects. 
                    Researchers in DH therefore need to obtain licenses for the use of data, which is not an easy task. The negotiations may be time-consuming, and the result is not always satisfactory. In practice, licenses signed with e.g. publishers are often very restrictive and non-transferable. 
                    Another legal framework that affects researchers in areas such as medicine or the social sciences, but also in the DH, is personal data protection. In principle, personal data (i.e. any information related to an identifiable person) can only be processed if the data subject has validly consented to the processing. While it is true that anonymised data can be freely processed, anonymisation may strip a dataset from most (if not all) of its informational and scientific value. The obligation to obtain consent is particularly burdensome when it comes to older data that has been collected without consent, or that has been collected for a different purpose (re-purposing normally necessitates a new consent). Also, in practice, consent rarely covers transfer and sharing of data with other researchers.
                
                
                    Social Science approach 
                    In the Social Sciences quantitative survey data is particularly interesting for sharing. The highly controlled and well-documented ways of gathering data in large-scale survey programmes make the data highly reusable in a methodologically sound way. The GESIS Data Archive for the Social Sciences in Germany provides survey data for secondary use and thus allows researchers to share collected data in a user-friendly, searchable and standardised manner. Due to data protection legislation participants of survey data provided by the archive must not be re-identifiable, or only with a disproportionate amount of time, expense and labour. Anonymization challenges usually occur once detailed geographical as well as demographical information has been collected. 
                    To improve data sharing several solutions have been found for the Social Sciences. For example, most data at the GESIS archive is anonymised and thus can be provided for download via the online data catalogue. Some datasets containing more detailed information are provided employing secure data access solutions. A combination of contractual, organizational and technical safeguards is employed to ensure that individuals’ rights to anonymity are protected. For example, for particularly disclosive data, researchers must visit a safe room where a completely encapsulated virtual research environment is provided via a thin client. They cannot download any data or access the internet. They are also not allowed to bring mobile phones or other electronic devices. Any analysis output they produce is intellectually assessed for its level of disclosiveness and only handed to researchers once the output criteria are fulfilled. Only users whose signed usage agreements (detailing the research topic and the methods applied) have been approved can use the safe room. 
                    Secure remote solutions either provide researchers with a secure connection to an encapsulated work environment as described above or allow the submission of code and syntax to be run on the data by the data provider. All remote solutions need to be secured from threats posed by using the internet. 
                
                
                    DH approach 
                    To cope with legal challenges, to make research data as openly accessible as possible, and to enable traceability and replicability without interfering with legitimate interests of rights holders, the disciplines that deal with language as their primary research data, particularly linguistics, have developed several strategies.
                    As already discussed above, usually the only way to acquire text for research purposes is to obtain licenses from copyright holders. The copyright holders can sometimes be convinced to provide scientific licenses for free or for comparatively low fees as long as they do not interfere with the company’s business model. Given that some institute is willing to conclude partially transferable license agreements with rights holders and license agreements with end-users, thus acting as an intermediary between both parties, and to provide software that enforces the license restrictions and provides all retrieval and analysis functions that researchers need, every group's interests can be satisfied. Indeed, this model has worked successfully at the Institute of German Language (IDS) since the beginning of the 90s and also for most other providers of national and reference corpora. 
                    The problem with the intermediary model alone is that it requires the intermediary to provide all functions that are required for any researcher. While, for example, the requirements for more traditional linguists could be satisfied, this was not possible for researchers in the field of text mining and computational linguistics where the methods of analysis themselves are in the center of research and therefore subject to rapid change. 
                    To meet the needs of such user groups, another idea, recently described for linguistics but traditionally used in data-intensive disciplines such as climate research, has to be put into action. Extending Gray’s (2003) famous claim “put the computation near the data” to situations where the data cannot be moved due to license restrictions, data providers also provide mechanisms that allow end-users to run their analysis software on the data located at the provider, making sure that the software does not violate any license restrictions.
                    Another remedy that is often applied in the context of corpora that are based solely on texts from the WWW, is not to share the research data itself, as this could be a copyright violation, but rather to share the software that retrieves the texts from the Web. Depending on the the application scenario and the local legislation, this technique can enable end-users to benefit from statutory exceptions. An unwanted side-effect of this approach is that the identity of the corpus data retrieved by different runs of the retrieval software cannot be guaranteed. However, there is some consensus in the research community that the sights with respect to demands on replicability and persistency of research data necessarily have to be lowered to a realistic standard that takes into account legal restrictions. This aspect has also recently entered the best-practice guidelines of the German Research Foundation. 
                
                
                    Conclusions 
                    D-SSH are dealing with data surrounded by legal issues. Traditionally, Social Science researchers process more privacy-sensitive information, whereas DH researchers work with data protected by intellectual property rights, usually belonging to third parties; the division, however, is not clear-cut. Some disciplines in DH (such as linguistics) also have to deal with privacy-sensitive material, and the Social Sciences are concerned if not by copyright, then by other branches of intellectual property (such as database right). Increasingly commercial data owners such as social media companies are becoming important.
                    Institutes in both disciplines have developed idiosyncratic ways of coping with legal restrictions which provide satisfactory results. This shows us that some legal issues may be resolved by appropriate organisational, technical and infrastructural solutions; the comparison between DH and Social Sciences, however, demonstrates that both disciplines still have room for improvement and that there is a lot that they can learn from each other.
                
            
            
                « Trust me. I’m a License Selector ». Licensing for Digital Humanities 
                by Paweł Kamocki (IDS Mannheim / Université Paris Descartes / WWU Münster) and Pavel Stranak (UFAL Prague)
                Lack of legal interoperability (i.e. a situation in which a dataset cannot be used due to incompatible licensing restrictions on its various parts) has been identified as one of the major obstacles for data access, sharing and re-use. This is particularly relevant in the field of Digital Humanities, where data are often protected by copyright (i.e., they are created by human authors). While it is true that some data (e.g. those obtained from press editors) are only available to researchers under very restrictive license agreements, in fact quite often legal interoperability problems can be solved by proper licensing of research outcomes. Indeed, despite the fact that openness and reproducibility of results have long been identified as cornerstones of the scientific community, in practice many digital datasets and tools are being shared under licenses that are unnecessarily restrictive or not fit for the purpose, or even without any licenses at all. This is probably due to the fact that the task of choosing an appropriate license may seem difficult for an average researcher with a limited access to legal advice. As a response to that problem, attempts have been made to build tools (referred to as License Choosers, License Selectors or even License Wizards) that would guide the users through the jungle of available public licenses and allow him to choose one that is the most suitable for his needs. 
                Before these License Selectors can be presented and assessed, it is essential to define the notion of a public license. A public license is a license that grants certain rights not to an individual user, but to the general public (every potential user). Public licenses for software has been known since 1980s (when software licenses such as BSDL, MIT or GNU GPL emerged). However, public licenses for other categories of works (including datasets) only appeared in the 21st century, mostly due to the creation of the Creative Commons foundation. The latest version of the CC license suit (including six licenses, a waiver and a public domain mark), CC 4.0, is well adapted for datasets, as it covers not only copyright, but also the sui generis database right, but older versions are still in use. While choosing a license, one has to keep in mind that the licenses which are appropriate for software are not appropriate for data and vice versa. Moreover, not all public licenses are ‘open’, i.e. not all of them meet the requirements for Open Access/Open Data/Open Source label. In our paper, we would like to briefly demonstrate three online tools made specifically for licensing of research material.
                The Licentia tool (http://licentia.inria.fr/visualize) has been developed in 2014 by Cardellino for INRIA (French Institute for Research in Computer Science and Automation) is in fact a conglomerate of three tools: a License Search Engine (which allows to identify licenses that meet a set of requirements defined by the user), a License Compatibility Checker (which assesses whether two licenses are compatible, i.e. whether material licensed under those two licenses can be ‘mixed’) and a License Visualiser (an interesting extra feature which produces graph-based visualisations of licenses expressed in ODRL - Open Digital Rights Language Deontology). 
                The ELRA (European Language Resources Association) License Wizard (http:// wizard.elda.org), released in April 2015, allows users to define a set of features and browse corresponding licenses. For now, the tool only includes CC, META-SHARE and ELRA licenses, so it is particularly useful for language resources.
                Finaly, the Public License Selector (http://ufal.github.io/public-license-selector/) developed by Kamocki, Stranak and Sedlak in 2014 as a cooperation between two CLARIN centres (IDS Mannheim and Charles University in Prague) uses an algorithm (a series of yes/ no questions) to assist the user in the licensing process. It allows to choose licenses for both data and software, and features a built-in License Interoperability Tool. Licenses that meet the ‘open’ requirement are clearly marked. Finally, unlike the two other tools, it is made available under Open Software/Open Data conditions. 
                All of these tools have both advantages and disadvantages; their biggest disadvantage is that they use (to a different degree) a very specific language, which in fact requires basic knowledge of Intellectual Property Law from the user. They also necessarily involve a certain degree of over- or undergeneralization, especially when it comes to assessing license interoperability. Nevertheless, they remain very useful for the research community and may indeed help facilitate re-use and sharing of tools and data in Digital Humanities.
            
            
                Legal and Ethical Aspects of Authorship Attribution Using Stylometry - EU and US Perspectives 
                by Erik Ketzan (IDS Mannheim) and Paweł Kamocki (IDS Mannheim / Université Paris Descartes / WWU Münster) 
                
                    Introduction
                    Authors have written anonymously since the invention of writing, and the growing digital humanities field known variously as stylometry / computational stylistics / authorship attribution often aims to discover the identify (or rule out the identity) of anonymous authors. 
                    Depending on whether such authors are living, whether the works in question are protected by copyright, and what the aims of the digital humanities research is, vastly different legal frameworks govern such research in the European Union and United States. The strong data protection laws of the EU seem to prohibit certain types of authorship attribution research, while researchers in the US have vastly fewer restrictions regarding data protection regulations. In addition to data protection, the acts of copying and analyzing texts for the purposes of stylometry raise copyright concerns. In the US, these acts seem to be largely allowed by the fair use doctrine. In the EU, these fall into more questionable legal territory, although new laws regarding text and data mining offer improved guidance to researchers. 
                    As laws concerning research in the digital age are being revisited in both the US and EU, it is important to see where stylometry falls under current legal frameworks, and how, and whether, researchers should advocate for changes to law. Finally, we argue that a parallel debate regarding the ethics of stylometric research should be begun. As stylometric research and technology continues to improve, with promises of improved reliability of authorship attribution, researchers should begin to debate which questions researchers should ethically tackle, not only which questions they can. 
                
                
                    Stylometry of anonymous authors under EU law 
                    In the EU, where the memory of totalitarian governments is still present, Member States value privacy very highly. This is translated in the legislation, where the right to be and remain anonymous is not only protected by rules on the processing of personal data, but sometimes also to an extent guaranteed by copyright laws. 
                    The Data Protection Directive is the primary source of laws governing the processing of personal data, and guides Member States in protecting "the fundamental rights and freedoms of natural persons, and in particular their right to privacy with respect to the processing of personal data." The Directive defines personal data as, "any information relating to an identified or identifiable natural person ('data subject'); an identifiable person is one who can be identified, directly or indirectly, in particular by reference to an identification number or to one or more factors specific to his physical, physiological, mental, economic, cultural or social identity." Processing of personal data is defined as, "any operation or set of operations which is performed upon personal data, whether or not by automatic means, such as collection, recording, organization, storage, adaptation or alteration, retrieval, consultation, use," etc.
                    In general, processing of personal data can only be done if the data subject (i.e. the person that the data refer to) has unambiguously given his consent. Moreover, the Data Protection Working Party (a body composed of representatives of National Data Protection Authorities from each Member State and whose purpose is to give expert advice on the interpretation of the Data Protection Directive) clearly stated that information that does not relate to an identified person, but is collected for the purpose of identification, shall also be regarded as personal data. 
                    European researchers engaging in stylometric research for the purpose of identifying a living author therefore engage in the processing of personal data, and are subject to the rules and restrictions of the Data Protection Directive and related Member State data protection laws. Alternatively, consent could be obtained to process the personal data, but this leads to the absurd suggestion that researchers obtain permission from an anonymous author so that they can guess at his/her identity. 
                    While the current framework allows for alternative grounds for lawfulness of processing (other than consent), such as e.g. pursuit of legitimate interests, these provisions remain vague and do not guarantee the necessary legal security for researchers. Exceptions from the rules set up by the Directive exist, but only cover very special cases such as freedom of journalistic and artistic expression, public security or (to a limited extent) historical, statistical and scientific research. 
                    An inevitable conclusion, however, is that Personal Data Protection law in the EU protects anonymous authors from being identified against their will, at least when they are still alive. 
                    Anonymity of authors is also addressed by many national laws on copyright. Although anonymous works benefit from a significantly shorter term of protection (70 years after publication, and not 70 years after the death of the author), the anonymity of the author is nevertheless protected; his rights can be exercised by a proxy (usually an agent or publisher). Moreover, in some jurisdictions (i.e., in France), inaccurate attribution of authorship can be regarded as violation of moral rights (i.e., a form of copyright infringement) of both the real author and the falsely attributed one. 
                
                
                    Stylometry of anonymous authors under US law 
                    The legal framework of the United States governing stylometry of anonymous authors is vastly different from the EU. The United States has no single general data protection law. The First Amendment of the United States Constitution guarantees the right to free speech, and a broad right to privacy has been inferred from the Constitution by the United States Supreme Court. A number of state constitutions, such as California, explicitly mention privacy as well.
                    Courts in the US have recognized certain rights to anonymity, most notably in McIntyre v. Ohio Elections Commission, 514 U.S. 334 (1995), where the Supreme Court held that the freedom to publish anonymously is protected by the First Amendment, and extends beyond the literary realm to the advocacy of political causes. Whether such a right extends to a researcher attempting to remove that anonymity is an open question. 
                    Regarding copyright, there are strong arguments that the acts of copying and data mining text for research purposes are covered by the fair use doctrine, especially after the landmark Google Books case, which held that the scanning of books and making snippets available in search engines is a fair use. As a typical stylometric analysis involves the copying of texts and analysis on a single computer, without distribution of snippets (in other words, infringing less upon exclusive rights than the facts in Google Books), the acts seem to be covered by the fair use framework. 
                
                
                    De Facto
                    Regardless of the letter of the law, the fact remains that many writers write anonymously and academics are increasingly asked to identify them. 
                    In courts of law, researchers with expertise in linguistics, computer science, and stylometry have acted as expert witnesses for decades now in criminal and civil disputes. 
                    Outside of courts, academics have published or given pronouncements to journalists in most news-worthy instances involving high-profile anonymously written works, including Primary Colors (a 1996 novel satirizing the Clinton Presidential campaign), The Cuckoo's Calling (a 2013 novel revealed to be the work of J.K Rowling), the Wanda Tinasky letters (dozens of eccentric and creative letters mailed to local California newspapers from 1984-88, which academics proved were not the work of Thomas Pynchon), and many more. In all of these instances, journalists and academics have not discussed the moral or legal right to make such analysis; they have simply done it.
                
                
                    Ethics and stylometry 
                    The purpose of the proposed paper is, through an analysis of different legal frameworks, to highlight the different norms and assumptions that surround the "un-masking" of anonymous authors. The radical difference in EU and US legal approaches proves that opinions can differ, and that serious debate needs to be begun among researchers. As the technology and approaches to stylometry yield increasingly accurate results, it is time for the digital humanities community to begin to discuss ethical standards.
                
            
        
    


        
            The central theme for this workshop is data mining and the connection between metadata and data in the context of digital libraries. Digital resources and search engines raise several questions about the relationship between metadata and the data they describe. For example, what is the relationship between metadata keywords and classification categories (e.g. Dewey)? How should topics found by topic modeling algorithms be labelled? With readily available search engine technology, using document relevance based on content words, is there a need for library classification systems at all, like Dewey or UDC?
            While there may be overlap between metadata and the texts described contentwise, metadata typically contain information not found within the text, such as author, geolocation and time data. In addition, subject or topic words typically consist of carefully constructed language models in the form of thesauri dedicated specifically towards specialized literary collections within different fields. The question is then how search engines may benefit from such metadata with a language model, and for what kind of library user?
            In this workshop, we invite colleagues to discuss the application of various methods related to digital library resources, including the structure of the metadata itself, as well as digital book collections. Many resources are available to libraries in digital form, like journals and new book titles, while some libraries also have launched digitization programs to create digital libraries, using scanners and OCR technology.
            Both the text data and the metadata of digital libraries can be scrutinized with data mining techniques, opening up the material for large-scale, quantitative analysis. This makes such collections highly relevant for Digital Humanities studies.
            
                Background
                The ongoing trend towards increased digitization in society in general poses numerous challenges at many levels, but also opens up for vast opportunities within many fields, including the library sector.
                At the National Library of Norway, a mass digitization project was initiated in 2006, with the goal of digitizing the entire collection of books, newspapers, movies, radio- and television-broadcasts, music etc., in sum everything published in the public domain in Norway of all media types, i.e. the entire cultural heritage of Norway. For books, the goal is to have the entire stock digitized by 2017. Thus far, some 435.000 of 450.000-500.000 books have been digitized. When all books and newspapers have been digitized, we estimate that our Norwegian text corpus will consist of some 80 - 100 billion tokens, which is big for a rather small language like Norwegian with approximately 5 million speakers. In comparison, the Google Books corpus contains approximately 500 billion tokens for English.
                The National Library cooperates with scholars of literary studies and linguistics in developing and applying methods of data mining to the digital collection. We develop services that make the content available for quantitative research, without challenging intellectual property rights. One such service is NB N-gram for Norwegian (see 
                    http://www.nb.no/sp_tjenester/beta/ngram_1/), comparable to Google Ngram Viewer for English and other languages.
                
            
            
                Workshop leaders
                Lars G. Johnsen: Research librarian at the Nation Library of Norway, PhD in linguistics. Fields of interest: semantics, grammar, philosophy of language, probability theory and applications. Email: 
                    Lars.Johnsen@nb.no, Phone: +47 23 27 61 84
                
                Arne Martinus Lindstad: Research librarian at the National Library of Norway, PhD in linguistics. Fields of interest: corpus linguistics, language change, comparative syntax, negation. Email: 
                    arne.lindstad@nb.no, Phone: +47 23 27 62 11
                
                Magnus Breder Birkenes: Research librarian at the National Library of Norway, PhD in linguistics. Fields of interest: corpus linguistics, history and dialectology of the Germanic languages. Email: 
                    magnus.birkenes@nb.no, Phone: +47 23 27 60 54
                
            
            
                Target audience
                Librarians, research librarians, scholars of literary studies, corpus and computational linguists
            
            
                Length and Format
                Half day:
                
                    
                        09.00 - 09.30
                        Introduction and opening discussion
                    
                    
                        09.30 - 10.30
                        Slot 1: Structure of metadata, Data mining and library classification systems
                    
                    
                        10.30 - 11.00
                        Coffee break
                    
                    
                        11.00 - 12.00
                        Slot 2: Metadata and modeling
                    
                    
                        12.00 - 12.30
                        Wrap-up and final discussion
                    
                
            
            
                Budget
                Coffee and snacks for the coffee break (max. 50€)
            
            
                Technical requirements
                A projector for presentations. Internet connection. We will bring our own computers.
            
            
                Call for papers (cfp)
                Are you interested in automatic classification of documents and what implications this has for libraries? How may search engines (like 
                    ElasticSearch) benefit from library metadata? Do you have any experience with developing public/academic web services on top of large amounts of library data? If these questions appeal to you, this workshop may be of interest. The central theme for this workshop is data mining and the connection between metadata and data in the context of digital libraries.
                
                We invite papers on topics such as:
                The structure of subject headings and descriptors, used in book classification (e.g. in building thesauri)
                The relationship between topic words and library classification systems
                The relationship between content words and topic words (of existing metadata, or as output from topic modeling algorithms)
                Automatic classification of digital documents
                Authorship attribution
                Development of computational services for research and the general public
                Legal issues arising with different data mining practices
                Please send us an abstract of max. 500 words that is situated within the above context.
            
            
                Program Committee
                Oddrun Ohren (National Library of Norway)
                Koenraad De Smedt (University of Bergen)
                Anders Nøklestad (University of Oslo)
                Elise Conradi (National Library of Norway)
            
        
    

While searching for medieval manuscripts suitable for the “Monumenta Germaniae Historica” (a collection of German historical sources) in 1841, Georg Waitz visited the library of the Merseburg Cathedral (Jankofsky, 2013). Rather coincidentally, he found a page with two magic spells in a theological composite manuscript: the Merseburg Incantations (see Fig. 1). Realizing the importance of his discovery, he asked Jacob Grimm to analyze and evaluate the text. The first Merseburg Incantation is a blessing of release telling about “Idisen” (female dieties) freeing either themselves or captured warriors from chains. The second Merseburg Incantation is a healing spell to cure a dislocated horse foot. The spells in Old High German were written down in the 10th century, but their origin is still unclear, maybe several hundred years earlier. The first time publicly presented at the Royal Prussian Academy of Sciences in Berlin, Jacob Grimm denoted the value of the Merseburg Incantations by calling them a gem, and nothing from the most popular libraries would have a similar value (Grimm, 1842).

'    ‘    iÂMwî)er,uît

. Uvlun    uuiJt îhfpî

b<MiJun i m^r    .W’    ,

I ¿1 ruiirui OîUti uupî*uîi -1 tot ! âifnw haLlvrçfon finuiu>x L irr*3 k'i^l, -rkJmziîôttm.nnlir^m •

........-

i 1

;T ikl Gtui 1 ' <• fi \ ¿4 lH5t .UustK \

|. Omj'f■feyar-rtiv ¿1’<Jn«jrt».tfm7raVil,,

'    nafoluf• j>n‘nJrfuJ,£vnuin n«,. W'

gnxnr fXlwtivrif- ^'irrpiurrrcatr-j-^i ct^nr j0nmw nf rvrvm-m^ trni* AÄf

& mfmfynJt'-J' '< -•<

¡¡fr

-IW



¿zW FW •' <- 'Sfj


Figure 1: Merseburg Incantations manuscript

A number of works interpret content, meaning and origin of the Merseburg Incantations (e.g., Schumacher, 2000, Beck, 2003, and Schmitt, 2011). In contrast to these rather profound analyses and interpretations of the Merseburg Incantations, our project aims at examining their global impact. We therefore

design a platform that brings together various types

of sources—tagged with citation year, source type, reference purpose—citing the Merseburg Incantations. Some examples are listed below.

Sources citing the first Merseburg Incantation:

•    “Die Südharzreise”, Frank Fischer, 2010, travelog

•    “Mara und der Feuerbringer, Band 03: Götterdämmerung”, Tommy Krappweis,

2012, novel

•    “Angelina Jolie - The Lightning Star”, C. Duthel, 2012, biography

•    “Seelenriss: Depression und Leistungsdruck”, Ines Geipel, 2013, psychology

•    “Das siebte Buch: Objektorientierung mit C++”, Ernst-Erich Doberkat, 2013, programming

•    “Die Schwarzen Musketiere - Das Buch der Nacht”, Oliver Pötzsch, 2015, novel

•    “Charlemagne”, Johannes Fried and Peter Lewis, 2016, historiography

Sources citing the second Merseburg Incantation:

•    “The Key to Music's Genetics: Why Music is Part of Being Human”, Christian Lehmann, 2014, musicology (example for healing through singing)

•    “Ring of the Nibelungs”, German dubbing, 2004, TV Movie (entertainment)

•    “Die Leute vom Domplatz”, Episode 1, 1979, German TV Series (entertainment)

•    “To Ride a White Horse”, Pamela Ford, 2015, novel

•    “Healing Symbols in Psychotherapy: A Ritual Approach”, Erik D. Goodwyn, 2016, psychology

•    “Harzer Pferdezucht im Spiegel der Geschichte”, Bernd Sternal, 2015, horse breeding

•    “Metallische Implantate in der Knochenchirurgie”, Erich Frank and Herbert Zitter, 1971, medicine (bone surgery)

• “Norse Magical and Herbal Healing”, Ben Waggoner, 2011, medicine (medieval medical text collection)

Sources citing both Merseburg Incantations:

• “Handbuch der germanischen Philologie”, Friedrich Stroh, 1985, philology

• “Götter und Kulte der Germanen”, Rudolf Simek, 2004, mythology

• A number of music songs, e.g., performed by In

Extremo, Saltatio Mortis, Corvus Corax or Ti-

betrea, music (entertainment)

• Merseburg Incantations Geocache,

https://opencaching.de/OC07B5, since 2004, geocaching puzzle

Although this is only a tiny snapshot of sources referring to the Merseburg Incantations, some tendencies are already visible. Whereas both Merseburg Incantations are cited in a number of books on mythology and philology (only one of each category is listed above), there are also some differences dependent on the spell contents, e.g., historiography for the first spell, and medicine for the second. Next to these reasonable source types, there are also unexpected findings like programming for the first spell, and horse breeding for the second. Both Merseburg Incantations are also often used for entertainment purposes in the form of novels, in music songs, or in movies. Also an interesting finding is a Geocaching puzzle guiding to a Geocache that is hidden in Merseburg.

The project is designed as a two-stage student internship, where Masters students of the humanities and computer science collaboratively work together in order to gain experiences for future digital humanities projects. The first step is data acquisition, which is performed by humanities students. Although web search engines, e.g., platforms with digitized contents such as Google Books or Internet Archive for textual sources, are the entry points for data acquisition, data extraction and structuring is done manually to ensure high data quality and to capture the purpose of the reference as precisely as possible. For example, we extract detailed information about the context in which a spell is cited, e.g., in the biography about Angelina Jolie the first Merseburg Incantation is referenced in a movie description of Beowulf, in which Angelina Jolie partook as a supporting actress. The second step of our project is the development of an interactive visualization system to support the dynamic exploration of the data collection. Therefore, computer science students design several visual interfaces that summarize different metadata information (e.g., a timeline to visualize the temporal impact, or a tag cloud to illustrate source types). One of the major functions of the system is a comparative view on the different contexts in which the Merseburg Incantations are cited together and each of them individually. Although the data is collected in a time limited project, the system is designed as a web-based crowdsourcing platform, so that the database can be extended after the project. A specific feature of the system is genericity, which makes it applicable to other texts in question. For example, creating a collection of citations of the Trierer Zaubersprüche or the Hildebrandslied will be possible, also in the form of a comparative analysis.

An interpretive approach using our proposed system will be complicated due to the fact that the collection contains serious scholarly as well as artistical references, and it is not possible to draw a clear line between such groups. Also, we consider each citation as equally relevant, which might not evolve a convincing representation of impact. While it is furthermore not possible to collect “all” citations, supporting hypotheses generation after distant reading analyses is not the prior purpose of our system. Our aim is rather to establish a starting point for exploring the far-reaching significance of the Merseburg Incantations—one of the most important written samples of German language, even the oldest and only known preserved text about Germanic Paganism in Old High German. With our project, we want to fulfill the responsibility to sustainably highlight this uniqueness.

Bibliography

Beck, W. (2003). Die Merseburger Zaubersprüche (Vol. 16).

Reichert Verlag.

Grimm, J. (1842). Über zwei entdeckte Gedichte aus der Zeit

des deutschen Heidenthums. Abhandlungen der Königlichen Akademie der Wissenschaften zu Berlin. Aus dem Jahre 1842.

Jankofsky, J. (2013). Merseburg: 1200 Jahre in 62 Porträts

& Geschichten. Mitteldeutscher Verlag.

Schmitt, M. (2011). Althochdeutsche Zaubersprüche als

Textzeugen einer Zeit des Übergangs zwischen germanischem Heidentum und sich etablierendem Christentum - Form und Inhalt frühmittelalterlicher Magiepraxis.

Magisterarbeit.

Schumacher, M. (2000). Geschichtenerzählzauber. Die 'Merseburger Zaubersprüche' und die Funktion der "his-toriola" im magischen Ritual. In R. Zymner (Ed.), Erzählte Welt - Welt des Erzählens. Festschrift für Dietrich Weber (pp. 201-215). Köln: Chora.
This panel explores the use of digital, rare book catalogs as platforms for collaboration and as sources of data to uncover patterns of book production and to offer new insights into the sociology of texts. Extractions from and additions to bibliographic data extend the catalog beyond its original use as a point of access and discovery. The use of catalogs as sources of bibliographic big data as well as the use of platforms that enable bibliographic record annotation allow us to reimagine the catalog’s trajectories of creation and utility. Considering the genealogy of the catalog, the panel will examine how research practices have both informed and been informed by catalogs since their inception and how the work of bibliography is reimagined in the catalog. Focusing on how limitations and aporia in the catalog can lead to critical making in the digital age, the panel will show how digital uses of the catalog currently enable mindful interrogation of catalog data and catalog making as well as consider possibilities for expanded use of rare book catalog data in the future. This consideration of the rare book catalog as a digital humanities project invites reassessment of legacy information architecture as well as the many hands that built the bibliographic structures on which so much of the work of the digital humanities rests.

In “Towards Speculative Catalogs,” Dawn Childress will open with a discussion of the transformative promise of the digital as we reconstruct catalogs in new forms and formats. To provide context for critical catalog constructions more broadly, Childress will highlight how bibliographies and catalogs have served as source material for research beyond that of points of access in both pre- and post-DH contexts, as well as consider how digital humanities use cases might differ from more analog approaches, whether qualitative or quantitative. In addressing these questions, Childress will explore the promise of applying current and emerging tools and standards (such as linked data, IIIF, PCDM, etc.) to the practice of interrogating bibliographic and catalog data. Childress will suggest how we might leverage these systems to record and analyze lacunae, erasures, and bias in capturing the bibliographic record and, drawing on Bethany Nowviskie’s notion of speculative collections, how these systems might support active reframing and interrogation by users.

In “‘The Technology of Shared Cataloging’: A Retrospective,” Molly O’Hagan Hardy will build on Childress’s remarks through a close look at the creation and re-creation of two rare book union catalogs: the English Short Title Catalog (ESTC) and the North American Imprints Program (NAIP). In 1981, in a Bibliographic Society of America Symposium from which the title of Hardy’s paper takes its name, William Todd wrote, “Perhaps we do not yet fully appreciate the situation, now rapidly materializing, whereby computers converse with each other in any mode, while the rest of us, mere mortals, stand mute before them.” Remarks like this, which abound in the excitement and trepidation expressed during the emergence of these rare book union catalogs, echo a similar exuberance and hesitancy around the transformation from MARC to linked data models. Examining what “machine readable” meant then and means now, Hardy will draw parallels between the current conversation around BIBFRAME and other such initiatives and those early efforts led by Robin Alston and Marcus McCorison to amass large amounts of special collections’ catalog data. She will then examine what it was these catalogs set out to capture and in what ways this work is being reimagined in the linked open data environment. She will do this through a close look at the American Antiquarian Society’s Printers’ File linked data project and its reliance on LCNAF and VIAF to merge MARC data with BIBFRAME. Ultimately, she will consider how such initiatives necessitate the reimagining of library and scholarly work, so those working on both sides of the reference desk are not left to “stand mute” before their creations. Hardy will point to examples of innovative uses of rare book catalog data for digital humanities projects. Such uses, Hardy will show, not only prove efficient and effective means of generating data, but they also productively unearth biases inherent in any information system.

Paige Morgan will conclude our presentations with “Searching For Common Ground: Modeling Bibliographic Data in Library and DH Contexts,” in which she will present the results of a survey of data use in DH projects examining the projects’ use and presentation of library data (including bibliographic data and data presented in digital collections platforms). This survey will focus on

•    whether each project includes bibliographic references;

•    how much granular detail any bibliographic references include,

•    whether or not such data is presented in the format of a specific model, and

•    the presence or absence of links to specific copies, whether in library or digital archive catalogs (such as HathiTrust, the ESTC, paywalled collections (ECCO, EEBO, etc.), or websites like Google Books and Project Gutenberg).

Gathering this data will allow Morgan to look for common priorities in the project creators’ use of library data; and to identify some of the assumptions that digital humanities has made about libraries and the bibliographic information that they produce. She will use the DH project survey data as the basis for a comparison with bibliographic ontologies (such as FRBRoo, BIBFRAME, Schema.org, and BIBO), and literature on challenges and best practices in bibliographic data modeling. Morgan will look for intersections, missed connections, and opportunities between digital humanities and cataloging work; and will consider how assumptions made in digital humanities elide the complexity and ongoing negotiations in the production of bibliographic data. In these projects, what drives the decisions to model (or not model) bibliographic data? How do the priorities of DH practitioners differ from those of library-based data creators? The increasing development of off-the-shelf tools, and the gradual growth of infrastructure for learning digital humanities skills and accessing data means that in many ways, it is possible for DHers, both new and experienced, to do more than before. However, increased access to materials and resources does not mean that the efforts of DH and library data communities will automatically complement each other. In FRBR, Before & After, Karen Coyle observes that library personnel working with data "have made little change in our approach to subject analysis in the last half-century, possibly because there isn't a clear direction for improving this aspect of our work." This paper will argue that similar ambiguities exist around the use of bibliographic data in DH projects, and that the apparent common ground of bibliographic data use is more complex than it appears.
Introduction

In the original Google Books Settlement Agreement in 2008 (Courant 2009), funds were to be set aside to create a research center that would enable researchers worldwide to accomplish data-mining and analysis on texts in the public domain and under copyright in a manner that was secure and compliant with appropriate U.S. copyright law. This did not happen, because the court rejected the agreement in 2011. Despite this, in 2011, the HTDL announced that Indiana University Bloomington and the University of Illinois at Urbana-Champaign would run the HTRC under a cooperative funding agreement with the HathiTrust Board of Governors and the University of Michigan. Since 2014, HTRC has made available as an active production service tools to analyze a set of out-of-copyright content equaling around 4.4 million volumes. In 2016, the HTRC plans to enable analysis of the entirety of the 15 million-volume corpus currently held by the HTDL, the largest digital academic library in North America.

HTRC and Non-Consumptive Research

The HTRC has developed a process to define and work within the concept of non-consumptive computational access to support the fair-use of the HTDL corpus as defined within the Google Books Settlement Agreement that was a part of the Authors Guild et al. v. Google Inc case.

Currently the HTRC defines the process for non-consumptive use of the HTDL corpus as:

Research in which computational analysis is performed on one or more books, but not research in which a researcher reads or displays.

Operationally, from the perspective of the HTRC research cyberinfrastructure, the HTRC defines non-consumptive research as:

That which requires that no action or set of actions on the part of users, either acting alone or in cooperation with other users over the duration of one or multiple sessions can result in sufficient information gathered from a collection of copyrighted works to reassemble pages from the collection.

This concept has been further refined in the course of the development of the HTRC Data Capsule (Zeng et al. 2014) for secure data analysis and the development

of the HTRC Workset Ontology (Jett et al. 2016) and has been codified in the recently released HathiTrust Research Center Non-Consumptive Use Research Policy (HTRC, 2016).

HTRC as Publisher

During the course of work with scholars using the HTRC tools and services to create derivative non-consumptive data sets, the Center has often taken on a set of the roles traditionally played by publishers. These data sets are reviewed by members of the HTRC staff for compliance with non-consumptive use standards prior to release to the authors.

As part of this work, the HTRC has offered as a service the capability to publish these non-consumptive, compliant data sets using a DOI scheme (Downie; 2015). This service enables the creation of new derivatives (Downie; 2015) of published non-consumptive compliant data sets.

A second benefit of opening access to these data sets is the ability to replicate current experiments that have been developed using the HTDL corpus and the HTRC tool set. From this standpoint the HTRC functions as a distant publisher of non-consumptive compliant data sets in support of new models of research inquiry.

Distant Publishing as Concept

Prior to defining the concept of distant publishing, it is first instructive to understand distant reading within the context of digital humanities. Distant reading was first codified in 2000 by noted humanist and scholar Franco Moretti:

Distant reading: where distance . . . is a condition of knowledge: it allows you to focus on

units that are much smaller or much larger than

the text: devices, themes, tropes - or genres and systems. And if, between the very small and the very large, the text itself disappears, well, it is one of those cases when one can justifiably say, less is more. (Moretti 2000)

Moretti later expanded the concept in his 2013 monograph of the same name (Moretti 2013). Much like Moretti's definition that focuses on enabling a broader view of the text, the distant publisher enables a broader view of data sets through bringing to bear the current corpus of computational tools for large-scale textual data mining and analysis. HTRC as a distant publisher is removed by at least one degree from the creator, and remains distinct from any standardized concept of publisher. Yet, data sets are published under the rubric of the HTRC, and these publications are freed from the constraints of copyright in this context due to their non-consumptive nature. Thus we define distant publishing as:

Publication of a non-consumptive data set outside of any standardized publishing construct, removed by x degree from the original creator, openly available to the community of scholars for replication and available for re-use in support of the advancement of knowledge.

This definition is one that the HTRC aims to further refine in the coming years. We welcome broader thoughts on this concept from those working to preserve open research data and the software that makes that data accessible for use in scientific experimental replication and re-use for the long-term benefit of the scholarly community.

Distant Publishing Use Cases

Currently the HTRC is developing models that support our current definition of distant publishing. These models are illustrated in several use cases, outlined below.

• Extracted Features Worksets - HTRC expects this concept to be further refined as we move toward the second round of HTRC Advanced Collaborative Support grants which will be funded in summer 2016. Our most progressive case for distant publishing at this point is leveraged through the publication and release of our main extracted features workset. The current workset is a prototype based on the 4.8 million volume public domain collection from the HTDL. Through 2016-17 this workset will be redefined to include more of the HTDL collection. From this initial workset publication we have seen further refinements of the workset by scholars such as Ted Underwood (Underwood et al. 2013), Colin Allen (Murdock, Zeng, and Allen 2016), and Matthew Wilkens (Wilkens 2013).

•    HT+Bookworm - The HathiTrust+Book-worm (HT+BW) project (2016) presents textual content through interactive visualization. Whereas HT+BW has previously been used in standalone contexts with pre-determined metadata, currently HT+BW is enabling scholars to analyze custom personal collections from within the larger corpus and the use of HT+BW as a supplement to other uses of the HTRC. This concept could eventually become a new possibility for derived workset publication in its own right.

•    HTRC Workset Ontology - Currently in development, the HTRC Workset Ontology is part of a collections data model by the Workset Creation for Scholarly Analysis project (HTRC 2016), an HTRC research initiative funded by the Andrew W. Mellon Foundation. The resulting HTRC Workset data model is designed to aid humanities scholars by helping them to describe selected portions of the HTDL corpus that serve as the objects of their research. The resulting worksets are persistent, citable, and can be assessed by other scholars for reuse in additional research processes.

Conclusion

Today's digital scholars are embracing new opportunities to explore their disciplines through the type of enhanced computational analysis that the HTRC provides. As the Center works to define emerging possibilities within the context of non-consumptive research, distant publishing will enable us to engage with the community of open data and open software publishers to ensure that our collections are accessible, open and available for the next generation of distant readers and their plans for new forms of scholarship.

Acknowledgment

The author would like to thank the Executive Management Team of the HTRC, J. Stephen Downie Co-Director, Beth A. Plale Co-Director, Beth Naymachchiv-aya, and John M. Unsworth, and all of the staff of the HathiTrust Research Center and the HathiTrust Digital Library for their contributions to the tools and services that make the concepts in this paper and the research of our users possible.

License

This article is licensed under CC BY 4.0.

Bibliography

Courant, P. N. (2009). “The Stakes in the Google Book

Search Settlement”. The Economists Voice 6 (9). Walter de Gruyter GmbH. doi:10.2202/1553-3832.1665.

Zeng, J., Ruan, G., Crowell, A., Prakash, A., and Plale, B.

(2014). “Cloud Computing Data Capsules for Non-Con-sumptiveuse of Texts”. In Proceedings of the 5th ACM Workshop on Scientific Cloud Computing - ScienceCloud 14. Association for Computing Machinery (ACM). doi:10.1145/2608029.2608031.

Jett, J., Cole, T. W., Maden, C., and Downie J. S.. (2016).

“The HathiTrust Research Center Workset Ontology: A

Descriptive Framework for Non-Consumptive Research

Collections”. Journal of Open Humanities Data 2 (March).

Ubiquity Press Ltd. doi:10.5334/johd.3.

HathiTrust Digital Library. (2016). “HathiTrust Research

Center Non-Consumptive Use Research Policy.” https://www.hathitrust.org/htrc_ncup.

Downie, J. S., Capitanu, B., Underwood, T., Organisciak,

P., Bhattacharyya, S., Auvil, L., Fallaw, C. (2015). “Extracted Feature Dataset from 4.8 Million HathiTrust Digital Library Public Domain Volumes”. HathiTrust Research Center. doi:10.13012/j8td9v7m.

Downie, J. S., Underwood, T., Capitanu, B., Organisciak,

P., Bhattacharyya, S., Auvil, L., Fallaw, C. (2015).

“Word Frequencies in English-Language Literature

1700-1922 (0.2)”. HathiTrust Research Center. doi:10.13012/J8JW8BSJ.

Moretti, F. (2000). “Conjectures on World Literature”. New

Left Review 1 (January): 57-58. https://newleftre-view.org/H/1/franco-moretti-conjectures-on-world-lit-

erature.

Moretti,    F.    (2013).    Distant    Reading.    Verso.

http://www.worldcat.org/oclc/813931586.

Underwood, T, Black, M. L., Auvil, L., and Capitanu, B.

(2013). “Mapping Mutable Genres in Structurally Complex Volumes”. In 2013 IEEE International Conference on Big    Data.    Institute of Electrical

& Electronics Engineers (IEEE). doi:10.1109/big-data.2013.6691676.

Murdock, J., Zeng, J., and Allen, C. (2016). “Towards Cultural-Scale Models of Full Text”. Arxiv.org. Arxiv.org.

http://arxiv.org/abs/1512.05004.

Wilkens, M. (2013). “Literary Geography at Corpus Scale”.

In Proceedings of Digital Humanities 2013. Alliance of

Digital    Humanities    Organizations.

http://dh2013.unl.edu/abstracts/ab-139.html.

Organisciak, P., Bhattacharyya, S., Auvil, L., Unnikrish-

nan, L., Schmidt, B., Shamim, M., McDonald, R., Downie, J., Aiden, E. (2016). “Adding Flexibility to Large-Scale Text Visualization with HathiTrust+Book-

worm”. In Digital Humanities 2016: Conference Abstracts.

Jagiellonian University & Pedagogical University, Krakow, pp. 854-856. http://dh2016.adho.org/ab-stracts/179.

HTRC. (2016). “Workset Creation for Scholarly Analysis - A HathiTrust Research Center Project Funded by the Andrew W. Mellon Foundation.” http: //worksets.htrc.illi-nois.edu/worksets.
Music manuscripts offer as much potential as text manuscripts for data mining and, as with resources like Google Books, there is a wealth of data available online. Currently the largest resource, the International Music Score Library Project (IMSLP), has more than 370,000 scores in its database. While many of these scores exist only in image-based formats, ongoing improvements in the area of Optical Music Recognition (OMR) allow for automatic conversion from images to symbolic representations, which include information such as instrumentation, key signature, time signature, and the notes' pitches, metrical positions, and durations. In order to exploit the research potential of these symbolic music databases, a representation that captures temporal relationships within the music is needed that highlights the structurally significant parts of the musical surface, while ignoring ornamentation.

This paper describes a representation that emphasizes the more structurally significant parts of the musical surface and de-emphasizes less significant parts, such as ornamentation, by integrating human domain expertise and data-driven approaches within a temporal machine learning model. The representation contains less information than the musical surface but more than corresponding chord labels, which discard information about musical texture and are too generalized to use for detailed similarity and classification tasks. The weighting for the various components of the musical surface is determined from an initial harmonic analysis. This harmonic analysis will be performed by a hierarchical model of chord labels and phrases, which will function like a “language model” in speech recognition. In music theory, phrase models describe musical phrases in terms of the tonic, predominant, and dominant functions (Laitz 2012). The inclusion of the expert domain knowledge expressed in the phrase function model helps to resolve the ambiguity between the musical surface and appropriate chord labels in the harmonic analysis, namely whether a particular chord is likely to occur in a particular part of a phrase. Taken in combination with OMR, this representation could be used to render searchable all available scanned music. These searches would not be limited to melody, as is the current state-of-the-art, but would also allow for querying by chord progressions and/or formal structures. The representation can also facilitate automatic hierarchical analysis of musical structures and provides a basis from which to undertake classifications and similarity tasks. Classification tasks include harmonic analysis or assessing the likelihood of a particular composer having composed a piece of unknown provenance, while similarity tasks include longitudinal studies over a composer's career or across composers.

Much of the work on analyzing the growing wealth of music data has been heavily influenced by text retrieval methods through their use of N-grams, sequences of N contiguous symbols. N-grams work well in modeling monophonic sequences, such as when directly applied to the musical surface for monophonic melody retrieval (Pickens, 2001) and for chord retrieval when the chords occur as distinct vertical units (Scholz et al., 2009). This has been demonstrated effectively on peachnote.com (Viro, 2011) with an N-gram viewer similar to the one Google makes available for Google Books. One significant area where N- grams have problems, however, is for more complex textures where the notes of chords are not played simultaneously, which is true of a large proportion of western art music since 1750. One way to address this problem is to automatically segment the musical surface into beat-length frames and treat the contents of each frame as a “chord” (Radicioni and Espositio, 2006), which is well suited to chorale textures but is problematic for arpeggiations or other textures where the chords notes don't occur simultaneously. Another approach is to analyze chord labels rather than the musical surface, such as the system of de Haas et. al (2011), although these are often not available.

The representation described in this paper takes a different approach, using a conditional random fields (CRFs) model for developing both a data-driven model, where all of the feature functions and potentials are learned from the data, and a hybrid data-driven/rule- driven approach, where domain knowledge “rules" are used to design feature and potential functions. Data for the purely data-driven approach comes from a domain expert-labeled dataset of

Mozart and Beethoven piano works in theme and variation form (Devaney et al. 2015). The rule-driven approach incorporates the rules presented in textbooks used in undergraduate music theory curricula, primarily Laitz (2012). This hybrid data- and rule- driven approach is motivated by previous work that demonstrated that a combination of data- and rule-driven models performed better than either approach alone on music analysis tasks (Devaney and Shanahan 2013).

This paper will also discuss the implications of this use CRFs for analyzing other metrically structured cultural products, such as poetry or song lyrics, as well as how this approach could be generalized to other digital humanities projects, specifically for relatively “data- poor” problems where there is a large amount of domain expertise that can be modeled, such as the study of narrative in natural language. More broadly, this work presents a vision of the digital humanities, where large-scale data-driven approaches are balanced by deep domain knowledge and the types of humanistic questions being asked require the development of more sophisticated technology than is currently available.

Technical Report.

Radicioni, D.P. and Esposito, R. (2006). Learning tonal harmony from Bach chorales. In Proceedings of the International Conference on Cognitive Modelling.

Scholz, R., Vincent, E., and F. Bimbot, F. (2009). Robust modeling of musical chord sequences using probabilistic N-grams. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing

(ICASSP). 53-6.

Viro, V. (2011). Peachnote: Music score search and analysis platform. In Proceedings of the International Society for Music Information Retrieval Conference (ISMIR). 35962.

Bibliography

de Haas, W.B., J.P. Magalhaes, R.C. Veltkamp, F. Wiering.

(2011). HarmTrace: Improving Harmonic similarity estimation using functional harmony analysis. In Proceedings of International Society of Music Information

Retrieval conference (ISMIR). 67-72.

Devaney, J., C. Arthur, N. Condit-Schultz, and K. Nisula.

(2015). Theme And Variation Encodings with Roman

Numerals (TAVERN): A new data set for symbolic music analysis. In Proceedings of the International Society for Music Information Retrieval (ISMIR) conference, 728-34.

Devaney, J., and D. Shanahan. (2014). Evaluating Rule- and

Exemplar-Based Computational Approaches to Modeling Harmonic Function in Music Theory Pedagogy. In

Proceedings of the 9th Conference on Interdisciplinary Musicology.

Laitz, S. G. (2011). The Complete Musician. Oxford: Oxford University Press, 3rd edition. Pickens, J. 2001. A survey

of feature selection techniques for music information retrieval.

Center for Intelligent Information Retrieval, Department of Computer Science, University of Massachusetts,
Introduction

We report on the work of a recent HathiTrust Research Center (HTRC) task force charged to draft an actionable, definitional Non-Consumptive Use Research Policy. As the research division of HathiTrust, the HTRC facilitates computational text analysis of materials in the HathiTrust Digital Library (HTDL) by adhering to a non-consumptive research paradigm. As the HTRC has integrated the text of the full HTDL corpus into its datastore, it has become increasingly important to clarify and codify the Center's policy for non-consumptive research. The task force, which consisted of copyright and scholarly communications librarians and representatives from HathiTrust operations and the HTRC, recommended a policy that clarifies acceptable researcher behavior and allowable exports from the HTRC Data Capsule (Plale, et al., 2015). This poster describes the task force's work to establish a Non-Consumptive Use Research Policy for the HTRC that aims to achieve the same goals as copyright itself: to promote progress in the discovery and spread of knowledge, without harming the commercial interests of authors, publishers, and other stakeholders. Background

While the concept of non-consumptive research has seeded the mission of the HTRC, the Non-Consumptive Use Research Policy task force sought to translate conceptual definitions of the term into practicable policy (Bhattacharyya, et al., 2015). When first used in 2010, the term was defined as “research in which computational analysis is performed on one or more Books, but not research in which a researcher reads or displays substantial portions of a Book to understand the intellectual content presented within the Book” (Amended Settlement Agreement: Authors Guild, Inc., et al., v Google Inc., 2009). Since then, legal scholar Matthew Sag and literary scholar Matthew Jockers have offered their own definitions and assessments, tending to favor instead the term non-expressive use (Sag, 2012; Jockers, 2013). Several recent court decisions pointed the task force toward the current legal understanding of non-consumptive research specifically and current interpretations of fair use broadly (Authors Guild v HathiTrust, 2014; Cambridge University Press v Becker, 2016; Fox News Network v TV Eyes, 2014). Additionally, the task force looked to existing access models for restricted data (ICPSR) as well as professional guidelines for non-consumptive research (Association of Research Libraries, 2012; Cox, 2015).

Policy Highlights

The group first created a framework drawing on fair use that, when paired with the HTRC technical infrastructure, would clarify non-consumptive access to the HTDL. This framework accounted for several considerations and safeguards:

• Mechanical data mining differs from researcher-driven computational text analysis, which requires interplay between scholar and text.

• Current case law suggests that it needs to be sufficiently difficult, but not strictly impossible, to reconstruct the expressive work (Authors Guild v Google Books, 2015).

• Users must agree that they will not treat HTRC tools as a reading application, and the tools should periodically remind them of this limitation.

• The HTRC must continue to block through technological measures and human review the export of protected textual data from the secure system.

The task force then drafted the HTRC Non-Consumptive Use Research Policy (HathiTrust, 2017). It

defines non-consumptive research as “Research in which computational analysis is performed on one or more volumes or textual objects in the HTDL, but not research in which a researcher reads or displays substantial portions of an in-copyright or rights-restricted work to understand the expressive content presented within that work.” Of key importance is the notion of substantial portion, which, according to the policy, is a portion of the work sufficient in quality or quantity to provide a substitute for access to the expressive content of the original text. The policy outlines acceptable in-capsule uses of corpus text that are limited to those which would facilitate scholarly text analysis, including checking results to refine algorithms. In addition to enumerating non-consumptive research practices— for example text extraction, textual analysis, and automated translation—the policy provides sample results that further model approved uses. These results, which may be exported from the HTRC Data Capsule, include non-binary, human-readable statistical summaries, derived results, keywords-in-context, and concordances that are not sufficient to reconstruct a substantial portion of the text.

Conclusions

The task force tailored the policy to address current infrastructure within the HTRC, both technical and human, as opposed to accounting for prospective updates to interface and design. As such, the policy is an iterable, living document that must be revisited as HTRC systems are further developed. Such technical developments, such as the HTRC’s exploration of machine-aided results verification to augment the current human-review system, will improve the scalability of the HTRC Data Capsule and may require updates to the policy. As more researchers interact with the HTRC Data Capsule, their use cases may prompt additional refinement of the policy, especially in the exemplar results it provides. The process followed in developing the policy, as well as the guidelines themselves, may be useful in other text mining research environments. They encourage an interpretation of non-consumptive research that values scholarship and intellectual progress, while still balancing the restrictions imposed by copyright law.

Acknowledgements

Task force members included: Aaron Elkiss, Brandon Butler, Bobby Glushko, Daniel G. Tracy, Eleanor Dickson, Robert McDonald, and Sandra McIntyre.

Bibliography

Amended Settlement Agreement: Authors Guild, Inc., et al., v Google Inc. (2009).

Association of Research Libraries (2012). Code of Best

Practices in Fair Use for Academic and Research Libraries. Available from: http://www.arl.org/storage/docu-ments/publications/code-of-best-practices-fair-use.pdf

(accessed 1 November 2016).

Authors Guild v Google Books (2015).

Authors Guild v HathiTrust (2014).

Bhattacharyya, S., Organisciak, P., and Downie, J. S.

(2015). “A fragmentising interface to a large corpus of digitized text.” Interdisciplinary Science Reviews, 40(1): 61-77.

Cambridge University Press v Becker (2016).

Cox, K. L. (2015). “ARL Issue Brief: Text and Data Mining and Fair Use in the United States,” Available from: http://www.arl.org/storage/documents/TDM-

5JUNE2015.pdf (accessed 1 November 2016).

ICPSR (2017). “Data Enclaves,” University of Michigan,

Available from:

http://www.icpsr.umich.edu/icpsrweb/con-tent/icpsr/access/restricted/enclave.html (accessed 1 November 2016)/

Fox News Network v TV Eyes (2014).

Jockers, M. (2013). Macroanalysis: digital methods and literary history. Champaign: University of Illinois Press.

HathiTrust (2017). “Nonconsumptive Use Research Policy.” Available from: https://www.ha-thitrust.org/htrc ncup (accessed 17 March 2017).

Plale, B., Prakash, A., and McDonald, R. (2015). “The Data Capsule for Non-Consumptive Research: Final Report.”

Sag, M. (2012). “Orphan Works as Grist for the Data Mill.” The Berkeley Technology Law Journal 27: 1503-50.
This paper discusses an exploratory project where a group of university academics, software developers, designers and librarians spent a week analysing a broad selection of the Wellcome Library's digital collections with an aim to explore new ways of conducting collaborative digital history research, identifying and documenting barriers and successes and also pointing to gaps in institutional support infrastructures.

Over the past two decades, significant quantities of cultural heritage have been digitised. Internet Archive has digitised over 20 million items in partnership with libraries and collections around the world. Google has digitised over 25 million books as Google Books. Alongside this the emergent field of digital humanities has sought to take advantage of new opportunities afforded by unprecedented access to collections. Numerous commentators and researchers have, in the words of Hayles (2015), voiced the opinion that “if there is an area of general agreement, it is the transformative potential of digital humanities for the humanities and for academic discourse” (see also Ogilvie, 2016; Alves, 2014).

Wellcome Library, part of the Wellcome Trust, is one of the world's most significant collections relating to health and medicine, with works ranging from posters and paintings to personal archives, printed books and packaging ephemera. Through its digitisation programmes, Wellcome is a major producer of digitised historical material and datasets, with over 40,000 digitised archives, nearly 100,000 digitised monographs and over 10,000 artworks, manuscripts, videos and reports. These have been made freely available under the most liberal licence possible, dependent on the copyright status of the material. In addition, IIIF image services (IIIF Consortium) including standardised image and presentation APIs, along with services for OCR and text search are provided (Chaplin, 2016).

A key aim for Wellcome is to enable new types of research and knowledge production; our mission is ‘to improve health for everyone by helping great ideas to thrive' (Wellcome Trust, 2016). One way we seek to do this is through enabling the exploration of cultural and social meanings of health in the past. Digitisation has undoubtedly increased use of our collections significantly and developed a large international audience, with over 50% of researchers accessing content from outside the UK. To better understand the users and usage of our digital collections we have carried out quantitative and qualitative research in collaboration with Prof. Pauline Leonard (Green and Andersen 2016). Our findings showed that beyond increasing access, the nature of collection usage has not changed significantly. The majority of researchers still access works within a single collection, page through digitised works in a linear fashion, make limited use of OCR search and little to no use of APIs. We are not yet seeing Hayle's “transformative potential” (2015) realised. To better understand this, we developed a number of hypotheses and research questions, which we categorise here under four loose umbrellas:

Lean working

In summarising the findings of multiple digital historical studies, Alves has argued that they ‘implicitly

confirm the efficiency of digital means... but also... that their application is, often, generally associated with expensive projects requiring extensive human resources with diverse skills' (2014). We questioned whether digital research is necessarily expensive or requiring of extensive resource. Drawing inspiration from commercial software development, we asked what is the Minimum Viable Product (Leanstack, 2016); can teams achieve meaningful research relatively inexpensively through an agile approach of iterative investigation and identification of emergent areas of interest rather than pre-defining fixed research questions.

Skills and knowledge

It could be argued that there is a skills or knowledge gap within traditional historical research communities which inhibits conducting digital research. However, we questioned if this was truly a barrier when evidence from citation patterns shows increases in collaborative approaches to digital humanities research (Nyhan and Williams, 2013). We questioned if extending the scope of traditional research teams to include commercial development partners could bring new insights and capabilities.

Crossing collections

Wellcome's digital collections include intensely heterogeneous material along with data sourced from multiple institutions. We questioned whether there are practical or technical barriers to break down divisions between collections and drawing from a range of content. Hitchcock (2013) has argued that ‘the lack of flexibility of the available digital tools [has] enabled only the effective utilization and analysis of quantitative sources or sources easily transformed into a quantitative format'. As many of Wellcome's collections are archival and contain large quantities of handwritten and pictorial material, we specifically wanted to explore the possibilities of digital for non-quantitative research, and research which still requires ‘close reading' of sources (Van Dijk, 1985).

Quality and consistency

Areas of enquiry in this umbrella included whether our digital collection is suitable in its scale and scope, and the quality and consistency of OCR and collection metadata. We also questioned if we we had the right kind of web services available and their usability for individuals with different levels of experience.

To explore and better understand these questions

we designed an experimental approach, adapting a

concept becoming familiar to cultural institutions -the ‘hack day’ - and extending it out into a week-long intensive R&D project, where small teams led by a mix of independent and academic researchers would work collaboratively to explore the Wellcome Library digital collections. Our participants included research staff from several UK universities, librarians and archivists, commercial designers and software developers. Through careful screening of participants, we selected researchers with shared enthusiasm for, but variable experience of digital historical research. This choice was deliberate in order to focus in on barriers relating to experience, skills and technical feasibility without confounding these variables with any reluctance to use digital methods. However, the enthusiasm for digital methods in the historical community is an important question and undoubtedly merits further investigation. Research areas were open-ended, with a focus on experimentation rather than production of finished work. However, we did agree broad areas, including handwritten records from a private asylum, 5000 Medical Officer of Health reports covering London from 1848-1972, 6,600 issues of the trade journal Chemist and Druggist and the 79,000 books digitised by the UK-MHL project (Wellcome Library, 2016).

Drawing from approaches of participative and cooperative inquiry (Reason and Bradbury, 2008) we embedded Wellcome staff in mixed teams of academic staff, developers and designers, positioning the teams as researchers of the collections, participants in a broader experiment of research production and observers and documenters of the experiment itself. We encouraged self-documentation by teams through use of project boards, blogs and wikis, conducted individual interviews with participants throughout the process and held daily reviews and a plenary session to discuss progress and reflect on the experiment.

During the week we produced multiple tools and visualisations, and also historical findings. These are documented on the project blog, along with the processes each team went through. This clearly demonstrated the potential of the collections, but also the barriers to working with them. Interestingly, the project using digitised archival material without OCR was one of the most successful - partly due to the synergy of the team members, but also the clarity of the challenge for the material. Two of the five projects seeded in the week continue to be developed, and we are continuing to provide support to the researchers leading them.

Feedback from participants in the project identified particularly the benefits of working in teams with mixed skillsets. Developers and library staff gained from acquiring greater understanding of research processes and interests, while researchers gained access to technical skills, and exposure to a different approach to digital working. One participant remarked: ‘I found working with other people, from a variety of different backgrounds, really generative - both for thinking about why people who do different kinds of work approach digital resources in different ways, and for thinking about my own research along new lines.’

The web services we provide for programmatic access to digital collections were identified as a particular barrier during the week. While the images and bibliographic metadata for our collections are exposed through an international standard, the Image Interoperability Framework (IIIF), this proved conceptually complex for developers to quickly prototype with, requiring combining and processing multiple JSON responses to access digital items. Adding further complexity to this, our collection OCR is available primarily through ALTO XML (Library of Congress, 2016) which our developers found challenging to process at scale.

As cultural heritage institution and a research funder, we are continuing to unpack the implications of the findings. As a library, we found great value in coproduction with research users so will be repeating a similar annual event, investigating different aspects of our collections. We have also identified particular issues related to the usability of our collections and added these to our development roadmap. As a funder we are considering options for increasing innovation by seeding early stage research through similar collaborative processes. To take this forward we will be running a series of pilot events through our interdisciplinary research residency, The Hub.

Bibliography

Alves, D. (2014). Guest Editor's Introduction: Digital Methods and Tools for Historical Research. International Journal of Humanities and Arts Computing. 8(1):1-12.

Bergold, J. and Thomas, S. (2012). Participatory Research Methods: A Methodological Approach in Motion. Forum Qualitative Sozialforschung / Forum: Qualitative Social

Research. 13 (1): Art. 30, http://nbn-resolv-ing.de/urn:nbn:de:0114-fqs1201302.

Chaplin, S. (2015). Why Creating a Digital Library for the

History of Medicine is Harder than You'd Think! Medical History. 60(01):126-129.

Green, A & Andersen, A. (2016). ‘Finding value beyond the dashboard', paper presented to MW2016: Museums and the Web 2016, Los Angeles, 6-9 April. Available at http://mw2016.museumsandtheweb.com/pro-posal/guaging-value-beyond-the-dashboard/ (Accessed: 1 Novemeber 2016).

Hayles, N. (2015) How We Think: Transforming Power and Digital Technologies. In Svensson, P. and Goldberg, D.

(eds), Between Humanities and the Digital. MIT; 2015. p. 503.

Hitchcock, T. (2013). Confronting the Digital. Cultural and Social History. 10(1):12-14.

IIIF Consortium. (no date). International image Interoperability framework. Available at: http://iiif.io/about/ (Accessed: 1 November 2016).

Library of Congress. (2016). About ALTO. Available at:

http://www.loc.gov/standards/alto/about.html (Ac

cessed: 5 April 2017)

Leanstack. (2016). Minimum viable product. Available at: https://leanstack.com/minimum-viable-product/ (Accessed: 1 November 2016).

Ogilvie, B. (2016). Scientific Archives in the Age of Digitization. Isis. 107(1): 77-85.

Nyhan, J. and Duke-Williams, O. (2016). Joint and multi-authored publication patterns in the Digital Humanities. Arche Logos. http://archelogos.hypotheses.org/103

Reason, P. & Bradbury, H. (2008). Introduction. In Reason, P. and Bradbury, H.(eds), The Sage handbook of action research. Participative inquiry and practice. London: Sage, pp.1-10.

Toon, E, Timmermann, C & Worboys, M. (2016). TextMining and the History of Medicine: Big Data, Big Questions?. Medical History. 60(02):294-296

van Dijk, T.A. (ed.) (1985). Handbook of discourse analysis: V. 1: Disciplines of discourse. 3rd ed. Orlando: Academic Press.

Wellcome Library. (2016). UK medical heritage library. Available at: https://wellcomelibrary.org/collec-tions/digital-collections/uk-medical-heritage-library/ (Accessed: 1 November 2016).

Wellcome Trust. (2016). Our Strategy. Available at: https://wellcome.ac.uk/about-us/our-strategy (Accessed: 1 November 2016).
Hermeneutica
In Hermeneutica, Geoffrey Rockwell and Stéfan Sinclair (2016) argue for an approach to the digital humanities that deemphasizes the tool and positivist notions of proof. Their proposed approach, also called Hermeneutica, champions tool accessibility over tool sophistication. Similarly, scholarly play is legitimated as a useful step in developing research questions and as a means to reconsider established notions within literary disciplines. The aim of Hermeneutica as a methodology seems to be the generation of interesting humanistic questions as much as the resolution of open questions.

Rockwell and Sinclair demonstrate the difference between Hermeneutica and typical DH approaches by quoting from Gary Wong's 2009 blog post:

[Typical DH] takes the worst part of the scientific papers (really really long sets of tabular data in the body of the text) and the worst part of papers from the humanities (really really complicated language where simple language would have done) and puts it in one. If this is what the cooperation of computational text analysis and traditional literary analysis yield, I

am scared.

Because Hermeneutica attempts to join the best parts of these fields, it has the potential to turn DH into a discipline that is more useful for the vast majority of non-DH humanists. It could be the means of accelerating the mainstreaming of DH methods and bringing us to the eventual point where all humanities are digital—a destination Claire Clivaz described succinctly (DARIAH, 2016).

Voyant
One feature that distinguishes Hermeneutica from many other DH approaches is its companion set of tools meant to demonstrate its application. Voyant Tools, now referred to simply as Voyant, is a web-based, modular suite of tools meant to be “worth thinking with” (Rockwell and Sinclair, 2016: 10, original emphasis). The goal is to accommodate playful exploration of text and sharing of corpora across the web. It is not designed as an industrial-grade text analysis tool, but as a “toy” that allows scholars to uncover new questions and gain new appreciation of texts.

Current limitations of Hermeneutica
A fundamental component of Hermeneutica is that the scholar views text through the lens of Voyant (or other computational text analysis tools), and then synthesizes that experience with their prior knowledge of the text and its milieu. A problem that Voyant addresses, but does not solve, is that many scholars who know the most about specific texts lack the technological skills that would be considered pre-novice in DH circles. Voyant allows everyone with a text and a browser to explore word frequencies, collocations, etc., but it presupposes that the text is available and clean enough for use. In order for Hermeneutica to appeal to non-DH humanities scholars, these issues of text availability and the lack of user skill must first be addressed.

On the issue of text availability, it is not often that scholars wish to analyze text that is rare or missing. More often they are interested in text that is protected by various copyright laws, which prohibit posting the text to public websites such as Voyant. Thankfully, in the Unites States at least, Google Books' recent court victory (Stohr, 2016) now permits scholars to publish online the analysis results derived from copyrighted texts, so long as the original text is not recoverable by the user. To this end Rockwell and Sinclair developed Voyant 2's “non-consumptive” mode which restricts access to tools that allow full-text views.

While such developments represent Rockwell and Sinclair's amenability to meet the ever-evolving needs of Hermeneuticans, accommodating users' lack of technology skill is beyond the scope of their involvement. For example, it is not reasonable to expect the Voyant developers to be concerned over issues of text acquisition or text preparation. Rather, those con-cerns—while critical to expanding the pool of potential Hermeneuticans—are issues of local implementation. Similarly, it makes sense that Voyant would offer the ability to link to a corpus after uploading the text, but uploading the text and keeping track of various versions of corpora is beyond the scope of Voyant. A local practice of adding some structure around the

Voyant suite ought to make Hermeneutica useful to a far greater audience than it is now.

Scaffolding
In the field of instructional design, such structure is called scaffolding. Specifically, scaffolding refers to the process of providing learners adequate introduction and examples before allowing them to attempt a task on their own (Bruner, 1978). For scaffolded Her-meneutica, DH-savvy professionals can work to acquire, clean, and upload text to Voyant (and other tools), and then provide public listings of the resulting corpora.

Examples of scaffolded Hermeneutica
We have implemented this scaffolded Hermeneu-tica approach in our Office of Digital Humanities beginning with the Cormac McCarthy Corpus Project (CMCP). The CMCP includes 13 Voyant corpora of McCarthy's 10 novels: one for the complete works, one for each novel, and two for novels (The Orchard Keeper and The Road) where the narration has been segregated from the dialogue. But the linchpin of scaffolded Hermeneutica is the CMCP's publicly-accessible website that organizes these Voyant corpora. The website is built on WordPress with the Pods content management plugin, and contains information on McCarthy's work, descriptions of Voyant (and other tools), and listings of links to the Voyant corpora. An essential feature of the website's structure is the ability to accommodate revisions to the current corpora as well as the addition of other tools in the future. Already, there is a non-Voyant sentence structure search tool attached as a beta-testing option.

A rough version of the Cormac McCarthy Corpus Project was presented at the 2015 conference of the Cormac McCarthy Society. The reaction to these tools being available for public use was strongly positive. One attendee referred to the website as “a game-changer.”

The same scaffolded Hermeneutica is being implemented on two other projects: Machado a longa distancia and The Modernist Short Fiction Project. Preliminary demonstrations of the approach have yielded similar reactions to what we observed with the CMCP. Non-DH scholars become excited rather than anxious when the digital analysis tools are scaffolded to provide them ready access. In fact, these demonstrations turn into play sessions where non-DH scholars repeatedly request for certain words to be added to the frequency charts and other Voyant panels.

Hermeneutica and Voyant represent the greatest potential for growth in DH not because they are the most technologically or theoretically advanced developments, but because they are the most accessible to non-DH scholars. Still, they don't quite reach the ground level of technology skills possessed by most researchers in the humanities. The scaffolded Herme-neutica approach proposed in this paper seems to span that gap to make Hermeneutica more accessible.

Bibliography
Bruner, J. S. (1978). “The role of dialogue in language acquisition.” In Sinclair, A., Jarvelle, R., J., and W. J.M. Levelt (eds), The Child's Concept of Language. New York: Springer-Verlag.

DARIAH (2016). My Digital Humanities - Part 1. YouTube.

https://www.youtube.com/watch?v=I8aRtHW3b6g

(accessed 1 November 2016).

Rockwell, G. and Sinclair, S. (2016). Hermeneutica. Cambridge: MIT Press.

Stohr, G. (2016). Google Book Project Can Proceed as Supreme Court Spurns Appeal. Bloomberg Politics. http://www.bloomberg.com/politics/articles/2016-04-18/google-book-project-can-proceed-as-top-u-s-court-spurns-appeal (accessed 1 November 2016).

Conclusion
Computationally-intensive research methods have seen increasing adoption among digital humanities scholars, but for scholars outside R1 institutions with robust computing environments, techniques like pho-togrammetry or text recognition within images can easily monopolize desktop computers for days at a time. Even at institutions with a research computing program, systems are configured for scientific applications, and IT staff may be unaccustomed to working with humanities scholars, particularly those who are not already proficient at using the command line. National compute infrastructures in North America (Compute Canada and XSEDE) are a compelling alternative, providing no-cost compute allocations for researchers and offering support from technical staff interested in and familiar with humanities computing needs. This workshop will start by introducing participants to Compute Canada and XSEDE, cover how to obtain a compute allocation (including for researchers outside of the US and Canada), and proceed through two hands-on tutorials on research methods that benefit from the additional compute power provided by these infrastructures: 1) photogrammetry using PhotoScan and 2) using OCR via Tesseract to extract metadata from images.

Photogrammetry
Photogrammetry (generating 3D models from a series of partially-overlapping 2D images) is quickly gaining favor as an efficient way to develop models of everything from small artifacts that fit in a light box to large archaeological sites, using drone photography. Stitching photographs together, generating point clouds, and generating the dense mesh that underlies a final model are all computationally-intensive processes that can take up to tens of hours for a small object to weeks for a landscape to be stitched on a high-powered desktop. Using a high-performance compute cluster can reduce the computation time to about ten hours for human-sized statues and twenty-four hours for small landscapes. Generating a dense cloud, in particular, sees a significant performance when run on GPU nodes, which are increasingly common in institutional HPC clusters and available through Compute Canada and XSEDE.

One disadvantage of doing photogrammetry on an HPC cluster is that it requires use of the command line and Photoscan's Python API. Since it is not reasonable to expect that all, or even most, scholars who would benefit from photogrammetry are proficient with Python, UC Berkeley has developed a Jupyter notebook that walks through the steps of the photogrammetry process, with opportunities for users to configure the settings along the way. Jupyter notebooks embed documentation along with code, and can serve both as a resource tool for researchers who are learning Python, and as a stand-alone utility for those who want to simply run the code, rather than write it. Indiana University, on the other hand, has developed a workflow using a remote desktop interface so that all the GUI capabilities and workflows of PhotoScan are still available. A python script is still needed so that the user may avail herself of the compute nodes, but the rest of the workflow is very similar traditional PhotoScan usage. Finally, both methods offload the processing the HPC cluster, allowing users to continue to work on a computer that might normally be tied up by the processing demands of photogrammetry.

The workshop will give participants hands-on experience creating a 3D model using two different approaches: first, by accessing the Photoscan graphical user interface on a virtual desktop running on XSEDE's Jetstream cloud resource; and second, by using a Jupy-ter notebook running on an HPC cluster.

OCR
Optical Character Recognition (OCR) is a tool used for extracting text from images and is perhaps most well known as a core technology behind the creation of the Google Books and HathiTrust corpora. OCR continues to open historical texts for analysis at large scale, fuelling a significant portion of research work

within the digital humanities to the point that it would

be difficult to think of the “million books problem” existing without this technology. While there are many OCR tools available the most popular tool that is also free and open source is Tesseract.

This portion of the workshop will also make use of Jupyter Notebooks to provide templates for learning the development process and that can then be taken away to speed development of future code. We will feature two projects for participants to practice with. A “traditional” OCR task that will have workshop participants processing images from the London Times in a demonstration of the improvements in OCR over the past few years and a task focusing on processing historical photographs to find text that can be added to the associated metadata to improve the searchability of an index.

Target Audience
We anticipate that this workshop will appeal particularly to scholars who work with cultural heritage materials (a field where photogrammetry is an increasingly common method for generating digital surrogates), as well as those who work with archival photographs, and scholars with large corpora of photographs. It will also be relevant for scholars who already engage in computational analysis of primary sources, who wish to increase the efficiency of their analysis by leveraging high-performance compute environments. No previous experience with HPC environments is necessary. This workshop can accommodate 25 participants.

Instructors
Quinn Dombrowski
Quinn is the Humanities Domain Expert at Berkeley Research Computing. At UC Berkeley, Quinn works with humanities researchers and research computing staff at Research IT to bridge the gap between humanities research questions and campus-provided resources for computation and research data management. She was previously a member of the program team for the Mellon-funded cyberinfrastructure initiative Project Bamboo, has led the DiRT tool directory and served as the technical editor of DHCommons. Quinn has an MLIS from the University of Illinois, and a BA and MA in Slavic linguistics from the University of Chicago.

Tassie Gniady
Tassie manages the Cyberinfrastructure for Digital Humanities group at Indiana University. She has a PhD in Early Modern English Literature from the University of California-Santa Barbara where she began her digital humanities journey in 2002 under the wing of Patricia Fumerton. She coded the first version of the NEH-funded English Broadside Ballad Archive, making many mistakes and learning much along the way. She now has an MIS from Indiana University, teaches a digital humanities course in the Department of Information and Library Science at IU, and holds regular workshops on text analysis with R and photogramme-try.

Megan Meredith-Lobay
Megan Meredith-Lobay is the digital humanities and social sciences analyst, as well as the Vice President, for Advanced Research Computing at the University of Briitsh Columbia. She holds a PhD from the University of Cambridge in medieval archaeology where she used a variety of computing resources to investigate ritual landscapes in early medieval Scotland Scotland. Megan has worked at the University of Alberta where she supported research computing for the Faculty of Arts, and at the University of Oxford where she was the programme coordinator for Digital Social Research, an Economic and Social Research Council project to promote advanced ICT in Social Science research.

John Simpson
John Simpson joined Compute Canada in January 2015 as a Digital Humanities Specialist and bringing a diverse background in Philosophy and Computing. Prior to Compute Canada, he was involved in a research-intensive postdoctoral fellowship focusing on developing semantic web expertise and prototyping tools capable of assisting academics in consuming and curating the new data made available by digital environments. He has a PhD in Philosophy from the University of Alberta, and an MA in Philosophy and BA in Philosophy & Economics from the University of Waterloo. In addition to his role at WestGrid, John is also a Member-at-Large of the Canadian Society for Digital Humanities (CSDH-SCHN), a Programming Instructor

with the Digital Humanities Summer Institute (DHSI), and the national coordinator for Software

        
            Introduction
            The relationship between the entropy of language and its complexity has been the subject of much speculation – some seeing the increase of linguistic entropy as a sign of linguistic complexification or interpreting entropy drop as a marker of greater regularity (Montemurro and Zanette 2011, Juola 2016, Bentz et al. 2017). Some evolutionary explanations, like the learning bottleneck hypothesis, argues that communication systems having more regular structures tend to have evolutionary advantages over more complex structures (Kirby 2001, Tamariz and Kirby 2016, Ferrer I Cancho 2017). Other structural effects of communication networks, like globalization of exchanges or algorithmic mediation, have been hypothesized to have a regularization effect on language (Kaplan 2014). 
            Longer-term studies are now possible thanks to the arrival of large-scale diachronic corpora, like newspaper archives or digitized libraries (Westin and Geisler 2002, Fries and Lehmann 2006, Lyse and Andersen 2012, Rochat et al. 2016). However, simple analyses of such datasets are prone to misinterpretations due to significant variations of corpus size over the years and the indirect effect this can have on various measures of language change and linguistic complexity (Buntinx et al. 2017). In particular, it is important not to misinterpret the arrival of new words as an increase in complexity as this variation is intrinsical, as is the variation of corpus size.
            This paper is an attempt to conduct an unbiased diachronic study of linguistic complexity over seven different languages using the Google Books corpus (Michel et al. 2011). The paper uses a simple entropy measure on a closed, but nevertheless large, subset of words, called kernels (Buntinx et al. 2016). The kernel contains only the words that are present without interruption for the whole length of the study. This excludes all the words that arrived or disappeared during the period. We argue that this method is robust towards variations of corpus size and permits to study change in complexity despite possible (and in the case of Google Books unknown) change in the composition of the corpus. Indeed, the evolution observed on the seven different languages shows rather different patterns that are not directly correlated with the evolution of the size of the respective corpora. The rest of the paper presents the methods followed, the results obtained and the next steps we envision.
            Method and Results
            We use the concept of kernel entropy (Buntinx et al. 2017), defined as the Shannon entropy measure applied on word occurrences distribution normalized on the kernel of a given corpus. To calculate this measure, the corpus is subdivided into yearly sub-corpora. Next, we then calculate the word occurrences for the words that are present in each sub-corpus for each year. These words form a set, called a kernel. The word frequencies are normalized on the kernel 
                
                    
                        K
                    
                 for each year 
                
                    
                        y
                    
                 and the formula of Shannon entropy (using napierian logarithm) is applied on these distributions providing a measure that can be compared diachronically with robustness to corpus size evolution and to noises. The kernel entropy of a kernel 
                
                    
                        K
                    
                 for the year 
                
                    
                        y
                    
                 is given by the formula:
            
            
                
            
            Where 
                
                    
                        
                            
                                N
                            
                            
                                K
                            
                        
                    
                 is the number of words composing the kernel and 
                
                    
                        
                            
                                f
                            
                            
                                i
                            
                            
                                K
                                ,
                                y
                            
                        
                    
                 the relative occurrence frequency of the word 
                
                    
                        i
                    
                 normalized on the kernel 
                
                    
                        K
                    
                 in the year 
                
                    
                        y
                    
                . The kernel entropy measure is computed for seven languages of Google Books corpora. 
                Figure 1 shows the kernel entropy variations normalized with respect to the average value (which change over the languages because kernels of different corpus also have different sizes).
            
            
                
            
            Figure 1: Normalized yearly kernel entropy evolution from 1800 to 2009 of seven Google Books corpora: British English, American English, French, German, Italian, Spanish and Russian.
            We observe that even if all the seven language have different patterns and inflection points, they tend generally to show an effect of negentropy with increasing years. We note that most languages have a crosspoint in 1905, except for the Russian language, showing variations particularly from 1920 to 1930. We present in Figure 2 the kernel entropy evolution for each language in comparison to the corpus size.
            
                
                    
                        
                            
                        
                    
                    
                        
                            
                        
                    
                
                
                    
                        
                            
                        
                    
                    
                        
                            
                        
                    
                
                
                    
                        
                            
                        
                    
                    
                        
                            
                        
                    
                
                
                    
                         Legend:
                        
                            (1) British / American English
                        
                        
                            (2) French / Italian
                        
                        
                            (3) Spanish / German
                        
                        
                            (4) Russian
                        
                        
                            Kernel Entropy: Blue
                        
                        
                            Size: Red
                        
                    
                    
                        
                            
                        
                    
                
            
            Figure 2: Yearly kernel entropy evolution and size evolution from 1800 to 2009 of seven Google Books corpora: British English, American English, French, German, Italian, Spanish and Russian.
            Google Books corpora may experience sudden changes in composition depending on the year. For example, the addition of scientific literature and medical journals (Pechenick et al., 2015). In this case, the words kernel distribution, even if it is robust because composed of the most stable words, can change for a year which is subject to a change of composition of the corpus. However, this effect is still reduced because the words appearing and disappearing during this transition phase are not taken into account. We observe that the entropy of the kernel seems not to be affected by the size variations of corpora and when it appear to be affected, the direction of variation is unpredictable.
            The British English and American English are the least affected languages by the negentropic effect. Their kernel entropy increases over time until 1960 (British English) and 1940 (American English). However, American English kernel entropy decrease quickly from 1940 to 1985. We observe that the obtained curve for the French language is similar to the one corresponding to the study of language evolution through 200 years of newspapers written in French despite a different kernel size (Buntinx et al. 2017). 
            Interesting inflection points are detected and should be poignant to specialists of the targeted language. We present in 
                Figure 3 the number of words in the kernel and inflections points for the seven languages.
            
            
                
                    Language
                    Number of words in the kernel
                    Inflection point 1
                    Inflection point 2
                
                
                    British English
                    82’332
                    1959
                    
                
                
                    American English
                    44’949
                    1931
                    1985
                
                
                    French
                    79’575
                    1825
                    1885
                
                
                    German
                    36’660
                    1850
                    1946
                
                
                    Italian
                    30’996
                    1983
                    
                
                
                    Spanish
                    25’582
                    1995
                    
                
                
                    Russian
                    5’123
                    1920
                    1988
                
            
            Figure 3: Number of words in the kernel and kernel entropy inflection points for the seven Google Books corpora: British English, American English, French, German, Italian, Spanish and Russian.
            Furthermore, it is possible to show the languages proximity in terms of kernel entropy evolution behavior through the determination of a distance based on kernel entropy correlations. A projection of the resulting matrix distance using PCA is presented in Figure 4.
            We observe that British English and American English are represented together to the left of the plan because they have a relative opposite pattern with respect to other languages. Russian is also particular because of the brutal effect of the negentropy observed between around 1920 and the sudden increase at the end of the 1980s. The last four languages, French, Spanish, German and Italian share a more similar behavior and are represented in the right-bottom part of the plan. 
            Although much more in-depth investigation must be done, it is reasonable to make the hypothesis of different internal and external factors for explaining these various patterns. The Russian case clearly invites to investigate correlations between linguistic policies during the Sovietic period and their actual effects of the Russian language.
            The similarity between French, German, Italian and Spanish pushes in the direction for similar processes of standardization, potentially due to linguistic convergence at national levels suppressing some regional particularities. In contrast, American and British English evolution is likely to be explained through the particular histories of the respective English-speaking populations and their relation to the rest of world. The progressive rise of English as a global language, spoken and written by many non-native speakers, is certainly playing a role in the shaping these particular curves. 
            
                
            
            Figure 4: PCA projection of distance matrix using kernel entropy correlation-based distance for Google Books corpora: British English, US English, French, German, Italian, Spanish and Russian.
        
        
            
                
                    Bibliography
                    C. Bentz, D. Alikaniotis, M. Cysouw and R. Ferrer-i-Cancho. The Entropy of Words—Learnability and Expressivity across More Than 1000 Languages. Entropy, 19(6), 275, 2017.
                    V. Buntinx, C. Bornet and F. Kaplan. Studying Linguistic Changes on 200 Years of Newspapers. 
                        DH2016, Kraków, Poland, July 11-16, 2016. 
                    
                    V. Buntinx
                        , F. Kaplan 
                        and A. Xanthos 
                        (Dirs.). Analyse multi-échelle de n-grammes sur 200 années d'archives de presse. Thèse EPFL, n° 8180, 2017.
                    
                    R. Ferrer-i-Cancho. Optimization models of natural communication. Journal of Quantitative Linguistics, 1-31, 2017.
                    U. Fries and H. M. Lehmann. The style of 18th century english newspapers: Lexical diversity. News Discourse in Early Modern Britain, pages 91–104, 2006.
                    P. Juola. Using the Google N-Gram corpus to measure cultural complexity. Literary and linguistic computing, 28(4), 668-675, 2013.
                    F. Kaplan. Linguistic capitalism and algorithmic mediation. Representations, 127 (1):57–63, 2014.
                    S. Kirby. Spontaneous evolution of linguistic structure-an iterated learning model of the emergence of regularity and irregularity. IEEE Transactions on Evolutionary Computation, vol. 5, no 2, p. 102-110, 2001.
                    G. I. Lyse and G. Andersen. Collocations and statistical analysis of n-grams. Exploring Newspaper Language: Using the Web to Create and Investigate a Large Corpus of Modern Norwegian. Studies in Corpus Linguistics, John Benjamins Publishing, Amsterdam, pages 79–109, 2012.
                    J. B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K. Gray, J. P. Pickett, 
                        D. Hoiberg, D. Clancy, P. Norvig, J. Orwant, S. Pinker, M. A. Nowak and E. Lieberman-Aiden. Quantitative analysis of culture using millions of digitized books. 
                        science, 
                        331(6014), 176-182, 2011.
                    
                    M. A. Montemurro and D. H. Zanette. Universal entropy of word ordering across linguistic families. PLoS One, 6(5), e19875, 2011.
                    E. A. Pechenick, C. M. Danforth and P. S. Dodds. Characterizing the Google Books corpus: Strong limits to inferences of socio-cultural and linguistic evolution. PloS one, 10(10), e0137041, 2015.
                    Y. Rochat, M. Ehrmann, V. Buntinx, C. Bornet and F. Kaplan. Navigating through 200 years of historical newspapers. In iPRES 2016, numéro EPFL-CONF-218707, 2016.
                    M. Tamariz and S. Kirby. The cultural evolution of language. Current Opinion in Psychology 8: 37-43, 2016.
                    I. Westin and C. Geisler. A multi-dimensional study of diachronic variation in british newspaper editorials. International Computer Archive of Modern and Medieval English, (26):133–152, 2002.
                
            
        
    

        
            
                Overview
                This panel intervenes in debates about interpretative methods that are often lumped under “reading,” and often measured by metaphors of scale, from close to distant. Mining data in vast corpora promises to transform literary history, and all scholars in the humanities rely upon online materials and tools. Yet many humanists stand aloof from DH because of its presumed hyperbolic claims, its apparent blurring of the detailed artifact (the domain of humanities), and to some, its collusion, post-critique, with neo-liberal globalization. Four panelists, collaborating for the first time, have encountered provocative concepts in each other’s work that moderate such stark oppositions between the humanist and the computational. The panelists’ previous studies have demonstrated the “payoff” or mutual instruction of DH and other recognized standards of scholarship. At the same time, in meticulous capture of language, style, form, and cultural production, the panelists highlight the limits that some champions of algorithms might want to leap in a single bound. Technological approaches to literary studies require highly curated corpora and modulation, often excision, of noisy results. Each paper addresses the loss inherent in categories and models, and the gain in tracing discarded, fuzzy, or inaccessible data. While our fields span centuries of Anglophone culture, our work advocates diversity, women’s history, and the DH community’s values of open access and collaborative technological innovation.
                Our papers address disruptions as well as continuities in observational scale as the tools and materials shift. Each panelist speaks from experience with a different dataset and her or his innovative approach to interpretation, touching on both language and technology. The first two speakers propose forms of mid-range reading to describe imaginative and interpretive leaps that scholars make between individual documents/texts and broader social forces; the second two address the reductions and abstractions that are necessary to the research project, themes common to all papers. As an archeologist of technologies, Wythoff rediscovers the concept of the gadget as an instance of human-inanimate interaction mirrored in DH. Booth expands on her response in 
                    PMLA to Franco Moretti’s 
                    Distant Reading, highlighting typologies as well as specific textual features in biographical nonfiction that enforce communal narratives. Allison, co-author on Stanford Lit Lab pamphlets associated with distant reading, proposes reductive reading, or explicit acknowledgment of necessary simplification, even of such ambitious problems as the nature of fictionality, which has been differently framed in studies by Piper, Underwood, and Eliot. While concepts of scale pervade claims for methods, Shore offers the approaches of construction grammar and corpus linguistics for particular insights into abstractions and categorizations. Shore, like Allison, calls on us to acknowledge the motivated reductions that are necessary to the research process. Our talks reflect on the history of technology and biographical representation, the forms of fiction and nonfiction, and the preconditions of selection and labeling of data—enduring issues in the humanities that become more telling with the expanding digital capacity to “read” at large and at speed.
                
                Grant Wythoff, Tacit computing and method in the humanities
                Humanistic research has always involved imaginative and interpretive leaps from the person to "the social," from the text to "the historical." Think for instance of the Annales school and its emphasis on the history of collective mentalities, or how Foucault described "discourse" by reverse-engineering historical ways of constituting knowledge. Today however, with the availability of big data, many of these forms of humanistic interpretation have become second nature. The search for broad cultural formations is implicit in the earliest steps we take in a research project, from keyword searches to frequency analyses. To what degree are certain kinds of historical argumentation baked into these mundane, day-to-day research activities, and what other kinds of cultural formations might we be overlooking?
                In my current book project, 
                    Gadgetry: A History of Techniques, I reconstruct the history of a discourse on technology. The book focuses on the many kinds of objects that were described as "gadgets" across the twentieth century, from dashboard gauges to atomic bombs, can-openers to smartphones. While “gadget” can be a placeholder for any kind of object, even imaginary ones, I argue that its evolving application to particular tools and techniques reveals important lessons about our relationship to technology.
                
                In this book, I explore the user's imagination of how their gadgets work. For example, a single iPhone contains over half the elements of the periodic table, extracted from almost every continent on the planet and compressed into a thin slab that allows the user to dip her toes into a river of collective affect generated by the social network of everyone she's ever met. This is a fantastically science-fictional experience that is now part of our everyday lives. But the emergence of new digital cultures, political movements, and forms of intimacy are all predicated on the unique habits each user adopts in order to understand these complex gadgets.
                For this book, I text mine archives of novels, magazines, and newspapers in order to explore the distinctly vernacular philosophies––the media theories from below––that emerge from users and their everyday practices. Using databases like the Corpus of Historical American English, Historical American Newspapers, and the Media History Digital Library, I proceed by collecting as many instances of the word "gadget" as possible and plugging them into categories of my own making based on how the term is applied: is the gadget handmade or mass produced, seen as important or a trinket, does the word refer to the entirety of the tool or a component within it, and so on. Because I have hand-coded this "dataset" and designated myself the categories into which I sort each instance of the word, the portrait that emerges of a discourse on technology could be described as entirely of my own making, as opposed to algorithmically-generated. But what really is the distance between these two categories of interpretation? In this talk, I will compare my digital methods to other methods throughout the history of the humanities that have attempted to paint a portrait of collective feeling.
                Alison Booth, Mid-Range reading: typologies, events, and discourse in a network of women’s biographies
                Although many investigate fictionality, scholars have attended much less to nonfiction and biography than to imaginative forms such as novels or film. Digital humanities (DH) expand the scale of literary history while building on existing maps of period, genre, and notable authors, with finding aids shaped by previous scholarship. Thus Andrew Piper’s impressive textual analysis, “Fictionality,” neglects life narrative. Collective Biographies of Women (CBW) accesses a corpus of 1270 English-language biographical collections published across centuries, in a feminist historical study of a “hidden collection” of nonfiction. CBW developed before Google Books glimmered on the horizon; we worked with WorldCat and analogue materials to rediscover such publications as 
                    Noted Negro Women (1893). Reversing the usual DH phases, I published the book before collaborating on an online resource. What could we learn about the trends in gender ideology already constructed by biographers and publishers, publication data, and contents? Biography is a model (i.e. reduction) of a life within networks of typologies based on social difference. Distant reading is not best adapted to ramifications within curated corpora, where there is no mystery of author or genre. We capture the distinctive form and rhetoric of biography (and changing meaning of words such as “noble”) in relation to such scenarios as inter-class contact or recognition of genius. Sentiment analysis or word vectors developed for large corpora of novels or newspapers would miss the mark. The actual dynamics of gender representation, for example, can hardly be captured as a grammatical binary or by rates of male or female agents per 300 words, while nationality is a shifting attribute across geopolitical and individual transformations.
                
                This paper extends Booth’s “Mid-range reading: not a manifesto” and builds on the findings from CBW’s method of mid-range reading as well as from the typologies and networks of women in the CBW database. CBW researchers are tagging discourse in biographies, such as first-person plural and plural proper names, and quantifying the distribution of types of events across versions of the same person or occupational types. Both scales of reading and typologies press upon ethics as well as epistemology: how to classify the individual text, or the character/person. Attention must be paid, yet cognition and knowledge depend on generalizations. CBW has focused on sets of books that document the ways women’s lives have been typologically interpreted. Our “sample corpora” range from all the books that include a short life of the saintly Victorian nurse, Sister Dora, and the distinct set of books that feature the famous adventuress, Lola Montez; other networks cluster around Queen Cleopatra, Frances Trollope, African Americans, women in medicine, Latinas, presenters (publishers, biographers), and others among the 8500 persons. A method we call mid-range reading uses the Biographical Elements and Structure Schema (BESS), a stand-aside XML schema (not TEI editing within the text file) that links element types (of stage of life, events, discourse, persona description, topos) to numbered paragraphs. BESS analyses, then, measure rates and distributions of element types across versions of lives sorted typologically by the contents of interrelated books. In 2018 we will obtain TEI files of remaining texts, with non-consumptive use of the copyright materials, through the HathiTrust Research Center. Becoming in this sense an archive as well as a testing ground for narrative theory of biography and network analysis across centuries of representation of women, CBW can demonstrate the comparative rewards of large-scale textual analysis and mid-range reading, and add to the understanding of biographical representation in many forms.
                Sarah Allison, Harnessing Pegasus: On Setting Reasonable Limits
                This paper takes up a theoretical question in digital humanities practice: how we understand the borders or boundaries of projects. “Reductive reading” is my term for critical methods that call attention to how they subordinate, or reduce, textual complexity. I argue that the explicit way with which DH research acknowledges this act of simplification creates an ethos of critical frankness. As Stephen Ramsay argues, code must “assert its utter lack of neutrality with candor, so that the demonstrably non-neutral act of interpretation can occur.” “Harnessing Pegasus” focuses on the poignant question of setting limits. How do researchers establish the right distance from the texts under consideration, or reduce the scope of their inquiry? Here, I consider how researchers set limits in three projects that aim to understand what we might take to be the constitutive feature of the novel: fictionality.
                It is axiomatic that the most irritating questions after a talk--but often also the best--are those that deal with a project’s limits. Researchers announce what they have done, and the members of the audience say, Ah, but why didn’t you do something else? This practice can help establish that one has taken a reasonable approach to a legitimate question in the field or open up future possibilities for research. It can also bring home the importance of narrowing one’s approach in order to answer a specific question, as in We 
                    didn’t do something else. We did the thing we did. In sharing work publically, researchers are called to account for the boundaries they have set--or, as it is often framed, that they have been forced to set.
                
                It is the latter attitude that interests me here, the moment when the scalar ambitions of distant reading meet pragmatic reality and intellectual justification. Mid-range reading leaves space to account for both. In this paper, I will consider three approaches to fictionality in literary history: by Andrew Piper in 
                    Cultural Analytics,
                     by Ted Underwood, Michael L. Black, Loretta Auvil, and Boris Capitanu in their work on genre in the HathiTrust Digital Library, and by Simon Eliot in his bibliographic work on trends in publishing, 1800-1914. In considering the way each project treats its limitations, I seek to create connections--bridges--across them. How do their definitions of fictionality intersect with Catherine Gallagher’s theoretical treatment of the topic, and what that that tell us about nonfictionality? In each of these three studies, non-fiction is represented by a discrete collection of texts. How does limiting the generic canon change the way we understand fictionality?
                
                Dan Shore, Other than Scale
                This paper explores the limits of the concept of scale in digital inquiry. Quantitative scholars in particular have naturally chosen scale as what sets their approach apart from other established methods. They speak of the computer as a “macroscope” that permits “macroanalysis.” Scholars counted things before computers, but computers let them count and compute lots of things. Contrasting themselves with close readers, distant readers propose, with the help of machines, to step back from the page to see more and see bigger. Claims of scalar difference are often quite quantitatively precise. Instead of offering a reading of a single novel, distant readers study the titles of 7,000 British novels from 1740-1850, or ask how not to read a million books, or search through the 60,237 full texts in EEBO TCP I and II. For nearly all quantitative analyses of texts, the authors could tell the reader exactly how many words they count in how many documents, in light of sophisticated metrics and models.
                Talk of scale in the digital humanities has not been simply ill advised. In spite of quantitative precision, we don’t really know what we talk about when we talk about scale. Individual texts are much bigger than are usually acknowledged. Even when bag-of-words approaches are forthright about discarding word order and syntax, they rarely itemize what they are discarding. What has been characterized as an increase in scale can be more accurately described as the sacrifice of one sort of information for another. The point is not to oppose reductionism, but to be fully aware of what is being reduced.
                Scalar conceptualization of digital tools and methods has tended to crowd out other, non-scalar distinctions. Some, like experimental design, theories of evidence, and falsifiability (an account of what it would mean to be wrong) should be more prominent in the conversation. I’ll focus on concepts - abstraction, categorization, hierarchy - that are central to meaning and linguistic creativity across languages. Here I turn to the insights of construction grammar and corpus linguistics to suggest further possibilities for investigation. The bigram 
                    thought leader is two words, but it is also a single compound noun, the meaning of which can’t be fully predicted from the meaning of its parts. How big is it? An abstract construction like 
                    Once upon a time… [] and they lived happily ever after may be only ten words, and yet as big as the fairy tale that fills its blank. How long is it? The relevant distinctions in these examples are not scalar in any simple sense, and the methods for understanding them cannot be captured by distance or proximity. I start with linguistic examples at the level of the utterance, propose a few ways forward for qualitative and quantitative inquiry, and close by suggesting how the non-scalar distinctions at work in construction grammar might be relevant for specifically literary questions such as genre and narrative form.
                
            
        
        
            
                
                    Bibliography
                    Allison, Sarah. 
                        Reductive Reading: A Syntax of Victorian Moralizing. Baltimore: Johns Hopkins University Press, forthcoming 2018.
                    
                    Allison, Sarah. “Other People’s Data: Humanities Edition,” 
                        Cultural Analytics, Dec. 8, 2016. 
                        
                    
                    Bode, Katherine. “The Equivalence of ‘Close’ And ‘Distant’ Reading; Or, toward a New Object for Data-Rich Literary History.” 
                        Modern Language Quarterly 78, no. 1 (March 1, 2017): 77–106,
                         
                        .
                    
                    Booth, Alison. 
                        How to Make It as a Woman: Collective Biographical History from Victoria to the Present. Chicago: University of Chicago Press, 2004.
                    
                    Booth, Alison. “Mid-Range Reading: Not a Manifesto.” 
                        PMLA 132: 3 (May 2017): 620-27.
                    
                    Burguiere, Andre. 
                        The Annales School: An Intellectual History. Trans. Jane Marie Todd. Ithaca, NY: Cornell University Press, 2009.
                    
                    Eliot, Simon. “Some Trends in Book Publishing, 1800-1914” in John O. Jordan and Robert L. Pattern (eds.), 
                        Literature in the Marketplace. Cambridge: Cambridge University Press, 2003.
                    
                    Eliot, Simon, and Jonathan Rose, eds. 
                        A Companion to the History of the Book. Malden, MA: Wiley-Blackwell, 2009.
                    
                    Gallagher, Catherine. 
                        Nobody’s Story: The Vanishing Acts of Women Writers in the Marketplace, 1670-1820. Berkeley, U. of California P, 1994. 
                    
                    Gallagher, Catherine. “The Rise of Fictionality.” 
                        The Novel. Ed. Franco Moretti, Vol. 1. Princeton: Princeton UP, 2006. 336-63. 
                    
                    Goldberg, Adele E. 
                        Constructions at Work: The Nature of Generalization in Language. New York: Oxford UP, 2006.
                    
                    Goldberg, Adele E. 
                        Constructions: A Construction Grammar Approach to Argument Structure. Chicago: U of Chicago P, 1995.
                    
                    Hancher, Michael. “Re: Search and Close Reading,” in 
                        Debates in the Digital Humanities 2016. University of Minnesota Press, 2016. 118–38.
                         
                        .
                    
                    Langacker, Ronald W. 
                        Cognitive Grammar: A Basic Introduction. Oxford: Oxford UP, 2008.
                    
                    Nunberg, Geoffrey, Ivan A. Sag, and Thomas Wasow. “Idioms,” 
                        Language 70 (1994): 491–538.
                    
                    Piper, Andrew. “Fictionality.” 
                        Journal of Cultural Analytics, December 20, 2016.
                         
                        .
                    
                    Robertson, Stephen, and Lincoln Mullen. “Digital History &amp; Argument White Paper – Roy Rosenzweig Center for History and New Media.” November 13, 2017. https://rrchnm.org/argument-white-paper/.
                    Shore, Daniel. 
                        Cyberformalism: Histories of Linguistic Forms in the Digital Archive. Baltimore: Johns Hopkins UP, forthcoming 2018.
                    
                    Shore, Daniel. “Shakespeare’s Constructicon,” 
                        Shakespeare Quarterly 66.2 (2015): 113-136.
                    
                    Smith, Barbara Herrnstein. “What Was Close Reading? A Century of Method in Literary Studies,” 
                        Minnesota Review 87 (2016): 57–75.
                    
                    Underwood, Ted. “Distant Reading and the Blurry Edges of Genre. ” 
                        The Stone and the Shell. 22 Oct. 2014.
                    
                    Underwood, Ted. “Understanding Genre in a Collection of a Million Volumes, Interim Report.” Figshare.
                         
                        
                    
                    Wythoff, Grant. 
                        Gadgetry: A History of Techniques, in progress.
                    
                    Wythoff, Grant. 
                        The Perversity of Things: Hugo Gernsback on Media, Tinkering, and Scientifiction. University of Minnesota Press, 2016.
                    
                
            
        
    

        
            
                Introduction
                In the last decades, quantitative linguistics (following exact and social sciences) has developed a considerable number of statistic methods providing an insight into measurable phenomena of natural language. Although to a lesser extent, it also applies to the analysis of diachronic changes. The basic tool used to assess the chronology of linguistic changes is a rather effective yet simple method of trend search: the examined features are analyzed by mapping the frequency of the described phenomenon on a timeline (Ellegård, 1953). This timeline-centric visualization has become a standard in several studies and corpus tools. The most spectacular example is the corpus of several dozens of million of documents (mainly in English) accompained by the service Google Books Ngram Viewer 
                    , which, according to its authors, enables to examine changes taking place not only in the language, but also in culture (Michel et al., 2011).
                
                A significant drawback of simple graphic representation of the trend, and hence of mapping the frequency of the examined phenomenon on a timeline, is a tacit assumption that the researcher knows in advance which elements of the language are subject to change. In other words, the method of plotting and inspecting the trend may be applied only to verify hypotheses stipulated earlier by traditional diachronic linguistics. For example, knowing in advance that Polish underwent the gradual replacement of the inflected ending 
                    -bychmy with 
                    -byśmy, one might draw the trendline and capture the dynamics of that change. Although many prominent diachronic works were based upon such an approach (Biber, 1988; Hilpert and Gries, 2009; Hu et al., 2007; Reppen et al., 2002; Smith and Kelly, 2002; Can and Patton, 2004), one might be interested in trend search without any 
                    a priori selection of the analyzed linguistic changes to be traced.
                
                Needles to say, 
                    some selection of potential language change predictors (e.g. a predefined set of words, certain collocates, etc.) will always be the case. The strategy followed in this study was to analyze a considerably large set of 1,000 most frequent words without any further filters, with the assumption that some of them will turn out stronger than others. Arguably, in such a big set one should find a few dozen of function words, and a vast majority of content words. Another remark that has to be formulated here is that the language change cannot be reliably separated from the stylistic drift (e.g. in literary taste of the epoch). This fact is well known in stylometric approaches to style (“stylochronometry”), where the actual changes in the system and stylistic signals of, say, the predominant genres are usually difficult to be told apart.
                
            
            
                Supervised classification and the timeline
                The most natural strategy to assess the discriminative power of numerous features at a time is to apply one of the multivariate methods. Since none of the out-of-the-box techniques is suitable to analyze temporal datasets, some tailored approaches have been proposed, e.g. using a variant of hierarchical clustering (Hilpert and Gries, 2009; Hulle and Kestemont, 2016). These methods, however, share a common drawback, namely their results are by no means stable. Also, no cross-validation can be considered a downside.
                To assess these issues, an iterative procedure of automatic text classification was applied (Eder and Górski, 2016). Its underlying idea is fairly simple: first, we formulate a working hypothesis that a certain year – be it 1835 – marks a major linguistic break. The procedure randomly picks 
                    n text samples written before and after the assumed break; the samples then go into the 
                    ante and 
                    post subsets. In this study, a period of 20 years before and after the assumed break was covered (with an additional gap of 10 years), 500 text samples of 1,000 tokens were harvested into each of the subsets. To give an example: for the year 1835, 500 random samples covering the time span 1810–1830 were picked into the first subset, and another 500 samples from the years 1840–1860 into the second subset. Next, the both subsets are randomly divided into two halves, so that the training set and the test contain 500 samples representing two classes (
                    ante and 
                    post). Then we train a supervised classifier – in this case, Nearest Shrunken Centroids – and record the cross-validated accuracy rates. Then we 
                    dismiss the original hypothesis, in order to test new ones: we iterate over the timeline, testing the years 1836, 1837, 1838, 1839, … for their discriminating power. The assumption is simple here: any acceleration of linguistic change will be reflected by higher accuracy scores.
                
            
            
                Data and results
                The above procedure has been applied to the Corpus of Historical American English (COHA), containing ca. 400 million tokens and covering the years 1810–2009 (Davies, 2010). The corpus provides the original word forms, part-of-speech tags, and the base word forms (lemmata). The results reported below were obtained using the lemmatized version of the corpus.
                
                    
                        
                        Fig. 1: Language change accelleration in the American English corpus: classification accuracy over the years 1835–1985.
                    
                
                In Fig. 1, the classification accuracy rates for the COHA corpus were shown (1,000 most frequent lemmata, NSC classifier). As one can observe, the scores obtained for each period are higher than the baseline, suggesting the existence of a temporal signal. Obviously, the higher the scores the faster the evolution of language, since the distinction between the period before and after the tested breakpoint is simpler for the classifier. More important, however, is the fact that the scores are not even: the signal becomes stronger in some periods, clearly indicating an acceleration of the language change. One of the stylistic breaks takes place in the 1870s (i.e. after the Civil War), the other in the 1920s (in the period of prosperity before the Great Depression); the third peak is not fully formed yet, even if one can observe an acceleration of language change at the end of the 20th century. Needless to say, any attempts at finding direct correlations between historical events and stylistic breaks are subject to human prejudices, and therefore might introduce substantial bias to the results. Even though, the coincidence of the three observed peaks and a few major changes in the American culture is rather striking.
            
            
                Distinctive features
                The results obtained in the above experiment seem to be rather promising. However, from the perspective of historical linguistic even more interesting is the question which features (words) were responsible for an given change observed in the dataset. It has been reported in several stylometric studies that attributing authorship relies, in most cases, on many features of individually very weak discriminative power. In the context of language change, a similar question can be asked: is it but a few characteristic words that trigger the change, or, alternatively, is the stylistic drift spread across dozens of tiny changes in word frequencies?
                To answer the above question, one has to extract the features that played a prominent role in telling apart the 
                    ante and 
                    post periods as described above. The features exhibiting the biggest variance (that is, the overall impact on the results) are shown in Fig. 2. An important caveat needs to be formulated here: the plot shows the outputted weights from the classifier, rather than direct word frequencies. The underlying assumption is that the features’ weights (to be precise: the 
                    a posteriori probabilities returned by the classifier) reflect the changes in actual word frequencies as combined with all the other frequencies being analyzed.
                
                
                    
                        
                        Fig. 2: Seventy-six linguistic features (words) that contributed considerably to the stylistic drift.
                    
                
                The main stylistic breaks form, again, three peaks that culminate roughly in the same years as presented in Fig. 1. What is counterintuitive, however, it is the fact that the features tend to form sinusoidal waves of their periodical discrimination power. Interestingly, these high impact features turned to be very frequent words that usually occupy the top positions on the frequency list. The 25 words of the highest discrimination strength are as follows:
                
                    the, 
                    and, 
                    week, 
                    that, 
                    ’s, 
                    last, 
                    is, 
                    be, 
                    of, 
                    it, 
                    we, 
                    i, 
                    to, 
                    was, 
                    mr., 
                    our, 
                    my, 
                    been, 
                    not, 
                    u.s., 
                    you, 
                    new, 
                    upon, 
                    there, 
                    has
                
                Even more interesting are individual trajectories of the high-impact words. In Fig. 3, one can observe a collinearity of function words: 
                    the, 
                    and, 
                    that, 
                    is, 
                    been, as opposed to the possessive 
                    ’s. These function words seem to have impacted the language change at the turn of the 19th century. The possessive, in turn, contributed to the evolution of language roughly at the times of the Prohibition. (Again, this is not to say that any direct links between function words and actual events in history should be drawn).
                
                
                    
                        
                        Fig. 3: Function words of the highest impact on the stylistic drift.
                    
                
                A different pattern is revealed by the “social” words, especially personal pronouns. It has been shown that these words, e.g. 
                    I, play prominent role in betraying someone’s personality (Pennebaker, 2011). Certainly, traces of such individual profiles will hardly be noticeable at the level of the entire corpus. One might try, however, to formulate some claims of the “personality” of the population in the function of time, in the belief that some general trends in culture might be reflected in the corpus. In Fig. 4 a few personal pronouns and some contractions have been shown. As one can see, their moderate presence over the past decades turns into a very hight impact at the end of the 20th century. Moreover, the impact of the words 
                    I and 
                    my seems to grow even further… These and similar examples provide a counterintuitive evidence that a language change might be due to minute differences in the usage of very common words.
                
                
                    
                        
                        Fig. 4: High impact personal pronouns and contractions.
                    
                
            
            
                Conclusions
                In this paper, we used a tailored stylometric method to assess the question of language change over time. Our chosen technique proved to be useful indeed, especially when one focuses on tracing the very linguistic features that were responsible for the observed change. The results were counterintuitive, since the set of strongly discriminative features contained common function words, which formed sinusoidal trajectories of their impact over time. One of the most interesting aspects of language development – overlooked in numerous existing studies – is the question of the dynamics of linguistic changes. Our study corroborated the hypothesis that epochs of substantial stylistic drift are followed by periods of stagnation, rather than forming purely linear trends.
            
            
                Acknowledgements
                This research is part of project UMO-2013/11/B/HS2/02795, supported by Poland’s National Science Centre.
            
        
        
            
                
                    Bibliography
                    
                        Biber, D. (1988). 
                        Variation Across Speech and Writing. Cambridge University Press.
                    
                    
                        Can, F. and Patton, J. M. (2004). Change of writing style with time. 
                        Computers and the Humanities, 
                        38(1): 61–82.
                    
                    
                        Davies, M. (2010). The Corpus of Historical American English (COHA): 400 million words, 1810-2009 
                        .
                    
                    
                        Eder, M. and Górski, R. L. (2016). Historical linguistics’ new toys, or stylometry applied to the study of language change. In, 
                        Digital Humanities 2016: Conference Abstracts. Kraków: Jagiellonian University &amp; Pedagogical University, pp. 182–84 
                        .
                    
                    
                        Ellegård, A. (1953). 
                        The Auxiliary Do: The Establishment and Regulation of Its Use in English. Stockholm: Almquist &amp; Wiksell.
                    
                    
                        Hilpert, M. and Gries, S. T. (2009). Assessing frequency changes in multistage diachronic corpora: Applications for historical corpus linguistics and the study of language acquisition. 
                        Literary and Linguistic Computing, 
                        24(4): 385–401.
                    
                    
                        Hu, X., McLaughlin, J. and Williamson, N. (2007). Syntactic Positions of Prepositional Phrases in the History of Chinese: Using the Developing Sheffield Corpus of Chinese for Diachronic Linguistic Studies. 
                        Literary and Linguistic Computing, 
                        22(4): 419–34.
                    
                    
                        Hulle, D. van and Kestemont, M. (2016). Stylochronometry and the Periodization of Samuel Beckett’s Prose. In, 
                        Digital Humanities 2016: Conference Abstracts. Kraków: Jagiellonian University &amp; Pedagogical University, pp. 393–95 
                        .
                    
                    
                        Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Pickett, J. P., Hoiberg, D., et al. (2011). Quantitative analysis of culture using millions of digitized books. 
                        Science, 
                        331(6014): 176–82.
                    
                    
                        Pennebaker, J. W. (2011). 
                        The Secret Life of Pronouns: What Our Words Say About Us. New York: Bloomsbury Press.
                    
                    
                        Reppen, R., Fitzmaurice, S. M. and Biber, D. (eds). (2002). 
                        Using Corpora to Explore Linguistic Variation. Amsterdam: John Benjamins.
                    
                    
                        Smith, J. A. and Kelly, C. (2002). Stylistic Constancy and Change Across Literary Corpora: Using Measures of Lexical Richness to Date Works. 
                        Computers and the Humanities, 
                        36(4): 411–30.
                    
                
            
        
    

        
            Digital archivists tend to disagree about the place of paratexts. Whereas 
                Google Books often scans texts at such a low resolution that anything but printed words are difficult to discern, Andrew Stauffer’s 
                
                    Book Traces
                 project and Steven Olsen-Smith and Peter Norberg's 
                
                    The Melville Marginalia Project
                 aims to identify individual copies of nineteenth- and early-twentieth-century books in libraries by highlighting their unique marginalia and inserts. Illustrations, advertisements, marginalia, boards, and decorative initials—the effluvium of the print form—does not digitize easily. Moreover, in terms of library and information science, paratexts resist standard means of categorization. Paratexts are problematic because they offer an exception rather than a type. To scholars, they often seem extraneous or even detrimental to the written texts they accompany. Marginalia, for instance, simultaneously defaces and compliments a text. Advertisements are a distracting and commercial accretion to an artwork. And yet, all paratexts provide necessary context for understanding the complexity and fullness of print history. The question I will address in this paper is how archivists ought broadly to understand paratexts, and how specifically should they treat nineteenth-century illustrations.
            
            Numerous digital archives have taken on the task of scanning, categorizing, and tagging illustrations (
                e.g. the 
                William Blake Archive, the
                 Cervantes Project, Cardiff University’s 
                Illustration Archive), and yet the purpose and constraints of this task remain unfixed. In fact, Julia Thomas notes in her recent 
                Nineteenth-Century Illustration and the Digital (2016), that owing to the uniquely important role of context for these paratexts—usually the book or periodical—"the digital might appear an alien environment for historic illustrations." While the role of the digital image archive concerned with illustrations remains unsettled, recent scholars have used the affordances of the digital archive to open up new avenues for curation and exploration. Using as a case study a digital archive that I direct and edit titled 
                Visual Haggard, a 
                NINES indexed and peer reviewed archive that contextualizes and improves access to the illustrations of Victorian novelist H. Rider Haggard (1856 - 1925), I argue that digitizing illustrations must be inclusive. 
            
            I will consider the problem of inclusion and exclusion in digital archive curation. As paratexts, illustrations are lumped together with a number of visual objects that initially accompanied fictions. For this reason I explain the necessity of using metadata to differentiate illustration types. The large decorative initials which appear in many nineteenth-century texts, but originated in medieval manuscripts, are less illustrations of the text than embellishments. However, their ideological function is significant and multifold. Similarly, advertisements were often in conversation with serialized fictions—whether thematically or stylistically. In this paper I discuss strategies to enable digital image archivists committed to creating an authentic encounter with the history of print to avoid ignoring or marginalizing these types of unique and difficult paratexts. 
        
    
This project digitally remediates selections from Henry Mayhew’s seminal survey of poverty, London Labour and the London Poor (weekly serial, 1850-52; 2 vols, 1851; 4 vols, reissued with additions, 1861). Print culture offers rich opportunities for digitization, and I engage with current conversations in Victorian studies about developing sustainable, scholar-driven digital resources produced outside of commercial ventures like GALE and ProQuest (Stauffer; Fyfe). For example, Felluga argues for resisting skeuomorphic forms of knowledge production by capitalizing on new technologies (44), while Wisnicki points to the success of nineteenth-century digital archives which emphasize materiality, are user-friendly, and are sustainable with little funding (984). I engage with work on the potential of digitization as a means of navigating and preserving Victorian print media (Mussell), on canonical scholarship of hypermedia and digital textualities (Landow; Bolter; Gaggi; McGann; Hayles), and on recent work on the hypermedia reading and cognitive hypertext comprehension (Ryan; Shang; Medina-Medina et al.) to examine how interactive media offers a model of digitally representing Victorian print culture which resists the form of the printed book. The theoretical framing of this project addresses the following questions: 1) What potential is there for interactive hypertext to represent the materiality, dissemination, and readerships of serialized print texts? 2) How do Victorian texts speak to the concerns of digital publishing and materiality in a public digital humanities context?As a seminal text in the history of urban poverty, London Labour has been subject to continuous remediation and repurposing. It was attributed to Henry Mayhew, but was the collaborative product of multiple actors, including multiple journalists, editors, publishers, readers, and the London poor themselves. The multimodal remediation of pieces of this collaborative work as a newspaper column, independent serial, expensive volume set, stage play, reading series, 'unfashionable' novel, and twenty-first century selected print editions, destabilizes readings of London Labour as an authoritative encyclopedic resource of nineteenth-century culture and poverty. Its volume and irregular production history make it difficult to engage with (Roddy et al. 482; Schroeder), a common problem when studying periodicals (Robson). In its current digitized forms on Google Books and Internet Archive, London Labour is represented using facsimile scans of its four volumes, which, despite being comprehensive and searchable, are not subject to bibliographic control, and feature unreliable OCR. However, London Labour invites a nonlinear and interconnected model of reading that speaks to, and even anticipates, hypertextual forms of reading. My project features twenty-five of Mayhew’s articles, selected according to two pressing themes: first, waste collection and recycling, and second, the publication history and the materiality of books in London Labour. These selections contain scholarly annotations and are hyperlinked in connecting pathways. They can be read chronologically, or by selecting keywords and thematic clusters. The selections also compare plain-text transcripts to nineteenth-century facsimiles, emphasizing the text’s uneven production. Drawing from Scanlan’s reading of the poor as producers circulating in an alternative waste economy, and Price’s work on the material cycles of paper and books in Mayhew’s work, my project interrogates the following carrefours: first, the intersections between Mayhew and the street-folk in the urban space of Victorian London represented in London Labour; and, more broadly, the intersection between Victorian print and the digital humanities. Receiving feedback will position me to expand this open-access resource as a collaborative project, and to develop its pedagogical potential for training students in digital methods and tools through the lens of print studies.
Travel literature represents a rich source of information about the past, and has been of increasing interest in the scholarly community (c.f. Salzani & Tötösy de Zepetnek, 2010; Belgum et al., 2018). The Travelogues project aims to study what we can learn from past views of foreign regions, cultures and religions in the light of present-day challenges such as mass tourism, migration and globalization. Comprising a team of historians, librarians and data scientists, Travelogues applies a transdisciplinary approach, combining quantitative and qualitative analytical methods to study a large-scale corpus of German language travelogues.The project focuses on German-language holdings of the Austrian National Library printed between 1500 and 1876, including 167,570 digitized volumes. Those volumes have previously been digitized and processed by Optical Character Recognition (OCR) in the Austrian Books Online project—a public-private partnership of the Library and Google Books. In order to facilitate analysis of this vast and heterogenous collection, the project faces a number of challenges. The first challenge is to compile a corpus that includes as many travelogues from the inventory as possible. The second challenge is the profiling of the corpus at scale, analyzing it specifically for aspects of geographical coverage, salient terms over time and intertextuality. Finally, the key challenge lies in the identification of depictions of otherness in the corpus, and evolution of those depictions over time.Travelogues is a work in progress. In this paper we will describe our results so far—how we created the corpus and approached the task of profiling—and present our plans for the upcoming steps required for a detailed analysis of the corpus.As the intellectual basis for the project, historians first established a definition for this project’s use of the term travelogue. In the context of the project, a travelogue is defined as a specific form of media that records the experiences of a factually undertaken journey. Applying this definition, we created a balanced ground truth of digitized travelogues and non-travelogues (works that could belong to any other genre). To account for variations in the data such as document length, OCR quality or orthographic differences, we created separate ground-truth datasets for different time periods: the 16th, 17th and 18th centuries and 1800–1876. This is a manual and time-consuming process involving several steps, including keyword and metadata searches of the collection, cleansing and enrichment of heterogeneous metadata and comparisons with both contemporary and modern travelogue bibliographies (e.g. Chatzipanagioti-Sangmeister, 2006; Griep & Luber, 1990; Treue, 2014; Yerasimos, 1991). Every book we identified using this method was independently verified by a historian and a librarian.Based on the ground truth for each period, we trained and evaluated different machine learning algorithms to classify works as either travelogues or non-travelogues. This evaluation was done using five-fold cross-evaluation on a training set and a validation set. Using the best-performing approach, we applied the models for each time period and classified all documents not part of the ground truth (a total of 161,522 books). Our model returned a confidence score, essentially quantifying the likelihood that a given document is or includes a travelogue. The top 200 findings for each time period (800 in total) were then manually evaluated by our domain experts in order to confirm the validity of the automated results. Our process revealed 345 previously-unknown volumes of travelogues that were not listed as travelogues in any  bibliography we consulted so far, nor could they be found using conventional metadata search methods (e.g. searching for different spelling variations of the German word for travel). Although the 345 newly-discovered travelogues did not noticeably differ in their content from the previously-known canon, their materiality was particular. A large number of them were originally published as part of larger documents (such as serial publications, collected volumes or diaries) that, due to the lack of metadata in the library system, usually cannot be found with the traditional methods of the humanities as described above. Although we did not segment the documents into smaller entities (e.g. pages or chapters) for classification (Underwood et al., 2013), this shows that our methodology leads to robust results concerning documents that are only partially considered part of a genre, as in our case with travelogues. Additionally, we successfully proved that our methodology can expand traditio­nal bibliographic research and help save time for domain experts. We have already described our classification task in detail (Rörden et al., 2020). Our next steps concern the analysis and historical contextualization of the corpus. We are currently creating a searchable index of the entire document inventory (travelogues as well as non-travelogues) to enable exploratory searches. Key exploration scenarios include plotting the number of travelogues published over time, optionally while filtering by various facets (such as authors, publishers, keywords or catalogue subject classifications); and exploring salient terms that feature more prominently in travelogues over non-travelogues, or in travelogues of a particular period vs. in those of another. We have also begun to generate maps of travelogues’ geographical coverage by performing Named Entity Recognition, and resolving coordinates against the GeoNames gazetteer. Furthermore, we have taken the first steps to analyze intertextual relations between documents (e.g. Dörr & Kurwinkel, 2014; Rajewsky, 2002), experimenting with a mix of approaches including n-gram fingerprinting (c.f. Stein, 2007) and paragraph vectors for document representation (Le & Mikolov, 2014). This has been combined with clustering and text passage alignment using the BLAST-based text reuse algorithm developed by Vierthaler and Gelein (2019). By applying the algorithm corpus-wide, we hope to learn more about the relationships between the documents and their authors, as well as how descriptions of and references to people, places and customs propagate through literature over extended time periods. Preliminary results seem promising, and have revealed what appear to be potential candidate cases of previously undocumented text-reuse. However, deeper analysis of the candidates and refinement of the methods are still ongoing. This method for the detection of intertextual relations is also a promising tool for clustering and relating works, not only based on literal title-strings but also on indexed full-texts, thus following the suggested implementation of the International Federation of Library Associations and Institutions Library Reference Model (IFLA LRM) into library catalogs (Decourselle et al., 2015; Rafferty 2015; Riva et al., 2017). With our method, we were able to create the largest curated corpus of German-language travelogues to date—3,595 volumes, 345 of which were, to the best of our knowledge, not previously identified or findable as travelogues—thus proving that methods like ours can successfully expand the classic bibliographic methodology of the humanities and library sciences. We have made first steps towards enabling the interactive exploration of a number of relevant properties of the corpus. The next year will be dedicated to addressing the main research goals: the identification of intertextual relations between the travelogues in our corpus to deepen our understanding of how they depended on each other, and what this tells us about the circulation of knowledge, stereotypes and prejudices. This will ultimately lead to the question of how notions of otherness were depicted, how and why they changed over time, and what conclusions this allows concerning today’s perceptions and biases. We have already published large parts of our corpus under a Creative Commons license. Beyond the goals of our own project, we feel that the open availability of the corpus marks a significant contribution to the research community at large, and will invite further scholarship and collaboration around this exciting resource.

        
            
                Introduction
                
                    Digital humanities is an area of research and teaching at the intersection of computing and the disciplines of the 
                    humanities . Developing from the fields of humanities computing, humanistic computing, 
                     Humanistic Computing, Proceedings of the IEEE, Vol. 86, No. 11, November, 1998, Pages 2123-2151. and digital humanities praxis 
                    
                        http://dhpraxisf13.commons.gc.cuny.edu/tag/dhpraxis/
                     digital humanities embraces a variety of topics, from curating online collections to data mining large cultural data sets. Digital humanities (often abbreviated DH) currently incorporates both digitized and 
                    born-digital materials and combines the methodologies from traditional humanities disciplines (such as 
                    history , 
                    philosophy , 
                    linguistics , 
                    literature , 
                    art , 
                    archaeology , 
                    music , and 
                    cultural studies ) and social sciences 
                    
                        "Digital Humanities Network". 
                        University of Cambridge. Retrieved 27 December 2012
                     with tools provided by 
                    computing (such as 
                    data visualisation , 
                    information retrieval , 
                    data mining , 
                    statistics , 
                    text mining , 
                    digital mapping ), and 
                    digital publishing . As well, related subfields of digital humanities have emerged like 
                    software studies , platform studies, and 
                    critical code studies . Digital Humanities also intersects with 
                    new media studies and 
                    information science as well as 
                    media theory of composition and 
                    game studies , particularly in areas related to digital humanities project design and production. 
                
                
                    
                        
                        Example of research which includes the use of digital methods: network analysis as an archival tool.
                    
                     League of Nations archives, United Nations Office in Geneva. Network visualization and analysis published in Grandjean, Martin (2014). 
                        "La connaissance est un réseau". 
                        Les Cahiers du Numérique
                        10 (3): 37–54. Retrieved 2014-10-15.
                    
                
            
            
                Areas of inquiry
                Digital humanities scholars use computational methods either to answer existing research questions or to challenge existing theoretical paradigms, generating new questions and pioneering new approaches. One goal is to systematically integrate computer technology into the activities of humanities scholars, 
                    
                        Opportunities/tabid/57/Default.aspx "Grant Opportunities". 
                        National Endowment for the Humanities, Office of Digital Humanities Grant Opportunities. Retrieved 25 January 2012.
                     as is done in contemporary empirical 
                    social sciences . Such technology-based activities might include incorporation into the traditional 
                    arts and 
                    humanities disciplines use of text-analytic techniques; 
                    GIS ; 
                    commons-based peer collaboration ; and interactive games and 
                    multimedia .
                
                Despite the significant trend in digital humanities towards networked and multimodal forms of knowledge, spanning social, visual, and haptic media, a substantial amount of digital humanities focuses on documents and text in ways that differentiate the field's work from digital research in 
                    Media studies, 
                    Information studies, 
                    Communication studies, and 
                    Sociology. Another goal of digital humanities is to create scholarship than transcends textual sources. This includes the integration of 
                    multimedia, 
                    metadata and dynamic environments. An example of this is 
                    The Valley of the Shadow project at the 
                    University of Virginia, the 
                    Vectors Journal of Culture and Technology in a Dynamic Vernacular at 
                    University of Southern California or 
                    Digital Pioneers projects at Harvard.
                
                A growing number of researchers in digital humanities are using computational methods for the analysis of large cultural data sets such as the 
                    Google Books corpus. 
                     Roth, S. (2014), Fashionable functions. A Google ngram view of trends in functional differentiation (1800-2000), International Journal of Technology and Human Interaction, Band 10, Nr. 2, S. 34-58 (online: 
                        ).
                     Examples of such projects were highlighted by the Humanities High Performance Computing competition sponsored by the Office of Digital Humanities in 2008, 
                     Bobley, Brett (December 1, 2008). 
                        "Grant Announcement for Humanities High Performance Computing Program". 
                        National Endowment for the Humanities. Retrieved May 1, 2012.
                     and also by the Digging Into Data challenge organized in 2009 
                    
                        "Awardees of 2009 Digging into Data Challenge". 
                        Digging into Data. 2009. Retrieved May 1, 2012.
                     and 2011 
                    
                        "NEH Announces Winners of 2011 Digging Into Data Challenge". 
                        National Endowment for the Humanities. January 3, 2012. Retrieved May 1, 2012.
                     by NEH in collaboration with NSF, 
                     Cohen, Patricia (2010-11-16). 
                        "Humanities Scholars Embrace Digital Technology". 
                        The New York Times (New York). 
                        ISSN  
                        0362-4331. Retrieved 2012-06-07.
                     and in partnership with 
                    JISC in the UK, and 
                    SSHRC in Canada. 
                     Williford, Christa; Henry, Charles (June 2012). 
                        "Computationally Intensive Research in the Humanities and Social Sciences: A Report on the Experiences of First Respondents to the Digging Into Data Challenge". 
                        Council on Library and Information Resources. 
                        ISBN  
                        978-1-932326-40-6.
                    
                
            
            
                
                    Environments and tools
                
                Digital humanities is also involved in the creation of software, providing "environments and tools for producing, curating, and interacting with knowledge that is 'born digital' and lives in various digital contexts." 
                     Presner, Todd (2010). 
                        "Digital Humanities 2.0: A Report on Knowledge". 
                        Connexions. Retrieved 2012-06-09.
                     In this context, the field is sometimes known as computational humanities. Many such projects share a "commitment to 
                    open standards and 
                    open source." 
                     Bradley, John (2012). "No job for techies: Technical contributions to research in digital humanities". In Marilyn Deegan and Willard McCarty (eds.). 
                        Collaborative Research in the Digital Humanities. Farnham and Burlington: Ashgate. pp. 11–26 [14]. 
                        ISBN  
                        9781409410683.
                    
                
            
            
                History
                Digital humanities descends from the field of humanities computing, of computationally enabled "formal representations of the human record," 
                     Unsworth, John (2002-11-08). 
                        "What is Humanities Computing and What is not?". 
                        Jahrbuch für Computerphilologie
                        4. Retrieved 2012-05-31.
                     whose origins reach back to the late 1940s in the pioneering work of 
                    Roberto Busa . 
                    
                     Svensson, Patrik (2009). 
                        "Humanities Computing as Digital Humanities". 
                        Digital Humanities Quarterly
                        3 (3). 
                        ISSN  
                        1938-4122. Retrieved 2012-05-30.
                    
                     Hockney, Susan (2004). "The History of Humanities Computing". In Susan Schreibman, Ray Siemens, John Unsworth (eds.). 
                        
                            Companion to Digital Humanities
                        . Blackwell Companions to Literature and Culture. Oxford: Blackwell. 
                        ISBN  
                        1405103213.
                    
                
                Other aspects of digital humanities were descended from the 
                    IRIS
                    Intermedia project on hypertext at 
                    Brown University in the 1980s.
                
                The 
                    Text Encoding Initiative, born from the desire to create a standard encoding scheme for humanities electronic texts, is the outstanding achievement of early humanities computing. The project was launched in 1987 and published the first full version of the 
                    TEI Guidelines in May 1994. 
                     Hockney, Susan (2004). "The History of Humanities Computing". In Susan Schreibman, Ray Siemens, John Unsworth (eds.). 
                        
                            Companion to Digital Humanities
                        . Blackwell Companions to Literature and Culture. Oxford: Blackwell. 
                        ISBN  
                        1405103213.
                    
                
                In the nineties, major digital text and image archives emerged at centers of humanities computing in the U.S. (e.g. the 
                    Women Writers Project, 
                    
                        
                        
                            Women Writers Project
                        , Brown University, retrieved 2012-06-16
                     the 
                    Rossetti Archive, 
                    
                        Jerome J. McGann (ed.), 
                        
                            Rossetti Archive
                        , Institute for Advanced Technology in the Humanities, University of Virginia, retrieved 2012-06-16 
                     and 
                    
                        The William Blake Archive
                    
                    
                        
                            Morris Eaves, Robert Essick, and Joseph Viscomi (ed.), 
                            
                                The William Blake Archive
                            , retrieved 2012-06-16
                        
                    ), which demonstrated the sophistication and robustness of text-encoding for literature. 
                     Liu, Alan (2004). 
                        "Transcendental Data: Toward a Cultural History and Aesthetics of the New Encoded Discourse". 
                        Critical Inquiry
                        31 (1): 49–84. 
                        doi: 
                        10.1086/427302. 
                        ISSN  
                        0093-1896. Retrieved 2012-06-16.
                     The Blake archive, in particular, was designed by its editors to take advantage of "the syntheses made possible by the electronic medium" and thus accomplish an "editorial transformation" in the publication of Blake's work which was, from the author's hands, multimedia. 
                    
                        "Editorial Principles". 
                        The William Blake Archive. Retrieved 17 December 2014.
                    
                
                The terminological change from "humanities computing" to "digital humanities" has been attributed to 
                    John Unsworth and Ray Siemens who, as editors of the monograph 
                    A Companion to Digital Humanities (2004), tried to prevent the field from being viewed as "mere digitization." 
                     Fitzpatrick, Kathleen (2011-05-08). 
                        "The humanities, done digitally". 
                        The Chronicle of Higher Education. Retrieved 2011-07-10.
                     Consequently, the hybrid term has created an overlap between fields like rhetoric and composition, which use "the methods of contemporary humanities in studying digital objects," 
                     Fitzpatrick, Kathleen (2011-05-08). 
                        "The humanities, done digitally". 
                        The Chronicle of Higher Education. Retrieved 2011-07-10.
                     and digital humanities, which uses "digital technology in studying traditional humanities objects". 
                     Fitzpatrick, Kathleen (2011-05-08). 
                        "The humanities, done digitally". 
                        The Chronicle of Higher Education. Retrieved 2011-07-10.
                     The use of computational systems and the study of computational media within the arts and humanities more generally has been termed the 'computational turn'. 
                     Berry, David (2011-06-01). 
                        "The Computational Turn: Thinking About the Digital Humanities". 
                        Culture Machine. Retrieved 2012-01-31.
                    
                
                In 2006 the 
                    National Endowment for the Humanities (NEH), launched the Digital Humanities Initiative (renamed Office of Digital Humanities in 2008), which made widespread adoption of the term "digital humanities" all but irreversible in the United States. 
                     Kirschenbaum, Matthew G. (2010). 
                        "What is Digital Humanities and What's it Doing in English Departments?". 
                        ADE Bulletin (150).
                    
                
                Digital humanities emerged from its former niche status and became "big news" 
                     Kirschenbaum, Matthew G. (2010). 
                        "What is Digital Humanities and What's it Doing in English Departments?". 
                        ADE Bulletin (150).
                     at the 2009 
                    MLA convention in Philadelphia, where digital humanists made "some of the liveliest and most visible contributions" 
                     Howard, Jennifer (2009-12-31). 
                        "The MLA Convention in Translation". 
                        The Chronicle of Higher Education. 
                        ISSN  
                        0009-5982. Retrieved 2012-05-31.
                     and had their field hailed as "the first 'next big thing' in a long time." 
                     Pannapacker, William (2009-12-28). 
                        "The MLA and the Digital Humanities" (The Chronicle of Higher Education). 
                        Brainstorm. Retrieved 2012-05
                    
                
            
            
                
                    Organizations and Institutions
                
                The field of digital humanities is served by several organisations: 
                    The Association for Literary and Linguistic Computing (ALLC), the 
                    Association for Computers and the Humanities (ACH), and the 
                    Society for Digital Humanities/Société pour l'étude des médias interactifs (SDH/SEMI), which are joined under the umbrella organisation of the 
                    Alliance of Digital Humanities Organizations (ADHO). The alliance funds a number of projects such as the 
                    Digital Humanities Quarterly, supports the 
                    Text Encoding Initiative, the organisation and sponsoring of workshops and conferences, as well as the funding of small projects, awards and bursaries. 
                     Vanhoutte, Edward (2011-04-01). 
                        "Editorial". 
                        Literary and Linguistic Computing
                        26 (1): 3–4. 
                        doi: 
                        10.1093/llc/fqr002. Retrieved 2011-07-11.
                    
                
                ADHO also oversees a joint annual conference, which began as the ACH/ALLC (or ALLC/ACH) conference, and is now known as the 
                    Digital Humanities conference.
                
                CenterNet is an international network of about 100 digital humanities centers in 19 countries, working together to benefit digital humanities and related fields. 
                    
                    
                        "About". 
                        CenterNet. Retrieved June 16, 2012.
                    
                     Caraco, Benjamin (1 January 2012). 
                        "Les digital humanities et les bibliothèques". 
                        Le Bulletin des Bibliothèques de France
                        57 (2). Retrieved 12 April 2012.
                    
                
            
            
                
                    Criticism and controversies
                
                An edited text, 
                    'Debates in the Digital Humanities' (2012) has identified a range of criticisms of digital humanities: 'a lack of attention to issues of race, class, gender, and sexuality; a preference for research-driven projects over pedagogical ones; an absence of political commitment; an inadequate level of diversity among its practitioners; an inability to address texts under copyright; and an institutional concentration in well-funded research universities'. 
                    
                        
                    
                
                The literary theorist 
                    Stanley Fish claims that the digital humanities pursue a revolutionary agenda and thereby undermine the conventional standards of "pre-eminence, authority and disciplinary power." 
                     Fish, Stanley (2012-01-09). 
                        "The Digital Humanities and the Transcending of Mortality". 
                        The New York Times (New York). Retrieved 2012-05-30.
                    
                
                There has also been some recent controversy amongst practitioners of digital humanities around the role that race and/or identity politics plays in digital humanities. Tara McPherson attributes some of the lack of racial diversity in digital humanities to the modality of 
                    UNIX and computers, themselves. 
                    
                        
                     An open thread on DHpoco.org recently garnered well over 100 comments on the issue of race in digital humanities, with scholars arguing about the amount that racial (and other) biases affect the tools and texts available for digital humanities research. 
                    
                        
                     This is a current source of debate within the digital humanities.
                
                At present, formal academic recognition of digital work in the humanities remains somewhat problematic, although there are signs that this might be changing.[ 
                    
                        citation needed
                    ] Some universities offer programs related to the field 
                    
                        "Digital Humanities Programs and Organizations". UCLA Digital Humanities. Retrieved 2 November 2014.
                     and some have dedicated Digital Humanities programmes. 
                    
                        
                    
                
            
            
                See also
                
                    Centers
                    
                        
                            The 
                            
                                Alliance of Digital Humanities Organizations
                            
                            maintains 
                            
                                a comprehensive list of digital humanities centers
                            
                        
                    
                    
                        
                            Department of Digital Humanities (King's College London, UK)
                        
                        
                            Humanities Advanced Technology and Information Institute (University of Glasgow, Scotland)
                        
                        
                            Institute for Advanced Technology in the Humanities (University of Virginia, USA)
                        
                        
                            Maryland Institute for Technology in the Humanities
                        
                        
                            Roy Rosenzweig Center for History and New Media (George Mason University, Virginia, USA)
                        
                        
                            UCL Centre for Digital Humanities (University College London, UK)
                        
                        
                            Center for Public History and Digital Humanities (Cleveland State University)
                        
                    
                
                
                    Journals
                    
                        
                            Digital Medievalist
                        
                        
                            Digital Humanities Quarterly
                        
                        
                            Literary and Linguistic Computing
                        
                        
                            Southern Spaces
                        
                    
                
                
                    Meetings
                    
                        
                            Digital Humanities conference
                        
                        
                            HASTAC
                        
                        
                            THATCamp
                        
                    
                
                
                    
                        Miscellaneous
                    
                    
                        
                            Computers and writing
                        
                        
                            Computational archaeology
                        
                        
                            Cybertext
                        
                        
                            Cultural analytics
                        
                        
                            Digital Classicist
                        
                        
                            Digital Humanities Summer Institute
                        
                        
                            Digital library
                        
                        
                            Digital Medievalist
                        
                        
                            Digital history
                        
                        
                            Digitizing
                        
                        
                            Digital rhetoric
                        
                        
                            Digital scholarship
                        
                        
                            Electronic Cultural Atlas Initiative
                        
                        
                            Electronic literature
                        
                        
                            EpiDoc
                        
                        
                            E-research
                        
                        
                            Humanistic informatics
                        
                        
                            Multimedia literacy
                        
                        
                            New media
                        
                        
                            Systems theory
                        
                        
                            Stylometry
                        
                        
                            Text Encoding Initiative
                        
                        
                            Text mining
                        
                        
                            Topic Modeling
                        
                        
                            Transliteracy
                        
                    
                    
                        
                            
                                Categories:
                            
                            
                                Digital humanities
                            
                        
                    
                
            
            
                
                    External links
                
                
                    
                        The Alliance of Digital Humanities Organizations
                    
                    
                        CenterNet
                    
                    
                        A Day in the Life of the Digital Humanities
                    
                    
                        Lev Manovich, Computational Humanities vs. Digital Humanities
                    
                
            
        
        
            
                
                    Bibliography
                    Beagle, Donald, (2014). 
                        
                            Digital Humanities in the Research Commons: Precedents &amp; Prospects
                        , Association of College &amp; Research Libraries: dh+lib.
                    
                    Berry, D. M., ed. (2012). 
                        
                            Understanding Digital Humanities
                        , Basingstoke: Palgrave Macmillan.
                    
                    Burdick, Anne, Johanna Drucker, Peter Lunenfeld, Todd Presner, &amp; Jeffrey Schnap (2012). 
                        
                            Digital_Humanities
                        , The MIT Press
                    
                    Busa, Roberto (1980). ‘The Annals of Humanities Computing: The Index Thomisticus’, in Computers and the Humanities vol. 14, pp. 83–90. Computers and the Humanities (1966-2004) 
                    Celentano, A., Cortesi, A. &amp; Mastandrea, P. (2004). Informatica Umanistica: una disciplina di confine, Mondo Digitale, vol. 4, pp. 44–55. 
                    Classen, Christoph, Kinnebrock, Susanne, &amp; Löblich, Maria, eds. (2012). 
                        Towards Web History: Sources, Methods, and Challenges in the Digital Age. 
                        
                            Historical Social Research
                        
                        , 37 (4), 97-188.
                    
                    Condron Frances, Fraser, Michael &amp; Sutherland, Stuart, eds. (2001). 
                        
                            Oxford University Computing Services Guide to Digital Resources for the Humanities
                        , West Virginia University Press.
                    
                    Fitzpatrick, Kathleen (2011). 
                        
                            Planned Obsolescence: Publishing, Technology, and the Future of the Academy
                        . New York; NYU Press.
                    
                    Gold, Matthew K., ed. (2012). 
                        
                            Debates In the Digital Humanities
                        . Minneapolis: University of Minnesota Press.
                    
                    Hancock, B., &amp; Giarlo, M.J. (2001). 
                        
                            Moving to XML: Latin texts XML conversion project at the Center for Electronic Texts in the Humanities
                        . Library Hi Tech, 19(3), 257-264.
                    
                    Hockey, Susan (2001). 
                        Electronic Text in the Humanities: Principles and Practice, Oxford: Oxford University Press.
                    
                    Honing, Henkjan (2008). 
                        
                            The role of ICT in music research: A bridge too far?
                        International Journal of Humanities and Arts Computing 
                        , 1 (1), 67-75.
                    
                    Inman James, Reed, Cheryl, &amp; Sands, Peter, eds. (2003). 
                        Electronic Collaboration in the Humanities: Issues and Options, Mahwah, NJ: Lawrence Erlbaum.
                    
                    Kenna, Stephanie &amp; Ross, Seamus, eds. (1995). 
                        Networking in the humanities: Proceedings of the Second Conference on Scholarship and Technology in the Humanities held at Elvetham Hall, Hampshire, UK 13–16 April 1994. London: Bowker-Saur.
                    
                    Kirschenbaum, Matthew (2008). 
                        
                            Mechanisms: New Media and the Forensic Imagination
                        . Cambridge, Mass.: MIT Press.
                    
                    McCarty, Willard (2005). 
                        Humanities Computing, Basingstoke: Palgrave Macmillan.
                    
                    Moretti, Franco (2007). 
                        
                            Graphs, Maps, Trees: Abstract Models for Literary History
                        . New York: Verso.
                    
                    Mullings, Christine, Kenna, Stephanie, Deegan, Marilyn, &amp; Ross, Seamus, eds. (1996). 
                        New Technologies for the Humanities London: Bowker-Saur.
                    
                    Newell, William H., ed. (1998). 
                        Interdisciplinarity: Essays from the Literature. New York: College Entrance Examination Board.
                    
                    Nowviskie, Bethany, ed. (2011). 
                        
                            Alt-Academy: Alternative Academic Careers for Humanities Scholars
                        . New York: MediaCommons.
                    
                    Ramsay, Steve (2011). 
                        
                            Reading Machines: Toward an Algorithmic Criticism
                        . Urbana: University of Illinois Press.
                    
                    Schreibman, Susan, Siemens, Ray &amp; Unsworth, John, eds. (2004). 
                        
                            A Companion To Digital Humanities
                         Blackwell Publishers.
                    
                    Selfridge-Field, Eleanor, ed. (1997). Beyond MIDI: The Handbook of Musical Codes. Cambridge, MA: The MIT Press. 
                    Thaller, Manfred, ed. (2012). 
                        Controversies around the Digital Humanities. 
                        
                            Historical Social Research
                        , 37 (3), 7-229.
                    
                    Unsworth, John (2005). 
                        
                            Scholarly Primitives: What methods do humanities researchers have in common, and how might our tools reflect this?
                        
                    
                    Warwick C., Terras, M., &amp; Nyhan, J., eds. (2012). 
                        Digital Humanities in Practice, Facet
                    
                
            
        
    

        
            
                Introduction 
                Although it is well known that word meanings evolve over time, there is still much to discover concerning the causes and pace of semantic change . In this context, computational modelling can shed new light on the problem by considering at the same time a large number of variables that are supposed to interact in a complex manner. This field has already given birth to a large number of publications ranging from early work involving statistical and mathematical formalism (Bailey, 1973 ; Kroch, 1989) to more recent work involving robotics and large-scale simulations (Steels, 2011). 
                We consider that semantic change includes all kinds of change in the meanings of lexical items happening over the years. For example, the word 
                    awful has dramatically changed in meaning, moving away from a rather positive perspective equivalent to 
                    impressive or 
                    majestic at the beginning of the nineteenth century to a negative one equivalent to 
                    disgusting and 
                    messy nowadays (Wijaya and Yeniterzi, 2011). 
                
                In this work, we address the question of semantic change from a computational point of view. Our aim is to capture the systemic change of word meanings in an empirical model that is also predictive, contrary to most previous approaches that meant to reproduce empirical observations. We will first describe our methodology, then the experiment and our results, before concluding. 
            
            
                Proposed methodology
                Our goal is to train a model representing semantic change over a certain period and, from there, to predict potential future semantic changes. The evaluation will thus be based on the observation of the gap between actual data and predicted data. 
                Our model is based on two main components: 
                
                    1- Diachronic word embeddings representing the meaning of words over time-periods, following Turney and Pantel (2010). Word embeddings are known to effectively represent the meaning of words by taking into account their surrounding contexts. The representation can be extended to include a diachronic perspective: word embeddings are first trained for each time-period and then aligned temporally, so as to be able to track semantic change over time, see Fig. 1. For our study, we used the pre-trained diachronic word embeddings released by Hamilton et al. (2016): for each decade from 1800 to 1990, a specific word embedding is built using the word2vec skip gram algorithm. The training corpus used to produce these word embeddings was derived from the English Google Books N-gram datasets (Lin et al., 2012), which contain a large number of historical texts in many languages (we used 5-grams with no part-of-speech tags). Each word in the corpus appearing from 1800 to 1999 is represented by a set of twenty 300-dimensional vectors, with one vector per decade.
                
                
                    
                
                Figure 1. Two-dimensional visualization of the semantic change in the English word “
                    cell” using diachronic word embedding. In the early 19th century the word cell was typically used to refer to a prison cell, hence the frequency of 
                    cage and 
                    dungeon in the context of 
                    cell in 1800, whereas in the late 19th century its meaning changed as it came to be frequently used in a scientific context, referring to a microscopic part of a living being (see 
                    protoplasm, 
                    ovum, etc. in the 1900 context).
                
                
                    2- Recurrent Neural Networks (RNNs) modelling semantic change itself. RNNs are known to be powerful at recognizing dynamic temporal behaviours in diachronic data (Medsker and Jain, 2001). In this experiment, we used the word embeddings representing the semantic space of each decade from 1800 to 1990 as input for the RNN, and from this we predicted the embedding corresponding to the 1990-1999 decade. Our RNNs have a long short-term memory (LSTM) and are implemented through Tensorflow. 
                
                To explore different scenarios, we ran several experiments with different vocabulary sizes (restricted to the 1,000, 5,000, 10,000, 20,000 and 50,000 most frequent words). We used the stratified 10-fold cross-validation method to estimate the prediction error (i.e. 90% of the words were used for training, and 10% for testing). The overall prediction accuracy is taken as the average performance over these 10 runs. 
            
            
                Experiment, Results and Discussion
                To get an overall estimation of the prediction accuracy, we compare each predicted embedding to the ground truth obtained from real data. Though it is impossible to predict exactly the vector corresponding to a given word “w”, as we are working in a continuous 300-dimensional space, one can assess the accuracy of the predicted meaning by extracting the closest vectors, i.e. the closest neighbours of a given word over time. 
                If the word “w” is actually the nearest semantic neighbour to the predicted vector, then it is considered to be a correct prediction. Otherwise, it is considered to be an error (a false prediction). The results are summarized in Table 1. 
                
                    
                        Vocabulary Size
                        Accuracy
                    
                    
                        1000
                        91.7%
                    
                    
                        5000
                        86.1%
                    
                    
                        10000
                        71.4%
                    
                    
                        20000
                        52.2%
                    
                    
                        50000
                        25%
                    
                
                
                    Table 1. Results of prediction accuracy measured for different vocabulary sizes. The training and the prediction using the RNNs model were performed on embeddings derived from the Google N-gram corpus.
                
                The results show that the model can be highly effective at capturing semantic change, and can achieve a high accuracy when predicting the evolution of word meaning through distributional semantics. As one can see from Table 1, the model was able to achieve 71.4% accuracy when trained and tested exclusively on embeddings based on the 10,000 most frequent words of the corpus. The model was even able to correctly predict word embeddings for words that have radically changed their meaning over time such as 
                    awful, 
                    nice, 
                    cell and 
                    record (Wijaya and Yeniterzi, 2011). 
                
                The results also show better results when using smaller vocabulary sizes containing top frequent words. The decrease of performance with large vocabularies is due to the fact that infrequent words do not have enough occurrences to derive meaningful and stable enough contexts so as to observe reliable evolutions. It is thus fundamental to use large corpora for this kind of experiments, but also to adapt the size of the vocabulary to the size of the corpus. 
            
            
                Conclusion
                We have proposed a new computational model of semantic change. Although this model is (partially) successful at representing this evolution, it can still appear to be too simple compared to the complexity of language change in general and semantic change in particular. For now, it may remain hard to understand precisely how this type of computational modelling can be combined with more traditional methods of linguistic analysis. However, we strongly believe that such empirical approaches based on diachronic vector-based representations can considerably help to refine and clarify theoretical insights on the foundations and mechanisms of semantic change, as well as provide an accurate empirical evaluation.
            
            
                Acknowledgements
                This work is supported by the project 2016-147 ANR OPLADYN TAP-DD2016. Thierry Poibeau is also supported by the CNRS International Research Network “Cyclades”. Our thanks go to the anonymous reviewers for their constructive comments.
            
        
        
            
                
                    Bibliography
                    
                        Bailey, C.-J.N. (1973). Variation and linguistic theory. Arlington: Centre for Applied Linguistics. 
                    
                    
                        Hamilton, W.L., Leskovec, J. and Jurafsky, D. (2016). Diachronic word embeddings reveal statistical laws of semantic change. 
                        Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 1489–1501. 
                    
                    
                        Kroch, A.S. (1989). Reflexes of grammar in patterns of language change. 
                        Language Variation and Change, 
                        1: 199–244. 
                    
                    
                        Lin, Y., Michel, J.-B., Aiden, E.L., Orwant, J., Brockman, W., Petrov, S. (2012). Syntactic annotations for the google books Ngram corpus. 
                        Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: 169–174. 
                    
                    
                        Medsker, L.R. and Jain, L.C. (2001). Recurrent neural networks. Boca Raton: CRC Press. 
                    
                    
                        Steels, L. (2011). Modeling the cultural evolution of language. 
                        Physical Life Review, 
                        8: 339–356.
                    
                    
                        Turney, P.D. and Pantel, P. (2010). From frequency to meaning: Vector space models of semantics. 
                        Journal of Artificial Intelligence Research, 
                        37: 141–188. 
                    
                    
                        Wijaya, D. and Yeniterzi, R. (2011). Understanding Semantic Change of Words Over Centuries. In the 
                        Workshop on Detecting and Exploiting Cultural Diversity on the Social Web (DETECT 2011) during CIKM 2011.
                    
                
            
        
    

        
            
                Introduction
                Many stories have been told of the novel’s emergence: the majority string a narrative of development through a handful of texts (e.g. Watt, 1957). More recently, methods in corpus linguistics and topic modeling have determined lexical differences in emerging genres, treating genres as bags of words and counting words to determine large lexical trends (e.g. Biber et al., 1998; Underwood, 2019). This talk offers a different account of the novel’s emergence by focusing on format. A preliminary survey of the 18th century literary field locates two formatting features that emerge within the novel as the novel emerges, monopolizing the novel and differentiating it from other genres. These two features are the modern quotation mark and the chapter number unaccompanied by title or table of contents. Here I will focus on the quotation mark, interpreting it as a high-fidelity index of genre emergence in four particular respects. As a visible index, the quotation mark offers a way of intuiting internal changes within the emerging genre. As a recurrent index, the quotation mark offers a way of gauging the pace of the novel’s emergence. As an evolutionary index, the quotation mark offers a way of registering alternative paths. And as an English index, the quotation mark offers a geographic point of comparison with the emerging French novel (which adopts a different format—the indented dash—to address the same essential question: how to represent conversational dialogue in prose). In other words, the quotation mark contains high information about the 
                    content, 
                    tempo, 
                    contingency, and 
                    geography of the novel’s emergence. What’s more, the quotation mark is fully adopted by the 19th century novel, thereby setting, for an extended period of time, the conditions within which the novel continues to evolve. It is a testimony to the significance of the quotation mark that it continues to affect the evolution of the novel—most notably, I will show, by making possible the celebrated novelistic invention of free indirect style.
                
            
            
                Methods
                A tendency among editors to modernize punctuation in editions of early modern texts has obscured the history of the modern quotation mark, which can now be reconstructed thanks to the wide-spread digitization of original editions. In order to attribute the modern quotation mark to the novel, I compiled my own corpus for each of the following genres—scientific articles (Philosophical Transactions Archive), trial transcripts (The Old Bailey Proceedings Online), literary reviews (Google Books), novels, poetry, drama, and history (ECCO)—then reviewed these pages looking for unique modes of quotation (as well as other genre-specific changes in format). Having attributed the modern quotation mark to the novel, I then reconstructed its emergence within the novel by recording methods of depicting dialogue in popular novels from each decade of the 18th century. As of now, these data are compiled by hand because computing within a larger dataset frequently encountered problems. In the case of the Chadwyck-Healey and HathiTrust databases, text-only transcriptions often remove marginal quotation marks and italics, and double quotation marks are sometimes replaced with single quotation marks (indistinguishable from apostrophes). This project thus raises questions about how best to encode OCR transcriptions to include formatting elements. Current practices have been shaped by the limitations of OCR and by the demands of corpus linguistics and topic modeling, both of which emphasize lexical differences above formatting ones. In what follows, I hope to demonstrate how quantitative analysis of the quotation mark results in a genuinely new account of the novel’s emergence, thereby demonstrating the value of developing new methods for preserving format in OCR transcriptions.
            
            
                Plotting emergence
                As the modifier modern suggests, the creation of the modern quotation mark was not a sudden ex nihilo invention, but rather a gradual but substantial reworking of an earlier species. This earlier species of quotation mark ran along the 
                    left-hand margin and was used to denote 
                    the transcription of written text (but not speech). One finds it throughout the 17th and 18th century: in the margins of 
                    Philosophical Transactions denoting passages excerpted from treatises and correspondence; in the margins of literary reviews, denoting sample passages from books reviewed; in the trial transcripts of the Old Baily, which reserve quotation marks solely for texts read aloud in court (leaving witness testimony, which quotes the words of others, unmarked); and even in some early novels, in 
                    Moll Flanders and 
                    Pamela, where dialogue is left alone and where quotation marks exclusively frame the margins of transcribed letters and notes. Based on my survey of the 18th century literary field, this earlier species would plausibly have remained the dominant model were it not for the novel: for it is in the novel that the quotation mark moved from the margin of the page into the gaps between words, and it is in the novel that the quotation mark was expanded beyond transcribed text to denote the larger grammatical category of direct discourse, written and spoken.
                
                I have started reconstructing the emergence of the modern quotation mark within the novel by recording the methods for depicting dialogue in five of the most popular novels from each decade of the 18th century (excepting 1700-1710 and 1730-1740 in which novel production was minimal). Within these novels, dialogue could be left unmarked, italicized in various fluid forms, or denoted by one of four forms of quotation mark: what I’ve termed marginal-inclusive, marginal-exclusive, endpoint-inclusive, and endpoint-exclusive. Figure 1 plots each novel’s most frequent form of depicting dialogue. While collapsing much of the struggle and variation within individual texts (many of which incorporate multiple quotation strategies), Figure 1 nevertheless captures a deep structural shift: the gradual inauguration of the modern quotation mark, which itself advances through three prototypical versions before arriving at the modern format.
                
                    
                
                Figure 1: Evolution of methods for representing dialogue in the 18th century novel. Novels were selected from three bibliographies based on number of editions (McBurney, 1960; Hahn et al., 1985; Garside et al., 2000). The earliest available edition was downloaded from ECCO.
            
            
                Tempo, contingency, content...
                Figure 1 captures a century-spanning trend: in this case, the gradual overlapping steps of an aggregating consensus. The gradual tempo of emergence suggests that a need is subconsciously intuited in the absence of a clear solution, that the quotation mark is being formed for an emerging content that is not yet fully understood. “During a paradigm shift,” writes Franco Moretti, “nobody knows what will work and what won’t” (Moretti, 2013: 74). And that is precisely what one sees here: expansive, almost blind, experimentation slowly whittled down to a single solution. Nor was experimentation limited to the 
                    form of the quotation mark: novelists also fiddled with 
                    purpose, most notably in the ultimately-failed experiment of printing indirect discourse within quotation marks:
                
                
                    
                
                Gale document number: CW3311810274
                
                    
                
                Gale document number: CW3311910127
                Instances of this practice abound in the 18th century novel, and not without a certain logic: in each case there is an attempt to acknowledge the residue of direct discourse exuding through. Together with the many morphological possibilities, these instances remind us that the novel could have molded a very different type of quotation mark, both in form and function. These were alternatives, not mistakes. If the modern solution seems obvious, then it merely emphasizes how deeply the novel has defined our worldview. And yet one must ask: why did the novel develop the quotation mark in the way it did? The simple answer is because new forms emerge alongside new content, and in this case that emerging content was conversational dialogue...[abridged from abstract].
            
            
                Free indirect style
                The quotation mark instituted a genuinely new matrix for structuring storytelling, one into which the oral stories of the past were slowly squeezed. Over the course of the century, narrators and characters begin to fit themselves more and more neatly into the confines of direct and indirect discourse. As a result, novels like 
                    Moll Flanders, which seem more like witness testimony, are replaced by novels like 
                    Evelina, which seem more like—novels. This is an exceptional ramification of the quotation mark, and nothing illustrates it better than the most celebrated of novelistic inventions: free indirect style.
                
                
                    
                
                (Austen, 1816)
                The introductory sentence is clearly a product of the narrator, and Emma clearly speaks the quoted section to herself; but who speaks those many dashed phrases that burst forth in frustration: 
                    It was a wretched business indeed!—Such an overthrow of every thing she had been wishing for! The first linguists to recognize this style described it as a blending of what they considered two discrete categories: direct and indirect discourse. So Alfred Tobler noted a “peculiar mix of indirect and direct discourse” (Tobler, 1887). A decade later, Charles Bally brought the style to academic attention, defining “three possibilities of rendering the words or thoughts of a character”—direct discourse, indirect discourse, and discours indirect libre—“the first two being long known to grammarians,” the latter being some new combination of the two (Pascal, 1977: 8). As such definitions make clear, free indirect style depends on a clear distinction between direct and indirect discourse. Yet this very distinction comes into being, not with the ancient grammarians, but rather with the solidification of the modern quotation mark solely for direct discourse (Moore, 2011: 131). Novels that denote indirect discourse within quotation marks blur this distinction (see examples above) precluding the possibility of free indirect style. Only with the modern quotation mark, which created a functional and sustained binary between direct and indirect discourse, does free indirect style become possible. Which is not to say that Bally was technically wrong: the Greeks did make a distinction between direct and indirect discourse. Rather, it is a classic case of a theoretical distinction holding less influence than a functional one, and a powerful example of how a new format can allow a genre to evolve and differentiate in new, previously unthinkable ways. 
                
            
        
        
            
                
                    Bibliography
                    
                        Austen, J. (1816). 
                        Emma: A novel. In three volumes. London: John Murray.
                    
                    
                        Biber, D., Conrad, S. and Reppen, R. (1998).
                         Corpus linguistics: Investigating language structure and use. Cambridge University Press.
                    
                    
                        Gale Group. (2003). 
                        Eighteenth century collections online. Detroit: Gale Group.
                    
                    
                        Garside, P., Raven, J., Schöwerling, R. and Forster, A. (2000). 
                        The English novel 1770-1829: A bibliographical survey of prose fiction published in the British Isles. Oxford University Press.
                    
                    
                        Hahn, H. and Behm, C. (1985). 
                        The eighteenth-century British novel and its background: An annotated bibliography and guide to topics. Scarecrow Press.
                    
                    
                        McBurney, W. 
                        (1960). 
                        A check list of English prose fiction, 1700-1739
                        . Harvard University Press.
                    
                    
                        Moore, C. (2011). 
                        Quoting speech in early English. Cambridge University Press.
                    
                    
                        Moretti, F. (2013). 
                        Distant reading. London: Verso.
                    
                    
                        Pascal, R. (1977). 
                        The dual voice: Free indirect speech and its functioning in the nineteenth-century European novel. Manchester University Press. 
                    
                    
                        Tobler, A. (1887). Vermischte Beiträge zur französischen Grammatik. 
                        Zeitschrift Für Romanische Philologie, 11(1): 433-61.
                    
                    
                        Underwood, T. (2019). 
                        Distant Horizons: Digital Evidence and Literary Change. University of Chicago Press.
                    
                    
                        Watt, I. (1957). 
                        The rise of the novel: studies in Defoe, Richardson, and Fielding. University of California Press.
                    
                
            
        
    

        
            
                Introduction
                The digitization of large time-labeled bibliographies has resulted in corpora such as the Google Ngram data set (Lin et al., 2012). Such corpora extremely accurately reflect how individual words are used over time. They are expected to reveal novel insights into the evolution of language and society, provided adequate analysis systems are available. In this context, developing a comprehensive query algebra that allows domain experts to formalize complex hypotheses would be a major contribution to successfully unlock this potential.
                The case of conceptual history serves as our example from the humanities. In conceptual history, researchers examine the evolution of concepts represented by words such as “peace” or “freedom”. In exploring the history of a concept, scholars commonly make use of, but are not restricted to, word-usage frequencies, word contexts, sentiment analysis, how words refer and relate to and contrast with each other, or they look for word pairs or word families whose usage is correlated (Brunner et al., 2004; Ritter and Gründer, 1971). Consider our example: how the words “East” and “West” change from merely cardinal directions to politically charged concepts after 1945.
                In this paper, we present a query algebra for empirical analyses of temporal text corpora, the Conceptual History Query Language (CHQL). A 
                    temporal text corpus in our sense is a set of words and word chains, i.e., ngrams, together with their usage frequency at various points of time. Our query language is meant to be useful for domain experts, i.e., be descriptive and complete (match all actual and potential hypotheses of conceptual history), and bear optimization potential to allow fast query processing on large data sets. We focus on an algebra inspired by the German tradition of 
                    Begriffsgeschichte (conceptual history), as exemplified by the work of Reinhart Koselleck (Olsen, 2012).
                
            
            
                Related Work
                Existing query algebras, like the one for the Structured Query Language (SQL), do not feature specific support for analyses of the kind we envisage. Other approaches from the literature, e.g., the Contextual Query Language (The Library of Congress, 2013), the Corpus Query Language (Jakubíček et al., 2010), or the ANNIS Query Language (Zeldes et al., 2009), have similar issues. The common relational algebra (Maier, 1983; Abiteboul et al., 1994), does not contain sufficiently specific operators, e.g., temporal or linguistic operators. Extensions exist to add temporal operators (Snodgrass, 1987; Snodgrass, 1995), but not linguistic operators. To query relations between words, there are special-purpose query languages. For example, SQWRL is a language to query an ontology (O'Connor and Das, 2009). Querying word relations, e.g., from an ontology, does not include all required linguistic relationships. Further, ontologies do not provide temporal information. SQWRL does not contain any temporal operator. All of these algebras have in common that they do not cover both linguistic and temporal operators required for research on conceptual history.
                Related work in the digital humanities mainly consists of data processing and the analysis of text corpora (Warwick et al., 2012; Hai-Jew, 2017). Some frameworks focus on linguistic and reflective properties as well as their evolution such as (Hamilton et al., 2016a; Hamilton et al., 2016b; Prabhakaran et al., 2016; Englhardt et al., 2019). Respective systems cannot output the required information to conduct research on conceptual history in a comprehensive way. In addition, such systems do not provide a sufficiently 
                    abstract interface, a reason why experts are reluctant in using them (Hai-Jew, 2017).
                
            
            
                Concept Types and Operators
                This section shows in the abstract how the operators of CHQL allow searching for concept types. A formal definition of all of our operators is given in (Willkomm et al., 2018) and will be presented at DH2019.
                Conceptual history claims that pragmatic properties of historical, cultural and economic relevance are incorporated in concepts, irrespectively of whether individual users are aware of this or not. It attempts to track changes of particular concepts (such as “socialism”) over time to determine how their pragmatic relevance changes (it might mostly express generic hopes at some moment and mostly specific fears at some other). Thus, concepts will be categorized as belonging to a particular 
                    concept type at a particular moment in time.
                
                Conceptual historians typically read and interpret large masses of texts which provide a variety of information types (e.g. word frequencies, what words appear in the context, how these words function pragmatically (individually as well as in sentences etc.)) which help to determine the concept type. Because we want to do the same using 
                    Distant Reading techniques (Moretti, 2013), these information types need to be translated into observable data characteristics for which individual operators in the query language are defined. Finding an adequate number of helpful information types, structuring them and converting them into computable and combinable items is the main challenge of our project.
                
                Since there is no accepted formal specifications of information types, we describe an interpretation of Koselleck’s information types in order to map them on to data characteristics. Data characteristics are quantitative feature either directly present in our data (e.g., the usage frequency of the word “socialism” in 1848), or a derived piece of information (e.g. the difference between the usage frequency of words “socialism” and “communism” from 1848 to 1989). We describe which data characteristics are needed to simulate Koselleck’s information needs and explain our realization of all data characteristics and their implementation as operators.
                
                    
                    The relationship between concept types, information types, data characteristics and operators to hypothesize concept types
                
                One of Koselleck’s implicit assumptions is that each concept type has specific characteristics. In our terminology: any concept type can be described using a specific combination of information types. For example, Koselleck may plausibly be read as claiming that words that form a 
                    parallel concept (concept type) would have “similar” 
                    word frequencies and have a significant number of identical 
                    surrounding words (information types). By contrast, 
                    counter concepts would also have similar word frequencies yet their surrounding words would behave differently. For instance, if “enlightenment” and “reason” are parallel concepts for a particular period, their relative word frequencies should be similar, and if “emancipation” occurs near “enlightenment”, it should occur near “reason” too, and both concepts should be endorsed rather than criticized (in some sense). By contrast, if “East” and “West” are counter concepts, their word contexts should contain different words, and there should be some sort of contrast in attitude between them.
                
                If every concept type has its own specific linguistic and pragmatic properties and hence should be representable by a specific 
                    combination of information types, it should be possible to develop a system that finds these information types in large corpora that are not amenable to conventional close reading. To this end, we need a formal definition of any information type which is observable and quantifiable.
                
                We present a selection of some of the data characteristics with the information type they are intended to represent:
                
                    
                        Individual Context: This requires two data characteristics: a set of surrounding words for a target word, i.e., the linguistic context, and the sentiment for this context, by summing up the sentiment values of the words of the context. Our 
                        surroundingwords operator and 
                        sentiment operator implement this.
                    
                    
                        Topic Grouping: Using 
                        topic modeling, groups of words may be classified as belonging to a particular topic (e.g. geography or politics).
                    
                    
                        Sentence Structure: This again requires two data characteristics: the function of a word, i.e., differentiate between parts of speech, and completing phrases, i.e., search for missing words in a phrase. The first data characteristic is implemented by our operator 
                        pfilter. We implement the second one as a pattern-matching operator which we call 
                        textsearch.
                    
                    
                        Frequency Data: Neologisms, which might be evidence for radical changes, would display abrupt increases in word-usage frequency over time. To find this and similar characteristics, we propose an operator 
                        time series-based selection that compares the time-series values with a constant. To allow for a temporal restriction, we also provide a 
                        subsequence operator that limits the selection to an arbitrary time interval. The combination of both operators facilitates the search for neologisms.
                    
                    
                        Sentiment Analysis: Using well-proven resources such as LIWC (Wolf et al., 2008) or customized dictionaries, our 
                        sentiment operator represents the emotions associated with a concept, relying on the words in its context.
                    
                
            
            
                Results
                Using CHQL, we have tested the hypotheses that (1) “East” and “West” have acquired a political context after 1945, whereas “North” and “South” haven’t, and that (2) the former have turned into counter concepts in the political sphere, their contexts expressing diverging attitudes, whereas the latter have remained parallel concepts in the geographical sphere. The operator trees 1 and 2 shown in Figures 2 and 4 illustrate how CHQL allows combining the operators mentioned to perform a single search, yielding the results shown in Figures 3 and 5.
                
                    
                    Formalisation of hypothesis 1 in CHQL
                
                
                    
                    The result of Query 1 on the Google Books Ngram Corpus
                
                
                    
                    Formalisation of hypothesis 2 in CHQL
                
                
                    
                    The result of Query 2 on the Google Books Ngram Corpus
                
            
        
        
            
                
                    Bibliography
                    
                        Abiteboul, S., Hull, R. and Vianu, V.
                         (1994). 
                        Foundations of Databases: The Logical Level
                        . Pearson.
                    
                    
                        Brunner, O., Conze, W. and Koselleck, R. (eds).
                         (2004). 
                        Geschichtliche Grundbegriffe (Volumes 1-8)
                        . Klett-Cotta Verlag.
                    
                    
                        Englhardt, A., Willkomm, J., Schäler, M. and Böhm, K.
                         (2019). Improving Semantic Change Analysis by Combining Word Embeddings and Word Frequencies. 
                        International Journal on Digital Libraries
                        .
                    
                    
                        Hai-Jew, S. (ed).
                         (2017). 
                        Data Analytics in Digital Humanities
                        . Springer-Verlag GmbH.
                    
                    
                        Hamilton, W., Leskovec, J. and Jurafsky, D.
                         (2016a). Cultural Shift or Linguistic Drift? Comparing Two Computational Measures of Semantic Change. 
                        Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’16)
                        . pp. 2116–2121.
                    
                    
                        Hamilton, W., Leskovec, J. and Jurafsky, D.
                         (2016b). Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change.
                    
                    
                        Jakubíček, M., Kilgarriff, A., McCarthy, D. and Rychlý, P.
                         (2010). Fast syntactic searching in very large corpora for many languages. 
                        Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation (PACLIC 24)
                        . pp. 741–747.
                    
                    
                        Lin, Y., Michel, J.-B., Aiden, E. L., Orwant, J., Brockman, W. and Petrov, S.
                         (2012). Syntactic annotations for the Google Books Ngram Corpus. 
                        Proceedings of the ACL 2012 System Demonstrations (ACL ’12)
                        . Association for Computational Linguistics, pp. 169–174.
                    
                    
                        Maier, D.
                         (1983). 
                        Theory of Relational Databases
                        . Computer Science Press.
                    
                    
                        Moretti, F.
                         (2013). 
                        Distant Reading
                        . Verso Books.
                    
                    
                        O’Connor, M. and Das, A.
                         (2009). SQWRL: a query language for OWL. 
                        Proceedings of the 6th International Conference on OWL: Experiences and Directions (OWLED ’09)
                        . pp. 208–215.
                    
                    
                        Olsen, N.
                         (2012). 
                        History in the Plural: An Introduction to the Work of Reinhart Koselleck
                        . Berghahn Books Inc.
                    
                    
                        Prabhakaran, V., Hamilton, W., McFarland, D. and Jurafsky, D.
                         (2016). Predicting the Rise and Fall of Scientific Topics from Trends in their Rhetorical Framing. 
                        Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL ’16)
                        . Association for Computational Linguistics, pp. 1170–1180 doi:10.18653/v1/p16-1111.
                    
                    
                        Ritter, J. and Gründer, K. (eds).
                        (1971). 
                        Historisches Wörterbuch Der Philosophie (13 Volume Set)
                        . Schwabe.
                    
                    
                        Snodgrass, R.
                         (1987). The temporal query language TQuel. 
                        ACM Transactions on Database Systems
                        , 
                        12
                        (2): 247–298 doi:10.1145/22952.22956.
                    
                    
                        Snodgrass, R. (ed).
                         (1995). 
                        The TSQL2 Temporal Query Language
                        . . Vol. 330. (The Springer International Series in Engineering and Computer Science). Springer US.
                    
                    
                        The Library of Congress
                         (2013). 
                        The Contextual Query Language
                        . http://www.loc.gov/standards/sru/cql/.
                    
                    
                        Warwick, C., Terras, M. and Nyhan, J. (eds).
                         (2012). 
                        Digital Humanities in Practice
                        . Facet Publishing.
                    
                    
                        Willkomm, J., Schmidt-Petri, C., Schäler, M., Schefczyk, M. and Böhm, K.
                         (2018). A Query Algebra for Temporal Text Corpora. 
                        Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries (JCDL ’18)
                        . ACM Press, pp. 183–192 doi:10.1145/3197026.3197044.
                    
                    
                        Wolf, M., Horn, A., Mehl, M., Haug, S., Pennebaker, J. and Kordy, H.
                         (2008). Computergestützte quantitative Textanalyse. 
                        Diagnostica
                        : 85–98 doi:10.1026/0012-1924.54.2.85.
                    
                    
                        Zeldes, A., Lüdeling, A., Ritz, J. and Chiarcos, C.
                         (2009). 
                        ANNIS: A Search Tool for Multi-Layer Annotated Corpora
                        . Humboldt-Universität zu Berlin, Philosophische Fakultät II doi:10.18452/13437.
                    
                
            
        
    

        
            The digitization of large time-labeled bibliographies has resulted in corpora such as the Google Ngram data set (Lin et al., 2012) which are expected to reveal novel insights into the evolution of language and society. We present our project of developing a query algebra to unlock this potential, the Conceptual History Query Language (CHQL). It is inspired by the German tradition of 
                Begriffsgeschichte as exemplified by the work of Koselleck (Olsen, 2012).
            
            Conceptual history claims that pragmatic properties of historical, cultural and economic relevance are incorporated in concepts and attempts to track any changes over time to determine how their pragmatic relevance changes (“socialism” might express hopes at some moment and fears at some other). Thus, concepts will be categorized as belonging to a particular 
                concept type at a particular moment in time.
            
            To determine the type of a particular concept, scholars commonly make use of word-usage frequencies, word contexts, sentiment analysis, how words refer and relate to and contrast with each other, or they look for word pairs or word families whose usage is correlated etc. (Brunner et al., 2004; Ritter and Gründer, 1971). They closely read and interpret large masses of texts, but because we want to do the same using 
                Distant Reading techniques (Moretti, 2013), these information types need to be translated into observable data characteristics for which individual operators in the query language are defined. Finding these information types and building computable items is the main challenge of our project.
            
            Since there is no accepted definition of information types, we describe an interpretation of them in order to map them on to data characteristics. Data characteristics are quantitative feature either directly present (e.g., the frequency of the word “socialism” in 1848), or a derived piece of information (e.g. the difference between the frequency of “socialism” and “communism” from 1848 to 1989). We describe which data characteristics are needed to simulate Koselleck’s information needs and explain our realization of all data characteristics and their implementation as operators.
            
                
                The relationship between concept types, information types, data characteristics and operators to hypothesize concept types
            
            The assumption is that each concept type has specific characteristics, that is, any concept type can be described using a specific combination of information types. For example, Koselleck may plausibly be read as claiming that word pairs that form 
                parallel concepts (concept type) would have “similar” 
                word frequencies and have a significant number of identical 
                surrounding words (information types). By contrast, 
                counter concepts would also have similar word frequencies yet their surrounding words would behave differently. For instance, if “enlightenment” and “reason” are parallel concepts for a particular period, their relative word frequencies should be similar, and if “emancipation” occurs near “enlightenment”, it should occur near “reason” too, and both concepts should be endorsed rather than criticized (in some sense). By contrast, if “East” and “West” are counter concepts, their word contexts should contain different words, and there should be some sort of contrast in attitude between them.
            
            We are developing a system that finds these information types in large corpora that are not amenable to conventional close reading (Willkomm et al., 2018). We present a selection of some of the data characteristics with the information type they are intended to represent:
            
                
                    Individual Context: Our 
                    surroundingwords and 
                    sentiment operators search for surrounding words and sum the associated sentiment, using sentiment analysis (e.g. LIWC (Wolf et al., 2008))
                
                
                    Topic Grouping: Using 
                    topic modeling, groups of words may be classified as belonging to a particular topic.
                
                
                    Sentence Structure: The function of a word, i.e. various parts of speech, and completing phrases, i.e., search for missing words in a phrase, are implemented by our operator 
                    pfilter and by a pattern-matching operator called 
                    textsearch.
                
                
                    Frequency Data: Abrupt increases in word-usage frequency over time and similar characteristics are implemented with the operator 
                    time series-based selection, a 
                    subsequence operator may limit the selection to an arbitrary time interval.
                
            
            Using CHQL, we have tested the hypotheses that (1) “East” and “West” have acquired a political context after 1945, whereas “North” and “South” haven’t, and that (2) the former have turned into counter concepts in the political sphere, their contexts expressing diverging attitudes, whereas the latter have remained geographical parallel concepts. The operator trees 1 and 2 shown in Figures 2 and 4 illustrate how CHQL allows combining the operators mentioned to perform a single search, yielding the results shown in Figures 3 and 5.
            
                
                Formalisation of hypothesis 1 in CHQL
            
            
                
                The result of Query 1 on the Google Books Ngram Corpus
            
            
                
                Formalisation of hypothesis 2 in CHQL
            
            
                
                The result of Query 2 on the Google Books Ngram Corpus
            
        
        
            
                
                    Bibliography
                    
                        Brunner, O., Conze, W. and Koselleck, R. (eds).
                        (2004). 
                        Geschichtliche Grundbegriffe (Volumes 1-8)
                        . Klett-Cotta Verlag.
                    
                    
                        Englhardt, A., Willkomm, J., Schäler, M. and Böhm, K.
                         (2019). Improving Semantic Change Analysis by Combining Word Embeddings and Word Frequencies. 
                        International Journal on Digital Libraries
                        .
                    
                    
                        Hai-Jew, S. (ed).
                         (2017). 
                        Data Analytics in Digital Humanities
                        . Springer-Verlag GmbH.
                    
                    
                        Lin, Y., Michel, J.-B., Aiden, E. L., Orwant, J., Brockman, W. and Petrov, S.
                         (2012). Syntactic annotations for the Google Books Ngram Corpus. 
                        Proceedings of the ACL 2012 System Demonstrations (ACL ’12)
                        . Association for Computational Linguistics, pp. 169–174.
                    
                    
                        Moretti, F.
                         (2013). 
                        Distant Reading
                        . Verso Books.
                    
                    
                        Olsen, N.
                         (2012). 
                        History in the Plural: An Introduction to the Work of Reinhart Koselleck
                        . Berghahn Books Inc.
                    
                    
                        Ritter, J. and Gründer, K. (eds).
                         (1971). 
                        Historisches Wörterbuch Der Philosophie (13 Volume Set)
                        . Schwabe.
                    
                    
                        Warwick, C., Terras, M. and Nyhan, J. (eds).
                         (2012). 
                        Digital Humanities in Practice
                        . Facet Publishing.
                    
                    
                        Willkomm, J., Schmidt-Petri, C., Schäler, M., Schefczyk, M. and Böhm, K.
                         (2018). A Query Algebra for Temporal Text Corpora. 
                        Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries (JCDL ’18)
                        . ACM Press, pp. 183–192 doi:10.1145/3197026.3197044.
                    
                    
                        Wolf, M., Horn, A., Mehl, M., Haug, S., Pennebaker, J. and Kordy, H.
                         (2008). Computergestützte quantitative Textanalyse. 
                        Diagnostica
                        : 85–98 doi:10.1026/0012-1924.54.2.85.
                    
                
            
        
    

        
            
                Introduction
                
                    The study of historical figures is of great significance in the field of history. To investigate historical figures with digital humanity methods, the first step is to identify the names of people in texts. Not only is the person's name recognized from the text, but the person's name has to be linked to a knowledge base for reference. This is because the same person's name may refer to different people or other entities. This task is called named entity disambiguation (NED). The problem of historical figures with the same name is particularly serious when studying Manchu and Mongolian historical figures in the Qing Dynasty. A typical example is that the main persons (Cherin-Dorji, Yung-Te, Siang-Lin and Gui-Xiang) involved in the disaster report of the Kharkha Four Leagues 
                    
                    (Ting-ting, 2016)
                    have the same name as other historical figures. Fig. 1 shows that there are 10 Siang-Lins in the Ming-Qing Archives Name Authority Database (MQANAD).
                
                
                    
                
                
                    Fig. 1
                     There are ten people named Siang-Lin in the MQANAD database. Siang-Lin in this text refers to this historical figure with ID 0079695
                
                
                    Therefore, we select 
                    Qing Shi-Lu
                    (
                    QSL
                    )
                    
                     and MQANAD
                    
                    (Liu, 2015)
                     as the target text and the reference knowledge base for developing our NED system, respectively. 
                    QSL
                     is the imperial annals of the Qing emperors, with a total of about 58 million characters, written in classical Chinese.
                
                
                    In 
                    
                    (Tsai et al., 2020)
                    , several procedures were proposed to automatically generate labeled data from 
                    Ming Shi-Lu
                     (
                    MSL
                    ) for training a NED model. In this work, we improve 
                    
                    (Tsai et al., 2020)
                     in two folds. First, we propose a new way of expressing person names in both texts and profiles. Second, we modify the original procedures to improve the quality of the generated labeling data.
                
            
            
                Method
                As mentioned earlier, an NED 
                    
                    (Cheng et al., 2019
                    ;
                    
                    Huang et al., 2015) model can link each mention to the correct profile in the reference knowledge base. 
                    An official’s profile is shown in Table 1, 
                    we can see that it contains the official’s attributes such as name, birth/death year, biography, resume, relatives, etc. The resume field is composed of all titles the official had held. We search 
                    QSL
                     for all mentions of these officials’ names. These mentions with the paragraphs containing them are used as our dataset. Table 2 shows an example instance of the search results.
                
                
                    
                
                
                    Table 1
                     An official 
                    Wang An-Kuo’s
                     profile
                
                
                    
                
                
                    Table 2
                     An example instance
                
                We formulate this NED task as a text classification problem. Given an instance whose name field is m and a profile whose name field is also m, classify them as positive (matched) or negative (not matched). Matched means the person mentioned in the instance’s paragraph field is just the official in the profile.
                
                    Tsai et al. was the first work to propose the procedures that compare instances and profiles to automatically generate labeled data to train the model 
                    (Tsai et al., 2020). In this work, we use all the procedures in 
                    (Tsai et al., 2020). These procedures based on the rules help us identify part of the data pairs. Then we deal with those data pairs that cannot be identified by the rules by using BERT as a binary classifier. With training data acquired from previous procedures, we make the classifier learn the relationships between instances and profiles and identify them through the context besides using rule-based procedures. In addition, we also propose methods to improve the quality of the labeled data, as described in the following paragraphs.
                
                First, we replace the officials’ names mentioned in personal profiles and instances with the symbol [unused_token], as shown in Fig. 2. This allows our model to focus on more information from the context and can be more robust to various person names.
                
                    
                
                
                    Fig. 2
                     Cherin-Dorji, the official’s name mentioned in personal profiles and instances, is replaced with the symbol [unused_token] 
                    
                    (Hucker, 2008, 
                    
                    Zhang et al., 2017)
                
                Second, we have found many examples in the text where a person’s name is the same as a location name. For example, Guilin (桂林) can be the name of a city or a minister’s name. Therefore, we use a self-developed NER 
                    (Lin et al., 2020) system to process the texts. We use Flair as the NER model for this paper, the training data is from 
                    MSL and the test data is 
                    QSL's 50 manually annotated data with F1 score 0.81. We revise 
                    (Tsai et al., 2020)’s method to correct an instance from positive to negative, an example is shown in Fig. 3.
                
                
                    
                
                
                    Fig. 3
                     Guilin (桂林) is considered a location name by our NER system, therefore, it is generated as a negative instance
                
                Lastly, since the text processed by 
                    (Tsai et al., 2020) is the Ming Dynasty, the text processed by this study is the Qing Dynasty, and the contents of 
                    MSL and 
                    QSL are slightly different, we only retain the first condition of Procedure 1.
                
                
                    
                
                
                    Fig
                    .
                     4
                     Procedure one is modified for writing-style difference between 
                    MSL
                     and 
                    QSL
                
                
                    
                
                
                    Fig. 5
                     Our BERT-based NED model
                
                After automatically labeling the training data, we use BERT 
                    (Vaswani et al., 2017, 
                    Devlin et al., 2019), the state-of-the-art natural language understanding model, to perform text classification. We use the model pre-trained on the Chinese Wikipedia. As shown in Fig. 5, for each pair of profile and instance, we concatenate the cls symbol, the profile, the sep symbol, the instance as the input. The BERT model will output all label probabilities at the output position corresponding to the cls symbol.
                
                We use the manually labeled data set of 
                    (Tsai et al., 2020) for performance evaluation. The results show that our NED model achieves an accuracy of 90%, which is 16% higher than the model proposed in 
                    (Tsai et al., 2020).
                
                Finally, we conduct an ablation study. If we remove NER, performance will drop by 3%. If we do not do unused token replacement, performance will drop by 13%. If we do neither, the performance drops by 16%.
            
            
                Conclusion
                We have refined the approach of automatically generating labeled data for training an NED model. To be more specific, we employ our own NER system to eliminate the location names incorrectly labeled as positive instances and use the unused token symbol to enhance the robustness of our model. Results show that our refinement can improve the performance by 16% and our NED model 
                    will help us investigate historical figures in the Qing dynasty.
                
            
        
        
            
                
                    Bibliography
                    
                        Cheng, J. et al. (2019). Entity Linking for Chinese Short Texts Based on BERT and Entity Name Embeddings. 
                    
                    
                        Devlin, J. et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs]. 
                    
                    
                        Hucker, C.O. (2008) A Dictionary of Official Titles in Imperial China. Peking University Press.
                    
                    
                        Huang, H., Heck, L. and Ji, H. (2015). Leveraging Deep Neural Networks and Knowledge Graphs for Entity Disambiguation’. arXiv:1504.07678 [cs].
                    
                    
                        Institute of History and Philology, Academia Sinica (1984) Scripta Sinica. http://hanchi.ihp.sinica.edu.tw/ihp/hanji.htm. 
                    
                    
                        Liu, C. (2015) Ming-Qing Archives Name Authority Database.
                    
                    
                        Ting-ting, L. (2016). 清季蒙古親王車林多爾濟史事鉤沉—以喀爾喀四盟報災案為中心[History of Mongolian Prince Cherin-Dorji in the Qing Dynasty-A Disaster Report by the Four Leagues of Khalkha]. Studies of Chinese Frontier Ethnic Groups.
                    
                    
                        Tsai, R.T.-H. et al. (2020). Automatic Labeled Data Generation for Person Named Entity Disambiguation on the Ming Shilu. DH.
                    
                    
                        Vaswani, A. et al. (2017). Attention Is All You Need. arXiv:1706.03762 [cs].
                    
                    
                        Lin, B.Y. et al. (2020). TriggerNER: Learning with Entity Triggers as Explanations for Named Entity Recognition. arXiv:2004.07493 [cs]. 
                    
                    
                        Zhang, Y. et al. (2017). 明代職官中英辭典 Chinese-English Dictionary of Ming Government Official Titles.
                    
                
            
        
    



        
            
                
                    A corpus of French literary fictions: 1050-1920
                    
                
                A decade ago, when he set out to explore the evolution of English novels in the period 1740–1850, F. Moretti had to focus on the titles as the main available data, yet noting that:
                in a few years, we will have a digital archive with the full texts of (almost) all novels ever published .
                Today, we propose to embark on a journey to model the evolution of French literary fictions, from their epic and chivalric origins up to the more modern productions of genres such as heroïc fantasy or historical novels. We offer to do that not from the titles, but by analysing a corpus, currently in construction, whose aim is to cover all French literary fiction digitized from the earliest sample of French literature to the 20th century (or more precisely, to the point were a significant share of published literature is not yet in the public domain).
                This project is at the intersection of two major trends in computational literary analysis: the creation and documentation of large literary corpora and the analysis of literary genre and discourse through machine learning classification. In comparison with previous examples of French literary corpus (like 
                    théâtre classique ) or the French corpus of the European literary text collection , French novels make up a massive amount of texts (80,000 registered work in the French National Library before the 20th century), a large share of which is non-canonical and little documented. Text mining techniques make it possible to explore and document large digitized corpora with little editorial work. Classification is not simply used as cataloguing tool: its limitations can in fact inform in a more complex way the development of genres and the intertextual interplay between one genre and another.
                
                The French fiction corpus initially results from the collocation of three different collections:
                
                    A collection of medieval fictions and 
                        chansons de gestes (1050-1450) - (cf. Camps et al., 2019).
                    
                    A collection of printed fictions of Gallica from the modern period and the 19th century (Langlais, 2021b).
                    A new collection comprising most of the digitized fictions from the early modern period (1450-1700).
                
                The second corpus relied on one of the oldest bibliographic database: the catalog of the French National Library. The classification scheme used by the library until 1996 was developed between 1684 and 1688 by the librarian of Louis XIV, Nicolas Clément. A specific category for novels (Y2) has been existing since 1730 as a duplication of the category for poetry (Y). The novels are now classified in Y2 or Y Bis. For cultural history, the BNF catalog presents a major interest: the categories are often (nearly) contemporary of the documents they aim to classify.
                This corpus was made possible thanks to the policy of open data and open content which the BNF has been engaged in for several years. While the Clément classification ceased to be used for the classification of physical collections in 1996, it has become available in 2017 for data analysis when the BNF opened a new catalog access service, the SRU
                    
                        Search/Retrieve via URL, 
                        .
                    . It was originally limited to novels digitized by the French National Library with a strong focus on the 19th century.
                
                The third corpus was created specifically for the project: it aims to cover all the digitized versions of early modern novels published in France between 1473 (publication date of our earliest entry, 
                    Le Roman de Jason et Médée) and 1700 (our arbitrary cutoff, for now). This collection originally aimed to bridge the two previous corpora which were focused on two extreme temporal points of the history of French literature (the medieval period and the post-1800 period). It became a more ambitious experiment: a systematic harvesting of all available digitizations. The collections of digital documents already available in 
                    Fictions littéraires de Gallica have been significantly expanded thanks to the semi-automatical retrieval of documents digitized by Google Books and other online digital library. The creation of this composite corpus underline that digitized collections may be already more representative than expected, although nobody knew it or could measure the scale of it.
                
                Using the combined collections of several digital libraries made it possible to cover a large amount of the novels registered in the catalog of the French National Library. For the 1470-1600 period, we have retrieved 275 novels out 349, that is 78.8% of the corpus. For the 1600-1700 period, we have 724 novels out of 1058, that is 68.4% of the corpus. Representativeness ratio is not only high but consistent on the entire time period as shown in figure 
                    2 as the size of the available digitized corpus remains proportional to the total amount of novels identified by the French National Library.
                
                
                    
                        
                    
                
                Figure 1: Composition of the corpus by decade
                Such high coverage seems to alleviate most concerns related to the representativeness of digital corpora: it becomes less likely that important genres or themas are neglected. Yet the novels registered on the French National Library catalog do not encompass the total sum of literary fiction published or circulated on the period. The comparison with Google Books showed that numerous editions were not recorded by Gallica. In the context of the project, we only checked novels recorded on the catalogue, but this discrepancy shows that some published novels could definitely have been overlooked, although by definition these documents are not necessarily expected to belong to those with the largest impact, as they failed to be noticed by bibliographic records. After the 19th century, nearly all published monographs are expected to be indexed in the catalogue of the French National Library due to progress in the implementation of the policy of legal deposit (
                    dépôt légal, established in 1537). Yet, at this time, a large amount of literature was increasingly being published in periodicals and newspapers.
                
                In short as representativeness of existing library catalogues becomes less of a concern, digital collections are bound to raise more complex issues and address a more critical perspective on existing bibliographic resources: what is a “novel”? what is a “publication”? what about piracy edition or periodical fictions?
                The creation of the third corpus also aims to benefit from unprecedented progresses in the detection of historical OCR. The OCR17+ model trained by Claire Jahan and Simon Gabay (2021) on a collection of 17th century prints already yields a usable text for most of the 16th to 18th century documents
                     Jahan and Gabay (2021). This model will be applied on the entire corpus. Preliminary results on a random selection of 60 pages from the corpus show that OCR quality is already high for the 17th century (nearly 99% character accuracy). The 16th century corpus is more challenging due to specific issues with Gothic fonts and segmentation, although we intend to solve these limitations by fine tuning a model for Gothic fonts in incunabula and 16th century prints. In comparison, the mean OCR quality in the collections held by Gallica is at roughly 80% for the 16th and 17th centuries (according to available metadata) and a large part is currently unavailable for full-text search, possibly because OCR quality was too low. The project will also benefit from the development made by the ongoing Gallic(orpor)a project, that aims to provide a fully reusable pipeline for Gallica documents in French from the Middle Ages to the Revolution. By the end of this project, we plan to create a small search engines of the entire collected corpus of 15th to 17th century documents. This would be the first resource to give access to a large share of the published literature in France in the early modern period. An initial version of this search engine may be available by the time of the conference and serve as a demonstration of our methodology of systematic collection and enhancement of available digitized collections..
                
            
            
                
                    Modelling literary genre: a back and forth approach
                    
                
                Chivalric romance is one of oldest and most enduring genre of European literature. It finds its origin in Old French epics, known as 
                    chansons de geste, whose first attestations go back to the 11th century, and in the genre of 
                    roman that emerges in the second half of the 12the century. Continuously modified, adapted and rewritten during the Middle Ages and the Early Modern times, this repertoire of fictions provided numerous tropes and patterns that arguably live on in more modern and even contemporary literature, especially in popular literature and even fantasy novels.
                
                Preliminary exploration of our large corpus of French literary fictions suggests that the chivalric novel, rather than ceasing to exist, has been continually evolving and gradually morphed into forms close to the contemporary fantasy novels. This structural transformation has been largely overlooked as forms of chilvalric romances have largely disappeared from the high literary canon after the Renaissance and continued to evolve in the production of lesser known and more obscure authors.
                Our analysis relies on a anachronistic use of classification recently pioneered in cultural analytics . Probability rates, cross-classifications of the same text and, even, classification failures are reinterpreted as a way to measure the complex evolution of literary genres.
                To investigate the transformation of chivalric novels into contemporary genres like fantasy, we created two historical models of literary genres: a 21st century model and an early modern “Fresnoy” model (from a 1731 catalog). The combined use of anachronistic classification aims to locate the two parallel processes of genre survival/transformation (for chivalric romance) and genre emergence/coagulation (for fantasy).
                Classification was made with SVM using an R library initially developed for the classification of newspaper genres in the 19th century, 
                    TidySupervise .
                
                The 21st century model is created using nine genre categories of the social cataloguing website 
                    Babelio
                    
                        .
                    . User-generated tags have recently emerged as an important source of information in computational analysis of contemporary literature and literary reception . A French counterpart to 
                    Goodreads, 
                    Babelio has a significant impact among French literary readers with nearly 1 million visits per month. In this project, we focused on a subset of generic tags that were either major acknowledged genres in contemporary literature or relevant for our ongoing projects: romance, fantasy, detective fiction, science fiction, historical novel, adventure novel, social novel, 
                    fantastique novel and erotica. Obviously, this is not a straightforward classification, since one novel could belong to several categories. We aimed rather to reconstruct the fuzzy space of contemporary literary genres in France with all its underlying uncertainties and overlaps.
                
                The model was trained on 4,081 segments of 1000 words extracted from 1,346 novels. We applied a random selection of three 1000 words segments by work. This selection aims to limit over-fitting and ensure that the model will be correctly trained on generic features and not on the style of specific novels. Bootstrap evaluation of the model yield a 75% accuracy, yet with significant variations among the genres (fantasy being the highest rated with science fiction and erotica).
                Preliminary results from the 21st century model have revealed significant examples of “missing links” between the early modern chivalric romances and the fantasy novels (Table 1). Of special interest is 
                    La Mort de Roland a 1858 rewrite of the 
                    Chanson de Roland by Alfred Assolant that explicitly claims to be an 
                    epic fantasy (
                    Fantaisie épique).
                
                
                    
                        
                        Table 1. Top 10 works classified as fantasy in the corpus of 19th century novel digitized by Gallica
                    Most of these works are poorly attested in literary history. In comparison, the results from Science-fiction yields much more expected and “canonic” works (especially from Jules Verne).
                
                The early modern model (or “Fresnoy” model) has been made possible by an exceptional historical source on literary genre classification: the second volumes of 
                    De l’usage des romans, où l’on fait voir leur utilité & leurs différens caractères by Nicolas Lenglet du Fresnoy (first published in 1731 under the pseudonym of Gordon de Percel). It is a catalog of a large among of French, Spanish and Italian novels published since 1731 broken down by genres according to the prevalent taxonomy of the time: 
                    Roman de chevalerie, 
                    Roman d’amour, 
                    Roman historique, 
                    Roman comique, 
                    Roman politique, etc. Ongoing work aims to reconcile the classification of Lenglet du Fresnoy with our corpus of digitized novels. Preliminary results suggest that the generic identity in the catalogue of Fresnoy is much stronger than in the Babelio dataset, with as much as 93% accuracy on four genres (
                    Roman d’amour, 
                    Roman historique, 
                    Roman de chevalerie and 
                    Roman comique & satirique) in our initial run. While this high accuracy may be caused by overfitting on a limited samples of novels, it seems also consistent with the significance of genre classification in the meta-discourse about the novel in the 18th century.
                
                
                    
                        
                    
                
                Figure 2: Main words used in the four categories of the Fresnoy model
                A this stage, the expansion of the corpus and the re-digitization of available documents with historical models for OCR will be crucial to move beyond an exploratory phase and design a more systematic examination of the metamorphosis of literary genre.
            
        
        
            
                
                    Bibliography
                    
                        Camps, J.B.
                         et al. 
                        (2019). 
                        Geste: un corpus de chansons de geste, 2016-... (Version 02). Paris. URL 
                        . 
                    
                    
                        Fièvre, 
                        P.
                         
                        (2007). Théâtre classique. URL 
                        .
                    
                    
                        Jahan, C. and Gabay, S. (2021). 
                        OCR17+ - Layout analysis and text recognition for 17th c. French prints, Paris/Genève: ENS Paris/UniGE, https://github.com/e-ditiones/OCR17plus.
                    
                    
                        Langlais, 
                        P.C.
                         
                        (2021). Classified News, Redefining the history of newspaper genre with supervised models. In 
                        Digital Newspaper: a new Eldorado for the historians. De Gruyter.
                    
                    
                        Langlais, 
                        P.C.
                         
                        (2021
                        b
                        ). 
                        Fictions littéraires de Gallica / Literary fictions of Gallica, URL 
                        .
                    
                    
                        Moretti, 
                        F.
                         
                        (2009). Style, Inc. Reflections on Seven Thousand Titles (British Novels, 1740–1850), 
                        Critical Inquiry, 36(1):134–158, 2009. doi: 10.1086/606125. URL 
                        .
                    
                    
                        Odebrecht, 
                        C.
                        , Burnard, 
                        L.
                        , and Schöch, 
                        C.
                         
                        (
                        2021
                        ). European literary text collection (eltec).
                    
                    
                        Calvo Tello, 
                        J.
                         
                        (2021). 
                        The Novel in the Spanish Silver Age: A Digital Analysis of Genre Using Machine Learning. Bielefeld University Press. doi: 10.1515/9783839459256.
                    
                    
                        Underwood, 
                        T.
                         
                        (2019). 
                        Distant Horizons: Digital Evidence and Literary Change. University of Chicago Press.
                    
                    
                        Walsh, 
                        M.
                         and Antoniak, 
                        M.
                         
                        (
                        2021
                        ). The goodreads “Classics”: A computational study of readers, amazon, and crowdsourced amateur criticism. 
                        Journal of Cultural Analytics, 6(2). doi: 10.22148/001c.22221.
                    
                
            
        
    



        
            With a few limited exceptions 
                (Blanke et al., 2020), the application of machine learning (ML) within the historical research process maintains a strong human-in-the-loop element that limits the extent of its proliferation. Part of the reason for this is surely the omnipresence of certain kinds of uncertainty in historical research, which the traditions of historiography have developed powerful (albeit analogue) tools to manage. As Myles Lavan recently suggested, the persistence of these longstanding methods may not be ‘a mistaken belief that uncertainty about the past is qualitatively different from that faced by other disciplines,’ 
                (Lavan, 2019) however. Instead, we propose that ML/AI methods challenge one of the most fundamental and foundational elements of historical research, namely provenance, in ways that are not simple to resolve or document. In historical research, provenance typically refers to the record of where an object, collection, or dataset has come from and the places and ‘experiences’ (additions, transformations, deletions, etc.) it has had since its original documentation. The entry of ML methods into DH might be changing the limits and implications of this definition. 
            
            For such methods to be meaningfully applied to historical research, provenance needs to be reconsidered, modelled from multiple perspectives, and documented differently from the current standards in computer science. Specifically, data transformations that are no longer performed by human actors but by autonomous or semi-autonomous computational systems need to be captured to enable provenance management. Conversely, historians’ reliance on provenance requires that we (a) agree upon a shared definition of data provenance and (b) ensure that ML systems designed for use in this specific context maintain legibility. Such a negotiation between research fields will require more than the current research in explainable AI promises to deliver, making the computational provenance not only be reconstructed but also comprehensible in the multidisciplinary space of DH.
            The research project “PROgressive VIsual DEcision-Making in Digital Humanities” (PROVIDEDH, 2017-2021) has contributed to this requirement by proposing a Visual Analytics (VA) approach to representing and managing uncertainty in DH research and demonstrating how a better communication of human or machine-induced uncertainty can enhance the user experience for humanities scholars using ML models. Among other outputs, the project developed an HCI-inspired uncertainty taxonomy (see Figure 1) differentiating between two main types of uncertainty: human-made and technology-made, which correspond to aleatoric (irreducible) and epistemic (reducible) uncertainty as per previous works in the literature 
                (Edmond, 2019; Therón Sánchez et al., 2019; Simon et al., 2018) (Edmond, 2019; Therón Sánchez et al., 2019; Simon, 2017). 
            
            
                
            
            
                Figure 1: Proposed uncertainty model. Top: human-induced uncertainty with four predefined categories that map to the epistemic categories previously introduced by Fisher and others. Users can add more categories on a per-project basis if required. Bottom: Machine-induced uncertainty showing the results of applying N different algorithms to the data.
            
            The first uncertainty category, technology-induced, can be mapped to aleatoric uncertainty (well-defined objects in Fisher’s 
                (Fisher, 1999) taxonomy) and results from applying computational algorithms to the data, which often give their results with a variable degree of bounded uncertainty (e.g., topic models). For this reason, this type of uncertainty is better represented as a continuous probability distribution. In addition, this representation allows a better understanding of speculative runs of a given algorithm and enhances the what-if analysis process. For example, a researcher could parametrise an algorithm with a fixed set of inputs and launch it several times, obtaining a range of mean values and deviations encoded in a probability distribution function (PDF), which, if correctly displayed, would allow her to get an idea of how the algorithm behaves. Analogously, the algorithm could be parametrised with a variable set of inputs created by the user running the computation or by other researchers. This operation mode would answer the questions of “what happens if I run the algorithm n times using my assumptions?” or “what happens if I run the algorithm n times using another person’s assumptions?” As in the case of running the algorithm with the same parameters many times, the results of multiple runs with different parameters could also be summarised in a continuous PDF, allowing the desired kind of what-if analysis. We argue this kind of insights are highly valuable, specifically in the case of probabilistic algorithms, such as topic models or word embeddings, and whose results –and thus, interpretations– can vary significantly between different runs 
                (Alexander and Gleicher, 2016).
            
            The other category, human-induced uncertainty, arises from 1) direct interpretations of the raw data (which in turn may be based on others’ previous interpretations and grounded expert knowledge of the user), 2) interpretations of computational analyses performed on the data, or 3) most likely, a combination of the two. Human actors report this category on a 5-point Likert scale, which is thus best modelled as a discrete PDF. The relationships of dependency between the categories in our taxonomy are bidirectional and self-recurring since, for example, input parameters and data — and therefore the results — are derived from a user’s previous interpretations of textual data and related machine- or human-generated annotations. In turn, these interpretations must necessarily be built upon previous insight obtained by the same or other users who apply computational techniques to the data. This creates a temporal belief network 
                (Druzdzel and Simon, 1993; Pearl and Mackenzie, 2018) (see Figure 2) in which the actors’ perspectives are fixated on the different versions of a dataset.
            
            
                
            
            
                Figure 2: A Bayesian Belief Network formed by different interactions of a machine and human actors with the data. Each of these interactions produces a new version of the data, which may, in turn, be used as an input by another actor to create a more recent version. 
            
            Our taxonomy was evaluated in different user studies 
                (Benito-Santos et al., 2021), and it can be used by other researchers in a digital research platform (
                https://providedh.ehum.psnc.pl/). Although this kind of encoding may still feel foreign to many researchers trained in the traditions of historical methods, it will only be through this kind of convergence between the affordances and constraints of ML on the one said, and the values and tolerances of historical research on the other, that we will be able to see more widespread integration of ML into historical research workflows.
            
        
        
            
                
                    Bibliography
                    
                        
                        Alexander, E. and Gleicher, M. (2016). Task-Driven Comparison of Topic Models. 
                        IEEE Transactions on Visualization and Computer Graphics, 
                        22(1): 320–29 doi:10.1109/TVCG.2015.2467618.
                    
                    
                        Benito-Santos, A., Doran, M., Rocha, A., Wandl-Vogt, E., Edmond, J. and Therón, R. (2021). Evaluating a Taxonomy of Textual Uncertainty for Collaborative Visualisation in the Digital Humanities. 
                        Information, 
                        12(11). Multidisciplinary Digital Publishing Institute: 436 doi:10.3390/info12110436.
                    
                    
                        Blanke, T., Bryant, M. and Hedges, M. (2020). Understanding memories of the Holocaust—A new approach to neural networks in the digital humanities. 
                        Digital Scholarship in the Humanities, 
                        35(1): 17–33 doi:10.1093/llc/fqy082.
                    
                    
                        Druzdzel, M. J. and Simon, H. A. (1993). Causality in Bayesian Belief Networks. In Heckerman, D. and Mamdani, A. (eds), 
                        Uncertainty in Artificial Intelligence. Morgan Kaufmann, pp. 3–11 doi:10.1016/B978-1-4832-1451-1.50005-6. http://www.sciencedirect.com/science/article/pii/B9781483214511500056 (accessed 2 December 2020).
                    
                    
                        Edmond, J. (2019). Strategies and Recommendations for the Management of Uncertainty in Research Tools and Environments for Digital History. 
                        Informatics, 
                        6(3): 36 doi:10.3390/informatics6030036.
                    
                    
                        Fisher, P. F. (1999). Models of uncertainty in spatial data. 
                        Geographical Information Systems, 
                        1: 191–205.
                    
                    
                        Lavan, M. (2019). Epistemic Uncertainty, Subjective Probability, and Ancient History. 
                        The Journal of Interdisciplinary History, 
                        50(1): 91–111 doi:10.1162/jinh_a_01377.
                    
                    
                        Pearl, J. and Mackenzie, D. (2018). 
                        The Book of Why: The New Science of Cause and Effect. 1st ed. USA: Basic Books, Inc.
                    
                    
                        Simon, C., Weber, P. and Sallak, M. (2018). 
                        Data Uncertainty and Important Measures. John Wiley & Sons.
                    
                    
                        Therón Sánchez, R., Benito-Santos, A., Santamaría Vicente, R. S. and Losada Gómez, A. (2019). Towards an Uncertainty-Aware Visualization in the Digital Humanities. 
                        Informatics, 
                        6(3): 31 doi:10.3390/informatics6030031.
                    
                
            
        
    



        
            Metadata analytics is a relatively new approach among the many data-driven methodologies engaged with the analysis of culture (e.g. Manovich, 2020; Michel et al., 2011; Moretti, 2013; Rogers, 2013). Using descriptive metadata for research, however, has a well-established tradition in the field of bibliometrics and in particular scientometrics. One of the reasons for the maturity of those fields of research is the long-standing availability of the data itself.
            The Japanese Visual Media Graph (JVMG) project, following in the footsteps of the Databased Infrastructure for Global Games Culture Research (diggr) project, is premised on the idea that there exist rich resources on various cultural subfields compiled by online fan or enthusiast communities. By working with these communities towards integrating these descriptive metadata resources into a single knowledge graph for a specific domain – in this case Japanese visual media such as anime, manga, video games and so on – the project aims to open up new avenues of quantitative analysis for researchers in the field, and at the same time provide a template for building similar resources in other areas of inquiry. Although the creation of open knowledge graphs in the digital humanities and the cultural heritage field specifically is becoming increasingly common (see for example Bikakis et al., 2021; Haslhofer et al., 2018), working with data compiled by online fan or enthusiast communities opens up a rich range of new possibilities for research.
            The project, which is funded by the German Research Foundation’s (Deutsche Forschungsgemeinschaft, DFG) e-Research Technologies program, will be reaching the end of its first project phase in 2022. After three years of work we present the most important aspects of both the knowledge graph that was created (available at https://mediagraph.link/) and the development process that made it possible.
            First, we discuss our data sources, the dimensions of the JVMG knowledge graph, and its coverage. We also present our approach to and results of measuring data quality within the project with an emphasis on the accuracy and completeness of the data. This question is especially important in order to, on the one hand, validate the feasibility of using community compiled data for research; and on the other hand, to be able to provide a clear picture for researchers regarding what to expect in relation to the capabilities and limits of the data.
            Second, we explain the most important steps and challenges of the data integration process. Working with heterogeneous data sources and ontologies was made possible by transforming all tabular data sources into an RDF linked data format. Matching the data points between the various ingested data sources was one of the significant technical challenges that the project had to resolve. Thus, our experiences with data matching and how much of it was actually automatable is also discussed. All software tools developed for the data ingestion, processing and matching are made openly available online.
            Third, the legal harmonization of the JVMG data, which surprised us with its complexity, is another important aspect of the knowledge graph development that we explain in more detail. Licensing and legal concerns are often an afterthought even in scientific projects. In our case, however, since we are working with heterogeneous data sources and an array of corresponding different licensing practices, the question of how to go about harmonizing the licenses of the various data sources became a central problem. This issue had to be solved for us to be able to open up the knowledge graph to researchers around the globe. Our solution, which we introduce along with the challenges that it had to overcome, involved settling on the Creative Commons BY-NC-SA (attribution, non-commercial, share-alike) 4.0 license as the smallest common denominator and requesting individual license agreements from communities whose licenses were not compatible with it.
            Fourth, one of the most important ideas underlying our development process was that it had to be directed by the actual needs of researchers working in the field. In order to implement a development process that could receive regular feedback from the domain experts working with the data we adopted and further refined the Tiny Use Case (TUC) methodology introduced by the diggr project (Freybe et al., 2019). This approach builds on ideas from agile software development practices. Each TUC is a small research project that can be tackled within a three to four months long period. Not only do the TUCs serve as examples for what type of research questions can be pursued with the help of the JVMG knowledge graph, but they also provide valuable lessons in relation to potential problems in data quality, generate new feature requests for the graph frontend – developed based on the Pubby project –, and help identify further types of data that the domain experts would like to be able to work with in the knowledge graph. Last, but not least, each TUC is an opportunity for the team members working on the IT and library and information science side of the project and the humanities and social science researchers to learn from each other and develop a common language and understanding for setting goals and discussing problems. We provide an overview of the TUCs that were conducted in the project and highlight the way they shaped the development of the graph frontend and the knowledge graph itself.
            Our hope is that the JVMG project can not only showcase the power of integrated research resources created from data sources compiled by online fan and enthusiast communities, but also provide an array of potential templates (from data integration, legal harmonization and development workflow solutions) for building similar knowledge graphs in other domains of interest.
        
        
            
                
                    Bibliography
                    
                        Bikakis, A., Hyvönen, E., Jean, S., Markhoff, B. and Mosca, A. (eds) (2021). Special Issue on Semantic Web for Cultural Heritage. 
                        Semantic Web, 12(2).
                    
                    
                        Freybe, K., Rämisch, F. and Hoffmann, T. (2019). With Small Steps to the Big Picture: A Method and Tool Negotiation Workflow. In Krauwer, S. and Fišer, D. (eds), 
                        Proceedings of the Twin Talks Workshop at DHN 2019 (CEUR Vol-2365). Aachen: CEUR-WS.org, pp. 13-24.
                    
                    
                        Haslhofer, B., Isaac, A. and Simon, R. (2018). Knowledge Graphs in the Libraries and Digital Humanities Domain. In Sakr, S. and Zomaya, A. (eds), 
                        Encyclopedia of Big Data Technologies. Cham: Springer, pp. 1-8. doi: 10.1007/978-3-319-63962-8_291-1.
                    
                    
                        Manovich, L. (2020). 
                        Cultural analytics. Cambridge, MA: MIT Press.
                    
                    
                        Michel, J. B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., The Google Books Team, Pickett, J. P., Hoiberg, D., Clancy, D., Norvig, P., Orwant, J., Pinker, S., Nowak, M. A. and Aiden, E. L. (2011). Quantitative analysis of culture using millions of digitized books. 
                        Science, 331(6014): 176-182.
                    
                    
                        Moretti, F. (2013). 
                        Distant reading. London: Verso.
                    
                    
                        Rogers, R. (2013). 
                        Digital methods. Cambridge, MA: MIT press.
                    
                
            
        
    

