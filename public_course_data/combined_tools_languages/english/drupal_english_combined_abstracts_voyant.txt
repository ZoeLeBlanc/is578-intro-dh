
Q: "What weighs 54 tons and can be held in the palm of your hand?"
A: "The AIDS Memorial Quilt."

AIDS Quilt Mobile Web App Development Team:

Mark NeuCollins (Lead Developer, Database, PHP, Drupal, CSS)
Lauren Haldeman (Drupal, CSS, jQuery Mobile)
Nikki Dudley (PHP, Drupal, Database, Import Scripting)
Kelly Thompson (Drupal, Mapping, Experimental Module Research, User Experience)
Kayla Haar (Graphic Design)
Jon Winet (Project Director, NAMES Foundation Liaison, Editor, Publicity Information)
The AIDS Memorial Quilt is the largest living monument in the world. Composed of over 48,000 three foot by six foot individual panels, it pays tribute to the lives of more than 98,000 individuals who have died during the AIDS pandemic. Maintained by the NAMES Foundation in Atlanta, each panel is painstakingly hand-crafted by those who knew and loved these individuals. Each panel carries the emotional weight of a life lived, of loving relationships, and of heartfelt loss.

Our interdisciplinary team —drawn from backgrounds in information technology, the humanities, and public art, and including undergraduate and graduate students, staff, and faculty — would like to share our experiences building a public digital humanities project that provides access to a virtual experience of the Quilt. Provided with data stored in a structure created in the 1980s, we were tasked with transforming this arcane information repository into something robust, agile, and modern. We aimed to design a system that would be usable by the community surrounding the quilt, comprised of people from all walks of life. The process of creating this public digital humanities project, with its goal of preserving the culture and purpose of the original Quilt, presented many learning opportunities we believe could be of value to others seeking to undertake similar projects. If accepted, we propose a discussion around the cultural, technological, artistic, and community-driven aspects of developing this project. We would also like to discuss the outpouring of community-sourced stories, memorials, and heart-felt comments contributed in the space of this digital memorial by those who visited the Quilt this summer, both physically and virtually.

Working in concert with project director Anne Balsamo at the USC Annenberg Innovation Lab, the University of Iowa Digital Studio for Public Humanities (DSPH) developed "AIDS Quilt Touch." A mobile web application for mobile devices and laptop|desktop computers, AIDS Quilt Touch is a digital extension of the Quilt, which celebrated its twenty-fifth anniversary this year.

DSPH was approached last spring to create this digital version of the Quilt in conjunction with a display of all 48,000 panels on the National Mall in Washington DC in July 2012. The immediate purposes of the app were to allow visitors to the Mall to find the panel of their loved one, and to leave digital remembrances. The timeline for the project was formidable, with just two months to produce a functional beta version of the app.

With records dating from the 1987, the NAMES database is a cobbled-together collection. Our first task was to parse the flat spreadsheet document from the NAMES foundation into a relational database format that we could use. Simultaneously we needed to learn enough about the Drupal Content Management System to deliver a complete and stable mobile web app based on these data. This web app was to be used by thousands of people trying to find the panel of their loved ones during the quilt’s display in Washington DC. The stakes were high, the challenges were great, but in an incredibly satisfying and successful collaborative effort, we pulled it off. You can see the results of our efforts here: http://www.aidsquilttouch.org/

To the question of “When will the quilt be on display?” we can now answer, “It is always on display.” To the question of “Where is it being displayed?” we can now answer, “Everywhere.” There is much that we can add to the quilt application, and plan to continue development with future displays of this living monument.

The AIDS Quilt Touch mobile web app allows people to leave comments, to extend the narrative that the quilt has begun to tell, and to create virtual celebrations of the lives lived. We plan to expand the types of media users are able to add in the near future: photographs, audio, video, and information and metadata about the quilt panels, those who constructed the panels, and those whose lives are memorialized by the panels.

The celebrations of life that users have left on the mobile app convey a deep resonance with the heart of the human experience. It is these comments that begin to show the promise of the technology. This is the good stuff—the material of human culture. It is not the mobile web app that is important, but the possibility that this technology can facilitate a deep conversation, can create a well of experience from which we can all draw. The mobile web app points to the possibility that these devices we carry in our pockets hold the potential to be portals to a larger and more inclusive cultural realm.
Introduction
One barrier to locating serialized fiction in a digital newspaper archive is the fact that the serialized fiction themselves are not indexed, and individual articles do not have subject terms or tags associated with them that would identify them as fiction. As a result, articles are difficult to find unless the reader browses a large volume of issues or simply hits upon a salutary keyword search. Keyword searching of the collection is more effective for articles on topics in farming or farm life than for works of fiction. Unless the reader is looking for stories by a specific author or for a known story title, keyword searching of fiction is highly ineffective.

While the software used by most archives does a good job of connecting articles in a single issue, the reader does not know where to find the next installment in a serialized work of fiction, so he or she has to find it manually by browsing the collection or doing a keyword search. Finally, while the OCR scanning was done to the highest standard, this is an imperfect process, and much of the content cannot be adequately OCR’d due to background noise and broken letters, features of the original newspapers that impede scanning.

This paper summarized the process of our project that was completed over 15 weeks in the summer of 2012. Our goal was to complete the manual indexing process that had already been started previously, display serialized fiction articles in a new repository, evaluate multiple software packages to see which ones were the most promising for use in the future, and evaluate any automated ways of finding serialized fiction.

Method and Results
Manual Indexing
We experimented with three digital library systems, a Drupal/Fedora based repository (Islandora) (http://islandora.ca/), converting the fiction into TEI P5 and displaying it in the California Digital Library’s eXtensible Text Framework (XTF) (http://www.cdlib.org/services/publishing/tools/xtf/) and Omeka (http://omeka.org) a PHP-based publishing platform for digital library objects. We were unable to get Islandora’s OCR correction module installed so we stopped using it in favor of Omeka. We used XSLT to transform the PrXML into very simple TEI5 files, which we were able to upload to XTF, but the lack of an editor and the intensely manual process of text encoding was also rejected in favor of Omeka. TEI Example: http://uller.grainger.uiuc.edu:8080/xtf/search

Ultimately, we decided on using Omeka with the Scripto Plugin for correction. Serialized fiction articles in one title, the Farmer’s Wife, was manually indexed in a spreadsheet, and graduate assistants converted those stories from PrXML into an exhibit, added Dublin Core metadata and links to the newspaper archive from the new serialized fiction collection. The end result was index of serialized fiction that would increase the accessibility of these articles. Omeka Exhibit: http://uller.grainger.illinois.edu/omeka/

Crowdsourcing OCR correction
The University of Illinois Digital Newspaper collections are in Olive Software ActivePaper Archive, which has a method for administrators to correct text but not users. Omeka provides a plugin called ‘Scripto’ for text correction that we were able to successfully use to correct the text in selected articles. We also evaluated Veridian (http://www.dlconsulting.com/veridian/), which is a commercial digital newspaper library solution used by Trove Digitised Newspapers (National Library of Australia), From the Page (http://beta.fromthepage.com/) and Islandora (http://www.islandlives.ca/). From the Page and Islandora were both very difficult to install and administer, and while not free we felt Veridian was a much better approach and we are evaluating it as part of our future newspaper digitization efforts.

Text Analysis
How can we identify serialized fiction without having to have a human find it, index it in a spreadsheet and manually extract it from the archive? Certain n-grams are common within serialized fiction such as ‘chapter’, ‘the end’, ‘to be continued’ and could be used to simply search for keywords within documents; we could also calculate which words occur most frequently in fiction vs. other types of articles and use those terms to automatically tag articles.

We also evaluated using topic analysis to find fiction. We evaluated the 580 articles we had already identified as serialized fiction using Mallet to find 25 topics with 25 words each. Figure 1. shows the top 25 topics modeled as a network using Gephi, while figure 2 shows the topic words ordered by frequency.


Figure 1


Figure 2

Nodes were ranked by betweenness centrality and topic 14 had the highest at 51,321.01 and its component n-grams along with the other top topics could be used to find serialized fiction in other titles.

One final text analysis technique that could be useful is identifying proper names is Named Entity Extraction. While we made an effort to manually remove names from the topic analysis, as you can see they kept reappearing in the results. By using named entity extraction we could eliminate proper names from the topic analysis to make them more accurate, and to link fiction together by the character’s names. All three of these techniques (keyword frequency, topic analysis, named entity extraction) I plan on evaluating in a future study.

Conclusion
Serialized fiction is an important component of historical newspapers and by making it more accessible to patrons and researchers we can expand the use and usefulness of our digital newspaper collections. The manual indexing approach was relatively inexpensive to accomplish but was time consuming and difficult to do over a large corpus of pages. Two promising approaches to find and digitize serialized fiction in our newspaper archive are adding a crowdsourcing feature to enable users to identify article types and correct mistakes, and utilizing text analysis techniques to identify fiction programmatically. We hope to report on our efforts at the latter at the DH 2013 conference.

References
Bastian M., S. Heymann, and M. Jacomy (2009). Gephi: an open source software for exploring and manipulating networks. International AAAI Conference on Weblogs and Social Media.
Brandes, U. (2001). A Faster Algorithm for Betweenness Centrality. Journal of Mathematical Sociology25 163-177.
Cohen, D. (2008). Introducing Omeka. Dan Cohen's Digital Humanities Blog. http://www.dancohen.org/2008/02/20/introducing-omeka/.
Islandora. http://islandora.ca/.
Island Lives. http://www.islandlives.ca/
McCallum, A. K. (2002). MALLET: A Machine Learning for Language Toolkit. http://www.cs.umass.edu/~mccallum/mallet.
Veridian. http://www.dlconsulting.com/veridian/.
Overview
Bibliopedia, which recently completed an NEH Digital Humanities Start-Up Grant, performs data-mining and cross-referencing of scholarly literature to create a humanities-centered collaboratory. Currently a working prototype, Bibliopedia can search resources including JSTOR and Library of Congress for metadata about scholarly articles and books, examine the articles and books for citations, then present the results in a publicly accessible database. Bibliopedia is designed to work with all humanities scholarship. It will also allow users to create browsable and customizable bibliographies of all the works cited by each article and book. Most importantly, it uses semantic web technology to enable automated textual analysis, data extraction, cross-referencing, and visualizations of the relationships among texts and authors. Using existing open source software, it extracts citation data from existing plain text resources and transforms them into linked open data. This process makes the information easily accessible to the wider scholarly and linked data communities, enables network visualizations of the scholarly landscape. This presentation will cover the details of the Bibliopedia system to show others how they can replicate it. We will also offer to all interested academic parties our existing installation and hosting platform for their experimentation. In particular, we will present our Drupal-based semantic wiki, which features a full web services API, and our custom citation crawler.

Linked open data, one of the core technologies of the semantic web, promotes open sharing of digital scholarly research while it encourages further, potentially unexpected uses. Bibliopedia's method for incorporating linked open data (via RDFa) requires only minimal technical expertise to reproduce. One of the central components of Bibliopedia is the Drupal content management sytem (CMS), which as of version 7 exposes data via RDF/RDFa as part of its core functionality. This functionality, moreover, is not limited to Drupal. For example, Omeka, another CMS developed at the Roy Rosenzweig Center for History and New Media, George Mason University, has some limited support for linked data through its DublinCoreExtended plugin. Bibliopedia demonstrates the power and flexibility of Drupal's approach to linked data while providing more general lessons for digital humanists who seek to incorporate this technology into their projects.

Project Details
Bibliopedia will aid humanities researchers of all levels of expertise by making simple the currently difficult tasks of discovering new scholarly works and the relationships among them. It will create an a scholarly community to verify and elaborate cross-referenced, linked bibliographic data through easy-to-use wiki pages. Scholarly literature will become browsable not only backwards in time, but also forwards, something that is currently impossible.

The semantic web is transforming the Internet from a collection of pages and data readable only by humans to one that machines can understand and process. Semantic web technology promises the ability automatically to determine meaning and then infer connections among different elements, thereby vastly improving search capabilities, discovery of new information, and the overall usefulness of the Internet. Just as information accessible only to humans comprises the great majority of the general Internet, so too is data about scholarly literature locked away in text that computers cannot process without great difficulty. At best, search engines for repositories such as JSTOR permit researchers to query author name, journal titles, and keywords, but once a work is found, the search stops. No connections among works are found precisely because machines cannot currently read that data. Although Google Scholar attempts to show citations of articles, its usefulness is highly limited because it does not make clear the relationships among articles, present very limited metadata about each article (if any), fails to provide for community elaboration or correction, and includes only works that are publicly available. Yet despite its limitations, Google Scholar stands as a significant technological advance beyond keyword-based search engines such as those provided by JSTOR and Project Muse.

Bibliopedia will, by aggregating data from as many sources as possible, converting citations into semantic web format, and then cross-referencing an ever-growing database of scholarly works, be able not only to overcome many of the limitations of Google Scholar and become a powerful research tool in its own right, but also to make a valuable contribution to the growing semantic web. Introducing high quality metadata about humanities scholarship to the semantic web will enable others in the semantic web/linked data world to process that data in new, unexpected ways that will accrue further benefits to the scholarly community. For example, the standards underlying the semantic web make data visualization and automated inferences about relationships trivially easy rather than the complex problems such tasks currently present. Bibliopedia will, then, through the innovation of placing metadata about scholarly literature into a linked data format, open up a vast range of possible future innovations and analyses based on that data, which is currently locked away and readable only by select humans.

Another virtue of a linked data format is that it will help resolve many of the challenges inherent in metadata, some will inevitably remain. Rather than attempt to solve this incredibly complex problem through automation alone, then, Bibliopedia will, in the process of displaying its results for human consumption, also provide for human feedback in the form of correction and elaboration. A common disadvantage of fully automated text analysis and data extraction tools such as Google Books, Google Scholar, and other digital research tools is that their automatic parsers have errors in their metadata that they do not allow subject matter experts to repair. Bibliopedia will pursue the goal of unifying that information into an environment that not only displays the information efficiently, but actively encourages crowd-sourcing metadata on books, articles, and publications of all kinds. In thus opening data up to revision by the scholarly community, Bibliopedia can build on the strong work of mature data silos, improve overall data quality, and provide the academic community at large a continuously evolving research tool.

There currently exists a multitude of projects and tools designed to work with book metadata, cross-reference scholarly articles (localized to the sciences), or create user communities around a chosen interest. Further, some of the most important trends currently revising the ways we use technology are social media, collaboration, and data aggregation. By incorporating the benefits realizable from each of these trends, Bibliopedia will create a powerful tool for scholarly research at all levels. None of the existing tools, however, focus on scholarship for the humanities, nor do they present the information in the linked data format necessary to the semantic web.
Introduction
The role of annotations in digital humanities is well known and documented (Agosti et al., 2004, 2007) (Bélanger, 2010). Subsequently, many different tools which allow for the annotation of digital humanities content have been developed. Unfortunately, tools designed specifically for an individual portal are typically only compatible with that system. More general solutions, which can be easily distributed across various sites, have been produced, but these systems often have limited functionality (only annotating a single content type, no sharing features etc.) (Okfn) (TILE, 2011).

FAST-CAT (Flexible Annotation Semantic Tool — Content Annotation Tool) is a generic annotation system that directly addresses this challenge by implementing a convenient and powerful means of annotating digital content. It provides a reliable, portable manner of annotating both textual and image content in documents. The annotations are stored remotely by the FAST service which means that they may be shared across different sites maintaining the same data without the need for modification.

FAST-CAT has been developed as a module for the Drupal 7 content management system making it extremely easy to add to an existing Drupal site.

This paper introduces FAST, the backend service providing powerful annotation functionalities, and CAT, the frontend Web annotation tool, and discusses how its features are tackling important challenges within the Digital Humanities field.

Features of FAST
The FAST annotation service adopts and implements the formal model for annotations proposed by (Agosti and Ferro, 2008). Since then, FAST has been completely reengineered with added functionality such as provenance, logging and extended searching. According to this model, an annotation is a compound multimedia object which is constituted by different signs of annotation. Each sign materializes part of the annotation itself; for example, a textual sign would contain the textual content of the annotation, an image sign would contain images, etc. Each sign is characterized by one or more meanings of annotation, which specify the semantics of the sign, e.g. a sign whose meaning corresponds to the “title” field in the Dublin Core (DC) metadata schema or a sign carrying a question from the author whose meaning may be “question” or similar.

The flexibility inherent in the annotation model allows us to create a connective structure, which is superimposed to the underlying documents managed by digital libraries. This can span and cross the boundaries of different digital libraries and the Web, allowing the users to create new paths and connections among resources at a global scale.

FAST defines three different scopes which determine the visibility of an annotation — private, public and group. With FAST-CAT, the scope of an annotation is set to private by default, meaning that only the person who created the annotation can see it. These annotations may serve as reminders or notes within the document for the user, akin to writing in the margin of a page.

The user can choose to make an annotation public, allowing other users to read their comments on a document. This has numerous applications for users of all levels of expertise. For instance, experienced users may choose to create public notes and annotations which can expand on the text of the document, helping less experienced users to comprehend the content. Less experienced users may indicate parts of the document which they would like to be explained further.

Group annotations allow users to provide viewing and editing permissions on an annotation to specific users. This means that a team of people working towards a similar goal could communicate directly through the medium of annotation. In this way, it can be seen that FAST-CAT can play a crucial role in collaboration.

Features of CAT
CAT is a web annotation tool whose development began in July 2012. It has been developed with the goal of being able to annotate multiple forms of document content and assist in collaboration in the field of digital humanities. At present, CAT allows for the annotation of both text and images. The current granularity for annotation of text is at the level of the letter. For image annotations, the granularity is at the level of the pixel. This allows for extremely precise document annotation, which is very relevant to the Digital Humanities domain due to the variety of different assets that prevail.

There are two types of annotation which may be created using CAT; a targeted annotation and a note. A targeted annotation is a comment which is associated with a specific part of the document. This may be a paragraph, a picture or an individual word, but the defining feature is that the text is directly associated with a specific entity. Conversely, a note is simply attached to the document. It is not associated with a specific item therein. Typically, this serves as a general comment or remark about the document as a whole. Further to allowing a user to comment on document text, the annotations created using CAT allow a user to link their annotations to other, external sources. Hence CAT can be used to construct a narrative through a number of documents. This is hugely beneficial for teachers using digital cultural collections and for students from primary to university level. For example, using these links a teacher can construct a predetermined path for their students to follow through a series of sites relevant to their chosen topic. Importantly, each link has comment text associated with it, allowing an educator to explain why this specific link is important or what the student should seek to gain from following this particular path.

While CAT is beneficial for researchers and educators, it is also being used as an important source of user data for the content provider. Websites such as Amazon and YouTube are able to provide increasingly accurate recommendations for their individual users. These recommendations are facilitated by a user model which is driven by a combination of ratings, recently viewed items and numerous other factors. For a digital humanities site, annotations provide an insight into which entities are of interest to a user. If a user is frequently annotating a document, it is likely that this document is of interest to them. Furthermore, if the text being annotated is analysed, it may be possible to discern specific items of interest within the document. A digital humanities site which can determine what a user is attempting to study, then anticipate and recommend sources that may be of use to them in the future is profoundly useful. If well implemented, curators of digital humanities portals will see a dramatic improvement in the effectiveness with which researchers interact with their domain.

FAST-CAT and CULTURA
At present, FAST-CAT is being developed as part of the CULTURA project (Hampson et al., 2012a, 2012b). A key aspect of CULTURA is the production of an online environment that empowers users, of various levels of expertise, to investigate, comprehend and contribute to digital cultural collections. FAST-CAT is a key component of this environment and is currently being trialled with the help of three different user groups.

A team of MPhil students and professional researchers will use the tool as part of their teaching, collaboration and research into the 1641 depositions. These users will be testing FAST-CAT in a free form manner. How they choose to annotate and what content they label is entirely determined by their own needs. The 1641 depositions are text only content, so these students will serve only to evaluate the text annotation aspect of the tool.

Providing an alternative insight is a group of secondary school students from Lancaster whose teacher will use the annotations to guide them through a lesson. These students will also be working with the 1641 depositions.

Masters students in Padua will test the image annotation functionality of FAST-CAT as part of their research into the IPSA collections of illuminated manuscripts. Similarly to the MPhil students, their approach to annotating documents will be determined by their own research methodology.

The various features offered by FAST-CAT and its user interface will be evaluated in detail and comparisons will be drawn between the manner in which different user groups availed of annotations depending on their level of expertise and document content. Furthermore, FAST-CAT will also help to drive CULTURA’s comprehensive user model by providing the site with updates on the user’s behaviour regarding document annotation.

Future Work
Much of the further enhancement of FAST-CAT will be based on the feedback given by the user groups mentioned in the previous section. However there are already plans to expand and improve the system for future versions.

While FAST-CAT is supported by modern browsers, to improve portability, implementations for older web browsers will be developed.

FAST-CAT is a Drupal 7 module which means that, at present, it is only available for the annotation of websites which are built using the Drupal content management system. However, it only utilises a small amount of Drupal functionality to relay messages from the client computer to FAST. This dependency is easily removed as the majority of functionality is either client side or independent of Drupal. Designing and implementing a more server agnostic php script will allow FAST-CAT to be deployed on any website. This is one of the main items for future development of the system and will help to ensure that FAST-CAT can be utilised by as wide a range of content based websites as possible.

References
Agosti, M., G. Bonfiglio-Dosio, and N. Ferro (2007). A Historical and Contemporary Study on Annotations to Derive Key Features for Systems Design. International Journal on Digital Libraries (IJoDL). 8.1. 1-19.
Agosti, M., & N. Ferro (2008). A formal model of annotations of digital content. ACM Transactions on Information Systems (TOIS). 26.1. 3:1-3:57.
Agosti and Ferro, I. Frommholz, & U. Thiel (2004). Annotations in Digital Libraries and Collaboratories — Facets, Models and Usage. In Proc. 8th European Conference on Research and Advanced Technology for Digital Libraries (ECDL 2004), 244-255. LNCS 3232, Springer: Heidelberg.
Bélanger, M.-E. (2010). Ideals. https://www.ideals.illinois.edu/bitstream/handle/2142/15035/belanger.pdf?sequence=2 (accessed October 25, 2012)
Hampson, C., M. Agosti, N. Orio, E. Bailey, S. Lawless, and O. Conlan (2012). The CULTURA Project: Supporting Next Generation Interaction with Digital Cultural Heritage Collections. Limassol, Cyprus.
Hampson, C., S. Lawless, E. Bailey, S. Yogev, N. Zwerdling, and D. Carmel (2012). CULTURA: A Metadata-Rich Environment to Support the Enhanced Interrogation of Cultural Collections. Cádiz, Spain.
Okfn. Okfn Annotator. http://okfnlabs.org/annotator/(accessed June 2012).
TILE. (2011). TILE: text-image linking environment. http://mith.umd.edu/tile/ (accessed July 2012).
Preservation of digital humanities (DH) projects is an emerging problem. As languages, frameworks, platforms, libraries, and databases evolve, the effort required to maintain meaningful access to existing projects is a challenge that competes with efforts to produce new work. Creators of DH projects need to be able to continue to iterate and innovate but also demonstrate good stewardship of digital materials.
The field recognizes this problem, from the 2004 Sustaining Digital Scholarship (SDS) final report and continuing through such projects as Memento, SiteStory, and TAPAS, and is actively researching ways to enable sustainable, long-term stewardship for certain project components, but these efforts do not provide a comprehensive platform for the full DH project. All of these efforts consider a project to be a set of static pages or resources that seldom change. None of them would be sufficient for hosting or describing ongoing projects such as World Shakespeare Bibliography Online.
Many new DH projects build their web presence with open source platforms such as Wordpress, Drupal, or Omeka, extending them through customizations. The long-term sustainability of such projects is an issue, involving the cost of hosting as well as migration of customizations as platforms and languages evolve. SalahEldeen and Nelson show evidence that after only a year, an average of eleven percent of cited on-line resources are lost. It is not surprising then that none of the common systems used to build DH projects allows reliable citation since the same platforms used in DH host a share of the resources lost each year. It is surprising that reliable citation is not seen as a greater problem by the scholarly community. Reliable citation is necessary for long-term preservation and therefore expresses a need for a temporal content and data management system that allows for reliable citation of scholarly narratives and resources.
We might base the value of a scholarly work on its place in the larger scholarly conversation. If it is not part of the conversation, then it has little effect on the field and thus has little value. Scholarly work can only be part of the conversation if it can be referenced. This is well developed for traditional publications (e.g., citing a particular edition of a printed work), but remains a problem for web-based scholarly work, not because particular pages can not be addressed, but because the information presented as part of that page is not stable. Without being able to reference the particular version of the page, scholars can not make reliable arguments about the work. What a reader sees might differ from what the author saw when researching and writing the work referencing the web¬based project. These problems increase when referencing a dynamic, algorithmic project.
We see three fundamental requirements for such a system to enable reliable citation of scholarly works and resources: temporal citation, reproducible citation, and sustainable citation. Any platform meeting these requirements should be able to provide level four preservation as described in the SDS final report (11).
A temporal citation of a web-based scholarly work must be able to address the view within the context of the project’s history. Not only must a scholar be able to point a reader to a particular resource in a project, but the scholar must be able to point to a particular resource at a particular date and time.
A reproducible citation of a web¬based scholarly work must show the same content over time. Fetching the cited resource year after year should show no significant changes in the scholarly content of the resource.
A sustainable citation of a web-based scholarly work allows a scholar to cite a project and know that their readers will be able to see the same information they saw by following the citation, for as long as their citation exists. Sustainability is a social issue as much as a technical one. We are not trying to address the social issues involved in sustainability in this poster.
The poster consists of diagrams and text explaining how reliable citation works with respect to resource versioning and project timelines. In addition, a demonstration of a temporal content and data management system hosted athttp://alpha.ookook.net/ providing reliable citation will accompany the poster so that attendees can interact with the system and see the platform affordances in action. The demonstration will also provide an opportunity for attendees to interact with the developer.
Reliable citation does not require any unique data model or software architecture. The poster outlines both the data model and the architecture as they are developed in the demonstration software, principally by segmenting a project’s history into discrete editions that aggregate changes to the project.
Discussion will quickly muddy if we don’t establish some nomenclature for dates and times. A resource date and time is the date and time for which the resource should be rendered. For example, if I specify a resource date and time of noon on January 1st, 2012, then I expect the see a rendering of the resource as it appeared at noon on January 1st, 2012. A request date and time is the date and time at which the request is made, even if the request is for a resource with a resource date and time different than the request date and time.
The data model partitions the project into two classes of objects: editioned objects, such as a project or theme, and versioned objects, such as pages or stylesheets. Editions are published for a span of time during which no public changes are made to the pages. Any changes made to a page require the creation of a new page version which will be aggregated with other versions when a new edition is created and published. Only one project edition is active for a resource date and time. By tracking the time spans for which an edition is active, we can reproduce the project as it existed at a particular date and time.
The demonstration software separates information into two editioned resources: Projects (web sites) and Themes (collections of style information). Editions of projects and themes are independent of each other, with each managing their own history.
References between different editioned resources are done by naming the referenced resource as well as the referenced resource date and time. This allows a project to select a theme in a reproducible fashion. References to a versioned resource within an editioned resource (e.g., a page within a project) may reference the page without referencing a particular version. The appropriate version will be retrieved based on the edition selected by the resource date and time of the request.
This poster will be of interest to anyone wishing to see how a platform supporting reliable citation might be designed.
References
Drupal. http://www.drupal.org/
Memento. http://www.mementoweb.org/
Omeka. http://www.omeka.org/
SalahEldeen, M. Hany, and M. L. Nelson Losing My Revolution: How Many Resources Shared on Social Media Have Been Lost? (2012). arXiv:1209.3026.
Sustaining Digital Scholarship Final Report. (2004) http://www2.iath.virginia.edu/sds/SDS_AR_2003.pdf
SiteStory. http://mementoweb.github.com/SiteStory/
TEI Archiving Publishing and Access Service (TAPAS) Project. http://www.tapasproject.org/
Wordpress. http://www.wordpress.org/
World Shakespeare Bibliography Online. http://www.worldshakesbib.org/
Introduction
This paper considers what can be gained from enhancing TEIencoded texts with RDF and OAC annotations and transforming to other representations, and how to facilitate their production, editing and storage. Our case study is the Sharing Ancient Wisdoms (SAWS) [1] project, which analyses the tradition of wisdom literatures. Scholarly interest is focused on semantic links within and between specific sections of these texts. SAWS produces TEIbased digital editions with semantic annotations in RDF to allow investigation of these links as Linked Data. This approach has the potential to be used widely to link and describe related sections of a variety of texts. Links can be extracted and transformed for manipulation and searching using alternative methods, illustrated by our TEItoRDF XSLT. In producing, storing and annotating such documents, the TEIediting process may present barriers to information enrichment for nontechnical users. The Islandora repository management software assists in creating and managing collections of documents through more intuitive, GUIdriven interactions with Fedora repositories. Within Islandora, the Digital Humanities Solution Pack provides a WYSIWIG online interface to help create, edit and annotate TEI documents, and simplifies the addition of semantic links. We demonstrate how TEI documents can be developed in diverse directions using Linked Data and RDF, and show how production of LinkedDataenhanced TEI documents can be facilitated using the Digital Humanities Solution Pack within Islandora. [2] RDF triples generated through the Islandora interface are exposed as standalone relationships which may be applicable in other contexts.

The Sharing Ancient Wisdoms (SAWS) use case
SAWS [3] [4] [5] is a key use case for this work, requiring an approach encapsulating various types of information including structural markup and semantic annotation. SAWS enables linking and comparisons within and between anthologies, their source texts, and their recipient texts, acting as a framework through which others can link their own materials via the Semantic Web.

SAWS focuses on gnomologia [6] , collections of sayings that transmitted moral or philosophical ideas. [7] These sayings were selected from earlier manuscripts, reorganised or reordered, and often modified or reattributed. The texts crossed linguistic barriers in the mediaeval period, and in later centuries were translated into western European languages. They form a complex network of interrelated texts, which when analysed can reveal much about the dynamics of the cultures that created and used them.

SAWS enables investigation of the relationships between specific sayings, tracing the links through different textual variants and languages. This has been achieved by enhancing our TEI with RDF: each saying can be linked to other relevant sections of text via a subject-predicate-object relationship defined as part of an ontology. The <relation> element, which has recently been updated with new attributes, allows us to enter RDF directly into the TEI document and combine this with information about scholarly responsibility. [8]

Combining TEI and RDF
TEI allows for extremely granular expression within a context; RDF is often meaningful in the absence of context. The strength of RDF lies in its apparent simplicity and its interoperability: its data is discoverable and reusable. Combining subject-predicate-object assertions can convey considerable metadata and tell complex stories. RDF can also be expressed as OAC annotations, which may have any number of targets of differing types. A target may indicate a section which overlaps another (via spatial or indexing coordinates) without breaking XML validation. SAWS implements the CITE/CTS citation scheme, [9] allowing overlapping sections to be described fully and referenced using anchor points in the TEI structure.

SAWS accommodates TEI and RDFcompatible markup within the same document and workflow, using established RDF syntax for marking up information of semantic interest. For SAWS, it is preferable to keep structural, syntactic and semantic markup in the same documents where possible, and to access the semantic information using standard tools such as XSLT. [10] [11] [12]

Previous approaches to the recording of semantic links within TEI documents have had limitations. The EARMARK ontology [13] provides an RDF model for XML information, but only for structure, so structural information is separated from text, and we cannot add semantic information while editing. RDFTEF [14] [15] requires documents to be edited in a separate environment within which standard XML tools cannot be used. [16] Approaches to incorporating RDF within XML documents do not transfer easily to a TEI representation: RDFa encodes RDF directly within specific XML attributes, but key attributes for RDFa [17] are not included in standard TEI schemas. [18]

While it would not technically be difficult to use RDFa by extending the TEI schema, this would introduce extra work which may not be necessary, and it would mean ignoring suitable alternatives proposed and accepted by the TEI community (discussed below) which require no extra schema work; if considered suitable, adopting such an alternative would enable SAWS to contribute towards establishing conventions within the TEI community for working with RDF within TEI.

Recognising the importance of combining RDF and TEI, a TEI Special Interest Group (SIG) in the use of ontologies [19] is developing XSLTs to transform TEI documents into RDF, using the CIDOC-CRM [20] as a basis. [21] [22] [23] [24] The SIG maps only a subset of elements to CIDOC-CRM, focusing on those that represent very particular entities. [25] SAWS would therefore not be able to retrieve many triples of scholarly interest such as manuscript structure and metadata.

To represent a wider range of data, a recent TEI recommendation [26] has been adopted by SAWS, using the <relation> element to represent links from one object [27] (@active) to another (@passive), using link types (@ref) which can incorporate a domain ontology. [28] [29] This increases the expressiveness of the markup without requiring changes within TEI. <relation> is an established element; the more recent addition of @ref has enabled <relation> to be used for RDF triples, along with the assertion of responsibility using @resp.

An XSLT stylesheet for extracting information from TEI to RDF
Semantic information can be accessed in limited ways via a TEI document, but when extracted, it can be placed in a triple store for access, querying and reasoning. [30] New knowledge can be derived by traversing internal links, and following links to related external Linked Data sources. [31]

We offer an XSLT stylesheet that transforms TEI, rerepresenting the structural, semantic and metadata information as RDF/XML triples. [32] Acknowledging practical difficulties concerning the size of the TEI tagset, we take the minimal required version of TEI, TEIBare. This forms a base for future extension, e.g. to TEILite. [33] [34] Using Dublin Core terms [35] such as dct:creator and dct:title, statements in the TEI header are transformed into corresponding RDF triples, and structural ordering of blocks within the TEI document are encoded using dct:isPartOf and dct:hasPart triples. We have extended the XSLT to include transformation of triples encoded through the <relation> element into RDF syntax, and further extensions can be added.

Transformations from TEI to RDF for the SAWS use case
The SAWS TEI version of the Kitāb alḤaraka (“Book of Happiness”), held at Ankara Üniversitesi, contains various metadata in its header. Applying the XSLT generates the following triples:


The SAWS TEI version of the Corpus Parisinum manuscript, held in the Digby collection in Oxford’s Bodleian library, contains a section <div xml:id="Aristippus01"> which is contained by its parent, <div xml:id="Part01">. From this we can derive the following structural triples:


Feedback on the editing and linking process
SAWS scholars studying documents in ‘right-to-left’ (RTL) languages noted the difficulties in working with standard XML editing software, and also requested more intuitive interfaces for editing documents and adding <relation> links.

Islandora is an open source project allowing users to manage a Fedora repository through PHP using a Drupal front end. Fedora repositories are adept at maintaining and versioning metadata accompanying scholarly objects. Islandora provides an intuitive way to use Fedora to create, access and manage document collections, and is currently being used across a varied number of use cases. [38]

Various “solution packs” within Islandora are available for different types of projects. The Digital Humanities Solution Pack is specifically designed for text editing and annotation, based on Shared Canvas and CWRC (for editing TEI and adding links). These tools are used to access, edit, and retrieve information held in repositories, including TEI transcriptions of texts, OCR tools related images, annotations and metadata. This Digital Humanities project within Islandora is sponsored by EMiC to develop a suite of applications for managing and critically analysing Canadian modernism. As one of the authors of this paper is the lead programmer of both these projects, he can incorporate these transformations into the workflow to expose the data publicly. Of particular interest is the ability to extract data from TEI to build and maintain authority lists.

The Islandora Critical Editions module exposes a GUI allowing the addition and viewing of RDF entities and TEI tags. No knowledge of XML is required. Entities tie textual offsets to objects from authority lists, userentered notes, external links, or date ranges through RDF. Image annotations are OAC RDF annotations.


Concluding remarks
We offer a functional XSLT for converting TEI to RDF, incorporating the recent application of <relation> for encoding RDF within TEI and extracting TEI <relation> elements and selected structural markup as RDF files.

Future SAWS/Islandora collaboration will investigate the enhancement of TEIencoded documents and a more user-friendly environment for editing, managing and linking texts. The DH Solution Pack by Islandora is available by request but has not yet been released in beta version. It is intended that SAWS will have implemented and tested a working version of the DH Solution Pack by June 2013. Any DH project that wants to link TEI files with other sources of information will, we argue, benefit from investigating the DH Solution Pack. It has wide implementation possibilities and will be particularly useful for projects using right-to-left languages.

The outcomes of this SAWS/Islandora collaboration should apply across a wide variety of texts. It is hoped that this paper will stimulate further interest in RDF and Linked Data within TEI, particularly amongst Digital Humanists wishing to work with a broader range of Humanities scholars.

Notes
1. http://www.ancientwisdoms.ac.uk. Last accessed October 2012.

2. John Unsworth. (2003). Tool-Time, or 'Haven't We Been Here Already?' Ten Years in Humanities Computing. Delivered as part of "Transforming Disciplines: The Humanities and Computer Science," Washington, DC. Available at: http://people.lis.illinois.edu/~unsworth/carnegieninch.03.html (last accessed 20th July 2012).

3. Anna Jordanous, K. Faith Lawrence, Mark Hedges, and Charlotte Tupman. (2012). Exploring manuscripts: sharing ancient wisdoms across the semantic web. In Proceedings of the 2nd International Conference on Web Intelligence, Mining and Semantics (WIMS '12), Craiova, Romania.

4. Tupman, Charlotte; Hedges, Mark; Jordanous, Anna; Lawrence, Faith; Roueche, Charlotte; Wakelnig, Elvira; Dunn, Stuart. (2012). Sharing Ancient Wisdoms: developing structures for tracking cultural dynamics by linking moral and philosophical anthologies with their source and recipient texts. In Proceedings of Digital Humanities (DH2012), Hamburg, Germany.

5. Hedges, Mark; Jordanous, Anna; Dunn, Stuart; Roueche, Charlotte; Kuster, Marc W.; Selig, Thomas; Bittorf, Michael; Artes, Waldemar;(2012). "New models for collaborative textual scholarship,", Proceedings of the 6th IEEE International Conference on Digital Ecosystems Technologies (DEST), Campione d’Italia, Italy.

6. F. Rodríguez Adrados, (1981). Greek wisdom literature and the Middle Ages: the lost Greek models and their Arabic and Castilian Translations (2001), English translation by Joyce Greer (2009), pp. 9197 on Greek models; D. Gutas, “Classical Arabic Wisdom Literature: Nature and Scope”, Journal of the American Oriental Society, Vol. 101, No. 1, Oriental Wisdom (Jan. Mar., 1981), pp. 4986

7. M. Richard, (1962). “Florilèges grecs”, Dictionnaire de Spiritualité V, cols. 475512

8. A full discussion of our TEI markup and use of the <relation> element can be found here: Tupman, Charlotte; Hedges, Mark; Jordanous, Anna; Lawrence, Faith; Roueche, Charlotte; Wakelnig, Elvira; Dunn, Stuart. Sharing Ancient Wisdoms: developing structures for tracking cultural dynamics by linking moral and philosophical anthologies with their source and recipient texts. In Proceedings of Digital Humanities (DH2012), Hamburg, Germany. 2012.

9. (http://www.homermultitext.org/hmtdoc/cite/)

10. M. O. Jewell. (2010). Semantic Screenplays: Preparing TEI for Linked Data. In Proceedings of Digital Humanities, London, UK.

11. 11 K. F. Lawrence. (2011). Wherefore Art Thou? Crowdsourcing Linked Data from Shakespeare to Dr Who. In Proceedings of Web Science, Koblenz, Germany.

12. Blanke, Tobias; Bodard, Gabriel; Bryant, Michael; Dunn, Stuart; Hedges, Mark; Jackson, Michael; Scott, David; (2012). "Linked data for humanities research — The SPQR experiment," 6th IEEE International Conference on Digital Ecosystems Technologies (DEST), Campione d’Italia, Italy

13. S. Peroni and F. Vitali. (2009). Annotations with EARMARK for arbitrary, overlapping and outof order markup. In Proceedings of the 9th ACM symposium on Document engineering, pages 171180, Munich, Germany.

14. G. Tummarello, C. Morbidoni, and E. Pierazzo. (2005). Toward textual encoding based on RDF. In Proceeding of the 9th International Conference on Electronic Publishing (ELPUB 2005), Kath. Univ. Leuven, June, pages 5763.

15. RDFTEF sourcecode: http://rdftef.sourceforge.net/ Last maintained 2007.

16. P. Portier, N. Chatti, S. Calabretto, E. Egyed-Zsigmond, and J. Pinon. Modeling, encoding and querying multistructured documents. Information Processing & Management. Forthcoming.

17. e.g. @rel, @rev, @href, @resource, @property, @vocab

18. A more detailed discussion of existing methods for encoding RDF within TEI markup can be found in: A. Jordanous, A. Stanley and C. Tupman. Contemporary transformation of ancient documents for recording and retrieving maximum information: when one form of markup is not enough. In Proceedings of Balisage: The Markup Conference 2012. Balisage Series on Markup Technologies, vol. 8 (2012), Montréal, Canada, August 2012.

19. TEIOntologies Special Interest Group http://www.tei-c.org/SIG/Ontologies/

20. ChristianEmil Ore and Øyvind Eide. (2009). TEI and cultural heritage ontologies: Exchange of information? Literary and Linguistic Computing 24(2): 161172

21. http://www.edd.uio.no/artiklar/tekstkoding/tei_crm_mapping.html, http://www.edd.uio.no/tei/teiontsig/test_crm_model.graphml

22. http://www.tei-c.org/release/xml/tei/stylesheet/rdf/

23. http://www.teic.org/SIG/Ontologies/guidelines/guidelinesTeiMappableCrm.xml

24. CIDOCCRM only direct represents textual material through one class (E33 Linguistic Object) and its two subclasses (E34 Inscription, E35 Title), but its selection as a base model is partially influenced by research aims of the SIG members in enhancing cultural heritage and museum documentation (http://www.teic.org/SIG/Ontologies/guidelines/guidelinesTeiMappableCrm.xml). Dicussions (see http://wiki.teic.org/index.php/SIG:Ontologies ) about the use of FRBRoo, a bibliographical records model harmonised with CIDOCCRM (http://www.cidoccrm.org/frbr_inro.html) have not been acted upon, to date. Some mappings from TEI to Dublin Core (a model of metadata information: http://www.dublincore.org) are occasionally present in stylesheets created by the SIG (http://www.teic.org/release/xml/tei/stylesheet/rdf/dc.xsl) but this output has not been highlighted, despite Dublin Core also being a realistic option for more detailed mappings of document metadata, especially from the TEI header.

25. http://wiki.teic.org/index.php/SIG:Ontologies

26. Sourceforge.net discussion: Encoding RDF relationships in TEI ID: 3309894, at http://tinyurl.com/lrbz53b

27. The application of <relation> to express RDF triples has been documented by TEI at http://www.teic.org/release/doc/teip5doc/en/html/refrelation.html with supporting examples.

28. The SAWS ontology (an extension of FRBRoo for representing relations of interest for study of wisdom manuscripts) is available at http://purl.org/saws/ontology.

29. S. Dunn, M. Hedges, A. Jordanous, K. F. Lawrence, C. Roueché, C. Tupman, and E. Wakelnig, Sharing Ancient Wisdoms: developing structures for tracking cultural dynamics by linking moral and philosophical anthologies with their source and recipient texts, Digital Humanities 2012, Hamburg, Germany.

30. For the SAWS use case, a SPARQL endpoint to access the RDF data is available ( http://www.ancientwisdoms.ac.uk/sparql )

31. To date, SAWS links to various collections of ancient data interlinked through Pelagios (http://pelagios.blogspot.com ) references to the Pleiades historical gazetteer (http://pleiades.stoa.org/ ). We are also in the process of linking to existing relevant documents such as in the Perseus Digital Library (http://www.perseus.tufts.edu/ ) and would like to link to information on people mentioned in the texts, such as through the Prosopography of the Byzantine World resource (http://www.perseus.tufts.edu/ ). The facility to traverse links between sets of data and discover related information serendipitously is a major benefit of Linked Data for the SAWS project.

32. XSLT available at http://www.ancientwisdoms.ac.uk/media/ontology/tei_to_rdf.xsl , with working versions available through https://github.com/ajstanley/TEI_to_RDF.

33. The Dublin Core Metadata Initiative is the main source model for structural and metadata mappings from TEIBare to RDF: http://dublincore.org/documents/dcmiterms/

34. http://www.teic.org/Guidelines/Customization/

35. The namespace ‘dct’ represents http://dublincore.org/documents/dcmiterms/

36. The manuscript ID is in CITE/CTS format for document citation (see http://www.homermultitext.org/hmtdoc/cite/ )

37. The dct:conformsTo relationship requires the object of the triple to be a string, rather than a resource

38. List of current Islandora installations: http://islandora.ca/current_installations
Introduccion
Desde la década de los 80s hasta los 2000 el Perú vivió en medio de un conflicto armado entre dos movimientos terroristas, Sendero Luminoso y el Movimiento Revolucionario Túpac Amaru (MRTA), los cuales buscaban derrocar el sistema democrático peruano, desatando uno de los más violentos pasajes de la historia moderna.
Estos movimientos fueron vencidos militarmente, desarmados y encarcelados, pero a un alto costo en vidas humanas. A pesar de la desarticulación de estos grupos aún se reportan células armadas y organizadas en los lugares más alejados de la selva peruana. Al mismo tiempo, en las ciudades buscan impunidad criminal y política para sus líderes arrestados a través de mecanismos legales que les permitirían introducirse en la política activa.
Hoy, casi 10 años después del fin de este episodio de la historia, el museo que conmemora el recuerdo de las víctimas de la violencia, el Lugar de la Memoria del Perú, no ha sido terminado. Las nuevas generaciones no conocen claramente lo sucedido debido al deficiente programa educativo; tal que estos grupos subversivos aprovechan la desinformación para entregar una imagen distorsionada de esta parte de la historia. (Pisa, 2009). Sin embargo, somos la octava comunidad con mayor uso de las redes sociales (Comscore Inc. 2012) e Internet en el mundo. Asimismo, Perú ya ha sobrepasado el 100% de penetración móvil.(Budde.com 2012)
Estos índices muestran que Perú tiene una oportunidad única y sin precedentes de desplegar servicios que asistan al desarrollo y mantenimiento de una memoria colectiva usando tecnologías móviles.
La Evolucion De La Idea
La misión del proyecto evolucionó con el tiempo y se ha trabajado y conversado con expertos del tema.
Un museo virtual
Memoragram nace a partir de la idea de crear un museo geo-localizado de la memoria, accesible desde cualquier dispositivo sin importar la hora ni el lugar. Los usuarios podrían explorar eventos históricos cercanos a su ubicación actual o hacia la fecha que deseen consultar, revisar los personajes involucrados, los lugares (en su mayoría demolidos), organizaciones e incluso revisar qué libros, revistas, periódicos, películas, reportajes de TV, cómics, entre otras fuentes, hacen referencia al hecho.
Por el lado tecnológico, los usuarios podrían usar smartphones o tablets para que, al acceder a una web móvil, esta solicite permiso para usar el GPS del dispositivo y así mostrar contenido relevante. La tecnología que hace posible este acceso es HTML5, CS3 y Javascript. Asimismo, los usuarios podrían descargar una tercera aplicación móvil llamada Junaio, la cual, usando el GPS, giroscopios, brújula y cámara puede mostrar contenido en una vista de tiempo real.
Si bien esta parte del alcance no ha sido modificada, surgieron otros problemas fuera del campo tecnológico.
El problema de las fuentes de datos
Con la idea y el alcance del proyecto se empezó por buscar fuentes de información fiables, entre ellas el Banco de Imágenes de la Comisión de la Verdad y Reconciliación Nacional del Perú (Comisión de la Verdad y Reconciliación del Perú), la cual se basa en los archivos fotográficos de los principales diarios y revistas locales peruanos. Este mismo banco se usó para formar la muestra Yuyanapaq (Programa de las Naciones Unidas para el Desarrollo — PNUD Perú) (término quechua que significa ‘Para recordar’). Este archivo posee alrededor de 1560 fotografías.
Luego de una evaluación de esta fuente y del mapeo de eventos posibles a registrar se concluyó que el número de eventos registrables sería mucho menor a 1560. De encontrarse otras fuentes de datos el número sería más limitado. Esta situación conllevaría a que la oferta de contenido fuera fija, lo cual podría evitar que los usuarios no se familiaricen con el sistema o no tengan una razón para regresar al servicio.
El fenómeno social del traspaso de la memoria y LoSoMo
La memoria colectiva se da como un proceso social y natural en el que las generaciones mayores traspasan conocimiento a las generaciones más jóvenes a través de relatos, documentos, mapas, elementos multimedia como fotos y videos, entre otros.
Este acto puede tener un efecto terapéutico en las personas que vivieron hechos traumatizantes.
A la vez, en el entorno de marketing en Internet (campo en donde se desenvuelve el autor) se hace presente la tendencia llamada “LoSoMo” que son las siglas de tres elementos que se recomienda considerar en una campaña digital: “Local”, “Social” y “Mobile”. Este enfoque se puede encontrar en las más exitosas start-ups como Foursquare y Groupon. A este momento, el alcance de Memoragram sólo abarcaba el primer y último aspecto. El fenómeno de traspaso de la memoria entonces se postulaba como el elemento social para generar una comunidad activa de usuarios.
Nuevas posibilidades
La inclusión de un fenómeno social en el alcance del proyecto abrió un nuevo espectro de posibilidades para la idea de formar un museo virtual. El sistema empezó a evolucionar: pasó de un servicio en el cual los usuarios sólo consumen contenido a una plataforma donde pueden aportar con sus propias memorias, sus propios recuerdos.
Este nuevo tipo de contenido obliga a generar nuevas acciones que se pueden expresar y medir con la plataforma, por ejemplo: los usuarios exploran el contenido y pueden registrar y medir el efecto que este genera en ellos, logrando que esta medida de rankings sea otro criterio de organización de los eventos listados.
El Proyecto
Con las consideraciones listadas previamente se definió el alcance del proyecto, el cual se alcanzará a través de una serie de prototipos funcionales en proceso de perfeccionamiento.
El alance elegido
Los usuarios podrán explorar los eventos históricos más cercanos a su ubicación actual, la fecha en que sucedieron, los personajes, organizaciones y las publicaciones que las mencionan como fotografías, video, audios, archivos periodísticos, libros, películas, entre otros. Asimismo podrán dejar sus propios recuerdos usando texto, audio, fotografías e incluso video, a manera de mensaje a las futuras generaciones (recuerdos públicos).
El contenido al inicio será creado con la asesoría de especialistas basados en fuentes confiables.
La Solucion
Eligiendo el gestor de contenido base
Para construir la plataforma se requiere de un paquete de software web modular, preferentemente gratuito para ahorrar costos y open-source para aprovechar el avance de la comunidad que lo mantiene. Se eligió entonces el CMS (Sistema de gestión de contenido) Drupal 7 basado en el lenguaje PHP, el cual se conecta a una base de datos MySQL (Metaio Inc.) Este sistema ya incorpora funciones de manejo de registro de usuarios, creación de perfiles de usuario, tipos de contenido y una robusta estructura de etiquetas así como soporte de distintos tipos de archivos multimedia.
Interfaces del usuario
En la versión 7 de Drupal ya se maneja HTML versión 5, llamadas asíncronas AJAX e incluso soporte de diseño responsivo, el cual permite crear una sola interfaz que adapta el contenido mostrado según el dispositivo desde el que se accede a él.
Asimismo, se decidió experimentar con una interfaz de realidad aumentada para móviles; para ello se eligió la aplicación móvil Junaio www.junaio.com, disponible para iOS y Android, a la cual basta alimentar con contenido desde una interfaz entre servidores usando XML.
Desarrollo de prototipo
A continuación pueden observarse unas capturas de pantalla. Todas las imágenes son de elaboración propia, se realizaron desde una PC con una navegador web y en un Apple iPhone 3GS.
 
Figura 1.
Home del sitio web prototipo con mapa de los últimos eventos registrados (Mostrando un pin del mapa con burbuja de información)
 
Figura 2.
Captura de pantalla web del detalle de un evento
 
Figura 3.
Vista en vivo en interfaz de realidad aumentada móvil usando la aplicación Junaio en un iPhone 3GS desde la Plaza Mayor de Lima
 
Figura 4.
Vista de mapa usando la aplicación Junaio (navegando a través de eventos)
 
Figura 5.
Vista de lista usando la aplicación Junaio (navegando a través de eventos)
 
Figura 6.
Vista de detalle de un evento usando la aplicación Junaio (los usuarios pueden realizar distintas acciones como obtener la ruta para llegar al punto, ver la imagen, entre otras)
Conclusiones
Es técnicamente viable crear una plataforma que llegue al alcance propuesto, sin embargo, a la fecha sólo se ha implementado el acceso web y realidad aumentada móvil sin la interfaz de creación de memorias por parte de usuarios. En el futuro cercano se irán extendiendo las funcionalidades y luego se pasará a realizar pruebas de usuario para diseñar una interfaz adecuada.
References
1. PISA 2009 Results: Executive Summary (Figura 1), OECD, 2010. Consultado el 2012-11-04.http://www.oecd.org/pisa/46643496.pdf.
2. Comscore Inc. “It’s a Social World: Social Networking Leads as Top Online Activity Globally, Accounting for 1 in Every 5 Online Minutes”. Consultado el 2012-11-04. http://tinyurl.com/bgn3sln
3. BuddeComm. “Peru - Telecoms, Mobile, Broadband and Forecasts”. Consultado el 2012-11-04.http://www.budde.com.au/Research/Peru-Telecoms-Mobile-Broadband-and-Forecasts.html.
4. Comisión de la Verdad y Reconciliación del Perú. Banco de Imágenes de la Comisión de la Verdad y Reconciliación Nacional del Perú. http://www2.memoriaparalosderechoshumanos.pe/apublicas/galeria/index.php
5. Programa de las Naciones Unidas para el Desarrollo - PNUD Perú. “Yuyanapaq. Para recordar”. Consultado el 2012-11-04. http://www.pnud.org.pe/yuyanapaq/yuyanapaq.html
6. Metaio Inc. “Junaio Demo Book”. http://www.junaio.com/fileadmin/upload/documents/Promo_Booklet/DOC-junaio_promo_book-EN-DIGI.pdf
1. Introduction

The Princeton Prosody Archive (PPA) is a full-text searchable database of nearly 10,000 digitized texts – comprising 800 million words – on prosody published in English between 1750 and 1923. During the 2012-2013 academic year, a grant from the Mellon Foundation supported the completion of the PPA’s first phase. This poster will reflect on the outcomes of the start-up stage, as well as some of the challenges and opportunities the PPA anticipates as the digital collection expands. Conference participants are encouraged to visit prosody.princeton.edu to access the Archive’s beta-site.
2. Prosody and Historical Poetics

In the nineteenth century, “prosody” – which refers to both pronunciation and the technicalities of versification – was codified as the fourth section of the grammar book, after “orthography,” “etymology,” and “syntax.” By the early twentieth century, “prosody” referred primarily to versification. In recent years, scholars of English literature have begun questioning the uniformity of poetic terminology, recognizing terms such as “prosody,” “meter, “tone,” or “rhythm” as culturally determined and fundamentally unstable concepts that have shifted through the centuries. By turning to historical texts, they are tracing how inherited notions of poetic form developed over time, and in turn, painting a more accurate picture of the evolution of English-language discourse on poetics.
As the field of historical poetics has grown, so too has our access to nineteenth-century materials through online archives. The majority of these digital resources, however, are primarily focused on prose works, and thus both technological and scholarly innovation has been made in the field of prose. The PPA is filling the gap as the only digital archive dedicated to the study of poetics, writ large, and allowing scholars to practice the kind of broad-view historical research the field demands. The PPA aggregates foundational texts in the history of poetics, reviews of these texts, debates about poetics in the public press, and grammar books and poetic handbooks that present contrary definitions and views of poetics so that “big questions” about literary movements and culture can be posed. With this large data-set, we can now ask: How did the changing science of linguistics and increased impulse toward education impact discussions of poetry over time? How often were particular poets used as examples in poetic pedagogy? How, when, and why did certain poetic terms and genres came in and out of use? 
3. Methodology

The PPA partnered with Google Books and HathiTrust in 2011, and the collection is currently composed of works digitized by Google and Hathi.[1] In 2013, the PPA began to develop a beta-site, which, though still under development, allows users to browse, search, and correct its content. To best serve its user community, the PPA functions as a freely-available, user-friendly repository, a trusted scholarly reference source, and a creative workspace that enhances traditional scholarly practices and pedagogies while enabling new ones.
3.1 Curation: Google Books and the HathiTrust Digital Library

The PPA’s initial corpus was selected from the holdings of the HathiTrust Digital Library. We began by gathering every out-of-copyright text referred to by prosody scholar T.V.F. Brogan in his annotated bibliography English Versification, 1570-1980 that had been digitized.[2] Though the availability of Hathi’s digital facsimiles and transcriptions is incredibly valuable, some aspects of their digitization and description present serious technical obstacles to the kinds of analysis the PPA intends to support. The most obvious is that the transcriptions were prepared by a range of Optical Character Recognition (OCR) systems, and few (if any) were hand corrected. Most were digitized as part of the Google Books program, whose OCR tools are not tailored to the vocabulary, orthographic conventions, or typefaces of eighteenth and nineteenth century texts. They were generally unable to capture indentation, italicization, or other formatting, variations in font size, or diacritical marks, not to mention musical notation or non-standard marks.
3.2 OCR and Diacritic Correction: Representing Scansion

When dealing with texts on prosody and versification, accurate representation of diacritics and typographical marks is particularly important. How do you render musical annotation, scansion, line spacing, or iambic markings, for example, into plain text? Because of the focus on notation and the transmission of concepts and terms, particular care must be taken to ensure that these issues do not interfere with (or silently distort) scholarly analysis. To that end, we are developing a model for encoding scansion – the non-textual elements such as musical notation, macron, breve, or other diacritics, including non-standard marks created by the many scholars who attempted to invent prosodic systems in English. Moreover, we will employ the kind of OCR that retains document coordinates for individual characters whose position on the page often conveys important information.
3.3 Metadata Correction: Scholarly Re-use and Linking Data

The metadata we ingest from HathiTrust also presents challenges. One of the PPA’s goals is to allow researchers to trace the development of prosodic discourse across time and place, and the ability to support this functionality depends on consistent and reliable metadata. While the HathiTrust provides the Machine-Readable Cataloging (MARC) records that have been supplied by contributing libraries, the fields indicating the place and date of publication are free text and vary widely in their conventions of encoding. In the PPA’s start-up phase, we developed an application that assembles text and metadata from the HathiTrust Digital Library, performs some initial automated correction, and loads the text and metadata into a Drupal 7 installation, where it can be browsed, searched, and corrected by scholars working with the Archive. Corrections to metadata can be credited to registered and authenticated users, and metadata fields can now even be versioned, using Drupal 7’s native revision control. In this initial phase, however, these corrections are essentially locked in the Drupal data store; they cannot be returned to the HathiTrust Digital Library or conveniently shared with other scholars working with the same HathiTrust volumes in other contexts. Going forward, the PPA will explore possibilities for enacting a workflow on its own metadata, engaging in the correction of HathiTrust metadata and connecting those corrections to linked data resources by working with the Maryland Institute for Technology in the Humanities and the Foreign Literatures in America project.
3.4 Connecting Prosody Networks: Topic Modeling and Visualization

Topic modeling, and specifically Latent Dirichlet allocation (LDA) has received attention in the digital humanities community over the past several years, in part because it is an unsupervised method – it does not require expensive training material or elaborate encodings – and also because it is relatively robust against textual errors. We have begun experimenting with LDA, not only to return a set of “topics” (which are simply distributions over the vocabulary) that often characterize the semantic and thematic composition of the PPA’s corpus in compelling ways, but also as a means by which we can identify mistranscription, special characters, and even musical notation. We also plan to begin experimenting with visualization tools in the following ways: 1) Plotting temporal and geographical metadata; tools such as Google Earth, MIT’s SIMILE, and Leaflet offer practical and intuitive ways to allow users to navigate temporally and geographically situated data sets interactively – for example, to view a three-dimensional chart on a globe indicating the relative prominence of cities as places of publication while moving a time slider through several centuries; 2) Mapping the documents in the corpus by its topical or lexical spaces; here, each document is represented as a point in a high-dimensional space, where the dimensions of the space are features such as counts or frequencies of individual words or n-grams, or the percentage of words allocated to a particular topic in a topic model; 3) tracking discursive networks by quotation identification and citation extraction; for example, the quotation of exemplars could be represented as a bimodal network, with nodes representing both volumes in the archive and lines of verse, and with edges from the former to the latter indicating instances of quotation.
3.5 Sharing Results

The PPA is committed to providing models so that other digital humanists struggling with the question of how to organize and present their own Hathi collections (in their research or in the classroom). Though these scholars might not be subject area experts in prosody or historical poetics, we would like to provide enough information that we might navigate unspecialized visitors through the corpus and share ideas about how they might build similar archives themselves. 
Notes

[1]  We negotiated a Google Distribution Agreement between the Princeton University Library, Princeton Counsel, and HathiTrust that allowed us to access, download, and host all of this data on our own servers.  A spreadsheet of all Archive monographs is available online at “Princeton Prosody Archive Database.” The PPA’s four collections can also be accessed through the HathiTrust site. See: 1) “Brogan's English Versification, 1570-1980” (578 works); 2) “Prosody Archive” (1,308 works); 3) “PPA Subject Search” (6,991 works); and 4) “Graphically/Typographically Unique“ (26 works set aside as possessing especially complex page images that would be misread by OCR).
[2] Brogan, Terry V. F. English Versification, 1570-1980: A Reference Guide with a Global Appendix. Baltimore: Johns Hopkins University Press, 1981.
1. Moving Towards Community Digital Heritage

Rural areas are characterised by a strong identity of people with place. These identities draw on a repertoire of cultural norms, knowledge, histories, customs and practices which, taken together, construct unique place identities. This cultural distinctiveness is dynamic given traditional cultural practices are reproduced and others introduced as cultural systems evolve and adapt. Forms of cultural expression, such as story-telling, music and song, poetry and literature, dance and drama together with material objects, artefacts, sites and cultural spaces, are resources for interacting with the past and for experiencing the present. In the collection and transmission of these collections there has been a growing sense that the traditional methods for doing this are failing, Nora [1]. In order to address this problem, digital solutions have been sought but this has been a problematic process due to a number of variances. These include the constant changing of file types, software and codes of best practice, as well problems to do with cost and the sheer amounts of ‘analogue’ data to convert. Leading the way in this process have been national institutions but with the production of such local cultural repertoires, which as Flynn [2] suggests ‘are the grassroots activities’ where ‘control and ownership of the project is essential’ there has been a failure to consider the needs of community heritage groups in these processes. As such groups do not want to be subsumed into national archives, which they do not control, is not sensitive to their needs and is juxtaposed ideologically to the production of their own ‘place history’. Following Creswell’s [3] claim that such archives represent ‘spaces  of marginalized memory’ CURIOS is therefore seeking a solution using open linked data in which a system can be developed that is attuned to the specificity of a local heritage but can also take advantage of already collected materials from elsewhere.
2. Case Study – Hebridean Connections

In the past 40 years around 22 ‘Comainn Eachdraidh’[1] (CE), have been established in the Outer Hebrides[2]. CE are community run groups that began in the 1970’s with a very specific political and cultural purpose – to preserve the culture, history and language of the primarily Gaelic regions of Scotland. Such community heritage practices have been described as a ‘messy’ endeavour with a wide variety of different formal and informal practices [5]. The archives embrace different registers of social memory from tangible to intangible heritage, which have been collected and ordered in a variety of different ways. Different CE groups collect and order their archives in a variety of different ways: from the highly ‘professional’ to the more bespoke and sporadic. As the CE groups are voluntary community archives, they are rooted in local historical values, hence there is often little consistency between groups regarding cataloguing, archiving and content management.
Hebridean Connections (HC), which is a community managed, online historical resource was formed due to the driving force of a single member of a CE who saw the benefit of digitising and connecting the different historical catalogues [5]. The idea was proposed to the different CE and four groups were actively involved in a Heritage Lottery Fund (HLF) bid that funded the creation of the HC website[3]. The project website was launched in 2006, holding some 100,000 records relating to the genealogy, history, archaeology, and cultural traditions of the Outer Hebrides. Currently, the system allows users to search using keywords, selecting relevant images, or with a map-based interface. Additionally, the website encourages contributions from its users and, therefore, has the potential to foster reciprocal knowledge exchange across geographical boundaries.
 2.1 Sustainability

HC is one example of a community-built digital cultural heritage repository where their long-term future is unclear. Many issues with the current system have arisen since the initial grant, particularly surrounding funding and scalability. There is a real practical question about how a project of this kind can be maintained over time with the resources available to a small-dispersed community, especially as the initial system was developed by a private development company, using proprietary software. As the project developed, this situation raised the problem that any changes to the system required more financial investment in the software. For the small community heritage groups involved, this was not feasible, especially as the CE became aware of what was possible through digitisation and wanted to expand. The process of digitisation has created three primary issues for HC:
How to expand the project remit without additional funding for developers?
Scalability issues, how can more CE collections be integrated in a closed system
Can empowering communities to control their own digital heritage improve long-term sustainability?
2.2 An Archive for the Future?

Motivated by the limitations of the current HC system, the CURIOS project’s aim is to produce a sustainable system that allows a community of users to manage a digital archive of cultural heritage data, or ‘cultural repository’, releasing them from any specific proprietary software platform. To achieve this goal, CURIOS has made use of existing open source content management system (CMS) software and Semantic Web standards. The emergence of the Semantic Web [6] has led to several standard formats for representing and interchanging data [7, 8]. By making use of linked data, cultural repositories would have the potential for reuse and integration with further related data sources.
In recent years content management systems have gained popularity on the web by allowing users to build and publish web pages without requiring in-depth knowledge of the underlying web technologies. The CURIOS project has extended the web CMS approach to allow users to manage repositories of linked data. This Linked Data CMS approach makes use of existing CMS software to retain the usability and scalability of existing tools that are familiar to users, whilst allowing the users to exploit the benefits of linked data.
The Linked Data CMS approach has been implemented as a module for the popular open source Drupal CMS[1]. Building the next generation of Hebridean Connections on open source software and web standards has distinct advantages for future development and use of the system. The Drupal-based system can be maintained by its community of users and can be extended additional functionality developed by the Drupal open source community, e.g., to support blogging or e-commerce features. This community led maintenance allows for further future development of the cultural repositories as the archives develop.
3. Conclusion

Open linked data can help make local cultural repositories sustainable and collective. Linked data allows for collaboration, mutual authoring, distributed responsibilities through community projects and the utilisation of other community or national resources [4]. The CURIOS project is enabling local cultural heritage repositories to become a meaningful identity resource for an international community, who previously had no access to them. By falling outside of national institutional frameworks, local people are the 'gatekeepers' of their own heritage and are selecting what to commemorate based on their own customs of remembering. This kind of digital archive can have, therefore, potentially significant social impacts which need to be better understood. The vision of Hebridean Connections is to expand the collections to incorporate those held by other Comainn Eachdraidh. Additionally, by making use of linked data, there is now the possibility to integrate further sources of data into HC from other historical societies or even national organisations. 
4. Acknowledgments

We would like to thank Hebridean Connections and the Comainn Eachdraidh for their ongoing commitment to this research. This work is supported by the Rural Digital Economy Research Hub (EPSRC EP/G066051/1).
References

Drupal is a popular open source web content management system: drupal.org.
Comainn Eachdraidh is a Gaelic phrase meaning ‘Historical Society’.
The Outer Hebrides is a group of islands off the West coast of mainland Scotland.
The Hebridean Connections website is hosted at www.hebrideanconnections.com.
Nora, P. (1996). Realms of memory: rethinking the French past. Volume 1: Conflicts and Divisions. Columbia: University Press.
Flinn, A. (2007). Community Histories, Community Archives: Some Opportunities and Challenges 1 in Journal of the Society of Archivists. Volume 28, Issue 2.
Creswell, T. (2012) Value, gleaning and the archive at Maxwell Street, Chicago. Transactions of the Institute of British Geographers. Vol. 37 (1),1-13.
Mellish, C., Wallace, C., Tait, E., Hunter, C., & MacLeod, M. (2011). Can Digital Technologies increase Engagement with Community History?Digital Engagement 2011. de2011.computing.dundee.ac.uk/wp-content/uploads/2011/10/Can-Digital-Technologies-increase-Engagement-with-Community-History.pdf
Wallace, C., Tait, E., MacLeod, M., Mellish, C., & Hunter, C. (2011). Supporting Digital Humanities Creating Sustainable Digital Community Heritage Resources Using Linked Data. In Supporting Digital Humanities: Answering theunaskable Conference. 17–18.
Spector, A. Z. (1989). Achieving application requirements. In Distributed Systems, S. Mullender, Ed. ACM Press Frontier Series. ACM, New York, NY, 19-33. DOI= doi.acm.org/10.1145/90417.90738.
Berners-Lee, T., Hendler, J., and Lassila. O. (2001). The Semantic Web. Scientific American, 284(5), 34–43
Hitzler, P., Krötzsch, M., Parsia, B., Patel-Schneider, P.F., and Rudolph, S. (2009). (eds.) OWL 2 Web Ontology Language: Primer. W3C Recommendation, www.w3.org/TR/owl2-primer.
Participatory and collaborative sense-making of complex phenomena is central to productive learning and knowledge work in today’s information-rich world.1 The Lacuna Stories Project creates an exploratory, interactive, and collaborative online space where users can research and discuss significant historical events like 9/11. Lacuna Stories draws together primary source documents, fiction, scholarship, wikis, and user-generated forums and blogs. The online space extends current digital annotation software with functionality that encourages skills such as historical thinking, close reading, and comparison of media and sources concerning 9/11. When approached independently, individual sources, genres, and media inevitably fall short of stitching together the “whole story.” The Lacuna Stories Project’s diverse, multimedia environment provides tools for instructors, students, and the general public to “mend” the gaps in our knowledge of major historical events in order to develop their own narratives. User data generated through the site’s design also will allow researchers to compare and better understand reading and engagement behaviors of students; along with other forms of user experience research and assessment, this research also provides directions for improving the platform and instructional materials.

The Lacuna Stories Project is a cross-disciplinary collaboration that includes faculty whose research interests span literature, pedagogy, historical thinking, public humanities, and platform studies. This short paper will describe the research and pedagogical goals of the Lacuna Stories Project as well as the technological innovations developed to support these goals. By the time of the conference, Amir Eshel (PI) and Brian Johnsrud (Project Manager) will have taught a course during Stanford's Winter Quarter in which students use the Lacuna Stories Platform in and outside the classroom. Johnsrud and Michael Widner (Technology Director) will also have taught a course in the same quarter on building in the digital humanities that will use Lacuna Stories as its primary example. This paper will touch upon how the prototype encouraged student learning and collaboration by presenting our data gathered from student use, interviews, and focus groups. By the time of the conference will also have piloted Lacuna Stories for a test-group of up to 30 public users without a college degree to compare experiences of different kinds of users in and out of the classroom, with and without formal academic training in approaching different kinds of historical texts and media.

Compared to other annotation and archive projects, the Lacuna Stories platform provides three key innovations. First, it creates an integrated multimedia environment that encourages the development of core skills for learning and knowledge work: navigation, critical reflection, linking, synthesis, and collaborative sense-making. Second, no existing digital annotation tool connects multiple types of media together to compare and generate new narratives. We are coordinating with MIT's HyperStudio to add this functionality to Annotation Studio (www.annotationstudio.org) and incorporate it into an ecosystem of digital tools for collaborative learning. Third, Lacuna Stories will provide a novel, curated set of diverse 9/11 resources for users to engage with and connect in innovative ways.


Fig. 1: Annotation Functionality

Lacuna Stories is also a platform that fosters good habits of close reading and thinking historically, whether the users are students, researchers, or members the general public. Within the humanities specifically, the interactive, multimedia functionality of Lacuna Stories goes beyond simply replacing print reading and viewing practices; rather, it creates new and innovative experiences for engaging with various texts and media that reflects the networked state of knowledge today. The platform thus builds upon the work done by Sam Wineburg, another member of the project team and Margaret Jacks Professor of Education and (by courtesy) of History at Stanford, to promote historical thinking (sheg.stanford.edu). Wineburg's work to date, however, has been focused on print resources in high school classrooms. Lacuna Stories will bring Wineburg's deep engagement with these matters to digital resources and make them available in the university setting.

The project also seeks to develop an inclusive, empowering, and engaging open-source platform to gather and encourage these responses in a generative and reparative mode. The site aims not to develop a fully coherent or conclusive “truth” of the event, but to encourage the cognitive and imaginative work that inspires responses to and stories about the event, its complexity, and its diverse meanings. Lacuna Stories’ subtitle, “mend the truth,” refers to site’s ability to connect in novel ways the different text, media, and user-generated content. One of our primary contributions to the development of Annotation Studio will be to enable all aspects of the available resources—from images to individual words, lines, or documents to user-generated content—to be archived or collected by registered users into their personal “sewing kit,” which provides users a workspace for the collection, connection, and annotation of materials relevant to their learning and scholarship.

We will incorporate and extend pre-existing open-source projects for the platform, most notably Annotation Studio from MIT’s Hyperstudio group. Annotation Studio is an exemplary, user-centered tool for digital humanities work, currently under active development in conjunction with a wider set of projects to develop shared standards for annotation of multimedia content, a key requirement for widespread adoption both in and outside of classrooms. We are working closely with the Annotation Studio team to ensure that the innovations developed for Lacuna Stories make their way back into the central code base and are thus available to the broadest possible number of users and institutions. One of the core technologies powering Annotation Studio is the javascript library Annotator.js (okfnlabs.org/annotator), a project of the Open Knowledge Foundation that is quickly becoming one of the most popular annotation technologies. By focusing first on developing extensions to Annotator.js, the Lacuna Stories Project ensures that our work will be useable in Annotation Studio and by any other projects that use Annotator.js.

We are also working to bring the functionality provided by Annotator.js into the Drupal platform that powers much of the rest of the Lacuna Stories site. Once this work is complete, users will be able to annotate not only texts available through Annotation Studio, but also blog posts, wiki entries, and any other content in a single, integrated environment. In our talk, we will discuss some of the challenges in the prototype phase for this work and our reasons for using Annotation Studio as a replacement until the Drupal work is complete.

O’Malley and Rosenzweig argue for the growing importance of the web generally because it allows for communication and exchange of divergent interpretations of the past. The web demonstrates how “meaning emerges in dialogue and that culture has no stable center, but rather proceeds from multiple ‘nodes’” (154).2 Being able to create links between annotations and sources and annotate the quality of those connections is central to the academic process of synthesizing information across documents and reflects the natural associative mechanisms that are central to deep learning.3 4

This functionality, however, does not exist in any current digital annotation tools. Lacuna Stories seeks to change this fact, with a tool that is scalable for use in a multitude of sense-making settings. Lacuna Stories will allow users to create categories and links between items in their kit, such as connecting a line from a novel with a paragraph from a user-submitted story, a forum discussion thread, and a section from the 9/11 Commission Report. These links can additionally be connected to a larger theme as described by the user; there can also be a shared set of themes developed collectively by the group or by an instructor. Social learning will be enabled through opt-in sharing functionality, where other learners can view and extend links and notes. Such open linking from users’ digital “sewing kits” exemplifies the idea that connections among narratives can be made quickly and simply, empowering users to “mend,” create, or share meaningful associations. Moreover, this aspect of the project responds to the work done by Fred Turner (Associate Professor of Communication), another faculty member of the team, to create digital humanities projects that are public-facing and that encourage community engagement.

Lacuna Stories is, then, a platform driven by the complementary research interests of a cross-disciplinary team of faculty made possible through technological innovation based on existing, open source tools. Although this paper will focus primarily upon the technology used and plans for future work, a secondary focus will be how the tools and innovations are grounded in research and pedagogy and how these interests influenced our technology choices and strategy.

References
1. U.S. Department of Education. (2010). National Education Technology Plan. Retrieved March 8, 2013, from www.ed.gov/technology/netp-2010

2. O’Malley, M. and Rosenzweig, R. (1997). Brave New World or Blind Alley? American History on the World Wide Web. Journal of American History 84(1): 132–55.

3. Tashman, C.S. and Edwards, W.K. (2011). Active reading and its discontents: the situations, problems and ideas of readers. In Proceedings of CHI ’11, 2927–2936. CHI ’11. ACM.

4. Marshall, C. (2005). Reading and Interactivity in the Digital Library: Creating an Experience that Transcends Paper. In Digital Library Development: The View from Kanazawa, D. Marcum and G. George, Eds.
1. Introduction
Today we have more information at our fingertips than at any other time in human history. The problem is no longer finding information, the problem is being overwhelmed with the amount of information. This is no different in the realm of the digital humanities.  Information on people, projects, resources, methods, and tools exists in quantity everywhere we look, and yet we still have difficulty finding what we need. This paper will describe a transatlantic effort on the part of DiRT in the United States and DARIAH in Europe to construct a taxonomy of scholarly methods, that can be used not only to organize single collections of DH information and resources but also to allow these collections to interface with each other, creating a web of linked data that can be effectively searched for information across distributed collections. DiRT and DARIAH are not trying to impose a restrictive, monolithic scheme on DH; rather, our goal is to construct a lightweight, basic taxonomy of higher order goals and first-order methods that can be easily expanded in all directions by linking lower order techniques to multiple goals and/or methods to create machine-readable paths among the various resources. In building this taxonomy, we heavily rely on input and feedback from the digital humanities community. Still, at least for the intended use cases, we believe a stable taxonomy has advantages over more open, folksonomy-based solutions.

The taxonomy as it exists now is based upon three primary sources: 1) the arts-humanities.net taxonomy of tools of DH projects, tools, centers, and other resources, especially as it has been expanded by digital.humanities@oxford in the UK and DRAPIer in Ireland; 2) the DiRT collection of digital research tools, re-launched under Project Bamboo in the US but now continuing on after the end of that project; and 3) the DARIAH ‘Doing Digital Humanities’ Zotero bibliography of literature on all facets of DH. These resources were studied and distilled into their essential parts, producing a simplified taxonomy of two levels: 8 top-level goals that are broadly based on the steps of the scholarly research process and a number of general methods under these goals that are typically used by scholars to achieve these research goals. The updating of the taxonomy and the definition of the types of relationships to be described in the resulting ontology will be carried out by a joint working group in the DARIAH-EU and the NeDiMAH projects in Europe, which will conduct large scale desk and field research into scholarly practice to determine how best to describe the relationships between and among the goals, methods, and techniques of scholarly practice.  The future expansion of this organizational system will not be as a hierarchical taxonomy but, instead, as a linked ontology as lower-level techniques are attached to one or more methods, linking all the existing entities in the ontology together. The projects and collections that use this schema will play an important role here: as resources are added to these collections and linked to the taxonomy, the resulting ontology will grow in complexity.  This complexity will be more help than hindrance precisely because it will be a machine-actionable complexity.  Computers will traverse this web of relationships for us, only bringing back results that are closely related to our needs.

This may seem excessively optimistic, but this paper will support these claims by describing three very different types of resources that have used and expanded the taxonomy not only to improve the findability within their own collections but, more importantly, to link to each other in a machine-actionable way. These resources are the DiRT directory of digital tools, the DARIAH ‘Doing Digital Humanities’ bibliography, and the DARIAH-DE service-oriented project portal. A brief description of each of these collections and how they will profit from this taxonomy/ontology follows.

2. DiRT
DiRT (Digital Research Tools, http://dirt.projectbamboo.org) is a longstanding US-based directory for scholars interested in digital tools, which provides basic information about software that can facilitate the research process at different stages. The classification of tools by category has always been fundamental to DiRT: in its earlier incarnation as a wiki, wiki pages each corresponded to a category of tools, and the tools were presented in a list on the page. In 2011, DiRT was rebuilt using the Drupal content management system, which allowed information about each tool to be stored in a structured manner that enables faceted search and browsing. While users can now create complex queries on DiRT (e.g. using operating system and price to narrow their results), tool categories remain the primary way of navigating the site.

With support from the Andrew W. Mellon Foundation, DiRT is currently undergoing a new phase of development, with the goal of making information about digital tools available outside the DiRT directory itself. Since its inception, DiRT has used its own ad-hoc list of categories. All tools must belong to at least one category, though these categories can be supplemented with user-generated tags. The shortcomings of DiRT’s categories list can be illustrated through the example of OCR tools-- some are classified as “transcription”, others as “conversion”, and while neither is ideal, both are a reasonable approximation given the other options. Replacing DiRT’s former categories with the taxonomy will improve the consistency and quality of the data, and also provide a shared facet that can connect DiRT’s tool data with information provided by other projects, once DiRT’s contents are made available using RDF.

3. 'Doing Digital Humanities' bibliography
Another resource directly connected to the taxonomy is DARIAH-DE's ‘Doing Digital Humanities’ bibliography. The bibliography can be accessed on Zotero (www.zotero.org/groups/113737) or on the DARIAH-DE portal (https://de.dariah.eu/bibliography). Like DiRT, the bibliography is one of the seed activities for the taxonomy at the same time as being one of the already defined use cases, representing the application domain of making medium-sized collections of bibliographic references discoverable. This Zotero-based bibliography offers suggestions for introductory readings as well as more in-depth coverage of research literature in various areas of digital research, teaching and infrastructure planning in and for the humanities. The bibliography is carefully curated collaboratively, is freely accessible, currently has around 800 entries, and is being updated continuously.

Right now, the bibliography is already divided into thematic collections based on the "goals" defined in the taxonomy. Each collection, hence, covers one prototypical aspect or goal of the research process in the humanities as it is practiced with digital tools, methods and data. In addition, all entries in the bibliography are discoverable through keywords covering, on the one hand, typical research methods and activities in the humanities, and on the other hand, a wide range of objects of research. The current closed list of keyword represents an early draft version of the taxonomy described here.

Once a first stable version of the taxonomy is available, the bibliography's keyword implementation will be updated. Sharing a keyword system with other projects will make it easier for users to find related resources. And the public documentation of the taxonomy, including concise scope notes for all methods and techniques, will make the bibliography's keyword-based search more transparent and increase its usability.

4. DARIAH-DE portal
A third use case aims to examine the taxonomy as a functional structure for DARIAH-DE’s service-oriented website, the DARIAH-DE-Portal. Launched in a first version in May 2013, it will receive a makeover in the early stage of the upcoming German DARIAH II project scheduled for March 2014 that is based on the taxonomy.

The website is designed to offer a wide range of services concerning Digital Humanities in Germany and addresses both researchers who already work digitally and those seeking information or advice. The services provided are as heterogeneous as the DH landscape. They cover informational aspects on specific research projects, information on DH Centers, Bachelor/Master Programmes and tools as well as their documentation, tutorials and teaching materials. Services offered by DARIAH-DE (like the embedded bibliography mentioned above, the DARIAH-DE Working Papers, or hosting services and a developer’s portal) are complemented by external resources like blogs and a DH-calendar (a cooperation with calenda.org being currently on its way).

The variety of this content leads to multi-purpose requirements that enable a flexible access to information relevant to individual users. This use case meets that challenge by implementing the taxonomy in RDF, thus interlinking content and making it multi-purpose. In that way, the taxonomy will function as a ‘meta-service’ that meets the interests of an active and interlinked community, that visualizes Digital Humanities and promotes its results.

 5. Conclusion
The purpose of this talk is not to convince the audience that we in DiRT and DARIAH have all the right answers.  Instead, it is to continue a conversation about the importance of ontologies for managing the over-abundance of DH information, present our own work on this problem and our approach to gathering and incorporating community feedback, in hopes of spurring further work in this area.

References
Anderson, Sheila; Tobias Blanke; Stuart Dunn. (2010). Methodological Commons: Arts and Humanities E-Science Fundamentals. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 368, no. 1925 (2010): 3779 –3796. http://rsta.royalsocietypublishing.org/content/368/1925/3779.abstract.

Benardou, Agiatis, Panos Constantopoulos, Costis Dallas, and Dimitris Gavrilis. Understanding the Information Requirements of Arts and Humanities Scholarship. International Journal of Digital Curation 5, no. 1 (June 22, 2010): 18–33. doi:10.2218/ijdc.v5i1.141.

Borgman, Christine (2010). Scholarship in the Digital Age : Information, Infrastructure, and the Internet. Cambridge: MIT Press.

CLIR (Commission on Cyberinfrastructure for the Humanities and Social Sciences). (2006). Our Cultural Commonwealth: The Report of the American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities and Social Sciences. New York: American Council of Learned Societies, 2006.

Gasteiner, Martin, and Peter Haber, eds. (2010). Digitale Arbeitstechniken für die Geistes- und Kulturwissenschaften. Vienna: UTB. http://www.utb-shop.de/digitale-arbeitstechniken.html.

Reiche, Ruth; Rainer Becker; Michael Bender; Matthew Munson; Stefan Schmunk; Christof Schöch. (2014). Verfahren der Digital Humanities in den Geistes- und Kulturwissenschaften. DARIAH-DE Working Papers Nr. 4. Göttingen: DARIAH-DE (to appear). Preprint: https://dev2.dariah.eu/wiki/download/attachments/2295542/M223_DH-Verfahren.pdf.

Siemens, Ray; John Unsworth; Susan Schreibman, eds. (2004). A Companion to Digital Humanities. Hardcover. Oxford: Blackwell. http://www.digitalhumanities.org/companion/.

Unsworth, John. (2000). Scholarly Primitives: What Methods Do Humanities Researchers Have in Common, and How Might Our Tools Reflect This? London: King’s College London. http://www3.isrl.illinois.edu/~unsworth/Kings.5-00/primitives.html.
Blumenbach-online, a project of the Göttingen Academy of Sciences and Humanities, started in January 2010 and aims at both digitizing and presenting the writings and collections of the influential Gottingen physician and naturalist Johann Friedrich Blumenbach (1752-1840), one of the founding fathers of physical anthropology, online. To date, almost half of the textual material (77.000 pages altogether) and roughly a quarter of the collections have been digitized and converted into TEI-encoded texts or entered into a database. It is through an exploration and application of Semantic Web technologies in a spin-off project called "Semantic Blumenbach" that we hope to establish robust and powerful methods for presenting and providing heterogeneous machine-readable linked data for Blumenbach-online. 

Two major tasks have been completed so far. The first is carrying out Named Entity Recognition (NER) on the TEI P5 Tite1 encoded full-texts that have been provided to Semantic Blumenbach2 by Blumenbach-online. These texts lacked the semantic markup e.g. for places, persons and objects from the natural history domain. In addition, we had to deal with historical and irregular orthography of multilingual texts from the second half of the 18th century. Currently we are able to recognize precisely (96%) most (96%) of the technical terms that appear in the text using a list-based algorithm. This algorithm is also able to detect binominal entities from the Linnaean taxonomy, even when they appear as separate strings in different parts of the text. For modeling the relationship between entities in the text and metadata in the collection, we use the WissKI Framework for scientific communication (www.wiss-ki.eu) that allows presenting and using data from various sources in a robust and open system, which is both scalable and reusable by other projects. With the help of the Erlangen CRM Ontology3, an OWL-DL 1.0 implementation of the CIDOC CRM4 and a special application ontology, we model the semantic relationships between objects described in TEI-encoded texts and metadata of these objects.5 We particularly focus on place names, persons and special terms from the natural history domain, including the Latin names of animals and geological objects and construct the relationship between both types of data by using our NER to encode reference strings in the TEI text. 

The Erlangen CRM provides a way to classify these objects in a meaningful way and to model the relationship between the occurrence of the objects in the writings of Blumenbach and the University of Göttingen’s collections. With the help of colleagues from the WissKI Project at Erlangen and Nurnberg we have been able to develop new modules for the Drupal-based system to ingest the TEI and triplify the metadata that we created in the texts. Following a policy of Open Access and Linked Open Data, we will test and implement ways to generate and publish results of academic research in a way that it can be reused in other contexts and by other researchers. Finally, we plan to use a full-text search index (Apache solr) to make both texts and object-related data available in a way that allows both triplyfied metadata and XML full-text to be searched efficiently. 

URL: dhfv-ent2.gcdh.de/blumenbach/wisski

Username and password available on request. 

References
1. www.tei-c.org/release/doc/tei-p5-exemplars/html/tei_tite.doc.html

2. Wettlaufer, Jörg & Thotempudi, Sree Ganesh (2013): Poster - NER in historical Text corpora. Lessons learned so far. 4.-6.03.2013, Mehr Personen – Mehr Daten – Mehr Repositorien, Tagung des Personendatenrepositoriums der BBAW, Berlin. www.gcdh.de/index.php/download_file/view/168/405

3. erlangen-crm.org

4. “CIDOC CRM,” n.d. www.cidoc-crm.org/index.html.

5. C.f. www.tei-c.org/SIG/Ontologies/meetings/m20131003.html
TEI is good at what it does: static documents rendered in glorious detail. But TEI is old. Its age doesn’t make TEI irrelevant, but it’s important to be conscious of how the way we weave the fabric of the web has changed since TEI was conceived in 1994, and reevaluate some of our assumptions about its use. In this early work, we are exploring this rethinking as part of a larger study within the center on general methods for isolating the complexity frequently associated with XML-based frameworks.

The Richmond Times Dispatch corpus of TEI-encoded newspapers comprises the Confederate newspaper’s Civil War run, 1860 — 1865. It is compelling both in terms of organization and content and amounts to a comprehensive textual index. In addition to the historical allure of its content, the formal properties of the digitized documents made available through the Perseus Collection make the Dispatch an extraordinary raw material for building a rich interactive visual experience that augments the textual one.

The Dispatch is not in need of a new home; the Perseus Collection hosts a perfectly functional version and uses best practices for data encoding and organization. Rather than strive to be the primary resource for the source material, our project uses the source material to explore recent patterns in web development as well as alternative, more visually compelling ways to interact with XML corpora in a web application. The goal is to produce a powerful reading environment that is tailored to its source material to an extent that the generalized project of the Perseus Collection can afford.

With respect to its implementation, our project fits the genre of a ‘single page application’ (SPA). This project demonstrates best practices for implementing this type of software project using a particular suite of tools; as an open source example of an SPA that is considerably more complex than the usual teaching examples for this kind of thing, we hope that our implementation will be useful to other people who are considering using the same tools, and especially to humanists interested in presenting TEI-encoded documents.

The SPA is du juor. We prefer beautiful URLs and smooth transitions. We are less fond of all these big lists cluttering our sidebars and clunky arrays of checkboxes. We don’t expect websites to always be inert collections of documents. We want to be able to control the connective tissues. The web development community has responded to our current expectations with tools to suite them.

The client-side MVC (Model-View-Controller) libraries that have recently emerged have reached a high level of maturity A client-side MVC library codifies conventional solutions to the generic problems posed by web traffic. It provides semantics for describing the interaction layer between data and presentation. The codification of conventions that MVC libraries manifest is exciting. It deeply simplifies matters for those who want to make interactive documents. Humanists who have a grasp of the language and concepts involved will be that much better able to articulate and realize project architectures that delight the contemporary reader.

For instance, our application is built around a client-side router. The router formalizes protocols for state transitions that allow for timely and efficient request management. We rely heavily on the concept of the run loop, which exposes powerful document management techniques and is tightly linked with a client-driven templating engine. We are able to achieve a remarkably clean separation of concerns in a highly condensed space by exploiting the conventional roles organized and implemented by these libraries. And by shifting our application’s emphasis to the client, we have constant access to a unified programming environment, limiting the context-switching required when developing different parts of the application.

In addition to our project’s strong client-focused application architecture, we also demonstrate a data architecture solution to the problems posed by the corpus’ rich TEI markup. To expose the facets embedded in the source XML, the implementation transforms the deeply nested structure inherent into flat relational representations that can be searched efficiently. Furthermore our project demonstrates a novel, pythonic approach to transforming the source XML to browser-ready HTML that is particularly amenable to the constraints of an SPA.

XSLT wasn’t very well suited to our TEI transformation problem. One of the key UI features of our application was the ability to discover and search for special entities such as people and places in the text. By implementing a custom transformer in python, we had the flexibility to both translate the TEI tag names into valid HTML versions and retain the original TEI tag names and attributes as attributes on the HTML element.

In addition to serving content thus transformed as needed, the role of the server in our application is limited to various precomputation and preprocessing tasks that only need to be run whenever the source material changes--a process that is fully automated with Unix batch processing (via cron) in the cloud. Users never notice. Research projects are often quagmired in a chaotic sprawl of one-off scripts; we demonstrate a coherent architectural pattern for orchestrating these preprocessors.

Sometimes the affordances of an SPA make it worthwhile to depart from the original document’s presentation. Content on the web wants a different kind of exposure than a stack of newspapers. You want to be able to find things quickly. You want to be able to highlight and hyperlink, associate and drill down. Once you’ve computed a graph of your stack of newspapers, now you can move laterally, staying in the same section and moving from date to date, just as easily as you could stay on the same date and move top-to-bottom through the articles. We demonstrate a novel, minimalistic navigation scheme for the Dispatch.

If you’re taking full advantage of a Javascript environment to render your XML content, you can use modern libraries to plug in visualizations with simplicity, and furthermore to turn these visualizations into interactive filters for a very powerful browsing experience. Using a cluster of technologies surrounding Mike Bostock’s work, we demonstrate how to integrate a visualization library into an SPA.

And yet we believe that datafication shouldn’t overwhelm the content. You want to be discrete about placing your controls lest you scare the casual user, but they should be powerful. Live feedback from search inputs has come to be a common expectation for user interfaces and the SPA environment makes it easy to architect that. We show one effective way to make your XML live-searchable.

We close with just a few screenshots of the work in progress. It is important to note that this interface is being further refined based on new work Trevor is doing in his new role as a front-end software developer. 


Fig. 1: An early version of the splash page, presenting user interface controls for the issue date, section, and subsection. Selecting a subsection would reveal the list of headlines it contains. The date selectors reload the issue content asynchronously, without reloading the page.


Fig. 2: The early version of the article reader, presenting the text in a modal context. The summary of facets across the top act as toggles for corresponding highlights in the text. You can page through the section content using the controls at the bottom of the modal.


Fig. 3: This screen capture shows the direction taken in the latest development. The URL bar demonstrates stateful client-side routing. The content selection controls have been flattened into a trio of type-ahead controls with pagination buttons for navigating forward and backwards both in the document structure and across documents.

This stuff is fun. The tools are a joy to use. The free/open source community behind it is excellent and innovative. We want to see more humanists building applications, and moving away from consuming and rather heavyweight content management systems such as Drupal. Based on our experience, humanists can learn the tools and frameworks quickly with excellent results to boot. We hope that our implementation of the Dispatch will set a strong example for our (and others’) future DH projects.

References
We note that we are proponents of using XML, especially for its originally intended purposes of self-describing data interchange, which remains tremendously valuable in developing type safe RESTful web services. 

George K. Thiruvathukal is leading a separate and parallel effort to develop the Standoff Markup text editor, standoffmarkup.org,  which is aimed at simplifying the encoding and maintenance of XML texts (without exposing tree-oriented abstractions). This is where we started exploring the use of SPA when it comes to building DH-facing tools in general.

Richmond Times Collection, www.perseus.tufts.edu/hopper/collection?collection=Perseus:collection:RichTimes.

Single Page Applications original conception, code.google.com/p/trimpath/wiki/SinglePageApplications

Conventionally, a to-do list. See, for instance, todomvc.com.

The Model-View-Controller design pattern (and paradigm) was introduced as part of the Xerox PARC Alto computer, which used the Smalltalk programming language. An excellent historical read about this paradigm can be found at heim.ifi.uio.no/~trygver/themes/mvc/mvc-index.html.

We use Ember.js: emberjs.com. Our technical term for “high level of maturity” is “rock” but we eschew this Americanism for the purpose of a conference paper submission.

We rely throughout on the wonderful lxml library for Python: lxml.de.

Our present implementation is deployed on Heroku, an agile and scalable framework for deploying apps like ours. The overall project is moving to Linode (a dedicated Linux-based cloud hosting provider).

In this we rely on the large-scale application planning features offered by the Flask web framework for python: flask.pocoo.org.

Author of d3js.org among other things.

We have found dc.js (nickqizhu.github.io/dc.js) to be a perfect storm of visualization functionality.

We use the well-known elasticsearch library (www.elasticsearch.org) to achieve an effect like Google’s live search results.
1. Background
The DiRT (Digital Research Tools, dirt.projectbamboo.org) directory is a longstanding resource for scholars interested in digital tools and methodologies, providing basic information about software that can facilitate different stages of the research process. DiRT was originally designed as a wiki, where a single wiki page contained information about all tools in a given category. In 2011, under the auspices of Project Bamboo, DiRT was completely rebuilt using the Drupal content management system, which allowed for data to be stored in a structured manner. This enabled more complex searching and browsing options (such as allowing the user to limit results based on criteria like platform or cost), and provided individual profile pages for each tool, which could then serve as a locus for specific comments, or be referenced in other tool profiles. For instance, if a profile page indicates that Neatline is a suite of add-on tools for Omeka, a link to Omeka appears on the Neatline tool profile page, and vice versa.

2. Current development project
One of the biggest limitations of DiRT has been the fact that its contents-- the product of a considerable amount of volunteer work-- have only been available via DiRT’s own web interface. Creating and curating the tool listings on DiRT is largely a manual process. A steering and curatorial board takes an active role in shaping the ongoing development of the site and ensuring data quality, but individual contributions by users make up a large portion of the data. DiRT is currently undergoing a new phase of development, supported by the Andrew W. Mellon Foundation, with the goals of making DiRT data available to others who want to incorporate information about tools into other projects, resources and environments, and also expanding the content provided by DiRT to more clearly situate the tools in the contexts of the projects, research workflows, and pedagogical activities that use them. This poster will demonstrate the accomplishments of the current development project and include information about opportunities to get involved with the project, by trying the DiRT API and plug-ins, or contributing to tool reviews and documented workflows.

3. Areas of work
The poster will also highlight the progress made on developing a DiRT plug-in for Commons In A Box (CBOX), an open source scholarly networking platform created by the City University of New York and used by the Modern Language Association (MLA), the regional NYC Digital Humanities group, and an increasing number of projects and organizations that could benefit from integrated access to information about tools. The CBOX plug-in will:  

provide users with the ability to display information about their DiRT site activity (e.g. tool contributions and edits, reviews, and tool usage information) on their Commons profile;
provide an interface for searching DiRT within the CUNY Academic Commons, for use by groups with an interest in digital humanities;
provide a link to DiRT to facilitate access for inputting new tools.
The poster will also illustrate other areas of development including:

Use of the DiRT API and the API for the DHCommons project directory to augment DiRT tool profiles with information about what projects are using a particular tool
Guidelines and examples of best practice for writing tool reviews, with potential pedagogical applications (e.g. providing a framework for instructors who want to assign students to write reviews of digital research tools, which could then be refined for ultimate publication on DiRT)
Guidelines and examples of best practice for documenting workflows or “recipes” that combine multiple tools in the DiRT registry to achieve some research objective.
Adoption of the taxonomy of research methods jointly developed between DiRT and DARIAH-DE, to replace the previous ad-hoc set of tool categories. DiRT will serve as one of three initial test cases for this taxonomy, which has benefitted from extensive public feedback.
Documentation for how to develop custom tool lists (e.g. tools to be used in a particular class, or tools that are particularly relevant for the disciplines that a subject specialist librarian supports) that pull from the information stored in DiRT, and display that information on other sites.
References
Babeu, Alison (2006). Rome Wasn’t Digitized in a Day”: Building a Cyberinfrastructure for Digital Classicists. CLIR reports, August 2011.Borgman, Christine L. “The Digital Future Is Now: A Call to Action for the Humanities.” Digital Humanities Quarterly 3, no. 4 (Fall 2009). http://www.digitalhumanities.org/dhq/vol/3/4/000077/000077.html.Unsworth, John. Our Cultural Commonwealth: The Report of the American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities and Social Sciences. American Council of Learned Societies. http://acls.org/uploadedFiles/Publications/Programs/Our_Cultural_Commonwealth.pdf.


    
        
            
                PhiloLogic4 And The Android PhiloReader Apps: Toward Building A Full-Featured PhiloLogic API
                
                    
                        Cooney
                        Charles M.
                    
                    ARTFL, United States of America
                    chu.cooney@gmail.com
                
                
                    
                        Gladstone
                        Clovis
                    
                    ARTFL, United States of America
                    clovisgladstone@gmail.com
                
                
                    
                        Shandruk
                        Walter
                    
                    ARTFL, United States of America
                    waltms@gmail.com
                
                
                    
                        Morrissey
                        Robert
                    
                    ARTFL, United States of America
                    rmorriss@uchicago.edu
                
                
                    
                        Roe
                        Glenn
                    
                    The Australian National University
                    glenn.roe@anu.edu.au
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Short Paper
                
                
                    PhiloLogic4
                    API
                    Android
                    text databases
                    search and retrieval interface
                
                
                    databases &amp; dbms
                    interface and user experience design
                    project design
                    organization
                    management
                    publishing and delivery systems
                    software design and development
                    text analysis
                    programming
                    mobile applications and mobile design
                    English
                
            
        
    
    
        
            The ARTFL Project would like to submit a proposal for a short paper on the development of an API for PhiloLogic4, our next-generation corpus query and text retrieval platform for digital humanities databases, and to demonstrate Android PhiloReader Apps that we are building to interact with that API.
                1
            
            A primary goal of the PhiloLogic4 project has been to allow ARTFL or any digital humanities group using this software to develop a variety of results display and user interfaces with ease.
                2 For example, the databases ARTFL has already released in beta form under PhiloLogic4 for traditional browser access have features such as frequency sidebars for query results, links within those sidebars for faceted browsing, and dynamic Time Series reports.
                3 ARTFL’s PhiloReader Apps extend and exemplify this fundamental design goal. They take advantage of PhiloLogic4’s simple set of query parameters and flexible results object formatting to enable text search and retrieval on handheld devices. 
            
            ARTFL intends these apps to serve as a lighter-weight alternative to web browser apps for interacting with the PhiloLogic4 installations of text databases on our main production servers. The interface has been designed with a focus on the reading functionality that PhiloLogic4 already offers in its web incarnation. Users can conduct word and/or metadata search from a toggling drawer with the aim of finding and reading text sections.
                4 Word search results can be returned in concordance and frequency reports. Any metadata value can serve as a frequency report option, though the most common are word use by title, by date, by author, and even by speaker in collections of plays. More intensive text analysis would require the search form accessible through a web browser. 
            
            Functionally, the Android code interacts with PhiloLogic4 databases simply by sending search queries and then displaying the results that PhiloLogic4 sends back over the network. Those results, like all the data the PhiloLogic4 API passes both internally and externally, are formatted as JSON objects. From the search terms the user enters, the app builds a query URI compliant with the PhiloLogic4 API, which includes parameters such as number of results per page and metadata values, as well as a parameter that calls specifically for a JSON object (‘&amp;format=json’). For basic concordance and bibliographic searches, the URI currently points at the main script PhiloLogic4 uses to handle all interaction from web browsers (‘dispatcher.py’).
                5 For example, a basic concordance query for ‘sun’ from the Shakespeare app has this URI: 
            
            http://artflsrv02.uchicago.edu/philologic4/shakespeare_demo/dispatcher.py?report=concordance&amp;q=sun&amp;method=proxy&amp;title=&amp;start=1&amp;end=25&amp;pagenum=25&amp;format=json 
            For the web version of PhiloLogic4, the dispatcher directs calls for secondary queries, like frequency searches and table of contents requests, to scripts that run behind the scenes. In order to minimize extra coding on the server side, the app calls those CGI scripts directly for these same queries. A frequency by title search for ‘sun’ from the app points to the frequency generation code to get PhiloLogic4’s internal JSON object: 
            http://artflsrv02.uchicago.edu/philologic4/shakespeare_demo/scripts/get_frequency.py?report=concordance &amp;q=sun&amp;method=proxy&amp;title=&amp;frequency_field=title&amp;format=json 
            The JSON object of a concordance result, for example, can contain chunks of the search result, bibliographic metadata, and a PhiloLogic id used for linking into larger sections of the text. The Android code renders the JSON into a string array and displays search results in a listview.
                6 The user can then select individual list items to get larger text sections. The Android code submits a second query to the PhiloLogic4 database for that specific text object which is, again, returned as JSON and also contains navigational links to the previous and next sections of the text. This full-text JSON is rendered for the user to read inside a webview.
                7 We use a webview to display text objects in order to apply the same CSS formatting rules that we use for web versions. The PhiloReader also allows users to bookmark text objects for easy access in later sessions by retaining its PhiloLogic object pointer. 
            
            At the time of writing this proposal, we have succeeded in developing a functional API and Android code to work with it as proof of concept. Going forward, we will continue to work to make the API as generic and simple to use as possible, and fully RESTful compliant. 
            Our ultimate goal is to allow other development teams to pick and choose any subset of PhiloLogic4 functionality through the API to build their own interfaces. For instance, developers could integrate concordance search functionality into a traditional desktop app under Windows or MacOS, or into a web environment like Drupal or Django. Developers could also use the API to plug frequency or collocation reports into modern visualization tools like d3.js. And since the API always returns pointers to original text objects, any new interface will be able to have links from results display into fuller document context. 
            ARTFL chose Android Java as the initial development language for the apps because of existing in-house capability. At the time of the conference, we will make available a skeleton version of our Android code for other groups to adapt or simply examine to see how we interact with the API. In the coming months we intend to have parallel apps developed for iPads, though we are not certain they will be ready in time to present at DH. Nevertheless, ARTFL believes these Android apps demonstrate the ease and flexibility of using the PhiloLogic4 API to develop new ways of interacting with text databases. 
            Screenshots
            
                
            
            Figure 1. Search for term ‘sun’ in 
                Romeo and Juliet in the Shakespeare database. 
            
            
                
            
            Figure 2. Full-text view from 
                Romeo and Juliet with search term ‘sun’ highlighted.
            
            
                
            
            Figure 3. Frequency by title search results for ‘flay’ in ECCOTCP.
            
                
            
            Figure 4. JSON object of frequency by title search results for ‘flay’ in ECCOTCP, http://artflsrv02.uchicago.edu/philologic4/ecco_tcp_demo/scripts/get_frequency.py?report=concor dance&amp;q=flay&amp;method=proxy&amp;title=&amp;author=&amp;frequency_field=title&amp;format=json.
            Notes
            1. At the time of writing, we have built demonstration apps around the ECCOTCP text collection and the MONK project’s Shakespeare’s Plays data. Versions of these apps can be downloaded from http://artflsrv02.uchicago.edu/downloads/app_download/.
            2. For general background on PhiloLogic4, see Allen, T., Gladstone, C. and Whaling, R., PhiloLogic4: An Abstract Query TEI System, 
                Journal of the Text Encoding Initiative, 5 (June 2013). Development is ongoing; code resides at https://github.com/ARTFLProject/ PhiloLogic4.
            
            3. See http://artflproject.uchicago.edu/.
            4. See the ‘Screenshots’ section for images of the interface and an example of a PhiloLogic4 JSON object.
            5. These calls that the app makes to the dispatcher script are subject to change as we continue to refine the API. Eventually, we might choose to bypass the dispatcher, as we do for frequency and table of contents calls from the app, and instead communicate with CGI scripts exclusively.
            6. ListView is Android terminology for a layout that displays a vertically scrollable list. See http://developer.android.com/guide/topics/ui/layout/listview.html.
            7. Again, Android terminology for a view that displays web pages. See http://developer.android.com/reference/android/webkit/WebView.html.
        
    


        
            Le portail d'édition numérique de sources, 
                
                    http://xml-portal.symogih.org/
                
                , que nous présentons aux DH2016 au travers de deux projets d’édition numérique n'est pas un outil de visualisation d'images ou de textes comme pourraient l'être des systèmes de type Omeka ou Drupal. Il s’agit de mettre à disposition de manière dynamique à la fois le texte d'une source mais aussi les données d’une base relationnelle qui constituent son apparat critique. Il faut concevoir ce portail d'édition davantage comme une brique de développement du projet symogih.org. 
            
            Le 
                
                    projet symogih.org
                 (Système Modulaire de Gestion de l'Information Historique) a développé un modèle générique de stockage des données historiques permettant leur interopérabilité et leur publication [
                http://symogih.org/, tous les sites web ont été consultés le 30 octobre 2015]. À partir de ce modèle a été mis en place un système d'information collaboratif pour la recherche en histoire qui est aujourd'hui utilisé par 15 projets de recherche et environ 50 utilisateurs individuels. La plateforme du projet symogih.org offre un outillage numérique accessible et pérenne pour le stockage et la publication de données extraites de l'étude de documents archivistiques et bibliographiques. Il est possible d'intégrer des données de nature variée qui décrivent l'activité humaine, sociale, économique ou intellectuelle rassemblant, autour d'événements datés et sourcés, des acteurs individuels ou collectifs, des concepts ou des objets géographiques. Le système autorise également l'
                
                    articulation de ces données avec des textes codés en XML
                 – traités selon les recommandations de la Text Encoding Initiative [
                
                    https://groupes.renater.fr/wiki/symogih/symogih_manuel/edition_de_textes_en_xml-tei
                ] – ou encore la mise en relation avec des images et leurs métadonnées. La réalisation d'un 
                
                    système d'information géographique
                 (SIG) [
                http://geo-larhra.org/] joue un rôle essentiel dans ce modèle afin d'associer à ces différents objets leur “empreinte spatiale” et ainsi permettre des analyses spatiales diachroniques.
            
            L'un des derniers développements du projet symogih.org a été la mise en ligne d'un 
                
                    portail de publication des éditions numériques
                 élaborées au sein du LARHRA. Si symogih.org permettait aux chercheurs de mettre à disposition les données structurées issues de leur travail analytique, le portail d'édition rend désormais possible la contextualisation d'une source écrite grâce à ces mêmes données. Les textes encodés en XML/TEI sont stockés sur un serveur eXist-db à partir duquel sont développés différents services (visualisations du texte et des données, géolocalisation, possibilité d'export, moteur de recherche). Le point de jonction entre le texte numérique et la base de données se situe à l'intérieur même des balises TEI où sont intégrés les identifiants du système d'information qui font le lien avec les données structurées. Des fonctionnalités de visualisation peuvent être développées à partir des textes encodés stockés dans une base de données native XML, offrant davantage d'interactivité avec les données et augmentant l'expérience de lecture. De plus, grâce aux technologies web, via les langages XQuery + HTML/Javascript, le portail d'édition numérique permet de collecter des données au-delà du référentiel commun de symogih.org directement à partir du web de données (DBpedia, IdRef, ...). 
            
            À ce jour, la partie publique du portail recueille deux projets, l'un d'édition de sources (les 
                Mémoires de Léonard Michon), l'autre d'annotation sémantique et de contextualisation de documents concernant l'histoire des savoirs scientifiques à l'époque moderne (Society religion science) [
                
                    http://xml-portal.symogih.org/web-publications.html
                ]. Un troisième projet d'édition de documents est en cours d'élaboration et verra bientôt le jour : l'édition des Actes des synodes des églises réformées de Bourgogne.
            
            L'édition numérique des 
                Mémoires de Léonard Michon fait partie d'une recherche doctorale [Letricot, Rosemonde. 
                Édition critique numérique des Mémoires
                 de Léonard Michon (1715-1746). Sous la direction de Hours, Bernard. Université Jean Moulin Lyon 3, LARHRA UMR5190] qui vise à mettre à disposition du public et de la communauté scientifique l'édition critique du Journal historique d'un notable lyonnais relatant la vie des élites bourgeoises de la ville de Lyon de la première moitié du XVIIIe siècle. Le travail d'encodage XML/TEI s'est principalement centré sur l'identification des segments d'information et des entités nommées (personnes, institutions, lieux, etc.) ce qui permettra de recourir à des analyses quantitatives sur les pratiques d'écriture (fréquence, récurrence de noms, etc.) et sur la nature des informations données dans l'ouvrage, que nous pourrons ensuite traduire en parcours de lecture pour le public (parcours thématique, biographique, chronologique, etc.).
            
            Quant aux Actes des synodes des églises réformées, ils représentent une source essentielle pour la connaissance du protestantisme français sous l’Ancien Régime. Ces assemblées réunissent régulièrement des représentants de toutes les églises d’une province pour traiter des affaires qui leur sont communes : questions financières, disciplinaires, doctrinales, etc. Si les sources sur le protestantisme français font l’objet d’une édition chez Droz dans une sous-série de la collection “Travaux d’Humanisme et Renaissance” intitulée “Archives des Églises réformées de France”, il n’y avait pas de projet en Humanités numériques sur le sujet [Une édition “papier” des Actes des Synodes Provinciaux des Églises Réformées est en cours chez Droz, le premier volume édité par Didier Boisson a été publié en 2012 et concerne l’Anjou-Touraine-Maine (1594-1683). Le second volume proposera les actes des églises de Bourgogne et sera édité par Yves Krumenacker]. Ce sera bientôt le cas avec l'édition sur le portail XML du projet symogih.org des Actes des synodes des églises réformées de Bourgogne au XVIIe siècle, réalisée sous la direction de Yves Krumenacker de l'Université Jean Moulin Lyon 3.
            Ces deux projets d'édition ne se limitent pas à la seule publication de sources mais proposent une édition enrichie par des renseignements récoltés dans des sources complémentaires, saisis dans la base collaborative du projet symogih.org et utilisés non seulement pour préciser l'un ou l'autre renseignement fourni par le texte, selon la démarche d'annotation classique d’une édition papier, mais en proposant également une explication contextuelle dynamique – que permet l'édition numérique – en croisant toute sorte de données et tout en conservant leur traçabilité, par l'enregistrement des sources et de la bibliographie pour chaque information. Il est ainsi possible de reconstituer, par exemple, la généalogie et la carrière d’un pasteur, ou sa bibliographie [L’utilisation d’une base de données collaborative et cumulative permettant de multiplier les sources : journal de pasteur (Bernus, Auguste (1888). Le ministre Antoine de Chandieu d'après son journal autographe inédit 1534-1591. 
                Bulletin historique et littéraire publié par la Société de l'histoire du protestantisme français, Tome XXXVII), sources régionales (Papillon, Philibert (1742). 
                Bibliothèque des auteurs de Bourgogne. Dijon: Philippe Marteret), 
                Registres de la Compagnie des pasteurs de Genève, édités chez Droz].
            
            Les lieux mentionnés dans les écrits et dans la documentation annexe, sont renseignés et géolocalisés dans le 
                
                    gazeteer du projet symogih.org
                . À partir des données spatiales encodées dans le texte XML, on pourra ainsi réaliser des cartes interactives illustrant différents aspects du codage : villes organisatrices des synodes, lieux d'origine des pasteurs, carte des églises absentes, parcours professionnels des pasteurs. Des fonctions interactives permettront de rebondir de la carte vers les textes ou les informations respectives.
            
            Nous soulignerons lors de la présentation l'apport de la visualisation dynamique des documents pour les deux éditions, chacune avec ces spécificités. En particulier, l'intégration directe des données de la recherche permet une plus grande interactivité : l’apparat critique peut être à tout moment complété au fil des découvertes des chercheurs par la mobilisation simultanée de données provenant d'un silo d'information commun. De plus, les interfaces de visualisation spatiale, ou les graphes mettant en évidence les relations entre les textes et leurs contenus, facilitent l'accès aux documents édités. Les nombreuses possibilités d'enrichissement et d'exploitation des textes amènent les chercheurs en histoire, mais aussi le public, à une découverte sous d'autres angles et avec de nouvelles perspectives du contenu des textes édités. 
            Nous souhaitons montrer l’apport pour la recherche historique d’un portail d'édition numérique de sources, en insistant sur les bénéfices d'une utilisation croisée de textes encodés en XML/TEI et d'une base de données collaborative. Les projets présentés permettent de retracer le processus de traitement de l'information, de la source à son édition numérique dynamique. 
        
    


        
            The George Washington Financial Papers Project exists at the intersection of two challenges editors currently face: managing complicated editorial work and navigating the world of digital publication. By focusing on a particularly difficult and dynamic dataset—financial documents—work has advanced on three interconnected fronts: 1) developing document templates for both traditional financial documents, such as account books and ledgers, as well as receipts, journals, and memoranda; 2) developing taxonomies and data visualizations; and 3) constructing an open-source content management/editorial/publication platform. The work has resulted in the development of both an open-access digital edition of Washington’s financial documents as well as the groundwork of Drupal for Editors prototype—a Drupal-based, open-source, editorial/publication platform—providing editors with a stable, flexible, and powerful platform to build engaging digital editions of financial documents.
            In 2013, The Papers of George Washington received a grant from the National Historical Publications and Records Commission (NHPRC) for the perfection and population of the content management database (DocTracker) with Washington’s three major ledger books; preparation of Gouverneur Morris’s 1811-1816 account book and its entry into the content management database in partnership with the Gouverneur Morris Papers at the New-York Historical Society; and the completion of a primary version of a web interface that will provide users with free access to the edition’s entire content and permit downloading and data manipulation. 
            During our partnership with DocTracker we helped design a viable content management and customized editorial workflow solution built on the proprietary, commercial database software FileMaker Pro. DocTracker allowed us to manage both document records and content identifications, and associate both with transcriptions. But as a publication platform it was limited because of its use of XML. We investigated alternative publication options and decided on Drupal, a highly-configurable open-source content management system. We determined Drupal to be the best publication solution for several reasons: 1) at its core, Drupal is a database in which imported content can be mapped to fields, allowing for robust displays and searching, querying, and browsing; 2) Drupal is accessible, both in terms of cost and usability and has a large user community; 3) both the backend (content/data) and frontend (website interface) are managed in the system; and 4) Drupal is open-source and its core and add-on (module) code are developed and actively maintained by a large international developer community. 
            Drupal has allowed the project to confront the numerous challenges inherent in these documents: (1) different types of financial documents are formatted in distinct, though standardized, ways, and the formatting of financial documents carries implied meanings; (2) transactions are full of dittos, abbreviations, and short hand, that raise a question of what kind of fields should be created to capture the transcription and clear text, thereby making both the text and content searchable; (3) the documents present issues of currency, valuation, and barter; and (4) a hierarchy of documents exist, and therefore the same transaction may be recorded in a day book, account, and ledger, etc., generating multiple instances of the same transaction.
            Indeed, one of the primary goals of the Project is to make accurate transcriptions of the documents available, in keeping with the long tradition of the Papers of George Washington documentary editing project. However, the types of information, or the “data,” contained in these documents are not easily accessible using common search and query techniques. The challenges, as described above, make it impossible to simply transcribe and put online, ready to be searched and understood document transcriptions. The solution involves a combination of transcription and corresponding data fields (where dittos, abbreviations, and short hand have been expanded), node references associating various content types, and term references connecting taxonomies. Additionally, Drupal provides a place to develop and manage taxonomy lists for specific content types, such as financial documents, to enhance the grouping and sorting of content and be used to identify relationships between different types of content.
            Developing this system has challenged us to think creatively about all aspects of the editorial and publication process, resulting in innovative ways for users to explore, analyze, and interact with content. This poster and hands-on demonstration will explore these issues and the technological solutions to make these documents available, as a free online resource as well as highlight strategies for content searchability, including annotation, glossaries, indexes, and linking. 
        
    


        
            The North Carolina (USA) Jukebox project transforms an inaccessible audio archive from the 1930s, of historic North Carolina folk music collected by Frank Clyde Brown, into a vital, publicly accessible digital archive and museum exhibition. Led by Trudi Abel, a librarian in the Rubenstein Special Collections Library at Duke University, and Victoria Szabo, a faculty member in Visual and Media Studies and Information Science + Studies at Duke, this interdisciplinary, collaborative effort also involves scholars in music and folklore, music and preservation librarians, digital media specialists, descendants of the original performers, and contemporary musicians who play this music professionally today (Archives Alive Initiative | Trinity College of Arts and` Sciences, 2016). 
            As a teaching experience associated with the Library’s Archives Alive initiative, the project offers opportunities for students from arts, music, computer science, and engineering programs to learn about the collection and develop an exhibition from initial concept to execution, and to do so in collaboration with a diverse set of mentors and collaborators who help them understand the histories and technologies involved, as well as stakes of their presentation choices. As a ongoing archival project, it demonstrates the challenges and opportunities inherent in working out a major library archive and preservation effort alongside a live curricular intervention and planned public exhibition. As a research project, it offers scholars in media studies a firsthand view of how material recording and playback technologies and their affordances help shape subsequent cultural histories, and affect what we can recirculate and share today. Taken together, these strands demonstrate that introducing digital cultural heritage project development as a shared objective enriches student learning, encourages library archiving and preservation projects to consider their public facing dimensions as they construct new resources, and offers digital humanities and media studies scholars meaningful opportunities to collaborate with colleagues in historically minded disciplines around new forms of scholarly production – in this case data-driven exhibitions at the Mountain Music Museum in Western NC, at the Rubenstein Special Collections Library at Duke, and online.
            
                
                Figure 1. NC Jukebox Project Advertisement
            
            Our project begins with the songcatcher himself. In the 1930s Frank Clyde Brown, Duke Professor of English, and co-organizer of the North Carolina Folklore Society (1913) as Zeke Graves in our Library tells us, began recording and archiving Western North Carolina folk music (Graves, 2015). Following in the tradition of folklorist Alan Lomax, and songcatcher/musician Bascom Lunsford as chronicled by Loyal Jones, along with other famous songcatchers of the period, he drove around region capturing a range of singers and songs using the technology available in the period, a notebook and Dictaphone equipped with first wax cylinders and later aluminum cylinders (Jones and Forbes, 1984). Like us, Brown involved his students in the project as well, encouraging them to capture songs and research their origins. Today most of those recordings are still housed on wax cylinders and glass disks in the Duke Libraries, and in the Library of Congress, with about 400 songs having already been converted to digital formats. The rest are being converted as part of a substantial Council of Library and Information Resources grant.
            In addition to learning about Brown, histories of the music, songcatching, and folklore practices of the period, students in a Fall 2015 NC Jukebox course began to work closely with the 400 digitized recordings we currently have available, developing metadata, transcribing songs, and organizing their materials in spreadsheet, blog, and database form. Our project also explores biographies of the singers, transcribes the songs as heard on the tapes in in comparison to other versions, and traces the Scotch‑English history and contemporary analogues of the songs themselves through research in Child’s 
                Ballads and other key sources (Child et al., 2001). In addition, we have begun to demonstrate change over time and space through maps, patterns, flows, timelines, and networks of the music - a kind of distant listening, or viewing of its collection and presentation, with more to come. In the physical exhibits, interactive touchscreens, period photos, and hybrid analog‑digital audio playback machines – a radio, a Jukebox, and perhaps a 78‑playing phonograph – will invoke the historical conditions of production and reception of the music for diverse audiences.
            
            As an historiographical research project, NC Jukebox is also offering opportunities to explore firsthand how social and material conditions affect the writing of cultural history itself. Over the course of this project we have learned about the history of songcatching and folklore as social and academic practices designed to verify expectations about a specific kind of musical heritage. Brown died before his work could be compiled into the published versions of his work. As his papers reveal, not all of the songs he collected were included in the final, posthumous collection of his work (Guide to the Frank Clyde Brown Papers, 1912-1974, 2016). His subsequent editors, like other before them who were seeking a pure musical tradition descended from that of the Scots-Irish settlers in the region, as seen in Ritchie and Orr’s 
                Wayfaring Strangers, for example, picked and chose songs to include in the published work (Ritchie, Orr and Parton, 2014). Their criteria are (helpfully for future researchers) sketched out in their editorial notes, which are also in the archives. Songs that were too popular, had been published, were too religious, or, perhaps most significantly, were from African American traditions, were excluded from the published collection, even if familiar from other sourdes like the Library of Congress Checklist of Recorded Songs (U.S Library of Congress, 1942). On another note, we also experienced the lingering effects of musical and cultural segregation first-hand as a class when our well-intentioned musical guest, a contemporary folksinger who is helping keep the Mountain Music traditions alive today, explained that he was going to perform a song as he had heard his “colored” neighbors sing it growing up (McKinney, 2015). This moment became part of a class conversation about representing tradition while at the same time acknowledging our contemporary perspectives upon it in how we frame the music.
            
            This project is also about the history of recording and reproduction technologies, which has deeply affected the content. Encountering recordings made from wax cylinders and glass disks, which included a white-glove visit from Special Collections as well as examining photographs of Brown and other “in the field,” encouraged conversations around our continuously evolving standards and expectations for archiving and reproduction. Our students had to consider whether it was more “authentic” to leave in the hisses and crackles that had made it into the third generation audio files they were listening to, or whether they could and should attempt to clean up the sound quality so it was closer to the “original” source – the singers themselves. Were we archiving the archives, or the ur-performances? Our students also learned about and from Charles Bond, the onetime Duke undergraduate (and now lawyer in San Francisco) who serendipitously took it upon himself in the 1980s to transfer some of the existing recordings to reel-to-reel tapes, using a moog synthesizer to clean up some pops and crackles. 
            
                
                Figure 2. Wax Cylinder from the FCB collection
            
            Further, we discovered how the limits of the recording medium itself reveal the priorities of the documentarians and archivists involved. Wax disks could only record 6-7 minutes of a song, as the 
                Federal Cylinder Project editors note (Gray and Schupman, 1990). Songcatchers might record just one stanza of a song, rather then the whole performance, a fact that highlights that it was the intellectual process of abstracting data from the performance, to be converted to written notation and lyrics, rather than the performance itself, that was most valued for academic folklore purposes. The recordings were data to mine rather than songs to hear. We have also confronted challenges to our efforts in repatriation and exhibition as we began to develop our downloadable “Greatest Hits” of the FCB collection. Because of copyright restrictions on published songs, in the end we may need to limit our final song choices to the purely folkloric – putting us in some cases right back in line with Brown’s purity-seeking editors! We have even begun to wonder about the NC Jukebox idea itself as a title and concept, given that the term jukebox didn’t come into common parlance until the 1940s, as we learned from 
                Jukeboxes:An American Social History, and the music we are sharing was most likely shared at the time either locally or over live radio, as our guest Terry McKinney told us (Segrave, 2002). Our presentism is a both a problem and an acknowledgement of our own historicity and subjectivity.
            
            NC Jukebox is serving as a prototype for future “Archives Alive” projects at Duke University in terms of pedagogical approach, access to primary and secondary source materials from Special Collections, community engagement, and digital platform and exhibition development. As a digital heritage project designed to serve multiple audiences, the digital and onsite exhibition components are being built with an eye towards multiple display formats and locations for a single set of materials within a grown growing database of content. This includes exhibition in a regiona museum organized by McKinney himself (Neufeld, 2015). It also necessitates metadata standards and library infrastructure, a conversation that is ongoing with our Library staff. For the exhibits in Western North Carolina and at the Rubenstein Library we have websites, Omeka exhibits, and interactive web graphics, as well as the downloadable playlist and physical exhibits and listening stations. For the archive, however, we are working with Library and technology partners on a more sustainable and flexible content management system in Drupal that draws content from the more permanent institutional repository solution, which will serve as the substrate for future development as well as, we hope, drive later installations. Our hope is that subsequent generations of students, librarians, and scholars will be able to build upon what we have done in bringing the archives alive.
        
        
            
                
                    Bibliography
                    
                        Brown, F.C. (2016). The Frank C. Brown Collection of North Carolina Folklore; the folklore of North Carolina, collected by Dr. Frank C. Brown during the years 1912 to 1943, in collaboration with the North Carolina Folklore Society : Frank C. Brown Collection of North Carolina Folklore : Free Download and Streaming: Internet Archive. (2016). [online] Internet Archive. Available at: https://archive.org/details/frankcbrowncolle00fran [Accessed 6 Mar. 2016].
                    
                    
                        Gray, J. and Schupman, E. (1990). 
                        The Federal cylinder project. Washington: American Folklife Center, Library of Congress.
                    
                    
                        Graves, Z. (2015). 
                        ...and We're Putting it on Wax (The Frank Clyde Brown Collection) - Bitstreams: The Digital Collections Blog. [online] Bitstreams: The Digital Collections Blog. Available at: http://blogs.library.duke.edu/bitstreams/2015/06/19/and-were-putting-it-on-wax-the-frank-clyde-brown-collection/ [Accessed 6 Mar. 2016].
                    
                    
                        McKinney, T. (2015). 
                        North Carolina Mountain Music. Durham, NC. Available at: http://bit.ly/1MAPQRs [Accessed 6 Mar. 2016].
                    
                    
                        Neufeld, R. (2015). Visiting Our Past: 1930s a Golden Age for music in WNC. [online] Citizen Times. Available at: http://www.citizen-times.com/story/life/2015/02/01/visiting-past-golden-age-music-wnc/22705455/ [Accessed 6 Mar. 2016].
                    
                    
                        Segrave,K. (2002). 
                        Jukeboxes: An American Social History. London: McFarland. 
                    
                    
                        Child, F., Heiman, M., Heiman, L. and Child, F. (2001). 
                        The English and Scottish popular ballads. Northfield, Minn.: Loomis House Press.
                    
                    
                        Duke University. (2016). Archives Alive Initiative | Trinity College of Arts and Sciences. [online] Trinity.duke.edu. Available at: https://trinity.duke.edu/initiatives/archives-alive [Accessed 6 Mar. 2016].
                    
                    
                        Duke University. (2016). Guide to the Frank Clyde Brown Papers, 1912-1974. [online] David M. Rubenstein Rare Book and Manuscript Library. Available at: http://library.duke.edu/rubenstein/findingaids/brownfrankclyde/ [Accessed 6 Mar. 2016].
                    
                    
                        Jones, L. and Forbes, J. (1984). 
                        Minstrel of the Appalachians. Boone, N.C.: Appalachian Consortium Press.
                    
                    
                        Ritchie, F., Orr, D. and Parton, D. (2014). 
                        Wayfaring strangers: The Musical Voyage from Scotland and Ulster to Appalachia. Chapel Hill: UNC Press.
                    
                    
                        
                            U. S. Library of Congress. (1942). Division of music. Archive of American folk song.,
                            Check-list of recorded songs in the English language in the Archive of American folk song to July, 1940. Alphabetical list with geographical index. Washington, D.C.: Library of Congress, Division of music.
                        
                    
                
            
        
    


        
            
                Background
                Online education has been advocated as the ultimate way of democratizing knowledge, but recent research indicates that there are reasons for concern. As the Allen & Seaman 2014 report underlines, 66% of higher education institutions report that online education remains critical to their long-term strategy while 74% of chief academic officers consider the learning outcomes for online courses to be ‘as good as or better’ than traditional face-to-face courses. But “despite this confidence in online education, researchers continue to report ‘compromised quality in online courses’ as one of the concerns of faculty, administration, and the general public” (Kidder, 2015; Selingo, 2014). In the landscape of online teaching, MOOCs (Massive Open Online Courses) have received much attention in both academic and popular publications  (Bayne and Ross, 2015; Bulfin et al., 2014; Clara and Barbera, 2013) despite the fact that they are not representative of the diverse modalities of online teaching.
                Siemens (2012) makes a useful distinction between xMOOCs (behaviorist MOOCs) and cMOOCs (connectivist MOOCs). The former emphasizes “a more traditional learning approach through video presentations and short quizzes and testing” with a focus on “knowledge duplication”, whereas the latter focus on “knowledge creation” (Siemens, 2012). Along the same lines, Ozturk recently reported that new variations of MOOCs have emerged becoming more market oriented “aligning with instructivist, cognitive, and behaviourist pedagogy” (Ozturk, 2015). Moreover, the financial model of the MOOCs raises questions about the audience for and motivations behind this method of teaching (Ozturk, 2015; Manjoo, 2015).
                Conscious of this present situation, the #dariahTeach project (funded by an Erasmus+ Strategic Partnership) is developing a  network based in seven partner countries exploring the production, dissemination, and promotion of high quality, dynamic, extensible, localisable, and integrated educational materials for the digital humanities specifically tailored for third level education. It is adopting a cMOOC philosophy which focuses on ‘creation, creativity, autonomy, and social networked learning’ (Siemens, 2012) to provide pedagogical content that can be easily integrated into diverse teaching and learning situations. 
                A key consideration in the design of the platform is interoperability between courses/modules (and units within those modules) since DH draws on a wealth of methods and tools from a variety of disciplines. Moreover, it is envisioned that these modules will be used beyond the DH community as the societal impact of a culturally-driven digital transition grows opening up new ways of collaborating on productive theory and critical thinking (Hayles, 2012). Thus a goal of #dariahTeach is to develop rich educational materials that 1) instructors in the growing number of Digital Humanities programmes can use as appropriate to their own institutional settings and learning outcomes; 2) instructors in other disciplines can draw on and; 3)  students who are not at institutions that have DH expertise can use to develop the skills and methods, as well as understand the theoretical basis, to engage in digital humanities and humanities research.
                The project team is currently developing the infrastructure and design of the modules based on the production of five modules: Introduction to Digital Humanities, Text Encoding, AudioVisual Media and Multimodal Literacies, Retrodigitizing Dictionaries, and Ontologies and Knowledge Management. This paper will present the results of preliminary research carried out through an extensive study of user requirements, as well as desk research on module and platform design informed by a workshop in Belgrade funded by the Digital Research Infrastructure for the Arts and Humanities (DARIAH) on developing open educational materials. 
            
            
                Analysis of User Requirements
                The design and the implementation of a successful platform-based learning environment  melds concepts from psychology, education, and human-computer interaction. Poor interface design can become a serious obstacle to the learning outcome, as it may slow the process down and impose cognitive obstacles. To this end, a qualitative analysis and interpretation of online teaching practices and recommendations in the DH domain and the elicitation of corresponding user requirements was based on a series of semi-structured interviews with experienced instructors of online courses within Europe.
                Findings of the user requirement process are a key component of the development of the #dariahTeach platform. These indicate that the platform needs to cater for the following needs: be adaptable to different learning methodologies; allow for persistent roles; provide an API or advanced forms of web services so that new unforeseen components can be added to the environment; support  ad hoc groupings and grouping of materials across modules and units; allow for both synchronous and asynchronous collaboration and communication and enable user customization.
            
            
                Module Design
                #dariahTeach modules are designed as building blocks tailored to the exigencies of teaching situations in different educational and cultural contexts, allowing  for localization and adaptation (via translation, subtitles, domain-specific examples etc.). By offering examples of and encouraging further adaptation of training materials to specific linguistic/cultural contexts, #dariahTeach will dispel any notion that the use of ICT methods leads to abstract representations of culturally impoverished outputs. 
                It is important to stress two levels of translatability of module design: 1) translatability and adaptability of the language of instruction; and b) selectability, translatability and adaptability of primary sources and materials that are used in instruction. This means that an English-language module on Text Encoding, for instance, is localizable both in terms of the instructional narrative, as well as the kind of texts that are used to exemplify the taught principles and methods of text modeling: different genres (poetry, prose, drama) but also language (Latin, Greek, Serbian, Dutch etc.) 
                Our “Introduction to DH” module will also not attempt to impose a single pedagogical narrative on what is a constantly evolving and highly diverse, interdisciplinary field. Instead, our Introduction to DH is based on a micromodular, polycentric approach: a collection of mutually-linked, cross-referenced, metadata-rich short videos that shed light on DH as a community of practice from multiple perspectives without creating a false sense of uniformity.
            
            
                Platform Design
                Modules will be made available via an online portal/ web application based on existing solutions. This paper will explore the decision tree in adopting a solution including whether to use a well-established Content Management Systems (eg Drupal, WordPress, Joomla) with Learning Management System plugins and appropriate customizations or the use of a customizable Learning Management System (such as  Moodle or Blackboard). Considerations feeding into the decision tree include the platform being open source, freely available, well documented and customizable with plugin development support;   support for multilinguality; an embedded xml editor; collaboration and interaction functionalities (eg chats, forums and wikis); test and assessment functionalities; extended search functionalities for available metadata (mapped to Dublin Core and LOM to facilitate sharing and support interoperability and reusability (Roy et al., 2010); and copyright attribution and licence management functionalities. 
            
            
                Conclusion
                The paper will conclude with longer-term prospects for the project. Oversight of #dariahTeach will be maintained after the grant has ended by a General Editor and Editorial Board under the oversight of the DARAH’s Research and Education Competency Centre.
            
        
        
            
                
                    Bibliography
                    
                        Allen, I. E. and Seaman, J. (2014). 
                        Grade Change – Tracking Online Education in the United States, Babson Survey Research Group and the Sloan Consortium, LLC. http://www.onlinelearningsurvey.com/reports/gradechange.pdf
                    
                    
                        Bayne, S. and Ross, J. (2015). MOOC Pedagogy. In Kim, P. (ed.) 
                        Massive Open Online Courses: The MOOC Revolution. Oxford: Routledge.
                    
                    
                        Bulfin, S., Pangrazio, L. and Selwyn, N. (2014). Making ‘MOOCs’: The construction of a new higher education within news media discourse. 
                        International Review of Research in Open and Distance Learning, 15(5): 209-305.
                    
                    
                        Clarà, M. and Barberà, E. (2013). Learning online: massive open online courses (MOOCs), connectivism, and cultural psychology. 
                        Distance Education 34(1): 129-36. 
                    
                    
                        Ferguson, R. and Sharples, M. (2014). 
                        Innovative Pedagogy at Massive Scale: Teaching and Learning in MOOCs. Open Learning and Teaching in Educational Communities. Springer.
                    
                    
                        Hayles, N. K. (2012). 
                        How We Think. Digital Media and Contemporary Technogenesis. Chicago University Press.
                    
                    
                        Kidder, L. C. (2015). The Multifaceted Endeavor of Online Teaching: The Need for a New Lens”. In Hokanson, B., Clinton, G., Tracey, M. (Eds.) 
                        The Design of Learning Experience Creating the Future of Educational Technology. Springer, pp. 77-91.
                    
                    
                        Manjoo, F. (2015, 16 September). ‘Udacity Says It Can Teach Tech Skills to Millions, and Fast’. 
                        The New York Times,  http://nyti.ms/1ihbcp7
                    
                    
                        Ozturk, H. T. (2015).
                        Examining Value Change in MOOCs in the Scope of Connectivism and Open Educational Resources Movement”. 
                        International Review of Research in Open and Distributed Learning 16/5, Creative Commons 4.0.
                    
                    
                    Peters, D. (2014).
                        Interface Design for Learning: Design Strategies for Learning Experiences. San Francisco: New Riders.
                    
                    
                        Roy, D., Sarkar S. and Ghose S. (2010). A Comparative Study of Learning Object Metadata, Learning Material Repositories, Metadata Annotation and an Automatic Metadata Annotation Tool. In Joshi, M., Boley, H., Akerkar, R. (eds.). 
                        Advances in Semantic Computing 2: 103-26.
                    
                    
                        Selingo, J. J. (2014). “Demystifying the MOOC”. 
                        The New York Times, 
                        http://nyti.ms/1u6MYCL
                    
                    
                        Siemens, G. (2012). MOOCs are really a platform, In idem 
                        Elearnspace blog, 
                        http://www.elearnspace.org/blog/2012/07/25/moocs-are-really-a-platform/
                    
                
            
        
    


        
            The nature of nineteenth-century culture, particularly literary, publication, and print culture, meant that many female writers plied their trade in periodicals. Even many American writers who we think of today primarily as novelists—Harriet Beecher Stowe, for instance—first published much of their material in periodicals. Stowe's 
                Uncle Tom's Cabin first appeared serially in 
                The National Era, an abolitionist newspaper. But writers like Stowe gained and have maintained notoriety in part because their work also existed in book form. Many writers whose work remained trapped in periodicals have since fallen off the literary map, their writing accessible only in increasingly fragile and scattered print runs of newspapers held in libraries and archives, or on microfilm. Attempts to digitize cultural heritage material in periodicals have been far spottier than comparable attempts to digitize books for a variety of mostly practical reasons.
                    
                         While there have been a number of large-scale newspaper digitization projects, most of them have been undertaken by large commercial entities that charge a fee for access to the content and provide material of mixed quality. Readex and Proquest are two of the more popular services, and genealogy site Ancestry.com has also undertaken its own mass digitization of government records, census data, and newspaper and periodical material; all three charge for access to the content. While these services can certainly be useful for certain types of research work, the quality of the digitization, particularly transcription, is in nearly all cases quite poor, with transcriptions being derived from optical character recognition (OCR) software, which often has difficulty accurately transcribing the small and often smudged print of nineteenth-century periodicals. 
                     Over the past decade or so, scholars such as Kenneth Price, Susan Belasco, and Meredith McGill have argued for both an increased acknowledgement of periodicals and periodical writing as a key site of intellectual and literary exchange in the nineteenth century, and the increased utilization of digital tools for the editing, study, and dissemination of periodicals.
                    
                         See Price and Belasco's introduction to their edited collection, 
                            Periodical Literature in Nineteenth-Century America. Charlottesville, VA: University Press of Virginia, 1995. Also, see: Belasco. "
                            Whitman's Poems in Periodicals: Prospects for Periodicals Scholarship in the Digital Age." 
                            The American Literature Scholar in the Digital Age. Ann Arbor, MI: University of Michigan Press, 2011; McGill, Meredith. 
                            American Literature and the Culture of Reprinting, 1834-1853. Philadelphia: University of Pennsylvania Press, 2003.
                        
                     While the advent of the digital archive has afforded well-documented possibilities for the recovery and, more importantly, dissemination of previously unknown and/or largely inaccessible material,
                    
                         Perhaps the most relevant example of a digital recovery project, for the purposes of my presentation, is the 
                            Women Writers Project (http://wwp.northeastern.edu), now run out of Northeastern University and directed by Julia Flanders. Begun in 1988, the project has been instrumental in the recovery of rare or inaccessible work by early modern women writers (the project covers a period from 1526-1850, although the vast majority of texts are from the sixteenth and seventeenth-centuries). In addition to providing access to digitally encoded texts, the project has also provides various research and teaching materials. However, the 
                            Women Writers Project is only accessible with a paid subscription and does not deal with periodicals.
                        
                     digital transcription, encoding, and publication of literary texts still remain skills that many academics feel are well beyond their technical capabilities. This means that many of the people best positioned to undertake such recovery work—literary scholars and other subject-specialists—are held back merely by a technological learning curve that they feel is too steep.
            
            For the past year I have been using basic and widely-available digital tools and resources to build a digital archive of the newspaper writing of the nineteenth century American writer Fanny Fern, who, in the 1850s, was the highest-paid periodical writer in the country, writing for the widest circulated American periodical of its day. The project, 
                Fanny Fern in The New York Ledger (http://fannyfern.org) is the first attempt to make available the full run of Fern's newspaper columns. Using the Drupal content management system, I have been making TEI-encoded transcriptions of Fern's columns, high-resolution digital images of the complete 
                Ledger issues, and brief critical apparatuses about both Fern and the 
                Ledger available for free public and scholarly use. While the digital methods and tools used to construct my project were fairly simple, the functionality and appearance of the finished product belie the relative ease (from a technical standpoint) of its creation. But the 
                Ledger is just one paper and Fanny Fern is just one writer, albeit a significant one. "There are countless other periodicals and writers, particularly women writers, that deserve this sort of attention," I thought. "If only other 19
                th century lit scholars could see how easy this is!" And an idea was born.
            
            I already had a relatively simple TEI template designed to handle the metadata, textual content, and linking of digital image files for periodicals. I had an XSLT style sheet that converted the TEI to HTML. I had project documentation for how I had set up my own domain name and hosting space (using Reclaim Hosting, a web hosting service specifically designed for educators and students). I had documentation about how I had installed and configured Drupal. I had documentation about how I had incorporated the HTML of the transcribed newspaper columns into the Drupal architecture. And I had documentation about how I had tweaked and played around with the design of the site. In short, I had everything anyone would need to build his or her own digital archive; all they would need is the content. If I could just share these files and instructions with other literature scholars, scholars who themselves are experts and masters of all kinds of content, then piece by piece and site by site, the gaps in literary and historical scholarship could to begin to close, be it ever so slightly. While I knew this had long been one of the (increasing number of) mantras of digital humanities, I now felt that I had the means to do my small part to contribute to its realization. Thus, using 
                Fanny Fern in The New York Ledger as an example and template, I plan to soon begin providing other literature scholars with a single package, a package that contains the tools and instructions they will need to construct their own digital archives—I'm going to start handing out FannyPacks.
            
            Geared mainly for those wishing to gather and display texts in a digital environment, these FannyPacks (a zipped collection of files) will include the needed TEI templates, style sheets, and thorough but simple documentation about digital imaging, hosting setup, and Drupal installation and execution. Scholars with a bit of tech savvy can choose to begin hosting their own projects right away. For those looking to first experiment before fully diving in, Reclaim Hosting easily allows for multiple subdomains to be hosted under an existing domain free of charge. Thus, I will provide testing space on my "fannyfern.org" domain where users can download their own installations of Drupal and experiment with adding their own content. While I have chosen to take the time to encode Fern's newspaper columns in TEI—for the preservation, interoperability, and potential for enhanced functionality of the files and their content—there are some users who might wish to simply "get their material out there" using HTML. Drupal's graphical user interface allows for the easy input of basic or full HTML. Thus, users unfamiliar with or not wanting to take the time to encode in TEI could simply encode their transcriptions in basic HTML and paste them into Drupal's GUI. The setup of both Reclaim Hosting and Drupal also provide ample opportunity for student involvement in the creation of such projects.
            Projects such as 
                Fanny Fern in The New York Ledger and those facilitated by the FannyPacks occupy a scholarly space somewhere between large-scale, institutionally-hosted TEI-based projects such as the 
                Willa Cather Archive or 
                Walt Whitman Archive (both of which are well-funded and boast a host of technical and subject specialists) and a low-cost collaborative TEI repository such as the TAPAS Project (http://tapasproject.org). While not requiring any institutional technical resources, projects based on the FannyPacks model possess greater customization and more autonomy than TAPAS projects. And, in the world of nineteenth-century American women's literature at least, there seems to be a desire for just such a model. A recently created listserv of the Society for the Study of American Women Writers (SSAWW) is geared specifically to those scholars already working on or interested in creating digital projects devoted to American women writers. And initial communication on the listserv has made clear the appeal and potential of a method and means for facilitating the publication of digital scholarly content centered around women writers, particularly periodical writers. But while my main and initial goal will be to work specifically with nineteenth century literature scholars, particularly scholars of women's literature, the FannyPacks certainly have a broader application and could be adapted to fit any manner of digital archival project.
            
            My poster will thus provide a brief overview of my current project, 
                Fanny Fern in The New York Ledger, a discussion of the project's potential to serve as a model, and specifics of the FannyPacks and their creation, distribution, and application.
            
        
    


        
            In recent years semantic technologies have become increasingly popular to represent, manage and publish data in the humanities. Virtual research environments with semantic backends are used to build complex networks, data is exposed as triples using RDF, and important vocabularies and thesauri are available as linked data. Ontologies like the CIDOC Conceptual Reference Model (CRM) are the semantic backbone of this approach and provide interoperability and data exchange beyond pure linking.
            WissKI (wiss-ki.eu) is a ready-to-be-used web-based virtual research environment and publishing framework that in its core relies on Semantic Web technologies to represent the curated knowledge. The user experience for data acquisition and presentation, however, intentionally borrows from traditional modes, while the user profits from the possibilities of linked and semantically enriched data. Thus, the system enables digital humanists to produce high-quality linked data, without having to cope with technical issues of the Semantic Web and ontologies in general or the often-quoted pecularities of CIDOC CRM in particular. This is achieved by defining a mapping between traditional index card or tabular style on the one hand and graph-based linked data on the other hand. The mapping may be opaque to the users and only be managed by an (ontological) administrator. Also, mappings may be shared between systems and projects, so that best practice patterns may evolve; this actually already has happened and still happens.
            By default, data may be input and displayed either as free text or as structured data via forms. Free text may be input through a graphical editor and is semantically indexed in terms of named entity recognition results, calendar date specifications, mentioned events, and also technical terms as far as appropriate authority files are available (e.g. Getty's Art and Architecture Thesaurus). Form input provides mechanisms for error reduction like spelling variants, e.g. by showing autocompletion hints that are again backed by available authorities. From the textual annotations, RDF triples may be generated and be reused as structured data. Furthermore, the system allows the upload, derivation and display of images. Other, more application-specific ways of data acquisition like mass imports or 2D/3D annotation may be included through extensions.
            From the technical perspective, WissKI is based on Drupal (drupal.org). Drupal is a widely used Web Content Management System with a big and active user and developer community. It has a modular architecture and there exists a vast variety of third party extension. Being such an extension, WissKI profits from a stable core system (security updates!) and also from these community contributions, providing all sorts of functionality. 
            As Drupal itself, WissKI is published as open source and can be downloaded from the project web site (wiss-ki.eu) or from github.
            Although WissKI in its core is domain-agnostic, it is designed to best fit the needs of object centered documentation and research as it is typical for many memory institutions, but also for research projects from art history, biodiversity, architecture, epigraphy, etc. As such it naturally goes together with the CIDOC CRM, an ontology designed for the documentation of cultural heritage. It is used by several academic and memory institutions in Germany in national and international research projects; it is used for such diverse purposes as research environment, curated collection management, virtual exhibition, or in courses and seminars.
            The tutorial aims at all researchers, archivists and curators who are interested in object documentation, in particular its semantic disclosure integrating data from (database and content management systems) form-based input and plain text fields. Furthermore it addresses people interested in applications of the CIDOC CRM.
            This half-day tutorial
            
                gives a short introduction to the (technical) approach of WissKI,
                presents current use cases and modes of use,
                shows how to install, configure, and use WissKI, and
                includes a hands-on for semantic modelling and data acquisition with WissKI and CIDOC CRM.
            
        
    
Brief summary
This paper provides a brief overview of library best practices for digital curation, with particular attention to the areas that highlight disciplinary tensions between library science and the humanities. The authors introduce the University of Victoria’s grant service “menu” for digital preservation and hosting services, and outline some of the most promising models for balancing creativity with sustainability in DH project design. We will suggest roles for libraries, researchers, administrators, and funders in helping to create technical and social conditions that nurture sustainable research projects in the digital humanities and beyond.

Abstract

Knowledge building is an iterative process that refines and extends previous research. Through citation, we acknowledge our debt to scholars and theorists whose work enables our own. The ephemeral nature of the digital world threatens to destabilize a centuries long system of scholarly communication and knowledge sharing. In a print ecosystem many immutable copies of an object are distributed globally and are curated by network of organizations. In the digital world a single copy of an object is served from a central location. Digital content is thus susceptible to manipulation, corruption, and erasure. The key to analog preservation is to ensure that artefacts remain the same. Digital preservation, in contrast, requires “active management” comprising constant changes, patches, and updates. Objects become quickly obsolete as the environments around them change.

Funding agencies are putting increased pressure on researchers to include sustainability plans in funding applications (NEH 2016, SSHRC 2016). Researchers often turn to the University Library to provide preservation solutions for digital projects without fully understanding the technical, policy, and funding implications of these requests. Libraries have made significant strides in planning for the long-term preservation of the many thousands of digital objects in our collections. Digitization projects adhere to strict standards for resolution, colour management, and file formats (FADGI 2016). Digital asset management systems like Hydra/Fedora provide a single place to store objects along with descriptive and administrative metadata that helps to determine the preservation actions that should be taken against each object (Goddard, 2016). Those actions include auditing and bit-checking of file systems to ensure against data loss, format migrations as media and file types become obsolete, replication of objects across different technology stacks and jurisdictions, and discovery interfaces that ensure continued discoverability and access. Libraries are building national networks that will allow us to replicate data across multiple jurisdictions to mitigate against disasters both natural and human (DPN 2016, Canadiana 2016, CARL 2016). Despite concerted efforts, only a handful of library repositories have so far met the stringent conditions that are necessary for certification as a Trusted Digital Repository, which requires technical and policy elements including plans for long term staffing and funding, and contingency plans in the event of organizational failure (CRL, 2015). Ultimately, libraries still can’t make guarantees about preservation for digital objects in our own collections, even those that are subject to internationally recognized best practices. This problem is compounded when DH research projects fail to adhere to adequate quality standards for objects (e.g. images, texts, video, maps, mark-up) and overlook established metadata models and vocabularies.

To this point we have outlined the challenges of curating fairly static digital files, but most DH projects are far more than the sum of their digital objects. Many DH research projects are complex software stacks with many layers of tools, objects, code and dependencies. If a project is built on Drupal, for example, librarians will have to not only maintain all of the unique objects and code produced by the project, but they are also committed to maintaining a specific version of a rapidly evolving software platform -- a version that will likely be obsolete before the project concludes. Drupal is, at the very least, well documented and widely deployed. Many DH projects also include custom-built tools, the inner workings of which are known only to a handful of people on the research team. The complex technology profiles of contemporary DH projects require ongoing active management including patching, tending, and rebuilding over time (Burpee, 2015). While the library may have sufficient resources to steward one or two unique project environments, this approach cannot scale to hundreds or even thousands of projects over time. In the current technical and funding environment is simply not possible for libraries to provide high-level curation for the enormous variety of funded digital projects that are produced by researchers within their organizations.

Libraries alone will not solve the problem of sustainability in DH projects. A fundamental characteristic of sustainability is that it must be established as a key design principle from the outset. It is almost impossible to retroactively render a project sustainable without rebuilding from the ground up. Initial choices about technologies, data models, formats, and documentation will influence the likelihood that a project will still be accessible in a decade. One complicating factor is that sustainability is largely at odds with a researcher’s freedom of choice when it comes to decisions about platforms, tools, and data models. Truly sustainable DH projects will require a level of standardization that is far from the current norm in DH project development, and which is unlikely to be unequivocally embraced by humanists. Research is an experimental process, and technological constraints can stifle creativity and independence of action. Models, by their very nature, seek to simplify, while the humanist tradition revels in nuance and complexity (McCarty, 2005; Quamen, 2013).

Leslie Johnston from the Library of Congress suggests that libraries can pursue two models for preserving complex DH projects. The first approach is to “preserve the content but forgo the look and feel. This is often extremely unpopular.” The second is to “preserve the content and the look and feel exactly as they were implemented. This is often close to impossible.” (Johnson, 2013) The tension between these two models is where libraries, researchers, and funders need to more clearly outline our assumptions and expectations.

The University of Victoria Libraries have developed a suite of preservation services for grant funded projects in order to plainly articulate our competencies, assets, and constraints (Goddard and Walde, 2017). This document acts as kind of a “menu” of services from which researchers can select as they develop their grant applications. These include the use of our Hydra/Fedora4 digital asset management system, metadata expertise that extends to consultations around interoperability and linked data, web hosting and discovery, exhibit building software, copyright consultation, open access publishing, research data management, and digitization services. We provide template paragraphs related to sustainability, preservation, open access, and knowledge mobilization that researchers can easily repurpose for any given funding proposal. We include a break down of the in-kind value of each of these services, along with any costs that will be charged back, so that researchers can easily estimate the value of the institutional commitment. We hope that this approach will enable critical conversations about sustainability to happen during the grant writing process, rather than towards the end of funding cycles as has been too often the case in the past. In order to offer our “gold standard” preservation services libraries will have to be involved in early conversations about technology preferences and data models. We certainly don’t assume that all decisions will be dictated by curation needs, but rather that our consultation will enable researchers to make cleareyed decisions about the impact of their choices on sustainability.

The preservation “menu” is an appealing model for researchers, as it enables them to quickly understand the variety of services and in-kind contributions that the library can offer in order to strengthen a funding application. There are also advantages to the library. By tying preservation services to grant funded projects we can avail of a rigorous review process that helps us to direct library resources towards research that has been deemed valuable by a network of disciplinary experts. It provides an easy formula for calculating in-kind contributions for letters of support, and to some extent standardizes the process of writing those letters. It helps to promote librarians as desirable co-applicants and collaborators on funding applications. It underscores to administrators the library’s value as a university research support. This model also provides mechanisms whereby grant funds can flow back into the development of new features for the library’s digital asset management and publishing platforms.

Susan Brown notes that “successful technologies rely on social resources.” (Brown, 2016) Part of our challenge is to muster support from researchers, librarians, administrators, and funders to create optimal conditions for long-term digital curation. The conversation about long-term preservation will be an ongoing negotiation that bridges different disciplinary perspectives, and balances ideals with resource constraints. Just as the traditional model of scholarly print publishing has shaped the means of scholarly production through the last two centuries, these conversations will ultimately will help to shape the future of humanities research platforms, resources, and methodologies.

Bibliography
Brown, S. (2016). “Tensions and tenets of socialized scholarship.” Digital Scholarship in the Humanities, 31(2): 283-300. http://doi.org/10.1093/llc/fqu063

Burpee, K. J. (2015). “Outside the Four Corners: Exploring Non-Traditional Scholarly Communication.” Scholarly and Research Communication, 6(2). http://src-online.ca/index.php/src/article/view/224/417.

Canadian Association of Research Libraries (CARL)

(2016). Compute Canada and the Canadian Association

of Research Libraries Join Forces to Build a National

Research Data Platform. Ottawa, Ontario: CARL.

https://www.computecanada.ca/research/compute-

canada-and-the-canadian-association-of-research-

libraries-join-forces-to-build-a-national-research-data-

platform/

Canadiana (2016). Preservation Policy and Strategy. Ottawa,    Ontario.

http://www.canadiana.ca/preservation-policy-strategy

Center for Research Libraries (CRL) (2015). Certification and Assessment of Digital Repositories. Chicago, IL. https://www.crl.edu/archiving-preservation/digital-archives/certification-assessment

Digital Preservation Network (DPN) (2016). Node Requirements.    http: //dpn.org/dpn-

admin/resources/dpnnodereqmarch2016.pdf

Federal Agencies Digitization Guidelines Initiative Working Group (FADGI) (2016). FADGI Technical Guidelines for Digitizing Cultural Heritage Materials. Washington,    DC.

http://www.digitizationguidelines.gov/guidelines/FAD GI_Still_Image_Tech_Guidelines_2016.pdf

Goddard, L. (2016). “The Read-Write Library.” Scholarly and    Research    Communication.    7(2).

http://dx.doi.org/10.22230/src.2016v7n2/3a255

Goddard, L. and Walde, C. (2017). Hosting and Preservation Services for Grant-Funded Research Projects. Victoria, BC: University of Victoria. http://www.uvic.ca/library/about/ul/UVicLibraries_Gr antServices_Feb2017.pdf

Johnston, L. (2013). “Digital Humanities and Digital Preservation.”    The    Signal.

https://blogs.loc.gov/thesignal/2013/04/digital-humanities-and-digital-preservation/

Kretzschmar, W. and Potter, W. (2010). “Library collaboration with large digital humanities projects.” Literary and Linguistics Computing, 25(4): 439-45. http://dx.doi.org/10.1093/llc/fqq022

Marcum, D. (2016). “Due diligence and stewardship in a time of change and uncertainty.” Ithaka S+R Issue Brief. https://doi.org/10.18665/sr.278232

McCarty, W. (2005). “Chapter 1: Modelling.” Humanities Computing. London, UK: Palgrave, pp. 20-72.

Muñoz, T. and Flanders, J. (2014). “An Introduction to Humanities Data Curation.” Digital Humanities Data Curation    Guide.

http://guide.dhcuration.org/contents/intro/

National Endowment for the Humanities (NEH) (2017). Data Management Plans for NEH Office of Digital Humanities    Proposals    and    Awards.

https://www.neh.gov/files/grants/data_management_ plans_2017.pdf

Quamen, H. (2013). “The Limits of Modelling: Data Culture and the Humanities.” Scholarly and Research Communication,    3(4).    http://src-

online.ca/index.php/src/article/view/69.

Social Sciences and Humanities Research Council of Canada (SSHRC) (2016). Tri-Agency Statement of Principles on Digital Data Management. Ottawa, Ontario. http://www.science.gc.ca/default.asp?lang=En&n=83F 7624E-1

Vandegrift, M. and Varner, S. (2013). “Evolving in Common: Creating Mutually Supportive Relationships Between Libraries and the Digital Humanities.” Journal of Library Administration, 53(1): 67-78.
The study of nineteenth-century Africa is troubled by issues of access on two fronts. First, explorers’ unedited field notes - the closest thing we have to a “‘raw’ record” - are rarely available, and, if they are, they are often crumbling, illegible, or located in far-flung archives (Bridges 1998). Second, even when such sources are available in later published forms, they present ethical and ideological problems. Written largely by European explorers and heavily edited, the published texts often exclude the voices of the very populations to which they attempt to provide access. Livingstone Online, a digital project dedicated to the written and visual legacy of nineteenth-century explorer David Livingstone (1813-73), works to counter these issues through its site design, transcription processes, and use of spectral imaging technology.

In order to do so, however, we have had to reconsider our understanding of access, both technologically and ideologically. As a publicly-funded project, we adhere to a high standard of transparency. Yet, as an archive of a contentious figure of imperial exploration, we are also responsive to the recent critiques of access - both of open access as privileging imperial knowledge expansion (Christen 2012; Risam 2017) and of the digital humanities as excluding consideration of race (Gallon 2016). To navigate this conflict, we strive to provide access that is not simply based on openness.

Instead, our project offers an understanding of access that moves in two directions temporally: striving to repair the past by being ethical in our digital treatment and remediation of historical materials, while also acting in a future-oriented fashion in developing and implementing our transparency policies, data standards, and code of collaboration in order to engage a variety of audiences, including those often excluded from DH practice. In this way, our project attempts to create a digital platform for culturally sensitive materials, while our documentation procedures seek to reveal every step of our decision-making process to critical review. Reparative

Livingstone Online, now in its twelfth year (2004 present), is a digital museum and library that draws on recent scholarship and international collaboration to restore one of the British Empire's most iconic figures to his global contexts. Our digital collection of highresolution manuscript images and critically-edited transcriptions - 11,000 images and 700 transcriptions by 2017 - is among the largest on the internet related to any single historical British visitor to Africa. Our site publishes important research on Livingstone's legacy and explores the many ways his ideas have circulated over time. Uniquely, we also takes its visitors far behind the scenes of our work - documenting step-bystep the international collaboration among archives, scholars, scientists, librarians, computer programmers, and other specialists that has made our project possible.

Our use of spectral imaging to uncover the material history of Livingstone’s manuscripts gives us important insights into the conditions under which Livingstone and other imperial explorers wrote - from unacknowledged contributors to the many environments through which the manuscripts circulated. In foregrounding these dimensions, we are also creating a new approach to using spectral imaging in cultural heritage projects because spectral imaging has primarily been used to unearth layers of text, rather than to examine the broader circumstances of imperial record-making and the preservation of imperial records over time. This new use of spectral imaging also constitutes an ethics of access, which is framed by critical essays that explore Livingstone’s uncredited information sources. Livingstone Online here puts forward an idea of access as uncovering the hidden hands and voices of the past.

For instance, in our study of Livingstone’s 1871 Field Diary, we have collaborated with spectral imaging scientists to develop pseudocolor (false color) images to differentiate passages that Livingstone originally wrote from those he added latter and those added by other hands. Likewise, the development of animated spectral images has enabled a chronological reconstruction of events in the life of the diary. By contrast, recourse to images made by principal component analysis (PCA) has uncovered stains on pages otherwise invisible to the naked eye and has introduced us to dimensions of manuscript history otherwise not even suspected to exist.

In addition, we frame these spectral images with paratextual tools that value equally the different kinds of information that Livingstone records. For example, few or no other record remains of many of the villages or the African and Arab individuals Livingstone mentions. As a result, our integrated glossary offers unique, otherwise unavailable geographical information that circulated during Livingstone's time in Central Africa and enumerates the names of people that might otherwise be lost to history. The glossary and other critical materials also provide insight into the complex social dynamics that operated in areas where Livingstone traveled. Overall, Livingstone Online offers a version of access in which the freely available manuscript pages are only a starting point; spectral imaging technology combined with critical building helps construct a reparative ethos of access.


Figure 1. A processed spectral image of a two-page spread from the 10 March 1870 'Retrospect' (Livingstone 1870a:[3]). The kaleidoscopic colors foreground and differentiate the wide range of substances that have left traces on the manuscript's pages over its 145-year history. Copyright National Library of Scotland. Creative Commons Attribution-NonCommercial 3.0 Unported

Future-Oriented
Alongside such reparative work, we also strive to design a project that looks toward the future. Livingstone Online makes our project documents available to an almost unprecedented degree in order to make our publicly-funded research fully accountable, to illuminate our work practices, and to support future digital projects. Our extensive downloadable primary materials (including 12,000 images, 3000 metadata records, and hundreds of transcriptions) are supplemented by freely available project materials, images, and working documents -the things often hidden behind the public face of digital projects. We have curated access to over 600 project documents, including planning documents, spectral image processing information, and essay notes, in order to illuminate the long-term history of our project work. Likewise, access to our grant narratives and working documents de-mystifies funding processes and international, interdisciplinary collaboration in order to support the work of other scholars, especially junior or independent scholars and those new to DH.

In addition, our site is technically accessible in a range of ways. We have built the site with sustainable, community-supported, open-source technologies such as Drupal for our front end and Fedora for our back end, which means that others can access our underlying code (which is fully available from Github) to reuse and modify it for their own projects. To promote use of our site more broadly, we've also worked to make the site inviting to scholars and general users alike, using intuitive, visually-driven site design. Through our site design, open-source code, and transparent documentation, we hope to foster user-led interpretation over passive reception of authorized knowledge.

LIVINGSTONE ONLINE

illuminating imperial exploration


Figures 2,3. Livingstone Online's six section pages, two of which are pictured here, each rely on a diverse range of historical illustrations and contemporary images to complicate the notion of a definitive Livingstone.

As part of this effort, our site is also fully mobile accessible, including for complex functions such as the review and study of archival manuscripts and transcriptions. This opens up Livingstone’s documents and our critical materials to the parts of the world where he worked and travelled; for many people on the African continent, for instance, mobile technology is the main access point to the internet. Likewise, just under a quarter of our collaborating archives are in Africa, and we are actively working on developing additional relationships with African-based archives, in the interests of not only bringing new Livingstone materials into our site, but also to encourage collaboration with African-based scholars and general audiences.

In these ways, we hope to initiate a conversation about the biases and assumptions inherent in the ways that technological advances shape our preservation of the past. We also hope to develop a nuanced practice of access that is embedded in our site design, spectral imaging processing, and transparent documentation, as well as made explicit in our critical materials. Thinking of access beyond openness means creating more historically-minded digital collections that also look to future knowledge creation by an array of populations, not all of them academic or based in the west.

Bibliography
Bridges, R. (1998). “Explorers’ Texts and the Problem of Reactions by Non-Literate Peoples: Some Nineteenth-Century East African Examples.” Studies in Travel Writing 2: 65-84.

Christen, K. (2012). “Does Information Really Want to be Free? Indigenous Knowledge Systems and the Question of Openness.” International Journal of Communication. Web.

Gallon, K. (2016). “Making a Case for the Black Digital Humanities.” In Debates in the Digital Humanities, eds. Lauren F. Klein and Matthew K. Gold. Univeristy of Minnesota    Press.

http://dhdebates.gc.cuny.edu/debates/text/55

Risam, R. (2017, forthcoming). “Decolonising Digital Humanities in Theory and Practice.” Routledge Companion to Digital Humanities and Media Studies. Ed. Jentary Sayers. London: Routledge.
Introduction

Spam, or unsolicited commercial communication, has evolved from telemarketing schemes to a highly sophisticated and profitable black-market business. Although many users are aware that email spam is prominent, they are less aware of blog spam (Thomason, 2007). Blog spam, also known as forum spam, is spam that is posted to a public or outward facing website. Blog spam can be to accomplish many tasks that email spam is used for, such as posting links to a malicious executable.

Blog spam can also serve some unique purposes. First, blog spam can influence purchasing decisions by featuring illegitimate advertisements or reviews. Second, blog spam can include content with target keywords designed to change the way a search engine identifies pages (Geerthik, 2013). Lastly, blog spam can contain link spam, which spams a URL on a victim page to increase the inserted URLs search engine ranking. Overall, blog spam weakens search engines' model of the Internet popularity distribution. Much academic and industrial effort has been spent to detect, filter, and deter spam (Dinh, 2013), (Spirin and Han, 2012).

Less effort has been placed in understanding the underlying distribution mechanisms of spambots and botnets. One foundational study in characterizing blog spam (Niu et al., 2007) provided a quantitative analysis of blog spam in 2007. This study showed that blogs in 2007 included incredible amounts of spam but does not try to identify linked behavior that would imply botnet behavior. A later study on blog spam (Stringhini, 2015) explores using IPs and usernames to detect botnets but does not characterize the behavior of these botnets. In 2011, a research team (Stone-Gross et al., 2011) infiltrated a botnet, which allowed for observations of the logistics around botnet spam campaigns. Overall, our understanding of blog spam generated by botnets is still limited.

Related Work

Various projects have attempted to identify the mechanics, characteristics, and behavior of botnets that control spam. In one important study (Shin et al., 2011), researchers fully evaluated how one of the most popular spam automation programs, XRumer, operates. Another study explored the behavior of botnets across multiple spam campaigns (Thonnard and Dacier, 2011). Others (Pitsillidis et al., 2012) examined the impact that spam datasets had on characterization results. (Lumezanu et al., 2012) explored the similarities between email spam and blog spam on Twitter. They show that over 50% of spam links from emails also appeared on Twitter.


Figure 1: Browser rendering of the ggjx honeypot

The underground ecosystem build around the botnet community has been explored (Stone-Gross et al., 2011). In a surprising result, over 95% of pharmaceuticals advertised in spam were handled by a small group of banks (Levchenko et al., 2011). Our work is similar in that we are trying to characterize the botnet ecosystem, focusing on the distribution and classification of certain spam producing botnets.

Experimental Design

In order to classify linguistic similarity and differences in botnets, we implement 3 honeypots to gather samples of blog spam. We configure our honeypots identically using the Drupal content management systems (CMS) as shown in Figure 1. Our honeypots are identical except for the content of their first post and their domain name. Ggjx.org is fashion themed, npcagent.com is sports themed, and gjams.com is pharmaceutical themed. We combine the data collected from Drupal with the Apache server logs (Apache, 2016) to allow for content analysis of data collected over 42 days. To allow botnets time to discover the honeypots, we activate the honeypots at least 6-weeks before data collection.

We generate three tables of content for each honeypot (Bevans and Khosmood, 2016). In the user table, we record the information the spambot enters while registering and user login statistics that we summarize in Table 1. This includes the user id, username, password, date of registration, registration IP, and number of logins. In the content table, we record the content of spam posts and comments which we summarize in Table 2. This includes the blog node id, the author's unique id, the date posted, the number of hits, type of post, title of the post, text of the post, links in the post, language of the post, and a taxonomy of the post from IBM's Alchemy API.

Honey pot

Quantity

Mean Logins/User

# of Countries

ggjx

62992

1.066

83

gjams

28230

1.102

40

npcagent

34332

1.05

53

Table 1: User table characteristics for three honeypots

Honeypot

Quantity

Avg. Hits

Avg. Links

English Posts

ggjx

2279

28.237

2.356

1962

gjams

2225

18.178

0.311

2137

npcagent

1430

29.043

1.823

1409

Table 2: Characteristics for the content tables

Honeypot

ggjx

gjams

npcagent

# Of Entities

3430

1790

1566

# of Users

62992

28230

34332

Mean Users/Entity

18.365

15.771

21.923

Max Users/Entity

37589

14249

23577

cr of Users/Entity

666.128

359.619

611.157

# of IPs

5291

3092

2120

Mean IPs/Entity

1.543

1.727

1.354

Maximum IPs/entity

118

135

60

<r of IP Quantity

4.277

5.551

2.406

Mean Posts/Entity

.664

1.243

.907

Max Posts/Entity

163

484

664

a of Posts/Entity

5.319

14.448

17.256

% of Entities Who Posted

15.2

12.4

13.5

Table 3: Characteristics of entities

Lastly, in the access table, we include data and meta-data from the Apache logs. This includes the user id, the access IP, the URL, the HTTP request type, the node ID, and an action keyword describing the type of access.

Our honeypots received a total of 1.1 million requests for ggjx, 481 thousand requests for gjams, and 591 thousand requests for npcagent.

Entity Reduction

It is widely accepted that spambot networks, or botnets, are responsible for most spam. Therefore, we algorithmically reduce spam instances into unique entities representing botnets. For each entity, we define 4 attributes: entity id, associated IPs, usernames, and associated user ids. To construct entities we scan through the users and assign each one to an entity as follows.

1. For a user, if an entity exists which contains its username or IP, the user is added to the entity.

2. If more than one entity matches the above criteria, all matching entities are merged.

3. If no entity matches the above criteria, a new entity is created.

We summarize the entity characteristics in Table 3. The maximum number of users in one entity is almost 38 thousand for ggjx with over 100 unique IP addresses. These results confirm what is expected - the vast majority of bots interacting with our honeypots are part of large botnets. This also allows us to perform content analysis exploring what linguistic qualities differentiate botnets.

Feature

Description

Indicates

Effective

Bag or Words

Set of words with count

Lexical content

Yes

Alchemy

Document taxonomy

Taxonomy

Yes

Link

URL core domain names

URL similarity

Variable

Vocab

Vocab complexity

Vocabulary complexity

No

Part-of-speech

A BoW of parts-of-speech

Simple syntax

No

Table 4: NLP feature sets we consider for our content analysis and their effectiveness at differentiating botnets

Content Analysis

To better understand botnets, we use natural language processing (Collobert and Weston, 2008) for analyzing the linguistic content of entities. For our analysis, we consider various feature sets as proxies for linguistic characteristics as summarized in Table 4. We use a Maximum Entropy classifier (Mega M, 2016) to test which features differentiate botnets. In order to test a feature, we train the classifier with 70% of the posts, randomly selected, from the N largest entities and test it with the remaining 30% of the posts. Our final results are the average of three runs.

The first feature set we test is Bag Of Words (BoW) which models the lexical content of posts. Put simply, each word in a document is put into a ‘bag' and the syntactic structure is discarded. For implementation details, see our technical report (Bevans, 2016). In Figure 2, we show our analysis of the BoW feature set.

When considering the top 5 contributing entities, the classification accuracy is less than 95% which implies that the lexical content of botnets varies greatly. The second feature we consider is the taxonomy provided by IBM Watson's AlchemyAPI. Alchemy's output is a list of taxonomy labels and associated confidences. For the purpose of our analysis, we discard any low or non-confident labels. In Figure 3, we show our analysis of the Alchemy Taxonomy feature set which highlights the accuracy of Alchemy's taxonomy. We note that the Alchemy Taxonomy feature set is dramatically smaller in size than the BoW feature set while still providing high performance. This indicates a full lexical analysis is not necessary but a taxonomic approach is sufficient. Our third feature is based on the links in the posts. To create the feature, we parse each post for any HTTP links and strip the link to its core domain name.

The classifier with the link feature set had varied results, as shown in Table 5, where it was reliable in differentiating ggjx entities but less reliable for the other two honeypots. These results correlate with link scarcity from Table 2.


We test the normalized vocabulary size of a post as a feature. We derive this from the number of unique words divided by the total number of words in the post. As shown in Table 5, the vocabulary size does not differentiate botnets.

We also form a feature set based on the part-of-speech (PoS) makeup of a post using the Stanford PoS Tagger. The Stanford PoS tagger returns a pair for each word in the text, the original word and corresponding PoS. We create a BoW from this response that creates an abstract representation of the document's syntax. As shown in Table 5, the PoS does not differentiate botnets.

Feature Set

Database

Accuracy (10 entities)

Accuracy (60 entities)

BoW

BoW

BoW

ggjx

gjams

npcagent

93%

92%

93%

71%

78%

83%

Alchemy

Alchemy

Alchemy

ggjx

gjams

npcagent

87%

91%

91%

80%

84%

82%

Link

Link

Link

ggjx

gjams

npcagent

89%

53%

72%

84%

37%

61%

PoS

PoS

PoS

ggjx

gjams

npcagent

32%

53%

70%

16%

39%

60%

Vocab

Vocab

Vocab

ggjx

gjams

npcagent

32%

50%

74%

17%

36%

60%

Table 5: Accuracies for various features when identifying 10 and 60 entities using the maximum entropy classifier

Conclusions

In this paper, we examine interesting characteristics of spam-generating botnets and release a novel corpus to the community. We find that hundreds of thousands of fake users are created by a small set of botnets and much fewer numbers of them actually post spam. The spam that is posted is highly correlated by subject language to the point where botnets labeled

by their network behavior are to a large degree re-discoverable using content classification (Figure 3).

While link and vocabulary analysis can be good differentiators of these botnets, it is the content labeling (provided by Alchemy) that is the best indicator. Our experiment only spans 42 days, thus it's possible the subject specialization is a feature of the campaign rather than the botnet itself.

Bibliography

Apache virtual    host.    (2016).

http://httpd.apache.org/docs/current/vhosts Accessed: 2016-08-10.

Bevans, B., and Khosmood, F. (2016). Forum Spam Corpus. http://users.csc.calpoly.edu/~foaad/bfbevans Accessed: 2017-04-01.

Bevans, B. (2016). “Categorizing Forum Spam.” Master's Theses at Cal Poly Digital Commons. http: //digitalcom-mons.calpoly.edu/theses/1623 Accessed: 2017-04-01.

Collobert, R., and Weston, J. (2008). “A unified architecture for natural language processing: Deep neural networks with multitask learning.” Proceedings of the 25th International Conference on Machine Learning, ACM: 160-67.

Dinh, S. et al. (2015). “Spam campaign detection, analysis, and investigation.” Digital Investigation, (12) S12-S21.

Geerthik, S. (2013). “Survey on internet spam: Classification and analysis.” International Journal of Computer Technology and Applications, 4(3): 384.

Levchenko, K. et al. (2011). “Click trajectories: End-to-end analysis of the spam value chain.” Symposium on Security and Privacy, IEEE. 431-446.

Lumezanu, C. and Feamster, N. (2012). “Observing common spam in twitter and email.” Proceedings of the 2012 ACM conference on Internet measurement, ACM. 461466.

Mega M. (2016). “Mega model optimization package.” https://www.umiacs.umd.edu/~hal/megam/, Accessed: 2016-08-10.

Shin, Y., Gupta, M., and Myers, S. A. (2011). “The nuts and bolts of a forum spam automator.” LEET.

Spirin, N., and Han, J. (2012). “Survey on web spam detection: Principles and algorithms.”

ACM SIGKDD Explorations Newsletter, 13(2): 50-64.

Stone-Gross, B., et al. “The underground economy of spam: A botmaster's perspective of coordinating large-scale spam campaigns.” LEET, 11: 4.

Stringhini, G. (2015). “Evilcohort: Detecting communities of malicious accounts on online services.” 24th USENIX Security Symposium (USENIX Security 15), 563-578.

Thomason, A. (2007). “Blog spam: A review.” CEAS, Citeseer.

Thonnard O. and Dacier, M. (2011). “A strategic analysis of spam botnets operations.” Proceedings of the 8th Annual

Collaboration, Electronic messaging, Anti-Abuse and Spam Conference, ACM, 162-171.

Niu, Y. et al. (2007). “A quantitative study of forum spamming using context-based analysis.” NDSS.

Pitsillidis, A. et al. (2012). “Taster's choice: A comparative analysis of spam feeds.”

Proceedings of the 2012 ACM conference on Internet measurement, ACM. 427-440.
Reading, writing, and discussion are the most common—and, most would agree, the most valuable—components of a university-level humanities seminar. In humanities courses, all three activities can be guided and supported with a variety of digital and analog tools. Digital texts can create novel opportunities for teaching and learning, particularly when students’ reading activity is made visible to other members of the course. In this short paper, we will introduce Lacuna, a web-based platform which hosts digital course materials to be read and annotated socially and collaboratively. We report these findings in relation to an observational case study of four different humanities courses at California community colleges using Lacuna.

Lacuna is the result of a collaborative and iterative effort at Stanford to design a platform that supports the practices of critical reading and dialogue in humanities courses. On Lacuna, which uses Drupal to manage content, course syllabus materials are digitized and uploaded to the platform. These materials can be organized by topic, class date, and other metadata such as medium (text, video, or audio). When students and instructors open up materials, they can digitally annotate selections from any text using a pop-up javascript annotator.

Annotation on Lacuna is a social as well as an individual practice, leveraging the participatory possibilities of web-based technologies. Lacuna users can choose to share annotations with one another and hover over highlighted passages to reveal others’ comments or questions. Social annotation makes explicit and visible for students the broad array of annotation practices within an interpretive community such as a classroom and helps students cocreate interpretations of texts. Students’ annotation activity on Lacuna is also made visible through a separate instructor dashboard, which helps instructors track engagement throughout the course. Finally, annotations can be connected across texts using the “Sewing Kit” in order to support intertextual analyses.

After introducing the features of the platform, we present a case study of the use of Lacuna in four California community-college classes. As part of a U.S. Department of Education Title VI grant, we worked with faculty from Foothill College, De Anza College, and College of San Mateo to incorporate Lacuna into their lesson plans and classroom pedagogy. Drawing on ethnographic methods, we describe how the faculty members used the platform’s affordances to integrate students’ online activity into course planning and seminar discussions and activities. We also explore students’ experience of social annotation and social reading, insights gathered from surveys, interviews, and classroom observation.

In our case study, we find that student annotations and writing on Lacuna give instructors more insight into students’ perspectives on texts and course materials. The visibility of shared annotations encourages students to take on a more active role as peer instructors and peer learners. Our short talk will close with a discussion of the new responsibilities, workflows, and demands on self-reflection introduced by these altered relationships between course participants. People at our talk will learn about the benefits and challenges encountered in using Lacuna, which are likely to be shared by individuals using other learning technologies with similar goals and features. We will also consider future directions for the enhancement of teaching and learning through the use of social reading and digital annotation.
Over the last five years, research computing has undergone a shift from focusing on running infrastructure for highly technical researchers, primarily in the sciences, to supporting medium- to large-scale computational needs across a wide range of disciplines, where practitioners fall across the spectrum of technical proficiency.

This shift in approach has opened up new opportunities, both for scholars whose research questions are facilitated by computationally intensive algorithms (e.g. photogrammetry, natural language processing, OCR at scale), and for humanists in support positions that require translation between technical and non-technical audiences. This panel will discuss opportunities for research and career development through the perspectives of research IT staff whose backgrounds in both humanistic inquiry and computational and digital methodologies enable them to engage in outreach to, training of, and support for humanities researchers. It will also provide practical suggestions for humanists who could benefit from research computing resources but find it difficult to navigate the expectations of research IT organizations.

The communication, writing, and teaching skills cultivated through an advanced degree in the humanities align with employment trends in research IT groups. While some familiarity and comfort with computation is generally a requirement for working in research IT, advanced programming or system administration skills are crucial for a minority of positions in these groups. As the mandate of research computing groups has expanded, it has become increasingly clear that successful research computing programs require documentation that is comprehensible to a non-technical audience, hands-on workshops for non-specialists, and staff capable of understanding the fundamentals of researchers’ work and identifying effective approaches to meeting their computation needs. To that end, in 2014 the US National Science Foundation funded the Advanced Cyberinfrastructure - Research and Education Facilitators (ACI-REF) program, which developed a cohort of computation “facilitators” across a number of universities who could "maximize the impact of investment in research computing" by “assisting] researchers in leveraging ACI resources for themselves” and sharing solutions among participating institutions (NSF 2014). This approach to providing support has been highly influential among campus research IT organizations, and related workshops such as the ACI-REF virtual residency (featuring plenaries such as “Effective Communication: How to talk to researchers about their research” and “Writing Grant Proposals”) have drawn large crowds (Neeman et al. 2016).

Similarly, the Extreme Science and Engineering Discovery Environment (XSEDE), funded by the National Science Foundation, has begun specifically reaching out to disciplines that have been typically under-represented in the high performance computing arena, including those in the humanities. XSEDE has a program called Extended Collaborative Support Services (ECSS) that partners humanities scholars and others from under-represented areas with computing professionals who can help them achieve their desired research objectives (Wilkins-Diehr et al. 2016). This has been both necessary and fruitful particularly in areas that are focused on data analytics (such as video analysis, image analysis, network analysis, etc.), as opposed to the more traditional simulation and modeling done by scientists and engineers. XSEDE’s Novel and Innovative Projects group includes a specialist in digital humanities, as well as specialists in pertinent related areas such as big data analytics.

Along the same lines, in January 2014, Compute Canada hired a full-time Digital Humanities coordinator to lead a national team supporting researchers wanting to engage with national advanced research computing (ARC) resources. Compute Canada, a national non-profit organization incorporated in 2012, plans and oversees panCanadian ARC resources used for big data analysis, visualization, data storage, software, portals and platforms for research computing serving Canadian academic and research institutes. The national support team for Compute Canada now consists of the full-time DH coordinator, and a large geographically dispersed team that meets bi-weekly to coordinate national initiatives like training at the Digital Humanities Summer Institute and national competitions such as the recent partnership between Compute Canada and the Canadian Social Sciences and Humanities Research Council. The DH team also meets locally with humanities researchers at their own institutions to help them leverage national infrastructure, and shares experiences and advice back to the national team to help in developing tools, services, and training opportunities that will benefit the national DH community (Simpson 2015).

One of the interesting ramifications of these changes in IT staffing trends is the addition of humanists to IT-based groups in high performance computing, cyberinfrastructure, visualization, and data architectures--humanists who are called upon to maintain both a deep understanding of computational systems, as well as track the needs of scholars in the humanities, which tend to be quite different from researchers in the “hard” or social sciences. Often, they are the lone humanist in a group dominated by engineers and computer scientists. There is a clear need and desire to expand the use of computational resources into "non-traditional" (i.e., non-hard-sciences) disciplines, both to justify an institutionwide investment in research computing, and as a way of building institutional capacity for supporting digital humanities scholarship (as described in the forthcoming 2017 ECAR/CNI white paper on institutional digital humanities support). Nonetheless, institutions are struggling with translating their services into comprehensible and relevant offerings for humanities researchers. Finding effective means of supporting these researchers within the traditional model of the research IT group has also been a challenge, given the ways that it differs from library-based support models with which scholars are more familiar. Panelists will reflect on projects they have worked on that successfully bridged the humanities-research computing divide, to the benefit of both groups.

As one example, at Indiana University, a workflow for teaching text analysis with R has been developed that uses web-based Shiny scripts to introduce an algorithm, highly annotated RNotebooks explaining each line of code, lightly annotated RScripts allowing for remixing and adaptation, and, finally, RScripts to leverage multicore environments. The scripts use data both from literature and Twitter, and these tutorials consistently draw the highest attendance in the series DH for Humanists, held throughout the semester. Individual research projects using more sophisticated algorithms such as NER and LDA have also grown out of this project.

As another example, at the University of Chicago, the recently-developed Visual Text Explorer provides a new type of framework for reading texts along with a range of user-customizable analytics, allowing for simultaneous close and distant reading. Other humanities research computing projects include web-based data-driven animated interactive mapping systems, tools for comparative sequence analysis across literary corpora, and automated aggregators of ranges of specific secondary data sources to inform the reading of specific texts/types of texts, all of which require no knowledge of computer programming by the user but nonetheless leverage research computing resources.

At UC Berkeley, 3D modeling work by Near Eastern Studies scholars that uses photogrammetry software running on the high-performance compute cluster has been helpful for testing new Graphics Processing Unit (GPU) nodes in the cluster.

Finally, panelists will provide recommendations for how humanities scholars can translate their research projects in ways that will make them more comprehensible and compelling for institutional research IT groups that may not have a humanist on their support staff.

Panel participants
Quinn Dombrowski is the Digital Humanities Coordinator in Research IT at UC Berkeley. Research IT supports research data management, museum informatics, and computationally intensive research across all domains. She is the author of Drupal for Humanists and has an MA in Slavic linguistics and an MLIS from the University of Illinois.

Tassie Gniady is the manager of the Cyberinfrastructure for Digital Humanities Group at Indiana University. The CyberDH group focuses on workflows for text analysis and photogrammetry. She also teaches an Introduction to Digital Humanities course in the Information and Library Science School. Tassie has a Ph.D in early modern literature from UC-Santa Barbara and an MIS from Indiana University.

Megan Meredith-Lobay is the Digital Humanities and Social Sciences Scientific Analyst for the University of British Columbia's Advanced Research Computing Department. She is also part of WestGrid and Compute Canada, the Canadian HPC national infrastructure platform. Megan has a PhD from the University of Cambridge Department of Archaeology in which she explored the early Christian archaeology of Argyll, Scotland using GIS and early online archaeological databases.

force". In XSEDE16 Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale. ACM: New York. DOI: 10.1145/2949550.2949584

Simpson, J. (2015). “Building Support for Digital Humanities”. Compute Canada blog. https://www.com-putecanada.ca/blog/building-support-for-digital-hu-

manities/

Wilkins-Diehr, N., et al. (2016)“An Overview of the XSEDE Extended Collaborative Support Program” in High Performance Computer Applications, ed. Isidoro Gitler and Jaime Klapp. Springer Link:    2016.

http://link.springer.com/chapter/10.1007%2F978-3-

319-32243-8 1

Lisa M. Snyde is the director of Campus Research Initiatives for UCLA's Office of Information Technology, and manager of the GIS, Visualization, and 3D Modeling group for the Institute for Digital Research and Education. She has a Ph.D. in Architecture and teaches Virtual Reality and 3D Modeling in UCLA's digital humanities program.

Jeffrey Tharsen is Computational Scientist for the Digital Humanities at the University of Chicago where he is the lead technical domain expert for digital and computational approaches to humanistic inquiry. Jeffrey has a Ph.D. from the University of Chicago's East Asian Languages & Civilizations department, specializing in the fields of premodern Chinese philology, phonology, poetics and paleography.

Bibliography
National Science Foundation. (2014) “Advanced Cyber-

infrastructre - Research and Educational Facilitation: Campus-Based Computational Research Support.” https://www.nsf.gov/awardsearch/show-

Award?AWD ID=1341935

Neeman, H., et al. (2016) "The Advanced Cyberinfrastructure Research and Education Facilitators Virtual Residency: Toward a National Cyberinfrastructure Work-

        
            Alan​ ​Liu​ ​has​ ​called​ ​upon​ ​digital​ ​humanists​ ​to​ ​think​ ​more​ ​critically​ ​about​ ​infrastructure​ ​-​ ​the​ ​“social cum​ ​technological​ ​milieu​ ​that​ ​at​ ​once​ ​enables​ ​the​ ​fulfillment​ ​of​ ​human​ ​experience​ ​and​ ​enforces constraints​ ​on​ ​that​ ​experience” (Liu, 2017).​ ​Liu’s​ ​invitation​ ​comes​ ​at​ ​the​ ​moment​ ​when​ ​researchers​ ​involved​ ​in large-scale,​ ​long-term​ ​projects​ ​are​ ​shifting​ ​focus​ ​from​ ​remediation​ ​and​ ​the​ ​creation​ ​of​ ​digital incunabula​ ​to​ ​transmediation​ ​and​ ​the​ ​development​ ​of​ ​systems​ ​that​ ​support​ ​sustained​ ​discourse across​ ​ever-morphing​ ​digital​ ​networks,​ ​when​ ​we​ ​are​ ​recognizing​ ​the​ ​potential​ ​for​ ​“dynamism​ ​of​ ​the base​ ​or​ ​serialized​ ​form​ ​of​ ​the​ ​text—the​ ​state​ ​in​ ​which​ ​it​ ​is​ ​stored—as​ ​opposed​ ​to​ ​dynamic​ ​modes​ ​of presentation” (Brown, 2016: 288)​. ​REED​ ​London​ ​is​ ​one​ ​such​ ​project​ ​with​ ​a​ ​polyvalent​ ​dataset​ ​that​ ​spans​ ​over​ ​500 years’​ ​worth​ ​of​ ​archival​ records, ​embracing​ ​from​ ​the​ ​start​ ​the​ ​need​ ​to​ ​establish​ ​a​ ​stable, ​responsive production​ ​and​ ​presentation​ ​environment​ ​primed​ ​for​ ​use​ ​by​ ​a​ ​wide​ ​range​ ​of​ ​scholarly​ ​audiences. Thus​ ​we​ ​find​ ​that​ ​we​ ​are​ ​immediately​ ​testing​ ​those​ ​infrastructural​ constraints. ​In​ ​this​ ​paper, members​ ​of​ ​the​ ​REED​ ​London​ ​project​ ​team​ ​will​ ​address​ ​the​ ​challenges​ ​we​ ​face​ ​as​ ​we​ ​develop​ ​and implement​ ​a​ ​framework​ ​that​ ​trains​ ​us​ ​to​ ​think​ ​about​ ​our​ ​collected​ ​data​ ​in​ ​relation​ ​to​ ​much​ ​larger networks​ ​of​ ​disparate​ ​resources​ ​and​ ​user​ ​needs.
            REED​ ​London​ ​develops​ ​from​ ​a​ ​partnership​ ​between​ ​the​ ​Records​ ​of​ ​Early​ ​English​ ​Drama​ ​(REED) and​ ​the​ ​Canadian​ ​Writing​ ​Research​ ​Collaboratory​ ​(CWRC). ​Together​ ​we​ ​are​ ​establishing​ ​an​ ​openly accessible​ ​online​ ​scholarly​ ​and​ ​pedagogical​ ​resource​ ​of​ ​London-centric​ documentary, editorial, and bibliographic​ ​materials​ ​related​ ​to​ ​performance,​ ​theatre,​ ​and​ ​music​ ​spanning​ ​the​ ​period​ ​1100-1642. With​ ​support​ ​from​ ​the​ ​Andrew​ ​W.​ ​Mellon​ ​Foundation​ ​and​ ​a​ ​CANARIE​ ​Research​ ​Software​ ​Program grant,​ ​a​ ​team​ ​of​ ​researchers​ ​in​ ​the​ ​digital​ ​humanities​ ​and​ ​performance​ ​history​ ​from​ ​the​ ​U.S., Canada,​ ​and​ ​the​ ​U.K.​ ​are​ ​building​ ​a​ ​stable,​ ​extensible​ ​editorial​ ​production​ ​and​ ​publication environment​ ​that​ ​will​ ​create​ ​new​ ​possibilities​ ​for​ ​scholarly​ ​presentation​ ​of​ ​archival​ ​materials​ ​gathered from​ ​legal,​ ​ecclesiastical,​ ​civic,​ ​political,​ ​and​ ​personal​ ​archival​ ​sources​ ​in​ ​and​ ​around​ ​London.​ ​The REED​ ​London​ ​project​ ​combines​ ​materials​ ​from​ ​three​ ​printed​ ​REED​ ​collections​ (
                Inns ​
                of​ ​
                Court, 
                Ecclesiastical​ ​
                London​,​ ​and​ 
                Civic​ ​
                London​ ​
                to​ 
                1558)​,​ ​the​ ​prosopographical​ ​material​ ​from​ ​REED’s 
                Patrons​ ​
                &amp;​ ​
                Performances​ ​
                (P&amp;P)​,​ ​the​ ​bibliographical​ ​materials​ ​of​ ​the​​ 
                Early​ ​
                Modern​ ​
                London​
                Theatres (EMLoT)​ ​​database,​ ​and​ ​in-progress​ ​and​ ​planned​ ​digital​ ​collections​ ​focusing​ ​on​ ​London​ ​area performance​ ​spaces,​ ​most​ ​notably​ ​the​ ​Globe,​ ​Rose,​ ​and​ ​Curtain​ ​theatres​ ​and​ ​Civic​ ​London 1559-1642.
            
            REED​ ​is​ ​an​ ​internationally​ ​renowned​ ​scholarly​ ​project​ ​that​ ​has​ ​worked​ ​to​ ​locate,​ ​transcribe,​ ​and​ ​edit evidence​ ​of​ ​drama,​ ​secular​ ​music,​ ​and​ ​other​ ​communal​ ​entertainment​ ​in​ ​Britain​ ​from​ ​the​ ​Middle Ages​ ​until​ ​1642.​ ​Since​ ​1979​ ​REED​ ​has​ ​published​ ​twenty-seven​ ​printed​ ​collections​ ​of​ ​transcribed records​ ​plus​ ​contextual​ ​materials.​ ​REED​ ​has​ ​long​ ​recognized​ ​the​ ​importance​ ​of​ ​online​ ​access​ ​to​ ​its resources,​​first​​ with 
                P&amp;P​​​ and​​​ 
                EMLoT​, ​​and​​ more​​ recently​​ with ​​the ​​born-digital​​ collection​ 
                Staffordshire​. REED​ ​has​ ​wrestled​ ​with​ ​the​ ​balance​ ​between​ ​what​ ​was​ ​once​ ​considered​ ​its​ ​“core”​ ​print​ ​publication activities​ ​and​ ​“adjunct”​ ​digital​ ​efforts,​ ​in​ ​the​ ​process​ ​migrating​ ​its​ ​data​ ​across​ ​a​ ​succession​ ​of programs​ ​and​ ​formats​ ​from​ ​Basic​ ​and​ ​dBASE​ ​to​ ​TEI​ ​P5​ ​XML​ ​and​ ​MySQL (Hagen,​ ​MacLean,​ ​and​ ​Pasin, 2014).​ ​REED​ ​has​ ​developed​ ​its digital​​ resources ​​in​​ ways ​​that​​ complicate ​​integration (
                P​
                &amp;P ​​​exists ​​in​​ a ​​Drupal​​instance;​​​ 
                EMLoT ​​​was built​ ​in​ ​a​ ​version​ ​of​ ​Django​ ​that​ ​is​ ​now​ ​out-of-date;​ ​
                REED​ ​Staffordshire​ ​was​ ​lightly​ ​tagged​ ​in​ ​TEI​ ​and relies​ ​on​ ​EATSML for entity management, an​ ​XML​ ​format​ ​used​ ​by​ ​the​ ​Entity​ ​Authority​ ​Tool​ ​Set​ ​(EATS)​ ​for​ ​serialisation​ ​of​ ​its​ ​data).​ ​The​ ​components​ ​of​ ​REED​ ​London​ ​must​ ​therefore​ ​first be​ ​made​ ​intra-operable​ ​before​ ​they​ ​can​ ​become​ ​interoperable (Jakacki, 2016).​ ​The​ ​partnership​ ​with​ ​CWRC supports​ ​broader​ ​adoption​ ​of​ ​standards​ ​for​ ​TEI​ ​text​ ​markup,​ ​RDF​ ​metadata​ ​specifications,​ ​and named​​ entity ​​aggregation,​​ most ​​immediately​​ with ​​the ​​ingestion​​ of ​
                EMLoT ​​​and ​​the​​ printed ​​​
                Inns ​​
                of Court​​ ​collection.
            
            CWRC​ ​is​ ​an​ ​online​ ​infrastructure​ ​project​ ​designed​ ​to​ ​enable​ ​unprecedented​ ​avenues​ ​for​ ​studying the​ ​words​ ​that​ ​most​ ​move​ ​people​ ​in​ ​and​ ​about​ ​Canada.​ ​Built​ ​with​ ​funding​ ​from​ ​the​ ​Canada Foundation​ ​for​ ​Innovation,​ ​the​ ​CWRC​ ​platform​ ​supports​ ​best​ ​practices​ ​in​ ​the​ ​production​ ​of​ ​online collections,​ ​editions,​ ​born-digital​ ​essays,​ ​anthologies,​ ​collections,​ ​monographs,​ ​articles,​ ​or bibliographies,​ ​and​ ​supports​ ​the​ ​inclusion​ ​of​ ​visual,​ ​audio,​ ​and​ ​video​ ​sources (​About​ ​CWRC/CSÉC).​ ​It​ ​supports collaboration​ ​through​ ​the​ ​use​ ​of​ ​interoperable​ ​data​ ​formats​ ​and​ ​interlinking​ ​of​ ​materials,​ ​and​ ​for teams​ ​like​ ​REED​ ​London​ ​provides​ ​invaluable​ ​tools​ ​for​ ​communicating,​ ​tracking​ ​activity,​ ​and workflow.​ ​We​ ​envision​ ​that​ ​as​ ​the​ ​partnership​ ​develops​ ​and​ ​as​ ​REED​ ​London​ ​advances​ ​through production​ ​toward​ ​publication​ ​we​ ​will​ ​take​ ​full​ ​advantage​ ​of​ ​CWRC’s​ ​functionality.​ ​From​ ​the​ ​start​ ​we have​ ​worked​ ​directly​ ​in​ ​CWRC’s​ ​unique​ ​editor,​ ​CWRC-Writer,​ ​which​ ​allows​ ​us​ ​to​ ​edit​ ​REED​ ​London records,​ ​essays,​ ​and​ ​bibliographical​ ​material​ ​using​ ​more​ ​diplomatic​ ​and​ ​critical​ ​TEI​ ​P5​ ​XML​ ​markup and​ ​at​ ​the​ ​same​ ​time​ ​creating​ ​semantic​ ​web​ ​annotations​ ​with​ ​RDF​ ​to​ ​identify,​ ​manage,​ ​and​ ​interlink entities​ ​contained​ ​within.​ ​The​ ​platform​ ​is​ ​also​ ​helping​ ​us​ ​to​ ​develop​ ​a​ ​better​ ​editorial​ ​workflow through​ ​management​ ​of​ ​access​ ​to​ ​data​ ​and​ ​editing​ ​by​ ​role,​ ​team​ ​communications,​ ​tracking​ ​and reporting​ ​of​ ​team​ ​activities.
            To​ ​ensure​ ​REED​ ​London’s​ ​stability​ ​and​ ​sustainability​ ​while​ ​extending​ ​its​ ​content​ ​and​ ​value​ ​to​ ​new generations​ ​of​ scholars​ ​the​ ​project​ ​is​ ​being​ ​built​ ​within​ ​the​ ​CWRC​ environment. ​The​ ​scope​ ​of​ ​REED London​ ​would​ ​not​ ​be​ ​possible​ ​without​ ​the​ ​sophisticated,​ ​integrated​ ​platform​ ​that​ ​CWRC​ ​provides. The​ ​focus​ ​of​ ​our​ ​first​ ​year​ ​is​ ​the​ ​design​ ​and​ ​construction​ ​of​ ​a​ ​collaborative​ ​online​ ​production​ ​and publication​ ​environment.​ ​Extending​ ​from​ ​CWRC’s​ ​existing​ ​integrated​ ​content​ ​management​ ​and preservation​ ​system,​ ​the​ ​enhanced​ ​environment​ ​will​ ​accommodate​ ​the​ ​range​ ​of​ ​record​ ​texts, editorial​ ​and​ ​bibliographical​ ​content​ ​from​ ​the​ ​source​ ​materials,​ ​while​ ​a​ ​customized​ ​browser-based CWRC-Writer​ ​platform​ ​will​ ​support​ ​the​ ​team’s​ ​goal​ ​of​ ​developing​ ​online​ ​editorial​ ​collaboration​ ​and review.​ ​The​ ​resulting​ ​streamlined​ ​production​ ​and​ ​publication​ ​environment​ ​will​ ​yield​ ​multi-faceted user-centered​ ​editions,​ ​meaning​ ​that​ ​agile​ ​component​ ​archival​ ​and​ ​editorial​ ​parts​ ​can​ ​cohere according​ ​to​ ​various​ ​criteria​ ​in​ ​response​ ​to​ ​scholars’​ ​research​ ​and​ ​teaching​ ​needs.​ ​In​ ​this​ ​way​ ​we are​ ​establishing​ ​a​ ​platform​ ​that​ ​produces​ ​new​ ​forms​ ​of​ ​“edition”​ ​that​ ​combine​ ​customized​ ​textual​ ​and contextual​ ​materials,​ ​exportable​ ​customized​ ​datasets​ ​and​ ​dynamic​ ​data​ ​visualizations.​ ​It​ ​also​ ​means that​ ​we​ ​will​ ​be​ ​able​ ​to​ ​realize​ ​the​ ​promise​ ​of​ ​extending​ ​the​ ​value​ ​of​ ​these​ ​materials​ ​to​ ​colleagues​ ​in fields​ ​beyond​ ​performance​ ​history,​ ​including​ ​political,​ ​religious,​ ​and​ ​cultural​ ​studies,​ ​and​ ​linguistics. 
            The​ ​partnership​ ​between​ ​CWRC​ ​and​ ​REED​ ​allows​ ​us​ ​to​ ​explore​ ​the​ ​potential​ ​for​ ​new​ ​research applications​ ​associated​ ​with​ ​prosopography,​ ​networks,​ ​and​ ​deep​ ​contextualization.​ ​REED​ ​London’s wealth​ ​of​ ​references​ ​to​ ​very​ ​itinerant​ ​individuals​ ​across​ ​contemporaneous​ ​records​ ​means​ ​that​ ​we will​ ​be​ ​able​ ​to​ ​discern​ ​patterns​ ​through​ ​linking,​ ​analysis,​ ​and​ ​visualization.​ ​We​ ​will​ ​leverage​ ​REED’s named​ ​entities​ ​for​ ​linking​ ​people,​ ​places,​ ​events,​ ​and​ ​organizations.​ ​Our​ ​team​ ​has​ ​healthy​ ​debates about​ ​the​ ​problematic​ ​present​ ​of​ ​linked​ ​data.​ ​Brown​ ​has​ ​stated​ ​that,​ ​“linking​ ​up​ ​with​ ​other​ ​data means​ ​connecting​ ​one​ ​ontology​ ​to​ ​another,​ ​and​ ​this​ ​brings​ ​with​ ​it​ ​a​ ​pressure​ ​toward​ ​generalization rather​ ​than​ ​specificity” (Brown,​ ​Simpson,​ ​et.​ ​al.,​ ​2015).​ ​Cummings​ ​has​ ​posited​ ​that​ ​“being​ ​able​ ​to​ ​seamlessly​ ​integrate​ ​highly complex​ ​and​ ​changing​ ​digital​ ​structures​ ​from​ ​a​ ​variety​ ​of​ ​heterogeneous​ ​sources​ ​through interoperable​ ​methods​ ​without​ ​either​ ​significant​ ​conditions​ ​or​ ​intermediary​ ​agents​ ​is​ ​a​ ​deluded fantasy” (Cummings​ ​2014).​ ​Still,​ ​as​ ​a​ ​group​ ​we​ ​hope​ ​that​ ​by​ ​publishing​ ​our​ ​ontologies​ ​as​ ​a​ ​means​ ​of​ ​relating​ ​these entities​ ​as​ ​linked​ ​open​ ​data,​ ​we​ ​will​ ​be​ ​able​ ​to​ ​contribute​ ​to​ ​larger​ ​dialogues​ ​about​ ​class​ ​and​ ​society in​ ​Britain​ ​-​ ​certainly​ ​over​ ​the​ ​500​ ​years​ ​covered​ ​by​ ​REED​ ​London,​ ​but​ ​also​ ​about​ ​the​ ​development​ ​of Britain​ ​and​ ​Europe.​ ​CWRC​ ​content​ ​will​ ​be​ ​aggregated​ ​by​ ​the​ ​Advanced​ ​Research​ ​Consortium (ARC),​ ​and​ ​REED​ ​London​ ​will​ ​benefit​ ​from​ ​that​ ​aggregation,​ ​as​ ​we​ ​anticipate​ ​that​ ​people​ ​who​ ​figure in​ ​the​ ​REED​ ​London​ ​corpus,​ ​such​ ​as​ ​Elizabeth​ ​I,​ ​Francis​ ​Bacon,​ ​and​ ​Inigo​ ​Jones​ ​will​ ​be discoverable​ ​by​ ​scholars​ ​searching​ ​for​ ​these​ ​known​ ​figures​ ​across​ ​other​ ​linked​ ​resources.​ ​Perhaps more​ ​important,​ ​REED​ ​London​ ​records​ ​include​ ​extended​ ​references​ ​to​ ​thousands​ ​of​ ​Londoners​ ​who were​ ​in​ ​some​ ​way​ ​connected​ ​to​ ​performance,​ ​but​ ​who​ ​were​ ​not​ ​defined​ ​by​ ​that​ ​connection:​ ​civic officials,​ ​guild​ ​members,​ ​lawyers,​ ​clerks,​ ​priests,​ ​etc.​ ​The​ ​work​ ​of​ ​this​ ​project​ ​thus​ ​holds​ ​as​ ​yet unrealized​ ​value​ ​for​ ​a​ ​much​ ​broader​ ​understanding​ ​of​ ​British​ ​historical​ ​subjects.
            Working​ ​within​ ​CWRC’s​ ​platform​ ​and​ ​optimizing​ ​CWRC-Writer​ ​has​ ​allowed​ ​the​ ​core​ ​REED​ ​London team​ ​to​ ​move​ ​efficiently​ ​to​ ​an​ ​advanced​ ​planning​ ​phase.​ ​By​ ​the​ ​end​ ​of​ ​2017​ ​we​ ​will​ ​have designed​ ​templates​ ​for​ ​all​ ​record​ ​formats​ ​from​​ 
                Inns of Court​ ​​and​ ​mapped​ ​database​ ​fields​ ​from 
                EMLoT​​ ​to​ ​align​ ​with​ ​the​ ​record​ ​parts​ ​from​ ​the​ ​print​ ​collections.​ ​We​ ​will​ ​have​ ​harvested​ ​a​ ​preliminary “white​ ​list”​ ​of​ ​named​ ​entities​ ​(people,​ ​places,​ ​organizations)​ ​from​ ​all​ ​three​ ​print​ ​​​collection​ ​indexes, P&amp;P,​ ​and​ ​Staffordshire.​ ​Because​ ​of​ ​this​ ​efficient​ ​onramp​ ​we​ ​will​ ​be​ ​able​ ​to​ ​focus​ ​in​ ​the​ ​first​ ​half​ ​of 2018​ ​on​ ​ingesting​ ​data,​ ​records,​ ​and​ ​contextual​ ​materials​ ​from​ ​Inns​ ​of​ ​Court​ ​and​ ​EMLoT.​ ​We​ ​will test​ ​the​ ​REED-specific​ ​entity​ ​list​ ​on​ ​ingested​ ​materials.​ ​We​ ​will​ ​also​ ​begin​ ​to​ ​user-test​ ​the​ ​editorial workflow​ ​system​ ​with​ ​the​ ​larger​ ​project​ ​team​ ​of​ ​REED​ ​editors​ ​and​ ​staff.​ ​By​ ​June​ ​2018​ ​we​ ​will​ ​have begun​ ​semantic​ ​tagging​ ​and​ ​experimentation​ ​with​ ​the​ ​CWRC​ ​HuViz​ ​semantic​ ​web​ ​visualization​ ​tool. At​ ​the​ ​DH​ ​2018​ ​conference​ ​we​ ​will​ ​report​ ​on​ ​further​ ​customization​ ​of​ ​the​ ​CWRC​ interface,​ our​ ​plans for​ ​data​ ​discovery​ ​and​ ​research​ ​collaboration,​ ​and​ ​present​ ​preliminary​ ​plans​ ​for​ ​user-responsive editions​ ​and​ ​data​ ​linkage.
            
        
        
            
                
                    Bibliography
                    
                        Brown, ​S. (2016). ​Tensions​ ​and​ ​Tenets​ ​of​ ​Socialized​ ​Scholarship. 
                        D​​
                        igital​ ​
                        Scholarship​ ​
                        in​ ​
                        the Humanities​, ​31 (2): 283-300.
                    
                    
                        Brown, S.​, ​Simpson, J., ​
                        CWRC​ ​Project​ ​Team, and ​Inke​ ​Project​ ​Team. (2015) ​An​ ​Entity​ ​By​ ​Any​ ​Other Name:​ ​Linked​ ​Open​ ​Data​ ​as​ ​a​ ​Basis​ ​for​ ​a​ ​Decentered,​ ​Dynamic​ ​Scholarly​ ​Publishing​ ​Ecology. 
                        Scholarly​ ​
                        and​ ​
                        Research​ ​
                        Communication​​ ​6 (2). 
                        http://src-online.ca/index.php/src/article/view/212/409.
                    
                    
                        Canadian Writing Research Collaboratory ​project​ ​website.​ ​​
                        http://www.cwrc.ca/en/.
                    
                    
                        Cummings,​ ​J. (2014).​ ​The​ ​Compromises​ ​and​ ​Flexibility​ ​of​ ​TEI​ ​Customisation.​ In​ ​Mills, C., ​ ​Pidd, M.​ ​and​ Ward, E.​ (eds), 
                        Proceedings of the Digital Humanities Congress 2012.​ ​
                    
                    CWRC:​ About​ ​CWRC/CSÉC​ ​webpage.​ ​​
                        http://www.cwrc.ca/about/#whatis
                    
                    CWRC​ ​Humanities​ ​Visualizer​ ​webpage. ​​
                        http://www.cwrc.ca/uncategorized/huviz-tool/
                    
                    
                        Early Modern London Theatres​​ ​website.​ ​​
                        http://www.emlot.kcl.ac.uk
                    
                    Entity Authority Tool Set​ ​(EATS)​ ​website. ​​
                        https://eats.readthedocs.io/en/latest/index.html
                    
                    
                        Hagen, T.,​ ​MacLean, S.,​ ​and​ ​Pasin, M.​ (​2014).​ ​Moving​ ​Early​ ​Modern​ ​Theatre​ ​Online: the Records​ ​of​ ​Early​ ​English​ ​Drama​ ​introduces​ ​the​ ​Early​ ​Modern​ ​London​ ​Theatres. http://static.michelepasin.org/public_articles/2014-REED_McLean-Pasin.pdf
                    
                    
                        Jakacki, D. (2017)​ ​REED​ ​London:​ ​Humanistic​ ​Roots,​ ​Humanistic​ ​Futures.​ ​Paper​ ​given​ ​at​ ​MLA​ ​2017. 
                        http://dx.doi.org/10.17613/M67794
                    
                    
                        Jakacki, D.​ (2016) REED​ ​and​ ​the​ ​Prospect​ ​of​ ​Networked​ ​Data. Paper​ ​given​ ​at​ ​the​ ​Conference​ ​of​ ​the​ ​Canadian Society​ ​for​ ​Renaissance​ ​Studies.​http://dx.doi.org/10.17613/M6CK59
                    
                    
                        Liu, A. (2017)​ ​Toward​ ​Critical​ ​Infrastructure​ ​Studies",​ ​paper​ ​given​ ​at​ ​the​ ​University​ ​of​ ​Connecticut.​ ​​
                        https://www.youtube.com/watch?v=2ojrtVx7iCw
                    
                    Records​ ​of​ ​Early​ ​English​ ​Drama​ ​project​ ​website. h​​ttp://reed.utoronto.ca 
                        
                    
                    
                        REED​ Patrons and Performances website. https://reed.library.utoronto.ca
                        
                    
                    
                        REED​ 
                        Staffordshire​​ ​Collection​ ​website. ​​https://ereed.library.utoronto.ca/collections/staff/
                    
                
            
        
    
The Spanish republican exile was the result of the Republican defeat in 1939 by the Francoist army, led by the general Francisco Franco. Nearly half a million-people had to go into mass exile during the months of January and February, through the French border crossings. Many other exiles did so, months later, from Alicante to the North African coasts. These places of destination were, in most cases, places of passage to successive destination countries in Europe and, especially, in Latin America. The international nature of this historical event means that there is currently a large number of personal files scattered in different places around the world. In order to recover these stories, the e-xiliad@s project was conceived in 2009, with a Digital Humanities and Public History perspective: www.exiliadosrepublicanos.info. It is an crowdsourcing project that, through a multilanguage digital platform, retrieves unpublished documents about the anonymous exiled. From the research point of view, the privileged target audience is composed by relatives and friends of the exiles and those interested in the subject. This initiative funded twice (2009 and 2011) by the General Directorate of Migrations of the Spanish Ministry of Employment and Social Security, uses a methodology created ad hoc to obtaining data based on public participation from citizen science. That is, the content is generated on-line by the public at an international level and coordinated by a scientific specialist. For almost a decade, this crowdsourcing project has been developing an online public engagement strategy for public participation based on open data, supported by a custom digital platform and its digital social networks, with more than 1.500 followers. At this stage, the project recovered around five hundred unpublished archives among photographs, memories, official documents, letters and interviews, that comes associated with about two hundred completed exile records. The vast majority of these data are public, thanks to the informed consent of the author.Figure 1. Examples of documents and photos provided by relatives of exiles registered on the project platform and collaborators.At technological level, e-xiliad@s has been built using Drupal 6 LTS (Long Term Support) with a MySQL database. The technological solution answer the need of the internal survey form that has been created using a specific data model connected with a strategic communication plan, that serves to get data online via user’ confidence with the project and to stimulate his/her family memoir.Figure 2. Detail of the internal form to be filledAs lesson learned, the need to maintain a technology that is getting older without official support (Drupal); the management of a project social network community continuously growing; and the spamming problem due to the project popularity within the republican exile community (more than 11.000 spamming users) are questions that we are trying to solve. Despite this, E-xiliad@s acts as a digital identity place for those connected to this topic and as a space that informs society, with scientific rigor, from the field of Digital Public History. Currently there are six large online social network communities that move much of the information on the subject of Republican exile and the Spanish Civil War, among which is the e-xiliad@s project with its social networks (Facebook: @exiliados.republicanos and Twitter: @exiliadas. They play a significant role at the national and international level regarding the recovery of historical memory, due to the quantity and quality of the information it offers. Graph analysis that represents the first and second level connections of the social networks of the E-xiliad@s project, which is represented in the community in green.
Our lightning talk offers solutions to some shortcomings in communication about DH projects and undertakings on university campuses, particularly through the development of institutional DH websites. By an “institutional DH website,” we mean a community website, hosted by a given university or institution, that is explicitly devoted to the advancement, support, and promotion of DH work collectively. A number of previous attempts to establish institutional DH websites have failed, and there is a growing need to understand how we can sustainably create and maintain such sites in a way that meets the diverse needs of DH scholars. To this end, we offer an alternative approach for creating such communal sites that is designed for specific communities. More than merely providing a definition of DH and a set of resources for those interested in the field, institutional DH websites can beneficially act as community hubs for DH practitioners by showcasing live projects and encouraging interdisciplinary collaboration. While there is no one-size-fits-all solution, an open development process can help scholars and DH staff who face long-standing DH challenges around methodological innovation, data reproducibility, reinvention of the wheel, and the balance of technical and humanistic priorities. In particular, we offer a user-focused development process for DH websites, which emphasizes the identification and enhancement of human networks and communities of practice. At the most basic level, user-focused design starts with a needs assessment of the website’s primary audience and is refined through attention to typical user needs and exemplary uses throughout the project’s lifecycle in order to maintain an active user community. This stands in contrast to the design of institutional DH sites as a means of cataloguing the services or offerings at a specific institution. At the structural level, user-focused design for institutional DH sites foregrounds open access and accessibility by thinking about these concerns throughout the design process (rather than as a last-minute add-on). The two speakers have designed institutional DH websites at the University of Virginia and Bucknell University, each employing a different platform (Drupal and WordPress); nonetheless, both sites promote similar design philosophies. The talk will model how institutions can create similar sites designed for their own communities with an eye toward developing appropriate use cases and sustainability practices.
Irish diaspora is how we refer to the historical process of migration from Ireland, recorded since the Early Middle Ages, but particularly evident since the XVIII century. By the 21st century, according to figures in the Irish Emigration Patterns and Citizens Abroad report published in 2017 by the Irish Department of Foreign Affairs and Trade, an estimated 70 million people worldwide claimed some Irish descent, that largely maintain a connection with Irish cultural identity and heritage.The online project Historic Graves (https://historicgraves.com/) capitalized on this global phenomenon, putting together a worldwide community of more than 15,000 users, collaborating in generating a nationwide genealogical dataset. The project began in 2010, from an idea of John Tierney developed by Maurizio Toscano, both still at the forefront of the initiative. It started and still runs as a community focused grassroots heritage project, where local community groups living in Ireland are trained in low-cost high-tech field survey of historic graveyards and recording of their own oral histories. In almost a decade, historicgraves.com published online more than 800 graveyards, recording at least a photo and the location of 99,235 graves and collaboratively transcribing the details of 199,851 people, and counting.Community co-production happens within a freely available online platform, created for the transcription of memorial epitaphs. Training workshops are offered to local communities interested in contributing to surveying and transcribing historic graveyards. The combination of online interaction with local workshops and meetings works best in terms of ensuring meaningful participation. Data gathering and management procedures have proved to be essential on two fronts: data collected in the field is normally available online in short time; on the website, memorials’ transcription is unmediated and immediately available to share. Instant publication proved to be highly engaging for the volunteer groups involved: they see immediate results for their work and are willing to share them with family and friends living abroad. Each local community can then download individual datasets of their own records, as tabular Open Data. The global community of users takes responsibility for quality control and completeness.In 2018, the project has received a Heritage Council grant, that supported the upgrading of the online platform (backend and frontend), and it has been part of the initiative European Year of Cultural Heritage (#EuropeForCulture). From a technical point of view, data, code and layout have been upgraded from Drupal 6 to Drupal 7, with a plan to move to BackdropCMS or Drupal 9 in late 2021, when both Drupal 7 and Drupal 8 will reach "End of Life".The Historic Graves project is fundamentally a Public History project, originated from local communities and that constantly relies on them to grow, solve ethical issues and stay sustainable. Technologies, and the Web in particular, have played a big role, both enabling remote transcriptions and stimulating participation, although they never been the focus of the initiative. Additionally, in line with public digital humanities principles, the project has always been open to participation, well beyond the academy, growing as a group of networked and engaged communities, not just as a repository for content.

        
            
                The Digital Humanities as a field of research and publication face both new opportunities and challenges due to the increasing number of media sources relevant to researchers and students, no matter their age and origin. Aggregator platforms like YouTube or Vimeo let formerly unknown materials surface that are relevant for new research in the humanities. Meanwhile, large public and private collections held by historical organizations, broadcasting corporations and NGOs, or individuals like artists, scientists or politicians remain to be indexed, investigated and re-published (Sommer, 2016). At the same time, researchers who are experimenting with new, creative ways to combine curating, scholarship and presentation draw attention to the immersive and narrative potential of sound-based media (see e.g. Barber, 2017; Murray and Wiercinski, 2014; Cohen, Rakerd, Rehberger &amp; Boyd 2012).
                Some of the use cases relevant to the field of digital humanities that will probably become even more frequent over the next years include:
                
                    Support for oral history researchers who need to transcribe large amounts of recordings
                    Using transcripts for lectures and online learning tools, both for students and the public at large
                    Enhancing the usability of digital A/V archives via full text search
                    Facilitating community contribution and citizen science projects
                    Furthering the accessibility of A/V content (e.g. for deaf people)
                    Intertwining multimedia material and text for research papers published in online journals
                
                Alternating between more theory-focused discussion and hands-on experience, we will different methods to integrate transcripts into multimedia formats that are useable for research and publications. After a brief introduction into the principles and current possibilities as well as the limitations of computerized speech to text transcription, particularly in comparison with tools for manual transcription such as WebAnno and OCTRA, we will introduce participants to a small number of different speech-to-text software packages that can be used to semi-automatically transcribe A/V content, while discussing their respective pros and cons.
                We will then demonstrate how to use the open source package ffmpeg to extract and convert various types of A/V recordings to formats suitable for further processing and try out automatic speech to text conversion with OH portal, an online transcription tool developed and maintained by the phonetics research team at the university of Munich (
                    https://www.phonetik.uni-muenchen.de/apps/oh-portal/) and Watson, a powerful, trainable natural language processing tool developed by IBM. 
                
                For many in the DH community, creating a good transcript is just the start. Presenting research to fellow academics and the public at large, garnering interest for archives and projects, creating lively and easy to use learning material - all while preserving the non-verbal aspects of raw sources - are often just as important. This is probably even more true for novel, hybrid formats which have been theorized to be result of an amalgamation of analog and digital publishing (Ludovico, 2012).
                As an exemplary solution, we will introduce Hyper Audio Linking, a technique for the presentation of digital content that allows to link transcripts to pre-defined jump marks in video or audio recordings via JavaScript. HAL augmentation deals with diverse content aspects of a source while letting the original untouched. It can be used for all kinds of sources, be it contemporary recordings of lectures and panel discussions, interviews, or historical footage, as well as some art genres like theatre plays, films, or audio dramas. By defining sections or “chapters” and linking them to timestamps, the transcript can be used to jump between those parts in the original media, allowing recipients to switch between reading or viewing/listening mode. The resulting multi-media hybrid can be further augmented by defining formatting options for different types of content and by including various metadata, images, annotations, keywords, and the like. The result is an elegant and easy to use frontend interface that allows for full-text search and easy navigation within A/V content.
                A simplified workflow of an oral history project that uses HAL both as support for researchers and to publish research results and multimedia documents can be seen in figure 1. Importantly, HAL-augmented files can not only be used for the final step of publishing and presenting results. Depending on the configuration of jump marks, they can also support members of the research team in navigating recordings or in tracing observations, selected utterances and topics over multiple sources. Meanwhile, the method is flexible enough to be introduced on the fly at any point in time. If desired, its use may also be restricted to a certain part of collected sources, e.g. only those that are selected for public access.
                
                    
                
                Figure 1: Sample workflow with HAL integrated in an oral history project
                Using an existing interface to a database of videos on history and politics of digital culture, participants will experiment with different ways to use HAL links within transcripts. We will discuss and compare our Drupal-based implementation with other existing tools such as ELAN, an annotation tool for A/V content developed at the Max Planck Institute for Psycholinguistics in Nijmegen. We will also talk about some of the key differences between software designed to work as a standalone tool for researchers versus a method developed for usage either as the interface of an online archive or as an enrichment or additional feature of web publishing formats.
                To round off the workshop, we will start to build a web page featuring A/V content, a transcript, and additional material such as photographs and annotations from scratch. Using just a text editor and a few simple lines of Html and JavaScript code, participants will learn to apply HAL augmentation to their own publications. We will use a pre-configured installation of the open source content management system Drupal that can be customized to fit different use cases, including curated online collections, project documentation, event pages etc.
                For this second practice part of the workshop, participants should ideally bring their own A/V sources and start working on their personal project. Those who don’t have a specific use case in mind (yet) will be able to choose among different sources provided by the workshop team instead.
                The last workshop unit will be a discussion dealing with the editorial planning for publication. Whether you are an individual producer, a research team or an online cluster, you will have to make certain decisions in how to manage the publication: Who is the audience that will be primarily addressed? Which specific needs have to be taken into account with regards to usability, accessibility and technological literacy? How much (if any) pre-existing knowledge on the topic can be assumed, and how much introductory information should be included?
                
                    Target audience and requirements
                    Previous workshops and presentations of Hyper-Audio-Linking have been held at BASICS, transmediale Berlin, at the Ludwig-Boltzmann-Institute for Media.Art.Research, Linz, and most recently as part of the EADH conference Galway 2018. Based on these earlier workshops on similar topics, we expect to work with a group of between ten and fifteen participants, which seems ideal for a more hands-on experience.
                    There are no skill requirements for participation, all introductions into software usage and programming will be suitable for beginners but can be easily adapted to participants with more advanced levels of pre-existing knowledge.
                    Participants should bring their own laptop. Participants who use a laptop provided by their employer should make sure to either have all software pre-installed or know that they are authorized to install software on their computer.
                    All required software is either open access or free to use, there will be no additional costs.
                
                
                    Pre-conference support and provision of material
                    Participants who would like to familiarize themselves with HAL in advance will be given access to stubs of prepared A/V content on our platform Transforming Freedom.org four weeks before the conference start. 
                    Download links for other software packages and instruction material on how to install and use the respective software will be made available to the participants about two weeks in advance of the workshop. 
                
                
                    Intended length and format
                    The duration of the workshop will be a half day, that is, approximately four hours including breaks. As far as the format is concerned, we will alternate between theory-focused presentation, group discussion, and practical tasks that let participants try out different techniques supported by the instructors.
                
            
        
        
            
                
                    Bibliography
                    
                        Barber, J. F. (2017). Radio Nouspace: Sound, Radio, Digital Humanities. 
                        Digital Studies/Le champ numérique, 7(1), 1. DOI: 
                        http://doi.org/10.16995/dscn.275 (accessed 05 May 2019).
                    
                    
                        Cohen, S., Rakerd, B., Rehberger, D., &amp; Boyd, D. A. (2012). Oral history in the digital age: the imperative for rethinking best practices based on a survey of the field(s). Retrieved from 
                        http://ohda.matrix.msu.edu/2012/07/ohda-survey/. (accessed 04 May 2019).
                    
                    
                        Ludovico, A. (2012). Post-Digital Print: The Mutation of Publishing since 1894. 
                        Onomatopee 77.
                    
                    
                        Murray, A. and Wiercinski, J. (2014). A Design Methodology for Sound-based Web Archives. 
                        Digital Humanities Quarterly 8.2, 
                        http://digitalhumanities.org/dhq/vol/8/2/000173/000173.html (accessed 05 May 2019).
                    
                    
                        Sommer, B. (2016). 
                        Practicing Oral History in Historical Organizations. London: Routledge.
                    
                
            
        
    

        
            
                The ISMI project
                The Islamic Scientific Manuscript Initiative (ISMI) project was founded in 2005 to make accessible information on all Islamic manuscripts in the exact sciences (astronomy, mathematics, optics, mathematical geography, music, mechanics, and related disciplines), whether in Arabic, Persian, Turkish, or other languages from the 9
                    th to the 19
                    th century
                    ISMI website: 
                        
                     The ISMI project limits itself to “scientific” manuscripts but it tries to encompass all such manuscripts worldwide regardless of their current location and it tries to record as much information about these manuscripts as available, including reader and ownership marks, annotations and illustrations, making it possible to learn more about structures and practices of knowledge in the islamicate world (Ragep et al., 2008).
                
                
                    The database
                    The database of the ISMI project is a cooperation project by the Max Planck Institute for the History of Science and the Institute of Islamic Studies at McGill University in Montreal. The database has been built up over more than ten years starting from an early personal database project by the involved scholars, extended by corrected information from catalog works like MAMS (Matvievskaya et al., 
                        1983
                        ) and personal research by the scholars in the project and outside. It currently contains information about over 4700 texts in 15000 witnesses in 8000 codices and 2500 persons and an accompanying secondary bibliography of 2700 titles and it is constantly being extended.
                    
                    The database development started in 2006 with a new data model based on the idea of a network of flexible objects and relations. Objects can have arbitrary attributes and the relations between objects are also like objects and can have attributes.
                    
                        
                            
                            Part of current ISMI data model showing relations between text, witness, person and codex objects.
                        
                    
                    The basic objects in the data model are for example the TEXT which is abstract, the WITNESS which is a concrete material manuscript and the PERSON (real or imaginary). These objects are connected by relations like 
                        is_exemplar_of which connects a text and its witnesses and 
                        was_created_by which connects a text and a person as its author (see Figure 1). The same person can at the same time also be connected to other witnesses as a copyist or as a dedicatee. This very flexible data model was regularly modified and extended to accommodate changes and refinements that were developed in close cooperation with the scholars entering the data as their understanding of the source material and the technical possibilities of the system changed. Examples of theses unique additions are the possibility to record misattributions of authorship and misidentifications of witnesses in existing literature as well as documented reading events and changes of ownership.
                    
                    This concept of a network of objects with flexible relations, an attribute-graph, exists today in database products like Neo4J
                        Neo4J: 
                            
                         but those were not available in 2006 which led to the development of a custom database called "OpenMind". The database software is Open Source, written in Java, uses a conventional SQL database backend and a Web-based frontend.
                    
                    A first version of a public website presenting a limited set of 130 codices by the Staatsbibliothek Berlin with digitalizations was published in 2015.
                
                
                    Towards new standards
                    The current database system OpenMind was a custom development which was necessary at the time of its creation but has not aged well and burdens the future development of the project with limited flexibility and high maintenance for software development. The data model was also not created based on existing ontologies due to a lack of usable tools at the time. Both features were acceptable during the development of the project but they pose a problem to the continued maintenance of the project and the reusability of its data.
                    Currently both software and data are migrated to new standard tools in two phases:
                    In the first phase data is still entered in the legacy OpenMind backend but there is a new public web frontend based on the Drupal CMS that is fed by an XML export from the legacy backend. The XML data is also fed into a Neo4J graph database for additional queries and visualisations. This is the architecture for the beta launch in September 2018 and the public launch end of November 2018.
                    In the second phase the data model will be migrated to the CIDOC-CRM
                        CIDOC-CRM: 
                            
                         reference ontology using the FRBRoo
                        FRBRoo: 
                            
                         model and other extensions. All data is converted to RDF following the new data model and a frontend based on the ResearchSpace
                        ResearchSpace: 
                            
                         software and a triple store backend is created for data entry and specialized queries and visualisations. This process is currently under way.
                    
                
                
                    The new ISMI website
                    The new public website presents data on 650 persons (selected chronologically following MAMS), 2300 texts, 6900 witnesses and related objects, representing authors from before 1350CE. The website will be public starting end of November 2018. Additional data publications are in preparation.
                    The new web frontend provides browsable lists of all major object types (persons, texts/works, witnesses, codices, places,…) as well as a general search and searches for specific object types. All objects on the pages are linked which makes it easy to get from a person to all their works and their witnesses as well as to the commentaries on the titles and their supercommentaries.
                    The search has a simple normalization for Arabic and a special normalization for romanized Arabic and is specially tuned to be very forgiving for differences in spelling especially for Arabic names. Feedback for the search and navigation during the beta test phase was very positive.
                    The website also shows currently 104 freely available digitized codices using the IIIF
                        IIIF: 
                            
                         image standard and the Diva.js
                        Diva.js: 
                            
                         viewer (see Figure 2). Most of the codices were scanned by the MPIWG in a cooperation with the Staatsbibliothek Berlin but some exemplars from the Gallica project of the Bibliothéque Nationale de France and the Qatar Digital Library are also present to demonstrate the potential of public IIIF image sources in an area that has been plagued in the past with proprietary data silos and restrictive access conditions making global electronic manuscript databases nearly impossible. We hope to expand the amount of scanned codices in the future.
                    
                    
                        
                            
                            Display of scanned manuscript (Codex Petermann I 671, Staatsbibliothek Berlin)
                        
                    
                    The experimental “ISMI Lab” section of the site offers access to the “Query Builder” tool which allows to construct custom queries to the database based on objects, attributes and relations and a full Neo4J graph database console with access to all published data (see Figure 3). These additional tools are very powerful but require some technical expertise and familiarity with the ISMI data model. There is some documentation but this section is more of an experimental offer to also get in contact with interested scholars in the hope that interesting queries and research questions can be exchanged and new, easier to use, tools can be developed in the future.
                    
                        
                            
                            Experimental Neo4J console showing partial graph of commentary relations.
                        
                    
                
                
                    A never ending project?
                    The history of the project in the last ten years has shown the difficulties of developing and maintaining a project of this complexity – organisationally, in terms of hardware, software, and scholarly support. We think this project shows the potential for a unifying manuscript database that is not limited to singular collections and presents the continually updated and expanded current knowledge of scholars in the field. We hope that scholars in the future will not have to figure out errors in decades-old printed catalogues individually again and again but that they can participate in a common database and share and enhance their individual findings. The collaborative phase of the ISMI database is only beginning and we would like to start the discussion now. We think we have laid the technical foundations to make the database maintainable and adaptable and the data shareable and linkable but the long term value of a shared resource lies in its users and its contributors.
                
            
        
        
            
                
                    Bibliography
                    Ragep, Jamil F., and Sally P. Ragep. 
                        The Islamic Scientific Manuscript Initiative (ISMI) Towards a Sociology of the Exact Sciences in Islam. In A Shared Legacy: Islamic Science East and West. Homage to Professor J. M. Millàs Vallicrosa, edited by Emilia Calvo, Mercè Comes, Roser Puig, and Monica Rius, 15–21. Barcelona: University of Barcelona, 2008. 
                        
                    
                    G. P. Matvievskaya and B. A. Rosenfeld, 
                        Matematiki i astronomi musulmanskogo srednevekovya i ikh trudi (VIII-XVII vv.) [Mathematicians and Astronomers of the Muslim Middle Ages and Their Works (VIII-XVII centuries)], 3 vols. (Moscow: Nauka, 1983), later extended as 
                        Boris A. Rosenfeld and 
                        Ekmeleddin İhsanoğlu
                        , Mathematicians, astronomers and other scholars of Islamic civilization and their works (7th-19th c.)
                    
                
            
        
    

        
            Despite the thousands of digital projects launched during the past 20 years, experts warn of a new “digital dark age” (Cerf, 2015; Davis, 2016) as our ability to produce digital information continues to outpace our capacity to preserve and access that knowledge for the long term, even (or especially) when using content management systems (Montoya, 2016).
            
                Project Endings is a collaboration between the Humanities Faculty and the University Library which aims to provide practical solutions to issues attendant on ending a project and archiving the digital products of research, including not only data but also interactive applications and web-based publications. 
                Project Endings endeavours to align the aims of faculty researchers producing projects and the archivists who will eventually be responsible for curating their work.
            
            The project divides digital projects into five primary components: data, products, processing, documentation, and release management. We aim at longevity primarily for data and products, but believe that this goal requires careful attention to processing, documentation and release management. We are developing preservation principles for all of these factors, using practice-based methods (Holmes, 2017; Arneil and Holmes, 2017; Holmes and Takeda, 2018), diagnostic tools (Holmes and Takeda, 2017), and scholarly research as listed in the project bibliography at 
                .
            
            The project conducted a survey on the 
                LimeSurvey platform consisting of 30 questions 
                 to discover how project leaders dealt with the issues of long-term sustainability for each of the five primary components. We promoted the survey to Canadian and international professional communities and received 128 responses. 25 detailed interviews were run with a sample of the respondents to get more information on the issues raised by the survey results.
            
            Results of the survey show that concerns about longevity for digital humanities projects are not exaggerated. 57% of survey respondents did not consider an endpoint for their project, despite the fact that project management principles include declarations of goals, timelines, and milestones (Zanduis and Stellingwerf, 2013). In the light of this, perhaps it is not surprising that 54% did not have long-term preservation plans. These findings suggest that many researchers do not distinguish between products generated to exploit the features of the processing environment and products generated to survive after active work on the project ends or independent of development work in the project. Furthermore, only 32% considered “benchmarks for assessing progress” and 41% included precise timelines in their plans.
            In a group of projects that were for the most part (74%) less than 10 years old and 58% still in progress, 22% reported that project outputs stopped working due to software obsolescence. This is in a field of projects in which 74% started with born-digital data. If a failure occurs during the active life of the project it might be repairable, but repair is much less likely if the project has ended.
            The value of using a standardized data model is not universally recognized, with 14% of survey respondents not using one at all and 26% making up their own. Although a home-made data model is by definition not standardized, it may still be viable for a long time if well documented. 60% claimed to have a clearly documented data model, but 90% of those that had documentation considered it to be partial or inadequate, so it appears that a project’s data model is well documented in only about 50% of cases.
            HTML is the most popular standard output for DH projects (68% of respondents used it), despite the continued popularity of PDF (45%), XML (38%), and various binary media formats (&gt;65%). Javascript is considered by many (30%) to be a major technology in their project. HTML and Javascript are robust long-term (Holmes, 2017), but if they are produced in a project only on-the-fly by a content management system (CMS) or database, then the longevity of the output is dependent on that of the CMS or database. 34% of the respondents used WordPress or Drupal, 31% used PHP/SQL databases, 38% used XML/XSLT/XQuery systems, and 41% used “other” software services and libraries. Some projects used more than one of these.
            Lack of ongoing funding was cited by 38% of respondents as the main obstacle to long term preservation. Perhaps more surprisingly, 33% of respondents rated either lack of expertise or bad technology choices as their main obstacle, which may explain the results reported above regarding software obsolescence. Early results from the interviews suggest that CMS and other software libraries and services are the likeliest sources of software failure over time. We hope that further analysis of the interviews will tell us whether a more expert assessment of software and output choices would have mitigated the issue of lack of ongoing funding.
            While a reassuringly high 42% of respondents reported that university services were responsible for long-term maintenance of the project’s work, an alarming 45% reported that this responsibility fell to the Principal Investigator or nobody, demonstrating either significant vulnerability or great confidence.
            Our survey results suggest that there is a limited use of project management (“What is PRINCE2?”, 2018; Sedlmayer et al., 2015) and software lifespan principles in DH projects. Results further suggest that there is a need for an improved understanding by researchers of specific attributes of a project which are likely to facilitate long-term viability of the project data, outputs and documentation at minimal cost for those charged with preservation. Blurring the lines between data, processing, outputs and the management of those components over time can result in vulnerabilities for long term preservability which may not be apparent until it is too late.
            With all of this in mind 
                Project Endings is working on a suite of recommendations that will provide guidance on project structure and management with long term viability as the goal. We are offering an online interactive questionnaire that assesses the long-term viability of each component in a project and provides recommendations for improving the prospects for long-term survival. Behind each question is the empirical evidence provided by survey/interview participants as well as the combined experience of the 
                Project Endings team. The questionnaire is intended primarily to be a thought-provoking activity for project leaders and principal investigators. An early draft of the questionnaire is available at 
                .
            
        
        
            
                
                    Bibliography
                    
                        Arneil, S. and Holmes, M. (2017). Archiving form and function: preserving a 2003 digital project. Brighton, U.K.
                    
                    
                        Cerf, V. (2015). Google’s Vint Cerf warns of ‘digital Dark Age’ 
                        .
                    
                    
                        Davis, R. C. (2016). Die Hard: The Impossible, Absolutely Essential Task of Saving the Web for Scholars. Skidmore College, Saratoga Springs, U.S.A. 
                        .
                    
                    
                        Holmes, M. (2017). Selecting Technologies for Long-Term Survival. Victoria, Canada 
                        .
                    
                    
                        Holmes, M. and Takeda, J. (2017). Beyond Validation: Using Programmed Diagnostics to Learn About, Monitor, and Successfully Complete Your DH Project. Montreal, Canada 
                        .
                    
                    
                        Holmes, M. and Takeda, J. (2018). Why do I need four search engines?. Tokyo, Japan 
                        .
                    
                    
                        Montoya, R. D. (2016). Advocating for Sustainability: Scaling-Down Library Digital Infrastructure. 
                        Journal of Library Administration, 
                        2016(56:5): 603–20 doi:10.1080/01930826.2016.1186969.
                    
                    
                        Sedlmayer, M., Coesmans, P., Fuster, M., Schreiner, J. G., Gonçalves, M., Huynink, S., Jaques, T., et al. (eds). (2015). 
                        Individual Competence Baseline for Project, Programme &amp; Portfolio Management. International Project Management Association 
                        .
                    
                    
                        Zanduis, A. and Stellingwerf, R. (2013). 
                        ISO21500: Guidance on Project Management – a Pocket Guide. Van Haring Publishing 
                        .
                    
                    What is PRINCE2? 
                        Projects in Controlled Environments
                        .
                    
                
            
        
    

        
            This poster aims at showing the first results of the ongoing collaborative research project 
                The reception of the English novel in the Italian literary press between 1700 and 1830: a transcultural enquiry into the early shaping of the modern Italian literary and cultural identity. The project aims at investigating the reception of English novels in the Italian literary press during the Long Eighteenth Century (1700-1830). The analysis focuses on an existing corpus of data relative to the publication, dissemination, translations, critical reviews, and editorial advertisements of English novels in Italian literary newspapers and journals of the time. The main purpose of the project is to uncover how the English novels were introduced to the Italian readership through literary journalism with the application of digital methodologies of investigation. One of the project goals is in fact to create a methodological paradigm that may be extended to the study of the reception of English novels in the literary journalism of other nations. 
            
            The present paper therefore has three primary objects:
            a) To show to the public the first research output: an open access, bilingual, and annotated digital repository, which consists of a Drupal-based software for corpora, and represents an immediate way to develop the research. The first step of the project has been the cataloguing, analysis, and digitization of the corpus of reviews. This preliminarily created digital database allows the subsequent computational, textual and critical surveys. 
            b) To show how the TEI has been applied to the analysis of the corpus. The text encoding of the reviews, conducted following the TEI standards, makes possible to point out the elements that are original and innovative with respect to the foreign reviews of the time which the Italian press copied from, often adapting the contents. In fact, the encoding of the reviews allows also to understand and visualise their “genealogical dimension” (i.e. the comparative analysis of reviews that were taken from French or English periodicals and made their way into the Italian press). These sources have already been identified, and the comparison between the encoded versions will allow to understand the extent of the influence French and English journalism had on the Italian press, and to outline the specific Italian input. The TEI will therefore make possible to focus on the re-interpretations of Italian reviewers who drew on the Italian literary tradition but challenged its subjects, genres and linguistic structures. Ultimately, the TEI will also be applied to integrate the stylometric analysis and the gathering of Geospatial information: the short presentation will allow me to show a case study and to explain how the mark-up process proves to be a fundamental part of the methodology for the analysis of the corpus. 
            c) To illustrate the two main lines of approach that will be applied in order to digitally explore the corpus. The first consists of a stylistic and linguistic analysis of the reviews, which will be pursued equalizing and comparing stylistic and lexical constellations belonging to different discursive practices from a number of periodicals and journalist. Digital stylometry, word frequency and statistical analyses tools such as 
                R, 
                MiniTab and 
                Intelligent Archive will be used during this phase. The study of the readers’ response to the contents, spread by the novels via the reviews, is deeply connected to the stylistic analysis of the reviews. In fact, the outlining of the reviews’ stylistic features is crucial to understanding in which ways the contents were revealed to the public, and how the audience was influenced in the perception of the moral values and the social messages of the novels. The second line of approach will concern the spatial analysis of the data, which will be mapped thanks to 
                GIS digital tools integrated with Geo-criticism. The spatial analysis allows the visualization of popular reading trends in 18th and early 19th century Italy.
            
        
        
            
                
                    Bibliography
                    
                        Altick, Richard D. 
                        The English Common Reader: A Social History of the Mass Reading Public, 1800-1900. 
                        Columbus: Ohio State UP, 1998. 
                    
                    
                        
                            Baines, Paul. The Long 18
                            th century. 
                            London
                            : 
                            Arnold, 
                            2004.
                        
                    
                    
                        Balay, Robert. 
                        Early Periodical Indexes: Bibliographies and Indexes of Literature Published in Periodicals before 1900
                        . Lanham, MD: Scarecrow, 2000. 
                    
                    Beller, Manfred and Leersen, Joep, 
                        Imagology. The cultural construction and literary representations of national characters. A critical survey, Amsterdam-Atlanta: Rodopi, 2007.
                    
                    
                        Benedict, Barbara M. “Readers, Writers, Reviewers, and the Professionalization of Literature.” 
                        Cambridge Companion to Literature, 1740-1830
                        . Ed. by T. Keymer and J. Mee. Oxford: Oxford U. Press, 2004.
                    
                    
                        Basker, James. “Criticism and the Rise of Periodical Literature.” 
                        The Cambridge History of Literary Criticism
                        . Vol. 4: 
                        The Eighteenth Century
                        . Ed. by H. B. Nisbetand C. Rawson. Cambridge: Cambridge UP, 1997. 316-32
                    
                    
                        Berengo, Marino. 
                        I giornali veneziani del Settecento
                        . Milan: Feltrinelli, 1962.
                    
                    
                        Colombo, Rosa Maria. 
                        Settecento senza amore: studi sulla narrative inglese
                        . Rome: Bulzoni, 1983. 
                    
                    
                        _____. 
                        Lo Spectator e i giornali veneziani del Settecento
                        . Bari: Adriatica editrice, 1966. 
                    
                    
                        De Stefanis Ciccone, Stefania, ed. 
                        La stampa periodica Milanese nella prima metà dell’Ottocento. Testi e concordanze
                        . Pisa: Giardini, 1983. 
                    
                    
                        D’Alia, Fiorella. 
                        La donna nel romanzo italiano del Settecento
                        . Rome: Fratelli Palombi, 1990.
                    
                    
                        Di Fino, Sharon Marie. 
                        The Intellectual Development of German Women in Selected Periodicals from 1725 to 1784
                        . New York: Peter Lang, 1990.
                    
                    
                        Donoghue, Frank. “Colonizing Readers: Review Criticism and the Formation of a Reading Public.” 
                        The Consumption of Culture, 1600-1800: Image, Object, Text
                        . Ed. by A. Bermingham and J. Brewer. New York: Routledge, 1995. 54-74.
                    
                    
                        Farinelli, Giuseppe. 
                        Storia del giornalismo italiano: dalle origini ai giorni nostri
                        . Turin: UTET, 1997. 
                    
                    
                        Forster, Antonia. 
                        Index to Book Reviews in England, 1749-1774
                        . Carbondale: Southern Illinois University, 1990. 
                    
                    
                        _____. 
                        Index to book reviews in England, 1775-1800
                        . London: British Library, 1997. 
                    
                    
                        Franchini, Silvia, Simonetta Soldani, eds. 
                        Donne e giornalismo: Percorsi e presenze di una storia di genere
                        . Milan: Franco Angeli, 2004. 
                    
                    
                        Infelise, Mario. 
                        L’editoria veneziana nel Settecento
                        . Milan: Franco Angeli, 1989. 
                    
                    
                        Klancher, Jon P. 
                        The Making of English Reading Audiences, 1790-1832
                        . Madison: University of Wisconsin Press, 1987.
                    
                    
                        MacMurran, Mary H. 
                        The Spread of Novels: Translation and Prose Fiction in the Eighteenth Century
                        . Princeton: Princeton University Press, 2010. 
                    
                    
                        Magnani, Giovanni. 
                        Giornalismo e attività letteraria dell’Ottocento
                        . Florence: Bulgarini, 1974. 
                    
                    
                        Mangione, Daniela. 
                        Prima di Manzoni: autore e lettore nel romanzo del Settecento
                        . Rome: Salerno editrice, 2012.
                    
                    
                        Marchesi, Giambattista. 
                        Studi e ricerche intorno ai nostri romanzi e romanzieri del Settecento coll’aggiunta di una bibliografia dei romanzieri editi in Italia in quel secolo
                        . Bergamo: Istituto italiano d'arti grafiche, 1903.
                    
                    
                        Mayo, Robert. 
                        The English Novel in the Magazines, 1740-1815. With a Catalogue of 1375 Magazine Novels and Novellettes
                        . Evanston: Northwestern University Press, 1962.
                    
                    
                        Moretti, Franco. 
                        Atlante del romanzo europeo. 1800-1900
                        . Torino: Einaudi, 1997. 
                    
                    
                        Murialdi, Paolo. 
                        Storia del giornalismo italiano
                        . Bologna: Il Mulino, 2000. 
                    
                    
                        
                            O’Gorman, Frank. The Long Eighteenth Century: British Political and Social History 1688–1832 (The Arnold History of Britain Series). 
                            Hodder Arnold, 1997.
                        
                    
                    
                        Pearson, Jacqueline. 
                        Women's Reading in Britain, 1750-1835: A Dangerous Recreation
                        . New York: Cambridge University Press, 1999.
                    
                    
                        Piccioni, Luigi. 
                        Il giornalismo letterario in Italia
                        . Turin-Rome: Loescher, 1984. 
                    
                    
                        Reiman, Donald H. 
                        The Romantics Reviewed; Contemporary Reviews of British Romantic Writers
                        . New York: Garland, 1972.
                    
                    
                        Rivers, Isabel. 
                        Books and their readers in eighteenth-century England: new essays
                        . London and New York: Leicester University Press, 2001.
                    
                    
                        Rose, Jonathan. “Rereading the English Common Reader: A Preface to a History of Audiences.” 
                        Journal of the History of Ideas 
                        53 (1992). 47-70. 
                    
                    
                        Sgard, Jean, ed. 
                        Dictionnaire des journaux (1600-1789)
                        . 2 Vols. Oxford: Voltaire Foundation; Paris: Universitas, 1991. 
                    
                    
                        _____. ed. 
                        Dictionnaire des journalistes 1600-1789
                        . Rev. ed. 2 Vols. Oxford: Voltaire Foundation, 1999.
                    
                    
                        Siskin, Clifford. “Eighteenth-century Periodicals and the Romantic Rise of the Novel.” 
                        Studies in the Novel 
                        26.2 (Summer 1994): 26-42.
                    
                    Spiering, Menno (ed.), 
                        Nation building and writing literary history, Amsterdam-Atlanta: Rodopi, 1999.
                    
                    
                        Streeter, Harold Wade. 
                        The Eighteenth-century English Novel in French Translation: A Bibliographical Study
                        . New York: Blom, 1970.
                    
                    
                        Ward, William S. 
                        Literary Reviews in British Periodicals 1788-1826: A Bibliography
                        . New York: Garland Publishing, 1977. 
                    
                    
                        Westphal, Bertrand. 
                        La Géocritique. Réel, Fiction, Espace
                        , («Paradoxe»), Paris: Éditions de Minuit, 2007.
                    
                    
                        Zambon, Maria Rosa. Bibliographie du roman français en Italie au 18e siècle. Florence: Sansoni, 1962.
                    
                
            
        
    

        
            The 
                Census of Antique Artworks and Architecture Known in the Renaissance (henceforth Census) identifies and collects antique monuments and related Renaissance documents in a database, such as works of architecture, statues, frescoes, sarcophagi, paintings, drawings, sketches, manuscripts and more. Established in 1983, data has continually been added to the database. Since then, the fundamentals of the underlying relational data model of the Census did not have to be changed. Its main focus is to help researchers in art history expand their understanding about the relation between works of art produced in the Antiquity and their reception and perception in the Renaissance.
            
            Although the data model is robust, the research environment using the Census database does not meet current user expectations like a modern and responsive user interface and search capabilities that are easy to understand. Moreover, the site does not make use of best practices established in the Digital Humanities community, such as providing a RESTful API or making use of Linked Open Data (LOD) technologies. Another issue the Census project is currently facing is the fact that the website runs on a proprietary digital asset management system (easydb 4) which handles data entry, retrieval front- and back-end. The support for easydb 4 will be running out shortly. In order to address the issues of a) openness, b) usability and c) maintainability, the we are currently currently evaluating how to port its data and research supporting functionalities in the coming two years into an open source-based system with LOD capabilities that also provides a modern user experience.
            In the beginning of the evaluation process, the Census project looked at solutions of other projects in the domain history that seem to fit the requirements mentioned above. While researching and speaking to other members of the Digital Humanities and art history community, we identified the following software solutions as possible contenders for the future of the Census project:
            
                
                    
                        Software
                    
                    
                        Description
                    
                    
                        Developer / Maintainer
                    
                
                
                    conedaKor
                        
                            
                        
                    
                    Open source web application for storing arbitrary entity types and interconnect them.
                    coneda (Germany)
                
                
                    Omeka-S
                        
                            
                        
                    
                    Open source web publishing platform and content management system for cultural heritage collections with LOD in mind
                    Roy Rosenzweig Center for History and New Media (USA); George Mason University (USA)
                
                
                    researchSpace
                        
                            
                        
                    
                    (Partly) open source Semantic Web environment for research and collaboration
                    The British Museum (UK); Metaphacts (Germany)
                
                
                    WissKI
                        
                            
                        
                    
                    Drupal extension for annotating arbitrary data using LOD Data in a CMS-based research environment
                    Germanisches Nationalmuseum (Germany)
                
                
                    
                        arches
                        
                            
                                
                            
                        
                    
                    
                        Open source data management system for (cultural) heritage data
                    
                    
                        Getty Conservation Institute (USA)
                    
                
                
                    
                        easydb 5
                        
                            
                                
                            
                        
                    
                    
                        Closed source successor of the current Census system with open source extensions
                    
                    
                        Programmfabrik (Germany)
                    
                
            
            We established a catalog of criteria to test these system against, taking inspirations from 
                
                Jackson et al. (2011)
                
                and 
                
                Knodel and Naab (2016)
                
                 while also taking advice from other members of the Digital Humanities community. This list includes criteria and questions such as:
            
            
                How easy is it to re-use the current data model of the Census in the new system?
                Is the new system easy to understand and handle for users and developers?
                Does the system have built-in LOD capabilities?
                Can the new system be installed and deployed easily?
                Can you extend the new system’s front-end and back-end components without breaking upgradeability?
                Is the new system available as open source software and is there (commercial) support available?
                How big is the user community for the new software?
            
            While Omeka-S, researchSpace, WissKI and arches are built with Semantic Web technologies in mind, conedaKOR just focuses on employing a non-RDF-based graph model. When comparing the systems regarding usability and maintainability, Omeka-S offered the best documentation, modern user interface with CMS functionalities as well a modular approach for extending its source code. researchSpace impressed us with its software architectural design by only relying on a triple store and possibilities to visualize any data using React
                
                    
                 components. However, it turns out researchSpace is very hard to deploy and complicated to maintain on a source level.
            
            While testing all these systems, we noticed there would not be an easy plug-and-play solution to re-use the Census database. These system either require a specific yet generic data model and/or Semantic Web ontology. Thus, we would have to re-design the current structure of Census relational database and thereby risk losing important data and relations without even having re-implemented basic functions such as searching and data entry.
            Instead of settling on a holistic system that covers database interaction, front-end, back-end and Linked Open Data, we had to rethink our approach to a new software architecture for the Census: We intend to establish a modular software architecture revolving around a RESTful LOD-API. Having a well documented API, e.g. in form of an OpenAPI specification
                
                    
                , allows us to build front-end components using that API endpoint for presentation and research, while also developing a back-end system that handles data base interactions and preparing the data for the API at the same time, In other words, having an API centric software architecture makes it programming language agnostic, making it easier to swap, extend and update front- and back-end components as well as the database if the need should arise, as long as the API still functions as specified.
            
            The Census project recently turned 35 years and aims to continue doing its research in the future. We conclude to not adapt the next “one size fits all” solution for the Census database, and instead focus on establishing a modular approach to remain flexible for future technologies and best practices.
        
        
            
                
                    Bibliography
                    
                        Jackson, M., Crouch, S. and Baxter, R. (2011). 
                        Software Evaluation: Criteria-Based Assessment. Software Sustainability Institue 
                        .
                    
                    
                        Knodel, J. and Naab, M. (2016). 
                        Pragmatic Evaluation of Software Architectures. (The Fraunhofer IESE Series on Software and Systems Engineering). Springer International Publishing 
                        //www.springer.com/de/book/9783319341767 (accessed 27 November 2018).
                    
                
            
        
    

      
         
            The goal of Bibliotheca Hertziana's project "Historical spaces in texts and maps" is to investigate the relations between historic geographical texts and maps to reconstruct a historical understanding of space and the knowledge associated with it. Starting with a cognitive-semantic analysis of Flavio Biondo's "Italia Illustrata" (1474), first of all, toponyms, place descriptions and spatial relations are annotated in the text and Renaissance maps. Our contribution to Spatial Humanities is based on the conviction that all maps are cognitive maps, depicting culture-specific spatial knowledge and practices (Goerz et al., 2018, 2019).
         
         
In general, our research combines
cognitive-semantic parameters such as toponyms, landmarks, spatial
frames of reference, geometric relations, gestalt principles and
different perspectives with computational linguistic analysis
(Thiering, 2015). We designed a workflow comprising the steps of
transcription, annotation, geographic verification, export and
ontology-based semantic enrichment of these data, finally stored and
published as Linked Open Data. We use Recogito
(https://recogito.pelagios.org,
15.09.2019) as our main tool for static annotations of places and
persons/peoples in text and maps. Toponyms are georeferenced with
gazetteers, in our case primarily with Pleiades
(https://pleiades.stoa.org,
15.09.2019), and the annotations can be exported in various formats,
in particular, CSV, GeoJSON, and KML. Spatial relations in texts are
annotated in terms of the cognitive-semantic parameters by means of
the brat tool. These annotation data are mapped into triples encoding
cognitive parameters, primarily in "figure-spatial_relation-ground"
constructions. Furthermore, dependency parsing
(http://ufal.mff.cuni.cz/udpipe, 15.09.2019) has been applied to the text for comparison. To achieve a generic semantic level for linguistic and map-related annotations, we perform a transition to an ontology-based representation. For this purpose, we defined a domain ontology 
hmap
for historical maps and geographical texts based on the event-centered
CIDOC Conceptual Reference Model (CRM, ISO standard 21127) and its
spatio-temporal extension CRMgeo in OWL-DL
(http://erlangen-crm.org,
15.09.2019). Using the CRM opens up a wide spectrum of
interoperability and linking to many web resources.  The domain
ontology 
hmap
            for the description of historical maps and their content offers a framework for the general metadata of maps and geographical texts as well as for descriptions of their content.
         
         
            As Linked Data platform we chose the Virtual Research Environment WissKI (Scholz et al., 2016; 
            http://wiss-ki.eu, 15.09.2019),
a semantic database extension of the CMS Drupal, in which we defined
our data model in terms of so-called ontology paths. These are
sequences of triples built from entities and properties of the
ontology. As an example, in a map production event
(hmap:M9_Map_Production) there is an actor, the Creator, defined by

         
  hmap:M28_Map --> hmap:A3i_was_produced_by
   -->   hmap:M9_Map_Production --> hmap:A4_carried_out_by_map_author
  --> hmap:M1_Map_Author --> ecrm:P131_is_identified_by
  --> ecrm:E82_Actor_Appellation.

         
For each map we may have several images, in which depicted objects are
annotated; so there is an analogous data model for images
(hmap:M34_Image). What the image depicts, in our case annotated places, is specified by

         
  hmap:M34_Image --> hmap:A43_depicts -->
  hmap:M3_Annotated_Place --> ecrm:P1_is_identified_by --> ecrm:E42_Identifier.

         
  For each annotated place
  (hmap:M3_Annotated_Place is a subclass of 
crmgeo:SP6_Declarative_Place) where
the (geographical) contents of the annotations are encoded in the
columns of the CSV tables, each column is transformed into a component
for which similar ontology paths are defined. The annotated place is
linked to the image by 

         
  hmap:M3_Annotated_Place --> hmap:A43i_is_depicted by -->
  hmap:M34_Image
  --> ecrm:P48_has_preferred_identifier --> ecrm:E42_Identifier.

         
            So, e.g., for 
            QUOTE_TRANSCRIPTION, the path is

         
  hmap:M3_Annotated_Place --> ecrm:P87_is_identified_by --> hmap:M42_Transcribed_Place_Appellation

         
            Each annotation, represented as a row in the table, has a unique ID (UUID) and refers, if geographically verified with a gazetteer (Pleiades), via a URL to a graph containing various information such as e.g. sources, archeological data, images, etc. In some maps, annotated places are additionally represented by a visual item (E36) such as a church, a tower, or a wall. There are also further data models for image series and works like map collections or atlases. From these paths, WissKI generates automatically input forms for map and text metadata and provides an interface for importing all table-formatted annotations and converting them into triples. Ontological enrichment of our data with CRM allows for a semantic interpretation of annotations such that, e.g., for each PlaceName, an instantiated CRM description in RDF/OWL triple format is generated and stored in a triple store. Using the semantically enriched geo-information from text (and map) annotations as CRM instances, spatial entities ("figure", "ground") and relations obtained by spatial role labeling as "figure-spatial_relation-ground" triples can now be upgraded to this rich semantic level by linking data. Due to the fundamental underlying triple structure for all kinds of annotations, the data are immediately ready for publication as standardized Linked (Open) Data; WissKI provides a SPARQL query interface. These triple data constitute a huge knowledge graph; they are the "raw material" for further research steps, i.e. the exploration of the historical understanding of spaces and the associated knowledge. Interpretation of the data has just begun: Actually, a study of Biondo’s spatial language – comprising the whole text for the first time – by Berthele and Thiering is being finalized (2020) and a comparative overview of the toponyms in the Latium book with different maps (traditional and "modern“ Ptolemaic maps, genuine maps of Italy and portolans) as well as a representation of the hodological dimension of the text is being prepared.
         
      
      
         
            
               Bibliography
               
                  Blakemore, Michael / Harley, Brian J. (1980): Concepts in the History of Cartography - A Review and Perspective. In: Cartographica. International Publications on Cartography, 17/4, Monograph 26. University of Toronto Press: Toronto.
                    
               
                  Bodenhamer, David J. / Corrigan, John / Harris, Trevor M. (eds.) (2010): The Spatial Humanities. GIS and the Future of Humanities Scholarship. Bloomington & Indianapolis: Indiana University Press.
                    
               
                  Görz, Günther / Geus, Klaus / Michalsky, Tanja / Thiering, Martin (2018): Spatial Cognition in Historical Geographical Texts and Maps: Towards a cognitive-semantic analysis of Flavio Biondo's 
                        “Italia Illustrata'', e-perimetron 13,4: 182-199.
                    
               
                  Görz Günther / Seidl Chiara / Thiering, Martin (2019): Linked Biondo: Modelling Geographical Features in Renaissance Texts and Maps
                        . In: Boutoura, Ch., Tsorlini, A., Livieratos, E. (Ed.): Proceedings 14th ICA Conference 
                        Digital Approaches to Cartographic Heritage, Thessaloniki, 8-10 May 2019.International Cartographic Association, Commission on Cartographic Heritage into the Digital. AUTH CartoGeoLab, ISSN 2459-3893, 124-140. 
                    
               
                  Michalsky, Tanja / Thiering, Martin (2020): 
                        Walking through history. An interdisciplinary approach to Flavio Biondo’s spaces in the “Italia illustrata”. Rome: Bibliotheca Hertziana.
               
               
                  Scholz, Martin / Merz, Dorian / Goerz, Günther (2016): Working with WissKI - A Virtual Research Environment for Object Documentation and Object-Based Research. Digital Humanities 2016, Conference Abstracts, Krakow, 11-16 July 2016. ISBN 978-83-942760-3-4, 944-945. 
                    
               
                  Thiering, Martin (2015): Spatial Semiotics and Spatial Mental Models: Figure-Ground Asymmetries in Language. Berlin: De Gruyter Mouton.
                    
            
         
      
   



        
            
                
                    Project Endings and static websites
                
                It has long been recognized that building DH web applications which do not have perpetual funding on complex computing stacks that require regular updates presents an overwhelming problem for long-term maintenance and archivability (Nowviskie and Porter 2010; Dombrowski 2019; Smithies et al. 2019). Project Endings is a collaboration between DH scholars, librarians and programmers, aiming to create tools and recommendations for building extremely low-maintenance, easily archivable, but still highly functional digital edition projects (Goddard 2018; Holmes and Takeda 2019a). Starting in 2016, the Endings team have converted a number of high-traffic, well-known digital edition projects that previously ran on XML databases and similar back-end infrastructure into entirely static websites
                    
                        See 
                             for the full list of staticized projects.
                        
                     built with HTML5, CSS and client-side JavaScript (Holmes 2017; Arneil, Holmes and Newton 2019). We have also developed a set of Principles
                    
                        
                            .
                        
                     to guide us in converting existing projects and creating new ones, as well as a client-side pure-JavaScript search engine, staticSearch
                    
                        
                            .
                        
                     (Holmes and Takeda 2020a, 2020b).
                
                At the outset, we assumed that only relatively small digital editions would be suitable candidates for complete staticization, and we began with sites consisting of only a few hundred pages.
                    
                        See for example 
                            My Norse Digital Image Repository (
                            ) or 
                            The Robert Graves Diary (
                            ).
                        
                     However, seeing how smoothly the process worked with smaller sites, we took on some of our larger projects, including The Map of Early Modern London (13,086 pages), The Colonial Despatches (10,826 pages), and Digital Victorian Periodical Poetry (20,685 pages). The results were very encouraging: the new sites were faster and more responsive than the old, and the staticSearch engine performed very effectively even at these scales.
                    
                        See 
                            ; 
                            ; 
                            .
                        
                    
                
                But there must, presumably, be some practical limits on the scale of a DH project which can be effectively converted to the static model, and it would be helpful to know where those limits might be. We reviewed our own catalogue and discovered a candidate which appears to be precisely the kind of project that would be suitable for testing this: 
                    VIHistory.
                
            
            
                
                    The 
                    VIHistory project
                
                
                    VIHistory (
                    ), a PostgreSQL/PHP project created about 15 years ago, presents census data from the City of Victoria and Vancouver Island, from censuses taken in 1871, 1881, 1891, 1892, 1901, and 1911, comprising nearly 150,000 individual census records, along with associated tables of occupations, familial relationships, locations, addresses, religion, languages, nationalities and other associated concepts. In terms of the number of individual HTML pages required for a static site, it is between five and ten times the size of the largest project we have previously staticized. VIHistory has undergone successive infrastructure migrations over the years, and various features have broken as a result. A looming server migration will render it non-functional, so we must take action within a short period. Furthermore, a new dataset (an addendum to the 1901 census) is now available for addition to the collection. Rather than invest more time in patching the existing site, we are instead creating a static version, with a completion deadline of April 2022.
                
                Census data brings with it a range of challenges, particularly when multiple censuses are to be presented as an integrated dataset. From one census to another, the range of data collected will vary; ward boundaries change; and descriptors such as nationality, race, occupation, familial relationships and languages mutate as social and political norms evolve.
                    
                        See, for example, Stanger-Ross 2008, which discusses the evolution of ethnicity in census data.
                     The old version of the VIHistory site addressed these issues through a set of PostgreSQL views, which merged disparate datasets into a normalized form which could be queried more easily. One of our challenges will be to accomplish this in static form.
                
                Another challenge will be to devise an appropriate granularity for the HTML pages which constitute the site. We expect to create a single HTML page for every distinct census entry, but other pages will be constructed to bring together collections of similar features (people with the same occupation, on the same street, with the same nationality, etc.) in order to provide a useful browsing approach to the data (something lacking in the existing site, which has only a search interface). The search itself will need to be carefully constructed so that all the features of the existing site search are preserved, and we expect to add more search options too.
            
            
                
                    Plan, approach, and prospects
                
                We initially considered converting all the existing census data into XML as the first phase of the project, but we have not found any XML standard suitable for this historical census data.
                    We then considered converting the data directly into HTML5, but found
                    it more effective to design an intermediate custom XML schema, which
                    enables records from all the different censuses to be encoded in a
                    single flexible structure, and also permits us to apply datatype
                    constraints and catch errors. The XML is then converted into XHTML5 for
                    the website.
                
                
                    Each census entry page will present a standardized tabular view of the data from the census record, with explanations to clarify differences between census datasets.
                    Each “page” will have a condensed single-line title capturing essential data, used for display in search results and listings pages. 
                    Values for datapoints such as nationality, language and so on will be constrained by the schema, and all pages will be validated during the build process. 
                    Detailed diagnostics (Holmes and Takeda 2019b) will expose inconsistencies and errors in the dataset.
                    Metadata will be encoded in the HTML header and used to create search filters, but the text of the pages will also be indexed for a new full-text search feature (the old site search allows only metadata filters).
                
                We fully expect the conversion to succeed, but we also know that we will be pushing the practical limits of this approach, and will need to devise optimizations and workarounds to make the site, and particularly the static search engine, usable. Our paper will report on this work, and present recommendations on strategies and limitations for creating static resources on this scale. 
            
        
        
            
                
                    Bibliography
                    Arneil, Stewart, Martin Holmes, and Greg Newton. 2019. “Clearing the Air for Maintenance and Repair: Strategies, Experiences, Full Disclosure; Paper Three: Ruthless Principles for Digital Longevity.” Presented at Digital Humanities 2019, Utrecht, the Netherlands. 
                        .
                    
                    Dombrowski, Quinn. 2019. “Sorry for all the Drupal: Reflections on the 3rd anniversary of ‘Drupal for Humanists.’” Quinn Dombrowski (blog), November 8, 2019.
                        .
                    
                    Goddard, Lisa. 2018. “The Endings Project @ UVic: Concluding, Archiving, and Preserving Digital Projects for Long-Term Usability.” @Risk North 2: Digital Collections, Montreal, Canada. 
                        .
                    
                    Holmes, Martin. 2017. “Selecting Technologies for Long-Term Survival.” Presented at the SHARP Conference 2017: Technologies of the Book, Victoria, BC, Canada. 
                        .
                    
                    Holmes, Martin. 2021. “Using ODD for HTML.” 
                        The Journal of the Text Encoding Initiative. Text Encoding Initiative Consortium. 
                        .
                    
                    Holmes, Martin and Joseph Takeda. 2019a. “The Prefabricated Website: Who needs a server anyway?” Text Encoding Initiative Conference, Graz, Austria. 
                        .
                    
                    Holmes, Martin and Joseph Takeda. 2019b. “Beyond Validation: Using Programmed Diagnostics to Learn About, Monitor, and Successfully Complete Your DH Project.” 
                        Digital Scholarship in the Humanities. Oxford University Press/EADH. 
                        .
                    
                    Holmes, Martin, and Joey Takeda. 2020a. “Static Search: An Archivable and Sustainable Search Engine for the Digital Humanities.” Presented at the Digital Humanities Summer Institute (DHSI) Colloquium (#VirtualDHSI). [
                        ].
                    
                    Holmes, Martin, and Joey Takeda. 2020b. “Nine Projects, One Codebase: A Static Search Engine for Digital Editions.” Presented at the COLLABORATION Digital Humanities Conference, University of British Columbia / online. 
                        .
                    
                    Nowviskie, Bethany, and Dot Porter. 2010. “
                            Graceful Degradation: Results of the Survey
                        .” Presented at Digital Humanities 2010, King’s College, London. 
                        .
                    
                    Smithies, James, Carina Westling, Anna-Maria Sichani, Pam Mellen, and Arianna Ciula. 2019. “Managing 100 Digital Humanities Projects: Digital Scholarship & Archiving in King’s Digital Lab.” 
                        Digital Humanities Quarterly 13, no 1 (2019). 
                        .
                    
                    Stanger-Ross, Jordan. 2008. “Citystats and the History of Community and Segregation in Post-WWII Urban Canada.” 
                        Journal of the Canadian Historical Association 19, 2 (2008), 3-22. 
                        .
                    
                
            
        
    



        
            This workshop will go over how to complete an initial Linux server setup for use with the web. We will go over security, firewalls, HTTPS, and high availability. Administering one’s own server rather than relying on managed web hosting empowers researchers, teachers, and students by providing them with complete control over their web assets. The resulting setup can be used for webapps, static sites like Jekyll and Hugo, or more robust sites like WordPress, Omeka, Scalar, and Drupal. These will be ready for use with domain names. In addition to providing an entry point to the web, servers can also enable teams of researchers and students to collaborate on programming projects or access shared data. 
        
    



        
            This short paper introduces LEAF (the Linked Editorial Academic Framework virtual research environment), an enhanced and expanded collaborative editorial platform that supports a variety of digital scholarly projects through a pipeline of integrated tools for collaborative production and publication of scholarly and documentary collections. Funded through the Canada Foundation for Innovation and the Andrew W. Mellon Foundation, LEAF aims to address the challenges that face many who undertake and maintain large-scale collaborative DH projects now: namely, the need to ensure that these projects can remain operational and available to editors and audiences over the long-haul. It is only by sharing physical, software, and human infrastructures across institutions that this can be accomplished. In so doing we can support scalability, interoperability, and preservation while allowing for dynamic, iterative, and collaborative editing, and therefore ensure that our materials, collections, and editions will remain viable and accessible. The LEAF team aims to do this by integrating best practices for text encoding, annotation, and metadata standards. This short paper will report on the development of LEAF and the functionalities that it will provide. 
            The implementation, and dissemination of LEAF is built upon a collaboration to extend the Canadian Writing Research Collaboratory (CWRC) built by the Universities of Alberta and Guelph (Susan Brown) with Bucknell University (Diane Jakacki), and Newcastle University (James Cummings) as founding partners. This work enhances CWRC’s functionality through collaborative software development that will ultimately support multiple instances of the LEAF platform in Canada, the US, and the UK. At Bucknell, this work will inform the Liberal Arts Based Digital Edition Publishing Cooperative and the Bucknell Digital Press, funded by an Andrew W. Mellon Digital Publishing Cooperative Implementation grant that will support an expanding portfolio of peer-reviewed digital editions and edition clusters. 
            
                The LEAF platform combines hardware, software, and personnel. LEAF is being built on a solid foundation in terms of its data models, core functionality, and code management, so that it is positioned for extension and long-term sustainability. The platform is based on the Islandora 8 framework, which combines Drupal 8 with a Fedora 5 repository for long-term preservation. The LEAF repository will customize and enhance Islandora to enable digital humanities workflows and publication needs. Enhancements include an innovative web-based editing tool that allows users to employ TEI XML along with Web Annotation and IIIF standards-compatible Linked Open Data annotations that enhance discoverability and interoperability. 
            
            
                The founding LEAF institutions are collaborating to upgrade the existing CWRC environment and produce a fully modular platform that will also be hosted on Bucknell’s servers, further tested at Newcastle University, and offered as containerized open-source code freely available for download and installation by other institutions. In particular, LEAF will facilitate the production and publication of dynamic digital scholarly editions and collections, offering multilingual transcription, translation, and image markup. Entirely browser-based, its functionality includes an in-browser XML markup editor, XML rendering tools, built-in text and data visualization tools including the Voyant Tools suite and its Dynamic Table of Contexts Browser. Overall the LEAF platform will provide a sophisticated interface for digital editions in which the XML markup is leveraged for navigation and active reading, and enhanced with Linked Open Data.
            
        
    

